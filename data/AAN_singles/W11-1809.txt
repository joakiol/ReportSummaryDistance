Proceedings of BioNLP Shared Task 2011 Workshop, pages 56?64,Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational LinguisticsBioNLP shared Task 2011 - Bacteria BiotopeRobert Bossy1, Julien Jourde1, Philippe Bessi?res1, Maarten van de Guchte2,Claire N?dellec11MIG UR1077 2Micalis UMR 1319INRA, Domaine de Vilvert78352 Jouy-en-Josas, Franceforename.name@jouy.inra.frAbstractThis paper presents the Bacteria Biotopetask as part of the BioNLP Shared Tasks2011.
The Bacteria Biotope task aims atextracting the location of bacteria fromscientific Web pages.
Bacteria location is acrucial knowledge in biology for phenotypestudies.
The paper details the corpusspecification, the evaluation metrics,summarizes and discusses the participantresults.1 IntroductionThe Bacteria Biotope (BB) task is one of the fivemain tasks of the BioNLP Shared Tasks 2011.
TheBB task consists of extracting bacteria locationevents from Web pages, in other words, citationsof places where a given species lives.
Bacterialocations range from plant or animal hosts forpathogenic or symbiotic bacteria, to naturalenvironments like soil or water.
Challenges forInformation Extraction (IE) of relations in Biologyare mostly devoted to the identification of bio-molecular events in scientific papers where theevents are described by relations between namedentities, e.g.
genic interactions (N?dellec, 2005),protein-protein interactions (Pyysalo et al, 2008),and more complex molecular events (Kim et al,2011).
However, this far from reflects the diversityof the potential applications of text mining tobiology.
The objective of previous challenges hasmostly been focused on modeling biologicalfunctions and processes using the information onelementary molecular events extracted from text.The BB task is the first step towards linkinginformation on bacteria at the molecular level toecological information.
The information onbacterial habitats and properties of these habitats isvery abundant in literature, in particular inSystematics literature (e.g.
International Journal ofSystematic and Evolutionary Microbiology),however it is rarely available in a structured way(Hirschman et al, 2008; Tamames and de Lorenzo,2009).
The NCBI GenBank nucleotide isolationsource field (GenBank) and the JGI GenomeOnLine Database (GOLD) isolation site field areincomplete with respect to the microbial diversityand are expressed in natural language.
The twocritical missing steps in terms of biotopeknowledge modeling are (1) the automaticpopulation of databases with organism/locationpairs that are extracted from text, and (2) thenormalization of the habitat name with respect tobiotope ontologies.
The BB task mainly aims atsolving the first information extraction issue.
Thesecond classification issue is handled through thecategorization of locations into eight types.2 ContextAccording to NCBI statistics there are nearly 900bacteria with complete genomes, which accountfor more than 87% of total complete genomes.Consequently, molecular studies in bacteriologyare shifting from species-centered to full diversityinvestigation.
The current trend in high-throughputexperiments targets diversity related fields,typically phylogeny or ecology.
In this context,adaptation properties, biotopes and biotopeproperties become critical information.
Illustrativequestions are:56?
Is there a phylogenetic correlation betweenspecies that share the same biotope??
What are common metabolic pathways ofspecies that live in given conditions, especiallyspecies that survive in extreme conditions??
What are the molecular signaling patterns inhost relationships or population relationships(e.g.
in biofilms)?Recent metagenomic experiments producemolecular data associated with a habitat rather thana single species.
This raises new challenges incomputational biology and data integration, suchas identifying known and new species that belongto a metagenome.Not only will these studies requirecomprehensive databases that associate bacterialspecies to their habitat, but they also require aformal description of habitats for propertyinference.
The bacteria biotope description ispotentially very rich since any physical object,from a cell to a continent, can be a bacterialhabitat.
However these relations are much simplerto model than with general formal spatialontologies.
A given place is a bacterial habitat ifthe bacteria and the habitat are physically incontact, while the relative position of the bacteriaand its dissemination are not part of the BB taskmodel.The BB Task requires the locations to beassigned different types (e.g.
soil, water).
We viewlocation typing as a preliminary step of more fine-grained modeling in location ontologies.
Someclassifications for bacteria biotopes have beenproposed by some groups (Floyd et al, 2005;Hirschman et al, 2008; Field et al, 2008;Pignatelli et al, 2009).
The Environment Ontologyproject (EnvO) is developing an ambitious detailedenvironment ontology for supporting standardmanual annotation of environments of all types oforganisms and biological samples (Field et al,2008).
In a similar way, the GOLD group at JGIdefined a standard classification for bacteriapopulation metagenome projects.
Developingmethods for the association of such biotope classesto organisms remains an open question.
EnvDB(Pignatelli et al, 2009) is an attempt to inventoryisolation sources of bacteria as recorded inGenBank and to map them to a three levelhierarchy of 71 biotope classes.
The assignment ofbacterial samples in one of the EnvDB classes issupported by a text-mining tool based on a Na?veBayes (NB) classifier applied to a bag of wordsrepresenting the associated reference title andabstract.
Unfortunately, the low number of paperreferences associated with the isolation source field(46 %) limits the scope of the method.The BB task has a similar goal, but directlyapplies to natural language texts thus avoiding theissue of database incompleteness.
As opposed todatabase-based approaches, biotope informationdensity is higher but the task has to includebacteria and location identification, as well asinformation extraction to relate them.The eight types of locations in the BB taskcapture high-level information for further ontologymappings.
The location types are Host, HostPart,Geographical and Environmental.
Environmentalis broadly defined to qualify locations that are notassociated to hosts, in a similar way to what wasdescribed by Floyd et al (Floyd et al, 2005).
Inaddition, the BB task types exclude artificiallyconstructed biotopes (e.g.
bacteria growing in labson a specific medium) and laboratory mutantbacteria.
The Environmental class is divided intoFood, Medical, Soil and Water.
Locations that arenone of these subtypes are classified asEnvironmental.The exact geographical location (e.g.
latitudeand longitude coordinates) has less importancehere than in eukaryote ecology because most of thebiotope properties vary along distances smallerthan the precision of the current positioningtechnologies.
Geographical names are only usefulin bacteria biotope studies when the physico-chemical properties of the location can be inferred.For the sake of simplicity, the locations of bacteriahost (e.g.
the stall of the infected cow) are nottaken into account despite their richness (Floyd etal., 2005).The important information conveyed by thelocations, especially of Environment type, is thefunction of the bacterium in its ecosystem ratherthan the substance of the habitat.
Indeed the finalgoal is to extract habitat properties and bacteriaphenotypes.
Beyond the identification of locations,their properties (e.g.
temperature, pH, salinity,oxygen) are of high interest for phenotypes (e.g.thermophily, acidophily, halophily) and trophismstudies.
This information is difficult to extract, andis often incomplete or even not available in papers(Tamames and de Lorenzo., 2009).
Hopefully,some properties can be automatically retrieved57with the help of specialized databases, which givethe physico-chemical properties of locations, suchas hosts (plant, animal, human organs), soils (seeWebSoilSurvey, Corine Land Cover), water, orchemical pollutants.From a linguistic point of view, the BB taskdiffers from other IE molecular biology tasks whileit raises some issues common to biomedicine andmore general IE tasks.
The documents arescientific Web pages intended for non-experts suchas encyclopedia notices.
The information is densecompared to scientific papers.
Documents arestructured as encyclopedia pages, with the mainfocus on a single species or a few species of thesame genus or family.
The frequency of anaphoraand coreferences is unusually high.
The locationentities are denoted by complex expressions withsemantic boundaries instead of rigid designators.3 Task descriptionThe goal of the BB task is illustrated in Figure 1.Bifidobacterium longum .
This organism is found inadult humans  and formula fed infants  as a normalcomponent of gut  flora.Figure 1.
Example of information to be extractedin the BB Task.The entities to be extracted are of two maintypes: bacteria and locations.
They are text-boundand their position has to be predicted.
Relations areof type Localization between bacteria andlocations, and PartOf between hosts and host parts.In the example in Figure 1, Bifidobacteriumlongum is a bacterium.
adult humans and formulafed infants denote host locations for the bacteria.gut is also a bacteria location, part of the two hostsand thus of type host part.Coreference relations between entities denotingthe same information represent valid alternativesfor the relation arguments.
For example, the threetaxon names in Figure 2 are equivalent.The green sulfur bacteria  (GSB ; Phylum Chlorobi )are commonly found in aquatic environments .Figure 2.
Coreference example.The coreference relation between pairs ofentities is binary, symmetric and transitive.Coreference sets are equivalence sets defined asthe transitive closure of the binary coreferencerelation.
Their annotation is provided in thetraining and development sets, but it does not haveto be predicted in the test set.4 Corpus descriptionThe corpus sources are the following bacteriasequencing project Web pages:?
Genome Projects referenced at NCBI;?
Microbial Genomics Program at JGI;?
Bacteria Genomes at EBI;?
Microorganisms sequenced at Genoscope;?
Encyclopedia pages from MicrobeWiki.The documents are publicly available and quiteeasy to understand by non-experts compared toscientific papers on similar topics.
From the 2,086downloaded documents, 105 were randomlyselected for the BB task.
A quarter of the corpuswas retained for test evaluation.
The rest was splitinto train and development sets.
Table 1 gives thedistribution of the entities and relations per corpus.The distribution of the five document sources inthe test corpus reflects the distribution of thetraining set and no other criteria.
Food is thereforeunderrepresented.Training+Dev TestDocument 78 (65 + 13) 27 (26 %)Bacteria 538 121 (18 %)Environment 62 16 (21 %)Host 486 101 (17 %)HostPart 217 84 (28 %)Geographical 111 25 (18 %)Water 70 21 (23 %)Food 46 0 (0 %)Medical 24 2 (8 %)Soil 26 20 (43 %)Coreference 484 100 (17 %)Total entities 1,580 39058Training+Dev TestLocalization 998 250 (20 %)Part of Host 204 78 (28 %)Total relations 1,202 328Table 1.
Corpus Figures.5 Annotation methodologyHTML tags and irrelevant metadata were strippedfrom the corpus.
The Alvis pipeline (N?dellec etal., 2009) pre-annotated the species names that arepotential bacteria and host names.
A team of 7scientists manually annotated the entities,coreferences and relations using the Cadixe XMLeditor (Cadixe).
Each document was processed bytwo independent annotators in a double-blindmanner.
Conflicts were automatically detected,resolved by annotator negotiation and irrelevantdocuments (e.g.
without bacterial location) wereremoved.
The remaining inconsistencies amongdocuments were resolved by the two annotatorsassisted by a third person acting as an arbitrator.The annotator group designed the detailedannotation guidelines in two phases.
First, theyannotated a set of 10 documents, discussed theoptions and wrote detailed guidelines withrepresentative and illustrative examples.
Duringthe annotation of the rest of the documents, newcases were discussed by email and the guidelinesamended accordingly.Location types.
The main issues under debatewere the definition of location types, boundaries ofannotations and coreferences.
Additionalannotation specifications concerned the exclusionof overly general locations (e.g.
environment,zone), artificially constructed biotopes and indirecteffects of bacteria on distant places.
For instance, adisease symptom occurring in a given host partdoes not imply the presence of the bacteria in thisplace, whereas infection does.
Boundaries of typeswere also an important point of discussion sincethe definite formalization of habitat categories wasat stake.
For instance we decided to exclude landenvironment citations (fields, deserts, savannah,etc.)
from the type Soil, and thus enforced a strictdefinition of soil bacteria.
The most controversialtype was host parts.
We decided to include fluids,secretions and excretions (which are not strictlyorgans).
Therefore, the host parts category requiredspecifications to determine at which point ofdissociation from the original host is a habitat not ahost part anymore (e.g.
mother?s milk vs. industrialmilk, rhizosphere as host part instead of soil).Boundaries.
The bacteria name boundaries donot include any external modifiers (e.g.
two A.baumannii strains).
Irrelevant modifiers oflocations are considered outside the annotationboundaries (e.g.
responsible for a hospitalepidemic).
All annotations are contiguous and spanon a single fragment in the same way as the otherBioNLP Shared Tasks.
This constraint led us toconsider cases where several annotations occurside by side.
The preferred approach was to haveone distinct annotation for each different location(e.g.
contact with infected animal products orthrough the air).
In the case of head or modifierfactorization, the annotation depends on theinformation conveyed by the factorized part.
If thehead is not relevant to determine the location type,then each term is annotated separately (e.g.tropical and temperate zones).
Conversely, if thehead is the most informative with regards to thelocation type, a single annotation spans the wholefragment (fresh and salt water).Coreferences.
Two expressions are consideredas coreferential and thus valid solution alternatives,if they convey the same information.
For instance,complete taxon names and non-ambiguousabbreviations are valid alternatives (e.g.
Borreliagarinii vs. B. garinii), while ambiguous anaphoraellipses are not (e.g.
as in ?[..]
infected withBorrelia duttonii.
Borrelia then multiplies [..]?
).The ellipsis of the omitted specific name(dutotonii) leaves the ambiguous generic name(Borrelia).The full guidelines document is available fordownload on the BioNLP Shared Task BacteriaBiotope page1.6 Evaluation procedure6.1 Campaign organizationThe training and development corpora with thereference annotations were made available to theparticipants by December 1st 2010 on the BioNLPShared Tasks pages together with the evaluationsoftware.
The test corpus, which does not contain1 https://sites.google.com/site/bionlpst/home/bacteria-biotopes/BioNLP-ST_2011_Bacteria_Biotopes_Guidelines.pdf59any annotation, was made available by March, 1st2011.
The participants sent the predictedannotations to the BioNLP Shared Task organizersby March 10th.
Each participant submitted a singlefinal prediction set.
The detailed evaluation resultswere computed, provided to the participants andpublished on the BioNLP website by March, 11th.6.2 Evaluation metricsThe evaluation metrics are based on precision,recall and the F-measure.
In the following section,the PartOf and Localization relations will both bereferred to as events.
The metrics measure theaccuracy of the participant prediction of eventswith respect to the reference annotation of the testcorpus.
Predicted entities that are not eventarguments are ignored and they do not penalize thescore.
Each event Er in the reference set is matchedto the predicted event Ep that maximizes the eventsimilarity function S. The recall is the sum of the Sresults divided by the number of events in thereference set.
Each event Ep in the predicted set ismatched to the reference event Er that maximizesS.
The precision is the sum of the S results dividedby the number of events in the predicted set.Participants were ranked by the F-score defined asthe harmonic mean between precision and recall.Eab, the event similarity between a referenceLocalization event a and a predicted Localizationevent b, is defined as:Eab = Bab .
Tab .
Jab?
Bab is the bacteria boundary component definedas: if the Bacterium arguments of both thepredicted and reference events have exactly thesame boundaries, then Bab = 1, otherwise Bab =0.
Bacteria name boundary matching is strictsince boundary mistakes usually yield adifferent taxon.?
Tab is the location type prediction componentdefined as: if the Location arguments of boththe predicted and reference events are of thesame type, then Tab = 1, otherwise Tab = 0.5.Thus type errors divide the score by two.?
Jab is the location boundary component definedas: if the Location arguments of the predictedand reference events overlap, then1?+=abbaab OVLENLENJwhere LENa and LENb are the length of theLocalization arguments of predicted andreference events, and OVab is the length of theoverlapping segment between the Localizationarguments of the predicted and referenceevents.
If the arguments do not overlap, then Jabis 0.
This formula is a Jaccard index applied tooverlapping segments.
Location boundarymatching is relaxed, though the Jaccard indexrewards predictions that approach the reference.For PartOf events between Hosts and HostParts,the matching score Pab is defined as: if the Hostarguments of the reference and predicted eventsoverlap and the Part arguments of the referenceand predicted events overlap, then Pab = 1,otherwise Pab = 0.
Boundary matching of PartOfarguments is relaxed, since boundary mistakes arealready penalized in Eab.Arguments belonging to the same coreferenceset are strictly equivalent.
In other words, theargument in the predicted event is correct if it isequal to the reference entity or to any item in thereference entity coreference set.7 Results7.1 Participating systemsThree teams submitted predictions to the BB task.The first team is from the University of Turku(UTurku); their system is generic and producedpredictions for every BioNLP Shared Task.
Thissystem uses ML intensely, especially SVMs, forentity recognition, entity typing and eventextraction.
UTurku adapted their system for the BBtask by using specific NER patterns and externalresources (Bj?rne and Salakoski, 2011).The second team is from the Japan AdvancedInstitute of Science and Technology (JAIST); theirsystem was specifically designed for this task.They used CRF for entity recognition and typing,and classifiers for coreference resolution and eventextraction (Nguyen and Tsuruoka, 2011).The third team is from Bibliome INRA; theirsystem was specifically designed for this task(Ratkovik et al, 2011).
This team has the sameaffiliation as the BB Task authors, however greatcare was taken to prevent communication on thesubject between task participants and the test setannotators.60The results of the three submissions according tothe official metrics are shown in Table 2.
Thescores are micro-averaged: Localization andPartOf relations have the same weight.
Given thenovelty and the complexity of the task, these firstresults are quite encouraging.
Almost half of therelations are correctly predicted.
The Bibliometeam achieved the highest F-measure with abalanced recall and precision (45%).Recall Precision F-scoreBibliome 45 45 45JAIST 27 42 33UTurku 17 52 26Table 2.
Bacteria Biotope Task results.7.2 Systems description and result analysisAll three systems perform the same distinct sub-tasks: bacteria name detection, detection andtyping of locations, coreference resolution andevent extraction.
The following description of theapproaches used by the three systems in eachsubtask will be supported by intermediate results.Bacteria name detection.
Interestingly the threeparticipants used three different resources for thedetection of bacteria names: the List of ProkaryoticNames with Standing in Nomenclature (LPNSN)by UTurku, names in the genomic BLAST page ofNCBI by JAIST and the NCBI Taxonomy byBibliome.Bibliome 84JAIST 55UTurku 16Table 3.
Bacteria entity recall.Table 3 shows a disparity in the bacteria entityrecall of participants.
The merits of each resourcecannot be deduced directly from these figures sincethey have been exploited in different manners.UTurku and JAIST systems injected the resourceas features in a ML algorithm, whereas Bibliomedirectly projected the resource on the corpus withadditional rule-based abbreviation detection.However there is some evidence that theresources have a major impact on the result.According to Sneath and Brenner (1992) LPNSNis necessarily incomplete.
NCBI BLAST onlycontains names of species for which a completegenome has been published.
The NCBI Taxonomyused by INRA only contains names of taxa forwhich some sequence was published.
It appearsthat all the lists are incomplete.
However, thebacteria referenced by the sequencing projects,which are mentioned in the corpus should all berecorded by the NCBI Taxonomy.Location detection and typing.
As stated before,locations are not necessarily denoted by rigiddesignators.
This was an interesting challenge thatcalled for the use of external resources andlinguistic analysis with a broad scope.UTurku and JAIST both used WordNet, asensible choice since it encompasses a widevocabulary and  is also structured with synsets andhyperonymy relations.
The WordNet entries wereinjected as features in the participant ML-basedentity recognition and typing subsystems.It is worth noting that JAIST also used wordclustering based on MEMM for entity detection.This method has things in common withdistributional semantics.
JAIST experimentsdemonstrated a slight improvement using wordclustering, but further exploration of this idea mayprove to be valuable.Alternatively, the Bibliome system extractedterms from the corpus using linguistic criteriaclassified them as locations and predicted theirtype, by comparing them to classes in a habitat-specific ontology.
This prediction uses bothlinguistic analysis of terms and the hierarchicalstructure of the ontology.
Bibliome also usedadditional resources for specific types: the NCBITaxonomy for type Host and Agrovoc countriesfor type Geographical.Bibliome JAIST UTurkuHost 82 49 28Host part 72 36 28Geo.
29 60 53Environment 53 10 11Water 83 32 2Soil 86 37 34Table 4.
Location entity recall by type.
Thenumber of entities of type Food and Medical in thetest set is too low to be significant.
The scores arecomputed using Tab and Jab.61The location entity recall in Table 4 shows thatBibliome consistently outperformed the othergroups for all types except for Geographical.
Thisdemonstrates the strength of exploiting a resourcewith strong semantics (ontology vs. lexicon) andwith mixed semantic and linguistic rules.In order to evaluate the impact of Location entityboundaries and types, we computed the final scoreby relaxing Tab and Jab measures.
We re-defined Tabas always equal to 1, in other words the type of thelocalization was not evaluated.
We also re-definedJab as: if the Location arguments overlap, then Jab =1, otherwise Jab = 0.
This means that boundarieswere relaxed.
The relaxed scores are shown inTable 5.
While the difference is not significant forJAIST and UTurku, the Bibliome results exhibit a9 point increase.
This demonstrates that theBibliome system is efficient at predicting whichentities are locations, while the other participantspredict more accurately the boundaries and types.Recall Prec.
F-score Diff.Bibliome 54 54 54 +9JAIST 29 45 35 +2UTurku 19 56 28 +2Table 5.
Participants score using relaxed locationboundaries and types.Coreference resolution.
The corpus exhibits anunusual number of anaphora, especially bacteriacoreferences since a single bacterium species isgenerally the central topic of a document.
TheBibliome submission is the only one thatperformed bacteria coreference resolution.
Theirsystem is rule-based and dealt with referential ?it?,bi-antecedent anaphora and more importantlysortal anaphora.
The JAIST system has a bacteriacoreference module based on ML.
However thesubmission was done without coreferenceresolution since their experiments did not showany performance improvement.Event extraction.
Both UTurku and JAISTapproached the event extraction as a classificationtask using ML (SVM).
Bibliome exploited the co-occurrence of arguments and the presence oftrigger words from a predefined list.
Both UTurkuand Bibliome generate events in the scope of asentence, whereas JAIST generates events in thescope of a paragraph.As shown in Table 6, UTurku achieved the bestscore for PartOf events.
For all participants, theprediction is often correct (between 60 and 80%)while the recall is rather low (20 to 32%).Recall Precis.
F-scoreHost 61 48 53Host part 53 42 47Geo.
13 38 19B.
Env.
29 24 26Water 60 55 57Soil 69 59 63Part-of 23 79 36Host 30 43 36Host part 18 68 28Geo.
52 35 42J.
Env.
5 0 0Water 19 27 23Soil 21 42 28Part-of 31 61 41Host 15 51 23Host part 9 40 15Geo.
32 40 36U.
Env.
6 50 11Water 1 7 2Soil 12 21 15Part-of 32 83 46Table 6.
Event extraction results per type.Conversely, the score of the Localization relationby UTurku has been penalized by its lowrecognition of bacteria names (16%).
This stronglyaffects the score of Localizations since thebacterium is the only expected agent argument.The good results of Bibliome are partly explainedby its high bacteria name recall of 84%.The lack of coreference resolution might penalizethe event extraction recall.
To test this hypothesis,we computed the recall by taking only into accountevents where both arguments occur in the samesentence.
The goal of this selection is to removemost events denoted through a coreference.
Therecall difference was not significant for Bibliomeand JAIST, however UTurku recall raised by 12points (29%).
That experiment confirms thatUTurku low recall is explained by coreferences62rather than the quality of event extraction.
Theparagraph scope chosen by JAIST probablycompensates the lack of coreference resolution.As opposed to Bibliome, the precision of theLocalization relation prediction by JAIST andUTurku, is high compared to the recall, with anoticeable exception of geographical locations.The difference between participants seems to becaused by the geographical entity recognition stepmore than the relation itself.
This is shown by thedifference between the entity and the event recall(Table 4 and 6 respectively)..
The worst predictedtype is Environment, which includes diverselocations, such as agricultural, natural andindustrial sites and residues.
This revealssignificant room for improvement for Water, Soiland Environment entity recognition.8 DiscussionThe participant papers describe complementarymethods for tackling BB Task?s new goals.
Thenovelty of the task prevents participants fromdeeply investing in all of the issues together.Depending on the participants, the effort wasfocused on different issues with variousapproaches: entity recognition and anaphoraresolution based on extensive use of backgroundknowledge, and relation prediction based onlinguistic analysis of syntactic dependencies.Moreover, these different approaches revealed tobe complementary with distinct strengths andlimitations.
In the future, one may expect that theintegration of these promising approaches willimprove the current score.The corpus of BioNLP BB Task 2011 consistsof a set of Web pages that were selected for theirreadability.
However, some corpus traits make theIE task more difficult compared to scientificpapers.
For example, the relaxed style of somepages tolerates some typographic errors (e.g.morrow instead of marrow) and ambiguousanaphora.
The genome sequencing projectdocuments aim at justifying the sequencing ofbacteria.
This results in abundant descriptions ofpotential uses and locations that should not bepredicted as actual locations.
Their correctprediction requires complex analysis of modalities(possibility, probability, negation).
Some pagesdescribe the action of hosted bacteria at themolecular level, such as cellular infection.
Termsrelated to the cell are ambiguous locations becausethey may refer to either bacteria or host cells.Scientific papers form a much richer source ofbacterial location information that is exempt fromsuch flaws.
However, as opposed to Web pages,most of them are not publicly available and theyare in PDF format.The typology of locations was designedaccording to the BB Task corpus with a strong biastowards natural environments since bioremediationand plant growth factor are important motivationsfor bacteria sequencing.
It could be necessary torevise it according to a broader view of bacterialstudies where pathogenicity and more generallyhuman and animal health are central issues.9 ConclusionThe Bacteria Biotope Task corpus and objectivesdiffer from molecular biology text-mining ofscientific papers.
The annotation strategy and theanalysis of the participant results contributed to theconstruction of a preliminary review of the natureand the richness of its linguistic specificities.
Theparticipant results are encouraging for the future ofthe Bacteria Biotope issue.
The degree ofsophistication of participating systems shows thatthe community has technologies, which are matureenough to address this crucial biology question.However, the results leave a large room forimprovement.The Bacteria Biotope Task was an opportunityto extend molecular biology text-mining goalstowards the support of bacteria biodiversity studiessuch as metagenomics, ecology and phylogeny.The prediction of bacterial location information isthe very first step in this direction.
The abundanceof scientific papers dealing with this issue anddescribing location properties form a potentiallyrich source for further extensions.AcknowledgmentsThe authors thank Valentin Loux for his valuablecontribution to the definition of the BacteriaBiotope task.
This work was partially supported bythe French Quaero project.63ReferencesJari Bj?rne and Taio Salakoski.
2011.
GeneralizingBiomedical Event Extraction.
Proceedings of theBioNLP 2011 Workshop Companion Volume forShared Task.Cadixe.
http://caderige.imag.fr/Articles/CADIXE-XML-Annotation.pdfCorine Land Cover.http://www.eea.europa.eu/themes/landuse/interactive/clc-downloadEnvDB database.
http://metagenomics.uv.es/envDB/EnvO Project.http://gensc.org/gc_wiki/index.php/EnvO_ProjectDawn Field [et al.
2008.
Towards a richer descriptionof our complete collection of genomes andmetagenomes: the ?Minimum Information about aGenome Sequence?
(MIGS) specification.
NatureBiotechnology.
26: 541-547.Melissa M. Floyd, Jane Tang, Matthew Kane and DavidEmerson.
2005.
Captured Diversity in a CultureCollection: Case Study of the Geographic andHabitat Distributions of Environmental Isolates Heldat the American Type Culture Collection.
Appliedand Environmental Microbiology.
71(6):2813-23.GenBank.
http://www.ncbi.nlm.nih.gov/GOLD.
http://www.genomesonline.org/cgi-bin/GOLD/bin/gold.cgiLynette Hirschman, Cheryl Clark, K. Bretonnel Cohen,Scott Mardis, Joanne Luciano, Renzo Kottmann,James Cole, Victor Markowitz, Nikos Kyrpides,Norman Morrison, Lynn M. Schriml, Dawn Field.2008.
Habitat-Lite: a GSC case study based on freetext terms for environmental metadata.
Omics.12(2):129-136.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo,Yoshinobu Kano, Jun?ichi Tsujii.
2010.
Extractingbio-molecular events from literature - the BioNLP?09shared task.
Special issue of the InternationalJournal of Computational Intelligence.MicrobeWiki.http://microbewiki.kenyon.edu/index.php/MicrobeWikiMicrobial Genomics Program at JGI.
http://genome.jgi-psf.org/programs/bacteria-archaea/index.jsfMicroorganisms sequenced at Genoscope.http://www.genoscope.cns.fr/spip/Microorganisms-sequenced-at.htmlClaire N?dellec.
2005.
Learning Language in Logic -Genic Interaction Extraction Challenge" inProceedings of the Learning Language in Logic(LLL05) workshop joint to ICML'05.
Cussens J. andN?dellec C. (eds).
Bonn.Claire N?dellec, Adeline Nazarenko, Robert Bossy.2008..Information Extraction.
Ontology Handbook.S.
Staab, R. Studer (eds.
), Springer Verlag, 2008.Nhung T. H. Nguyen and Yoshimasa Tsuruoka.
2011.Extracting Bacteria Biotopes with Semi-supervisedNamed Entity Recognition and CoreferenceResolution.
Proceedings of the BioNLP 2011Workshop Companion Volume for Shared Task.Miguel Pignatelli, Andr?s Moya, Javier Tamames.(2009).
EnvDB, a database for describing theenvironmental distribution of prokaryotic taxa.Environmental Microbiology Reports.
1:198-207.Prokaryote Genome Projects at NCBI.http://www.ncbi.nlm.nih.gov/genomes/lproks.cgiSampo Pyysalo, Antti Airola, Juho Heimonen, JariBj?rne, Filip Ginter and Tapio Salakoski.
2008.Comparative analysis of five protein-proteininteraction corpora.
BMC Bioinformatics.
vol 9.Suppl 3.
S6.Zorana Ratkovic, Wiktoria Golik, Pierre Warnier,Philippe Veber, Claire N?dellec.
2011.
BioNLP 2011Task Bacteria Biotope ?
The Alvis System.Proceedings of the BioNLP 2011 WorkshopCompanion Volume for Shared Task.Peter H. A. Sneath and Don J. Brenner.
1992.
?Official?Nomenclature Lists.
American Society forMicrobioloy News.
58, 175.Javier Tamames and Victor de Lorenzo.
2010.EnvMine: A text-mining system for the automaticextraction of contextual information.
BMCBioinformatics.
11:294.Web Soil Survey.
http://websoilsurvey.nrcs.usda.gov/64
