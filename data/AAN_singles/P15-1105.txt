Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1085?1094,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAutomatic Spontaneous Speech Grading: A Novel Feature DerivationTechnique using the CrowdVinay ShashidharAspiring Mindsvinay.shashidhar@aspiringminds.comNishant PandeyAspiring Mindsnishant.pandey@aspiringminds.comVarun AggarwalAspiring Mindsvarun@aspiringminds.comAbstractIn this paper, we address the problemof evaluating spontaneous speech us-ing a combination of machine learningand crowdsourcing.
Machine learningtechniques inadequately solve the statedproblem because automatic speaker-independent speech transcription isinaccurate.
The features derived from itare also inaccurate and so is the machinelearning model developed for speechevaluation.
To address this, we post thetask of speech transcription to a largecommunity of online workers (crowd).We also get spoken English grades fromthe crowd.
We achieve 95% transcriptionaccuracy by combining transcriptionsfrom multiple crowd workers.
Speechand prosody features are derived by forcealigning the speech samples on thesehighly accurate transcriptions.
Addi-tionally, we derive surface and semanticlevel features directly from the transcrip-tion.
To demonstrate the efficacy of ourapproach we performed experiments onan expert?graded speech sample of 319adult non?native speakers.
Using thesefeatures in a regression model, we areable achieve a Pearson correlation of0.76 with expert grades, an accuracymuch higher than any previously reportedmachine learning approach.
Our approachhas an accuracy that rivals that of expertagreement.
This work is timely giventhe huge requirement of spoken Englishtraining and assessment.1 IntroductionAutomatic evaluation of spoken English has beenof keen interest for more than two decades (Zech-ner et al, 2007; Neumeyer et al, 1996; Francoet al, 2000; Cucchiarini et al, 1997).
It canhelp learners get feedback in a scalable manner,help build better English training software andalso help companies and institutions filter and se-lect prospective employees more effectively.
Theproblem acquires significance given the evidencethat better English leads to better employment out-come, wages and promotions (Guven and Islam,2013).There has been a considerable success in auto-matically scoring spoken English, when the spo-ken text is known a priori (Cucchiarini et al,2000; Franco et al, 2000).
In these cases, thecandidate is asked to either read a given text orlisten to some speech and repeat it.
For thesetasks, the scores generated by an automatic sys-tem on parameters such as pronunciation and flu-ency closely mimic those given by human ex-perts.
The primary approach behind a majority ofthese systems is to force align the speech sampleon the known text using an HMM?based acous-tic model.
Features such as likelihood, posteriorprobability and fluency related features are derivedfrom the aligned speech and a machine learningmodel is used to predict expert grades (Neumeyeret al, 1996; Franco et al, 2000; Cucchiarini et al,1997).
Some approaches additionally use prosodyand energy related features (Dong et al, 2004).More recently, this research has moved towardsthe assessment of higher granularity metrics likethe mispronunciation of particular phonemes (Liet al, 2009; Ito et al, 2006; Koniaris and Engwall,2011).In spontaneous speech evaluation, the candidateis asked to speak on a topic or answer a questionand what he/she speaks isn?t known priori.
Evalu-ation of spontaneous speech is the ultimate test ofa candidate?s proficiency in speaking a language(Hagley, 2010; Halleck, 1995).
While scores fromthe evaluation of read/repeat speech do correlatewith spontaneous speech evaluation, there remains1085an unexplained variance in the spontaneous speechscores (see Section 5).
Generally, candidates whoscore high on spontaneous speech also score highon read speech and not vice versa.Given the primacy of spontaneous speech eval-uation in judging a person?s language capabil-ity, there is considerable interest in doing it au-tomatically (Cucchiarini et al, 1997; Dong etal., 2004).
Automated approaches for the samehave not worked well (Powers et al, 2002; Cuc-chiarini et al, 2000) primarily because speaker-independent speech recognition is a tough com-puter science problem.
This is exacerbated whenthe speakers are not proficient in the language orare non-natives (Powers et al, 2002).
Given thatspeech to text conversion for such candidates has alow accuracy, force alignment of the speech on thisinaccurate text makes the features and the modelinaccurate.We present a semi-automated approach to gradeshort duration (45 seconds) spontaneous speech.We accurately predict a holistic score which isbased on the pronunciation, fluency, content char-acteristics and grammar of the speech sample, asdetermined by experts.
Multiple previous studiesin language acquisition and second language re-search conclusively show that proficiency in a sec-ond language can be characterized by these factors(Bhat et al, 2014).
Being able to provide a holisticscore is of high interest in both educational test-ing (Zechner et al, 2009) and job related testing(Streeter et al, 2011).
Institutions and firms lookfor a holistic score, say based on CEFR, a stan-dard to describe spoken English assessment (Lit-tle, 2006; Little, 2007), to make an accept or rejectdecision on candidates.
Currently, an expert basedassessment is used for these purposes.Our method involves combining machine learn-ing with a crowdsourcing layer.
Crowdsourcing(Estell?es-Arolas and Gonz?alez-Ladr?on-de Gue-vara, 2012) is the process of getting human in-telligence tasks performed by a large communityof online workers (crowd) as opposed to tradi-tional employees.1The responses from the hu-man intelligence tasks are then used to create rel-evant features for machine learning.
Human in-1Our approach is different from peer grading (Lejkand Wyvill, 2001) or crowd grading (Van Houdnos, 2011;Tetreault et al, 2010; Madnani et al, 2011) approaches.These approaches directly ask the crowd to grade the re-sponse.
The primary feature of our technique is using thecrowd in the feature extraction step of machine learning.telligence tasks are defined as those which mosthumans find easy, but are hard for machines.
Forinstance, a classic example is the task of findinga particular object in an image.
There is a largeresearch community that uses crowdsourcing andhas demonstrated that it can help perform tasks in-expensively, in large volumes and within reason-able time (Howe, 2006; Whitla, 2009).Our system design for evaluation of sponta-neous speech is illustrated in Figure 1.
We postthe task2of speech transcription to the crowd.
Weget a final accurate transcription by combining thetranscriptions from more than one crowd workerfor the same speech sample.
Once we have thisaccurate transcription, we force-align (Erling andSeargeant, 2013; Sj?olander, 2003) the speech ofthe candidate on this text to derive various featureswhich go into a machine learning engine.
We alsocollect spoken English grades of the speech fromthe crowd (Lejk and Wyvill, 2001), which are usedas additional features.
With these accurately iden-tified features and crowd grades, machine learningis able to grade spontaneous speech with high ac-curacy.
We found that this approach does muchbetter than a pure machine learning approach.Crowdsourcing has been used for almost adecade in various problems in speech analysis,grading and language learning (Kunath and Wein-berger, 2010; Peabody, 2011; Wang et al, 2014).Within assessment of speech, currently all suchapproaches use the crowd to directly grade cer-tain parts of the speech (Wang and Meng, 2012).Our work is uniquely positioned where we use thecrowd to do accurate transcription, a human intel-ligence task, and use it in a machine learning basedalgorithm.3We show that such a system providesan accuracy rivaling that of experts.In this paper, we solve a hitherto unsolved prob-lem of spontaneous speech evaluation (Zechner etal., 2009).
The paper makes the following contri-butions:?
We show that spoken English can be gradedwith accuracy by combining machine learn-ing and crowdsourcing higher than a puremachine learning approach.2Even though speaker-independent speech recognition isa hard problem for machines, it is fairly easy for a nativespeaker or anyone with reasonable command over the lan-guage.3Again, speech transcription has been done previously us-ing crowdsourcing (Zaidan and Callison-Burch, 2011), butnot used for a grading purpose or combined with machinelearning.1086Figure 1: System Design?
We show that the features derived fromcrowdsourced transcriptions perform as wellas crowd grades in predicting expert grades.However, crowd grades add additional pre-dictive value.?
We propose a scalable and accurate way toperform evaluation of spontaneous speech, ahuge requirement in the industry and else-where.The paper is organized as follows?
Section 2describes the procedure and aim of the speechassessment task; Section 3 describes the featureclasses used in the prediction algorithm; Section4 describes the crowdsourcing framework whichis used as an input to machine learning meth-ods; Section 5 demonstrates how this frameworkis used with machine learning techniques to pre-dict a composite spoken English score; Section 6discusses the future work and concludes the paper.2 Grading TaskWe want to assess the quality of spoken Englishof candidates based on their spontaneous speechsamples.
The speech samples of the candidateswere collected using Aspiring Minds?
automatedspeech assessment tool?
SVAR (SVAR, 2014).SVAR is conducted over phone as well as on acomputer.
The test has multiple sections wherethe candidate is required to: read sentences aloud,listen and repeat sentences, listen to a passage orconversation and answer multiple choice questionsand finally spontaneously speak on a given topic.In the spontaneous speech section, the candidates4are provided with a topic and given 30 seconds5tothink, take notes and then speak on the topic for45 seconds.
The topic is repeated to ensure taskclarity.
The complete test takes 16-20 minutes tocomplete, depending on the test version.Currently, SVAR evaluates speech samplesfrom the read and repeat sections with high accu-racy (SVAR, 2014).
Our goal in this paper is toevaluate the spontaneous speech of the candidateand provide a composite score based on it.A 5 point rubric for the composite score, similarto CEFR (Examinations, 2011), was prepared withthe help of experts.
This score is a function of thepronunciation, fluency, content organization andgrammar quality of the speech sample.
Broadlyspeaking, Pronunciation (Dobson, 1957) refers tothe correctness in the utterance of the phonemesof a word by the students as per neutral accent.Fluency (Brumfit and Brumfit, 1984) refers to adesired rate of speech along with the absence ofhesitations, false starts and stops etc.
Content or-ganization (Stalnaker, 1999) measures the candi-date?s ability to structure the information disposi-tion and present it coherently.
Grammar (Brazil,1995) measures how well the syntax of the lan-guage was followed by the candidate.4The subjects of our study use English as their second lan-guage and hail from various backgrounds, dialects and edu-cational qualifications.5This is as per global standards of spoken English assess-ment.
High stake tests such as TOEFL provide the candidate15-30 seconds to think before responding to a spontaneousspeech task.1087Figure 2: Our intuition of how different featurespredict the holistic score.In the next section we discuss the features whichare used in the prediction algorithm.3 FeaturesWe use three classes of features?
Crowd Grades(CG), Force Alignment features (FA) and Natu-ral Language Processing features (NLP).
The spo-ken English samples are posted to the crowd to getthe transcription and spoken English grades (Fig-ure 1).
Each task was completed by three workers.The crowd grades become one set of features.
Asecond set, i.e., FA features, are derived by align-ing (Erling and Seargeant, 2013; Sj?olander, 2003)the speech sample on the crowdsourced transcrip-tions.
A third set, i.e., NLP features, are also de-rived from the crowdsourced text.
These are ex-plained in the succeeding paragraphs.?
Crowd Grades: The crowd transcribes thespeech in addition to providing scores oneach of the following?
pronunciation, flu-ency, content organization and grammar.These grades are combined to form a com-posite score per worker per candidate.
Theseare further averaged across workers to give afinal score.6?
FA features: The speech sample is forcedaligned (Erling and Seargeant, 2013;Sj?olander, 2003) on the crowdsourced tran-scription using the HTK speech recognizer(Young et al, 2006).
We used an acousticmodel based on TIMIT (Garofolo et al,1993) for our experiments.
TIMIT is a6Advanced Expectation-Maximization techniques (Hos-seini et al, 2012) may also be used for an aggregation strat-egy, once the number of tasks done by every individualworker increases.
In our current experiments, this numberwasn?t very high.corpus of phonemically and lexically tran-scribed speech of American English speakersof different sexes and dialects.A number of speech quality features are de-rived, which include?
rate of speech, posi-tion and length of pauses, log likelihood ofrecognition, posterior probability, hesitationsand repetitions etc.
These features are wellknown in literature and may be referred from(Neumeyer et al, 1996; Zechner et al, 2009;Cucchiarini et al, 2000).
These features arepredictive of the pronunciation and fluency ofthe candidate.?
NLP features: These features predict the con-tent quality and grammar of the spoken con-tent7.
They were derived using standard NLPpackages (LightSide, 2013; AfterTheDead-line, 2014) on the crowdsourced transcrip-tion.
The package calculates surface levelfeatures such as the number of words, com-plexity or difficulty of words and the num-ber of common words used.
It also calculatessemantic features like the coherency in text,context of the words spoken, sentiment of thetext and grammar correctness.
In the currentsystem, we do not use any prompt specificfeatures such as occurrence of specific wordsor phrases.
These features are predictive ofthe grammar and content organization of thesample.All the features described above were obtainedfor the spontaneous speech sample.
We alsoderived features similar to FA features for thecandidate?s read and repeat speech samples col-lected during his/her SVAR test.
The speech andprosody features are calculated by force aligningthe speech on the known text.
One of the mod-els (RS/LR) in our experiments is based on thesefeatures and has been included for comparison.These features do not have any bearing on our finalmodel for spontaneous speech evaluation.4 CrowdsourcingThe spoken English sample was given to thecrowd to transcribe and provide grades.
The taskwas posted on a popular crowdsourcing platform?Amazon Mechanical Turk (AMT) (Paolacci et al,7We were looking at prompt independent features only, atthis point.10882010).
AMT is a popular crowdsourcing market-place.
It is inspired by the famous 18thcentury au-tomated chess playing machine, running on the in-telligence of a hidden human operator.
It has morethan 500, 000 online workers from 190 countries(Turk, 2014).
One can post tasks on the platformonline and offer fixed remuneration for their com-pletion.A clean and simple interface was provided tothe worker with standard features needed for tran-scription.
Additionally, an advanced audio playerwas embedded with the ability to play the speechsample in repeat mode, rewind and forward, apartfrom standard play/pause functionality to help theworker.
The different transcriptions were com-bined using the ROVER algorithm (Fiscus, 1997).ROVER is a sophisticated voting algorithm tocombine multiple transcriptions with errors, to ob-tain the best estimate of the correct transcription.It is reported to lead to an error reduction of 20-25%.
ROVER proceeds in two stages: first theoutputs are aligned and a single word transcriptionnetwork (WTN) is built.
The second stage consistsof selecting the best scoring word (with the highestnumber of votes) at each node.Several methods have been used in the past forincreasing the reliability of the grades given by thecrowd by identifying and correcting any biases andremoving non-serious/low quality workers (Akeret al, 2012).
One of the key techniques for thisinvolves inserting gold standard tasks with knownanswers to get an estimate of the worker?s abil-ity (Nguyen et al, 2013).
The gold standard tasksare similar to real tasks and the workers have noway to distinguish between the two.
Our taskstook workers a reasonable amount of time (8-10minutes).
It wasn?t hence feasible to insert a goldstandard task, as done typically, with every task tobe completed.To overcome this problem, we propose an in-novative approach where a risk is assigned to aworker based on his/her performance on the goldstandard tasks.
We conceptualized this system asa state machine that determines the risk level ofa worker and proposes actions based on it (Re-fer to Figure 3).
All workers started with an ini-tial risk level of 0.2.
Gold standard tasks wereprobabilistically inserted among real tasks basedon the worker?s risk level.
Workers with a higherrisk level saw more gold standard tasks.
Also,the risk level of the worker was updated based onFigure 3: Risk Level State Diagram: In the abovefigure, each node corresponds to a risk level asso-ciated with a worker.
The values range between0 (min) - 1 (max).
The worker is either assigneda gold standard task (G) or a normal task (N) onthe basis of his/her present risk level.
The risklevel changes every time a task is Accepted (A) orRejected (R).
Additionally worker may be warned(W) or blocked (B) in case of rejection.his/her performance on the gold standard tasks.Workers who consistently performed poorly ongold standard tasks were allocated a higher risklevel and a notification was sent to them witha corrective course of action.
Beyond a certainlevel, the worker was barred from attempting fu-ture work.
We did not do any retrospective correc-tion of the barred worker?s completed tasks andsimply stopped him/her from attempting newertasks.
This approach allowed us to control for thequality of workers, provide feedback, remove un-suitable workers and also adaptively control thebalance between real and gold standard tasks.8We describe the experimental setup and the re-sults in the next section.5 ExperimentsWe conducted the experiments to answer the fol-lowing questions:?
Can read/repeat features predict spontaneousspeech grades accurately??
How accurate is a pure machine learning ap-proach (without crowdsourced transcription)in predicting grades as compared to gradesgiven by human experts??
How much better is the ML-CS approach in8Specific details of the implementation are beyond thescope of the paper.1089predicting grades as compared to a pure MLapproach and to using Crowd Grades only??
Do Crowd Grades add additional value inpredicting grades over and above the featuresderived from the crowdsourced transcription?We conducted the experiments on 319 sponta-neous speech samples which were graded by ex-pert assessors.
To answer the questions statedabove, we used different sets of features to developmodels and compared their accuracy.
The mod-els were built against expert grades using super-vised learning techniques.
We experimented withthree machine learning techniques?
Ridge Regres-sion, SVMs and Neural Networks with differentfeatures selection algorithms.
The data set used inthe experiments is discussed in the next section.5.1 Data SetOur data set contains 319 spontaneous speech re-sponses.
The speech samples were from seniors(non?native English speakers in final year of un-dergraduate education) pursuing bachelor?s degreein India.
The candidates were asked to describeone of the following scenes: a hospital, flood, acrowded market and a school playground.
Thecandidates were given 30 seconds to think and takenotes and were then asked to speak for the next45 seconds.
The responses were collected on thephone during the SVAR test (SVAR, 2014).
Apartfrom the spontaneous speech response, each can-didate was asked to read 12 given sentences andrepeat 9 given sentences immediately after listen-ing to each of them.
Empty or very noisy re-sponses (not humanly discernible) were not in-cluded in the final 319 sample set.These responses were graded by two expertswho had more than fifteen years of experience ingrading spoken English responses.
There weretwo set of scores.
The first was a holistic scoreon the spontaneous speech samples based on itspronunciation, fluency, content characteristics andgrammar.
The second was a score on the pronun-ciation and fluency quality of the read/repeat sen-tences.
The correlation between grades given bythe two experts was 0.86 and 0.83 respectively forthe two cases.
For each of the two scores, the av-erage of the scores by the two expert grades wasused for further purposes.The correlation between the expert scores onspontaneous speech and read/repeat speech was0.54.
This shows that there is a considerable unex-plained variance (70%) in the spontaneous speechscore, not addressed by the read/repeat scores.This could be due to a difference in the pronun-ciation quality and fluency of the candidates inreading/repeating text vs. speaking spontaneouslyand also due to the additional parameters of gram-mar and content characteristics in the spontaneousspeech score.
Thus, an automatic score mimick-ing the read/repeat expert grades, which is a solvedproblem, is inadequate for our task.The first score is used for all subsequent discus-sion and development of models.5.2 Crowdsourced TasksThe 319 speech sample assessment task wasposted on Amazon Mechanical Turk (AMT).
Eachtask was completed by three workers.
In total, 71unique workers completed the tasks.
The majorityof workers (90%) belonged to USA and India.The task took on an average 8?9 minutes tocomplete and a worker was paid between 6?10cents per task including a bonus which was paidon completion of every 4 tasks.
We also got thespeech transcribed by experts to find the accuracywe could get from turks.
The average transcrip-tion accuracy for a worker was 82.4%9.
This sig-nificantly improved to 95.4% when the transcrip-tions of the three workers were combined usingthe ROVER algorithm.
In comparison, the aver-age automatic transcription of a speech recogni-tion engine was 59.8%.5.3 Regression ModelingThe data set was split into two sets: train and vali-dation.
The train-set had 75% of the sample pointswhereas the validation set had 25%.
The split wasdone randomly making sure that the grade distri-bution in both the sets was similar.
While learningthe model, a 4-fold cross validation was performedon the train sample.Linear ridge regression, Neural Networks andSVM regression with different kernels were usedto build the models.
The least cross-validation er-ror was used to select the models.
We used somesimple techniques for feature selection includingforward feature selection and the algorithm whichremoves all but the k highest correlating features.Regression parameters: For linear regressionwith regularization, optimal ridge coefficient ?,9PHP similar text function was used as similarity metric.1090Table 1: Regression ResultsTechnique Model Code Feature Type Train r Validation rRR-1 RS/LR 0.51 0.47Ridge RegressionRR-2 Pure ML 0.54 0.47RR-3 Crowd Grades 0.63 0.57RR-4 ML-CS 0.55 0.60RR-5 All 0.76 0.76SVM-1 RS/LR 0.50 0.46SVMSVM-2 Pure ML 0.53 0.46SVM-3 Crowd Grades 0.62 0.57SVM-4 ML-CS 0.60 0.61SVM-5 All 0.75 0.74NN-1 RS/LR 0.56 0.51Neural NetworksNN-2 Pure ML 0.60 0.44NN-3 Crowd Grades 0.63 0.57NN-4 ML-CS 0.66 0.57NN-5 All 0.80 0.76between 1 and 1000, was selected based on the theleast RMS error in cross-validation.
For supportvector machines we tested two kernels: linear andradial basis function.
In order to select the optimalSVM model, we varied the penalty factor C, pa-rameters ?
and , the SVM kernel and the selectedset of values that gave us the lowest RMS error incross-validation.
The Neural Networks model hadone hidden layer and 5 to 10 neurons.Feature sets used: The experiments were car-ried out on five sets of features:?
RS/LR: A set of features generated by forcealigning read/repeated by candidates.?
Pure ML: Features generated by automaticspeech transcription of spontaneous speechusing a speech recognizer.?
Crowd Grades: A set of features pertaining togrades given by the crowd.?
ML?CS: NLP and FA features generated byforce aligning free speech on crowdsourcedtranscription.?
All: NLP and FA features from crowd-sourced transcription and Crowd Grades.Here, the first set, RS/LR, helps us to know howwell we can predict spontaneous speech grades bysimply using the read/speak speech of the candi-date and without using his/her spontaneous speechsample.
This provides a comparison baseline.
Thesecond approach evaluates how well we can gradespontaneous speech of the candidate using ma-chine learning approaches only.
The third featureshows the efficacy of directly using grades givenby crowd, while the fourth finds how well machinelearning can do if it has a fairly accurate transcrip-tion of the speech by the crowd.
The final fifthset tests what happens if we combine the third andfourth set of features, i.e.
make use of both thecrowdsourced transcription and the crowd grades.In the following subsection, the features per-taining to ML-CS approach are referred to as ML-CS, those pertaining to natural language process-ing on crowdsourced transcription are referred toas NLP features while the one pertaining to crowdgrades are referred to as Crowd Grades.5.4 ObservationsThe results of the experiments are tabulated in Ta-ble 1.
We report the Pearson coefficient of corre-lation (r) for the different models against the ex-pert grades.
These are the results for the modelsselected according to least cross-validation error.The best cross-validation error in case of SVMswas obtained for the linear kernel.All the following observations are based on thevalidation error.
All three techniques perform sim-ilarly with Neural Networks doing slightly worsein some cases.
The broad trends across fea-ture?sets remain similar across different modeling1091techniques.
We will be referring to the ridge re-gression results for further discussion.Firstly, it is observed that the read/repeat fea-tures predict the spontaneous speech score withlow accuracy (r = 0.47).
This implies that read-/repeat speech and derived features are inadequateto grade a person?s spontaneous speech, the ulti-mate test of a person?s spoken language skills.The second observation is that the ML-only ap-proach using spontaneous speech features (ModelRR-2) is also inadequate to grade spontaneousspeech and does worse than approaches that usesfeatures from crowdsourced transcription (ModelRR-4).
This clearly shows the value of getting ac-curate transcription from workers towards betterfeatures and model.Further, among the crowdsourcing approaches,we find that the crowd-grades (Model RR-3) doesequivalently well (and sometimes worse) than themodel using features derived from the crowd-sourced speech (Model RR-4).
However, whenwe combine all the features from crowdsourcingincluding the crowd grades, we find much betterprediction accuracy (r = 0.76).
This shows thatthe crowd grades feature provides some orthogo-nal information as compared to the features fromthe crowdsourced transcription, towards predict-ing the grade given by experts.The validation r for Model RR-5 is 0.76.
Wefind that the expert agreement on the validationsample is 0.78.
Thus, our predicted score rivals theagreement of experts.
This shows great promisefor the technique to be used in a high-stake testsetting.In summary, we show the following:?
Read/repeat speech features are inadequate topredict spontaneous speech scores.?
ML only approach based on spontaneousspeech samples is also inadequate for the pur-pose.?
Features derived from crowdsourced tran-scription (or even crowd grades) do betterthan a ML only approach.?
When considering features from crowd-sourced transcription and crowd grades to-gether, we can predict spontaneous speechscores as well as those done by experts.6 ConclusionsWe addressed the problem of evaluating spon-taneous speech using a combination of machinelearning and crowdsourcing.
To achieve this, wepost the task of speech transcription to the crowd.Additionally, we also get spoken English gradesfrom the crowd.
We are able to derive accuratefeatures by force aligning the speech sample onthe crowdsourced text.
We experimented our tech-nique on expert?graded speech samples of adultnon?native speakers.
Using these features in aregression model, we are able to predict expertgrades with much higher accuracy than a machinelearning only approach.
These features also pre-dict equivalent or better than crowd grades and acombination of these two outperforms all other ap-proaches.
Our approach shows an accuracy thatrivals that of expert agreement.Our technique has a promise of higher accuracybut has some trade-offs compared to fully auto-mated approaches.
First, there is a cost for ev-ery assessment done and the scalability dependson the number of non-expert workers available.Though these drawbacks exist, we were able gettasks done inexpensively.
We recently had thecrowd rate a hundred samples in a day without anychallenge.
Second, our approach doesn?t provideinstant grades.
This works fine in many scenarios,but doesn?t cater well to providing real-time feed-back.
Real time crowdsourcing has been an activearea of research (Bernstein et al, 2011; Lasecki etal., 2013) and is an area for future work for us aswell.ReferencesAfterTheDeadline.
2014. www.afterthedeadline.com.Ahmet Aker, Mahmoud El-Haj, M-Dyaa Albakour, andUdo Kruschwitz.
2012.
Assessing crowdsourcingquality through objective tasks.
In LREC, pages1456?1461.
Citeseer.Michael S Bernstein, Joel Brandt, Robert C Miller, andDavid R Karger.
2011.
Crowds in two seconds: En-abling realtime crowd-powered interfaces.
In Pro-ceedings of the 24th annual ACM symposium onUser interface software and technology, pages 33?42.
ACM.Suma Bhat, Huichao Xue, and Su-Youn Yoon.
2014.Shallow analysis based assessment of syntactic com-plexity for automated speech scoring.
In Proceed-ings of the 52nd Annual Meeting of the Association1092for Computational Linguistics (Volume 1: Long Pa-pers), pages 1305?1315.
Association for Computa-tional Linguistics.David Brazil.
1995.
A grammar of speech.
OxfordUniversity Press, USA.Christopher Brumfit and Christopher J Brumfit.
1984.Communicative methodology in language teaching:The roles of fluency and accuracy, volume 129.Cambridge University Press Cambridge.Catia Cucchiarini, Helmer Strik, and Lou Boves.
1997.Automatic evaluation of dutch pronunciation by us-ing speech recognition technology.
In AutomaticSpeech Recognition and Understanding, 1997.
Pro-ceedings., 1997 IEEE Workshop on, pages 622?629.IEEE.Catia Cucchiarini, Helmer Strik, and Lou Boves.
2000.Quantitative assessment of second language learn-ers?
fluency by means of automatic speech recogni-tion technology.
The Journal of the Acoustical Soci-ety of America, 107(2):989?999.Eric John Dobson.
1957.
English Pronunciation,1500-1700: Phonology, volume 2.
Clarendon Press.Bin Dong, Qingwei Zhao, Jianping Zhang, andYonghong Yan.
2004.
Automatic assessment ofpronunciation quality.
In Chinese Spoken Lan-guage Processing, 2004 International Symposiumon, pages 137?140.
IEEE.Elizabeth J Erling and Philip Seargeant.
2013.
Englishand development: Policy, pedagogy and globaliza-tion, volume 17.
Multilingual Matters.Enrique Estell?es-Arolas and Fernando Gonz?alez-Ladr?on-de Guevara.
2012.
Towards an integratedcrowdsourcing definition.
Journal of Informationscience, 38(2):189?200.Cambridge EOCL Examinations.
2011.
Using theCEFR: Principles of good practice.
at University ofCambridge.Jonathan G Fiscus.
1997.
A post-processing systemto yield reduced word error rates: Recognizer out-put voting error reduction (rover).
In AutomaticSpeech Recognition and Understanding, 1997.
Pro-ceedings., 1997 IEEE Workshop on, pages 347?354.IEEE.Horacio Franco, Victor Abrash, Kristin Precoda, HarryBratt, Ramana Rao, John Butzberger, RomainRossier, and Federico Cesari.
2000.
The srieduspeaktm system: Recognition and pronunciationscoring for language learning.
Proceedings of In-STILL 2000, pages 123?128.John S Garofolo, Lori F Lamel, William M Fisher,Jonathon G Fiscus, and David S Pallett.
1993.Darpa timit acoustic-phonetic continous speech cor-pus cd-rom.
nist speech disc 1-1.1.
NASA STI/ReconTechnical Report N, 93:27403.Cahit Guven and Asadul Islam.
2013.
Age at migra-tion, language proficiency and socio-economic out-comes: Evidence from australia.
Technical report.Eric Hagley.
2010.
Creation of speaking tests for eflcommunication classes.
?, (8):33?41.Gene B Halleck.
1995.
Assessing oral proficiency: Acomparison of holistic and objective measures.
TheModern Language Journal, 79(2):223?234.Mehdi Hosseini, Ingemar J Cox, Nata?sa Mili?c-Frayling, Gabriella Kazai, and Vishwa Vinay.
2012.On aggregating labels from multiple crowd workersto infer relevance of documents.
In Advances in in-formation retrieval, pages 182?194.
Springer.Jeff Howe.
2006.
The rise of crowdsourcing.
Wiredmagazine, 14(6):1?4.Akinori Ito, Tadao Nagasawa, Hirokazu Ogasawara,Motoyuki Suzuki, and Shozo Makino.
2006.
Au-tomatic detection of english mispronunciation usingspeaker adaptation and automatic assessment of en-glish intonation and rhythm.
Educational technol-ogy research, 29(1):13?23.Christos Koniaris and Olov Engwall.
2011.
Percep-tual differentiation modeling explains phoneme mis-pronunciation by non-native speakers.
In Acous-tics, Speech and Signal Processing (ICASSP), 2011IEEE International Conference on, pages 5704?5707.
IEEE.Stephen A Kunath and Steven H Weinberger.
2010.The wisdom of the crowd?s ear: speech accent rat-ing and annotation with amazon mechanical turk.
InProceedings of the NAACL HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk, pages 168?171.
Association forComputational Linguistics.Walter S Lasecki, Christopher D Miller, and Jeffrey PBigham.
2013.
Warping time for more effec-tive real-time crowdsourcing.
In Proceedings of theSIGCHI Conference on Human Factors in Comput-ing Systems, pages 2033?2036.
ACM.Mark Lejk and Michael Wyvill.
2001.
The effect ofthe inclusion of selfassessment with peer assessmentof contributions to a group project: A quantitativestudy of secret and agreed assessments.
Assessment& Evaluation in Higher Education, 26(6):551?561.Hongyan Li, Shijin Wang, Jiaen Liang, Shen Huang,and Bo Xu.
2009.
High performance automatic mis-pronunciation detection method based on neural net-work and trap features.
In INTERSPEECH, pages1911?1914.LightSide.
2013. http://lightsidelabs.com/.David Little.
2006.
The common european frameworkof reference for languages: Content, purpose, origin,reception and impact.
Language Teaching, 39:167?190, 7.1093David Little.
2007.
The common european frameworkof reference for languages: Perspectives on the mak-ing of supranational language education policy.
TheModern Language Journal, 91(4):645?655.Nitin Madnani, Joel Tetreault, Martin Chodorow, andAlla Rozovskaya.
2011.
They can help: usingcrowdsourcing to improve the evaluation of gram-matical error detection systems.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies: short papers-Volume 2, pages 508?513.
Asso-ciation for Computational Linguistics.Leonardo Neumeyer, Horacio Franco, Mitchel Wein-traub, and Patti Price.
1996.
Automatic text-independent pronunciation scoring of foreign lan-guage student speech.
In Spoken Language, 1996.ICSLP 96.
Proceedings., Fourth International Con-ference on, volume 3, pages 1457?1460.
IEEE.Quoc Viet Hung Nguyen, Tam Nguyen Thanh, TranLam Ngoc, and Karl Aberer.
2013.
An evaluationof aggregation techniques in crowdsourcing.
In The14th International Conference on Web InformationSystem Engineering (WISE), 2013, number EPFL-CONF-187456.Gabriele Paolacci, Jesse Chandler, and Panagiotis GIpeirotis.
2010.
Running experiments on amazonmechanical turk.
Judgment and Decision making,5(5):411?419.Mitchell Aaron Peabody.
2011.
Methods for pronunci-ation assessment in computer aided language learn-ing.
Ph.D. thesis, Massachusetts Institute of Tech-nology.Donald E Powers, Jill C Burstein, Martin Chodorow,Mary E Fowles, and Karen Kukich.
2002.
Stumpinge-rater: challenging the validity of automated essayscoring.
Computers in Human Behavior, 18(2):103?134.K?are Sj?olander.
2003.
An hmm-based system for au-tomatic segmentation and alignment of speech.
InProceedings of Fonetik, volume 2003, pages 93?96.Citeseer.Robert Stalnaker.
1999.
The problem of logical omni-science, ii.
context and content: Essays on intention-ality in speech and thought (pp.
255?273).Lynn Streeter, Jared Bernstein, Peter Foltz, and Don-ald DeLand.
2011.
Pearsons automated scoring ofwriting, speaking, and mathematics.SVAR.
2014. http://www.aspiringminds.in/talent-evaluation/spoken-english-SVAR.html.Joel R Tetreault, Elena Filatova, and Martin Chodorow.2010.
Rethinking grammatical error annotation andevaluation with the amazon mechanical turk.
In Pro-ceedings of the NAACL HLT 2010 Fifth Workshopon Innovative Use of NLP for Building EducationalApplications, pages 45?48.
Association for Compu-tational Linguistics.Amazon Mechanical Turk.
2014.https://requester.mturk.com/tour.Nathan Van Houdnos.
2011.
Can the internet grademath?
crowdsourcing a complex scoring task andpicking the optimal crowd size.
Dietrich College ofHumanities and Social Sciences at Research Show-case @ CMU.Hao Wang and Helen Meng.
2012.
Deriving percep-tual gradation of l2 english mispronunciations usingcrowdsourcing and the workerrank algorithm.
Proc.of the 15th Oriental COCOSDA, Macau, China,pages 9?12.Hao Wang, Xiaojun Qian, and Helen Meng.
2014.Phonological modeling of mispronunciation gra-dations in l2 english speech of l1 chinese learn-ers.
In Acoustics, Speech and Signal Processing(ICASSP), 2014 IEEE International Conference on,pages 7714?7718.
IEEE.Paul Whitla.
2009.
Crowdsourcing and its applicationin marketing activities.
Contemporary ManagementResearch, 5(1).Steve Young, Gunnar Evermann, Mark Gales, ThomasHain, Dan Kershaw, Xunying Liu, Gareth Moore,Julian Odell, Dave Ollason, Dan Povey, et al 2006.The htk book (for htk version 3.4).
Cambridge uni-versity engineering department, 2(2):2?3.Omar F Zaidan and Chris Callison-Burch.
2011.Crowdsourcing translation: Professional qualityfrom non-professionals.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies-Volume 1, pages 1220?1229.
Association for Com-putational Linguistics.Klaus Zechner, Derrick Higgins, and Xiaoming Xi.2007.
Speechrater: A construct-driven approachto scoring spontaneous non-native speech.
Proc.SLaTE.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M Williamson.
2009.
Automatic scoring ofnon-native spontaneous speech in tests of spoken en-glish.
Speech Communication, 51(10):883?895.1094
