Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 37?42,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Comparison of Structural Correspondence Learning and Self-training forDiscriminative Parse SelectionBarbara PlankUniversity of Groningen, The Netherlandsb.plank@rug.nlAbstractThis paper evaluates two semi-supervisedtechniques for the adaptation of a parse selec-tion model to Wikipedia domains.
The tech-niques examined are Structural Correspon-dence Learning (SCL) (Blitzer et al, 2006)and Self-training (Abney, 2007; McClosky etal., 2006).
A preliminary evaluation favors theuse of SCL over the simpler self-training tech-niques.1 Introduction and MotivationParse selection constitutes an important part of manyparsing systems (Hara et al, 2005; van Noord andMalouf, 2005; McClosky et al, 2006).
Yet, thereis little to no work focusing on the adaptation ofparse selection models to novel domains.
This ismost probably due to the fact that potential gainsfor this task are inherently bounded by the under-lying grammar.
The few studies on adapting parsedisambiguation models, like Hara et al (2005), havefocused exclusively on supervised domain adapta-tion, i.e.
one has access to a comparably small, butlabeled amount of target data.
In contrast, in semi-supervised domain adaptation one has only unla-beled target data.
It is a more realistic situation, butat the same time also considerably more difficult.In this paper we evaluate two semi-supervisedapproaches to domain adaptation of a discrimina-tive parse selection model.
We examine Struc-tural Correspondence Learning (SCL) (Blitzer etal., 2006) for this task, and compare it to severalvariants of Self-training (Abney, 2007; McClosky etal., 2006).
For empirical evaluation (section 4) weuse the Alpino parsing system for Dutch (van Noordand Malouf, 2005).
As target domain, we exploitWikipedia as primary test and training collection.2 Previous WorkSo far, Structural Correspondence Learning hasbeen applied successfully to PoS tagging and Sen-timent Analysis (Blitzer et al, 2006; Blitzer etal., 2007).
An attempt was made in the CoNLL2007 shared task to apply SCL to non-projective de-pendency parsing (Shimizu and Nakagawa, 2007).However, the system just ended up at rank 7 outof 8 teams.
Based on annotation differences in thedatasets (Dredze et al, 2007) and a bug in their sys-tem (Shimizu and Nakagawa, 2007), their results areinconclusive.
A recent attempt (Plank, 2009) showspromising results on applying SCL to parse disam-biguation.
In this paper, we extend that line of workand compare SCL to bootstrapping approaches suchas self-training.Studies on self-training have focused mainly ongenerative, constituent based parsing (Steedman etal., 2003; McClosky et al, 2006; Reichart and Rap-poport, 2007).
Steedman et al (2003) as well as Re-ichart and Rappoport (2007) examine self-trainingfor PCFG parsing in the small seed case (< 1k la-beled data), with different results.
In contrast, Mc-Closky et al (2006) focus on large seeds and exploita reranking-parser.
Improvements are obtained (Mc-Closky et al, 2006; McClosky and Charniak, 2008),showing that a reranker is necessary for successfulself-training in such a high-resource scenario.
Whilethey self-trained a generative model, we examineself-training and SCL for semi-supervised adapta-tion of a discriminative parse selection system.373 Semi-supervised Domain Adaptation3.1 Structural Correspondence LearningStructural Correspondence Learning (Blitzer et al,2006) exploits unlabeled data from both source andtarget domain to find correspondences among fea-tures from different domains.
These correspon-dences are then integrated as new features in the la-beled data of the source domain.
The outline of SCLis given in Algorithm 1.The key to SCL is to exploit pivot features to au-tomatically identify feature correspondences.
Piv-ots are features occurring frequently and behavingsimilarly in both domains (Blitzer et al, 2006).They correspond to auxiliary problems in Ando andZhang (2005).
For every such pivot feature, a binaryclassifier is trained (step 2 of Algorithm 1) by mask-ing the pivot feature in the data and trying to predictit with the remaining non-pivot features.
Non-pivotsthat correlate with many of the same pivots are as-sumed to correspond.
These pivot predictor weightvectors thus implicitly align non-pivot features fromsource and target domain.
Intuitively, if we are ableto find good correspondences through ?linking?
piv-ots, then the augmented source data should transferbetter to a target domain (Blitzer et al, 2006).Algorithm 1 SCL (Blitzer et al, 2006)1: Select m pivot features.2: Train m binary classifiers (pivot predictors).
Cre-ate matrix Wn?m of binary predictor weight vectorsW = [w1, .., wm], with n number of nonpivots.3: Dimensionality Reduction.
Apply SVD to W :Wn?m = Un?nDn?mV Tm?m and select ?
= UT[1:h,:](the h top left singular vectors of W ).4: Train a new model on the original and new featuresobtained by applying the projection x ?
?.SCL for Discriminative Parse Selection So far,pivot features on the word level were used (Blitzeret al, 2006; Blitzer et al, 2007).
However, for parsedisambiguation based on a conditional model theyare irrelevant.
Hence, we follow Plank (2009) andactually first parse the unlabeled data.
This allowsa possibly noisy, but more abstract representationof the underlying data.
Features thus correspond toproperties of parses: application of grammar rules(r1,r2 features), dependency relations (dep), PoStags (f1,f2), syntactic features (s1), precedence (mf ),bilexical preferences (z), apposition (appos) and fur-ther features for unknown words, temporal phrases,coordination (h,in year and p1, respectively).
Thesefeatures are further described in van Noord and Mal-ouf (2005).Selection of pivot features As pivot featuresshould be common across domains, here we restrictour pivots to be of the type r1,p1,s1 (the most fre-quently occurring feature types).
In more detail, r1indicates which grammar rule applied, p1 whethercoordination conjuncts are parallel, and s1 whetherlocal/non-local extraction occurred.
We count howoften each feature appears in the parsed source andtarget domain data, and select those r1,p1,s1 fea-tures as pivot features, whose count is > t, wheret is a specified threshold.
In all our experiments, weset t = 5000.
In this way we obtained on average360 pivot features, on the datasets described in Sec-tion 4.3.2 Self-trainingSelf-training (Algorithm 2) is a simple single-viewbootstrapping algorithm.
In self-training, the newlylabeled instances are taken at face value and addedto the training data.There are many possible ways to instantiate self-training (Abney, 2007).
One variant, introduced inAbney (2007) is the notion of ?(in)delibility?
: in thedelible case the classifier relabels all of the unla-beled data from scratch in every iteration.
The clas-sifier may become unconfident about previously se-lected instances and they may drop out (Steven Ab-ney, personal communication).
In contrast, in theindelible case, labels once assigned do not changeagain (Abney, 2007).In this paper we look at the following variants ofself-training:?
single versus multiple iterations,?
selection versus no selection (taking all self-labeled data or selecting presumably higherquality instances); different scoring functionsfor selection,?
delibility versus indelibility for multiple itera-tions.38Algorithm 2 Self-training (indelible) (Abney, 2007).1: L0 is labeled [seed] data, U is unlabeled data2: c?
train(L0)3: repeat4: L?
L + select(label(U ?
L, c))5: c?
train(L)6: until stopping criterion is metScoring methods We examine three simple scor-ing functions for instance selection: i) Entropy(?
?y?Y (s) p(?|s, ?)
log p(?|s, ?)).
ii) Number ofparses (|Y (s)|); and iii) Sentence Length (|s|).4 Experiments and ResultsExperimental Design The system used in thisstudy is Alpino, a two-stage dependency parser forDutch (van Noord and Malouf, 2005).
The firststage consists of a HPSG-like grammar that consti-tutes the parse generation component.
The secondstage is a Maximum Entropy (MaxEnt) parse selec-tion model.
To train the MaxEnt model, parame-ters are estimated based on informative samples (Os-borne, 2000).
A parse is added to the training datawith a score indicating its ?goodness?
(van Noordand Malouf, 2005).
The score is obtained by com-paring it with the gold standard (if available; other-wise the score is approximated through parse proba-bility).The source domain is the Alpino Treebank (vanNoord and Malouf, 2005) (newspaper text; approx.7,000 sentences; 145k tokens).
We use Wikipediaboth as testset and as unlabeled target data source.We assume that in order to parse data from a veryspecific domain, say about the artist Prince, thendata related to that domain, like information aboutthe New Power Generation, the Purple rain movie,or other American singers and artists, should be ofhelp.
Thus, we exploit Wikipedia?s category systemto gather domain-specific target data.
In our empiri-cal setup, we follow Blitzer et al (2006) and balancethe size of source and target data.
Thus, dependingon the size of the resulting target domain dataset, andthe ?broadness?
of the categories involved in creat-ing it, we might wish to filter out certain pages.
Weimplemented a filter mechanism that excludes pagesof a certain category (e.g.
a supercategory that is hy-pothesized to be ?too broad?).
Further details aboutthe dataset construction are given in (Plank, 2009).Table 1 provides information on the target domaindatasets constructed from Wikipedia.Related to Articles Sents Tokens RelationshipPrince 290 9,772 145,504 filtered superPaus 445 8,832 134,451 allDeMorgan 394 8,466 132,948 allTable 1: Size of related unlabeled data; relationship in-dicates whether all related pages are used or some arefiltered out.The size of the target domain testsets is given inTable 2.
As evaluation measure concept accuracy(CA) (van Noord and Malouf, 2005) is used (similarto labeled dependency accuracy).The training data for the pivot predictors are the1-best parses of source and target domain data asselected by the original Alpino model.
We reporton results of SCL with dimensionality parameter setto h = 25, and remaining settings identical to Plank(2009) (i.e., no feature-specific regularization and nofeature normalization and rescaling).Baseline Table 2 shows the baseline accuracies(model trained on labeled out-of-domain data) onthe Wikipedia testsets (last column: size in numberof sentences).
The second and third column indicatelower (first parse) and upper- (oracle) bounds.Wikipedia article baseline first oracle sentPrince (musician) 85.03 71.95 88.70 357Paus Johannes Paulus II 85.72 74.30 89.09 232Augustus De Morgan 80.09 70.08 83.52 254Table 2: Supervised Baseline results.SCL and Self-training results The results forSCL (Table 3) show a small, but consistent increasein absolute performance on all testsets over the base-lines (up to +0.27 absolute CA or 7.34% relativeerror reduction, which is significant at p < 0.05 ac-cording to sign test).In contrast, basic self-training (Table 3) achievesroughly only baseline accuracy and lower perfor-mance than SCL, with one exception.
On the De-Morgan testset, self-training scores slightly higherthan SCL.
However, the improvements of both SCLand self-training are not significant on this rather39small testset.
Indeed, self-training scores better thanthe baseline on only 5 parses out of 254, while itsperformance is lower on 2, leaving only 3 parses thataccount for the difference.CA ?
Rel.ERPrince baseline 85.03 78.06 0.00SCL ?
85.30 79.67 7.34Self-train (all-at-once) 85.08 78.38 1.46Paus baseline 85.72 77.23 0.00SCL 85.82 77.87 2.81Self-train (all-at-once) 85.78 77.62 1.71DeMorgan baseline 80.09 74.44 0.00SCL 80.15 74.92 1.88Self-train (all-at-once) 80.24 75.63 4.65Table 3: Results of SCL and self-training (single itera-tion, no selection).
Entries marked with ?
are statisticallysignificant at p < 0.05.
The ?
score incorporates upper-and lower-bounds.To gauge whether other instantiations of self-training are more effective, we evaluated the self-training variants introduced in section 3.2 on thePrince dataset.
In the iterative setting, we fol-low Steedman et al (2003) and parse 30 sentencesfrom which 20 are selected in every iteration.With regard to the comparison of delible versusindelible self-training (whether labels may change),our empirical findings shows that the two casesachieve very similar performance; the two curveshighly overlap (Figure 1).
The accuracies of bothcurves fluctuate around 85.13, showing no upwardor downward trend.
In general, however, indelibilityis preferred since it takes considerably less time (theclassifier does not have to relabel U from scratch inevery iteration).
In addition, we tested EM (whichuses all unlabeled data in each iteration).
Its per-formance is consistently lower, varying around thebaseline.Figure 2 compares several self-training variantswith the supervised baseline and SCL.
It summa-rizes the effect of i) selection versus no selection(and various selection techniques) as well as ii) sin-gle versus multiple iterations of self-training.
Forclarity, the figure shows the learning curve of thebest selection technique only, but depicts the perfor-mance of the various selection techniques in a singleiteration (non-solid lines).In the iterative setting, taking the whole self-labeled data and not selecting certain instances (greycurve in Figure 2) degrades performance.
In con-trast, selecting shorter sentences slightly improvesaccuracy, and is the best selection method amongthe ones tested (shorter sentences, entropy, fewerparses).For all self-training instantiations, running multi-ple iterations is on average just the same as runninga single iteration (the non-solid lines are roughly theaverage of the learning curves).
Thus there is no realneed to run several iterations of self-training.The main conclusion is that in contrast to SCL,none of the self-training instantiations achieves asignificant improvement over the baseline.5 Conclusions and Future WorkThe paper compares Structural CorrespondenceLearning (Blitzer et al, 2006) with (various in-stances of) self-training (Abney, 2007; McCloskyet al, 2006) for the adaptation of a parse selectionmodel to Wikipedia domains.The empirical findings show that none of the eval-uated self-training variants (delible/indelible, singleversus multiple iterations, various selection tech-niques) achieves a significant improvement over thebaseline.
The more ?indirect?
exploitation of unla-beled data through SCL is more fruitful than pureself-training.
Thus, favoring the use of the morecomplex method, although the findings are not con-firmed on all testsets.Of course, our results are preliminary and, ratherthan warranting yet many definite conclusions, en-courage further investigation of SCL (varying sizeof target data, pivots selection, bigger testsets aswell as other domains etc.)
as well as related semi-supervised adaptation techniques.AcknowledgmentsThanks to Gertjan van Noord and the anonymous re-viewers for their comments.
The Linux cluster of theHigh-Performance Computing Center of the Univer-sity of Groningen was used in this work.400 50 100 150 20085.0085.0585.1085.1585.2085.2585.30number of iterationsaccuracyIndelibility versus delibilitybaselineSCLIndelible SelfTrainDelible SelfTrainEMFigure 1: Delible versus Indelible self-training and EM.
Delible and indelible self-training achieve very similar per-formance.
However, indelibility is preferred over delibility since it is considerably faster.0 50 100 150 20085.0085.0585.1085.1585.2085.2585.30number of iterationsaccuracyshorter sententropyfewer parses / no selectionbaselineSCLIndelibility with different selection techniquesselect shorter sentno selectionFigure 2: Self-training variants compared to supervised baseline and SCL.
The effect of various selection techniques(Sec.
3.2) in a single iteration is depicted (non-solid lines; fewer parses and no selection achieve identical results).
Forclarity, the figure shows the learning curve for the best selection technique only (shorter sent) versus no selection.
Onaverage running multiple iterations is just the same as a single iteration.
In all cases SCL still performs best.41ReferencesSteven Abney.
2007.
Semi-supervised Learning forComputational Linguistics.
Chapman & Hall.Rie Kubota Ando and Tong Zhang.
2005.
A frameworkfor learning predictive structures from multiple tasksand unlabeled data.
Journal of Machine Learning Re-search, 6:1817?1853.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Conference on Empirical Methodsin Natural Language Processing, Sydney, Australia.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InAssociation for Computational Linguistics, Prague,Czech Republic.Mark Dredze, John Blitzer, Pratha Pratim Talukdar, Kuz-man Ganchev, Joao Graca, and Fernando Pereira.2007.
Frustratingly hard domain adaptation for pars-ing.
In Proceedings of the CoNLL Shared Task Session- Conference on Natural Language Learning, Prague,Czech Republic.Tadayoshi Hara, Miyao Yusuke, and Jun?ichi Tsujii.2005.
Adapting a probabilistic disambiguation modelof an hpsg parser to a new domain.
In Proceedingsof the International Joint Conference on Natural Lan-guage Processing.David McClosky and Eugene Charniak.
2008.
Self-training for biomedical parsing.
In Proceedings ofACL-08: HLT, Short Papers, pages 101?104, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conferenceof the NAACL, Main Conference, pages 152?159, NewYork City.
Association for Computational Linguistics.Miles Osborne.
2000.
Estimation of stochastic attribute-value grammars using an informative sample.
In Pro-ceedings of the Eighteenth International Conferenceon Computational Linguistics (COLING 2000).Barbara Plank.
2009.
Structural correspondence learn-ing for parse disambiguation.
In Proceedings of theStudent Research Workshop at EACL 2009, Athens,Greece, April.Roi Reichart and Ari Rappoport.
2007.
Self-trainingfor enhancement and domain adaptation of statisticalparsers trained on small datasets.
In Proceedings ofAssociation for Computational Linguistics, Prague.Nobuyuki Shimizu and Hiroshi Nakagawa.
2007.
Struc-tural correspondence learning for dependency parsing.In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007.Mark Steedman, Miles Osborne, Anoop Sarkar, StephenClark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Bootstrap-ping statistical parsers from small datasets.
In In Pro-ceedings of the EACL, pages 331?338.Gertjan van Noord and Robert Malouf.
2005.
Widecoverage parsing with stochastic attribute value gram-mars.
Draft.
A preliminary version of this paper waspublished in the Proceedings of the IJCNLP workshopBeyond Shallow Analyses, Hainan China, 2004.42
