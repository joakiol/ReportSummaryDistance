Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1034?1043,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsDeep Fusion LSTMs for Text Semantic MatchingPengfei Liu, Xipeng Qiu?, Jifan Chen, Xuanjing HuangShanghai Key Laboratory of Intelligent Information Processing, Fudan UniversitySchool of Computer Science, Fudan University825 Zhangheng Road, Shanghai, China{pfliu14,xpqiu,jfchen14,xjhuang}@fudan.edu.cnAbstractRecently, there is rising interest in mod-elling the interactions of text pair withdeep neural networks.
In this paper, wepropose a model of deep fusion LSTMs(DF-LSTMs) to model the strong inter-action of text pair in a recursive match-ing way.
Specifically, DF-LSTMs con-sist of two interdependent LSTMs, eachof which models a sequence under the in-fluence of another.
We also use exter-nal memory to increase the capacity ofLSTMs, thereby possibly capturing morecomplicated matching patterns.
Experi-ments on two very large datasets demon-strate the efficacy of our proposed archi-tecture.
Furthermore, we present an elab-orate qualitative analysis of our models,giving an intuitive understanding how ourmodel worked.1 IntroductionAmong many natural language processing (NLP)tasks, such as text classification, question answer-ing and machine translation, a common problemis modelling the relevance/similarity of a pair oftexts, which is also called text semantic matching.Due to the semantic gap problem, text semanticmatching is still a challenging problem.Recently, deep learning is rising a substan-tial interest in text semantic matching and hasachieved some great progresses (Hu et al, 2014;Qiu and Huang, 2015; Wan et al, 2016).
Accord-ing to their interaction ways, previous models canbe classified into three categories:Weak interaction Models Some early worksfocus on sentence level interactions, such as ARC-I(Hu et al, 2014), CNTN(Qiu and Huang, 2015)?Corresponding authorand so on.
These models first encode two se-quences into continuous dense vectors by sepa-rated neural models, and then compute the match-ing score based on sentence encoding.
In thisparadigm, two sentences have no interaction un-til arriving final phase.Semi-interaction Models Another kind of mod-els use soft attention mechanism to obtain the rep-resentation of one sentence by depending on rep-resentation of another sentence, such as ABCNN(Yin et al, 2015), Attention LSTM (Rockt?aschelet al, 2015; Hermann et al, 2015).
These modelscan alleviate the weak interaction problem to someextent.Strong Interaction Models Some models buildthe interaction at different granularity (word,phrase and sentence level), such as ARC-II (Huet al, 2014), MultiGranCNN (Yin and Sch?utze,2015), Multi-Perspective CNN (He et al, 2015),MV-LSTM (Wan et al, 2016), MatchPyramid(Pang et al, 2016).
The final matching score de-pends on these different levels of interactions.In this paper, we adopt a deep fusion strat-egy to model the strong interactions of two sen-tences.
Given two texts x1:mand y1:n, we definea matching vector hi,jto represent the interactionof the subsequences x1:iand y1:j. hi,jdepends onthe matching vectors hs,ton previous interactions1 ?
s < i and 1 ?
t < j.
Thus, text match-ing can be regarded as modelling the interactionof two texts in a recursive matching way.Following this idea, we propose deep fusionlong short-term memory neural networks (DF-LSTMs) to model the interactions recursively.More concretely, DF-LSTMs consist of two in-terconnected conditional LSTMs, each of whichmodels a piece of text under the influence of an-other.
The output vector of DF-LSTMs is fed intoa task-specific output layer to compute the match-1034Gymnast get ready for a competitionFemale gymnast warm up before a competition?
?Figure 1: A motivated example to illustrate ourrecursive composition mechanism.ing score.The contributions of this paper can be summa-rized as follows.1.
Different with previous models, DF-LSTMsmodel the strong interactions of two texts ina recursive matching way, which consist oftwo inter- and intra-dependent LSTMs.2.
Compared to the previous works on textmatching, we perform extensive empiricalstudies on two very large datasets.
Exper-iment results demonstrate that our proposedarchitecture is more effective.3.
We present an elaborate qualitative analysisof our model, giving an intuitive understand-ing how our model worked.2 Recursively Text Semantic MatchingTo facilitate our model, we firstly give some defi-nitions.Given two sequences X = x1, x2, ?
?
?
, xmandY = y1, y2, ?
?
?
, yn, most deep neural models tryto represent their semantic relevance by a match-ing vector h(X,Y ), which is followed by a scorefunction to calculate the matching score.The weak interaction methods decomposematching vector by h(X,Y ) = f(h(X),h(Y )),where function f(?)
may be one of some basic op-erations or the combination of them: concatena-tion, affine transformation, bilinear, and so on.In this paper, we propose a strong interactionof two sequences to decompose matching vec-tor h(X,Y ) in a recursive way.
We refer to theinteraction of the subsequences x1:iand y1:jashi,j(X,Y ), which depends on previous interac-tions hs,t(X,Y ) for 1 ?
s < i and 1 ?
t < j.Figure 1 gives an example to illustrate this.
Forsentence pair X =?Female gymnast warmup before a competition?, Y =?Gym-nast get ready for a competition?,considering the interaction (h4,4) between x1:4= ?Female gymnast warm up?
and y1:4= ?Gymnast get ready for?, which iscomposed by the interactions between theirsubsequences (h1,4, ?
?
?
,h3,4,h4,1, ?
?
?
,h4,3).We can see that a strong interaction betweentwo sequences can be decomposed in recursivetopology structure.The matching vector hi,j(X,Y ) can be writtenashi,j(X,Y ) = hi,j(X|Y )?
hi,j(Y |X), (1)where hi,j(X|Y ) refers to conditional encoding ofsubsequence x1:iinfluenced by y1:j. Meanwhile,hi,j(Y |X) is conditional encoding of subsequencey1:jinfluenced by subsequence x1:i; ?
is concate-nation operation.These two conditional encodings depend ontheir history encodings.
Based on this, we proposedeep fusion LSTMs to model the matching of textsby recursive composition mechanism, which canbetter capture the complicated interaction of twosentences due to fully considering the interactionsbetween subsequences.3 Long Short-Term Memory NetworkLong short-term memory neural network(LSTM) (Hochreiter and Schmidhuber, 1997)is a type of recurrent neural network (RNN)(Elman, 1990), and specifically addresses theissue of learning long-term dependencies.
LSTMmaintains a memory cell that updates and exposesits content only when deemed necessary.While there are numerous LSTM variants, herewe use the LSTM architecture used by (Jozefow-icz et al, 2015), which is similar to the architec-ture of (Graves, 2013) but without peep-hole con-nections.We define the LSTM units at each time step t tobe a collection of vectors in Rd: an input gate it,a forget gate ft, an output gate ot, a memory cellctand a hidden state ht.
d is the number of theLSTM units.
The elements of the gating vectorsit, ftand otare in [0, 1].The LSTM is precisely specified as follows.????c?totitft????=????tanh??????
?TA,b[xtht?1], (2)1035tanhtanhV VVix( )1,xi jc fi ojy VtanhVVtanh......( ), xi jr??
( )2,xi jh( )1,xi jh( ) ,xi K jh( ),yi j Kh ( ), 2yi jh ( ), 1yi jh  ( ),xi jc ( ),xi jh ( ),yi jh( ),yi jc( ), 1yi jc ( ), yi jr fi oFigure 2: Illustration of DF-LSTMs unit.ct= c?tit+ ct?1ft, (3)ht= ottanh (ct) , (4)where xtis the input at the current time step; TA,bis an affine transformation which depends on pa-rameters of the network A and b. ?
denotes thelogistic sigmoid function and  denotes elemen-twise multiplication.
Intuitively, the forget gatecontrols the amount of which each unit of thememory cell is erased, the input gate controls howmuch each unit is updated, and the output gatecontrols the exposure of the internal memory state.The update of each LSTM unit can be writtenprecisely as(ht, ct) = LSTM(ht?1, ct?1,xt).
(5)Here, the function LSTM(?, ?, ?)
is a shorthandfor Eq.
(2-4).LSTM can map the input sequence of arbi-trary length to a fixed-sized vector, and has beensuccessfully applied to a wide range of NLPtasks, such as machine translation (Sutskever etal., 2014), language modelling (Sutskever et al,2011), text matching (Rockt?aschel et al, 2015)and text classification (Liu et al, 2015).4 Deep Fusion LSTMs for RecursivelySemantic MatchingTo deal with two sentences, one straightforwardmethod is to model them with two separateLSTMs.
However, this method is difficult tomodel local interactions of two sentences.Following the recursive matching strategy, wepropose a neural model of deep fusion LSTMs(DF-LSTMs), which consists of two interdepen-dent LSTMs to capture the inter- and intra-interactions between two sequences.
Figure 2gives an illustration of DF-LSTMs unit.To facilitate our model, we firstly give somedefinitions.
Given two sequences X =x1, x2, ?
?
?
, xnand Y = y1, y2, ?
?
?
, ym, we letxi?
Rddenotes the embedded representation ofthe word xi.
The standard LSTM has one temporaldimension.
When dealing with a sentence, LSTMregards the position as time step.
At position i ofsentence x1:n, the output hireflects the meaningof subsequence x1:i= x1, ?
?
?
, xi.To model the interaction of two sentences in arecursive way, we define hi,jto represent the in-teraction of the subsequences x1:iand y1:j, whichis computed byhi,j= h(x)i,j?
h(y)i,j, (6)where h(x)i,jdenotes the encoding of subsequencex1:iin the first LSTM influenced by the output ofthe second LSTM on subsequence y1:j; h(y)i,jis theencoding of subsequence y1:jin the second LSTMinfluenced by the output of the first LSTM on sub-sequence x1:i.More concretely,(h(x)i,j, c(x)i,j) = LSTM(Hi,j, c(x)i?1,j,xi), (7)(h(y)i,j, c(y)i,j) = LSTM(Hi,j, c(y)i,j?1,xj), (8)where Hi,jis information consisting of historystates before position (i, j).The simplest setting is Hi,j= h(x)i?1,j?
h(y)i,j?1.In this case, our model can be regarded as gridLSTMs (Kalchbrenner et al, 2015).However, there are totalm?n interactions in re-cursive matching process, LSTM could be stressedto keep these interactions in internal memory.Therefore, inspired by recent neural memory net-work, such as neural Turing machine(Graves etal., 2014) and memory network (Sukhbaatar etal., 2015), we introduce two external memories tokeep the history information, which can relieve thepressure on low-capacity internal memory.Following (Tran et al, 2016), we use exter-nal memory constructed by history hidden states,which is defined asMt= {ht?K, .
.
.
,ht?1} ?
RK?d, (9)where K is the number of memory segments,which is generally instance-independent and pre-defined as hyper-parameter; d is the size of eachsegment; and htis the hidden state at time t emit-ted by LSTM.1036At position i, j, two memory blocksM(x),M(y)are used to store contextual in-formation of x and y respectively.M(x)i,j= {h(x)i?K,j, .
.
.
,h(x)i?1,j}, (10)M(y)i,j= {h(y)i,j?K, .
.
.
,h(y)i,j?1}, (11)where h(x)and h(x)are outputs of two conditionalLSTMs at different positions.The history information can be read from thesetwo memory blocks.
We denote a read vector fromexternal memories as ri,j?
Rd, which can becomputed by soft attention mechanisms.r(x)i,j= a(x)i,jM(x)i,j, (12)r(y)i,j= a(y)i,jM(y)i,j, (13)where ai,j?
RKrepresents attention distributionover the corresponding memory Mi,j?
RK?d.More concretely, each scalar ai,j,kin attentiondistribution ai,jcan be obtained:a(x)i,j,k= softmax(g(M(x)i,j,k, r(x)i?1,j,xi)), (14)a(y)i,j,k= softmax(g(M(y)i,j,k, r(y)i,j?1,yj)), (15)where Mi,j,k?
Rdrepresents the k-th row mem-ory vector at position (i, j), and g(?)
is an alignfunction defined byg(x,y, z) = vTtanh(Wa[x;y, z]), (16)where v ?
Rdis a parameter vector and Wa?Rd?3dis a parameter matrix.The history information Hi,jin Eq (7) and (8)is computed byHi,j= r(x)i,j?
r(y)i,j.
(17)By incorporating external memory blocks, DF-LSTMs allow network to re-read history interac-tion information, therefore it can more easily cap-ture complicated and long-distance matching pat-terns.
As shown in Figure 3, the forward passof DF-LSTMs can be unfolded along two dimen-sional ordering.4.1 Related ModelsOur model is inspired by some recently proposedmodels based on recurrent neural network (RNN).One kind of models is multi-dimensional re-current neural network (MD-RNN) (Graves et al,Female gymnastwarmupbeforeacompetitionGymnast get ready for a competitionFigure 3: Illustration of unfolded DF-LSTMs.2007; Graves and Schmidhuber, 2009; Byeon etal., 2015) in machine learning and computer vi-sion communities.
As mentioned above, if we justuse the neighbor states, our model can be regardedas grid LSTMs (Kalchbrenner et al, 2015).What is different is the dependency relationsbetween the current state and history states.
Ourmodel uses external memory to increase its mem-ory capacity and therefore can store large usefulinteractions of subsequences.
Thus, we can dis-cover some matching patterns with long depen-dence.Another kind of models is memory augmentedRNN, such as long short-term memory-network(Cheng et al, 2016) and recurrent memory net-work (Tran et al, 2016), which extend memorynetwork (Bahdanau et al, 2014) and equip theRNN with ability of re-reading the history infor-mation.
While they focus on sequence modelling,our model concentrates more on modelling the in-teractions of sequence pair.5 Training5.1 Task Specific OutputThere are two popular types of text matching tasksin NLP.
One is ranking task, such as communityquestion answering.
Another is classification task,such as textual entailment.We use different ways to calculate matchingscore for these two types of tasks.1.
For ranking task, the output is a scalar match-ing score, which is obtained by a linear trans-formation of the matching vector obtained byFD-LSTMs.2.
For classification task, the outputs are theprobabilities of the different classes, which1037is computed by a softmax function on thematching vector obtained by FD-LSTMs.5.2 Loss FunctionAccordingly, we use two loss functions to dealwith different sentence matching tasks.Max-Margin Loss for Ranking Task Given apositive sentence pair (X,Y ) and its correspond-ing negative pair (X,?Y ).
The matching scores(X,Y ) should be larger than s(X,?Y ).For this task, we use the contrastive max-margincriterion (Bordes et al, 2013; Socher et al, 2013)to train our model on matching task.The ranking-based loss is defined asL(X,Y,?Y ) = max(0, 1?
s(X,Y ) + s(X,?Y )).
(18)where s(X,Y ) is predicted matching score for(X,Y ).Cross-entropy Loss for Classification TaskGiven a sentence pair (X,Y ) and its label l. Theoutput?l of neural network is the probabilities ofthe different classes.
The parameters of the net-work are trained to minimise the cross-entropy ofthe predicted and true label distributions.L(X,Y ; l,?l) = ?C?j=1ljlog(?lj), (19)where l is one-hot representation of the ground-truth label l;?l is predicted probabilities of labels;C is the class number.5.3 OptimizerTo minimize the objective, we use stochastic gra-dient descent with the diagonal variant of Ada-Grad (Duchi et al, 2011).To prevent exploding gradients, we performgradient clipping by scaling the gradient when thenorm exceeds a threshold (Graves, 2013).5.4 Initialization and HyperparametersOrthogonal Initialization We use orthogonalinitialization of our LSTMs, which allows neuronsto react to the diverse patterns and is helpful totrain a multi-layer network (Saxe et al, 2013).Unsupervised Initialization The word embed-dings for all of the models are initialized with the100d GloVe vectors (840B token version, (Pen-nington et al, 2014)).
The other parameters areinitialized by randomly sampling from uniformdistribution in [?0.1, 0.1].Hyper-parameters MQA RTEK 9 9Embedding size 100 100Hidden layer size 50 100Initial learning rate 0.05 0.005Regularization 5E?5 1E?5Table 1: Hyper-parameters for our model on twotasks.Hyperparameters For each task, we useda stacked DF-LSTM and take the hyperpa-rameters which achieve the best performanceon the development set via an small gridsearch over combinations of the initial learn-ing rate [0.05, 0.0005, 0.0001], l2regularization[0.0, 5E?5, 1E?5, 1E?6] and the values of K[1, 3, 6, 9, 12].
The final hyper-parameters are setas Table 1.6 ExperimentIn this section, we investigate the empirical per-formances of our proposed model on two differenttext matching tasks: classification task (recogniz-ing textual entailment) and ranking task (matchingof question and answer).6.1 Competitor Methods?
Neural bag-of-words (NBOW): Each se-quence is represented as the sum of the em-beddings of the words it contains, then theyare concatenated and fed to a MLP.?
Single LSTM: Two sequences are encoded bya single LSTM, proposed by (Rockt?aschel etal., 2015).?
Parallel LSTMs: Two sequences are first en-coded by two LSTMs separately, then theyare concatenated and fed to a MLP.?
Attention LSTMs: Two sequences are en-coded by LSTMs with attention mechanism,proposed by (Rockt?aschel et al, 2015).?
Word-by-word Attention LSTMs: An im-proved strategy of attention LSTMs, whichintroduces word-by-word attention mecha-nism and is proposed by (Rockt?aschel et al,2015).1038Model k Train TestNBOW 100 77.9 75.1single LSTM(Rockt?aschel et al, 2015)100 83.7 80.9parallel LSTMs(Bowman et al, 2015)100 84.8 77.6Attention LSTM(Rockt?aschel et al, 2015)100 83.2 82.3Attention(w-by-w) LSTM(Rockt?aschel et al, 2015)100 83.7 83.5DF-LSTMs 100 85.2 84.6Table 2: Accuracies of our proposed model againstother neural models on SNLI corpus.6.2 Experiment-I: Recognizing TextualEntailmentRecognizing textual entailment (RTE) is a task todetermine the semantic relationship between twosentences.
We use the Stanford Natural LanguageInference Corpus (SNLI) (Bowman et al, 2015).This corpus contains 570K sentence pairs, and allof the sentences and labels stem from human an-notators.
SNLI is two orders of magnitude largerthan all other existing RTE corpora.
Therefore, themassive scale of SNLI allows us to train powerfulneural networks such as our proposed architecturein this paper.6.2.1 ResultsTable 2 shows the evaluation results on SNLI.
The2nd column of the table gives the number of hid-den states.From experimental results, we have several ex-perimental findings.The results of DF-LSTMs outperform all thecompetitor models with the same number of hid-den states while achieving comparable results tothe state-of-the-art and using much fewer param-eters, which indicate that it is effective to modelthe strong interactions of two texts in a recursivematching way.All models outperform NBOW by a large mar-gin, which indicate the importance of words orderin semantic matching.The strong interaction models surpass the weakinteraction models, for example, compared withparallel LSTMs, DF-LSTMs obtain improvementby 7.0%.6.2.2 Understanding Behaviors of Neurons inDF-LSTMsTo get an intuitive understanding of how the DF-LSTMs work on this problem, we examined theA dog is being chased by a catdog anotherby beingtoy petwith runningDog ?0.4 ?0.2 0 0.2 0.4(a) 5-th neuronAfamily is at the beach feettheir atlap wavesocean feelingenjoys familyyoung A?0.2 0 0.2 0.4 0.6 0.8(b) 11-th neuronFigure 4: Illustration of two interpretable neuronsand some word-pairs captured by these neurons.The darker patches denote the corresponding acti-vations are higher.neuron activations in the last aggregation layerwhile evaluating the test set.
We find that somecells are bound to certain roles.We refer to hi,j,kas the activation of the k-th neuron at the position of (i, j), where i ?
{1, .
.
.
, n} and j ?
{1, .
.
.
,m}.
By visualizingthe hidden state hi,j,kand analyzing the maximumactivation, we can find that there exist multipleinterpretable neurons.
For example, when somecontextualized local perspectives are semanticallyrelated at point (i, j) of the sentence pair, the ac-tivation value of hidden neuron hi,j,ktends to bemaximum, meaning that the model could capturesome reasoning patterns.Figure 4 illustrates this phenomenon.
In Fig-ure 4(a), a neuron shows its ability to monitor theword pairs with the property of describing differ-ent things of the same type.The activation in the patch, containing the wordpair ?
(cat, dog)?, is much higher than others.This is an informative pattern for the relation pre-diction of these two sentences, whose ground truthis contradiction.
An interesting thing is there aretwo ?dog?
in sentence ?
Dog running withpet toy being by another dog?.
Ourmodel ignores the useless word, which indicatesthis neuron selectively captures pattern by contex-tual understanding, not just word level interaction.In Figure 4(b), another neuron shows that it cancapture the local contextual interactions, such as?
(ocean waves, beach)?.
These patternscan be easily captured by final layer and providea strong support for the final prediction.1039Index of Cell Word or Phrase Pairs Explanation5-th(jeans, shirt), (dog, cat)(retriever, cat), (stand, sitting)different entities or eventsof the same type11-th(pool, swimming), (street, outside)(animal, dog), (grass,outside)word pair relatedto lexical entailment20-th(skateboard, skateboarding), (running, runs)(advertisement, ad), (grassy, grass)words with differentmorphology49-th(blue, blue), (wearing black, wearing white),(green uniform, red uniform)words related to color55-th(a man, two other men), (a man, two girls)(Two women, No one)subjects with singularor plural formsTable 3: Multiple interpretable neurons and the word-pairs/phrase-pairs captured by these neurons.
Thethird column gives the explanations of corresponding neuron?s behaviours.Table 3 illustrates multiple interpretable neu-rons and some representative word or phrase pairswhich can activate these neurons.
These casesshow that our model can capture contextual inter-actions beyond word level.6.2.3 Case Study for Attention AddressingMechanismExternal memory with attention addressing mech-anism enables the network explicitly to utilize thehistory information of two sentences simultane-ously.
As a by-product, the obtained attention dis-tribution over history hidden states also help usinterpret the network and discover underlying de-pendencies present in the data.To this end, we randomly sample two goodcases with entailment relation from test dataand visualize attention distributions over exter-nal memory constructed by last 9 hidden states.As shown in Figure 5(a), For the first sentencepair, when the word pair ?(competition,competition)?
are processed, the model si-multaneously selects ?warm, before?
fromone sentence and ?gymnast,ready,for?from the other, which are informative patterns andindicate our model has the capacity of capturingphrase-phrase pair.Another case in Figure 5(b) also shows by at-tention mechanism, the network can sufficientlyutilize the history information and the fusion ap-proach allows two LSTMs to share the history in-formation of each other.6.2.4 Error AnalysisAlthough our model DF-LSTMs are more sensi-tive to the discrepancy of the semantic capacitybetween two sentences, some cases still cannot be solved by our model.
For example, ourmodel gives a wrong prediction of the sen-tence pair ?A golden retriever nursespuppies/Puppies next to theirmother?, whose ground truth is entailment.
Themodel fails to realize ?nurses?
means ?nextto?.Besides, despite the large size of the trainingcorpus, it?s still very difficult to solve some cases,which depend on the combination of the worldknowledge and context-sensitive inferences.
Forexample, given an entailment pair ?Severalwomen are playing volleyball/Thewomen are hitting a ball withtheir arms?, all models predict ?neutral?.These analysis suggests that some architecturalimprovements or external world knowledge arenecessary to eliminate all errors instead of simplyscaling up the basic model.6.3 Experiment-II: Matching Question andAnswerMatching question answering (MQA) is a typicaltask for semantic matching (Zhou et al, 2013).Given a question, we need select a correct answerfrom some candidate answers.In this paper, we use the dataset collected fromYahoo!
Answers with the getByCategory func-tion provided in Yahoo!
Answers API, which pro-duces 963, 072 questions and corresponding bestanswers.
We then select the pairs in which thelength of questions and answers are both in theinterval [4, 30], thus obtaining 220, 000 questionanswer pairs to form the positive pairs.For negative pairs, we first use each question?sbest answer as a query to retrieval top 1, 000 re-1040Female  gymnast  warm  up  before  a  competitionGymnast  get  ready  for  a  competition(a)A   female  gymnast  in  black   and  red  being  coach ed  on  bar  s k il l s   T h e  female  gymnast  is  tr a ining(b)Figure 5: Examples of external memory positions attended when encoding the next word pair (bold andmarked by a box)Model k P@1(5) P@1(10)Random Guess - 20.0 10.0NBOW 50 63.9 47.6single LSTM 50 68.2 53.9parallel LSTMs 50 66.9 52.1Attention LSTMs 50 73.5 62.0Attention(w-by-w) LSTMs 50 75.1 64.0DF-LSTMs 50 76.5 65.0Table 4: Results of our proposed model againstother neural models on Yahoo!
question-answerpairs dataset.sults from the whole answer set with Lucene,where 4 or 9 answers will be selected randomlyto construct the negative pairs.The whole dataset1is divided into training, val-idation and testing data with proportion 20 : 1 : 1.Moreover, we give two test settings: selecting thebest answer from 5 and 10 candidates respectively.6.3.1 ResultsResults of MQA are shown in the Table 4. we cansee that the proposed model also shows its supe-riority on this task, which outperforms the state-of-the-arts methods on both metrics (P@1(5) andP@1(10)) with a large margin.By analyzing the evaluation results of question-answer matching in Table 4, we can see stronginteraction models (attention LSTMs, our DF-LSTMs) consistently outperform the weak interac-tion models (NBOW, parallel LSTMs) with a largemargin, which suggests the importance of mod-elling strong interaction of two sentences.7 Related WorkOur model can be regarded as a strong interactionmodel, which has been explored in previous meth-ods.One kind of methods is to compute similari-ties between all the words or phrases of the twosentences to model multiple-granularity interac-tions of two sentences, such as RAE (Socher et1http://nlp.fudan.edu.cn/data/.al., 2011), Arc-II (Hu et al, 2014),ABCNN (Yinet al, 2015),MultiGranCNN (Yin and Sch?utze,2015), Multi-Perspective CNN (He et al, 2015),MV-LSTM (Wan et al, 2016).Socher et al (2011) firstly used this paradigmfor paraphrase detection.
The representations ofwords or phrases are learned based on recursiveautoencoders.Hu et al (2014) proposed to an end-to-endarchitecture with convolutional neural network(Arc-II) to model multiple-granularity interactionsof two sentences.Wan et al (2016) used LSTM to enhance thepositional contextual interactions of the words orphrases between two sentences.
The input ofLSTM for one sentence does not involve anothersentence.Another kind of methods is to model the con-ditional encoding, in which the encoding of onesentence can be affected by another sentence.Rockt?aschel et al (2015) and Wang and Jiang(2015) used LSTM to read pairs of sequences toproduce a final representation, which can be re-garded as interaction of two sequences.
By incor-porating an attention mechanism, they got furtherimprovements to the predictive abilities.Different with these two kinds of methods, wemodel the interactions of two texts in a recursivelymatching way.
Based on this idea, we propose amodel of deep fusion LSTMs to accomplish recur-sive conditional encodings.8 Conclusion and Future WorkIn this paper, we propose a model of deep fu-sion LSTMs to capture the strong interaction fortext semantic matching.
Experiments on two largescale text matching tasks demonstrate the efficacyof our proposed model and its superiority to com-petitor models.
Besides, our visualization analysisrevealed that multiple interpretable neurons in ourmodel can capture the contextual interactions ofthe words or phrases.1041In future work, we would like to investigate ourmodel on more text matching tasks.AcknowledgmentsWe would like to thank the anonymous reviewersfor their valuable comments.
This work was par-tially funded by National Natural Science Foun-dation of China (No.
61532011, 61473092, and61472088), the National High Technology Re-search and Development Program of China (No.2015AA015408).ReferencesD.
Bahdanau, K. Cho, and Y. Bengio.
2014.
Neuralmachine translation by jointly learning to align andtranslate.
ArXiv e-prints, September.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.2013.
Translating embeddings for modeling multi-relational data.
In NIPS.Samuel R. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning.
2015.
A large anno-tated corpus for learning natural language inference.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing.Wonmin Byeon, Thomas M Breuel, Federico Raue,and Marcus Liwicki.
2015.
Scene labeling withlstm recurrent neural networks.
In Proceedings ofthe IEEE Conference on Computer Vision and Pat-tern Recognition, pages 3547?3555.Jianpeng Cheng, Li Dong, and Mirella Lapata.
2016.Long short-term memory-networks for machinereading.
arXiv preprint arXiv:1601.06733.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Jeffrey L Elman.
1990.
Finding structure in time.Cognitive science, 14(2):179?211.Alex Graves and J?urgen Schmidhuber.
2009.
Offlinehandwriting recognition with multidimensional re-current neural networks.
In Advances in Neural In-formation Processing Systems, pages 545?552.Alex Graves, Santiago Fern?andez, and J?urgen Schmid-huber.
2007.
Multi-dimensional recurrent neuralnetworks.
In Artificial Neural Networks?ICANN2007, pages 549?558.
Springer.Alex Graves, Greg Wayne, and Ivo Danihelka.2014.
Neural turing machines.
arXiv preprintarXiv:1410.5401.Alex Graves.
2013.
Generating sequenceswith recurrent neural networks.
arXiv preprintarXiv:1308.0850.Hua He, Kevin Gimpel, and Jimmy Lin.
2015.
Multi-perspective sentence similarity modeling with con-volutional neural networks.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 1576?1586.Karl Moritz Hermann, Tomas Kocisky, EdwardGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom.
2015.
Teaching ma-chines to read and comprehend.
In Advances in Neu-ral Information Processing Systems, pages 1684?1692.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network archi-tectures for matching natural language sentences.In Advances in Neural Information Processing Sys-tems.Rafal Jozefowicz, Wojciech Zaremba, and IlyaSutskever.
2015.
An empirical exploration of recur-rent network architectures.
In Proceedings of The32nd International Conference on Machine Learn-ing.Nal Kalchbrenner, Ivo Danihelka, and Alex Graves.2015.
Grid long short-term memory.
arXiv preprintarXiv:1507.01526.PengFei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu,and Xuanjing Huang.
2015.
Multi-timescale longshort-term memory neural network for modellingsentences and documents.
In Proceedings of theConference on EMNLP.Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu,Shengxian Wan, and Xueqi Cheng.
2016.
Textmatching as image recognition.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors forword representation.
Proceedings of the EmpiricialMethods in Natural Language Processing (EMNLP2014), 12:1532?1543.Xipeng Qiu and Xuanjing Huang.
2015.
Con-volutional neural tensor network architecture forcommunity-based question answering.
In Proceed-ings of International Joint Conference on ArtificialIntelligence.Tim Rockt?aschel, Edward Grefenstette, Karl MoritzHermann, Tom?a?s Ko?cisk`y, and Phil Blunsom.
2015.Reasoning about entailment with neural attention.arXiv preprint arXiv:1509.06664.1042Andrew M Saxe, James L McClelland, and Surya Gan-guli.
2013.
Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks.
arXivpreprint arXiv:1312.6120.Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-pher D Manning, and Andrew Y Ng.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural In-formation Processing Systems.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
In Ad-vances in Neural Information Processing Systems,pages 926?934.Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al2015.
End-to-end memory networks.
In Advancesin Neural Information Processing Systems, pages2431?2439.Ilya Sutskever, James Martens, and Geoffrey E Hin-ton.
2011.
Generating text with recurrent neuralnetworks.
In Proceedings of the 28th InternationalConference on Machine Learning (ICML-11), pages1017?1024.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Process-ing Systems, pages 3104?3112.Ke M. Tran, Arianna Bisazza, and Christof Monz.2016.
Recurrent memory network for languagemodeling.
CoRR, abs/1601.01272.Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu,Liang Pang, and Xueqi Cheng.
2016.
A deep ar-chitecture for semantic matching with multiple po-sitional sentence representations.
In AAAI.Shuohang Wang and Jing Jiang.
2015.
Learning nat-ural language inference with lstm.
arXiv preprintarXiv:1512.08849.Wenpeng Yin and Hinrich Sch?utze.
2015.
Convolu-tional neural network for paraphrase identification.In Proceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 901?911.Wenpeng Yin, Hinrich Sch?utze, Bing Xiang, andBowen Zhou.
2015.
Abcnn: Attention-based con-volutional neural network for modeling sentencepairs.
arXiv preprint arXiv:1512.05193.Guangyou Zhou, Yang Liu, Fang Liu, Daojian Zeng,and Jun Zhao.
2013.
Improving question retrieval incommunity question answering using world knowl-edge.
In IJCAI.1043
