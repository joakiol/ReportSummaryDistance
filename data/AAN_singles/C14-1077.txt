Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 806?815, Dublin, Ireland, August 23-29 2014.Predicate-Argument Structure Analysis with Zero-Anaphora Resolutionfor Dialogue SystemsKenji Imamura, Ryuichiro Higashinaka, and Tomoko IzumiNTT Media Intelligence Laboratories, NTT Corporation1-1 Hikari-no-oka, Yokosuka, 239-0847, Japan{imamura.kenji,higashinaka.ryuchiro,izumi.tomoko}@lab.ntt.co.jpAbstractThis paper presents predicate-argument structure analysis (PASA) for dialogue systems inJapanese.
Conventional PASA and semantic role labeling have been applied to newspaper arti-cles.
Because pronominalization and ellipses frequently appear in dialogues, we base our PASAon a strategy that simultaneously resolves zero-anaphora and adapt it to dialogues.
By incor-porating parameter adaptation and automatically acquiring knowledge from large text corpora,we achieve a PASA specialized to dialogues that has higher accuracy than that for newspaperarticles.1 IntroductionSemantic role labeling (SRL) and predicate-argument structure analysis (PASA) are important analysistechniques for acquiring ?who did what to whom?
from sentences1.
These analyses have been applied towritten texts because most annotated corpora comprise newspaper articles (Carreras and M`arquez, 2004;Carreras and M`arquez, 2005; Matsubayashi et al., 2014).Recently, systems for speech dialogue between humans and computers (e.g., Siri of Apple Inc. andShabette Concier of NTT DoCoMo) have become familiar with the popularization of smart phones.
Aman-machine dialogue system has to interpret human utterances to associate them with system utter-ances.
The predicate-argument structure could be an effective data structure for dialogue management.However, it is unclear whether we can apply the SRL/PASA for newspaper articles to dialogues becausethere are many differences between them, such as the number of speakers, written or spoken language,and context processing.
For example, the following dialogue naturally includes pronouns, and thusanaphora resolution is necessary for semantic role labeling.A: [I]ARG0want [an iPad Air]ARG1.B: [When]ARGMwill [you]ARG0buy [it(=an iPad Air)]ARG1?Similar phenomena exist in Japanese dialogues.
However, most pronouns are omitted (called zero-pronouns), and zero-anaphora resolution is necessary for Japanese PASA.A: [iPad Air]NOM-ga hoshii-na.iPad Air NOM.
want??
want an iPad Air.
?B: itsu ?NOM?ACCkau-no?when buy?
?When will ?
buy ??
?This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1Recent SRL systems assign labels of predicates and their arguments as semantic roles.
Consequently, SRL and PASA arevery similar tasks.
We use the term predicate-argument structure analysis in this paper because most Japanese analyzers usethis term.806This paper presents predicate-argument structure analysis with zero-anaphora resolution for Japanesechat dialogues.
Here, we regard the task of constructing PASA for dialogues as a kind of domain adap-tation from newspaper articles to dialogues.
M`arquez et al.
(2008) and Pradhan et al.
(2008) indicatedthat the tuning of parameter distribution and reducing the out-of-vocabulary are important for the do-main adaptation of SRL.
We also focus on parameter distribution and out-of-vocabulary to construct aPASA adapted to dialogues.
To the best of our knowledge, this is the first paper to describe a PASA fordialogues that include many zero-pronouns.The paper is organized as follows.
Section 2 briefly reviews SRL/PASA in English and Japanese.Section 3 discusses characteristics of chat dialogues by comparing two annotated corpora, newspaperarticles and dialogues.
Section 4 describes the basic strategy of our PASA, and Section 5 shows how itwas adapted for dialogues.
Experiments are presented in Section 6, and Section 7 concludes the paper.2 Related Work2.1 Semantic Role Labeling in EnglishThe advent of the supervised method proposed by Gildea and Jurafsky (2002) has led to the creation ofannotated corpora for semantic role labeling.
In the CoNLL-2004 and 2005 shared task (Carreras andM`arquez, 2004; Carreras and M`arquez, 2005), evaluations were carried out using the Proposition Bank(Palmer et al., 2005).
Because the Proposition Bank was annotated to the Penn Treebank (i.e., the sourcetexts were from the Wall Street Journal), the shared tasks were evaluated on newspaper articles.
M`arquezet al.
(2008) provides a review of SRL.OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news,broadcast conversation.
The annotation to OntoNotes includes semantic role labels compliant with theProposition Bank.
It is currently used for coreference resolution (Pradhan et al., 2012), and is expectedto be applied to dialogue analysis.A few SRL studies have focused on not only verbal predicates (e.g., ?decide?)
but also nominal predi-cates (e.g., ?decision?)
(Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013).
Becausethe subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase ?thedecision?
is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL ofnominal predicates.2.2 Predicate-Argument Structure Analyses in JapaneseJapanese material includes the NAIST Text Corpus (Iida et al., 2007)2, which is an annotated corpusof predicate-argument structures and coreference information for newspaper articles.
Argument nounphrases of the nominative, accusative, and dative cases are assigned to each predicate.
The predicate andthe noun phrases are not limited to the same sentence.
If arguments of the predicate are represented aszero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments.Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al.,2008; Imamura et al., 2009; Yoshikawa et al., 2011).
In Japanese, some of them simultaneously resolvethe zero-anaphora caused by zero-pronouns.Most English SRL and Japanese PASA currently target newspaper articles, and it is unclear whetherthe methods for newspapers can be applied to dialogue conversations.3 Characteristics of Chat DialoguesWe first collected chat dialogues of two speakers and annotated them with the predicate-argument struc-ture.
The participants chatted via keyboard input.
Therefore, fillers and repetitions, which are frequentin speech dialogues, were rare.
The theme was one of 20 topics, such as meals, travel, hobbies, andTV/radio programs.
Annotation of the predicate-argument structure complied with the NAIST Text Cor-pus.
Figure 1 shows a chat dialogue example and its predicate-argument structure annotation.2http://cl.naist.jp/nldata/corpus/.
We use version 1.5 with our own preprocessing in this paper.
NAIST isan acronym of ?Nara Institute of Science and Technology.
?807A: natsu-wa (exo2)NOM(exog)DATdekake-tari-shimashi-ta-ka?
?Did (you)NOMgo (anywhere)DATin this summer?
?B: 8-gatsu-wa Ito-no [hanabi-taikai]DAT-ni (exo1)NOMyuki-mashi-ta.?
(I)NOMwent to[the fireworks?1]DATat Ito in August.
?A:[hanabi?2]ACC,[watashi?3]NOM-mo mi-takatta-desu.?
[Fireworks?2]ACC,[I?3]NOMalso wanted to see (it).
?A: demo, kotoshi-wa (exo1)NOMisogashiku-te (exo1)NOM(*2)ACCmi-ni (*2)DATike-masen-deshita.
?But (I)NOMcouldn?t go (?2)DATto see (it=*2)ACCthis year because (I)NOMwas busy.
?Figure 1: Chat Dialogue Example and Its Predicate-Argument Structure AnnotationLower lines denote glosses of the upper lines.
The bold words denote predicates, the square brack-ets [] denote intra-sentential arguments, and the round brackets () denote inter-sentential or exophoricarguments.# of Articles # of Sentences # of Words # of PredicatesCorpus Set /Dialogues /Utterances (per Sentence) (per Sentence)NAIST Text Corpus Training 1,751 24,283 664,898 (27.4) 68,602 (2.83)Development 480 4,833 136,585 (28.3) 13,852 (2.87)Test 696 9,284 255,624 (27.5) 26,309 (2.83)Chat Dialog Corpus Training 184 6,960 61,872 (8.9) 7,470 (1.07)Test 101 4,056 38,099 (9.4) 5,333 (1.31)Table 1: Sizes of CorporaZero- Zero- ExophoraCase Corpus # of Arguments Dep Intra Inter exo1 exo2 exogNominative NAIST 68,598 54.5% 17.3% 11.4% 2.0% 0.0% 14.7%Dialogue 7,467 31.8% 7.4% 12.6% 23.9% 5.6% 18.8%Accusative NAIST 27,986 89.2% 6.9% 3.4% 0.0% 0.0% 0.4%Dialogue 1,901 46.6% 12.8% 27.5% 0.8% 0.1% 12.2%Datative NAIST 6,893 84.7% 10.2% 4.3% 0.0% 0.0% 0.8%Dialogue 2,089 37.6% 7.8% 15.0% 2.5% 1.1% 36.1%Table 2: Distribution of Arguments in Training CorporaTable 1 shows the statistics of the NAIST Text Corpus and the Chat Dialogue Corpus we created3.The size of the Dialogue Corpus is about 10% of the NAIST Corpus.
The NAIST Corpus is divided intothree parts: training, development, and test.
The Dialogue Corpus is divided into training and test.Table 2 shows distributions of arguments in the training sets of the NAIST/Dialogue corpora.
We clas-sified the arguments into the following six categories because each argument presents different difficultiesfor analysis by its position and syntactic relation.
The first two categories (Dep and Zero-Intra) arethe ones that in which the predicate and the argument occupy the same sentence.?
Dep: The argument directly depends on the predicate and vice versa on the parse tree.?
Zero-Intra: Intra-sentential zero-pronoun.
The predicate and the argument are in the samesentence, but there is no direct dependency.?
Zero-Inter: Inter-sentential zero-pronoun.
The predicate and the argument are in differentsentences.?
exo1/exo2/exog: These are exophoric and denote zero-pronouns of the first person, second per-son, and the others (general), respectively.By Table 2, we can see that the ratios of Dep in all cases decreased in the Dialogue Corpus.
In the othercategories, the tendencies between the nominative case and the accusative/dative cases were different.
Inthe nominative case, the Zero-Intra also decreased in the Dialogue Corpus, and the declines were3We regard a dialogue and an utterance as an article and a sentence, respectively.808exo1 exo2 exogNULL Phrase 1 Phrase 2 Phrase 3 Phrase 4 ?Special Noun Phrases Candidate Argumentsin Past  Sentences Candidate Argumentsin Current SentenceCandidate ArgumentsSelectorNominativeModel SelectorAccusativeModel SelectorDativeModelexo1exophoric(first person) zero-anaphoric(inter-sentential)Phrase 2 NULLno argumentFigure 2: Structure of Argument Identification and Classificationassigned to exo1 and exo2.
Namely, the arguments in a sentence were reduced, and zero-pronounsincreased compared with the newspaper articles.
Note that many antecedents were the first or secondperson.
On the other hand, in the accusative and dative cases, the declines of the Dep were assigned tothe Zero-Inter or the exog in the Dialogue Corpus.
Namely, anaphora resolution across multiplesentences is important to dialogue analysis.
In contrast, most arguments and the predicate appear in thesame sentence in the accusative/dative cases of newspapers.4 Basic Strategy for Predicate-Argument Structure Analysis and Zero-AnaphoraResolution4.1 ArchitectureWe use Imamura et al.
(2009)?s method developed for newspaper articles as the base PASA in this paper.It can simultaneously identify arguments of a predicate in the sentence, those in other sentences, andexophoric arguments.
The analyzer receives the entire article (dialogue) and performs the followingsteps for each sentence (utterance).1.
The input sentences are tagged and parsed.
During parsing, the base phrases and their headwordsare also identified.
At this time, the part-of-speech tags and the parse trees of the Dialogue Corpusare supplied by applying the morphological analyzer MeCab (Kudo et al., 2004) and the dependencyparser CaboCha (Kudo and Matsumoto, 2002).
The NAIST Corpus version 1.5 already includes thepart-of-speech tags and the parse trees.2.
Predicate phrases are identified from the sentences.
We use the correct predicates in the corporafor the evaluation.
When we build dialogue systems on PASA, predicate phrases will be identifiedusing part-of-speech patterns that include verbs, adjectives, and copular verbs.3.
For each predicate, candidate arguments are acquired from the sentence that includes the predicate(called the current sentence) and the past sentences.
Concretely, the following base phrases areregarded as candidates.?
All noun phrases in the current sentence are extracted as intra-sentential candidates regardlessof syntactic relations.?
From the past sentences, noun phrases are contextually extracted as inter-sentential candidates.Details are described in Section 4.4.?
Exophoric labels (exo1, exo2, and exog) and the NULL (the argument is not required) areadded as special noun phrases.8094.
The features are generated from the predicate phrase, candidate arguments, and their relations.
Thebest candidate for each case is independently selected (Figure 2).4.2 ModelsThe models for the selector are based on maximum entropy classification.
The selector identifies the bestnoun phrase n?
that satisfies the following equations from the candidate argument set N.n?
= argmaxnj?NP (d(nj) = 1|Xj;Mc) (1)P (d(nj) = 1|Xj;Mc) =1Zc(X)exp?k{?ckfk(d(nj) = 1, Xj)} (2)Zc(X) =?nj?Nexp?k{?ckfk(d(nj) = 1, Xj)} (3)Xj= ?nj, v, A?
(4)where n denotes a candidate argument, N denotes a set of candidate arguments of predicate v, d(n) isa function that returns 1 iff candidate n becomes the argument, and Mcdenotes the model of case c. Inaddition, fk(d(nj) = 1, Xj) is a feature function, ?ckdenotes a weight parameter of the feature function,and A denotes the article from which all sentences are parsed.Training phase optimizes the weight parameters in order to maximize the difference in posterior prob-abilities among the correct noun phrase and the other candidates.
Specifically, the model of case Mcislearnt by minimizing the following loss function `c.`c= ?
?ilogP (d(ni) = 1|Xi;Mc) +12C?k||?ck||2(5)where nidenotes the correct noun phrase of the i-th predicate in the training set, Xidenotes the i-thtuple of the correct noun phrase, the predicate, and the article ?ni, vi, Ai?.
Since the posterior probabilityis normalized for each set of candidate arguments of a predicate by Equation (3), the probability ofthe correct noun phrase approaches closer to 1.0, and the probabilities of the other candidates approachcloser to 0.0 in Equation (5).4.3 FeaturesSimilar to other studies (e.g., (Gildea and Jurafsky, 2002)), we use three types of features: 1) predicatefeatures, 2) noun phrase (NP) features, and 3) the relationship between predicates and noun phrases(Table 3).
We also introduce combined features of the ?Noun?
with all other binary features because thefeatures aim to select the best noun phrase.The special features in this paper are the dependency language models (three types) and the obligatorycase information (?Frame?
feature), which are automatically acquired from large text corpora.
We discussthem in Section 5.2.4.4 Context ProcessingContexts of dialogues and newspaper articles are different.
We should employ context processing spe-cialized for the dialogues.
However, contexts, including system and user utterances, should be managedcollectively by the dialogue manager from the viewpoint of dialogue systems.
Thus, this study uses thesame context processing for the newspaper articles and dialogues.
Note that the method in this papercontrols the context by selecting the inter-sentential candidates.
We can easily alter context managementby providing candidate arguments from an external manager.Context processing in this paper is as follows.?
From the current sentence, trace back to the past, and find a sentence that contains the other pred-icate (we call this the prior sentence).
This process aims to ignore utterances that do not containpredicates.810Type Name Value RemarkPredicate Pred Binary Lemma of the predicate.PType Binary Type of predicate.
One of ?verb?, ?adjective?, and ?copular verb?.Voice Binary Declarative or not.
If not, the passive/causative auxiliary verb is assigned.Suffix Binary Sequence of the functional words of the main clause.
This feature aims to reflectthe speech act of the utterance.Frame Binary Obligatory case information.
The case requires argument (1) or not (0).Noun Phrase Noun Binary Headword of the NPParticle Binary Case particle of the base phrase.
If the NP is a special noun phrase, this is NULL.NType Binary If the substance of the NP is in the article, this is ?NP?
; otherwise the same valueof the ?Noun?
feature.Surround Binary POS tags of the surrounding words of the NP.
The window size is ?2.RelationbetweenPredicate andNPPhPosit Binary Distance between the predicate and the NP.
If they are in different sentences, orthe NP is an exophora, this is NULL.Syn Binary Dependency path between the predicate and the NP.
If they are in different sen-tences, or the NP is an exophora, this is NULL.Speaker Binary Whether the speakers of the predicate and the NP are the same (SAME) or not(OTHER).DependencyLanguageModelslog P (n|c, v) Real Generation probability of NP n given predicate v and case c.log P (v|c, n) Real Generation probability of predicate v given NP n and case c.log P (c|n) Real Generation probability of case c given NP n.Table 3: List of Features?
All noun phrases that lie between the prior to the current sentence are added to the candidate argu-ments.
In addition, noun phrases that are used as arguments of any predicates are also added (calledargument recycling (Imamura et al., 2009)).
Argument recycling covers wide contexts because itcan employ distant noun phrases if the past predicates have inter-sentential arguments.5 Adaptation to Chat DialoguesThe method described in the previous section is common to dialogues and newspaper articles.
Thissection describes the adaptation made to target dialogues.5.1 Adaptation of Model ParametersIn order to tune the difference in the argument distribution, model parameters of the selectors are adaptedto the dialogue domain.
We use the feature augmentation method (Daum?e, 2007) as the domain adap-tation technique; it has the same effect as regarding the source domain to be prior knowledge, and theparameters are optimized to the target domain.
Concretely, the models of the selectors are learnt andapplied as follows.1.
First, the feature space is segmented into three parts: common, source, and target.2.
The NAIST Corpus and the Dialogue Corpus are regarded as the source and the target domains,respectively.
The features from the NAIST Corpus are deployed to the common and the sourcespaces, and those from the Dialogue Corpus are deployed to the common and the target spaces.3.
The parameters are estimated in the usual way on the above feature space.
The weights of thecommon features are emphasized if the features are consistent between the source and target.
Withregard to domain-dependent features, the weights in the respective space, source or target, are em-phasized.4.
When the argument is identified, the selectors use only the features in the common and target spaces.The parameters in the spaces are optimized to the target domain, plus we can utilize the featuresthat appear only in the source domain data.5.2 Weak Knowledge Acquisition from Very Large ResourcesIn this paper, we use two types of knowledge to reduce the harmful effect of out-of-vocabulary in thetraining corpus.
Both types are constructed by automatically analyzing, summing up, and filtering large811text corpora (Kawahara and Kurohashi, 2002; Sasano et al., 2008; Sasano et al., 2013).
They provideinformation about unknown words with some confidence but they do contain some errors.
We use themas the features of the models, and parameters are optimized by the discriminative learning of the selectors.5.2.1 Obligatory Case Information (Frame Feature)Case frames are important clues for SRL and PASA.
The obligatory case information (OCI) comprisessubsets of the case frames that only clarify whether the cases of each predicate are necessary or not.The OCI dictionary is automatically constructed from large text corpora as follows.
The processassumes that 1) most of the cases match the case markers if the noun phrase directly depends on thepredicate, and 2) if the case is obligatory, the occurrence rate on a specific predicate is higher than theaverage rate of all predicates.1.
Similar to PASA in this paper (c.f., Section 4.1), predicates and base phrases are identified bytagging and parsing raw texts.2.
Noun phrases that directly depend on the predicate and accompany a case marker are extracted.
Wesum up the frequency of the predicate and cases.3.
Highly frequent predicates are selected according to the final dictionary size.
Obligation of the casesis determined so as to satisfy the following two conditions.?
Co-occurrence of the predicate and the case ?v, c?
are higher than the significance level (p ?0.001; LLR ?
10.83) by the log-likelihood-ratio test.?
The case of the predicate appears at least 10% more frequently than the average of all predi-cates.We constructed two OCI dictionaries.
The Blog dictionary contains about 480k predicates from oneyear of blogs (about 2.3G sentences,).
The News dictionary contains about 200k predicates from 12years of newspaper articles (about 7.7M sentences).
The coverage of predicates in the training set of theDialogue Corpus was 98.5% by the Blog dictionary and 96.4% by the News dictionary.5.2.2 Dependency Language ModelsDependency language models (LMs) represent semantic/pragmatic collocations among predicate v, casec, and noun phrase n. The generation probabilities of v, c, and n are computed by n-gram models.
Moreconcretely, the following real values are computed.
The purpose of the biases (probabilities involved<unk>) is to correct the values to be positive.?
logP (n|c, v) ?
logP (<unk>|c, v)?
logP (v|c, n) ?
logP (v|c,<unk>)?
logP (c|n) ?
logP (c|<unk>)Each dependency LM is constructed from the tuples of ?v, c, n?
extracted in Section 5.2.1 using theSRILM (Stolcke et al., 2011).
Note that since the obligatory case information corresponds to the gener-ation probability of the case (P (c|v)), we exclude it from the dependency LMs.Similar to the OCI dictionaries, we constructed two sets of dependency language models from the Blogand the News sentences.
The coverage of triples ?v, c, n?
appeared in the training set of the DialogueCorpus was 76.4% by the Blog LMs and 38.3% by the News LMs.
The Blog LMs cover the DialogueCorpus more comprehensively than the News LMs.6 ExperimentsWe evaluate the accuracies of the proposed PASA on the Dialogue Corpus (Table 1) from the perspectivesof parameter adaptation and the effect of the automatically acquired knowledge.
The evaluation metricis F-measure of each case (includes exophora identification).812a) Adaptation b) NAIST?
c) Dialogue?
d) Adaptation e) Adaptation# of OCI:Blog OCI:Blog OCI:Blog OCI:News?
OCI:BlogCase Type Args.
LMs:Blog LMs:Blog LMs:Blog LMs:Blog LMs:News?Nominative Dep 1,811 83.3%??
77.6% 82.7% 83.0% 82.7%Zero-Intra 511 37.4% 43.7%?
36.6% 36.5% 38.1%Zero-Inter 767 8.6% ?
9.1% 9.0% 8.3% 4.5%exo1 1,193 70.2%?
13.5% 69.9% 70.1% 70.3%exo2 281 46.8%??
0.0% 43.1% 47.2% 46.8%exog 767 46.8%?
32.5% 27.9% 47.2% 47.7%?Total 5,330 61.5%?
44.4% 61.1% 61.4% 61.4%Accusative Dep 614 84.2%??
?
78.6% 81.5% 84.2% 82.4%Zero-Intra 149 42.9%?
??
27.1% 45.0% 38.9% 34.3%Zero-Inter 399 30.4%?
?
0.5% 30.9% 29.4% 24.3%exo1 19 0.0% 0.0% 0.0% 9.5% 10.0%exo2 7 0.0% 0.0% 0.0% 0.0% 0.0%exog 98 25.6%?
0.0% 27.9% 25.2% 25.6%Total 1,286 59.0%?
?
51.6% 58.9% 58.4% 56.0%Dative Dep 566 80.5%??
54.0% 79.0% 80.1% 80.7%Zero-Intra 70 20.7%?
?
0.0% 20.0% 20.7% 11.8%Zero-Inter 169 14.6%?
0.0% 14.8% 14.4% 13.4%exo1 32 0.0% 0.0% 0.0% 0.0% 0.0%exo2 4 0.0% 0.0% 0.0% 0.0% 0.0%exog 265 45.4%??
0.0% 43.1% 44.0% 44.9%Total 1,106 58.6%??
32.2% 57.2% 58.2% 58.4%Table 4: F-measures among Methods/OCI dictionary/Dependency LMs on Dialogue Test SetThe bold values denote the highest F-measures among all methods.
The marks ?, ?, ?, ?
denote sig-nificantly better methods by comparing a) with b), c), d), and e), respectively.
We used the bootstrapresampling method (1,000 iterations) as the significance test, in which the significance level was 0.05.6.1 Experiment 1: Effect of Parameter AdaptationWe compared three methods in order to evaluate parameter adaptation: a) The feature augmentation isapplied to the training (Adaptation).
b) Only the NAIST Corpus is used for training (NAIST Training).c) Only the Dialogue Corpus is used (Dialogue Training).
The NAIST Training corresponds to a conven-tional PASA for newspaper articles.
The results on the Dialogue test set are shown in the 4th, 5th, and6th columns in Table 4.First, comparing methods a) Adaptation and b) NAIST training, Adaptation was better than the NAISTtraining for most types (The ?
mark denotes ?significantly better?).
In particular, the total F-measuresof all cases were significantly better than NAIST training.
Focusing on the types of arguments, the mostcharacteristic results were exophoras of the first/second persons (exo1 and exo2) of the nominativecase.
These two types dominate of the nominative case (about 28%), and exo1 (70.2%) and exo2(46.8%) became analyzable.
Other types such as the Zero-Inter and the exog of the accusative anddative cases, which could not be analyzed by NAIST training, became analyzable.Comparing methods a) Adaptation and c) Dialogue training (c.f., ?
), the F-measures of Dialoguetraining approached those of Adaptation even though the size of the Dialogue Corpus was small.
Onlythe F-measure of the dative case of Adaptation was significantly better than Dialogue training in total.This does not imply that the corpus size is sufficient.
Rather, we suppose that the Adaptation strategycould not adequately utilize the advantages of the NAIST Corpus.
Adding more dialogue data wouldfurther improve the accuracies on the Dialogue test set.6.2 Experiment 2: Differences among Automatically Acquired KnowledgeThe columns a), d), and e) in Table 4 show the results for the proposed method (Adaptation).
Note thatthe combination of the OCI dictionary and the dependency language models were changed to a) ?Blog,Blog?, d) ?News, Blog?, and e) ?Blog, News?.When the OCI dictionary was changed from a) Blog to d) News (c.f., ?
), there were no significantdifferences in almost all types except for the Zero-Intra of the accusative case.
We suppose that this813is because the coverage of the Blog and News dictionaries were almost the same, and obligatory cases ofpredicates are general information regardless of the domain.On the contrary, when the dependency LMs were changed from a) Blog to e) News (c.f., ?
), the F-measures of some types significantly dropped, especially the Zero-Intra and Zero-Inter types,which are strongly influenced by semantic relation.
For example, the Zero-Inter type of the ac-cusative case was changed from 30.4% to 24.3%, and the F-measure consequently decreased by 3.0points in total in the accusative case.
Zero-anaphora resolution cannot rely on syntax, and the dependencyLMs that measure semantic collocation become relatively important.
The Blog LMs yielded greater cov-erage than the News LMs in this experiment.
We can conclude that high-coverage LMs are better forimproving the zero-anaphora resolution.7 ConclusionThis paper presented predicate-argument structure analysis with zero-anaphora resolution for dialogues.We regarded this task as a kind of domain adaptation from newspaper articles, which are conventionallystudied, to dialogues.
The model parameters were adapted to the dialogues by using a domain adapta-tion technique.
In order to address the out-of-vocabulary issue, the obligatory case information and thedependency language models were constructed from large text corpora and applied to the selectors.As a result, arguments that could not be analyzed by PASA for newspaper articles (e.g., zero-pronounsof the first and second persons in the nominative case) became analyzable by adding only a small numberof dialogues.
The parameter adaptation achieved some improvement.
Moreover, we confirmed that high-coverage dependency LMs contribute to improving zero-anaphora resolution and the overall accuracy.Although we focused on parameter distribution and out-of-vocabulary in this paper, there are the otherdifferences between dialogues and newspaper articles.
For example, we did not discuss the exchangeof turns, which is a special phenomenon of dialogues.
To consider further phenomena is our futurework.
We are also evaluating the effectiveness of our PASA by incorporating it into a dialogue system(Higashinaka et al., 2014).ReferencesXavier Carreras and Llu?
?s M`arquez.
2004.
Introduction to the CoNLL-2004 shared task: Semantic role labeling.In Hwee Tou Ng and Ellen Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Conference on ComputationalNatural Language Learning (CoNLL-2004), pages 89?97, Boston, Massachusetts, USA, May.Xavier Carreras and Llu?
?s M`arquez.
2005.
Introduction to the CoNLL-2005 shared task: Semantic role labeling.In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages152?164, Ann Arbor, Michigan, June.Hal Daum?e, III.
2007.
Frustratingly easy domain adaptation.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 256?263, Prague, Czech Republic, June.Matthew Gerber and Joyce Y. Chai.
2012.
Semantic role labeling of implicit arguments for nominal predicates.Computational Linguistics, 38(4):755?798.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic labeling of semantic roles.
Computational Linguistics,28(3):245?288.Ryuichiro Higashinaka, Kenji Imamura, Toyomi Meguro, Chiaki Miyazaki, Nozomi Kobayashi, HiroakiSugiyama, Toru Hirano, Toshiro Makino, and Yoshihiro Matsuo.
2014.
Towards an open domain conversa-tional system fully based on natural language processing.
In Proceedings of the 25th International Conferenceon Computational Linguistics (COLING 2014), Dublin, Ireland, August.Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel.
2006.
OntoNotes: The90% solution.
In Proceedings of the Human Language Technology Conference of the NAACL, CompanionVolume: Short Papers, pages 57?60, New York City, USA, June.Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Matsumoto.
2007.
Annotating a Japanese text corpus withpredicate-argument and coreference relations.
In Proceedings of the Linguistic Annotation Workshop, pages132?139, Prague, Czech Republic, June.814Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009.
Discriminative approach to predicate-argument structureanalysis with zero-anaphora resolution.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,pages 85?88, Singapore, August.Zheng Ping Jiang and Hwee Tou Ng.
2006.
Semantic role labeling of NomBank: A maximum entropy approach.In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 138?145,Sydney, Australia, July.Daisuke Kawahara and Sadao Kurohashi.
2002.
Fertilization of case frame dictionary for robust Japanese caseanalysis.
In Proceedings of the 19th International Conference on Computational Linguistics (COLING-2002),pages 425?431, Taipei, Taiwan, August.Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Matsumoto.
2007.
Learning-based argument structure analy-sis of event-nouns in Japanese.
In Proceedings of the Conference of the Pacific Association for ComputationalLinguistics (PACLING), pages 208?215, Melbourne, Australia, September.Taku Kudo and Yuji Matsumoto.
2002.
Japanese dependency analysis using cascaded chunking.
In CoNLL2002: Proceedings of the 6th Conference on Natural Language Learning 2002 (COLING 2002 Post-ConferenceWorkshops), pages 63?69, Taipei, Taiwan.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004.
Applying conditional random fields to Japanesemorphological analysis.
In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 230?237,Barcelona, Spain, July.Egoitz Laparra and German Rigau.
2013.
ImpAr: A deterministic algorithm for implicit semantic role labelling.In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), pages 1180?1189, Sofia, Bulgaria, August.Llu?
?s M`arquez, Xavier Carreras, Kenneth C. Litkowski, and Suzanne Stevenson.
2008.
Semantic role labeling:An introduction to the special issue.
Computational Linguistics, 34(2):145?159.Yuichiro Matsubayashi, Ryu Iida, Ryohei Sasano, Hikaru Yokono, Suguru Matsuyoshi, Atsushi Fujita, YusukeMiyao, and Kentaro Inui.
2014.
Issues on annotation guidelines for Japanese predicate-argument structures.Journal of Natural Language Processing, 21(2):333?377, April.
in Japanese.Martha Palmer, Daniel Gildia, and Paul Kingsbury.
2005.
The proposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?105.Sameer S. Pradhan, Wayne Ward, and James H. Martin.
2008.
Towards robust semantic role labeling.
Computa-tional Linguistics, 34(2):289?310.Sameer Pradhan, Alessandro Moschitti, and Nianwen Xue, editors.
2012.
Joint Conference on EMNLP andCoNLL: Proceeding of the Shared Task: Modeling Multilingual Unrestricted Coreference in Onto Notes, Jeju,Korea, July.Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi.
2008.
A fully-lexicalized probabilistic model forJapanese zero anaphora resolution.
In Proceedings of the 22nd International Conference on ComputationalLinguistics (Coling 2008), pages 769?776, Manchester, UK, August.Ryohei Sasano, Daisuke Kawahara, Sadao Kurohashi, and Manabu Okumura.
2013.
Automatic knowledge ac-quisition for case alternation between the passive and active voices in Japanese.
In Proceedings of the 2013Conference on Empirical Methods in Natural Language Processing, pages 1213?1223, Seattle, Washington,USA, October.Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash.
2011.
SRILM at sixteen: Update and outlook.In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2011), Waikoloa,Hawaii, December.Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata.
2008.
A Japanese predicate argument structure analysis usingdecision lists.
In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,pages 523?532, Honolulu, Hawaii, October.Katsumasa Yoshikawa, Masayuki Asahara, and Yuji Matsumoto.
2011.
Jointly extracting Japanese predicate-argument relation with markov logic.
In Proceedings of 5th International Joint Conference on Natural Lan-guage Processing, pages 1125?1133, Chiang Mai, Thailand, November.815
