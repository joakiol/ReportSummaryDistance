Efficient Parsing for Bilexical Context-Free Grammarsand Head Automaton  Grammars*Jason EisnerDept.
of Computer ~ Information ScienceUniversity of Pennsylvania200 South 33rd Street,Philadelphia, PA 19104 USAj eisner@linc, cis.
upenn, eduGiorgio SattaDip.
di Elettronica e InformaticaUniversit?
di Padovavia Gradenigo 6/A,35131 Padova, Italysatt a@dei, unipd, itAbst ractSeveral recent stochastic parsers use bilexicalgrammars, where each word type idiosyncrat-ically prefers particular complements with par-ticular head words.
We present O(n 4) parsingalgorithms for two bilexical formalisms, improv-ing the prior upper bounds of O(n5).
For a com-mon special case that was known to allow O(n 3)parsing (Eisner, 1997), we present an O(n 3) al-gorithm with an improved grammar constant.1 Introduct ionLexicalized grammar formalisms are of boththeoretical and practical interest o the com-putational inguistics community.
Such for-malisms pecify syntactic facts about each wordof the language--in particular, the type ofarguments that the word can or must take.Early mechanisms of this sort included catego-rial grammar (Bar-Hillel, 1953) and subcatego-rization frames (Chomsky, 1965).
Other lexi-calized formalisms include (Schabes et al, 1988;Mel'~uk, 1988; Pollard and Sag, 1994).Besides the possible arguments of a word, anatural-language rammar does well to specifypossible head words for those arguments.
"Con-vene" requires an NP object, but some NPs aremore semantically or lexically appropriate herethan others, and the appropriateness dependslargely on the NP's head (e.g., "meeting").
Weuse the general term bilexical for a grammarthat records such facts.
A bilexical grammarmakes many stipulations about the compatibil-ity of particular pairs of words in particularroles.
The acceptability of "Nora convened the" The authors were supported respectively under ARPAGrant N6600194-C-6043 "Human Language Technology"and Ministero dell'Universitk e della Ricerca Scientificae Tecnologica project "Methodologies and Tools of HighPerformance Systems for Multimedia Applications.
"party" then depends on the grammar writer'sassessment of whether parties can be convened.Several recent real-world parsers have im-proved state-of-the-art parsing accuracy by re-lying on probabilistic or weighted versions ofbilexical grammars (Alshawi, 1996; Eisner,1996; Charniak, 1997; Collins, 1997).
The ra-tionale is that soft selectional restrictions playa crucial role in disambiguation, iThe chart parsing algorithms used by most ofthe above authors run in time O(nS), becausebilexical grammars are enormous (the part ofthe grammar elevant o a length-n input hassize O(n 2) in practice).
Heavy probabilisticpruning is therefore needed to get acceptableruntimes.
But in this paper we show that thecomplexity is not so bad after all:?
For bilexicalized context-free grammars,O(n 4) is possible.?
The O(n 4) result also holds for head au-tomaton grammars.?
For a very common special case of thesegrammars where an O(n 3) algorithm waspreviously known (Eisner, 1997), the gram-mar constant can be reduced withoutharming the O(n 3) property.Our algorithmic technique throughout is to pro-pose new kinds of subderivations that are notconstituents.
We use dynamic programming toassemble such subderivations into a full parse.2 Notat ion  for context - f reegrammarsThe reader is assumed to be familiar withcontext-free grammars.
Our notation fol-1Other elevant parsers imultaneously consider twoor more words that are not necessarily n a dependencyrelationship (Lafferty et al, 1992; Magerman, 1995;Collins and Brooks, 1995; Chelba nd Jelinek, 1998).457lows (Harrison, 1978; Hopcroft and Ullman,1979).
A context-free grammar (CFG) is a tupleG = (VN, VT, P, S), where VN and VT are finite,disjoint sets of nonterminal and terminal sym-bols, respectively, and S E VN is the start sym-bol.
Set P is a finite set of productions havingthe form A --+ a, where A E VN, a E (VN U VT)*.If every production in P has the form A -+ BCor A --+ a, for A ,B ,C  E VN,a E VT, then thegrammar is said to be in Chomsky Normal Form(CNF).
2 Every language that can be generatedby a CFG can also be generated by a CFG inCNF.In this paper we adopt the following conven-tions: a, b, c, d denote symbols in VT, w, x, y de-note strings in V~, and a, ~, .
.
.
denote stringsin (VN t_J VT)*.
The input to the parser will be aCFG G together with a string of terminal sym-bols to be parsed, w = did2.., dn.
Also h, i , j ,kdenote positive integers, which are assumed tobe ~ n when we are treating them as indicesinto w. We write wi,j for the input substringdi'."
dj  (and put wi , j  = e for i > j).A "derives" relation, written =~, is associatedwith a CFG as usual.
We also use the reflexiveand transitive closure of o ,  written ~*,  anddefine L(G) accordingly.
We write a fl 5 =~*a75 for a derivation in which only fl is rewritten.3 B i lex ica l  context - f ree  grammarsWe introduce next a grammar formalism thatcaptures lexical dependencies among pairs ofwords in VT.
This formalism closely resem-bles stochastic grammatical formalisms that areused in several existing natural language pro-cessing systems (see ?1).
We will specify a non-stochastic version, noting that probabilities orother weights may be attached to the rewriterules exactly as in stochastic CFG (Gonzalesand Thomason, 1978; Wetherell, 1980).
(See?4 for brief discussion.
)Suppose G = (VN, VT, P,T\[$\]) is a CFG inCNF.
3 We say that G is bi lexical iff there existsa set of "delexicalized nonterminals" VD suchthat VN = {A\[a\] : A E VD,a E VT} and everyproduction in P has one of the following forms:2Product ion S --~ e is also allowed in a CNF grammarif S never appears  on the r ight side of any product ion.However, S --+ e is not allowed in our bilexical CFGs.,awe have a more general definit ion that  drops therestr ict ion to CNF,  but  do not give it here.?
A\[a\] ~ B\[b\] C\[a\] (1)?
A\[a\] --+ C\[a\] B\[b\] (2)?
A\[a\] ~ a (3)Thus every nonterminal is lexical ized at someterminal a.
A constituent of nonterminal typeA\[a\] is said to have terminal symbol a as its lex-ical head,  "inherited" from the constituent'shead chi ld in the parse tree (e.g., C\[a\]).Notice that the start symbol is necessarily alexicalized nonterminal, T\[$\].
Hence $ appearsin every string of L(G); it is usually convenientto define G so that the language of interest isactually L'(G) = {x: x$ E L(G)}.Such a grammar can encode lexically specificpreferences.
For example, P might contain theproductions?
VP \[solve\] --+ V\[solve\] NP\[puzzles\]?
NP\[puzzles\] --+ DEW\[two\] N\[puzzles\]?
V\[solve\] ~ solve?
N\[puzzles\] --4 puzzles?
DEW\[two\] --+ twoin order to allow the derivation VP\[solve\] ~*solve two puzzles, but meanwhile omit the sim-ilar productions?
VP\[eat\] -+ V\[eat\] NP\[puzzles\]?
VP\[solve\] --~ V\[solve\] NP\[goat\]?
VP\[sleep\] -+ V\[sleep\] NP\[goat\]?
NP\[goat\] -+ DET\[two\] N\[goat\]since puzzles are not edible, a goat is not solv-able, "sleep" is intransitive, and "goat" cannottake plural determiners.
(A stochastic versionof the grammar could implement "soft prefer-ences" by allowing the rules in the second groupbut assigning them various low probabilities.
)The cost of this expressiveness i  a very largegrammar.
Standard context-free parsing algo-rithms are inefficient in such a case.
The CKYalgorithm (Younger, 1967; Aho and Ullman,1972) is time O(n 3.
IPI), where in the worst caseIPI = \[VNI 3 (one ignores unary productions).For a bilexical grammar, the worst case is IPI =I VD 13.
IVT 12, which is large for a large vocabularyVT.
We may improve the analysis omewhat byobserving that when parsing dl ... dn, the CKYalgorithm only considers nonterminals of theform A\[di\]; by restricting to the relevant pro-ductions we obtain O(n 3.
IVDI 3. min(n, IVTI)2).458We observe that in practical applications wealways have n << IVTI.
Let us then restrictour analysis to the (infinite) set of input in-stances of the parsing problem that satisfy re-lation n < IVTI.
With this assumption, theasymptotic time complexity of the CKY algo-rithm becomes O(n 5.
IVDt3).
In other words,it is a factor of n 2 slower than a comparablenon-lexicalized CFG.4 B i lex ica l  CFG in t ime O(n 4)In this section we give a recognition algorithmfor bilexical CNF context-free grammars, whichruns in time O(n 4. max(p, IVDI2)) = O(n 4.IVDI3).
Here p is the maximum number of pro-ductions haring the same pair of terminal sym-bols (e.g., the pair (b, a) in production (1)).
Thenew algorithm is asymptotically more efficientthan the CKY algorithm, when restricted to in-put instances atisfying the relation n < IVTI.Where CKY recognizes only constituent sub-strings of the input, the new algorithm can rec-ognize three types of subderivations, shown anddescribed in Figure l(a).
A declarative specifi-cation of the algorithm is given in Figure l(b).The derivability conditions of (a) are guaran-teed by (b), by induction, and the correctness ofthe acceptance condition (see caption) follows.This declarative specification, like CKY, maybe implemented by bottom-up dynamic pro-gramming.
We sketch one such method.
Foreach possible item, as shown in (a), we maintaina bit (indexed by the parameters of the item)that records whether the item has been derivedyet.
All these bits are initially zero.
The algo-rithm makes a single pass through the possibleitems, setting the bit for each if it can be derivedusing any rule in (b) from items whose bits arealready set.
At the end of this pass it is straight-forward to test whether to accept w (see cap-tion).
The pass considers the items in increas-ing order of width, where the width of an itemin (a) is defined as max{h,i,j} -min{h,i,j}.Among items of the same width, those of typeA should be considered last.The algorithm requires pace proportional tothe number of possible items, which is at mostna\]VDI 2.
Each of the five rule templates caninstantiate its free variables in at most n4p or(for COMPLETE rules) n41VDI 2 different ways,each of which is tested once and in constanttime; so the runtime is O(n 4 max(p, IVDI2)).By comparison, the CKY algorithm uses onlythe first type of item, and relies on rules whoseB Cinputs are pairs .~ .~ .
z~: :~ .
Such rulescan be instantiated in O(n 5) different ways for afixed grammar, yielding O(n 5) time complexity.The new algorithm saves a factor of n by com-bining those two constituents in two steps, oneof which is insensitive to k and abstracts over itspossible values, the other of which is insensitiveto h ~ and abstracts over its possible values.It is straightforward to turn the new O(n 4)recognition algorithm into a parser for stochas-tic bilexical CFGs (or other weighted bilexicalCFGs).
In a stochastic CFG, each nonterminalA\[a\] is accompanied by a probability distribu-tion over productions of the form A\[a\] --+ ~.
ATis just a derivation (proof tree) of lZ~n , .o  parseand its probability--like that of any derivationwe find--is defined as the product of the prob-abilities of all productions used to condition in-ference rules in the proof tree.
The highest-probability derivation for any item can be re-constructed recursively at the end of the parse,provided that each item maintains not only abit indicating whether it can be derived, butalso the probability and instantiated root ruleof its highest-probability derivation tree.5 A more  eff ic ient var iantWe now give a variant of the algorithm of ?4; thevariant has the same asymptotic complexity butwill often be faster in practice.Notice that the ATTACH-LEFT rule of Fig-ure l(b) tries to combine the nonterminal labelB\[dh,\] of a previously derived constituent withevery possible nonterminal label of the formC\[dh\].
The improved version, shown in Figure 2,restricts C\[dh\] to be the label of a previously de-rived adjacent constituent.
This improves peedif there are not many such constituents and wecan enumerate hem in O(1) time apiece (usinga sparse parse table to store the derived items).It is necessary to use an agenda data struc-ture (Kay, 1986) when implementing the declar-ative algorithm of Figure 2.
Deriving narroweritems before wider ones as before will not workhere because the rule HALVE derives narrowitems from wide ones.459(a)Ai4 ,AAh z j(i g h < j ,  A E VD)(i < j <h,A ,  C E VD)(h < i < j, A, C E VD)is derived iff A\[dh\] ~* wi,jis derived iff A\[dh\] ~ B\[dh,\]C\[dh\] ~* wi,jC\[dh\] for some B, h'is derived iff A\[dh\] ~ C\[dh\]B\[dh,\] ~* C\[dh\]wi,j for some B, h'(b) STAaT: ~ A\[dh\] ~ dhh@hATTACH-LEFT: BA.
/Q" .
c~ 3 hATTACH-RIGHT: B.4h ~ 3A\[dh\] -~ B\[dh,\]C\[dh\]A\[dh\] -~ C\[dh\]B\[dh,\]COMPLETE-RIGHT:COMPLETE-LEFT:A C3 h jAiz kC AA iz@kFigure 1: An O(n 4) recognition algorithm for CNF bilexical CFG.
(a) Types of items in theparse table (chart).
The first is syntactic sugar for the tuple \[A, A, i, h,j\], and so on.
The statedconditions assume that d l , .
.
.dn  are all distinct.
(b) Inference rules.
The algorithm derives theitem below - -  if the items above - -  have already been derived and any condition to the rightof is met.
It accepts input w just if item I/k, T, 1, h, n\] is derived for some h such that dh -= $.
(a)AAi//\]h ( i <_ h, A e VD)Ah~ (h < j, A E VD),~ .
~C (i _< j < h, A,C E VD)3 hAAC ~ .
(h < i < j, A,C E VD)h ~ 3(i < h _< j, A E VD) is derived iff A\[dh\] ~* wi,jis derived iff A\[dh\] ~* wi,j for some j _> his derived iff A\[dh\] ~* w~,j for some i _< his derived iff A\[dh\] ~ B\[dh,\]C\[dh\] ~* wi,jC\[dh\] ~* wi,k forsome B, h ~, kis derived iff A\[dh\] ~ C\[dh\]B\[dh,\] ~* C\[dh\]wi,j ~* Wk,j forsome B, h ~, k(b) As in Figure l(b) above, but add HALVE and change ATTACH-LEFT and ATTACH-RIGHT as shown.HALVE:  ATTACH-LEFT: ATTACH-RIGHT:A B C C BA A A A\[dh\] ---4 B\[dh,\]V\[dh\] d d\[dh\] ---+ C\[dh\]B\[dh,\]Figure 2: A more efficient variant of the O(n 4) algorithm in Figure 1, in the same format.4606 Mu l t ip le  word  sensesRather than parsing an input string directly, itis often desirable to parse another string relatedby a (possibly stochastic) transduction.
Let Tbe a finite-state transducer that maps a mor-pheme sequence w E V~ to its orthographic re-alization, a grapheme sequence v~.
T may re-alize arbitrary morphological processes, includ-ing affixation, local clitic movement, deletionof phonological nulls, forbidden or dispreferredk-grams, typographical errors, and mapping ofmultiple senses onto the same grapheme.
Givengrammar G and an input @, we ask whetherE T(L(G)).
We have extended all the algo-r ithms in this paper to this case: the items sim-ply keep track of the transducer state as well.Due to space constraints, we sketch only thespecial case of multiple senses.
Suppose thatthe input is ~ =d l  .
.
.
dn, and each di has up to?
g possible senses.
Each item now needs to trackits head's sense along with its head's position in@.
Wherever an item formerly recorded a headposition h (similarly h~), it must now record apair (h, dh) , where dh E VT is a specific sense ofd-h. No rule in Figures 1-2 (or Figure 3 below)will mention more than two such pairs.
So thetime complexity increases by a factor of O(g2).7 Head automaton  grammars  int ime O(n 4)In this section we show that a length-n stringgenerated by a head automaton grammar (A1-shawi, 1996) can be parsed in time O(n4).
Wedo this by providing a translation from headautomaton grammars to bilexical CFGs.
4 Thisresult improves on the head-automaton parsingalgorithm given by Alshawi, which is analogousto the CKY algorithm on bilexical CFGs and islikewise O(n 5) in practice (see ?3).A head  automaton  grammar  (HAG) is afunction H : a ~ Ha that defines a head  au-tomaton  (HA) for each element of its (finite)domain.
Let VT =- domain(H) and D = {~,  +---}.
A special symbol $ E VT plays the role ofstart symbol.
For each a E VT, Ha is a tuple(Qa, VT, (~a, In, Fa),  where?
Qa is a f inite set of  states;4Translation in the other direction is possible if theHAG formalism is extended to allow multiple senses perword (see ?6).
This makes the formalisms equivalent.?
In, Fa C Qa are sets of initial and finalstates, respectively;?
5a is a transition function mapping Qa xVT ?
D to 2 Qa, the power set of Qa.A single head automaton is an acceptor for alanguage of string pairs (z~, Zr) E V~ x V~.
In-formally, if b is the leftmost symbol of Zr andq~ E 5a(q, b, -~), then Ha can move from state qto state q~, matching symbol b and removing itfrom the left end of Zr.
Symmetrically, if b is therightmost symbol of zl and ql E 5a(q, b, ~---) thenfrom q Ha can move to q~, matching symbol band removing it from the right end of zl.5More formally, we associate with the head au-tomaton Ha a "derives" relation F-a, defined asa binary relation on Qa ?
V~ x V~.
For ev-ery q E Q, x,y E V~, b E VT, d E D, andq' E ~a(q, b, d), we specify that(q, xb, y) ~-a (q',x,Y) if d =+-;(q, x, by) ~-a (q', x, y) if d =--+.The reflexive and transitive closure of F-a is writ-ten ~-~.
The language generated by Ha is the setL(Ha) = {<zl,Zr) I (q, zl,Zr) I - ;  (r,e,e),qEIa,  rEFa}.We may now define the language generatedby the entire grammar H. To generate, we ex-pand the start word $ E VT into xSy for some(x, y) E L(H$), and then recursively expand thewords in strings x and y.
More formally, givenH, we simultaneously define La for all a E VTto be minimal such that if (x,y) E L(Ha),x r E Lx, yl ELy ,  then x~ay ~ E La, whereLal...ak stands for the concatenation languageLal "'" La k. Then H generates language L$.We next present a simple construction thattransforms a HAG H into a bilexical CFG Ggenerating the same language.
The construc-tion also preserves derivation ambiguity.
Thismeans that for each string w, there is a linear-t ime 1-to-1 mapping between (appropriately de-~Alshawi (1996) describes HAs as accepting (or equiv-alently, generating) zl and z~ from the outside in.
Tomake Figure 3 easier to follow, we have defined HAs asaccepting symbols in the opposite order, from the in-side out.
This amounts to the same thing if transitionsare reversed, Is is exchanged with Fa, and any transi-tion probabilities are replaced by those of the reversedMarkov chain.461fined) canonical derivations of w by H andcanonical derivations of w by G.We adopt the notation above for H and thecomponents of its head automata.
Let VD bean arbitrary set of size t = max{\[Qa\[ : a ?
VT},and for each a, define an arbitrary injection fa :Qa --+ YD.
We define G -- (VN, VT, P,T\[$\]),where(i) VN = {A\[a\] : A ?
VD, a ?
VT}, in the usualmanner for bilexical CFG;(ii) P is the set of all productions having oneof the following forms, where a, b ?
VT:?
A\[a\] --+ B\[b\] C\[a\] whereA = fa(r), B = fb(q'), C = f~(q) forsome qr ?
Ib, q ?
Qa, r ?
5a(q, b, +-)?
A\[a\] -~ C\[a\] Bib\] whereA = fa(r), B = fb(q'), C = fa(q) forsome q' ?
Ib, q ?
Qa, r ?
5a (q, b,--+)\] ?
A\[a --+ a whereA = fa(q) for some q ?
Fa(iii) T = f$(q), where we assume WLOG thatI$ is a singleton set {q}.We omit the formal proof that G and Hadmit isomorphic derivations and hence gen-erate the same languages, observing only thatif (x,y) = (bib2... bj, bj+l.
.
,  bk) E L(Ha)- -a condition used in defining La above--theng\[a\] 3"  BI\[bl\]"" Bj\[bj\]aBj+l\[bj+l\]... Bk\[bk\],for any A, B1, .
.
.
Bk that map to initial statesin Ha, Hbl , .
.
.
Hb~ respectively.In general, G has p = O(IVDI 3) = O(t3).
Theconstruction therefore implies that we can parsea length-n sentence under H in time O(n4t3).
Ifthe HAs in H happen to be deterministic, thenin each binary production given by (ii) above,symbol A is fully determined by a, b, and C. Inthis case p = O(t2), so the parser will operatein time O(n4t2).We note that this construction can bestraightforwardly extended to convert stochas-tic HAGs as in (Alshawi, 1996) into stochasticCFGs.
Probabilit ies that Ha assigns to state q'svarious transition and halt actions are copiedonto the corresponding productions A\[a\] --~ c~of G, where A = fa(q).8 Sp l i t  head  automaton  grammarsin t ime O(n 3)For many bilexical CFGs or HAGs of practicalsignificance, just as for the bilexical version oflink grammars (Lafferty et al, 1992), it is possi-ble to parse length-n inputs even faster, in timeO(n 3) (Eisner, 1997).
In this section we de-scribe and discuss this special case, and give anew O(n 3) algorithm that has a smaller gram-mar constant han previously reported.A head automaton Ha is called spl i t  if it hasno states that can be entered on a +-- transi-tion and exited on a ~ transition.
Such an au-tomaton can accept (x, y) only by reading all ofy-- immediately after which it is said to be ina flip s ta te - -and  then reading all of x. For-mally, a flip state is one that allows entry on a--+ transition and that either allows exit on a e--transition or is a final state.We are concerned here with head automa-ton grammars H such that every Ha is split.These correspond to bilexical CFGs in whichany derivation A\[a\] 3"  xay has the formA\[a\] 3"  xB\[a\] =~* xay.
That is, a word's leftdependents are more oblique than its right de-pendents and c-command them.Such grammars are broadly applicable.
Evenif Ha is not split, there usually exists a split headautomaton H~ recognizing the same language.H a' exists iff {x#y : {x,y) e L(Ha)} is regular(where # ?
VT).
In particular, H~a must existunless Ha has a cycle that includes both +-- and--+ transitions.
Such cycles would be necessaryfor Ha itself to accept a formal language suchas {(b n, c n) : n > 0}, where word a takes 2n de-pendents, but we know of no natural-languagemotivation for ever using them in a HAG.One more definition will help us bound thecomplexity.
A split head automaton Ha is saidto be g-spl it  if its set of flip states, denotedQa C_ Qa, has size < g. The languages that canbe recognized by g-split HAs are those that cang be written as \[Ji=l Li x Ri, where the Li andRi are regular languages over VT. Eisner (1997)actually defined (g-split) bilexical grammars interms of the latter property.
66That paper associated a product language Li x Ri, orequivalently a 1-split HA, with each of g senses of a word(see ?6).
One could do the same without penalty in ourpresent approach: confining to l-split automata wouldremove the g2 complexity factor, and then allowing g462We now present our result: Figure 3 specifiesan O(n3g2t 2) recognition algorithm for a headautomaton grammar H in which every Ha isg-split.
For deterministic automata, the run-time is O(n3g2t)--a considerable improvementon the O(n3g3t 2) result of (Eisner, 1997), whichalso assumes deterministic automata.
As in ?4,a simple bottom-up implementation will suffice.sFor a practical speedup, add .
\["'.
as an an-h jtecedent o the MID rule (and fill in the parsetable from right to left).Like our previous algorithms, this one takestwo steps (ATTACH, COMPLETE) to attach achild constituent to a parent constituent.
Butinstead of full constituents--strings xd~y ELd~--it uses only half-constituents like xdi anddiy.
Where CKY combines z~i h j j+ lnwe save two degrees of freedom i, k (so improv-ing O(n 5) to O(n3)) and combine, ,~:~.
.
.~ J ;n 2 J~1 nThe other halves of these constituents can be at-tached later, because to find an accepting pathfor (zl, Zr) in a split head automaton, one canseparately find the half-path before the flip state(which accepts zr) and the half-path after theflip state (which accepts zt).
These two half-paths can subsequently be joined into an ac-cepting path if they have the same flip state s,i.e., one path starts where the other ends.
An-notating our left half-constituents with s makesthis check possible.9 F ina l  remarksWe have formally described, and given fasterparsing algorithms for, three practical gram-matical rewriting systems that capture depen-dencies between pairs of words.
All three sys-tems admit naive O(n 5) algorithms.
We givethe first O(n 4) results for the natural formalismof bilexical context-free grammar, and for AI-shawi's (1996) head automaton grammars.
Forthe usual case, split head automaton grammarsor equivalent bilexical CFGs, we replace theO(n 3) algorithm of (Eisner, 1997) by one with asmaller grammar constant.
Note that, e.g., allsenses would restore the g2 factor.
Indeed, this approachgives added flexibility: a word's sense, unlike its choiceof flip state, is visible to the HA that reads it.three models in (Collins, 1997) are susceptibleto the O(n 3) method (cf.
Collins's O(nh)).Our dynamic programming techniques forcheaply attaching head information to deriva-tions can also be exploited in parsing formalismsother than rewriting systems.
The authors havedeveloped an O(nT)-time parsing algorithm forbilexicalized tree adjoining grammars (Schabes,1992), improving the naive O(n s) method.The results mentioned in ?6 are related to theclosure property of CFGs under generalized se-quential machine mapping (Hopcroft and Ull-man, 1979).
This property also holds for ourclass of bilexical CFGs.ReferencesA.
V. Aho and J. D. Ullman.
1972.
The Theoryof Parsing, Translation and Compiling, volume 1.Prentice-Hall, Englewood Cliffs, NJ.H.
Alshawi.
1996.
Head automata nd bilingualtiling: Translation with minimal representations.In Proc.
of ACL, pages 167-176, Santa Cruz, CA.Y.
Bar-Hillel.
1953.
A quasi-arithmetical notationfor syntactic description.
Language, 29:47-58.E.
Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
InProc.
o\] the l~th AAAI, Menlo Park.C.
Chelba and F. Jelinek.
1998.
Exploiting syntac-tic structure for language modeling.
In Proc.
ofCOLING-ACL.N.
Chomsky.
1965.
Aspects of the Theory o\] Syntax.MIT Press, Cambridge, MA.M.
Collins and J. Brooks.
1995.
Prepositionalphrase attachment through a backed-off model.In Proe.
of the Third Workshop on Very LargeCorpora, Cambridge, MA.M.
Collins.
1997.
Three generative, lexicalised mod-els for statistical parsing.
In Proc.
of the 35thA CL and 8th European A CL, Madrid, July.J.
Eisner.
1996.
An empirical comparison of proba-bility models for dependency grammar.
TechnicalReport IRCS-96-11, IRCS, Univ.
of Pennsylvania.J.
Eisner.
1997.
Bilexical grammars and a cubic-time probabilistic parser.
In Proceedings of the~th Int.
Workshop on Parsing Technologies, MIT,Cambridge, MA, September.R.
C. Gonzales and M. G. Thomason.
1978.
Syntac-tic Pattern Recognition.
Addison-Wesley, Read-ing, MA.M.
A. Harrison.
1978.
Introduction to Formal Lan-guage Theory.
Addison-Wesley, Reading, MA.J.
E. Hopcroft and J. D. Ullman.
1979.
Introduc-tion to Automata Theory, Languages and Com-putation.
Addison-Wesley, Reading, MA.463(a)qqi4 qhqs:6h h(h < j, q E Qdh)(i <_ h, q E Qdh U {F}, s E (~dh)(h < h', q E Qdh, s' E Qd h,)(h' < h, q ?
Qdh, s ?
Qd~, s' ?
Q. dh)is derived iff dh : I z ~ q where Whq_l, j E L~is derived iff dh : q ( x s where W~,h-1 E Lxis derived iff dh : I xdh~ q and dh, : F ( Y S I whereWhTl ,h ' - i  ~ Lzyis derivediffdh, : I =~ s ~ and dh : q ~h,Y s whereWhTl,h'--I  E i xy(b)START: - -  q E Ida MID: - -q sh 'h hA h8 E Odh FINISH:ATTACH-RIGHT: q Fh \[~ _ l i ~h ' ,  r E 5d~ (q, dh,, --->)rATTACH-LEFT: s ~ q' s' E Qdh,, r E 5dh (q, dh,, t--)r s:6h hF s(e) Accept input w just if l z~ 'nandn n '~"COMPLETE-RIGHT:  qCOMPLETE-LEFT:S Ih h l~ iqF qi h h hqi4are derived for some h, s such that dh ---- $.qF- -  q E FdhFigure 3: An O(n 3) recognit ion algorithm for split head automaton  grammars.
The format is asin Figure 1, except that (c) gives the acceptance condition.
The following notat ion indicates thata head automaton  can consume a string x from its left or right input: a : q x) qr means that(q, e, x) ~-a (q', e, c), and a : I x ~ q, means this is true for some q E Ia.
Similarly, a : q' ~ x q meansthat (q, x, e) t-* (q~, c, c), and a : F (x q means this is true for some q~ E Fa.
The special symbolF also appears as a literal in some items, and effectively means "an unspecif ied final state."M.
Kay.
1986.
Algorithm schemata nd data struc-tures in syntactic processing.
In K. Sparck JonesB.
J. Grosz and B. L. Webber, editors, Natu-ral Language Processing, pages 35-70.
Kaufmann,Los Altos, CA.J.
Lafferty, D. Sleator, and D. Temperley.
1992.Grammatical trigrams: A probabilistic model oflink grammar.
In Proc.
of the AAAI  Conf.
onProbabilistic Approaches to Nat.
Lang., October.D.
Magerman.
1995.
Statistical decision-tree mod-els for parsing.
In Proceedings of the 33rd A CL.I.
Mel'~uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press.C.
Pollard and I.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.Y.
Schabes, A. Abeill@, and A. Joshi.
1988.
Parsingstrategies with 'lexicalized' grammars: Applica-tion to Tree Adjoining Grammars.
In Proceedingsof COLING-88, Budapest, August.Yves Schabes.
1992.
Stochastic lexicalized tree-adjoining grammars.
In Proc.
of the l~th COL-ING, pages 426-432, Nantes, France, August.C.
S. Wetherell.
1980.
Probabilistic languages: Areview and some open questions.
Computing Sur-veys, 12(4):361-379.D.
H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n 3.
Informationand Control, 10(2):189-208, February.464
