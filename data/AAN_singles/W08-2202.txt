Combining Knowledge-basedMethods and SupervisedLearning for Effective ItalianWord Sense DisambiguationPierpaolo BasileMarco de GemmisPasquale LopsGiovanni SemeraroUniversity of Bari (Italy)email: basilepp@di.uniba.itAbstractThis paper presents a WSD strategy which combines a knowledge-basedmethod that exploits sense definitions in a dictionary and relations amongsenses in a semantic network, with supervised learning methods on anno-tated corpora.
The idea behind the approach is that the knowledge-basedmethod can cope with the possible lack of training data, while supervisedlearning can improve the precision of a knowledge-based method whentraining data are available.
This makes the proposed method suitable fordisambiguation of languages for which the available resources are lackingin training data or sense definitions.
In order to evaluate the effectivenessof the proposed approach, experimental sessions were carried out on thedataset used for the WSD task in the EVALITA 2007 initiative, devoted tothe evaluation of Natural Language Processing tools for Italian.
The mosteffective hybrid WSD strategy is the one that integrates the knowledge-based approach into the supervised learning method, which outperformsboth methods taken singularly.56 Basile, de Gemmis, Lops, and Semeraro1 Background and MotivationsThe inherent ambiguity of human language is a greatly debated problem in manyresearch areas, such as information retrieval and text categorization, since the presenceof polysemous words might result in a wrong relevance judgment or classification ofdocuments.
These problems call for alternative methods that work not only at thelexical level of the documents, but also at the meaning level.The task of Word Sense Disambiguation (WSD) consists in assigning the most ap-propriate meaning to a polysemous word within a given context.
Applications suchas machine translation, knowledge acquisition, common sense reasoning and others,require knowledge about word meanings, and WSD is essential for all these applica-tions.
The assignment of senses to words is accomplished by using two major sourcesof information (Nancy and V?ronis, 1998):1. the context of the word to be disambiguated, e.g.
information contained withinthe text in which the word appears;2. external knowledge sources, including lexical resources, as well as hand-devisedknowledge sources, which provide data useful to associate words with senses.All disambiguation work involves matching the context of the instance of the wordto be disambiguated with either information from an external knowledge source (alsoknown as knowledge-driven WSD), or information about the contexts of previouslydisambiguated instances of the word derived from corpora (data-driven or corpus-based WSD).Corpus-basedWSD exploits semantically annotated corpora to train machine learn-ing algorithms to decide which word sense to choose in which context.
Words in suchannotated corpora are tagged manually using semantic classes chosen from a particu-lar lexical semantic resource (e.g.
WORDNET (Fellbaum, 1998)).
Each sense-taggedoccurrence of a particular word is transformed into a feature vector, which is then usedin an automatic learning process.
The applicability of such supervised algorithms islimited to those few words for which sense tagged data are available, and their accu-racy is strongly influenced by the amount of labeled data available.Knowledge-based WSD has the advantage of avoiding the need of sense-annotateddata, rather it exploits lexical knowledge stored in machine-readable dictionaries orthesauri.
Systems adopting this approach have proved to be ready-to-use and scalable,but in general they reach lower precision than corpus-based WSD systems.Our hypothesis is that the combination of both types of strategies can improveWSDeffectiveness, because knowledge-based methods can cope with the possible lack oftraining data, while supervised learning can improve the precision of knowledge-basedmethods when training data are available.This paper presents a method for solving the semantic ambiguity of all words con-tained in a text1.
We propose a hybrid WSD algorithm that combines a knowledge-based WSD algorithm, called JIGSAW, which we designed to work by exploitingWORDNET-like dictionaries as sense repository, with a supervised machine learning1all words task tries to disambiguate all the words in a text, while lexical sample task tries to disam-biguate only specific wordsCombining Knowledge-based Methods and Supervised Learning 7algorithm (K-Nearest Neighbor classifier).
WORDNET-like dictionaries are used be-cause they combine the characteristics of both a dictionary and a structured semanticnetwork, supplying definitions for the different senses of words and defining groupsof synonymous words by means of synsets, which represent distinct lexical concepts.WORDNET also organize synsets in a conceptual structure by defining a number ofsemantic relationship (IS-A, PART-OF, etc.)
among them.Mainly, the paper concentrates on two investigations:1.
First, corpus-based WSD is applied to words for which training examples areprovided, then JIGSAW is applied to words not covered in the first step, with theadvantage of knowing the senses of the context words already disambiguated inthe first step;2.
First, JIGSAW is applied to assign the most appropriate sense to those wordsthat can be disambiguated with a high level of confidence (by setting a specificparameter in the algorithm), then the remaining words are disambiguated by thecorpus-based method.The paper is organized as follows: After a brief discussion about the main worksrelated to our research, Section 3 gives the main ideas underlying the proposed hybridWSD strategy.
More details about the K-NN classification algorithm and JIGSAW,on which the hybrid WSD approach is based, are provided in Section 4 and Section5, respectively.
Experimental sessions have been carried out in order to evaluate theproposed approach in the critical situation when training data are not much reliable,as for Italian.
Results are presented in Section 6, while conclusions and future workclose the paper.2 Related WorkFor some Natural Language Processing (NLP) tasks, such as part of speech tagging ornamed entity recognition, there is a consensus on what makes a successful algorithm,regardless of the approach considered.
Instead, no such consensus has been reachedyet for the task of WSD, and previous work has considered a range of knowledgesources, such as local collocational clues, common membership in semantically ortopically related word classes, semantic density, and others.
In recent SENSEVAL-3evaluations2, the most successful approaches for all wordsWSD relied on informationdrawn from annotated corpora.
The system developed by Decadt et al (2002) uses twocascaded memory-based classifiers, combined with the use of a genetic algorithm forjoint parameter optimization and feature selection.
A separate word expert is learnedfor each ambiguous word, using a concatenated corpus of English sense tagged texts,including SemCor, SENSEVAL datasets, and a corpus built from WORDNET exam-ples.
The performance of this system on the SENSEVAL-3 English all words datasetwas evaluated at 65.2%.
Another top ranked system is the one developed by Yuret(2004), which combines two Na?ve Bayes statistical models, one based on surround-ing collocations and another one based on a bag of words around the target word.The statistical models are built based on SemCor and WORDNET, for an overall dis-ambiguation accuracy of 64.1%.
All previous systems use supervised methods, thus2http://www.senseval.org8 Basile, de Gemmis, Lops, and Semerarorequiring a large amount of human intervention to annotate the training data.
In thecontext of the current multilingual society, this strong requirement is even increased,since the so-called ?sense-tagged data bottleneck problem?
is emphasized.To address this problem, different methods have been proposed.
This includesthe automatic generation of sense-tagged data using monosemous relatives (Leacocket al, 1998), automatically bootstrapped disambiguation patterns (Mihalcea, 2002),parallel texts as a way to point out word senses bearing different translations in a sec-ond language (Diab, 2004), and the use of volunteer contributions over the Web (Mi-halcea and Chklovski, 2003).
More recently, Wikipedia has been used as a source ofsense annotations for building a sense annotated corpus which can be used to trainaccurate sense classifiers (Mihalcea, 2007).
Even though the Wikipedia-based senseannotations were found reliable, leading to accurate sense classifiers, one of the lim-itations of the approach is that definitions and annotations in Wikipedia are availablealmost exclusively for nouns.On the other hand, the increasing availability of large-scale rich (lexical) knowledgeresources seems to provide new challenges to knowledge-based approaches (Navigliand Velardi, 2005; Mihalcea, 2005).
Our hypothesis is that the complementarity ofknowledge-based methods and corpus-based ones is the key to improve WSD effec-tiveness.
The aim of the paper is to define a cascade hybrid method able to exploitboth linguistic information coming from WORDNET-like dictionaries and statisticalinformation coming from sense-annotated corpora.3 A Hybrid Strategy for WSDThe goal of WSD algorithms consists in assigning a word wi occurring in a documentd with its appropriate meaning or sense s. The sense s is selected from a predefined setof possibilities, usually known as sense inventory.
We adopt ITALWORDNET (Roven-tini et al, 2003) as sense repository.
The algorithm is composed by two procedures:1.
JIGSAW - It is a knowledge-based WSD algorithm based on the assumptionthat the adoption of different strategies depending on Part-of-Speech (PoS) isbetter than using always the same strategy.
A brief description of JIGSAW isgiven in Section 5, more details are reported in Basile et al (2007b), Basile et al(2007a) and Semeraro et al (2007).2.
Supervised learning procedure - A K-NN classifier (Mitchell, 1997), trainedon MultiSemCor corpus3 is adopted.
Details are given in Section 4.
MultiSem-Cor is an English/Italian parallel corpus, aligned at the word level and annotatedwith PoS, lemma and word senses.
The parallel corpus is created by exploitingthe SemCor corpus4, which is a subset of the English Brown corpus containingabout 700,000 running words.
In SemCor, all the words are tagged by PoS, andmore than 200,000 content words are also lemmatized and sense-tagged withreference to the WORDNET lexical database.
SemCor has been used in severalsupervised WSD algorithms for English with good results.
MultiSemCor con-tains less annotations than SemCor, thus the accuracy and the coverage of thesupervised learning for Italian might be affected by poor training data.3http://multisemcor.itc.it/4http://www.cs.unt.edu/~rada/downloads.html\#semcorCombining Knowledge-based Methods and Supervised Learning 9The idea is to combine both procedures in a hybrid WSD approach.
A first choicemight be the adoption of the supervised method as first attempt, then JIGSAW couldbe applied to words not covered in the first step.
Differently, JIGSAWmight be appliedfirst, then leaving the supervised approach to disambiguate the remaining words.
Aninvestigation is required in order to choose the most effective combination.4 Supervised Learning MethodThe goal of supervised methods is to use a set of annotated data as little as possible,and at the same time to make the algorithm general enough to be able to disambiguateall content words in a text.
We use MultiSemCor as annotated corpus, since at presentit is the only available semantic annotated resource for Italian.
The algorithm startswith a preprocessing stage, where the text is tokenized, stemmed, lemmatized andannotated with PoS.Also, the collocations are identified using a sliding window approach, where acollocation is considered to be a sequence of words that forms a compound conceptdefined in ITALWORDNET (e.g.
artificial intelligence).
In the training step, a semanticmodel is learned for each PoS, starting with the annotated corpus.
These models arethen used to disambiguate words in the test corpus by annotating them with theircorrespondingmeaning.
The models can only handle words that were previously seenin the training corpus, and therefore their coverage is not 100%.
Starting with anannotated corpus formed by all annotated files in MultiSemCor, a separate trainingdataset is built for each PoS.
For each open-class word in the training corpus, a featurevector is built and added to the corresponding training set.
The following featuresare used to describe an occurrence of a word in the training corpus as in Hoste et al(2002):?
Nouns - 2 features are included in feature vector: the first noun, verb, or adjec-tive before the target noun, within a window of at most three words to the left,and its PoS;?
Verbs - 4 features are included in feature vector: the first word before and thefirst word after the target verb, and their PoS;?
Adjectives - all the nouns occurring in two windows, each one of six words(before and after the target adjective) are included in the feature vector;?
Adverbs - the same as for adjectives, but vectors contain adjectives rather thannouns.The label of each feature vector consists of the target word and the correspondingsense, represented as word#sense.
Table 1 describes the number of vectors for eachPoS.To annotate (disambiguate) new text, similar vectors are built for all content-wordsin the text to be analyzed.
Consider the target word bank, used as a noun.
The algo-rithm catches all the feature vectors of bank as a noun from the training model, andbuilds the feature vector v f for the target word.
Then, the algorithm computes the sim-ilarity between each training vector and v f and ranks the training vectors in decreasingorder according to the similarity value.10 Basile, de Gemmis, Lops, and SemeraroTable 1: Number of feature vectorsPoS #feature vectorsNoun 38,546Verb 18,688Adjective 6,253Adverb 1,576The similarity is computed as Euclidean distance between vectors, where POS dis-tance is set to 1, if POS tags are different, otherwise it is set to 0.
Word distancesare computed by using the Levenshtein metric, that measures the amount of differencebetween two strings as the minimum number of operations needed to transform onestring into the other, where an operation is an insertion, deletion, or substitution of asingle character (Levenshtein, 1966).
Finally, the target word is labeled with the mostfrequent sense in the first K vectors.5 JIGSAW - Knowledge-based ApproachJIGSAW is a WSD algorithm based on the idea of combining three different strategiesto disambiguate nouns, verbs, adjectives and adverbs.
The main motivation behindour approach is that the effectiveness of a WSD algorithm is strongly influenced bythe POS tag of the target word.JIGSAW takes as input a document d = (w1, w2, .
.
.
, wh) and returns a list ofsynsets X = (s1, s2, .
.
.
, sk) in which each element si is obtained by disambiguatingthe target word wi based on the information obtained from the sense repository abouta few immediately surrounding words.
We define the context C of the target wordto be a window of n words to the left and another n words to the right, for a totalof 2n surrounding words.
The algorithm is based on three different procedures fornouns, verbs, adverbs and adjectives, called JIGSAWnouns, JIGSAWverbs, JIGSAWothers,respectively.JIGSAWnouns - Given a set of nouns W = {w1,w2, .
.
.
,wn}, obtained from docu-ment d, with each wi having an associated sense inventory Si = {si1,si2, .
.
.
,sik} ofpossible senses, the goal is assigning each wi with the most appropriate sense sih ?
Si,according to the similarity of wi with the other words in W (the context for wi).
Theidea is to define a function ?
(wi,si j), wi ?W , si j ?
Si, that computes a value in [0,1]representing the confidence with which word wi can be assigned with sense si j. Inorder to measure the relatedness of two words we adopted a modified version of theLeacock and Chodorow (1998) measure, which computes the length of the path be-tween two concepts in a hierarchy by passing through their Most Specific Subsumer(MSS).
We introduced a constant factor depth which limits the search for the MSS todepth ancestors, in order to avoid ?poorly informative?
MSSs.
Moreover, in the simi-larity computation, we introduced both a Gaussian factor G(pos(wi), pos(w j)), whichtakes into account the distance between the position of the words in the text to be dis-ambiguated, and a factor R(k), which assigns sik with a numerical value, according tothe frequency score in ITALWORDNET.JIGSAWverbs - We define the description of a synset as the string obtained byCombining Knowledge-based Methods and Supervised Learning 11concatenating the gloss and the sentences that ITALWORDNET uses to explain theusage of a synset.
JIGSAWverbs includes, in the contextC for the target verb wi, all thenouns in the window of 2n words surrounding wi.
For each candidate synset sik of wi,the algorithm computes nouns(i,k), that is the set of nouns in the description for sik.Then, for each w j inC and each synset sik, the following value is computed:(1) max jk = maxwl?nouns(i,k){sim(w j,wl,depth)}where sim(w j,wl,depth) is the same similarity measure adopted by JIGSAWnouns.Finally, an overall similarity score among sik and the whole contextC is computed:(2) ?
(i,k) = R(k) ?
?w j?CG(pos(wi), pos(w j)) ?max jk?hG(pos(wi), pos(wh))where both R(k) and G(pos(wi), pos(w j)), that gives a higher weight to words closerto the target word, are defined as in JIGSAWnouns.
The synset assigned to wi is the onewith the highest ?
value.JIGSAWothers - This procedure is based on the WSD algorithm proposed in Baner-jee and Pedersen (2002).
The idea is to compare the glosses of each candidate sensefor the target word to the glosses of all the words in its context.6 ExperimentsThe main goal of our investigation is to study the behavior of the hybrid algorithmwhen available training resources are not much reliable, e.g.
when a lower numberof sense descriptions is available, as for Italian.
The hypothesis we want to evaluateis that corpus-based methods and knowledge-based ones can be combined to improvethe accuracy of each single strategy.Experiments have been performed on a standard test collection in the context of theAll-Words-Task, in whichWSD algorithms attempt to disambiguate all words in a text.Specifically, we used the EVALITA WSD All-Words-Task dataset5, which consists ofabout 5,000 words labeled with ITALWORDNET synsets.An important concern for the evaluation of WSD systems is the agreement ratebetween human annotators on word sense assignment.While for natural language subtasks like part-of-speech tagging, there are relativelywell defined and agreed-upon criteria of what it means to have the ?correct?
part ofspeech assigned to a word, this is not the case for word sense assignment.
Two humanannotators may genuinely disagree on their sense assignment to a word in a context,since the distinction between the different senses for a commonly used word in adictionary like WORDNET tend to be rather fine.What we would like to underline here is that it is important that human agreementon an annotated corpus is carefully measured, in order to set an upper bound to theperformance measures: it would be futile to expect computers to agree more with thereference corpus that human annotators among them.
For example, the inter-annotatoragreement rate during the preparation of the SENSEVAL-3 WSD English All-Words-Task dataset (Agirre et al, 2007) was approximately 72.5%.5http://evalita.itc.it/tasks/wsd.html12 Basile, de Gemmis, Lops, and SemeraroUnfortunately, for EVALITA dataset, the inter-annotator agreement has not beenmeasured, one of the reasons why the evaluation for Italian WSD is very hard.
In ourexperiments, we reasonably selected different baselines to compare the performanceof the proposed hybrid algorithm.6.1 Integrating JIGSAW into a supervised learning methodThe design of the experiment is as follows: firstly, corpus-based WSD is applied towords for which training examples are provided, then JIGSAW is applied to wordsnot covered by the first step, with the advantage of knowing the senses of the contextwords already disambiguated in the first step.
The performance of the hybrid methodwas measured in terms of precision (P), recall (R), F-measure (F) and the percentageA of disambiguation attempts, computed by counting the words for which a disam-biguation attempt is made (the words with no training examples or sense definitionscannot be disambiguated).
Table 2 shows the baselines chosen to compare the hybridWSD algorithm on the All-Words-Task experiments.Table 2: Baselines for Italian All-Words-TaskSetting P R F A1stsense 58.45 48.58 53.06 83.11Random 43.55 35.88 39.34 83.11JIGSAW 55.14 45.83 50.05 83.11K-NN 59.15 11.46 19.20 19.38K-NN + 1stsense 57.53 47.81 52.22 83.11The simplest baseline consists in assigning a random sense to each word (Random),another common baseline inWord Sense Disambiguation is first sense (1stsense): eachword is tagged using the first sense in ITALWORDNET that is the most commonly(frequent) used sense.
The other baselines are the two methods combined in the hybridWSD, taken separately, namely JIGSAW and K-NN, and the basic hybrid algorithm?K-NN + 1stsense?, which applies the supervised method, and then adopts the firstsense heuristic for the words without examples into training data.
The K-NN baselineachieves the highest precision, but the lowest recall due to the low coverage in thetraining data (19.38%) makes this method useless for all practical purposes.
Noticethat JIGSAW was the only participant to EVALITA WSD All-Words-Task, thereforeit currently represents the only available system performing WSD All-Words task forthe Italian language.Table 3: Experimental results of K-NN+JIGSAWSetting P R F AK-NN + JIGSAW 56.62 47.05 51.39 83.11K-NN + JIGSAW (?
?
0.90) 61.88 26.16 36.77 42.60K-NN + JIGSAW (?
?
0.80) 61.40 32.21 42.25 52.06K-NN + JIGSAW (?
?
0.70) 60.02 36.29 45.23 60.46K-NN + JIGSAW (?
?
0.50) 59.58 37.38 45.93 62.74Combining Knowledge-based Methods and Supervised Learning 13Table 3 reports the results obtained by the hybrid method on the EVALITA dataset.We study the behavior of the hybrid approach with relation to that of JIGSAW, sincethis specific experiment aims at evaluating the potential improvements due to the in-clusion of JIGSAW into K-NN.
Different runs of the hybrid method have been per-formed, each run corresponding to setting a specific value for ?
(the confidence withwhich a word wi is correctly disambiguated by JIGSAW).
In each different run, thedisambiguation carried out by JIGSAW is considered reliable only when ?
values ex-ceed a certain threshold, otherwise any sense is assigned to the target word (this thereason why A decreases by setting higher values for ?
).A positive effect on precision can be noticed by varying ?
between 0.50 and 0.90.
Ittends to grow and overcomes all the baselines, but a corresponding decrease of recallis observed, as a consequence of more severe constraints set on ?.
Anyway, recall isstill too low to be acceptable.Better results are achieved when no restriction is set on ?
(K-NN+JIGSAW in Ta-ble 3): the recall is significantly higher than that obtained in the other runs.
On theother hand, the precision reached in this run is lower than in the others, but it is stillacceptable.To sum up, two main conclusions can be drawn from the experiments:?
when no constraint is set on the knowledge-basedmethod, the hybrid algorithmK-NN+JIGSAW in general outperforms both JIGSAW and K-NN taken singu-larly (F values highlighted in bold in Tables 3 and 4);?
when thresholding is introduced on ?, no improvement is observed on the wholecompared to K-NN+JIGSAW.A deep analysis of results revealed that lower recall was achieved for verbs andadjectives rather than for nouns.
Indeed, disambiguation of Italian verbs and adjec-tives is very hard, but the lower recall is probability due also to the fact that JIGSAWuses glosses for verbs and adjectives disambiguation.
As a consequence, the perfor-mance depends on the accuracy of word descriptions in the glosses, while for nounsthe algorithm relies only the semantic relations between synsets.6.2 Integrating supervised learning into JIGSAWIn this experiment we test whether the supervised algorithm can help JIGSAW to dis-ambiguate more accurately.
The experiment has been organized as follows: JIGSAWis applied to assign the most appropriate sense to the words which can be disam-biguated with a high level of confidence (by setting the ?
threshold), then the remain-ing words are disambiguated by the K-NN classifier.
The dataset and the baselines arethe same as in Section 6.1.Note that, differently from the experiments described in Table 3, run JIGSAW+K-NN has not been reported since JIGSAW covered all the target words in the first stepof the cascade hybrid method, then the K-NN method is not applied at all.
Therefore,for this run, results obtained by JIGSAW+K-NN correspond to those get by JIGSAWalone (reported in Table 2).Table 4 reports the results of all the runs.
Results are very similar to those obtainedin the runs K-NN+JIGSAW with the same settings on ?.
Precision tends to grow,14 Basile, de Gemmis, Lops, and SemeraroTable 4: Experimental results of JIGSAW+K-NNSetting P R F AJIGSAW (?
?
0.90) + K-NN 61.48 27.42 37.92 44.61JIGSAW (?
?
0.80) + K-NN 61.17 32.59 42.52 53.28JIGSAW (?
?
0.70) + K-NN 59.44 36.56 45.27 61.52while a corresponding decrease in recall is observed.
The main outcome is that theoverall accuracy of the best combination JIGSAW+K-NN (?
?
0.70, F value high-lighted in bold in Table 4) is outperformed by K-NN+JIGSAW.
Indeed, this resultwas largely expected because the small size of the training set does not allow to coverwords not disambiguated by JIGSAW.Even if K-NN+JIGSAW is not able to achieve the baselines set on the 1stsenseheuristic (first and last row in Table 2), we can conclude that a step toward these hardbaselines has been moved.
The main outcome of the study is that the best hybridmethod on which further investigations are possible is K-NN+JIGSAW.7 Conclusions and Future WorkThis paper presented a method for solving the semantic ambiguity of all words con-tained in a text.
We proposed a hybrid WSD algorithm that combines a knowledge-based WSD algorithm, called JIGSAW, which we designed to work by exploitingWORDNET-like dictionaries as sense repository, with a supervised machine learningalgorithm (K-Nearest Neighbor classifier).
The idea behind the proposed approach isthat JIGSAW can cope with the possible lack of training data, while K-NN can im-prove the precision of JIGSAW method when training data are available.
This makesthe proposed method suitable for disambiguation of languages for which the availableresources are lacking in training data or sense definitions, such as Italian.Extensive experimental sessions were performed on the EVALITAWSDAll-Words-Task dataset, the only dataset available for the evaluation of WSD systems for theItalian language.
An investigation was carried out in order to evaluate several com-binations of JIGSAW and K-NN.
The main outcome is that the most effective hybridWSD strategy is the one that runs JIGSAW after K-NN, which outperforms both JIG-SAW and K-NN taken singularly.
Future work includes new experiments with othercombination methods, for example the JIGSAW output could be used as feature intosupervised system or other different supervised methods could be exploited.ReferencesAgirre, E., B. Magnini, O. L. de Lacalle, A. Otegi, G. Rigau, and P. Vossen (2007).SemEval-2007 Task 1: EvaluatingWSD on Cross-Language Information Retrieval.In Proceedings of SemEval-2007.
Association for Computational Linguistics.Banerjee, S. and T. Pedersen (2002).
An adapted lesk algorithm for word sense disam-biguation using wordnet.
In CICLing ?02: Proceedings of the Third InternationalCombining Knowledge-based Methods and Supervised Learning 15Conference on Computational Linguistics and Intelligent Text Processing, London,UK, pp.
136?145.
Springer-Verlag.Basile, P., M. de Gemmis, A. Gentile, P. Lops, and G. Semeraro (2007a).
JIGSAWalgorithm for Word Sense Disambiguation.
In SemEval-2007: 4th InternationalWorkshop on Semantic Evaluations, pp.
398?401.
ACL press.Basile, P., M. de Gemmis, A. L. Gentile, P. Lops, and G. Semeraro (2007b).
The JIG-SAW Algorithm for Word Sense Disambiguation and Semantic Indexing of Doc-uments.
In R. Basili and M. T. Pazienza (Eds.
), AI*IA, Volume 4733 of LectureNotes in Computer Science, pp.
314?325.
Springer.Decadt, B., V. Hoste, W. Daelemans, and A. V. den Bosch (2002).
Gambl, GeneticAlgorithm optimization of Memory-based WSD.
In Senseval-3: 3th InternationalWorkshop on the Evaluation of Systems for the Semantic Analysis of Text.Diab, M. (2004).
Relieving the data acquisition bottleneck in word sense disambigua-tion.
In Proceedings of ACL.
Barcelona, Spain.Fellbaum, C. (1998).
WordNet: An Electronic Lexical Database.
MIT Press.Hoste, V., W. Daelemans, I. Hendrickx, and A. van den Bosch (2002).
Evaluating theresults of a memory-based word-expert approach to unrestricted word sense dis-ambiguation.
In Proceedings of the ACL-02 workshop on Word sense disambigua-tion: recent successes and future directions, Volume 8, pp.
95?101.
Association forComputational Linguistics Morristown, NJ, USA.Leacock, C. and M. Chodorow (1998).
Combining local context and WordNet simi-larity for word sense identification, pp.
305?332.
MIT Press.Leacock, C., M. Chodorow, and G. Miller (1998).
Using corpus statistics and Word-Net relations for sense identification.
Computational Linguistics 24(1), 147?165.Levenshtein, V. I.
(1966).
Binary codes capable of correcting deletions, insertions,and reversals.
Soviet Physics Doklady 10(8), 707?710.Mihalcea, R. (2002).
Bootstrapping large sense tagged corpora.
In Proceedings of the3rd International Conference on Language Resources and Evaluations.Mihalcea, R. (2005).
Unsupervised large-vocabulary word sense disambiguation withgraph-based algorithms for sequence data labeling.
In HLT ?05: Proceedings ofthe conference on Human Language Technology and Empirical Methods in Nat-ural Language Processing, Morristown, NJ, USA, pp.
411?418.
Association forComputational Linguistics.Mihalcea, R. (2007).
Using Wikipedia for AutomaticWord Sense Disambiguation.
InProceedings of the North American Chapter of the Association for ComputationalLinguistics.Mihalcea, R. and T. Chklovski (2003).
Open Mind Word Expert: Creating LargeAnnotated Data Collections with Web Users?
Help.
In Proceedings of the EACLWorkshop on Linguistically Annotated Corpora, Budapest.16 Basile, de Gemmis, Lops, and SemeraroMitchell, T. (1997).
Machine Learning.
New York: McGraw-Hill.Nancy, I. and J. V?ronis (1998).
Introduction to the special issue on word sensedisambiguation: The state of the art.
Computational Linguistics 24(1), 1?40.Navigli, R. and P. Velardi (2005).
Structural semantic interconnections: A knowledge-based approach to word sense disambiguation.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence 27(7), 1075?1086.Roventini, A., A. Alonge, F. Bertagna, N. Calzolari, J. Cancila, C. Girardi, B. Magnini,R.
Marinelli, M. Speranza, and A. Zampolli (2003).
ItalWordNet: building a largesemantic database for the automatic treatment of Italian.
Computational Linguis-tics in Pisa - Linguistica Computazionale a Pisa.
Linguistica Computazionale, Spe-cial Issue XVIII-XIX, Tomo II, 745?791.Semeraro, G., M. Degemmis, P. Lops, and P. Basile (2007).
Combining learning andword sense disambiguation for intelligent user profiling.
In Proceedings of theTwentieth International Joint Conference on Artificial Intelligence IJCAI-07, pp.2856?2861.
M. Kaufmann, San Francisco, California.
ISBN: 978-I-57735-298-3.Yuret, D. (2004).
Some experiments with a naive bayes WSD system.
In Senseval-3:3th Internat.
Workshop on the Evaluation of Systems for the Semantic Analysis ofText.
