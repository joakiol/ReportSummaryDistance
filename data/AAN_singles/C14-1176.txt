Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1865?1874, Dublin, Ireland, August 23-29 2014.Synchronous Constituent Context Model for Inducing BilingualSynchronous StructuresXiangyu Duan Min Zhang?Qiaoming ZhuSchool of Computer Science & Technology, Soochow University{xiangyuduan;minzhang;qmzhu}@suda.edu.cnAbstractTraditional Statistical Machine Translation (SMT) systems heuristically extract synchronousstructures from word alignments, while synchronous grammar induction provides better so-lutions that can discard heuristic method and directly obtain statistically sound bilingual syn-chronous structures.
This paper proposes Synchronous Constituent Context Model (SCCM) forsynchronous grammar induction.
The SCCM is different to all previous synchronous grammarinduction systems in that the SCCM does not use the Context Free Grammars to model the bilin-gual parallel corpus, but models bilingual constituents and contexts directly.
The experimentsshow that valuable synchronous structures can be found by the SCCM, and the end-to-end ma-chine translation experiment shows that the SCCM improves the quality of SMT results.1 IntroductionTraditional Statistical Machine Translation (SMT) learns translation model from bilingual corpus thatis sentence aligned.
No large-scale hand aligned structures inside the parallel sentences are usuallyavailable to the SMT community, while the aligned structures are essential for training the translationmodel.
Thus, various unsupervised methods had been explored to automatically obtain aligned structuresinside the parallel sentences.
Currently, the dominant method is a two step pipeline that obtains wordalignments by unsupervised learning (Brown et al., 1993) at the first step, then obtains aligned structuresat the second step by heuristically extracting all bilingual structures that are consistent with the wordalignments.The second step in this two step pipeline is problematic due to its obtained aligned structures, whosecounts are heuristically collected and violate valid translation derivations, while most SMT decodersperform translation via valid translation derivations.
This problem leads to researches on synchronousgrammar induction that discards the heuristic method and the two separate steps pipeline.Synchronous grammar induction aims to directly obtain aligned structures by using one statisticallysound model.
The aligned structures in synchronous grammar induction are hierarchical/syntax level(Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context FreeGrammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao andXiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al.,2011; Cohn and Haffari, 2013).
Both SCFGs and ITGs are studied in recent years by using generative ordiscriminative modeling.This paper departs from using the above two traditional CFGs-based grammars, and proposes Syn-chronous Constituent Context Model (SCCM) which models synchronous constituents and contextsdirectly so that bilingual translational equivalences can be directly modeled.
The proposed SCCM isinspired by researches on monolingual grammar induction, whose experience is valuable to the syn-chronous grammar induction community due to its standard evaluation on released monolingual tree-banks, while no hand annotated bilingual synchronous treebank is available for evaluating synchronous?Corresponding AuthorThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1865grammar induction.
According to the evaluation results, the state-of-the-art monolingual grammar induc-tion was achieved by Bayesian modeling of the Constituent Context Model (CCM) (Duan et al., 2013;Klein and Manning, 2002), while traditional CFGs based monolingual grammar induction methods per-form well below the CCM.In view of the significant achievements of the CCM in monolingual grammar induction, we proposethe SCCM to apply the CCM to the bilingual case.
The tremendous possible constituents and contextsincurred in this bilingual case put a challenge for the SCCM to model such kind of sparse variables.
Wefurther propose a non-parametric Bayesian Modeling of the SCCM to cope with the sparse variables.Experiments on Chinese-English machine translation show that meaningful synchronous phrases can bedetected by our SCCM, and the performance of the end-to-end SMT is significantly improved.The rest of the paper is structured as follows: we propose the SCCM in Section 2.
The non-parametricBayesian modeling of the SCCM is presented in Section 3, followed by the presentation of posteriorinference for the Bayesian SCCM.
Then experiments and results are presented.
Conclusion are presentedin the final section.2 Synchronous Constituent Context Model (SCCM)We propose the SCCM to model synchronous structures explicitly.
Unlike Synchronous Context FreeGrammars (SCFGs) which are defined on latent production rules of parallel corpus, the SCCM dealswith both synchronous tree spans (syn spans) and non-synchronous spans (non-syn spans).
All spansare represented by two kinds of strings: bilingual constituents and bilingual contexts.
The SCCM is agenerative model defined over such representations.2.1 Bilingual Constituents and ContextsBy extending the concept of constituents and contexts introduced in (Klein and Manning, 2002), wedefine bilingual constituents and contexts as follows.
Bilingual constituents are pairs of contiguoussurface strings of sentence spans (bilingual subsequences), bilingual contexts are tokens preceding andfollowing the bilingual constituents.
In the SCCM, each bi-span in a sentence pair, either a syn span ora non-syn span, is represented by a bilingual constituent and a bilingual context.Fig.
1 gives an illustration of the bilingual constituents and contexts.
In Fig.
1-(a), a latent syn-chronous tree over the example sentence pair is illustrated.
With the word alignments shown in thesentence pair, the latent tree over the target sentence ?e1e2e3?
can be inferred.
For the ease of presen-tation, the latent target side tree is neglected in Fig.
1-(a).Given the synchronous tree, two sets of bilingual constituents and contexts can be extracted as shownin the two tables of Fig.
1.
One is about syn spans, the other is about non-syn spans.
3 appearing inthe contexts denotes a sentence boundary.
nil appearing in the constituents of the non-tree spans denotesan empty span, which is actually a space between two terminals (or between a terminal and 3).2.2 Generative ModelThe SCCM computes the joint probability of a sentence pair S and its synchronous tree T as below:P (S, T ) = P (S|T )P (T ) = P (S|T )P (B)P (T |B) (1)= P (S|T )P (B)?0?i?j?m0?p?q?nP (?ij,pq|Bij,pq)P (?ij,pq|Bij,pq)where B denotes a synchronous bracketing skeleton, in which no words are populated.
Fig.
1-(b) showsthe skeleton of Fig.
1-(a).
The skeleton B is considered being filled by the synchronous tree T , andP (T |B) is decomposed into conditional probabilities of bilingual constituents ?
and contexts ?
condi-tioning on Bij,pq, a Boolean variable indicating whether the under-consideration bi-span <i, j><p, q>is a syn span or not.
In particular, ?ij,pqdenotes the bilingual constituent spanning from i to j on sourceside sentence, and spanning from p to q on target side sentence.
?ij,pqdenotes the context of ?ij,pq.1866f1 f2 f3 0 1 2 3e1 e2 e3 0 1 2 3syn span <0,1><2,3> <1,2> <1,2> <2,3><0,1> <0,2><1,3> <0,3><0,3>constituent (f1)(e3) (f2)(e2) (f3)(e1) (f1 f2)(e2 e3) (f1 f2 f3)(e1 e2 e3)context (-f2)(e2-) (f 1-f3)(e1- e3) (f2-)(- e2) (-f3)(e1-) (-)(-)non-syn span <1,3><0,1> <1,1><1,2>       ?constituent (f2 f3)(e1) (nil)(e2)         ?context (f1-)(-e2) (f 1-f2)(e1- e3)      ?      (a) (b)Figure 1: Illustration of bilingual constituents and contexts over a sentence pair which consists of asource side sentence ?f1f2f3?
and a target side sentence ?e1e2e3?.
In (a), the bottom numbers aroundeach word are indexes for denoting spans.
A synchronous tree is illustrated in (a), based on which twosets of bilingual constituents and contexts are extracted as shown in the two tables below the tree.
Take asyn span <1,2><1,2> for example, the source side span <1,2> is ?f2?
and the target side span <1,2>is ?e2?.
They constitutes a bilingual constituent ?
(f2)(e2)?, whose context is ?(f1-f3)(e1-e3)?
that ispreceding and following the bilingual constituent.
Figure (b) shows the skeleton of figure (a).Bij,pqis defined as below:Bij,pq={1 if bispan < i, j >< p, q > is a syn span0 otherwiseIn the SCCM, skeletons Bs are restricted to be binary branching and are distributed uniformly.
Fur-thermore, since T and S are consistent, P (S|T ) is always equal to 1 in Eq.
(1).
Therefore, we can infer(with the expansion of the continued multiplication operator of Eq.
(1) ):P (S, T ) ?
?<i,j><p,q>?T(P (?ij,pq|Bij,pq= 1)P (?ij,pq|Bij,pq= 1)) (2)?<i,j><p,q> ?
?T(P (?ij,pq|Bij,pq= 0)P (?ij,pq|Bij,pq= 0))where <i, j><p, q> ?
T indicates that bi-span <i, j><p, q> is a syn span contained in T ,<i, j><p, q> ??
T indicates otherwise case.
Formula (2) is the basis for Bayesian modeling of theSCCM and the posterior inference that are proposed in the following sections.3 Bayesian Modeling for the SCCMFor the SCCM, the posterior of a synchronous tree T given the observation of a sentence pair S is:P (T |S) ?
P (S, T ).
As shown in formula (2), it turns out that the posterior P (T |S) depends on the fourkinds of distributions:P (?ij,pq|Bij,pq= 1) P (?ij,pq|Bij,pq= 1)P (?ij,pq|Bij,pq= 0) P (?ij,pq|Bij,pq= 0)1867We propose to define two kinds of Bayesian priors over the constituents related variables ?ij,pq|Bij,pqand the contexts related variables ?ij,pq|Bij,pqrespectively.
Since constituents exhibits richer appear-ances than contexts, the proposed Bayesian prior over ?ij,pq|Bij,pqis more complicate than that over?ij,pq|Bij,pq.Specifically, one of the non-parametric Bayesian priors, the Pitman-Yor-Process (PYP) prior, is definedon ?ij,pq|Bij,pq.
The PYP prior can produce the power-law distribution (Goldwater et al., 2009) that iscommonly observed in natural languages, and can flexibly model distributions on layer structures due toits defined distribution on distribution hierarchy.
The PYP prior had been successfully applied on manyNLP tasks such as language modeling (YeeWhye, 2006), word segmentation (Johnson et al., 2007b;Goldwater et al., 2011), dependency grammar induction (Cohen et al., 2008; Cohn et al., 2010), grammarrefinement (Liang et al., 2007; Finkel et al., 2007) and Tree-Substitution Grammar induction (Cohn etal., 2010).
We use the PYP to model the constituents?
layered structure by using the PYP?s distributionhierarchy.
On ?ij,pq|Bij,pq, we use the Dirichlet distribution for its simplicity because contexts appear inmuch fewer kinds of surface strings than those of constituents.3.1 The PYP Prior over Bilingual ConstituentsConstituents consist of both words and POS tags.
Though in much monolingual grammar inductionworks, only POS tag sequences were used as the observed constituents for their significant hints ofphrases (Klein and Manning, 2002; Cohn et al., 2010), our work needs considering raw words as obser-vation data too because word alignments encode the important translation correspondence and contributeto synchronous bi-spans.
But it causes severe data sparse problem due to the quite large number of uniqueconstituents consisting of both words and POS tags.
Besides, constituents can be extremely long whichintensify the data sparse problem.
So, solely using the surface strings of constituents is impractical.In this section, we propose a hierarchical representation of constituents to overcome the data sparseproblem and use the PYP prior on this kind of representation.
From top to bottom, the hierarchical rep-resentation encodes the information of a bilingual constituent from fine-grained level to coarse-grainedlevels.
The probability of a fine-grained level can be backed-off to the probabilities of coarse-grainedlevels.The first (top) level of the hierarchical representation is the bilingual constituent itself.
The secondlevel is composed of two sequences: one is word sequence, the other is POS tags sequence.
The thirdlevel mainly decomposes the second level into boundaries and middle words/POSs.
Since the target ofinducing synchronous structures in this paper is to induce the latent phrasal equivalences of a parallelsentence, boundaries of bilingual constituents play the key role of identifying phrasal equivalences.
Thethird level is the function to make use of boundaries.
Fig.
2 gives an illustration of the hierarchicalrepresentation.w1p1 w2p2 w3p3 w4p4w1 w2 w3 w4 p1 p2 p3 p4w1 w2 w4 w3 p1 p2 p4 p3Figure 2: Illustration of the hierarchical representation of a bilingual constituent ?w1p1w2p2w3p3w4p4?.
Here w and p denote word and POS respectively, and the suffixes denote positions.
Note thatboth w and p are composite, w denotes a source side word and a target side word, and p denotes thePOS case.
The second level decomposes the first level into a word sequence and a POS sequence, andthe third level decomposes further into boundaries and middle words/POSs.
The boundary width in thisfigure is two for left side boundary and one for right side boundary.1868The PYP prior encodes distribution on distribution.
Recursively using the PYP prior can create adistribution hierarchy, which is appropriate for modeling the distribution over the hierarchical repre-sentations of constituents.
Smoothing is fulfilled through backing-off fine-grained level distributions tocoarse-grained level distributions.3.1.1 The PYP HierarchyWe define the PYP hierarchy over the hierarchical representation of bilingual constituents in a top-downmanner.
For the topmost (first) level:?ij,pq|Bij,pq= b ?
GfirstbGfirstb?
PY P (dfirstb, ?firstb, Pword?pos(.|Bij,pq= b))The PYP has three parameters: (dfirstb, ?firstb, Pword?pos).
Pword?pos(.|Bij,pq= b) is a basedistribution over infinite space of bilingual constituents conditioned on span type b, which providesthe back-off probability of P (?ij,pq|Bij,pq= b).
The remaining parameters dfirstband ?firstbcontrol thestrength of the base distribution.The back-off probability Pword?pos(?ij,pq= x|Bij,pq= b) is defined as below:Pword?pos(?ij,pq= x|Bij,pq= b)) = Pword(Rw(x)|b)?
Ppos(Rp(x)|b)where Rw(x) is the function returning a word sequence of a bilingual constituent x, Rp(x) returningthe correspondent POS sequence.
This is the second level of the hierarchical representation of bilingualconstituents as illustrated in Fig.
2.
Further, Rw(x) and Rp(x) are decomposed into the third level ofthe hierarchy.
Taking Rw(x) for example:Pword(Rw(x)|Bij,pq= b)) = Pword?bound(Rwb(x)|b)?1|W ||Rw(x)|?|Rwb(x)|where Rwb is a function returning a word sequence?s boundary representation, |W | is the vocabularysize, |Rw(x)| ?
|Rwb(x)| is the number of the words in Rw(x) excluding those in the boundary rep-resentation.
The above equation models the generation of a word sequence with surface string Rw(x)(given b) by first generating its boundary representation Rwb(x), then generating its middle words froma uniform distribution over the vocabulary.
Ppos(Rp(x)|Bij,pq= b)) is defined similarly.We put the Dirichlet prior over Pword?bound(Rwb(x)|b):Rwb(x)|b ?
Discrete(GRwbb)GRwbb?
Dirichlet(?b)For Ppos?bound(Rpb(x)|b), similar definition to Pword?bound(Rwb(x)|b) is applied.3.2 The Dirichlet Prior over Bilingual ContextsThe Dirichlet prior is defined as below:?ij,pq|Bij,pq= b ?
Discrete(GDirb)GDirb?
Dirichlet(?b)A context ?ij,pq(given the specific span type b) is drawn i.i.d according to a multinomial parameterGDirb, which is drawn from the Dirichlet distribution with a real value parameter ?b.18694 MCMC Sampling for Inferring the Latent Synchronous TreesWe approximate the distribution over latent synchronous trees by sampling them from the posteriorP (T |S), where T is a latent synchronous tree of a sentence pair S. As presented in the beginning ofsection 3, the posterior depends on P (?ij,pq|Bij,pq= b) and P (?ij,pq|Bij,pq= b), on which we putthe PYP prior and the Dirichlet prior respectively.
Because of integrating out all Gs in all of the priors,interdependency between samples of ?ij,pq|Bij,pq= b or ?ij,pq|Bij,pq= b is introduced, resulting insimultaneously obtaining multiple samples impractical.
On the other hand, blocked sampling, which ob-tains sentence-level samples simultaneously (Blunsom and Cohn, 2010; Cohn et al., 2010; Johnson et al.,2007a) is attractive for the fast mixing speed and the easy application of standard dynamic programmingalgorithms.4.1 Metropolis-Hastings (MH) SamplerWe apply a MH sampler similar to (Johnson et al., 2007a) to overcome the difficulty of obtaining multi-ple samples simultaneously from posterior.
The MH sampler is a MCMC technique that draws samplesfrom a true distribution by first drawing samples simultaneously from a proposal distribution, and thencorrecting the samples to the true distribution by using an accept/reject test.
In practical, the proposaldistribution is designed to facilitate the use of blocked sampling that applies standard dynamic program-ming, and the resulting samples are corrected by the accept/reject test to the true distribution.In our case, the proposal distribution is theMaximum-a-Posteriori (MAP) estimate of P (?i,j|Bi,j= b)and P (?i,j|Bi,j= b), and the blocked sampling of T applies a dynamic programming algorithm that isbased on the inside chart derived from a transformation of Eq.
(1):P (S, T ) = K(S)?<i,j><p,q>?T?
(ij, pq)where ?
(ij, pq) =P (?ij,pq|Bij,pq= 1)P (?ij,pq|Bij,pq= 1)P (?ij,pq|Bij,pq= 0)P (?ij,pq|Bij,pq= 0)K(S) is a constant given S. The inside chart I can be constructed recursively as below:Iij,pq=????????????
(ij, pq) if j ?
i ?
1 and q ?
p ?
1?
(ij, pq)?i?u?jp?v?q(Iiu,pvIuj,vq+ Iiu,vqIuj,pv) otherwiseBased on this inside chart, a synchronous tree can be top-down sampled (Johnson et al., 2007a), thenis accepted or rejected by the MH-test to correct to the true distribution.5 ExperimentsThe experiments were conducted on both a pilot word alignment task and an end-to-end Chinese-to-English machine translation task to test the quality of the learned synchronous structures by the SCCM.The bi-side monolingual gold bracketings contained in Penn treebanks were not used for evaluating thequality of the learned synchronous structures because of great syntactic divergence between source treeand target tree, which results in that gold monolingual syntactic trees on both sides are asynchronous(large number of tree nodes can not be aligned).
The synchronous grammar induction community as-sumes the existence of synchronous grammar for MT, and do not evaluate synchronous grammar induc-tion on monolingual gold treebanks because of their asynchronous property.
The synchronous grammarinduction community is not the same with the multilingual grammar induction community, which targetsat inducing bi-side monolingual syntactic trees.
Due to the same reason, our synchronous bracketinginduction method was not evaluated on bi-side monolingual bracketing trees which are asynchronous.18705.1 Sampler ConfigurationOur sampler was initialised with trees through a random split process.
Firstly, we used GIZA++ mod-el 4 to get source-to-target and target-to-source word alignments, and used grow-diag-final-and (gdfa)heuristic to extract reliable word alignments for each sentence pair.
Secondly, we randomly split eachsentence pair in a top-down manner, and make sure that each split is consistent with the GIZA++ gdfaword alignments.
For example, given a sentence pair of m source words and n target words, we random-ly choose a split point at each side and the alignment type (straight alignment or inverted alignment),then recursively build bi-spans further on each new split.
Finally, a synchronous binary tree is built atthe end of this process1.
Note that all splits must be consistent with the GIZA++ gdfa word alignments.When a piece of word alignments (such as non-ITG alignment structure) do not permit binary split, wekeep this structure unsplitted and continue split only on its sub-structures that are ITG derivable.Our sampler ran 200 iterations for all data.
After each sampling iteration, we resample all the hyper-parameters using slice-sampling, with the following priors: d ?
Beta(1, 1), ?
?
Gamma(10, 0.1).The time complexity of our inference algorithm is O(n6), which is not practical in applications.
Wereduce the time complexity by only considering bi-spans that do not violate GIZA++ intersection wordalignments (intersection of source-to-target and target-to-source word alignments) (Cohn and Haffari,2013).5.2 Word Alignment Task5.2.1 Experimental SettingSince there are no annotated synchronous treebanks, we evaluate the SCCM indirectly by evaluating itsoutput word alignments on a gold standard English Chinese parallel tree bank with hand aligned wordalignments referred as HIT corpus2.
The HIT corpus, which was collected from English learning textbooks in China as well as example sentences in dictionaries, was originally designed for annotatingbilingual tree node alignments.
The annotation strictly reserves the semantic equivalence of the alignedsub-tree pair.
The byproduct of this corpus is the hand aligned word alignments, which was utilizedto evaluate word alignment performance3.
The word segmentation, tokenization and parse-tree in thecorpus were manually constructed or checked.
The statistics of the HIT corpus are shown in table 1.Table 1: Corpus statistics of the HIT corpus.ch ensent 16131word 210k 209kavg.
len.
13.06 13.05.2.2 ResultsWe adopt the commonly used metric: the alignment error rate (AER) to evaluate our proposed align-ments (a) against hand-annotated alignments, which are marked with sure (s) and possible (p) align-ments.
The AER is given by (the lower the better):AER(a, s, p) = 1?|a ?
s|+ |a ?
p||a|+ |s|In the HIT corpus, only sure alignments were annotated, possible alignments were bypassed becauseof the strict annotation standard of semantic equivalence.The word alignments evaluation results are reported in Table 2.
The baseline was GIZA++ model4 in both directions with symmetrization by the grow-diag-final-and heuristic (Koehn et al., 2003).
A1The initialization with different random split bi-trees results in marginal variance of performances.2HIT corpus is designed and constructed by HIT-MITLAB.
http://mitlab.hit.edu.cn/index.php/resources.html3We did not use annotated tree node alignments for synchronous structure evaluation because the coverage of tree nodesthat can be aligned is quite low.
The reason of low coverage is that Chinese and English exhibit great syntax divergences frommonolingual treebank point of view.1871released induction system - PIALIGN (Neubig et al., 2011)4was also experimented to compare with ourproposed induction system - SCCM.PIALIGN is a model that generalizes adaptor grammars for machine translation (MT), while our mod-el is to generalize CCM for MT.
Adaptor grammars has been successfully applied on shallow unsuper-vised tasks such as morphlogical/word analysis, while CCM has obtained state-of-the-art performanceon the more complex unsupervised task - inducing syntactic trees.
In view of CCM?s successful mono-lingual application, we generalize it to bilingual case.
In depth comparison: our SCCM deals with bothconsituents and distituents, and contexts of them, while PIALIGN only deals with constituents.
Fur-thermore, SCCM does not model non-terminal rewriting rules, while PIALIGN model those rules whichcan rewrite a non-terminal into a complete subtree as adaptor grammars does.
In addition, PIALIGNadopts a beam search algorithm of (Saers et al., 2009).
Through setting small beam size, PIALIGN?stime complexity is almost O(n3).
But as critisized by (Cohn and Haffari, 2013), their heuristic beamsearch algorithm does not meet either of the Markov Chain Monte Carlo (MCMC) criteria of ergodic-ity or detailed balance.
Our method adopts MCMC sampling (Johnson et al., 2007a) which meets theMCMC criteria.We can see that the two induction systems perform significantly better than GIZA++, and our proposedSCCM performs better than PIALIGN.
Manual evaluation for the quality of the phrase pairs generatedfrom word alignments is also reported in Table 2.
We considered the top-100 high frequency phrase pairsthat are beyond word level and less than six words on both sides, and report the proportion of reasonablywell phrase pairs through manual check.
We found that more good phrase pairs can be derived from theSCCM?s word alignments than from others.Table 2: Quality of word alignments and their generated phrase pairs.AER good phrase pairs proportionGIZA++ 0.322 0.493PIALIGN 0.263 0.531SCCM 0.255 0.5345.3 Machine Translation Task5.3.1 Experimental SettingA released tourism-related domain machine translation data was used in our experiment.
It consists of aparallel corpus extracted from the Basic Travel Expression Corpus (BTEC), which had been used inevaluation campaigns of the yearly International Workshop on Spoken Language Translation (IWSLT).Table 3 lists statistics of the corpus used in the experiment.Table 3: Statistics of the corpus used by IWSLTch ensent 23kword 190k 213kavg.
len.
8.3 9.2We used CSTAR03 as development set, used IWSLT04 and IWSLT05 official test set for test.
A4-gram language model with modified Kneser-Ney smoothing was trained on English side of parallelcorpus.
We use minimum error rate training (Och, 2003) with nbest list size 100 to optimize the fea-ture weights for maximum development BLEU.
Experimental results were evaluated by case-insensitiveBLEU-4 (Papineni et al., 2001).
Closest reference sentence length was used for brevity penalty.5.3.2 ResultsFollowing (Levenberg et al., 2012; Neubig et al., 2011; Cohn and Haffari, 2013), we evaluate our modelby using the SCCM?s output word alignments to construct a phrase table.
As a baseline, we train aphrase-based model using the moses toolkit5based on the word alignments obtained using GIZA++4http://www.phontron.com/pialign/5http://www.statmt.org/moses1872model 4 in both directions and symmetrized using the grow-diag-final-and heuristic (Koehn et al., 2003).For comparison with CFG-based induction systems, word alignments generated by the PIALIGN werealso used to train a phrase-based model.In the end-to-end MT evaluation, we used the standard set of features: relative-frequency and lexicaltranslation model probabilities in both directions; distance-based distortion model; language model andword count.
The evaluation results are reported in table 4.
Word alignments derived by the two inductionsystems can be more helpful to obtain better translations than GIZA++ derived word alignments.
TheSCCM, while departing from traditional CFG-based methods, achieves comparable translation perfor-mance to the PIALIGN.Table 4: BLEU on both the development set: CSTAR03, and the two test sets: IWSLT04 and IWSLT05.CSTAR03 IWSLT04 IWSLT05GIZA++ 0.4304 0.4190 0.4866PIALIGN 0.4661 0.4556 0.5248SCCM 0.4560 0.4469 0.51936 ConclusionA new model for synchronous structure induction is proposed in this paper.
Different to all the previousworks that are based on Context Free Grammars, our proposed SCCM deals with bilingual constituentsand contexts explicitely so that bilingual translational equivalences can be directly modeled.
A non-parametric Bayesian modeling of the SCCM is applied to cope with the sparse representations of bilin-gual constituents and contexts.
Both intrinsic evaluation on word alignments and extrinsic evaluation onend-to-end machine translation were conducted.
The intrinsic evaluation show that the highest qualityword alignments were obtained by our proposed SCCM.
Such statistically sound word alignments ofthe SCCM were used in the extrinsic evaluation on machine translation, showing that significantly bettertranslations were achieved than those obtained by using the word alignments of GIZA++, the widelyused word aligner in the two-step pipeline.AcknowledgmentsThis work was supported by the National Natural Science Foundation of China under grant No.61273319, and grant No.
61373095.
Thanks for the helpful advices of anonymous reviewers.ReferencesPhil Blunsom and Trevor Cohn.
2010.
Unsupervised induction of tree substitution grammars for dependencyparsing.
In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages1204?1213.
Association for Computational Linguistics.Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter estimation.
Computational linguistics, 19(2):263?311.Shay B Cohen, Kevin Gimpel, and Noah A Smith.
2008.
Logistic normal priors for unsupervised probabilisticgrammar induction.
In Proceedings of the Advances in Neural Information Processing Systems.Trevor Cohn and Phil Blunsom.
2009.
A bayesian model of syntax-directed tree to string grammar induction.
InProceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume1, pages 352?361.
Association for Computational Linguistics.Trevor Cohn and Gholamreza Haffari.
2013.
An infinite hierarchical bayesian model of phrasal translation.
InProceedings of the 51th Annual Meeting of the Association for Computational Linguistics.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010.
Inducing tree-substitution grammars.
Journal ofMachine Learning Research, 11:3053?3096.Xiangyu Duan, Zhang Min, and Chen Wenliang.
2013.
Smoothing for bracketing induction.
In Proceedings of23rd International Joint Conference on Artificial Intelligence.
AAAI Press/International Joint Conferences onArtificial Intelligence.1873Jenny Rose Finkel, Trond Grenager, and Christopher D Manning.
2007.
The infinite tree.
In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 272.Sharon Goldwater, Thomas L Griffiths, and Mark Johnson.
2011.
Producing power-law distributions and dampingword frequencies with two-stage language models.
Journal of Machine Learning Research, 12:2335?2382.Mark Johnson, Thomas Griffiths, and Sharon Goldwater.
2007a.
Bayesian inference for pcfgs via markov chainmonte carlo.
In Proceedings of Human Language Technologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139?146.Mark Johnson, Thomas L Griffiths, and Sharon Goldwater.
2007b.
Adaptor grammars: A framework for specify-ing compositional nonparametric bayesian models.
Proceedings of Advances in neural information processingsystems, 19:641.Dan Klein and Christopher Manning.
2002.
A generative constituent-context model for improved grammar induc-tion.
In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 128?135.Association for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.
Statistical phrase-based translation.
In Proceedingsof the 2003 Conference of the North American Chapter of the Association for Computational Linguistics onHuman Language Technology-Volume 1, pages 48?54.
Association for Computational Linguistics.Abby Levenberg, Chris Dyer, and Phil Blunsom.
2012.
A bayesian model for learning scfgs with discontiguousrules.
In Proceedings of the 2012 joint conference on empirical methods in natural language processing andcomputational natural language learning, pages 223?232.
Association for Computational Linguistics.P.
Liang, S. Petrov, M. I. Jordan, and D. Klein.
2007.
The infinite PCFG using hierarchical Dirichlet process-es.
In Empirical Methods in Natural Language Processing and Computational Natural Language Learning(EMNLP/CoNLL).Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara.
2011.
An unsupervisedmodel for joint phrase alignment and extraction.
In Proceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human Language Technologies-Volume 1, pages 632?641.
Association forComputational Linguistics.Franz Josef Och.
2003.
Minimum error rate training in statistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160?167.
Association forComputational Linguistics.Markus Saers, Joakim Nivre, and Dekai Wu.
2009.
Learning stochastic bracketing inversion transduction gram-mars with a cubic time biparsing algorithm.
In Proceedings of the 11th International Conference on ParsingTechnologies, pages 29?32.
Association for Computational Linguistics.Xinyan Xiao and Deyi Xiong.
2013.
Max-margin synchronous grammar induction for machine translation.
InEMNLP.Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, and Shouxun Lin.
2012.
Unsupervised discriminative induction ofsynchronous grammar for machine translation.
In COLING, pages 2883?2898.Teh YeeWhye.
2006.
A bayesian interpretation of interpolated kneser-ney.
In Technical Report TRA2/06.
Schoolof Computing, National University of Singapore.1874
