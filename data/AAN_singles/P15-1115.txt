Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1191?1201,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsSemantic Role Labeling Improves Incremental ParsingIoannis Konstas and Frank KellerInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh{ikonstas,keller}@inf.ed.ac.ukAbstractIncremental parsing is the task of assign-ing a syntactic structure to an input sen-tence as it unfolds word by word.
Incre-mental parsing is more difficult than full-sentence parsing, as incomplete input in-creases ambiguity.
Intuitively, an incre-mental parser that has access to seman-tic information should be able to reduceambiguity by ruling out semantically im-plausible analyses, even for incomplete in-put.
In this paper, we test this hypothesisby combining an incremental TAG parserwith an incremental semantic role labelerin a discriminative framework.
We showa substantial improvement in parsing per-formance compared to the baseline parser,both in full-sentence F-score and in incre-mental F-score.1 IntroductionWhen humans listen to speech, the input becomesavailable gradually as the speech signal unfolds.Reading happens in a similarly gradual mannerwhen the eyes scan a text.
There is good evidencethat the human language processor is adapted tothis and works incrementally, i.e., computes an in-terpretation for an incoming sentence on a word-by-word basis (Tanenhaus et al, 1995; Altmannand Kamide, 1999).
Also language processingsystems often deal with speech as it is spoken, ortext as it is typed.
A dialogue system should startinterpreting a sentence while it is spoken, and aninformation retrieval system should start retrievingresults while the user is typing.Incremental processing is therefore essentialboth for realistic models of human language pro-cessing and for NLP applications that react touser input in real time.
In response to this, anumber of incremental parsers have been devel-oped, which use context-free grammar (Roark,2001; Schuler et al, 2010), dependency grammar(Chelba and Jelinek, 2000; Nivre, 2007; Huangand Sagae, 2010), or tree-substitution grammars(Sangati and Keller, 2013).
Typical applicationsof incremental parsers include speech recognition(Chelba and Jelinek, 2000; Roark, 2001; Xu et al,2002), machine translation (Schwartz et al, 2011;Tan et al, 2011), reading time modeling (Dembergand Keller, 2008), or dialogue systems (Stonesset al, 2004).Incremental parsing, however, is considerablyharder than full-sentence parsing: when process-ing the n-th word in a sentence, an, the parser onlyhas access to the left context (words a1.
.
.an?1);the right context (words an+1.
.
.aN) is not knownyet.
This can lead to local ambiguity, i.e., pro-duce additional syntactic analyses that are validfor the sentence prefix, but become invalid as theright context is processed.
As an example considerthe sentence prefix in (1):(1) The athlete realized her goals .
.
.a.
at the competitionb.
were out of reachThe prefix could continue as in (1-a), i.e., as amain clause structure.
Or the next words couldbe as in (1-b), in which case her goals is part of asubordinate clause.Intuitively, an incremental parser that has accessto semantic information would be able to decidewhich of these two analyses is likely to be correct,even without knowing the rest of the sentence.
Ifthe NP her goals is a likely ARG1 of realized theparser should prefer the main clause structure.
Onthe other hand, if the NP is a likely ARG0 of an (asyet unseen) embedded verb, then the parser shouldgo for the subordinate clause structure.
This is il-lustrated in Figure 2.
Note that the preference caneasily be reversed: if the prefix was the athlete re-alized her shoes, then her shoes is very likely tobe an ARG0 rather than an ARG1.1191The basis of this paper is the hypothesis thatsemantic information can aid incremental parsing.To test this hypothesis, we combine an incremen-tal TAG parser with an incremental semantic rolelabeling (iSRL) system.
The iSRL system takesprefix trees and computes their most likely seman-tic role assignments.
We show that these role as-signments can be used to re-rank the output ofthe incremental parser, leading to substantial im-provements in parsing performance compared tothe baseline parser, both in full-sentence F-scoreand in incremental F-score.2 Incremental Semantic Role LabelingThe current work builds on an existing incremen-tal parser, the Psycholinguistically Motivated TreeAdjoining Grammar (PLTAG) parser of Demberget al (2013).
The distinguishing feature of thisparser is that it builds fully connected structures(no words are left unattached during incrementalparsing); this requires it to make predictions aboutthe right context, which are verified as more ofthe input becomes available.
Konstas et al (2014)show that semantic information can be attached toPLTAG structures, making it possible to assign se-mantic roles incrementally.
In the present paper,we use these semantic roles to re-rank the outputof the PLTAG parser.2.1 Psycholinguistically Motivated TAGPLTAG extends standard TAG (Joshi and Sch-abes, 1992) in order to enable incremental parsing.Standard TAG assumes a lexicon of elementarytrees, each of which contains at least one lexicalitem as an anchor and at most one leaf node asa foot node, marked with A?.
All other leavesare marked with A?
and are called substitutionnodes.
To derive a TAG parse for a sentence, westart with the elementary tree of the head of thesentence and integrate the elementary trees of theother lexical items of the sentence using two oper-ations: adjunction at an internal node and substi-tution at a substitution node (the node at which theoperation applies is the integration point).
Stan-dard TAG derivations are not guaranteed to be in-cremental, as adjunction can happen anywhere ina sentence, possibly violating left-to-right process-ing order.
PLTAG addresses this limitation by in-troducing prediction trees, elementary trees with-out a lexical anchor.
These are used to predictsyntactic structure anchored by words that appearslater in an incremental derivation.
This ensuresaSB?C?aSBC?baSB?Cc(a) valid (b) invalidFigure 1: The current fringe (dashed line) indi-cates where valid substitutions can occur.
Othersubstitutions result in an invalid prefix tree.that fully connected prefix trees can be built forevery prefix of the input.In order to efficiently parse PLTAG, Demberget al (2013) introduce the concept of fringes.Fringes capture the fact that in an incrementalderivation, a prefix tree can only be combined withan elementary tree at a limited set of nodes.
Forinstance, the prefix tree in Figure 1 has two substi-tution nodes, for B and C. However, only substi-tution into B leads to a valid new prefix tree; if wesubstitute into C, we obtain the tree in Figure 1b,which is not a valid prefix tree (i.e., it represents anon-incremental derivation).2.2 Incremental Role PropagationThe output of a semantic role labeler is a set ofsemantic dependency triples ?l,r, p?, where l is asemantic role label (e.g., ARG0, ARG1, ARGM inPropbank), and r and p are the words (argumentand predicate) to which the role applies.
An incre-mental semantic role labeler assigns semantic de-pendency triples to a prefix of the input sentence.Note that not every word is an argument to a pred-icate, therefore the set of triples will not necessar-ily change at every input word.
Also, triples can beincomplete, as either the predicate or the argumentmay not have been observed yet.Konstas et al (2014) propose an iSRL systembased on a PLTAG parser with a semantically aug-mented lexicon.
They parse an input sentence in-crementally, applying their incremental role prop-agation algorithm (IRPA) to the resulting prefixtrees.
This creates new semantic triples (or up-dates existing, incomplete ones) whenever an el-ementary or prediction tree that carries semanticrole information is attached to the prefix tree.
Assoon as a triple is completed a two-stage classifica-tion process is applied, that first identifies whetherthe predicate/argument pair is a good candidate,and then disambiguates the role label (often multi-ple roles are possible for a lexical entry).
Figure 2shows the incremental role assignment for the tworeadings of the prefix the athlete realized her goals21192SVPNPNNSgoals{A1}DTherVPVBDrealizedNPNNathlete{A0}DTThe?A0,athlete,realized??A1,goals,realized?(a)SVPSBARVP{A1}NPNNSgoals{A0}DTherVPVBDrealizedNPNNathlete{A0}DTThe?A0,athlete,realized??A1,nil,realized??A0,goals,nil?
(b)Figure 2: Incremental Role Propagation Algorithm application for two different prefix trees of the sen-tence prefix the athlete realized her goals.
In (a) the parser builds a main clause, so IRPA assigns an A1to goals with realized as predicate.
In (b) the parser predicts an embedded clause, so IRPA delays theassignment of the A1 to realized, and instead introduces two incomplete triples: the first one is predicate-incomplete, with the argument goals assigned an A0, waiting to be attached to a predicate.
The secondone is argument-incomplete with predicate realized assigned an A1, waiting for an argument to follow.
(see Section 1).
Note the use of incomplete seman-tic role triples in Figure 2b.3 ModelWe use a discriminative model in order to re-rankthe output of the baseline PLTAG parser based onsemantic roles assigned by the iSRL system.3.1 Problem FormulationOur overall approach is closely related to thediscriminative incremental parsing framework ofCollins and Roark (2004).
The goal is to learna mapping from input sentences x ?
X to parsetrees y ?
Y .
For a given set of training pairs ofsentences and gold-standard parse trees (x,y) ?X ?Y , the output y?
can be defined as:y?
= argmaxy?GEN(x)?
(x,y) ?
w?
(1)where GEN(x) is a function that enumerates can-didate parse trees for a given input x, ?
is a rep-resentation that maps each training example (x,y)to a feature vector ?
(x,y) ?
Rd, and w?
?
Rdis avector of feature weights.During training, the task is to estimate w?
giventhe training examples.
In terms of efficiency, acrucial part of Equation (1) is the search strategyover parses produced by GEN and, to a smallerdegree, the dimensionality of w?.
One common de-coding technique is to implement a dynamic pro-gram, thus avoiding the explicit enumeration ofall analyses for a given timestamp (Huang, 2008).However, central to the discriminative approach isthe exploration of features that cannot be straight-forwardly embedded into the parser using a dy-namic program.
These include arbitrarily long-range dependencies contained in a parse tree, andmore importantly non-isomorphic representationsof the input sentence such as its semantic frame,i.e., the set of all semantic roles tripes that pertainto the same predicate.
In order to accommodatethese, we decode via beam search over candidateparses.
We keep a list of the k-best analyses andprune those whose score scr(x) = ?
(x,y) ?
w?
fallsbelow a threshold.3.2 Incremental k-best ParsingWhat we described in the previous section couldequally apply to k-best re-ranking for full-sentenceparsing (e.g., Charniak and Johnson, 2005).
Forincremental parsing, in addition to outputting y?
forthe full sentence, we need to output prefix treesy?nfor every prefix of length n ?
{1 .
.
.N} of sen-tence x = a1.
.
.aNwith length N. Let ?xn, y?n,n?,be the state of our model after we have parsed thefirst n words of sentence x, resulting in analysis y?n.The initial state is defined as ?x0,/0,0?, where/0 isthe empty analysis, and the final state is ?x, y?,N?,which represents a full analysis for the input sen-tence.
We need a function ADV that transitionsfrom a state at word anto a set of states at word31193an+1by combining the prefix tree y?nwith an+1:ADV(?xn, y?n,n?
)= ?xn, y?n,n?
?an+1= {?xn+1, y?n+1,n+1?
}Next, we define the set of states representingprefix trees as pi, with pi0= {?x0,/0,0?
}, andpin= ?pi?
?pin?1ADV (pi?).
We can now redefineGEN(xn) = pin, for any prefix of length n.We enumerate prefix trees (function GEN) withthe incremental parser of Demberg et al (2013).The states of the model are stored in a chart; eachcell holds the top-k prefix trees.
The transitionto the next state (function ADV ) is performed bycombining each prefix tree with a set of candidateof elementary (and prediction) trees via adjunc-tion and substitution, subject to restrictions im-posed by incrementallity (see Figure 2).
In or-der to efficiently compute all combinations, thePLTAG parser computes only the fringes (see Sec-tion 2) of the prefix tree, and the candidate ele-mentary trees and matches these two; this avoidsthe computation of the prefix tree entirely.1Each prefix tree is weighted using a probabil-ity model estimated over PLTAG operations andthe lexicon.
This probability is used as a featurein ?.
In addition, we define a set of features ofincreasing sophistication, which include featuresspecific to PLTAG, standard tree-based features,and, crucially, features extracted from the seman-tic role triples produced incrementally by the iSRLsystem of Konstas et al (2014).
The features arecomputed for each prefix tree yn, so ?
can be re-written as ?
(xn,yn), and therefore Equation (1) be-comes:y?n= argmaxyn?pin?
(xn,yn) ?
w?
(2)Our goal now becomes to learn mappings betweensentence prefixes xnand prefix trees y?n.
In contrastto models that estimate features weights on fullsentence parses (Collins and Roark, 2004; Char-niak and Johnson, 2005), we do not observe gold-standard prefix trees during training.
However, wecan use gold-standard lexicon entries when pars-ing the training data with the PLTAG parser, whichgives an approximation of gold-standard prefixtrees y+n.
Finally, during testing, given an unseensentence x and a trained set of feature weights w?,our model generates prefix trees ynfor every sen-tence prefix of size n.1As in a chart parser, the prefix tree can be re-constructedby following backpointers in the chart.
This is done onlyfor evaluation at the end of the sentence or incrementally ondemand.4 Reranking FeaturesThis section describes the features used for rerank-ing the prefix trees generated by the incrementalparser.
We include three different classes of fea-tures, based on local information from PLTAG el-ementary trees, based on global and structural in-formation from prefix trees, and based on seman-tic information provided by iSRL triples.
In con-trast to work on discriminative full-sentence pars-ing (e.g., Charniak and Johnson, 2005; Collins andKoo, 2005), we can only use features extractedfrom the prefix trees being constructed incremen-tally as the sentence is parsed.
The right context ofthe current word cannot be used, as this would vio-late incrementality.
Every feature combination wetry also includes the following baseline features:Prefix Tree Probability is the log probability ofthe prefix tree as scored by the probability modelof the baseline parser.
The score is normalized byprefix length, to avoid getting larger negative logprobability scores for longer prefixes.Elementary Tree Probability is the log proba-bility of the elementary tree corresponding to theword just added to the prefix tree according to theprobability model of the baseline parser.4.1 PLTAG FeaturesThe baseline generative model of the PLTAGparser employs features based on parsing actions,the elementary trees used at each timestamp, andthe previous word and PoS tag.
In the discrimi-native model, we extend the locality of these fea-tures, as well as addressing sparsity issues arisingfrom rare elementary trees.
In all cases, both lex-icalized and unlexicalized versions of the elemen-tary trees are used.Unigram Trees is a family of binary featuresthat record the local elementary trees chosen bythe parser for the n-th word, i.e., current word forn = 1 and previous word for n = 2.Parent-Unigram Trees is a variation of the pre-vious feature, where we encode the elementarytree of the current word along with the category ofthe node it attaches to in the prefix tree.
This cap-tures the attachment decisions the parser makes.Bigram Trees are pairs of elementary trees foradjacent words (i.e., the elementary tree currentlyadded to the prefix tree and the previous one).This extends the history the parser has access to,41194and captures pairs of elementary trees that are fre-quently chosen together, e.g., a verb-headed treewith a PP foot node, followed by an NP-headedprepositional tree.4.2 Tree FeaturesThe following features are inspired by Charniakand Johnson (2005) and attempt to encode proper-ties of the prefix tree, as well as capture regulari-ties for specific syntactic construction such as co-ordination.
Even though the PLTAG parser buildsfully connected structures and predicts upcomingcontext, some constituents in a given prefix treemay be incomplete.
We therefore compute the fea-tures in this group only for those constituents thathave been completed in the current prefix tree (i.e.,constituents that are complete at word an, but wereincomplete at word an?1).
This ensures each ofthe features is only counted once per constituent.For example, the coordination parallelism feature(see below) should be computed only after all thewords in the yield of the CC non-terminal havebeen observed.Right Branch encodes the number of nodes onthe longest path from the root of the prefix tree tothe rightmost pre-terminal.
We also include thesymmetrical feature which records the number ofthe remaining nodes in the prefix tree.
This featureallows the parser to prefer right-branching trees,commonly found in English syntax.Coordination Parallelism records whether thetwo sibling subtrees of a coordination node areidentical in terms of structure and node categoriesup to depth l. We encode identity in a bit mask,and set l = 4 (e.g., 1100 means the subtrees haveidentical children and grandchildren).Coordination Parallelism Length indicates thebinned difference in size between the yields ofeach sibling subtree under a coordination node.
Italso stores whether the second subtree is at the endof the sentence.Heavy stores the category of each node in acompleted constituent, along with the binnedlength of its yield and whether it is at the end ofthe sentence.
This feature captures the tendencyof larger constituents to occur towards the end ofthe sentence.Neighbors encodes the category of each node inthe completed constituent, the binned yield size,and the PoS tags of the l preceding words, werel = 1 or 2.Word stores the current word along with the cat-egories of its l immediate ancestor nodes (exclud-ing pre-terminals); l = 2 or 3.4.3 SRL FeaturesThe features in this group are extracted from theoutput of iSRL system of Konstas et al (2014),which annotates prefix trees with semantic roles.The setup proposed in the current paper makesit possible to feed the semantic information backto the PLTAG parser by using it to re-rank the k-best prefix trees generated by the parser.
(The re-ranked prefix trees could then also result in betteriSRL performance, an issue we will return to inSection 6.3.
)Recall that the SRL information comes in theform of triples ?l,r, p?, where l is a semantic rolelabel and r and p are the words to which the roleapplies (see Figure 2 for examples).
For each fea-ture, we also compute an unlexicalized versionby replacing the argument and predicates in thetriples with their PoS tags.Complete SRL Triples stores the completetriples (if any) generated by the current word.
Theword can be the predicate or the argument in oneor more dependency relations involving previouswords.Semantic Frame records all the arguments ofa predicate (if present) for frequent semantic la-bels, i.e., A0, A1 and A2, as well as the presenceof a modifier (e.g., AM-TMP, AM-LOC, etc.
).This feature usually fires when a verb is added tothe prefix tree and generates several complete SRLtriples.
The feature captures the semantic frame ofa verb as a whole (while the previous feature justrecords it as a collection of triples).Back-off SRL Triples are generated by remov-ing either the argument, or the predicate, or therole label, from a complete triple.
This providesa way of generalizing between triples that sharesome information without being completely iden-tical.Predicate/Argument/Role encodes the ele-ments of a complete SRL triple individually(argument, predicate, or role).
This allows forfurther generalization and reduces sparsity.511955 Feature Weight EstimationWe estimate the vector of feature weights w?
inEquation (2) using the averaged structured percep-tron algorithm of Collins (2002); we give the pseu-docode in Algorithm 1.
The perceptron makesT passes over L training examples.
In each it-eration, for each sentence prefix/prefix tree pair(xn,yn), it computes the best scoring prefix tree y?namong the candidate prefix trees, given the cur-rent feature weights w?.
In line 7, the algorithmupdates w?
with the difference (if any) between thefeature representations of the best scoring prefixtree y?nand the approximate gold-standard prefixtree y+n(see Section 3.2).
Note that since we usea constant beam during decoding with the PLTAGparser in order to enumerate the set of prefix treespin, there is no guarantee that the argmax in line 5will find the highest scoring (in terms of F-score)prefix tree y?n6= y?n.
Search errors due to the bestanalysis falling out of the beam at a given pre-fix length will create errors both when decodingunseen sentences at test time, and when learningthe feature weights with the perceptron algorithm.The final weight vector w?
is the average of theweight vectors over T iterations, L examples andN words.
The averaging avoids overfitting andproduces more stable results (Collins, 2002).Note that features are computed for every prefixof the input sentence.
Recall that the parser avoidsthe explicit computation of the prefix trees in pinthrough the use of the fringes (see Sections 2.1and 3.2).
This is sufficient for the computation ofPLTAG and SRL features, but we need to explic-itly calculate every prefix tree ynfor the computa-tion of the tree features (see Section 4.2).
This isan expensive operation if we are parsing the wholetraining corpus.
To overcome this time bottleneck,we compute features only for those analyses ofevery input sentence prefix that belongs to the k-best analyses at the end of the sentence.
In otherwords, pinis the set of only those prefix trees thatare used by the k-best analyses at the end of thesentence.
This results in a much smaller numberof prefix trees that need to be computed for eachword.
However, during testing, given the trainedw?
and an unseen sentence, we compute all featuresfor each prefix length of the sentence, hence calcu-late all prefix trees in pinand incrementally re-rankthe chart entries of the parser on the fly.Algorithm 1: Averaged Structured PerceptronInput: Training Examples: (x,y)Li=1,xi= a1.
.
.aN1 w??
02 for t?
1 .
.
.T do3 for i?
1 .
.
.L do4 for n?
1 .
.
.N do5 y?n= argmaxyn?pin?
(xn,yn) ?
w?6 if y+n6= y?nthen7 w??
w?+?(xn,y+n)??
(xn,yn)8 return1T?Tt=11L?Li=1?Nn=11Nwt,i,n6 Experiments6.1 SetupWe use the PLTAG parser of Demberg et al (2013)to enumerate prefix trees ynand to compute theprefix tree and word probability scores which weuse as features.
We also use the iSRL systemof Konstas et al (2014) to generate incrementalSRL triples.
Their system includes a semantically-enriched lexicon extracted from the Wall StreetJournal (WSJ) part of the Penn Treebank corpus(Marcus et al, 1993), converted to PLTAG for-mat.
Semantic role annotation is sourced fromPropbank.
We trained the probability model ofthe parser and the identification and labeling clas-sifiers of the iSRL system using the intersection ofSections 2?21 of WSJ and the English portion ofthe CoNLL 2009 Shared Task (Haji?c et al, 2009).We learn the weight vector w?
by training the per-ceptron algorithm also on Sections 2?21 of WSJ(see Section 5 for details).
We use the PoS tagspredicted by the parser, rather than gold standardPoS tags.
Testing is performed on section 23 ofWSJ, for sentences up to 40 words.6.2 EvaluationIn addition to standard full-sentence labeledbracket score, we evaluate our model incremen-tally, by scoring the prefix trees generated for eachsentence prefix (Sangati and Keller, 2013).
Foreach prefix of the input sentence (two words ormore), we compute the labeled bracket score forthe minimal structure spanning that prefix.
Theminimal structure is defined as the subtree rootedin the lowest common ancestor of the prefix nodes,while removing any leftover intermediate nodeson the right edge of the subtree that do not havea word in the prefix as their yield.Although not the main focus of this paper, wealso report full-sentence combined SRL accuracy(counting verb-predicates only).
This score is ob-tained by re-applying the iSRL system to the syn-61196System Prec Rec F AUC SRLBASELINE 75.51 76.93 76.21 71.49 69.43TREE 75.99 77.52 76.75 73.02 68.80SRL 75.99 77.65 76.81 73.97 69.96TREE+PLTAG 76.67 78.27 77.47 72.27 70.27TREE+PLTAG+SRL77.00 78.57 77.77 74.97 70.00Table 1: Full-sentence parsing results2, area underthe curve (AUC) for the incremental parsing re-sults of Figure 3, and combined SRL score acrossdifferent groups of features.tactic parses output by our re-ranker.
(In contrast,Konstas et al (2014) work with gold-standard syn-tactic parses.
)We evaluate four variants of our model (see Sec-tion 4 for an explanation of the different groups offeatures):TREE is the model that uses tree featuresonly; this essentially simulates standard parse re-ranking approaches such as the one of Charniakand Johnson (2005).SRL uses only features based on iSRL triples;it provides a proof-of-concept, demonstrating thatthe semantic information encoded in SRL triplescan help the parser building better syntactic trees.TREE+PLTAG adds PLTAG Features to theTREE model, taking advantage of local infor-mation specific to elementary PLTAG trees;TREE+PLTAG essentially provides a strongsyntax-only baseline.TREE+PLTAG+SRL combines SRL featuresand syntactic features.Finally, our baseline is the PLTAG parser ofDemberg et al (2013), using the original proba-bility model without any re-ranking.
A compari-son with other incremental parsers would be de-sirable, but is not trivial to achieve.
This is be-cause the PLTAG parser is trained and evaluatedon a version of the Penn Treebank that was con-verted to PLTAG format.
This renders our resultsnot directly comparable to parsers that reproducethe Penn Treebank bracketing.
For example, thePLTAG parser produces deeper tree structures in-formed by Propbank and the noun phrase annota-tion of Vadas and Curran (2007).10 20 30 400.650.70.750.80.850.9Prefix LengthF-scoreBASELINETREESRLTREE+PLTAGTREE+PLTAG+SRLFigure 3: Incremental parsing F-score for increas-ing sentence prefixes, up to 40 words.6.3 ResultsFigure 3 gives the results of evaluating incre-mental parsing performance.
The x-axis showsprefix length, and the y-axis shows incrementalF-score computed as suggested by Sangati andKeller (2013).
Each point is averaged over all pre-fixes of a given length in the test set.
To quantifythe trends shown in this figure, we also computethe area under the curve (AUC) for each featurecombination; this is given in Table 1.We find that TREE performs consistently bet-ter than the baseline for short prefixes (up to thefirst 20 words), and then is very close to the base-line.
This is expected given that tree features addstructure-specific information (e.g., about coordi-nation) to the baseline model, and is consistentwith results obtained using similar features in theliterature (Charniak and Johnson, 2005).
AddingPLTAG features (TREE+PLTAG) hurts incremen-tal performance for short prefixes (up to about 20words), but then performance gradually increasesover the baseline and over TREE alone.
It seemsthat the PLTAG features, which are specific to thegrammar formalism used, are able to help withlonger and more complex prefixes, but introducenoise in smaller prefixes.The SRL feature set, on the other hand, resultsin a consistent increase in performance compared2Note that the baseline score is lower than the publishedF = 77.41 of Demberg et al (2013).
This is expected, sincewe use a semantically-enriched lexicon, which increases thesize of the lexicon, resulting in higher ambiguity per word aswell as increased sparsity in the probability model.71197to the baseline, across all prefix lengths.
SRL pro-vides semantic knowledge, while TREE providessyntactic knowledge, but the performance of bothfeature sets is very close to each other, up to aprefix length of about 30 words, after which SRLhas a clear advantage.
SRL features seem to fil-ter out local ambiguity caused by creating pre-fix trees incrementally and result in correct parsescloser to the end of sentence, even without theuse of the syntactic information contained in theTREE+PLTAG feature set.
Recall that SRL uses in-formation provided by the semantic frame, some-thing that a syntax-only model does not have ac-cess to.
It seems that this makes it possible for SRLto (partially) compensate for mistakes made by theparser.
The AUC of SRL is higher by 0.95 and 1.7points compared to TREE and TREE+PLTAG, re-spectively.We observe an additional boost in perfor-mance when using all features together in theTREE+PLTAG+SRL configuration, which outper-forms SRL alone by 1.0 points in AUC.
Recall thatSRL features do not apply to every word; they fireonly when semantic information is introduced tothe parser via the semantically-enriched lexicon.Hence by adding tree and PLTAG features, whichnormally apply for every new word, we are able toperform effective re-ranking for all sentence pre-fixes, which explains the boost in performance.Note that for all variants of our model we observea dip in performance at around 38 words.
This isprobably due to noise, caused by the small numberof sentences of this length.
The upward trend seenaround word 40 is probably the effect of observ-ing the end of the sentence, which boosts parsingaccuracy.Turning to full sentence evaluation (Table 1),we observe a similar trend.
Both TREE andSRL beat the baseline by about 0.55 points in F-score.
Progressively adding features increases per-formance, with the greatest gain of 1.56 pointsattained by the combination of all features inTREE+PLTAG+SRL.We also report combined SRL F-score com-puted on the re-ranked syntactic trees (rightmostcolumn of Table 1).
We find that compared tothe baseline, only a small improvement of 0.55points is achieved by TREE+PLTAG+SRL, whileTREE+PLTAG improves by 0.84 points.
Thesyntax-only variant therefore outperforms the fullmodel, but only by a small margin.7 Related WorkThe most similar approach in the literature isCollins and Roark?s (2004) re-ranking model forincremental parsing.
They learn the syntactic fea-tures of Roark (2001) using the perceptron modelof Collins (2002).
Similar to us, they use the in-cremental parser to search over candidate parses.However, they limited themselves to local deriva-tion features (akin to our PLTAG features), and donot explore global syntactic feature (tree features)or SRL features.
Even though they re-rank theoutput of an incremental parser, they only evalu-ate full sentence parsing performance.
Other re-ranking approaches to syntactic parsing make useof an extensive set of global features, but apply iton the k-best list of full sentence parses (Charniakand Johnson, 2005; Collins and Koo, 2005) or thek-best list of derivations of a packed forest (Huang,2008), i.e., these approaches are not incremental.Based on the CoNLL Shared Tasks (e.g., Haji?cet al, 2009), a number of systems exist that per-form syntactic parsing and semantic role label-ing jointly.
Toutanova et al (2008), Sutton andMcCallum (2005) and Li et al (2010) combinethe scores of two separate models, i.e., a syntac-tic parser and a semantic role labeler, and re-rankthe combination using features from each domain.Titov et al (2009) and Gesmundo et al (2009),instead of combining models, create a commonsearch space for syntactic parsing and SRL, usinga shift reduce-style technique (Nivre, 2007) andlearn a latent variable model (Incremental SigmoidBelief Networks) that optimizes over both tasks atthe same time.
Volokh and Neumann (2008) use avariant of Nivre?s (2007) incremental shift-reduceparser and rely only on the current word and pre-vious content to output partial dependency trees;then they output role labels given the full parseroutput.
In contrast to all the joint approaches, weperform both parsing and semantic role labelingstrictly incrementally, without having access to thewhole sentence, outputting prefix trees and iSRLtriples for every sentence prefix.
Our approachcreates a feedback loop, i.e., we generate a prefixtree using the baseline model, give it as input toiSRL, then re-rank it using a set of syntactic andSRL features.
The resulting new prefix tree canthen be fed back into iSRL, etc.811988 ConclusionsWe started from the observation that human pars-ing uses semantic knowledge to rule out parsesthat lead to implausible interpretations.
Based onthis, we hypothesized that also in NLP, an incre-mental syntactic parser should benefit from se-mantic information.
To test this hypothesis, wecombined an incremental TAG parser with an in-cremental semantic role labeler.
We used the out-put of the iSRL system to derive features that canbe used to re-rank the prefix trees generated by theincremental parser.
We found that SRL features,both in isolation and together with standard syn-tactic features, improve parsing performance, bothwhen measured using full-sentence F-score, and interms of incremental F-score.In future work, we plan to combine our incre-mental parsing/role labeling approach with a com-positional model of semantics, which would haveto be modified to take semantic role triples as in-put (rather than words or word pairs).
The re-sulting plausibility estimates could then be usedas another source of semantic information for theparser, or employed in down-stream tasks.AcknowledgmentsEPSRC support through grant EP/I032916/1 ?Anintegrated model of syntactic and semantic pre-diction in human language processing?
to FrankKeller and Mirella Lapata is gratefully acknowl-edged.ReferencesAltmann, Gerry T. M. and Yuki Kamide.
1999.Incremental interpretation at verbs: Restrictingthe domain of subsequent reference.
Cognition73:247?264.Charniak, Eugene and Mark Johnson.
2005.Coarse-to-fine n-best parsing and maxent dis-criminative reranking.
In Proceedings of the43rd Annual Meeting of the Association forComputational Linguistics.
Ann Arbor, Michi-gan, pages 173?180.Chelba, Ciprian and Frederick Jelinek.
2000.Structured language modeling.
ComputerSpeech and Language 14:283?332.Collins, Michael.
2002.
Discriminative trainingmethods for hidden markov models: Theoryand experiments with perceptron algorithms.
InProceedings of the 2002 Conference on Empir-ical Methods in Natural Language Processing.pages 1?8.Collins, Michael and Terry Koo.
2005.
Discrim-inative reranking for natural language parsing.Computational Linguistics 31(1):25?70.Collins, Michael and Brian Roark.
2004.
Incre-mental parsing with the perceptron algorithm.In Proceedings of the 42nd Meeting of the As-sociation for Computational Linguistics, MainVolume.
Barcelona, Spain, pages 111?118.Demberg, Vera and Frank Keller.
2008.
Data fromeye-tracking corpora as evidence for theoriesof syntactic processing complexity.
Cognition101(2):193?210.Demberg, Vera, Frank Keller, and AlexanderKoller.
2013.
Incremental, predictive pars-ing with psycholinguistically motivated tree-adjoining grammar.
Computational Linguistics39(4):1025?1066.Gesmundo, Andrea, James Henderson, PaolaMerlo, and Ivan Titov.
2009.
A latent vari-able model of synchronous syntactic-semanticparsing for multiple languages.
In Proceed-ings of the Thirteenth Conference on Compu-tational Natural Language Learning: SharedTask.
pages 37?42.Haji?c, Jan, Massimiliano Ciaramita, Richard Jo-hansson, Daisuke Kawahara, Maria Ant`oniaMart?
?, Llu?
?s M`arquez, Adam Meyers, JoakimNivre, Sebastian Pad?o, Jan?St?ep?anek, PavelStra?n?ak, Mihai Surdeanu, Nianwen Xue, andYi Zhang.
2009.
The CoNLL-2009 shared task:91199Syntactic and semantic dependencies in multi-ple languages.
In Proceedings of the 13th Con-ference on Computational Natural LanguageLearning.
Boulder, Colorado, USA.Huang, Liang.
2008.
Forest reranking: Dis-criminative parsing with non-local features.
InProceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics.Columbus, Ohio, pages 586?594.Huang, Liang and Kenji Sagae.
2010.
Dynamicprogramming for linear-time incremental pars-ing.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguis-tics.
Uppsala, pages 1077?1086.Joshi, Aravind K. and Yves Schabes.
1992.
Treeadjoining grammars and lexicalized grammars.In Maurice Nivat and Andreas Podelski, editors,Tree Automata and Languages, North-Holland,Amsterdam, pages 409?432.Konstas, Ioannis, Frank Keller, Vera Demberg,and Mirella Lapata.
2014.
Incremental seman-tic role labeling with tree adjoining grammar.
InProceedings of the 2014 Conference on Empir-ical Methods in Natural Language Processing.Doha, Qatar, pages 301?312.Li, Junhui, Guodong Zhou, and Tou Hwee Ng.2010.
Joint syntactic and semantic parsing ofchinese.
In Proceedings of the 48th AnnualMeeting of the Association for ComputationalLinguistics.
Association for Computational Lin-guistics, pages 1108?1117.Marcus, Mitchell P., Mary Ann Marcinkiewicz,and Beatrice Santorini.
1993.
Building a largeannotated corpus of english: The Penn tree-bank.
Computational Linguistics 19(2):313?330.Nivre, Joakim.
2007.
Incremental non-projectivedependency parsing.
In Human Language Tech-nologies: The Conference of the North Amer-ican Chapter of the Association for Compu-tational Linguistics; Proceedings of the MainConference.
Rochester, New York, pages 396?403.Roark, Brian.
2001.
Probabilistic top-down pars-ing and language modeling.
ComputationalLinguististics 27:249?276.Sangati, Federico and Frank Keller.
2013.
In-cremental tree substitution grammar for pars-ing and word prediction.
Transactions ofthe Association for Computational Linguistics1(May):111?124.Schuler, William, Samir AbdelRahman, TimMiller, and Lane Schwartz.
2010.
Broad-coverage parsing using human-like memoryconstraints.
Computational Linguististics36(1):1?30.Schwartz, Lane, Chris Callison-Burch, WilliamSchuler, and Stephen Wu.
2011.
Incremen-tal syntactic language models for phrase-basedtranslation.
In Proceedings of the 49th An-nual Meeting of the Association for Computa-tional Linguistics: Human Language Technolo-gies, Volume 1.
Portland, OR, pages 620?631.Stoness, Scott C., Joel Tetreault, and James Allen.2004.
Incremental parsing with reference inter-action.
In Frank Keller, Stephen Clark, MatthewCrocker, and Mark Steedman, editors, Proceed-ings of the ACL Workshop Incremental Parsing:Bringing Engineering and Cognition Together.Barcelona, pages 18?25.Sutton, Charles and Andrew McCallum.
2005.Joint parsing and semantic role labeling.
In Pro-ceedings of the Ninth Conference on Computa-tional Natural Language Learning.
Ann Arbor,Michigan, pages 225?228.Tan, Ming, Wenli Zhou, Lei Zheng, and ShaojunWang.
2011.
A large scale distributed syntactic,semantic and lexical language model for ma-chine translation.
In Proceedings of the 49thAnnual Meeting of the Association for Compu-tational Linguistics: Human Language Tech-nologies, Volume 1.
Portland, OR, pages 201?210.Tanenhaus, Michael K., Michael J. Spivey-Knowlton, Kathleen M. Eberhard, and Julie C.Sedivy.
1995.
Integration of visual and linguis-tic information in spoken language comprehen-sion.
Science 268:1632?1634.Titov, Ivan, James Henderson, Paola Merlo, andGabriele Musillo.
2009.
Online graph planari-sation for synchronous parsing of semantic andsyntactic dependencies.
In Proceedings of the21st International Jont Conference on Artifi-cal Intelligence.
Morgan Kaufmann PublishersInc., San Francisco, CA, USA, pages 1562?1567.Toutanova, Kristina, Aria Haghighi, and Christo-pher D. Manning.
2008.
A global joint modelfor semantic role labeling.
Computational Lin-guistics 34(2):161?191.101200Vadas, David and James Curran.
2007.
Addingnoun phrase structure to the penn treebank.In Proceedings of the 45th Annual Meetingof the Association of Computational Linguis-tics.
Association for Computational Linguistics,Prague, Czech Republic, pages 240?247.Volokh, Alexander and G?unter Neumann.
2008.Proceedings of the Twelfth Conference on Com-putational Natural Language Learning, Coling2008 Organizing Committee, chapter A Puris-tic Approach for Joint Dependency Parsing andSemantic Role Labeling, pages 213?217.Xu, Peng, Ciprian Chelba, and Frederick Jelinek.2002.
A study on richer syntactic dependen-cies for structured language modeling.
In Pro-ceedings of the 40th Annual Meeting on Associ-ation for Computational Linguistics.
Philadel-phia, pages 191?198.111201
