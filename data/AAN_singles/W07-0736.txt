Proceedings of the Second Workshop on Statistical Machine Translation, pages 240?247,Prague, June 2007. c?2007 Association for Computational LinguisticsSentence Level Machine Translation Evaluation as a Ranking Problem: onestep aside from BLEUYang YeUniversity of Michiganyye@umich.eduMing ZhouMicrosoft Research Asiamingzhou@microsoft.comChin-Yew LinMicrosoft Research Asiacyl@microsoft.comAbstractThe paper proposes formulating MT evalu-ation as a ranking problem, as is often donein the practice of assessment by human.
Un-der the ranking scenario, the study also in-vestigates the relative utility of several fea-tures.
The results show greater correlationwith human assessment at the sentence level,even when using an n-gram match score asa baseline feature.
The feature contributingthe most to the rank order correlation be-tween automatic ranking and human assess-ment was the dependency structure relationrather than BLEU score and reference lan-guage model feature.1 IntroductionIn recent decades, alongside the growing researchon Machine Translation (MT), automatic MT evalu-ation has become a critical problem for MT systemdevelopers, who are interested in quick turnarounddevelopment cycles.
The state-of-the-art automaticMT evaluation is an n-gram based metric repre-sented by BLEU (Papineni et al, 2001) and its vari-ants.
Ever since its creation, the BLEU score hasbeen the gauge of Machine Translation system eval-uation.
Nevertheless, the research community hasbeen largely aware of the deficiency of the BLEUmetric.
BLEU captures only a single dimensionof the vitality of natural languages: a candidatetranslation gets acknowledged only if it uses ex-actly the same lexicon as the reference translation.Natural languages, however, are characterized bytheir extremely rich mechanisms for reproductionvia a large number of syntactic, lexical and semanticrewriting rules.
Although BLEU has been shownto correlate positively with human assessments atthe document level (Papineni et al, 2001), efforts toimprove state-of-the-art MT require that human as-sessment be approximated at sentence level as well.Researchers report the BLEU score at documentlevel in order to combat the sparseness of n-gramsin BLEU scoring.
But, ultimately, document-levelMT evaluation has to be pinned down to the gran-ularity of the sentence.
Unfortunately, the corre-lation between human assessment and BLEU scoreat sentence level is extremely low (Liu et al, 2005,2006).
While acknowledging the appealing simplic-ity of BLEU as a way to access one perspective of anMT candidate translation?s quality, we observe thefollowing facts of n-gram based MT metrics.
First,they may not reflect the mechanism of how humanbeings evaluate sentence translation quality.
Morespecifically, optimizing BLEU does not guaranteethe optimization of sentence quality approved by hu-man assessors.
Therefore, BLEU is likely to havea low correlation with human assessment at sen-tence level for most candidate translations.
Second,it is conceivable that human beings are more reli-able ranking the quality of multiple candidate trans-lations than assigning a numeric value to index thequality of the candidate translation even with signif-icant deliberation.
Consequently, a more intuitiveapproach for automatic MT evaluation is to repli-cate the quality ranking ability of human assessors.Thirdly, the BLEU score is elusive and hard to in-terpret; for example, what can be concluded for a240candidate translation?s quality if the BLEU score is0.0168, particularly when we are aware that evena human translation can receive an embarrassinglylow BLEU score?
In light of the discussion above,we propose an alternative scenario for MT evalua-tion, where, instead of assigning a numeric score toa candidate translation under evaluation, we predictits rank with regard to its peer candidate translations.This formulation of the MT evaluation task fills thegap between an automatic scoring function and hu-man MT evaluation practice.
The results from thecurrent study will not only interest MT system eval-uation moderators but will also inform the researchcommunity about which features are useful in im-proving the correlation between human rankings andautomatic rankings.2 Problem Formulation2.1 Data and Human Annotation ReliabilityWe use two data sets for the experiments:the test data set from the LDC MTC corpus(LDC2003T171) and the data set from the MT eval-uation workshop at ACL052.
Both data sets are forChinese-English language pairs and each has fourreference translations and seven MT system transla-tions as well as human assessments for fluency andadequacy on a scale of 1 to 5, with 5 indicating thebest quality.
For the LDC2003T17 data, human as-sessments exist for only three MT systems; for theACL05 workshop data, there are human assessmentsfor all seven MT systems.
Table 1 summarizes theinformation from these two data sets.The Kappa scores (Cohen, 1960) for the humanassessment scores are negative, both for fluency andadequacy, indicating that human beings are not con-sistent when assigning quality scores to the candi-date translations.
We have much sympathy with aconcern expressed in (Turian, 2003) that ?AutomaticMT evaluation cannot be faulted for poor correlationwith the human judges, when the judges do not cor-relate well each other.
?To determine whether humanassessor might be more consistent when rankingpairs of sentences, we examined the ?ranking con-sistency score?of the human assessment data for theLDC2003T17 data.
For this consistency score, we1http://www.ldc.upenn.edu/Catalog/2http://www.isi.edu/?
cyl/MTSE2005/are only concerned with whether multiple judges areconsistent in terms of which sentence of the two sen-tences is better: we are not concerned with the quan-titative difference between judges.
Since some sen-tences are judged by three judges while others arejudged by only two judges, we calculated the consis-tency scores under both circumstances, referred to as?Consistent 2?and ?Consistent 3?in the following ta-ble.
For ?Consistent 2?, for every pair of sentences,where sentence 1 is scored higher (or lower or equal)than sentence 2 by both judges, then the two judgesare deemed consistent.
For ?Consistent 3?, the pro-portion of sentences that achieved the above consis-tency from triple judges is reported.
Additionally,we also considered a consistency rate that excludespairs for which only one judge says sentence 1 is bet-ter and the other judge(s) say(s) sentence 2 is better.We call these ?Consistent 2 with tie?and ?Consistent3 with tie?.
From the rank consistency scores in Ta-ble 2, we observe that two annotators are more con-sistent with the relative rankings for sentence pairsthan with the absolute quality scores.
This findingfurther supports the task of ranking MT candidatesentences as more reliable than the one of classify-ing the quality labels.2.2 Ranking Over Classification andRegressionAs discussed in the previous section, it is difficult forhuman assessors to perform MT candidate transla-tion evaluation with fine granularity (e.g., using real-valued numeric score).
But humans?
assessmentsare relatively reliable for judgments of quality rank-ing using a coarser ordinal scale, as we have seenabove.
Several approaches for automatically assign-ing quality scores to candidate sentences are avail-able, including classification, regression or ranking,of which ranking is deemed to be a more appropri-ate approach.
Nominalize the quality scores and for-mulating the task as a classification problem wouldresult in a loss of the ordinal information encodedin the different scores.
Additionally, the low Kappascores in the human annotation reliability analysisreported above also confirms our previous specula-tion that a classification approach is less appropriate.Regression would be more reasonable than classifi-cation because it preserves the ordinal informationin the quality labels, but it also inappropriately im-241Data Index MT Systems References Documents SentencesLDC2003T17 7 4 100 878ACL05 Workshop 7 4 100 919Table 1: Data Sets InformationInter-Judge Score Consistent2Consistent3Consistent2 with TieConsistent3 with TieRanking Consistency Score 45.3% 23.4% 92.6% 87.0%Table 2: Ranking Consisteny Scores for LDC2003T17 Dataposes interval scaling onto the quality labels.
Incontrast, ranking considers only the relative rank-ing information from human labels and does not im-pose any extra information onto the quality labelsassigned by human beings.The specific research question addressed in thispaper is three-fold: First, in addition to investigatingthe correlation between automatic numeric scoringand human assessments, is ranking of peer candidatetranslations an alternative way of examining correla-tion that better suits the state of affairs of human an-notation?
Second, if the answer to the above ques-tion is yes, can better correlation be achieved withhuman assessment under the new task scenario?
Fi-nally, in addition to n-gram matching, which otherknowledge sources can combat and even improvethe rank order correlation?
The process of rank-ing is a crucial technique in Information Retrieval(IR) where search engines rank web pages depend-ing on their relevance to a query.
In this work, sen-tence level MT evaluation is considered as a rankingproblem.
For all candidate translations of the samesource Chinese sentence, we predict their transla-tion quality ranks.
We evaluate the ranker by Spear-man?s rank order correlation coefficient between hu-man ranks and predicted ranks described by the fol-lowing formula (Siegel,1956):r = 1?
( 6?D2N(N2 ?
1)) (1)where D is the difference between each pair of ranksand N is the number of candidates for ranking.3 Related WorksPapineni et al(2001) pioneered the automatic MTevaluation study, which scores translation quality vian-gram matching between the candidate and refer-ence translations.
Following the growing awarenessof the deficiency of n-gram based automatic MTevaluation, many studies attempted to improve uponn-gram based metrics (Zhou et al, 2006; Liu, etal., 2005,2006) as well as propose ways to evaluateMT evaluation metrics (Lin, et al 2004).
Previousstudies, however, have focused on MT evaluation atthe document level in order to fight n-gram sparse-ness problem.
While document level correlationprovides us with a general impression of the qual-ity of an MT system, researchers desire to get moreinformative diagnostic evaluation at sentence levelto improve the MT system instead of just an over-all score that does not provide details.
Recent yearshave seen several studies investigating MT evalu-ation at the sentence level (Liu et al, 2005,2006;Quirk, 2004).
The state-of-the-art sentence levelcorrelations reported in previous work between hu-man assessments and automatic scoring are around0.20.
Kulesza et al(2004) applied Support Vec-tor Machine classification learning to sentence levelMT evaluation and reported improved correlationwith human judgment over BLEU.
However, theclassification taxonomy in their work is binary, be-ing either machine translation or human translation.Additionally, as discussed above, the inconsistencyfrom the human annotators weakens the legitimacyof the classification approach.
Gamon et al(2005)reported a study of English to French sentence-levelMT evaluation without reference translations.
In or-der to improve on the correlation between human as-sessments and the perplexity score alone, they com-bined a perplexity score with a classification scoreobtained from an SVM binary classifier distinguish-ing machine-translated sentences from human trans-242lations.
The results showed that even the combi-nation of the above two scores cannot outperformBLEU.To sum up, very little consideration has beentaken in previous research as to which learning ap-proach is better motivated and justified by the stateof affairs of human annotation reliability.
Presum-ably, research that endeavors to emulate human per-formance on tasks that demontrate good inter-judgereliability is most useful.a learning approach that is better supported byhuman annotation reliability can alleviate the noisefrom human assessments and therefore achieve morereliable correlations.4 Experiments and Evaluation4.1 Ranking SVM Learning AlgorithmRanking peer candidate sentence translations is atask in which the translation instances are classi-fied into a number of ranks.
This is a canonical or-dinal regression scenario, which differs from stan-dard classification and metric regression.
For imple-mentation, we use the Ranking SVM of SVMlight(Joachims, 2004), which was originally developedto rank the web pages returned upon a certain queryin search engines.
Given an instance of a candidatetranslation, Ranking SVM assigns it a score basedon:U(x) = W Tx (2)where W represents a vector of weights (Xu et al,2005).
The higher the value of U(x), the better x is asa candidate translation.
In an ordinal regression, thevalues of U(x) are mapped into intervals correspond-ing to the ordinal categories.
An instance fallinginto one interval is classified into the correspondingtranslation quality.
In ranking experiments, we usethe Ranking SVM scores to rank the candidate sen-tences under evaluation.4.2 FeaturesWe experiment with three different knowledgesources in our ranking experiments:1.
N-gram matching between the candidate trans-lation and the reference translation, for whichwe use BLEU scores calculated by the NISTscript with smoothing3 to avoid undefined logprobabilities for zero n-gram probabilities.2.
Dependency relation matching between thecandidate translation and the reference transla-tion.3.
The log of the perplexity score of the candidatetranslation, where the perplexity score is ob-tained from a local language model trained onall sentences in the four reference translationsusing CMU SLM toolkit.
The n-gram order isthe default trigram.4.2.1 N-gram matching featureN-gram matching is certainly an important cri-terion in some cases for evaluating the translationquality of a candidate translation.
We use the BLEUscore calculated by the BLEU score script fromNIST for this feature.As has been observed by many researchers,BLEU fails to capture any non n-gram based match-ing between the reference and candidate transla-tions.
We carried out a pair-wise experiment onfour reference translations from the LDC2003T17test data, where we took one reference sentence asthe reference and the other three references as can-didate translations.
Presumably, since the candidatesentences are near-optimal translations, the BLEUscores obtained in such a way should be close to1.
But our analysis shows a mean BLEU of only0.1456398, with a standard deviation of 0.1522381,which means that BLEU is not very predictive ofsentence level evaluation.
The BLEU score is, how-ever, still informative in judging the average MTsystem?s translation.4.2.2 Dependency Structure MatchingDependency relation information has been widelyused in Machine Translation in recent years.
Fox(2002) reported that dependency trees correspondbetter across translation pairs than constituent trees.The information summarization community has alsoseen successful implementation of ideas similar tothe depedency structure.
Zhou et al(2005) and Hovyet al(2005) reported using Basic Elements (BE) intext summarization and its evaluation.
In the current3We added an extremely small number to both matched n-grams and total number of n-grams.243paper, we match a candidate translation with a ref-erence translation on the following five dependencystructure (DS) types:?
Agent - Verb?
Verb - Patient?
Modified Noun - Modifier?
Modified Verb - Modifier?
Preposition - ObjectBesides the consideration of the presence of cer-tain lexical items, DS captures information as tohow the lexical items are assembled into a good sen-tence.
By using their dependency relation match forranking the quality of peer translations, we assumethat the dependency structure in the source languageshould be well preserved in the target language andthat multiple translations of the same source sen-tence should significantly share dependency struc-tures.
Liu et al(2005) make use of dependencystructure in sentence level machine translation eval-uation in the form of headword chains, which arelexicalized dependency relations.
We propose thatunlexicalized dependency relations can also be in-formative.
Previous research has shown that key de-pendency relations tend to have a strong correspon-dence between Chinese and English (Zhou et al,2001).
More than 80 % of subject-verb, adjective-noun and adverb-verb dependency relations wereable to be mapped, although verb-object DS map-ping is weaker at a rate of 64.8%.
In our paper, weconsidered three levels of matching for dependencyrelation triplets, where a triplet consists of the DStype and the two lexical items as the arguments.We used an in-house dependency parser to extractthe dependency relations from the sentences.
Figure1 illustrates how dependency relation matching cango beyond n-gram matching.
We calculated 15 DSscores for each sentence correponding to the countsof match for the 5 DS types at the 3 different levels.4.2.3 Reference language model (RLM) featureStatistical Language Modeling (SLM) is a keycomponent in Statistical Machine Translation.
Themost dominant technology in SLM is n-gram mod-els, which are typically trained on a large corpusfor applications such as SMT and speech recogni-tion.
Depending on the size of the corpora usedto train the language model, a language model canFigure 1: Dependency Relation Matching SchemeFigure 2: An Example - A Sentence Gets Credits forDependency Relation Matching244be tuned to reflect n-gram probabilities for both anarrowed scope as well as a general scope coveringthe distribution of n-gram probabilities of the wholelanguage.
In the BLEU calculation, the candidatesentence is evaluated against an extremely local lan-guage model of merely the reference sentence.
Wespeculate that a language model that stands in be-tween such an immediate local language model andthe large general English language model could helpcapture the variation of lexical and even structuralselections in the translations by using informationbeyond the scope of the local sentence.
Addition-ally, this language model could represent the styleof a certain group of translators in a certain domainon the genre of news articles.
To pursue such a lan-guage model, we explore a language model that istrained on all sentences in the four references.
Weobtain the perplexity score of each candidate sen-tence based on the reference language model.
Theperplexity score obtained this way reflects the de-gree to which a candidate translation can be gen-erated from the n-gram probability distribution ofthe whole collection of sentences in the four refer-ences.
It adds new information to BLEU because itnot only compares the candidate sentence to its cor-responding reference sentence but also reaches outto other sentences in the current document and otherdocuments on the same topics.
We choose perplex-ity over the language model score because the per-plexity score is normalized with regard to the lengthof the sentence; that is, it does not favor sentences ofrelatively shorter length.In our ranking experiments, for training, both theseven MT translations and the four reference trans-lations of the same source sentence are evaluatedas ?candidate?
translations, and then each of theseeleven sentences is evaluated against the four ref-erence sentences in turn.
The BLEU score of eachof these sentences is calculated with multiple refer-ences.
Each dependency score is the average scoreof the four references.
For the reference languagemodel feature, the perplexity score is used for eachsentence.Conceptually, the reference language model anddependency structure features are more relevant tothe fluency of the sentence than to the adequacy.Because the candidate sentences?
adequacy scoresare based on arbitrary reference sentences out of theFeature Set Mean Corr Corr VarBLEU 0.3590644 0.0076498DS 0.4002753 0.0061299PERP 0.4273000 0.0014043BLEU+DS 0.4128991 0.0027576BLEU+PERP 0.4288112 0.0013783PERP+DS 0.4313611 0.0014594All 0.4310457 0.0014494Table 3: Training and Testing on Within-year Data(Test on 7 MT and 4 Human)four references in the human assessment data, wedecided to focus on fluency ranking for this paper.The ranking scenario and features can easily be gen-eralized to adequacy evaluation: the full and partialmatch dependency structure features are relevant toadeqaucy too.
The high correlation between ade-quacy and fluency scores from human assessments(both pearson and spearman correlations are 0.67)also indicates that the same features will achieve im-provements for adequacy evaluation.4.3 Sentence Ranking on Within-year DataIn the first experiment, we performed the rankingexperiment on the ACL05 workshop data and test onthe same data set.
We did three-fold cross-validationon two different test scenarios.
On the first sce-nario, we tested the ranking models on the seven MTsystem output sentences and the four human refer-ence sentences.
It is widely agreed upon among re-searchers that a good evalutation metric should rankreference translation as higher than machine trans-lation (Lin et al, 2004).
We include the four hu-man reference sentences into the ranking to test theranker?s ability to discriminate optimal translationsfrom poor ones.
For the second scenario, we testthe ranking models on only the seven MT systemoutput sentences.
Because the quality differencesacross the seven system translations are more subtle,we are particularly interested in the ranking qualityon those sentences.
Tables 3 and 4 summarize theresults from both scenarios.The experimental results in the above tables con-veyed several important messages: in the rankingsetup, for both the MT and human mixed output andMT only output scenarios, we have a significantly245Feature Set Mean Corr Corr VarBLEU 0.2913541 0.0324386DS 0.3058766 0.0226442PERP 0.2921684 0.0210605BLEU+DS 0.315106 0.0206144BLEU+PERP 0.2954833 0.0211094PERP+DS 0.3067157 0.0217037All 0.305248 0.0218777Table 4: Training and Testing on Within-year Data(Test on MT only)improved correlation between human scoring andautomatic ranking at sentence level compared to thestate-of-the-art sentence level correlation for fluencyscore of approximately 0.202 found previously (Liuet al, 2006).
When the ranking task is performed ona mixture of MT sentences and human translations,dependency structure and reference language modelperplexity scores sequentially improve on BLEU inincreasing the correlation.
When the ranking taskis performed only on MT system output sentences,dependency structure still significantly outperformsBLEU in increasing the correlation, and the refer-ence language model, even trained on a small num-ber of sentences, demonstrates utility equal to thatof BLEU.
The dependency structure feature provesto have robust utility in informing fluency qualityin both scenarios, even with noise from the depen-dency parser, likely because a dependency tripletwith inaccurate arguments is still rewarded as a typematch or partial match.
Additionally, the feature isreward-based and not penalty-based.
We only re-ward matches and do not penalize mismatches, suchthat the impact of the noise from the MT system andthe dependency parser is weakened.4.4 Sentence Ranking on Across-year DataIt is trivial to retrain the ranking model and test ona new year?s data.
But we speculate that a modeltrained from a different data set can have almost thesame ranking power as a model trained on the samedata set.
Therefore, we conducted an experimentwhere we trained the ranking model on the ACL2005 workshop data and test on the LDC2003T17data.
We do not need to retrain the ranking SVMmodel; we only need to retrain the reference lan-Feature Set Mean Corr Corr VarBLEU 0.3133257 0.1957059DS 0.4896355 0.0727430PERP 0.4582005 0.0542485BLEU+DS 0.4907745 0.0678395BLEU+PERP 0.4577449 0.0563994PERP+DS 0.4709567 0.0549708All 0.4707289 0.0565538Table 5: Training and Testing on Across-year Data(test on 3 MT plus 1 human)guage model on the multiple references from thenew year?s data to obtain the perplexity scores.Because LDC2003T17 has human assessments foronly three MT systems, we test on the three systemoutputs plus a human translation chosen randomlyfrom the four reference translations.
The results inTable 5 show an encouraging rank order correlationwith human assessments.
Similar to training andtesting on within-year data, both dependency struc-ture and perplexity scores achieve higher correlationthan the BLEU score.
Combining BLEU and depen-dency structure achieves the best correlation.4.5 Document Level Ranking TestingPreviously, most researchers working on MT evalu-ation studied the correlation between automatic met-ric and human assessment on the granularity of thedocument to mitigate n-gram sparseness.
Presum-ably, good correlation at sentence level should leadto good correlation at document level but not viceversa.
Table 6 reports the correlations using themodel trained on the 2005 workshop data and testedon the 100 documents of the LDC 2003 data.
Com-paring these correlations with the correlations re-ported in the previous section, we see that using thesame model, the document level rank order corre-lation is substantially higher than the sentence levelcorrelation, with the dependency structure showingthe highest utility.5 Conclusion and Future WorkThe current study proposes to formulate MT evalu-ation as a ranking problem.
We believe that a reli-able ranker can inform the improvement of BLEUfor a better automatic scoring function.
Ranking in-246Feature Set Mean Corr Corr VarBLEU 0.543 0.0853DS 0.685 0.0723PERP 0.575 0.0778BLEU+DS 0.639 0.0773BLEU+PERP 0.567 0.0785PERP+DS 0.597 0.0861All 0.599 0.0849Table 6: Document Level Ranking Testing Resultsformation could also be integrated into tuning pro-cess to better inform the optimization of weights ofthe different factors for SMT models.
Our rankingexperiments show a better correlation with humanassessments at sentence level for fluency score com-pared to the previous non-ranking scenario, evenwith BLEU as the baseline feature.
On top of BLEU,both the dependency structure and reference lan-guage model have shown encouraging utility for dif-ferent testing scenarios.
Looking toward the fu-ture work, more features could be explored, e.g., aparsing-based score of each candidate sentence andbetter engineering for dependency triplet extraction.Additionally, the entire research community on MTevaluation would benefit from a systematic and de-tailed analysis of real data that can provide a quanti-tative breakdown of the proportions of different ?op-erations?
needed to rewrite one sentence to another.Such an effort will guide MT evaluation researchersto decide which features to focus on.ReferencesJ.
Cohen, A Coefficient of Agreement for NominalScales, Educational and Psychological Measurement,20, 37-46, 1960.G.
Doddington.
Automatic Evaluation of Machine Trans-lation Quality Using N-gram Co-occurrence Statistics.HLT, pages 128?132, 2002.H.
J.
Fox, Phrasal Cohesion and Statistical MachineTranslation.
EMNLP, 2002.M.
Gamon, et al, Sentence-level MT Evaluation withoutReference Translations: Beyond Language Modeling,Proceedings of EAMT, 2005.T.
Joachims, Making Large-scale Support Vector Ma-chine Learning Practical, in B. Scholkopf, C. Burges,A.
Smola.
Advances in Kernel Methods: Support Vec-tor Machines, MIT Press, Cambridge, MA, December,1998.A.
Kulesza and S. M. Shieber, A Learning Approach toImproving Sentence-Level MT Evaluation, 10th Inter-national Conference on Theoretical and Methodologi-cal Issues in Machine Translation, 2004.C.
Lin, et al, ORANGE: a Method for Evaluating Au-tomatic Evaluation Metrics for Machine Translation.COLING, 2004.C.
Lin, et al, Automatic Evaluation of Machine Trans-lation Quality Using Longest Common Subsequenceand Skip-Bigram Statistics, ACL, 2004.D.
Liu, et al, Syntactic Features for Evaluation of Ma-chine Translation, ACLWorkshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Translationand/or Summarization, 2005.D.
Liu, et al, Stochastic Iterative Alignment for Ma-chine Translation Evaluation, COLING/ACL PosterSession, Sydney, 2006.C.
B. Quirk, Training a Sentence-Level Machine Trans-lation Confidence Measure, In Proceedings of LREC,2004.E.
Hovy, et al, Evaluating DUC 2005 using Basic El-ements.
Document Understanding Conference (DUC-2005), 2005.K.
Papineni, et al, BLEU: a Method for Automatic Eval-uation of Machine Translation, IBM research divisiontechnical report, RC22176 (W0109-022), 2001.S.
Siegel and N.J. Catellan, Non-parametric Statistics forthe Behavioral Sciences, McGraw-Hill, 2nd edition,1988.M.
Snover, et al, A Study of Translation Error Rate withTargeted Human Annotation, LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-58, University of Maryland,2005.J.
Turian, et al, Evaluation of Machine Translation andits Evaluation, MT Summit IX, 2003.J.
Xu, et al, Ranking Definitions with Supervised Learn-ing Method, WWW?05 industry track, 811-819, 2005.L.
Zhou, et al, A BE-based Multi-document Summarizerwith Query Interpretation.
Document UnderstandingConference (DUC-2005), 2005.L.
Zhou, C. Lin, E-evaluating Machine Translation Re-sults with Paraphrase Support, EMNLP, 2006.M.
Zhou, C. Huang, Approach to the Chinese depen-dency formalism for the tagging of corpus.
Journal ofChinese Information Processing, 8(3): 35-52, 1994.247
