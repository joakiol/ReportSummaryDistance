REMARKS ON PROCi.
;S,'glNG, CONSTRAINTS, ANDTill.
: LEXICON*Thomas WasowStanford UniversityLinguists have long recognized the desirability of embedding atheory of grammar within a theory of linguistic performance(scc, e.g., Chomsky (1965;10-15)).
It has bccn widely assumedby transformationalists that an adequate model of a languageuser would inchlde as one component some sort of generativegranlm:.~r.
Yet transformational grammarians have devotedrelatively little energy to the problem that Bresnan (in press)calls "the grammatical realization problem": "How would areasonable model of language use incorporate atransformational grammar?"
When this question has beenraised, little support could be adduced for the hypothesis thatthe operations of transformational grammar play a part inspeakers' or hearers' processing of sentences (see Fodor, et al(1974; chapter 5)).
Instead of concerning themselves withquestions of processing, transformationalists have concentratedtheir efforts (at least in the last decade or so) on the problemof constraining the power of their theory.
The goal of muchrecent research has been to construct as restrictive a theory ofgrammar as possible, within the bounds set by the knowndivcrsity of human languages (scc, e.g., Ross (1967), Chomsky(1973), Bresnan (1976), Emonds (1976), and Culicover andWexler (1977) for examples of this type of research).Computational linguists, on the other hand, have not explicitlyconcerned themselves very much with the problem ofconstraints {,but see Woods (1973; 124-5) for an exception).Rather, their go~,l has been to find effective procedures for theparsing and processing of natural language.
While this isimplicitly a restriction to recursive languages, thecomputational literature has dealt more with questions ofprocessing than with how to limit the class of availablegrammars or languages.In previous papers (Osherson and Wasow (1976), Wasow (inpress a, 1978)) I have argued for the legitimacy of the questfor constraints as a research strategy.
1 have argued that atheory that places limits on the class of possible languagesutakes significant empirical claims about human mentalcapacities, and can contribute to a solution to "theft, ndamental empirical problem of linguistics" (as Chomskyhas called it) of how childre,t are able to lea,n languages withsuch facility.
I have tried to show that such psychologicalclaims can be made, ~ithout making any assumptions aboutwhat role grammars play in.performance, ht short, I haveargued that a theory of .grammar can make significantcoutributio,as to psychology, indcpcnde,tt of the answer to thegramlnatical realization problem.Recent work by Joan Bresnan (iq press) takes a very differentposition: she has suggested that transforntationalists ought topay more attention to the grammatical realization problem,and that considerations of processing suggest radicalmodifications in the theory of transformational grammar.Further, she argues that there is ample grammatical evidencefor these modifications.
In this paper 1 will suggest someextensions of her proposals, aud will explore some of theirempirical consequences.
Further, I will argue that hertrame~ork makes it possible to impose rather restrictiveconstraints on grammatical theory.
Thus.
I will argue that thegrammatical realization problem and the problem ofconstraining transformational theory, while logicallyindependent, are both addressed by Bresnan's proposals.
If Iam correct in this, then Bresnan's "realistic transformationalgrammar" represents a major convergence of the concerns oftransformational and co,nputational linguists.My presentation will consist of three parts.
First, I willbriefly sketch Bresnan's framework.
Second, I will suggestsome extensions of her proposals and point out someconseqt, ences of these extensions.
Third, I will propose howher framework can be constrained, and indicate certaindesirable consequences of my proposals.The primary innovation of Bresnan's framework is that iteliminates a large class of transformations in favor of anenriched conception of the lexicon.
The grammar that resultsis one that Bresnan claims is far more realistic from aprocessing point of view than other versions oftransformational grammar.
She points out striking similaritiesbetween her proposals and recent compt, tational andpsycholinguistic work by Kaplan and Wanner, and she arguesthat Augmented Transition Networks can provide at least apartial answer to the grammatical realization problem withinher framework.I will now sketch very roughly what Bresnan's "realistic"transformational grammar is like.
Rules like passive, dative,and raising rules, which are "structt, re-preserving" (in thesense that their outputs are structurally identical toindependently required base-generated structures) and "local"(in the sense that the elements affected are always in theimmediate environment of some governing lexical item,ust,ally a verb), are eliminated from the transformationalcomponent and relegated to tile lexicon.
Lcxical entriesinclude, among other things, (strict) subcategorization framesand more abstract rcprcscntatitms ~llich l.\]rcsuan calls"functional structt, rcs" or "predicate argument structures".Subcatcgorization frames give the syntactic enviro,unents inwhich the Icxical item may appear; these are expressed interms of a basic set of grammatical relations, including"subject" and "object".
These notions, while universal, areinstantiatcd ifferently in different hmgnagcs; for example,Bresnan takes essentially the structural definitions of "subject"and "object" proposed by Chomsky (1965; 71) as language-specific characterizations of these notions for English.Functional structt,res give a more abstract representation ofthe elements mentioned iu the subeategorization frame,indicating what their "logical" relationships are.
Thus, the247functional structure corresponds very roughly to the deepstructure in the standard theory of transformational grammar;,and the subcategorization frame corresponds even moreroughly to the surface structure.What the standard theory did with local structure-preservingtransformations Bresnan can do in either of two ways.Relationships like active/passive are handled by positing twoseparate lexical entries for active and passive verb forms.
Theproductivity of this relationship can be accounted for bymeans of a lexical redundancy rule, which would say, in effect,that corresponding to the typical transitive verb there is anintransitive verb which looks morphologically like the perfectform of the transitive, and whose subject plays the samelogical role (i.e., in the functional structure) as the object ofthe transitive verb.
Bresnan's other way of replacing localstructure-preserving rules is ilh,strated most clearly with theraising rules.
Raising to object position, for example, is usedto capture the fact that the NP which is syntactically theobject of one clause is logically not an argument of that clauseat all, but a subject of the subordinate clause.
Bresnanexpresses this simply in terms of the relationship between thesubcategorization frame and the functional structure; that is,the object of tile main clause plays no role in the functionalstructure of that clause, but is "passed down" to play a role inthe next clause down.
in the interests of brevity i will notillustrate Bresnan's framework here.
Rather, 1 will refer theinterested reader to her paper, and go on to indicate myreasons for seeking to modify her proposals.My primary motiw~tion comes from some earlier work ofmine (Wasow (1977)), which argued against he elimination oflocal, structure-preserving transformations.
My argument wasbased on the observation that there are two similar but distinctclasses of linguistic relationships whose differences can beexpressed rather naturally as the differences betweentransformational rules and lexical redundancy rules.
Theclearest example of this is the English passive.
It has oftenbeen suggested that some passive parliciples are a:ljectives andothers verbs; I pointed out that adjectival passives and verbalpassives differed in certain systematic_ ways.
My central claim_was that the surface subject of adjectival passives was alwaysthe deep direct object of the correspording verb.
For example,a passive participle which is demonstrably adjectival (e.g.,because it is prefixed with un- or imraefiately follows seem)may not have as its surface subject the "logical" subject of alower clause, the indirect object, or a chunk of an idiom:"John is unknown to be a communist; "John seemed told thestory, "Advantage seemed taken o f  JohrL A verbal passive, incontrast, could have as its subject any N P which couldimmediately follow the corresponding active verb: John isknown to be a communist; John was told the story;, Advantagewas taken o f  John.
This, !
claimed, would follow from thehypothesis that adjectival igassives are formed by a lexicalredundancy rule, whereas verbal passives aretransformationaily derived, if lexical redundancy rules are"relational", in the sense that they are formulated in terms ofgrammatical relations such as subject and object, whereastransformations are "structural", i.e., they are operations onphrase structure tree.It is evident that my earlier position is inconsistent with\[~resnan's recent proposals.
My extensions of her ideas,developed in collaboration with Ron Kaplan, are in part anattempt o capture within her framework the distinction myearlier paper sought to explicate in terms of thelexicon/transformation contrast.
They are also motivated bythe very interesting comments of Anderson (1977).
Ande-sonsuggests that I was mistaken in claiming that the operativefactor in formulating rules like the adjectival passive rule wasthe deep grammatical relation of the surface subject.
Rather,he argues, it is thematic relations like "theme", "agent", "goal",and "source" (see Gruber (1965) and Jackendoff (1972)) whichare crucial 1.
Assuming Anderson to be correct, an obviousmodification of Bresnan's ystem suggests itself, which wouldpermit the distinctions of my earlier paper to be captured.
Letus suppose that the functional structure in lexical entries is aspecification of which thematic relations hould be assigned tothe elements mentioned in the subcategorization frame.
Thenwe may distinguish two types of lexical rules: those that makereference to thematic relations and those that do not.
Theformer would correspond to rules that my earlier paper calledlexical, and the latter to those that 1 called transformations.This is the extension of Bresnan's framework that I wish topropose.
I will illustrate by formulating the two pa:;sive rulesand the dative rnle and applying them to a fragment of thelexicon of English.My formalism is based on .the assumption that thegrammatical relations are given language-wide definitions inslructural terms (at least in English) along the lines indicatedby Bresn~.n, and that a verb's subcategorization frame merelyindicates which relations it has, and what grammaticalcategories those relations are assigned to.
(Thus, !
differ fromBresnan in this respect, for she assumed that grammaticalrelations would be limited to NP's).
I will adopt the followingabbreviations: "SS" = (surface) subject; SO = (surface) object;"SO2" -- (surface) second object; ' T '  = theme; "2" = agent; "3"= goal; "4" = complement.
The rule forming ,erbal passiveparticiples from the corresponding active lexical entries cannow be formulated 2 quite simply as SS,-SO.
This is to beinterpreted as follows: eliminate "SS" wherever it appears inthe entry for the active verb (eliminating also any assignmentit may have to a thematic relation) and change all occurrencesof "SO" to "SS "3.
The adjectival passive rule will differfrom this in that it has an additional condition on it: if SO=1,then SS~-SO.
This condition insures that the SO is "local", inthe sense that it bears a thematic relation to the verb.
Thedative rule 4 also has a "localness" condition: if SO2=1, thenSO~-SO2.
Let me illustrate these rules with a simple example,namely the verb sell.
The basic lexical entry I posit for thisverb includes the following information: SS=NP, SO=NP,SO2=NP; SS=2, SO=3, SO2=1.
This, !
claim, is among theinformation that must be included in a representation of sellin such uses as They sold John two cars.
Apphing the verbalpassive rule to this entry, we get the following: SS=NP,SO2=NP; SS=3, SO2=1.
This verb appears in examples likeJohn was sold two cers.
Since the original entry for sell didnot meet the condition SO=l, the adjectival passive rule is notapplicable; correspondingly, forms like "John was unsold twocars are impossible.
The condition for application of dative,SO2=1 is met, so we can derive an entry in which SS=NP,SO=NP; SS=2, SO=I.
This corresponds to examples like Theysold two cars.
Notice that this last entry does satisfy thecondition on the adjectiva: passive rule, so we can derive thefollowing entry for an adjectival passive participle for sell:SS=NP; SS=I.
This corresponds to examples like Two carswere unsold.Let us now turn to some more complex examples.
Specifically,!
now want to look at several different verbs which share thesame strict subcategorization frame, namely, SS=NP, SO=NP,SO2=VP.
The verbs in question differ front one another a longtwo dimensions, namely, the assigr, ment of thematic relations,and control properties.
What i mean by this latter phrase isquite simple: the understood subject of the VP ill the SO2positicm will be the SS in some cases and tile SO in others.
Iwill regresent his in the functional structure by assigning athematic relation apt simply to S02.
but to S02(SS) orS02(S0).
depending on the control properties 5.
Myassig,ments of thematic relations are intended to xeflectcertain intuitions about the semantic roles of the variouseiements, but I cannot, in general, provide empirical arguments248for my assignments, other than the fact that they give me theright results.
I do have ao.
operational criterion for decidingwhelher to call the SO a 1 or a 3: when the verb i~l questioncould appear in a d_ouble object construction (i.e., immediatelyfollowed by two NP's), !
called the SO a 3; otherwise, I calledit a 1.
Thus, in what follows, the assignments are correlatedwith the fact that promise and tell have double object forms ( Ipromised~told him nothing), but persuade and believe do not(*1 persuaded~bel ieved him nothing).Consider first persuade.
\] 'he functional structure for thisverb in examples like They persuaded John to leave would beSS=2, SO--l, SO2(SO)=4.
The passive rule yields a a entrywhose functional structure is SS=I, SO2(SS)=4.
Since SO=I inthe original entry, this passive may be either verbal oradjectival.
Hence, we ,:an get both John was persuaded toleave and John seemed persuaded to leave.
On the otherhand, the condition for application of dative is not met, and,accordingly, we cannot get *They persuaded to leave.Transformational studies going back t~ Rosenbaum (1967)have pointed out numerous differences between the behaviorof perusade and that of believe.
The standard analysis ofthese ciifferences has involved the claim that th.
?
surface objectof believe was raised from the subject position of thecomplement.
The system proposed here can mimic thatanalysis by assigning to believe a functional structure in whichthe SO bears no thematic relat ion6:SS=2 , SO2(SO)=1.
Theseare the assignments for examples like I believe John to be athome.
The verbal passive rule ~ill appiy, yielding thefunctional structure SO2(SS)=I, for examples like John isbelieved to be at ha,he.
Since neither the condition on theadjectival passive rule nor that on the dative rule is met, wecan predict the non-occurrence of examples like "John seemsbelieved to be at home and *!
believe to be at home.
The nextverb I wish to consider is tell, which standardtransformational accounts would not distinguish in anyrelevant wa?
from persuade.
For reasons noted above, Iassign tell the functional structure SS=2, SO=3, SO2(SO)=1, asin examples like We told John to bring the beer.
Applying theverbal passive ttde we get SS=3, SO2(SS)=l, covering exarupleslike John was told to bring the beer.
-i'he condition on theadjectival passive rule is not satisfied, so we cannot derive?
John seemed told to br.
!ng the beer.
Notice now that theccndition for applying the dative rule is met  Applying therule results in the following functional structure: SS=2,SO0--1; this structure is i l l-formed, since there is nocontroller.
Accordingly, examples like "tVe told to bring thebeer are impossible.
Finally, consider promise in exampleslike i promised John to mow the lawn.
Promise is exactly liketell, except Ihat the controller is the subject, not the object,i.e., the functional structure is SS=2, SO:3, SO2{SS)=l.
If wetry to apply either passive rule, we will get the followingf,mctional sh-ucture: SS=3, SO20=1.
q'his is i l l-forrned forthe same reason that the dative of tell was, namely, lack of acontroller.
-Ihe corresponding examples are also impossible:?
John was promised to mow the lv wn or *John seemedpromised to mow the lawn.
Dative, however, can apply,yielding an entry whose functional structure is SS=2, SO(SS)=I.This corresponds to examples like i promised to mow thelawn.I hope that this fragment of the lexicon suffices to show thatmy propos.~d modification of Bresnan's system permits anelegant and natural account of a number of syntacticdistinctions, including some which have not been discussed inthe literature, to my knowledge.
One nice feature that I wouldlike to emphasize is that my proposals provide a ratherstraightforward accosnt of Visser's (1973; 2118) observation:"A passive transform is onl) possible when the complementrelates to the immediately preceding (pro)noun."
In myterminology, passive will be impossible when the active has ~icomplement controlled by the SS, as in the case of promise,for passivization will always lead to an uncontrolledcomplement.
Thus, to take another standard example ofVisser's generalization, we can account for the distinctionbetween strike and regard much as we accounted for thedifference between promise and te l l  Both will have thefollowing subcategorization frame: SS=NP, SO:NP, SO2=AP.Their functional structures will include the assignments SS=2and SO=l; they will differ in that regard will haveSO2(SO)=4, while strike has SO2(SS)=4.
These assignments arefor examples like John regards~str ikes Mary as pompous.
Ifwe apply passive to regard we get SS=I, SO2(SS)=4, as in Maryis regarded as pompous.
Applying passive to strike we getSS=I, SO20=4, which is i l l -formed, as is *Mary is struck aspompous.
Notice, incidentally, that this example illustratesthat, in the system I advocate here, constituents other thanVP's can serve as predicates and be subject to control.This concludes my suggestions for modifying Bresnan'sframework.
!
hope !
have succeeded in indicating how agrammar which makes extensive use of the lexicon in place ofsyntactic transformations can handle an array of syntacticfacts in a satisfying manner.
Next, !
wish to argue that asystem of the sort outlined hare can be effectively constrainedin reasonable and interesting ways.
Intuitively, it seems quiteplausible that such a system w~uld be easy to conslrain, for bydrastically reducing the role of transformations, it opens theway for reductions in the power of transformations.
Anumber of candidate constraints on transformations coine tomind.
For example, within Bresnan's framework one mightplausibly argue that no transformation ca~l create newgrammatical relations (e.g., there will be no "subject-creating"transformations like passive or raising to subject), or that notransformation can change the words in the sentencemorphologically (e.g., there will be no nominalization,agreement, or case-markin- transformalions--cf.
P, rame(1978)).
Various ways it, which lexical rules might beconstrained also come to mind; most immediately, it seems tome that many of the "laws" of relatiot~al grammar proposed byPostal and Perhnutter in recent yea:-s could be translatedstraightforwardly into the kind of framework discussed here.in this oaper, however, I would like to consider theconsequences of a constraint on transformations modeled onthe Freezing Principle of Culicover and Wexler 11977).
Myproposal depends on distinguishing two classes oftransformations: root transformations (Emonds (1976)), andWhat I will call unbounded rules.
Root transformations arerules like English subject-auxiliary inversion in questions,which apply only to main clauses; unbounded rules aretransformations (e.g., wh-movement) which involve a crucialvariable, i.e., they move something over a variable or theydelete something under identity with something on the otherside of a variable 7 (see the contributions by Chomsky, Bach,Bresnan, and Partee in Culicover, et al(1977) for discussionof whe\[her unbounded rules are truly unbounded).
Theconstraint I wish to propose, which I will call the interactionconstraint is the following: once a rule of one of these classeshas applied to a given structure, no further rule of the sametype may apply to that structure.
More specifically, when atransformation applies, the smallest constituent containing allof the affected elements becomes frozen, in the sense that nofurther transformations of the same type may analyze it.
Thismeans, in effect, that there will be no interactions among roottransformations, nor among unbounded transformations(though a root transformation may interact with anunbounded rule, as in the case of English wh-questions).
Ibelieve tl'~at there are several desirable consequences ofprohibit ing such interactions.First of all, let me mention a somewhat conjectural reason foradvocating the interaction constraint.
As noted above, a verysimilar proposal emerged from the learnability studies ofWexler, Culicover, and Hamburger; they were able to prove249that a class of grammars in which nodes were frozen undersimilar conditions was learnable by a fairly simple learningdevice.
Hence, it seems plausible to conjecture that theinteraction constraint might be useful in devising alearnability proof for some version of \[Iresnan's theory.
Inany event, it seems that the interaction constraint would makethe language-learner's task easier by lintiting the extent towhich surface structures could deviate from base forms (seeCoker & Crain (in preparation)).Second, there is empirical support for the interactionconstraint.
Emonds (1972; 38-,10) shows that only one rootpreposing transformation can apply per sentence.
Since thesmalle;t structure containing initial position in a root sentenceis the whole sentence, Emonds's observation is an immediateconsequer:ce of the interaction constraint.
Similarly, many ofthe ways in which unbounded transformations are prohibitedfrom interacting are familiar.
For example, the fact thatelements in relative clauses are inaccessible to unboundedtransformations has been extensively discussed in the literature(e.g., Ross (1967), Chomsky (1973), to cite only two accounts).This fact follows from the interaction constraint, since anunbounded transformation is involved in the formation ofrelative clauses.
Hence, examples like "Who do you know aman who saw?
or "John is taller-than !
know d man who is areexcluded by th.
~ interaction constraint.
The fact thatcomparative clauses and embedded questions are also "islands"has been less widely discussed in the literature, but is also aconsequence of the interaction constraint.
Thus, suchexamples as *Who is John louder than Mary persuaded to be?or "Who does John wonder when Bill will see?
are excludedbecause they involve wh-moveraent extracting material fromclauses in which wh-movement or comparative deletion hastaken place.
Likewise, comparative clauses are impervious tofurther applications of comparative deletion: *John was kindto more people than he l iked Bill more than !
l iked (wherethis would mean, if grammatical, that the number of peopleJohn was kind to exceeded the number of people liked betterby Bill than by me).
In short, the interaction constraint seemsto make the right predictions about a substantial array of data.Finally, I would like to suggest hat the interaction constraintserves not only to restrict the class of grammars madeavailable by linguistic theory, but also to l imit the class oflanguages generable by the available grammars (see Wasow (inpress a) for discussion of this distinction).
I will not attemptany formal demonstration of this conclusion here, but willsketch briefly why 1 believe it to be the case.
Peters andRitchie (1973) prove that the language generated by atransformational grammar is recursive if it is possible, on thebasis of a surface string, to effectively compute a maximumsize of a deep structure from which that string could bederived.
The interaction constraint, together with the standardcondition on recoverability of deletions (see Peters andRitchie (1973)), l imit the extent to which deletions may shrinka structure.
To show why this is the case, it will be useful toinvent some terminology: let us call A a parent of B if B. canbe derived from A by a single application of onetransformation.
A parent's parent will be called a grandparent,and so on.
Now consider a string of length n. Because 3f therecoverability condition, its parent cannot be longer than 2n(measuring length in terms of number of terminal symbols).Likewise, its grandparent cannot be longer than 4n.
However,if the grandparent were the full 4n long, then the parent wouldbe frozen by the interaction constraint, and the original stringwould be underivable.
In fact, each (length r,) half of theparent must have a parent of length no more than 2n- | ,  if weare to avoid blocking the derivation by the interactioncoustraint.
Thus, the maximum size of a grandparent is 4n-2.By similar reasoning it is not hard to see that :he maximumsize of any ancestor m+l generations removed is 2m(2n-m).Since this number becomes zero when m=2n,  there is aneffective upper bound o:~ the size of arty at~,cestor.
Hence, theinteraction constraint, togt:ther with the st~mda~d condition onrecoverability of deletions, limits the class of languages whichcan be generated to a subclass of the recursive sets 8.
Thisprovides yet another point of convergence with computationalconcerns, since, as noted above, a language must be recursivein order to be effectively processed.1 have sketched a version of transformational grammar which,,eems to hold considerable promise.
There are a number ofproblems with this approach which l am aware of andundoubtedly many more l am blissfully ignorant of.
What !have presented here was intended, more than anything else, asan indication of a program of research, and i have hence feltfree to ignore many important issues.
The primary point !wish to make is that the study of language appears to haveprogressed to a point where the concerns of thetransformational ist and the concerns of the computationallinguist need not conflict, and indeed may be addressed by asingle theory.
* I wish to express my gratitude to Adrian Akmajian, JoanBresnan, and especially Ron Kaplan for ver~ stimulatingdiscussions of some of the material in this paper.
They are, ofcourse, absolved of any responsibility for its shortcomings.
Iam also very grateful to the Xerox Corporation for making itsresources, human and electronic, available to me in thepreparation of this paper.
Some of the research reported onhere was begun under a Summer Stipend from the Natio,lalEndowment for the Humanities.Footnotes1.
No rigorous definition of these notions has ever been offered in theliterature, and certain problems with the way they' have been used have beenpointed out (e.g.. ItJst and Brame (1976)).
I do not wish to commit myselfto all of the claims which have been made in the literature about thesenotions, and my notation below is intended to reflect this.
I do, however,believe that tho~,?
who have discussed thematic relation,,; are onto somethingimportant.2.
Obviously, there is more to forming passives than this; for example, iignore morphology.3.
Those familiar with Postal and Perhnutter's version of relationalgrammar will recognize the resemblance of last semence to the RelationalAnnihilation Law.
Notice by the way, that aly passive rules say .othingaboJt the by phrase.
I am assuming, with Bresn:m (in press), that there isan independent rule assigning agent status to the objects of some by phrases.This rule would operate not only in passives, but also in examples like Thesymphony was b.v Beethoven.4.
Notice that I am formulating the dative rule "hackwards", that is, withthe double object construction as the input.
My rule says nothing about theprepositions to and I~r because I assume that the functional role ~f theirobjects will be covered by ~epatate rules, as is the case with by.
Exampleslike John's call was to Mary and This present is for you lend ~:redence tomy assumption.5.
This is to be traders:pod as saying that the SO2 will be treated as apredicate, with its own assignments of thematic relations, and with theelement iJ~ paretbeses treated as if it were the SS of tha: predicate.6.
Jane Robinson has suggested to me that it might be more appropriatesemantically to :teat the subject of believe as a 3.
This would be perfectlycompatible with my analysis.7.
My treatment here ignores anaphora rules like VP deletion and sluicing.t am assuming that these rules are not transforatations, but a separatecategory of rules, subject to their awn unique conditions (see Wasow (inpress b) for discussion).8.
As given, my argument does not take into account root transformationsor specified deletions (see Wasow tin press a)).
I~: is quite trivial, however,to extend the argument to cover these cases.ReferencesAnderson, S. (1977) "Comments on the Paper byWasow", in Culicover, et al(1977).Brame, M. (1978) "The Base Hypothesis and theSpelling Prohibition".
Linguistic Analysis 4.1.Bresnan, J.
(1976) "On the Form and Functioning of"lransformations".
Linguistic Inquiry 7.1.250Bresnan, J.
(in press) "A Realistic TransformationalGrammar", in M. Halle, J. Bresnan, and G. Miller (eds),Linguistic Theory and Psychological Reality.
MIT Press,Cambridge, Massachusetts.Chomsk?, N. (1965) Aspects of the Theory of  Syr,.tax.MIT Press.
C:~mbridge, Massachusetts.Chomsky, N. (\] 973) "Conditions on Transforraations",in S. Anderson and P. Kiparsky (eds), A Festsckrift forMorris Halle.
Holt, Rinehart, and W,nston, New York.Coker, P. and S. Crair (in preparation) "LinguisticProcessing: The Grammatical Basis of SentenceInterpretation".
Claremont Graduate School, Claremont,California.Culicover, P. and K. Wexler (1977) "Some SyntacticImplications of a Theory of Language Learnability", inCulicover, et al(1977).Culicover, P., T. Wasow, and A. Akmajian, eds (1977)Formal Syntax.
Academic Press, New York.Emonds, J.
(1972) "A Reformulation of CertainSyntactic Transformations", in S. Peters (ed), Goals ofLinguistic Theory.
Prentice-Hall, Englewood Cliffs, N.J.Emonds, J.
(1976) A Tram~Jo.~mational Approach toEnglish Syntax: Rogt, Structure-Preserving and LocalTransformations.
Academic Press, New York.Fodor, J.
A., T. Bever, and M. Garrett (1974) ThePsychology of Language.
McGraw-Hill, New York.Gruber, J.
(1965) Studies in Lexical ~elations.
MITd isseltation.Hust, J. and M Brame (1976) "Jackendoff onlntcrpretiw.
~ Semantics".
Linguistic Analysis 2.3.J~tckcndoff, I1.. (1972) Semantic Interpretation inGeJ,erative Grammar.
MIT Pre~;s, Cambridge, Massachusetts.Osherson, D. and T. Wa~,ow (1976) "Ta~k Specificityand Species Specificity in the Stady of Language: AM?lhodolegical Note".
Cogt,ition 4.Peters, S. and R. Ritchie (1973) "On the GenerativePower of Transf.
)rmational Grammars".
lnformafion Sciences6.Rosenbaum, P. (1967) The Gre, mmar of EnglishPredicate Complement Constructions.
MIT Press, Cambridge,Massachusetts.Ross, J.
{1967) Constraints on Variables in Syntax.MIT dissertation.Wasow, T. (1977) "Transformations and the Lexicon",in Culicover, et al(197711.Wasow, T. (1978) "Some Thoughts on MentalRepresentation and Transformational Grammar".
Paperdelivered at MIT Sloan Foundation Workshop on MentalRepresentation.Wasow, T (in press a) "On Constrailfing the Class of"~ ransformational Languages".
Synthese.Wasow, 1".
(in press b) Anaphora in GenerativeGrammar.
Story-Scientia, Ghent.Woods, W. (1973) "An Experimental Parsing Systemfor Transition Network Grammar".
in R. Rustin (ed), NaturalLanguage Processing.
Algorithmics Press, New York.251
