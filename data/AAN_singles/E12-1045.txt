Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 439?448,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsCutting the Long Tail: Hybrid Language Modelsfor Translation Style AdaptationArianna Bisazza and Marcello FedericoFondazione Bruno KesslerTrento, Italy{bisazza,federico}@fbk.euAbstractIn this paper, we address statistical ma-chine translation of public conference talks.Modeling the style of this genre can be verychallenging given the shortage of availablein-domain training data.
We investigate theuse of a hybrid LM, where infrequent wordsare mapped into classes.
Hybrid LMs areused to complement word-based LMs withstatistics about the language style of thetalks.
Extensive experiments comparingdifferent settings of the hybrid LM are re-ported on publicly available benchmarksbased on TED talks, from Arabic to Englishand from English to French.
The proposedmodels show to better exploit in-domaindata than conventional word-based LMs forthe target language modeling component ofa phrase-based statistical machine transla-tion system.1 IntroductionThe translation of TED conference talks1 is anemerging task in the statistical machine transla-tion (SMT) community (Federico et al 2011).The variety of topics covered by the speeches, aswell as their specific language style, make this avery challenging problem.Fixed expressions, colloquial terms, figures ofspeech and other phenomena recurrent in the talksshould be properly modeled to produce transla-tions that are not only fluent but that also em-ploy the right register.
In this paper, we proposea language modeling technique that leverages in-domain training data for style adaptation.1http://www.ted.com/talksHybrid class-based LMs are trained on textwhere only infrequent words are mapped to Part-of-Speech (POS) classes.
In this way, topic-specific words are discarded and the model fo-cuses on generic words that we assume more use-ful to characterize the language style.
The factor-ization of similar expressions made possible bythis mixed text representation yields a better n-gram coverage, but with a much higher discrimi-native power than POS-level LMs.Hybrid LM also differs from POS-level LM inthat it uses a word-to-class mapping to determinePOS tags.
Consequently, it doesn?t require the de-coding overload of factored models nor the tag-ging of all parallel data used to build phrase ta-bles.
A hybrid LM trained on in-domain data canthus be easily added to an existing baseline sys-tem trained on large amounts of background data.The proposed models are used in addition tostandard word-based LMs, in the framework oflog-linear phrase-based SMT.The remainder of this paper is organized as fol-lows.
After discussing the language style adapta-tion problem, we will give an overview of relevantwork.
In the following sections we will describein detail hybrid LM and its possible variants.
Fi-nally, we will present an empirical analysis of theproposed technique, including intrinsic evaluationand SMT experiments.2 BackgroundOur working scenario is the translation of TEDtalks transcripts as proposed by the IWSLT Eval-uation Campaign2.
This genre covers a varietyof topics ranging from business to psychology.The available training material ?
both parallel and2http://www.iwslt2011.org439Beginning of Sentence: [s] End of Sentence: [/s]TED NEWS TED NEWS1st [s] Thank you .
[/s] 1st [s] ( AP ) - 1st [s] Thank you .
[/s] 1st ?
he said .
[/s]2 [s] Thank you very much 2 [s] WASHINGTON ( ... 2 you very much .
[/s] 2 ?
she said .
[/s]3 [s] I ?m going to 3 [s] NEW YORK ( AP 3 in the world .
[/s] 3 , he said .
[/s]4 [s] And I said , 4 [s] ( CNN ) ?
4 and so on .
[/s] 4 ?
he said .
[/s]5 [s] I don ?t know 5 [s] NEW YORK ( R... 5 , you know .
[/s] 5 in a statement .
[/s]6 [s] He said , ?
6 [s] He said : ?
6 of the world .
[/s] 6 the United States .
[/s]7 [s] I said , ?
7 [s] ?
I don ?t 7 around the world .
[/s] 7 to this report .
[/s]8 [s] And of course , 8 [s] It was last updated 8 .
Thank you .
[/s] 8 ?
he added .
[/s]9 [s] And one of the 9 [s] At the same time 9 the United States .
[/s] 9 , police said .
[/s]10 [s] And I want to ... 10 all the time .
[/s] 10 , officials said .
[/s]11 [s] And that ?s what 69 [s] I don ?t know 11 to do it .
[/s] ...12 [s] We ?re going to 612 [s] I ?m going to 12 and so forth .
[/s] 13 in the world .
[/s]13 [s] And I think that 2434 [s] ?
I said , 13 don ?t know .
[/s] 17 around the world .
[/s]14 [s] And you can see 7034 [s] He said , ?
14 to do that .
[/s] 46 of the world .
[/s]15 [s] And this is a 8199 [s] And I said , 15 in the future .
[/s] 129 all the time .
[/s]16 [s] And this is the 8233 [s] Thank you very much 16 the same time .
[/s] 157 and so on .
[/s]17 [s] And he said , ... 17 , you know ?
[/s] 1652 , you know .
[/s]18 [s] So this is a ?
[s] Thank you .
[/s] 18 to do this .
[/s] 5509 you very much .
[/s]Table 1: Common sentence-initial and sentence-final 5-grams, as ranked by frequency, in the TED and NEWScorpora.
Numbers denote the frequency rank.monolingual ?
consists of a rather small collectionof TED talks plus a variety of large out-of-domaincorpora, such as news stories and UN proceed-ings.Given the diversity of topics, the in-domaindata alone cannot ensure sufficient coverage to anSMT system.
The addition of background datacan certainly improve the n-gram coverage andthus the fluency of our translations, but it may alsomove our system towards an unsuitable languagestyle, such as that of written news.In our study, we focus on the subproblem oftarget language modeling and consider two En-glish text collections, namely the in-domain TEDand the out-of-domain NEWS3, summarized inTable 2.
Because of its larger size ?
two ordersof magnitude ?
the NEWS corpus can provide abetter LM coverage than the TED on the test data.This is reflected both on perplexity and on the av-erage length of the context (or history h) actually3http://www.statmt.org/wmt11/translation-task.htmlLM Data |S| |W | |V | PP h5gTED-En 124K 2.4M 51K 112 1.7NEWS-En 30.7M 782M 2.2M 104 2.5Table 2: Training data and coverage statistics of two5-gram LMs used for the TED task: number of sen-tences and tokens, vocabulary size; perplexity and av-erage word history.used by these two LMs to score the test?s refer-ence translations.
Note that the latter measure isbounded at the LM order minus one, and is in-versely proportional to the number of back-offsperformed by the model.
Hence, we use this valueto estimate how well an n-gram LM fits the testdata.
Indeed, despite the genre mismatch, the per-plexity of a NEWS 5-gram LM on the TED-2010test reference translations is 104 versus 112 forthe in-domain LM, and the average history size is2.5 versus 1.7 words.TED NEWS1st , 1st the... ...9 I 40 I12 you 64 you90 actually 965 actually268 stuff 2479 guy370 guy 2861 stuff436 amazing 4706 amazingTable 3: Excerpts from TED and NEWS training vo-cabularies, as ranked by frequency.
Numbers denotethe frequency rank.Yet we observe that the style of public speechesis much better represented in the in-domain cor-pus than in the out-of-domain one.
For instance,let us consider the vocabulary distribution4 of the4Hesitations and filler words, typical of spoken language,are not covered in our study because they are generally notreported in the TED talk transcripts.440two corpora (Table 3).
The very first forms, asranked by frequency, are quite similar in the twocorpora.
However, there are important excep-tions: the pronouns I and you are among the top20 frequent forms in the TED, while in the NEWSthey are ranked only 40th and 64th respectively.Other interesting cases are the words actually,stuff, guy and amazing, all ranked about 10 timeshigher in the TED than in the NEWS corpus.We can also analyze the most typical waysto start and end a sentence in the two text col-lections.
As shown in Table 1, the frequencyranking of sentence-initial and sentence-final 5-grams in the in-domain corpus is notably differentfrom the out-of-domain one.
TED?s most frequentsentence-initial 5-gram ?
[s] Thank you .
[/s] ?
isnot at all attested in the NEWS corpus.
As forthe 4th most common sentence start ?
[s] And Isaid ,?
is only ranked 8199th in the NEWS, andso on.
Notably, the top ranked NEWS 5-grams in-clude names of cities (Washington, New York) andof news agency (AP, Reuters).
As regards sen-tence endings, we observe similar contrasts: forinstance, the word sequence ?and so on .
[/s] ?is ranked 4th in the TED and 157th in the NEWSwhile ?, you know .
[/s] ?
is 5th in the TED andonly 1652th in the NEWS.These figures confirm that the talks have a spe-cific language style, remarkably different fromthat of the written news genre.
In summary, talksare characterized by a massive use of first and sec-ond persons, by shorter sentences, and by morecolloquial lexical and syntactic constructions.3 Related WorkThe brittleness of n-gram LMs in case of mis-match between training and task data is a wellknown issue (Rosenfeld, 2000).
So called do-main adaptation methods (Bellegarda, 2004) canimprove the situation, once a limited amountof task specific data become available.
Ideally,domain-adaptive LMs aim to improve model ro-bustness under changing conditions, involvingpossible variations in vocabulary, syntax, content,and style.
Most of the known LM adaption tech-niques (Bellegarda, 2004), however, address allthese variations in a holistic way.
A possible rea-son for this is that LM adaptation methods wereoriginally developed under the automatic speechrecognition framework, which typically assumesthe presence of one single LM.
The progressiveadoption of the log-linear modeling framework inmany NLP tasks has recently introduced the useof multiple LM components (features), which per-mit to naturally factor out and integrate differentaspects of language into one model.
In SMT, thefactored model (Koehn and Hoang, 2007), for in-stance, permits to better tailor the LM to the tasksyntax, by complementing word-based n-gramswith a part-of-speech (POS) LM , that can be es-timated even on a limited amount of task-specificdata.
Besides many works addressing holistic LMdomain adaptation for SMT, e.g.
Foster and Kuhn(2007), recently methods were also proposed toexplicitly adapt the LM to the discourse topic of atalk (Ruiz and Federico, 2011).
Our work makesanother step in this direction by investigating hy-brid LMs that try to explicitly represent the speak-ing style of the talk genre.
As a difference fromstandard class-based LMs (Brown et al 1992) orthe more recent local LMs (Monz, 2011), whichare used to predict sequences of classes or word-class pairs, our hybrid LM is devised to pre-dict sequences of classes interleaved by words.While we do not claim any technical novelty inthe model itself, to our knowledge a deep investi-gation of hybrid LMs for the sake of style adap-tation is definitely new.
Finally, the term hybridLM was inspired by Yazgan and Sarac?lar (2004),which called with this name a LM predicting se-quences of words and sub-words units, devised tolet a speech recognizer detect out-of-vocabulary-words.4 Hybrid Language ModelHybrid LMs are n-gram models trained on amixed text representation where each word is ei-ther mapped to a class or left as is.
This choiceis made according to a measure of word common-ness and is univocal for each word type.The rationale is to discard topic-specific words,while preserving those words that best character-ize the language style (note that word frequencyis computed on the in-domain corpus only).
Map-ping non-frequent terms to classes naturally leadsto a shorter tail in the frequency distribution, asvisualized by Figure 1.
A model trained on suchdata has a better n-gram coverage of the test setand may take advantage of a larger context whenscoring translation hypotheses.As classes, we use deterministically assignedPOS tags, obtained by first tagging the data with441???????????????????
???
???
???
???
???
??????????
?Figure 1: Type frequency distribution in the EnglishTED corpus before and after POS-mapping of wordswith less than 500 occurrences (25% of tokens).
Therank in the frequency list (x-axis) is plotted against therespective frequency in logarithmic scale.
Types withless than 20 occurrences are omitted from the graph.Tree Tagger (Schmid, 1994) and then choosingthe most likely tag for each word type.
In thisway, we avoid the overload of searching for thebest tagging decisions at run-time at the cost ofa slightly higher imprecision (see Section 5.1).The hybridly mapped data is used to train a high-order n-gram LM that is plugged into an SMT de-coder as an additional feature on target word se-quences.
During the translation process, wordsare mapped to their class just before querying thehybrid LM, therefore translation models can betrained on plain un-tagged data.As exemplified in Table 4, hybrid LMs candraw useful statistics on the context of commonwords even from a small corpus such as the TED.To have an idea of data sparseness, consider thatin the unprocessed TED corpus the most frequent5-gram containing the common word guy occursonly 3 times.
After the mapping of words withfrequency <500, the highest 5-gram frequencygrows to 17, the second one to 9, and so on.guy 598 actually 3978a guy VBN NP NP 17 [s] This is actually a 20guy VBN NP NP , 9 [s] It ?s actually a 17guy , NP NP , 8 , you can actually VB 13a guy called NP NP 8 is actually a JJ NN 13this guy , NP NP 6 This is actually a NN 12guy VBN NP NP .
6 [s] And this is actually 12by a guy VBN NP 5 [s] And that ?s actually 10a JJ guy .
[/s] 5 , but it ?s actually 10I was VBG this guy 4 NN , it ?s actually 9guy VBN NP .
[/s] 4 we?re actually going to 8Table 4: Most common hybrid 5-grams containing thewords guy and actually, along with absolute frequency.4.1 Word commonness criteriaThe most intuitive way to measure word common-ness is by absolute term frequency (F ).
We willuse this criterion in most of our experiments.
Afiner solution would be to also consider the com-monness of a word across different talks.
At thisend, we propose to use the fdf statistics, that is theproduct of relative term f requency and documentf requency5:fdfw =c(w)?w?
c(w?
)?c(dw)c(d)where dw are the documents (talks) containing atleast one occurrence of the word w.If available, real talk boundaries can be usedto define the documents.
Alternatively, we cansimply split the corpus into chunks of fixed size.In this work we use this approximation.Another issue is how to set the threshold.
In-dependently from the chosen commonness mea-sure, we can reason in terms of the ratio of tokensthat are mapped to POS classes (WP ).
For in-stance, in our experiments with English, we canset the threshold to F=500 and observe that WPcorresponds to 25% of the tokens (and 99% of thetypes).
In the same corpus, a similar ratio is ob-tained with fdf=0.012.In our study, we consider three ratios WP ={.25,.50, .75} that correspond to different levels of lan-guage modeling: from a domain-generic word-level LM to a lexically anchored POS-level LM.4.2 Handling morphologyToken frequency-based measures may not be suit-able for languages other than English.
Whentranslating into French, for instance, we have todeal with a much richer morphology.As a solution we can use lemmas, univocallyassigned to word types in the same manner asPOS tags.
Lemmas can be employed in two ways:only for word selection, as a frequency measure,or also for word representation, as a mapping forcommon words.
In the former, we preserve in-flected variants that may be useful to model thelanguage style, but we also risk to see n-gram cov-erage decrease due to the presence of rare types.In the latter, only canonical forms and POS tags5This differs from the tf-idf widely used in informationretrieval, which is used to measure the relevance of a term ina document.
Instead, we measure commonness of a term inthe whole corpus.442appear in the processed text, thus introducing afurther level of abstraction from the original text.Here follows a TED sentence in its originalversion (first line) and after three different hy-brid mappings ?
namely WP =.25, WP =.25 withlemma forms, and WP =.50:Now you laugh, but that quote has kind of a sting to it, right.Now you VB , but that NN has kind of a NN to it, right.Now you VB , but that NN have kind of a NN to it, right.RB you VB , CC that NN VBZ NN of a NN to it, RB .5 EvaluationIn this section we perform an intrinsic evaluationof the proposed LM technique, then we measureits impact on translation quality when integratedinto a state-of-the-art phrase-based SMT system.5.1 Intrinsic evaluationWe analyze here a set of hybrid LMs trained onthe English TED corpus by varying the ratio ofPOS-mapped words and the word representationtechnique (word vs lemma).
All models weretrained with the IRSTLM toolkit (Federico et al2008), using a very high n-gram order (10) andWitten-Bell smoothing.First, we estimate an upper bound of the POStagging errors introduced by deterministic tag-ging.
At this end, the hybridly mapped data iscompared with the actual output of Tree Tagger onthe TED training corpus (see Table 5).
Naturally,the impact of tagging errors correlates with the ra-tio of POS-mapped tokens, as no error is countedon non-mapped tokens.
For instance, we note thatthe POS error rate is only 1.9% in our primary set-ting, WP =.25 and word representation, whereason a fully POS-mapped text it is 6.6%.
Note thatthe English tag set used by Tree Tagger includes43 classes.Now we focus on the main goal of hybrid textrepresentation, namely increasing the coverage ofthe in-domain LM on the test data.
Here too, wemeasure coverage by the average length of wordhistory h used to score the test reference transla-tions (see Section 2).
We do not provide perplex-ity figures, since these are not directly compara-ble across models with different vocabularies.
Asshown by Table 5, n-gram coverage increases withthe ratio of POS-mapped tokens, ranging from 1.7on an all-words LM to 4.4 on an all-POS LM.
OfHybrid 10g LM |V | POS-Err h10gall words 51299 0.0% 1.7all lemmas 38486 0.0% 1.9.25 POS/words 475 1.9% 2.7.50 POS/words 93 4.1% 3.5.75 POS/words 50 5.7% 4.1allPOS 43 6.6% 4.4.25 POS/lemmas 302 1.8% 2.8.25 POS/words(fdf) 301 1.9% 2.7Table 5: Comparison of LMs obtained from differenthybrid mappings of the English TED corpus: vocabu-lary size, POS error rate, and average word history onIWSLT?tst2010?s reference translations.course, the more words are mapped, the less dis-criminative our model will be.
Thus, choosing thebest hybrid mapping means finding the best trade-off between coverage and informativeness.We also applied hybrid LM to the French lan-guage, again using Tree Tagger to create the POSmapping.
The tag set in this case comprises 34classes and the POS error rate with WP =.25 is1.2% (compare with 1.9% in English).
As previ-ously discussed, morphology has a notable effecton the modeling of French.
In fact, the vocabu-lary reduction obtained by mapping all the wordsto their most probable lemma is -45% (57959 to31908 types in the TED corpus), while in Englishit is only -25%.5.2 SMT baselineOur SMT experiments address the translation ofTED talks from Arabic to English and from En-glish to French.
The training and test datasetswere provided by the organizers of the IWSLT11evaluation, and are summarized in Table 6.Marked in bold are the corpora used for hybridLM training.
Dev and test sets have a single ref-erence translation.For both language pairs, we set up com-petitive phrase-based systems6 using the Mosestoolkit (Koehn et al 2007).
The decoder fea-tures a statistical log-linear model including aphrase translation model and a phrase reorderingmodel (Tillmann, 2004; Koehn et al 2005), twoword-based language models, distortion, wordand phrase penalties.
The translation and re-ordering models are obtained by combining mod-els independently trained on the available paral-6The SMT systems used in this paper are thoroughly de-scribed in (Ruiz et al 2011).443Corpus |S| |W | `AR-ENTED 90K 1.7M 18.9UN 7.9M 220M 27.8ENTED 124K 2.4M 19.5NEWS 30.7M 782M 25.4AR testdev2010 934 19K 20.0tst2010 1664 30K 18.1EN-FRTED 105K 2.0M 19.5UN 11M 291M 26.5NEWS 111K 3.1M 27.6FRTED 107K 2.2M 20.6NEWS 11.6M 291M 25.2EN testdev2010 934 20K 21.5tst2010 1664 32K 19.1Table 6: IWSLT11 training and test data statistics:number of sentences |S|, number of tokens |W | andaverage sentence length `.
Token numbers are com-puted on the target language, except for the test sets.lel corpora: namely TED and NEWS for Arabic-English; TED, NEWS and UN for English-French.
To this end we applied the fill-up method(Nakov, 2008; Bisazza et al 2011) in which out-of-domain phrase tables are merged with the in-domain table by adding only new phrase pairs.Out-of-domain phrases are marked with a binaryfeature whose weight is tuned together with theSMT system weights.For each target language, two standard 5-gramLMs are trained separately on the monolingualTED and NEWS datasets, and log-linearly com-bined at decoding time.
In the Arabic-Englishtask, we use a hierarchical reordering model (Gal-ley and Manning, 2008; Hardmeier et al 2011),while in the English-French task we use a defaultword-based bidirectional model.
The distortionlimit is set to the default value of 6.
Note thatthe use of large n-gram LMs and of lexicalizedreordering models was shown to wipe out the im-provement achievable by POS-level LM (Kirch-hoff and Yang, 2005; Birch et al 2007).Concerning data preprocessing we apply stan-dard tokenization to the English and French text,while for Arabic we use an in-house tokenizer thatremoves diacritics and normalizes special charac-ters and digits.
Arabic text is then segmented withAMIRA (Diab et al 2004) according to the ATBscheme7.
The Arabic-English system uses cased7The Arabic Treebank tokenization scheme isolates con-junctions w+ and f+, prepositions l+, k+, b+, future markers+, pronominal suffixes, but not the article Al+.translation models, while the English-French sys-tem uses lowercased models and a standard re-casing post-process.Feature weights are tuned on dev2010 bymeans of a minimum error training procedure(MERT) (Och, 2003).
Following suggestions byClark et al(2011) and Cettolo et al(2011) oncontrolling optimizer instability, we run MERTfour times on the same configuration and use theaverage of the resulting weights to evaluate trans-lation performance.5.3 Hybrid LM integrationAs previously stated, hybrid LMs are trained onlyon in-domain data and are added to the log-lineardecoder as an additional target LM.
To this end,we use the class-based LM implementation pro-vided in Moses and IRSTLM, which applies theword-to-class mapping to translation hypothesesbefore LM querying8.
The order of the additionalLM is set to 10 in the Arabic-English evaluationand 7 in the English-French, as these appeared tobe the best settings in preliminary tests.Translation quality is measured by BLEU (Pa-pineni et al 2002), METEOR (Banerjee andLavie, 2005) and TER (Snover et al 2006)9.
Totest whether differences among systems are statis-tically significant we use approximate randomiza-tion as done in (Riezler and Maxwell, 2005)10.Model variants.
The effect on MT quality ofvarious hybrid LM variants is shown in Table 7.Note that allPOS and allLemmas refer to deter-ministically assigned POS tags and lemmas, re-spectively.
Concerning the ratio of POS-mappedtokens, the best performing values are WP =.25 inArabic-English and WP =.50 in English-French.These hybrid mappings outperform all the uni-form representations (words, lemmas and POS)with statistically significant BLEU and METEORimprovements.The fdf experiment involves the use of doc-ument frequency for the selection of commonwords.
Its performance is very close to that of hy-8Detailed instructions on how to build and use hybridLMs can be found at http://hlt.fbk.eu/people/bisazza.9We use case-sensitive BLEU and TER, but case-insensitive METEOR to enable the use of paraphrase tablesdistributed with the tool (version 1.3).10Translation scores and significance tests were com-puted with the Multeval toolkit (Clark et al 2011):https://github.com/jhclark/multeval.444(a) Arabic to English, IWSLT?tst2010Added InDomain 10gLM BLEU?MET ?
TER ?.00 POS/words (all words)?
26.1 30.5 55.4.00 POS/lemmas (all lem.)
26.0 30.5 55.41.0 POS/words (all POS)?
25.9 30.6 55.3.25 POS/words?
26.5 30.6 54.7.50 POS/words 26.5 30.6 54.9.75 POS/words 26.3 30.7 55.0.25 POS/words(fdf) 26.5 30.7 54.7.25 POS/lemmaF 26.4 30.6 54.8.25 POS/lemmas 26.5 30.8 54.6(b) English to French, IWSLT?tst2010Added InDomain 7gLM BLEU?MET ?
TER ?.00 POS/words (all words) 31.1 52.5 49.9.00 POS/lemmas (all lem.)?
31.2 52.6 49.71.0 POS/words (all POS)?
31.4 52.8 49.8.25 POS/lemmas?
31.5 52.9 49.7.50 POS/lemmas 31.9 53.3 49.5.75 POS/lemmas 31.7 53.2 49.6.50 POS/lemmas(fdf) 31.9 53.3 49.5.50 POS/lemmaF 31.6 53.0 49.6.50 POS/words 31.7 53.1 49.5Table 7: Comparison of various hybrid LM variants.
Translation quality is measured with BLEU, METEOR andTER (all in percentage form).
The settings used for weight tuning are marked with ?.
Best models according toall metrics are highlighted in bold.brid LMs simply based on term frequency; onlyMETEOR gains 0.1 points in Arabic-English.
Apossible reason for this is that document fre-quency was computed on fixed-size text chunksrather than on real document boundaries (see Sec-tion 4.1).
The lemmaF experiment refers to theuse of canonical forms for frequency measuring:this technique does not seem to help in either lan-guage pair.
Finally, we compare the use of lem-mas versus surface forms to represent commonwords.
As expected, lemmas appear to be help-ful for French language modeling.
Interestinglythis is also the case for English, even if by a smallmargin (+0.2 METEOR, -0.1 TER).Summing up, hybrid mapping appears as awinning strategy compared to uniform map-ping.
Although differences among LM variantsare small, the best model in Arabic-English is.25-POS/lemmas, which can be thought of asa domain-generic lemma-level LM.
In English-French, instead, the highest scores are achievedby .50-POS/lemmas or .50-POS/lemmas(fdf), thatis POS-level LM with few frequently occurringlexical anchors (vocabulary size 59).
An inter-pretation of this result is that, for French, mod-eling the syntax is more helpful than modelingthe style.
We also suspect that the French TEDcorpus is more irregular and diverse with respectto the style, than its English counterpart.
In fact,while the English corpus include transcripts oftalks given by English speakers, the French one ismostly a collection of (human) translations.
Typi-cal features of the speech style may have been lostin this process.Comparison with baseline.
In Table 8 thebest performing hybrid LM is compared againstthe baseline that only includes the standard LMsdescribed in Section 5.2.
To complete our eval-uation, we also report the effect of an in-domainLM trained on 50 word classes induced from thecorpus by maximum-likelihood based clustering(Och, 1999).In the two language pairs, both types of LMresult in consistent improvements over the base-line.
However, the gains achieved by the hybridapproach are larger and all statistically signifi-cant.
The hybrid approach is significantly bet-ter than the unsupervised one by TER in Arabic-English and by BLEU and METEOR in English-French (these siginificances are not reported in(a) Arabic to English, IWSLT?tst2010Added InDomainBLEU?
MET ?
TER ?10g LMnone (baseline) 26.0 30.4 55.6unsup.
classes 26.4?
30.8?
55.1?hybrid 26.5?
(+.5) 30.8?
(+.4) 54.6?
(-1.0)(b) English to French, IWSLT?tst2010Added InDomainBLEU?
MET ?
TER ?7g LMnone (baseline) 31.2 52.7 49.8unsup.
classes 31.5 52.9 49.6hybrid 31.9?
(+.7) 53.3?
(+.6) 49.5?
(-.3)Table 8: Final MT results: baseline vs unsupervisedword classes-based LM and best hybrid LM.
Statis-tically significant improvements over the baseline aremarked with ?
at the p < .01 and ?
at the p < .05 level.445the table for clarity).
The proposed method ap-pears to better leverage the available in-domaindata, achieving improvements according to allmetrics: +0.5/+0.4/-1.0 BLEU/METEOR/TERin Arabic-English and +0.7/-0.6/-0.3 in English-French, without requiring any bitext annotation ordecoder modification.Talk-level analysis.
To conclude the study,we analyze the effect of our best hybrid LMon Arabic-English translation quality, at the sin-gle talk level.
The test used in the experiments(tst2010) consists of 11 transcripts with an av-erage length of 151?73 sentences.
For eachtalk, we compare the baseline BLEU score withthat obtained by adding a .25-POS/lemmas hybridLM.
Results are presented in Figure 2.
The darkand light columns denote baseline and hybrid-LMBLEU scores, respectively, and refer to the left y-axis.
Additional data points, plotted on the righty-axis in reverse order, represent talk-level per-plexities (PP) of a standard 5-gram LM trainedon TED (?)
and those of the .25-POS/lemmas10-gram hybrid LM (M), computed on referencetranslations.What emerges first is a dramatic variation ofperformance among the speeches, with baselineBLEU scores ranging from 33.95 on talk ?00?
toonly 12.42 on talk ?02?.
The latter talk appears asa corner case also according to perplexities (397by word LM and 111 by hybrid LM).
Notably, theperplexities of the two LMs correlate well witheach other, but the hybrid?s PP is much more sta-ble across talks: its standard deviation is only 14??????????????????????????????????????????????????????
??
??
??
??
??
??
??
??
??
????????
??????
???????
????
?Figure 2: Talk-level evaluation on Arabic-English(IWSLT-tst2010).
Left y-axis: BLEU impact of a .25-POS/lemma hybrid LM.
Right y-axis: perplexities byword LM and by hybrid LM.points, while that of the word-based PP is 79.
TheBLEU improvement given by hybrid LM, how-ever modest, is consistent across the talks, withonly two outliers: a drop of -0.2 on talk ?00?, anda drop of -0.7 on talk ?02?.
The largest gain (+1.1)is observed on talk ?10?, from 16.8 to 17.9 BLEU.6 ConclusionsWe have proposed a language modeling techniquethat leverages the in-domain data for SMT styleadaptation.
Trained to predict mixed sequencesof POS classes and frequent words, hybrid LMsare devised to capture typical lexical and syntacticconstructions that characterize the style of speechtranscripts.Compared to standard language models, hy-brid LMs generalize better to the test data andpartially compensate for the disproportion be-tween in-domain and out-of-domain training data.At the same time, hybrid LMs show more dis-criminative power than merely POS-level LMs.The integration of hybrid LMs into a competi-tive phrase-based SMT system is straightforwardand leads to consistent improvements on the TEDtask, according to three different translation qual-ity metrics.Target language modeling is only one aspectof the statistical translation problem.
Now thatthe usability of the proposed method has been as-sessed for language modeling, future work willaddress the extension of the idea to the modelingof phrase translation and reordering.AcknowledgmentsThis work was supported by the T4ME networkof excellence (IST-249119), funded by the DGINFSO of the European Commission through the7th Framework Programme.
We thank the anony-mous reviewers for their valuable suggestions.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65?72, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.446Jerome R. Bellegarda.
2004.
Statistical languagemodel adaptation: review and perspectives.
SpeechCommunication, 42(1):93 ?
108.Alexandra Birch, Miles Osborne, and Philipp Koehn.2007.
CCG supertags in factored statistical ma-chine translation.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages9?16, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Arianna Bisazza, Nick Ruiz, and Marcello Fed-erico.
2011.
Fill-up versus Interpolation Meth-ods for Phrase-based SMT Adaptation.
In Interna-tional Workshop on Spoken Language Translation(IWSLT), San Francisco, CA.P.
F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,and R. L. Mercer.
1992.
Class-based n-gram mod-els of natural language.
Computational Linguistics,18(4):467?479.Mauro Cettolo, Nicola Bertoldi, and Marcello Fed-erico.
2011.
Methods for smoothing the optimizerinstability in SMT.
In MT Summit XIII: the Thir-teenth Machine Translation Summit, pages 32?39,Xiamen, China.Jonathan Clark, Chris Dyer, Alon Lavie, andNoah Smith.
2011.
Better hypothesis testingfor statistical machine translation: Controllingfor optimizer instability.
In Proceedings ofthe Association for Computational Lingustics,ACL 2011, Portland, Oregon, USA.
Associa-tion for Computational Linguistics.
available athttp://www.cs.cmu.edu/ jhclark/pubs/significance.pdf.Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.2004.
Automatic Tagging of Arabic Text: FromRaw Text to Base Phrase Chunks.
In Daniel MarcuSusan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Short Papers, pages 149?152,Boston, Massachusetts, USA, May 2 - May 7.
As-sociation for Computational Linguistics.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an Open Source Toolkit forHandling Large Scale Language Models.
In Pro-ceedings of Interspeech, pages 1618?1621, Mel-bourne, Australia.Marcello Federico, Luisa Bentivogli, Michael Paul,and Sebastian Stu?ker.
2011.
Overview of theIWSLT 2011 Evaluation Campaign.
In Interna-tional Workshop on Spoken Language Translation(IWSLT), San Francisco, CA.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for SMT.
In Proceedings of theSecond Workshop on Statistical Machine Transla-tion, pages 128?135, Prague, Czech Republic, June.Association for Computational Linguistics.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In EMNLP ?08: Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 848?856, Morristown, NJ, USA.Association for Computational Linguistics.Christian Hardmeier, Jo?rg Tiedemann, Markus Saers,Marcello Federico, and Mathur Prashant.
2011.The Uppsala-FBK systems at WMT 2011.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 372?378, Edinburgh, Scot-land, July.
Association for Computational Linguis-tics.Katrin Kirchhoff and Mei Yang.
2005.
Improved lan-guage modeling for statistical machine translation.In Proceedings of the ACL Workshop on Buildingand Using Parallel Texts, pages 125?128, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.Philipp Koehn and Hieu Hoang.
2007.
Factoredtranslation models.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 868?876, Prague, Czech Republic, June.
Association forComputational Linguistics.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.In Proc.
of the International Workshop on SpokenLanguage Translation, October.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Con-stantin, and E. Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the 45th Annual Meeting of the Associa-tion for Computational Linguistics Companion Vol-ume Proceedings of the Demo and Poster Sessions,pages 177?180, Prague, Czech Republic.Christof Monz.
2011.
Statistical Machine Translationwith Local Language Models.
In Proceedings of the2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 869?879, Edinburgh,Scotland, UK., July.
Association for ComputationalLinguistics.Preslav Nakov.
2008.
Improving English-SpanishStatistical Machine Translation: Experiments inDomain Adaptation, Sentence Paraphrasing, Tok-enization, and Recasing.
.
In Workshop on Statis-tical Machine Translation, Association for Compu-tational Linguistics.Franz Josef Och.
1999.
An efficient method for de-termining bilingual word classes.
In Proceedings ofthe 9th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL),pages 71?76.Franz Josef Och.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
In ErhardHinrichs and Dan Roth, editors, Proceedings of the44741st Annual Meeting of the Association for Compu-tational Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Pro-ceedings of the 40th Annual Meeting of the Asso-ciation of Computational Linguistics (ACL), pages311?318, Philadelphia, PA.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significancetesting for MT.
In Proceedings of the ACL Work-shop on Intrinsic and Extrinsic Evaluation Mea-sures for Machine Translation and/or Summariza-tion, pages 57?64, Ann Arbor, Michigan, June.
As-sociation for Computational Linguistics.R.
Rosenfeld.
2000.
Two decades of statistical lan-guage modeling: where do we go from here?
Pro-ceedings of the IEEE, 88(8):1270 ?1278.Nick Ruiz and Marcello Federico.
2011.
Topic adap-tation for lecture translation through bilingual la-tent semantic models.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages294?302, Edinburgh, Scotland, July.
Associationfor Computational Linguistics.Nick Ruiz, Arianna Bisazza, Fabio Brugnara, DanieleFalavigna, Diego Giuliani, Suhel Jaber, RobertoGretter, and Marcello Federico.
2011.
FBK @IWSLT 2011.
In International Workshop on Spo-ken Language Translation (IWSLT), San Francisco,CA.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of In-ternational Conference on New Methods in Lan-guage Processing.Matthew Snover, Bonnie Dorr, Rich Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In 5th Conference of the Association for MachineTranslation in the Americas (AMTA), Boston, Mas-sachusetts, August.Christoph Tillmann.
2004.
A Unigram OrientationModel for Statistical Machine Translation.
In Pro-ceedings of the Joint Conference on Human Lan-guage Technologies and the Annual Meeting of theNorth American Chapter of the Association of Com-putational Linguistics (HLT-NAACL).A.
Yazgan and M. Sarac?lar.
2004.
Hybrid languagemodels for out of vocabulary word detection in largevocabulary conversational speech recognition.
InProceedings of ICASSP, volume 1, pages I ?
745?8vol.1, may.448
