Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 180?189,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA New Dataset and Method for Automatically Grading ESOL TextsHelen YannakoudakisComputer LaboratoryUniversity of CambridgeUnited KingdomHelen.Yannakoudakis@cl.cam.ac.ukTed BriscoeComputer LaboratoryUniversity of CambridgeUnited KingdomTed.Briscoe@cl.cam.ac.ukBen MedlockiLexIR LtdCambridgeUnited Kingdomben@ilexir.co.ukAbstractWe demonstrate how supervised discrimina-tive machine learning techniques can be usedto automate the assessment of ?English as aSecond or Other Language?
(ESOL) examina-tion scripts.
In particular, we use rank prefer-ence learning to explicitly model the grade re-lationships between scripts.
A number of dif-ferent features are extracted and ablation testsare used to investigate their contribution tooverall performance.
A comparison betweenregression and rank preference models furthersupports our method.
Experimental results onthe first publically available dataset show thatour system can achieve levels of performanceclose to the upper bound for the task, as de-fined by the agreement between human exam-iners on the same corpus.
Finally, using a setof ?outlier?
texts, we test the validity of ourmodel and identify cases where the model?sscores diverge from that of a human examiner.1 IntroductionThe task of automated assessment of free text fo-cuses on automatically analysing and assessing thequality of writing competence.
Automated assess-ment systems exploit textual features in order tomeasure the overall quality and assign a score to atext.
The earliest systems used superficial features,such as word and sentence length, as proxies forunderstanding the text.
More recent systems haveused more sophisticated automated text processingtechniques to measure grammaticality, textual co-herence, prespecified errors, and so forth.Deployment of automated assessment systemsgives a number of advantages, such as the reducedworkload in marking texts, especially when appliedto large-scale assessments.
Additionally, automatedsystems guarantee the application of the same mark-ing criteria, thus reducing inconsistency, which mayarise when more than one human examiner is em-ployed.
Often, implementations include feedbackwith respect to the writers?
writing abilities, thus fa-cilitating self-assessment and self-tutoring.Implicitly or explicitly, previous work has mostlytreated automated assessment as a supervised textclassification task, where training texts are labelledwith a grade and unlabelled test texts are fitted to thesame grade point scale via a regression step appliedto the classifier output (see Section 6 for more de-tails).
Different techniques have been used, includ-ing cosine similarity of vectors representing text invarious ways (Attali and Burstein, 2006), often com-bined with dimensionality reduction techniques suchas Latent Semantic Analysis (LSA) (Landauer et al,2003), generative machine learning models (Rudnerand Liang, 2002), domain-specific feature extraction(Attali and Burstein, 2006), and/or modified syntac-tic parsers (Lonsdale and Strong-Krause, 2003).A recent review identifies twelve different auto-mated free-text scoring systems (Williamson, 2009).Examples include e-Rater (Attali and Burstein,2006), Intelligent Essay Assessor (IEA) (Landaueret al, 2003), IntelliMetric (Elliot, 2003; Rudner etal., 2006) and Project Essay Grade (PEG) (Page,2003).
Several of these are now deployed in high-stakes assessment of examination scripts.
Althoughthere are many published analyses of the perfor-180mance of individual systems, as yet there is no pub-lically available shared dataset for training and test-ing such systems and comparing their performance.As it is likely that the deployment of such systemswill increase, standardised and independent evalua-tion methods are important.
We make such a datasetof ESOL examination scripts available1 (see Section2 for more details), describe our novel approach tothe task, and provide results for our system on thisdataset.We address automated assessment as a superviseddiscriminative machine learning problem and par-ticularly as a rank preference problem (Joachims,2002).
Our reasons are twofold:Discriminative classification techniques oftenoutperform non-discriminative ones in the context oftext classification (Joachims, 1998).
Additionally,rank preference techniques (Joachims, 2002) allowus to explicitly learn an optimal ranking model oftext quality.
Learning a ranking directly, rather thanfitting a classifier score to a grade point scale aftertraining, is both a more generic approach to the taskand one which exploits the labelling information inthe training data efficiently and directly.Techniques such as LSA (Landauer and Foltz,1998) measure, in addition to writing competence,the semantic relevance of a text written in responseto a given prompt.
However, although our corpusof manually-marked texts was produced by learnersof English in response to prompts eliciting free-textanswers, the marking criteria are primarily based onthe accurate use of a range of different linguisticconstructions.
For this reason, we believe that anapproach which directly measures linguistic compe-tence will be better suited to ESOL text assessment,and will have the additional advantage that it maynot require retraining for new prompts or tasks.As far as we know, this is the first applicationof a rank preference model to automated assess-ment (hereafter AA).
In this paper, we report exper-iments on rank preference Support Vector Machines(SVMs) trained on a relatively small amount of data,on identification of appropriate feature types derivedautomatically from generic text processing tools, oncomparison with a regression SVM model, and onthe robustness of the best model to ?outlier?
texts.1http://www.ilexir.com/We report a consistent, comparable and replicableset of results based entirely on the new dataset andon public-domain tools and data, whilst also exper-imentally motivating some novel feature types forthe AA task, thus extending the work described in(Briscoe et al, 2010).In the following sections we describe in more de-tail the dataset used for training and testing, the sys-tem developed, the evaluation methodology, as wellas ablation experiments aimed at studying the con-tribution of different feature types to the AA task.We show experimentally that discriminative modelswith appropriate feature types can achieve perfor-mance close to the upper bound, as defined by theagreement between human examiners on the sametest corpus.2 Cambridge Learner CorpusThe Cambridge Learner Corpus2 (CLC), developedas a collaborative project between Cambridge Uni-versity Press and Cambridge Assessment, is a largecollection of texts produced by English languagelearners from around the world, sitting CambridgeAssessment?s English as a Second or Other Lan-guage (ESOL) examinations3.For the purpose of this work, we extracted scriptsproduced by learners taking the First Certificate inEnglish (FCE) exam, which assesses English at anupper-intermediate level.
The scripts, which areanonymised, are annotated using XML and linkedto meta-data about the question prompts, the candi-date?s grades, native language and age.
The FCEwriting component consists of two tasks askinglearners to write either a letter, a report, an article,a composition or a short story, between 200 and 400words.
Answers to each of these tasks are anno-tated with marks (in the range 1?40), which havebeen fitted to a RASCH model (Fischer and Mole-naar, 1995) to correct for inter-examiner inconsis-tency and comparability.
In addition, an overallmark is assigned to both tasks, which is the one weuse in our experiments.Each script has been also manually tagged withinformation about the linguistic errors committed,2http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/item3646603/Cambridge-International-Corpus-Cambridge-Learner-Corpus/?site locale=en GB3http://www.cambridgeesol.org/181using a taxonomy of approximately 80 error types(Nicholls, 2003).
The following is an example error-coded sentence:In the morning, you are <NS type = ?TV?>waken|woken</NS> up by a singing puppy.In this sentence, TV denotes an incorrect tense ofverb error, where waken can be corrected to woken.Our data consists of 1141 scripts from the year2000 for training written by 1141 distinct learners,and 97 scripts from the year 2001 for testing writtenby 97 distinct learners.
The learners?
ages followa bimodal distribution with peaks at approximately16?20 and 26?30 years of age.The prompts eliciting the free text are providedwith the dataset.
However, in this paper we makeno use of prompt information and do not make anyattempt to check that the text answer is appropriateto the prompt.
Our focus is on developing an accu-rate AA system for ESOL text that does not requireprompt-specific or topic-specific training.
There isno overlap between the prompts used in 2000 and in2001.
A typical prompt taken from the 2000 trainingdataset is shown below:Your teacher has asked you to write a story for theschool?s English language magazine.
The story mustbegin with the following words: ?Unfortunately, Patwasn?t very good at keeping secrets?.3 ApproachWe treat automated assessment of ESOL text (seeSection 2) as a rank preference learning problem(see Section 1).
In the experiments reported herewe use Support Vector Machines (SVMs) (Vap-nik, 1995) through the SVMlight package (Joachims,1999).
Using the dataset described in Section 2, anumber of linguistic features are automatically ex-tracted and their contribution to overall performanceis investigated.3.1 Rank preference modelSVMs have been extensively used for learning clas-sification, regression and ranking functions.
In itsbasic form, a binary SVM classifier learns a linearthreshold function that discriminates data points oftwo categories.
By using a different loss function,the ?-insensitive loss function (Smola, 1996), SVMscan also perform regression.
SVMs in regressionmode estimate a function that outputs a real numberbased on the training data.
In both cases, the modelgeneralises by computing a hyperplane that has thelargest (soft-)margin.In rank preference SVMs, the goal is to learn aranking function which outputs a score for each datapoint, from which a global ordering of the data isconstructed.
This procedure requires a setR consist-ing of training samples ~xn and their target rankingsrn:R = {(~x1, r1), (~x2, r2), ..., (~xn, rn)} (1)such that ~xi R ~xj when ri < rj , where1 ?
i, j ?
n and i 6= j.A rank preference model is not trained directly onthis set of data objects and their labels; rather a set ofpair-wise difference vectors is created.
The goal ofa linear ranking model is to compute a weight vec-tor ~w that maximises the number of correctly rankedpairs:?
(~xi R ~xj) : ~w(~xi ?
~xj) > 0 (2)This is equivalent to solving the following opti-misation problem:Minimise:12?~w?2 + C?
?ij (3)Subject to the constraints:?
(~xi R ~xj) : ~w(~xi ?
~xj) ?
1?
?ij (4)?ij ?
0 (5)The factor C allows a trade-off between the train-ing error and the margin size, while ?ij are non-negative slack variables that measure the degree ofmisclassification.The optimisation problem is equivalent to that forthe classification model on pair-wise difference vec-tors.
In this case, generalisation is achieved by max-imising the differences between closely-ranked datapairs.The principal advantage of applying rank prefer-ence learning to the AA task is that we explicitly182model the grade relationships between scripts anddo not need to apply a further regression step to fitthe classifier output to the scoring scheme.
The re-sults reported in this paper are obtained by learninga linear classification function.3.2 Feature setWe parsed the training and test data (see Section2) using the Robust Accurate Statistical Parsing(RASP) system with the standard tokenisation andsentence boundary detection modules (Briscoe et al,2006) in order to broaden the space of candidate fea-tures suitable for the task.
The features used in ourexperiments are mainly motivated by the fact thatlexical and grammatical features should be highlydiscriminative for the AA task.
Our full feature setis as follows:i. Lexical ngrams(a) Word unigrams(b) Word bigramsii.
Part-of-speech (PoS) ngrams(a) PoS unigrams(b) PoS bigrams(c) PoS trigramsiii.
Features representing syntax(a) Phrase structure (PS) rules(b) Grammatical relation (GR) distance mea-suresiv.
Other features(a) Script length(b) Error-rateWord unigrams and bigrams are lower-cased andused in their inflected forms.
PoS unigrams, bigramsand trigrams are extracted using the RASP tagger,which uses the CLAWS4 tagset.
The most proba-ble posterior tag per word is used to construct PoSngram features, but we use the RASP parser?s op-tion to analyse words assigned multiple tags whenthe posterior probability of the highest ranked tag isless than 0.9, and the next n tags have probabilitygreater than 150 of it.4http://ucrel.lancs.ac.uk/claws/Based on the most likely parse for each identifiedsentence, we extract the rule names from the phrasestructure (PS) tree.
RASP?s rule names are semi-automatically generated and encode detailed infor-mation about the grammatical constructions found(e.g.
V1/modal bse/+-, ?a VP consisting of a modalauxiliary head followed by an (optional) adverbialphrase, followed by a VP headed by a verb with baseinflection?).
Moreover, rule names explicitly repre-sent information about peripheral or rare construc-tions (e.g.
S/pp-ap s-r, ?a S with preposed PP withadjectival complement, e.g.
for better or worse, heleft?
), as well as about fragmentary and likely extra-grammatical sequences (e.g.
T/txt-frag, ?a text unitconsisting of 2 or more subanalyses that cannot becombined using any rule in the grammar?).
There-fore, we believe that many (longer-distance) gram-matical constructions and errors found in texts canbe (implicitly) captured by this feature type.In developing our AA system, a number of dif-ferent grammatical complexity measures were ex-tracted from parses, and their impact on the accuracyof the system was explored.
For the experiments re-ported here, we use complexity measures represent-ing the sum of the longest distance in word tokensbetween a head and dependent in a grammatical re-lation (GR) from the RASP GR output, calculatedfor each GR graph from the top 10 parses per sen-tence.
In particular, we extract the mean and medianvalues of these distances per sentence and use themaximum values per script.
Intuitively, this featurecaptures information about the grammatical sophis-tication of the writer.
However, it may also be con-founded in cases where sentence boundaries are notidentified through, for example, poor punctuation.Although the CLC contains information about thelinguistic errors committed (see Section 2), we tryto extract an error-rate in a way that doesn?t requiremanually tagged data.
However, we also use anerror-rate calculated from the CLC error tags to ob-tain an upper bound for the performance of an auto-mated error estimator (true CLC error-rate).In order to estimate the error-rate, we build a tri-gram language model (LM) using ukWaC (ukWaCLM) (Ferraresi et al, 2008), a large corpus of En-glish containing more than 2 billion tokens.
Next,we extend our language model with trigrams ex-tracted from a subset of the texts contained in the183FeaturesPearson?s Spearman?scorrelation correlationword ngrams 0.601 0.598+PoS ngrams 0.682 0.687+script length 0.692 0.689+PS rules 0.707 0.708+complexity 0.714 0.712Error-rate features+ukWaC LM 0.735 0.758+CLC LM 0.741 0.773+true CLC error-rate 0.751 0.789Table 1: Correlation between the CLC scores and the AAsystem predicted values.CLC (CLC LM).
As the CLC contains texts pro-duced by second language learners, we only extractfrequently occurring trigrams from highly rankedscripts to avoid introducing erroneous ones to ourlanguage model.
A word trigram in test data iscounted as an error if it is not found in the languagemodel.
We compute presence/absence efficiently us-ing a Bloom filter encoding of the language models(Bloom, 1970).Feature instances of types i and ii are weightedusing the tf*idf scheme and normalised by the L2norm.
Feature type iii is weighted using frequencycounts, while iii and iv are scaled so that their finalvalue has approximately the same order of magni-tude as i and ii.The script length is based on the number of wordsand is mainly added to balance the effect the lengthof a script has on other features.
Finally, featureswhose overall frequency is lower than four are dis-carded from the model.4 EvaluationIn order to evaluate our AA system, we use two cor-relation measures, Pearson?s product-moment cor-relation coefficient and Spearman?s rank correla-tion coefficient (hereafter Pearson?s and Spearman?scorrelation respectively).
Pearson?s correlation de-termines the degree to which two linearly depen-dent variables are related.
As Pearson?s correlationis sensitive to the distribution of data and, due tooutliers, its value can be misleading, we also re-port Spearman?s correlation.
The latter is a non-parametric robust measure of association which isAblated Pearson?s Spearman?sfeature correlation correlationnone 0.741 0.773word ngrams 0.713 0.762PoS ngrams 0.724 0.737script length 0.734 0.772PS rules 0.712 0.731complexity 0.738 0.760ukWaC+CLC LM 0.714 0.712Table 2: Ablation tests showing the correlation betweenthe CLC and the AA system.sensitive only to the ordinal arrangement of values.As our data contains some tied values, we calculateSpearman?s correlation by using Pearson?s correla-tion on the ranks.Table 1 presents the Pearson?s and Spearman?scorrelation between the CLC scores and the AA sys-tem predicted values, when incrementally addingto the model the feature types described in Sec-tion 3.2.
Each feature type improves the model?sperformance.
Extending our language model withfrequent trigrams extracted from the CLC improvesPearson?s and Spearman?s correlation by 0.006 and0.015 respectively.
The addition of the error-rate ob-tained from the manually annotated CLC error tagson top of all the features further improves perfor-mance by 0.01 and 0.016.
An evaluation of our besterror detection method shows a Pearson correlationof 0.611 between the estimated and the true CLC er-ror counts.
This suggests that there is room for im-provement in the language models we developed toestimate the error-rate.
In the experiments reportedhereafter, we use the ukWaC+CLC LM to calculatethe error-rate.In order to assess the independent as opposed tothe order-dependent additive contribution of eachfeature type to the overall performance of the sys-tem, we run a number of ablation tests.
An ablationtest consists of removing one feature of the systemat a time and re-evaluating the model on the test set.Table 2 presents Pearson?s and Spearman?s correla-tion between the CLC and our system, when remov-ing one feature at a time.
All features have a positiveeffect on performance, while the error-rate has a bigimpact, as its absence is responsible for a 0.061 de-crease of Spearman?s correlation.
In addition, the184ModelPearson?s Spearman?scorrelation correlationRegression 0.697 0.706Rank preference 0.741 0.773Table 3: Comparison between regression and rank pref-erence model.removal of either the word ngrams, the PS rules, orthe error-rate estimate contributes to a large decreasein Pearson?s correlation.In order to test the significance of the improvedcorrelations, we ran one-tailed t-tests with a = 0.05for the difference between dependent correlations(Williams, 1959; Steiger, 1980).
The results showedthat PoS ngrams, PS rules, the complexity measures,and the estimated error-rate contribute significantlyto the improvement of Spearman?s correlation, whilePS rules also contribute significantly to the improve-ment of Pearson?s correlation.One of the main approaches adopted by previ-ous systems involves the identification of featuresthat measure writing skill, and then the applicationof linear or stepwise regression to find optimal fea-ture weights so that the correlation with manuallyassigned scores is maximised.
We trained a SVMregression model with our full set of feature typesand compared it to the SVM rank preference model.The results are given in Table 3.
The rank preferencemodel improves Pearson?s and Spearman?s correla-tion by 0.044 and 0.067 respectively, and these dif-ferences are significant, suggesting that rank prefer-ence is a more appropriate model for the AA task.Four senior and experienced ESOL examiners re-marked the 97 FCE test scripts drawn from 2001 ex-ams, using the marking scheme from that year (seeSection 2).
In order to obtain a ceiling for the perfor-mance of our system, we calculate the average corre-lation between the CLC and the examiners?
scores,and find an upper bound of 0.796 and 0.792 Pear-son?s and Spearman?s correlation respectively.In order to evaluate the overall performance of oursystem, we calculate its correlation with the four se-nior examiners in addition to the RASCH-adjustedCLC scores.
Tables 4 and 5 present the results ob-tained.The average correlation of the AA system with theCLC and the examiner scores shows that it is closeCLC E1 E2 E3 E4 AACLC - 0.820 0.787 0.767 0.810 0.741E1 0.820 - 0.851 0.845 0.878 0.721E2 0.787 0.851 - 0.775 0.788 0.730E3 0.767 0.845 0.775 - 0.779 0.747E4 0.810 0.878 0.788 0.779 - 0.679AA 0.741 0.721 0.730 0.747 0.679 -Avg 0.785 0.823 0.786 0.782 0.786 0.723Table 4: Pearson?s correlation of the AA system predictedvalues with the CLC and the examiners?
scores, where E1refers to the first examiner, E2 to the second etc.CLC E1 E2 E3 E4 AACLC - 0.801 0.799 0.788 0.782 0.773E1 0.801 - 0.809 0.806 0.850 0.675E2 0.799 0.809 - 0.744 0.787 0.724E3 0.788 0.806 0.744 - 0.794 0.738E4 0.782 0.850 0.787 0.794 - 0.697AA 0.773 0.675 0.724 0.738 0.697 -Avg 0.788 0.788 0.772 0.774 0.782 0.721Table 5: Spearman?s correlation of the AA system pre-dicted values with the CLC and the examiners?
scores,where E1 refers to the first examiner, E2 to the secondetc.to the upper bound for the task.
Human?machineagreement is comparable to that of human?humanagreement, with the exception of Pearson?s correla-tion with examiner E4 and Spearman?s correlationwith examiners E1 and E4, where the discrepanciesare higher.
It is likely that a larger training set and/ormore consistent grading of the existing training datawould help to close this gap.
However, our system isnot measuring some properties of the scripts, such asdiscourse cohesion or relevance to the prompt elicit-ing the text, that examiners will take into account.5 Validity testsThe practical utility of an AA system will dependstrongly on its robustness to subversion by writerswho understand something of its workings and at-tempt to exploit this to maximise their scores (in-dependently of their underlying ability).
Surpris-ingly, there is very little published data on the ro-bustness of existing systems.
However, Powers etal.
(2002) invited writing experts to trick the scoring185capabilities of an earlier version of e-Rater (Bursteinet al, 1998).
e-Rater (see Section 6 for more de-tails) assigns a score to a text based on linguistic fea-ture types extracted using relatively domain-specifictechniques.
Participants were given a description ofthese techniques as well as of the cue words that thesystem uses.
The results showed that it was easierto fool the system into assigning higher than lowerscores.Our goal here is to determine the extent to whichknowledge of the feature types deployed poses athreat to the validity of our system, where certaintext generation strategies may give rise to large pos-itive discrepancies.
As mentioned in Section 2, themarking criteria for FCE scripts are primarily basedon the accurate use of a range of different grammati-cal constructions relevant to specific communicativegoals, but our system assesses this indirectly.We extracted 6 high-scoring FCE scripts from theCLC that do not overlap with our training and testdata.
Based on the features used by our system andwithout bias towards any modification, we modifiedeach script in one of the following ways:i. Randomly order:(a) word unigrams within a sentence(b) word bigrams within a sentence(c) word trigrams within a sentence(d) sentences within a scriptii.
Swap words that have the same PoS within asentenceAlthough the above modifications do not ex-haust the potential challenges a deployed AA systemmight face, they represent a threat to the validity ofour system since we are using a highly related fea-ture set.
In total, we create 30 such ?outlier?
texts,which were given to an ESOL examiner for mark-ing.
Using the ?outlier?
scripts as well as their origi-nal/unmodified versions, we ran our system on eachmodification separately and calculated the correla-tion between the predicted values and the examiner?sscores.
Table 6 presents the results.The predicted values of the system have a highcorrelation with the examiner?s scores when testedon ?outlier?
texts of modification types i(a), i(b) andModificationPearson?s Spearman?scorrelation correlationi(a) 0.960 0.912i(b) 0.938 0.914i(c) 0.801 0.867i(d) 0.08 0.163ii 0.634 0.761Table 6: Correlation between the predicted values and theexaminer?s scores on ?outlier?
texts.i(c).
However, as i(c) has a lower correlation com-pared to i(a) and i(b), it is likely that a random order-ing of ngrams with N > 3 will further decrease per-formance.
A modification of type ii, where wordswith the same PoS within a sentence are swapped,results in a Pearson and Spearman correlation of0.634 and 0.761 respectively.Analysis of the results showed that our systempredicted higher scores than the ones assigned by theexaminer.
This can be explained by the fact that textsproduced using modification type ii contain a smallportion of correct sentences.
However, the markingcriteria are based on the overall writing quality.
Thefinal case, where correct sentences are randomly or-dered, receives the lowest correlation.
As our sys-tem is not measuring discourse cohesion, discrepan-cies are much higher; the system?s predicted scoresare high whilst the ones assigned by the examinerare very low.
However, for a writer to be able togenerate text of this type already requires significantlinguistic competence, whilst a number of genericmethods for assessing text and/or discourse cohe-sion have been developed and could be deployed inan extended version of our system.It is also likely that highly creative ?outlier?
essaysmay give rise to large negative discrepancies.
Recentcomments in the British media have focussed on thisissue, reporting that, for example, one deployed es-say marking system assigned Winston Churchill?sspeech ?We Shall Fight on the Beaches?
a low scorebecause of excessive repetition5.
Our model pre-dicted a high passing mark for this text, but not thehighest one possible, that some journalists clearlyfeel it deserves.5http://news.bbc.co.uk/1/hi/education/8356572.stm1866 Previous workIn this section we briefly discuss a number of themore influential and/or better described approaches.Pe?rez-Mar?
?n et al (2009), Williamson (2009), Dikli(2006) and Valenti et al (2003) provide a more de-tailed overview of existing AA systems.Project Essay Grade (PEG) (Page, 2003), one ofthe earliest systems, uses a number of manually-identified mostly shallow textual features, which areconsidered to be proxies for intrinsic qualities ofwriting competence.
Linear regression is used to as-sign optimal feature weights that maximise the cor-relation with the examiner?s scores.
The main is-sue with this system is that features such as wordlength and script length are easy to manipulate in-dependently of genuine writing ability, potentiallyundermining the validity of the system.In e-Rater (Attali and Burstein, 2006), textsare represented using vectors of weighted features.Each feature corresponds to a different property oftexts, such as an aspect of grammar, style, discourseand topic similarity.
Additional features, represent-ing stereotypical grammatical errors for example,are extracted using manually-coded task-specific de-tectors based, in part, on typical marking criteria.
Anunmarked text is scored based on the cosine simi-larity between its weighted vector and the ones inthe training set.
Feature weights and/or scores canbe fitted to a marking scheme by stepwise or lin-ear regression.
Unlike our approach, e-Rater mod-els discourse structure, semantic coherence and rel-evance to the prompt.
However, the system containsmanually developed task-specific components andrequires retraining or tuning for each new promptand assessment task.Intelligent Essay Assessor (IEA) (Landauer et al,2003) uses Latent Semantic Analysis (LSA) (Lan-dauer and Foltz, 1998) to compute the semantic sim-ilarity between texts, at a specific grade point, anda test text.
In LSA, text is represented by a ma-trix, where rows correspond to words and columnsto context (texts).
Singular Value Decomposition(SVD) is used to obtain a reduced dimension matrixclustering words and contexts.
The system is trainedon topic and/or prompt specific texts while test textsare assigned a score based on the ones in the trainingset that are most similar.
The overall score, which iscalculated using regression techniques, is based onthe content score as well as on other properties oftexts, such as style, grammar, and so forth, thoughthe methods used to assess these are not describedin any detail in published work.
Again, the systemrequires retraining or tuning for new prompts andassessment tasks.Lonsdale and Strong-Krause (2003) use a mod-ified syntactic parser to analyse and score texts.Their method is based on a modified version ofthe Link Grammar parser (Sleator and Templerley,1995) where the overall score of a text is calculatedas the average of the scores assigned to each sen-tence.
Sentences are scored on a five-point scalebased on the parser?s cost vector, which roughlymeasures the complexity and deviation of a sentencefrom the parser?s grammatical model.
This approachbears some similarities to our use of grammaticalcomplexity and extragrammaticality features, butgrammatical features represent only one componentof our overall system, and of the task.The Bayesian Essay Test Scoring sYstem(BETSY) (Rudner and Liang, 2002) uses multino-mial or Bernoulli Naive Bayes models to classifytexts into different classes (e.g.
pass/fail, grades A?F) based on content and style features such as wordunigrams and bigrams, sentence length, number ofverbs, noun?verb pairs etc.
Classification is basedon the conditional probability of a class given a setof features, which is calculated using the assumptionthat each feature is independent of the other.
Thissystem shows that treating AA as a text classifica-tion problem is viable, but the feature types are allfairly shallow, and the approach doesn?t make effi-cient use of the training data as a separate classifieris trained for each grade point.Recently, Chen et al (2010) has proposed an un-supervised approach to AA of texts addressing thesame topic, based on a voting algorithm.
Texts areclustered according to their grade and given an ini-tial Z-score.
A model is trained where the initialscore of a text changes iteratively based on its sim-ilarity with the rest of the texts as well as their Z-scores.
The approach might be better described asweakly supervised as the distribution of text gradesin the training data is used to fit the final Z-scores togrades.
The system uses a bag-of-words represen-tation of text, so would be easy to subvert.
Never-187theless, exploration of the trade-offs between degreeof supervision required in training and grading ac-curacy is an important area for future research.7 Conclusions and future workThough many of the systems described in Section6 have been shown to correlate well with examin-ers?
marks on test data in many experimental con-texts, no cross-system comparisons are available be-cause of the lack of a shared training and test dataset.Furthermore, none of the published work of whichwe are aware has systematically compared the con-tribution of different feature types to the AA task,and only one (Powers et al, 2002) assesses the easewith which the system can be subverted given someknowledge of the features deployed.We have shown experimentally how rank prefer-ence models can be effectively deployed for auto-mated assessment of ESOL free-text answers.
Basedon a range of feature types automatically extractedusing generic text processing techniques, our sys-tem achieves performance close to the upper boundfor the task.
Ablation tests highlight the contribu-tion of each feature type to the overall performance,while significance of the resulting improvements incorrelation with human scores has been calculated.A comparison between regression and rank prefer-ence models further supports our approach.
Prelim-inary experiments based on a set of ?outlier?
textshave shown the types of texts for which the system?sscoring capability can be undermined.We plan to experiment with better error detectiontechniques, since the overall error-rate of a script isone of the most discriminant features.
Briscoe etal.
(2010) describe an approach to automatic off-prompt detection which does not require retrainingfor each new question prompt and which we planto integrate with our system.
It is clear from the?outlier?
experiments reported here that our systemwould benefit from features assessing discourse co-herence, and to a lesser extent from features as-sessing semantic (selectional) coherence over longerbounds than those captured by ngrams.
The additionof an incoherence metric to the feature set of an AAsystem has been shown to improve performance sig-nificantly (Miltsakaki and Kukich, 2000; Miltsakakiand Kukich, 2004).Finally, we hope that the release of the trainingand test dataset described here will facilitate furtherresearch on the AA task for ESOL free text and, inparticular, precise comparison of different systems,feature types, and grade fitting methods.AcknowledgementsWe would like to thank Cambridge ESOL, a divisionof Cambridge Assessment, for permission to use anddistribute the examination scripts.
We are also grate-ful to Cambridge Assessment for arranging for thetest scripts to be remarked by four of their senior ex-aminers.
Finally, we would like to thank Marek Rei,?istein Andersen and the anonymous reviewers fortheir useful comments.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater v.2.
Journal of Technology, Learn-ing, and Assessment, 4(3):1?30.Burton H. Bloom.
1970.
Space/time trade-offs in hashcoding with allowable errors.
Communications of theACM, 13(7):422?426, July.E.J.
Briscoe, J. Carroll, and R Watson.
2006.
The secondrelease of the RASP system.
In ACL-Coling?06 In-teractive Presentation Session, pages 77?80, Sydney,Australia.E.J.
Briscoe, B. Medlock, and ?.
Andersen.
2010.
Au-tomated Assessment of ESOL Free Text Examinations.Cambridge University, Computer Laboratory, TR-790.Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,Martin Chodorow, Lisa Braden-Harder, and Mary DeeHarris.
1998.
Automated scoring using a hybrid fea-ture identification technique.
Proceedings of the 36thannual meeting on Association for Computational Lin-guistics, pages 206?210.YY Chen, CL Liu, TH Chang, and CH Lee.
2010.An Unsupervised Automated Essay Scoring System.IEEE Intelligent Systems, pages 61?67.Semire Dikli.
2006.
An overview of automated scoringof essays.
Journal of Technology, Learning, and As-sessment, 5(1).S.
Elliot.
2003.
IntelliMetric: From here to validity.
InM.D.
Shermis and J.C. Burstein, editors, Automatedessay scoring: A cross-disciplinary perspective, pages71?86.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukWaC, a very large web-derived corpus of English.188In S. Evert, A. Kilgarriff, and S. Sharoff, editors, Pro-ceedings of the 4th Web as Corpus Workshop (WAC-4).G.H.
Fischer and I.W.
Molenaar.
1995.
Rasch models:Foundations, recent developments, and applications.Springer.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many relevantfeatures.
In Proceedings of the European Conferenceon Machine Learning, pages 137?142.
Springer.Thorsten Joachims.
1999.
Making large scale SVMlearning practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods - Sup-port Vector Learning.
MIT Press.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the ACMConference on Knowledge Discovery and Data Mining(KDD), pages 133?142.
ACM.T.K.
Landauer and P.W.
Foltz.
1998.
An introduction tolatent semantic analysis.
Discourse processes, pages259?284.T.K.
Landauer, D. Laham, and P.W.
Foltz.
2003.
Au-tomated scoring and annotation of essays with the In-telligent Essay Assessor.
In M.D.
Shermis and J.C.Burstein, editors, Automated essay scoring: A cross-disciplinary perspective, pages 87?112.Deryle Lonsdale and D. Strong-Krause.
2003.
Auto-mated rating of ESL essays.
In Proceedings of theHLT-NAACL 2003 Workshop: Building EducationalApplications Using Natural Language Processing.Eleni Miltsakaki and Karen Kukich.
2000.
Automatedevaluation of coherence in student essays.
In Proceed-ings of LREC 2000.Eleni Miltsakaki and Karen Kukich.
2004.
Evaluationof text coherence for electronic essay scoring systems.Natural Language Engineering, 10(01):25?55, March.D.
Nicholls.
2003.
The Cambridge Learner Corpus: Er-ror coding and analysis for lexicography and ELT.
InProceedings of the Corpus Linguistics 2003 confer-ence, pages 572?581.E.B.
Page.
2003.
Project essay grade: PEG.
In M.D.Shermis and J.C. Burstein, editors, Automated essayscoring: A cross-disciplinary perspective, pages 43?54.D.
Pe?rez-Mar?
?n, Ismael Pascual-Nieto, and P.
Rodr??guez.2009.
Computer-assisted assessment of free-textanswers.
The Knowledge Engineering Review,24(04):353?374, December.D.E.
Powers, J.C. Burstein, M. Chodorow, M.E.
Fowles,and K. Kukich.
2002.
Stumping e-rater: challengingthe validity of automated essay scoring.
Computers inHuman Behavior, 18(2):103?134.L.M.
Rudner and Tahung Liang.
2002.
Automated essayscoring using Bayes?
theorem.
The Journal of Tech-nology, Learning and Assessment, 1(2):3?21.L.M.
Rudner, Veronica Garcia, and Catherine Welch.2006.
An Evaluation of the IntelliMetric Essay Scor-ing System.
Journal of Technology, Learning, and As-sessment, 4(4):1?21.D.D.K.
Sleator and D. Templerley.
1995.
Parsing En-glish with a link grammar.
Proceedings of the 3rd In-ternational Workshop on Parsing Technologies, ACL.AJ Smola.
1996.
Regression estimation with supportvector learning machines.
Master?s thesis, TechnischeUniversita?t Munchen.J.H.
Steiger.
1980.
Tests for comparing elements of acorrelation matrix.
Psychological Bulletin, 87(2):245?251.Salvatore Valenti, Francesca Neri, and Alessandro Cuc-chiarelli.
2003.
An overview of current researchon automated essay grading.
Journal of InformationTechnology Education, 2:3?118.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer.E.
J. Williams.
1959.
The Comparison of RegressionVariables.
Journal of the Royal Statistical Society.
Se-ries B (Methodological), 21(2):396?399.DM Williamson.
2009.
A Framework for Implement-ing Automated Scoring.
In Annual Meeting of theAmerican Educational Research Association and theNational Council on Measurement in Education, SanDiego, CA.189
