Learning a Radically Lexical GrammarDanny Solomon Mary McGee WoodDepartment of Computer ScienceUniversity of ManchesterManchester M13 9PL, UK{ danny, mary } @cs.
man.
ac.
ukAbstractWe describe a prototype systemwhich induces a categorialgrammar from a simple textcorpus of children's readingbooks.
Unlike previous attemptsat grammar induction, (I) thereare no rules of grammar, only arichly structured lexicon; (2) werely both on an informinglinguistic theory and on statisticalmethods applied to a corpus.1 In t roduct ion  - Learn ing  a'Grammar'The work we describe was originallymotivated by dissatisfaction withattempts to induce a rule-basedgrammar from a corpus (eg Berwick1985), and the suspicion that a word-based grammar - at the logical extreme,a categorial grammar (Wood 1993) -might be easier to 'learn'.
With therecent rapid growth of 'empiricism' inNLP, the same work now manifests122our dissatisfaction with the view thatthrowing a large computer at a largecorpus to produce a large set of sets ofnumbers is in itself 'linguistics': weare committed to the honestdescription of 'real 'data, but onlywithin an independently motivatedtheoretical framework.Neither of these are widely heldviews at present.
For the first, evenBrent's (1993) ' learning of lexicalsyntax' takes a distinct component ofgrammar ules as its starting point inlearning a lexicon.
For the second, asan example, the tension whichpervaded the EACL meet ing inUtrecht in April  1993, stoked byinvited talks from Church (1993) andSag (1993), made clear the deepdivision - even antagonism - betweenstatisticians and theorists on the frontline.
However the success o far of ourimplementation gives some evidencefor the viability of a totally lexicalapproach to the automatic learning ofg rammar ,  respons ive  to bothprinciples and facts.The next section describes furthersome of the theoretical and practicalmotivations behind our work.
Section3 briefly sets out the exact categorialgrammar used.
In Section 4 wedescribe L - a prototype grammarinduction system which illustrates thedirection we believe will beproductive.
A discussion and a looktowards some prospects complete thepaper.2 Mot ivat ion2.1 Theo~ticalIt is hard to imagine an automatic (oreven semi-automatic) grammar orlexicon induction procedure that couldmanipulate traditional grammaticalentities such that human-meaningfulresults might obtain.
Brute-forcemethods (ie those that exploit themassive raw computing powercurrently available cheaply) may wellproduce some useful results (eg Brownet al1993).
However, ff any linguisticinsight is to appear as a result, webelieve an underpinning in linguistictheory is essential - overall success willresult from the combination andintegration of that at which computersand human linguists excel.
Webelieve this is only possible if bothpartners work in the same basicframework - and we also believe thattraining linguists to read disk sectorswill probably be unproductive.Our premise is that:1) giving grammatical entitiesstructure is useful - for providinga framework to describe them tocomputers, and, e.g., forgenerating (automatically) ataxonomy of such entities;2) such a structure can embody thegrammar of a language - all thenecessary linguistic knowledgecan be incorporated into thestructure of the grammaticalentity (or category) of each wordin a language;3) given 1 & 2, we can demonstratethat a semi-automatic means forinferring a structural lexicon(equivalent to a grammar) from acorpus of natural language ispossible.To illustrate 1 & 2, we suggest hatthe lexicon can be unified withgrammar, using the theoreticalframework of Categorial Grammar(CG).
CG is recognized withinlinguistic theory as the logical ultimatein 'lexical syntax', a model in whichall syntactic information is held in thelexical categories of individual words,and there is no separate component of'grammar ules' - thus Kartunnen's(1989) 'radical exicalism' (from whichour title derives).
A CG inductionsystem has been built which providesevidence for 3.2.2 PracticalTwo major challenges of NLP systemsare to support wide-ranging anddeveloping vocabularies, and to workwith more than strictly syntacticallycorrect 'input.
We believe that bothcan be addressed by (at least semi-)automatic lexicon growth, orinduction.Imagine an application that operateson 'real-world' textual input.
Forexample, in the medical domain, asystem that produces a semanticanalysis of free-text hospital dischargesummaries.
A fixed lexicon is clearly123impracticable.
Practitioners shouldnot (and indeed will not) accept anyartificiafly imposed limited vocabulary- it is simply not appropriate for theirtask.
There are (at least) twoimaginable strategies for handling thesituation when a previously unknownword is encountered (either whenencountering new domains, or extracomplexity in a 'known' domain):I) Bail out, and ask alinguist/lexicographer tomanua l ly  augment  thelexicon/grammar rules2) Have the NLP  system itself makesensible and useful suggestions asto the new word's syntacticcategory (and, potentially, itsconceptual structure).Similarly, there are (at least) threepotential strategies for dealing withthe situation in which a systemencounters 'syntactically incorrect'input - but for which an error messageis neither useful or appropriate - theinput comes from something that hasactually been said.
These threestrategies are:1) Throw it out;2) Invent a new rule to cope withthe particular scenario;3) Augment  the lexicon wi thadditional categories.We believe that augmenting thelexicon is the only realistic approach tothis problem.An  experimental prototype of aCategorial Grammar induction system(known as L) has been produced whichillustrates and encourages our beliefthat a lexicon in which richlystructured entities are the means ofencoding syntactic and semanticknowledge can be 'grown' to meetthese demands.1 243 Categor ia l  GrammarThe CG induced by L is a simple oneon the scale of categorial calculi.
Ituses three atomic categories, S, N, andNP (we wifl return at the end of thispaper to discuss the limitations of thisatomic notation); two connectives: /( forward combinat ion)  and \(backward combination); and threecombinat ion  ru les :  fo rwardapplication, backward application, andforward composition.
The notationused is consistently result-f irst,regardless of the direction of theconnective.
Complex categories areimplicitly left-bracketed, i.e.
S\NP/NPis equivalent to (S\NP)/NP.Complex categories are functionsnamed by their arguments andoutputs; there is no concept of 'verb',but rather of a function from onenominal to a sentence (S \NP ,'intransitive verb'), from twonominals to a sentence (S\NP/NP,'transitive verb'), and so on.
This'combinatory transparency', theirvisible information structure, makesCGs well suited to corpus-basedinduction of a lexicon/grammar.We rely on the property of'parametric neutrality' (Pareschi 1986)- not only can we determine the resultof combining two known categories,but from one category and the resultwe can determine the second category.Thus:Given NP S\NP -> X then X = 5;Given X S\NP -> S then X = NP;Given NP X -> S then X = S\NP.Each category assignment we inducegives us more information to helpwith the next (as will be seen below);unlike traditional categories, thesehave a rich information structurewhich we can query for help inmaking further decisions.
We cantherefore approach a text knowingonly the identity of nouns andsentences and the principles offunction application and composition,and from these we can induce thecomplex categories of the other wordsin the sentence; in other words, we canlearn the lexicon, and in it thegrammar.4 L - a Categorial Granuna~Induction System4.1 OverviewL is a simple Categorial Grammargrammar induction system, based onholding theoretically motivated andempirically demonstrated linguisticinformation by representing wordsand their behaviour as complexstructured objects in a format readableby both human and machine.
Theinput is a simple text corpus in whichthe boundaries of sentences and,initially, the identity of a few nounsare known.
The output - the result ofthe induction process - is a set oflexical categories for all the words inthe corpus - which, as we haveexplained, constitutes a grammar forthe corpus.
This is made possible bythe characteristic of 'parametricneutrality' described above.
Manywords have multiple categories (eg'toy', as N 'noun' or N/N 'adjective') -this is how ambiguity is handled.
Lsuccessfully infers multiple categoriesfor many words in the corpus, and,critically, the categories it proposes domake cognitive sense to human125linguists.
This gives us hope that thesystem could be usefully guided andhelped by humans in what is - in thelimit - a difficult task: categoryassignments proposed by the systemcan be readily evaluated by its user.The lexicon induced by L has beenused successfully, as a test, for simpletext generation.The system is implemented inCProlog on a SUN 3/50 workstation.4.2 The CorpusThe corpus used is a selection of booksfrom the Ladybird Key Words ReadingScheme, a series of books designed tohelp children learn to read, ordered ina graded sequence which was followedby the system.
The system is'bootstrapped' by a few examples ofprimitive categories - this is anexample of where we feel that bestresults are obtained by not hog-tyingthe system for its own sake.
Sentenceboundaries are given by punctuation.A few nouns are defined in the corpusitself: the first books in the series beginwith a sort of 'picture dictionary' oftheir central characters and objects,and we gave this starting point to thesystem also.
Notice that this fitsexactly the use of S and N(P) as atomiccategories in CG.We are encouraged here that ourapproach also has some psychologicalplausibility.
Children learning alanguage do so by learning the namesof things first, then how those namescan be fitted together intopropositions.
That a sequencedesigned to help human learnersshould prove suitable for teaching acomputer suggests that they may beworking along similar lines.An interesting side-effect of thischoice is that the corpus is oftensyntactically odd - it was designed tohelp children's reading, rather than toteach grammar.
However, the successof L on such an 'unsyntactic' (thoughunderstandable) corpus gives promisefor its application on other 'real-world' corpora - real text and perhapsspoken word.
(It must be admittedthat L was completely baffled, inreading 'The Three Billy Goats Gruff,by the 'sentence' 'Trip trap, trip trap,trip trap!')
- but this is surely allowableat this stage as an extreme case.
)4.3 Principles of OperationL works due to a combination ofcomputer  processing power andstatistical evidence applied to anunderlying linguistic theory - in ourview, the 'best of both worlds'.It is a simple system; this simplicityitself we  regard as a significantachievement.In the description which follows,some example output from a simpletext-based interface to L is included toillustrate the various processesinvolved.The system has a few "boot-strapped'primitive categories, as explainedabove.
A multi-pass iterativeapproach is used to analyse andfurther annotate the corpus - the firstpass uses just the few identifiedcategories.
On  each pass, L assignscategories to more words of the corpus.
(The strategy bears some resemblanceto island-based parsing, whichsimilarly begins with the point(s) ofgreatest certainty in an input stringand works outwards from them.)
Eachpass has three parts:1261) The system selects which word totry to categorise in this pass.
It usesstatistical evidence to choose foranalysis the word which occurs in thecorpus with the most consistent,already categorised, (immediate)nearest neighbours.
Clearly aprecondition for this approach is tohave at least some categorised wordsin the corpus - having some boot-strapped categories enables L toembark in a sensible direction.2) Assign a category.
If the word to beassigned is the last remaininguncategorised word in a sentence, thenthe principle of parametric neutralityis applied.
Due to the compositional,recursive nature of CategorialGrammar categories, L can always finda category to fit.
For example:Pass 4 ...missing link completionassigning 's\np/np' to 'likes'with a confidence of 14/16EXAMPLE:original sentence : Peter likesthe ball.before this pass : np likes npnow reduced to : sNote that the assignment found isonly applied to those instances in thecorpus which are both sanctioned byCG, and 'deemed appropriate' by Litself- in this case, 14 out of 16occurrences.
This mechanism allowsL the opportunity of assigningmultiple different categories to a word,to cope with ambiguity.Note also that the information givenby the 'confidence' measure is morethan the probability which would beexpressed by reducing it to 'one in ...'.16/16 indicates a common (in thissmall corpus) and unambiguous word,while a word given I/I has beenfound a category on its oneappearance; a word with a rating of10/15 is established as regularlyambiguous, but 2/3 could prove onfurther exposure to be mainly regular,with only one occurence of analternative category.
These degrees ofprobability are taken into account bythe algorithm which assigns categoriesin new text.If the word is not the last in thesentence to be categorised, a morecomplex approach is required.
Theargument and direction of theresulting category is obtained as a by-product of the previous stage - theresult is determined by an?
examination of the 'behaviour' of theresulting category in the corpus.
Inthis context, 'behaviour' is defined asthe pattern of neighbours' categoriesin the corpus so far.
For example:Pass 5 .
.
.assigning 'np/n' to 'jane's'with a confidence of 5/6EXAMPLE :or ig inal  sentence : Here isjane's shop.before this pass : Here isjane's nnow reduced to : Here is npIn this case, the word jane's ischosen because, informally, itfrequently occurs before a N. This tellsus that the category to assign musthave an argument of N, and thedirection must be forward - in otherwords a x/N.
This category iscompleted with a NP  because this is areasonable behavioural match withthe rest of the corpus NP  often127appears after is.
Note that statisticalevidence is again essential.3) Having obtained a putativecategory, this is then applied to theoccurrences of the word in the corpusas long as it is sanctioned by thesemantics of CG.
In this way,ambiguity is captured - only thoseoccurrences which 'fit' are categorisedat each pass.4.4.
Results and EvaluationL's initial corpus was books la and lbof the Ladybird Key Words ReadingScheme.
They contain 351 wordtokens, using a vocabulary of about 20different words.
This corpus wascompletely processed in 17 passes.Processing later books in the series hasbrought L's current vocabulary up tosome 55 words.
This is still small, ofcourse, but the nature of the inductionprocess means that growth should'snowball' as each known word helpsin the categorization of further newwords.
(And see Shieber quoted inRitchie (1987) for a revealingdiscussion of the vocabulary sizes ofmost NLP  research prototypes).Within this limited vocabulary, Lhas 'correctly' induced examples of thefollowing categories: determiners,adjectives, prepositions, conjunctions,intransitive, transitive and di-transitive verbs, imperatives, andsome auxiliaries.
Furthermore, Ldiscovers and represents ambiguity ofthe following types: adjective vs noun;sentence co-ordination vs noun-phrase co-ordination; prepositionalform; noun-phrase vs determiner, andverbs of quotation.
An example of thelatter are four structural forms that Linduces for says (as in 'Rhubarbrhubarb says Jane' or 'Jane saysrhubarb rhubarb').L has also inadvertently re-inventedtype-raising, assigning the category(S/(S\NP))/N to a sentence-initialdeterminer: the system's exact methodof exploiting parametric neutrality toldit that this word needed a followingnoun to form a function into asentence from a following 'verbphrase'.
A slightly different algorithmwould have given the more standardNP/N, and reduction mechanismscould easily be implemented to findsimpler equivalents, where possible, ofhighly complex proposed categories.Indeed they will almost certainly beneeded, as witness L's assignment ofthe category:S\NP/(S\NP\S)/(S\NP\NP)to you as the last uncategorized wordin the sentence 'Here you are Jane saysPeter'.
)Evaluation of the results took twoforms:I) Use of the lexicon for generation.Many  different lexicons couldhave been produced which wouldaccount only for the trainingcorpus.
Using the lexicon forgeneration of new text providedevidence that it was moregeneral.
The text generated was incharacter with the corpus - forexample, 'Peter you are in it saysr.
This is an important result; wehave evidence that the grammarcreated is general, but does notover-generate.2) Inspection.
L producedcognitively plausible results - i.e.as well as producing categoriesthat enable the entire corpus toreduce to a sequence of Ss, theresults reflect what  aretraditionally (manually) assigned128to each word - for a wide range ofsyntactic constructions, providingfurther evidence that the lexiconproduced is not just corpus-specific.5 Discussion and ProspectsL, as an experimental prototype, hasdemonstrated the practicability of aradically lexical approach formanaging some of the majorchallenges for NLP.
It learns thegrammar of (the words of) new textand represents what it has learned instructured linguistic entities which arereadable equally by computer andcomputational linguist.
Its onlystarting point is the general rule offunction application and the identityof a few nouns and sentences.
Wefirmly believe that this minimalistmethodology - assume as little aspossible, and use principles which areas general as possible - is sound.L copes with that bugbear of NLP  -ambiguity; it successfully infersmultiple categories for many words inthe corpus, and, critically, thecategories it has so far proposed havebeen hand-checked and do makecognitive sense to human linguists.This makes us optimistic as to the easewith which such systems and theirusers will be able to co-operate.
Thelexicon induced by L has been usedsuccessfully, as a test, for simple textgeneration.Obviously there is a great deal ofwork still to be done.
The simpleatomic S, N, NP  category notationused by L cannot represent finer-grained morphological informationsuch as number, case, gender, andtense.
This is clearly shown by the firstsentence L generated: 'I likes r. Ourfirst priority is therefore to introduce amore complex notation, probablyusing bundles of attribute/value pairsin the style of Unification CategorialGrammar (Zeevat 1988).
(Clearly ourcomments at the beginning of thispaper about the value of structuredrepresentations apply afortiori herealso.)
We expect that a minimalexplicit seeding of the corpus withthese values will allow the system to'learn' them also.Secondly, lexical semantics has notyet been addressed.
The move to afeature/value notation will alsoprovide a framework in which this ispossible.
The seeding and learningwill be a more difficult task, whichawaits investigation.
A log of whichparticular words regularly co-occurshould at least help in automaticallyestablishing broad semantic fields -again, we expect a judicious balance ofstatistical and theoretical informationto be appropriate here.Thirdly - once we have established anotation adequate to these demands -we will grow the lexicon/grammar byprocessing further texts of increasingcomplexity - first within the Ladybirdseries, but going on to real-world text,possibly in a medical domain.
We donot anticipate serious difficulty inprogressing through the Ladybirdseries, but we are well aware that thereis a significant step from there to'adult' text, at which point scaling upmay well not be trivial.We have mentioned in passing thatit is encouraging from a psychologicalperspective to find that a text corpusdesigned to aid human learningshould prove well suited to machinelearning.
Of course learning to read aknown language is a different task129from learning a language.
Howeverwe hope eventually to explore thepotential of our approach formodelling children's learning, andperhaps the use of its text generationability in producing teaching material.The success of our approach - at leastat prototype level - should becontrasted with other attempts atgrammar induction.
Some, typically,use traditional atomic 'grammaticalcategories' with no inherentinformation content, mapped incomplex ways (which must also belearned) onto a large set of 'grammarrules'.
Others 'learn' columns ofnumbers which could equally welldescribe the co-occurrence of bird-tracks in snow with various gardenshrubs.
To quote Pustejovsky et at(1993:354), 'statistical resultsthemselves reveal nothing, andrequire careful and systematicinterpretation by the investigator tobecome linguistic data.
'L is inspired and informed by anindependently motivated andrespected theory of natural anguage,and depends for its realization on acorpus of real-world text.
Ourtheoretical understanding of howwords combine gives us a principledway into a text corpus; statisticalevidence suggests and confirms thebehaviour of words and thereforetheir" lexical/grammatical tegories.NLP appears currently to be split bycivil war between theorists with soundprinciples but no real data andstatisticians with volumes of data butno linguistic principles.
There willonly be significant progress with realprospects in NLP when the theory-driven and empiricist approachesrespect each other and work together.We hope we have shown one way inwhich this can be done.ReferencesBerwick, Robert.
1985.
The Acquistionof Syntactic Knowledge.
MIT Press,Cambridge, Mass.Brent, Michael.
1993.
From Grammarto Lexicon: Unsupervised Learningof Lexical Syntax.
ComputationalLinguistics 19.2 pp 243-262.Brown, Peter F., Stephen A. DellaPietra, Vincent J Della Pietra, andRobert L. Mercer.
1993.
TheMathematics of MachineTranslation: Parameter Estimation.Computational Linguistics 19.2 pp263-311.Church, Ken.
1993.
Termworks:Toolsfor Human Translators.
Invited talk,Sixth Conference of the EuropeanChapter of the Association forComputational Linguistics, Utrecht,21 April 1993.Kartunnen, Lauri.
1989.
RadicalLexicalism.
In Baltin & Kroch (eds).Alternative Conceptions of PhraseStructure.
University of ChicagoPress.Pareschi, Remo.
1986.
Combinatorygrammar, logic programming andnatural language processing.
Ms,Dept of Artificial Intelligence,University of Edinburgh.Pustejovsky, James, Sabine Bergler,and Peter Anick.
1993.
LexicalSemantic Techniques for CorpusAnalysis.
Computational Linguistics19.2 pp 331-358.130Ritchie, Graeme.
1987.
The Lexicon.
InWhitelock, Wood, Somers,Johnson, and Bennett (eds).Linguistic Theory and ComputerApplications.
Academic Press,London.Sag, Ivan.
Extraction without traces,empty COMPs or functioncomposition.
Invited talk, SixthConference of the European Chapterof the Assoc iat ion forComputational Linguistics, Utrecht,22 April 1993.Solomon, W.D.
1991 Learning aGrammar.
University of ManchesterDept of Computer Science TechnicalReport UMCS-AI-91-12-1.Wood, Mary McGee.
1993.
CategorialGrammars.
Routledge, London.Zeevat, Henk.
1988.
Combin ingcategorial grammar and unificationgrammar.
In Reyle & Rohrer (eds).Natural Language Parsing andLinguistic Theories.
Reidel,Dordrecht.
