Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 321?326,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLinguistic Considerations in Automatic Question GenerationKaren MazidiHiLT LabUniversity of North TexasDenton TX 76207, USAKarenMazidi@my.unt.eduRodney D. NielsenHiLT LabUniversity of North TexasDenton TX 76207, USARodney.Nielsen@unt.eduAbstractAs students read expository text, compre-hension is improved by pausing to answerquestions that reinforce the material.
Wedescribe an automatic question generatorthat uses semantic pattern recognition tocreate questions of varying depth and typefor self-study or tutoring.
Throughout, weexplore how linguistic considerations in-form system design.
In the described sys-tem, semantic role labels of source sen-tences are used in a domain-independentmanner to generate both questions and an-swers related to the source sentence.
Eval-uation results show a 44% reduction in theerror rate relative to the best prior systems,averaging over all metrics, and up to 61%reduction in the error rate on grammatical-ity judgments.1 IntroductionStudies of student learning show that answeringquestions increases depth of student learning, fa-cilitates transfer learning, and improves students?retention of material (McDaniel et al, 2007; Car-penter, 2012; Roediger and Pyc, 2012).
The aimof this work is to automatically generate questionsfor such pedagogical purposes.2 Related WorkApproaches to automatic question generation fromtext span nearly four decades.
The vast ma-jority of systems generate questions by select-ing one sentence at a time, extracting portionsof the source sentence, then applying transfor-mation rules or patterns in order to construct aquestion.
A well-known early work is Wolfe?sAUTOQUEST (Wolfe, 1976), a syntactic patternmatching system.
A recent approach from Heil-man and Smith (2009, 2010) uses syntactic pars-ing and transformation rules to generate questions.Syntactic, sentence-level approaches outnumberother approaches as seen in the Question Gen-eration Shared Task Evaluation Challenge 2010(Boyer and Piwek, 2010) which received only oneparagraph-level, semantic entry.
Argawal, Shahand Mannem (2011) continue the paragraph-levelapproach using discourse cues to find appropriatetext segments upon which to construct questionsat a deeper conceptual level.
The uniqueness oftheir work lies in their use of discourse cues toextract semantic content for question generation.They generate questions of types: why, when, givean example, and yes/no.In contrast to the above systems, other ap-proaches have an intermediate step of transform-ing input into some sort of semantic represen-tation.
Examples of this intermediate step canbe found in Yao and Zhang (2010) which usesMinimal Recursive Semantics, and in Olney etal.
(2012) which uses concept maps.
These ap-proaches can potentially ask deeper questions dueto their focus on semantics.
A novel question gen-erator by Curto et al (2012) leverages lexico-syntactic patterns gleaned from the web with seedquestion-answer pairs.Another recent approach is Lindberg et al(2013), which used semantic role labeling to iden-tify patterns in the source text from which ques-tions can be generated.
This work most closelyparallels our own with a few exceptions: our sys-tem only asks questions that can be answeredfrom the source text, our approach is domain-independent, and the patterns also identify the an-swer to the question.3 ApproachThe system consists of a straightforward pipeline.First, the source text is divided into sentenceswhich are processed by SENNA1software, de-1http://ml.nec-labs.com/senna/321scribed in (Collobert et al, 2011).
SENNA pro-vides the tokenizing, pos tagging, syntactic con-stituency parsing and semantic role labeling usedin the system.
SENNA produces separate seman-tic role labels for each predicate in the sentence.For each predicate and its associated semantic ar-guments, a matcher function is called which willreturn a list of patterns that match the source sen-tence?s predicate-argument structure.
Then ques-tions are generated and stored by question type ina question hash table.Generation patterns specify the text, verb formsand semantic arguments from the source sentenceto form the question.
Additionally, patterns indi-cate the semantic arguments that provide the an-swer to the question, required fields, and filter con-dition fields.
As these patterns are matched, theywill be rejected as candidates for generation for aparticular sentence if the required arguments areabsent or if filter conditions are present.
For ex-ample, a filter for personal pronouns will preventa question being generated with an argument thatstarts with a personal pronoun.
From: It meansthat the universe is expanding, we do not want togenerate a vague question such as: What does itmean?
Coreference resolution, which could helpavoid vague question generation, is discussed inSection 5.
Table 1 shows selected required and fil-ter fields, Section 3.3 gives examples of their use.Patterns specify whether verbs should be in-cluded in their lexical form or as they appear in thesource text.
Either form will include subsequentparticles such as: The lungs take in air.
The mostcommon use of the verb as it appears in the sen-tence is with the verb be, as in: What were fusedinto helium nuclei?
This pattern takes the copu-lar be as it appears in the source text.
However,most patterns use the lexical form of the main verbalong with the appropriate form of the auxiliary do(do, does, did), for the subject-auxiliary inversionrequired in forming interrogatives.3.1 Pattern AuthoringThe system at the time of this evaluation had 42patterns.
SENNA uses the 2005 PropBank cod-ing scheme and we followed the documentation in(Babko-Malaya, 2005) for the patterns.
The mostcommonly used semantic roles are A0, A1 and A2,as well as the ArgM modifiers.22Within PropBank, the precise roles of A0 - A6 vary bypredicate.Field MeaningAx Sentence must contain an Ax!Ax Sentence must not contain an AxAxPER Ax must refer to a personAxGER Ax must contain a gerundAxNN Ax must contain nouns!AxIN Ax cannot start with a preposition!AxPRP Ax cannot start with per.
pronounV=verb Verb must be a form of verb!be Verb cannot be a form of benegation Sentence cannot contain negationTable 1: Selected required and filter fields (Ax is asemantic argument such as A0 or ArgM)3.2 Software Tools and Source TextThe system was created using SENNA andPython.
Importing NLTK within Python providesa simple interface to WordNet from which we de-termine the lexical form of verbs.
SENNA pro-vided all the necessary processing of the data,quickly, accurately and in one run.In order to generate questions, passages wereselected from science textbooks downloaded fromwww.ck12.org.
Textbooks were chosen ratherthan hand-crafted source material so that a morerealistic assessment of performance could beachieved.
For the experiments in this paper, weselected three passages from the subjects of bi-ology, chemistry, and earth science, filtering outreferences to equations and figures.
The passagesaverage around 60 sentences each, and representchapter sections.
The average grade level is ap-proximately grade 10 as indicated by the on-linereadability scorer read-able.com.3.3 ExamplesTable 2 provides examples of generated questions.The pattern that generated Question 1 requires ar-gument A1 (underlined in Table 2) and a causationArgM (italicized).
The pattern also filters out sen-tences with A0 or A2.
The patterns are designedto match only the arguments used as part of thequestion or the answer, in order to prevent overgeneration of questions.
The system inserted thecorrect forms of release and do, and ignored thephrase As this occurs since it is not part of the se-mantic argument.The pattern that generated Question 2 requiresA0, A1 and a verb whose lexical form is mean(V=mean in Table 1).
In this pattern, A1 (itali-322Question 1: Why did potential energy release?Answer: because the new bonds have lower potential energy than the original bondsSource: As this occurs, potential energy is released because the new bonds have lower potentialenergy than the original bonds.Question 2: What does an increased surface area to volume ratio indicate?Answer: increased exposure to the environmentSource: An increased surface area to volume ratio means increased exposure to the environment.Question 3: What is another term for electrically neutral particles?Answer: neutronsSource: The nucleus contains positively charged particles called protons andelectrically neutral particles called neutrons.Question 4: What happens if you continue to move atoms closer and closer together?Answer: eventually the two nuclei will begin to repel each otherSource: If you continue to move atoms closer and closer together, eventually the two nuclei willbegin to repel each other.Table 2: Selected generated questions with source sentencescized) forms the answer and A0 (underlined) be-comes part of the question along with the appro-priate form of do.
This pattern supplies the wordindicate instead of the source text?s mean whichbroadens the question context.Question 3 is from the source sentence?s 3rdpredicate-argument set because this matched thepattern requirements: A1, A2, V=call.
The answeris the text from the A2 argument.
The ability togenerate questions from any predicate-argumentset means that sentence simplification is not re-quired as a preprocessing step, and that the sen-tence can match multiple patterns.
For example,this sentence could also match patterns to gener-ate questions such as: What are positively chargedparticles called?
or Describe the nucleus.Question 4 requires A1 and an ArgM that in-cludes the discourse cue if.
The ArgM (under-lined) becomes part of the question, while the restof the source sentence forms the answer.
This pat-tern also requires that ArgM contain nouns (AxNNfrom Table 1), which helps filter vague questions.4 ResultsThis paper focuses on evaluating generated ques-tions primarily in terms of their linguistic quality,as did Heilman and Smith (2010a).
In a relatedwork (Mazidi and Nielsen, 2014) we evaluatedthe quality of the questions and answers from apedagogical perspective, and our approach outper-formed comparable systems in both linguistic andpedagogical evaluations.
However, the task hereis to explore the linguistic quality of generatedquestions.
The annotators are university studentswho are science majors and native speakers of En-glish.
Annotators were given instructions to read aparagraph, then the questions based on that para-graph.
Two annotators evaluated each set of ques-tions using Likert-scale ratings from 1 to 5, where5 is the best rating, for grammaticality, clarity, andnaturalness.
The average inter-annotator agree-ment, allowing a difference of one between theannotators?
ratings was 88% and Pearson?s r=0.47was statistically significant (p<0.001), suggestinga high correlation and agreement between annota-tors.
The two annotator ratings were averaged forall the evaluations reported here.We present results on three linguistic evalua-tions: (1) evaluation of our generated questions,(2) comparison of our generated questions withthose from Heilman and Smith?s question gener-ator, and (3) comparison of our generated ques-tions with those from Lindberg, Popowich, Nesbitand Winne.
We compared our system to the H&Sand LPN&W systems because they produce ques-tions that are the most similar to ours, and for thesame purpose: reading comprehension reinforce-ment.
The Heilman and Smith system is availableonline;3Lindberg graciously shared his code withus.4.1 Evaluation of our Generated QuestionsThis evaluation was conducted with one file(Chemistry: Bonds) which had 59 sentences, fromwhich the system generated 142 questions.
The3http://www.ark.cs.cmu.edu/mheilman/questions/323purpose of this evaluation was to determine if anypatterns consistently produce poor questions.
Theaverage linguistics score per pattern in this evalu-ation was 5.0 to 4.18.
We were also interested toknow if first predicates make better questions thanlater ones.
The average score by predicate positionis shown in Table 3.
Note that the Rating columngives the average of the grammaticality, clarity andnaturalness scores.Predicate Questions RatingFirst 58 4.7Second 35 4.7Third 23 4.5Higher 26 4.6Table 3: Predicate depth and question qualityBased on this sample of questions there isno significant difference in linguistic scores forquestions generated at various predicate positions.Some question generation systems simplify com-plex sentences in initial stages of their system.
Inour approach this is unnecessary, and simplifyingcould miss many valid questions.4.2 Comparison with Heilman and SmithThis task utilized a file (Biology: the body) with56 source sentences from which our system gener-ated 102 questions.
The Heilman and Smith sys-tem, as they describe it, takes an over-generate andrank approach.
We only took questions that scoreda 2.0 or better with their ranking system,4whichresulted in less than 27% of their top questions.In all, 84 of their questions were evaluated.
Thequestions again were presented with accompany-ing paragraphs of the source text.
Questions fromthe two systems were randomly intermingled.
An-notators gave 1 - 5 scores for each category ofgrammaticality, clarity and naturalness.As seen in Table 4, our results represent a 44%reduction in the error rate relative to Heilman andSmith on the average rating over all metrics, andas high as 61% reduction in the error rate on gram-maticality judgments.
The error reduction calcu-lation is shown below.
Note that rating?is themaximum rating of 5.0.ratingsystem2?
ratingsystem1rating??
ratingsystem1?
100.0 (1)4In our experiments, their rankings ranged from verysmall negative numbers to 3.0.System Gram Clarity Natural AvgH&S 4.38 4.13 3.94 4.15M&N 4.76 4.26 4.53 4.52Err.
Red.
61% 15% 56% 44%Table 4: Comparison with Heilman and SmithSystem Gram Clarity Natural AvgLPN&W 4.57 4.56 4.55 4.57M&N 4.80 4.69 4.78 4.76Err.
Red.
54% 30% 51% 44%Table 5: Comparison with Lindberg et al4.3 Comparison with Lindberg et alFor a comparison with the Lindberg, Popowich,Nesbit and Winne system we used a file (Earthscience: weather fronts) that seemed most sim-ilar to the text files for which their system wasdesigned.
The file has 93 sentences and our sys-tem generated 184 questions; the LPN&W sys-tem generated roughly 4 times as many questions.From each system, 100 questions were randomlyselected, making sure that the LPN&W questionsdid not include questions generated from domain-specific templates such as: Summarize the influ-ence of the maximum amount on the environment.The phrases Summarize the influence of and onthe environment are part of a domain-specific tem-plate.
The comparison results are shown in Table5.
Interestingly, our system again achieved a 44%reduction in the error rate when averaging over allmetrics, just as it did in the Heilman and Smithcomparison.5 Linguistic ChallengesNatural language generation faces many linguisticchallenges.
Here we briefly describe three chal-lenges: negation detection, coreference resolution,and verb forms.5.1 Negation DetectionNegation detection is a complicated task becausenegation can occur at the word, phrase or clauselevel, and because there are subtle shades of nega-tion between definite positive and negative polar-ities (Blanco and Moldovan, 2011).
For our pur-poses we focused on negation as identified by theNEG label in SENNA which identified not in verbphrases.
We have left for future work the task of324identifying other negative indicators, which occa-sionally does lead to poor question/answer qualityas in the following:Source sentence: In Darwin?s time and to-day, many people incorrectly believe that evolu-tion means humans come from monkeys.Question: What does evolution mean?Answer: that humans come from monkeysThe negation in the word incorrectly is not iden-tified.5.2 Coreference ResolutionCurrently, our system does not use any type ofcoreference resolution.
Experiments with existingcoreference software performed well only for per-sonal pronouns, which occur infrequently in mostexpository text.
Not having coreference resolutionleads to vague questions, some of which can befiltered as discussed previously.
However, furtherwork on filters is needed to avoid questions suchas:Source sentence: Air cools when it comes intocontact with a cold surface or when it rises.Question: What happens when it comes intocontact with a cold surface or when it rises?Heilman and Smith chose to filter out ques-tions with personal pronouns, possessive pronounsand noun phrases composed simply of determinerssuch as those.
Lindberg et al used the emPronounsystem from Charniak and Elsner, which only han-dles personal pronouns.
Since current state-of-the-art systems do not deal well with relative and pos-sessive pronouns, this will continue to be a limi-tation of natural language generation systems forthe time being.5.3 Verb FormsSince our focus is on expository text, system pat-terns deal primarily with the present and simplepast tenses.
Some patterns look for modals and socan handle future tense:Source sentence: If you continue to moveatoms closer and closer together, eventually thetwo nuclei will begin to repel each other.Question: Discuss what the two nuclei will re-pel.Light verbs pose complications in NLG becausethey are highly idiosyncratic and subject to syn-tactic variability (Sag et al, 2002).
Light verbscan either carry semantic meaning (take your pass-port) or can be bleached of semantic content whencombined with other words as in: make a deci-sion, have a drink, take a walk.
Common Englishverbs that can be light verbs include give, have,make, take.
Handling these constructions as wellas other multi-word expressions may require bothrule-based and statistical approaches.
The catena-tive construction also potentially adds complexity(Huddleston and Pullum, 2005), as shown in thisexample: As the universe expanded, it became lessdense and began to cool.
Care must be taken notto generate questions based on one predicate in thecatenative construction.We are also hindered at times by the perfor-mance of the part of speech tagging and parsingsoftware.
The most common error observed wasconfusion between the noun and verb roles of aword.
For example in: Plant roots and bacterialdecay use carbon dioxide in the process of respira-tion, the word use was classified as NN, leaving nopredicate and no semantic role labels in this sen-tence.6 ConclusionsRoediger and Pyc (2012) advocate assisting stu-dents in building a strong knowledge base be-cause creative discoveries are unlikely to occurwhen students do not have a sound set of factsand principles at their command.
To that end, au-tomatic question generation systems can facilitatethe learning process by alternating passages of textwith questions that reinforce the material learned.We have demonstrated a semantic approach toautomatic question generation that outperformssimilar systems.
We evaluated our system ontext extracted from open domain STEM textbooksrather than hand-crafted text, showing the robust-ness of our approach.
Our system achieved a 44%reduction in the error rate relative to both the Heil-man and Smith, and the Lindberg et al system onthe average over all metrics.
The results shows arestatistically significant (p<0.001).
Our questiongenerator can be used for self-study or tutoring,or by teachers to generate questions for classroomdiscussion or assessment.
Finally, we addressedlinguistic challenges to question generation.AcknowledgmentsThis research was supported by the Institute ofEducation Sciences, U.S. Dept.
of Ed., GrantR305A120808 to UNT.
The opinions expressedare those of the authors.325ReferencesAgarwal, M., Shah, R., and Mannem, P. 2011.
Auto-matic question generation using discourse cues.
InProceedings of the 6th Workshop on Innovative Useof NLP for Building Educational Applications, As-sociation for Computational Linguistics.Babko-Malaya, O.
2005.
Propbank annotation guide-lines.
URL: http://verbs.colorado.eduBlanco, E., and Moldovan, D. 2011.
Some issues ondetecting negation from text.
In FLAIRS Confer-ence.Boyer, K. E., and Piwek, P., editors.
2010.
In Proceed-ings of QG2010: The Third Workshop on QuestionGeneration.
Pittsburgh: questiongeneration.orgCarpenter, S. 2012.
Testing enhances the transfer oflearning.
In Current directions in psychological sci-ence, 21(5), 279-283.Charniak, E., and Elsner, M. 2009.
EM works for pro-noun anaphora resolution.
In Proceedings of the12th Conference of the European Chapter of theAssociation for Computational Linguistics.
Associ-ation for Computational Linguistics.Collobert, R., Weston, J., Bottou, L., Karlen, M.,Kavukcuoglu, K., & Kuksa, P. 2011.
Natural lan-guage processing (almost) from scratch.
The Jour-nal of Machine Learning Research, 12, 2493-2537.Curto, S., Mendes, A., and Coheur, L. 2012.
Ques-tion generation based on lexico-syntactic patternslearned from the web.
Dialogue & Discourse, 3(2),147-175.Heilman, M., and Smith, N. 2009.
Question gener-ation via overgenerating transformations and rank-ing.
Technical Report CMU-LTI-09-013, LanguageTechnologies Institute, Carnegie-Mellon University.Heilman, M., and Smith, N. 2010a.
Good ques-tion!
statistical ranking for question generation.
InProceedings of NAACL/HLT 2010.
Association forComputational Linguistics.Heilman, M., and Smith, N. 2010b.
Rating computer-generated questions with Mechanical Turk.
In Pro-ceedings of the NAACL-HLT Workshop on CreatingSpeech and Language Data with Amazon?s Mechan-ical Turk.
Association for Computational Linguis-tics.Huddleston, R. and Pullum, G. 2005.
A Student?s In-troduction to English Grammar, Cambridge Univer-sity Press.Lindberg, D., Popowich, F., Nesbit, J., and Winne, P.2013.
Generating natural language questions to sup-port learning on-line.
In Proceedings of the 14th Eu-ropean Workshop on Natural Language Generation,(2013): 105-114.Mannem, P., Prasad, R. and Joshi, A.
2010.
Questiongeneration from paragraphs at UPenn: QGSTECsystem description.
In Proceedings of QG2010: TheThird Workshop on Question Generation.Mazidi, K. and Nielsen, R.D.
2014.
Pedagogical eval-uation of automatically generated questions.
In In-telligent Tutoring Systems.
LNCS 8474, Springer In-ternational Publishing Switzerland.McDaniel, M. A., Anderson, J. L., Derbish, M. H., andMorrisette, N. 2007.
Testing the testing effect in theclassroom.
European Journal of Cognitive Psychol-ogy, 19(4-5), 494-513.Olney, A., Graesser, A., and Person, N. 2012.
Ques-tion generation from concept maps.
Dialogue &Discourse, 3(2), 75-99.Roediger III, H. L., and Pyc, M. 2012.
Inexpensivetechniques to improve education: Applying cog-nitive psychology to enhance educational practice.Journal of Applied Research in Memory and Cogni-tion, 1.4: 242-248.Sag, I.
A., Baldwin, T., Bond, F., Copestake, A., andFlickinger, D. 2002.
Multiword expressions: A painin the neck for NLP.
In Computational Linguisticsand Intelligent Text Processing, (pp.
1-15).
SpringerBerlin Heidelberg.Sternberg, R. J., & Grigorenko, E. L. 2003.
Teach-ing for successful intelligence: Principles, proce-dures, and practices.
Journal for the Education ofthe Gifted, 27, 207-228.Wolfe, J.
1976.
Automatic question generation fromtext-an aid to independent study.
In Proceedings ofACM SIGCSE-SIGCUE.Yao, X., and Zhang, Y.
2010.
Question generationwith minimal recursion semantics.
In Proceedingsof QG2010: The Third Workshop on Question Gen-eration.326
