Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 111?119,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsInvestigating Automatic Alignment Methods for Slide Generation fromAcademic PapersBrandon Beamer and Roxana GirjuDepartment of LinguisticsUniversity of IllinoisUrbana, IL{bbeamer,girju}@illinos.eduAbstractIn this paper we investigate the task of auto-matic generation of slide presentations fromacademic papers, focusing initially on slideto paper alignment.
We compare and eval-uate four different alignment systems whichutilize various combinations of methods usedwidely in other alignment and question an-swering approaches, such as TF-IDF termweighting and query expansion.
Our bestaligner achieves an accuracy of 75% and ourfindings show that for this application, av-erage TF-IDF scoring performs more poorlythan a simpler method based on the number ofmatched terms, and query expansion degradesaligner performance.1 IntroductionAutomatic generation of slide presentations is a taskthe Computational Linguistics community has notyet pursued in much depth.
A robust system capableof generating slide presentations from papers wouldsave the author much tedium when organizing herpresentations.
In this paper we investigate this taskfrom a novel perspective.
While others have devel-oped interesting approaches to slide generation fromdocuments by modeling the problem in a uniqueway (Utiyama and Hasida, 1999; Shibata and Kuro-hashi, 2005), the aim of the research this paper initi-ates is to discover how humans create slide presen-tations, focusing more specifically on academic pa-pers.
Thus we take a corpus-based approach to theproblem, and as a first step focus on the task of au-tomatically aligning slide presentations to academicpapers.We built a corpus of 296 slide-paper pairs and im-plemented four slide to paper aligners which utilizepopular information retrieval methods such as TF-IDF term weighting and query expansion.
In thispaper we show that, in this application, TF-IDF termweighting is inferior to a simpler scoring mecha-nism based only on the number of matched termsand query expansion degrades aligner performance.Our best aligner achieves an accuracy of 75%.2 Related WorkAutomatic slide generation from documents is a thusfar under-investigated topic.
Utiyama and Hasida(1999) generate slides from GDA1 (global documentannotation) tagged documents.
They detect topicswithin the documents by analyzing GDA corefer-ence links, modeled each slide as a topic and item-ized elaborations (which were also tagged with theGDA tag set).
Shibata and Kurohashi (2005) convertJapanese documents to slide representation by pars-ing their discourse structures and representing theresulting tree in an outline format.
While (Utiyamaand Hasida, 1999) and (Shibata and Kurohashi,2005) generate slides from documents by modelingthe task in creative ways, we aim to learn somethingdeeper regarding how humans actually go about thetask.
Creating a corpus of slide/paper pairs will en-able us to study the intricacies involved in how realhumans approach this task.Our current focus is slide to paper (region) align-ment, which can be categorized best as align-ment between monolingual comparable corpora, but1The GDA tag set is designed to allow machines to auto-matically infer the underlying structure of documents.
Moreinformation is available at http://i-content.org/gda.111could also be easily construed as document passageretrieval, which is a well-researched topic in the In-formation Retrieval community.
Barzilay and El-hadad (2003) incorporate context to facilitate align-ment between monolingual comparable corpora byfirst learning paragraph matching rules in a super-vised way, and then refining the alignment at the sen-tence level within paragraphs.
Nelken and Shieber(2008) used TF-IDF term weighting with logisticregression to align sentences from pericopes in thegospels of the new testament.
Callan (1994) ana-lyzed various ways to define document passages andidentified three main passage types, discourse (basedon physical structure of the document), semantic(based on topic boundaries), and window (based ontoken distance) and suggests that while discoursepassages may be an attractive way to define and re-trieve document passages, due to reasons related tosloppy writing, visual aids, or other factors, para-graph boundaries may not be the best indicators ofcontent boundaries.
Our alignment task differs fromthat of (Barzilay and Elhadad, 2003) and (Nelkenand Shieber, 2008) in two ways.
First, Barzilayand Elhadad (2003) and Nelken and Shieber (2008)align like-chunks between the two documents.
Thatis, they are either aligning sentences to sentences orparagraphs to paragraphs.
In our task we are align-ing slide regions which are usually bullets spanningat most a couple lines, to paper regions which canbe a whole paragraph long.
Second, Barzilay andElhadad (2003) and (Nelken and Shieber, 2008) areworking with comparable corpora in which the sameinformation is assumed to be present in each docu-ment, but expressed in a different way.
We are notable to necessarily make this assumption, in fact weshow in this paper that as much as half of the infor-mation in slide presentations may not be present inthe corresponding paper.The concept of query expansion that we im-plement in some of our aligners is also not new.Voorhees (1994) suggests that query expansiontends to help performance with short, incompletequeries but degrades performance with longer, morecomplete queries.
van der Plas and Tiedemann(2008) investigated several types of lexico-semanticinformation for query expansion in their questionanswering system.
They found that expansions thatbridge the terminology gap (synonyms, etc.)
did notresult in improvement but expansions that bridge theknowledge gap (words belonging to the same subjectfield) did.
In this paper, to get an idea of the base-line performance of query expansion with regard toour unique task, we implement a more rudimentaryform of query expansion which only expands syn-onyms of terms.
Since our slide regions don?t varymuch in length, it?s hard to say how our results relateto the findings of Voorhees (1994).
Our results par-tially support (van der Plas and Tiedemann, 2008) inthat our implementation only bridges the terminol-ogy gap, and isn?t very successful.3 The CorpusThe first step to understanding how humans generateslides from papers is to collect real-world examplesof academic papers and corresponding slide presen-tations.
To build our corpus, we searched the in-ternet for web pages containing workshop proceed-ings from various fields using generic queries suchas ?workshop slide paper?.
The collected papers andpresentations come from a variety of fields but tendto be focused generally on science and technology.Workshop proceedings are an ideal source for ourdata because they often provide the papers and slidepresentations side-by-side.
Using this strategy, wemanually extracted 296 slide-paper pairs.
The pa-pers were downloaded in PDF format and the slideswere a mixture of PDF and Powerpoint formats.
Be-fore working with these files, we converted them to acustom XML format which represents relevant partsof the original data as logical regions.
In the case ofslides, regions include bullets, headings, and othertext spans.
In the case of papers, regions include re-gions (or passages) which correspond to paragraphs,section headings, and list items.To work with PDF data, we convert it to a cus-tom XML format which represents logical chunksor regions of the paper.
In our approach we delimitregions by orthographic boundaries.
Orthographicboundaries delimit the physical structure of a paperand describe the paper in a physical fashion in termsof paragraphs, headings, bullets, etc.
We do recog-nize that there are other ways to define paper regionsthough.
As Callan (1994) observes, academic paperscould also be represented via semantic boundarieswhich delimit the topical structure of papers and de-112scribe them in terms of where new topics are intro-duced and where old ones are no longer discussed.We prefer using orthographic boundaries in our ap-proach for two reasons.
First, detecting orthographicboundaries can be accomplished with simple heuris-tics while topic boundary detection requires moresophisticated methods2, thus implementation is eas-ier.
Second, because orthographic boundaries are farless subjective than topic boundaries, it?s easier toverify the validity of orthographic boundaries thansemantic ones.Preprocessing Powerpoint files is significantlysimpler than PDF files.
To convert the Powerpointdata to our custom XML, we first convert the Pow-erpoint file to an OpenOffice.org3 ODP file via thedocument converter tool that comes standard withOpenOffice.
ODP files are already encoded with arich XML which already describes physical regionssuch as list items, bullets, and other text, so regionidentification is unnecessary.
We only needed to im-plement a filter that translates the available data tothe custom XML format.4 Alignment MethodsDiscovering how humans generate slide presenta-tions from papers starts with observing where slideregions originate from.
We make the general as-sumption that a slide region either a) is a summa-rization (excerpt or abstract) from the associated pa-per, or b) comes from other sources including butnot limited to the author?s personal (world and/orspecific) knowledge.
A complete alignment modulewould thus need to be able to discern if the informa-tion in a region comes from the target paper or if itdoes not.
When it does, the task of the aligner is thento choose the region in the paper that is summarizedor from which the excerpt is taken.
Our original hy-pothesis was that the vast majority of the data in agiven slide presentation would come from the targetpaper and concluded that a reasonable first attemptat building an aligner could be made under this as-sumption.We approach the task of aligning slide regions topaper regions with methods popular in information2Reynar (1998) provides a detailed overview of the basictopic detection and segmentation methods3OpenOffice.org is a freely available office suite available athttp://www.openoffice.org.Aligner Scoring Query ExpansionA Method 1 NoB Method 1 YesC Method 2 NoD Method 2 YesTable 1: Features implemented by each aligner.retrieval.
When aligning a slide region to a paperregion, we treat the slide region as a search queryand the target regions as documents in the informa-tion retrieval sense.
We compare two TF-IDF basedscoring methods and the effect of query expansionby building four different aligners, each of whichcorresponds to one combination of scoring type andusage of query expansion.
Table 1 shows a diagramindicating which aligners have which features.To prepare both the slide region and paper foralignment, certain preprocessing tasks are executedby all our aligners.
The general procedure all ouraligners follow is outlined below:1.
For each token in each region in the paper, the to-ken?s TF-IDF score is calculated, where the token?sterm frequency is the frequency of the token?s stemin the region and the term?s document frequency isthe number of regions containing the token?s stem.2.
The slide region is tokenized and part-of-speechtagged with the SNoW tagger (Roth, 1998) and non-content words are removed.
We consider contentwords to be any token which is either a noun, adjec-tive, verb, adverb, or cardinal number.3.
Each token in the slide region is stemmed and, inthe case of aligners B and D, query expansion isperformed.4.
A score is calculated for each region in the target pa-per according to the scoring function implementedby the aligner?method 1 for aligners A and B andmethod 2 for aligners C and D.These methods are presented in detail below.4.1 Scoring MethodsIn this paper we investigate two scoring methods,which we?ll refer to as scoring method 1 and scor-ing method 2.
Scoring method 1 is implemented byaligners A and B and is equivalent to the average TF-IDF score of the search terms relative to the targetregion.
I.e.
to calculate the score for a slide regionrelative to a target paper region with method 1, theTF-IDF scores of all the search terms are added andthe sum is divided by the number of terms, and the113target region with the highest average score wins.Scoring method 2 is implemented by aligners C andD and is based on the quantity of matched terms, re-verting to scoring method 1 only in the case of a tie.Thus, to calculate the score for a slide region rela-tive to a target paper region with method 2, the num-ber of search terms with non-zero TF-IDF scores forthe paper region is counted and the region with thelargest number of such search terms wins.
In thecase of a tie, the average score is calculated as it isin method 1 and the region with the highest averagescore wins the tie.With either scoring method, a zero score resultsin the system predicting that the slide region is notderived from any paper region.4.2 Query ExpansionOne common problem with rudimentary TF-IDFbased information retrieval systems is that match-ing tokens must have a form identical to the searchterms.
Hence, synonyms and other semantically-related words that probably should match do not.Query expansion is one way to consider terms whichare semantically near, but orthographically differ-ent from the search terms.
The general principle ofquery expansion is that, via an external knowledgebase, semantic neighbors of search terms are addedto the search query before the score is calculated.Our implementation of query expansion is utilizedby aligners B and D and uses Wordnet (Fellbaum,1998) to extract synonyms of search terms.
When aslide region undergoes query expansion our alignerexecutes the following steps:1.
The search terms are part-of-speech tagged usingthe SNoW part-of-speech tagger (Roth, 1998) andlemmatized with a morphological analyzer4.2.
The resulting lemmas and parts of speech are usedto query Wordnet for matching synsets.3.
Synonyms for all retrieved synsets are recorded.4.
When scoring occurs, the TF-IDF score of a searchterm changes from the score of the stem to the maxi-mum score among the stem and all its synonyms.
Inthe case of scoring method 2, a search term matchesif it stem is found in the target region or if any of itssynonyms?
stems are found.4The morphological analyzer we use is called mor-pha and is freely available and can be downloaded athttp://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html5 EvaluationTo evaluate our aligners, we manually checked thealignment of each on four randomly chosen slidepresentation-paper pairs.
We refer to these presen-tations here as P1, P2, P3, and P4.
Collectively,these four presentations with their respective papersamount to 587 alignment decisions which were eval-uated according to the following guidelines.
If theslide region is either an excerpt from the chosen pa-per region or if the slide region is an abstract ofthe chosen paper region, the alignment is judged asgood.
In cases where the matching excerpt or ab-stract text spans more than one paper region, thealignment is judged as good if the aligner selectedany of the involved regions.
Otherwise, the align-ment is judged as bad and an error code is recorded.The three error codes we utilize are BR, NR, and ER.BR is short for ?better region?
and indicates that thealignment is bad because the chosen paper region isnot the paper region from which the slide region isextracted or generated, but such a region does in-deed exist.
NR is short for ?no region?
and indi-cates that the alignment is bad because there is noregion in the paper to which the slide region shouldbe aligned.
ER is short for ?existing region?
and in-dicates that the alignment is bad because the alignerdecided there was no paper region to which the slideregion should be aligned, but in fact there was.
Also,the type of each slide region was recorded as eitherfrontmatter (which covers text spans such as titles,authors, dates, and addresses), outline, heading, bul-let, or diagram.
Table 2 illustrates the compositionof the four presentations insofar as slide region typeis concerned.The distribution of slide region types is not sur-prising.
Table 2 shows that two of our presentationsincluded diagrams and the other two did not, andthat bullets not surprisingly account for more slideregions than any other region type.5.1 Alignability of Slide RegionsTable 3 shows the percentage of slide regions whichhave a target paper region (i.e.
the percentage ofalignable slide regions).
One surprising observationis that only about half (57%) of the slide bullets werealignable.
This goes against our initial hypothesisthat the vast majority of slide regions would come114Presentation Frontmatter Outline Heading Bullet DiagramP1 3/174 (1.7%) 0/174 (0.0%) 5/174 (2.9%) 74/174 (42.5%) 92/174 (52.9%)P2 9/181 (5.0%) 9/181 (5.0%) 34/181 (18.8%) 129/181 (71.3%) 0/181 (0.0%)P3 5/114 (4.4%) 1/114 (0.9%) 52/114 (45.6%) 55/114 (48.2%) 0/114 (0.0%)P4 5/118 (4.2%) 1/118 (0.8%) 13/118 (11.0%) 47/118 (39.8%) 52/118 (44.0%)Total 22/587 (3.7%) 11/587 (1.9%) 104/587 (17.7%) 305/587 (52.0%) 144/587 (24.5%)Table 2: Breakdown of slide text spans by type.
Columns correspond to slide text span types.
Percentages in each column measure the fractionof text spans which are of the given type.from the associated paper, and not from the author?sknowledge.Another important observation from the data intable 3 is that the fraction of slide regions which arealignable for any given presentation can vary wildly.82% of P4?s regions were alignable while 60% ofP3?s and only 14% of P1?s regions were alignable.5.2 Aligner AccuracyTables 4 and 5 show the raw accuracy and alignableaccuracy of the four aligners respectively.
Rawaccuracy is the number of slide regions correctlyaligned out of the total number of slide regions.Alignable accuracy is the percentage of alignableslide regions which were aligned correctly.Given the surprising results that a large percent-age of slide regions need not come from the paper,any fully fledged slide to paper aligner would needa module which first filters out the unalignable slideregions.
Because such a module is not implementedin our aligners, as our aligners make the assumptionthat each slide region has a corresponding paper re-gion, we limit most of our accuracy evaluation toalignable accuracy rather than raw accuracy.From tables 4 and 5 we can easily see the im-portance of such a filtering module.
As our bestaligner, which achieves an average alignable accu-racy of 75%, only achieves an average raw accuracyof 50%.5.3 Error AnalysisTables 6 and 7 show what percentage of an aligner?serrors correspond to which error types.
Because ouraligners are based on term matching, the only wayfor them to predict no alignment is for the averageTF-IDF score of the terms to be zero (no matchingterms anywhere).
Because this is a very rare event,ER-type errors are also extremely rare, and are ex-cluded from our error analysis.We can see from tables 6 and 7 that our pooreraligners (A and B) have a fairly even split betweenBR-type and NR-type errors, while our better align-ers (C and D) have a far greater percentage of NR-type errors, indicating that the features we are in-vestigating can only reduce BR-type errors.
Thisverifies the importance of the proposed alignabilitymodule which first filters out unalignable slide re-gions.5.4 Error ReductionTables 8 and 9 analyze how well query expansionand scoring method 2 reduce errors by measuringthe percentage of errors made by one aligner, whichwere not made by another.
Four pairings of align-ers are considered: A and B, A and C, B and D, andC and D. By comparing aligner A to B and C to D,we have one measure of the error reduction achievedby adding query expansion to an aligner.
If the ad-dition of query expansion enables an aligner to cor-rectly align slide regions which its query expansion-less counterpart could not, then we should see largepercentages of errors being corrected when compar-ing aligner A to B and C to D. By comparing alignerA to C and B to D, we have a measure of the errorreduction achieved by implementing scoring method2 instead of method 1.Tables 8 and 9 show that aligner D significantlyreduced aligner B?s errors and aligner C significantlyreduced aligner A?s errors, but aligner B did not im-prove much on A, nor did D on C. In other words,adding query expansion did not significantly reduceerrors, but using scoring method 2 instead of 1 did.6 Discussion6.1 On AlignabilityBefore mentioning alignment performance, it is im-portant to notice from our data that there is great va-riety among slide presentations.
For example, ta-115Presentation Frontmatter Outline Heading Bullet Diagram OverallP1 3/3 (100.0%) 0/0 0/5 (0.0%) 21/74 (28.4%) 0/92 (0.0%) 24/174 (13.8%)P2 9/9 (100.0%) 8/9 (88.9%) 24/34 (70.6%) 104/129 (80.6%) 0/0 145/181 (80.1%)P3 5/5 (100.0%) 0/1 (0.0%) 48/52 (92.3%) 15/55 (27.3%) 0/0 68/114 (59.5%)P4 4/5 (80.0%) 0/1 (0.0%) 11/13 (74.6%) 33/47 (70.2%) 49/52 (94.2%) 97/118 (82.2%)Total 21/22 (95.5%) 8/11 (72.7%) 83/104 (79.8%) 173/305 (56.7%) 49/144 (34.0%) 334/587 (56.9%)Table 3: Breakdown of alignable slide text spans by type.
Columns correspond to slide text span types.
Percentages in each column measure thefraction of text spans of that type which are alignable.
E.g.
of the 129 bullets in presentation P2, 104 are alignable.
The ?Overall?
column measuresthe fraction of all text spans which are alignable.
E.g.
of the 181 text spans in presentation P2, 145 are alignable.Presentation Aligner A Aligner B Aligner C Aligner DP1 34/174 (19.5%) 129/174 (16.7%) 37/174 (21.3%) 35/174 (20.1%)P2 71/181 (39.2%) 64/181 (35.4%) 101/181 (55.8%) 97/181 (53.6%)P3 66/114 (57.9%) 64/114 (56.1%) 77/114 (67.5%) 77/114 (67.5%)P4 50/118 (42.4%) 48/118 (40.7%) 78/118 (66.1%) 77/118 (65.3%)Total 221/587 (37.6%) 205/587 (34.9%) 293/587 (49.9%) 286/587 (48.7%)Table 4: Raw accuracy.
Each column corresponds to one of the four aligners evaluated.
Percentages measure the fraction of text spans whichwere aligned correctly.Presentation Aligner A Aligner B Aligner C Aligner DP1 12/24 (50.0%) 9/24 (37.5%) 15/24 (62.5%) 15/24 (62.5%)P2 63/145 (43.4%) 56/145 (38.6%) 93/145 (64.1%) 90/145 (62.1%)P3 55/68 (80.9%) 54/68 (79.4%) 66/68 (97.1%) 67/68 (98.5%)P4 49/97 (50.5%) 47/97 (48.5%) 77/97 (79.4%) 76/97 (78.4%)Total 179/334 (53.6%) 166/334 (49.7%) 251/334 (75.1%) 248/334 (74.3%)Table 5: Alignable accuracy.
Each column corresponds to one of the four aligners evaluated.
Percentages measure the fraction of alignable textspans which were aligned correctly.Aligner A Aligner BPresentation BR NR BR NRP1 11/140 (7.9%) 128/140 (91.4%) 14/145 (9.7%) 130/145 (89.7%)P2 82/110 (74.5%) 28/110 (25.5%) 89/117 (76.1%) 28/117 (23.9%)P3 13/48 (27.1%) 35/48 (72.9%) 14/50 (28.0%) 36/50 (72.0%)P4 48/68 (70.6%) 20/68 (29.4%) 50/70 (71.4%) 20/70 (28.6%)Total 154/366 (42.1%) 211/366 (57.7%) 167/382 (43.7%) 214/382 (56.0%)Table 6: Error type breakdown for aligners A and B.
Columns correspond to specific types of alignment errors.
?BR?
is short for ?better region?and ?NR?
is short for ?no region?.
An error of type ?BR?
means that the aligner choose an incorrect region in the paper, and a better region existed.An error of type ?NR?
means the aligner choose an incorrect region, and there was no correct region.Aligner C Aligner DPresentation BR NR BR NRP1 8/137 (5.8%) 128/137 (93.4%) 8/139 (5.8%) 130/139 (93.5%)P2 52/80 (65.0%) 28/80 (35.0%) 55/84 (65.5%) 29/84 (34.5%)P3 2/37 (5.4%) 35/37 (94.6%) 1/37 (2.7%) 36/37 (97.3%)P4 20/40 (50.0%) 20/40 (50.0%) 21/41 (51.2%) 20/41 (48.8%)Total 82/294 (27.9%) 211/294 (71.8%) 85/301 (28.2%) 215/301 (71.4%)Table 7: Error type breakdown for aligners C and D. Columns correspond to specific types of alignment errors.
?BR?
is short for ?better region?and ?NR?
is short for ?no region?.
An error of type ?BR?
means that the aligner choose an incorrect region in the paper, and a better region existed.An error of type ?NR?
means the aligner choose an incorrect region, and there was no correct region.116Aligner A?
B Aligner A?
CPresentation BR NR Overall BR NR OverallP1 0/11 (0.0%) 0/128 (0.0%) 0/140 (0.0%) 4/11 (36.4%) 0/128 (0.0%) 4/140 (2.9%)P2 0/82 (0.0%) 0/28 (0.0%) 0/110 (0.0%) 38/82 (46.3%) 0/28 (0.0%) 38/110 (34.5%)P3 0/13 (0.0%) 0/35 (0.0%) 0/48 (0.0%) 11/13 (84.6%) 0/35 (0.0%) 11/48 (22.9%)P4 0/48 (0.0%) 0/20 (0.0%) 0/68 (0.0%) 31/48 (64.6%) 0/20 (0.0%) 31/68 (45.6%)Total 0/154 (0.0%) 0/211 (0.0%) 0/366 (0.0%) 84/154 (54.5%) 0/211 (0.0%) 84/366 (23.0%)Table 8: Error reduction between aligners A and B, and between aligners A and C. Major columns correspond to aligner pairs and minor columnscorrespond to error types.
A pair denoted by X ?
Y indicates that the corresponding percentages are measuring the fraction of slide text spansaligned incorrectly by aligner X , which were aligned correctly by aligner Y .
E.g.
from this table you can see that in presentation P1, aligner Aincorrectly aligned 140 text spans.
11 of them were BR-type errors and 128 of them were NR-type errors.
Four of aligner A?s BR-type errors werealigned correctly by aligner C.Aligner B?
D Aligner C?
DPresentation BR NR Overall BR NR OverallP1 7/14 (50.0%) 0/130 (0.0%) 7/145 (4.8%) 0/8 (0.0%) 0/128 (0.0%) 0/137 (0.0%)P2 42/89 (47.2%) 0/28 (0.0%) 42/117 (35.9%) 1/52 (1.9%) 0/28 (0.0%) 1/80 (1.2%)P3 13/14 (92.9%) 0/36 (0.0%) 13/50 (26.0%) 1/2 (50.0%) 0/35 (0.0%) 1/37 (2.7%)P4 32/50 (64.0%) 0/20 (0.0%) 32/70 (45.7%) 1/20 (5.0%) 0/20 (0.0%) 1/40 (2.5%)Total 94/167 (56.3%) 0/214 (0.0%) 94/382 (24.6%) 3/82 (3.7%) 0/211 (0.0%) 3/294 (1.0%)Table 9: Error reduction between aligners B and D, and between aligners C and D. Major columns correspond to aligner pairs and minor columnscorrespond to error types.
A pair denoted by X ?
Y indicates that the corresponding percentages are measuring the fraction of slide text spansaligned incorrectly by aligner X , which were aligned correctly by aligner Y .
E.g.
from this table you can see that in presentation P1, aligner Bincorrectly aligned 145 text spans.
14 of them were BR-type errors and 130 of them were NR-type errors.
7 of aligners B?s BR-type errors werecorrectly aligned by aligner D.ble 3 shows that 28% of P1?s bullets were alignable,while 81% of P2?s were alignable.
P1 and P4 bothcontained diagrams, but only P4?s diagram existedin the paper.
Our initial hypothesis was that the vastmajority of slide regions would either be excerpts orabstracts from/of the paper regions.
Table 3 showsthat a nontrivial amount of slide regions does notmap to the paper at all.
Also, tables 6 and 7 showthat as a result, NR-type errors make up the majorityof the errors made by the better aligners.
Thus, thedata indicates that the task of slide-presentation gen-eration is highly dependent on the end purpose thepresentation will serve, as well as the target audienceand other factors.
We will focus more on identify-ing these factors in future research.
Once identified,these factors should be quantified and controlled infuture corpora of presentation-paper pairs used forthis task.6.2 On Scoring Methods and Query ExpansionOur results clearly show that, for this task, queryexpansion has little or negative impact on alignersand that scoring method 2 is indeed superior to scor-ing method 1.
Tables 4 and 5 show that alignerC consistently outperforms aligner A and aligner Dconsistently outperforms aligner B, especially whenlimited to alignable slide regions.
Hence, scoringmethod 2 is better than method 1.
We can alsosee from tables 4 and 5 that aligner B consistentlyunder-performs A and aligner D consistently under-performs C, which shows that query expansion doesnot improve performance and in fact, it degrades it.Tables 8 and 9 show the same results from a differ-ent perspective: aligner C correctly aligned 55% ofthe aligner A?s erroneous alignable slide regions andaligner D correctly aligned 56% of aligner B?s erro-neous alignable slide regions.
But aligner B did notcatch any of aligner A?s errors and aligner D onlycaught 4% of aligner C?s errors ?
but ended up mak-ing more in the end anyway.With regard to query expansion, there are twopossibilities.
Query expansion was not very help-ful here because either (a) slide authors tend to usewording identical to that in the paper, or (b) usingsynonyms from Wordnet is not aggressive enoughand we should consider expanding our query expan-sion approach to include hypernyms, immediate hy-ponyms, and other semantically related terms.
Wethink the data suggests that (a) is more the case than(b).
If (b) were the case, including synonyms in oursearch should have improved the performance, justnot by a lot.
In actually, aligner B performed worse117on average than aligner A, and likewise with alignerD when compared to C. Synonyms are semanti-cally closer to the original term than hypernyms,hyponyms, or other semantically related terms, andour results show that introducing this small amountof semantic distance is (a little bit) detrimental.
Byadding hypernyms and other relations, only a wider,less focused group of terms will be introduced whichwill probably just result in more false positives.One possible criticism against our argument for(a) could be that our implementation of query expan-sion performed poorly because we don?t word sensedisambiguate, and thus we introduce synonyms fromincorrect senses of each term.
This probably isn?tthe case because the search terms are not in isolation,but are part of a larger query.
For an incorrect paperregion to be select based on an error of this type,it would have to contain many of the terms in thequery as well as the semantically inaccurate sense ofthe one in question.
This situation is unlikely due toone of the most basic assumptions made when sensedisambiguating: that context restricts the possiblesenses of any word.
So, if a paper region containsmany of the terms in a slide region, it is unlikelythat it will also contain the off-topic, semanticallyawkward term pertaining to a bad sense of one ofthem.With regard to scoring methods.
Average TF-IDFscoring is probably ineffective in this application be-cause of the nature of paper regions.
When retriev-ing whole documents given a search query, one doc-ument?s contents are probably independent of anyother, so terms related to the document?s topic arestated explicitly.
Paper regions, however, are in thecontext of each other.
The topic of one can be verysimilar to another, only because it?s nearby, not be-cause of the terms explicitly mentioned in the re-gion.
Add to this the fact that paper regions are ex-tremely non-uniform in length and TF-IDF scoresend up skewed.6.3 On ImprovementThere is a lot of room for improvement on slideto paper alignment.
As mentioned previously insection 6.1, unalignable slide regions account for amuch larger portion of the slide presentations thanour initial hypothesis predicted; around 70% of theerrors made by our better aligners (C and D) wereNR-type errors, meaning the alignment was bad be-cause the system selected a paper region when in factthere was no correct paper region.
A robust slide topaper aligner would need to have a module capableof filtering out unalignable slide regions.
If this taskwere solved and implemented on our better aligners,raw accuracy would raise from 50% to about 75%on average which is nearing the level of robustnessnecessary for real-world applications.We also suggest that, in regard to alignable slideregions, performance would be significantly boostedby taking context into account, both on the slide andpaper side.
We noticed during evaluation that manyof the BR-type errors occurred when the slide regionin question lacked the necessary terms, but the termsexisted in nearby slide regions.
Examples of this in-clude when for instance, the title is broken acrosstwo lines and the second line only has a word ortwo in it, or when a heading is rather non-descriptivebut the sub-bullets beneath it contain many relevantterms to the topic.
Incorporating terms of nearbyslide regions (perhaps in query-expansion fashion),rather than just treating each one as an independentsearch query will certainly boost performance.Likewise on the paper end, it is reasonable to as-sume that in most cases, the topic of one region issimilar to the topics of adjacent regions.
And just asterms from nearby slide regions could supplementterm-poor slide regions, terms from nearby paper re-gions could supplement term-poor paper regions.7 ConclusionIn this paper we investigated the task of automaticslide to paper alignment.
We built a corpus ofslide-paper pairs and used four presentations fromit to evaluate four aligners which utilize methodssuch as TF-IDF term weighting and query expan-sion.
We showed that query expansion does not im-prove performance in our application and that TF-IDF term weighting is inferior to a much simplerscoring mechanism based on the number of matchedterms.
For future improvements, we suggest that amodule capable of robustly filtering out unalignableslide regions is necessary.
We also suggest that per-formance can be improved by taking context into ac-count and using terms in nearby regions to supple-ment both slide regions and paper regions.118ReferencesRegina Barzilay and Noemie Elhadad.
2003.
Sen-tence alignment for monolingual comparable corpora.In Proceedings of the 2003 Conference on EmpiricalMethods in Natural Language Processing (EMNLP).James P. Callan.
1994.
Passage-level evidence in docu-ment retrieval.
In Proceedings of the 17th Annual In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval.Christiane Fellbaum.
1998.
WordNet - An ElectronicLexical Database.
Cambridge MA: MIT Press.Rani Nelken and Stuart M. Shieber.
2008.
Towards ro-bust context-sensitive sentence alignment for monolin-gua corpora.
In Proceedings of the 11th Conference ofthe European Chapter of the Association for Compu-tational Linguistics.Jeffrey C. Reynar.
1998.
Topic Segmentation: Algo-rithms and Applications.
Ph.D. thesis, University ofPennsylvania.Dan Roth.
1998.
Learning to resolve natural lanuageambiguities: A unified approach.
In Proceedings ofthe 15th Conference of the American Association forArtificial Intelligence (AAAI).Tomohide Shibata and Sadao Kurohashi.
2005.
Au-tomatic slide generation based on discourse structureanalysis.
In Proceedings of the second internationaljoint conference on natural language processing (IJC-NLP).Masao Utiyama and Koiti Hasida.
1999.
Automatic slidepresentation from semantically annotated documents.In Proceedings of the workshop held in conjunctionwith the 37th annual meeting of the Association forComputational Linguistics (ACL).Lonneke van der Plas and Jo?rg Tiedemann.
2008.
Us-ing lexico-semantic information for query expansionin passage retrieval for question answering.
In Pro-ceedings of the 9th SIGdial Workshop on Discourseand Dialogue.Ellen M. Voorhees.
1994.
Query expansion usinglexical-semantic relations.
In Proceedings of the 17thAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval.119
