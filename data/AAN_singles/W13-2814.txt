Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 94?101,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsA Hybrid Word Alignment Model for Phrase-Based Statistical Ma-chine TranslationSantanu Pal*, Sudip Kumar Naskar?
and Sivaji Bandyopadhyay**Department of Computer Science & EngineeringJadavpur University, Kolkata, Indiasantanu.pal.ju@gmail.com, sivaji_cse_ju@yahoo.com?Department of Computer & System SciencesVisva-Bharati University, Santiniketan, Indiasudip.naskar@gmail.comAbstractThis paper proposes a hybrid word alignmentmodel for Phrase-Based Statistical Machinetranslation (PB-SMT).
The proposed hybridalignment model provides most informativealignment links which are offered by both un-supervised and semi-supervised word align-ment models.
Two unsupervised word align-ment models (GIZA++ and Berkeley aligner)and a rule based aligner are combined togeth-er.
The rule based aligner only aligns namedentities (NEs) and chunks.
The NEs arealigned through transliteration using a jointsource-channel model.
Chunks are alignedemploying a bootstrapping approach by trans-lating the source chunks into the target lan-guage using a baseline PB-SMT system andsubsequently validating the target chunks us-ing a fuzzy matching technique against thetarget corpus.
All the experiments are carriedout after single-tokenizing the multi-wordNEs.
Our best system provided significantimprovements over the baseline as measuredby BLEU.1 IntroductionWord alignment is the backbone of PB-SMT sys-tem or any data driven approaches to MachineTranslation (MT) and it has received a lot of at-tention in the area of statistical machine transla-tion (SMT) (Brown et al 1993; Och and Ney,2003; Koehn et al 2003).
Word alignment is notan end task in itself and is usually used as an in-termediate step in SMT.
Word alignment is de-fined as the detection of corresponding alignmentof words from parallel sentences that are transla-tion of each other.
Statistical machine translationusually suffers from many-to-many word linkswhich existing statistical word alignment algo-rithms can not handle well.The unsupervised word alignment models arebased on IBM models 1?5 (Brown et al 1993)and the HMM model (Ney and Vogel, 1996; Ochand Ney, 2003).
Models 3, 4 and 5 are based onfertility based models which are asymmetric.
Toimprove alignment quality, the Berkeley Aligneris based on the symmetric property by intersect-ing alignments induced in each translation direc-tion.In the present work, we propose improvementof word alignment quality by combining threeword alignment tables (i) GIZA++ alignment (ii)Berkeley Alignment and (iii) rule based align-ment.
Our objective is to perceive the effective-ness of the Hybrid model in word alignment byimproving the quality of translation in the SMTsystem.
In the present work, we have implement-ed a rule based alignment model by consideringseveral types of chunks which are automaticallyextracted on the source side.
Each individualsource chunk is translated using a baseline PB-SMT system and validated with the target chunkson the target side.
The validated source-targetchunks are added in the rule based alignmenttable.
Work has been carried out into three direc-tions: (i) three alignment tables are combinedtogether by taking their union; (ii) extra align-ment pairs are added into the alignment table.This is a well-known practice in domain adapta-tion in SMT (Eck et al 2004; Wu et al 2008);(iii) the alignment table is updated through semi-supervised alignment technique.94The remainder of the paper is organized as fol-lows.
Section 2 discusses related work.
The pro-posed hybrid word alignment model is describedin Section 3.
Section 4 presents the tools and re-sources used for the various experiments.
Section5 includes the results obtained, together withsome analysis.
Section 6 concludes and providesavenues for further work.2 Related WorksZhou et al(2004) proposed a multi lingual filter-ing algorithm that generates bilingual chunkalignment from Chinese-English parallel corpus.The algorithm has three steps, first, from the par-allel corpus; the most frequent bilingual chunksare extracted.
Secondly, the participating chunksfor alignments are combined into a cluster andfinally one English chunk is generated corre-sponding to a Chinese chunk by analyzing thehighest co-occurrences of English chunks.
Bilin-gual knowledge can be extracted using chunkalignment (Zhou et.
al., 2004).
Pal et, al.
(2012)proposed a bootstrapping method for chunkalignment; they used an SMT based model forchunk translation and then aligned the source-target chunk pairs after validating the translatedchunk.
Ma et.
al.
(2007) simplified the task ofautomatic word alignment as several consecutivewords together correspond to a single word in theopposite language by using the word aligner it-self, i.e., by bootstrapping on its output.
A Max-imum Entropy model based approach for Eng-lish?Chinese NE alignment which significantlyoutperforms IBM Model4 and HMM has beenproposed by Feng et al(2004).
They considered4 features: translation score, transliteration score,source NE and target NE's co-occurrence scoreand the distortion score for distinguishing identi-cal NEs in the same sentence.
Moore (2003) pre-sented an approach where capitalization cueshave been used for identifying NEs on the Eng-lish side.
Statistical techniques are applied to de-cide which portion of the target language corre-sponds to the specified English NE, for simulta-neous NE identification and translation.To improve the learning process of unlabeleddata using labeled data (Chapelle et al 2006),the semi-supervised learning method is the mostuseful learning technique.
Semi-supervisedlearning is a broader area of Machine Learning.Researchers have begun to explore semi-supervised word alignment models that use bothlabeled and unlabeled data.
Fraser and Marcu(2006) proposed a semi-supervised training algo-rithm.
The weighting parameters are learnedfrom discriminative error training on labeled da-ta, and the parameters are estimated by maxi-mum-likelihood EM training on unlabeled data.They have also used a log-linear model which istrained on the available labeled data to improveperformance.
Interpolating human alignmentswith automatic alignments has been proposed byCallison-Burch et al(2004), where the align-ments of higher quality have gained much higherweight than the lower-quality alignments.
Wu etal.
(2006) have developed two separate modelsof standard EM algorithm which learn separatelyfrom both labeled and unlabeled data.
Two mod-els are then interpolated as a learner in the semi-supervised Ada-Boost algorithm to improveword alignment.
Ambati et al(2010) proposedactive learning query strategies to identify highlyuncertain or most informative alignment linksunder an unsupervised word alignment model.Intuitively, multiword NEs on the source andthe target sides should be both aligned in the par-allel corpus and translated as a whole.
However,in the state-of-the-art PB-SMT systems, the con-stituents of multiword NE are marked andaligned as parts of consecutive phrases, sincePB-SMT (or any other approaches to SMT) doesnot generally treat multiword NEs as special to-kens.
This is the motivations behind consideringNEs for special treatment in this work by con-verting into single tokens that makes sure thatPB-SMT also treats them as a wholeAnother problem with SMT systems is the er-roneous word alignment.
Sometimes some wordsare not translated in the SMT output sentencebecause of the mapping to NULL token or erro-neous mapping during word alignment.
Verbphrase translation also creates major problems.The words inside verb phrases are generally notaligned one-to-one; the alignments of the wordsinside source and target verb phrases are mostlymany-to-many particularly so for the English?Bengali language pair.The first objective of the present work is to seehow single tokenization and alignment of NEs onboth the sides affects the overall MT quality.
Thesecond objective is to see whether Hybrid wordalignment model of both unsupervised and semi-supervised techniques enhance the quality oftranslation in the SMT system rather than thesingle tokenized NE level parallel corpus appliedto the hybrid model.We carried out the experiments on English?Bengali translation task.
Bengali shows highmorphological richness at lexical level.
Lan-95guage resources in Bengali are not widely avail-able.3 Hybrid Word Alignment ModelThe hybrid word alignment model is described asthe combination of three word alignment modelsas follows:3.1 Word Alignment Using GIZA++GIZA++ (Och and Ney, 2003) is a statisticalword alignment tool which incorporates all theIBM 1-5 models.
GIZA++ facilitates fast devel-opment of statistical machine translation (SMT)systems.
In case of low-resource language pairsthe quality of word alignments is typically quitelow and it also deviates from the independenceassumptions made by the generative models.Although huge amount of parallel data enablesthe model parameters to acquire better estimation,a large number of language pairs still lacks fromthe unavailability of sizeable amount of paralleldata.
GIZA++ has some draw-backs.
It allows atmost one source word to be aligned with eachforeign word.
To resolve this issue, some tech-niques have already been applied such as: theparallel corpus is aligned bidirectionally; then thetwo alignment tables are reconciled using differ-ent heuristics e.g., intersection, union, and mostrecently grow-diagonal-final and grow-diagonal-final-and heuristics have been applied.
In spite ofthese heuristics, the word alignment quality forlow-resource language pairs is still low and callsfor further improvement.
We describe our ap-proach of improving word alignment quality inthe following three subsections.3.2 Word Alignment Using Berkley AlignerThe recent advancements in word alignment isimplemented in Berkeley Aligner (Liang et al2006) which allows both unsupervised and su-pervised approach to align word from parallelcorpus.
We initially train the parallel corpus us-ing unsupervised technique.
We make a fewmanual corrections to the alignment table pro-duced by the unsupervised aligner.
Then we ap-ply this corrected alignment table as gold stand-ard training data for the supervised aligner.
TheBerkeley aligner is an extension of the Cross Ex-pectation Maximization word aligner.
Berkeleyaligner is a very useful word aligner because itallows for supervised training, enabling us toderive knowledge from already aligned parallelcorpus or we can use the same corpus by updat-ing the alignments using some rule based meth-ods.
Our approach deals with the latter case.
Thesupervised technique of Berkeley aligner helpsus to align those words which could not bealigned by rule based word aligner.3.3 Rule Based Word AlignmentThe proposed Rule based aligner aligns NamedEntities (NEs) and chunks.
For NE alignment,we first identify NEs from the source side (i.e.English) using Stanford NER.
The NEs on thetarget side (i.e.
Bengali) are identified using amethod described in (Ekbal and Bandyopadhyay,2009).
The accuracy of the Bengali Named Enti-ty recognizers (NER) is much poorer comparedto that of English NER due to several reasons: (i)there is no capitalization cue for NEs in Bengali;(ii) most of the common nouns in Bengali arefrequently used as proper nouns; (iii) suffixes(case markers, plural markers, emphasizers,specifiers) get attached to proper names as wellin Bengali.
Bengali shallow parser 1  has beenused to improve the performance of NE identifi-cation by considering proper names as NE.Therefore, NER and shallow parser are jointlyemployed to detect NEs from the Bengali sen-tences.
The source NEs are then transliteratedusing a modified joint source-channel model(Ekbal et al 2006) and aligned to their targetside equivalents following the approach of Pal etal.
(2010).
The target side equivalents NEs aretransformed into canonical form after omittingtheir ?matras?.
Similarly Bengali NEs are alsotransformed into canonical forms as Bengali NEsmay differ in their choice of matras (vowel mod-ifiers).
The transliterated NEs are then matchedwith the corresponding parallel target NEs andfinally we align the NEs if match is found.After identification of multiword NEs on bothsides, we pre-processed the corpus by replacingspace with the underscore character (?_?).
Wehave used underscore (?_?)
instead of hyphen (?-?)
since there already exists some hyphenatedwords in the corpus.
The use of the underscore(?_?)
character also facilitates to de-tokenize thesingle-tokenized NEs after decoding.For chunk alignment, the source sentences ofthe parallel corpus are parsed using StanfordPOS tagger.
The chunks of the sentences are ex-tracted using CRF chunker2.
The chunker detectsthe boundaries of noun, verb, adjective, adverb1http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow_parser.php2 http://crfchunker.sourceforge.net/96and prepositional chunks from the sentences.
Incase of prepositional phrase chunks, we havetaken a special attention: we have expanded theprepositional phrase chunk by examining a singlenoun chunk followed by a preposition or a seriesof noun chunks separated by conjunctions suchas 'comma', 'and' etc.
For each individual chunk,the head word is identified.
Similarly target sidesentences are parsed using a shallow parser.
Theindividual target side Bengali chunks are extract-ed from the parsed sentences.
The head wordsfor all individual chunks on the target side arealso marked.
If the translated head word of asource chunk matches with the headword of atarget chunk then we hypothesize that these twochunks are translations of each other.The extracted source chunks are translated us-ing a baseline SMT model trained on the samecorpus.
The translated chunks are validatedagainst the target chunks found in the corre-sponding target sentence.
During the validationprocess, if any match is found between the trans-lated chunk and a target chunk then the sourcechunk is directly aligned with the original targetchunk.
Otherwise, the source chunk is ignored inthe current iteration for any possible alignmentand is considered in the next iterations.Figure 1.a: Rule based alignmentsFigure 1.b: Gold standard alignmentsFigure 1: Establishing alignments through Rulebased methods.The extracted chunks on the source side maynot have a one to one correspondence with thetarget side chunks.
The alignment validation pro-cess is focused on the proper identification of thehead words and not between the translatedsource chunk and target chunk.
The matchingprocess has been carried out using a fuzzymatching technique.
If both sides contain onlyone chunk after aligning the remaining chunksthen the alignment is trivial.
After aligning theindividual chunks, we also establish word align-ments between the matching words in thosealigned chunks.
Thus we get a sentence levelsource-target word alignment table.Figure 1 shows how word alignments are es-tablished between a source-target sentence pairusing the rule based method.
Figure 1.a showsthe alignments obtained through rule basedmethod.
The solid links are established throughtransliteration (for NEs) and translation.
The dot-ted arrows are also probable candidates for intra-chunk word alignments; however they are notconsidered in the present work.
Figure 1.b showsthe gold standard alignments for this sentencepair.3.4  Hybrid Word alignment ModelThe hybrid word alignment method combinesthree different kinds of word alignments ?
Gi-za++ word alignment with grow-diag-final-and(GDFA) heuristic, Berkeley aligner and rulebased aligner.
We have followed two differentstrategies to combine the three different wordalignment tables.UnionIn the union method all the alignment tables areunited together and duplicate entries are removed.ADD additional AlignmentsIn this method we consider either of the align-ments generated by GIZA++ GDFA (A1) orBerkeley aligner (A2) as the standard alignmentas the rule based aligner fails to align all wordsin the parallel sentences.
From the three set ofalignments A1, A2 and A3, we propose analignment combination method as described inalgorithm 1.ALGORITHM: 1Step 1: Choose either A1 or A2 as the standardalignment (SA).Step 2: Correct the alignments in SA using thealignment table of A3.Step 3: if A2 is considered as SA then find addi-tional alignment from A1 and A3 using intersec-tion method (A1?A3) otherwise find additionalalignment from A2 and A3 (using A2?A3).Step 4: Add additional entries with SA.
[Jaipur] [golapi sohor name] [porichito] [.
][Jaipur] [is known] [as [Pink City]] [.
][Jaipur] [golapi sohor name] [porichito] [.
][Jaipur] [is known] [as [Pink City]] [.
]973.5 Berkeley Semi-supervised AlignmentThe correctness of the alignments is verified bymanually checking the performance of the vari-ous alignment system.
We start with the com-bined alignment table which is produced by Al-gorithm 1.
Iinitially, we take a subset of thealignments by manually inspecting from thecombined alignment table.
Then we train theBarkley supervised aligner with this labeled data.A subset of the unlabeled data from the com-bined alignment table is tested with the super-vised model.
The output is then added as addi-tional labeled training data for the supervisedtraining method for the next iteration.
Using thisbootstrapping approach, the amount of labeledtraining data for the supervised aligner is gradu-ally increased.
The process is continued untilthere are no more unlabelled training data.
In thisway we tune the whole alignment table for theentire parallel corpus.
The process is carried outin a semi-supervised manner.4 Tools and resources UsedA sentence-aligned English-Bengali parallel cor-pus containing 23,492 parallel sentences fromthe travel and tourism domain has been used inthe present work.
The corpus has been collectedfrom the consortium-mode project ?Developmentof English to Indian Languages Machine Trans-lation (EILMT) System - Phase II?
3.
The Stan-ford Parser4 and CRF chunker5 have been usedfor identifying chunks and Stanford NER hasbeen used to identify named entities in the sourceside of the parallel corpus.The target side (Bengali) sentences are parsedby using the tools obtained from the consortiummode project ?Development of Indian Languageto Indian Language Machine Translation (IL-ILMT) System - Phase II6?.The effectiveness of the present work has beentested by using the standard log-linear PB-SMTmodel as our baseline system: phrase-extractionheuristics described in (Koehn et al 2003), ,MERT (minimum-error-rate training) (Och,2003) on a held-out development set, target3  The EILMT project is funded by the Department of Elec-tronics and Information Technology (DEITY), Ministry ofCommunications and Information Technology (MCIT),Government of India.4 http://nlp.stanford.edu/software/lex-parser.shtml5 http://crfchunker.sourceforge.net/6   The IL-ILMT project is funded by the Department ofElectronics and Information Technology (DEITY), Ministryof Communications and Information Technology (MCIT),Government of India.language model trained using SRILM toolkit(Stolcke, 2002) with Kneser-Ney smoothing(Kneser and Ney, 1995) and the Moses decoder(Koehn et al 2007) have been used in thepresent study.5 Experiments and ResultsWe have randomly selected 500 sentences eachfor the development set and the test set from theinitial parallel corpus.
The rest are considered asthe training corpus.
The training corpus was fil-tered with the maximum allowable sentencelength of 100 words and sentence length ratio of1:2 (either way).
Finally the training corpus con-tained 22,492 sentences.
In addition to the targetside of the parallel corpus, a monolingual Benga-li corpus containing 488,026 words from thetourism domain was used for building the targetlanguage model.
We experimented with differentn-gram settings for the language model and themaximum phrase length and found that a 4-gramlanguage model and a maximum phrase length of7 produced the optimum baseline result.
We car-ried out the rest of the experiments using thesesettings.We experimented with the system  overvarious combinations of word alignment models.Our hypothesis focuses mainly on the theme thatproper alignment of words will result inimprovement of the system performance in termsof translation quality.141,821 chunks were identified from thesource corpus, of which 96,438 (68%) chunkswere aligned by the system.
39,931 and 28,107NEs were identified from the source and targetsides of the parallel corpus respectively, of which22,273 NEs are unique in English and 22,010NEs in Bengali.
A total of 14,023 NEs have beenaligned through transliteration.The experiments have been carried out withvarious experimental settings: (i) singletokenization of NEs on both sides of the parallelcorpus, (ii) using Berkeley Aligner withunsupervised training, (iii) union of the threealignment models: rule based, GIZA++ withGDFA and Berkeley Alignment, (iv)hybridization of the three alignment models and(v) supervised Berkeley Aligner.
Eextrinsicevaluation was carried out on the MT qualityusing BLEU (Papineni et al 2002) and NIST(Doddington, 2002).98Experiment Expno.BLEU NISTBaseline system using GIZA++ with GDFA 1 10.92 4.13PB-SMT system using Berkeley Aligner 2 11.42 4.16Union of all Alignments 3 11.12 4.14PB-SMT System with Hybrid Alignment by considering (a)GIZA++ as the standard alignment) (b) Berkeley alignmentas the standard alignment)4a?
15.38 4.304b?
15.92 4.36Single tokenized NE + Exp 1 5 11.68 4.17Single tokenized NE + Exp 2 6 11.82 4.19Single tokenized NE + (a) Exp 4a (b) Exp 4b 7a?
16.58 4.457b?
17.12 4.49PB-SMT System with semi-supervised Berkeley Aligner +Single tokenized NE8?
20.87 4.71Table: 1 Evaluation results for different experimental setups.
(The ???
marked systems produce statis-tically significant improvements on BLEU over the baseline system)The baseline system (Exp 1) is the state-of-artPB-SMT system where GIZA++ with grow-diag-final-and has been used as the word alignmentmodel.
Experiment 2 provides better results thanexperiment 1 which signifies that BerkeleyAligner performs better than GIZA++ for theEnglish-Bengali translation task.
The union of allthee alignments (Exp 3) provides better scoresthan the baseline; however it cannot beat the re-sults obtained with the Berkeley Aligner alone.Hybrid alignment model with GIZA++ as thestandard alignment (Exp 4a) produces statistical-ly significant improvements over the baseline.Similarly the use of Berkeley Aligner as thestandard alignment for hybrid alignment model(Exp 4b) also results in statistically significantimprovements over Exp 2.
These two experi-ments (Exp 4a and 4b) demonstrate the effec-tiveness of the hybrid alignment model.
It is tobe noticed that hybrid alignment model worksbetter with the Berkeley Aligner than withGIZA++.Single-tokenization of the NEs (Exp 5, 6, 7aand 7b) improves the system performance tosome extent over the corresponding experimentswithout single-tokenization (Exp 1, 2, 4a and4b); however, these improvements are not statis-tically significant.
The Berkeley semi-supervisedalignment method using a bootstrapping ap-proach together with single-tokenization of NEsprovided the overall best performance in terms ofboth BLEU and NIST and the corresponding im-provement is statistically significant on BLEUover rest of the experiments.6 Conclusion and Future WorkThe paper proposes a hybrid word alignmentmodel for PB-SMT.
The paper also shows howeffective pre-processing of NEs in the parallelcorpus and direct incorporation of their align-ment in the word alignment model can improveSMT system performance.
In data driven ap-proaches to MT, specifically for scarce resourcedata, this approach can help to upgrade the state-of-art machine translation quality as well as theword alignment quality.
.
The hybrid model withthe use of the semi-supervised technique of theBerkeley word aligner in a bootstrapping manner,together with single tokenization of NEs, pro-vides substantial improvements (9.95 BLEUpoints absolute, 91.1% relative) over the base-line.
On manual inspection of the output wefound that our best system provides more accu-99rate lexical choice as well as better word order-ing than the baseline system.As future work we would like to explore howto get the best out of multiple word alignments.Furthermore, integrating the knowledge aboutmulti-word expressions into the word alignmentmodels is another future direction for this work.AcknowledgementThe work has been carried out with support fromthe project ?Development of English to IndianLanguages Machine Translation (EILMT) Sys-tem - Phase II?
funded by Department of Infor-mation Technology, Government of India.ReferencesAlexander Fraser and Daniel Marcu.
2006.
Semi-supervised training for statistical word alignment.In ACL-44: Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Compu-tational Linguistics (ACL-2006), Morristown, NJ,USA.
pages 769?776.Brown, Peter F., Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: pa-rameter estimation.
Computational Linguistics,19(2):263-311.Chris Callison-Burch, David Talbot, and Miles Os-borne.
2004.
Statistical machine translation withword- and sentence-aligned parallel corpora.
InACL 2004, page 175, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Dempster, A.P., N.M. Laird, and D.B.
Rubin.
1977).Maximum Likelihood from Incomplete Data viathe EM Algorithm.
Journal of the Royal StatisticalSociety, Series B (Methodological) 39 (1): 1?38.Doddington, George.
2002.
Automatic evaluation ofmachine translation quality using n-gram cooccur-rence statistics.
In Proceedings of the Second In-ternational Conference on Human Language Tech-nology Research (HLT-2002), San Diego, CA, pp.128-132.Eck, Matthias, Stephan Vogel, and Alex Waibel.2004.
Improving statistical machine translation inthe medical domain using the Unified MedicalLanguage System.
In Proc.
of the 20th Internation-al Conference on Computational Linguistics (COL-ING 2004), Geneva, Switzerland, pp.
792-798.Ekbal, Asif, and Sivaji Bandyopadhyay.
2008.
Maxi-mum Entropy Approach for Named Entity Recog-nition in Indian Languages.
International Journalfor Computer Processing of Languages (IJCPOL),Vol.
21 (3), 205-237.Ekbal, Asif, and Sivaji Bandyopadhyay.
2009.
VotedNER system using appropriate unlabeled data.
Inproceedings of the ACL-IJCNLP-2009 Named En-tities Workshop (NEWS 2009), Suntec, Singapore,pp.202-210.Feng, Donghui, Yajuan Lv, and Ming Zhou.
2004.
Anew approach for English-Chinese named entityalignment.
In Proceedings of the 2004 Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP-2004), Barcelona, Spain, pp.372-379.Feng, Donghui, Yajuan Lv, and Ming Zhou.
2004.
Anew approach for English-Chinese named entityalignment.
In Proceedings of the 2004 Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP-2004), Barcelona, Spain, pp.372-379.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignmentmodels.
Computational Linguistics, pages 19?51.Huang, Fei, Stephan Vogel, and Alex Waibel.
2003.Automatic extraction of named entity translingualequivalence based on multi-feature cost minimiza-tion.
In Proceedings of the ACL-2003 Workshopon Multilingual and Mixed-language Named EntityRecognition, 2003, Sapporo, Japan, pp.
9-16.HuaWu, HaifengWang, and Zhanyi Liu.
2006.
Boost-ing statistical word alignment using labeled and un-labeled data.
In Proceedings of the COLING/ACLon Main conference poster sessions, pages 913?920, Morristown, NJ, USA.
Association for Com-putational Linguistics.Kneser, Reinhard, and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
InProceedings of the IEEE Internation Conference onAcoustics, Speech, and Signal Processing(ICASSP), vol.
1, pp.
181?184.
Detroit, MI.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL 2003: conference com-bining Human Language Technology conferenceseries and the North American Chapter of the As-sociation for Computational Linguistics conferenceseries,  Edmonton, Canada, pp.
48-54.Koehn, Philipp, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Ber-toldi, Brooke Cowan, Wade Shen, Christine Mo-ran, Richard Zens, Chris Dyer, Ond?ej Bojar, Alex-andra Constantin, and Evan Herbst.
2007.
Moses:open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual meeting ofthe Association for Computational Linguistics(ACL 2007): Proceedings of demo and poster ses-sions, Prague, Czech Republic, pp.
177-180.Koehn, Philipp.
2004.
Statistical significance tests formachine translation evaluation.
In  EMNLP-2004:100Proceedings of the 2004 Conference on EmpiricalMethods in Natural Language Processing, 25-26July 2004, Barcelona, Spain, pp 388-395.O.
Chapelle, B. Sch?olkopf, and A. Zien, editors.2006.
Semi-Supervised Learning.
MIT Press,Cambridge, MA.Och, Franz J.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofthe 41st Annual Meeting of the Association forComputational Linguistics (ACL-2003), Sapporo,Japan, pp.
160-167.Pal, Santanu, Sivaji Bandyopadhyay.
2012, ?Boot-strapping Chunk Alignment in Phrase-Based Sta-tistical Machine Translation?, Joint Workshop onExploiting Synergies between Information Retriev-al and Machine Translation (ESIRMT) and HybridApproaches to Machine Translation (HyTra),EACL-2012, Avignon, France, pp.
93-100 .Pal, Santanu., Sudip Kumar Naskar, Pavel Pecina,Sivaji Bandyopadhyay and Andy Way.
2010, Han-dling Named Entities and Compound Verbs inPhrase-Based Statistical Machine Translation, Inproc.
of the workshop on Multiword expression:from theory to application (MWE-2010), The 23rdInternational conference of computational linguis-tics (Coling 2010),Beijing, Chaina, pp.
46-54.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for automat-ic evaluation of machine translation.
In Proceed-ings of the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL-2002), Phila-delphia, PA, pp.
311-318.Percy Liang, Ben Taskar, Dan Klein.
2006.
6th Pro-ceedings of the main conference on Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association of ComputationalLinguistics, HLT-NAACL-2006, Pages 104-111Stolcke, A. SRILM?An Extensible Language Mod-eling Toolkit.
Proc.
Intl.
Conf.
on Spoken Lan-guage Processing, vol.
2, pp.
901?904, Denver(2002).Vamshi Ambati, Stephan Vogel, Jaime Carbonell.2010, 10th Proceedings of the NAACL HLT 2010Workshop on Active Learning for Natural Lan-guage Processing (ALNLP-2010), Pages 10-17.Vogel, Stephan, Hermann Ney, and Christoph Till-mann.
1996.
HMM-based word alignment in statis-tical translation.
In Proc.
of the 16th InternationalConference on Computational Linguistics (COL-ING 1996), Copenhagen, pp.
836-841.Wu, Hua Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine transla-tion with domain dictionary and monolingual cor-pora.
In Proc.
of the 22nd International Conferenceon Computational Linguistics (COLING 2008),Manchester, UK, pp.
993-1000.X.
Zhu.
2005.
Semi-Supervised Learning LiteratureSurvey.
Technical Report 1530, Computer Scienc-es, University of Wisconsin-Madison.http://www.cs.wisc.edu/_jerryzhu/pub/ssl_survey.pdf.101
