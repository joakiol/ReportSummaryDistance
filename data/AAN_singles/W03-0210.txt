A Hybrid Text Classication Approach for Analysis of Student EssaysCarolyn P.
Rose?, Antonio Roque, Dumisizwe Bhembe, Kurt VanlehnLearning Research and Development Center, University of Pittsburgh,3939 O?Hara St., Pittsburgh, PA 15260rosecp,roque,bhembe,vanlehn@pitt.eduAbstractWe present CarmelTC, a novel hybrid text clas-sification approach for analyzing essay answersto qualitative physics questions, which buildsupon work presented in (Rose?
et al, 2002a).CarmelTC learns to classify units of text basedon features extracted from a syntactic analysisof that text as well as on a Naive Bayes clas-sification of that text.
We explore the trade-offs between symbolic and ?bag of words?
ap-proaches.
Our goal has been to combine thestrengths of both of these approaches whileavoiding some of the weaknesses.
Our evalu-ation demonstrates that the hybrid CarmelTCapproach outperforms two ?bag of words?
ap-proaches, namely LSA and a Naive Bayes, aswell as a purely symbolic approach.1 IntroductionIn this paper we describe CarmelTC, a novel hybridtext classification approach for analyzing essay answersto qualitative physics questions.
In our evaluation wedemonstrate that the novel hybrid CarmelTC approachoutperforms both Latent Semantic Analysis (LSA) (Lan-dauer et al, 1998; Laham, 1997) and Rainbow (Mc-Callum, 1996; McCallum and Nigam, 1998), which isa Naive Bayes approach, as well as a purely symbolicapproach similar to (Furnkranz et al, 1998).
WhereasLSA and Rainbow are pure ?bag of words?
approaches,CarmelTC is a rule learning approach where rules forclassifying units of text rely on features extracted froma syntactic analysis of that text as well as on a ?bagof words?
classification of that text.
Thus, our evalu-ation demonstrates the advantage of combining predic-tions from symbolic and ?bag of words?
approaches fortext classification.
Similar to (Furnkranz et al, 1998),neither CarmelTC nor the purely symbolic approach re-quire any domain specific knowledge engineering or textannotation beyond providing a training corpus of textsmatched with appropriate classifications, which is alsonecessary for Rainbow, and to a much lesser extent forLSA.CarmelTC was developed for use inside of the Why2-Atlas conceptual physics tutoring system (VanLehn et al,2002; Graesser et al, 2002) for the purpose of grad-ing short essays written in response to questions such as?Suppose you are running in a straight line at constantspeed.
You throw a pumpkin straight up.
Where will itland?
Explain.?
This is an appropriate task domain forpursuing questions about the benefits of tutorial dialoguefor learning because questions like this one are knownto elicit robust, persistent misconceptions from students,such as ?heavier objects exert more force.?
(Hake, 1998;Halloun and Hestenes, 1985).
In Why2-Atlas, a stu-dent first types an essay answering a qualitative physicsproblem.
A computer tutor then engages the student ina natural language dialogue to provide feedback, cor-rect misconceptions, and to elicit more complete expla-nations.
The first version of Why2-Atlas was deployedand evaluated with undergraduate students in the springof 2002; the system is continuing to be actively devel-oped (Graesser et al, 2002).In contrast to many previous approaches to automatedessay grading (Burstein et al, 1998; Foltz et al, 1998;Larkey, 1998), our goal is not to assign a letter gradeto student essays.
Instead, our purpose is to tally whichset of ?correct answer aspects?
are present in student es-says.
For example, we expect satisfactory answers to theexample question above to include a detailed explana-tion of how Newton?s first law applies to this scenario.From Newton?s first law, the student should infer that thepumpkin and the man will continue at the same constanthorizontal velocity that they both had before the release.Thus, they will always have the same displacement fromthe point of release.
Therefore, after the pumpkin risesand falls, it will land back in the man?s hands.
Our goalis to coach students through the process of constructinggood physics explanations.
Thus, our focus is on thephysics content and not the quality of the student?s writ-ing, in contrast to (Burstein et al, 2001).2 Student Essay AnalysisWe cast the Student Essay Analysis problem as a textclassification problem where we classify each sentence inthe student?s essay as an expression one of a set of ?cor-rect answer aspects?, or ?nothing?
in the case where no?correct answer aspect?
was expressed.After a student attempts an initial answer to the ques-tion, the system analyzes the student?s essay to assesswhich key points are missing from the student?s argu-ment.
The system then uses its analysis of the student?sessay to determine which help to offer that student.
Inorder to do an effective job at selecting appropriate inter-ventions for helping students improve their explanations,the system must perform a highly accurate analysis of thestudent?s essay.
Identifying key points as present in es-says when they are not (i.e., false alarms), cause the sys-tem to miss opportunities to help students improve theiressays.
On the other hand, failing to identify key pointsthat are indeed present in student essays causes the sys-tem to offer help where it is not needed, which can frus-trate and even confuse students.
A highly accurate inven-tory of the content of student essays is required in orderto avoid missing opportunities to offer needed instructionand to avoid offering inappropriate feedback, especiallyas the completeness of student essays increases (Rose?
etal., 2002a; Rose?
et al, 2002c).In order to compute which set of key points, i.e., ?cor-rect answer aspects?, are included in a student essay, wefirst segment the essay at sentence boundaries.
Note thatrun-on sentences are broken up.
Once an essay is seg-mented, each segment is classified as corresponding toone of the set of key points or ?nothing?
if it does notinclude any key point.
We then take an inventory of theclassifications other than ?nothing?
that were assigned toat least one segment.
Thus, our approach is similar inspirit to that taken in the AUTO-TUTOR system (Wiemer-Hastings et al, 1998), where Latent Semantic Analysis(LSA) (Landauer et al, 1998; Laham, 1997) was used totally which subset of ?correct answer aspects?
studentsincluded in their natural language responses to short es-say questions about computer literacy.We performed our evaluation over essays collectedfrom students interacting with our tutoring system in re-sponse to the question ?Suppose you are running in astraight line at constant speed.
You throw a pumpkinstraight up.
Where will it land?
Explain.
?, which we referto as the Pumpkin Problem.
Thus, there are a total of sixalternative classifications for each segment:Class 1 Sentence expresses the idea that after the releasethe only force acting on the pumpkin is the down-ward force of gravity.Class 2 Sentence expresses the idea that the pumpkincontinues to have a constant horizontal velocity afterit is released.Class 3 Sentence expresses the idea that the horizontalvelocity of the pumpkin continues to be equal to thehorizontal velocity of the man.Class 4 Sentence expresses the idea that the pumpkinand runner cover the same distance over the sametime.Class 5 Sentence expresses the idea that the pumpkinwill land on the runner.Class 6 Sentence does not adequately express any of theabove specified key points.Note that this classification task is strikingly differentfrom those typically used for evaluating text classifica-tion systems.
First, these classifications represent spe-cific whole propositions rather than general topics, suchas those used for classifying web pages (Craven et al,1998), namely ?student?, ?faculty?, ?staff?, etc.
Sec-ondly, the texts are much shorter, i.e., one sentence incomparison with a whole web page, which is a disadvan-tage for ?bag of words?
approaches.In some cases what distinguishes sentences from oneclass and sentences from another class is very subtle.For example, ?Thus, the pumpkin?s horizontal velocity,which is equal to that of the man when he released it, willremain constant.?
belongs to Class 2 although it couldeasily be mistaken for Class 3.
Similarly, ?So long asno other horizontal force acts upon the pumpkin while itis in the air, this velocity will stay the same.
?, belongsto Class 2 although looks similar on the surface to ei-ther Class 1 or 3.
A related problem is that sentencesthat should be classified as ?nothing?
may look very sim-ilar on the surface to sentences belonging to one or moreof the other classes.
For example, ?It will land on theground where the runner threw it up.?
contains all of thewords required to correctly express the idea correspond-ing to Class 5, although it does not express this idea, andin fact expresses a wrong idea.
These very subtle distinc-tions also pose problems for ?bag of words?
approachessince they base their decisions only on which words arepresent regardless of their order or the functional relation-ships between them.
That might suggest that a symbolicapproach involving syntactic and semantic interpretationmight be more successful.
However, while symbolic ap-proaches can be more precise than ?bag of words?
ap-proaches, they are also more brittle.
And approaches thatrely both on syntactic and semantic interpretation requirea larger knowledge engineering effort as well.3 CarmelTCFigure 1: This example shows the deep syntactic parseof a sentence.Sentence: The pumpkin moves slower because theman is not exerting a force on it.Deep Syntactic Analysis((clause2((mood *declarative)(root move)(tense present)(subj((cat dp)(root pumpkin)(specifier ((cat detp)(def +)(root the)))(modifier ((car adv) (root slow)))))))(clause2(mood *declarative)(root exert)(tense present)(negation +)(causesubj((cat dp)(root man)(agr 3s)(specifier((cat detp)(def +)(root the)))))(subj((cat dp)(root force)(specifier ((cat detp)(root a)))))(obj ((cat dp)(root it))))(connective because))The hybrid CarmelTC approach induces decision treesusing features from both a deep syntactic functional anal-ysis of an input text as well as a prediction from the Rain-bow Naive Bayes text classifier (McCallum, 1996; Mc-Callum and Nigam, 1998) to make a prediction about thecorrect classification of a sentence.
In addition, it usesfeatures that indicate the presence or absence of wordsfound in the training examples.
Since the Naive Bayesclassification of a sentence is more informative than anysingle one of the other features provided, CarmelTC canbe conceptualized as using the other features to decidewhether or not to believe the Naive Bayes classification,and if not, what to believe instead.From the deep syntactic analysis of a sentence, we ex-tract individual features that encode functional relation-Figure 2: This example shows the features extractedfrom the deep syntactic parse of a sentence.Sentence: The pumpkin moves slower because theman is not exerting a force on it.Extracted Features(tense-move present)(subj-move pumpkin)(specifier-pumpkin the)(modifier-move slow)(tense-exert present)(negation-exert +)(causesubj-exert man)(subj-exert force)(obj-exert it)(specifier-force a)(specifier-man the)ships between syntactic heads (e.g., (subj-throw man)),tense information (e.g., (tense-throw past)), and infor-mation about passivization and negation (e.g., (negation-throw +) or (passive-throw -)).
See Figures 1 and 2.
Rain-bow has been used for a wide range of text classificationtasks.
With Rainbow, P(doc,Class), i.e., the probability ofa document belonging to class Class, is estimated by mul-tiplying P(Class), i.e., the prior probability of the class,by the product over all of the words   found in the text of 	 , i.e., the probability of the word given thatclass.
This product is normalized over the prior probabil-ity of all words.
Using the individual features extractedfrom the deep syntactic analysis of the input as well asthe ?bag of words?
Naive Bayes classification of the in-put sentence, CarmelTC builds a vector representationof each input sentence, with each vector position corre-sponding to one of these features.
We then use the ID3decision tree learning algorithm (Mitchell, 1997; Quin-lin, 1993) to induce rules for identifying sentence classesbased on these feature vectors.The symbolic features used for the CarmelTC ap-proach are extracted from a deep syntactic functionalanalysis constructed using the CARMEL broad coverageEnglish syntactic parsing grammar (Rose?, 2000) and thelarge scale COMLEX lexicon (Grishman et al, 1994),containing 40,000 lexical items.
For parsing we use anincremental version of the LCFLEX robust parser (Rose?et al, 2002b; Rose?
and Lavie, 2001), which was designedfor efficient, robust interpretation.
While computing adeep syntactic analysis is more computationally expen-sive than computing a shallow syntactic analysis, we cando so very efficiently using the incrementalized versionof LCFLEX because it takes advantage of student typ-ing time to reduce the time delay between when studentssubmit their essays and when the system is prepared torespond.Syntactic feature structures produced by the CARMELgrammar factor out those aspects of syntax that modifythe surface realization of a sentence but do not changeits deep functional analysis.
These aspects include tense,negation, mood, modality, and syntactic transformationssuch as passivization and extraction.
In order to do thisreliably, the component of the grammar that performs thedeep syntactic analysis of verb argument functional re-lationships was generated automatically from a featurerepresentation for each of COMLEX?s verb subcatego-rization tags.
It was verified that the 91 verb subcatego-rization tags documented in the COMLEX manual werecovered by the encodings, and thus by the resulting gram-mar rules.
These tags cover a wide range of patterns ofsyntactic control and predication relationships.
Each tagcorresponds to one or more case frames.
Each case framecorresponds to a number of different surface realizationsdue to passivization, relative clause extraction, and wh-movement.
Altogether there are 519 syntactic patternscovered by the 91 subcategorization tags, all of which arecovered by the grammar.There are nine syntactic functional roles assigned bythe grammar.
These roles include subj (subject), caus-esubj (causative subject), obj (object), iobj (indirect ob-ject), pred (descriptive predicate, like an adjectival phraseor an adverb phrase), comp (a clausal complement), mod-ifier, and possessor.
The roles pertaining to the rela-tionship between a verb and its arguments are assignedbased on the subcat tags associated with verbs in COM-LEX.
However, in some cases, arguments that COM-LEX assigns the role of subject get redefined as caus-esubj (causative subject).
For example, the subject in ?thepumpkin moved?
is just a subject but in ?the man movedthe pumpkin?, the subject would get the role causesubjinstead since ?move?
is a causative-inchoative verb andthe obj role is filled in in the second case 1.
The modifierrole is used to specify the relationship between any syn-tactic head and its adjunct modifiers.
Possessor is usedto describe the relationship between a head noun and itsgenitive specifier, as in man in either ?the man?s pump-kin?
or ?the pumpkin of the man?.With the hybrid CarmelTC approach, our goal has beento keep as many of the advantages of both symbolic anal-ysis as well as ?bag of words?
classification approachesas possible while avoiding some of the pitfalls of each.Since the CarmelTC approach does not use the syntacticanalysis as a whole, it does not require that the system beable to construct a totally complete and correct syntacticanalysis of the student?s text input.
It can very effectively1The causative-inchoative verb feature is one that we addedto verb entries in COMLEX, not one of the features providedby the lexicon originally.make use of partial parses.
Thus, it is more robust thanpurely symbolic approaches where decisions are based oncomplete analyses of texts.
And since it makes use onlyof the syntactic analysis of a sentence, rather than alsomaking use of a semantic interpretation, it does not re-quire any sort of domain specific knowledge engineering.And yet the syntactic features provide information nor-mally not available to ?bag of words?
approaches, suchas functional relationships between syntactic heads andscope of negation and other types of modifiers.4 Related Work: Combining Symbolic andBag of Words ApproachesCarmelTC is most similar to the text classification ap-proach described in (Furnkranz et al, 1998).
In the ap-proach described in (Furnkranz et al, 1998), features thatnote the presence or absence of a word from a text aswell as extraction patterns from AUTOSLOG-TS (Riloff,1996) form the feature set that are input to the RIPPER(Cohen, 1995), which learns rules for classifying textsbased on these features.
CarmelTC is similar in spiritin terms of both the sorts of features used as well as thegeneral sort of learning approach.
However, CarmelTC isdifferent from (Furnkranz et al, 1998) in several respects.Where (Furnkranz et al, 1998) make use ofAUTOSLOG-TS extraction patterns, CarmelTC makesuse of features extracted from a deep syntactic analysisof the text.
Since AUTOSLOG-TS performs a surfacesyntactic analysis, it would assign a different representa-tion to all aspects of these texts where there is variation inthe surface syntax.
Thus, the syntactic features extractedfrom our syntactic analyses are more general.
For exam-ple, for the sentence ?The force was applied by the manto the object?, our grammar assigns the same functionalroles as for ?The man applied the force to the object?
andalso for the noun phrase ?the man that applied the force tothe object?.
This would not be the case for AUTOSLOG-TS.Like (Furnkranz et al, 1998), we also extract wordfeatures that indicate the presence or absence of a rootform of a word from the text.
However, in contrast forCarmelTC one of the features for each training text that ismade available to the rule learning algorithm is the clas-sification obtained using the Rainbow Naive Bayes clas-sifier (McCallum, 1996; McCallum and Nigam, 1998).Because the texts classified with CarmelTC are somuch shorter than those of (Furnkranz et al, 1998), thefeature set provided to the learning algorithm was smallenough that it was not necessary to use a learning algo-rithm as sophisticated as RIPPER (Cohen, 1995).
Thus,we used ID3 (Mitchell, 1997; Quinlin, 1993) instead withexcellent results.
Note that in contrast to CarmelTC, the(Furnkranz et al, 1998) approach is purely symbolic.Thus, all of its features are either word level features orsurface syntactic features.Recent work has demonstrated that combining multi-ple predictors yields combined predictors that are supe-rior to the individual predictors in cases where the in-dividual predictors have complementary strengths andweaknesses (Larkey and Croft, 1996; Larkey and Croft,1995).
We have argued that this is the case with symbolicand ?bag of words?
approaches.
Thus, we have reason toexpect a hybrid approach that makes a prediction basedon a combination of these single approaches would yieldbetter results than either of these approaches alone.
Ourresults presented in Section 5 demonstrate that this is true.Other recent work has demonstrated that symbolic and?Bag of Words?
approaches can be productively com-bined.
For example, syntactic information can be usedto modify the LSA space of a verb in order to make LSAsensitive to different word senses (Kintsch, 2002).
How-ever, this approach has only been applied to the analysisof mono-transitive verbs.
Furthermore, it has never beendemonstrated to improve LSA?s effectiveness at classify-ing texts.In the alternative Structured Latent Semantic Analy-sis (SLSA) approach, hand-coded subject-predicate in-formation was used to improve the results obtained byLSA for text classification (Wiemer-Hastings and Zipi-tria, 2001), but no fully automated evaluation of this ap-proach has been published.In contrast to these two approaches, CarmelTC is bothfully automatic, in that the symbolic features it uses areobtained without any hand coding whatsoever, and fullygeneral, in that it applies to the full range of verb subcat-egorization frames covered by the COMLEX lexicon, notonly mono-transitive verbs.
In Section 5 we demonstratethat CarmelTC outperforms both LSA and Rainbow, twoalternative bag of words approaches, on the task of stu-dent essay analysis.5 EvaluationWe conducted an evaluation to compare the effective-ness of CarmelTC at analyzing student essays in compar-ison to LSA, Rainbow, and a purely symbolic approachsimilar to (Furnkranz et al, 1998), which we refer tohere as CarmelTCsymb.
CarmelTCsymb is identical toCarmelTC except that it does not include in its featureset the prediction from Rainbow.
Thus, by comparingCarmelTC with Rainbow and LSA, we can demonstratethe superiority of our hybrid approach to purely ?bag ofwords?
approaches.
And by comparing with CarmelTC-symb, we can demonstrate the superiority of our hybridapproach to an otherwise equivalent purely symbolic ap-proach.We conducted our evaluation over a corpus of 126 pre-viously unseen student essays in response to the PumpkinProblem described above, with a total of 500 text seg-ments, and just under 6000 words altogether.
We firsttested to see if the text segments could be reliably taggedby humans with the six possible Classes associated withthe problem.
Note that this includes ?nothing?
as a class,i.e., Class 6.
Three human coders hand classified textsegments for 20 essays.
We computed a pairwise Kappacoefficient (Cohen, 1960) to measure the agreement be-tween coders, which was always greater than .75, thusdemonstrating good agreement according to the Krippen-dorf scale (Krippendorf, 1980).
We then selected twocoders to individually classify the remaining sentences inthe corpus.
They then met to come to a consensus onthe tagging.
The resulting consensus tagged corpus wasused as a gold standard for this evaluation.
Using thisgold standard, we conducted a comparison of the fourapproaches on the problem of tallying the set of ?correctanswer aspects?
present in each student essay.The LSA space used for this evaluation was trainedover three first year physics text books.
The other threeapproaches are trained over a corpus of tagged examplesusing a 50 fold random sampling evaluation, similar to across-validation methodology.
On each iteration, we ran-domly selected a subset of essays such that the numberof text segments included in the test set were greater than10 but less than 15.
The randomly selected essays werethen used as a test set for that iteration, and the remain-der of the essays were used for training in addition to acorpus of 248 hand tagged example sentences extractedfrom a corpus of human-human tutoring transcripts inour domain.
The training of the three approaches dif-fered only in terms of how the training data was parti-tioned.
Rainbow and CarmelTCsymb were trained us-ing all of the example sentences in the corpus as a singletraining set.
CarmelTC, on the other hand, required parti-tioning the training data into two subsets, one for trainingthe Rainbow model used for generating the value of itsRainbow feature, and one subset for training the decisiontrees.
This is because for CarmelTC, the data for train-ing Rainbow must be separate from that used to train thedecision trees so the decision trees are trained from a re-alistic distribution of assigned Rainbow classes based onits performance on unseen data rather than on Rainbow?straining data.In setting up our evaluation, we made it our goal topresent our competing approaches in the best possiblelight in order to provide CarmelTC with the strongestcompetitors as possible.
Note that LSA works by usingits trained LSA space to construct a vector representationfor any text based on the set of words included therein.
Itcan thus be used for text classification by comparing thevector obtained for a set of exemplar texts for each classwith that obtained from the text to be classified.
We testedLSA using as exemplars the same set of examples usedFigure 3: This Table compares the performance of the 4 alternative approaches in the per essay evaluation interms of precision, recall, false alarm rate, and f-score.Approach Precision Recall False Alarm Rate F-ScoreLSA 93% 54% 3% .70Rainbow 81% 73% 9% .77CarmelTCsymb 88% 72% 7% .79CarmelTC 90% 80% 8% .85as Rainbow training data, but it always performed betterwhen using a small set of hand picked exemplars.
Thus,we present results here using only those hand picked ex-emplars.
For every approach except LSA, we first seg-mented the essays at sentence boundaries and classifiedeach sentence separately.
However, for LSA, rather thanclassify each segment separately, we compared the LSAvector for the entire essay to the exemplars for each class(other than ?nothing?
), since LSA?s performance is betterwith longer texts.
We verified that LSA also performedbetter specifically on our task under these circumstances.Thus, we compared each essay to each exemplar, and wecounted LSA as identifying the corresponding ?correctanswer aspect?
if the cosine value obtained by compar-ing the two vectors was above a threshold.
We testedLSA with threshold values between .1 and .9 at incre-ments of .1 as well as testing a threshold of .53 as isused in the AUTO-TUTOR system (Wiemer-Hastings etal., 1998).
As expected, as the threshold increases from.1 to .9, recall and false alarm rate both decrease togetheras precision increases.
We determined based on comput-ing f-scores2 for each threshold level that .53 achieves thebest trade off between precision and recall.
Thus, we useda threshold of .53, to determine whether LSA identifiedthe corresponding key point in the student essay or notfor the evaluation presented here.We evaluated the four approaches in terms of precision,recall, false alarm rate, and f-score, which were computedfor each approach for each test essay, and then averagedover the whole set of test essays.
We computed preci-sion by dividing the number of ?correct answer aspects?
(CAAs) correctly identified by the total number of CAAsidentified3 We computed recall by dividing the number ofCAAs correctly identified over the number of CAAs actu-ally present in the essay4 False alarm rate was computedby dividing the number of CAAs incorrectly identified bythe total number of CAAs that could potentially be incor-2We computed our f-scores with a beta value of 1 in order totreat precision and recall as equally important.3For essays containing no CAAs, we counted precision as 1if none were identified and 0 otherwise.4For essays with no CAAs present, we counted recall as 1for all approaches.rectly identified5.
F-scores were computed using 1 as thebeta value in order to treat precision and recall as equallyimportant.The results presented in Figure 3 clearly demon-strate that CarmelTC outperforms the other approaches.In particular, CarmelTC achieves the highest f-score,which combines the precision and recall scores into asingle measure.
In comparison with CarmelTCsymb,CarmelTC achieves a higher recall as well as a slightlyhigher precision.
While LSA achieves a slightly higherprecision, its recall is much lower.
Thus, the differencebetween the two approaches is clearly shown in the f-score value, which strongly favors CarmelTC.
Rainbowachieves a lower score than CarmelTC in terms of preci-sion, recall, false alarm rate, and f-score.6 Conclusion and Current DirectionsIn this paper we have introduced the CarmelTC text clas-sification approach as it is applied to the problem of stu-dent essay analysis in the context of a conceptual physicstutoring system.
We have evaluated CarmelTC over datacollected from students interacting with our system in re-sponse to one of its 10 implemented conceptual physicsproblems.
Our evaluation demonstrates that the novelhybrid CarmelTC approach outperforms both Latent Se-mantic Analysis (LSA) (Landauer et al, 1998; Laham,1997) and a Naive Bayes approach (McCallum, 1996;McCallum and Nigam, 1998) as well as a purely sym-bolic approach similar to (Furnkranz et al, 1998).
Weplan to run a larger evaluation with essays from multipleproblems to test the generality of our result.
We also planto experiment with other rule learning approaches, suchas RIPPER (Cohen, 1995).7 AcknowledgmentsThis research was supported by the Office of Naval Re-search, Cognitive Science Division under grant numberN00014-0-1-0600 and by NSF grant number 9720359to CIRCLE, Center for Interdisciplinary Research onConstructive Learning Environments at the University ofPittsburgh and Carnegie Mellon University.5For essays containing all possible CAAs, false alarm ratewas counted as 0 for all approaches.ReferencesJ.
Burstein, K. Kukich, S. Wolff, C. Lu, M. Chodorow,L.
Braden-Harder, and M. D. Harris.
1998.
Au-tomated scoring using a hybrid feature identificationtechnique.
In Proceedings of COLING-ACL?98, pages206?210.J.
Burstein, D. Marcu, S. Andreyev, and M. Chodorow.2001.
Towards automatic classification of discourseelements in essays.
In Proceedings of the 39th AnnualMeeting of the Association for Computational Linguis-tics, Toulouse, France.J.
Cohen.
1960.
A coefficient of agreement for nominalscales.
Educational and Psychological Measurement,20(Winter):37?46.W.
W. Cohen.
1995.
Fast effective rule induction.
InProceedings of the 12th International Conference onMachine Learning.M.
Craven, D. DiPasquio, D. Freitag, A. McCallum,T.
Mitchell, K. Nigam, and S. Slattery.
1998.
Learn-ing to extract symbolic knowledge from the world wideweb.
In Proceedings of the 15th National Conferenceon Articial Intelligence.P.
W. Foltz, W. Kintsch, and T. Landauer.
1998.
Themeasurement of textual coherence with latent semanticanalysis.
Discourse Processes, 25(2-3):285?307.J.
Furnkranz, T. Mitchell Mitchell, and E. Riloff.
1998.A case study in using linguistic phrases for text cat-egorization on the www.
In Proceedings from theAAAI/ICML Workshop on Learning for Text Catego-rization.A.
Graesser, K. Vanlehn, TRG, and NLT Group.
2002.Why2 report: Evaluation of why/atlas, why/autotutor,and accomplished human tutors on learning gains forqualitative physics problems and explanations.
Tech-nical report, LRDC Tech Report, University of Pitts-burgh.R.
Grishman, C. Macleod, and A. Meyers.
1994.
COM-LEX syntax: Building a computational lexicon.
InProceedings of the 15th International Conference onComputational Linguistics (COLING-94).R.
R. Hake.
1998.
Interactive-engagement versus tra-ditional methods: A six-thousand student survey ofmechanics test data for introductory physics students.American Journal of Physics, 66(64).I.
A. Halloun and D. Hestenes.
1985.
The initial knowl-edge state of college physics students.
American Jour-nal of Physics, 53(11):1043?1055.W.
Kintsch.
2001.
Predication.
Cognitive Science,25:173?202.K.
Krippendorf.
1980.
Content Analysis: An Introduc-tion to Its Methodology.
Sage Publications.D.
Laham.
1997.
Latent semantic analysis approachesto categorization.
In Proceedings of the Cognitive Sci-ence Society.T.
K. Landauer, P. W. Foltz, and D. Laham.
1998.
In-troduction to latent semantic analysis.
Discourse Pro-cesses, 25(2-3):259?284.L.
S. Larkey and W. B. Croft.
1995.
Automatic assign-ment of icd9 codes to discharge summaries.
TechnicalReport IR-64, University of Massachusetts Center forIntelligent Information Retrieval.L.
S. Larkey and W. B. Croft.
1996.
Combining classi-fiers in text categorization.
In Proceedings of SIGIR.L.
Larkey.
1998.
Automatic essay grading using textcategorization techniques.
In Proceedings of SIGIR.A.
McCallum and K. Nigam.
1998.
A comparison ofevent models for naive bayes text classification.
InProceedings of the AAAI-98 Workshop on Learning forText Classication.Andrew Kachites McCallum.
1996.
Bow: A toolkitfor statistical language modeling, text retrieval, clas-sification and clustering.
http://www.cs.cmu.edu/ mc-callum/bow.T.
M. Mitchell.
1997.
Machine Learning.
McGraw Hill.J.
R. Quinlin.
1993.
C4.5: Programs for Machine Learn-ing.
Morgan Kaufmann Publishers: San Mateo, CA.E.
Riloff.
1996.
Using learned extraction patterns for textclassification.
In S. Wermter, R. Riloff, and G. Scheler,editors, Connectionist, Statistical, and Symbolic Ap-proaches for Natural Language Processing.
Springer-Verlag.C.
P. Rose?
and A. Lavie.
2001.
Balancing robustnessand efficiency in unification augmented context-freeparsers for large practical applications.
In J. C. Junquaand G. Van Noord, editors, Robustness in Languageand Speech Technologies.
Kluwer Academic Press.C.
P.
Rose?, D. Bhembe, A. Roque, S. Siler, R. Srivas-tava, and K. Vanlehn.
2002a.
A hybrid language un-derstanding approach for robust selection of tutoringgoals.
In Proceedings of the Intelligent Tutoring Sys-tems Conference.C.
P.
Rose?, D. Bhembe, A. Roque, and K. VanLehn.2002b.
An efficient incremental architecture for ro-bust interpretation.
In Proceedings of the Human Lan-guages Technology Conference, pages 307?312.C.
P.
Rose?, P. Jordan, and K. VanLehn.
2002c.
Can wehelp students with high initial competency?
In Pro-ceedings of the ITS Workshop on Empirical Methodsfor Tutorial Dialogue Systems.C.
P. Rose?.
2000.
A framework for robust semantic in-terpretation.
In Proceedings of the First Meeting of theNorth American Chapter of the Association for Com-putational Linguistics, pages 311?318.K.
VanLehn, P. Jordan, C. P.
Rose?, and The Natural Lan-guag e Tutoring Group.
2002.
The architecture ofwhy2-atlas: a coach for qualitative physics essay writ-ing.
In Proceedings of the Intelligent Tutoring SystemsConference, pages 159?167.P.
Wiemer-Hastings and I. Zipitria.
2001.
Rules forsyntax, vectors for semantics.
In Proceedings of theTwenty-third Annual Conference of the Cognitive Sci-ence Society.P.
Wiemer-Hastings, A. Graesser, D. Harter, and the Tu-toring Res earch Group.
1998.
The foundationsand architecture of autotutor.
In B. Goettl, H. Halff,C.
Redfield, and V. Shute, editors, Intelligent Tutor-ing Systems: 4th International Conference (ITS ?98 ),pages 334?343.
Springer Verlag.
