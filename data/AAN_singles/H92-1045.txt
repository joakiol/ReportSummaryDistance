One Sense Per  D iscourseWilliam A. GaleKenneth W. ChurchDavid YarowskyAT&T Bell Laboratories600 Mountain AvenueMurray Hill NJ 07974-06361.
ABSTRACTIt is well-known that there are polysemous words likesentence whose "meaning" or "sense" depends on thecontext of use.
We have recently reported on two newword-sense disambiguation systems, one trained on bilin-gual material (the Canadian Hansards) and the othertrained on monolingual material (Roget's Thesaurus andGrolier's Encyclopedia).
As this work was nearing com-pletion, we observed a very strong discourse ffect.
Thatis, if a polysemous word such as sentence appears twoor more times in a well-written discourse, it is extremelylikely that they will all share the same sense.
This paperdescribes an experiment which confirmed this hypothesisand found that the tendency to share sense in the samediscourse is extremely strong (98%).
This result can beused as an additional source of constraint for improvingthe performance of the word-sense disambiguation algo-rithm.
In addition, it could also be used to help evaluatedisambiguation algorithms that did not make use of thediscourse constraint.2.
OUR PREVIOUS WORK ONWORD-SENSE D ISAMBIGUATION2.1.
Data  DeprivationAlthough there has been a long history of work onword-sense disambiguation, much of the work has beenstymied by difficulties in acquiring appropriate testingand training materials.
AI approaches have tended tofocus on "toy" domains because of the difficulty in ac-quiring large lexicons.
So too, statistical approaches,e.g., Kelly and Stone (1975), Black (1988), have tendedto focus on a relatively small set of polysemous wordsbecause they have depended on extremely scarce hand-tagged materials for use in testing and training.We have achieved considerable progress recently by usinga new source of testing and training materials and theapplication of Bayesian discrimination methods.
Ratherthan depending on small amounts of hand-tagged text,we have been making use of relatively large amounts ofparallel text, text such as the Canadian Hansards, whichare available in multiple languages.
The translation canoften be used in lieu of hand-labeling.
For example, con-sider the polysemous word sentence, which has two ma-jor senses: (1) a judicial sentence, and (2), a syntacticsentence.
We can collect a number of sense (1) examplesby extracting instances that are translated as peine, andwe can collect a number of sense (2) examples by extract-ing instances that are translated as phrase.
In this way,we have been able to acquire a considerable amount oftesting and training material for developing and testingour disambiguation algorithms.The use of bilingual materials for discrimination deci-sions in machine tranlation has been discussed by Brownand others (1991), and by Dagan, Itai, and Schwall(1991).
The use of bilingual materials for an essentiallymonolingual purpose, sense disambiguation, is similar inmethod, but differs in purpose.2.2.
Bayesian DiscriminationSurprisingly good results can be achieved using Bayesiandiscrimination methods which have been used very suc-cessfully in many other applications, especially authoridentification (Mosteller and Wallace, 1964) and infor-mation retrieval (IR) (Salton, 1989, section 10.3).
Ourword-sense disambiguation algorithm uses the words ina 100-word context 1 surrounding the polysemous wordvery much like the other two applications use the wordsin a test document.Information Retreival (IR):Pr(wlret)I1  'r(wlirrd)w in  doel i t  is common to use very smal l  contexts  (e.g., 5-words) basedon the observat ion that  people do not  need very much context inorder to performance the d isambiguat ion  task.
In contrast ,  we usemuch larger contexts  (e.g., 100 words).
A l though people may beable to make do with much less context,  we believe the machineneeds all the help it can get, and  we have found that  the largercontext  makes  the task much easier.
In fact, we have been ableto  measure  in format ion at extremely large d istances (10,000 wordsaway from the po lysemous word in quest ion),  though obviouslymost  of the useful in format ion appears  relatively near  the poly-semous word (e.g., within the first 100 words or so).
Needless tosay, our 100-word contexts  are considerably larger than  the smal ler5-word windows that  one normal ly  f inds in the l i terature.233Author Identification:Pr(w\[authorl)rI Pr(wlauthor2)w in  doeWord-Sense Disambiguation:Pr(wlsensel)~I Pr(wlsense2)w in  contes tThis model treats the context as a bag of words and ig-nores a number of important linguistic factors such asword order and collocations (correlations among wordsin the context).
Nevertheless, even with these over-simplifications, the model still contains an extremelylarge number of parameters: 2V ~ 200,000.
It is anon-trivial task to estimate such a large number of pa-rameters, especially given the sparseness of the trainingdata.
The training material typically consists of approx-imately 12,000 words of text (100 words words of contextfor 60 instances of each of two senses).
Thus, there aremore than 15 parameters to be estimated from each datapoint.
Clearly, we need to be fairly careful given that wehave so many parameters and so little evidence.2.3.
Parameter EstimationIn principle, the conditional probabilities Pr(toklsense )can be estimated by selecting those parts of the entirecorpus which satisfy the required conditions (e.g., 100-word contexts urrounding instances of one sense of sen-fence, counting the frequency of each word, and dividingthe counts by the total number of words satisfying theconditions.
However, this estimate, which is known asthe maximum likelihood estimate (MLE), has a numberof well-known problems.
In particular, it will assign zeroprobability to words that do not happen to appear inthe sample, which is obviously unacceptable.We will estimate Pr(toklsense ) by interpolating betweenlocal probabilities, probabilities computed over the 100-word contexts, and global probabilities, probabilitiescomputed over the entire corpus.
There is a trade-off be-tween measurement error and bias error.
The local prob-abilities tend to be more prone to measurement errorwhereas the global probabilities tend to be more proneto bias error.
We seek to determine the relevance of thelarger corpus to the conditional sample in order to findthe optimal trade-off between bias and measurement er-ror.The interpolation procedure makes use of a prior expec-tation of how much we expect the local probabilities todiffer from the global probabilities.
In their author iden-tification work Mosteller and Wallace "expect\[ed\] bothauthors to have nearly identical rates for almost anyword" (p. 61).
In fact, just as they had anticipated,we have found that only 2% of the vocabulary in theFederalist corpus has significantly different probabilitiesdepending on the author.
In contrast, we expect fairlylarge differences in the sense disambiguation application.Approximately 20% of the vocabulary in the Hansardshas a local probability that is significantly different fromits global probability.
Since the prior expectation de-pends so much on the application, we set the prior for aparticular application by estimating the fraction of thevocabulary whose local probabilities differ significantlyfrom the global probabilities.2.4.
A Small StudyWe have looked at six polysemous nouns in some de-tail: duty, drug, land, language, position and sentence,as shown in Table 1.
The final column shows that per-formance is quite encouraging.Table 1: Six Polysemous WordsEnglish French sense N %duty droit tax 1114 97devoir obligation 691 84drug m~dicament medical 2992 84drogue illicit 855 97land terre property 1022 86pays country 386 89language langue medium 3710 90langage style 170 91position position place 5177 82poste job 577 86sentence peine judicial 296 97phrase grammatical 148 100These nouns were selected because they could be disam-biguated by looking at their French translation in theCanadian Hansards (unlike a polysemous word such asinterest whose French translation inter~1 is just as am-biguous as the English source).
In addition, for test-ing methodology, it is helpful that the corpus containplenty of instances of each sense.
The second condition,for example, would exclude bank, perhaps the canoni-cal example of a polysemous noun, since there are veryfew instances of the "river" sense of bank in our corpusof Canadian Hansards.
We were somewhat surprisedto discover that these two conditions are actually fairlystringent, and that there are a remarkably small numberof polysemous words which (1) can be disambiguated bylooking at the French translation, and (2) appear 150 ormore times in two or more senses.234Table 2: Automatic Sense Tagging Using Roget's CategoriesInput  OutputTreadmills attached to cranes were used to lift heavy objects TOOLS/MACHINERY (348)and for supplying power for cranes, hoists, and lifts.
The centrifug TOOLS/MACHINERY (348)Above this height, a tower crane is often used.
This comprises TOOLS/MACHINERY (348)elaborate courtship rituals cranes build a nest of vegetation on ANIMALS/INSECTS (414)are more closely related to cranes and rails.
They range in length ANIMALS/INSECTS (414)low trees..PP At least five crane species are in danger of extincti ANIMALS/INSECTS (414)2.5.
Tra in ing on Monolingual MaterialAt first, we thought that the method was completely de-pendent on the availability of parallel corpora for train-ing.
This has been a problem since parallel text remainssomewhat difficult o obtain in large quantity, and whatlittle is available is often fairly unbalanced and unrepre-sentative of general language.
Moreover, the assumptionthat differences in translation correspond to differencesin word-sense has always been somewhat suspect.Recently, Yarowsky (1991) has found a way to train onthe Roget's Thesaurus (Chapman, 1977) and Grolier'sEncyclopedia (1991) instead of the Hansards, thus cir-cumventing many of the objections to our use of theHansards.
Yarowsky's method inputs a 100-word con-text surrounding a polysemous word and scores each ofthe 1042 Roget Categories by:H Pr(wlRoget Categoryi)w in  contes tTable 2 shows some results for the polysemous nouncrane.Each of the 1042 models, Pr(wlRoget Categoryl), istrained by interpolating between local probabilities andglobal probabilities just as before.
However, the localprobabilities are somewhat more difficult to obtain inthis case since we do not have a corpus tagged withRoget Categories, and therefore, it may not be obvi-ous how to extract subsections of the corpus meetingthe local conditions.
Consider the Roget Category:TOOLS/MACHINERY (348).
Ideally, we would extract100-word contexts in the 10 million word Grolier En-cyclopedia surrounding words in category 348 and usethese to compute the local probabilities.
Since we don'thave a tagged corpus, Yarowsky suggested extractingcontexts around all words in category 348 and weightingappropriately in order to compensate for the fact thatsome of these contexts hould not have been included inthe training set.
Table 3 below shows a sample of the30,924 concordances for the words in category 348.Table 3: Some Concordances for TOOLS/MACHINERYCARVING .SB The gutterequipment such as a hydraulicResembling a powerequipment, valves for nuclear8000 BC, flint-edged woodenel-penetrating carbide-tippedheightens the colors .SBtraditional ABC method andcenter of rotation .PP A towermarshy areas .SB The crownedadz has a concave blade for formshovel capable of lifting 26 cubicshovel mounted on a floating hull,generators,  oil-refinery turbines,sickles were used to gather wilddri l ls  forced manufacturers to findDr i l l s  live in the forests ofdri l l  were unchanged, andcrane is an assembly of fabricatedcrane, however, occasionally nestsNote that some of the words in category 348 are polyse-mous (e.g., drill and crane), and consequently, not all oftheir contexts hould be included in the training set forcategory 348.
In particular, lines 7, 8 and 10 in Table 3illustrate the problem.
If one of these spurious enses wasfrequent and dominated the set of examples, the situa-tion could be disastrous.
An attempt is made to weightthe concordance data to minimize this effect and to makethe sample representative of all tools and machinery, notjust the more common ones.
If a word such as drill oc-curs k times in the corpus, all words in the context ofdrill contribute weight 1/k to frequency sums.
Althoughthe training materials still contain a substantial level ofnoise, we have found that the resulting models workremarkably well, nontheless.
Yarowsky (1991) reports93% correct disambiguation, averaged over the followingwords selected from the word-sense disambiguation liter-ature: bow, bass, galley, mole, sentence, slug, star, duty,issue, taste, cone, interest.3.
A HYPOTHESIS :  ONE SENSEPER DISCOURSEAs this work was nearing completion, we observed thatsenses tend to appear in clumps.
In particular, it ap-peared to be extremely unusual to find two or moresenses of a polysemous word in the same discourse.
2 Asimple (but non-blind) preliminary experiment providedsome suggestive evidence confirming the hypothesis.
Arandom sample of 108 nouns was extracted for further2This hypothes is  might  help to expla in some of the long-rangeeffects ment ioned in the previous footnote.235study.
A panel of three judges (the authors of this pa-per) were given 100 sets of concordance lines.
Each setshowed all of the instances of one of the test words in aparticular Grolier's article.
The judges were asked to in-dicate if the set of concordance lines used the same senseor not.
Only 6 of 300 w,-ticle-judgements were judged tocontain multiple senses of one of the test words.
Allthree judges were convinced after grading 100 articlesthat there was considerable validity to the hypothesis.With this promising preliminary verification, the follow-ing blind test was devised.
Five subjects (the three au-thors and two of their colleagues) were given a ques-tionnaire starting with a set of definitions elected fromOALD (Crowie et al, 1989) and followed by a numberof pairs of concordance lines, randomly selected fromGrolier's Encyclopedia (1991).
The subjects were askedto decide for each pair, whether the two concordancelines corresponded to the same sense or not.antenna1.
jointed organ found in pairs on the headsof insects and crustaceans, used for feeling,etc.
~ the illus at insect.2.
radio or TV aerial.lack eyes, legs, wings, antennae and distinct mouthpartsThe Brachycera have short antennae and include the more evolThe questionnaire contained a total of 82 pairs of concor-dance lines for 9 polysemous words: antenna, campaign,deposit, drum, hull, interior, knife, landscape, and ma-rine.
54 of the 82 pairs were selected from the samediscourse.
The remaining 28 pairs were introduced as acontrol to force the judges to say that some pairs weredifferent; they were selected from different discourses,and were checked by hand as an attempt o assure thatthey did not happen to use the same sense.
The judgesfound it quite easy to decide whether the pair used thesame sense or not.
Table 4 shows that there was veryhigh agreement among the judges.
With the exceptionof judge 2, all of the judges agreed with the majorityopinion in all but one or two of the 82 cases.
The agree-ment rate was 96.8%, averaged over all judges, or 99.1%,averaged over the four best judges.Table 4: Strong AgreementJudge n %1 82 100.0%2 72 87.8%3 81 98.7%4 82 100.0%5 80 97.6%Average 96.8%Average (without Judge 2) 99.1%As we had hoped, the experiment did, in fact, confirmthe one-sense-per-discourse hypothesis.
Of 54 pairs se-lected from the same article, the majority opinion foundthat 51 shared the same sense, and 3 did not.
~We conclude that with probability about 94% (51/54),two polysemous nouns drawn from the same article willhave the same sense.
In fact, the experiment tested aparticularly difficult case, since it did not include anyunambiguous words.
If we assume a mixture of 60% un-ambiguous words and 40% polysemous words, then theprobability moves from 94% to 100% x .60 + 94% x .4098%.
In other words, there is a very strong tendency(98%) for multiple uses of a word to share the same sensein well-written coherent discourse.One might ask if this result is specific to Grolier's orto good writing or some other factor.
The first authorlooked at the usage of these same nine words in theBrown Corpus, which is believed to be a more balancedsample of general language and which is also more widelyavailable than Grolier's and is therefore more amenableto replication.
The Brown Corpus consists of 500 dis-course fragments of 2000 words, each.
We were able tofind 259 concordance lines like the ones above, show-ing two instances of one of the nine test words selectedfrom the same discourse fragment.
However, four of thenine test words are not very interesting in the BrownCorpus antenna, drum, hull, and knife, since only onesense is observed.
There were 106 pairs for the remain-ing five words: campaign, deposit, interior, landscape,and marine.
The first author found that 102 of the 106pairs were used in the same sense.
Thus, it appears thatone-sense-per-discourse tendency is also fairly strong inthe Brown Corpus (102/106 ~ 96%), as well as in theGrolier's Encyclopedia.4.
IMPL ICAT IONSThere seem to be two applications for the one-sense-per-discourse observation: first it can be used as an ad-ditional source of constraint for improving the perfor-mance of the word-sense disambiguation algorithm, and3In contrast, of the 28 control pairs, the majority opinion foundthat only 1 share the same sense, and 27 did not.236secondly, it could be used to help evaluate disambigua-tion algorithms that did not make use of the discourseconstraint.
Thus far, we have been more interested in thesecond use: establishing a group of examples for whichwe had an approximate ground truth.
Rather than tag-ging each instance of a polysemous word one-by-one, wecan select discourses with large numbers of the polyse-mous word of interest and tag all of the instances in onefell swoop.
Admittedly, this procedure will introducea small error rate since the one-sense-per-discourse ten-dency is not quite perfect, but thus far, the error ratehas not been much of a problem.
This procedure hasenabled us to tag a much larger test set than we wouldhave been able to do otherwise.Having tagged as many words as we have (all instancesof 97 words in the Grolier's Encyclopedia), we are nowin a position to question some widely held assumptionsabout the distribution of polysemy.
In particular, it iscommonly believed that most words are highly polyse-mous, but in fact, most words (both by token and bytype) have only one sense, as indicated in Table 5 below.Even for those words that do have more than one possi-ble sense, it rarely takes anywhere near log2senses bitsto select the appropriate sense since the distribution ofsenses is generally quite skewed.
Perhaps the word-sensedisambiguation problem is not as difficult as we mighthave thought.Table 5: Distribution of Polysemysenses types tokens avg.
entropy1 67 7569 02 16 2552 0.583 7 1313 0.564 5 1252 1.25 1 1014 0.436 1 594 1.35.
CONCLUSIONIn conclusion, it appears that our hypothesis correct;well-written discourses tend to avoid multiple senses ofa polysemous word.
This result can be used in two basicways: (1) as an additional source of constraint for im-proving the performance ofa word-sense disambiguationalgorithm, and (2) as an aide in collecting annotated testmaterials for evaluating disamhiguation algorithms.6.
REFERENCES1.
Black, Ezra (1988), "An Experiment in Computa-tional Discrimination ofEnglish Word Senses," IBMJournal of Research and Development, v 32, pp 185-194.2.
Brown, Peter, Stephen Della Pietra, Vincent DellaPietra, and Robert Mercer (1991), "Word Sense Dis-ambiguation using Statistical Methods," Proceed-ings off the 29th Annual Meeting of the Associationfor Computational Linguistics, pp 264-270.3.
Chapman, Robert (1977).
Roger's InternationalThesaurus (Fourth Edition), Harper and Row, NewYork.4.
Dagan, Ido, Alon Itai, and Ulrike Schwall (1991),"Two Languages are more Informative than One,"Proceedings of the 29th Annual Meeting of the Asso-ciation for Computational Linguistics, pp 130-137.5.
Gale, Church, and Yarowsky, 1992, "DiscriminationDecisions for 100,000-Dimensional Spaces" AT&TStatistical Research Report No.
103.6.
Grolier's Inc. (1991) New Grolier's Electronic En-cyclopedia.7.
Hirst, G. (1987), Semantic Interpretation and theResolution of Ambiguity, Cambridge UniversityPress, Cambridge.8.
Kelly, Edward, and Phillip Stone (1975), Com-puter Recognition of English Word Senses, North-Holland, Amsterdam.9.
Mosteller, Fredrick, and David Wallace (1964) In-ference and Disputed Authorship: The Federalist,Addison-Wesley, Reading, Massachusetts.10.
Sutton, G. (1989) Automatic Text Processing,Addison-Wesley Publishing Co.11.
Yarowsky, David (1991) "Word-Sense Disamhigua-tion Using Statistical Models of Roget's CategoriesTrained on Large Corpora", submitted to COLING-92.237
