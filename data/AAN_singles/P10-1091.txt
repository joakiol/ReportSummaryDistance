Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 886?896,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsEstimating Strictly Piecewise DistributionsJeffrey HeinzUniversity of DelawareNewark, Delaware, USAheinz@udel.eduJames RogersEarlham CollegeRichmond, Indiana, USAjrogers@quark.cs.earlham.eduAbstractStrictly Piecewise (SP) languages are asubclass of regular languages which en-code certain kinds of long-distance de-pendencies that are found in natural lan-guages.
Like the classes in the Chom-sky and Subregular hierarchies, there aremany independently converging character-izations of the SP class (Rogers et al, toappear).
Here we define SP distributionsand show that they can be efficiently esti-mated from positive data.1 IntroductionLong-distance dependencies in natural languageare of considerable interest.
Although much at-tention has focused on long-distance dependencieswhich are beyond the expressive power of modelswith finitely many states (Chomsky, 1956; Joshi,1985; Shieber, 1985; Kobele, 2006), there aresome long-distance dependencies in natural lan-guage which permit finite-state characterizations.For example, although it is well-known that voweland consonantal harmony applies across any ar-bitrary number of intervening segments (Ringen,1988; Bakovic?, 2000; Hansson, 2001; Rose andWalker, 2004) and that phonological patterns areregular (Johnson, 1972; Kaplan and Kay, 1994),it is less well-known that harmony patterns arelargely characterizable by the Strictly Piecewiselanguages, a subregular class of languages withindependently-motivated, converging characteri-zations (see Heinz (2007, to appear) and especiallyRogers et al (2009)).As shown by Rogers et al (to appear), theStrictly Piecewise (SP) languages, which makedistinctions on the basis of (potentially) discon-tiguous subsequences, are precisely analogous tothe Strictly Local (SL) languages (McNaughtonand Papert, 1971; Rogers and Pullum, to appear),which make distinctions on the basis of contigu-ous subsequences.
The Strictly Local languagesare the formal-language theoretic foundation forn-gram models (Garcia et al, 1990), which arewidely used in natural language processing (NLP)in part because such distributions can be estimatedfrom positive data (i.e.
a corpus) (Jurafsky andMartin, 2008).
N -gram models describe prob-ability distributions over all strings on the basisof the Markov assumption (Markov, 1913): thatthe probability of the next symbol only dependson the previous contiguous sequence of lengthn ?
1.
From the perspective of formal languagetheory, these distributions are perhaps properlycalled Strictly k-Local distributions (SLk) wherek = n. It is well-known that one limitation of theMarkov assumption is its inability to express anykind of long-distance dependency.This paper defines Strictly k-Piecewise (SPk)distributions and shows how they too can be effi-ciently estimated from positive data.
In contrastwith the Markov assumption, our assumption isthat the probability of the next symbol is condi-tioned on the previous set of discontiguous subse-quences of length k ?
1 in the string.
While thissuggests the model has too many parameters (onefor each subset of all possible subsequences), infact the model has on the order of |?|k+1 parame-ters because of an independence assumption: thereis no interaction between different subsequences.As a result, SP distributions are efficiently com-putable even though they condition the probabil-ity of the next symbol on the occurrences of ear-lier (possibly very distant) discontiguous subse-quences.
Essentially, these SP distributions reflecta kind of long-term memory.On the other hand, SP models have no short-term memory and are unable to make distinctionson the basis of contiguous subsequences.
We donot intend SP models to replace n-gram models,but instead expect them to be used alongside of886them.
Exactly how this is to be done is beyond thescope of this paper and is left for future research.Since SP languages are the analogue of SL lan-guages, which are the formal-language theoreticalfoundation for n-gram models, which are widelyused in NLP, it is expected that SP distributionsand their estimation will also find wide applica-tion.
Apart from their interest to problems in the-oretical phonology such as phonotactic learning(Coleman and Pierrehumbert, 1997; Hayes andWilson, 2008; Heinz, to appear), it is expected thattheir use will have application, in conjunction withn-gram models, in areas that currently use them;e.g.
augmentative communication (Newell et al,1998), part of speech tagging (Brill, 1995), andspeech recognition (Jelenik, 1997).
?2 provides basic mathematical notation.
?3provides relevant background on the subregular hi-erarchy.
?4 describes automata-theoretic charac-terizations of SP languages.
?5 defines SP distri-butions.
?6 shows how these distributions can beefficiently estimated from positive data and pro-vides a demonstration.
?7 concludes the paper.2 PreliminariesWe start with some mostly standard notation.
?denotes a finite set of symbols and a string over?
is a finite sequence of symbols drawn fromthat set.
?k, ?
?k, ?
?k, and ??
denote allstrings over this alphabet of length k, of lengthless than or equal to k, of length greater thanor equal to k, and of any finite length, respec-tively.
?
denotes the empty string.
|w| denotesthe length of string w. The prefixes of a stringw are Pfx(w) = {v : ?u ?
??
such that vu = w}.When discussing partial functions, the notation ?and ?
indicates that the function is undefined, re-spectively is defined, for particular arguments.A language L is a subset of ??.
A stochasticlanguage D is a probability distribution over ?
?.The probability p of word w with respect to D iswritten PrD(w) = p. Recall that all distributionsD must satisfy ?w???
PrD(w) = 1.
If L is lan-guage then PrD(L) =?w?L PrD(w).A Deterministic Finite-state Automaton (DFA)is a tuple M = ?Q,?, q0, ?, F ?
where Q is thestate set, ?
is the alphabet, q0 is the start state,?
is a deterministic transition function with do-main Q ?
?
and codomain Q, F is the set ofaccepting states.
Let d?
: Q ?
??
?
Q bethe (partial) path function of M, i.e., d?
(q, w)is the (unique) state reachable from state qvia the sequence w, if any, or d?
(q, w)?
other-wise.
The language recognized by a DFA M isL(M) def= {w ?
??
| d?
(q0, w)?
?
F}.A state is useful iff for all q ?
Q, there existsw ?
??
such that ?
(q0, w) = q and there existsw ?
??
such that ?
(q, w) ?
F .
Useless statesare not useful.
DFAs without useless states aretrimmed.Two strings w and v over ?
are distinguishedby a DFA M iff d?
(q0, w) 6= d?
(q0, v).
They areNerode equivalent with respect to a language Lif and only if wu ?
L ??
vu ?
L forall u ?
??.
All DFAs which recognize L mustdistinguish strings which are inequivalent in thissense, but no DFA recognizing L necessarily dis-tinguishes any strings which are equivalent.
Hencethe number of equivalence classes of strings over?
modulo Nerode equivalence with respect to Lgives a (tight) lower bound on the number of statesrequired to recognize L.A DFA is minimal if the size of its state setis minimal among DFAs accepting the same lan-guage.
The product of n DFAs M1 .
.
.Mn isgiven by the standard construction over the statespace Q1 ?
.
.
.
?Qn (Hopcroft et al, 2001).A Probabilistic Deterministic Finite-state Automaton (PDFA) is a tupleM = ?Q,?, q0, ?, F, T ?
where Q is the stateset, ?
is the alphabet, q0 is the start state, ?
isa deterministic transition function, F and T arethe final-state and transition probabilities.
Inparticular, T : Q ?
?
?
R+ and F : Q ?
R+such thatfor all q ?
Q, F (q) +?a?
?T (q, a) = 1.
(1)Like DFAs, for all w ?
?
?, there is at most onestate reachable from q0.
PDFAs are typically rep-resented as labeled directed graphs as in Figure 1.A PDFA M generates a stochastic languageDM.
If it exists, the (unique) path for a word w =a0 .
.
.
ak belonging to ??
through a PDFA is asequence ?
(q0, a0), (q1, a1), .
.
.
, (qk, ak)?, whereqi+1 = ?
(qi, ai).
The probability a PDFA assignsto w is obtained by multiplying the transition prob-abilities with the final probability along w?s path if887A:2/10b:2/10c:3/10B:4/9a:3 /10a:2/9b:2/9c:1/9Figure 1: A picture of a PDFA with states labeledA and B.
The probabilities of T and F are locatedto the right of the colon.it exists, and zero otherwise.PrDM(w) =( k?i=1T (qi?1, ai?1))?F (qk+1) (2)if d?
(q0, w)?
and 0 otherwiseA probability distribution is regular deterministiciff there is a PDFA which generates it.The structural components of a PDFA M areits states Q, its alphabet ?, its transitions ?, andits initial state q0.
By structure of a PDFA, wemean its structural components.
Each PDFA Mdefines a family of distributions given by the pos-sible instantiations of T and F satisfying Equa-tion 1.
These distributions have |Q|?
(|?| + 1) in-dependent parameters (since for each state thereare |?| possible transitions plus the possibility offinality.
)We define the product of PDFA in terms of co-emission probabilities (Vidal et al, 2005a).Definition 1 Let A be a vector of PDFAs and let|A| = n. For each 1 ?
i ?
n let Mi =?Qi,?, q0i, ?i, Fi, Ti?
be the ith PDFA in A. Theprobability that ?
is co-emitted from q1, .
.
.
, qn inQ1, .
.
.
, Qn, respectively, isCT (?
?, q1 .
.
.
qn?)
=n?i=1Ti(qi, ?
).Similarly, the probability that a word simultane-ously ends at q1 ?
Q1 .
.
.
qn ?
Qn isCF (?q1 .
.
.
qn?)
=n?i=1Fi(qi).Then?A = ?Q,?, q0, ?, F, T ?
where1.
Q, q0, and ?
are defined as with DFA product.2.
For all ?q1 .
.
.
qn?
?
Q, letZ(?q1 .
.
.
qn?)
=CF (?q1 .
.
.
qn?)
+???
?CT (?
?, q1 .
.
.
qn?
)be the normalization term; and(a) let F (?q1 .
.
.
qn?)
= CF (?q1 ...
qn?
)Z(?q1 ...
qn?)
;and(b) for all ?
?
?, letT (?q1 .
.
.
qn?, ?)
= CT (?
?, q1 ...
qn?
)Z(?q1 ...
qn?
)In other words, the numerators of T and F are de-fined to be the co-emission probabilities (Vidal etal., 2005a), and division by Z ensures that M de-fines a well-formed probability distribution.
Sta-tistically speaking, the co-emission product makesan independence assumption: the probability of ?being co-emitted from q1, .
.
.
, qn is exactly whatone expects if there is no interaction between theindividual factors; that is, between the probabil-ities of ?
being emitted from any qi.
Also noteorder of product is irrelevant up to renaming ofthe states, and so therefore we also speak of tak-ing the product of a set of PDFAs (as opposed toan ordered vector).Estimating regular deterministic distributions iswell-studied problem (Vidal et al, 2005a; Vidal etal., 2005b; de la Higuera, in press).
We limit dis-cussion to cases when the structure of the PDFA isknown.
Let S be a finite sample of words drawnfrom a regular deterministic distribution D. Theproblem is to estimate parameters T and F of Mso that DM approaches D. We employ the widely-adopted maximum likelihood (ML) criterion forthis estimation.(T?
, F? )
= argmaxT,F(?w?SPrM(w))(3)It is well-known that if D is generated by somePDFA M?
with the same structural components asM, then optimizing the ML estimate guaranteesthat DM approaches D as the size of S goes toinfinity (Vidal et al, 2005a; Vidal et al, 2005b;de la Higuera, in press).The optimization problem (3) is simple for de-terministic automata with known structural com-ponents.
Informally, the corpus is passed throughthe PDFA, and the paths of each word through thecorpus are tracked to obtain counts, which are thennormalized by state.
Let M = ?Q,?, ?, q0, F, T ?be the PDFA whose parameters F and T are to beestimated.
For all states q ?
Q and symbols a ?
?, The ML estimation of the probability of T (q, a)is obtained by dividing the number of times thistransition is used in parsing the sample S by the888A:2b:2c:3B:4a :3a :2b:2c:1Figure 2: The automata shows the countsobtained by parsing M with sampleS = {ab, bba, ?, cab, acb, cc}.SL SPLT PTLTTSFFOReg MSOProp+1 <Figure 3: Parallel Sub-regular Hierarchies.number of times state q is encountered in the pars-ing of S. Similarly, the ML estimation of F (q) isobtained by calculating the relative frequency ofstate q being final with state q being encounteredin the parsing of S. For both cases, the division isnormalizing; i.e.
it guarantees that there is a well-formed probability distribution at each state.
Fig-ure 2 illustrates the counts obtained for a machineM with sample S = {ab, bba, ?, cab, acb, cc}.1Figure 1 shows the PDFA obtained after normaliz-ing these counts.3 Subregular HierarchiesWithin the class of regular languages there aredual hierarchies of language classes (Figure 3),one in which languages are defined in terms oftheir contiguous substrings (up to some length k,known as k-factors), starting with the languagesthat are Locally Testable in the Strict Sense (SL),and one in which languages are defined in termsof their not necessarily contiguous subsequences,starting with the languages that are Piecewise1Technically, this acceptor is neither a simple DFA orPDFA; rather, it has been called a Frequency DFA.
We donot formally define them here, see (de la Higuera, in press).Testable in the Strict Sense (SP).
Each languageclass in these hierarchies has independently mo-tivated, converging characterizations and each hasbeen claimed to correspond to specific, fundamen-tal cognitive capabilities (McNaughton and Pa-pert, 1971; Brzozowski and Simon, 1973; Simon,1975; Thomas, 1982; Perrin and Pin, 1986; Garc?
?aand Ruiz, 1990; Beauquier and Pin, 1991; Straub-ing, 1994; Garc?
?a and Ruiz, 1996; Rogers and Pul-lum, to appear; Kontorovich et al, 2008; Rogers etal., to appear).Languages in the weakest of these classes aredefined only in terms of the set of factors (SL)or subsequences (SP) which are licensed to oc-cur in the string (equivalently the complement ofthat set with respect to ?
?k, the forbidden fac-tors or forbidden subsequences).
For example, theset containing the forbidden 2-factors {ab, ba} de-fines a Strictly 2-Local language which includesall strings except those with contiguous substrings{ab, ba}.
Similarly since the parameters of n-gram models (Jurafsky and Martin, 2008) assignprobabilities to symbols given the preceding con-tiguous substrings up to length n?
1, we say theydescribe Strictly n-Local distributions.These hierarchies have a very attractive model-theoretic characterization.
The Locally Testable(LT) and Piecewise Testable languages are exactlythose that are definable by propositional formulaein which the atomic formulae are blocks of sym-bols interpreted factors (LT) or subsequences (PT)of the string.
The languages that are testable in thestrict sense (SL and SP) are exactly those that aredefinable by formulae of this sort restricted to con-junctions of negative literals.
Going the other way,the languages that are definable by First-Order for-mulae with adjacency (successor) but not prece-dence (less-than) are exactly the Locally Thresh-old Testable (LTT) languages.
The Star-Free lan-guages are those that are First-Order definablewith precedence alone (adjacency being FO defin-able from precedence).
Finally, by extending toMonadic Second-Order formulae (with either sig-nature, since they are MSO definable from eachother), one obtains the full class of Regular lan-guages (McNaughton and Papert, 1971; Thomas,1982; Rogers and Pullum, to appear; Rogers et al,to appear).The relation between strings which is funda-mental along the Piecewise branch is the subse-889quence relation, which is a partial order on ??
:w ?
v def??
w = ?
or w = ?1 ?
?
?
?n and(?w0, .
.
.
, wn ?
??
)[v = w0?1w1 ?
?
?
?nwn].in which case we say w is a subsequence of v.For w ?
?
?, letPk(w)def= {v ?
?k | v ?
w} andP?k(w)def= {v ?
?
?k | v ?
w},the set of subsequences of length k, respectivelylength no greater than k, of w. Let Pk(L) andP?k(L) be the natural extensions of these to setsof strings.
Note that P0(w) = {?
}, for all w ?
?
?,that P1(w) is the set of symbols occurring in w andthat P?k(L) is finite, for all L ?
?
?.Similar to the Strictly Local languages, StrictlyPiecewise languages are defined only in terms ofthe set of subsequences (up to some length k)which are licensed to occur in the string.Definition 2 (SPk Grammar, SP) A SPk gram-mar is a pair G = ?
?, G?
where G ?
?k.
Thelanguage licensed by a SPk grammar isL(G) def= {w ?
??
| P?k(w) ?
P?k(G)}.A language is SPk iff it is L(G) for some SPkgrammar G. It is SP iff it is SPk for some k.This paper is primarily concerned with estimat-ing Strictly Piecewise distributions, but first weexamine in greater detail properties of SP lan-guages, in particular DFA representations.4 DFA representations of SP LanguagesFollowing Sakarovitch and Simon (1983),Lothaire (1997) and Kontorovich, et al (2008),we call the set of strings that contain w as asubsequence the principal shuffle ideal2 of w:SI(w) = {v ?
??
| w ?
v}.The shuffle ideal of a set of strings is defined asSI(S) = ?w?SSI(w)Rogers et al (to appear) establish that the SP lan-guages have a variety of characteristic properties.Theorem 1 The following are equivalent:32Properly SI(w) is the principal ideal generated by {w}wrt the inverse of ?.3For a complete proof, see Rogers et al (to appear).
Weonly note that 5 implies 1 by DeMorgan?s theorem and thefact that every shuffle ideal is finitely generated (see alsoLothaire (1997)).1bc2abcFigure 4: The DFA representation of SI(aa).1.
L =?w?S [SI(w)], S finite,2.
L ?
SP3.
(?k)[P?k(w) ?
P?k(L) ?
w ?
L],4. w ?
L and v ?
w ?
v ?
L (L is subse-quence closed),5.
L = SI(X), X ?
??
(L is the complementof a shuffle ideal).The DFA representation of the complement of ashuffle ideal is especially important.Lemma 1 Let w ?
?k, w = ?1 ?
?
?
?k,and MSI(w) = ?Q,?, q0, ?, F ?, where Q ={i | 1 ?
i ?
k}, q0 = 1, F = Q and for allqi ?
Q,?
?
?:?
(qi, ?)
=??
?qi+1 if ?
= ?i and i < k,?
if ?
= ?i and i = k,qi otherwise.Then MSI(w) is a minimal, trimmed DFA that rec-ognizes the complement of SI(w), i.e., SI(w) =L(MSI(w)).Figure 4 illustrates the DFA representation ofthe complement of SI(aa) with ?
= {a, b, c}.
It iseasy to verify that the machine in Figure 4 acceptsall and only those words which do not contain anaa subsequence.For any SPk language L = L(?
?, G?)
6= ?
?,the first characterization (1) in Theorem 1 aboveyields a non-deterministic finite-state representa-tion of L, which is a set A of DFA representationsof complements of principal shuffle ideals of theelements of G. The trimmed automata product ofthis set yields a DFA, with the properties below(Rogers et al, to appear).Lemma 2 Let M be a trimmed DFA recognizinga SPk language constructed as described above.Then:1.
All states of M are accepting states: F = Q.890abcbcbacabbcbbab?
?,a?,b?,c?,a,b?,b,c?,a,c?,a,b,cFigure 5: The DFA representation of the of theSP language given by G = ?
{a, b, c}, {aa, bc}?.Names of the states reflect subsets of subse-quences up to length 1 of prefixes of the language.Note this DFA is trimmed, but not minimal.2.
For all q1, q2 ?
Q and ?
?
?, if d?
(q1, ?
)?and d?
(q1, w) = q2 for some w ?
??
thend?
(q2, ?)?.
(Missing edges propagate down.
)Figure 5 illustrates with the DFA representa-tion of the of the SP2 language given by G =?
{a, b, c}, {aa, bc}?.
It is straightforward to ver-ify that this DFA is identical (modulo relabeling ofstate names) to one obtained by the trimmed prod-uct of the DFA representations of the complementof the principal shuffle ideals of aa and bc, whichare the prohibited subsequences.States in the DFA in Figure 5 correspond to thesubsequences up to length 1 of the prefixes of thelanguage.
With this in mind, it follows that theDFA of ??
= L(?,?k) has states which corre-spond to the subsequences up to length k ?
1 ofthe prefixes of ??.
Figure 6 illustrates such a DFAwhen k = 2 and ?
= {a, b, c}.In fact, these DFAs reveal the differences be-tween SP languages and PT languages: they areexactly those expressed in Lemma 2.
Within thestate space defined by the subsequences up tolength k ?
1 of the prefixes of the language, if theconditions in Lemma 2 are violated, then the DFAsdescribe languages that are PT but not SP.
Pictori-ally, PT2 languages are obtained by arbitrarily re-moving arcs, states, and the finality of states fromthe DFA in Figure 6, and SP2 ones are obtained bynon-arbitrarily removing them in accordance withLemma 2.
The same applies straightforwardly forany k (see Definition 3 below).abca bcb accababcac bbcaabc?
?,a?,b?,c?,a,b?,b,c?,a,c?,a,b,cFigure 6: A DFA representation of the of the SP2language given by G = ?
{a, b, c},?2?.
Namesof the states reflect subsets of subsequences up tolength 1 of prefixes of the language.
Note thisDFA is trimmed, but not minimal.5 SP DistributionsIn the same way that SL distributions (n-grammodels) generalize SL languages, SP distributionsgeneralize SP languages.
Recall that SP languagesare characterizable by the intersection of the com-plements of principal shuffle ideals.
SP distribu-tions are similarly characterized.We begin with Piecewise-Testable distributions.Definition 3 A distribution D is k-PiecewiseTestable (written D ?
PTDk) def??
D can be de-scribed by a PDFA M = ?Q,?, q0, ?, F, T ?
with1.
Q = {P?k?1(w) : w ?
??}2.
q0 = P?k?1(?)3.
For all w ?
??
and all ?
?
?,?
(P?k?1(w), a) = P?k?1(wa)4.
F and T satisfy Equation 1.In other words, a distribution is k-PiecewiseTestable provided it can be represented by a PDFAwhose structural components are the same (mod-ulo renaming of states) as those of the DFA dis-cussed earlier where states corresponded to thesubsequences up to length k ?
1 of the prefixesof the language.
The DFA in Figure 6 shows the891structure of a PDFA which describes a PT2 distri-bution as long as the assigned probabilities satisfyEquation 1.The following lemma follows directly from thefinite-state representation of PTk distributions.Lemma 3 Let D belong to PTDk and let M =?Q,?, q0, ?, F, T ?
be a PDFA representing D de-fined according to Definition 3.PrD(?1 .
.
.
?n) = T (P?k?1(?
), ?1) ???
?2?i?nT (P?k?1(?1 .
.
.
?i?1), ?i)??
(4)?
F (P?k?1(w))PTk distributions have 2|?|k?1(|?|+1) parameters(since there are 2|?|k?1 states and |?|+1 possibleevents, i.e.
transitions and finality).Let Pr(?
| #) and Pr(# | P?k(w)) denotethe probability (according to some D ?
PTDk)that a word begins with ?
and ends after observ-ing P?k(w).
Then Equation 4 can be rewritten interms of conditional probability asPrD(?1 .
.
.
?n) = Pr(?1 | #) ???
?2?i?nPr(?i | P?k?1(?1 .
.
.
?i?1))??(5)?
Pr(# | P?k?1(w))Thus, the probability assigned to a word dependsnot on the observed contiguous sequences as in aMarkov model, but on observed subsequences.Like SP languages, SP distributions can be de-fined in terms of the product of machines very sim-ilar to the complement of principal shuffle ideals.Definition 4 Let w ?
?k?1 and w = ?1 ?
?
?
?k?1.Mw = ?Q,?, q0, ?, F, T ?
is a w-subsequence-distinguishing PDFA (w-SD-PDFA) iffQ = Pfx(w), q0 = ?, for all u ?
Pfx(w)and each ?
?
?,?
(u, ?)
= u?
iff u?
?
Pfx(w) andu otherwiseand F and T satisfy Equation 1.Figure 7 shows the structure of Ma which isalmost the same as the complement of the princi-pal shuffle ideal in Figure 4.
The only differenceis the additional self-loop labeled a on the right-most state labeled a. Ma defines a family of dis-tributions over ?
?, and its states distinguish thosebcaaabc?Figure 7: The structure of PDFA Ma.
It is thesame (modulo state names) as the DFA in Figure 4except for the self-loop labeled a on state a.strings which contain a (state a) from those thatdo not (state ?).
A set of PDFAs is a k-set of SD-PDFAs iff, for each w ?
?
?k?1, it contains ex-actly one w-SD-PDFA.In the same way that missing edges propagatedown in DFA representations of SP languages(Lemma 2), the final and transitional probabili-ties must propagate down in PDFA representa-tions of SPk distributions.
In other words, the fi-nal and transitional probabilities at states furtheralong paths beginning at the start state must be de-termined by final and transitional probabilities atearlier states non-increasingly.
This is captured bydefining SP distributions as a product of k-sets ofSD-PDFAs (see Definition 5 below).While the standard product based on co-emission probability could be used for this pur-pose, we adopt a modified version of it definedfor k-sets of SD-PDFAs: the positive co-emissionprobability.
The automata product based on thepositive co-emission probability not only ensuresthat the probabilities propagate as necessary, butalso that such probabilities are made on the ba-sis of observed subsequences, and not unobservedones.
This idea is familiar from n-gram models:the probability of ?n given the immediately pre-ceding sequence ?1 .
.
.
?n?1 does not depend onthe probability of ?n given the other (n?
1)-longsequences which do not immediately precede it,though this is a logical possibility.Let A be a k-set of SD-PDFAs.
For eachw ?
?
?k?1, let Mw = ?Qw,?, q0w, ?w, Fw, Tw?be the w-subsequence-distinguishing PDFA in A.The positive co-emission probability that ?
is si-multaneously emitted from states q?, .
.
.
, qu fromthe statesets Q?, .
.
.
Qu, respectively, of each SD-892PDFA in A isPCT (?
?, q?
.
.
.
qu?)
=?qw?
?q?...qu?qw=wTw(qw, ?)
(6)Similarly, the probability that a word simultane-ously ends at n states q?
?
Q?, .
.
.
, qu ?
Qu isPCF (?q?
.
.
.
qu?)
=?qw?
?q?...qu?qw=wFw(qw) (7)In other words, the positive co-emission proba-bility is the product of the probabilities restrictedto those assigned to the maximal states in eachMw.
For example, consider a 2-set of SD-PDFAs A with ?
= {a, b, c}.
A contains fourPDFAs M?,Ma,Mb,Mc.
Consider state q =?
?, ?, b, c?
?
?A (this is the state labeled ?, b, c inFigure 6).
ThenCT (a, q) = T?
(?, a)?
Ta(?, a)?
Tb(b, a)?
Tc(c, a)butPCT (a, q) = T?
(?, a)?
Tb(b, a)?
Tc(c, a)since in PDFA Ma, the state ?
is not the maximalstate.The positive co-emission product (?+) is de-fined just as with co-emission probabilities, sub-stituting PCT and PCF for CT and CF, respec-tively, in Definition 1.
The definition of ?+ en-sures that the probabilities propagate on the basisof observed subsequences, and not on the basis ofunobserved ones.Lemma 4 Let k ?
1 and let A be a k-set of SD-PDFAs.
Then ?+S defines a well-formed proba-bility distribution over ?
?.Proof Since M?
belongs to A, it is alwaysthe case that PCT and PCF are defined.
Well-formedness follows from the normalization termas in Definition 1.
?Definition 5 A distribution D is k-Strictly Piece-wise (written D ?
SPDk) def??
D can be describedby a PDFA which is the positive co-emissionproduct of a k-set of subsequence-distinguishingPDFAs.By Lemma 4, SP distributions are well-formed.Unlike PDFAs for PT distributions, which distin-guish 2|?|k?1 states, the number of states in a k-set of SD-PDFAs is?i<k(i + 1)|?|i, which is?(|?|k+1).
Furthermore, since each SD-PDFAonly has one state contributing |?|+1 probabilitiesto the product, and since there are |?
?k| = |?|k?1|?|?1many SD-PDFAs in a k-set, there are|?|k ?
1|?| ?
1 ?
(|?|+ 1) =|?|k+1 + |?|k ?
|?| ?
1|?| ?
1parameters, which is ?
(|?|k).Lemma 5 Let D ?
SPDk.
Then D ?
PTDk.Proof Since D ?
SPDk, there is a k-set ofsubsequence-distinguishing PDFAs.
The productof this set has the same structure as the PDFAgiven in Definition 3.
?Theorem 2 A distribution D ?
SPDk if D canbe described by a PDFA M = ?Q,?, q0, ?, F, T ?satisfying Definition 3 and the following.For all w ?
??
and all ?
?
?, letZ(w) =?s?P?k?1(w)F (P?k?1(s)) +???????
?s?P?k?1(w)T (P?k?1(s), ??)??
(8)(This is the normalization term.)
Then T must sat-isfy: T (P?k?1(w), ?)
=?s?P?k?1(w) T (P?k?1(s), ?
)Z(w) (9)and F must satisfy: F (P?k?1(w)) =?s?P?k?1(w) F (P?k?1(s))Z(w) (10)Proof That SPDk satisfies Definition 3 Followsdirectly from Lemma 5.
Equations 8-10 followfrom the definition of positive co-emission proba-bility.
?The way in which final and transitional proba-bilities propagate down in SP distributions is re-flected in the conditional probability as defined byEquations 9 and 10.
In terms of conditional prob-ability, Equations 9 and 10 mean that the prob-ability that ?i follows a sequence ?1 .
.
.
?i?1 isnot only a function of P?k?1(?1 .
.
.
?i?1) (Equa-tion 4) but further that it is a function of eachsubsequence in ?1 .
.
.
?i?1 up to length k ?
1.893In particular, Pr(?i | P?k?1(?1 .
.
.
?i?1)) is ob-tained by substituting Pr(?i | P?
k?1(s)) forT (P?
k?1(s), ?)
and Pr(# | P?
k?1(s)) forF (P?k?1(s)) in Equations 8, 9 and 10.
For ex-ample, for a SP2 distribution, the probability ofa given P?1(bc) (state ?, b, c in Figure 6) is thenormalized product of the probabilities of a givenP?1(?
), a given P?1(b), and a given P?1(c).To summarize, SP and PT distributions are reg-ular deterministic.
Unlike PT distributions, how-ever, SP distributions can be modeled with only?
(|?|k) parameters and ?
(|?|k+1) states.
Thisis true even though SP distributions distinguish2|?|k?1states!
Since SP distributions can be rep-resented by a single PDFA, computing Pr(w) oc-curs in only ?
(|w|) for such PDFA.
While suchPDFA might be too large to be practical, Pr(w)can also be computed from the k-set of SD-PDFAsin ?
(|w|k) (essentially building the path in theproduct machine on the fly using Equations 4, 8, 9and 10).6 Estimating SP DistributionsThe problem of ML estimation of SPk distribu-tions is reduced to estimating the parameters of theSD-PDFAs.
Training (counting and normaliza-tion) occurs over each of these machines (i.e.
eachmachine parses the entire corpus), which gives theML estimates of the parameters of the distribution.It trivially follows that this training successfullyestimates any D ?
SPDk.Theorem 3 For any D ?
SPDk, let D generatesample S. Let A be the k-set of SD-PDFAs whichdescribes exactly D. Then optimizing the MLE ofS with respect to each M ?
A guarantees that thedistribution described by the positive co-emissionproduct of ?+A approaches D as |S| increases.Proof The MLE estimate of S with respect toSPDk returns the parameter values that maximizethe likelihood of S. The parameters of D ?
SPDkare found on the maximal states of each M ?
A.By definition, each M ?
A describes a proba-bility distribution over ?
?, and similarly definesa family of distributions.
Therefore finding theMLE of S with respect to SPDk means finding theMLE estimate of S with respect to each of the fam-ily of distributions which each M ?
A defines,respectively.Optimizing the ML estimate of S for eachM ?
A means that as |S| increases, the estimatesT?M and F?M approach the true values TM andFM.
It follows that as |S| increases, T?N+ A andF?N+ A approach the true values of TN+ A andFN+ A and consequently DN+ A approaches D. ?We demonstrate learning long-distance depen-dencies by estimating SP2 distributions given acorpus from Samala (Chumash), a language withsibilant harmony.4 There are two classes of sibi-lants in Samala: [-anterior] sibilants like [s] and[>ts] and [+anterior] sibilants like [S] and [>tS].5Samala words are subject to a phonological pro-cess wherein the last sibilant requires earlier sibi-lants to have the same value for the feature [an-terior], no matter how many sounds intervene(Applegate, 1972).
As a consequence of thisrule, there are generally no words in Samalawhere [-anterior] sibilants follow [+anterior].
E.g.
[StojonowonowaS] ?it stood upright?
(Applegate1972:72) is licit but not *[Stojonowonowas].The results of estimating D ?
SPD2 withthe corpus is shown in Table 6.
The resultsclearly demonstrate the effectiveness of the model:the probability of a [?
anterior] sibilant givenP?1([-?
anterior]) sounds is orders of magnitudeless than given P?1(?
anterior]) sounds.xPr(x | P?1(y))s>ts S>tSs 0.0335 0.0051 0.0011 0.0002?ts 0.0218 0.0113 0.0009 0.y S 0.0009 0.
0.0671 0.0353>tS 0.0006 0.
0.0455 0.0313Table 1: Results of SP2 estimation on the Samalacorpus.
Only sibilants are shown.7 ConclusionSP distributions are the stochastic version of SPlanguages, which model long-distance dependen-cies.
Although SP distributions distinguish 2|?|k?1states, they do so with tractably many parametersand states because of an assumption that distinctsubsequences do not interact.
As shown, thesedistributions are efficiently estimable from posi-tive data.
As previously mentioned, we anticipatethese models to find wide application in NLP.4The corpus was kindly provided by Dr. Richard Apple-gate and drawn from his 2007 dictionary of Samala.5Samala actually contrasts glottalized, aspirated, andplain variants of these sounds (Applegate, 1972).
These la-ryngeal distinctions are collapsed here for easier exposition.894ReferencesR.B.
Applegate.
1972.
Inesen?o Chumash Grammar.Ph.D.
thesis, University of California, Berkeley.R.B.
Applegate.
2007.
Samala-English dictionary : aguide to the Samala language of the Inesen?o Chu-mash People.
Santa Ynez Band of Chumash Indi-ans.Eric Bakovic?.
2000.
Harmony, Dominance and Con-trol.
Ph.D. thesis, Rutgers University.D.
Beauquier and Jean-Eric Pin.
1991.
Languages andscanners.
Theoretical Computer Science, 84:3?21.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4):543?566.J.
A. Brzozowski and Imre Simon.
1973.
Character-izations of locally testable events.
Discrete Mathe-matics, 4:243?271.Noam Chomsky.
1956.
Three models for the descrip-tion of language.
IRE Transactions on InformationTheory.
IT-2.J.
S. Coleman and J. Pierrehumbert.
1997.
Stochasticphonological grammars and acceptability.
In Com-putational Phonology, pages 49?56.
Somerset, NJ:Association for Computational Linguistics.
ThirdMeeting of the ACL Special Interest Group in Com-putational Phonology.Colin de la Higuera.
in press.
Grammatical Infer-ence: Learning Automata and Grammars.
Cam-bridge University Press.Pedro Garc?
?a and Jose?
Ruiz.
1990.
Inference of k-testable languages in the strict sense and applica-tions to syntactic pattern recognition.
IEEE Trans-actions on Pattern Analysis and Machine Intelli-gence, 9:920?925.Pedro Garc?
?a and Jose?
Ruiz.
1996.
Learning k-piecewise testable languages from positive data.
InLaurent Miclet and Colin de la Higuera, editors,Grammatical Interference: Learning Syntax fromSentences, volume 1147 of Lecture Notes in Com-puter Science, pages 203?210.
Springer.Pedro Garcia, Enrique Vidal, and Jose?
Oncina.
1990.Learning locally testable languages in the strictsense.
In Proceedings of the Workshop on Algorith-mic Learning Theory, pages 325?338.Gunnar Hansson.
2001.
Theoretical and typologicalissues in consonant harmony.
Ph.D. thesis, Univer-sity of California, Berkeley.Bruce Hayes and Colin Wilson.
2008.
A maximum en-tropy model of phonotactics and phonotactic learn-ing.
Linguistic Inquiry, 39:379?440.Jeffrey Heinz.
2007.
The Inductive Learning ofPhonotactic Patterns.
Ph.D. thesis, University ofCalifornia, Los Angeles.Jeffrey Heinz.
to appear.
Learning long distancephonotactics.
Linguistic Inquiry.John Hopcroft, Rajeev Motwani, and Jeffrey Ullman.2001.
Introduction to Automata Theory, Languages,and Computation.
Addison-Wesley.Frederick Jelenik.
1997.
Statistical Methods forSpeech Recognition.
MIT Press.C.
Douglas Johnson.
1972.
Formal Aspects of Phono-logical Description.
The Hague: Mouton.A.
K. Joshi.
1985.
Tree-adjoining grammars: Howmuch context sensitivity is required to provide rea-sonable structural descriptions?
In D. Dowty,L.
Karttunen, and A. Zwicky, editors, Natural Lan-guage Parsing, pages 206?250.
Cambridge Univer-sity Press.Daniel Jurafsky and James Martin.
2008.
Speechand Language Processing: An Introduction to Nat-ural Language Processing, Speech Recognition, andComputational Linguistics.
Prentice-Hall, 2nd edi-tion.Ronald Kaplan and Martin Kay.
1994.
Regular modelsof phonological rule systems.
Computational Lin-guistics, 20(3):331?378.Gregory Kobele.
2006.
Generating Copies: An In-vestigation into Structural Identity in Language andGrammar.
Ph.D. thesis, University of California,Los Angeles.Leonid (Aryeh) Kontorovich, Corinna Cortes, andMehryar Mohri.
2008.
Kernel methods for learn-ing languages.
Theoretical Computer Science,405(3):223 ?
236.
Algorithmic Learning Theory.M.
Lothaire, editor.
1997.
Combinatorics on Words.Cambridge University Press, Cambridge, UK, NewYork.A.
A. Markov.
1913.
An example of statistical studyon the text of ?eugene onegin?
illustrating the linkingof events to a chain.Robert McNaughton and Simon Papert.
1971.Counter-Free Automata.
MIT Press.A.
Newell, S. Langer, and M. Hickey.
1998.
Thero?le of natural language processing in alternative andaugmentative communication.
Natural LanguageEngineering, 4(1):1?16.Dominique Perrin and Jean-Eric Pin.
1986.
First-Order logic and Star-Free sets.
Journal of Computerand System Sciences, 32:393?406.Catherine Ringen.
1988.
Vowel Harmony: TheoreticalImplications.
Garland Publishing, Inc.895James Rogers and Geoffrey Pullum.
to appear.
Auralpattern recognition experiments and the subregularhierarchy.
Journal of Logic, Language and Infor-mation.James Rogers, Jeffrey Heinz, Matt Edlefsen, DylanLeeman, Nathan Myers, Nathaniel Smith, MollyVisscher, and David Wellcome.
to appear.
On lan-guages piecewise testable in the strict sense.
In Pro-ceedings of the 11th Meeting of the Assocation forMathematics of Language.Sharon Rose and Rachel Walker.
2004.
A typology ofconsonant agreement as correspondence.
Language,80(3):475?531.Jacques Sakarovitch and Imre Simon.
1983.
Sub-words.
In M. Lothaire, editor, Combinatorics onWords, volume 17 of Encyclopedia of Mathemat-ics and Its Applications, chapter 6, pages 105?134.Addison-Wesley, Reading, Massachusetts.Stuart Shieber.
1985.
Evidence against the context-freeness of natural language.
Linguistics and Phi-losophy, 8:333?343.Imre Simon.
1975.
Piecewise testable events.
InAutomata Theory and Formal Languages: 2ndGrammatical Inference conference, pages 214?222,Berlin ; New York.
Springer-Verlag.Howard Straubing.
1994.
Finite Automata, FormalLogic and Circuit Complexity.
Birkha?user.Wolfgang Thomas.
1982.
Classifying regular events insymbolic logic.
Journal of Computer and SystemsSciences, 25:360?376.Enrique Vidal, Franck Thollard, Colin de la Higuera,Francisco Casacuberta, and Rafael C. Carrasco.2005a.
Probabilistic finite-state machines-part I.IEEE Transactions on Pattern Analysis and MachineIntelligence, 27(7):1013?1025.Enrique Vidal, Frank Thollard, Colin de la Higuera,Francisco Casacuberta, and Rafael C. Carrasco.2005b.
Probabilistic finite-state machines-part II.IEEE Transactions on Pattern Analysis and MachineIntelligence, 27(7):1026?1039.896
