Semantic Extraction with Wide-Coverage Lexical ResourcesBehrang MohitSchool of Information Management & SystemsUniversity of California, BerkeleyBerkeley, CA 94720, USAbehrangm@sims.berkeley.eduSrini NarayananInternational Computer Science InstituteBerkeley, CA 94704, USAsnarayan@icsi.berkeley.eduAbstractWe report on results of combining graphicalmodeling techniques with InformationExtraction resources (Pattern Dictionary andLexicon) for both frame and semantic roleassignment.
Our approach demonstrates theuse of two human built knowledge bases(WordNet and FrameNet) for the task ofsemantic extraction.1.
IntroductionPortability and domain independence are criticalchallenges for Natural Language Processing (NLP)systems.
The ongoing development of publicknowledge bases such as WordNet, FrameNet, CYC,etc.
has the potential to support domain independentsolutions to NLP.
The task of harnessing theappropriate information from these resources for anapplication remains significant.
This paper reports onthe use of semantic resources for a necessarycomponent of scalable NLP systems, SemanticExtraction (SE) .Semantic Extraction pertains to the assignmentof semantic bindings to short units of text (usuallysentences).
The SE problem is quite similar to theInformation Extraction (IE) task, in that in both caseswe are interested only in certain predicates and theirargument bindings and not in full understanding.However there are major differences as well.
IE is apre-specified and autonomous task with a narrowdomain of focus, where all the information of interestis represented in the extraction template.
SE involvesfinding predicate-argument structures in opendomains and is a crucial semantic parsing step in atext understanding task.In this paper we report results obtained fromcombining IE and graphical modeling techniques,with semantic resources (WordNet and FrameNet)for automatic Semantic Extraction.2.
BackgroundSemantic Extraction has become a strongresearch focus in the last few years.
A good exampleis the work of Gildea and Jurafsky (2002) (GJ).
GJpresent a comprehensive empirical approach to theproblem of semantic role assignment.
Their worklooked at the problem of assigning semantic roles totext based on a statistical model of the FrameNet1data.
In their work, GJ assume that the frame ofinterest is determined a-priori for every sentence.In the IE community, there has been an ongoingeffort to build systems that can automaticallygenerate required pattern sets as well as theextraction relevant lexicon.
Jones and Riloff (JR)(1999) describe a bootstrapping approach to theproblem of IE pattern extension.
They use a smallseed lexicon and pattern set, to iteratively generatenew patterns and expand their lexicon until theyachieve an optimized set of patterns and lexicon.In the area of lexicon acquisition, manyresearchers have employed public knowledge basessuch as WordNet in IE systems.
Bagga et.
al.
(1997)and later Harabagiu and Maiorano (HM) (2000)investigated the acquisition of the lexical conceptspace using WordNet and have applied their methodsto the Information Extraction task.In this paper, we describe work that blends thesemantic labeling approach exemplified by the GJeffort and the bootstrapping approach of JR and HM.Our work differs from the previous efforts in thefollowing respects.
1) We used FrameNet annotationsas seeds both for patterns and for the extractionlexicon.
We expand the seed lexicon using WordNet.2) We built a graphical model for the semanticextraction task, which allows us to integrateautomatic frame assignment as part of the extraction.3) We employed IE methods (including pattern setsand Named Entity Recognition) as initial extractionsteps.1 http://www.icsi.berkeley.edu/~framenet3.
FrameNetFrameNet (Baker et.
al.
1998) is building alexicon based on the theory of Frame Semantics.Frame Semantics suggests that the meanings oflexical items (lexical units (LU)) are best definedwith respect to larger conceptual chunks, calledFrames.
Individual lexical units evoke specific framesand establish a binding pattern to specific slots orroles (frame elements (FE)) within the frame.
TheBerkeley FrameNet project describes the underlyingframes for different lexical units, examines sentencesrelated to the frames using a very large corpus, andrecords (annotates) the ways in which informationfrom the associated frames are expressed in thesesentences.
The result is a database that contains a setof frames (related through hierarchy andcomposition), a set of frame elements for each frame,and a set of frame annotated sentences that covers thedifferent patterns of usage for lexical units in theframe.3.1 FrameNet data as seed patterns for IE:Using the FrameNet annotated dataset, wecompiled a set of IE patterns and also the lexicon foreach of the lexical units in FrameNet.We filtered out all of the non-relevant terms inall frame element lexicons.
We hypothesized thatusing a highly precise set of patterns along withprecise lexicon should enable a promising IEperformance.
For our Information Extractionexperiments, we used GATE (Cunningham et.
al.2002), an open source natural language engineeringsystem.
The component-based architecture of GATEenabled us to plug-in our FrameNet based lexiconand pattern set and run IE experiments on thissystem.3.2 Initial Experiment:As a preliminary test, we compiled a set of 100news stories from Yahoo News Service with topicsrelated to Criminal Investigation.
We also compiled aset of IE patterns and also the lexicon from the crimerelated frames (?Arrest?, ?Detain?, ?Arraign?
and?Verdict?.)
We ran the GATE system on this corpuswith our FrameNet data.
We evaluated the IEperformance by human judgment and hand countingthe semantic role assignments.
The systems achievedan average of 55% Recall while the precision was68.8%.
The fairly high precision (given just theFrameNet annotations) is the result of  a highlyprecise lexicon and pattern set, while we see the lowrecall as the result of the small coverage.
That is thereason that employed WordNet to enlarge ourlexicon.4.
Expanding the LexiconIn order to expand our lexicon for each of theframe elements, we used the human-built knowledgebase (WordNet (Fellbaum 1998)) and its richhierarchical structure.We built a graphical model of WordNet makingsome assumptions about the structure of the inducedWordNet graph.
For our initial experiments, we builta graph whose leaf was the enclosing category of theFrameNet annotated frame element.
We then lookedat an ancestor tree following the WordNet hypernymrelation.
This gave us a graphical model of the formshown in Figure 1 for the FrameNet frame elementSuspect and WordNet category Thief.Figure 1We then used the sum-product algorithm (Frey1998) for statistical inference on the frame elementinduced graph (such as in Figure 1).
We nowillustrate our use of the algorithm to expand theFrameNet derived lexicon.4.1 Statistical InferenceWe employed a statistical inference algorithm tofind the relevant nodes of WordNet.
For each of theframe elements, we took the terms in FrameNet FEannotations as ground truth which means that therelevance probability of the WordNet nodes for thoseterms is equal to 1.
The Sum Product algorithm helpsus find the relevance probability of higher levelnodes as a lexical category for the frame elementthrough a bottom up computation of the inter-nodemessages.
For example the message between nodes 1and 0 in the Figure 1 can be computed as:?
??
?1 0\)1(1101000,1 )()|()()(N Nkk NmNNPNPNmWe should note that based on the WordNet?shypernym relation, the conditional relevanceprobability of each parent node (given any childnode) is equal to 1.
Therefore the Sum Product inter-node messages are computed as:?
?
?=jN ijNkjkjjiji NmNPNm\)()()()(and the probability of each WordNet node can becomputed by a normalized interpolation of all of theincoming messages from the children nodes:|)(\)(|)()( )(\)(iparentiNNmNp iparentiNjijii?
?=4.2 Relevance of a WordNet NodesThroughout our experiments with the trainingdata, we discovered that some infrequent tail terms inthe frame element lexicon that might not be filteredout by the statistical inference algorithm but still arefrequently used in relevant text.Therefore, we defined the relevance metric forthe WordNet nodes to achieve a larger coverage.
Wecompiled a large corpus of text (News stories) andmade a second smaller corpus from the original onewhich contains only sentences which are relevant tothe IE task.
For each of the WordNet nodes wedefined the relevance of the node based on theproportion of the occurrence of the node in IE relatedText (Orel) to the occurrence of the node in thegeneral text (Ogen).genrelOONl =)(ReUsing this relevance metric, we evaluated all ofthe WordNet nodes for the training data (found in theprevious step) and re-ranked and picked the top ?m?relevant nodes (m=5 for our reported experiment) andadded them to the previous set of WordNet nodes.With a set of relevant WordNet nodes, weextended the lexicon for the IE system and re-ran ourIE task on the same 100 Yahoo news stories thatwere used in the initial experiments.
The averagerecall rose up to 76.4% this time with an averageprecision equal to 66%.5.
Frame AssignmentUsing FrameNet data with IE techniques showspromising results for semantic extraction.
Our currentefforts are geared toward extending the extractionprogram to include automatic frame assignment.
Forthis task, we assume that that the frame is a latentclass variable (whose domain is the set of lexicalunits) and the frame elements are variables whosedomain is the expanded lexicon (FrameNet +WordNet).
We assume that the frame elements areconditionally independent from each other, given theframe.
For our initial experiments, we assume thateach frame is an independent model and frameassignment is the task of selecting the Maximum APosteriori (MAP) frame given the input and the priorsof the frame.
Figure 2 shows the graphical modelexemplifying this assertion.
With this model, we areable to estimate the overall joint distribution for eachFrameNet frame, given the lexical items in thecandidate sentence from the corpus.
During trainingframe priors and model parameters )|( framefep areestimated from a large corpus using our SEmachinery outlined in sections 3 and 4.
While ourinitial results seem promising, the work is ongoingand we should have more results to report on thisaspect of the work by the time of the conference.Figure 26.
ReferencesBagga A., Chai J.Y.
& Biermann A.
1997.
The Roleof WordNet in The Creation of a TrainableMessage Understanding System.
In Proceedingsof the Sixth Message Understanding Conferenceon Artificial Intelligence (AAAI/IAAI-97)Baker C., Fillmore C. & Lowe J.
1998, The BerkeleyFrameNet project, In Proceedings of COLING/ACLpages 86?90, Montreal, Canada.Cunningham H., Maynard D., Bontcheva K., TablanV.
2002, GATE: A Framework and GraphicalDevelopment Environment for Robust NLP Toolsand Applications.
In Proceedings of the 40thAnniversary Meeting of the Association forComputational Linguistics (ACL'02).Fellbaum C., WordNet: an Electionic LexicalDatabase, Cambridge, MA, The MIT Press.Frey B.J.
1998, Graphical Models for MachineLearning and Digital Communication,Cambridge, MA, MIT PressGildea D., Jurafsky D. 2002, Automatic labeling ofsemantic roles, Computational Linguistics,28(3):245-288.Harabagiu S., Maiorano, S. 2000, Acquisition ofLinguistic Patterns for Knowledge-BasedInformation Extraction, in Proceedings of LREC-2000,Athens Greece.Riloff, E. and Jones, R. 1999, Learning Dictionariesfor Information Extraction by Multi-LevelBootstrapping, In Proceedings AAAI-99  pp.
474-479.
