Proceedings of the 43rd Annual Meeting of the ACL, pages 589?596,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsJoint Learning Improves Semantic Role LabelingKristina ToutanovaDept of Computer ScienceStanford UniversityStanford, CA, 94305kristina@cs.stanford.eduAria HaghighiDept of Computer ScienceStanford UniversityStanford, CA, 94305aria42@stanford.eduChristopher D. ManningDept of Computer ScienceStanford UniversityStanford, CA, 94305manning@cs.stanford.eduAbstractDespite much recent progress on accu-rate semantic role labeling, previous workhas largely used independent classifiers,possibly combined with separate label se-quence models via Viterbi decoding.
Thisstands in stark contrast to the linguisticobservation that a core argument frame isa joint structure, with strong dependen-cies between arguments.
We show how tobuild a joint model of argument frames,incorporating novel features that modelthese interactions into discriminative log-linear models.
This system achieves anerror reduction of 22% on all argumentsand 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank.1 IntroductionThe release of semantically annotated corpora suchas FrameNet (Baker et al, 1998) and PropBank(Palmer et al, 2003) has made it possible to develophigh-accuracy statistical models for automated se-mantic role labeling (Gildea and Jurafsky, 2002;Pradhan et al, 2004; Xue and Palmer, 2004).
Suchsystems have identified several linguistically mo-tivated features for discriminating arguments andtheir labels (see Table 1).
These features usuallycharacterize aspects of individual arguments and thepredicate.It is evident that the labels and the features of ar-guments are highly correlated.
For example, thereare hard constraints ?
that arguments cannot overlapwith each other or the predicate, and also soft con-straints ?
for example, is it unlikely that a predicatewill have two or more AGENT arguments, or that apredicate used in the active voice will have a THEMEargument prior to an AGENT argument.
Several sys-tems have incorporated such dependencies, for ex-ample, (Gildea and Jurafsky, 2002; Pradhan et al,2004; Thompson et al, 2003) and several systemssubmitted in the CoNLL-2004 shared task (Carrerasand Ma`rquez, 2004).
However, we show that thereare greater gains to be had by modeling joint infor-mation about a verb?s argument structure.We propose a discriminative log-linear jointmodel for semantic role labeling, which incorpo-rates more global features and achieves superiorperformance in comparison to state-of-the-art mod-els.
To deal with the computational complexity ofthe task, we employ dynamic programming and re-ranking approaches.
We present performance re-sults on the February 2004 version of PropBank ongold-standard parse trees as well as results on auto-matic parses generated by Charniak?s parser (Char-niak, 2000).2 Semantic Role Labeling: Task Definitionand ArchitecturesConsider the pair of sentences,?
[The GM-Jaguar pact]AGENT gives[the car market]RECIPIENT[a much-needed boost]THEME?
[A much-needed boost]THEME was given to[the car market]RECIPIENTby [the GM-Jaguar pact]AGENTDespite the different syntactic positions of the la-beled phrases, we recognize that each plays the same589role ?
indicated by the label ?
in the meaning ofthis sense of the verb give.
We call such phrasesfillers of semantic roles and our task is, given a sen-tence and a target verb, to return all such phrasesalong with their correct labels.
Therefore one sub-task is to group the words of a sentence into phrasesor constituents.
As in most previous work on se-mantic role labeling, we assume the existence of aseparate parsing model that can assign a parse tree tto each sentence, and the task then is to label eachnode in the parse tree with the semantic role of thephrase it dominates, or NONE, if the phrase does notfill any role.
We do stress however that the jointframework and features proposed here can also beused when only a shallow parse (chunked) represen-tation is available as in the CoNLL-2004 shared task(Carreras and Ma`rquez, 2004).In the February 2004 version of the PropBank cor-pus, annotations are done on top of the Penn Tree-Bank II parse trees (Marcus et al, 1993).
Possi-ble labels of arguments in this corpus are the coreargument labels ARG[0-5], and the modifier argu-ment labels.
The core arguments ARG[3-5] do nothave consistent global roles and tend to be verb spe-cific.
There are about 14 modifier labels such asARGM-LOC and ARGM-TMP, for location and tem-poral modifiers respectively.1 Figure 1 shows an ex-ample parse tree annotated with semantic roles.We distinguish between models that learn to la-bel nodes in the parse tree independently, called lo-cal models, and models that incorporate dependen-cies among the labels of multiple nodes, called jointmodels.
We build both local and joint models for se-mantic role labeling, and evaluate the gains achiev-able by incorporating joint information.
We startby introducing our local models, and later build onthem to define joint models.3 Local ClassifiersIn the context of role labeling, we call a classifierlocal if it assigns a probability (or score) to the labelof an individual parse tree node ni independently ofthe labels of other nodes.We use the standard separation of the task of se-mantic role labeling into identification and classifi-1For a full listing of PropBank argument labels see (Palmeret al, 2003)cation phases.
In identification, our task is to clas-sify nodes of t as either ARG, an argument (includ-ing modifiers), or NONE, a non-argument.
In clas-sification, we are given a set of arguments in t andmust label each one with its appropriate semanticrole.
Formally, let L denote a mapping of the nodesin t to a label set of semantic roles (including NONE)and let Id(L) be the mapping which collapses L?snon-NONE values into ARG.
Then we can decom-pose the probability of a labeling L into probabili-ties according to an identification model PID and aclassification model PCLS .PSRL(L|t, v) = PID(Id(L)|t, v) ?PCLS(L|t, v, Id(L)) (1)This decomposition does not encode any indepen-dence assumptions, but is a useful way of thinkingabout the problem.
Our local models for semanticrole labeling use this decomposition.
Previous workhas also made this distinction because, for example,different features have been found to be more effec-tive for the two tasks, and it has been a good wayto make training and search during testing more ef-ficient.Here we use the same features for local identifi-cation and classification models, but use the decom-position for efficiency of training.
The identificationmodels are trained to classify each node in a parsetree as ARG or NONE, and the classification modelsare trained to label each argument node in the train-ing set with its specific label.
In this way the train-ing set for the classification models is smaller.
Notethat we don?t do any hard pruning at the identifica-tion stage in testing and can find the exact labelingof the complete parse tree, which is the maximizerof Equation 1.
Thus we do not have accuracy lossas in the two-pass hard prune strategy described in(Pradhan et al, 2005).In previous work, various machine learning meth-ods have been used to learn local classifiers for rolelabeling.
Examples are linearly interpolated rela-tive frequency models (Gildea and Jurafsky, 2002),SVMs (Pradhan et al, 2004), decision trees (Sur-deanu et al, 2003), and log-linear models (Xue andPalmer, 2004).
In this work we use log-linear mod-els for multi-class classification.
One advantage oflog-linear models over SVMs for us is that they pro-duce probability distributions and thus identification590Standard Features (Gildea and Jurafsky, 2002)PHRASE TYPE: Syntactic Category of nodePREDICATE LEMMA: Stemmed VerbPATH: Path from node to predicatePOSITION: Before or after predicate?VOICE: Active or passive relative to predicateHEAD WORD OF PHRASESUB-CAT: CFG expansion of predicate?s parentAdditional Features (Pradhan et al, 2004)FIRST/LAST WORDLEFT/RIGHT SISTER PHRASE-TYPELEFT/RIGHT SISTER HEAD WORD/POSPARENT PHRASE-TYPEPARENT POS/HEAD-WORDORDINAL TREE DISTANCE: Phrase Type withappended length of PATH featureNODE-LCA PARTIAL PATH Path from constituentto Lowest Common Ancestor with predicate nodePP PARENT HEAD WORD If parent is a PPreturn parent?s head wordPP NP HEAD WORD/POS For a PP, retrievethe head Word / POS of its rightmost NPSelected Pairs (Xue and Palmer, 2004)PREDICATE LEMMA & PATHPREDICATE LEMMA & HEAD WORDPREDICATE LEMMA & PHRASE TYPEVOICE & POSITIONPREDICATE LEMMA & PP PARENT HEAD WORDTable 1: Baseline Featuresand classification models can be chained in a princi-pled way, as in Equation 1.The features we used for local identification andclassification models are outlined in Table 1.
Thesefeatures are a subset of features used in previouswork.
The standard features at the top of the tablewere defined by (Gildea and Jurafsky, 2002), andthe rest are other useful lexical and structural fea-tures identified in more recent work (Pradhan et al,2004; Surdeanu et al, 2003; Xue and Palmer, 2004).The most direct way to use trained local identifi-cation and classification models in testing is to se-lect a labeling L of the parse tree that maximizesthe product of the probabilities according to the twomodels as in Equation 1.
Since these models are lo-cal, this is equivalent to independently maximizingthe product of the probabilities of the two modelsfor the label li of each parse tree node ni as shownbelow in Equation 2.P `SRL(L|t, v) =?ni?tPID(Id(li)|t, v) (2)?
?ni?tPCLS(li|t, v, Id(li))A problem with this approach is that a maximizinglabeling of the nodes could possibly violate the con-straint that argument nodes should not overlap witheach other.
Therefore, to produce a consistent set ofarguments with local classifiers, we must have a wayof enforcing the non-overlapping constraint.3.1 Enforcing the Non-overlapping ConstraintHere we describe a fast exact dynamic programmingalgorithm to find the most likely non-overlapping(consistent) labeling of all nodes in the parse tree,according to a product of probabilities from localmodels, as in Equation 2.
For simplicity, we de-scribe the dynamic program for the case where onlytwo classes are possible ?
ARG and NONE.
The gen-eralization to more classes is straightforward.
In-tuitively, the algorithm is similar to the Viterbi al-gorithm for context-free grammars, because we candescribe the non-overlapping constraint by a ?gram-mar?
that disallows ARG nodes to have ARG descen-dants.Below we will talk about maximizing the sum ofthe logs of local probabilities rather than the prod-uct of local probabilities, which is equivalent.
Thedynamic program works from the leaves of the treeup and finds a best assignment for each tree, usingalready computed assignments for its children.
Sup-pose we want the most likely consistent assignmentfor subtree t with children trees t1, .
.
.
, tk each stor-ing the most likely consistent assignment of nodesit dominates as well as the log-probability of the as-signment of all nodes it dominates to NONE.
Themost likely assignment for t is the one that corre-sponds to the maximum of:?
The sum of the log-probabilities of the mostlikely assignments of the children subtreest1, .
.
.
, tk plus the log-probability for assigningthe node t to NONE?
The sum of the log-probabilities for assign-ing all of ti?s nodes to NONE plus the log-probability for assigning the node t to ARG.Propagating this procedure from the leaves to theroot of t, we have our most likely non-overlappingassignment.
By slightly modifying this procedure,we obtain the most likely assignment according to591a product of local identification and classificationmodels.
We use the local models in conjunction withthis search procedure to select a most likely labelingin testing.
Test set results for our local model P `SRLare given in Table 2.4 Joint ClassifiersAs discussed in previous work, there are strong de-pendencies among the labels of the semantic argu-ment nodes of a verb.
A drawback of local modelsis that, when they decide the label of a parse treenode, they cannot use information about the labelsand features of other nodes in the tree.Furthermore, these dependencies are highly non-local.
For instance, to avoid repeating argument la-bels in a frame, we need to add a dependency fromeach node label to the labels of all other nodes.A factorized sequence model that assumes a finiteMarkov horizon, such as a chain Conditional Ran-dom Field (Lafferty et al, 2001), would not be ableto encode such dependencies.The need for Re-rankingFor argument identification, the number of possi-ble assignments for a parse tree with n nodes is2n.
This number can run into the hundreds of bil-lions for a normal-sized tree.
For argument label-ing, the number of possible assignments is ?
20m,if m is the number of arguments of a verb (typi-cally between 2 and 5), and 20 is the approximatenumber of possible labels if considering both coreand modifying arguments.
Training a model whichhas such huge number of classes is infeasible if themodel does not factorize due to strong independenceassumptions.
Therefore, in order to be able to in-corporate long-range dependencies in our models,we chose to adopt a re-ranking approach (Collins,2000), which selects from likely assignments gener-ated by a model which makes stronger independenceassumptions.
We utilize the top N assignments ofour local semantic role labeling model P `SRL to gen-erate likely assignments.
As can be seen from Table3, for relatively small values of N , our re-rankingapproach does not present a serious bottleneck toperformance.
We used a value of N = 20 for train-ing.
In Table 3 we can see that if we could pick, us-ing an oracle, the best assignment out for the top 20assignments according to the local model, we wouldachieve an F-Measure of 98.8 on all arguments.
In-creasing the number of N to 30 results in a verysmall gain in the upper bound on performance anda large increase in memory requirements.
We there-fore selected N = 20 as a good compromise.Generation of top N most likely jointassignmentsWe generate the top N most likely non-overlapping joint assignments of labels to nodes ina parse tree according to a local model P `SRL, byan exact dynamic programming algorithm, whichis a generalization of the algorithm for finding thetop non-overlapping assignment described in section3.1.Parametric ModelsWe learn log-linear re-ranking models for joint se-mantic role labeling, which use feature maps from aparse tree and label sequence to a vector space.
Theform of the models is as follows.
Let ?
(t, v, L) ?Rs denote a feature map from a tree t, target verbv, and joint assignment L of the nodes of the tree,to the vector space Rs.
Let L1, L2, ?
?
?
, LN denotetop N possible joint assignments.
We learn a log-linear model with a parameter vector W , with oneweight for each of the s dimensions of the featurevector.
The probability (or score) of an assignmentL according to this re-ranking model is defined as:P rSRL(L|t, v) =e??
(t,v,L),W ?
?Nj=1 e??
(t,v,Lj ).W ?
(3)The score of an assignment L not in the top Nis zero.
We train the model to maximize the sumof log-likelihoods of the best assignments minus aquadratic regularization term.In this framework, we can define arbitrary fea-tures of labeled trees that capture general propertiesof predicate-argument structure.Joint Model FeaturesWe will introduce the features of the joint re-ranking model in the context of the example parsetree shown in Figure 1.
We model dependencies notonly between the label of a node and the labels of592S1NP1-ARG1Final-hour tradingVP1VBD1 PREDacceleratedPP1 ARG4TO1toNP2108.1 million sharesNP3 ARGM-TMPyesterdayFigure 1: An example tree from the PropBank with Semantic Role Annotations.other nodes, but also dependencies between the la-bel of a node and input features of other argumentnodes.
The features are specified by instantiation oftemplates and the value of a feature is the number oftimes a particular pattern occurs in the labeled tree.TemplatesFor a tree t, predicate v, and joint assignment Lof labels to the nodes of the tree, we define the can-didate argument sequence as the sequence of non-NONE labeled nodes [n1, l1, .
.
.
, vPRED, nm, lm] (liis the label of node ni).
A reasonable candidate ar-gument sequence usually contains very few of thenodes in the tree ?
about 2 to 7 nodes, as this is thetypical number of arguments for a verb.
To makeit more convenient to express our feature templates,we include the predicate node v in the sequence.This sequence of labeled nodes is defined with re-spect to the left-to-right order of constituents in theparse tree.
Since non-NONE labeled nodes do notoverlap, there is a strict left-to-right order amongthese nodes.
The candidate argument sequence thatcorresponds to the correct assignment in Figure 1will be:[NP1-ARG1,VBD1-PRED,PP1-ARG4,NP3-ARGM-TMP]Features from Local Models: All features includedin the local models are also included in our jointmodels.
In particular, each template for local fea-tures is included as a joint template that concatenatesthe local template and the node label.
For exam-ple, for the local feature PATH, we define a joint fea-ture template, that extracts PATH from every node inthe candidate argument sequence and concatenatesit with the label of the node.
Both a feature withthe specific argument label is created and a featurewith the generic back-off ARG label.
This is similarto adding features from identification and classifi-cation models.
In the case of the example candidateargument sequence above, for the node NP1 we havethe features:(NP?S?
)-ARG1, (NP?S?
)-ARGWhen comparing a local and a joint model, we usethe same set of local feature templates in the twomodels.Whole Label Sequence: As observed in previouswork (Gildea and Jurafsky, 2002; Pradhan et al,2004), including information about the set or se-quence of labels assigned to argument nodes shouldbe very helpful for disambiguation.
For example, in-cluding such information will make the model lesslikely to pick multiple fillers for the same role orto come up with a labeling that does not contain anobligatory argument.
We added a whole label se-quence feature template that extracts the labels ofall argument nodes, and preserves information aboutthe position of the predicate.
The template alsoincludes information about the voice of the predi-cate.
For example, this template will be instantiatedas follows for the example candidate argument se-quence:[ voice:active ARG1,PRED,ARG4,ARGM-TMP]We also add a variant of this feature which uses ageneric ARG label instead of specific labels.
Thisfeature template has the effect of counting the num-ber of arguments to the left and right of the predi-cate, which provides useful global information aboutargument structure.
As previously observed (Prad-han et al, 2004), including modifying arguments insequence features is not helpful.
This was confirmedin our experiments and we redefined the whole labelsequence features to exclude modifying arguments.One important variation of this feature uses theactual predicate lemma in addition to ?voice:active?.Additionally, we define variations of these featuretemplates that concatenate the label sequence withfeatures of individual nodes.
We experimented with593variations, and found that including the phrase typeand the head of a directly dominating PP ?
if oneexists ?
was most helpful.
We also add a feature thatdetects repetitions of the same label in a candidateargument sequence, together with the phrase typesof the nodes labeled with that label.
For example,(NP-ARG0,WHNP-ARG0) is a common pattern of thisform.Frame Features: Another very effective class of fea-tures we defined are features that look at the label ofa single argument node and internal features of otherargument nodes.
The idea of these features is to cap-ture knowledge about the label of a constituent giventhe syntactic realization of all arguments of the verb.This is helpful to capture syntactic alternations, suchas the dative alternation.
For example, considerthe sentence (i) ?
[Shaw Publishing]ARG0 offered [Mr.Smith]ARG2 [a reimbursement]ARG1 ?
and the alterna-tive realization (ii) ?
[Shaw Publishing]ARG0 offered[a reimbursement]ARG1 [to Mr. Smith]ARG2 ?.
Whenclassifying the NP in object position, it is useful toknow whether the following argument is a PP.
Ifyes, the NP will more likely be an ARG1, and if not,it will more likely be an ARG2.
A feature templatethat captures such information extracts, for each ar-gument node, its phrase type and label in the con-text of the phrase types for all other arguments.
Forexample, the instantiation of such a template for [areimbursement] in (ii) would be[ voice:active NP,PRED,NP-ARG1,PP]We also add a template that concatenates the identityof the predicate lemma itself.We should note that Xue and Palmer (2004) definea similar feature template, called syntactic frame,which often captures similar information.
The im-portant difference is that their template extracts con-textual information from noun phrases surroundingthe predicate, rather than from the sequence of ar-gument nodes.
Because our model is joint, we areable to use information about other argument nodeswhen labeling a node.Final PipelineHere we describe the application in testing of ajoint model for semantic role labeling, using a localmodel P `SRL, and a joint re-ranking model P rSRL.P `SRL is used to generate top N non-overlappingjoint assignments L1, .
.
.
, LN .One option is to select the best Li according toP rSRL, as in Equation 3, ignoring the score fromthe local model.
In our experiments, we noticed thatfor larger values of N , the performance of our re-ranking model P rSRL decreased.
This was probablydue to the fact that at test time the local classifierproduces very poor argument frames near the bot-tom of the top N for large N .
Since the re-rankingmodel is trained on relatively few good argumentframes, it cannot easily rule out very bad frames.
Itmakes sense then to incorporate the local model intoour final score.
Our final score is given by:PSRL(L|t, v) = (P `SRL(L|t, v))?
P rSRL(L|t, v)where ?
is a tunable parameter 2 for how much in-fluence the local score has in the final score.
Such in-terpolation with a score from a first-pass model wasalso used for parse re-ranking in (Collins, 2000).Given this score, at test time we choose among thetop N local assignments L1, .
.
.
, LN according to:arg maxL?{L1,...,LN}?
log P `SRL(L|t, v) + log P rSRL(L|t, v)5 Experiments and ResultsFor our experiments we used the February 2004 re-lease of PropBank.
3 As is standard, we used theannotations from sections 02?21 for training, 24 fordevelopment, and 23 for testing.
As is done insome previous work on semantic role labeling, wediscard the relatively infrequent discontinuous argu-ments from both the training and test sets.
In addi-tion to reporting the standard results on individualargument F-Measure, we also report Frame Accu-racy (Acc.
), the fraction of sentences for which wesuccessfully label all nodes.
There are reasons toprefer Frame Accuracy as a measure of performanceover individual-argument statistics.
Foremost, po-tential applications of role labeling may require cor-rect labeling of all (or at least the core) argumentsin a sentence in order to be effective, and partiallycorrect labelings may not be very useful.2We found ?
= 0.5 to work best3Although the first official release of PropBank was recentlyreleased, we have not had time to test on it.594Task CORE ARGMF1 Acc.
F1 Acc.Identification 95.1 84.0 95.2 80.5Classification 96.0 93.3 93.6 85.6Id+Classification 92.2 80.7 89.9 71.8Table 2: Performance of local classifiers on identification, classification, and identification+classification onsection 23, using gold-standard parse trees.N CORE ARGMF1 Acc.
F1 Acc.1 92.2 80.7 89.9 71.85 97.8 93.9 96.8 89.520 99.2 97.4 98.8 95.330 99.3 97.9 99.0 96.2Table 3: Oracle upper bounds for performance on the complete identification+classification task, usingvarying numbers of top N joint labelings according to local classifiers.Model CORE ARGMF1 Acc.
F1 Acc.Local 92.2 80.7 89.9 71.8Joint 94.7 88.2 92.1 79.4Table 4: Performance of local and joint models on identification+classification on section 23, using gold-standard parse trees.We report results for two variations of the seman-tic role labeling task.
For CORE, we identify andlabel only core arguments.
For ARGM, we identifyand label core as well as modifier arguments.
Wereport results for local and joint models on argu-ment identification, argument classification, and thecomplete identification and classification pipeline.Our local models use the features listed in Table 1and the technique for enforcing the non-overlappingconstraint discussed in Section 3.1.The labeling of the tree in Figure 1 is a specificexample of the kind of errors fixed by the joint mod-els.
The local classifier labeled the first argument inthe tree as ARG0 instead of ARG1, probably becausean ARG0 label is more likely for the subject position.All joint models for these experiments used thewhole sequence and frame features.
As can be seenfrom Table 4, our joint models achieve error reduc-tions of 32% and 22% over our local models in F-Measure on CORE and ARGM respectively.
With re-spect to the Frame Accuracy metric, the joint errorreduction is 38% and 26% for CORE and ARGM re-spectively.We also report results on automatic parses (seeTable 5).
We trained and tested on automatic parsetrees from Charniak?s parser (Charniak, 2000).
Forapproximately 5.6% of the argument constituentsin the test set, we could not find exact matches inthe automatic parses.
Instead of discarding thesearguments, we took the largest constituent in theautomatic parse having the same head-word as thegold-standard argument constituent.
Also, 19 of thepropositions in the test set were discarded becauseCharniak?s parser altered the tokenization of the in-put sentence and tokens could not be aligned.
As ourresults show, the error reduction of our joint modelwith respect to the local model is more modest in thissetting.
One reason for this is the lower upper bound,due largely to the the much poorer performance ofthe identification model on automatic parses.
ForARGM, the local identification model achieves 85.9F-Measure and 59.4 Frame Accuracy; the local clas-sification model achieves 92.3 F-Measure and 83.1Frame Accuracy.
It seems that the largest boostwould come from features that can identify argu-ments in the presence of parser errors, rather thanthe features of our joint model, which ensure globalcoherence of the argument frame.
We still achieve10.7% and 18.5% error reduction for CORE argu-ments in F-Measure and Frame Accuracy respec-tively.595Model CORE ARGMF1 Acc.
F1 Acc.Local 84.1 66.5 81.4 55.6Joint 85.8 72.7 82.9 60.8Table 5: Performance of local and joint models on identification+classification on section 23, using Charniakautomatically generated parse trees.6 Related WorkSeveral semantic role labeling systems have success-fully utilized joint information.
(Gildea and Juraf-sky, 2002) used the empirical probability of the setof proposed arguments as a prior distribution.
(Prad-han et al, 2004) train a language model over labelsequences.
(Punyakanok et al, 2004) use a linearprogramming framework to ensure that the only ar-gument frames which get probability mass are onesthat respect global constraints on argument labels.The key differences of our approach comparedto previous work are that our model has all of thefollowing properties: (i) we do not assume a finiteMarkov horizon for dependencies among node la-bels, (ii) we include features looking at the labelsof multiple argument nodes and internal features ofthese nodes, and (iii) we train a discriminative modelcapable of incorporating these long-distance depen-dencies.7 ConclusionsReflecting linguistic intuition and in line with cur-rent work, we have shown that there are substantialgains to be had by jointly modeling the argumentframes of verbs.
This is especially true when wemodel the dependencies with discriminative modelscapable of incorporating long-distance features.8 AcknowledgementsThe authors would like to thank the review-ers for their helpful comments and Dan Juraf-sky for his insightful suggestions and useful dis-cussions.
This work was supported in part bythe Advanced Research and Development Activity(ARDA)?s Advanced Question Answering for Intel-ligence (AQUAINT) Program.ReferencesCollin Baker, Charles Fillmore, and John Lowe.
1998.
TheBerkeley Framenet project.
In Proceedings of COLING-ACL-1998.Xavier Carreras and Lu?
?s M a`rquez.
2004.
Introduction to theCoNLL-2004 shared task: Semantic role labeling.
In Pro-ceedings of CoNLL-2004.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of NAACL, pages 132?139.Michael Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proceedings of ICML-2000.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3):245?288.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.Conditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedings ofICML-2001.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: The Penn Treebank.
Computational Linguistics,19(2):313?330.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2003.
Theproposition bank: An annotated corpus of semantic roles.Computational Linguistics.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin,and Dan Jurafsky.
2004.
Shallow semantic parsing usingsupport vector machines.
In Proceedings of HLT/NAACL-2004.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, WayneWard, James Martin, and Dan Jurafsky.
2005.
Support vec-tor learning for semantic argument classification.
MachineLearning Journal.Vasin Punyakanok, Dan Roth, Wen tau Yih, Dav Zimak, andYuancheng Tu.
2004.
Semantic role labeling via generalizedinference over classifiers.
In Proceedings of CoNLL-2004.Mihai Surdeanu, Sanda Harabagiu, John Williams, and PaulAarseth.
2003.
Using predicate-argument structures for in-formation extraction.
In Proceedings of ACL-2003.Cynthia A. Thompson, Roger Levy, and Christopher D. Man-ning.
2003.
A generative model for semantic role labeling.In Proceedings of ECML-2003.Nianwen Xue and Martha Palmer.
2004.
Calibrating featuresfor semantic role labeling.
In Proceedings of EMNLP-2004.596
