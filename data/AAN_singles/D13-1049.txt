Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 513?523,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsSource-Side Classifier Preordering for Machine TranslationUri LernerGoogle Inc.Mountain View, CA, USAuri@google.comSlav PetrovGoogle Inc.New York, NY, USAslav@google.comAbstractWe present a simple and novel classifier-basedpreordering approach.
Unlike existing pre-ordering models, we train feature-rich dis-criminative classifiers that directly predict thetarget-side word order.
Our approach com-bines the strengths of lexical reordering andsyntactic preordering models by performinglong-distance reorderings using the structureof the parse tree, while utilizing a discrimina-tive model with a rich set of features, includ-ing lexical features.
We present extensive ex-periments on 22 language pairs, including pre-ordering into English from 7 other languages.We obtain improvements of up to 1.4 BLEUon language pairs in the WMT 2010 sharedtask.
For languages from different families theimprovements often exceed 2 BLEU.
Many ofthese gains are also significant in human eval-uations.1 IntroductionGenerating the appropriate word order for the tar-get language has been one of the fundamental prob-lems in machine translation since the ground settingwork of Brown et al(1990).
Lexical reordering ap-proaches (Tillmann, 2004; Zens and Ney, 2006) adda reordering component to standard phrase-basedtranslation systems (Och and Ney, 2004).
Becausethe reordering model is trained discriminatively, itcan use a rich set of lexical features.
However,it only has access to the local context which oftentimes is insufficient to make the long-distance re-ordering decisions that are necessary for languagepairs with significantly different word order.Preordering (sometimes called pre-reordering orsimply reordering) approaches (Xia and McCord,2004; Collins et al 2005) preprocess the input insuch a way that the words on the source side appearcloser to their final positions on the target side.
Be-cause preordering is performed prior to word align-ment, it can improve the alignment process and canthen be combined with any subsequent translationmodel.
Most preordering models use a source-sidesyntactic parser and perform a series of tree trans-formations.
Approaches that do not use a parser ex-ist as well and typically induce a hierarchical rep-resentation that also allows them to perform long-distance changes (Tromble and Eisner, 2009; DeN-ero and Uszkoreit, 2011; Neubig et al 2012).Models that use a source-side parser differ on twomain dimensions: the way tree transformations areexpressed, and whether they are built manually orlearned from data.
One common type of tree trans-formation are rewrite rules.
These typically involvesome condition under which the transformation canbe applied (e.g., a noun and an adjective found inthe same clause) and the transformation itself (e.g.,move the adjective after the noun).
These rules canbe designed manually (Collins et al 2005; Wanget al 2007) or learned from data (Xia and McCord,2004; Habash, 2007; Genzel, 2010; Wu et al 2011).Another type of tree transformations uses rankingfunctions to implement precedence-based reorder-ing.
Here, a function assigns a numerical valueto every word in a clause, intended to express theprecedence of the word in the target language.
Thereordering operation is then to sort the words accord-ing to their assigned values.
The ranking function513can be designed manually (Xu et al 2009) or trainedfrom data (Yang et al 2012).
This approach isparticularly effective for Subject-Object-Verb (SOV)languages.In this work we present a simple classifier-basedpreordering model.
Our model operates over de-pendency parse trees and is therefore able to per-form long-distance reordering decisions, as is typ-ical for preordering models.
But instead of deter-ministic rules or ranking functions, we use discrim-inative classifiers to directly predict the final wordorder, using rich (bi-)lexical and syntactic features.We present two models.
The first model uses aclassifier to directly predict the permutation order inwhich a family of words (a head word and all itschildren) will appear on the target side.
This ap-proach is similar in spirit to the work of Li et al(2007), except that they use constituency parse treesand consider only nodes with 2 or 3 children.
Weinstead work with dependency trees and considermuch larger head-children sets.
Our second model isdesigned to decompose the exponential search spaceof all possible permutations.
The prediction task isbroken into two separate steps.
In the first step, foreach child word a binary classifier decides whetherit appears before or after its parent in the target lan-guage.
In the second step, we predict the best orderof the words on each side of the parent.
We showthat the second approach is never worse than the firstone and sometimes significantly better.We present experiments on 22 language pairsfrom different language families using our preorder-ing approach in a phrase-based system (Och andNey, 2004), as well as a forest-to-string system(Zhang et al 2011).
In a first set of experiments,we use the WMT 2010 shared task data (Callison-Burch et al 2010) and show significant improve-ments of up to 1.4 BLEU (Papineni et al 2002)on three out of eight language pairs.
In a secondset of experiments, we use automatically mined par-allel data from the web and build translation sys-tems for languages from various language families.We obtain especially big improvements in transla-tion quality (2-7 BLEU) when the language pairshave divergent word order (for example English toIndonesian, Japanese, Korean or Malay).
In our ex-periments on English to and from Hungarian, Dutch,and Portuguese translation, we find that we can ob-tain consistent improvements in both translation di-rections.
To additionally verify our improvementswe use human raters, who confirm the significanceof the BLEU score improvements.Finally, we compare training the preordering clas-sifiers on small amounts of manually aligned data totraining on large quantities of automatically aligneddata for English to Arabic, Hebrew, and Japanese.When evaluated on a pure reordering task, the mod-els trained on manually aligned data perform slightlybetter, but similar BLEU scores are obtained in bothscenarios on an end-to-end translation task.2 Classifier ReorderingOur goal is to learn a model that can transform theword order of an input sentence to an order that isnatural in the target language.
For example, whentranslating the English sentence:The black cat climbed to the tree top.to Spanish, we would like to reorder it as:The cat black climbed to the top tree.When translating to Japanese, we would like to get:The black cat the tree top to climbed.Such a model can then be used in combination withany translation model.In our approach we first part-of-speech (POS) tagand parse the input sentence, producing the POStags and head-modifier dependencies shown in Fig-ure 1.
Reordering is then done by traversing thedependency tree starting at the root.
For each headword we determine the order of the head and its chil-dren (independently of other decisions) and continuethe traversal recursively in that order.
In the exam-ple, we first need to decide on the order of the head?climbed?
and the children ?cat?, ?to?, and ?.
?.2.1 Classification Model & FeaturesThe reordering decisions are made by multi-classclassifiers where class labels correspond to permu-tation sequences.
We train a separate classifier foreach number of possible children.
Crucially, we donot learn explicit tree transformations rules, but letthe classifiers learn to trade off between a rich set ofoverlapping features.514Obviously, it is possible to use any classificationmodel and learning algorithm.
We use maximumentropy classifiers with l1/l?
regularization trainedwith the GradBoost algorithm (Duchi and Singer,2009).
We chose this setup since it naturally sup-ports multi-class prediction and can therefore beused to select one out of many possible permuta-tions.
Additionally, the learning algorithm producesa sparse set of features.
In our experiments the finalmodels have typically only a few 100K non-zero fea-ture weights per language pair.
Given this relativelysmall number of features, it is possible to manuallyinspect the feature weights and gain insights into thebehavior of the model.
We show an example analy-sis in Section 5.Our features encode information about the contextin which a word occurs in the sentence.
We modelcontext as ?informative?
words:?
The head itself.?
The children.
We indicate whether each child isbefore, immediately before, immediately after,or after the head.?
For every child, if there is a gap between it andthe head, then the first and last word of that gap.?
For every pair of consecutive children, if thereis a gap between them, then the first and lastword of that gap.?
The head?s immediate sibling to the left/right oran indication that none exists.When extracting the features, every word can be rep-resented by its word identity, its fine-grained POStag from the treebank, and a coarse-grained POS cat-egory, similar to the universal categories describedin Petrov et al(2012).
We also include pairs of thesefeatures, resulting in potentially bilexical features.2.2 Training DataThe training data for the classifiers is generated fromthe word aligned parallel text.
Since parallel datais plentiful, we can afford to be selective.
We firstconstruct the intersection of high-confidence source-to-target and target-to-source alignments.
For everyfamily in the source dependency tree we generate atraining instance if and only if the intersection de-fines a full order on the source words:?
Every source word must be aligned to at leastone target word.The black cat climbed to the tree top .DT JJ NN VBD IN DT NN NN .DET ADJ NOUN VERB ADP DET NOUN NOUN .detamod nsubj prepdetncpobjpROOTFigure 1: A sentence, its dependency parse and its fine-grained and coarse-grained POS tags.?
No two source words can be aligned to thesame target word.?
If a source word is aligned to multiple targetwords, then no target word in this range can bealigned to a different source word.While this might sound restrictive, we can usuallygenerate at least some training instances from everysentence and discard the remaining families in thetree.
In particular, we do not need to extract train-ing instances for all words in a given sentence sincethe reordering decisions are made independently forevery head word.A potential concern might be that our method forselecting training data can exclude all instances ofcertain words.
Consider the English phrase ?theboy?.
For languages without articles (e.g.
Russianor Japanese) the determiner ?the?
may either not bealigned to any word or get alned to the foreignword for ?boy?.
In both cases the family will bediscarded according to either the first or the secondcondition above.
The concern is therefore that wewould have no training data with the English word?the?.
In practice, however, this does not seem to bea problem.
First, there are instances where the En-glish word ?the?
gets aligned to something (perhapsa preposition), and second, since the word ?the?
isomitted in the target language its location in the re-ordered sentence is not very important.Naturally we learn better classifier models frombetter alignments.
The other direction is also true?
if we run preordering on the source side then thealignment task becomes easier and tends to producebetter results.
Therefore it can be useful to iter-ate between generating the alignment and learninga preordering model.
Empirically, the gains fromthis bootstrapping approach are not dramatic and arerealized after just one iteration, i.e., create the align-515ment, train a preordering model, use the preorderingmodel to learn a new alignment, and then train thefinal preordering model.2.3 1-Step ClassifierAs a first approach we use a single classifier to di-rectly predict the correct permutation of a given fam-ily.
Consider the family headed by ?climbed?
inFigure 1.
There are three children and the originalorder of the words is ?cat?, ?climbed?, ?to?, and?.?.
A possible outcome of the classifier can be thepermutation 0-2-1-3, representing the order ?cat?,?to?, ?climbed?, and ?.
?.The number of permutations for the head and nchildren is of course (n + 1)!, which becomes largevery quickly and causes some problems.
In practicewe therefore limit ourselves to the K most commonpermutations.
Unfortunately, this means that when-ever there are many children, the correct permuta-tion order might not be available as an option.
Evenwhen the correct permutation is available, classifica-tion accuracy typically deteriorates as the number ofpossible classes increases.An additional subtle issue is that the 1-step classi-fier cannot share useful information across differentnumbers of children.
For example, in Spanish adjec-tives usually appear after the noun, but sometimesthey appear before the noun.
The decision dependson the adjective itself and sometimes the head noun,but does not depend on other children.
Ideally, if forsome adjective we have enough examples with 1 or2 children we would like to make the same decisionfor a larger number of children, but these classifiersmay not have enough relevant examples.2.4 2-Step ClassifierOur 2-step approach addresses the exponentialblowup of the number of children by decomposingthe prediction into two steps:1.
For every child, decide whether it should ap-pear before or after the head.2.
Determine the order of the children that appearbefore the head and the order of the childrenafter the head.The two steps make the reordering of the modifiersbefore and after the head independent of each other,which is reminiscent of the lexicalized parse treegeneration approach of Collins (1997).
In the run-ning example, for the head ?climbed?
we might firstmake the following three binary decisions: the word?cat?
should appear before the head and the words?to?
and ?.?
should appear after the head.
In thesecond step there is only one word before the headso there is nothing to do.
There are two words afterthe head, so we use another classifier to determinetheir order.
The first step is implemented using a bi-nary classifier, called the pivot classifier (since thehead functions like the pivot in quicksort).
The sec-ond step classifiers directly predict the correct per-mutation of the children before / after the head.To illustrate the effectiveness of the 2-step ap-proach, consider a head word with 4 children.
The 1-step approach must predict 1 of 5!
= 120 outcomes.In the 2-step approach, in the worst case the secondstep must predict 1 of 4!
= 24 outcomes (if all thechildren are on one side of the head); if we are luckyand the children split evenly, then we only need twobinary decisions in the second step (for the two pairsbefore and after the head).
If we define hard cases ascases involving 5 or more words, 5.54% of the non-leaves are hard cases with the 1-step approach, butonly 1.07% are hard cases with the 2-step approach.3 Experimental SetupTo provide a through evaluation of our approach, weconduct experiments on two sets of data and withtwo translation systems.
The first translation systemis a phrase-based system (Och and Ney, 2004).
Inaddition to the regular distance distortion model, weincorporate a maximum entropy based lexicalizedphrase reordering model (Zens and Ney, 2006).
Oursecond system is a forest-to-string system (Zhang etal., 2011).
The forest-to-string system uses a one-best parse tree but factorizes it into a packed forestof binary elementary trees ?
hence the name forest-to-string rather than tree-to-string.The systems are configured and tuned for eachlanguage pair to produce the best results.
We thenadd our 1-step and 2-step preordering classifiers aspreprocessing steps at training and test time.
Wetrain the reordering classifiers on up to 15M train-ing instances.
We train separate classifiers for everynumber of involved words, and restrict each one tothe K = 20 most frequent outcomes.516In our implementation, in the 1-step approach wedid not do any reordering for nodes with 7 or morechildren.
In the 2-step approach we did not reorderthe children on either side of the head if there were 7or more of them.
Even though there was no techni-cal reason that prevented us from raising the thresh-olds, there was no good reason to do so.
There werevery few cases where children were not reorderedbecause of these thresholds, many of them corre-sponded to bad parses, and they had very little im-pact on the final scores.
Thus, for the 1-step ap-proach we had 6 classifiers: 1 binary classifier for ahead and a single child and 5 multi-class classifiersfor 3?7 words.
For the 2-step approach we had 11classifiers: 1 pivot classifier, 5 classifiers for wordsbefore the head, and 5 for words after the head.For a direct comparison to a strong preorderingsystem, we compare to the system of Genzel (2010),which learns a set of unlexicalized reordering rulesfrom automatically aligned data by minimizing thenumber of crossing alignments.
We used a slidingwindow of size 3 and tried all three of their vari-ants.
There were about 40-50 rules per languagepair.
While conceptually possible, it is not practi-cal to learn more rules (including lexicalized rules)with this system, because of the computational com-plexity of the learning algorithm and the incrementalnature in which the rules are learned and applied.3.1 WMT SetupIn our first set of experiments, we use the data pro-vided for the WMT 2010 shared task (Callison-Burch et al 2010).
We build systems for all lan-guage pairs: English to and from Czech, French,German, and Spanish.
Since this is a publicly avail-able dataset, it is easy to compare our results to othersubmissions to the shared task.During word alignment, we filter out sentencesexceeding 60 words in the parallel texts and per-form 6 iterations of IBM Model-1 training (Brownet al 1993), followed by 6 iterations of HMM train-ing (Vogel et al 1996).
We do not use Model-4because it is slow and did not add much value to oursystems in a pilot study.
Standard phrase extractionheuristics (Koehn et al 2003) are applied to extractphrase pairs with a length limit of 6 from alignmentssymmetrized with the ?union?
heuristic.
Maximumjump width is set to 8.
Rule extraction for the forest-to-string system is limited to 16 rules per tree node.There are no length-based reordering constraints inthe forest-to-string system.
We train two 5-gram lan-guage models with Kneser-Ney smoothing for eachof the target languages.
One is trained on the tar-get side of the parallel text, the other on a news cor-pus provided by the shared task.
We tune the fea-ture weights for every configuration with 10 roundsof hypergraph-based Minimum Error Rate Training(MERT) (Kumar et al 2009).3.2 Additional LanguagesIn our second set of experiments, we explore the im-pact of classifier preordering for a number of lan-guages with different word orders.
Some of the lan-guages included in our study are verb-subject-object(VSO) languages (Arabic, Irish, Welsh), subject-object-verb (SOV) languages (Japanese, Korean),and fairly free word order languages (Dutch, Hun-garian).
Where a parser is available, we also conductexperiments on translating into English.Since there are no standard training sets for manyof these language pairs, we use parallel data auto-matically mined from the web.
The amount of par-allel text for each language pair is between 120Mand 160M words.
For evaluation, we use a set of 9KEnglish sentences collected from the web and trans-lated by humans into each of the target languages.Each sentence has one reference translation.
We use5K sentences for evaluation and the rest for tuning.The systems and training configurations are sim-ilar to the WMT setup.
The word alignment stepincludes 3 iterations of IBM Model-1 training and2 iterations of HMM training.
Lexical reordering isincluded where it helps, but typically makes only asmall difference.
We again use a 5-gram languagemodel trained on a large amount of monolingualtext.
Overall, we use between 20 and 30 features,whose weights are optimized using hypergraph-based MERT.
All experiments for a given languagepair use the same set of MERT weights.
This po-tentially underestimates the improvements that canbe obtained, but also eliminates MERT as a pos-sible source of improvement, allowing us to traceback improvements in translation quality directly tochanges in preordering of the input data.5173.3 EvaluationWe use case-sensitive BLEU (Papineni et al 2002)to assess translation quality.
For Japanese and Ko-rean we use character-level BLEU.
We use bootstrapresampling to compute confidence intervals.Additionally, we also conduct a side-by-side hu-man evaluation on 750 sentences for each languagepair (sampled from the same sentences used forcomputing BLEU).
For each sentence, we ask bilin-gual annotators to compare the translations from twodifferent systems and say whether one is better, lead-ing to three possible scores of -1, 0, and +1.
We fo-cus on this relative comparison since absolute scoresare difficult to calibrate across languages and raters.3.4 Syntactic ParsersTable 1 shows our treebank sources and parsing ac-curacies.
For English, we use the updated WSJ withOntoNotes-style annotations converted to Stanforddependencies (de Marneffe et al 2006).
The re-maining treebanks are all available in dependencyformat.
In all cases, we apply a set of heuristics tothe treebank data to make the tokenization as simi-lar as possible to the one of the bitext.
Our heuristicscan split treebank tokens but do not merge treebanktokens.
We found that adjusting the treebank tok-enization is crucial for obtaining good results.
How-ever, this makes the reported parsing accuracies notcomparable to other numbers in the literature.
Whennecessary, we projectivize the treebanks by raisingarcs until the tree becomes projective, as describedin Nivre and Nilsson (2005); we do not reconstructnon-projective arcs at parsing time, since our subse-quent systems expect projective trees.Our part-of-speech tagger is a conditional randomfield model (Lafferty et al 2001) with simple word-identity and affix features.
The parsing model isa shift-reduce dependency parser, using the higher-order features from Zhang and Nivre (2011).
Addi-tionally, we include 256 word-cluster features (Kooet al 2008) trained on a large amount of unlabeledmonolingual text (Uszkoreit and Brants, 2008).4 ExperimentsDue to the large number of experiments and lan-guage pairs we divide the experiments into groupsand discuss each in turn.
We only include the resultsUAS LAS POSen: English1 92.28 90.28 97.05cs: Czech2 84.66 72.01 98.97de: German3 89.30 86.98 97.69es: Spanish4 86.24 82.32 96.62fr: French5 88.57 86.40 97.48hu: Hungarian2 87.66 82.51 94.47nl: Dutch3 86.09 82.31 97.38pt: Portuguese4 90.22 87.26 98.10Table 1: Parsing accuracies on the retokenized treebanks.UAS is unlabeled attachment score, LAS is labeled at-tachment score, and POS is part-of-speech tagging accu-racy.
The treebank sources are (1): Marcus et al(1993)+ Judge et al(2006) + Petrov and McDonald (2012), (2):Nivre et al(2007), (3): Buchholz and Marsi (2006), (4):McDonald et al(2013), (5): Abeille?
et al(2003).from the forest-to-string system when they are bet-ter than the phrase-based results.
We use * to denoteresults from the forest-to-string system.4.1 WMT ExperimentsTable 2 presents detailed results on the WMT setup.Lexical reordering (Zens and Ney, 2006) never hurtsand is thus included in all systems.
Overall, our re-sults are a little better than the best results of theWMT 2010 shared task for two language pairs andwithin reach of the best results in most other cases.The 2-step classifier preordering approach pro-vides statistically significant improvements over thelexical reordering baseline on three out of the eightlanguage pairs: English-Spanish (en-es: 1.4 BLEU),German-English (de-en: 1.2 BLEU), and English-French (en-fr: 1.0 BLEU).
These improvements aresignificant in our human side-by-side evaluation.We also observe gains when combining our pre-ordering approach with the forest-to-string systemfor English-Spanish and German-English.
While theforest-to-string system is capable of performing longdistance reordering in the decoder, it appears thatan explicitly trained lexicalized preordering modelcan provide complementary benefits.
These bene-fits are especially pronounced for German-Englishwhere long distance verb movement is essential.
Forthe romance languages (Spanish and French), wordordering depends highly on lexical choice which iscaptured by the lexical features in our classifiers.518base lexical rule 1-step 2-step wmt besten-cs 14.9 15.1 15.2 15.2 15.2 15.4en-de 15.3 15.6 15.9 15.9 15.7 16.3en-es 27.4 27.8s 28.4p 29.0 28.8??
28.6en-es* 28.9 - 28.7 29.0 29.2 28.6en-fr 26.3 26.5s 26.8p 27.2 27.3?p 27.6cs-en 21.6 21.6 21.5 21.6 21.7 21.9de-en 20.6 21.1s 21.9 21.9 21.8?
22.8de-en* 22.1 - 22.5 22.5 22.7 22.8es-en 28.3 28.7 28.7 28.8 28.9 28.8fr-en 26.8 27.0 26.9 26.9 27.0 28.3Table 2: BLEU scores on the WMT 2010 setup.
Results from the forest-to-string system are marked with * and areonly included when better than the phrase-based results.
The base system includes a distance distortion model; thelexical system adds lexical reordering; rule is the rule preordering system of Genzel (2010) plus lexical reordering;1-step and 2-step are our classifier-based systems plus lexical reordering.
Bolded results are statistically significantlybetter than non-bolded results as measured by a bootstrap sample test with a 99% confidence interval.
Human evalsare conducted only where indicated; we use ?
and ?
to indicate a significantly better result than s and p in the humaneval at 95%.
Also included are the best results from the WMT 2010 task.Compared to a state-of-the-art preordering sys-tem, the automatic rule extraction system of Gen-zel (2010), we observe significant gains in severalcases and no losses at all.
The improvements onEnglish-Spanish are significant also in the humanevaluation, while the English-French improvementsare positive, but not statistically significant.Comparing the different languages, Czech (cs) ap-pears the most immune to improvements from pre-ordering (and lexical reordering).
One possible ex-planation is that Czech has a relatively free wordorder with a default SVO structure.
It is thereforedifficult to learn reordering changes from Englishto Czech.
Additionally, the accuracy (LAS) of ourCzech parser is by far the lowest of all parsers thatwe used, potentially limiting the benefits that can beobtained when translating from Czech into English.On this setup there is fairly little difference in per-formance between the 1-step and 2-step approaches.The main benefit of the 2-step approach is compact-ness: the set of 2-step classifiers has about half thenumber of non-zero features as the 1-step classifiers.4.2 Additional Languages ExperimentsTable 3 shows our first set of results on the additionallanguages, including some languages with a widedisparity in word order relative to English.
The SOVlanguages Korean (ko) and Japanese (ja) benefit themost from preordering and gain more than 7 BLEUrelative to the phrase-based baseline and still morethan 3 BLEU for the forest-to-string system.
Simi-lar improvements were reported by Xu et al(2009)with manual reordering rules.
Indonesian (id) andMalay (ms) are next with gains of 2.5 BLEU.
Malaydoes not have a grammatical subject in the sensethat English does, but instead uses a concept of anagent and an object, whose order is determined bythe voice of the verb.
It appears that our classifiershave learned to model some of these highly lexical,but systematic ordering preferences.
Welsh (cy) andIrish (ga) as VSO languages also exhibit large gainsof 2.1 BLEU.
For Arabic (ar) and Hebrew (iw), thegains are smaller, but still significant and exceed 1BLEU relative to the baseline.The benefits of our 2-step approach over the 1-step approach become apparent on this set of lan-guages where reordering is most important.
By pre-dicting the target word order in two steps, we reducesparsity and make two easier decisions in place ofa single difficult high entropy decision.
Indeed, the2-step approach produces improvements over the 1-step approach on five out of nine language pairs.
Theimprovements are as large as 0.9 BLEU for Koreanand 0.5 BLEU for Japanese and Welsh.
We per-formed human evaluation for all language pairs witha noticeable BLEU gain for the 2-step system over519base rule 1-step 2-stepen-ar 11.4 12.3 12.5 12.6en-cy 29.3 31.1 31.9p 32.4?en-ga 17.0 18.5 18.8p 19.1?en-iw 18.8 19.7 20.2 20.2en-id 31.0 33.4 34.0p 34.3pen-ja 10.4 16.4 17.5p 18.0?en-ja* 14.9 18.0 18.2p 18.6?en-ko 24.1 31.8 31.8p 32.7?en-ms 20.4 22.5 22.9 22.9Table 3: BLEU scores for language from various lan-guage families: Arabic (ar), Welsh (cy), Irish (ga), In-donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),and Malay (ms).
Lexical reordering is not included inany of the systems.
Bolded results are significant at 99%.?
is significantly better than p in a human eval at 95%.the 1-step system.
The human judgments exactlyagree with the results of the BLEU significance tests.The gains relative to the rule reordering system ofGenzel (2010) and the no-preordering baseline areeven larger and therefore clearly also significant.In Table 4 we show results for Hungarian (hu),Dutch (nl), and Portuguese (pt).
In all cases butEnglish-Hungarian we observe significant improve-ments over the no preordering baseline.
It should benoted that the gains are not symmetric ?
sometimesthere are larger gains for translating out of English,while for Hungarian the gains are higher for trans-lating into English.
Hungarian has a free word orderwhich is difficult to predict which might partially ex-plain why there are no improvements for translatinginto Hungarian.
For Dutch-English, the forest-to-string system yields the best results, which was alsothe case for German-English, further supporting theobservation that combining different types of syn-tactic reordering approaches can be beneficial.4.3 Manually Aligned DataFor Arabic (ar), Hebrew (iw), and Japanese (ja) weconducted some additional experiments with man-ually aligned data.
We asked bilingual speakers totranslate about 20K English sentences into the re-spective target language and to mark the alignmentbetween the words.
We reserved 20% of this data forevaluation and used the rest for training.
For evalu-ation we used the fuzzy metric defined by Talbot etbase rule 1-step 2-stepen-hu 12.7 12.6 12.8 12.7en-nl 25.3 26.1 26.4 26.4en-pt 30.2 31.9 32.6 32.8hu-en 22.0 22.2 22.7 22.7nl-en 34.9 35.7 35.2 35.1nl-en* 36.3 36.5 36.6 36.7pt-en 39.8 40.1 40.1 40.1Table 4: BLEU scores for translating to and from En-glish for: Hungarian (hu), Dutch (nl), and Portuguese(pt).
Lexical reordering is not used for any language pair.Bolded results are significant at 99%.al.
(2011), which counts the fraction of words thatare reordered into the correct position.The BLEU scores in Table 5 show that trainingfrom small amounts of manually aligned data orlarge amounts of automatically aligned data resultsin models of similar quality.
In terms of the fuzzymetric, the models trained from manually aligneddata were better.
A possible explanation is that thesemodels were trained on data which was much moresimilar to the evaluation data (both were subsets ofthe manually aligned data), biasing the metric intheir favor.
In absolute terms, the reordering ac-curacy is around 80% for Arabic and Japanese andclose to 90% for Hebrew.
Most impressively, morethan 60% of the Hebrew sentences are exactly in thecorrect word order, implying that monotonic trans-lation may suffice.We also examined the accuracy of the individualclassifiers and found that the pivot classifier has anaccuracy around 95%.
It is therefore unlikely thata word is reordered to the wrong side of its head inthe 2-step reordering approach.
The classifiers thatpredict the final word order have an accuracy above90% when there are only two words and drop to stillrespectable 70%-80% when there are 4 or more chil-dren or 20 possible options.5 AnalysisIn this section, we analyze an example whose trans-lation is significantly improved by our preorderingapproach, demonstrating the usefulness of our lexi-calized features.
Consider the English sentence:It was a real whirlwind.520no reordering manual automaticfuzzy exact BLEU fuzzy exact BLEU fuzzy exact BLEUen-ar 63.2 19.8 11.4 83.5 47.6 12.4 79.0 38.9 12.6en-iw 67.9 22.2 18.8 89.8 62.4 20.3 89.2 61.2 20.2en-ja* 44.1 0.0 14.9 80.9 41.5 18.4 78.5 36.8 18.6Table 5: Preordering accuracy for the 2-step classifiers using manual alignments vs. automatic alignments.
Fuzzyrefers to the metric defined in Talbot et al(2011) and exact is the percentage of sentences with a perfect preordering.taken from the WMT test set.
The dependency parsetree is shown in Figure 2.
In our experiments therule-based approach of (Genzel, 2010) reordered thesource sentence into:It was a whirlwind real.and produced the translation:Es un torbellino real.In comparison, our 2-step system kept the Englishsentence unchanged and produced the translation:Fue un aute?ntico torbellino.The second translation is better than the first becauseof the correct tense (which is not related directly tothe preordering) and because the noun phrase ?realwhirlwind?
is ordered correctly.The main reason for the difference in the orderingis that the rule-based system can only use the unlex-icalized information from the parse tree.
The head?whirlwind?
is a noun and the child ?real?
is an ad-jective; since adjectives typically appear after nounsin Spanish, their order is reversed.To understand why the classifier-based systemkeeps ?real?
before ?whirlwind?
we can examinethe features used by the classifier to make this deci-sion.
In Table 6 we consider the 3 strongest featuresin favor of the child ?real?
appearing after the head?whirlwind?
and the three strongest features in favorof the child appearing before the head.
Recall thatthe pivot is a binary classifier: positive features sup-port one decision (in our case: the child should beafter the head) and the negative features support theother decision (the child should be before the head).The three features that have the highest positiveweight encode the fact that the child is an adjective,since in general, adjectives in Spanish appear afterthe noun.
On the other hand, the three features withthe most negative weights all encode the fact that thechild is the word ?real?
which unlike most adjec-tives tends to appear before the noun.
It is interestingto note that for this particular ordering decision thechild word is much more informative than the headword and indeed, all the important features containinformation about the child and none of them con-tains any information about the head.6 Conclusions & Future WorkWe presented a simple and novel preordering ap-proach that produces substantial improvements intranslation accuracy on a large number of languages.We use a source-side syntactic parser and train dis-criminative classifiers to predict the order of a parentand its children in the target language, using featuresfrom the dependency tree as well as (bi-)lexical fea-tures.
To decompose the exponential space of allpossible permutations, we introduce the 2-step ap-proach.
We show empirically that this approach issignificantly better than directly predicting the fullpermutation for some languages, and never signifi-cantly worse.We obtain strong results on the WMT 2010 sharedtask data, observing gains of up to 1.4 BLEU over astate-of-the-art system.
We also show gains of upto 0.5 BLEU over a strong directly comparable pre-ordering system that is based on learning unlexical-ized reordering rules.
We obtain improvements ofmore than 2 BLEU in experiments on additional lan-guages.
The gains are especially large for languageswhere the sentence structure is very different fromEnglish.
These positive results are confirmed in hu-man side-by-side evaluations.When comparing our approach to syntax-basedtranslation systems (Yamada and Knight, 2001; Gal-ley et al 2004; Huang et al 2006; Dyer and Resnik,2010) we note that both approaches use syntactic in-formation for reordering decisions.
Our preorder-ing approach has several advantages.
First, be-521It was a real whirlwind .NN VBD DT JJ NN .NOUN VERB DET ADJ NOUN .nsubjdetamodattrpROOTFigure 2: An example where lexical information is nec-essary for choosing the correct word order.cause preordering is performed before learning wordalignments, it has the potential to improve the wordalignments.
Second, by using discriminative clas-sifiers we can take advantage of lexical features.Finally, preordering can be combined with syntax-based translation models and our results confirm thecomplementary benefits that can be obtained.Compared to other preordering models, our ap-proach has the obvious problem of having to makepredictions over an exponential set of permutations.We show that this is not an insurmountable diffi-culty: our 2-step approach decomposes the exponen-tial space, often leading to much easier predictiontasks.
Even when the number of possible permuta-tions is large we can limit ourselves to the K mostpopular permutations.On the other hand, our approach provides im-portant advantages.
Compared to systems that userewrite rules, it is much easier to encode usefulknowledge that by itself is not enough to determinea full rewrite rule, such as ?a determiner is unlikelyto be the last word in a clause.?
Perhaps more im-portantly, our model provides an elegant answer tothe question of what to do when multiple rewriterules can be applied.
Previous work has employeddifferent heuristics: use the most specific rule (Xiaand McCord, 2004), use all applicable rules (Gen-zel, 2010), or use the most frequent rule (Wu et al2011).
In our model there is no need for such heuris-tics ?
all the ?rules?
are treated as features to a dis-criminative classifier, and the task of analyzing theirinteractions is handled by the learning algorithm.Compared to preordering systems that use rank-ing functions, our model has the advantage that itcan encode information about the complete permu-tation.
For example, for three source words A, B,and C, we can naturally express the useful prior thatFeature WeightPrevChild:tag=JJ,PrevSibling:a 0.448PrevChild:cat=ADJ,PrevSibling:a 0.292PrevChild:cat=ADJ,NoNextSibling 0.212...PrevChild:real,NoNextHeadSibling -0.310PrevChild:real,PrevSibling:cat=DET -0.516PrevChild:real,PrevSibling:a -0.979Table 6: The three features with the highest and low-est weights for choosing the position of ?real?
relativeto ?whirlwind.?
PrevChild means that the child is theimmediate word before the head.
PrevSibling refers tothe child?s sibling immediately to the left (the determiner?a?).
NoNextSibling and NoNextHeadSibling mean thatthe child and head do not have a sibling to the right.A-B-C and C-B-A are likely orders but C-A-B is not.Promising directions for future work are jointparsing and reordering models, and measuring theinfluence of parsing accuracy on preordering and fi-nal translation quality.ReferencesA.
Abeille?, L. Cle?ment, and F. Toussenel.
2003.
Build-ing a Treebank for French.
In A.
Abeille?, editor, Tree-banks: Building and Using Parsed Corpora, chap-ter 10.
Kluwer.P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, andP.
S. Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2).P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.
ofCoNLL ?06.C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson,M.
Przybocki, and O. Zaidan.
2010.
Findings ofthe 2010 joint workshop on statistical machine trans-lation and metrics for machine translation.
In Proc.
ofACL?05 WMT.M.
Collins, P. Koehn, and I. Kuc?erova?.
2005.
Clause re-structuring for statistical machine translation.
In Proc.of ACL ?05.M.
Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In ACL ?97.M.-C. de Marneffe, B. MacCartney, and C. Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proc.
of LREC ?06.522J.
DeNero and J. Uszkoreit.
2011.
Inducing sentencestructure from parallel corpora for reordering.
In Proc.of EMNLP ?11.J.
Duchi and Y.
Singer.
2009.
Boosting with structuralsparsity.
In Proc.
of ICML ?09.C.
Dyer and P. Resnik.
2010.
Context-free reordering,finite-state translation.
In Proc.
of NAACL-HLT ?10.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.
2004.What?s in a translation rule?
In Proc.
of NAACL-HLT?04.D.
Genzel.
2010.
Automatically learning source-side re-ordering rules for large scale machine translation.
InProc.
of COLING ?10.N.
Habash.
2007.
Syntactic preprocessing for statisticalmachine translation.
In Proc.
of MTS ?07.L.
Huang, K. Knight, and A. Joshi.
2006.
Statisticalsyntax-directed translation with extended domain oflocality.
In Proc.
of AMTA ?06.J.
Judge, A. Cahill, and J. v. Genabith.
2006.
Question-Bank: creating a corpus of parse-annotated questions.In Proc.
of ACL ?06.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase based translation.
In Proc.
of NAACL-HLT ?03.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In Proc.
of ACL-HLT?08.S.
Kumar, W. Macherey, C. Dyer, and F. Och.
2009.
Effi-cient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.In Proc.
of ACL ?09.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional Random Fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
of ICML?01.C.
H. Li, M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan.2007.
A Probabilistic Approach to Syntax-based Re-ordering for Statistical Machine Translation.
In Proc.of ACL ?07.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: ThePenn Treebank.
In Computational Linguistics.R.
McDonald, J. Nivre, Y. Quirmbach-Brundagez,Y.
Goldberg, D. Das, K. Ganchev, K. Hall, S. Petrov,H.
Zhang, O. Ta?ckstro?m, C. Bedini, N.
BertomeuCastello?, and J. Lee.
2013.
Universal dependency an-notation for multilingual parsing.
In Proc.
of ACL ?13.G.
Neubig, T. Watanabe, and S. Mori.
2012.
Inducing adiscriminative parser to optimize machine translationreordering.
In Proc.
of EMNLP-CoNLL ?12.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective depen-dency parsing.
In Proc.
of ACL ?05.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Proc.
EMNLP-CoNLL ?07.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Computa-tional Linguistics, 30(4).K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL ?02.S.
Petrov and R. McDonald.
2012.
Overview of the 2012shared task on parsing the web.
In Proc.
of NAACL ?12SANCL.S.
Petrov, D. Das, and R. McDonald.
2012.
A universalpart-of-speech tagset.
In Proc.
of LREC ?12.D.
Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,M.
Seno, and F. Och.
2011.
A lightweight evalua-tion framework for machine translation reordering.
InProc.
of EMNLP ?11 WMT.C.
Tillmann.
2004.
A unigram orientation model for sta-tistical machine translation.
In Proc.
of NAACL-HLT?04.R.
Tromble and J. Eisner.
2009.
Learning linear orderingproblems for better translation.
In Proc.
of EMNLP?09.J.
Uszkoreit and T. Brants.
2008.
Distributed word clus-tering for large scale class-based language modeling inmachine translation.
In Proc.
of ACL-HLT ?08.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In In Proc.
ofCOLING ?96.C.
Wang, M. Collins, and P. Koehn.
2007.
Chinese syn-tactic reordering for statistical machine translation.
InProc.
of EMNLP-CoNLL ?07.X.
Wu, K. Sudoh, K. Duh, H. Tsukada, and M. Nagata.2011.
Extracting pre-ordering rules from predicate-argument structures.
In Proc.
of IJCNLP ?11.F.
Xia and M. McCord.
2004.
Improving a statistical MTsystem with automatically learned rewrite patterns.
InProc.
of COLING ?04.P.
Xu, J. Kang, M. Ringgaard, and F. Och.
2009.
Using adependency parser to improve SMT for subject-object-verb languages.
In Proc.
of NAACL-HLT ?09.K.
Yamada and K. Knight.
2001.
A syntax-based statis-tical translation model.
In Proc.
of ACL ?01.N.
Yang, M. Li, D. Zhang, and N. Yu.
2012.
A ranking-based approach to word reordering for statistical ma-chine translation.
In Proc.
of ACL ?12.R.
Zens and H. Ney.
2006.
Discriminative reorderingmodels for statistical machine translation.
In Proc.
ofNAACL ?06 WMT.Y.
Zhang and J. Nivre.
2011.
Transition-based depen-dency parsing with rich non-local features.
In Proc.
ofACL-HLT ?11.H.
Zhang, L. Fang, P. Xu, and X. Wu.
2011.
Binarizedforest to string translation.
In Proc.
of ACL-HLT ?11.523
