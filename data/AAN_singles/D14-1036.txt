Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 301?312,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsIncremental Semantic Role Labeling with Tree Adjoining GrammarIoannis Konstas?, Frank Keller?, Vera Demberg?and Mirella Lapata??
: Institute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh{ikonstas,keller,mlap}@inf.ed.ac.uk?
: Cluster of Excellence Multimodal Computing and Interaction,Saarland Universityvera@coli.uni-saarland.deAbstractWe introduce the task of incremental se-mantic role labeling (iSRL), in which se-mantic roles are assigned to incompleteinput (sentence prefixes).
iSRL is thesemantic equivalent of incremental pars-ing, and is useful for language model-ing, sentence completion, machine trans-lation, and psycholinguistic modeling.
Wepropose an iSRL system that combinesan incremental TAG parser with a seman-tically enriched lexicon, a role propaga-tion algorithm, and a cascade of classi-fiers.
Our approach achieves an SRL F-score of 78.38% on the standard CoNLL2009 dataset.
It substantially outper-forms a strong baseline that combinesgold-standard syntactic dependencies withheuristic role assignment, as well as abaseline based on Nivre?s incremental de-pendency parser.1 IntroductionHumans are able to assign semantic roles such asagent, patient, and theme to an incoming sentencebefore it is complete, i.e., they incrementally buildup a partial semantic representation of a sentenceprefix.
As an example, consider:(1) The athlete realized [hergoals]PATIENT/THEMEwere out of reach.When reaching the noun phrase her goals, the hu-man language processor is faced with a semanticrole ambiguity: her goals can either be the PA-TIENT of the verb realize, or it can be the THEMEof a subsequent verb that has not been encoun-tered yet.
Experimental evidence shows that thehuman language processor initially prefers the PA-TIENT role, but switches its preference to thetheme role when it reaches the subordinate verbwere.
Such semantic garden paths occur becausehuman language processing occurs word-by-word,and are well attested in the psycholinguistic litera-ture (e.g., Pickering et al., 2000).Computational systems for performing seman-tic role labeling (SRL), on the other hand, proceednon-incrementally.
They require the whole sen-tence (typically together with its complete syntac-tic structure) as input and assign all semantic rolesat once.
The reason for this is that most featuresused by current SRL systems are defined globally,and cannot be computed on sentence prefixes.In this paper, we propose incremental SRL(iSRL) as a new computational task that mimicshuman semantic role assignment.
The aim of aniSRL system is to determine semantic roles whilethe input unfolds: given a sentence prefix and itspartial syntactic structure (typically generated byan incremental parser), we need to (a) identifywhich words in the input participate in the seman-tic roles as arguments and predicates (the task ofrole identification), and (b) assign correct seman-tic labels to these predicate/argument pairs (thetask of role labeling).
Performing these two tasksincrementally is substantially harder than doing itnon-incrementally, as the processor needs to com-mit to a role assignment on the basis of incom-plete syntactic and semantic information.
As anexample, take (1): on reaching athlete, the proces-sor should assign this word the AGENT role, eventhough it has not seen the corresponding predicateyet.
Similarly, upon reaching realized, the pro-cessor can complete the AGENT role, but it shouldalso predict that this verb also has a PATIENT role,even though it has not yet encountered the argu-ment that fills this role.
A system that performsSRL in a fully incremental fashion therefore needsto be able to assign incomplete semantic roles,unlike existing full-sentence SRL models.The uses of incremental SRL mirror the applica-tions of incremental parsing: iSRL models can beused in language modeling to assign better stringprobabilities, in sentence completion systems to301provide semantically informed completions, inany real time application systems, such as dia-log processing, and to incrementalize applicationssuch as machine translation (e.g., in speech-to-speech MT).
Crucially, any comprehensive modelof human language understanding needs to com-bine an incremental parser with an incremental se-mantic processor (Pad?o et al., 2009; Keller, 2010).The present work takes inspiration from thepsycholinguistic modeling literature by proposingan iSRL system that is built on top of a cogni-tively motivated incremental parser, viz., the Psy-cholinguistically Motivated Tree Adjoining Gram-mar parser of Demberg et al.
(2013).
This parserincludes a predictive component, i.e., it predictssyntactic structure for upcoming input during in-cremental processing.
This makes PLTAG par-ticularly suitable for iSRL, allowing it to predictincomplete semantic roles as the input string un-folds.
Competing approaches, such as iSRL basedon an incremental dependency parser, do not sharethis advantage, as we will discuss in Section 4.3.2 Related WorkMost SRL systems to date conceptualize seman-tic role labeling as a supervised learning prob-lem and rely on role-annotated data for modeltraining.
Existing models often implement atwo-stage architecture in which role identificationand role labeling are performed in sequence.
Su-pervised methods deliver reasonably good perfor-mance with F-scores in the low eighties on stan-dard test collections for English (M`arquez et al.,2008; Bj?orkelund et al., 2009).Current approaches rely primarily on syntacticfeatures (such as path features) in order to iden-tify and label roles.
This has been a mixed bless-ing as the path from an argument to the predi-cate can be very informative but is often quitecomplicated, and depends on the syntactic formal-ism used.
Many paths through the parse tree arelikely to occur infrequently (or not at all), result-ing in very sparse information for the classifier tolearn from.
Moreover, as we will discuss in Sec-tion 4.4, such path information is not always avail-able when the input is processed incrementally.There is previous SRL work employing Tree Ad-joining Grammar, albeit in a non-incremental set-ting, as a means to reduce the sparsity of syntax-based features.
Liu and Sarkar (2007) extract arich feature set from TAG derivations and demon-strate that this improves SRL performance.In contrast to incremental parsing, incrementalsemantic role labeling is a novel task.
Our modelbuilds on an incremental Tree Adjoining Gram-mar parser (Demberg et al., 2013) which predictsthe syntactic structure of upcoming input.
This al-lows us to perform incremental parsing and incre-mental SRL in tandem, exploiting the predictivecomponent of the parser to assign (potentially in-complete) semantic roles on a word-by-word ba-sis.
Similar to work on incremental parsing thatevaluates incomplete trees (Sangati and Keller,2013), we evaluate the incomplete semantic struc-tures produced by our model.3 Psycholinguistically Motivated TAGDemberg et al.
(2013) introduce Psycholin-guistically Motivated Tree Adjoining Grammar(PLTAG), a grammar formalism that extends stan-dard TAG (Joshi and Schabes, 1992) in order toenable incremental parsing.
Standard TAG as-sumes a lexicon of elementary trees, each ofwhich contains at least one lexical item as an an-chor and at most one leaf node as a foot node,marked with A?.
All other leaves are marked withA?
and are called substitution nodes.
Elementarytrees that contain a foot node are called auxiliarytrees; those that do not are called initial trees.
Ex-amples for TAG elementary trees are given in Fig-ure 1a?c.To derive a TAG parse for a sentence, we startwith the elementary tree of the head of the sen-tence and integrate the elementary trees of theother lexical items of the sentence using two oper-ations: adjunction at an internal node and substi-tution at a substitution node (the node at which theoperation applies is the integration point).
Stan-dard TAG derivations are not guaranteed to be in-cremental, as adjunction can happen anywhere ina sentence, possibly violating left-to-right process-ing order.
PLTAG addresses this limitation by in-troducing prediction trees, elementary trees with-out a lexical anchor.
These can be used to predictsyntactic structure anchored by words that appearlater in an incremental derivation.
The use of pre-diction trees ensures that fully connected prefixtrees can be built for every prefix of the input sen-tence.Each node in a prediction tree carries mark-ers to indicate that this node was predicted, ratherthan being anchored by the current sentence pre-fix.
An example is Figure 1d, which contains aprediction tree with marker ?1?.
In PLTAG, mark-ers are eliminated through a new operation calledverification, which matches them with the nodes302(a) NPNNSBanks(b) SVPVBopenNP?
(c) VPVP*APRBrarely(d) S1VP11NP1?Figure 1: PLTAG lexicon entries: (a) and (b) ini-tial trees, (c) auxiliary tree, (d) prediction tree.aSB?C?aSBC?baSB?Cc(a) valid (b) invalidFigure 3: The current fringe (dashed line) indi-cates where valid substitutions can occur.
Othersubstitutions result in an invalid prefix tree.of non-predictive elementary trees.
An exampleof a PLTAG derivation is given in Figure 2.
Instep 1, a prediction tree is introduced through sub-stitution, which then allows the adjunction of anadverb in step 2.
Step 3 involves the verificationof the marker introduced by the prediction treeagainst the elementary tree for open.In order to efficiently parse PLTAG, Demberget al.
(2013) introduce the concept of fringes.Fringes capture the fact that in an incrementalderivation, a prefix tree can only be combined withan elementary tree at a limited set of nodes.
Forinstance, the prefix tree in Figure 3 has two substi-tution nodes, for B and C. However, only substi-tution into B leads to a valid new prefix tree; if wesubstitute into C, we obtain the tree in Figure 3b,which is not a valid prefix tree (i.e., it represents anon-incremental derivation).The parsing algorithm proposed by Demberget al.
(2013) exploits fringes to tabulate interme-diate results.
It manipulates a chart in which eachcell (i, f ) contains all the prefix trees whose firsti leaves are the first i words and whose currentfringe is f .
To extend the prefix trees for i tothe prefix trees for i+ 1, the algorithm retrievesall current fringes f such that the chart has entriesin the cell (i, f ).
For each such fringe, it needsto determine the elementary trees in the lexiconthat can be combined with f using substitution oradjunction.
In spite of the large size of a typi-cal TAG lexicon, this can be done efficiently, asit only requires matching the current fringes.
Foreach match, the parser then computes the new pre-Banks refused to open todayA0A1A1AM-TMPnsbj auxxcomptmod?A0,Banks,refused??A1,to,refused??A1,Banks,open?
?AM-TMP,today,open?Figure 4: Syntactic dependency graph with se-mantic role annotation and the accompanying se-mantic triples, for Banks refused to open today.fix trees and its new current fringe f?and enters itinto cell (i+1, f?
).Demberg et al.
(2013) convert the Penn Tree-bank (Marcus et al., 1993) into TAG for-mat by enriching it with head information andargument/modifier information from Propbank(Palmer et al., 2005).
This makes it possibleto decompose the Treebank trees into elementarytrees as proposed by Xia et al.
(2000).
Predic-tion trees can be learned from the converted Tree-bank by calculating the connection path (Mazzeiet al., 2007) at each word in a tree.
Intuitively,a prediction tree for word wncontains the struc-ture that is necessary to connect wnto the prefixtree w1.
.
.wn?1, but is not part of any of the ele-mentary trees of w1.
.
.wn?1.
Using this lexicon, aprobabilistic model over PLTAG operations can beestimated following Chiang (2000).4 Model4.1 Problem FormulationIn a typical semantic role labeling scenario, thegoal is to first identify words that are predicatesin the sentence and then identify and label all thearguments for each predicate.
This translates intospotting specific words in a sentence that repre-sent the predicate?s arguments, and assigning pre-defined semantic role labels to them.
Note that inthis work we focus on verb predicates only.
Theoutput of a semantic role labeler is a set of seman-tic dependency triples ?l,a, p?, with l ?
R , anda, p ?
w, where R is a set of semantic role labelsdenoting a specific relationship between a predi-cate and an argument (e.g., ARG0, ARG1, ARGMin Propbank), w is the list of words in the sentence,l denotes a specific role label, a the argument, andp the predicate.
An example is shown in Figure 4.As discussed in the introduction, standard se-mantic role labelers make their decisions based onevidence from the whole sentence.
In contrast, ouraim is to assign semantic roles incrementally, i.e.,303NPNNSBanksS1VP11NPNNSBanksS1VP1VP1APRBrarelyNPNNSBanksSVPVPVBopenAPRBrarelyNPNNSBanks1.
subst2.
adj3.
verifFigure 2: Incremental parse for Banks rarely open using the operations substitution (with a predictiontree), adjunction, and verification.we want to produce a set of (potentially incom-plete) semantic dependency triples for each prefixof the input sentence.
Note that not every wordis an argument to a predicate, therefore the set oftriples will not necessarily change at every inputword.
Furthermore, the triples themselves maybe incomplete, as either the predicate or the argu-ment may not have been observed yet (predicate-incomplete or argument-incomplete triples).Our iSRL system relies on PLTAG, using a se-mantically augmented lexicon.
We parse an in-put sentence incrementally, applying a novel in-cremental role propagation algorithm (IRPA) thatcreates or updates existing semantic triple candi-dates whenever an elementary (or prediction) treecontaining role information is attached to the ex-isting prefix tree.
As soon as a triple is completedwe apply a two-stage classification process, thatfirst identifies whether the predicate/argument pairis a good candidate, and then disambiguates rolelabels in case there is more than one candidate.4.2 Semantic Role LexiconRecall that Propbank is used to construct thePLTAG treebank, in order to distinguish betweenarguments and modifiers, which result in elemen-tary trees with substitution nodes, and auxiliarytrees, i.e., trees with a foot node, respectively (seeFigure 1).
Conveniently, we can use the same in-formation to also enrich the extracted lexicon withthe semantic role annotations, following the pro-cess described by Sayeed and Demberg (2013).1For arguments, annotations are retained on thesubstitution node in the parental tree, while formodifiers, the role annotation is displayed on thefoot node of the auxiliary tree.
Note that we dis-play role annotation on traces that are leaf nodes,1Contrary to Sayeed and Demberg (2013) we put role la-bel annotations for PPs on the preposition rather than theirNP child, following of the CoNLL 2005 shared task (Carrerasand M`arquez, 2005).which enables us to recover long-range dependen-cies (third and fifth tree in Figure 5a).
Likewise,we annotate prediction trees with semantic roles,which enables our system to predict upcoming in-complete triples.Our annotation procedure unavoidably intro-duces some role ambiguity, especially for fre-quently occurring trees.
This can give rise to twoproblems when we generate semantic triples incre-mentally: IRPA tends to create many spurious can-didate semantic triples for elementary trees thatcorrespond to high frequency words (e.g., preposi-tions or modals).
Secondly, a semantic triple maybe identified correctly but is assigned several rolelabels.
(See the elementary tree for refuse in Fig-ure 5a.)
We address these issues by applying clas-sifiers for role label disambiguation at every pars-ing operation (substitution, adjunction, or verifica-tion), as detailed in Section 4.4.4.3 Incremental Role Propagation AlgorithmThe main idea behind IRPA is to create or up-date existing semantic triples as soon as there isavailable role information during parsing.
Our al-gorithm (lines 1?6 in Algorithm 1) is applied af-ter every PLTAG parsing operation, i.e., when anelementary or prediction tree T is adjoined to aparticular integration point node piipof the prefixtree of the sentence, via substitution or adjunction(lines 3?4).2In case an elementary tree Tvverifiesa prediction tree Tpr(lines 5?6), the same method-ology applies, the only difference being that wehave to tackle multiple integration point nodesTpr,ip, one for each prediction marker of Tprthatmatches the corresponding nodes in Tv.For simplicity of presentation, we will use aconcrete example, see Figure 5.
Figure 5a showsthe lexicon entries for the words of the sentence2Prediction tree Tprin our algorithm is only used duringverification, so it set to nil for substitution and adjunction op-erations.304Banks refused to open.
Naturally, some nodes inthe lexicon trees might have multiple candidaterole labels.
For example, the substitution NP nodeof the second tree takes two labels, namely A0and A1.
These stem from different role signatureswhen the same elementary tree occurs in differ-ent contexts during training (A1 only on the NP;A0 on the NP and A1 on S).
For simplicity?s sake,we collapse different signatures, and let a classi-fier labeller to disambiguate such cases (see Sec-tion 4.4).Algorithm 1 Incremental Role Propagation Alg.1: procedure IRPA(piip, T , Tpr)2: ???
.
?
is a dictionary of (piip, ?l,a, p?)
pairs3: if parser operation is substitution or adjunction then4: CREATE-TRIPLES(piip, T )5: else if parser operation is verification then6: CREATE-TRIPLES-VERIF(piip, T , Tpr)return set of triples ?l,a, p?
for prefix tree pi7: procedure CREATE-TRIPLES(piip, T )8: if HAS-ROLES(piip) then9: UPDATE-TRIPLE(piip, T )10: else if HAS-ROLES(T ) then11: Tip?
substitution or foot node of T12: ADD-TRIPLE(piip, Tip, T )13: for all remaining nodes n ?
T with roles do14: ADD-TRIPLE(piip, n, T ) .
incomplete triples15: procedure CREATE-TRIPLES-VERIF(piip, Tv, Tpr)16: if HAS-ROLES(Tv) then17: anchor?
lexeme of Tv18: for all Tip?
node in Tvwith role do19: Tpr,ip?
matching node of Tipin Tpr20: CREATE-TRIPLES(Tpr,ip, Tv).
Process the rest of covered nodes in Tprwith roles21: for all remaining Tpr,ip?
node in Tprwith role do22: UPDATE-TRIPLE(Tpr,ip, Tpr)23: function UPDATE-TRIPLE(piip, T )24: dep?
FIND-INCOMPLETE(?, Tip)25: anchor?
lexeme of T26: if anchor of T is predicate then27: SET-PREDICATE(dep, anchor)28: else if anchor of T is argument then29: SET-ARGUMENT(dep, anchor)return dep30: procedure ADD-TRIPLE(piip, Tip, T )31: dep?
?
[roles of Tip], nil, nil?32: anchor?
lexeme of T33: if anchor of T is predicate then34: SET-PREDICATE(dep, anchor)35: SET-ARGUMENT(dep, head of piip)36: else if anchor of T is argument then37: if T is auxiliary then .
adjunction38: SET-ARGUMENT(dep, anchor)39: else .
substitution: arg is head of prefix tree40: SET-ARGUMENT(dep, head of Tip)41: pred?
find dep ?
?
with matching piip42: SET-PREDICATE(dep, pred)43: ??
(piip, dep)Once we process Banks, the prefix tree becomesthe lexical entry for this word, see the first col-umn of Figure 5b.
Next, we process refused:the parser substitutes the prefix tree into the ele-mentary tree T of refused;3the integration pointpiipon the prefix tree is the topmost NP.
Sincethe operation is a substitution (line 3), we createtriples between T and piipvia CREATE-TRIPLES(lines 7?12).
piipdoes not have any role infor-mation (line 8), so we proceed to add a new se-mantic triple between the role-labeled integrationpoint Tip, i.e., substitution NP node of T , and piip,via ADD-TRIPLE (lines 30?43).
First, we createan incomplete semantic triple with all roles fromTip(line 31).
Then we set the predicate to the an-chor of T to be the word refused, and the argu-ment to be the head word of the prefix tree, Banks(lines 34?35).
Note that predicate identification isa trivial task based on part-of-speech informationin the elementary tree.4Then, we add the pair (NP?
?{A0,A1},Banks,refused?)
to a dictionary (line 43).
Storing the in-tegration point along with the semantic triple isessential, to be able to recover incomplete triplesin later stages of the algorithm.
Finally, we re-peat this process for all remaining nodes on T thathave roles, in our example the substitution node S(lines 13?14).
This outputs an incomplete triple,?
{A1},nil,refused?.Next, the parser decides to substitute a predic-tion tree (third tree in Figure 5a) into the substitu-tion node S of the prefix tree.
Since the integrationpoint is on the prefix tree and has role information(line 8), the corresponding triple should already bepresent in our dictionary.
Upon retrieving it, weset the nil argument to the anchor of the incomingtree.
Since it is a prediction tree, we set it to theroot of the tree, namely S2(phrase labels in triplesare denoted by italics), but mark the triple as yetincomplete.
This distinction allows us to fill in thecorrect lexical information once it becomes avail-able, i.e, when the tree gets verified.
We also addan incomplete triple for the trace t in the subjectposition of the prediction tree, as described above.Note that this triple contains multiple roles; this isexpected given that prediction trees are unlexical-ized and occur in a wide variety of contexts.When the next verb arrives, the parser success-fully verifies it against the embedded prediction3PLTAG parsing operations can occur in two ways: Anelementary tree can be substituted into the substitution nodeof the prefix tree, or the prefix tree can be substituted into anode of an elementary tree.
The same holds for adjunction.4Most predicates can be identified as anchors of non-modifier auxiliary trees.
However, there are exceptions tothis rule, i.e., modifier auxiliary trees and non-modifier non-auxiliary trees being also verbs in our lexicon, hence the useof the more reliable POS tags.305IRPA MaltParserBanks ?
?refused ?{A0,A1},Banks,refused?,?A1,S2,refused?,?{A0,A1,A2},t,nil?
?A0,Banks,refused?to ?
?open ?A1,to,refused?,?A1,Banks,open?
?A1,to,refused?,?A0,Banks,open?today ?AM-TMP,today,open?
?AM-TMP,today,open?Table 1: Complete and incomplete semantic triplegeneration, comparing IRPA and a system thatmaps gold-standard role labels onto MaltParser in-cremental dependencies for Figure 4.tree within the prefix tree (last step of Figure 5b).Our algorithm first cycles through all nodes thatmatch between the verification tree Tvand the pre-diction tree Tprand will complete or create newtriples via CREATE-TRIPLES (lines 18?20).
Inour example, the second semantic triple gets com-pleted by replacing S2with the head of the sub-tree rooted in S. Normally, this would be the verbopen, but in this case the verb is followed by theinfinitive marker to, hence we heuristically set itto be the argument of the triple instead, followingCarreras and M`arquez (2005).
For the last triple,we set the predicate to the anchor of Tvopen, andnow are able to remove the excess role labels A0and A2.
This illustrated how the lexicalized veri-fication tree disambiguates the semantic informa-tion stored in the prediction tree.
Finally, trace t isset to the closest NP head that is below the samephrase subtree, in this case Banks.
Note that Banksis part of two triples as shown in the last tree ofFigure 5b: it is either an A0 or an A1 for refusedand an A1 for open.We are able to create incomplete semantictriples after the prediction of the upcoming verb atstep 2, as shown in Figure 5b.
This is not possibleusing an incremental dependency parser such asMaltParser (Nivre et al., 2007) that lacks a predic-tive component.
Table 1 illustrates this by compar-ing the output of IRPA for Figure 5b with the out-put of a baseline system that maps role labels ontothe syntactic dependencies in Figure 4, generatedincrementally by MaltParser (see Section 5.3 fora description of the MaltParser baseline).
Malt-Parser has to wait for the verb open before out-putting the relevant semantic triples.
In contrast,IRPA outputs incomplete triples as soon as the in-formation is available, and later on updates its de-cision.
(MaltParser also incorrectly assigns A0 forthe Banks?open pair.
)4.4 Argument Identification and Role LabelDisambiguationIRPA produces semantic triples for every role an-notation present in the lexicon entries, which willoften overgenerate role information.
Furthermore,some triples have more than one role label at-tached to them.
During verification, we are able tofilter out the majority of labels in the correspond-ing prediction trees; However, most triples are cre-ated via substitution and adjunction.In order to address these problems we adhere tothe following classification and ranking strategy:after each semantic triple gets completed, we per-form a binary classification that evaluates its suit-ability as a whole, given bilexical and syntactic in-formation.
If the triple is identified as a good can-didate, then we perform multi-class classificationover role labels: we feed the same bilexical andsyntactic information to a logistic classifier, andget a ranked list of labels.
We then use this list tore-rank the existing ambiguous role labels in thesemantic triple, and output the top scoring ones.The identifier is a binary L2-loss support vec-tor classifier, and the role disambiguator an L2-regularized logistic regression classifier, both im-plemented using the efficient LIBLINEAR frame-work of Fan et al.
(2008).
The features used arebased on Bj?orkelund et al.
(2009) and Liu andSarkar (2007), and are listed in Table 2.The bilexical features are: predicate POS tag,predicate lemma, argument word form, argumentPOS tag, and position.
The latter indicates the po-sition of the argument relative to the predicate, i.e.,before, on, or after.
The syntactic features are:the predicate and argument elementary trees with-out the anchors (to avoid sparsity), the category ofthe integration point node on the prefix tree wherethe elementary tree of the argument attaches to,an alphabetically ordered set of the categories ofthe fringe nodes of the prefix tree after attachingthe argument tree, and the path of PLTAG opera-tions applied between the argument and the pred-icate.
Note that most of the original features usedby Bj?orkelund et al.
(2009) and others are not ap-plicable in our context, as they exploit informationthat is not accessible incrementally.
For example,sibling information to the right of the word is notavailable.
Furthermore, our PLTAG parser doesnot compute syntactic dependencies, hence thesecannot serve as features (and in any case not alldependencies are available incrementally, see Fig-ure 4).
To counterbalance this, we use local syn-tactic information stored in the fringe of the pre-306NPNNSBanksSVPS?{A1}VPVBDrefusedNP?
{A0,A1}S2VP22VB22NP21t11{A0,A1,A2}VPVP?TOtoSVPVBopenNPt{A1}(a) Lexicon entriesNPNNSBanksSVPS?{A1}VPVBDrefusedNPNNSBanks{A0,A1}SVPS2{A1}VP22VB22NP21t11{A0,A1,A2}VPVBDrefusedNPNNSBanks{A0,A1}SVPS2{A1}VP2VP2VB22TOtoNP21t11{A0,A1,A2}VPVBDrefusedNPNNSBanks{A0,A1}SVPSVPVPVBopenTOto{A1}NPtVPVBDrefusedNPNNSBanks{A0,A1}/{A1}1.
subst 2. subst3.
adj4.
verif1.
NP ?
?
{A0,A1},Banks,refused?S ?
?A1,nil,refused?2.
NP ?
?
{A0,A1},Banks,refused?S ?
?A1,S2,refused?NP ?
?{A0,A1,A2},t,nil?3.
?4.
NP ?
?
{A0,A1},Banks,refused?S ?
?A1,to,refused?NP ?
?A1,Banks,open?
(b) Incremental parsing using PLTAG and incremental propagation of rolesFigure 5: Incremental Role Propagation Algorithm application for the sentence Banks refused to open.Bilexical SyntacticPredPOS PredElemTreePredLemma ArgElemTreeArgWord IntegrationPointArgPOS PrefixFringePosition OperationPathTable 2: Features for argument identification androle label disambiguation.fix tree.
We also store the series of operations ap-plied by our parser between argument and predi-cate, in an effort to emulate the effect of recover-ing longer-range patterns.5 Experimental Design5.1 PLTAG and Classifier TrainingWe extracted the semantically-enriched lexiconand trained the PLTAG parser by converting theWall Street Journal part of Penn Treebank toPLTAG format.
We used Propbank to retrievesemantic role annotation, as described in Sec-tion 4.2.
We trained the PLTAG parser accordingto Demberg et al.
(2013) and evaluated the parseron section 23, on sentences with 40 words or less,given gold POS tags for each word, and achieveda labeled bracket F1score of 79.41.In order to train the argument identification androle label disambiguation classifiers, we used theEnglish portion of the CoNLL 2009 Shared Task(Haji?c et al., 2009; Surdeanu et al., 2008).
Itconsists of the Penn Treebank, automatically con-verted to dependencies following Johansson and307Nugues (2007), accompanied by semantic role la-bel annotation for every argument pair.
The latteris converted from Propbank based on Carreras andM`arquez (2005).
We extracted the bilexical fea-tures for the classifiers directly from the gold stan-dard annotation of the training set.
The syntacticfeatures were obtained as follows: for every sen-tence in the training set we applied IRPA using thetrained PLTAG parser, with gold standard lexiconentries for each word of the input sentence.
Thisensures near perfect parsing accuracy.
Then foreach semantic triple predicted incrementally, weextracted the relevant syntactic information in or-der to construct training vectors.
If the identifiedpredicate-argument pair was in the gold standardthen we assigned a positive label for the identifi-cation classifier, otherwise we flagged it as nega-tive.
For those pairs that are not identified by IRPAbut exist in the gold standard (false negatives), weextracted syntactic information from already iden-tified similar triples, as follows: We first look forcorrectly identified arguments, wrongly attachedto a different predicate and re-create the triple withcorrect predicate/argument information.
If no ar-gument is found, we then pick the argument in thelist of identified arguments for a correct predicatewith the same POS-tag as the gold-standard argu-ment.
In the case of the role label disambigua-tion classifier we just assign the gold label for ev-ery correctly identified pair, and ignore the (possi-bly ambiguous) predicted one.
After tuning on thedevelopment set, the argument identifier achievedan accuracy of 92.18%, and the role label disam-biguation classifier, 82.37%.5.2 EvaluationThe focus of this paper is to build a system that isable to output semantic role labels for predicate-argument pairs incrementally, as soon as they be-come available.
In order to properly evaluate sucha system, we need to measure its performance in-crementally.
We propose two different cumulativescores for assessing the (possibly incomplete) se-mantic triples that have been created so far, as theinput is processed from left to right, per word.
Thefirst metric is called Unlabeled Prediction Score(UPS) and gets updated for every identified argu-ment or predicate, even if the corresponding se-mantic triple is incomplete.
Note that UPS doesnot take into account the role label, it only mea-sures predicate and argument identification.
In thisrespect it is analogous to unlabeled dependencyaccuracy reported in the parsing literature.
We ex-pect a model that is able to predict semantic rolesto achieve an improved UPS result compared to asystem that does not do prediction, as illustrated inTable 1.
Our second score, Combined IncrementalSRL Score (CISS), measures the identification ofcomplete semantic role triples (i.e., correct predi-cate, predicate sense, argument, and role label) perword; by the end of the sentence, CISS coincideswith standard combined SRL accuracy, as reportedin CoNLL 2009 SRL-only task.
This score is anal-ogous to labeled dependency accuracy in parsing.Note that conventional SRL systems such asBj?orkelund et al.
(2009) typically assume goldstandard syntactic information.
In order to emu-late this, we give our parser gold standard lexiconentries for each word in the test set; these containall possible roles observed in the training set fora given elementary tree (and all possible sensesfor each predicate).
This way the parser achievesa syntactic parsing F1score of 94.24, thus ensur-ing the errors of our system can be attributed toIRPA and the classifiers.
Also note that we evalu-ate on verb predicates only, therefore trivially re-ducing the task of predicate identification to thesimple heuristic of looking for words in the sen-tence with a verb-related POS tag and excludingauxiliaries and modals.
Likewise, predicate sensedisambiguation on verbs presumably is trivial, aswe observed almost no ambiguity of senses amonglexicon entries of the same verb (we adhered to asimple majority baseline, by picking the most fre-quent sense, given the lexeme of the verb, in thefew ambiguous cases).
It seems that the syntacticinformation held in the elementary trees discrimi-nates well among different senses.5.3 System ComparisonWe evaluated three configurations of our system.The first configuration (iSRL) uses all seman-tic roles for each PLTAG lexicon entry, appliesthe PLTAG parser, IRPA, and both classifiers toperform identification and disambiguation, as de-scribed in Section 4.
The second one (Majority-Baseline), solves the problem of argument identifi-cation and role disambiguation without the classi-fiers.
For the former we employ a set of heuristicsaccording to Lang and Lapata (2014), that rely ongold syntactic dependency information, sourcedfrom CoNLL input.
For the latter, we choose themost frequent role given the gold standard depen-dency relation label for the particular argument.Note that dependencies have been produced inview of the whole sentence and not incrementally.308System Prec Rec F1iSRL-Oracle 91.00 80.26 85.29iSRL 81.48 75.51 78.38Majority-Baseline 71.05 58.10 63.92Malt-Baseline 60.90 46.14 52.50Table 3: Full-sentence combined SRL scoreThis gives the baseline a considerable advantageespecially in case of longer range dependencies.The third configuration (iSRL-Oracle), is identicalto iSRL, but uses the gold standard roles for eachPLTAG lexicon entry, and thus provides an upper-bound for our methodology.
Finally, we evalu-ated against Malt-Baseline, a variant of Majority-Baseline that uses the MaltParser of Nivre et al.
(2007) to provide labeled syntactic dependenciesMaltParser is a state-of-the-art shift-reduce depen-dency parser which uses an incremental algorithm.Following Beuck et al.
(2011), we modified theparser to provide intermediate output at each wordby emitting the current state of the dependencygraph before each shift step.
We trained Malt-Parser using the arc-eager algorithm (which out-performed the other parsing algorithms availablewith MaltParser) on the CoNLL dataset, achiev-ing a labeled dependency accuracy of 89.66% onsection 23.6 ResultsFigures 6 and 7 show the results on the incremen-tal SRL task.
We plot the F1for Unlabeled Predic-tion Score (UPS) and Combined Incremental SRLScore (CISS) per word, separately for sentencesof lengths 10, 20, 30, and 40 words.
The task getsharder with increasing sentence length, hence wecan only meaningfully compare the average scoresfor sentence of the same length.
(This approachwas proposed by Sangati and Keller 2013 for eval-uating the performance of incremental parsers.
)The UPS results in Figure 6 clearly show thatour system (iSRL) outperforms both baselineson unlabeled argument and predicate prediction,across all four sentence lengths.
Furthermore,we note that the iSRL system achieves a near-constant performance for all sentence prefixes.Our PLTAG-based prediction/verification archi-tecture allows us to correctly predict incompletesemantic role triples, even at the beginning of thesentence.
Both baselines perform worse than theiSRL system in general.
Moreover, the Malt-Baseline performs badly on the initial sentenceprefixes (up to word 10), presumably as it doesnot benefit from syntactic prediction, and thus can-not generate incomplete triples early in the sen-tence, as illustrated in Table 1.
The Majority-Baseline also does not do prediction, but it has ac-cess to gold-standard syntactic dependencies, andthus outperforms the Malt-Baseline on initial sen-tence prefixes.
Note that due to prediction, oursystem tends to over-generate incomplete triplesin the beginning of sentences, compared to non-incremental output, which may inflate UPS forthe first words.
However, this cancels out laterin the sentence if triples are correctly completed;failure to do so would decrease UPS.
The near-constant performance of our output illustrates thisphenomenon.
Finally, the iSRL-Oracle outper-forms all other systems, as it benefits from correctrole labels and correct PLTAG syntax, thus provid-ing an upper limit on performance.The CISS results in Figure 7 present a simi-lar picture.
Again, the iSRL system outperformsboth baselines at all sentence lengths.
In addition,it shows particularly strong performance (almostat the level of the iSRL-Oracle) at the beginningof the sentence.
This presumably is due to thefact that our system uses prediction and is able toidentify correct semantic role triples earlier in thesentence.
The baselines also show higher perfor-mance early in the sentence, but to a lesser degree.Table 3 reports traditional combined SRL scoresfor full sentences over all sentence lengths, asdefined for the CoNLL task.
Our iSRL systemoutperforms the Majority-Baseline by almost 15points, and the Malt-Baseline by 25 points.
It re-mains seven points below the iSRL-Oracle upperlimit.Finally, in order to test the effect of syntacticparsing on our system, we also experimented witha variant of our iSRL system that utilizes all lex-icon entries for each word in the test set.
This issimilar to performing the CoNLL 2009 joint task,which is designed for systems that carry out bothsyntactic parsing and semantic role labeling.
Thisvariant achieved a full sentence F-score of 68.0%,i.e., around 10 points lower than our iSRL system.This drop in score correlates with the differencein syntactic parsing F-score between the two ver-sions of PLTAG parser (94.24 versus 79.41), andis expected given the high ambiguity of the lex-icon entries for each word.
Note, however, thatthe full-parsing version of our system still outper-forms Malt-Baseline by 15 points.3092 4 6 8 100.20.40.60.81wordsF1(a) 10 words5 10 15 200.20.40.60.81wordsF1iSRL-Oracle iSRLMajority-Baseline Malt-Baseline(b) 20 words5 10 15 20 25 300.20.40.60.81wordsF1(c) 30 words10 20 30 400.20.40.60.81wordsF1(d) 40 wordsFigure 6: Unlabeled Prediction Score (UPS)2 4 6 8 100.20.40.60.81wordsF1(a) 10 words5 10 15 200.20.40.60.81wordsF1iSRL-Oracle iSRLMajority-Baseline Malt-Baseline(b) 20 words5 10 15 20 25 300.20.40.60.81wordsF1(c) 30 words10 20 30 400.20.40.60.81wordsF1(d) 40 wordsFigure 7: Combined iSRL Score (CISS)7 ConclusionsIn this paper, we introduced the new task of incre-mental semantic role labeling and proposed a sys-tem that solves this task by combining an incre-mental TAG parser with a semantically enrichedlexicon, a role propagation algorithm, and a cas-cade of classifiers.
This system achieved a full-sentence SRL F-score of 78.38% on the standardCoNLL dataset.
Not only is the full-sentencescore considerably higher than the Majority-Baseline (which is a strong baseline, as it usesgold-standard syntactic dependencies), but wealso observe that our iSRL system performs wellincrementally, i.e., it predicts both complete andincomplete semantic role triples correctly early onin the sentence.
We attributed this to the fact thatour TAG-based architecture makes it possible topredict upcoming syntactic structure together withthe corresponding semantic roles.AcknowledgmentsEPSRC support through grant EP/I032916/1 ?Anintegrated model of syntactic and semantic predic-tion in human language processing?
to FK and MLis gratefully acknowledged.ReferencesBeuck, Niels, Arne Khn, and Wolfgang Menzel.2011.
Incremental parsing and the evaluationof partial dependency analyses.
In Proceedingsof the 1st International Conference on Depen-dency Linguistics.
Depling 2011.Bj?orkelund, Anders, Love Hafdell, and PierreNugues.
2009.
Multilingual semantic role la-beling.
In Proceedings of the Thirteenth Con-ference on Computational Natural LanguageLearning: Shared Task.
Association for Com-putational Linguistics, Stroudsburg, PA, USA,CoNLL ?09, pages 43?48.Carreras, Xavier and Llu?
?s M`arquez.
2005.
Intro-duction to the conll-2005 shared task: Semanticrole labeling.
In Proceedings of the Ninth Con-ference on Computational Natural LanguageLearning.
Association for Computational Lin-guistics, Stroudsburg, PA, USA, CONLL ?05,pages 152?164.Chiang, David.
2000.
Statistical parsing withan automatically-extracted tree adjoining gram-mar.
In Proceedings of the 38th Annual Meetingon Association for Computational Linguistics.pages 456?463.Demberg, Vera, Frank Keller, and AlexanderKoller.
2013.
Incremental, predictive pars-ing with psycholinguistically motivated tree-adjoining grammar.
Computational Linguistics39(4):1025?1066.Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh,Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Li-310blinear: A library for large linear classification.Journal of Machine Learning Research 9:1871?1874.Haji?c, Jan, Massimiliano Ciaramita, Richard Jo-hansson, Daisuke Kawahara, Maria Ant`oniaMart?
?, Llu?
?s M`arquez, Adam Meyers, JoakimNivre, Sebastian Pad?o, Jan?St?ep?anek, PavelStra?n?ak, Mihai Surdeanu, Nianwen Xue, andYi Zhang.
2009.
The CoNLL-2009 shared task:Syntactic and semantic dependencies in multi-ple languages.
In Proceedings of the 13th Con-ference on Computational Natural LanguageLearning (CoNLL-2009), June 4-5.
Boulder,Colorado, USA.Johansson, Richard and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversionfor english.
In Joakim Nivre, Heiki-JaanKalep, Kadri Muischnek, and Mare Koit, edi-tors, NODALIDA 2007 Proceedings.
Universityof Tartu, pages 105?112.Joshi, Aravind K. and Yves Schabes.
1992.
Treeadjoining grammars and lexicalized grammars.In Maurice Nivat and Andreas Podelski, editors,Tree Automata and Languages, North-Holland,Amsterdam, pages 409?432.Keller, Frank.
2010.
Cognitively plausible modelsof human language processing.
In Proceedingsof the 48th Annual Meeting of the Associationfor Computational Linguistics, Companion Vol-ume: Short Papers.
Uppsala, pages 60?67.Lang, Joel and Mirella Lapata.
2014.
Similarity-driven semantic role induction via graph par-titioning.
Computational Linguistics Acceptedpages 1?62.
To appear.Liu, Yudong and Anoop Sarkar.
2007.
Experimen-tal evaluation of LTAG-based features for se-mantic role labeling.
In Proceedings of the 2007Joint Conference on Empirical Methods in Nat-ural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL).Prague, Czech Republic, pages 590?599.Marcus, Mitchell P., Mary Ann Marcinkiewicz,and Beatrice Santorini.
1993.
Building a largeannotated corpus of english: The penn treebank.Computational Linguistics 19(2):313?330.M`arquez, Llu?
?s, Xavier Carreras, Kenneth C.Litkowski, and Suzanne Stevenson.
2008.
Se-mantic Role Labeling: An Introduction tothe Special Issue.
Computational Linguistics34(2):145?159.Mazzei, Alessandro, Vincenzo Lombardo, andPatrick Sturt.
2007.
Dynamic TAG and lexi-cal dependencies.
Research on Language andComputation 5:309?332.Nivre, Joakim, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, Sve-toslav Marinov, and Erwin Marsi.
2007.
Malt-parser: A language-independent system fordata-driven dependency parsing.
Natural Lan-guage Engineering 13:95?135.Pad?o, Ulrike, Matthew W. Crocker, and FrankKeller.
2009.
A probabilistic model of semanticplausibility in sentence processing.
CognitiveScience 33(5):794?838.Palmer, Martha, Daniel Gildea, and Paul Kings-bury.
2005.
The proposition bank: An anno-tated corpus of semantic roles.
ComputationalLinguistics 31(1):71?106.Pickering, Martin J., Matthew J. Traxler, andMatthew W. Crocker.
2000.
Ambiguity reso-lution in sentence processing: Evidence againstfrequency-based accounts.
Journal of Memoryand Language 43(3):447?475.Sangati, Federico and Frank Keller.
2013.
In-cremental tree substitution grammar for pars-ing and word prediction.
Transactions ofthe Association for Computational Linguistics1(May):111?124.Sayeed, Asad and Vera Demberg.
2013.
The se-mantic augmentation of a psycholinguistically-motivated syntactic formalism.
In Proceed-ings of the Fourth Annual Workshop on Cog-nitive Modeling and Computational Linguistics(CMCL).
Association for Computational Lin-guistics, Sofia, Bulgaria, pages 57?65.Surdeanu, Mihai, Richard Johansson, Adam Mey-ers, Llu?
?s M`arquez, and Joakim Nivre.
2008.The CoNLL-2008 shared task on joint pars-ing of syntactic and semantic dependencies.
InProceedings of the 12th Conference on Compu-tational Natural Language Learning (CoNLL-2008).Witten, Ian H. and Timothy C. Bell.
1991.
Thezero-frequency problem: estimating the proba-bilities of novel events in adaptive text compres-sion.
Information Theory, IEEE Transactionson 37(4):1085?1094.Xia, Fei, Martha Palmer, and Aravind Joshi.
2000.A uniform method of grammar extraction andits applications.
In Proceedings of the JointSIGDAT Conference on Empirical Methods in311Natural Language Processing and Very LargeCorpora.
pages 53?62.312
