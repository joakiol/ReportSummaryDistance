Identifying Semantic Roles Using Combinatory Categorial GrammarDaniel Gildea and Julia HockenmaierUniversity of Pennsylvania{dgildea,juliahr}@cis.upenn.eduAbstractWe present a system for automaticallyidentifying PropBank-style semantic rolesbased on the output of a statistical parserfor Combinatory Categorial Grammar.This system performs at least as well asa system based on a traditional Treebankparser, and outperforms it on core argu-ment roles.1 IntroductionCorrectly identifying the semantic roles of sentenceconstituents is a crucial part of interpreting text, andin addition to forming an important part of the infor-mation extraction problem, can serve as an interme-diate step in machine translation or automatic sum-marization.
Even for a single predicate, semanticarguments can have multiple syntactic realizations,as shown by the following paraphrases:(1) John will meet with Mary.John will meet Mary.John and Mary will meet.
(2) The door opened.Mary opened the door.Recently, attention has turned to creating cor-pora annotated with argument structures.
ThePropBank (Kingsbury and Palmer, 2002) and theFrameNet (Baker et al, 1998) projects both doc-ument the variation in syntactic realization of thearguments of predicates in general English text.Gildea and Palmer (2002) developed a system topredict semantic roles (as defined in PropBank) fromsentences and their parse trees as determined by thestatistical parser of Collins (1999).
In this paper, weexamine how the syntactic representations used bydifferent statistical parsers affect the performanceof such a system.
We compare a parser based onCombinatory Categorial Grammar (CCG) (Hocken-maier and Steedman, 2002b) with the Collins parser.As the CCG parser is trained and tested on a cor-pus of CCG derivations that have been obtained byautomatic conversion from the Penn Treebank, weare able to compare performance using both gold-standard and automatic parses for both CCG and thetraditional Treebank representation.
The Treebank-parser returns skeletal phrase-structure trees with-out the traces or functional tags in the original PennTreebank, whereas the CCG parser returns word-word dependencies that correspond to the under-lying predicate-argument structure, including long-range dependencies arising through control, raising,extraction and coordination.2 Predicate-argument relations inPropBankThe Proposition Bank (Kingsbury and Palmer,2002) provides a human-annotated corpus ofsemantic verb-argument relations.
For each verbappearing in the corpus, a set of semantic roles isdefined.
Roles for each verb are simply numberedArg0, Arg1, Arg2, etc.
As an example, the entry-specific roles for the verb offer are given below:Arg0 entity offeringArg1 commodityArg2 priceArg3 benefactive or entity offered toThese roles are then annotated for every instanceof the verb appearing in the corpus, including thefollowing examples:(3) [ARG0 the company] to offer [ARG1 a 15% stake]to [ARG2 the public].
(4) [ARG0 Sotheby?s] ... offered [ARG2 the Dorranceheirs] [ARG1 a money-back guarantee](5) [ARG1 an amendment] offered by [ARG0 Rep.Peter DeFazio](6) [ARG2 Subcontractors] will be offered [ARG1 asettlement]A variety of additional roles are assumedto apply across all verbs.
These secondaryroles can be thought of as being adjuncts,rather than arguments, although no claims aremade as to optionality or other traditional argu-ment/adjunct tests.
The secondary roles include:Location in Tokyo, outsideTime last week, on Tuesday, neverManner easily, dramaticallyDirection south, into the windCause due to pressure from WashingtonDiscourse however, also, on the other handExtent 15%, 289 pointsPurpose to satisfy requirementsNegation not, n?tModal can, might, should, willAdverbial (none of the above)and are represented in PropBank as ?ArgM?
with anadditional function tag, for example ArgM-TMP fortemporal.
We refer to PropBank?s numbered argu-ments as ?core?
arguments.
Core arguments repre-sent 75% of the total labeled roles in the PropBankdata.
Our system predicts all the roles, includingcore arguments as well as the ArgM labels and theirfunction tags.3 Predicate-argument relations in CCGCombinatory Categorial Grammar (CCG) (Steed-man, 2000), is a grammatical theory which providesa completely transparent interface between surfacesyntax and underlying semantics, such that eachsyntactic derivation corresponds directly to an in-terpretable semantic representation which includeslong-range dependencies that arise through control,raising, coordination and extraction.In CCG, words are assigned atomic cate-gories such as NP, or functor categories like(S[dcl]\NP)/NP (transitive declarative verb) or S/S(sentential modifier).
Adjuncts are representedas functor categories such as S/S which expectand return the same type.
We use indices tonumber the arguments of functor categories, eg.
(S[dcl]\NP1)/NP2, or S/S1, and indicate the word-word dependencies in the predicate-argument struc-ture as tuples ?wh, ch, i, wa?, where chis the lexicalcategory of the head word wh, and wais the headword of the constituent that fills the ith argument ofch.Long-range dependencies can be projectedthrough certain types of lexical categories orthrough rules such as coordination of functorcategories.
For example, in the lexical category of arelative pronoun, (NP\NPi)/(S[dcl]/NPi), the headof the NP that is missing from the relative clauseis unified with (as indicated by the indices i) thehead of the NP that is modified by the entire relativeclause.Figure 1 shows the derivations of an ordinarysentence, a relative clause and a right-node-raisingconstruction.
In all three sentences, the predicate-argument relations between London and denied andplans and denied are the same, which in CCG isexpressed by the fact that London fills the first (ie.subject) argument slot of the lexical category of de-nied, (S[dcl]\NP1)/NP2, and plans fills the second(object) slot.
The relations extracted from the CCGderivation for the sentence ?London denied plans onMonday?
are shown in Table 1.The CCG parser returns the local and long-rangeword-word dependencies that express the predicate-argument structure corresponding to the derivation.These relations are recovered with an accuracy ofaround 83% (labeled recovery) or 91% (unlabeledrecovery) (Hockenmaier, 2003).
By contrast, stan-dard Treebank parsers such as (Collins, 1999) onlyreturn phrase-structure trees, from which non-localdependencies are difficult to recover.S[dcl]NP1NLondonS[dcl]\NP1S[dcl]\NP1(S[dcl]\NP1)/NP2deniedNP2Nplans(S\NP)\(S\NP)((S\NP)\(S\NP))/NPonNPNMondayNPNP2NplansNP\NP2(NP\NPi)/(S[dcl]/NPi)thatS[dcl]/NP2S/(S\NP1)NPNLondon(S[dcl]\NP1)/NP2deniedS[dcl]S[dcl]/NP2S[dcl]/NP2S/(S\NP1)NPNLondon(S[dcl]\NP1)/NP2deniedS[dcl]/NP[conj]conjbutS[dcl]/NPS/(S\NP)NPNParis(S[dcl]\NP)/NPadmittedNP2NplansFigure 1: CCG derivation trees for three clauses containing the same predicate-argument relations.whchi wadenied (S[dcl]\NP1)/NP21 Londondenied (S[dcl]\NP1)/NP22 planson ((S\NP1)\(S\NP)2)/NP32 deniedon ((S\NP1)\(S\NP)2)/NP33 MondayTable 1: CCG predicate-argument relations for thesentence ?London denied plans on Monday?The CCG parser has been trained and tested onCCGbank (Hockenmaier and Steedman, 2002a), atreebank of CCG derivations obtained from the PennTreebank, from which we also obtain our trainingdata.4 Mapping between PropBank andCCGbankOur aim is to use CCG derivations as input to a sys-tem for automatically producing the argument labelsof PropBank.
In order to do this, we wish to cor-relate the CCG relations above with PropBank ar-guments.
PropBank argument labels are assignedto nodes in the syntactic trees from the Penn Tree-bank.
While the CCGbank is derived from the PennTreebank, in many cases the constituent structuresdo not correspond.
That is, there may be no con-stituent in the CCG derivation corresponding to thesame sequence of words as a particular constituentin the Treebank tree.
For this reason, we computethe correspondence between the CCG derivation andthe PropBank labels at the level of head words.
Foreach role label for a verb?s argument in PropBank,we first find the head word for its constituent accord-ing to the the head rules of (Collins, 1999).
We thenlook for the label of the CCG relation between thishead word and the verb itself.5 The ExperimentsIn previous work using the PropBank corpus,Gildea and Palmer (2002) developed a system topredict semantic roles from sentences and theirparse trees as determined by the statistical parser ofCollins (1999).
We will briefly review their proba-bility model before adapting the system to incorpo-rate features from the CCG derivations.5.1 The model of Gildea and Palmer (2002)For the Treebank-based system, we use the proba-bility model of Gildea and Palmer (2002).
Proba-bilities of a parse constituent belonging to a givensemantic role are calculated from the following fea-tures:The phrase type feature indicates the syntactictype of the phrase expressing the semantic roles: ex-amples include noun phrase (NP), verb phrase (VP),and clause (S).The parse tree path feature is designed to capturethe syntactic relation of a constituent to the pred-icate.
It is defined as the path from the predicatethrough the parse tree to the constituent in question,represented as a string of parse tree nonterminalslinked by symbols indicating upward or downwardmovement through the tree, as shown in Figure 2.Although the path is composed as a string of sym-bols, our systems will treat the string as an atomicvalue.
The path includes, as the first element of thestring, the part of speech of the predicate, and, as thelast element, the phrase type or syntactic category ofthe sentence constituent marked as an argument.SNP VPNPHe ate some pancakesPRPDT NNVBFigure 2: In this example, the path from the predi-cate ate to the argument NP He can be represented asVB?VP?S?NP, with ?
indicating upward movementin the parse tree and ?
downward movement.The position feature simply indicates whether theconstituent to be labeled occurs before or after thepredicate.
This feature is highly correlated withgrammatical function, since subjects will generallyappear before a verb, and objects after.
This featuremay overcome the shortcomings of reading gram-matical function from the parse tree, as well as errorsin the parser output.The voice feature distinguishes between activeand passive verbs, and is important in predicting se-mantic roles because direct objects of active verbscorrespond to subjects of passive verbs.
An instanceof a verb was considered passive if it is tagged asa past participle (e.g.
taken), unless it occurs as adescendent verb phrase headed by any form of have(e.g.
has taken) without an intervening verb phraseheaded by any form of be (e.g.
has been taken).The head word is a lexical feature, and providesinformation about the semantic type of the role filler.Head words of nodes in the parse tree are determinedusing the same deterministic set of head word rulesused by Collins (1999).The system attempts to predict argument rolesin new data, looking for the highest probabil-ity assignment of roles rito all constituents iin the sentence, given the set of features Fi={pti, pathi, posi, vi, hi} at each constituent in theparse tree, and the predicate p:argmaxr1..nP (r1..n|F1..n, p)We break the probability estimation into twoparts, the first being the probability P (ri|Fi, p) ofa constituent?s role given our five features for theconsituent, and the predicate p. Due to the sparsityof the data, it is not possible to estimate this proba-bility from the counts in the training data.
Instead,probabilities are estimated from various subsets ofthe features, and interpolated as a linear combina-tion of the resulting distributions.
The interpolationis performed over the most specific distributions forwhich data are available, which can be thought of aschoosing the topmost distributions available from abackoff lattice, shown in Figure 3.P(r | h)P(r | h, pt, p)P(r | pt, p)P(r | p)P(r | pt, path, p)P(r | h, p)P(r | pt, pos, v, p)P(r | pt, pos, v)Figure 3: Backoff lattice with more specific distri-butions towards the top.The probabilities P (ri|Fi, p) are combined withthe probabilities P ({r1..n}|p) for a set of roles ap-pearing in a sentence given a predicate, using thefollowing formula:P (r1..n|F1..n, p) ?
P ({r1..n}|p)?iP (ri|Fi, p)P (ri|p)This approach, described in more detail inGildea and Jurafsky (2002), allows interaction be-tween the role assignments for individual con-stituents while making certain independence as-sumptions necessary for efficient probability estima-tion.
In particular, we assume that sets of roles ap-pear independent of their linear order, and that thefeatures F of a constituents are independent of otherconstituents?
features given the constituent?s role.5.2 The model for CCG derivationsIn the CCG version, we replace the features abovewith corresponding features based on both the sen-tence?s CCG derivation tree (shown in Figure 1)and the CCG predicate-argument relations extractedfrom it (shown in Table 1).The parse tree path feature, designed to capturegrammatical relations between constituents, is re-placed with a feature defined as follows: If there isa dependency in the predicate-argument structure ofthe CCG derivation between two words w and w?,the path feature from w to w?
is defined as the lexicalcategory of the functor, the argument slot i occupiedby the argument, plus an arrow (?
or?)
to indicatewhether w or w?
is the categorial functor.
For exam-ple, in our sentence ?London denied plans on Mon-day?, the relation connecting the verb denied withplans is (S[dcl]\NP)/NP.2.
?, with the left arrowindicating the lexical category included in the rela-tion is that of the verb, while the relation connectingdenied with on is ((S\NP)\(S\NP))/NP.2.
?, withthe right arrow indicating the the lexical category in-cluded in the relation is that of the modifier.If the CCG derivation does not define a predicate-argument relation between the two words, we usethe parse tree path feature described above, definedover the CCG derivation tree.
In our training data,77% of PropBank arguments corresponded directlyto a relation in the CCG predicate-argument repre-sentation, and the path feature was used for the re-maining 23%.
Most of these mismatches arise be-cause the CCG parser and PropBank differ in theirdefinition of head words.
For instance, the CCGparser always assumes that the head of a PP isthe preposition, whereas PropBank roles can be as-signed to the entire PP (7), or only to the NP argu-ment of the preposition (8), in which case the headword comes from the NP:(7) ... will be offered [PPARGM-LOC in the U.S].
(8) to offer ...[PP to [NPARG2 the public]].In embedded clauses, CCG assumes that the head isthe complementizer, whereas in PropBank, the headcomes from the embedded sentence itself.
In com-plex verb phrases (eg.
?might not have gone?
), theCCG parser assumes that the first auxiliary (might)is head, whereas PropBank assumes it is the mainverb (gone).
Therefore, CCG assumes that not mod-ifies might, whereas PropBank assumes it modi-fies gone.
Although the head rules of the parsercould in principle be changed to reflect more di-rectly the dependencies in PropBank, we have notattempted to do so yet.
Further mismatches occurbecause the predicate-argument structure returnedby the CCG parser only contains syntactic depen-dencies, whereas the PropBank data also containsome anaphoric dependencies, eg.
:(9) [ARG0 Realist ?s] negotiations to acquireAmmann Laser Technik AG...(10) When properly applied, [ARG0 the adhesive] isdesigned to...Such dependencies also do not correspond to a rela-tion in the predicate-argument structure of the CCGderivation, and cause the path feature to be used.The phrase type feature is replaced with the lex-ical category of the maximal projection of the Prop-Bank argument?s head word in the CCG derivationtree.
For example, the category of plans is N, andthe category of denied is (S[dcl]\NP)/NP.The voice feature can be read off the CCG cate-gories, since the CCG categories of past participlescarry different features in active and passive voice(eg.
sold can be (S[pt]\NP)/NP or S[pss]\NP).The head word of a constituent is indicated in thederivations returned by the CCG parser.SARG0NPNNPLondonVPVBDdeniedARG1NPNNSplansARGM-TMPPPINonNPNNPMondayS[dcl]ARG0NPNLondonS[dcl]\NPS[dcl]\NP(S[dcl]\NP)/NPdeniedARG1NPNplansARGM-TMP(S\NP)\(S\NP)((S\NP)\(S\NP))/NPonNPNMondayFigure 4: A sample sentence as produced by the Treebank parser (left) and by the CCG parser (right).
Nodesare annotated with PropBank roles ARG0, ARG1 and ARGM-TMP.Treebank-based CCG-basedFeatures extracted from Args Precision Recall F-score Precision Recall F-scoreAutomatic parses core 75.9 69.6 72.6 76.1 73.5 74.8all 72.6 61.2 66.4 71.0 63.1 66.8Gold-standard parses core 85.5 81.7 83.5 82.4 78.6 80.4all 78.8 69.9 74.1 76.3 67.8 71.8Gold-standard w/o traces core 77.6 75.2 76.3all 74.4 66.5 70.2Table 2: Accuracy of semantic role prediction5.3 DataWe use data from the November 2002 release ofPropBank.
The dataset contains annotations for72,109 predicate-argument structures with 190,815individual arguments (of which 75% are core, ornumbered, arguments) and has includes examplesfrom 2462 lexical predicates (types).
Annotationsfrom Sections 2 through 21 of the Treebank wereused for training; Section 23 was the test set.
Bothparsers were trained on Sections 2 through 21.6 ResultsBecause of the mismatch between the constituentstructures of CCG and the Treebank, we score bothsystems according to how well they identify the headwords of PropBank?s arguments.
Table 2 gives theperformance of the system on both PropBank?s core,or numbered, arguments, and on all PropBank rolesincluding the adjunct-like ArgM roles.
In order toanalyze the impact of errors in the syntactic parses,we present results using features extracted from bothautomatic parser output and the gold standard parsesin the Penn Treebank (without functional tags) andin CCGbank.
Using the gold standard parses pro-vides an upper bound on the performance of the sys-tem based on automatic parses.
Since the Collinsparser does not provide trace information, its up-per bound is given by the system tested on thegold-standard Treebank representation with tracesremoved.
In Table 2, ?core?
indicates results onPropBank?s numbered arguments (ARG0...ARG5)only, and ?all?
includes numbered arguments as wellas the ArgM roles.
Most of the numbered argu-ments (in particular ARG0 and ARG1) correspondto arguments that the CCG category of the verb di-rectly subcategorizes for.
The CCG-based systemoutperforms the system based on the Collins parseron these core arguments, and has comparable perfor-mance when all PropBank labels are considered.
Webelieve that the superior performance of the CCGsystem on this core arguments is due to its ability torecover long-distance dependencies, whereas we at-tribute its lower performance on non-core argumentsmainly to the mismatches between PropBank andCCGbank.The importance of long-range dependencies forour task is indicated by the fact that the performanceon the Penn Treebank gold standard without tracesTreebank-based CCG-basedScoring Precision Recall F-score Precision Recall F-scoreAutomatic parses Head word 72.6 61.2 66.4 71.0 63.1 66.8Boundary 68.6 57.8 62.7 55.7 49.5 52.4Gold-standard parses Head word 77.6 75.2 76.3 76.3 67.8 71.8(Treebank: w/o traces) Boundary 74.4 66.5 70.2 67.5 60.0 63.5Table 3: Comparison of scoring regimes, using automatic parser output and gold standard parses.
The firstrow in this table corresponds to the second row in Table 2.is significantly lower than that on the Penn Treebankwith trace information.
Long-range dependenciesare especially important for core arguments, shownby the fact that removing trace information from theTreebank parses results in a bigger drop for corearguments (83.5 to 76.3 F-score) than for all roles(74.1 to 70.2).
The ability of the CCG parser to re-cover these long-range dependencies accounts for itshigher performance, and in particular its higher re-call, on core arguments.The CCG gold standard performance is belowthat of the Penn Treebank gold standard with traces.We believe this performance gap to be caused bythe mismatches between the CCG analyses and thePropBank annotations described in Section 5.2.
Forthe reasons described, the head words of the con-stituents that have PropBank roles are not necessar-ily the head words that stand in a predicate-argumentrelation in CCGbank.
If two words do not stand in apredicate-argument relation, the CCG system takesrecourse to the path feature.
This feature is muchsparser in CCG: since CCG categories encode sub-categorization information, the number of categoriesin CCGbank is much larger than that of Penn Tree-bank labels.
Analysis of our system?s output showsthat the system trained on the Penn Treebank goldstandard obtains 55.5% recall on those relations thatrequire the CCG path feature, whereas the systemusing CCGbank only achieves 36.9% recall on these.Also, in CCG, the complement-adjunct distinctionis represented in the categories for the complement(eg.
PP) or adjunct (eg.
(S\NP)\(S\NP) and inthe categories for the head (eg.
(S[dcl]\NP)/PPor S[dcl]\NP).
In generating the CCGbank, variousheuristics were used to make this distinction.
In par-ticular, for PPs, it depends on the ?closely-related?
(CLR) function tag, which is known to be unreli-able.
The decisions made in deriving the CCGbankoften do not match the hand-annotated complement-adjunct distinctions in PropBank, and this inconsis-tency is likely to make our CCGbank-based featuresless predictive.
A possible solution is to regeneratethe CCGbank using the Propbank annotations.The impact of our head-word based scoring is an-alyzed in Table 3, which compares results when onlythe head word must be correctly identified (as in Ta-ble 2) and to results when both the beginning andend of the argument must be correctly identified inthe sentence (as in Gildea and Palmer (2002)).
Evenif the head word is given the correct label, the bound-aries of the entire argument may be different fromthose given in the PropBank annotation.
Since con-stituents in CCGbank do not always match those inPropBank, even the CCG gold standard parses ob-tain comparatively low scores according to this met-ric.
This is exacerbated when automatic parses areconsidered.7 ConclusionOur CCG-based system for automatically labelingverb arguments with PropBank-style semantic rolesoutperforms a system using a traditional Treebank-based parser for core arguments, which comprise75% of the role labels, but scores lower on adjunct-like roles such as temporals and locatives.
The CCGparser returns predicate-argument structures that in-clude long-range dependencies; therefore, it seemsinherently better suited for this task.
However, theperformance of our CCG system is lowered by thefact that the syntactic analyses in its training corpusdiffer from those that underlie PropBank in impor-tant ways (in particular in the notion of heads and thecomplement-adjunct distinction).
We would expecta higher performance for the CCG-based system ifthe analyses in CCGbank resembled more closelythose in PropBank.Our results also indicate the importance of recov-ering long-range dependencies, either through thetrace information in the Penn Treebank, or directly,as in the predicate-argument structures returned bythe CCG parser.
We speculate that much of theperformance improvement we show could be ob-tained with traditional (ie.
non-CCG-based) parsersif they were designed to recover more of the infor-mation present in the Penn Treebank, in particularthe trace co-indexation.
An interesting experimentwould be the application of our role-labeling sys-tem to the output of the trace recovery system ofJohnson (2002).
Our results also have implicationsfor parser evaluation, as the most frequently usedconstituent-based precision and recall measures donot evaluate how well long-range dependencies canbe recovered from the output of a parser.
Measuresbased on dependencies, such as those of Lin (1995)and Carroll et al (1998), are likely to be more rele-vant to real-world applications of parsing.Acknowledgments This work was supported by the In-stitute for Research in Cognitive Science at the University ofPennsylvania, the Propbank project (DoD Grant MDA904-00C-2136), an EPSRC studentship and grant GR/M96889, and NSFITR grant 0205 456.
We thank Mark Steedman, Martha Palmerand Alexandra Kinyon for their comments on this work.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of COLING/ACL, pages 86?90, Montreal.John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998.Parser evaluation: a survey and a new proposal.
InProceedings of the 1st International Conference onLanguage Resources and Evaluation, pages 447?454,Granada, Spain.Michael John Collins.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Daniel Gildea and Martha Palmer.
2002.
The necessityof syntactic parsing for predicate argument recogni-tion.
In Proceedings of the 40th Annual Conference ofthe Association for Computational Linguistics (ACL-02), Philadelphia, PA.Julia Hockenmaier and Mark Steedman.
2002a.
Acquir-ing Compact Lexicalized Grammars from a CleanerTreebank.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC-2002), pages 1974?1981, Las Palmas.Julia Hockenmaier and Mark Steedman.
2002b.
Gener-ative models for statistical parsing with CombinatoryCategorial Grammar.
In Proceedings of the 40th An-nual Conference of the Association for ComputationalLinguistics (ACL-02), Philadelphia, PA.Julia Hockenmaier.
2003.
Data and models for statis-tical parsing with Combinatory Categorial Grammar.Ph.D.
thesis, School of Informatics, University of Ed-inburgh.Mark Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proceedings of the 40th Annual Confer-ence of the Association for Computational Linguistics(ACL-02), Philadelphia, PA.Paul Kingsbury and Martha Palmer.
2002.
From Tree-bank to PropBank.
In Proceedings of the 3rd Interna-tional Conference on Language Resources and Evalu-ation (LREC-2002), Las Palmas.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proceedings ofthe 19th International Joint Conference on ArtificialIntelligence (IJCAI-95), pages 1420?1425, Montreal.Mark Steedman.
2000.
The Syntactic Process.
The MITPress, Cambridge Mass.
