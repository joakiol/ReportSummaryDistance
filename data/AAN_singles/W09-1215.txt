Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 97?102,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsParsing Syntactic and Semantic Dependencies for Multiple Languageswith A Pipeline ApproachHan Ren, Donghong Ji Jing Wan, Mingyao ZhangSchool of Computer Science Center for Study of Language & InformationWuhan University Wuhan UniversityWuhan 430079, China Wuhan 430079, Chinacslotus@mail.whu.edu.cndonghong_ji@yahoo.com{jennifer.wanj, my.zhang}@gmail.comAbstractThis paper describes a pipelined approach forCoNLL-09 shared task on joint learning ofsyntactic and semantic dependencies.
In thesystem, we handle syntactic dependency pars-ing with a transition-based approach and util-ize MaltParser as the base model.
For SRL,we utilize a Maximum Entropy model to iden-tify predicate senses and classify arguments.Experimental results show that the averageperformance of our system for all languagesachieves 67.81% of macro F1 Score, 78.01%of syntactic accuracy, 56.69% of semantic la-beled F1, 71.66% of macro precision and64.66% of micro recall.1 IntroductionGiven a sentence with corresponding part-of-speech for each word, the task of syntactic and se-mantic dependency parsing contains two folds: (1)identifying the syntactic head of each word andassigning the dependency relationship between theword and its head; (2) identifying predicates withproper senses and labeling semantic dependenciesfor them.For data-driven syntactic dependency parsing,many approaches are based on supervised learningusing treebank or annotated datasets.
Currently,graph-based and transition-based algorithms aretwo dominating approaches that are employed bymany researchers, especially in previous CoNLLshared tasks.
Graph-based algorithms (Eisner,1996; McDonald et al, 2005) assume a series ofdependency tree candidates for a sentence and thegoal is to find the dependency tree with highestscore.
Transition-based algorithms (Yamada andMatsumoto, 2003; Nivre et al, 2004) utilize transi-tion histories learned from dependencies withinsentences to predict next state transition and buildthe optimal transition sequence.
Although differentstrategies were considered, two approaches yieldedcomparable results at previous tasks.Semantic role labeling contains two problems:identification and labeling.
Identification is a bi-nary classification problem, and the goal is to iden-tify annotated units in a sentence; while labeling isa multi-class classification problem, which is toassign arguments with appropriate semantic roles.Hacioglu (2004) utilized predicate-argument struc-ture and map dependency relations to semanticroles.
Liu et al (2005) combined two problemsinto a classification one, avoiding some annotatedunits being excluded due to some incorrect identi-fication results.
In addition, various features arealso selected to improve accuracy of SRL.In this paper, we propose a pipelined approachfor CoNLL-09 shared task on joint learning of syn-tactic and semantic dependencies, and describe oursystem that can handle multiple languages.
In thesystem, we handle syntactic dependency parsingwith a transition-based approach.
For SRL, we util-ize Maximum Entropy model to identify predicatesenses and classify arguments.The remain of the paper is organized as follows.In Section 2, we discuss the processing mechanismcontaining syntactic and semantic dependencyparsing of our system in detail.
In Section 3, wegive the evaluation results and analysis.
Finally,the conclusion and future work are given in Sec-tion 4.972 System DescriptionThe system, which is a two-stage pipeline, proc-esses syntactic and semantic dependencies respec-tively.
To reduce the difficulties in SRL, predicatesof each sentence in all training and evaluation dataare labeled, thus predicate identification can beignored.Figure 1.
System ArchitecturesFor syntactic dependencies, we employ a state-of-the-art dependency parser and basic plus ex-tended features for parsing.
For semantic depend-encies, a Maximum Entropy Model is used both inpredicate sense identification and semantic rolelabeling.
Following subsections will show compo-nents of our system in detail.2.1 Syntactic Dependency ParsingIn the system, MaltParser1 is employed for syntac-tic dependency parsing.
MaltParser is a data-drivendeterministic dependency parser, based on a Sup-port Vector Machine classifier.
An extensive re-search (Nivre, 2007) parsing with 9 differentlanguages shows that the parser is language-independent and yields good results.MaltParser supports two kinds of parsing algo-rithms: Nivre?s algorithms and Covington?s incre-mental algorithms.
Nivre?s algorithms, which aredeterministic algorithms consisting of a series ofshift-reduce procedures, defines four operations:?Right.
For a given triple <t|S, n|I, A>, Srepresents STACK and I represents INPUT.
Ifdependency relation t ?
n exists, it will be1 http://w3.msi.vxu.se/~jha/maltparser/pendency relation t?n exists, it will be appendedinto A and t will be removed from S.?Left.
For a given triple <t|S, n|I, A>, if de-pendency relation n?t exists, it will be appendedinto A and n will be pushed into S.?Reduce.
If dependency relation n?t does notexist, and the parent node of t exists left to it, t willbe removed from S.?Shift.
If none of the above satisfies, n will bepushed into S.The deterministic algorithm simplifies determi-nation for Reduce operation.
As a matter of fact,some languages, such as Chinese, have more flexi-ble word order, and some words have a long dis-tance with their children.
In this case, t should notbe removed from S, but be handled with Shift op-eration.
Otherwise, dependency relations between tand its children will never be identified, thus se-quential errors of dependency relations may occurafter the Reduce operation.For syntactic dependencies with long distance,an improved Reduce strategy is: if the dependencyrelation between n and t does not exist, and theparent node of t exists left to it and the dependencyrelation between the parent node and n, t will beremoved from S. The Reduce operation is projec-tive, since it doesn?t influence the following pars-ing procedures.
The Improved algorithm isdescribed as follows:(1) one of the four operations is performed ac-cording to the dependency relation between t and nuntil EOS; if only one token remains in S, go to (3).
(2) continue to select operations for remainingtokens in S; when Shift procedure is performed,push t to S; if only one token remains in S and Icontains more tokens than only EOS, goto (1).
(3) label all odd tokens in S as ROOT, pointingto EOS.We also utilize history-based feature modelsimplemented in the parser to predict the next actionin the deterministic derivation of a dependencystructure.
The parser provides some default fea-tures that is general for most languages: (1) part-of-speech features of TOP and NEXT and follow-ing 3 tokens; (2) dependency features of TOP con-taining leftmost and rightmost dependents, and ofNEXT containing leftmost dependents; (3) Lexical98features of TOP, head of TOP, NEXT and follow-ing one token.
We also extend features for multiplelanguages: (1) count of part-of-speech features offollowing tokens extend to 5; (2) part-of-speechand dependent features of head of TOP.2.2 Semantic Dependency ParsingEach defacto predicate in training and evaluationdata of CoNLL09 is labeled with a sign ?Y?, whichsimplifies the work of semantic dependency pars-ing.
In our system, semantic dependency parsing isa pipeline that contains two parts: predicate senseidentification and semantic role labeling.
Forpredicate sense identification, each predicate isassigned a certain sense number.
For semantic rolelabeling, local and global features are selected.Features of each part are trained by a classificationalgorithm.
Both parts employ a Maximum EntropyTool MaxEnt in a free package OpenNLP 2 as aclassifier.2.2.1  Predicate Sense IdentificationThe goal of predicate sense identification is to de-cide the correct frame for a predicate.
According toPropBank (Palmer, et al, 2005), predicates containone or more rolesets corresponding to differentsenses.
In our system, a classifier is employed toidentify each predicate?s sense.Suppose { }01, 02, , LC = ?
Ns tis the sense set(NL is the count of categories corresponding to thelanguage L, eg., in Chinese training set NL = 10since predicates have at most 10 senses in the set),and ti is the ith sense of word w in sentence s. Themodel is implemented to assign each predicate tothe most probatilistic sense.
( | , )i C it P w?=argmax                (1)Features for predicate sense identification arelisted as follows:?WORD, LEMMA, DEPREL: The lexicalform and lemma of the predicate; the dependencyrelation between the predicate and its head; forChinese and Japanese, WORD is ignored.?
HEAD_WORD, HEAD_POS: The lexicalform and part-of-speech of the head of the predi-cate.2 http://maxent.sourceforge.net/?
CHILD_WORD_SET, CHILD_POS_SET,CHILD_DEP_SET: The lexical form, part-of-speech and dependency relation of dependents ofthe predicate.
?LSIB_WORD, LSIB_POS, LSIB_DEPREL,RSIB_WORD, RSIB_POS, RSIB_DEPREL: Thelexical form, part-of-speech and dependency rela-tion of the left and right sibling token of the predi-cate.
Features of sibling tokens are adopted,because senses of some predicates can be inferredfrom its left or right sibling.For English data set, we handle verbal andnominal predicates respectively; for other lan-guages, we handle all predicates with one classifier.If a predicate in the evaluation data does not existin the training data, it is assigned the most frequentsense label in the training data.2.2.2  Semantic Role LabelingSemantic role labeling task contains two parts: ar-gument identification and argument classification.In our system the two parts are combined as oneclassification task.
Our reason is that those argu-ment candidates that potentially become semanticroles of corresponding predicates should not bepruned by incorrect argument identification.
In oursystem, a predicate-argument pair consists of anytoken (except predicates) and any predicate in asentence.
However, we find that argument classifi-cation is a time-consuming procedure in the ex-periment because the classifier spends much timeon a great many of invalid predicate-argumentpairs.
To reduce useless computing, we add a sim-ple pruning method based on heuristic rules to re-move invalid pairs, such as punctuations and somefunctional words.Features used in our system are based on (Ha-cioglu, 2004) and (Pradhan et al 2005), and de-scribed as follows:?WORD, LEMMA, DEPREL: The same withthose mentioned in section 2.2.1.?VOICE: For verbs, the feature is Active orPassive; for nouns, it is null.
?POSITION: The word?s position correspond-ing to its predicate: Left, Right or Self.
?PRED: The lemma plus sense of the word.
?PRED_POS: The part-of-speech of the predi-cate.99?LEFTM_WORD, LEFTM_POS, RIGHTM_WORD, RIGHTM_POS: Leftmost and rightmostword and their part-of-speech of the word.?
POS_PATH: All part-of-speech from theword to its predicate, including Up, Down, Leftand Right, eg.
?NN?VV?CC?VV?.
?DEPREL_PATH: Dependency relations fromthe word to its predicate, eg.
?COMP?RELC?COMP??.
?ANC_POS_PATH, ANC_DEPREL_PATH:Similar to POS_PATH and DEPREL_PATH, part-of-speech and dependency relations from the wordto the common ancestor with its predicate.
?PATH_LEN: Count of passing words fromthe word to its predicate.?
FAMILY: Relationship between the wordand its predicate, including Child, Parent, Descen-dant, Ancestor, Sibling, Self and Null.?
PRED_CHD_POS, PRED_CHD_DEPREL:Part-of-speech and dependency relations of allchildren of the word?s predicate.For different languages, some features men-tioned above are invalid and should be removed,and some extended features could improve the per-formance of the classifier.
In our system we mainlyfocus on Chinese, therefore, WORD and VOICEshould be removed when processing Chinese dataset.
We also adopt some features proposed by (Xue,2008):?
POS_PATH_BA, POS_PATH_SB, POS_PATH_LB: BA and BEI are functional words thatimpact the order of arguments.
In PropBank, BAwords have the POS tag BA, and BEI words havetwo POS tags: SB (short BEI) and LB (long BEI).3 Experimental ResultsOur experiments are based on a PC with a IntelCore 2 Duo 2.1G CPU and 2G memory.
Trainingand evaluation data (Taul?
et al, 2008; Xue et al,2008; Haji?
et al, 2006; Palmer et al, 2002; Bur-chardt et al, 2006; Kawahara et al, 2002) havebeen converted to a uniform CoNLL Shared Taskformat.
In all experiments, SVM and ME modelare trained using training data, and tested withdevelopment data of all languages.The system for closed challenge is designed astwo parts.
For syntactic dependency training andparsing, we utilize the projective model in Malt-Parser for data sets.
We also follow default settingsin MaltParser, such as assigned parameters forLIBSVM and combined prediction strategy, andutilize improved approaches mentioned in section2.
For semantic dependency training and parsing,we choose the count of iteration as 100 and cutoffvalue as 10 for the ME model.
Table 1 shows thetraining time for syntactic and semantic depend-ency of all languages.
Parsing time for syntactic isnot more than 30 minutes, and for semantic is notmore than 5 minutes of each language.syn prd semEnglish 7h 12min 47minChinese 8h 18min 61minJapanese 7h 14min 46minCzech 13h 46min 77minGerman 6h 16min 54minSpanish 6h 15min 55minCatalan 6h 15min 50minTable 1.
Training cost for all languages.
syn, prd andsem mean training time for syntactic dependency, predi-cate identification and semantic dependency.3.1 Syntactic Dependency ParsingWe utilize MaltParser with improved algorithmsmentioned in section 2.1 for syntactic dependencyparsing, and the results are shown in Table 2.LAS UAS label-acc.English 87.57 89.98 92.19Chinese 79.17 81.22 85.94Japanese 91.47 92.57 97.28Czech 57.30 75.66 65.39German 76.63 80.31 85.97Spanish 76.11 84.40 84.69Catalan 77.84 86.41 85.78Table 2.
Performance of syntactic dependency parsingTable 2 indicates that parsing for Japanese andEnglish data sets has a better performance thanother languages, partly because determinative algo-rithm and history-based grammar are more suitedfor these two languages.
To compare the perform-ance of our approach of improved deterministicalgorithm and extended features, we make anotherexperiment that utilize original arc-standard algo-rithm and base features for syntactic experiments.Due to time limitation, the experiments are onlybased on Chinese training and evaluation data.
Theresults show that LAS and UAS drops about 2.7%and 2.2% for arc-standard algorithm, 1.6% and1.2% for base features.
They indicate that our de-100terministic algorithm and the extend features canhelp to improve syntactic dependency parsing.
Wealso notice that the results of Czech achieve alower performance than other languages.
It mainlybecause the language has more rich morphology,usually accompanied by more flexible word order.Although using a large training set, linguistic prop-erties greatly influence the parsing result.
In addi-tion, extended features are not suited for thislanguage and the feature model should be opti-mized individually.For all of the experiments we mainly focus onthe language of Chinese.
When parsing Chinesedata sets we find that the focus words where mostof the errors occur are almost punctuations, such ascommas and full stops.
Apart from errors of punc-tuations, most errors occur on prepositions such asthe Chinese word ?at?.
Most of these problemscome from assigning the incorrect dependencies,and the reason is that the parsing algorithm con-cerns the form rather than the function of thesewords.
In addition, the prediction of dependencyrelation ROOT achieves lower precision and recallthan others, indicating that MaltParser overpredictsdependencies to the root.3.2 Semantic Dependency ParsingMaxEnt is employed as our classifier to train andparse semantic dependencies, and the results areshown in Table 3, in which all criterions are la-beled.P R F1English 76.57 60.45 67.56Chinese 75.45 69.92 72.58Japanese 91.93 43.15 58.73Czech 68.83 57.78 62.82German 62.96 47.75 54.31Spanish 40.11 39.50 39.80Catalan 41.34 40.66 41.00Table 3.
Performance of semantic dependency parsingAs shown in Table 3, the scores of the latterfive languages are quite lower than those of theformer two languages, and the main reason couldbe inferred from the scores of Table 2 that the dropof the performance of semantic dependency pars-ing comes from the low performance of syntacticdependency parsing.
Another reason is that, mor-phological features are not be utilized in the classi-fier.
Our post experiments after submission showthat average performance could improve the per-formance after adding morphological and somecombined features.
In addition, difference betweenprecision and recall indicates that the classificationprocedure works better than the identificationprocedure in semantic role labeling.For Chinese, semantic role of some words withpart-of-speech VE have been mislabeled.
It?smainly because that these words in Chinese havemultiple part-of-speech.
The errors of POS andPRED greatly influence the system to performthese words.
Another main problem occurs on thepairs NN + A0/A1.
Identification of the two pairsare much lower than VA/VC/VE/VV + A0/A1pairs.
The reason is that the identification of nomi-nal predicates have more errors than that of verbalpredicates due to the combination of SRL for thesetwo kinds of predicates.
For further study, verbalpredicates and nominal predicates should be han-dled respectively so that the overall performancecan be improved.3.3 Overall PerformanceThe average performance of our system for all lan-guages achieves 67.81% of macro F1 Score,78.01% of syntactic accuracy, 56.69% of semanticlabeled F1, 71.66% of macro precision and 64.66%of micro recall.4 ConclusionIn this paper, we propose a pipelined approach forCoNLL-09 shared task on joint learning of syntac-tic and semantic dependencies, and describe oursystem that can handle multiple languages.
Oursystem focuses on improving the performance ofsyntactic and semantic dependency respectively.Experimental results show that the overall per-formance can be improved for multiple languagesby long distance dependency algorithm and ex-tended history-based features.
Besides, the systemfits for verbal predicates than nominal predicatesand the classification procedure works better thanidentification procedure in semantic role labeling.For further study, respective process should behandled between these two kinds of predicates, andargument identification should be improved byusing more discriminative features for a betteroverall performance.101AcknowledgmentsThis work is supported by the Natural ScienceFoundation of China under Grant Nos.60773011,90820005, and Independent Research Foundationof Wuhan University.ReferencesAljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pad?
and Manfred Pinkal.
2006.The SALSA Corpus: a German Corpus Resource forLexical Semantics.
Proceedings of the 5th Interna-tional Conference on Language Resources andEvaluation (LREC-2006).
Genoa, Italy.Jason M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proceed-ings of the 16th International Conference on Compu-tational Linguistics (COLING), pp.340?345.Kadri Hacioglu.
2004.
Semantic Role Labeling UsingDependency Trees.
In Proceedings of the Interna-tional Conference on Computational Linguistics(COLING).Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall,Petr Pajas, Jan ?t?p?nek, Ji??
Havelka, Marie Mikulo-v?
and Zden?k ?abokrtsk?.
2006.
The Prague De-pendency Treebank 2.0.
CD-ROM.
Linguistic DataConsortium, Philadelphia, Pennsylvania, USA.
ISBN1-58563-370-4.
LDC Cat.
No.
LDC2006T01.
URL:http://ldc.upenn.edu.Jan Haji?, Massimiliano Ciaramita, Richard Johansson,Daisuke Kawahara, Maria Antonia Mart?, Llu?sM?rquez, Adam Meyers, Joakim Nivre, SebastianPad?, Jan ?t?p?nek, Pavel Stra?
?k, Mihai Surdeanu,Nianwen Xue and Yi Zhang.
2009.
The CoNLL 2009Shared Task: Syntactic and Semantic Dependenciesin Multiple Languages.
Proceedings of the 13thConference on Computational Natural LanguageLearning (CoNLL-2009).
Boulder, Colorado, USA.June 4-5. pp.3-22.Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida.2002.
Construction of a Japanese Relevance-taggedCorpus.
Proceedings of the 3rd International Confer-ence on Language Resources and Evaluation (LREC-2002).
Las Palmas, Spain.
pp.2008-2013.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics (ACL),pp.91?98.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In Proceedingsof the 8th Conference on Computational Natural Lan-guage Learning (CoNLL), pp.49?56.Joakim Nivre.
2004.
Incrementality in DeterministicDependency Parsing.
In Incremental Parsing: Bring-ing Engineering and Cognition Together.
Workshopat ACL-2004, Barcelona, Spain, pp.50-57.Joakim Nivre and Johan Hall.
2005.
MaltParser: A lan-guage-independent system for data-driven depend-ency parsing.
In Proceedings of the Fourth Workshopon Treebanks and Linguistic Theories (TLT).Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gulsen Eryigit, Sandra Kubler, Svetoslav Marinovand Erwin Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural language Engineering, Volume 13, Is-sue 02, pp.95-135.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin and Daniel Jurafsky.2005.
Support Vector Learning for Semantic Argu-ment classification.
Machine Learning Journal, 2005,60(3): 11?39.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?s M?rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 Shared Task on Joint Parsing of Syn-tactic and Semantic Dependencies.
In Proceedings ofthe 12th Conference on Computational Natural Lan-guage Learning (CoNLL-2008).Mariona Taul?, Maria Ant?nia Mart?
and Marta Reca-sens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
Proceedings of the 6th In-ternational Conference on Language Resources andEvaluation (LREC-2008).
Marrakech, Morocco.Liu Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, andHuaijun Liu.
2005.
Semantic role labeling system us-ing maximum entropy classifier.
In Proceedings ofthe 8th Conference on Computational Natural Lan-guage Learning (CoNLL).Nianwen Xue.
2008.
Labeling Chinese Predicates withSemantic roles.
Computational Linguistics, 34(2):225-255.Nianwen Xue and Martha Palmer.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143-172.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisti-cal dependency analysis with support vector ma-chines.
In Proceedings of the 8th InternationalWorkshop on Parsing Technologies (IWPT), pp.195?206.102
