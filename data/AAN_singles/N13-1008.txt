Proceedings of NAACL-HLT 2013, pages 74?84,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsRelation Extraction with Matrix Factorization and Universal SchemasSebastian RiedelDepartment of Computer ScienceUniversity College Londons.riedel@ucl.ac.ukLimin Yao, Andrew McCallum, Benjamin M. MarlinDepartment of Computer ScienceUniversity of Massachusetts at Amherst{lmyao,mccallum,marlin}@cs.umass.eduAbstractTraditional relation extraction predicts rela-tions within some fixed and finite targetschema.
Machine learning approaches to thistask require either manual annotation or, inthe case of distant supervision, existing struc-tured sources of the same schema.
The needfor existing datasets can be avoided by us-ing a universal schema: the union of all in-volved schemas (surface form predicates as inOpenIE, and relations in the schemas of pre-existing databases).
This schema has an al-most unlimited set of relations (due to surfaceforms), and supports integration with existingstructured data (through the relation types ofexisting databases).
To populate a database ofsuch schema we present matrix factorizationmodels that learn latent feature vectors for en-tity tuples and relations.
We show that suchlatent models achieve substantially higher ac-curacy than a traditional classification ap-proach.
More importantly, by operating simul-taneously on relations observed in text and inpre-existing structured DBs such as Freebase,we are able to reason about unstructured andstructured data in mutually-supporting ways.By doing so our approach outperforms state-of-the-art distant supervision.1 IntroductionMost previous work in relation extraction uses a pre-defined, finite and fixed schema of relation types(such as born-in or employed-by).
Usually some tex-tual data is labeled according to this schema, andthis labeling is then used in supervised training ofan automated relation extractor, e.g.
Culotta andSorensen (2004).
However, labeling textual rela-tions is time-consuming and difficult, leading to sig-nificant recent interest in distantly-supervised learn-ing.
Here one aligns existing database records withthe sentences in which these records have been ?ren-dered??
?effectively labeling the text?and from thislabeling we can train a machine learning system asbefore (Craven and Kumlien, 1999; Mintz et al2009; Bunescu and Mooney, 2007; Riedel et al2010).
However, this method relies on the availabil-ity of a large database that has the desired schema.The need for pre-existing datasets can be avoidedby using language itself as the source of the schema.This is the approach taken by OpenIE (Etzioni et al2008).
Here surface patterns between mentions ofconcepts serve as relations.
This approach requiresno supervision and has tremendous flexibility, butlacks the ability to generalize.
For example, Ope-nIE may find FERGUSON?historian-at?HARVARDbut does not know FERGUSON?is-a-professor-at?HARVARD.
OpenIE has traditionally relied on alarge diversity of textual expressions to provide goodcoverage.
But this diversity is not always available,and, in any case, the lack of generalization greatlyinhibits the ability to support reasoning.One way to gain generalization is to cluster tex-tual surface forms that have similar meaning (Linand Pantel, 2001; Pantel et al 2007; Yates andEtzioni, 2009; Yao et al 2011).
While the clus-ters discovered by all these methods usually containsemantically related items, closer inspection invari-ably shows that they do not provide reliable impli-cature.
For example, a typical representative clus-ter may include historian-at, professor-at, scientist-at, worked-at.
Although these relation types are in-deed semantically related, note that scientist-at doesnot necessarily imply professor-at, and worked-at74certainly does not imply scientist-at.
In fact, wecontend that any relational schema would inherentlybe brittle and ill-defined?
?having ambiguities, prob-lematic boundary cases, and incompleteness.1 Forexample, Freebase, in spite of its extensive effort to-wards high coverage, has no critized nor scientist-atrelation.In response to this problem, we present a new ap-proach: implicature with universal schemas.
Herewe embrace the diversity and ambiguity of originalinputs; we avoid forcing textual meaning into pre-defined boxes.
This is accomplished by definingour schema to be the union of all source schemas:original input forms, e.g.
variants of surface pat-terns similarly to OpenIE, as well as relations inthe schemas of many available pre-existing struc-tured databases.
But then, unlike OpenIE, our fo-cus lies on learning asymmetric implicature amongrelations.
This allows us to probabilistically ?fillin?
inferred unobserved entity-entity relations inthis union.
For example, after observing FERGU-SON?historian-at?HARVARD our system infers thatFERGUSON?professor-at?HARVARD, but not viceversa.At the heart of our approach is the hypothesis thatwe should concentrate on predicting source data?
?arelatively well defined task that can be evaluated andoptimized?
?as opposed to modeling semantic equiv-alence, which we believe will always be illusive.Note that by operating simultaneously on rela-tions observed in text and in pre-existing structureddatabases such as Freebase, we are able to reasonabout unstructured and structured data in mutually-supporting ways.
For example, we can predict sur-face pattern relations that effectively serve as addi-tional features when predicting Freebase relations,hence improving generalization.
Also notice thatusers of our system will not have to study and un-derstand the complexities of a particular schema inorder to issue queries; they can ask in whatever formnaturally occurs to them, and our system will likelyalready have that relation in our universal schema.Our technical approach is based on extensionsto probabilistic models of matrix factorization and1At NAACL 2012 Lucy Vanderwende asked ?Where do therelation types come from??
There was no satisfying answer.
Atthe same meeting, and in line with Brachman (1983), Ed Hovystated ?We don?t even know what is-a means.
?collaborative filtering (Collins et al 2001; Koren,2008; Rendle et al 2009).
We represent the prob-abilistic knowledge base as a matrix with entity-entity pairs in the rows and relations in the columns(see figure 1).
The rows come from running cross-document entity resolution across pre-existing struc-tured databases and textual corpora.
The columnscome from the union of surface forms and DB rela-tions.
We present a series of models that learn lowerdimensional manifolds for tuples, relations and enti-ties, and a set of weights that capture direct correla-tions between relations.
Weights and lower dimen-sional representations act, through dot products, asthe natural parameters of a single log-linear modelto derive per-cell probabilities.In experiments we show that our models can ac-curately predict surface patterns relationships whichdo not appear explicitly in text, and that learning la-tent representations of entities, tuples and relationssubstantially improves results over a traditional clas-sifier approach.
Moreover, we can improve accu-racy by simultaneously operating on relations ob-served in the New York Times corpus and in Free-base.
In particular, our model outperforms the cur-rent state-of-the-art distant supervision method (Sur-deanu et al 2012) by 10% points Mean AveragePrecision through joint implicature among surfacepatterns and Freebase relations.2 ModelBefore we present our approach in more detail, webriefly introduce some notation.
We use R to de-note the set of relations we seek to predict (such asworks-written in Freebase, or the X?historian-at?Ypattern), and T to denote the set of input tuples.
Forsimplicity we assume each relation to be binary, al-though our approach can be easily generalized to then-ary case.
Given a relation r ?
R and a tuple t ?
Tthe pair ?r, t?
is a fact, or relation instance.
The in-put to our model is a set of observed facts O, andthe observed facts for a given tuple is denoted byOt := {?r, t?
?
O}.Our goal is a model that can estimate, for agiven relation r (such as X?historian-at?Y) and agiven tuple t (such as <FERGUSON,HARVARD>),the probability p (yr,t = 1) where yr,t is a binaryrandom variable that is true iff t is in relation r. We75Train0.95TestSurface Patterns KB RelationsX-professor-at-Y110.05X-historian-at-Y employee(X,Y) member(X,Y)1 1110.97Rel.
Extraction1 0.930.97Cluster AlignReasoning with Universal SchemaFerguson,HarvardOman,OxfordFirth,OxfordG?del,Princeton0.95Figure 1: Filling up a database of universal schema.Dark circles are observed facts, shaded circles are in-ferred facts.
Relation Extraction (RE) maps surface pat-tern relations (and other features) to structured relations.Surface form clustering models correlations between pat-terns, and can be fed into RE (Yao et al 2011).
Databasealignment and integration models correlations betweenstructured relations (not done in this work).
Reasoningwith the universal schema incorporates these tasks in ajoint fashion.introduce a series of exponential family models thatestimate this probability using a natural parameter?r,t and the logistic function:p (yr,t = 1|?r,t) := ?
(?r,t) =11 + exp (?
?r,t).We will first describe our models through differ-ent definitions of the natural parameter ?r,t.
In eachcase ?r,t will be a function of r, t and a set of weightsand/or latent feature vectors.
In section 2.5 we willthen show how these weights and vectors can be es-timated based on the observed facts O.Notice that we can interpret p (yr,t = 1) as theprobability that a customer t likes product r. Thisanalogy allows us to draw from a large body of workin collaborative filtering, such as work in probabilis-tic matrix factorization and implicit feedback.2.1 Latent Feature ModelOne way to define ?r,t is through a latent featuremodel F. Here we measure compatibility betweenrelation r and tuple t as dot product of two latentfeature representations of size KF: ar for relation r,and vt for tuple t. This gives:?Fr,t :=KF?kar,kvt,k.This corresponds to generalized PCA (Collins et al2001), a model were the matrix ?
= (?r,t) of naturalparameters is defined as the low rank factorizationAV.Notice that we intentionally omit any per-relationbias-terms.
In section 4 we evaluate ranked answersto queries on a per-relation basis, and a per-relationbias term will have no effect on ranking facts of thesame relation.
Also consider that such latent featuremodels can capture asymmetry by assigning morepeaked vectors to specific relations, and more uni-form vectors to general relations.2.2 Neighborhood ModelWe can interpolate the confidence for a given tupleand relation based on the trueness of other similarrelations for the same tuple.
In collaborative filter-ing this is referred to as a neighborhood-based ap-proach (Koren, 2008).
In terms of our natural pa-rameter, we implement a neighborhood model N viaa set of weights wr,r?
, where each corresponds to adirected association strength between relations r andr?.
For a given tuple t and relation r we then sumup the weights corresponding to all relations r?
thathave been observed for tuple t:?Nr,t :=?(r?,t)?O\{(r,t)}wr,r?
.Notice that the neighborhood model amounts toa collection of local log-linear classifiers, one foreach relation r with feature functions fr,r?
(t) =I [r?
6= r ?
(r?, t) ?
O] and weights wr.
This meansthat in contrast to model F, this model cannot har-ness any synergies between textual and pre-existingDB relations.762.3 Entity ModelRelations have selectional preferences: they allowonly certain types in their argument slots.
Whileknowledge bases such as Freebase or DBPedia haveextensive ontologies of types of entities, these are of-ten not sufficiently fine to allow relations to discrim-inate (Yao et al 2012b).
Hence, instead of using apredetermined set of entity types, in our entity modelE we learn a latent entity representation from data.More concretely, for each entity e we introduce a la-tent feature vector te of dimension KE.
In addition,for each relation r and argument slot i we introducea feature vector di of the same dimension.
For ex-ample, binary relations have feature representationsd1 for argument 1, and d2 for argument 2.
Mea-suring compatibility of an entity tuple and relationamounts to measuring, and summing up, compati-bility between each argument slot representation andthe corresponding entity representation.
This leadsto:?Er,t :=arity(r)?i=1KE?kdi,ktti,k.Note that due to entity resolution, tuples mayshare entities, and hence parameters are sharedacross rows.2.4 Combined ModelIn practice all the above models can capture impor-tant aspects of the data.
Hence we also use variouscombinations, such as:?NFEr,t := ?Nr,t + ?Fr,t + ?Er,t.2.5 Parameter EstimationOur models are parametrized through weights andlatent component vectors.
We could estimate theseparameters by maximizing the loglikelihood of theobserved data akin to Collins et al(2001).
How-ever, as we do not have access to negative facts, themodel would simply learn to predict all facts to betrue.
In our initial attempt to overcome this issuewe sampled a set of unobserved facts as designatednegative facts, as is done in related distant supervi-sion approaches.
However, we found that (a) ourresults were sensitive to the choice of negative dataand (b) runtime was increased substantially becauseof a large number of required negative facts.In collaborative filtering positive-only data is alsoknown as implicit feedback.
This type of feedbackarises, for example, when users buy but not rateitems.
One successful approach to learning with im-plicit feedback is based on the observation that theactual task is not necessarily one of prediction (here:to predict a number between 0 and 1) but one of(generally simpler) ranking: to give true ?user-item?cells higher scores than false ones.
Bayesian Person-alized Ranking (BPR) uses a variant of this ranking:giving observed true facts higher scores than unob-served (true or false) facts (Rendle et al 2009).
Thisrelaxed constraint is to be contrasted with the log-likelihood setting that essentially requires (randomlysampled) negative facts to score below a globally de-fined threshold.2.5.1 ObjectiveWe first create a dataset of ranked pairs: for eachrelation r and each observed fact f+ := ?r, t+?
?
Owe choose all tuples t?
such that f?
:= ?r, t??
/?O?that is, tuples we have not observed to be inrelation r. For each pair of facts f+ and f?
wewant p (f+) > p (f?)
and hence ?f+ > ?f?
.
InBPR this is achieved by maximizing a sum terms ofthe form Objf+,f?
:= log(?
(?f+ ?
?f?
)), one foreach ranked pair:Obj :=??r,t+??O??r,t??/?OObj?r,t+?,?r,t??.
(1)Notice that this objective differs slightly from theone used by Rendle et al(2009).
Consider tuplesas users and items as relations.
We rank differentusers with respect to the same item, while BPR ranksitems with respect to the same user.
Also notice thatthe BPR objective is an approximation to the per-relation AUC (area under the ROC curve), and hencedirectly correlated to what we want to achieve: well-ranked tuples per relation.Note that all parameters are regularized withquadratic penalty which we omit here for brevity.2.5.2 OptimizationTo maximize the objective2 in equation 1 we fol-low Rendle et al(2009) and employ Stochastic Gra-dient Descent (SGD).
In particular, in each epoch2This objective is non-convex for all models excluding theN model.77we sample |O| facts with replacement from O. Foreach sampled fact ?r, t+?
we then sample a tuplet?
?
T such that ?r, t??
/?
O is not an observedfact.
This gives us |O| fact pairs ?f+, f?
?, and foreach pair we do an SGD update using the corre-sponding gradients of Objf+,f?
.
For the F modelthe gradients correspond to those presented by Ren-dle et al(2009).
The remaining gradients are easyto derive; we omit details for brevity.3 Related WorkThis work extends a previous workshop paper (Yaoet al 2012a) by introducing the neighborhood andentity model, by working with the BPR objective,and by more extensive experiments.Relational Clustering There is a large body ofwork aiming to discover latent relations by clus-tering surface patterns (Hasegawa et al 2004;Shinyama and Sekine, 2006; Kok and Domingos,2008; Yao et al 2011; Takamatsu et al 2011), orby inducing synonymy relationships between pat-terns independently of the entities (Yates and Et-zioni, 2009; Pantel et al 2007; Lin and Pantel,2001).
Our approach has a fundamentally differentobjective: we are not (primarily) interested in clus-ters of patterns or their semantic representation, butin predicting patterns where they are not observed.Moreover, these related methods rely on a symmetricnotion of synonymy in which clustered patterns areassumed to have the same meaning.
Our approachrejects this assumption in favor of a model whichlearns that certain patterns, or combinations thereof,entail others in one direction, but not necessarily theother.
This is similar in spirit to work on learningentailment rules (Szpektor et al 2004; Zanzotto etal., 2006; Szpektor and Dagan, 2008).
However, forus even entailment rules are just a by-product of ourgoal to improve prediction, and it is this goal we di-rectly optimize for and evaluate.Matrix Factorization Our approach is also re-lated to work on factorizing YAGO to predict newlinks (Nickel et al 2012).
The primary differencesare that we include surface patterns in our schema,use a ranking objective, and learn latent vectors forentities and tuples.
Likewise, matrix factorization invarious flavors has received significant attention inthe lexical semantics community, from LSA to re-cent work on non-negative sparse embeddings (Mur-phy et al 2012).
In our problem columns corre-spond to relations, and rows correspond to entity tu-ples.
By contrast, there columns are words, and rowsare contextual features such as ?words in a local win-dow.?
Consequently, our objective is to completethe matrix, whereas their objective is to learn betterlatent embeddings of words (which by themselvesagain cannot capture any sense of asymmetry).OpenIE Open IE (Etzioni et al 2008) extractsfacts mentioned in text, but does not predict poten-tial facts not mentioned in text.
Finding answersrequires explicit mentions, and hence suffers fromlower recall for not-so-frequently mentioned facts.Methods that learn rules between textual patterns inOpenIE aim at a similar goal as our proposed ap-proach (Schoenmackers et al 2008; Schoenmack-ers et al 2010).
However, their approach is sub-stantially more complex, requires a categorizationof entities into fine grained entity types, and needsinference in high tree-width Markov Networks.
Bycontrast, our approach is based on a single unifiedmodel, requires no entity types, and for us inferringa fact amounts to not more than a few dot products.In addition, in our Universal Schema approach Ope-nIE surface patterns are just one kind of relations,and our aim is populate relations of all kinds.
In thefuture we may even include relations between enti-ties and continuous attributes (say, gene expressionmeasurements).Distant Supervision In Distant Supervision (DS)a set of facts from pre-existing structured sourcesis aligned with surface patterns mentioned intext (Bunescu and Mooney, 2007; Mintz et al 2009;Riedel et al 2010; Hoffmann et al 2011; Surdeanuet al 2012), and this alignment is then used to traina relation extractor.
A core difference to our ap-proach is the number of target relations: In DS itis the relatively small schema size of the knowledgebase, while we also include surface patterns.
Thisallows us to answer more expressive queries.
More-over, by learning from surface-pattern correlations,our latent models induce feature representations forpatterns that do not appear in the DS training set.
Aswe will see in section 4, this allows us to outperformstate-of-the-art DS models.78Never-Ending Learning and Bootstrapping Ourlatent feature models are capable of never-endinglearning (Carlson et al 2010).
That is, we can con-tinue to train these models with incoming data, evenif no structured annotation is available.
In bootstrap-ping approaches the current model is used to predictnew relations, and these hypothesized relations areused as new supervision targets (i.e.
self-training).By contrast, our model only strengthens the correla-tions between incoming co-occurring observations.This has the advantage that wrong predictions areless likely be reinforced, hence reducing the risk ofsemantic drift.4 ExperimentsHow accurately can we fill a database of UniversalSchema, and does reasoning jointly across a uni-versal schema help to improve over more isolatedapproaches?
In the following we seek to answerthis question empirically.
To this end we train ourmodels on observed facts in a newswire corpus andFreebase, and then manually evaluate ranked predic-tions: first for structured relations and then for sur-face form relations.4.1 DataFollowing previous work (Riedel et al 2010),our documents are taken from the NYTimes cor-pus (Sandhaus, 2008).
Articles after 2000 are usedas training corpus, articles from 1990 to 1999 astest corpus.
We also split Freebase facts 50/50 intotrain and test facts, and their corresponding tuplesinto train and test tuples.
Then we align training tu-ples with the training corpus, and test tuples with thetest corpus.
This alignment relies on a preprocessingstep that links NER mentions in text with entities inFreebase.
In our case we use a simple string-matchheuristic to find this linking.
Now we align an entitytuple ?t1, t2?
with a pair of mentions ?m1,m2?
inthe same sentence if m1 is linked to t1 and m2 to t2.Based on this alignment we filter out all relations forwhich we find fewer than 10 tuples with mentions intext.The above alignment and filtering process reducesthe total number of tuples related according to Free-base to 16k: approximately 8k tuples with factsmentioned in the training set, and approximately 8ksuch tuples for the test set.
In addition we have aset of approximately 200k training tuples for whichboth arguments appear in the same sentence andboth can be linked to Freebase entities, but for whichno Freebase fact is recorded.
This can either be be-cause they are not related, or simply because Free-base does not contain the relationship yet.
We alsohave about 200k such tuples in the test set.
To sim-plify evaluation, we create a subsampled test set byrandomly choosing 10k of the original test set tuples.The above alignment allows us to determine, foreach tuple t, the observed facts Ot as follows.
Tofind the surface pattern facts OPATt for the tuple t =?t1, t2?
we extract, for each mention m = ?m1,m2?of t, the lexicalized dependency path p between m1and m2.
Then we add ?p, t?
to OPATt .
For example,we get ?<-subj<-head->obj->?
for ?M1 heads M2.
?Filtering out patterns with fewer than 10 mentionsin text yields approximately 4k patterns.
For train-ing tuples we add as Freebase facts OFBt all facts?r, t?
that appear in Freebase, and for which r hasnot been filtered out beforehand.
For the test setOFBtremains empty.
The total set of observed facts Ot isOFBt ?OPATt , and their union over all tuples forms theset of observed facts O.4.2 EvaluationFor evaluation we use collections of relations: sur-face patterns in one experiment and Freebase re-lations in the other.
In either case we comparethe competing systems with respect to their rankedresults for each relation in the collection.
Giventhis ranking task, our evaluation is inspired by theTREC competitions and work in information re-trieval (Manning et al 2008).
That is, we treateach relation as query and receive the top 1000 (rundepth) entity pairs from each system.
Then we poolthe top 100 (pool depth) answers from each systemand manually judge their relevance or ?truth.?
Thisgives a set of relevant results that we can use to cal-culate recall and precision measures.
In particular,we can use these annotations to measure an averageprecision across the precision-recall curve, and anaggregate mean average precision (MAP) across allrelations.
This metric has shown to be very robustand stable (Manning et al 2008).
In addition wealso present a weighted version of MAP (weightedMAP) in which the average precision for each re-79Relation # MI09 YA11 SU12 N F NF NFEperson/company 103 0.67 0.64 0.70 0.73 0.75 0.76 0.79location/containedby 74 0.48 0.51 0.54 0.43 0.68 0.67 0.69author/works_written 29 0.50 0.51 0.52 0.45 0.61 0.63 0.69person/nationality 28 0.14 0.40 0.13 0.13 0.19 0.18 0.21parent/child 19 0.14 0.25 0.62 0.46 0.76 0.78 0.76person/place_of_death 19 0.79 0.79 0.86 0.89 0.83 0.85 0.86person/place_of_birth 18 0.78 0.75 0.82 0.50 0.83 0.81 0.89neighborhood/neighborhood_of 12 0.00 0.00 0.08 0.43 0.65 0.66 0.72person/parents 7 0.24 0.27 0.58 0.56 0.53 0.58 0.39company/founders 4 0.25 0.25 0.53 0.24 0.77 0.80 0.68film/directed_by 4 0.06 0.15 0.25 0.09 0.26 0.26 0.30sports_team/league 4 0.00 0.43 0.18 0.21 0.59 0.70 0.63team/arena_stadium 3 0.00 0.06 0.06 0.03 0.08 0.09 0.08team_owner/teams_owned 2 0.00 0.50 0.70 0.55 0.38 0.61 0.75roadcast/area_served 2 1.00 0.50 1.00 0.58 0.58 0.83 1.00structure/architect 2 0.00 0.00 1.00 0.27 1.00 1.00 1.00composer/compositions 2 0.00 0.00 0.00 0.50 0.67 0.83 0.12person/religion 1 0.00 1.00 1.00 0.50 1.00 1.00 1.00film/produced_by 1 1.00 1.00 1.00 1.00 0.50 0.50 0.33MAP 0.32 0.42 0.56 0.45 0.61 0.66 0.63Weighted MAP 0.48 0.52 0.57 0.52 0.66 0.67 0.69Table 1: Average and (weighted) Mean Average Precisions for Freebase relations based on pooled results.
The #column shows the number of true facts in the pool.
NFE is statistically different to all but NF and F according to thesign test.
Bold faced are winners per relation, italics indicate ties.lation is weighted by the relation?s number of truefacts.Notice that we deviate from previous work in dis-tant supervision that (a) combines the results fromseveral relations in a single precision recall curve,and (b) uses held-out evaluation to measure howwell the predictions match existing Freebase facts.This has several benefits.
First, when aggregatingacross relations results are often dominated by a fewvery frequent relations, such as containedby, provid-ing little information about how the models performacross the board.
Second, evaluating with Freebaseheld-out data is biased.
For example, we find thatfrequently mentioned entity pairs are more likely tohave relations in Freebase.
Systems that rank suchtuples higher receives higher precision than thosethat do not have such bias, regardless of how cor-rect their predictions are.
Third, we can aggregateper-relation comparisons to establish statistical sig-nificance, for example via the sign test.Also note that while we run our models on thecomplete training and test set, evaluation is re-stricted to the subsampled test set.4.3 Predicting Freebase RelationsTable 1 shows our results for Freebase relations,omitting those for which none of the systems canfind any relevant facts.
Our first baseline is MI09,a distantly supervised classifier based on the workof Mintz et al(2009).
This classifier only learnsfrom observed pattern-relation pairs in the trainingset (of which we only have about 8k).
By contrast,our latent feature models can learn pattern-patterncorrelations both on the unlabeled training and testset (comparable to bootstrapping).
We hence alsocompare against YA11, a version of MI09 that usespreprocessed cluster features according to Yao et al(2011).
The third baseline is SU12, the state-of-the-art Multi-Instance Multi-Label system by Surdeanuet al(2012).The remaining systems are our neighborhood800.1 0.2 0.3 0.4 0.5 0.6 0.7 0.80.9 10  0.2  0.4  0.6  0.8  1Precision RecallAveraged 11-point Precision/Recall MI09YA11SU12NFNFNFEFigure 2: Averaged 11-point precision recall curve forFreebase relations in table 1.model (N), the factorized model (F), their combi-nation (NF) and the combined model with a latententity representation (NFE).
For all our models weuse the same number of components when applica-ble (KF = KE = 100), 1000 epochs, and 0.01 asregularizer for component weights and 0.1 for neigh-borhood weights.Table 1 shows that adding pattern cluster features(and hence incorporating more data) helps YA11to improve over MI09.
Likewise, we see that thefactorized model F improves over N, again learn-ing from unlabeled data.
This improvement is big-ger than the corresponding change between MI09and YA11, possibly indicating that our latent rep-resentations are optimized directly towards improv-ing prediction performance.
The combination of N,F and E outperforms all other models in terms ofweighted MAP, indicating the power of selectionalpreferences learned from data.
Note that NFE issignificantly different (p  0.05 in sign test) to allbut the NF and F models.
In terms of MAP the NFmodel outperforms NFE, indicating that it does notdo as well for frequent relations, but better for infre-quent ones.Figure 2 shows an averaged 11-point precision re-call graph (Manning et al 2008) for Freebase re-lations.
We notice that our latent models outper-form all remaining models across all recall levels,and that combining neighborhood and latent modelsis helpful.
This finding is consistent with our MAPresults.
Figure 3 shows the recall-precision curve forthe works_written relation with respect to our threebaselines and the NFE model.
Observe how preci-0 0.2 0.4 0.6 0.810  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Precision RecallRecall/Precision MI09YA11SU12NFEFigure 3: Precision and recall for works_written(X,Y).Relation # N F NF NFEvisit 80 0.19 0.68 0.49 0.42attend 69 0.23 0.10 0.07 0.10base 61 0.46 0.87 0.81 0.68head 38 0.47 0.67 0.70 0.68scientist 36 0.25 0.84 0.79 0.73support 18 0.16 0.29 0.32 0.38adviser 11 0.19 0.15 0.19 0.28criticize 9 0.09 0.60 0.67 0.64praise 4 0.01 0.03 0.05 0.10vote 3 0.18 0.18 0.34 0.34MAP 0.22 0.44 0.44 0.43Weighted MAP 0.28 0.56 0.50 0.46Table 2: Average and (weighted) Mean Average Preci-sions for surface patterns.2sion drops for both MI09 and SU12 at about 50%recall.
At this point the remaining unretrieved factshave patterns that have not been seen together withworks_written in the training set.
By using clusterfeatures, YA11 can overcome this problem partly,but not as dramatically as NFE?a pattern we ob-serve for many relations.All our models are fast to train.
The slowestmodel trains in just 45 minutes.
By contrast, trainingthe topic model in YA11 alone takes 4 hours.
Train-ing SU12 takes two hours (on less data).
Also noticethat our models not only learn to predict Freebaserelations, but also approximately 4k surface patternrelations.4.4 Predicting Surface PatternsTable 2 presents a comparison of our models with re-spect to 10 surface pattern relations.
These relations810 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.80.9 10  0.2  0.4  0.6  0.8  1Precision RecallAveraged 11-point Precision/Recall NFNFNFEFigure 4: Averaged 11-point precision recall curve forsurface pattern relations in table 2.were chosen according to what we believe are inter-esting questions not currently captured in Freebase.We again see that learning a latent representation (F,NF and NFE) from additional data helps quite sub-stantially over the N model.
For in the weightedMAP metric we note that incorporating entity rep-resentations (in the NFE model) in fact hurts totalperformance.3 One reason may be the fact that Free-base relations are typed?they require very specifictypes of entities as arguments.
By contrast, for asurface pattern like ?X visits Y?
X could be a personor organization, and Y could be a location, organi-zation or person.
However, in terms of MAP scorethis time there is no obvious winner among the la-tent models.
This is also confirmed by the averaged11-point precision recall curve in figure 4.Notice that we can accurately predict the X?scientist-at?Y surface pattern relation in table 2,as well as the more general person/company (em-ployedBy) relation in table 1.
This indicates thatour models can capture asymmetry?a symmetricmodel would either over-predict X?scientist-at?Yor under-predict person/company.5 ConclusionWe present relation extraction into universalschemas.
Such schemas contain surface patternsas relations, as well as relations from structuredsources.
By predicting missing tuples for surfacepattern relations we can populate a database with-out any labelled data, and answer questions not sup-3Due to the small set of relations only N is significantly dif-ferent to F, NF and NFE (p 0.05 in sign test).ported by the structured schema alone.
By predict-ing missing tuples in the structured schema we canexpand a knowledge base of fixed schema, and onlyrequire a set of existing facts from this schema.
Cru-cially, by predicting and modeling both surface pat-terns and structured relations simultaneously we canimprove performance.
We show this experimentallyby contrasting a series of the popular weakly super-vised models to our collaborative filtering modelsthat learn latent feature representations across sur-face patterns and structured relations.
Moreover, ourmodels are computationally efficient, requiring lesstime than comparable methods, while learning morerelations.Reasoning with universal schemas is not merely atool for information extraction.
It can also serve asa framework for various data integration tasks.
Forexample, we could integrate facts from one schema(say, Freebase) into another (say, the TAC KBPschema) by adding both sets of relations to the setof surface patterns.
Reasoning with this schemawill mean populating each database with facts fromthe other, and would leverage information in surfacepatterns to improve integration.
In future work wealso plan to integrate universal entity types and at-tributes into the model.The source code of our system, its output, andall data annotations are available at http://www.riedelcastro.org/uschema.AcknowledgmentsWe thank the reviewers for very helpful comments.This work was supported in part by the Center for In-telligent Information Retrieval and the University ofMassachusetts, in part by UPenn NSF medium IIS-0803847, in part by DARPA under agreement num-ber FA8750-13-2-0020 and FA8750-09-C-0181, andin part by an award from Google.
Any opinions,findings, and conclusion or recommendations ex-pressed in this material are those of the authorsand do not necessarily reflect the view of DARPA,AFRL, or the US government.ReferencesRonald J. Brachman.
1983.
What is-a is and isn t:An analysis of taxonomic links in semantic networks.IEEE Computer, 16(10):30?36.82Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to extract relations from the web using min-imal supervision.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Linguis-tics (ACL ?07).Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka, and Tom M. Mitchell.2010.
Toward an architecture for never-ending lan-guage learning.
In Proceedings of the 25th AAAI Con-ference on Artificial Intelligence (AAAI ?10).Michael Collins, Sanjoy Dasgupta, and Robert E.Schapire.
2001.
A generalization of principal com-ponent analysis to the exponential family.
In Proceed-ings of NIPS.M.
Craven and J. Kumlien.
1999.
Constructing biolog-ical knowledge-bases by extracting information fromtext sources.
In Proceedings of the Seventh Interna-tional Conference on Intelligent Systems for MolecularBiology, pages 77?86, Germany.Aron Culotta and Jeffery Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofACL, Barcelona, Spain.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S. Weld.
2008.
Open information extractionfrom the web.
Commun.
ACM, 51(12):68?74.T.
Hasegawa, S. Sekine, and R. Grishman.
2004.
Dis-covering Relations among Named Entities from LargeCorpora.
Proceedings of the 42nd Annual Meeting ofthe Association for Computational Linguistics (ACL?04), pages 415?422.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S. Weld.
2011.
Knowledge-based weak supervision for information extraction ofoverlapping relations.
In Proceedings of ACL.Stanley Kok and Pedro Domingos.
2008.
Extracting Se-mantic Networks from Text Via Relational Clustering.In ECML.Yehuda Koren.
2008.
Factorization meets the neighbor-hood: a multifaceted collaborative filtering model.
InProceedings of the 14th ACM SIGKDD internationalconference on Knowledge discovery and data min-ing, KDD ?08, pages 426?434, New York, NY, USA.ACM.Dekang Lin and Patrick Pantel.
2001.
DIRT - discoveryof inference rules from text.
In Knowledge Discoveryand Data Mining, pages 323?328.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Sch?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, Cambridge, UK.Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP (ACL ?09),pages 1003?1011.
Association for Computational Lin-guistics.Brian Murphy, Partha Pratim Talukdar, and TomMitchell.
2012.
Learning effective and interpretablesemantic models using non-negative sparse embed-ding.
In COLING, pages 1933?1950.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2012.
Factorizing yago: scalable machinelearning for linked data.
In Proceedings of the 21stinternational conference on World Wide Web, WWW?12, pages 271?280, New York, NY, USA.
ACM.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard Hovy.
2007.
ISP:Learning Inferential Selectional Preferences.
In Pro-ceedings of NAACL HLT.Steffen Rendle, Christoph Freudenthaler, Zeno Gantner,and Lars Schmidt-Thieme.
2009.
Bpr: Bayesian per-sonalized ranking from implicit feedback.
In Proceed-ings of the Twenty-Fifth Conference on Uncertainty inArtificial Intelligence, UAI ?09, pages 452?461, Ar-lington, Virginia, United States.
AUAI Press.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions withoutlabeled text.
In Proceedings of the European Confer-ence on Machine Learning and Knowledge Discoveryin Databases (ECML PKDD ?10).Evan Sandhaus, 2008.
The New York Times AnnotatedCorpus.
Linguistic Data Consortium, Philadelphia.Stefan Schoenmackers, Oren Etzioni, and Daniel S.Weld.
2008.
Scaling textual inference to the web.In EMNLP ?08: Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 79?88, Morristown, NJ, USA.
Association forComputational Linguistics.Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,and Jesse Davis.
2010.
Learning first-order hornclauses from web text.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?10, pages 1088?1098,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted relationdiscovery.
In Proceedings of the main conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics, HLT-NAACL ?06, pages 304?311, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, andChristopher D. Manning.
2012.
Multi-instance multi-label learning for relation extraction.
In Proceedings83of the Conference on Empirical methods in naturallanguage processing (EMNLP ?12), pages 455?465.Idan Szpektor and Ido Dagan.
2008.
Learning entail-ment rules for unary templates.
In Proceedings ofthe 22nd International Conference on ComputationalLinguistics - Volume 1, COLING ?08, pages 849?856,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisition ofentailment relations.
In Proceedings of EMNLP.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.2011.
Probabilistic matrix factorization leveragingcontexts for unsupervised relation discovery.
In Pro-ceedings of PAKDD.Limin Yao, Aria Haghighi, Sebastian Riedel, and AndrewMcCallum.
2011.
Structured relation discovery usinggenerative models.
In Proceedings of the Conferenceon Empirical methods in natural language processing(EMNLP ?11), July.Limin Yao, Sebastian Riedel, and Andrew McCallum.2012a.
Probabilistic databases of universal schema.In Proceedings of the AKBC-WEKEX Workshop atNAACL 2012, June.Limin Yao, Sebastian Riedel, and Andrew McCallum.2012b.
Unsupervised relation discovery with sensedisambiguation.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguis-tics (ACL ?12), July.Alexander Yates and Oren Etzioni.
2009.
Unsupervisedmethods for determining object and relation synonymson the web.
Journal of Artificial Intelligence Research,34:255?296.Fabio Massimo Zanzotto, Marco Pennacchiotti, andMaria Teresa Pazienza.
2006.
Discovering asym-metric entailment relations between verbs using selec-tional preferences.
In Proceedings of the 44th AnnualMeeting of the Association for Computational Linguis-tics (ACL ?06).84
