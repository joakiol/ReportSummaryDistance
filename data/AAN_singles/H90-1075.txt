Spoken Letter RecognitionRonald Cole, Mark FantyDepartment of Computer Science and EngineeringOregon Graduate Institute of Science and Technology19600 NW Von Neumann Dr.Beaverton, OR 97006IntroductionAutomatic recognition of spoken letters is one of themost challenging tasks in the field of computer speechrecognition.
The difficulty of the task is due to the acous-tic similarity of many of the letters.
Accurate recognitionrequires the system to perform fine phonetic distinctions,such asB vs. D, B vs. P ,D  vs. T, Tvs .
G ,C  vs.Z, V vs. Z, M vs. N and J vs. K. The ability to per-form fine phonetic distinctions--to discriminate amongthe minimal sound units of the language---is a fundamen-tal unsolved problem in computer speech recognition.We describe two systems that apply speech knowledgeand neural network classification to speaker-independentrecognition of spoken letters.
The first system, calledEAR (English Alphabet Recognizer), recognizes lettersspoken in isolation.
First choice recognition accuracy is96% correct on 30 test speakers.
A second system locatesand recognizes letters spoken with brief pauses betweenthem.
First choice recognition accuracy is 95.7% on 10test speakers for letters correctly located.
This systemwas used to retrieve spelled names from a database of50,000 common last names.
Of the 68 names pelled byten test speakers, 65 were retrieved as the first choice,and the remaining three were the second choice.We attribute the high level of accuracy obtained bythese systems to (a) accurate location of segment bound-aries, which allows feature measurements to be com-puted in the most informative regions of the signal, (b)the use of speech knowledge to design feature measure-ment algorithms, and (c) the ability of neural networkclassifiers to model the variability in speech.Isolated Letter RecognitionSystem Overv iewFigure 1 shows the system modules that transform aninput utterance into a classified letter.
The system isable to accept microphone input or classify letters fromdigitized waveform files.Data  CaptureSpeech is recorded using a Sennheiser HMD 224 noise-canceling microphone, lowpass filtered at 7.6 kHz andDigitize SpeechSignal RepresentationsNeural NetPitch TrackerSegmentation & BroadClassificationFeature MeasurementLetter ClassificationFigure 1: EAR Modulessampled at 16 kHz.
Data capture is performed us-ing the AT&T DSP32 board installed in a Sun4/ll0.The utterance is recorded in a two second buffer usingthe WAVES+ software distributed by Entropic systems.In order to speed recognition time, the spoken letter--typically 400 to 500 msec long--is located within the 2sec buffer based on values observed in two waveform pa-rameters; the zero crossing rate and peak-to-peak mpli-tude.
The remaining representations, such as the DFT,are then computed in the region of the utterance only.Signal Process ingSignal processing routines produce the following set ofrepresentations.
All parameters are computed every 3msec.ze0-8000:  the number of zero crossings of the wave-form in a 10 msec window;385ptp0-8000: the peak-to-peak amplitude (largest posi-tive value minus largest negative value) in a10 msec window in the waveform;ptp0-700: the peak-to-peak amplitude in a 10 msecwindow in the waveform lowpass filtered at700 Hz;DFT:  a 256 point DFT (128 real numbers) com-puted on a 10 msec Hanning window; andspectral  difference: the squared ifference of the av-eraged spectra in adjacent 12 msec intervals.P i tch TrackingA neural network pitch tracker is used to locate pitchperiods in the filtered (0-700 Hz) waveform \[2\].
Thealgorithm locates all plausible candidate peaks in the fil-tered waveform, computes a set of feature measurementsin the region of the candidate peak, and uses a neuralnetwork (trained with backpropagation) to decide if thepeak begins a pitch period.
The neural classifier agreeswith expert labelers about 98% of the time--just slightlyless than they agree with each other.Segmentat ion and Broad ClassificationA rule-based segmenter, modified from \[3\], was designedto segment speech into contiguous intervals and to assignone of four broad category labels to each interval: CLOS(closure or background noise), SON (sonorant interval),FRIC (fricative) and STOP.The segmenter uses cooperating knowledge sources tolocate the broad category segments.
Each knowledgesource locates a different broad category segment by ap-plying rules to the parameters described above.
Forexample, the SON knowledge source uses informationabout pitch, ptp0-700, zero crossings and spectral differ-ence to locate and assign boundaries to sonorant inter-vals.Feature MeasurementA total of 617 features were computed for each utter-ance.
Spectral coefficients account for 352 of the fea-tures.
For convenience, the features are grouped intofour categories, which are briefly summarized here:?
Contour  features were designed to capture thebroad phonetic ategory structure of English letters.The contour features describe the envelope of thezc0-8000, ptp0-700, ptp0-8000 and spectral differ-ence parameters.
Each contour is represented by 33features panning (a) the interval 200 msec beforethe sonorant; (b) the sonorant; and (c) the interval200 msec after the sonorant.
The 33 features arederived by dividing each of these intervals into 11equal segments, and taking the average (zc0-8000,ptp0-700, ptp0-8000) or maximum (spectral differ-ence) value of the parameter in each segment.?
Sonorant features were designed to discriminateamong: (a) Letters with different vowels (e.g., E,A, O); (b) letters with the same vowel with impor-tant information i the sonorant interval (e.g., L, M,N); and (c) letters with redundant information nearthe sonorant onset (e.g., B, D, E).
Sonorant fea-tures include averaged spectra in seven equal inter-vals within the sonorant, additional spectral slicesafter vowel onset (to determine the place of articu-lation of a preceding consonant), and estimates ofpitch and duration.?
P re -sonorant  features  were designed to discrimi-nate among pre-vocalic onsonants (e.g., V vs .
Z, Tvs .
G) and to discriminate vowels with glottalizedonsets from stops (e.g., E vs .
B, A vs .
K).
Thesefeatures include estimates of prevoicing, and spec-tra sampled within the STOP or FRIC precedingthe SON.
If no STOP or FRIC were found, featureswere computed on an interval 200 msec before thevowel.?
Post-sonorant features were designed to discrim-inate among F, S, X and H. Much of this informa-tion is captured by the contour features.
The mainpost-sonorant feature is the spectrum at the point ofmaximum zero crossing rate within 200 msec afterthe sonorant.Let ter  ClassificationLetter classification is performed by fully connectedfeed-forward networks.
The input to the first networkconsists of 617 feature values, normalized between 0 and1.
There are 52 hidden units in a single layer and 26output units corresponding to the letters A through Z.The classification response of the first network is takento be the neuron with the largest output response.If the first classification response is within the E-set, asecond classification is performed by a specialized net-work with 390 inputs (representing features from theconsonant and consonant-vowel transition), 27 hiddenunits and 9 output units.
Similarly, if the first classifi-cation response is M or N, a second classification is per-formed by a specialized network with 310 inputs (repre-senting features mainly in the region of the vowel-nasalboundary), 16 hidden units and 2 output units.
Thisstrategy is possible because almost all E-set and M-Nconfusions are with other letters in the same set.
If theclassification response of the first network is not M or Nor in the E-set, the output of the first net is final.Sys tem Deve lopmentDevelopment of the EAR system began in June 1989.The first speaker-independent recognition result, 86%,was obtained in September 1989.
The system achieved95% in January 1990 and 96% in May 1990.
The rapidimprovement to 95% in 5 months was obtained by im-proving the segmentation algorithm, the feature mea-surements and the classification strategy.
The improve-386ment to 96% resulted from increased training data andthe use of specialized nets for more difficult discrimina-tions.
This section briefly describes the research thatlead to the current system.DatabaseThe system was trained and tested on the ISOLETdatabase \[4\], which consists of two tokens of each let-ter produced by 150 American English speakers, 75 maleand 75 female.
The database was divided into 120 train-ing and 30 test speakers.
All experiments during systemdevelopment were performed on subsets of the trainingdata.Segmenter  Deve lopmentThe behavior of the segmentation algorithm profoundlyaffects the performance of the entire system.
Segmentboundaries determine where in the signal the featuremeasurements are computed.
The feature values usedto train the letter classification etwork are therefore di-rectly influenced by the segmenter.The rule-based segmenter was originally developed toperform segmentation and broad phonetic classificationof natural continuous peech.
The algorithm was modi-fied to produce optimum performance on spoken letters.It was improved by studying its performance on lettersin the training set and modifying the rules to eliminateobserved errors.Feature  Deve lopmentThe selection of features was based on past experiencedeveloping isolated letter recognition systems \[5\] andknowledge gained by studying visual displays of lettersin the training set.
(Letters in the 30 speaker test setwere never studied.)
Several features were designed todiscriminate among individual letter pairs, such as Band V. For these features, histograms of the feature val-ues were examined, and different feature normalizationstrategies were tried in order to produce better separa-tion of the feature distributions.
Feature developmentwas also guided by classification experiments.
For ex-ample, a series of studies on classification of the let-ters by vowel category showed that the best results wereobtained using spectra between 0-4 kHz averaged overseven equal intervals within the vowel.Network  TrainingNeural networks were trained using backpropagationwith conjugate gradient optimization \[6\].
Each networkwas trained on 80 iterations through the set of featurevectors.
The trained network was then evaluated on aseparate "cross-validation" test set (consisting of speak-ers not in the ISOLET database) to measure general-ization.
This process was continued through sets of 80iterations until the network had converged; convergencewas observed as a consistent decrease or leveling off ofthe classification percentage on the cross-validation dataA 98.3B 88.3C 100.0D 93.3E 100.0F 96.7G 98.3tt 100.0I 98.3J 98.3K 96.7L 100.0M 88.1N 80.0O 100.0P 91.7Q 100.0R 100.0S 93.3T 90.0U 98.3V 93.3W 98.3X 98.3Y 100.0Z 96.7Table h Classification performance for individual lettersfor 30 test speakers (with E-set and M-N nets).over successive sets of iterations.
Convergence always oc-curred by 240 iterations, about 36 hours on a Sun 4/60.The main (26 letter) network was trained with 240feature vectors for each letter (6240 vectors), computedfrom two tokens of each letter produced by 60 male and60 female speakers.
The specialized E-set and MN net-works were trained on the appropriate subset of lettersfrom the same training set.Recogn i t ion  Per fo rmanceThe EAR system was evaluated on two tokens of eachletter produced by 30 speakers.
The main network (26outputs, no specialized nets) performed at 95.9%.
Thespecialized E-set network improved performance slightly,while the MN network hurt performance on this dataset (experiments on subsets of the training data showedsubstantial improvement with the MN network).
Thecombined three-network system performed at 96%.
Ta-ble 1 shows the individual etter scores for the combinedthree-net system.
The specialized E-set network scores95% when run on all the E-set, and scores 94.2% whentrained and tested on just B,D,E and V.Multiple Letter RecognitionThe approach used to classify letters spoken in isolationhas been extended to automatic recognition of multipleletters--letters spoken with brief pauses between them.We have implemented and evaluated a system that usesmultiple letter strings to retrieve names from a databaseof 50,000 common last names.The recognition system differs from EAR in two im-portant ways: (a) the DFT was reduced to 128 points,and (b) a neural network was used to segment speechinto broad phonetic categories.
1 The processing stagesare shown in Figures 2 and 3.Neura l  Network  Segmentat ion  and  BroadC lass i f i ca t ionThe neural network segmenter, developed by MuraliGopalakrishnan as part of his Master's research, con-1 Performance of the EAR system is about  1% better  using theru le-based segraenter, but  the rules are not  easily extended to con-t inuous speech.387\[\] ~uto_ l~:F'e\[Lw"e Wrsion of "rhu Ita~ 24 I0:47:53 PDT 19._~.J \[~...I..T.Jadc\[Set Pitch File\]Mrlte Pitch Fi le\]dft\[Set Lole Fils\]NNNNletsl~t Lola Fi le\], .
.
... :.
: .
.
.
-: ,.".
, : . '
: '~: ( i : . "
; .
.
: .
.
.
.
.
.
.
:  .
*::~:~ "'.
~::~:;~" " " ?
: .
,  ?
"; ~.
:-""' : :" ' .
:~j~'::~'~'~ ": .
.
.
.
.
.
""~ : ':~:~'~'~ '"':~'~i :~,~'i:,  " :.':!
:' ...... "' i~.~'~'::~:~:" ."
-~"':~:~,~,~:~: !.
."
~: '~ , ' ( ,T :~.
. '
..~%:.~'.
:':' :~'o~aoho: / ~c/pr,o.~ eot s/spe~oh/1 etriter/d.~_ .
.
.
.
.
.
.
.
.
.
.
.
~.C 0.73 Z 0.22 T 0.18 I 0.09O 0.80 L 0.18 R 0.14 C 0.11L 0.73 O 0.39 Y 0.18 F 0.14E 0.71 A 0.35 V 0.22 H 0.20Figure 3: X windows display for the utterance "C O L E." The top four panels show (a) the digitized waveform,(b) the spectrogram, (c) the output of the segmenter, and (d) the location of the letters.
The lower panel shows theclassification performance of the system.
Each row has the top four system outputs for a letter.sists of a fully connected feed-forward net with 244 in-put units, 16 hidden units and 4 output units.
The seg-reenter produces an output every 3 msec for each broadcategory label.
The frame-by-frame output of the clas-sifier is converted to a string of broad category labels bytaking the largest output value at each time frame af-ter applying a 5-point median smoothing to the outputsacross successive frames.
Simple duration rules are thenapplied to the resulting string to prevent short spurioussegments, such as a SON less than 80 msec.The network was trained on multiple letter strings pro-duced by 30 male and 30 female speakers.
The featuresused to train the network consist of the spectrum for theframe to be classified, and the waveform and spectral dif-ference parameters in a 300 msec window centered on theframe.
The features were designed to provide detailed in-formation in the immediate vicinity of the frame and lessdetailed information about the surrounding context.Let ter  Segmentat ionLetter segmentation is performed by applying rules tothe sequence of broad category labels produced by theneural network.
The rules are relatively simple becausethe speakers are required to pause between letters.
Ex-cept for W, all letters have a single sonorant segment.We assume every sonorant is part of a distinct letter.The boundary between adjacent sonorants is placed inthe center of the last closure before the second sonorant.In the English alphabet, all within-letter closures occurafter the sonorant (i.e.
X and H), so these simple rulescapture every case except W, which is usually realizedas two sonorants.
Our system usually treats W as twoletters; we recover from this over-segmentation duringthe search process.Le t ter  C lass i f i ca t ionA single fully connected feed-forward network with 617inputs, 52 hidden units and 26 outputs was used to clas-sify letters.
This is similar to the first network used inEAR, although spectral coefficients were based on a 128-point DFT.
The network was trained on a combinationof data from the ISOLET database and 60 additionalspeakers pelling names and random strings with pauses.Name Ret r ieva lAfter the individual letters are classified, a database ofnames is searched to find the best match.
For this search,the values of the 26 output units are used as the scores388Digitize SpeechCompute RepresentationsEstimate PitchLocate Broad CategorySegmentsLocate LettersClassify LettersRetrieve NamesFigure 2: Name Retrieval Modulesfor the 26 letters.
For each letter classified, 26 scores arereturned.
The score for a name is equal to the productof the scores returned for the letters in that name in thecorresponding positions.The number of letters found may not match the num-ber of letters in the target name for a number of reasons:there can be segmentation errors; non-letter sounds canbe mistaken for letters; the name can be mis-spelled.
Be-cause of such errors, letters inserted into or deleted fromnames are penalized but do not invalidate a match.We deal with split Ws during name retrieval in the fol-low way.
If a string of letters does not score well againstany name, then all pairs of letters in the string for whichthe second letter is U are collapsed into a W with a scoreof 0.5 and the search is repeated.
This trick has workedsurprisingly well in our initial studies because the secondpart of W is almost always classified as U and becausereplacing W in a name with something-U does not usu-ally yield another name.
Future systems will deal withW in a more elegant manner.ResultsWe tested our system on 100 spelled names from 10new speakers.
Name retrieval was evaluated on twodatabases.
The first database consisted of 10,940 namesfrom a local mailing list.
Ignoring split Ws (which causedno name-retrieval errors), 697 of 719 letters (97%) werecorrectly located.
Of these, 95.7% Were correctly classi-fied.
The correct name was returned as the first choice97 of 100 times.
For the three errors, the correct namewas the second or third choice twice.
(The other namecontained three letters spoken without pause.
We didnot strictly screen the database for pauses because wewanted some borderline cases as well.)
Sixty-eight ofthe one-hundred names were also in a database of 50,000common last names.
When using this database, 65 of 68names were returned as the first choice.
The correctname was the second choice for the 3 errors.DiscussionEnglish alphabet recognition has been a popular taskdomain in computer speech recognition for a numberof years.
Early work, reviewed in \[7\], applied dynamicprogramming to frame by frame matching of input andreference patterns to achieve speaker-dependent recog-nition rates of 60% to 80%.
A substantial improvementin recognition accuracy was demonstrated in the FEA-TURE system, which combined knowledge-based featuremeasurements and multivariate classifiers to obtain 89%speaker-independent recognition of spoken letters \[5\].
Inrecent years, increased recognition accuracy, to a levelof 93%, has been obtained using hidden Markov models\[8, 9\].It is difficult to compare recognition results across lab-oratories because of differences in databases, recordingconditions, signal bandwidth, signal to noise ratio andexperimental procedures.
Still, as Table 2 reveals, per-formance of the EAR system compares favorably to pre-viously reported systems.We attribute the success of the EAR system to theuse of speech knowledge to design features that cap-ture the important acoustic-phonetic information, andthe ability of neural network classifiers to use these fea-tures to model the variability in the data.
Our researchhas clearly shown that the addition of specific featuresfor difficult discriminations, uch as B vs. V, improvesrecognition accuracy.
For example, networks trainedwith spectral features alone perform about 10% worsethan networks trained with the complete set of features.Explicit segmentation f the speech signal is an impor-tant feature of our approach.
The location of segmentboundaries allows us to measure features of the signalthat are most important for recognition.
For example,the information eeded to discriminate B from D is con-tained in two main regions: the interval extending 20msec after the release burst and the 15 msec intervalafter the vowel onset.
By locating the stop burst andthe vowel onset, we can measure the important featuresneeded for classification and ignore irrelevant variationin the signal.We are impressed with the level of performance ob-tained with the neural network segmenter.
We believethe algorithm can be substantially improved with ad-ditional features, recurrent networks and more train-ing data.
Neural network segmenters have the impor-tant advantage of being easily retrained for differentdatabases (e.g., telephone speech, continuous peech),whereas rule-based segmenters require substantial hu-389Study  Condit ions Speakers Approach Letters ResultsBrown 20 kHz Sampling 100 speakers HMM E-set 92.0%(1987) 16.4 dB SNR (multi-speaker)Euler 6.67 kHz Sam- 100 speakers HMM 26 letters + 93.0%et al i pling (telephone (multi-speaker) 10 digits +(1990) bandwidth) 3 control wordsLang Brown's data 100 speakers Neural networks B,D,E,V 93.0%et.
al (multi-speaker)(1990)Cole, 16 kHz Sampling 120 training Knowledge-based 26 letters; 96.0%Fanty 31 dB SNR 30 test (speaker- features and neural E-set; 95.0%(1990) independent) networks B,D,E,V 94.2%Table 2: Recent letter classification resultsman engineering.The application of spoken letter recognition to nameretrieval is an obvious and important application.
Earlywork with databases of 18,000 names suggested thatspelled names are sufficiently unique so that accurate \[6\]name retrieval could be obtained without accurate let-ter recognition \[10\].
One insight we have gained fromour experiments with the 50,000 names is that larger \[7\]databases do require accurate letter recognition to re-trieve names.
For example, the 3724 4-letter names inour database generate 20,192 pairs that differ by one let-ter.
Of these, 1372 differ by an acoustically similar letter,such as B-D (152), M-N (128), etc.
Correct retrieval ofthese names requires the system to perform fine phonetic \[8\]distinctions.References\[1\] Lang, K. J., A. H. Waibel, and G. E. Hinton, "Atime-delay neural network architecture for isolatedword recognition," Neural Networks, 3, pp.
23-43,(1990).\[2\] Barnard, E., R. A. Cole, M. P. Vea and F. All-eva, "Pitch detection with a neural-net classifier,"IEEE Transactions on Acoustics, Speech ~ SignalProcessing, (Accepted for publication), (1991).\[3\] Cole, R. A. and L. Hou, "Segmentation and broadclassification of continuous peech," Proceedings ofthe IEEE International Conference on Acoustics,Speech, and Signal Processing , New York, (April1988).\[4\] Cole, R. A., Y. Muthusamy and M. A. Fanty, "TheISOLET Spoken Letter Database," Technical Re-port 90-004, Computer Science Department, Ore-gon Graduate Institute, (1990).\[5\] Cole, R. A., R. M. Stern, M. S. Phillips, S. M.Brill, A. P. Pilant, and P. Specker, "Feature-based speaker-independent recognition of isolated\[9\]\[lO\]English letters," Proceedings of the IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing, pp.
731-734, (April 1983).Barnard, E. and D. Casasent, "Image processing forimage understanding with neural nets," in Interna-tional Joint Conference on Neural Nets, (1989).Cole, R. A., R. M. Stern, and M. J. Lasry, "Per-forming fine phonetic distinctions: Templates vs.features," in Invariance and Variability of SpeechProcesses, ed.
J. Perkell and D. Klatt, LawrenceErlbaum, New York, (1984).Brown, P. F., "The acoustic-modeling problem inautomatic speech recognition," Doctoral Disserta-tion, Carnegie Mellon University, Dept.
of Com-puter Science (1987).Euler, S. A., B. H. Juang, C. H. Lee, and F. K.Soong, "Statistical segmentation and word model-ing techniques in isolated word recognition," in Pro-ceedings IEEE International Conference on Acous-tics, Speech, and Signal Processing, (1990).Aldefeld, B., L. R. Rabiner, A. E. Rosenberg and J.G.
Wilpon, "Automated irectory listing retrievalsystem based on isolated word recognition," Pro-ceedings of the IEEE, 68, pp.
1364-1378, (1980).390
