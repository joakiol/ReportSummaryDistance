Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 120?123,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP?-extension Hidden Markov Models and Weighted Transducers forMachine TransliterationBalakrishnan VardarajanDept.
of Electrical and Computer EngineeringJohns Hopkins Universitybvarada2@jhu.eduDelip RaoDept.
of Computer ScienceJohns Hopkins Universitydelip@cs.jhu.eduAbstractWe describe in detail a method for translit-erating an English string to a foreignlanguage string evaluated on five differ-ent languages, including Tamil, Hindi,Russian, Chinese, and Kannada.
Ourmethod involves deriving substring align-ments from the training data and learning aweighted finite state transducer from thesealignments.
We define an ?-extension Hid-den Markov Model to derive alignmentsbetween training pairs and a heuristic toextract the substring alignments.
Ourmethod involves only two tunable parame-ters that can be optimized on held-out data.1 IntroductionTransliteration is a letter by letter mapping of onewriting system to another.
Apart from the obvi-ous use in writing systems, transliteration is alsouseful in conjunction with translation.
For exam-ple, machine translation BLEU scores are knownto improve when named entities are transliterated.This engendered several investigations into auto-matic transliteration of strings, named entities inparticular, from one language to another.
SeeKnight and Graehl(1997) and later papers on thistopic for an overview.Hidden Markov Model (HMM) (Rabiner,1989) is a standard sequence modeling tool usedin various problems in natural language process-ing like machine translation, speech recognition,part of speech tagging and information extraction.There have been earlier attempts in using HMMsfor automatic transliteration.
See (Abdul Jaleeland Larkey, 2003; Zhou et al, 2008) for exam-ple.
In this paper, we define an ?-extension Hid-den Markov Model that allows us to align sourceand target language strings such that the charac-ters in the source string may be optionally alignedto the ?
symbol.
We also introduce a heuristic thatallows us to extract high quality sub-alignmentsfrom the ?-aligned word pairs.
This allows us todefine a weighted finite state transducer that pro-duces transliterations for an English string by min-imal segmentation.The overview of this paper is as follows: Sec-tion 2 introduces ?-extension Hidden MarkovModel and describes our alignment procedure.Section 3 describes the substring alignmentheuristic and our weighted finite state transducerto derive the final n-best transliterations.
We con-clude with a result section describing results fromthe NEWS 2009 shared task on five different lan-guages.2 Learning AlignmentsThe training data D is given as pairs of strings(e, f) where e is the English string with the cor-responding foreign transliteration f .
The Englishstring e consists of a sequence of English letters(e1, e2, .
.
.
, eN ) while f = (f1, f2, .
.
.
, fM ) .We represent E as the set of all English symbolsand F as the set of all foreign symbols.1 We alsoassume both languages have a special null symbol?, that is ?
?
E and ?
?
F .Our alignment model is a Hidden MarkovModel H(X,Y,S,T,Ps), where?
X is the start state and Y is the end state.?
S is the set of emitting states with S = |S|.The emitting states are indexed from 1 to S.The start state X is indexed as state 0 and theend state Y is indexed as state S + 1.?
T is an (S + 1) ?
(S + 1) stochastic matrixwith T = [tij] for i ?
{0, 1, .
.
.
, S} and j ?
{1, 2, .
.
.
, S + 1}.1Alphabets and diacritics are treated as separate symbols.120?
Ps = [pef ] is an |E| ?
|F| matrix of jointemission probabilities with pef = P (e, f |s)?s ?
S .We define s?
to be an ?-extension of a string ofcharacters s = (c1, c2, .
.
.
, ck) as the string ob-tained by pumping an arbitrary number of ?
sym-bols between any two adjacent characters cl andcl+1.
That is, s?
= (di1 , .
.
.
, di2 , .
.
.
, dik) wheredij = cj and dl = ?
for im < l < im+1 where1 ?
l < k. Observe that there are countably infi-nite ?-extensions for a given string s since an arbi-trary number of ?
symbols can be inserted betweencharacters cm and cm+1.
Let T (s) denote the setof all possible ?-extensions for a given string s.For a given pair of strings (u, v), we define ajoint ?-extension of (u, v) as the pair (u?, v?)
s.t.
u?
?T (u) and v?
?
T (v) with |u?| = |v?| and ?i s.t.u?i = v?i = ?.
Due to this restriction, there are finite?-extensions for a pair (u, v) with the length of u?and v?
bounded above by |u| + |v|.
2 Let J(u, v)denote the set of all joint ?-extensions of (u, v).Given a pair of strings (e, f) with e =(e1, e2, .
.
.
, eN ) and f = (f1, f2, .
.
.
, fM ), wecompute the probability ?
(e, f, s?)
that they aretransliteration pairs ending in state s?
as?
(e, f, s?)
=?
(e?,?f)?J(e,f)?0=s0,...,s|e?|=s?t0,s1|e?|?i=1tsi,si+1P (e?i, f?i|si)In order to compute the probability Q(e, f) of agiven transliteration pair, the final state has to bethe end state S + 1.
HenceQ(e, f) =S?s=1?
(e, f, s)ts,S+1 (1)We also write the probability ?
(e, f, s?)
that theyare transliteration pairs starting in state s?
as?
(e, f, s?)
=?
(e?,?f)?J(e,f)?s?=s0,...,s|e?|+1=S+1ts0,s1|e?|?i=1tsi,si+1P (e?i, f?i|si)Again noting that the start state of the HMMH is 0, we have Q(e, f) =S?s=1?
(e, f, s)t0,s.
We2|u?| = |v?| > |u| + |v| would imply ?i s.t.
u?i = v?i = ?which contradicts the definition of joint ?-extension.denote a subsequence of a string u as umn =(un, un+1, .
.
.
, um) .
Using these definitions, wecan define ?
(ei1, fj1 , s) as????????
?1 i = j = 0, s = 00 i = j = 0, s 6= 0t0,sP (e1, f1|s) i = j = 1PSs?=1 ts?,s?
(ei1, fj?11 , s?
)P (?, fj |s) i = 1, j > 1PSs?=1 ts?,s?
(ei?11 , fj1 , s?
)P (ei, ?|s) i > 1, j = 1Finally for i > 1 and j > 1,?
(ei1, f j1 , s) =?s??Sts?,s[?
(ei1, f j?11 , s?
)P (?, fj |s)+?
(ei?11 , fj1 , s?
)P (ei, ?|s)+?
(ei?11 , fj?11 , s?
)P (ei, fj|s)]Similarly the recurrence for ?
(eNi , fMj , s)????
?ts,S+1 i = N + 1,j = M + 1PSs?=1 ts,s??
(eNi , fMj+1, s?
)P (?, fj |s?)
i = N, j < MPSs?=1 ts,s??
(eNi+1, fMj , s?
)P (ei, ?|s?)
i < N, j = MFor i < N and j < M , ?
(eNi , fMj , s) =?s??Sts,s?[?
(eNi , fMj+1, s?
)P (?, fj |s?)+?
(eNi+1, fMj , s?
)P (ei, ?|s?)+?
(eNi+1, fMj+1, s?
)P (ei, fj|s?
)]In order to proceed with the E.M. estimationof the parameters T and Ps , we collect thesoft counts c(e, f |s) for emission probabilities bylooping over the training data D as shown in Fig-ure 1.Similarly the soft counts ct(s?, s) for the tran-sition probabilities are estimated as shown in Fig-ure 2.Finally the probabilities P (e, f |s) and tij are re-estimated asP?
(e, f |s) = c(e, f |s)?e?E,f?F c(e, f |s)(2)t?s?,s =ct(s?, s)?s ct(s?, s)(3)We can also compute the most probable align-ment (e?, f? )
between the two strings e and f as121c(e, f |s) =?
(e,f)?D1Q(e, f)N?i=1M?j=1?s??
(ei?11 , fj?11 , s?
)ts?,sP (ei, fj |s)?
(eNi , fMj , s)1(ei = e, fj = f)+?
(e,f)?D1Q(e, f)N?i=1M?j=1?s??
(ei?11 , fj1 , s?
)ts?,sP (ei, ?|s)?
(eNi , fMj , s)1(ei = e, fj = f)+?
(e,f)?D1Q(e, f)N?i=1M?j=1?s??
(ei1, f j?11 , s?
)ts?,sP (?, fj |s)?
(eNi , fMj , s)1(ei = e, fj = f)Figure 1: EM soft count c(e, f |s) estimation.ct(s?, s) =?
(e,f)?D1Q(e, f)N?i=1M?j=1?
(ei?11 , fj?11 , s?
)ts?,sP (ei, fj|s)?
(eNi , fMj , s)+?
(e,f)?D1Q(e, f)N?i=1M?j=1?
(ei?11 , fj1 , s?
)ts?,sP (ei, ?|s)?
(eNi , fMj , s)+?
(e,f)?D1Q(e, f)N?i=1M?j=1?
(ei1, f j?11 , s?
)ts?,sP (?, fj|s)?
(eNi , fMj , s)+?
(e,f)?D1Q(e, f)?
(eN1 , fM1 , s?
)ts?,S+11(s = S + 1)Figure 2: EM soft count ct(s?, s) estimation.122arg max(e?,?f)?J(e,f)?0=s0,...,s|e?|+1=S+1t0,s1|e?|?i=1tsi,si+1P (e?i, f?i|si)The pair (e?, f?)
is considered as an alignment be-tween the training pair (e, f).3 Transduction of the TransliteratedOutputGiven an alignment (e?, f?
), we consider all possi-ble sub-alignments (e?ji , f?ji ) as pairs of substringsobtained from (e?, f? )
such that e?i 6= ?, f?i 6= ?,e?j+1 6= ?
and f?j+1 6= ?
.
We extract all pos-sible sub-alignments of all the alignments fromthe training data.
Let A be the bag of all sub-alignments obtained from the training data.
Webuild a weighted finite state transducer that trans-duces any string in E+ to F+ using these sub-alignments.Let (u,v) be an element of A.
From the train-ing data D, observe that A can have multiple re-alizations of (u,v).
Let N(u,v) be the numberof times (u,v) is observed in A.
The empiricalprobability of transducing string u to v is simplyP (v|u) = N(u,v)?v:(u,v?
)?A N(u,v?
)For every pair (u,v) ?
A , we also compute theprobability of transliteration from the HMM H asQ(u,v) from Equation 1.We construct a finite state transducer Fu,v thataccepts only u and emits v with a weight wu,vdefined aswu,v = ?
log(P (v|u))??
log(Q(u,v))+?
(4)Finally we construct a global weighted finitestate transducer F by taking the union of all theFu,v and taking its closure.F =???(u,v)?AFu,v?
?+(5)The weight ?
is typically sufficiently high sothat a new english string is favored to be brokeninto fewest possible sub-strings whose translitera-tions are available in the training data.We tune the weights ?
and ?
by evaluating theaccuracy on the held-out data.
The n-best pathsin the weighted finite state transducer F representour n-best transliterations.4 ResultsWe evaluated our system on the standard track dataprovided by the NEWS 2009 shared task orga-nizers on five different languages ?
Tamil, Hindi,Russian, and Kannada was derived from (Ku-maran and Kellner, 2007) and Chinese from (Li etal., 2004).
The results of this evaluation on the testdata is shown in Table 1.
For a detailed descriptionLanguage Top-1 mean MRRAccuracy F1 scoreTamil 0.327 0.870 0.458Hindi 0.398 0.855 0.515Russian 0.506 0.901 0.609Chinese 0.450 0.755 0.514Kannada 0.235 0.817 0.353Table 1: Results on NEWS 2009 test data.of the evaluation measures used we refer the read-ers to NEWS 2009 shared task whitepaper (Li etal., 2009).5 ConclusionWe described a system for automatic translitera-tion of pairs of strings from one language to an-other using ?-extension hidden markov models andweighted finite state transducers.
We evaluatedour system on all the languages for the NEWS2009 standard track.
The system presented is lan-guage agnostic and can be trained for any languagepair within a few minutes on a single core desktopcomputer.ReferencesNasreen Abdul Jaleel and Leah Larkey.
2003.
Statistical transliteration for english-arabiccross language information retrieval.
In Proceedings of the twelfth international con-ference on Information and knowledge management, pages 139?146.Kevin Knight and Jonathan Graehl.
1997.
Machine transliteration.
In Computational Lin-guistics, pages 128?135.A.
Kumaran and Tobias Kellner.
2007.
A generic framework for machine transliteration.In SIGIR ?07: Proceedings of the 30th annual international ACM SIGIR conferenceon Research and development in information retrieval, pages 721?722, New York, NY,USA.
ACM.Haizhou Li, Min Zhang, and Jian Su.
2004.
A joint source-channel model for machinetransliteration.
In ACL ?04: Proceedings of the 42nd Annual Meeting on Associationfor Computational Linguistics, page 159, Morristown, NJ, USA.
Association for Com-putational Linguistics.Haizhou Li, A Kumaran, Min Zhang, and Vladimir Pervouchine.
2009.
Whitepaper ofnews 2009 machine transliteration shared task.
In Proceedings of ACL-IJCNLP 2009Named Entities Workshop (NEWS 2009).Lawrence Rabiner.
1989.
A tutorial on hidden markov models and selected applications inspeech recognition.
In Proceedings of the IEEE, pages 257?286.Yilu Zhou, Feng Huang, and Hsinchun Chen.
2008.
Combining probability models and webmining models: a framework for jproper name transliteration.
Information Technologyand Management, 9(2):91?103.123
