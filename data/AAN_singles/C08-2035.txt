Coling 2008: Companion volume ?
Posters and Demonstrations, pages 139?142Manchester, August 2008Sentence Compression as a Step in Summarization or an Alternative Pathin Text ShorteningMehdi Yousfi-MonodUniversity of Montpellier 2, CNRSLIRMM, 161 rue Ada34392 Montpellier Cedex 5yousfi@lirmm.frViolaine PrinceUniversity of Montpellier 2, CNRSLIRMM, 161 rue Ada34392 Montpellier Cedex 5prince@lirmm.frAbstractThe originality of this work leads in tack-ling text compression using an unsuper-vised method, based on a deep linguisticanalysis, and without resorting on a learn-ing corpus.
This work presents a systemfor dependent tree pruning, while preserv-ing the syntactic coherence and the maininformational contents, and led to an op-erational software, named COLIN.
Exper-iment results show that our compressionsget honorable satisfaction levels, with amean compression ratio of 38 %.1 IntroductionAutomatic summarization has become a crucialtask for natural language processing (NLP) sinceinformation retrieval has been addressing it as oneof the most usual user requirements in its panelof products.
Most traditional approaches are con-sidering the sentence as a minimal unit in thesummarization process.
Some more recent worksget into the sentence in order to reduce the num-ber of words by discarding incidental informa-tion.
Some of these approaches rely on statisticalmodels (Knight and Marcu, 2002; Lin and Hovy,2002; Hovy et al, 2005), while some other worksuse rule-based linguistically-motivated heuristics(McKeown et al, 2002; Dorr et al, 2003; Gagnonand Sylva, 2005) to improve the determination ofthe importance of textual segments.
Consideringa deeper linguistic analysis could considerably im-prove the quality of reduced sentences, we decidedc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.to develop a sentence compression approach ex-clusively focused on linguistic heuristics.
Our firstwork (Yousfi-Monod and Prince, 2005), slightlyanterior to (Gagnon and Sylva, 2005), showed in-teresting results, and led to a deeper and more com-plete work fully detailed in (Yousfi-Monod, 2007).This paper sums up our approach.2 Theoretical frameworkThe main hypothesis of this work leans on the ob-servation that incident sentence constituents areoften not as important as principal constituents.For instance, let us consider the temporal adver-bial in the following sentence: ?Taiwan electedon Saturday its first president?.
While the subject?Taiwan?
and the verb ?elected?
are principal con-stituents of the sentence, ?on Saturday?
is incidentand can be removed without causing neither anagrammaticality nor a weighty content lost.
Twoaspects of the constituent significance: Grammati-cality and content, are dealt with in this section.2.1 Grammaticality preservation thanks tothe syntactic functionThe principal/incident constituent principle can befound in constituent or dependency grammar rep-resentations.
The approach embedded in COLINis based on such grammars, while adapting themto the relevant proprieties for sentence compres-sion.
As we aim at preserving sentences gram-maticality, our first goal is to get a syntactic treebased on the grammatical importance, where foreach node, a daughter node is an incident con-stituent which may be removed under certain con-ditions.
We opted for the X-bar theory (Chomsky,1970), which represents a sentence through a treeof constituents, composed by heads and governedconstituents (also dependents).
While a head is139grammatically mandatory, its dependents can oftenbe removed, depending on some of their linguisticproperties and/or those of their heads.
Our goal isfirst to have a syntactic structure modeling basedon constituents grammatical importance.
Syntacticwriting rules of the X-bar theory are focusing onsentence construction by placing specifiers, com-plements and adjuncts in the subtree of their con-stituent.
While adjuncts are systematically remov-able, we have had to adopt a case-by-case studyfor specifiers and complements.
For instance, in anoun phrase (NP), the article, if present, is a speci-fier, and it cannot be removed, while in an adjectivephrase, the specifier is typically an adverb, whichis removable.
The removability of a complementdepends on the subcategorisation properties of itshead.
On a clause scale, the dependents are notwell defined in the X-bar theory and may includethe subject and verbal groups, as, respectively, thespecifier and the complement of the clause.
Thus,the specifier (subject) cannot be removed.
Ourstudy has then consisted in a categorization of theX-bar?s functional entities according to their re-moval property.
We have decided (i) to considermandatory specifiers as complements required bytheir head and (ii) to bring together optional spec-ifiers and adjuncts in a different category: Modi-fiers1.We have defined two classes of functions: Comple-ments (X-bar complements and mandatory speci-fiers) and Modifiers (X-bar adjuncts and optionalspecifiers).
This syntactic function classificationallows us to clearly define which sentence objectscan be candidates for removal.Nevertheless, the syntactic function information,although crucial, is not sufficient.
One has to useother linguistic properties in order to refine the as-sessment of the constituent importance.2.2 Important content preservation thanks tolinguistic proprietiesSubcategorisation.
For noun and clause heads,some of our complements have been identifiedas systematically mandatory in order to preservethe sentence coherence (subject, verbal group, ar-ticles.
.
.
).
Other heads (verb, adjective, adverb,preposition and pronoun) may admit optional ormandatory complements, depending on either thelexical head category or a particular head instance1We have chosen the term ?modifier?
as its definitions inthe literature fit quite well our needs.
(a lexical entry).
Indeed, prepositions are sys-tematically requiring a complement2, while otherheads must be considered on a case-by-case basis.Once we get the subcategorisation information fora head, we are able to determine whether its com-plement(s) can be removed without causing an in-coherence.Other linguistic proprieties.
We identified sev-eral other linguistic clues that may help assessingthe importance of dependents.
We do not detail ouranalysis here for space reasons, refer to (Yousfi-Monod, 2007) for the full description.
These cluesinclude lexical functions, fixed expressions, typeof the article (definite or indefinite), parentheticalphrases, detached noun modifiers, the dependentconstituent position in the sentence, negation andinterrogation.3 COLIN?s compressor: Systemarchitecture and implementation3.1 ArchitectureWe assume we have a raw text as an input, whichmay be the output of an extract summarizer, andwe have to produce a compressed version of it,by reducing as many sentences as we can, withoutdeleting a single one.Syntactic analysis.
This step consists in usinga syntactic analyzer to produce, from the sourcetext, dependent trees according to our syntacticmodel (heads, complements, modifiers).
In orderto handle the important content assessment, theparser uses linguistic resources including subcat-egorisation information, lexical functions, and theother linguistic properties (section 2.2), and thenenriches the trees with this information.Pruning and linearization.
The trees will bethen pruned according to a set of compression rulesdefined from our theoretical analysis.
Several setof rules can be defined according to (i) the desiredimportance conservation, (ii) the desired compres-sion rate, (iii) the confidence in syntactic analy-sis results, (iv) the trust in the identified linguisticclues, (v) the textual genre of the source text.
Inorder to get effective rules, we have first defineda relatively reliable kernel of rules.
Then we havedecided to define and test, during our evaluation2Accordingly to the X-bar structure as well as ours: Thepreposition is the head of the prepositional syntagm.140described in the next section, several rules configu-rations, taking into account each of the five points,in order to find the most effective ones.
Rules tageach tree node (complements and modifiers) whichwill be removed, then trees are pruned and lin-earized to get back new sentences, compressed.3.2 ImplementationThe first step in our implementation was to se-lect a parser satisfying our syntactic requirementsas much as possible.
SYGFRAN (described in(Yousfi-Monod, 2007)), is the one that has beenchosen as: (i) It produces constituent trees veryclose to our model, (ii) it has a good syntactic cov-erage, (iii) it has a very good parsing complexity(O(n.log(n)), with n the size of the data in words),and (iv) its author and developer, Jacques Chauch?e,works in our research team at LIRMM3, whichconsiderably eases the adaptation of the syntac-tic model to ours.
SYGFRAN consists in a setof grammar networks, each of them containingseveral set of transformational rules.
COLIN andSYGFRAN?s rules are implemented with the parserSYGMART, a tree transducers system (Chauch?e,1984).
COLIN?s rules are split into several gram-mars including (i) a basic anaphora resolution, (ii)a tagging of candidate nodes4, (iii) a pruning oftagged constituents and a linearization of leaves.4 Evaluation, experimentation andresultsThis section sums up the validation process usedfor our approach.
Our evaluation protocol is man-ual intrinsic, focuses on facilitating the evalua-tor?
task and is inspired from (Knight and Marcu,2002)?s one.
For space reasons, we do not detailthe protocol here, a full description of the proto-col as well as the experimentation is available in(Yousfi-Monod, 2007).Setting up.
As our approach deeply relies onsyntactic properties, which are not always properlydetected by current parsers, we decided to manu-ally improve the syntactic analysis of our evalua-tion corpus.
Otherwise, the evaluation would havemore reflected the users?
satisfaction about parsingthan their opinion about the quality of our impor-tance criteria.
In order to assess the genre influ-3http://www.lirmm.fr4We tag trees before pruning them as COLIN can work ina semi-automatic mode (not presented here) where a user canmodify the tagging.ence, we selected three genres: Journalistic, narra-tive and scientific.
We composed 5 texts per gen-res, each of them contained about 5 paragraphs,16 sentences and 380 words, thus a total of 240sentences.
We decided to test the importance ofdifferent clause modifiers, i.e.
adverbial phrases,according to their type.
We considered the follow-ing types: Temporal, locative and other ones.
So,while keeping the core rules for each rules config-uration, we tested the removal of (i) all adverbials,(ii) temporal adverbials, (iii) locative adverbials,(iv) only other adverbials (keeping temporal andplace ones).We got 25 active users participating to the evalua-tion, who were mainly composed of PhD studentsand PhDs, working in NLP or computational lin-guistic domains, and being fluent in French.
Someof them did a set of manual compressions, used tocompare the quality with COLIN compressions inthe scoring stage of the evaluation.
59 text com-pressions were done, corresponding to about 3,9compressions per text.
In the scoring stage, judgesgave about 5,2 notations per compressed paragraphfor manual and automatic compressions.Results.
Tables 1 and 2 present the results ofrespectively obtained average compression rates5and paragraph scorings6, per genre.
For COLIN?sevaluation, we only display the rules configurationwhich has obtained the best results for the com-pression rate relatively to the paragraph scoring,i.e.
the rules configuration (iv).Jour.
Narr.
Scien.
MeanManual 36 % 17 % 23 % 25 %COLIN 38 % 35 % 41 % 38 %Table 1: Average compression rates.Jour.
Jour.
Scien.
MeanManual 4,03 3,67 3,41 3,7COLIN 3,7 3 3 3,23Table 2: Average paragraph scorings.The compression rate proposed by COLIN isquite better than the manual one for a quality scor-ing just below the latter.
COLIN is obviously5A higher percentage means a shorter compressed text.6Scores are between 1 and 5, a value of 1 means a com-pletely unsatisfying compression, while a value of 5 means avery good compression for the judge.141far better in compression time, with about 5 sec-onds per document versus between 200 and 300seconds for the manual compressions.
COLIN?scompression?quality?time ratio is therefore reallybetter than the manual compressions.
Each genreobtained a good compression rate as well as a cor-rect quality scoring, particularly for the journalis-tic one.
Note that our results could have been im-proved if they weren?t sensibly degraded becauseof an imperfect parsing, despite some focused im-provements we did on it.A performance comparison with similar ap-proaches was an issue for our approach for at leasttwo reasons: (i) As our parser is exclusively forFrench, we had to do comparisons with Frenchtongue systems only.
The system presented in(Gagnon and Sylva, 2005) is the only that matchesthis constraint.
(ii) Our evaluation protocol drasti-cally differs from traditional ones in several points:1.
Having a single human judge who compressessentences produces compressions which are toomuch subjective to the latter, that?s why each ofour texts were compressed about 4 times by dif-ferent humans.
Evaluating compressions rise thesame issue of subjectivity, so each of our compres-sions were evaluated about 5 times.
2.
We con-sider assessing the quality of separated compressedsentences is harder and less relevant for evaluatorsthan assessing full paragraphs as we did.
3.
Textgenre has an actual influence on NLP approaches,thus we took into account this factor in our eval-uation, as described above, while the above citedsystem extracted random sentences in a single cor-pus.
For all these reasons, we haven?t been able toperform a comparison with the above cited systemyet.5 ConclusionIn this paper we have addressed the task of sen-tence compression based on a deep linguistic anal-ysis.
The system we developed, called COLIN,theoretically relies on a constituents and depen-dencies sentence tree pruning, removing thosebranches which could be cut without jeopardiz-ing the sentence construction, or tempering toostrongly with the sentence meaning.
A carefulstudy of syntactic properties, lexical functions,verbs arguments has led us to design several differ-ent configurations in which the sentence compres-sion quality could degrade if compression goes toofar.
The appreciation of a compression quality hasbeen here demonstrated as a user satisfaction pro-tocol.
If COLIN has been able to shorten texts byan average 38%, humans were not able to removemore than 25%.
At the same time, the satisfactionmean score is 3.23 over 5, whereas the same usersattribute to human compressors a satisfaction meanscore of 3.7, really not so much more.ReferencesChauch?e, Jacques.
1984.
Un outil multidimensionnelde l?analyse du discours.
In Coling?84, pages 11?15.Chomsky, Noam.
1970.
Remarks on nominalization.In R. Jacobs and P. Rosenbaum (eds.)
Reading in En-glish Transformational Grammar, pages 184?221,Waltham: Ginn.Dorr, Bonnie, David Zajic, and Richard Schwartz.2003.
Hedge trimmer: A parse-and-trim approachto headline generation.
In In R. Radev & S.
Teufel(Eds.
), Proceedings of the HLT-NAACL 2003 Work-shop on Text Summarization.
Omnipress, pages 1?8.Gagnon, Michel and Lyne Da Sylva.
2005.
Textsummarization by sentence extraction and syntac-tic pruning.
In Computational Linguistics in theNorth East (CliNE?05), Universit?e du Qu?ebec enOutaouais, Gatineau, 26 August.Hovy, Eduard H., Chin-Yew Lin, and Liang Zhou.2005.
A be-based multi-document summarizer withsentence compression.
In the Multilingual Summa-rization Evaluation Workshop at the ACL 2005 con-ference.Knight, Kevin and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: a probabilistic ap-proach to sentence compression.
Artificial Intelli-gence archive, 139(1):91?107, July.Lin, Chin-Yew and Eduard H. Hovy.
2002.
Auto-mated multi-document summarization in neats.
Inthe DARPA Human Language Technology Confer-ence, pages 50?53.McKeown, K., D. Evans, A. Nenkova, R. Barzi-lay, V. Hatzivassiloglou, B. Schiffman, S. Blair-Goldensohn, J. Klavans, and S. Sigelman.
2002.The columbia multi-document summarizer for duc2002.Yousfi-Monod, Mehdi and Violaine Prince.
2005.
Au-tomatic summarization based on sentence morpho-syntactic structure: narrative sentences compres-sion.
In the 2nd International Workshop on Natu-ral Language Understanding and Cognitive Science(NLUCS 2005), pages 161?167, Miami/USA, May.Yousfi-Monod, Mehdi.
2007.
Compression automa-tique ou semi-automatique de textes par ?elagage desconstituants effac?ables : une approche interactive etind?ependante des corpus.
Ph.D. thesis, University ofMontpellier II, Montpellier, November.142
