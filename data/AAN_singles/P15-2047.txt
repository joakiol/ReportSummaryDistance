Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 285?290,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Dependency-Based Neural Network for Relation ClassificationYang Liu1,2?Furu Wei3Sujian Li1,2Heng Ji4Ming Zhou3Houfeng Wang1,21Key Laboratory of Computational Linguistics, Peking University, MOE, China2Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China3Microsoft Research, Beijing, China4Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA{cs-ly, lisujian, wanghf}@pku.edu.cn{furu, mingzhou}@microsoft.com jih@rpi.eduAbstractPrevious research on relation classificationhas verified the effectiveness of using de-pendency shortest paths or subtrees.
Inthis paper, we further explore how to makefull use of the combination of these de-pendency information.
We first proposea new structure, termed augmented de-pendency path (ADP), which is composedof the shortest dependency path betweentwo entities and the subtrees attached tothe shortest path.
To exploit the semanticrepresentation behind the ADP structure,we develop dependency-based neural net-works (DepNN): a recursive neural net-work designed to model the subtrees, anda convolutional neural network to capturethe most important features on the shortestpath.
Experiments on the SemEval-2010dataset show that our proposed methodachieves state-of-art results.1 IntroductionRelation classification aims to classify the seman-tic relations between two entities in a sentence.
Itplays a vital role in robust knowledge extractionfrom unstructured texts and serves as an interme-diate step in a variety of natural language process-ing applications.
Most existing approaches followthe machine learning based framework and focuson designing effective features to obtain betterclassification performance.The effectiveness of using dependency relation-s between entities for relation classification hasbeen reported in previous approaches (Bach andBadaskar, 2007).
For example, Suchanek et al(2006) carefully selected a set of features fromtokenization and dependency parsing, and extend-ed some of them to generate high order features?Contribution during internship at Microsoft Research.in different ways.
Culotta and Sorensen (2004)designed a dependency tree kernel and attachedmore information including Part-of-Speech tag,chunking tag of each node in the tree.
Interesting-ly, Bunescu and Mooney (2005) provided an im-portant insight that the shortest path between twoentities in a dependency graph concentrates mostof the information for identifying the relation be-tween them.
Nguyen et al (2007) developed theseideas by analyzing multiple subtrees with the guid-ance of pre-extracted keywords.
Previous workshowed that the most useful dependency informa-tion in relation classification includes the shortestdependency path and dependency subtrees.
Thesetwo kinds of information serve different functionsand their collaboration can boost the performanceof relation classification (see Section 2 for detailedexamples).
However, how to uniformly and ef-ficiently combine these two components is stillan open problem.
In this paper, we propose anovel structure named Augmented DependencyPath (ADP) which attaches dependency subtreesto words on a shortest dependency path and focuson exploring the semantic representation behindthe ADP structure.Recently, deep learning techniques have beenwidely used in exploring semantic representation-s behind complex structures.
This provides usan opportunity to model the ADP structure in aneural network framework.
Thus, we propose adependency-based framework where two neuralnetworks are used to model shortest dependencypaths and dependency subtrees separately.
Oneconvolutional neural network (CNN) is appliedover the shortest dependency path, because CNNis suitable for capturing the most useful features ina flat structure.
A recursive neural network (RN-N) is used for extracting semantic representationsfrom the dependency subtrees, since RNN is goodat modeling hierarchical structures.
To connectthese two networks, each word on the shortest285A thief who tried to steal the truck broke the ignition with screwdriver.det rcmodnsubj xcompaux dobj det dobjnsubjdetprep-withOn the Sabbath the priests broke the commandment with priestly work.detprep-ondet nsubjdetprep-withamoddobjS1:S2:Figure 1: Sentences and their dependency trees.broke prep-with screwdriverignitiondobjnsubjthiefdettheAdet(a) Augmented dependency path in S1.thebroke prep-with workcommandmentdobjdetnsubjpriestsSabbathprep-ondetthethedetpriestlyamod(b) Augmented dependency path in S2.Figure 2: Augmented dependency paths.path is combined with a representation generatedfrom its subtree, strengthening the semantic rep-resentation of the shortest path.
In this way, theaugmented dependency path is represented as acontinuous semantic vector which can be furtherused for relation classification.2 Problem Definition and MotivationThe task of relation classification can be definedas follows.
Given a sentence S with a pair ofentities e1and e2annotated, the task is to identifythe semantic relation between e1and e2in ac-cordance with a set of predefined relation classes(e.g., Content-Container, Cause-Effect).
For ex-ample, in Figure 2, the relation between two en-tities e1=thief and e2=screwdriver is Instrument-Agency.Bunescu and Mooney (2005) first used short-est dependency paths between two entities tocapture the predicate-argument sequences (e.g.,?thief?broke?screwdriver?
in Figure 2), whichprovide strong evidence for relation classification.As we observe, the shortest paths contain moreinformation and the subtrees attached to each nodeon the shortest path are not exploited enough.
Forexample, Figure 2a and 2b show two instanceswhich have similar shortest dependency paths butbelong to different relation classes.
Methods onlyusing the path will fail in this case.
However, wecan distinguish these two paths by virtue of theattached subtrees such as ?dobj?commandment?and ?dobj?ignition?.
Based on many observa-tions like this, we propose the idea that combinesthe subtrees and the shortest path to form a moreprecise structure for classifying relations.
Thiscombined structure is called ?augmented depen-dency path (ADP)?, as illustrated in Figure 2.Next, our goal is to capture the semantic repre-sentation of the ADP structure between two enti-ties.
We first adopt a recursive neural network tomodel each word according to its attached depen-dency subtree.
Based on the semantic informationof each word, we design a convolutional neuralnetwork to obtain salient semantic features on theshortest dependency path.3 Dependency-Based Neural NetworksIn this section, we will introduce how we use neu-ral network techniques and dependency informa-tion to explore the semantic connection betweentwo entities.
We dub our architecture of model-ing ADP structures as dependency-based neuralnetworks (DepNN).
Figure 3 illustrates DepNNwith a concrete example.
First, we associate eachword w and dependency relation r with a vectorrepresentation xw,xr?
Rdim.
For each wordw on the shortest dependency path, we developan RNN from its leaf words up to the root togenerate a subtree embedding cwand concatenatecwwith xwto serve as the final representation ofw.
Next, a CNN is designed to model the shortestdependency path based on the representation ofits words and relations.
Finally our frameworkcan efficiently represent the semantic connectionbetween two entities with consideration of morecomprehensive dependency information.3.1 Modeling Dependency SubtreeThe goal of modeling dependency subtrees is tofind an appropriate representation for the words onthe shortest path.
We assume that each word wcan be interpreted by itself and its children on thedependency subtree.
Then, for each word w on thesubtree, its word embedding xw?
Rdimand sub-tree representation cw?
Rdimcare concatenatedto form its final representation pw?
Rdim+dimc.For a word that does not have a subtree, we setits subtree representation as cLEAF.
The subtreerepresentation of a word is derived through trans-forming the representations of its children words.286Wdetpriests nsubj broke prep_with workthe comman-dament SabbaththepriestlyMax Over TimeSubtree EmbeddingsWindow ProcessingRecursiveNeural NetworkConvolutionalNeural NetworkShortest PathWdet Wdobj WamodW1Word EmbeddingSubtree EmbeddingWprep-onWdettheFigure 3: Illustration of Dependency-based NeuralNetworks.During the bottom-up construction of the subtree,each word is associated with a dependency rela-tion such as dobj as in Figure 3.
For each depen-dency relation r, we set a transformation matrixWr?
Rdimc?
(dim+dimc)which is learned duringtraining.
Then we can get,cw= f(?q?Children(w)WR(w,q)?
pq+ b) (1)pq= [xq, cq] (2)where R(w,q)denotes the dependency relation be-tween word w and its child word q.
This processcontinues recursively up to the root word on theshortest path.3.2 Modeling Shortest Dependency PathTo classify the semantic relation between two en-tities, we further explore the semantic representa-tion behind their shortest dependency path, whichcan be seen as a sequence of words and dependen-cy relations as the bold-font part in Figure 2.
Asthe convolutional neural network (CNN) is goodat capturing the salient features from a sequenceof objects, we design a CNN to tackle the shortestdependency path.A CNN contains a convolution operation overa window of object representations, followed bya pooling operation.
As we know, a word won the shortest path is associated with the repre-sentation pwthrough modeling the subtree.
Fora dependency relation r on the shortest path,we set its representation as a vector xr?Rdim.
As a sliding window is applied on thesequence, we set the window size as k. Forexample, when k = 3, the sliding windows ofa shortest dependency path with n words are:{[rsw1r1], [r1w2r2], .
.
.
, [rn?1wnre]} wherersand reare used to denote the beginning andend of a shortest dependency path between twoentities.We concatenate k neighboring words (or de-pendency relations) representations into a newvector.
Assume Xi?
Rdim?k+dimc?nwas theconcatenated representation of the i-th window,where nwis the number of words in one window.A convolution operation involves a filter W1?Rl?
(dim?k+dimc?nw), which operates on Xito pro-duce a new feature vector Liwith l dimensions,Li= W1Xi(3)where the bias term is ignored for simplicity.Then W1is applied to each possible windowin the shortest dependency path to produce afeature map: [L0, L1, L2, ?
?
?
].
Next, we adop-t the widely-used max-over-time pooling opera-tion (Collobert et al, 2011), which can retainthe most important features, to obtain the finalrepresentation L from the feature map.
That is,L = max(L0,L1,L2, .
.
.
).3.3 LearningLike other relation classification systems, we al-so incorporate some lexical level features suchas named entity tags and WordNet hypernyms,which prove useful to this task.
We concatenatethem with the ADP representation L to producea combined vector M .
We then pass M to afully connected softmax layer whose output isthe probability distribution y over relation labels.M = [L,LEX] (4)y = softmax(W2M + b2) (5)Then, the optimization objective is to minimizethe cross-entropy error between the ground-truthlabel vector and the softmax output.
Pa-rameters are learned using the back-propagationmethod (Rumelhart et al, 1988).4 ExperimentsWe compare DepNN against multiple baselines onSemEval-2010 dataset (Hendrickx et al, 2010).The training set includes 8000 sentences, andthe test set includes 2717 sentences.
There are 9287relation types, and each type has two directions.Instances which don?t fall in any of these classesare labeled as Other.
The official evaluation metricis the macro-averaged F1-score (excluding Other)and the direction is considered.
We use dependen-cy trees generated by the Stanford Parser (Kleinand Manning, 2003) with the collapsed option.4.1 Contributions of different componentsWe first show the contributions from differentcomponents of DepNN.
Two different kinds ofword embeddings for initialization are used in theexperiments.
One is the 50-d embeddings pro-vided by SENNA (Collobert et al, 2011).
Thesecond is the 200-d embeddings used in (Yu etal., 2014), trained on Gigaword with word2vec1.All the hyperparameters are set with 5-fold cross-validation.ModelF150-d 200-dbaseline (Path words) 73.8 75.5+Depedency relations 80.3 81.8+Attached subtrees 81.2 82.8+Lexical features 82.7 83.6Table 1: Performance of DepNN with differentcomponents.We start with a baseline model using a CNNwith only the words on the shortest path.
We thenadd dependency relations and attached subtrees.The results indicate that both parts are effectivefor relation classification.
The rich linguistic in-formation embedded in the dependency relationsand subtrees can on one hand, help distinguish dif-ferent functions of the same word, and on the otherhand infer an unseen word?s role in the sentence.Finally, the lexical features are added and DepNNachieves state-of-the-art results.4.2 Comparison with BaselinesIn this subsection, we compare DepNN with sev-eral baseline relation classification approaches.Here, DepNN and the baselines are all based onthe 200-d embeddings trained on Gigaword due tothe larger corpus and higher dimensions.SVM (Rink and Harabagiu, 2010): This is thetop performed system in SemEval-2010.
It utilizesmany external corpora to extract features from thesentence to build an SVM classifier.1https://code.google.com/p/word2vec/Model Additional Features F1SVMPOS, PropBank, morphological82.2WordNet, TextRunner, FrameNetdependency parse, etc.MV-RNN POS, NER, WordNet 81.82CNN WordNet 82.7FCM NER 83.0DT-RNN NER 73.1DepNNWordNet 83.0NER 83.6Table 2: Results on SemEval-2010 dataset withGigaword embeddings.MV-RNN (Socher et al, 2012): This modelfinds the path between the two entities in the con-stituent parse tree and then learns the distributedrepresentation of its highest node with a matrix foreach word to make the compositions specific.CNN: Zeng et al (2014) build a convolutionalmodel over the tokens of a sentence to learn thesentence level feature vector.
It uses a specialposition vector that indicates the relative distancesof current input word to two marked entities.FCM (Yu et al, 2014): FCM decomposes thesentence into substructures and extracts featuresfor each of them, forming substructure embed-dings.
These embeddings are combined by sum-pooling and input into a softmax classifier.DT-RNN (Socher et al, 2014) : This is anRNN for modeling dependency trees.
It combinesnode?s word embedding with its children througha linear combination but not a subtree embedding.We adapt the augmented dependency path into adependency subtree and apply DT-RNN.As shown in Table 2, DepNN achieves the bestresult (83.6) using NER features.
WordNet fea-tures can also improve the performance of DepN-N, but not as obvious as NER.
Yu et al (2014)had similar observations, since the larger numberof WordNet tags may cause overfitting.
SVMachieves a comparable result, though the qualityof feature engineering highly relies on human ex-perience and external NLP resources.
MV-RNNmodels the constituent parse trees with a recursiveprocedure and its F1-score is about 1.8 percentlower than DepNN.
Meanwhile, MVR-NN is veryslow to train, since each word is associated with amatrix.
Both CNN and FCM use features from thewhole sentence and achieve similar performance.DT-RNN is the worst of all baselines, though it2MV-RNN achieves a higher F1-score (82.7) on SENNAembeddings reported in the original paper.288also considers the information from shortest de-pendency paths and attached subtrees.
As we ana-lyze, shortest dependency paths and subtrees playdifferent roles in relation classification.
However,we can see that DT-RNN does not distinguish themodeling processes of shortest paths and subtrees.This phenomenon is also seen in a kernel-basedmethod (Wang, 2008), where the tree kernel per-forms worse than the shortest path kernel.
We alsolook into the DepNN model and find it can identifydifferent patterns of words and the dependencyrelations.
For example, in the Instrument-Agencyrelation, the word ?using?
and the dependency re-lation ?prep with?
are found playing a major role.5 ConclusionIn this paper, we propose to classify relationsbetween entities by modeling the augmented de-pendency path in a neural network framework.We present a novel approach, DepNN, to takingadvantages of both convolutional neural networkand recursive neural network to model this struc-ture.
Experiment results demonstrate that DepNNachieves state-of-the-art performance.AcknowledgmentsWe thank all the anonymous reviewers for theirinsightful comments.
This work was partially sup-ported by National Key Basic Research Programof China (No.
2014CB340504), National NaturalScience Foundation of China (No.
61273278 and61370117), and National Social Science Fund ofChina (No: 12&ZD227).
The correspondenceauthor of this paper is Sujian Li.ReferencesNguyen Bach and Sameer Badaskar.
2007.
A surveyon relation extraction.
Language Technologies Insti-tute, Carnegie Mellon University.Razvan C. Bunescu and Raymond J. Mooney.
2005.A Shortest Path Dependency Kernel for RelationExtraction.
In North American Chapter of the As-sociation for Computational Linguistics.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Aron Culotta and Jeffrey S. Sorensen.
2004.
De-pendency Tree Kernels for Relation Extraction.
InMeeting of the Association for Computational Lin-guistics, pages 423?429.Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Se-bastian Pad ok, Marco Pennacchiotti, Lorenza Ro-mano, and Stan Szpakowicz.
2010.
SemEval-2010Task 8: Multi-Way Classification of Semantic Rela-tions Between Pairs of Nominals.Dan Klein and Christopher D. Manning.
2003.
Ac-curate Unlexicalized Parsing.
In Meeting of the As-sociation for Computational Linguistics, pages 423?430.Dat PT Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka.2007.
Relation extraction from wikipedia usingsubtree mining.
In Proceedings of the National Con-ference on Artificial Intelligence, volume 22, page1414.
Menlo Park, CA; Cambridge, MA; London;AAAI Press; MIT Press; 1999.Bryan Rink and Sanda Harabagiu.
2010.
Utd: Clas-sifying semantic relations by combining lexical andsemantic resources.
In Proceedings of the 5th Inter-national Workshop on Semantic Evaluation, pages256?259, Uppsala, Sweden, July.
Association forComputational Linguistics.David E Rumelhart, Geoffrey E Hinton, and Ronald JWilliams.
1988.
Learning representations by back-propagating errors.
Cognitive modeling, 5.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic composi-tionality through recursive matrix-vector spaces.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211, Jeju Island, Korea, July.
Association forComputational Linguistics.Richard Socher, Andrej Karpathy, Quoc V Le, Christo-pher D Manning, and Andrew Y Ng.
2014.
Ground-ed compositional semantics for finding and describ-ing images with sentences.
Transactions of theAssociation for Computational Linguistics, 2:207?218.Fabian M Suchanek, Georgiana Ifrim, and GerhardWeikum.
2006.
Combining linguistic and statisticalanalysis to extract relations from web documents.In Proceedings of the 12th ACM SIGKDD interna-tional conference on Knowledge discovery and datamining, pages 712?717.
ACM.Mengqiu Wang.
2008.
A re-examination of dependen-cy path kernels for relation extraction.
In IJCNLP,pages 841?846.Mo Yu, Matthew Gormley, and Mark Dredze.
2014.Factor-based compositional embedding models.
InNIPS Workshop on Learning Semantics.Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,and Jun Zhao.
2014.
Relation classification viaconvolutional deep neural network.
In Proceedings289of COLING 2014, the 25th International Conferenceon Computational Linguistics: Technical Papers,pages 2335?2344, Dublin, Ireland, August.
DublinCity University and Association for ComputationalLinguistics.290
