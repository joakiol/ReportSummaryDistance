Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 88?92,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsImage and Collateral Text in Support of Auto-annotation and SentimentAnalysisPamela Zontone and Giulia BoatoUniversity of TrentoTrento, Italy.
{zontone|boato}@disi.unitn.itJonathon Hare and Paul LewisUniversity of SouthamptonSouthampton, United Kingdom{jsh2|phl}@ecs.soton.ac.ukStefan Siersdorfer and Enrico MinackL3S Research CentreHannover, Germany{siersdorfer|minack}@l3s.deAbstractWe present a brief overview of the wayin which image analysis, coupled withassociated collateral text, is being usedfor auto-annotation and sentiment analy-sis.
In particular, we describe our ap-proach to auto-annotation using the graph-theoretic dominant set clustering algo-rithm and the annotation of images withsentiment scores from SentiWordNet.
Pre-liminary results are given for both, and ourplanned work aims to explore synergiesbetween the two approaches.1 Automatic annotation of images usinggraph-theoretic clusteringRecently, graph-theoretic approaches have be-come popular in the computer vision field.
Thereexist different graph-theoretic clustering algo-rithms such as minimum cut, spectral clustering,dominant set clustering.
Among all these algo-rithms, the Dominant Set Clustering (DSC) is apromising graph-theoretic approach based on thenotion of a dominant set that has been proposedfor different applications, such as image segmen-tation (Pavan and Pelillo, 2003), video summariza-tion (Besiris et al, 2009), etc.
Here we describethe application of DSC to image annotation.1.1 Dominant Set ClusteringThe definition of Dominant Set (DS) was intro-duced in (Pavan and Pelillo, 2003).
Let us con-sider a set of data samples that have to be clus-tered.
These samples can be represented as anundirected edge-weighted (similarity) graph withno self-loops G = (V,E,w), where V = 1, .
.
.
, nis the vertex set, E ?
V ?
V is the edge set,and w : E ?
R?+ is the (positive) weight func-tion.
Vertices in G represent the data points,whereas edges represent neighborhood relation-ships, and finally edge-weights reflect similaritybetween pairs of linked vertices.
An n ?
n sym-metric matrix A = (aij), called affinity (or simi-larity) matrix, can be used to represent the graphG, where aij = w(i, j) if (i, j) ?
E, and aij = 0if i = j.
To define formally a Dominant Set, otherparameters have to be introduced.
Let S be a non-empty subset of vertices, with S ?
V , and i ?
S.The (average) weighted degree of i relative to S isdefined as:awdegS(i) =1|S|?j?Saijwhere |S| denotes the number of elements in S.It can be observed that awdeg{i}(i) = 0 for anyi ?
V .
If j /?
S we can define the parameter?S(i, j) = aij ?
awdegS(i) that is the similaritybetween nodes j and i with respect to the averagesimilarity between node i and its neighbors in S. Itcan be noted that ?
{i}(i, j) = aij , for all i, j ?
Vwith i 6= j.
Now, if i ?
S, the weight wS(i) of irelative to S is:wS(i) ={ 1 if |S| = 1Pj?S\{j} ?S\{i}(j, i)wS\{i}(j) otherwise.This is a recursive equation where to calculatewS(i) the weights of the set S\{i} are needed.
Wecan deduce that wS(i) is a measure of the overallsimilarity between the node i and the other nodesin S\{i}, considering the overall similarity amongthe nodes in S\{i}.
So, the total weight of S canbe defined as:W (S) =?i?SwS(i).A non-empty subset of vertices S ?
V such thatW (T ) > 0 for any non-empty T ?
S is definedas a dominant set if the following two conditions88are satisfied: 1.
?i ?
S, wS(i) > 0; and 2.?i 6?
S,wS?
{i}(i) < 0.
These conditions characterize theinternal homogeneity of the cluster and the exter-nal inhomogeneity of S. As a consequence of thisdefinition, a dominant set cluster can be derivedfrom a graph by means of a quadratic program (Pa-van and Pelillo, 2003).
Let x be an n-dimensionalvector, where n is the number of vertices of thegraph and its components indicate the presence ofnodes in the cluster.
Let A be the affinity matrix ofthe graph.
Let us consider the following standardquadratic program:max f(x) = xTAxs.t.
x ?
?
(1)where ?
= {x ?
0 and eT x = 1} is the standardsimplex of Rn.
If a point x?
?
?
is a local max-imum of f , and ?(x?)
= {i ?
V : x?i > 0} isthe support of x?, it can be shown that the support?(x?)
is a dominant set for the graph.
So, a dom-inant set can be derived by solving the equation(1).
The following iterative equation can be usedto solve (1):xi(t + 1) = xi(t)(Ax(t))ix(t)TAx(t)where t denotes the number of iterations.
To sum-marize the algorithm, a dominant set is found andremoved from the graph.
A second dominant clus-ter is extracted from the remaining part of thegraph, and so on.
This procedure finishes whenall the elements in the graph have been assigned toa cluster.1.2 Image annotation using DSCHere we present an approach to automatically an-notate images using the DSC algorithm.
In theinitialization phase (training) the image databaseis split into L smaller subsets, corresponding tothe different image categories or visual conceptsthat characterize the images in the database.
Inthis process only tags are exploited: an image isincluded in all subsets corresponding to its tags.Given a subset l, the corresponding affinity ma-trix Al is calculated and used by the DSC algo-rithm.
Following (Wang et al, 2008), the ele-ments of the affinity matrix Al = (aij) are de-fined as aij = e?w(i,j)/r2where w(i, j) representsthe similarity function between images i and j inthe considered subset l, and r > 0 is the scalingfactor used as an adjustment function that allowsthe control of clustering sensitivity.
We use theMPEG.7 descriptors (Sikora, 2001) as features forcomputing the similarity between images.
Follow-ing the DSC approach, we can construct all clus-ters of subset l with similar images, and associatethem with the tag of subset l.In the test phase, a new image is annotated asso-ciating to it the tag of the cluster that best matchesthe image.
To do this, we use a decision algo-rithm based on the computation of the MSE (MeanSquare Error), where for each cluster we derive afeature vector that represents all the images in thatcluster (e.g., the average of all the feature vectors).The tag of the cluster with smaller MSE is used forthe annotation.For our experiments, we consider a subset ofthe Corel database, that consists of 4287 imagesin 49 categories (L = 49).
The 10% of images ineach category have been randomly selected fromthe database and used only for testing.
In Fig-ure 1 we report the annotation accuracy results ob-tained on 15 different classes with optimal param-eter r = 0.2.
For some classes the accuracy isvery high, whereas for others the accuracy is verylow (under 30%).
The total annotation accuracyconsidering all the 49 classes is roughly 69%.In a second set of experiments we consider aset of 6531 images from the MIR Flickr database(Huiskes and Lew, 2008), where each image istagged with at least one of the chosen 30 visualconcepts (L = 30).
Images are characterized bymultiple tags associated to them, thus an image isincluded in all the corresponding subsets.
For test-ing we use 875 images.
To evaluate the annotationaccuracy we compare the automatically associatedtag with the user defined tags of that image.
In Fig-ure 1 we report the annotation accuracy obtainedfor the 30 different categories, with the optimal pa-rameter r = 0.2.
The total annotation accuracy isabout 87%.Further simulations are in progress to evaluatethe accuracy of multiple tags that can be associ-ated to the test set in the MIR Flickr database.
In-deed, our idea is to annotate the images consider-ing the other common tags of the images belong-ing to each cluster.2 Annotating SentimentIn the previous section we were concerned withannotating images with visual concepts, typicallyobject names or descriptors.
A separate strand of89Figure 1: Annotation accuracy for 15 classes of the Corel database (left) and for 30 classes of the MIRFlickr database (right).our work is concerned with opinion analysis inmultimedia information and the automatic identi-fication of sentiment.
The study of image indexingand retrieval in the library and information sciencefields has long recognized the importance of sen-timent in image retrieval (Jo?rgensen, 2003; Neal,2006).
It is only recently however, that researchersinterested in automated image analysis and re-trieval have become interested in the sentiment as-sociated with images (Wang and He, 2008).To date, investigations that have looked at theassociation between sentiment and image con-tent have been limited to small datasets (typicallymuch less than 1000) and rather specific, spe-cially designed image features.
Recently, we havestarted to explore how sentiment is related to im-age content using much more generic visual-termbased features and much larger datasets collectedwith the aid of lexical resources such as Senti-WordNet.2.1 SentiWordNet and Image DatabasesSentiWordNet (Esuli and Sebastiani, 2006) is alexical resource built on top of WordNet.
Word-Net (Fellbaum, 1998) is a thesaurus containingtextual descriptions of terms and relationships be-tween terms (examples are hypernyms: ?car?
is asubconcept of ?vehicle?
or synonyms: ?car?
de-scribes the same concept as ?automobile?).
Word-Net distinguishes between different part-of-speechtypes (verb, noun, adjective, etc.).
A synset inWordNet comprises all terms referring to the sameconcept (e.g., {car, automobile}).
In SentiWord-Net a triple of three senti-values (pos, neg, obj)(corresponding to positive, negative, or rather neu-tral sentiment flavor of a word respectively) areassigned to each WordNet synset (and, thus, toeach term in the synset).
The senti-values are inthe range of [0, 1] and sum up to 1 for each triple.For instance (pos, neg, obj) = (0.875, 0.0, 0.125)for the term ?good?
or (0.25, 0.375, 0.375) forthe term ?ill?.
Senti-values were partly createdby human assessors and partly automatically as-signed using an ensemble of different classifiers(see (Esuli, 2008) for an evaluation of these meth-ods).Popular social websites, such as Flickr, con-tain massive amounts of visual information in theform of photographs.
Many of these photographshave been collectively tagged and annotated bymembers of the respective community.
Recentlyin the image analysis community it has becomepopular to use Flickr as a resource for buildingdatasets to experiment with.
We have been explor-ing how we can crawl Flickr for images that havea strong (positive or negative) sentiment associ-ated with them.
Our initial explorations have beenbased around crawling Flickr for images taggedwith words that have very high positive or negativesentiment according to their SentiWordNet classi-fication.Our image dataset has been refined by assign-ing an overall sentiment value to each image basedon its textual metadata and discarding images withlow overall sentiment.
At the simplest level weuse a dictionary of clearly positive and negativeSentiWords, with which we assign a positive (+1)sentiment value if the text representation only con-90positivenegativeFigure 2: Top 16 most discriminative colours (from left to right) for positive and negative sentimentclasses.tains positive sentiment terms, and a negative (-1)sentiment value if it only contains negative senti-ment terms.
We discarded images with neither apositive nor negative score.
Currently we are alsoexploring more powerful ways to assign sentimentvalues to images.2.2 Combining Senti-values and VisualTermsIn the future we intend to exploit the use of tech-niques such as the one described in Section 1.2in order to develop systems that are able to pre-dict sentiment from image features.
However, as apreliminary study, we have performed some small-scale experiments on a collection of 10000 imagescrawled from Flickr in order to try and see whethera primitive visual-bag-of-terms (Sivic and Zisser-man, 2003; Hare and Lewis, 2005) can be asso-ciated with positive and negative sentiment valuesusing a linear Support Vector Machine and Sup-port Vector Regression.
The visual-term bag-of-words for the study was based upon a quantisationof each pixel in the images into a set of 64 dis-crete colours (i.e., each pixel corresponds to oneof 64 possible visual terms).
Our initial resultslook promising and indicate a considerable cor-relation between the visual bag-of-words and thesentiment scores.Discriminative Analysis of Visual Features.
Inour small-scale study we have also performedsome analysis in order to investigate which visual-term features are most predictive of the positiveand negative sentiment classes.
For this analysiswe have used the Mutual Information (MI) mea-sure (Manning and Schuetze, 1999; Yang and Ped-ersen, 1997) from information theory which canbe interpreted as a measure of how much the jointdistribution of features (colour-based visual-termsin our case) deviate from a hypothetical distribu-tion in which features and categories (?positive?and ?negative?
sentiment) are independent of eachother.Figure 2 illustrates the 16 most discriminativecolours for the positive and negative classes.
Thedominant visual-term features for positive senti-ment are dominated by earthy colours and skintones.
Conversely, the features for negative sen-timent are dominated by blue and green tones.Interestingly, this association can be explainedthrough intuition because it mirrors human per-ception of warm (positive) and cold (negative)colours.Currently we are working on expanding ourpreliminary experiments to a much larger imagedataset of over half a million images and incor-porating more powerful visual-term based imagefeatures.
In addition to seeking improved ways ofdetermining image sentiment for the training setwe are planning to combine the dominant set clus-tering approach to annotation presented in Sec-tion 1.2 with the sentiment annotation task of thissection and compare the combined approach withother state of the art approaches as a step towardsachieving robust image sentiment annotation.3 ConclusionsThe use of dominant set clustering as a basis forauto-annotation has shown promise on image col-lections from both Corel and from Flickr.
We havealso shown how that visual-term feature represen-tations show some promise as indicators of sen-timent in images.
In future work we plan to com-bine these approaches to provide better support foropinion analysis of multimedia web documents.AcknowledgmentsThis work was supported by the EuropeanUnion under the Seventh Framework Programme(FP7/2007-2013) project LivingKnowledge (FP7-IST-231126), and the LiveMemories project, gra-ciously funded by the Autonomous Province ofTrento (Italy).
The authors are also grateful to thecreators of Flickr for providing an API that canbe used in scientific evaluations and the broaderFlickr community for making images and meta-data available.91ReferencesD.
Besiris, A. Makedonas, G. Economou, and S. Fo-topoulos.
2009.
Combining graph connectivity anddominant set clustering for video summarization.Multimedia Tools and Applications, 44 (2):161?186.A.
Esuli and F. Sebastiani.
2006.
Sentiwordnet: Apublicly available lexical resource for opinion min-ing.
LREC, 6.Andrea Esuli.
2008.
Automatic Generation of Lexi-cal Resources for Opinion Mining: Models, Algo-rithms and Applications.
PhD in Information Engi-neering, PhD School ?Leonardo da Vinci?, Univer-sity of Pisa.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Jonathon S. Hare and Paul H. Lewis.
2005.
Onimage retrieval using salient regions with vector-spaces and latent semantics.
In Wee Kheng Leow,Michael S. Lew, Tat-Seng Chua, Wei-Ying Ma,Lekha Chaisorn, and Erwin M. Bakker, editors,CIVR, volume 3568 of LNCS, pages 540?549, Sin-gapore.
Springer.Mark J. Huiskes and Michael S. Lew.
2008.
The MIRFlickr Retrieval Evaluation.
In MIR ?08: Proceed-ings of the 2008 ACM International Conference onMultimedia Information Retrieval, New York, NY,USA.
ACM.Corinne Jo?rgensen.
2003.
Image Retrieval: Theoryand Research.
Scarecrow Press, Lanham, MD.C.
Manning and H. Schuetze.
1999.
Foundationsof Statistical Natural Language Processing.
MITPress.Diane Neal.
2006.
News Photography Image RetrievalPractices: Locus of Control in Two Contexts.
Ph.D.thesis, University of North Texas, Denton, TX.M.
Pavan and M. Pelillo.
2003.
A new graph-theoretic approach to clustering and segmentation.IEEE Conf.
Computer Vision and Pattern Recogni-tion, 1:145?152.Thomas Sikora.
2001.
The mpeg-7 visual standard forcontent description - an overview.
IEEE Trans.
Cir-cuits and Systems for Video Technology, 11 (6):262?282.J Sivic and A Zisserman.
2003.
Video google: A textretrieval approach to object matching in videos.
InICCV, pages 1470?1477, October.Weining Wang and Qianhua He.
2008.
A survey onemotional semantic image retrieval.
In ICIP, pages117?120, San Diego, USA.
IEEE.M.
Wang, Z. Ye, Y. Wang, and S. Wang.
2008.
Domi-nant sets clustering for image retrieval.
Signal Pro-cessing, 88 (11):2843?2849.Yiming Yang and Jan O. Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In ICML, pages 412?420.92
