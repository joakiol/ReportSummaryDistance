Proceedings of the Workshop on Statistical Machine Translation, pages 23?30,New York City, June 2006. c?2006 Association for Computational LinguisticsQuasi-Synchronous Grammars:Alignment by Soft Projection of Syntactic DependenciesDavid A. Smith and Jason EisnerDepartment of Computer ScienceCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218, USA{dasmith,eisner}@jhu.eduAbstractMany syntactic models in machine trans-lation are channels that transform onetree into another, or synchronous gram-mars that generate trees in parallel.
Wepresent a newmodel of the translation pro-cess: quasi-synchronous grammar (QG).Given a source-language parse tree T1, aQG defines a monolingual grammar thatgenerates translations of T1.
The treesT2 allowed by this monolingual gram-mar are inspired by pieces of substruc-ture in T1 and aligned to T1 at thosepoints.
We describe experiments learningquasi-synchronous context-free grammarsfrom bitext.
As with other monolinguallanguage models, we evaluate the cross-entropy of QGs on unseen text and showthat a better fit to bilingual data is achievedby allowing greater syntactic divergence.When evaluated on a word alignment task,QG matches standard baselines.1 Motivation and Related Work1.1 Sloppy Syntactic AlignmentThis paper proposes a new type of syntax-basedmodel for machine translation and alignment.
Thegoal is to make use of syntactic formalisms, such ascontext-free grammar or tree-substitution grammar,without being overly constrained by them.Let S1 and S2 denote the source and target sen-tences.
We seek to model the conditional probabilityp(T2, A | T1) (1)where T1 is a parse tree for S1, T2 is a parse treefor S2, and A is a node-to-node alignment betweenthem.
This model allows one to carry out a varietyof alignment and decoding tasks.
Given T1, one cantranslate it by finding the T2 and A that maximize(1).
Given T1 and T2, one can align them by findingthe A that maximizes (1) (equivalent to maximizingp(A | T2, T1)).
Similarly, one can align S1 and S2by finding the parses T1 and T2, and alignment A,that maximize p(T2, A | T1) ?
p(T1 | S1), wherep(T1 | S1) is given by a monolingual parser.
Weusually accomplish such maximizations by dynamicprogramming.Equation (1) does not assume that T1 and T2 areisomorphic.
For example, a model might judge T2and A to be likely, given T1, provided that many?but not necessarily all?of the syntactic dependen-cies in T1 are aligned with corresponding depen-dencies in T2.
Hwa et al (2002) found that hu-man translations from Chinese to English preservedonly 39?42% of the unlabeled Chinese dependen-cies.
They increased this figure to 67% by usingmore involved heuristics for aligning dependenciesacross these two languages.
That suggests that (1)should be defined to consider more than one depen-dency at a time.This inspires the key novel feature of our models:A does not have to be a ?well-behaved?
syntacticalignment.
Any portion of T2 can align to any por-tion of T1, or to NULL.
Nodes that are syntacticallyrelated in T1 do not have to translate into nodes thatare syntactically related in T2?although (1) is usu-ally higher if they do.This property makes our approach especiallypromising for aligning freely, or erroneously, trans-lated sentences, and for coping with syntactic diver-23gences observed between even closely related lan-guages (Dorr, 1994; Fox, 2002).
We can patch to-gether an alignment without accounting for all thedetails of the translation process.
For instance, per-haps a source NP (figure 1) or PP (figure 2) appears?out of place?
in the target sentence.
A linguistmight account for the position of the PP auf dieseFrage either syntactically (by invoking scrambling)or semantically (by describing a deep analysis-transfer-synthesis process in the translator?s head).But an MT researcher may not have the wherewithalto design, adequately train, and efficiently computewith ?deep?
accounts of this sort.
Under our ap-proach, it is possible to use a simple, tractable syn-tactic model, but with some contextual probabilityof ?sloppy?
transfer.1.2 From Synchronous to Quasi-SynchronousGrammarsBecause our approach will let anything align toanything, it is reminiscent of IBM Models 1?5(Brown et al, 1993).
It differs from the many ap-proaches where (1) is defined by a stochastic syn-chronous grammar (Wu, 1997; Alshawi et al, 2000;Yamada and Knight, 2001; Eisner, 2003; Gildea,2003; Melamed, 2004) and from transfer-based sys-tems defined by context-free grammars (Lavie et al,2003).The synchronous grammar approach, originallydue to Shieber and Schabes (1990), supposes that T2is generated in lockstep to T1.1 When choosing howto expand a certain VP node in T2, a synchronousCFG process would observe that this node is alignedto a node VP?
in T1, which had been expanded in T1by VP?
?
NP?
V?.
This might bias it toward choos-ing to expand the VP in T2 as VP ?
V NP, with thenew children V aligned to V?
and NP aligned to NP?.The process then continues recursively by choosingmoves to expand these children.One can regard this stochastic process as an in-stance of analysis-transfer-synthesis MT.
Analysischooses a parse T1 given S1.
Transfer maps thecontext-free rules in T1 to rules of T2.
Synthesis1The usual presentation describes a process that generatesT1 and T2 jointly, leading to a joint model p(T2, A, T1).
Divid-ing by the marginal p(T1) gives a conditional model p(T2, A |T1) as in (1).
In the text, we directly describe an equivalentconditional process for generating T2, A given T1.deterministically assembles the latter rules into anactual tree T2 and reads off its yield S2.What is worrisome about the synchronous pro-cess is that it can only produce trees T2 that areperfectly isomorphic to T1.
It is possible to relaxthis requirement by using synchronous grammar for-malisms more sophisticated than CFG:2 one can per-mit unaligned nodes (Yamada and Knight, 2001),duplicated children (Gildea, 2003)3, or alignmentbetween elementary trees of differing sizes ratherthan between single rules (Eisner, 2003; Ding andPalmer, 2005; Quirk et al, 2005).
However, onewould need rather powerful and slow grammar for-malisms (Shieber and Schabes, 1990; Melamed etal., 2004), often with discontiguous constituents, toaccount for all the linguistic divergences that couldarise from different movement patterns (scrambling,wh-in situ) or free translation.
In particular, a syn-chronous grammar cannot practically allow S2 to beany permutation of S1, as IBM Models 1?5 do.Our alternative is to define a ?quasi-synchronous?stochastic process.
It generates T2 in a way that isnot in thrall to T1 but is ?inspired by it.?
(A humantranslator might be imagined to behave similarly.
)When choosing how to expand nodes of T2, we areinfluenced both by the structure of T1 and by mono-lingual preferences about the structure of T2.
Just asconditional Markov models can more easily incor-porate global features than HMMs, we can look atthe entire tree T1 at every stage in generating T2.2 Quasi-Synchronous GrammarGiven an input S1 or its parse T1, a quasi-synchronous grammar (QG) constructs a monolin-gual grammar for parsing, or generating, the possi-ble translations S2?that is, a grammar for findingappropriate trees T2.
What ties this target-languagegrammar to the source-language input?
The gram-mar provides for target-language words to take on2When one moves beyond CFG, the derived trees T1 andT2 are still produced from a single derivation tree, but may beshaped differently from the derivation tree and from each other.3For tree-to-tree alignment, Gildea proposed a clone opera-tion that allowed subtrees of the source tree to be reused in gen-erating a target tree.
In order to preserve dynamic programmingconstraints, the identity of the cloned subtree is chosen indepen-dently of its insertion point.
This breakage of monotonic treealignment moves Gildea?s alignment model from synchronousto quasi-synchronous.24Then:1 we:2could:3deal:4 .
:10with:5 later:9Chernobyl:6some:7time:8Tschernobyl/NE:6koennte/VVFIN:3dann/ADV:1 etwas/ADV:0 spaeter/ADJ:1 an/PREP:0 kommen/VVINF:0 ./S-SYMBOL:10Reihe/NN:0die/ART:0Figure 1: German and English dependency parses and their alignments from our system where Germanis the target language.
Tschernobyl depends on ko?nnte even though their English analogues are not in adependency relationship.
Note the parser?s error in not attaching etwas to spa?ter.German: Tschernobyl ko?nnte dann etwas spa?ter an die Reihe kommen .Literally: Chernobyl could then somewhat later on the queue come.English: Then we could deal with Chernobyl some time later .I:1did:2not:3 unfortunately:4 receive:5 .
:11answer:7an:6 to:8question:10this:9Auf/PREP:8Frage/NN:10diese/DEM:9habe/VHFIN:2 ich/PPRO:1 leider/ADV:4keine/INDEF:3Antwort/NN:7bekommen/VVpast:5./S-SYMBOL:11Figure 2: Here the German sentence exhibits scrambling of the phrase auf diese Frage and negates the objectof bekommen instead of the verb itself.German: Auf diese Frage habe ich leider keine Antwort bekommen .Literally: To this question have I unfortunately no answer received.English: I did not unfortunately receive an answer to this question .25multiple hidden ?senses,?
which correspond to (pos-sibly empty sets of) word tokens in S1 or nodes inT1.
To take a familiar example, when parsing theEnglish side of a French-English bitext, the wordbank might have the sense banque (financial) in onesentence and rive (littoral) in another.The QG4 considers the ?sense?
of the former banktoken to be a pointer to the particular banque tokento which it aligns.
Thus, a particular assignment ofS1 ?senses?
to word tokens in S2 encodes a wordalignment.Now, selectional preferences in the monolingualgrammar can be influenced by these T1-specificsenses.
So they can encode preferences for how T2ought to copy the syntactic structure of T1.
For ex-ample, if T1 contains the phrase banque nationale,then the QG for generating a corresponding T2 mayencourage any T2 English noun whose sense isbanque (more precisely, T1?s token of banque) togenerate an adjectival English modifier with sensenationale.
The exact probability of this, as well asthe likely identity and position of that English mod-ifier (e.g., national bank), may also be influenced bymonolingual facts about English.2.1 DefinitionA quasi-synchronous grammar is a monolingualgrammar that generates translations of a source-language sentence.
Each state of this monolingualgrammar is annotated with a ?sense?
?a set of zeroor more nodes from the source tree or forest.For example, consider a quasi-synchronouscontext-free grammar (QCFG) for generating trans-lations of a source tree T1.
The QCFG generates thetarget sentence using nonterminals from the crossproduct U ?
2V1 , where U is the set of monolingualtarget-language nonterminals such as NP, and V1 isthe set of nodes in T1.Thus, a binarized QCFG has rules of the form?A,??
?
?B, ??
?C, ??
(2)?A,??
?
w (3)where A,B,C ?
U are ordinary target-languagenonterminals, ?, ?, ?
?
2V1 are sets of source tree4By abuse of terminology, we often use ?QG?
to refer to theT1-specific monolingual grammar, although the QG is properlya recipe for constructing such a grammar from any input T1.nodes to which A,B,C respectively align, and w isa target-language terminal.Similarly, a quasi-synchronous tree-substitutiongrammar (QTSG) annotates the root and frontiernodes of its elementary trees with sets of sourcenodes from 2V1 .2.2 Taming Source NodesThis simple proposal, however, presents two maindifficulties.
First, the number of possible senses foreach target node is exponential in the number ofsource nodes.
Second, note that the senses are setsof source tree nodes, not word types or absolute sen-tence positions as in some other translation models.Except in the case of identical source trees, sourcetree nodes will not recur between training and test.To overcome the first problem, we want further re-strictions on the set ?
in a QG state such as ?A,??.
Itshould not be an arbitrary set of source nodes.
In theexperiments of this paper, we adopt the simplest op-tion of requiring |?| ?
1.
Thus each node in the tar-get tree is aligned to a single node in the source tree,or to ?
(the traditional NULL alignment).
This allowsone-to-many but not many-to-one alignments.To allow many-to-many alignments, one couldlimit |?| to at most 2 or 3 source nodes, perhaps fur-ther requiring the 2 or 3 source nodes to fall in a par-ticular configuration within the source tree, such aschild-parent or child-parent-grandparent.
With thatconfigurational requirement, the number of possi-ble senses ?
remains small?at most three times thenumber of source nodes.We must also deal with the menagerie of differ-ent source tree nodes in different sentences.
In otherwords, how can we tie the parameters of the differentQGs that are used to generate translations of differ-ent source sentences?
The answer is that the proba-bility or weight of a rule such as (2) should dependon the specific nodes in ?, ?, and ?
only throughtheir properties?e.g., their nonterminal labels, theirhead words, and their grammatical relationship inthe source tree.
Such properties do recur betweentraining and test.For example, suppose for simplicity that |?| =|?| = |?| = 1.
Then the rewrite probabilities of (2)and (3) could be log-linearly modeled using featuresthat ask whether the single node in ?
has two chil-dren in the source tree; whether its children in the26source are the nodes in ?
and ?
; whether its non-terminal label in the source is A; whether its fringein the source translates as w; and so on.
The modelshould also consider monolingual features of (2) and(3), evaluating in particular whether A ?
BC islikely in the target language.Whether rule weights are given by factored gener-ative models or by naive Bayes or log-linear models,we want to score QG productions with a small set ofmonolingual and bilingual features.2.3 Synchronous Grammars AgainFinally, note that synchronous grammar is a specialcase of quasi-synchronous grammar.
In the context-free case, a synchronous grammar restricts senses tosingle nodes in the source tree and the NULL node.Further, for any k-ary production?X0, ?0?
?
?X1, ?1?
.
.
.
?Xk, ?k?a synchronous context-free grammar requires that1.
(?i 6= j) ?i 6= ?j unless ?i = NULL,2.
(?i > 0) ?i is a child of ?0 in the source tree,unless ?i = NULL.Since NULL has no children in the source tree, theserules imply that the children of any node aligned toNULL are themselves aligned to NULL.
The con-struction for synchronous tree-substitution and tree-adjoining grammars goes through similarly but op-erates on the derivation trees.3 Parameterizing a QCFGRecall that our goal is a conditional model ofp(T2, A | T1).
For the remainder of this paper, weadopt a dependency-tree representation of T1 andT2.
Each tree node represents a word of the sentencetogether with a part-of-speech tag.
Syntactic depen-dencies in each tree are represented directly by theparent-child relationships.Why this representation?
First, it helps us con-cisely formulate a QG translation model where thesource dependencies influence the generation of tar-get dependencies (see figure 3).
Second, for evalu-ation, it is trivial to obtain the word-to-word align-ments from the node-to-node alignments.
Third, thepart-of-speech tags are useful backoff features, andin fact play a special role in our model below.When stochastically generating a translation T2,our quasi-synchronous generative process will be in-fluenced by both fluency and adequacy.
That is, itconsiders both the local well-formedness of T2 (amonolingual criterion) and T2?s local faithfulnessto T1 (a bilingual criterion).
We combine these ina simple generative model rather than a log-linearmodel.
When generating the children of a node inT2, the process first generates their tags using mono-lingual parameters (fluency), and then fills in in thewords using bilingual parameters (adequacy) that se-lect and translate words from T1.5Concretely, each node in T2 is labeled by a triple(tag, word, aligned word).
Given a parent node(p, h, h?)
in T2, we wish to generate sequences ofleft and right child nodes, of the form (c, a, a?
).Our monolingual parameters come from a simplegenerative model of syntax used for grammar induc-tion: the Dependency Model with Valence (DMV) ofKlein and Manning (2004).
In scoring dependencyattachments, DMV uses tags rather than words.
Theparameters of the model are:1. pchoose(c | p, dir): the probability of generat-ing c as the next child tag in the sequence ofdir children, where dir ?
{left, right}.2. pstop(s | h, dir, adj): the probability of gener-ating no more child tags in the sequence of dirchildren.
This is conditioned in part on the ?ad-jacency?
adj ?
{true, false}, which indicateswhether the sequence of dir children is emptyso far.Our bilingual parameters score word-to-wordtranslation and aligned dependency configurations.We thus use the conditional probability ptrans(a |a?)
that source word a?, which may be NULL, trans-lates as target word a.
Finally, when a parent wordh aligned to h?
generates a child, we stochasticallydecide to align the child to a node a?
in T1 withone several possible relations to h?.
A ?monotonic?dependency alignment, for example, would haveh?
and a?
in a parent-child relationship like theirtarget-tree analogues.
In different versions of themodel, we allowed various dependency alignmentconfigurations (figure 3).
These configurations rep-5This division of labor is somewhat artificial, and could beremedied in a log-linear model, Naive Bayes model, or defi-cient generative model that generates both tags and words con-ditioned on both monolingual and bilingual context.27resent cases where the parent-child dependency be-ing generated by the QG in the target language mapsonto source-language child-parent, for head swap-ping; the same source node, for two-to-one align-ment; nodes that are siblings or in a c-command re-lationship, for scrambling and extraposition; or ina grandparent-grandchild relationship, e.g.
when apreposition is inserted in the source language.
Wealso allowed a ?none-of-the-above?
configuration, toaccount for extremely mismatched sentences.The probability of the target-language depen-dency treelet rooted at h is thus:P (D(h) | h, h?, p) =?dir?
{l,r}?c?depsD(p,dir)P (D(c) | a, a?, c) ?
pstop(nostop | p, dir, adj)?pchoose(c | p, dir)?pconfig(config) ?
ptrans(a | a?
)pstop(stop | p, dir, adj)4 ExperimentsWe claim that for modeling human-translated bitext,it is better to project syntax only loosely.
To evaluatethis claim, we train quasi-synchronous dependencygrammars that allow progressively more divergencefrom monotonic tree alignment.
We evaluate thesemodels on cross-entropy over held-out data and onerror rate in a word-alignment task.One might doubt the use of dependency treesfor alignment, since Gildea (2004) found that con-stituency trees aligned better.
That experiment, how-ever, aligned only the 1-best parse trees.
We too willconsider only the 1-best source tree T1, but in con-strast to Gildea, we will search for the target tree T2that aligns best with T1.
Finding T2 and the align-ment is simply a matter of parsing S2 with the QGderived from T1.4.1 Data and TrainingWe performed our modeling experiments with theGerman-English portion of the Europarl EuropeanParliament transcripts (Koehn, 2002).
We obtainedmonolingual parse trees from the Stanford Germanand English parsers (Klein and Manning, 2003).Initial estimates of lexical translation probabilitiescame from the IBM Model 4 translation tables pro-duced by GIZA++ (Brown et al, 1993; Och andNey, 2003).All text was lowercased and numbers of two ormore digits were converted to an equal number ofhash signs.
The bitext was divided into trainingsets of 1K, 10K, and 100K sentence pairs.
We heldout one thousand sentences for evaluating the cross-entropy of the various models and hand-aligned100 sentence pairs to evaluate alignment error rate(AER).We trained the model parameters on bitext usingthe Expectation-Maximization (EM) algorithm.
TheT1 tree is fully observed, but we parse the target lan-guage.
As noted, the initial lexical translation proba-bilities came from IBM Model 4.
We initialized themonolingual DMV parameters in one of two ways:using either simple tag co-occurrences as in (KleinandManning, 2004) or ?supervised?
counts from themonolingual target-language parser.
This latter ini-tialization simulates the condition when one has asmall amount of bitext but a larger amount of tar-get data for language modeling.
As with any mono-lingual grammar, we perform EM training with theInside-Outside algorithm, computing inside prob-abilities with dynamic programming and outsideprobabilities through backpropagation.Searching the full space of target-language depen-dency trees and alignments to the source tree con-sumed several seconds per sentence.
During train-ing, therefore, we constrained alignments to comefrom the union of GIZA++ Model 4 alignments.These constraints were applied only during trainingand not during evaluation of cross-entropy or AER.4.2 Conditional Cross-Entropy of the ModelTo test the explanatory power of our QCFG, we eval-uated its conditional cross-entropy on held-out data(table 1).
In other words, we measured how well atrained QCFG could predict the true translation ofnovel source sentences by summing over all parsesof the target given the source.
We trained QCFGmodels under different conditions of bitext size andparameter initialization.
However, the principal in-dependent variable was the set of dependency align-ment configurations allowed.From these cross-entropy results, it is clear thatstrictly synchronous grammar is unwise.
We ob-28(a) parent-child (b) child-parent (c) same nodeseheichseeIschwimmtgern swimminglikes Voelkerrecht lawinternational(d) siblings (e) grandparent-grandchild (f) c-commandbekommenauf Antwort toanswer Wahlkampfvoncampaign20032003sagteWas dass whatkaufteboughtFigure 3: When a head h aligned to h?
generates a new child a aligned to a?
under the QCFG, h?
and a?
may be related in thesource tree as, among other things, (a) parent?child, (b) child?parent, (c) identical nodes, (d) siblings, (e) grandparent?grandchild,(f) c-commander?c-commandee, (g) none of the above.
Here German is the source and English is the target.
Case (g), not picturedabove, can be seen in figure 1, in English-German order, where the child-parent pair Tschernobyl ko?nnte correspond to the wordsChernobyl and could, respectively.
Since could dominates Chernobyl, they are not in a c-command relationship.Permitted configurations CE CE CEat 1k 10k 100k?
or parent-child (a) 43.82 22.40 13.44+ child-parent (b) 41.27 21.73 12.62+ same node (c) 41.01 21.50 12.38+ all breakages (g) 35.63 18.72 11.27+ siblings (d) 34.59 18.59 11.21+ grandparent-grandchild (e) 34.52 18.55 11.17+ c-command (f) 34.46 18.59 11.27No alignments allowed 60.86 53.28 46.94Table 1: Cross-entropy on held-out data with different depen-dency configurations (figure 3) allowed, for 1k, 10k, and 100ktraining sentences.
The big error reductions arrive when weallow arbitrary non-local alignments in condition (g).
Distin-guishing some common cases of non-local alignments improvesperformance further.
For comparison, we show cross-entropywhen every target language node is unaligned.tain comparatively poor performance if we requireparent-child pairs in the target tree to align to parent-child pairs in the source (or to parent-NULL orNULL-NULL).
Performance improves as we allowand distinguish more alignment configurations.4.3 Word AlignmentWe computed standard measures of alignment preci-sion, recall, and error rate on a test set of 100 hand-aligned German sentence pairs with 1300 alignmentlinks.
As with many word-alignment evaluations,we do not score links to NULL.
Just as for cross-entropy, we see that more permissive alignmentslead to better performance (table 2).Having selected the best system using the cross-entropy measurement, we compare its alignment er-ror rate against the standard GIZA++ Model 4 base-lines.
As Figure 4 shows, our QCFG for German ?English consistently produces better alignments thanthe Model 4 channel model for the same direction,German ?
English.
This comparison is the appro-priate one because both of these models are forcedto align each English word to at most one Germanword.
65 ConclusionsWith quasi-synchronous grammars, we have pre-sented a new approach to syntactic MT: construct-ing a monolingual target-language grammar that de-scribes the aligned translations of a source-languagesentence.
We described a simple parameterization6For German ?
English MT, one would use a German ?English QCFG as above, but an English ?
German channelmodel.
In this arguably inappropriate comparison, Figure 4shows, the Model 4 channel model produces slightly betterword alignments than the QG.29Permitted configurations AER AER AERat 1k 10k 100k?
or parent-child (a) 40.69 39.03 33.62+ child-parent (b) 43.17 39.78 33.79+ same node (c) 43.22 40.86 34.38+ all breakages (g) 37.63 30.51 25.99+ siblings (d) 37.87 33.36 29.27+ grandparent-grandchild (e) 36.78 32.73 28.84+ c-command (f) 37.04 33.51 27.45Table 2: Alignment error rate (%) with different dependencyconfigurations allowed.0.20.250.30.350.40.450.51000  10000  100000  1e+06alignmenterror ratetraining sentence pairsQCFGGiza4Giza4 bkFigure 4: Alignment error rate with best model (all break-ages).
The QCFG consistently beat one GIZA++ model andwas close to the other.with gradually increasing syntactic domains of lo-cality, and estimated those parameters on German-English bitext.The QG formalism admits many more nuancedoptions for features than we have exploited.
In par-ticular, we now are exploring log-linear QGs thatscore overlapping elementary trees of T2 while con-sidering the syntactic configuration and lexical con-tent of the T1 nodes to which each elementary treealigns.Even simple QGs, however, turned out to do quitewell.
Our evaluation on a German-English word-alignment task showed them to be competitive withIBM model 4?consistently beating the German-English direction by several percentage points ofalignment error rate and within 1% AER of theEnglish-German direction.
In particular, alignmentaccuracy benefited from allowing syntactic break-ages between the two dependency structures.We are also working on a translation decoding us-ing QG.
Our first system uses the QG to find optimalT2 aligned to T1 and then extracts a synchronoustree-substitution grammar from the aligned trees.Our second system searches a target-language vo-cabulary for the optimal T2 given the input T1.AcknowledgementsThis work was supported by a National ScienceFoundation Graduate Research Fellowship for thefirst author and by NSF Grant No.
0313193.ReferencesH.
Alshawi, S. Bangalore, and S. Douglas.
2000.
Learningdependency translation models as collections of finite statehead transducers.
CL, 26(1):45?60.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
CL, 19(2):263?311.Y.
Ding and M. Palmer.
2005.
Machine translation using prob-abilistic synchronous dependency insertion grammars.
InACL, pages 541?548.B.
J. Dorr.
1994.
Machine translation divergences: A formaldescription and proposed solution.
Computational Linguis-tics, 20(4):597?633.J.
Eisner.
2003.
Learning non-isomorphic tree mappings formachine translation.
In ACL Companion Vol.H.
J.
Fox.
2002.
Phrasal cohesion and statistical machine trans-lation.
In EMNLP, pages 392?399.D.
Gildea.
2003.
Loosely tree-based alignment for machinetranslation.
In ACL, pages 80?87.D.
Gildea.
2004.
Dependencies vs. constituents for tree-basedalignment.
In EMNLP, pages 214?221.R.
Hwa, P. Resnik, A. Weinberg, and O. Kolak.
2002.
Evalu-ating translational correspondence using annotation projec-tion.
In ACL.D.
Klein and C. D. Manning.
2003.
Accurate unlexicalizedparsing.
In ACL, pages 423?430.D.
Klein and C. D. Manning.
2004.
Corpus-based induction ofsyntactic structure: Models of dependency and constituency.In ACL, pages 479?486.P.
Koehn.
2002.
Europarl: A multilingualcorpus for evaluation of machine translation.http://www.iccs.informatics.ed.ac.uk/?pkoehn/-publications/europarl.ps.A.
Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst, A. F.Llitjo?s, R. Reynolds, J. Carbonell, and R. Cohen.
2003.
Ex-periments with a Hindi-to-English transfer-basedMT systemunder a miserly data scenario.
ACM Transactions on AsianLanguage Information Processing, 2(2):143 ?
163.I.
D. Melamed, G. Satta, and B. Wellington.
2004.
Generalizedmultitext grammars.
In ACL, pages 661?668.I.
D. Melamed.
2004.
Statistical machine translation by pars-ing.
In ACL, pages 653?660.F.
J. Och and H. Ney.
2003.
A systematic comparison of variousstatistical alignment models.
CL, 29(1):19?51.C.
Quirk, A. Menezes, and C. Cherry.
2005.
Dependencytreelet translation: Syntactically informed phrasal SMT.
InACL, pages 271?279.S.
M. Shieber and Y. Schabes.
1990.
Synchronous tree-adjoining grammars.
In ACL, pages 253?258.D.
Wu.
1997.
Stochastic inversion transduction grammars andbilingual parsing of parallel corpora.
CL, 23(3):377?403.K.
Yamada and K. Knight.
2001.
A syntax-based statisticaltranslation model.
In ACL.30
