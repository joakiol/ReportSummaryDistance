Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1651?1660,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMorphological Smoothing and Extrapolation of Word EmbeddingsRyan CotterellDepartment of Computer ScienceJohns Hopkins University, USAryan.cotterell@jhu.eduHinrich Sch?utzeCISLMU Munich, Germanyinquiries@cis.lmu.orgJason EisnerDepartment of Computer ScienceJohns Hopkins University, USAjason@cs.jhu.eduAbstractLanguages with rich inflectional morphol-ogy exhibit lexical data sparsity, since theword used to express a given concept willvary with the syntactic context.
For in-stance, each count noun in Czech has 12forms (where English uses only singular andplural).
Even in large corpora, we are un-likely to observe all inflections of a givenlemma.
This reduces the vocabulary cover-age of methods that induce continuous rep-resentations for words from distributionalcorpus information.
We solve this prob-lem by exploiting existing morphologicalresources that can enumerate a word?s com-ponent morphemes.
We present a latent-variable Gaussian graphical model that al-lows us to extrapolate continuous represen-tations for words not observed in the train-ing corpus, as well as smoothing the repre-sentations provided for the observed words.The latent variables represent embeddingsof morphemes, which combine to create em-beddings of words.
Over several languagesand training sizes, our model improves theembeddings for words, when evaluated onan analogy task, skip-gram predictive accu-racy, and word similarity.1 IntroductionRepresentations of words as high-dimensional realvectors have been shown to benefit a wide varietyof NLP tasks.
Because of this demonstrated utility,many aspects of vector representations have beenexplored recently in the literature.
One of the mostinteresting discoveries is that these representationscapture meaningful morpho-syntactic and seman-tic properties through very simple linear relations:in a semantic vector space, we observe thatvtalked?
vtalk?
vdrank?
vdrink.
(1)That this equation approximately holds acrossmany morphologically related 4-tuples indicatesbebieroncomieronbebemoscomemosFigure 1: A visual depiction of the vector offset method formorpho-syntactic analogies in R2.
We expect bebieron andbebemos to have the same relation (vector offset shown assolid vector) as comieron and comemos.that the learned embeddings capture a featureof English morphology?adding the past tensefeature roughly corresponds to adding a certainvector.
Moreover, manipulating this equationyields what we will call the vector offset method(Mikolov et al, 2013c) for approximating othervectors.
For instance, if we only know the vectorsfor the Spanish words comieron (ate), comemos(eat) and bebieron (drank), we can produce an ap-proximation of the vector for bebemos (drink), asshown in Figure 1.Many languages exhibit much richer morphol-ogy than English.
While English nouns com-monly take two forms ?
singular and plural?Czech nouns take 12 and Turkish nouns take over30.
This increase in word forms per lemma createsconsiderable data sparsity.
Fortunately, for manylanguages there exist large morphological lexi-cons, or better yet, morphological tools that cananalyze any word form?meaning that we haveanalyses (usually accurate) for forms that were un-observed or rare in our training corpus.Our proposed method runs as a fast post-processor (taking under a minute to process 100-dimensional embeddings of a million observedword types) on the output of any existing tool thatconstructs word embeddings, such as WORD2VEC.1651Indicative SubjunctiveSg Pl Sg Pl1 bebo bebemos beba bebamos2 bebes beb?eis bebas beb?ais3 bebe beben beba bebanTable 1: The paradigm of the Spanish verb BEBER (to drink).The paradigm actually consists of> 40 word forms; only thepresent tense portion is shown here.In this output, some embeddings are noisy or miss-ing, due to sparse training data.
We correct theseproblems by using a Gaussian graphical modelthat jointly models the embeddings of morpholog-ically related words.
Inference under this modelcan smooth the noisy embeddings that were ob-served in the WORD2VEC output.
In the limitingcase of a word for which no embedding was ob-served (equivalent to infinite noise), inference canextrapolate one based on the observed embeddingsof related words?a kind of global version of thevector offset method.
The structure of our graphi-cal model is defined using morphological lexicons,which supply analyses for each word form.We conduct a comprehensive study of our abil-ity to modify and generate vectors across five lan-guages.
Our model also dramatically improvesperformance on the morphological analogy task inmany cases: e.g., accuracy at selecting the nom-inative plural forms of Czech nouns is 89%, tentimes better than the standard analogy approach.2 Background: Inflectional MorphologyMany languages require every verb token to be in-flected for certain properties, such as person, num-ber, tense, and mood.
A verbal paradigm suchas Table 1 lists all the inflected forms of a givenverb.
We may refer to this verb in the abstract byits lemma, BEBER?but when using it in a sen-tence, we must instead select from its paradigm theword type, such as beb?eis, that expresses the con-textually appropriate properties.
Noun tokens in alanguage may similarly be required to be inflectedfor properties such as case, gender, and number.A content word is chosen by specifying a lemma(which selects a particular paradigm) togetherwith some inflectional attributes (which select aparticular slot within that paradigm).
For example,[Lemma=EAT, Person=3, Number=SINGULAR,Tense=PRESENT ] is a bundle of attribute-valuepairs that would be jointly expressed in English bythe word form eats (Sylak-Glassman et al, 2015).The regularities observed by Mikolov et al(2013c) hold between words with similar attribute-value pairs.
In Spanish, the word beben ?theydrink?
(Table 1) can be analyzed as expressingthe bundle [Lemma=BEBER, Person=3, Num-ber=PLURAL, Tense=PRESENT ].
Its vector sim-ilarity to bebemos ?we drink?
is due to the factthat both word forms have the same lemma BE-BER.
Likewise, the vector similarity of bebento comieron ?they ate?
is due to the conceptualsimilarity of their lemmas, BEBER ?drink?
andCOMER ?eat?.
Conversely, that beben is similar topreguntan ?they ask?
is caused by shared inflec-tional attributes [Person=3, Number=PLURAL,Tense=PRESENT ].
Under cosine similarity, themost similar words are often related on both axesat once: e.g., one of the word forms closest tobeben typically is comen ?they eat?.3 ApproachFollowing this intuition, we fit a directed Gaussiangraphical model (GGM) that simultaneously con-siders (i) each word?s embedding (obtained froman embedding model like WORD2VEC) and (ii)its morphological analysis (obtained from a lexi-cal resource).
We then use this model to smooththe provided embeddings, and to generate embed-dings for unseen inflections.
For a lemma cov-ered by the resource, the GGM can produce em-beddings for all its forms (if at least one of theseforms has a known embedding); this can be ex-tended to words not covered using a guesser likeMORFESSOR (Creutz and Lagus, 2007) or CHIP-MUNK (Cotterell et al, 2015a).A major difference of our approach from re-lated techniques is that our model uses existingmorphological resources (e.g., morphological lex-icons or finite-state analyzers) rather than seman-tic resources (e.g., WordNet (Miller et al, 1990)and PPDB (Ganitkevitch et al, 2013)).
The for-mer tend to be larger: we often can analyze morewords than we have semantic representations for.It would be possible to integrate our GGM intothe training procedure for a word embedding sys-tem, making that system sensitive to morpholog-ical attributes.
However, the postprocessing ap-proach in our present paper lets us use any exist-ing word embedding system as a black box.
It issimple to implement, and turns out to get excellentresults, which will presumably improve further as1652veatsvranweatswatewrunswran wrunningminfl=vbgminfl=vbd mlem=eatminfl=vbpweatingmlem=runveatingFigure 2: A depiction of our directed Gaussian graphical model (GGM) for the English verbal paradigm.
Each variablerepresents a vector in Rn; thus, this is not the traditional presentation of a GGM in which each node would be a single real-valued random variable, but each node represents a real-valued random vector.
The shaded nodes viat the bottom are observedword embeddings.
The nodes wiat the middle layer are smoothed or extrapolated word embeddings.
The nodes mkat the topare latent embeddings of morphemes.better black boxes become available.4 A Generative ModelFigure 2 draws our GGM?s structure as a Bayesnet.
In this paper, we loosely use the term ?mor-pheme?
to refer to an attribute-value pair (possi-bly of the form Lemma=.
.
.
).
Let M be the setof all morphemes.
In our model, each morphemek ?
M has its own latent embedding mk?
Rn.These random variables are shown as the top layerof Figure 2.
We impose an IID spherical Gaussianprior on them (similar to L2regularization withstrength ?
> 0):mk?
N (0, ?
?1I), ?k (2)Let L be the lexicon of all word types that ap-pear in our lexical resource.
(The noun and verbsenses of bat are separate entries in L.) In ourmodel, each word i ?
L has a latent embeddingwi?
Rn.
These random variables are shown asthe middle layer of Figure 2.
We assume that eachwiis simply a sum of the mkfor its componentmorphemes Mi?
M (shown in Figure 2 as wi?sparents), plus a Gaussian perturbation:wi?
N (?k?Mimk,?i), ?i (3)This perturbation models idiosyncratic usage ofword i that is not predictable from its morphemes.The covariance matrix ?iis shared for all words iwith the same coarse POS (e.g., VERB).Our system?s output will be a guess of all of thewi.
Our system?s input consists of noisy estimatesvifor some of the wi, as provided by a black-boxword embedding system run on some large corpusC.
(Current systems estimate the same vector forboth senses of bat.)
These observed random vari-ables are shown as the bottom layer of Figure 2.We assume that the black-box system would haverecovered the ?true?
wiif given enough data, butinstead it gives a noisy small-sample estimatevi?
N (wi,1ni?
?i), ?i (4)where niis the count of word i in training corpusC.This formula is inspired by the central limittheorem, which guarantees that vi?s distributionwould approach (4) (as ni?
?)
if it were es-timated by averaging a set of ninoisy vectorsdrawn IID from any distribution with meanwi(thetruth) and covariance matrix ??i.
A system likeWORD2VEC does not precisely do that, but it doeschoose viby aggregating (if not averaging) the in-fluences from the contexts of the nitokens.The parameters ?,?i,?
?inow have likelihoodp(v) =?p(v,w,m) dw dm, where (5)p(v,w,m) =?k?Mp(mk) ?
?i?Lp(wi|mk: k ?Mi)?
p(vi|wi) (6)Here m = {mk: k ?
M} represents the collec-tion of all latent morpheme embeddings, and sim-ilarly w = {wi: i ?
L} and v = {vi: i ?
L}.We take p(vi| wi) = 1 if no observation viexists.How does the model behave qualitatively?
Ifw?iis the MAP estimate of wi, then w?i?
viasni?
?, but w?i?
?k?Mimkas ni?
0.
This1653is because (3) and (4) are in tension; when niissmall, (4) is weaker and we get more smoothing.The morpheme embeddings mkare largely deter-mined from the observed embeddings viof the fre-quent words (since mkaims via (2)?
(3) to explainwi, which ?
viwhen i is frequent).
That deter-mines the compositional embedding?k?Mimktoward which the wiof a rarer word is smoothed(away from vi).
If viis not observed or if ni= 0,then w?i=?k?Mimkexactly.5 InferenceSuppose first that the model parameters areknown, and we want to reconstruct the latent vec-tors wi.
Because the joint density p(v,w,m) in(6) is a product of (sometimes degenerate) Gaus-sian densities, it is itself a highly multivariateGaussian density over all elements of all vectors.1Thus, the posterior marginal distribution of eachwiis Gaussian as well.
A good deal is knownabout how to exactly compute these marginal dis-tributions of a Gaussian graphical model (e.g., bymatrix inversion) or at least their means (e.g., bybelief propagation) (Koller and Friedman, 2009).For this paper, we adopt a simpler method?MAP estimation of all latent vectors.
That is, weseek the w,m that jointly maximize (6).
This isequivalent to minimizing?k?||mk||22+?i||wi??k?Mimk||2?i+?i||vi?
wi||2?
?i/ni, (7)which is a simple convex optimization problem.2We apply block coordinate descent until numericalconvergence, in turn optimizing each vector mkor wiwith all other vectors held fixed.
This findsthe global minimum (convex objective) and is ex-tremely fast even when we have over a hundredmillion real variables.
Specifically, we updatemk?
(?I +?i?Wk?i)?1?i?Wk?i(wi?
?j?Mi,j 6=kmj),where?def= ?
?1is the inverse covariance matrixand Wkdef= {i : k ?
Mi}.
This updates mkso1Its inverse covariance matrix is highly sparse: its pat-tern of non-zeros is related to the graph structure of Figure 2.
(Since the graphical model in Figure 2 is directed, the inversecovariance matrix has a sparse Cholesky decomposition thatis even more directly related to the graph structure.
)2By definition, ||x||2Adef= xTAx.the partial derivatives of (7) with respect to thecomponents of mkare 0.
In effect, this updatesmkto a weighted average of several vectors.
Mor-pheme k participates in words i ?
Wk, so its vec-tor mkis updated to the average of the contribu-tions (wi?
?j?Mi,j 6=kmj) that mkwould ideallymake to the embeddings wiof those words.
Thecontribution of wiis ?weighted?
by the inverse co-variance matrix?i.
Because of prior (2), 0 is alsoincluded in the average, ?weighted?
by ?I .Similarly, the update rule for wiiswi?
(ni??i+?i)?1(ni?
?ivi+?i?k?Mimk),which can similarly be regarded as a weighted av-erage of the observed and compositional represen-tations.3See Appendix C for the derivations.6 Parameter LearningWe wish to optimize the model parameters?,?i,?
?iby empirical Bayes.
That is, we do nothave a prior on these parameters, but simply domaximum likelihood estimation.
A standard ap-proach is the Expectation-Maximization or EM al-gorithm (Dempster et al, 1977) to locally maxi-mize the likelihood.
This alternates between re-constructing the latent vectors given the parame-ters (E step) and optimizing the parameters giventhe latent vectors (M step).
In this paper, we usethe Viterbi approximation to the E step, that is,MAP inference as described in section 5.
Thus,our overall method is Viterbi EM.As all conditional probabilities in the modelare Gaussian, the M step has closed form.
MLEestimation of a covariance matrix is a standardresult?in our setting the update to ?itakes theform:?c?1Nc?i:c?C(i)(wi??k?Mimk)(wi?
?k?Mimk)T,where C(i) are i?s POS tags, Nc= |{i|c ?
C(i)}|and ?cis the matrix for the cthPOS tag (the ma-trices are tied by POS).
In this paper we simplyfix ?
?i= I rather than fitting it.4Also, we tunethe hyperparameter ?
on a development set, usinggrid search over the values {0.1, 0.5, 1.0}.3If viis not observed, take ni= 0.
In fact it is not neces-sary to represent this widuring optimization.
Simply omit ifrom all Wk.
After convergence, set wi?
?k?Mimk.4Note that it is not necessary to define it as ?
?I , introduc-ing a new scale parameter ?
?, since doubling ?
?would havethe same effect on the MAP update rules as halving ?
and ?i.1654Viterbi EM can be regarded as block coordinatedescent on the negative log-likelihood function,with E and M steps both improving this commonobjective along different variables.
We update theparameters (M step above) after each 10 passes ofupdating the latent vectors (section 5?s E step).7 Related WorkOur postprocessing strategy is inspired by Faruquiet al (2015), who designed a retrofitting procedureto modify pre-trained vectors such that their rela-tions match those found in semantic lexicons.
Wefocus on morphological resources, rather than se-mantic lexicons, and employ a generative model.More importantly, in addition to modifying vec-tors of observed words, our model can generatevectors for forms not observed in the training data.Wieting et al (2015) compute compositionalembeddings of phrases, with their simplestmethod being additive (like ours) over the phrase?swords.
Their embeddings are tuned to fit observedphrase similarity scores from PPDB (Ganitkevitchet al, 2013), which allows them to smooth and ex-tend PPDB just as we do to WORD2VEC output.Using morphological resources to enhance em-beddings at training time has been examined bynumerous authors.
Luong et al (2013) used MOR-FESSOR (Creutz and Lagus, 2007), an unsuper-vised morphological induction algorithm, to seg-ment the training corpus.
They then trained a re-cursive neural network (Goller and Kuchler, 1996;Socher, 2014) to generate compositional word em-beddings.
Our model is much simpler and fasterto train.
Their evaluation was limited to Englishand focused on rare English words.
dos Santos andZadrozny (2014) introduced a neural tagging ar-chitecture (Collobert et al, 2011) with a character-level convolutional layer.
Qiu et al (2014) andBotha and Blunsom (2014) both use MORFESSORsegmentations to augment WORD2VEC and a log-bilinear (LBL) language model (Mnih and Hinton,2007), respectively.
Similar to us, they have anadditive model of the semantics of morphemes,i.e., the embedding of the word form is the sumof the embeddings of its constituents.
In contrastto us, however, both include the word form itselfin the sum.
Finally, Cotterell and Sch?utze (2015)jointly trained an LBL language model and a mor-phological tagger (Haji?c, 2000) to encourage theembeddings to encode rich morphology.
With theexception of (Cotterell and Sch?utze, 2015), all ofthe above methods use unsupervised methods toinfuse word embeddings with morphology.
Ourapproach is supervised in that we use a morpho-logical lexicon, i.e., a manually built resource.Our model is also related to other generativemodels of real vectors common in machine learn-ing.
The simplest of them is probabilistic prin-cipal component analysis (Roweis, 1998; Tippingand Bishop, 1999), a generative model of matrixfactorization that explains a set of vectors via la-tent low-dimensional vectors.
Probabilistic canon-ical correlation analysis similarly explains a set ofpairs of vectors (Bach and Jordan, 2005).Figure 2 has the same topology as our graphicalmodel in (Cotterell et al, 2015b).
In that work, therandom variables were strings rather than vectors.Morphemes were combined into words by con-catenating strings rather than adding vectors, andthen applying a stochastic edit process (modelingphonology) rather than adding Gaussian noise.8 ExperimentsWe perform three experiments to test the abilityof our model to improve on WORD2VEC.
To re-iterate, our approach does not generate or ana-lyze a word?s spelling.
Rather, it uses an existingmorphological analysis of a word?s spelling (con-structed manually or by a rule-based or statisticalsystem) as a resource to improve its embedding.In our first experiment, we attempt to identify acorpus word that expresses a given set of morpho-logical attributes.
In our second experiment, weattempt to use a word?s embedding to predict thewords that appear in its context, i.e., the skip-gramobjective of Mikolov et al (2013a).
Our third ex-ample attempts to use word embeddings to predicthuman similarity judgments.We experiment on 5 languages: Czech, English,German, Spanish and Turkish.
For each language,our corpus data consists of the full Wikipedia text.Table 5 in Appendix A reports the number of typesand tokens and their ratio.
The lexicons we use arecharacterized in Table 6: MorfFlex CZ for Czech(Haji?c and Hlav?a?cov?a, 2013), CELEX for Englishand German (Baayen et al, 1993) and lexicons forSpanish and Turkish that were scraped from Wik-tionary by Sylak-Glassman et al (2015).Given a finite training corpus C and a lexi-con L,5we generate embeddings vifor all word5L is finite in our experiments.
It could be infinite (thoughstill incomplete) if a morphological guesser were used.1655NomSgNomPlGenSgGenPlDatSgDatPlAccSgAccPlInsSgInsPlVocSgVocPlVoc PlVoc SgIns PlIns SgAcc PlAcc SgDat PlDat SgGen PlGen SgNom PlNom SgGGM4.7 0 0 3.5 1.3 5.5 2.2 0 2.1 3.7 0 ?0 0 0 0 0 5 5 0 0.83 0 ?
02 6 3 2.5 1.7 4.3 3.6 7.7 0.69 ?
0 4.64 1.6 1.6 2.3 2.7 0.36 1.6 2.4 ?
0.18 0.83 2.91.3 15 1.2 6.2 2.6 2.6 2.2 ?
2.8 2.7 0 03.9 2.7 0.74 2.1 1.7 0 ?
2.4 0.58 2.5 5 2.23.8 4.8 2.4 8 0.86 ?
0.33 7.2 1 2.7 0 52 2 0 3.3 ?
0 1.3 0.61 2.8 0 0 21.8 4.9 4.8 ?
4.7 2.5 3.3 7.2 3.1 0.92 1.7 3.72 1.7 ?
2.9 0.67 0.5 0.98 0.5 1.3 1.6 0 0.334.6 ?
1.7 3.4 1.6 2.2 2.3 10 1.9 7 0 0?
4.4 2.6 1.7 3.9 0.33 2.1 1.6 3.5 0.59 0 5.248 89 81 43 30 33 36 87 33 48 49 891psPs2psPs3psPs1ppPs2ppPs3ppPs1psPt2psPt3psPt1ppPt2ppPt3ppPt3pp Pt2pp Pt1pp Pt3ps Pt2ps Pt1ps Pt3pp Ps2pp Ps1pp Ps3ps Ps2ps Ps1ps PsGGM7.3 1.3 7.2 3.4 0 13 1.4 0 37 1.8 0 ?0 0 0 0 0 0 0 0 0 0 ?
01.7 0 3.3 0 0 2.8 1.7 0 3.4 ?
0 27.6 0 13 1.3 0 7.2 0.86 0 ?
3.5 0 360 0 0 0 0 0 0 ?
0 0 0 00.14 0.5 2 3.2 0 0.33 ?
0 1.4 0.83 0 1.71.6 0 30 4 0 ?
0.24 0 8 4.4 0 140 0 0 0 ?
0 0 0 0 0 0 00.24 0 1.9 ?
0 2.7 2.1 0 2.6 0 0 2.21.2 9.3 ?
1.2 0 29 1.7 3.3 13 2.5 0 8.20 ?
14 0.5 5 0.091 0.83 0 0.17 0 0 0.28?
0 0.79 2.1 0 0.33 0.83 0 3.4 1 0 38.8 4.5 30 14 10 49 9.4 8.1 41 9.5 0 43Table 2: The two tables show how the Gaussian graphical model (GGM) compares to various analogies on Czech nouns (left)and Spanish verbs (right).The numbers in each cell represent the accuracy (larger is better).
The columns represent the inflectionof the word i to be predicted.
Our GGM model is the top row.
The other rows subdivide the baseline analogy results accordingto the inflection of source word a. Abbreviations: in the Czech noun table (left), the first word indicates the case and the secondthe number, e.g., Dat Sg = Dative Singular.
In the Spanish verb table (right), the first word is the person and number and thesecond the tense, e.g., 3pp Pt = 3rd-person plural past.types i ?
C, using the GENSIM implementation(?Reh?u?rek and Sojka, 2010) of the WORD2VEC hi-erarchical softmax skip-gram model (Mikolov etal., 2013a), with a context size of 5.
We set thedimension n to 100 for all experiments.6We then apply our GGM to generate smoothedembeddings wifor all word types i ?
C ?
L. (Re-call that the noun and verb sense of bats are sep-arate types in L, even if conflated in C, and getseparate embeddings.)
How do we handle otherword types?
For an out-of-vocabulary (OOV) testword i 6?
C, we will extrapolate wi?
?k?Mimkon demand, as the GGM predicts, provided i ?
L.If any of these morphemes mkwere themselvesnever seen in C, we back off to the mode of theprior to take mk= 0.7Our experiments also en-counter out-of-lexicon (OOL) test words i 6?
L,for which we have no morphological analysis;here we take wi= vi(unsmoothed) if i ?
C andwi= 0 otherwise.8.1 Experiment 1: Extrapolation vs. AnalogyOur first set of experiments uses embeddings forword selection.
Our prediction task is to iden-tify the unique word i ?
C that expresses the6An additional important hyperparameter is the numberof epochs.
The default value in the GENSIM package is 5,which is suitable for larger corpora.
We use this value for Ex-periments 1 and 3.
Experiment 2 involves training on smallercorpora and we found it necessary to set the number of epochsto 10.7One could in principle learn ?backoff mor-phemes.?
For instance, if borogoves is analyzed as[ Lemma=OOV NOUN,Num=PL ], we might wantmLemma=OOV NOUN6= 0 to represent novel nouns.morphological attributes Mi.
To do this, we pre-dict a target embedding x, and choose the mostsimilar unsmoothed word by cosine distance, ??
=argmaxj?Cvj?
x.
We are scored correct if ??
= i.Our experimental design ensures that i 6?
L, sinceif it were, we could trivially find i simply by con-sulting L. The task is to identify missing lexicalentries, by exploiting the distributional propertiesin C.8Given the input bundle Mi, our method pre-dicts the embedding x =?k?Mimk, and so looksfor a word j ?
C whose unsmoothed embeddingvj?
x.
The GGM?s role here is to predict that thebundle Miwill be realized by something like x.The baseline method is the analogy method ofequation (1).
This predicts the embedding x viathe vector-offset formula va+ (vb?
vc), wherea, b, c ?
C ?
L are three other words sharingi?s coarse part of speech such that Mican beexpressed as Ma+ (Mb?
Mc).9Specifically,the baseline chooses a, b, c uniformly at randomfrom all possibilities.
(This is not too inefficient:given a, at most one choice of (b, c) is possible.
)Note that the baseline extrapolates from the un-smoothed embeddings of 3 other words, whereasthe GGM considers all words in C ?
L that sharei?s morphemes.8The argmax selection rule does not exploit the fact thatthe entry is missing: it is free to incorrectly return some ??
?L.9More formally, Mi= Ma+ (Mb?Mc), if we defineM by (M)k= I(k ?
M) for all morphemes k ?
M. Thisconverts morpheme bundleM to a {0, 1} indicator vectorMoverM.1656analogyGGManalogyGGManalogyGGManalogyGGManalogyGGMInsP.067 .343?AccS.131 .582?VBD .052 .202?MascSPart .178 .340?LocS.002 .036?GenS.059 .632?NomS.136 .571?VBG .06 .102?FemPPart .230 .286 AblS.001 .019?GenP.078 .242?DatP.074 .447?NN .051 .080?FemSPart .242 .235 DatS.001 .037?NomP.076 .764?AccP.075 .682?VBN .052 .218?AdjS.201 .286?AccS.001 .023?NomS.066 .290?GenP.075 .666?NNS .052 .114?GerundS.186 .449?InsS.001 .004VocP.063 .789?NomP.064 .665?VB .056 .275?GerundP.172 .311?NomS.001 .023?Czech German English Spanish Turkishnouns nouns nouns & verbs nouns, verbs & adj?s nounsTable 3: Test results for Experiment 1.
The rows indicate the inflection of the test word i to be predicted (superscriptPindicatesplural, superscriptSsingular).
The columns indicate the prediction method.
Each number is an average over 10 training-testsplits.
Improvements marked with a ?
are statistically significant (p < 0.05) under a paired permutation test over these 10 runs.Experimental Setup: A lexical resource con-sists of pairs (word form i, analysis Mi).
For eachlanguage, we take a random 80% of these pairs toserve as the training lexicon L that is seen by theGGM.
The remaining pairs are used to constructour prediction problems (givenMi, predict i), witha random 10% each as dev and test examples.
Wecompare our method against the baseline methodon ten such random training-test splits.
We are re-leasing all splits for future research.For some dev and test examples, the baselinemethod has no choice of the triple a, b, c. Ratherthan score these examples as incorrect, our base-line results do not consider them at all (which in-flates performance).
For each remaining example,to reduce variance, the baseline method reports theaverage performance on up to 100 a, b, c triplessampled uniformly without replacement.The automatically created analogy problems(a, b, c ?
i) solved by the baseline are simi-lar to those of Mikolov et al (2013c).
How-ever, most previous analogy evaluation sets evalu-ate only on 4-tuples of frequent words (Nicolai etal., 2015), to escape the need for smoothing, whileours also include infrequent words.
Previous eval-uation sets also tend to be translations of the orig-inal English datasets?leaving them impoverishedas they therefore only test morpho-syntactic prop-erties found in English.
E.g., the German analogyproblems of K?oper et al (2015) do not explore thefour cases and two numbers in the German adjec-tival system.
Thus our baseline analogy results areuseful as a more comprehensive study of the vec-tor offset method for randomly sampled words.Results: Overall results for 5 languages areshown in Table 3.
Additional rows break downperformance by the inflection of the target word i.
(The inflections shown are the ones for which thebaseline method is most accurate.
)For almost all target inflections, GGM is sig-nificantly better than the analogy baseline.
Anextreme case is the vocative plural in Czech, forwhich GGM predicts vectors better by more than70%.
In other cases, the margin is slimmer; butGGM loses only on predicting the Spanish fem-inine singular participle.
For Czech, German,English and Spanish the results are clear?GGMyields better predictions.
This is not surprising asour method incorporates information from multi-ple morphologically related forms.More detailed results for two languages aregiven in Table 2.
Here, each row constrains thesource word a to have a certain inflectional tag;again we average over up to 100 analogies, nowchosen under this constraint, and again we discarda test example i from the test set if no such analogyexists.
The GGM row considers all test examples.Past work on morphosyntactic analogies hasgenerally constrained a to be the unmarked(lemma) form (Nicolai et al, 2015).
However, weobserve that it is easier to predict one word formfrom another starting from a form that is ?closer?in morphological space.
For instance, it is easier topredict Czech forms inflected in the genitive plu-ral from forms in nominative plural, rather than thenominative singular.
Likewise, it is easier to pre-dict a singular form from another singular formrather than from a plural form.
It also is easier topredict partially syncretic forms, i.e., two inflectedforms that share the same orthographic string; e.g.,in Czech the nominative plural and the accusativeplural are identical for inanimate nouns.1657105 106 107 108200040006000800010000PerplexityPerWord English105 106 10705000100001500020000250003000035000 Czech105 106 107 10810002000300040005000600070008000 Spanish [0,?
)-U[0, 1)-U[1, 10)-U[10, 20)-U[0,?
)-S[0, 1)-S[1, 10)-S[10, 20)-SFigure 3: Results for the WORD2VEC skip-gram objective score (perplexity per predicted context word) on a held-out testcorpus.
The x-axis measures the size in tokens of the training corpus used to generate the model.
We plot the held-outperplexity for the skip-gram model with Unsmoothed observed vectors v (solide) and Smoothed vectors w (barredc).
Thethickest, darkest curves show aggregate performance.
The thinner, lighter versions show a breakdown according to whetherthe predicting word?s frequency in the smallest training corpus falls in the range [0, 1), [1, 10), or [10, 20) (from lightest todarkest and roughly from top to bottom).
(These are the words whose representations we smooth; footnote 10 explains whywe do not smooth the predicted context word.)
We do not show [20,?)
since WORD2VEC randomly removes some tokens ofhigh-frequency words (?subsampling?
), similar in spirit to removing stop words.
See Appendix B for more graphs.8.2 Experiment 2: Held-Out EvaluationWe now evaluate the smoothed and extrapolatedrepresentations wi.
Fundamentally, we want toknow if our approach improves the embeddingsof the entire vocabulary, as if we had seen moreevidence.
But we cannot simply compare oursmoothed vectors to ?gold?
vectors trained onmuch more data, since two different runs ofWORD2VEC will produce incomparable embed-ding schemes.
We must ask whether our embed-dings improve results on a downstream task.To avoid choosing a downstream task witha narrow application, we evaluate our embed-ding using the WORD2VEC skip-gram objectiveon held-out data?as one would evaluate a lan-guage model.
If we believe that a better scoreon the WORD2VEC objective indicates generallymore useful embeddings?which indeed we doas we optimize for it?then improving this scoreindicates that our smoothed vectors are superior.Concretely, the objective is?s?t?j?
[t?5,t+5],j 6=tlog2pword2vec(Tsj|Tst), (8)where Tsis the sthsentence in the test corpus, t in-dexes its tokens, and j indexes tokens near t. Theprobability model pword2vecis defined in Eq.
(3) of(Mikolov et al, 2013b).
It relies on an embeddingof the word form Tst.10Our baseline approach10In the hierarchical softmax version, it also relies on aseparate embedding for a variable-length bit-string encod-ing of the context word Tsj.
Unfortunately, we do not cur-rently know of a way to smooth these bit-string encodings(also found by WORD2VEC).
However, it might be possibleto directly incorporate morphology into the construction ofthe vocabulary tree that defines the bit-strings.simply uses WORD2VEC?s embeddings (or 0 forOOV words Tst6?
C).
Our GGM approach substi-tutes ?better?
embeddings when Tstappears in thelexicon L (if Tstis ambiguous, we use the meanwivector from all i ?
L with spelling Tst).Note that (8) is itself a kind of task of predictingwords in context, resembling language modelingor a ?cloze?
task.
Also, Taddy (2015) showed howto use this objective for document classification.Experimental Setup: We evaluate GGM on thesame 5 languages, but now hold out part of thecorpus instead of part of the lexicon.
We takethe training corpus C to be the initial portion ofWikipedia of size 105, 106, 107or 108.
(We skipthe 108case for the smaller datasets: Czech andTurkish).
The 107tokens after that are the dev cor-pus; the next 107tokens are the test corpus.Results: We report results on three languagesin Figure 3 and all languages in Appendix B.Smoothing from vito wihelps a lot, reducing per-plexity by up to 48% (Czech) with 105trainingtokens and up to 10% (Spanish) even with 108training tokens.
This roughly halves the perplex-ity, which in the case of 105training tokens, isequivalent to 8?more training data.
This is a clearwin for lower-resource languages.
We get largergains from smoothing the rarer predicting words,but even words with frequency ?
10?4benefit.
(The exception is Turkish, where the large gainsare confined to rare predicting words.)
See Ap-pendix B for more analysis.1658English German SpanishForms / Lemma 1.8 6.3 8.1Skip-Gram 58.9 36.2 37.8GGM 58.9 37.6 40.3Table 4: Word similarity results (correlations) using the WS-353 dataset in the three languages, in which it is available.Since all the words in WS-353 are lemmata, we report theaverage inflected form to lemma ratio for forms appearing inthe datasets.8.3 Experiment 3: Word SimilarityAs a third and final experiment, we consider wordsimilarity using the WS-353 data set (Finkelsteinet al, 2001), translated into Spanish (Hassan andMihalcea, 2009) and German (Leviant, 2016).11The datasets are composed of 353 pairs of words.Multiple native speakers were then asked to givean integral value between 1 and 10 indicating thesimilarity of that pair, and those values were thenaveraged.
In each case, we train the GGM on thewhole Wikipedia corpus for the language.
Sincein each language every word in the WS-353 setis in fact a lemma, we use the latent embeddingour GGM learns in the experiment.
In Span-ish, for example, we use the learned latent mor-pheme embedding for the lemma BEBER (recallthis takes information from every element in theparadigm, e.g., bebemos and beben), rather thanthe embedding for the infinitival form beber.
Inhighly inflected languages we expect this to im-prove performance, because to get the embeddingof a lemma, it leverages the distributional signalfrom all inflected forms of that lemma, not just asingle one.
Note that unlike previous retrofittingapproaches, we do not introduce new semantic in-formation into the model, but rather simply allowthe model to better exploit the distributional prop-erties already in the text, by considering wordswith related lemmata together.
In essence, ourapproach embeds a lemma as the average of allwords containing that lemma, after ?correcting?those forms by subtracting off their other mor-phemes (e.g., inflectional affixes).Results: As is standard in the literature, we re-port Spearman?s correlation cofficient ?
betweenthe averaged human scores and the cosine distancebetween the embeddings.
We report results in Ta-ble 4.
We additionally report the average num-11This dataset has yet to be translated into Czech or Turk-ish, nor are there any comparable resources in these lan-guages.ber of forms per lemma.
We find that we improveperformance on the Spanish and German datasetsover the original skip-gram vectors, but only equalthe performance on English.
This is not surprisingas German and Spanish have roughly 3 and 4 timesmore forms per lemma than English.
We spec-ulate that cross-linguistically the GGM will im-prove word similarity scores more for languageswith richer morphology.9 Conclusion and Future WorkFor morphologically rich languages, we generallywill not observe, even in a large corpus, a highproportion of the word forms that exist in lex-ical resources.
We have presented a Gaussiangraphical model that exploits lexical relations doc-umented in existing morphological resources tosmooth vectors for observed words and extrapo-late vectors for new words.
We show that ourmethod achieves large improvements over strongbaselines for the tasks of morpho-syntactic analo-gies and predicting words in context.
Future workwill consider the role of derivational morphologyin embeddings as well as noncompositional casesof inflectional morphology.AcknowledgmentsThe first author was funded by a DAAD Long-Term Research Grant.
This work was also par-tially supported by Deutsche Forschungsgemein-schaft (grat SCHU-2246/2-2 WordGraph) and bythe U.S. National Science Foundation under GrantNo.
1423276.ReferencesR Harald Baayen, Richard Piepenbrock, and Rijn vanH.
1993.
The CELEX lexical data base on CD-ROM.Francis R Bach and Michael I Jordan.
2005.
A proba-bilistic interpretation of canonical correlation analy-sis.
Technical report, UC Berkeley.Jan A. Botha and Phil Blunsom.
2014.
Composi-tional Morphology for Word Representations andLanguage Modelling.
In ICML.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
JMLR, 12.Ryan Cotterell and Hinrich Sch?utze.
2015.
Morpho-logical word embeddings.
In NAACL.1659Ryan Cotterell, Thomas M?uller, Alexander Fraser, andHinrich Sch?utze.
2015a.
Labeled morphologicalsegmentation with semi-Markov models.
In CoNLL.Ryan Cotterell, Nanyun Peng, and Jason Eisner.2015b.
Modeling word forms using latent underly-ing morphs and phonology.
TACL, 3.Mathias Creutz and Krista Lagus.
2007.
Unsuper-vised models for morpheme segmentation and mor-phology learning.
ACM Transactions on Speech andLanguage Processing, 4(1):3.Arthur P Dempster, Nan M Laird, and Donald B Ru-bin.
1977.
Maximum likelihood from incompletedata via the EM algorithm.
Journal of the Royal Sta-tistical Society B, pages 1?38.C?cero Nogueira dos Santos and Bianca Zadrozny.2014.
Learning character-level representations forpart-of-speech tagging.
In ICML.Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, ChrisDyer, Eduard Hovy, and Noah A Smith.
2015.Retrofitting word vectors to semantic lexicons.
InNAACL.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2001.
Placing search in context: Theconcept revisited.
In WWW.
ACM.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In NAACL.Christoph Goller and Andreas Kuchler.
1996.
Learn-ing task-dependent distributed representations bybackpropagation through structure.
Neural Net-works.Jan Haji?c and Jaroslava Hlav?a?cov?a.
2013.
MorfFlexCZ.
LINDAT/CLARIN digital library at Institute ofFormal and Applied Linguistics, Charles Universityin Prague.Jan Haji?c.
2000.
Morphological tagging: Data vs. dic-tionaries.
In NAACL.Samer Hassan and Rada Mihalcea.
2009.
Cross-lingual semantic relatedness using encyclopedicknowledge.
In EMNLP.Daphne Koller and Nir Friedman.
2009.
ProbabilisticGraphical Models: Principles and Techniques.
MITpress.Maximilian K?oper, Christian Scheible, andSabine Schulte im Walde.
2015.
Multilingualreliability and semantic structure of continuousword spaces.
In IWCS.Ira Leviant.
2016.
Separated by an Un-common Lan-guage: Towards Judgment Language Informed Vec-tor Space Modeling.
Ph.D. thesis, Technion.Minh-Thang Luong, Richard Socher, and C Manning.2013.
Better word representations with recursiveneural networks for morphology.
In CoNLL.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
In ICLR Workshop.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013b.
Distributed repre-sentations of words and phrases and their composi-tionality.
In NIPS.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013c.
Linguistic regularities in continuous spaceword representations.
In NAACL.George A. Miller, Richard Beckwith, ChristianeFellbaum, Derek Gross, and Katherine J Miller.1990.
Introduction to WordNet: An on-line lexi-cal database.
International Journal of Lexicogra-phy, 3(4):235?244.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In ICML.Garrett Nicolai, Colin Cherry, and Grzegorz Kondrak.2015.
Morpho-syntactic regularities in continuousword representations: A multilingual study.
In Pro-ceedings of Workshop on Vector Space Modeling forNLP, pages 129?134.Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Liu Tie-Yan.
2014.
Co-learning of word representations andmorpheme representations.
In COLING.Radim?Reh?u?rek and Petr Sojka.
2010.
SoftwareFramework for Topic Modelling with Large Cor-pora.
In Proceedings of the LREC 2010 Workshopon New Challenges for NLP Frameworks, pages 45?50.Sam Roweis.
1998.
EM algorithms for PCA andSPCA.
In NIPS.Richard Socher.
2014.
Recursive Deep Learning forNatural Language Processing and Computer Vision.Ph.D.
thesis, Stanford University.John Sylak-Glassman, Christo Kirov, David Yarowsky,and Roger Que.
2015.
A language-independent fea-ture schema for inflectional morphology.
In ACL.Matt Taddy.
2015.
Document classification by in-version of distributed language representations.
InACL.Michael E Tipping and Christopher M Bishop.
1999.Probabilistic principal component analysis.
Journalof the Royal Statistical Society B, 61(3):611?622.John Wieting, Mohit Bansal, Kevin Gimpel, KarenLivescu, and Dan Roth.
2015.
From paraphrasedatabase to compositional paraphrase model andback.
TACL.1660
