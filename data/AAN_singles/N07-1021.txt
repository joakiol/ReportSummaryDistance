Proceedings of NAACL HLT 2007, pages 164?171,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsProbabilistic Generation of Weather Forecast TextsAnja BelzNatural Language Technology GroupUniversity of Brighton, UKa.s.belz@brighton.ac.ukAbstractThis paper reports experiments inwhich pCRU ?
a generation frameworkthat combines probabilistic generationmethodology with a comprehensivemodel of the generation space ?
isused to semi-automatically create sev-eral versions of a weather forecast textgenerator.
The generators are evaluatedin terms of output quality, developmenttime and computational efficiency against(i) human forecasters, (ii) a traditionalhandcrafted pipelined NLG system, and(iii) a HALOGEN-style statistical genera-tor.
The most striking result is that despiteacquiring all decision-making abilitiesautomatically, the best pCRU generatorsreceive higher scores from human judgesthan forecasts written by experts.1 Introduction and backgroundOver the last decade, there has been a lot of in-terest in statistical techniques among researchers innatural language generation (NLG), a field that waslargely unaffected by the statistical revolution inNLP that started in the 1980s.
Since Langkilde andKnight?s influential work on statistical surface real-isation (Knight and Langkilde, 1998), a number ofstatistical and corpus-based methods have been re-ported.
However, this interest does not appear tohave translated into practice: of the 30 implementedsystems and modules with development starting inor after 2000 that are listed on a key NLG website1,only five have any statistical component at all (an-other six involve techniques that are in some waycorpus-based).
The likely reasons for this lack oftake-up are that (i) many existing statistical NLGtechniques are inherently expensive, requiring theset of alternatives to be generated in full before thestatistical model is applied to select the most likely;and (ii) statistical NLG techniques have not beenshown to produce outputs of high enough quality.There has also been a rethinking of the traditionalmodular NLG architecture (Reiter, 1994).
Some re-search has moved towards a more comprehensiveview, e.g.
construing the generation task as a singleconstraint satisfaction problem.
Precursors to cur-rent approaches were Hovy?s PAULINE which kepttrack of the satisfaction status of global ?rhetori-cal goals?
(Hovy, 1988), and Power et al?s ICON-OCLAST which allowed users to fine-tune differentcombinations of global constraints (Power, 2000).In recent comprehensive approaches, the focus is onautomatic adaptability, e.g.
automatically determin-ing degrees of constraint violability on the basis ofcorpus frequencies.
Examples include Langkilde?s(2005) general approach to generation and parsingbased on constraint optimisation, and Marciniak andStrube?s (2005) integrated, globally optimisable net-work of classifiers and constraints.Both probabilistic and recent comprehensivetrends have developed at least in part to address twointerrelated issues in NLG: the considerable amount1Bateman and Zock?s list of NLG systems,http://www.fb10.uni-bremen.de/anglistik/langpro/NLG-table/, 20/01/2006.164of time and expense involved in building new sys-tems, and the almost complete lack in the field ofreusable systems and modules.
Both trends havethe potential to improve on development time andreusability, but have drawbacks.Existing statistical NLG (i) uses corpus statistics toinform heuristic decisions in what is otherwise sym-bolic generation (Varges and Mellish, 2001; White,2004; Paiva and Evans, 2005); (ii) applies n-grammodels to select the overall most likely realisationafter generation (HALOGEN family); or (iii) reusesan existing parsing grammar or treebank for surfacerealisation (Velldal et al, 2004; Cahill and van Gen-abith, 2006).
N -gram models are not linguisticallyinformed, (i) and (iii) come with a substantial man-ual overhead, and (ii) overgenerates vastly and has ahigh computational cost (see also Section 3).Existing comprehensive approaches tend to in-cur a manual overhead (finetuning in ICONOCLAST,corpus annotation in Langkilde and Marciniak &Strube).
Handling violability of soft constraints isproblematic, and converting corpus-derived prob-abilities into costs associated with constraints(Langkilde, Marciniak & Strube) turns straightfor-ward statistics into an ad hoc search heuristic.
Olderapproaches are not globally optimisable (PAULINE)or involve exhaustive search (ICONOCLAST).The pCRU language generation framework com-bines a probabilistic generation methodology witha comprehensive model of the generation space,where probabilistic choice informs generation as itgoes along, instead of after all alternatives have beengenerated.
pCRU uses existing techniques (Belz,2005), but extends these substantially.
This paperdescribes the pCRU framework and reports experi-ments designed to rigorously test pCRU in practiceand to determine whether improvements in develop-ment time and reusability can be achieved withoutsacrificing quality of outputs.2 pCRU language generationpCRU (Belz, 2006) is a probabilistic language gen-eration framework that was developed with the aimof providing the formal underpinnings for creatingNLG systems that are driven by comprehensive prob-abilistic models of the entire generation space (in-cluding deep generation).
NLG systems tend to becomposed of generation rules that apply transforma-tions to representations (performing different tasksin different modules).
The basic idea in pCRU isthat as long as the generation rules are all of theform relation(arg1, ...argn) ?
relation1(arg1, ...argp)... relationm(arg1, ...argq), m ?
1, n, p, q ?
0, then theset of all generation rules can be seen as defininga context-free language and a single probabilisticmodel can be estimated from raw or annotated textto guide generation processes.pCRU uses straightforward context-free technol-ogy in combination with underspecification tech-niques, to encode a base generator as a set of ex-pansion rules G composed of n-ary relations withvariable and constant arguments (Section 2.1).
Innon-probabilistic mode, the output is the set of fullyexpanded (fully specified) forms that can be de-rived from the input.
The pCRU (probabilistic CRU)decision-maker is created by estimating a proba-bility distribution over the base generator from anunannotated corpus of example texts.
This distri-bution is used in one of several ways to drive gen-eration processes, maximising the likelihood eitherof individual expansions or of entire generation pro-cesses (Section 2.2).2.1 Specifying the range of alternativesUsing context-free representational underspecifica-tion, or CRU, (Belz, 2004), the generation space isencoded as (i) a set G of expansion rules composedof n-ary relations relation(arg1, ...argn) where theargi are constants or variables over constants; and(ii) argument and relation type hierarchies.
Any sen-tential form licensed by G can be the input to thegeneration process which expands it under unify-ing variable substitution until no further expansion ispossible.
The output (in non-probabilistic mode) isthe set of fully expanded forms (i.e.
consisting onlyof terminals) that can be derived from the input.The rules in G define the steps in which inputs canbe incrementally specified from, say, content to se-mantic, syntactic and finally surface representations.G therefore defines specificity relations between allsentential forms, i.e.
defines which representation isunderspecified with respect to which other represen-tations.
The generation process is construed explic-itly as the task of incrementally specifying one ormore word strings.165Within the limits of context-freeness and atom-icity of feature values, CRU is neutral with respectto actual linguistic knowledge representation for-malisms used to encode generation spaces.
Themain motivation for a context-free formalism isthe advantage of low computational cost, while theinclusion of arguments on (non)terminals permitskeeping track of contextual features.2.2 Selection among alternativesThe pCRU decision-making component is created byestimating a probability distribution over the set ofexpansion rules that encodes the generation space(the base generator), as follows:1 Convert corpus into multi-treebank: determinefor each sentence all (left-most) derivation treeslicensed by the base generator?s CRU rules, us-ing maximal partial derivations if there is no com-plete derivation tree; annotate the (sub)strings inthe sentence with the derivation trees, resulting ina set of generation trees for the sentence.2 Train base generator: Obtain frequency countsfor each individual generation rule from the multi-treebank, adding 1/n to the count for every rule,where n is the number of alternative derivationtrees; convert counts into probability distributionsover alternative rules, using add-1 smoothing andstandard maximum likelihood estimation.The resulting probability distribution is used inone of the following three ways to control gener-ation.
Of these, only the first requires the genera-tion forest to be created in full, whereas both greedymodes prune the generation space to a single path:1 Viterbi generation: do a Viterbi search of the gen-eration forest for a given input, which maximisesthe joint likelihood of all decisions taken in thegeneration process.
This selects the most likelygeneration process, but is considerably more ex-pensive than the greedy modes.2 Greedy generation: make the single most likelydecision at each choice point (rule expansion) ina generation process.
This is not guaranteed toresult in the most likely generation process, butthe computational cost is very low.3 Greedy roulette-wheel generation: use a non-uniform random distribution proportional to thelikelihoods of alternatives.
E.g.
if there are twoalternative decisions D1 and D2, with the modelgiving p(D1) = 0.8 and p(D2) = 0.2, then theproportion of times the generator decides D1 ap-proaches 80% and D2 20% in the limit.2.3 The pCRU-1.0 generation packageThe technology described in the two preceding sec-tions has been implemented in the pCRU-1.0 soft-ware package.
The user defines a generation spaceby creating a base generator composed of:1. the set N of underspecified n-ary relations2.
the set W of fully specified n-ary relations3.
a set R of context-free generation rules n ?
?,n ?
N , ?
?
(W ?N)?4.
a typed feature hierarchy defining argumenttypes and valuesThis base generator is then trained (as describedabove) on raw text corpora to provide a probabilitydistribution over generation rules.
Optionally, an n-gram language model can also be created from thesame corpus.
The generator is then run in one of thethree modes above or one of the following:1.
Random: ignoring pCRU probabilities, ran-domly select generation rules.2.
N -gram: ignoring pCRU probabilities, gener-ate set of alternatives and select the most likelyaccording to the n-gram language model.The random mode serves as a baseline for gen-eration quality: a trained generator must be able todo better, otherwise all the work is done by the basegenerator (and none by the probabilities).
The n-gram mode works exactly like HALOGEN-style gen-eration: the generator generates all realisations thatthe rules allow and then picks one based on the n-gram model.
This is a point of comparison withexisting statistical NLG techniques and also servesas a baseline in terms of computational expense: agenerator using pCRU probabilities should be ableto produce realisations faster.3 Building and evaluating pCRU windforecast text generatorsThe automatic generation of weather forecasts isone of the success stories of NLP.
The restrictive-ness of the sublanguage has made the domain of166Oil1/Oil2/Oil3_FIELDS05-10-0005/06 SSW 18 22 27 3.0 4.8 SSW 2.5905/09 S 16 20 25 2.7 4.3 SSW 2.3905/12 S 14 17 21 2.5 4.0 SSW 2.2905/15 S 14 17 21 2.3 3.7 SSW 2.28...FORECAST FOR:-Oil1/Oil2/Oil3 FIELDS...2.FORECAST 06-24 GMT, THURSDAY, 05-Oct 2000=====WARNINGS: RISK THUNDERSTORM.
=======WIND(KTS) CONFIDENCE: HIGH10M: SSW 16-20 GRADUALLY BACKING SSETHEN FALLING VARIABLE 04-08 BYLATE EVENING...Figure 1: Meteorological data file and wind forecastfor 05-10-2000, a.m. (oil fields anonymised).weather forecasting particularly attractive to NLG re-searchers, and a number of weather forecast genera-tion systems have been created.A recent example of weather forecast text gener-ation is the SUMTIME project (Reiter et al, 2005)which developed a commercially used NLG systemthat generates marine weather forecasts for offshoreoil rigs from numerical forecast data produced byweather simulation programs.
The SUMTIME cor-pus is used in the experiments below.3.1 DataEach instance in the SUMTIME corpus consists ofthree numerical data files (the outputs of weathersimulators) and the forecast file written by the fore-caster on the basis of the data (Figure 1 shows anexample).
The experiments below focused on a.m.forecasts of wind characteristics.
Content determi-nation (deciding which meteorological data to in-clude in a forecast) was carried out off-line.The corpus consists of 2,123 instances (22,985words) of which half are a.m. forecasts.
This maynot seem much, but considering the small number ofvocabulary items and syntactic structures, the cor-pus provides extremely good coverage (an initial im-pression confirmed by the small differences betweentraining and testing data results below).3.2 The base generatorThe base generator2 was written semi-auto-matically in two steps.
First, a simple chunkerwas run over the corpus to split wind statements2For a fragment of the rule set, see Belz (2006).into wind direction, wind speed, gust speed,gust statements, time expressions, verb phrases,pre-modifiers, and post-modifiers.
Preterminalgeneration rules were automatically created fromthe resulting chunks.
Then, higher-level rules whichcombine chunks into larger components, takingcare of text structuring, aggregation and elision,were manually authored.
The top-level generationrules interpret wind statements as sequences ofindependent units of information, ensuring a linearincrease in complexity with increasing input length.Inputs encode meteorological data (as shown in Ta-ble 1), and were pre-processed to determine certaintypes of information, including whether a changein wind direction was clockwise or anti-clockwise,and whether change in wind speed was an increaseor a decrease.
The final generator takes as inputsnumber vectors of length 7 to 60, and generates upto 1.6 ?
1031 alternative realisations for an input.The job of the base generator is to describe thetextual variety found in the corpus.
It makes no deci-sions about when to prefer one variant over another.3.3 TrainingThe corpus was divided at random into 90% train-ing data and 10% testing data.
The training setwas multi-treebanked with the base generator andthe multi-treebank then used to create the probabil-ity distribution for the base generator (as describedin Section 2.2).
A back-off 2-gram model withGood-Turing discounting and no lexical classes wasalso created from the training set, using the SRILMtoolkit, (Stolcke, 2002).
pCRU-1.0 was then run inall five modes to generate forecasts for the inputs inboth training and test sets.This procedure was repeated five times for hold-out cross-validation.
The small amount of variationacross the five repeats, and the small differences be-tween results for training and test sets (Table 2) in-dicated that five repeats were sufficient.3.4 Evaluation3.4.1 Evaluation methodsThe two automatic metrics used in the evalua-tions, NIST and BLEU have been shown to correlatehighly with expert judgments (Pearson correlationcoefficients 0.82 and 0.79 respectively) in this do-main (Belz and Reiter, 2006).167Input [[1,SSW,16,20,-,-,0600],[2,SSE,-,-,-,-,NOTIME],[3,VAR,04,08,-,-,2400]]Corpus SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY LATE EVENINGReference 1 SSW?LY 16-20 GRADUALLY BACKING SSE?LY THEN DECREASING VARIABLE 4-8 BY LATE EVENINGReference 2 SSW 16-20 GRADUALLY BACKING SSE BY 1800 THEN FALLING VARIABLE 4-8 BY LATE EVENINGSUMTIME-Hyb.
SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHTpCRU-greedy SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY LATE EVENINGpCRU-roulette SSW 16-20 GRADUALLY BACKING SSE AND VARIABLE 4-8pCRU-viterbi SSW 16-20 BACKING SSE VARIABLE 4-8 LATERpCRU-2gram SSW 16-20 BACKING SSE VARIABLE 4-8 LATERpCRU-random SSW 16-20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOON THEN VARIABLE 4-8Table 1: Forecast texts (for 05-10-2000) generated by each of the pCRU generators, the SUMTIME-Hybridsystem and three experts.
The corresponding input to the generators is shown in the first row.BLEU (Papineni et al, 2002) is a precision met-ric that assesses the quality of a translation in termsof the proportion of its word n-grams (n ?
4 hasbecome standard) that it shares with several refer-ence translations.
BLEU also incorporates a ?brevitypenalty?
to counteract scores increasing as length de-creases.
BLEU scores range from 0 to 1.The NIST metric (Doddington, 2002) is an adapta-tion of BLEU, but where BLEU gives equal weight toall n-grams, NIST gives more weight to less frequent(hence more informative) n-grams.
There is evi-dence that NIST correlates better with human judg-ments than BLEU (Doddington, 2002; Belz and Re-iter, 2006).The results below include human scores from twoseparate experiments.
The first was an experimentwith 9 subjects experienced in reading marine fore-casts (Belz and Reiter, 2006), the second is a newexperiment with 14 similarly experienced subjects3.The main differences were that in Experiment 1,subjects rated on a scale from 0 to 5 and were askedfor overall quality scores, whereas in Experiment 2,subjects rated on a 1?7 scale and were asked for lan-guage quality scores.In comparing different pCRU modes, NIST andBLEU scores were computed against the test set partof the corpus which contains texts by five differentauthors.
In the two human experiments, NIST andBLEU scores were computed against sets of multi-ple reference texts (2 for each date in Experiment 1,and 3 in Experiment 2) written by forecasters whohad not contributed to the corpus.
One-way ANOVAswith post-hoc Tukey HSD tests were used to analysevariance and statistical significance of all results.Table 1 shows forecast texts generated by each of3Belz and Reiter, in preparation.NIST-5 BLEU-4T pCRU-greedy 8.208 (0.033) 0.647 (0.002)R pCRU-roulette 7.035 (0.138) 0.496 (0.010)A pCRU-2gram 6.734 (0.086) 0.523 (0.008)I pCRU-viterbi 6.643 (0.023) 0.524 (0.002)N pCRU-random 4.799 (0.036) 0.296 (0.002)pCRU-greedy 6.927 (0.131) 0.636 (0.016)T pCRU-roulette 6.193 (0.121) 0.496 (0.022)E pCRU-2gram 5.663 (0.185) 0.514 (0.019)S pCRU-viterbi 5.650 (0.161) 0.519 (0.021)T pCRU-random 4.535 (0.078) 0.313 (0.005)Table 2: NIST-5 and BLEU-4 scores for training andtest sets (average variation from the mean).the systems included in the evaluations reported be-low, together with the corresponding input and threetexts created by humans for the same data.3.4.2 Comparing different generation modesTable 2 shows results for the five different pCRUgeneration modes, for training sets (top) and test sets(bottom), in terms of NIST-5 and BLEU-4 scores av-eraged over the five runs of the hold-out validation,with average mean deviation figures across the runsshown in brackets.The Tukey Test produced the following results forthe differences between means in Table 2.
For thetraining set, results are the same for NIST and BLEUscores: all differences are significant at P < 0.01,except for the differences in scores for pCRU-2gramand pCRU-viterbi.
For the test set and NIST, again alldifferences are significant at P < 0.01, except forpCRU-2gram vs. pCRU-viterbi.
For the test set andBLEU, three differences are non-significant: pCRU-2gram vs. pCRU-viterbi, pCRU-2gram vs. pCRU-168Experiment 1 Experiment 2SUMTIME-Hyb.
3.82 (1) 4.61 (2)pCRU-greedy 3.59 (2) 4.79 (1)pCRU-roulette 3.22 (3) 4.54 (3)Table 3: Scores for handcrafted system and two bestpCRU-systems from two human experiments.roulette, and pCRU-viterbi vs. pCRU-roulette.NIST-5 depends on test set size, and is necessar-ily lower for the (smaller) test set, but the BLEU-4scores indicate that performance was slightly worseon test sets.
The deviation figures show that varia-tion was also higher on the test sets.The clearest result is that pCRU-greedy is rankedhighest, and pCRU-random lowest, by considerablemargins.
pCRU-roulette is ranked second by NIST-5 and fourth by BLEU-4.
pCRU-2gram and pCRU-viterbi are virtually indistinguishable.Experts in both human experiments agreed withthe NIST-5 rankings of the modes exactly.3.4.3 Text quality against handcrafted systemThe pCRU modes were also evaluated againstthe SUMTIME-Hybrid system (running in ?hybrid?mode, taking inputs as in Table 1).
Table 3 showsaveraged evaluation scores by subjects in the two in-dependent experiments described above.
There werealtogether 6 and 7 systems evaluated in these experi-ments, respectively, and the differences between thescores shown here were not significant when sub-jected to the Tukey Test, meaning that both experi-ments failed to show that experts can tell the differ-ence in the language quality of the texts generated bythe handcrafted SUMTIME-Hybrid system and thetwo best pCRU-greedy systems.3.4.4 Text quality against human forecastersIn the first experiment, the human evaluators gavean average score of 3.59 to pCRU-greedy, 3.22 tothe corpus texts, and 3.03 to another (human) fore-caster.
In Experiment 2, the average human scoreswere 4.79 for pCRU-greedy, and 4.50 for the corpustexts.
Although in each experiment separately, sta-tistical significance could not be shown for the dif-ferences between these means, in combination thescores provide evidence that the evaluators thoughtpCRU-greedy better than the human-written texts.3.4.5 Computing timeThe following table shows average number of sec-onds taken to generate one forecast, averaged overthe five cross-validation runs (mean variation figuresacross the runs in brackets):Training sets Test setspCRU-greedy: 1.65s (= 0.02) 1.58s (< 0.04)pCRU-roulette: 1.61s (< 0.02) 1.58s (< 0.05)pCRU-viterbi: 1.74s (< 0.02) 1.70s (= 0.04)pCRU-2gram: 2.83s (< 0.02) 2.78s (< 0.09)Forecasts for the test sets were generated some-what faster than for the training sets in all modes.Variation was greater for test sets.
Differencesbetween pCRU-greedy and pCRU-roulette are verysmall, but pCRU-viterbi took 1/10 of a secondlonger, and pCRU-2gram took more than 1 secondlonger to generate the average forecast4.3.4.6 Brevity biasN -gram models have a built-in bias in favour ofshorter strings, because they calculate the likelihoodof a string of words as the joint probability of thewords, or, more precisely, as the product of the prob-abilities of each word given the n ?
1 precedingwords.
The likelihood of any string will thereforegenerally be lower than that of any of its substrings.Using a smaller data set for which all systems hadoutputs, the average number of words in the fore-casts generated by the different systems was:pCRU-random: 19.43SUMTIME-Hybrid: 12.39pCRU-greedy: 11.51Corpus: 11.28pCRU-roulette: 10.48pCRU-2gram: 7.66pCRU-viterbi: 7.54pCRU-random has no preference for shorterstrings, its average string length is almost twice thatof the other pCRU-generators.
The 2-gram generatorprefers shorter strings, while the Viterbi generatorprefers shorter generation processes, and these pref-erences result in the shortest texts.
The poor evalu-ation results above for the n-gram and Viterbi gen-erators indicate that this brevity bias can be harm-4The Viterbi and the 2-gram generator were implementedidentically, except for the n-gram model look-up.169ful in NLG.
The remaining generators achieve goodmatches to the average forecast length in the corpus.3.4.7 Development timeThe most time-consuming part of NLG system de-velopment is not encoding the range of alternatives,but the decision-making capabilities that enable se-lection among them.
In SUMTIME (Section 3), thesewere the result of corpus analysis and consultationwith writers and readers of marine forecasts.
In thepCRU wind forecast generators, the decision-makingcapabilities are acquired automatically, no expertknowledge or corpus annotation is used.The SUMTIME team estimate5 that very approx-imately 12 person months went directly into devel-oping the SUMTIME microplanner and realiser (thecomponents functionally analogous to the pCRU-generators), and 24 on generic activities such asexpert consultation, which also benefited the mi-croplanner/realiser.
The pCRU wind forecasterswere built in less than a month, including familiari-sation with the corpus, building the chunker and cre-ating the generation rules themselves.
However, theSUMTIME system also generates wave forecasts andappropriate layout and canned text.
A generous esti-mate is that it would take another two person monthsto equip the pCRU forecaster with these capabilities.This is not to say that the two research efforts re-sulted in exactly the same thing.
It is clear that fore-cast readers prefer the SUMTIME system, but thepoint is that it did come with a substantial price tagattached.
The pCRU approach allows control overthe trade-off between cost and quality.4 DiscussionThe main contributions of the research describedin this paper are: (i) a generation methodologythat improves substantially on development time andreusability compared to traditional hand-crafted sys-tems; (ii) techniques for training linguistically in-formed decision-making components for probabilis-tic NLG from raw corpora; and (iii) results that showthat probabilistic NLG can produce high-quality text.Results also show that (i) a preference for shorterrealisations can be harmful in NLG; and that (ii)linguistically literate, probabilistic NLG can outper-5Personal communication with E. Reiter and S. Sripada.form HALOGEN-style shallow statistical methods, interms of quality and efficiency.An interesting question concerns the contributionof the manually built component (the base genera-tor) to the quality of the outputs.
The random modeserves as an absolute baseline in this respect: it in-dicates how well a particular base generator per-forms on its own.
However, different base genera-tors have different effects on the generation modes.The base generator that was used in previous exper-iments (Belz, 2005) encoded a less structured gen-eration space and the set of concepts it used wereless fine-grained (e.g.
it did not distinguish betweenan increase and a decrease in wind speed, consid-ering both simply a change), and therefore it lackedsome information necessary for deriving conditionalprobabilities for lexical choice (e.g.
freshening vs.easing).
As predicted (Belz, 2005, p. 21), improve-ments to the base generator made little difference tothe results for pCRU-2gram (up from BLEU 0.45 to0.5), but greatly improved the performance of thegreedy mode (up from 0.43 to 0.64).A basic question for statistical NLG is whethersurface string likelihoods are enough to resolve re-maining non-determinism in generators, or whetherlikelihoods at the more abstract level of generationrules are needed.
The former always prefers themost frequent variant regardless of context, whereasin the latter probabilities can attach to linguistic ob-jects and be conditioned on contextual features (e.g.one useful feature in the forecast text generators en-coded whether a rule was being applied at the be-ginning of a text).
The results reported in this paperprovide evidence that probabilistic generation can bemore powerful than n-gram based post-selection.5 ConclusionsThe pCRU approach to generation makes it possi-ble to combine the potential accuracy and subtletyof symbolic generation rules with detailed linguis-tic features on the one hand, and the robustness andhandle on nondeterminism provided by probabili-ties associated with these rules, on the other.
Theevaluation results for the pCRU generators show thatoutputs of high quality can be produced with thisapproach, that it can speed up development and im-prove reusability of systems, and that in some modes170it is more efficient and less brevity-biased than exist-ing HALOGEN-style n-gram techniques.The current situation in NLG recalls NLU in thelate 1980s, when symbolic and statistical NLP wereseparate research paradigms, a situation memorablycaricatured by Gazdar (1996), before rapidly mov-ing towards a paradigm merger in the early 1990s.A similar development is currently underway in MTwhere ?
after several years of statistical MT dom-inating the field ?
researchers are now beginningto bring linguistic knowledge into statistical tech-niques (Charniak et al, 2003; Huang et al, 2006),and this trend looks set to continue.
The lesson fromNLU and MT appears to be that higher quality re-sults when the symbolic and statistical paradigmsjoin forces.
The research reported in this paper isintended to be a first step in this direction for NLG.AcknowledgmentsThis research was in part supported under UK EPSRCGrant GR/S24480/01.
Many thanks to the anony-mous reviewers for very helpful comments.ReferencesA.
Belz and E. Reiter.
2006.
Comparing automatic andhuman evaluation of NLG systems.
In Proc.
EACL?06,pages 313?320.A.
Belz.
2004.
Context-free representational underspec-ification for NLG.
Technical Report ITRI-04-08, Uni-versity of Brighton.A.
Belz.
2005.
Statistical generation: Three methodscompared and evaluated.
In Proc.
of ENLG?05, pages15?23.A.
Belz.
2006. pCRU: Probabilistic generation usingrepresentational underspecification.
Technical ReportNLTG-06-01, University of Brighton.A.
Cahill and J. van Genabith.
2006.
Robust PCFG-based generation using automatically acquired LFGapproximations.
In Proc.
ACL?06, pages 1033?44.E.
Charniak, K. Knight, and K. Yamada.
2003.
Syntax-based language models for machine translation.
InProc.
MT Summit IX.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In Proceedings of the ARPA Workshop on HumanLanguage Technology.G.
Gazdar.
1996.
Paradigm merger in NLP.
In RobinMilner and Ian Wand, editors, Computing Tomor-row: Future Research Directions in Computer Sci-ence, pages 88?109.
Cambridge University Press.E.
Hovy.
1988.
Generating Natural Language underPragmatic Constraints.
Lawrence Erlbaum.L.
Huang, K. Knight, and A. Joshi.
2006.
Statisticalsyntax-directed translation with extended domain oflocality.
In Proc.
AMTA, pages 66?73.K.
Knight and I. Langkilde.
1998.
Generation that ex-ploits corpus-based statistical knowledge.
In Proceed-ings of COLING-ACL?98, pages 704?710.I.
Langkilde.
2005.
An exploratory application of con-straint optimization in Mozart to probabilistic naturallanguage processing.
In Proceedings of CSLP?05, vol-ume 3438 of LNAI.
Springer-Verlag.T.
Marciniak and M. Strube.
2005.
Using an annotatedcorpus as a knowledge source for language generation.In Proceedings of UCNLG?05, pages 19?24.D.
S. Paiva and R. Evans.
2005.
Empirically-based con-trol of natural language generation.
In ProceedingsACL?05.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: A method for automatic evaluation of machinetranslation.
In Proc.
ACL ?02, pages 311?318.R.
Power.
2000.
Planning texts by constraint satisfaction.In Proceedings of COLING?00.E.
Reiter, S. Sripada, J.
Hunter, and J. Yu.
2005.
Choos-ing words in computer-generated weather forecasts.Artificial Intelligence, 167:137?169.E.
Reiter.
1994.
Has a consensus NL generation architec-ture appeared and is it psycholinguistically plausible?In Proceedings of INLG?94, pages 163?170.A.
Stolcke.
2002.
SRILM: An extensible language mod-eling toolkit.
In Proceedings of ICSLP?02, pages 901?904,.S.
Varges and C. Mellish.
2001.
Instance-based NLG.
InProc.
of NAACL?01, pages 1?8.E.
Velldal, S. Oepen, and D. Flickinger.
2004.
Para-phrasing treebanks for stochastic realization ranking.In Proc.
of TLT?04.M.
White.
2004.
Reining in CCG chart realization.
InProceedings INLG?04, volume 3123 of LNAI, pages182?191.
Springer.171
