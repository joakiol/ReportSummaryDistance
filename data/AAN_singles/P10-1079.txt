Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 770?779,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsA hybrid rule/model-based finite-state frameworkfor normalizing SMS messagesRichard Beaufort1 Sophie Roekhaut2 Louise-Am?lie Cougnon1 C?drick Fairon1(1) CENTAL, Universit?
catholique de Louvain ?
1348 Louvain-la-Neuve, Belgium{richard.beaufort,louise-amelie.cougnon,cedrick.fairon}@uclouvain.be(2) TCTS Lab, Universit?
de Mons ?
7000 Mons, Belgiumsophie.roekhaut@umons.ac.beAbstractIn recent years, research in naturallanguage processing has increasinglyfocused on normalizing SMS messages.Different well-defined approaches havebeen proposed, but the problem remainsfar from being solved: best systemsachieve a 11% Word Error Rate.
Thispaper presents a method that sharessimilarities with both spell checkingand machine translation approaches.
Thenormalization part of the system is entirelybased on models trained from a corpus.Evaluated in French by 10-fold-crossvalidation, the system achieves a 9.3%Word Error Rate and a 0.83 BLEU score.1 IntroductionIntroduced a few years ago, Short MessageService (SMS) offers the possibility of exchangingwritten messages between mobile phones.
SMShas quickly been adopted by users.
Thesemessages often greatly deviate from traditionalspelling conventions.
As shown by specialists(Thurlow and Brown, 2003; Fairon et al,2006; Bieswanger, 2007), this variability isdue to the simultaneous use of numerous codingstrategies, like phonetic plays (2m1 read ?demain?,?tomorrow?
), phonetic transcriptions (kom insteadof ?comme?, ?like?
), consonant skeletons (tjrsfor ?toujours?, ?always?
), misapplied, missingor incorrect separators (j esper for ?j?esp?re?, ?Ihope?
; j?croibi1k, instead of ?je crois bien que?,?I am pretty sure that?
), etc.
These deviationsare due to three main factors: the small numberof characters allowed per text message by theservice (140 bytes), the constraints of the smallphones?
keypads and, last but not least, the factthat people mostly communicate between friendsand relatives in an informal register.Whatever their causes, these deviationsconsiderably hamper any standard naturallanguage processing (NLP) system, whichstumbles against so many Out-Of-Vocabularywords.
For this reason, as noted by Sproat et al(2001), an SMS normalization must be performedbefore a more conventional NLP process canbe applied.
As defined by Yvon (2008), ?SMSnormalization consists in rewriting an SMS textusing a more conventional spelling, in orderto make it more readable for a human or for amachine.
?The SMS normalization we present here wasdeveloped in the general framework of an SMS-to-speech synthesis system1.
This paper, however,only focuses on the normalization process.Evaluated in French, our method sharessimilarities with both spell checking and machinetranslation.
The machine translation-like moduleof the system performs the true normalizationtask.
It is entirely based on models learned froman SMS corpus and its transcription, alignedat the character-level in order to get parallelcorpora.
Two spell checking-like modulessurround the normalization module.
The firstone detects unambiguous tokens, like URLsor phone numbers, to keep them out of thenormalization.
The second one, applied on thenormalized parts only, identifies non-alphabeticsequences, like punctuations, and labels themwith the corresponding token.
This greatly helpsthe system?s print module to follow the basic rulesof typography.This paper is organized as follows.
Section 2proposes an overview of the state of the art.Section 3 presents the general architecture ofour system, while Section 4 focuses on how welearn and combine our normalization models.Section 5 evaluates the system and compares it to1The Vocalise project.See cental.fltr.ucl.ac.be/team/projects/vocalise/.770previous works.
Section 6 draws conclusions andconsiders some future possible improvements ofthe method.2 Related workAs highlighted by Kobus et al (2008b), SMSnormalization, up to now, has been handledthrough three well-known NLP metaphors: spellchecking, machine translation and automaticspeech recognition.
In this section, we onlypresent the pros and cons of these approaches.Their results are given in Section 5, focused onour evaluation.The spell checking metaphor (Guimier de Neefet al, 2007; Choudhury et al, 2007; Cook andStevenson, 2009) performs the normalization taskon a word-per-word basis.
On the assumptionthat most words should be correct for the purposeof communication, its principle is to keep In-Vocabulary words out of the correction process.Guimier de Neef et al (2007) proposed a rule-based system that uses only a few linguisticresources dedicated to SMS, like specific lexiconsof abbreviations.
Choudhury et al (2007)and Cook and Stevenson (2009) preferred toimplement the noisy channel metaphor (Shannon,1948), which assumes a communication processin which a sender emits the intended messageW through an imperfect (noisy) communicationchannel, such that the sequence O observed bythe recipient is a noisy version of the originalmessage.
On this basis, the idea is to recover theintended message W hidden behind the sequencesof observations O, by maximizing:Wmax = arg maxP (W |O) (1)= arg maxP (O|W )P (W )P (O)where P (O) is ignored because constant,P (O|W ) models the channel?s noise, and P (W )models the language of the source.
Choudhury etal.
(2007) implemented the noisy channel througha Hidden-Markov Model (HMM) able to handleboth graphemic variants and phonetic plays asproposed by (Toutanova and Moore, 2002), whileCook and Stevenson (2009) enhanced the modelby adapting the channel?s noise P (O|W,wf)according to a list of predefined observedword formations {wf}: stylistic variation, wordclipping, phonetic abbreviations, etc.
Whateverthe system, the main limitation of the spellchecking approach is the excessive confidence itplaces in word boundaries.The machine translation metaphor, which ishistorically the first proposed (Bangalore et al,2002; Aw et al, 2006), considers the process ofnormalizing SMS as a translation task from asource language (the SMS) to a target language(its standard written form).
This standpoint isbased on the observation that, on the one side,SMS messages greatly differ from their standardwritten forms, and that, on the other side, mostof the errors cross word boundaries and requirea wide context to be handled.
On this basis,Aw et al (2006) proposed a statistical machinetranslation model working at the phrase-level,by splitting sentences into their k most probablephrases.
While this approach achieves really goodresults, Kobus et al (2008b) make the assertionthat a phrase-based translation can hardly capturethe lexical creativity observed in SMS messages.Moreover, the translation framework, which canhandle many-to-many correspondences betweensources and targets, exceeds the needs of SMSnormalization, where the normalization task isalmost deterministic.Based on this analysis, Kobus et al (2008b)proposed to handle SMS normalization throughan automatic speech recognition (ASR) metaphor.The starting point of this approach is theobservation that SMS messages present a lotof phonetic plays that sometimes make theSMS word (sr?, mwa) closer to its phoneticrepresentation ([sKe], [mwa]) than to its standardwritten form (serai, ?will be?, moi, ?me?
).Typically, an ASR system tries to discover thebest word sequence within a lattice of weightedphonetic sequences.
Applied to the SMSnormalization task, the ASR metaphor consistsin first converting the SMS message into a phonelattice, before turning it into a word-based latticeusing a phoneme-to-grapheme dictionary.
Alanguage model is then applied on the wordlattice, and the most probable word sequence isfinally chosen by applying a best-path algorithmon the lattice.
One of the advantages of thegrapheme-to-phoneme conversion is its intrinsicability to handle word boundaries.
However,this step also presents an important drawback,raised by the authors themselves: it preventsnext normalization steps from knowing whatgraphemes were in the initial sequence.771Our approach, which is detailed in Sections 3and 4, shares similarities with both the spellchecking approach and the machine translationprinciples, trying to combine the advantagesof these methods, while leaving aside theirdrawbacks: like in spell checking systems, wedetect unambiguous units of text as soon aspossible and try to rely on word boundaries whenthey seem reliable enough; but like in the machinetranslation task, our method intrinsically handlesword boundaries in the normalization process ifneeded.3 Overview of the system3.1 Tools in useIn our system, all lexicons, language modelsand sets of rules are compiled into finite-statemachines (FSMs) and combined with the inputtext by composition (?).
The reader who isnot familiar with FSMs and their fundamentaltheoretical properties, like composition, is urgedto consult the state-of-the-art literature (Rocheand Schabes, 1997; Mohri and Riley, 1997; Mohriet al, 2000; Mohri et al, 2001).We used our own finite-state tools: a finite-statemachine library and its associated compiler(Beaufort, 2008).
In conformance with the formatof the library, the compiler builds finite-statemachines from weighted rewrite rules, weightedregular expressions and n-gram models.3.2 AimsWe formulated four constraints before fixing thesystem?s architecture.
First, special tokens, likeURLs, phones or currencies, should be identifiedas soon as possible, to keep them out of thenormalization process.Second, word boundaries should be taken intoaccount, as far as they seem reliable enough.
Theidea, here, is to base the decision on a learningable to catch frequent SMS sequences to includein a dedicated In-Vocabulary (IV) lexicon.Third, any other SMS sequence should beconsidered as Out-Of-Vocabulary (OOV), onwhich in-depth rewritings may be applied.Fourth, the basic rules of typography andtypesettings should be applied on the normalizedversion of the SMS message.3.3 ArchitectureThe architecture depicted in Figure 1 directlyrelies on these considerations.
In short, anSMS message first goes through three SMSmodules, which normalize its noisy parts.Then, two standard NLP modules produce amorphosyntactic analysis of the normalized text.A last module, finally, takes advantage of thislinguistic analysis either to print a text that followsthe basic rules of typography, or to synthesize thecorresponding speech signal.Because this paper focuses on the normalizationtask, the rest of this section only presents theSMS modules and the ?smart print?
output.
Themorphosyntactic analysis, made of state-of-the-artalgorithms, is described in (Beaufort, 2008), andthe text-to-speech synthesis system we use ispresented in (Colotte and Beaufort, 2005).3.3.1 SMS modulesSMS preprocessing.
This module relieson a set of manually-tuned rewrite rules.
Itidentifies paragraphs and sentences, but also someSMS ModulesSMS PreprocessingSMS NormalizationSMS PostprocessingStandard NLP ModulesMorphological analysisContextual disambiguationTTS engineSmart printSMS messageStandardwritten messageSpeechFigure 1: Architecture of the system772unambiguous tokens: URLs, phone numbers,dates, times, currencies, units of measurementand, last but not least in the context of SMS,smileys2.
These tokens are kept out of thenormalization process, while any other sequenceof characters is considered ?
and labelled ?
asnoisy.SMS normalization.
This module only usesmodels learned from a training corpus (cf.
Section4).
It involves three steps.
First, an SMS-dedicated lexicon look-up, which differentiatesbetween known and unknown parts of a noisytoken.
Second, a rewrite process, which creates alattice of weighted solutions.
The rewrite modeldiffers depending on whether the part to rewriteis known or not.
Third, a combination of thelattice of solutions with a language model, and thechoice of the best sequence of lexical units.
Atthis stage, the normalization as such is completed.SMS postprocessing.
Like the preprocessor,the postprocessor relies on a set of manually-tuned rewrite rules.
The module is only appliedon the normalized version of the noisy tokens,with the intention to identify any non-alphabeticsequence and to isolate it in a distinct token.At this stage, for instance, a point becomes a?strong punctuation?.
Apart from the list oftokens already managed by the preprocessor,the postprocessor handles as well numeric andalphanumeric strings, fields of data (like bankaccount numbers), punctuations and symbols.3.3.2 Smart printThe smart print module, based on manually-tunedrules, checks either the kind of token (chosenby the SMS pre-/post-processing modules)or the grammatical category (chosen by themorphosyntactic analysis) to make the righttypography choices, such as the insertion ofa space after certain tokens (URLs, phonenumbers), the insertion of two spaces aftera strong punctuation (point, question mark,exclamation mark), the insertion of two carriagereturns at the end of a paragraph, or the uppercase of the initial letter at the beginning of thesentence.2Our list contains about 680 smileys.4 The normalization models4.1 Overview of the normalization algorithmOur approach is an approximation of the noisychannel metaphor (cf.
Section 2).
It differsfrom this general framework, because we adaptthe model of the channel?s noise dependingon whether the noisy token (our sequenceof observations) is In-Vocabulary or Out-Of-Vocabulary:P (O|W ) =??
?PIV (O|W ) if O ?
IVPOOV (O|W ) else(2)Indeed, our algorithm is based on the assumptionthat applying different normalization models to IVand OOV words should both improve the resultsand reduce the processing time.For this purpose, the first step of the algorithmconsists in composing a noisy token T with anFST Sp whose task is to differentiate betweensequences of IV words and sequences of OOVwords, by labelling them with a special IV or OOVmarker.
The token is then split in n segments sgiaccording to these markers:{sg} = Split(T ?
Sp) (3)In a second step, each segment is composedwith a rewrite model according to its kind: the IVrewrite modelRIV for sequences of IV words, andthe OOV rewrite model ROOV for sequences ofOOV words:sg?i =??
?sgi ?RIV if sgi ?
IVsgi ?ROOV else(4)All rewritten segments are then concatenatedtogether in order to get back the complete token:T = ni=1(sg?i) (5)where  is the concatenation operator.The third and last normalization step is appliedon a complete sentence S. All tokens Tj of Sare concatenated together and composed with thelexical language model LM .
The result of thiscomposition is a word lattice, of which we takethe most probable word sequence S?
by applyinga best-path algorithm:S?
= BestPath( (mj=1Tj) ?
LM ) (6)where m is the number of tokens of S. In S?,each noisy token Tj of S is mapped onto its mostprobable normalization.7734.2 The corpus alignmentOur normalization models were trained on aFrench SMS corpus of 30,000 messages, gatheredin Belgium, semi-automatically anonymized andmanually normalized by the Catholic Universityof Louvain (Fairon and Paumier, 2006).
Together,the SMS corpus and its transcription constituteparallel corpora aligned at the message-level.However, in order to learn pieces of knowledgefrom these corpora, we needed a string alignmentat the character-level.One way of implementing this string alignmentis to compute the edit-distance of two strings,which measures the minimum number ofoperations (substitutions, insertions, deletions)required to transform one string into the other(Levenshtein, 1966).
Using this algorithm,in which each operation gets a cost of 1, twostrings may be aligned in different ways withthe same global cost.
This is the case, forinstance, for the SMS form kozer ([koze]) andits standard transcription caus?
(?talked?
), asillustrated by Figure 2.
However, from a linguisticstandpoint, alignment (1) is preferable, becausecorresponding graphemes are aligned on their firstcharacter.In order to automatically choose this preferredalignment, we had to distinguish the three edit-operations, according to the characters to bealigned.
For that purpose, probabilities wererequired.
Computing probabilities for eachoperation according to the characters to be alignedwas performed through an iterative algorithmdescribed in (Cougnon and Beaufort, 2009).
Inshort, this algorithm gradually learns the best wayof aligning strings.
On our parallel corpora, itconverged after 7 iterations and provided us witha result from which the learning could start.
(1) ko_ser (2) k_osercaus?_ caus?_(3) ko_ser (4) k_osercaus_?
caus_?Figure 2: Different equidistant alignments, usinga standard edit-cost of 1.
Underscores (?_?)
meaninsertion in the upper string, and deletion in thelower string.4.3 The split model SpIn natural language processing, a word iscommonly defined as ?a sequence of alphabeticcharacters between separators?, and an IV word issimply a word that belongs to the lexicon in use.In SMS messages however, separators aresurely indicative, but not reliable.
For this reason,our definition of the word is far from the previousone, and originates from the string alignment.After examining our parallel corpora aligned atthe character-level, we decided to consider as aword ?the longest sequence of characters parsedwithout meeting the same separator on both sidesof the alignment?.
For instance, the followingalignmentJ esper_ k___tu va_J?esp?re que tu vas(I hope that you will)is split as follows according to our definition:J esper_ k___tu va_J?esp?re que tu vassince the separator in ?J esper?
is differentfrom its transcription, and ?ktu?
does notcontain any separator.
Thus, this SMS sequencecorresponds to 3 SMS words: [J esper], [ktu] and[va].A first parsing of our parallel corpora providedus with a list of SMS sequences corresponding toour IV lexicon.
The FST Sp is built on this basis:Sp = ( S?
(I|O) ( S+(I|O) )?
S? )
?G (7)where:?
I is an FST corresponding to the lexicon,in which IV words are mapped onto the IVmarker.?
O is the complement of I3.
In this OOVlexicon, OOV sequences are mapped onto theOOV marker.?
S is an FST corresponding to the list ofseparators (any non-alphabetic and non-numeric character), mapped onto a SEPmarker.3Actually, the true complement of I accepts sequenceswith separators, while these sequences were removed fromO.774?
G is an FST able to detect consecutivesequences of IV (resp.
OOV) words, and togroup them under a unique IV (resp.
OOV)marker.
By gathering sequences of IVs andOOVs, SEP markers disappear from Sp.Figure 3 illustrates the composition of Sp withthe SMS sequence J esper kcv b1 (J?esp?re que ?ava bien, ?I hope you are well?).
For the example,we make the assumption that kcv was never seenduring the training.eJsperkcvb1'  ''  ''  'IVIVOOVFigure 3: Application of the split model Sp.
TheOOV sequence starts and ends with separators.4.4 The IV rewrite model RIVThis model is built during a second parsingof our parallel corpora.
In short, the parsingsimply gathers all possible normalizations foreach SMS sequence put, by the first parsing, inthe IV lexicon.
Contrary to the first parsing, thissecond one processes the corpus without takingseparators into account, in order to make surethat all possible normalizations are collected.Each normalization w?
for a given SMSsequence w is weighted as follows:p(w?|w) =Occ(w?, w)Occ(w)(8)where Occ(x) is the number of occurrences of x inthe corpus.
The FST RIV is then built as follows:RIV = SIV?
IVR ( SIV+ IVR )?
SIV?
(9)where:?
IVR is a weighted lexicon compiled into anFST, in which each IV sequence is mappedonto the list of its possible normalizations.?
SIV is a weighted lexicon of separators, inwhich each separator is mapped onto the listof its possible normalizations.
The deletionis often one of the possible normalization ofa separator.
Otherwise, the deletion is addedand is weighted by the following smoothedprobability:p(DEL|w) =0.1Occ(w) + 0.1(10)4.5 The OOV rewrite model ROOVIn contrast to the other models, this one is not aregular expression made of weighted lexicons.It corresponds to a set of weighted rewrite rules(Chomsky and Halle, 1968; Johnson, 1972; Mohriand Sproat, 1996) learned from the alignment.Developed in the framework of generativephonology, rules take the form??
?
: ?
_ ?
/ w (11)which means that the replacement ?
?
?
isonly performed when ?
is surrounded by ?
onthe left and ?
on the right, and gets the weight w.However, in our case, rules take the simpler form??
?
/ w (12)which means that the replacement ?
?
?
isalways performed, whatever the context.Inputs of our rules (?)
are sequences of1 to 5 characters taken from the SMS sideof the alignment, while outputs (?)
are theircorresponding normalizations.
Our rules aresorted in the reverse order of the length of theirinputs: rules with longer inputs come first in thelist.Long-to-short rule ordering reduces the numberof proposed normalizations for a given SMSsequence for two reasons:1. the firing of a rule with a longer input blocksthe firing of any shorter sub-rule.
This is dueto a constraint expressed on lists of rewriterules: a given rule may be applied only if nomore specific and relevant rule has been methigher in the list;2. a rule with a longer input usually has feweralternative normalizations than a rule with ashorter input does, because the longer SMSsequence likely occurred paired with feweralternative normalizations in the trainingcorpus than did the shorter SMS sequence.Among the wide set of possible sequencesof 2 to 5 characters gathered from the corpus,we only kept in our list of rules the sequencesthat allowed at least one normalization solelymade of IV words.
It is important to notice thathere, we refer to the standard notion of IV word:while gathering the candidate sequences from thecorpus, we systematically checked each word ofthe normalizations against a lexicon of French775standard written forms.
The lexicon we usedcontains about 430,000 inflected forms and isderived from Morlex4, a French lexical database.Figure 4 illustrates these principles by focusingon 3 input sequences: aussi, au and a. Asshown by the Figure, all rules of a set dedicatedto the same input sequence (for instance, aussi)are optional (??
), except the last one, which isobligatory (?).
In our finite-state compiler, thisconvention allows the application of all concurrentnormalizations on the same input sequence, asdepicted in Figure 5.In our real list of OOV rules, the input sequencea corresponds to 231 normalizations, while auaccepts 43 normalizations and aussi, only 3.
Thishighlights the interest, in terms of efficiency, of thelong-to-short rule ordering.4.6 The language modelOur language model is an n-gram of lexicalforms, smoothed by linear interpolation (Chenand Goodman, 1998), estimated on the normalizedpart of our training corpus and compiled into aweighted FST LMw.At this point, this FST cannot be combined withour other models, because it works on lexical unitsand not on characters.
This problem is solvedby composing LMw with another FST L, whichrepresents a lexicon mapping each input word,considered as a string of characters, onto the sameoutput words, but considered here as a lexicalunit.
Lexical units are then permanently removedfrom the language model by keeping only the firstprojection (the input side) of the composition:LM = FirstProjection( L ?
LMw ) (13)In this model, special characters, likepunctuations or symbols, are represented bytheir categories (light, medium and strongpunctuations, question mark, symbol, etc.
), whilespecial tokens, like URLs or phone numbers,are handled as token values (URL, phone, etc.
)instead of as sequences of characters.
Thisreduces the complexity of the model.As we explained earlier, tokens of a samesentence S are concatenated together at the endof the second normalization step.
During thisconcatenation process, sequences correspondingto special tokens are automatically replaced bytheir token values.
Special characters, however,4See http://bach.arts.kuleuven.be/pmertens/.
"aussi" ?-> "au si" / 8.4113 (*)"aussi" ?-> "ou si" / 6.6743 (*)"aussi" -> "aussi" / 0.0189 (*)......"au" ?-> "ow" / 14.1787..."au" ?-> "?t" / 12.5938"au" ?-> "du" / 12.1787 (*)"au" ?-> "o" / 11.8568..."au" ?-> "on" / 10.8568 (*)..."au" ?-> "aud" / 9.9308"au" ?-> "aux" / 6.1731 (*)"au" -> "au" / 0.0611 (*)......"a" ?-> "a d" / 17.8624"a" ?-> "ation" / 17.8624"a" ?-> "?ts" / 17.8624..."a" ?-> "ablement" / 16.8624"a" ?-> "anisation" / 16.8624..."a" ?-> "u" / 15.5404"a" ?-> "y a" / 15.5404..."a" ?-> "abilit?"
/ 13.4029"a" ?-> "?-" / 12.1899"a" ?-> "ar" / 11.5225"a" ?-> \DEL / 9.1175"a" ?-> "?a" / 6.2019"a" ?-> "?"
/ 3.5013"a" -> "a" / 0.3012Figure 4: Samples from the list of OOVrules.
Rules?
weights are negative logarithmsof probabilities: smaller weights are thus better.Asterisks indicate normalizations solely made ofFrench IV words.aa:o/6.67 uu ?
:" "/8.41s/0.02 s s i?
:" "Figure 5: Application of the OOV rules onthe input sequence aussi.
All normalizationscorresponding to this sequence were allowed,while rules corresponding to shorter inputsequences were ignored.776are still present in S. For this reason, S is firstcomposed with an FST Reduce, which maps eachspecial character onto its corresponding category:S ?Reduce ?
LM (14)5 EvaluationThe performance and the efficiency of our systemwere evaluated on a MacBook Pro with a 2.4 GHzIntel Core 2 Duo CPU, 4 GB 667 MHz DDR2SDRAM, running Mac OS X version 10.5.8.The evaluation was performed on the corpusof 30,000 French SMS presented in Section 4.2,by ten-fold cross-validation (Kohavi, 1995).
Theprinciple of this method of evaluation is to splitthe initial corpus into 10 subsets of equal size.
Thesystem is then trained 10 times, each time leavingout one of the subsets from the training corpus, butusing only this omitted subset as test corpus.The language model of the evaluation is a3-gram.
We did not try a 4-gram.
This choicewas motivated by the experiments of Kobus etal.
(2008a), who showed on a French corpuscomparable to ours that, if using a larger languagemodel is always rewarded, the improvementquickly decreases with every higher level and isalready quite small between 2-gram and 3-gram.Table 1 presents the results in terms ofefficiency.
The system seems efficient, while wecannot compare it with other methods, which didnot provide us with this information.Table 2, part 1, presents the performance ofour approach (Hybrid) and compares it to a trivialcopy-paste (Copy).
The system was evaluatedin terms of BLEU score (Papineni et al, 2001),Word Error Rate (WER) and Sentence Error Rate(SER).
Concerning WER, the table presents thedistribution between substitutions (Sub), deletions(Del) and insertions (Ins).
The copy-paste resultsjust inform about the real deviation of our corpusfrom the traditional spelling conventions, andhighlight the fact that our system is still at painsto significantly reduce the SER, while resultsin terms of WER and BLEU score are quiteencouraging.Table 2, part 2, provides the results of thestate-of-the-art approaches.
The only results trulycomparable to ours are those of Guimier de Neefet al (2007), who evaluated their approach onthe same corpus as ours5; clearly, our method5They performed an evaluation without ten-fold cross-mean dev.bps 1836.57 159.63ms/SMS (140b) 76.23 22.34Table 1: Efficiency of the system.outperforms theirs.
Our results also seem a bitbetter than those of Kobus et al (2008a), althoughthe comparison with this system, also evaluated inFrench, is less easy: they combined the Frenchcorpus we used with another one and performeda single validation, using a bigger training corpus(36.704 messages) for a test corpus quite similarto one of our subsets (2.998 SMS).
Other systemswere evaluated in English, and results are moredifficult to compare; at least, our results seem inline with them.The analysis of the normalizations producedby our system pointed out that, most often, errorsare contextual and concern the gender (quel(le),?what?
), the number (bisou(s), ?kiss?
), the person([tu t?
]inqui?te(s), ?you are worried?)
or thetense (arriv?/arriver, ?arrived?/?to arrive?).
Thatcontextual errors are frequent is not surprising.
InFrench, as mentioned by Kobus et al (2008b), n-gram models are unable to catch this information,as it is generally out of their scope.On the other hand, this analysis confirmedour initial assumptions.
First, special tokens(URLs, phones, etc.)
are not modified.
Second,agglutinated words are generally split (Pensa ms?
Pense ?
mes, ?think to my?
), while misappliedseparators tend to be deleted (G t ?
J?
?tais, ?Iwas?).
Of course, we also found some errors atword boundaries ([il] l?arrange ?
[il] la range,?
[he] arranges?
?
?
[he] pits in order?
), but theywere fairly rare.6 Conclusion and perspectivesIn this paper, we presented an SMS normalizationframework based on finite-state machines anddeveloped in the context of an SMS-to-speechsynthesis system.
With the intention to avoidwrong modifications of special tokens and tohandle word boundaries as easily as possible, wedesigned a method that shares similarities withboth spell checking and machine translation.
Ourvalidation, because their rule-based system did not need anytraining.7771.
Our approach 2.
State of the artTen-fold cross-validation, French French EnglishCopy Hybrid Guimier Kobus 2008 Aw Choud.
Cookx?
?
x?
?
2007 1 2?
2006 2006??
2009??Sub.
25.90 1.65 6.69 0.45 11.94Del.
8.24 0.74 1.89 0.31 2.36Ins.
0.46 0.08 0.72 0.10 2.21WER 34.59 2.37 9.31 0.78 16.51 10.82 41.00 44.60SER 85.74 0.87 65.07 1.85 76.05BLEU 0.47 0.03 0.83 0.01 0.736 0.8 0.81x?=mean, ?=standard deviationTable 2: Performance of the system.
(?)
Kobus 2008-1 corresponds to the ASR-like system, whileKobus 2008-2 is a combination of this system with a series of open-source machine translation toolkits.(??)
Scores obtained on noisy data only, out of the sentence?s context.normalization algorithm is original in two ways.First, it is entirely based on models learned froma training corpus.
Second, the rewrite modelapplied to a noisy sequence differs depending onwhether this sequence is known or not.Evaluated by ten-fold cross-validation, thesystem seems efficient, and the performancein terms of BLEU score and WER are quiteencouraging.
However, the SER remains too high,which emphasizes the fact that the system needsseveral improvements.First of all, the model should take phoneticsimilarities into account, because SMS messagescontain a lot of phonetic plays.
The phoneticmodel, for instance, should know that o, au,eau, .
.
.
, aux can all be pronounced [o], while?, ais, ait, .
.
.
, aient are often pronounced [E].However, unlike Kobus et al (2008a), we feelthat this model must avoid the normalization stepin which the graphemic sequence is convertedinto phonemes, because this conversion preventsthe next steps from knowing which graphemeswere in the initial sequence.
Instead, we proposeto learn phonetic similarities from a dictionaryof words with phonemic transcriptions, and tobuild graphemes-to-graphemes rules.
These rulescould then be automatically weighted, by learningtheir frequencies from our aligned corpora.Furthermore, this model should be able to allowfor timbre variation, like [e]?
[E], in order toallow similarities between graphemes frequentlyconfused in French, like ai ([e]) and ais/ait/aient([E]).
Last but not least, the graphemes-to-graphemes rules should be contextualized, inorder to reduce the complexity of the model.It would also be interesting to test the impact ofanother lexical language model, learned on non-SMS sentences.
Indeed, the lexical model mustbe learned from sequences of standard writtenforms, an obvious prerequisite that involves amajor drawback when the corpus is made of SMSsentences: the corpus must first be transcribed,an expensive process that reduces the amountof data on which the model will be trained.
Forthis reason, we propose to learn a lexical modelfrom non-SMS sentences.
However, the corpus ofexternal sentences should still share two importantfeatures with the SMS language: it should mimicthe oral language and be as spontaneous aspossible.
With this in mind, our intention isto gather sentences from Internet forums.
Butnot just any forum, because often forums shareanother feature with the SMS language: theirlanguage is noisy.
Thus, the idea is to choosea forum asking its members to pay attention tospelling mistakes and grammatical errors, and toavoid the use of the SMS language.AcknowledgmentsThis research was funded by grants no.
716619and 616422 from the Walloon Region of Belgium,and supported by the Multitel research centre.We sincerely thank our anonymous reviewersfor their insightful and helpful comments on thefirst version of this paper.ReferencesAiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.A phrase-based statistical model for SMS text778normalization.
In Proc.
COLING/ACL 2006.Srinivas Bangalore, Vanessa Murdock, and GiuseppeRiccardi.
2002.
Bootstrapping bilingual datausing consensus translation for a multilingual instantmessaging system.
In Proc.
the 19th internationalconference on Computational linguistics, pages 1?7, Morristown, NJ, USA.Richard Beaufort.
2008.
Application desmachines ?
etats finis en synth?se de la parole.S?lection d?unit?s non uniformes et correctionorthographique.
Ph.D. thesis, FUNDP, Namur,Belgium, March.
605 pages.Markus Bieswanger.
2007. abbrevi8 or not 2 abbrevi8:A contrastive analysis of different space and time-saving strategies in English and German textmessages.
In Texas Linguistics Forum, volume 50.Stanley F. Chen and Joshua Goodman.
1998.An empirical study of smoothing techniques forlanguage modeling.
Technical Report 10-98,Computer Science Group, Harvard University.Noam Chomsky and Morris Halle.
1968.
The soundpattern of English.
Harper and Row, New York, NY.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar1, and Anupam Basu.2007.
Investigation and modeling of the structureof texting language.
International Journal onDocument Analysis and Recognition, 10(3):157?174.Vincent Colotte and Richard Beaufort.
2005.Linguistic features weighting for a text-to-speechsystem without prosody model.
In Proc.Interspeech?05, pages 2549?2552.Paul Cook and Suzanne Stevenson.
2009.
Anunsupervised model for text message normalization.In Proc.
Workshop on Computational Approaches toLinguistic Creativity, pages 71?78.Louise-Am?lie Cougnon and Richard Beaufort.
2009.SSLD: a French SMS to standard languagedictionary.
In Sylviane Granger and Magali Paquot,editors, Proc.
eLexicography in the 21st century:New applications, new challenges (eLEX 2009).Presses Universitaires de Louvain.
To appear.C?drick Fairon and S?bastien Paumier.
2006.
Atranslated corpus of 30,000 French SMS.
In Proc.LREC 2006, May.C?crick.
Fairon, Jean R. Klein, and S?bastien Paumier.2006.
Le langage SMS: ?tude d?un corpusinformatis?
?
partir de l?enqu?te Faites don devos SMS ?
la science.
Presses Universitaires deLouvain.
136 pages.Emilie Guimier de Neef, Arnaud Debeurme, andJungyeul Park.
2007.
TILT correcteur de SMS:?valuation et bilan quantitatif.
In Actes de TALN2007, pages 123?132, Toulouse, France.C.
Douglas Johnson.
1972.
Formal aspects ofphonological description.
Mouton, The Hague.Catherine Kobus, Fran?ois Yvon, and G?raldineDamnati.
2008a.
Normalizing SMS: are twometaphors better than one?
In Proc.
COLING 2008,pages 441?448, Manchester, UK.Catherine Kobus, Fran?ois Yvon, and G?raldineDamnati.
2008b.
Transcrire les SMS comme onreconna?t la parole.
In Actes de la Conf?rence surle Traitement Automatique des Langues (TALN?08),pages 128?138, Avignon, France.Ron Kohavi.
1995.
A study of cross-validationand bootstrap for accuracy estimation and modelselection.
In Proc.
IJCAI?95, pages 1137?1143.Vladimir Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions and reversals.
SovietPhysics, 10:707?710.Mehryar Mohri and Michael Riley.
1997.
Weighteddeterminization and minimization for largevocabulary speech recognition.
In Proc.Eurospeech?97, pages 131?134.Mehryar Mohri and Richard Sproat.
1996.
Anefficient compiler for weighted rewrite rules.
InProc.
ACL?96, pages 231?238.Mehryar Mohri, Fernando Pereira, and Michael Riley.2000.
The design principles of a weighted finite-state transducer library.
Theoretical ComputerScience, 231(1):17?32.Mehryar Mohri, Fernando Pereira, and Michael Riley.2001.
Generic -removal algorithm for weightedautomata.
Lecture Notes in Computer Science,2088:230?242.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
ACL2001, pages 311?318.Emmanuel Roche and Yves Schabes, editors.
1997.Finite-state language processing.
MIT Press,Cambridge.Claude E. Shannon.
1948.
A mathematical theory ofcommunication.
The Bell System Technical Journal,27:379?423.Richard Sproat, A.W.
Black, S. Chen, S. Kumar,M.
Ostendorf, and C. Richards.
2001.Normalization of non-standard words.
ComputerSpeech & Language, 15(3):287?333.Crispin Thurlow and Alex Brown.
2003.
Generationtxt?
The sociolinguistics of young people?s text-messaging.
Discourse Analysis Online, 1(1).Kristina Toutanova and Robert C. Moore.
2002.Pronunciation modeling for improved spellingcorrection.
In Proc.
ACL?02, pages 144?151.Fran?ois Yvon.
2008.
Reorthography of SMSmessages.
Technical Report 2008, LIMSI/CNRS,Orsay, France.779
