A Bayesian Model of Grounded Color SemanticsBrian McMahanRutgers Universitybrian.mcmahan@rutgers.eduMatthew StoneRutgers Universitymatthew.stone@rutgers.eduAbstractNatural language meanings allow speakers toencode important real-world distinctions, butcorpora of grounded language use also re-veal that speakers categorize the world indifferent ways and describe situations withdifferent terminology.
To learn meaningsfrom data, we therefore need to link underly-ing representations of meaning to models ofspeaker judgment and speaker choice.
Thispaper describes a new approach to this prob-lem: we model variability through uncertaintyin categorization boundaries and distributionsover preferred vocabulary.
We apply the ap-proach to a large data set of color descrip-tions, where statistical evaluation documentsits accuracy.
The results are available as aLexicon of Uncertain Color Standards (LUX),which supports future efforts in grounded lan-guage understanding and generation by prob-abilistically mapping 829 English color de-scriptions to potentially context-sensitive re-gions in HSV color space.1 IntroductionTo ground natural language semantics in real-worlddata at large scale requires researchers to confrontthe vocabulary problem (Furnas et al., 1987).
Muchof what people say falls in a long tail of increas-ingly infrequent and specialized items.
Moreover,the choice of how to categorize and describe real-world data varies across people.
We can?t accountfor this complexity by deriving one definitive map-ping between words and the world.We see this complexity already in free text de-scriptions of color patches.
English has fewer thanHue0123456Entropy (bits)Figure 1: A visualization of the variability of the de-scriptions used to name colors within small bins of colorspace.
For each Hue value, the entropy values foreach bin along the Saturation and Value dimensions aregrouped and plotted as box plots.
The dotted line cor-responds to a random choice out of fourteen items and tothe perplexity of a histogram model trained on the corpus.a dozen basic color words (Berlin, 1991), but peo-ple?s descriptions of colors are much more variablethan this would suggest.
Measured on the corpusdescribed in Section 4.1, there?s an average of 3.845bits of information in a color description given thecolor it describes?comparable to rolling a 14-sideddie.
Figure 1 summarizes the data and plots the en-tropy of descriptions encountered within small binsof color space.
The bins are aggregated over the Sat-uration and Value dimensions and indexed on the x-axis by the Hue dimension.
There?s little reason tothink that this variability conceals consistent mean-ings.
In formal semantics, one of the hallmarks ofvague language is that speakers can make it moreprecise in alternative, incompatible ways (Barker,2002).
We see this in practice as well, for exam-ple with the image of Figure 2, where subjects com-103Transactions of the Association for Computational Linguistics, vol.
3, pp.
103?115, 2015.
Action Editor: Lillian Lee.Submission batch: 11/2014; Published 2/2015.
c?2015 Association for Computational Linguistics.Figure 2: Image by flickr user Joanne Bacon (jlbacon)from the data set of Young et al.
(2014), whose subjectsdescribe these dogs as a brown dog and a tan one or a tandog and a white one.prehensibly describe either of two dogs as the tanone.
Systems that robustly understand or generatedescriptions of colors in situated dialogue need mod-els of meaning that capture this variability.This paper makes two key contributions towardsthis challenge.
First, we present a methodologyto infer a corpus-based model of meaning that ac-counts for possible differences in word usage acrossdifferent speakers.
As we explain in Section 2,our approach differs from the typical perspective ingrounded semantics (Tellex et al., 2011a; Matuszeket al., 2012; Krishnamurthy and Kollar, 2013),where a meaning is reduced to a single classifier thatcollapses patterns of variation.
Instead, our modelallows for variability in meaning by positing uncer-tainty in classification boundaries that can get re-solved when a speaker chooses to use a word on aspecific occasion.
We explain the model and its the-oretical rationale in Section 3.Second, we develop and release a Lexicon ofUncertain Color Standards (LUX) by applying ourmethodology to color descriptions.
LUX is an inter-pretation of 829 distinct English color descriptionsas distributions over regions of the Hue?Saturation?Value color space that describe their possible mean-ings.
As we describe in Section 4, the model istrained by machine learning methods from a subsetof Randall Munroe?s 2010 publicly-available cor-pus of 3.4 million crowdsourced free-text descrip-tions of color patches (Munroe, 2010).
Data, modelsand visualization software are available at http://mcmahan.io/lux/.Statistical evaluation of our model against twoalternative approaches documents its effectiveness.The model makes better quantitative predictionsthan a brute-force memorization model; it seems togeneralize to unseen data in more meaningful ways.At the same time, our meanings work as well asspecial-purpose models to explain speaker choice,even though our model supports diverse other rea-soning.
See Section 5.We see color as the first of many applications ofour methodology, and are optimistic about learn-ing vague meanings for other continuous domainsas quantity, space, and time.
At the same time,the methodology opens up new prospects for re-search on negotiating meaning interactively (Lars-son, 2013) with principled representations and withbroad coverage.
In fact, many practical situated dia-logue systems already identify unfamiliar objects bycolor.
We expect that LUX will provide a broadlyuseful resource to extend the range of descriptionssuch systems can generate and understand.2 Related WorkGrounded semantics is the task of mapping rep-resentations of linguistic meaning to the physicalworld, whether by perceptual mechanisms (Har-nad, 1990) or with the assistance of social inter-action (DeVault et al., 2006).
In this paper, weare particularly concerned with grounding the mean-ings of primitive vocabulary.
However, the ulti-mate test of grounded semantics?whether it is un-derstanding commands (Winograd, 1970; Tellex etal., 2011b), describing states of the world (Chen andMooney, 2008), or identifying objects (Matuszek etal., 2012; Krishnamurthy and Kollar, 2013; Dawsonet al., 2013)?is the ability to interpret or generateutterances using lexical and compositional seman-tics so as to evoke appropriate real-world referents.Grounded semantics therefore involves more thanjust quantifying the associations between words andperceptual representations, as Chuang et al.
(2008)and Heer and Stone (2012) do for color.
Groundedsemantics involves interpreting semantic primitivesin terms of composable categories that let systemsdiscriminate between cases where a word appliesand cases where the word does not apply.
(Our eval-uation compares models of grounded semantics tomore direct models of word?world associations.
)Previous research has modeled these categories as104regions of suitable perceptual feature spaces.
Re-searchers have explored explicit spaces of high-levelperceptual attributes (Farhadi et al., 2009; Silbereret al., 2013), approximations to such spaces (Ma-tuszek et al., 2012), or low-level feature spaces suchas Bag of Visual Words (Bruni et al., 2012) orHistogram of Gradients (Krishnamurthy and Kollar,2013).
We specifically follow Ga?rdenfors (2000)and Ja?ger (2010) in assuming that color categoriesare convex regions in an underlying color space, andare not just determined by prototypical color values,such as in Andreas and Klein (2014).However, unlike previous grounded semantics,we do not assume that words name categories un-equivocally.
Speakers may vary in how they inter-pret a word, so we treat the link between words andcategories probabilistically.
The difference makestraining our model more indirect than previous ap-proaches to grounded meaning.
In particular, ourmodel introduces a new layer of uncertainty that de-scribes what category the speaker uses.Similar kinds of uncertainty can be found inBayesian models of speaker strategy, such as thatof Smith et al.
(2013).
However, this research hasassumed that speakers aim to be as informative aspossible.
We have no evidence that our speakers dothat.
We assume only that speakers?
utterances arereliable and mirror prevailing usage.Prior work by cognitive scientists has studiedcolor terms extensively, but focused on basic ones?monolexemic, top-level color words with generalapplication and high frequency in a language (Kay etal., 2009; Lammens, 1994).
These color categoriesseem to shape people?s expectations and memoryfor colors (Persaud and Hemmer, 2014), and pat-terns of color naming can therefore enhance soft-ware for helping people organize and interact withcolor (Chuang et al., 2008; Heer and Stone, 2012).Moreover, crosslinguistic evidence suggests that thehuman perceptual system places strong biases onthe meanings of the basic color terms (Regier et al.,2005), perhaps because basic terms must partitionthe perceptual space in an efficient way (Regier etal., 2007).
We depart from research on basic colornaming in considering a much wider range of terms,much like Andreas and Klein (2014).
We considersubordinate, non-basic terms like beige or lavender;modified colors like light blue or bright green; andnamed subcategories like olive green, navy blue orbrick red.In order to use semantic primitives for under-standing, it?s necessary to combine them into anintegrated sentence-level representation: this is theproblem of semantic parsing.
Semantic parsers canbe built by hand (Winograd, 1970), induced throughinductive logic programming (Zelle and Mooney,1996), or treated as a structured classification prob-lem (Zettlemoyer and Collins, 2005).
Once a suit-able logical form is derived, interpretation typicallyinvolves a recursive process of finding referents thatfit lexical categories and relationships (Mavridis andRoy, 2006; Tellex et al., 2011a).
While this pa-per does not explicitly address how our meaningsmight be used in conjunction with such techniques,we see no fundamental obstacle to doing so?for ex-ample, by resolving references probabilistically andmarginalizing over uncertainty in meaning.3 Using Vague Color Terms: A ModelOur model involves two significant innovations overprevious approaches to grounded meaning.
Thefirst is to capture the vagueness and flexibility ofgrounded meaning with semantic representationsthat treat meaning as uncertain.
We represent thesemantics of a color description with a distribu-tion over color categories, which weights possiblemeanings by the relative likelihood of a speaker us-ing this meaning on any particular occasion.
Forexample, speakers might associate yellowish greenwith a range of possible meanings, differing in howfar the color category extends into green hues.
Byrepresenting uncertainty about meaning, our modelmakes room to capture variability in language use.For example, it implicitly quantifies how likelyspeakers are to use words differently, as with the twointerpretations of tan in Figure 2.Our second contribution is our simple model ofthe relationship between semantics and pragmatics.We assume that speakers?
choices mirror establishedpatterns.
In particular, the model learns a measureof availability for each color term that tracks howfrequently speakers tend to use it when it is appli-cable.
For example, although the expressions yel-lowish green and chartreuse are associated with verysimilar color categories, people say yellowish green105much more often: it has a higher availability.
Empir-ically, we find few terms with high availability anda long tail of terms with lower availabilities.
We as-sume speakers simply sample applicable terms fromthis distribution, which predicts the long tail of ob-served responses.Mathematically, we develop our approachthrough the rational analysis methodology forexplaining human behavior proposed by Anderson(1991), along with methodological insights fromthe linguistics and philosophy of vagueness.
In theremainder of this section, we explain the theoreticalantecedents in perceptual science, linguistics andcognitive modeling that inform our approach.3.1 Color CategoriesColor can be defined as sensations by which the per-ceptual system tracks the diffuse reflectance of ob-jects, despite variability, uncertainty and ambiguityin the visual input.
Red, green, and blue cones inthe retina allow the visual system to coarsely es-timate frequency bands in the spectrum of incom-ing light.
Cameras and screens that use the red?green?blue (RGB) color space are designed roughlyto correspond to these responses.
However, colors inthe visual system summarize spectral profiles ratherthan mere wavelengths of light.
For example, we seecolors like cyan (green plus blue without red), ma-genta (blue plus red without green) and yellow (redplus green without blue) as intermediate saturatedcolors between the familiar primaries.
This natu-rally leads to a wheel of hues describing the relativeprominence of different spectral components alonga continuum.
Fairchild (2013) provides an overviewof color appearance.To capture this variation, we?ll work in the sim-ple hue?saturation?value (HSV) color space that?scommon in computer graphics and color picker userinterfaces (Hughes et al., 2013) and implemented inpython?s native colorsys package.
This coordinatesystem represents colors with three distinct qualita-tive dimensions: Hue (H) represents changes in tintaround a color wheel, Saturation (S) represents therelative proportion of color versus gray, and Value(V) represents the location on the white?black con-tinuum.
We will associate color categories with rect-angular box-shaped regions in HSV space.
Moresophisticated color spaces have been developed todescribe the psychophysics of color more precisely,but they depend on the photometric illumination andother aspects of the viewing context that were notcontrolled in the collection of the data we are using(Fairchild, 2013).3.2 Semantic RepresentationOur assumption is that color terms are associatedprobabilistically with color categories.
We illustratethe idea for the color label yellowish-green throughthe plot in Figure 3.
The plot shows variation in useof the term across the Hue dimension: the bar graphis a scaled histogram of the responses in the data weuse.
There is a range of colors where people use yel-lowish green often, surrounded by borderline caseswhere it becomes increasingly infrequent.Hue?Lower ?Upper0.20.40.60.81.0Probability ?Lower ?Upper?HueY ellowishGreenYellowish Green dataFigure 3: The LUX model for ?yellowish green?
on theHue axis plotted against the scaled histogram of the re-sponses in the data.
The ?
curve represents the likeli-hood of ?yellowish green?
for different Hue values.
The?
curves represent possible boundaries.We represent this variability by assuming that theboundaries that delimit the color are uncertain.
Inany utterance, yellowish green fits only those Huevalues that are above a minimum threshold ?Lowerand below a maximum threshold ?Upper.
However,it is uncertain which thresholds a speaker will use.The model describes this variability with probabil-ity density functions.
They are shown for yellowishgreen in Figure 3 as the ?
distributions.
The figureshows that there is a central range of hues, betweenthe ?
distributions, that is definitely yellowish green.The ?
distributions peak at the most likely bound-aries for yellowish green, encompassing a broad re-gion that?s frequently called yellowish green.
Fur-ther away, threshold values and yellowish green ut-terances alike become rapidly less likely.106Our representation is motivated by Barker (2002)and Lassiter (2009), who show how sets of possi-ble thresholds1 can account for many of our intu-itions about the use of vague language.
Their analy-sis invites us to capture semantic variability throughtwo geometric constructs.
First, there is a certaininterval, parameterized by two points, ?Lower and?Upper, within which a color description definitelyapplies.
Outside this interval are regions of bor-derline cases, delimited by probabilistically-varyingthresholds ?Lower and ?Upper, where the color de-scription sometimes applies.
We represent the po-sition of the threshold with a ?
(?, ?)
distribution, astandard statistical tool to model processes that start,continue indefinitely, and stop, like waiting times.2We can determine a likelihood that a description fitsa color by marginalizing over the thresholds: thisgives the black curve visualized in Figure 3.
As wedescribe in Section 3.3, we can use this to accountfor the graded responses from subjects that we ob-serve near color boundaries.We summarize with a formal definition of our se-mantic representation.
Let X be the 3D space ofHSV colors and let x ?
X be a measured colorvalue.
Each color label k has definite boundaries,?Lower and ?Upper in X , delimiting a box of HSVcolor space.
Surrounding the definite region areregions of uncertainty: the set of possible bound-aries beyond ?.
These are represented by probabil-ity distributions over lower and upper threshold val-ues in each dimension.
We?ll represent these thresh-olds by ?
j,dk where k ?
K indexes the color label,j ?
{Lower/L, Upper/U} indexes the boundary,and d ?
{H, S, V} indexes color components.
Weassume the thresholds are distributed as follows:?Lower,dk ?
?Lower,dk ?
?
(?Lower,dk , ?Lower,dk )?Upper,dk ?
?Upper,dk + ?
(?Upper,dk , ?Upper,dk ) (1)The meaning of a color term is thus a ?blurry box?.The distribution lets us determine the probability of1We treat the terms ?boundary?, ?threshold?, and ?standard?to be synonymous, but useful in different contexts.2?
distributions rise quickly away from the origin point,then trail off from the peak in an open-ended exponential de-cay.
One intuition for applying them in this case is Graff Fara?s(2000) suggestion that a particular categorization decision in-volves waiting to find a natural break among salient colors.However, we choose them for mathematical convenience ratherthan psychological or linguistic considerations.Figure 4: The Rational Observer observes a color patch,x.
The applicability of each label (ktrue) is based uponthe label parameters (?, ?, ?)
and x.
The label (ksaid)is sampled proportional to the applicability and a back-ground weight: how often a label is said when it applies.a point x falling into the color category k as in Eq.
2.We also use the compact notation in Eq.
3.P (?Lower,Hk < xH < ?Upper,Hk )?P (?Lower, Sk < xS < ?Upper, Sk )?P (?Lower,Vk < xV < ?Upper,Vk ) (2)=?dP (?L,dk < xdi < ?U,dk ) (3)3.3 Rational Observer ModelOur goal is to learn probabilistic representationsof the meanings of color terms from subjects?
re-sponses.
To do this, we need not only a frameworkfor representing colors but also a model of how sub-jects choose color terms.
Inspired by rational anal-ysis (Anderson, 1991), we assume that speakers?choices match their communicative goals and theirsemantic knowledge.
We leverage this assumptionto derive a Bayes Rational Observer model linkingsemantics to observed color descriptions.The graphical model in Figure 4 formalizes ourapproach.
We start from an observed color patch, x.The Rational Observer uses the ?
-distributions foreach color description k to determine the likelihoodthat the speaker judges k applicable.
As defined inEq.
3, the likelihood is the subset of possible bound-aries which contain the target color value.
Normally,many descriptions will be applicable.
Which thespeaker chooses depends further on the availabil-ity of the label?a background measure of how fre-quently a label is chosen when it?s applicable.
In-tuitively, availability creates a bias for easy descrip-tions, capturing how natural or ordinary a descrip-107tion is in language use, how easily it springs to mindor how easily it is understood.We formalize this as a generative model.
As weexplain in Section 4, we infer the parameters fromour data.
In Eq.
4, we consider the conditional dis-tribution of a subject observing a color patch givenHSV value x and labeling it k:(4)P (ksaid, ktrue|x) = P (ksaid|ktrue)P (ktrue|x)In this equation, ksaid is the event that the subjectresponds to x with label k and ktrue is the event thatthe subject judges k true of the HSV value x. Thetwo factors of Eq.
4 are respectively the availabilityand applicability of the color label.Availability: The prior P (ksaid|ktrue) quantifiesthe rate at which label k is used when it applies.
Werefer to this quantity as the availability and denoteit as ?k.
Availability captures the observed bias forfrequent color terms.
When multiple color labels fita color value, those with higher availability will beused more often, but those with lower availabilitywill still get used.
This effect is partially responsiblefor the long tail of subjects?
responses.Applicability: The second factor, P (ktrue|x),is the probability that k is true of, or applies to,the color value x.
We calculate the applicabilityby marginalizing over all possible thresholds as inEq.
3.
In other words, we calculate the probabil-ity mass of the boundaries which allow for this de-scription to apply.
We treat each applicability judg-ment as independent of others.
This implies that therelative frequency at which we see a color descrip-tion used is directly proportional to the proportion ofboundaries which license it.For clearer notation and parameter estimation, wetrack thresholds with a piecewise function ?dk(xd) asin Eq.
5 and Figure 3.?dk(xd) =????
?P (xd > ?L,dk ), xd ?
?L,dkP (xd < ?U,dk ), xd ?
?U,dk1, otherwise(5)Finally, Eq.
6 rewrites Eq.
4 to make the applica-bility and availability explicit.
The model treats thisequation as the probability of success for a Bernoullitrial and the data as sampled from Categorical dis-tributions formed by the set of K Bernoulli randomvariables.
This is discussed further in Section 4.2.
(6)P (ksaid, ktrue|x) = ?k?d?dk(xd)4 Learning ExperimentWe worked with Randall Munroe?s crowdsourcedcorpus of color judgments, and fit the model us-ing the Metropolis-Hastings Markov Chain MonteCarlo, a Gaussian random walk optimizationmethod.
This form of approximate Bayesian infer-ence is described in Section 4.2.4.1 Munroe Color CorpusIn 2010, Munroe elicited descriptions of colorpatches over the web.
His platform asked usersfor background information such as sex, color-blindness, and monitor type, then presented colorpatches and let the user freely name them.
The setupdidn?t ensure that users see controlled colors or thatusers?
responses are reliable, but the experiment col-lected over 3.4M items pairing RGB values with textdescriptions.
Munroe?s methodology, data and re-sults are published online (Munroe, 2010).3Munroe summarizes his results with 954 idealizedcolors?RGB values that best exemplify high fre-quency color labels.
In effect, Munroe?s summaryoffers a prototype theory of color vocabulary, likethat of Andreas and Klein (2014).
An alternativetheory, which we explore, is that variability in theapplicability of labels is an important part of peo-ple?s knowledge of color semantics.
We comparethe two theories explicitly in Section 5.Our experiments focus on a subset of Munroe?sdata comprising 2,176,417 data points and 829 colordescriptions, divided into a training set of 70%,a 5% development set, and a held-out test set of25%.
To minimize variability in language use, weselected data from users who self-report as non-colorblind English speakers.
This accounts for2.5M of Munroe?s 3.4M items.
To get our sub-set, we further restrict attention to labels used 100times or more, to ensure that there?s substantial ev-idence of each term?s breadth of applicability.
Wehand curated the responses to correct some mi-nor spelling variations involving a single-character3http://blog.xkcd.com/2010/05/03/color-survey-results/108change (?yellow green?
vs ?yellow-green?
; ?fuch-sia?
vs ?fuschia?, ?fushia?, ?fuchia?, and ?fucsia?
)and to remove high-frequency spam labels.
We areleft with 829 color labels that fit these restrictions.Finally, we used python?s colorsys to convert fromRGB to HSV, where we hypothesize color meaningscan be represented more simply.
We include thesedata sets with our release at http://mcmahan.io/lux/ so our results can be replicated.4.2 Fitting the Model ParametersOptimization of the model?s parameters is framed ina Bayesian framework and interpreted as maximiz-ing the likelihood of the data given the parameters.We fit each label and each dimension independently.The data on each dimension is binned, as in Figure 3,so we have Binomial random variables for each bin.For each color label k, the probability of success isbased on the model?s parameters.
Non-k data in thebin are observations of failure.
This gives Eq.
7:P (ndi,k|ndi , Zdk , ?k) ?
Bin(ndi , Zdk?dk(i)) (7)Here ndi is the number of data points in bin i on di-mension d, ndi,k is the number of data points for la-bel k in bin i on dimension d, and Zdk is a normal-ization constant, implicitly reflecting both the avail-ability ?k and the distribution of responses of theterm across other color dimensions.
The optimiza-tion process is a parameter search method whichuses as an objective function the probability of ndi,kin Eq.
7 for all d,i, and k.Parameter Search: We adopt a Bayesian coor-dinate descent which sequentially samples the cer-tain region parameter, ?, and the shape and rate pa-rameters (?
and ?)
of the ?
distributions for all dand k independently.
It also samples the estimatednormalization constant, ZdK .
More specifically, thesampling is done using Metropolis-Hastings MarkovChain Monte Carlo (Metropolis et al., 1953; Chiband Greenberg, 1995), which performs a Gaussianrandom walk on the parameters4.
For each sample,the likelihood of the data, derived from the Bino-mial variables, is compared for the new and old set4We set the standard deviation of the sampling Gaussian tobe 1 for each ?
and 0.3 for each ?
and ?
after finding experi-mentally that it led to effective parameter search (Gelman et al.,1996).of parameters.
The new parameters are acceptedproportionally to the ratio of the two likelihoods.Multiple chains were run using 4 different bin sizesper dimension and monitored for convergence usingthe generalized Gelman-Rubin diagnostic method(Brooks and Gelman, 1998).
This methodologyleaves us not only with the Monte Carlo estimateof the expected value for each parameter, but also asampling distribution that quantifies the uncertaintyin the parameters themselves.Availability: Availability is estimated as the ratioof the observed frequency of a label to its expectedfrequency given the parameters which define its dis-tribution.
The expected frequency, a marginalizationof the color space for the ?
function, is calculatedusing the midpoint integration approximation.
(8)?k =P (ksaid, ktrue)P (ktrue)= count(k)/N?x P (ktrue|x)P (x)5 Model EvaluationLUX explains Munroe?s data via speakers?
rationaluse of probabilistic meanings, represented as sim-ple ?blurry boxes?.
In this section, we assess theeffectiveness of this explanation.
We anticipate twoarguments against our model: first, that the represen-tation is too simple; second, that factoring speakers?choices through a model of meaning is too cumber-some.
We rebut these arguments by providing met-rics and results that suggest that LUX escapes theseobjections and captures almost all of the structure insubjects?
responses.5.1 Alternative ModelsTo test LUX?s representations, we built a brute-forcehistogram model (HM) that discretizes HSV spaceand tracks frequency distributions of labels directlyin each discretized bin.
Similar histogram modelshave been developed by Chuang et al.
(2008) and(Heer and Stone, 2012) to build interfaces for inter-acting with color that are informed by human cat-egorization and naming.
More precisely, our HMuses a linear interpolation method (Chen and Good-man, 1996) to combine three histograms of various109granularity.5 This amounts to predicting responsesby querying the training data.
HM has the potentialto expose whether LUX is missing important fea-tures of the distribution of color descriptions.We also built a direct model of subjects?
choicesof color terms.
Instead of appealing to the applica-bility and availability of a color label, it works withthe observed frequency of a color label and a Gaus-sian model of the probability of a color value foreach label, as in Eq.
9:(9)P (ksaid, ktrue|x)?
P (x|ktrue)P (ksaid, ktrue)This Gaussian model (GM) generalizes Munroe?spairing of labels with prototypical colors:P (x|ktrue) is a Gaussian with diagonal covari-ance, so it associates each color term with a meanHSV value and with variances in each dimensionthat determine a label-specific distance metric.
GMpredicts speaker choice by weighting these distancesprobabilistically against the priors.
GM completelysidesteps the need to model meaning categorically.It therefore has the potential to expose whether ourassumptions about semantic representations andspeaker choices hinder LUX?s performance.5.2 Evaluation MetricsWe evaluate the models using two classes of met-rics on a held-out test set consisting of 25% of thecorpus.
The first type is based upon the posteriordistribution over labels and the ranked position ofsubjects?
actual labels of color values.
The secondtype is based upon the log likelihood of the models,which quantifies model fit.5.2.1 Decision-Based MetricsTo answer how accurate a model?s predictions are,we can locate subjects?
responses in the weightedrankings computed by the models.The TOPK Measures: Each model provides aposterior distribution over the possible labels.
Themost likely label of this posterior is the maximumlikelihood estimate (MLE).
We track how often theMLE color label is what the user actually said as5Specifically, the histograms are of size (90,10,10), (45,5,5),and (1,1,1) across Hue, Saturation, and Value with interpolationweights of 0.322, 0.643, and 0.035 respectively.
These parame-ters were determined by taking the training set as 5-fold valida-tion sets.the TOP 1 measure.
For the Histogram Model, theTOP 1 approximates the most frequent label ob-served in the data for a color value.
We also measurehow often the correct label appears in the first 5 and10 most likely labels.
These are denoted TOP 5 andTOP 10 respectively.5.2.2 Likelihood-Based MetricsWe can also measure how well a model explainsspeaker choice using the log likelihood of the labelsgiven the model and the color values, denoted asLLV (M).
This is calculated using Eq.
10 acrossall N data points in the held-out test set.
LLV (M)is used when computing perplexity and Aikake In-formation Criterion (AIC).
We report all measuresin bits.LLV (M) = log2 PM (Ktrue,Ksaid|X)=?ilog2 PM (ktruei , ksaidi |xi) (10)A more general measure of model fit is the log like-lihood of the color values and their labels jointlyacross the training set, LL(V ), given the model.
Itis defined and calculated analogously.Perplexity Perplexity has been used in past re-search to measure the performance of statistical lan-guage models (Jelinek et al., 1977; Brown et al.,1992).
Lower perplexity means that the model is lesssurprised by the data and so describes it more pre-cisely.
We use it here to measure how well a modelencodes the regularities in color descriptions.Akaike Information Criterion: AIC is derivedfrom information theory (Akaike, 1974) and bal-ances the model?s fit to the data with the complexityof the model by penalizing a larger number of pa-rameters.
The intuition is that a smaller AIC indi-cates a better balance of parameters and model fit.5.3 Evaluation ResultsTable 1 summarizes the decision-based evalua-tion results.6 We see little penalty for LUX and6There is a caveat to these performance measures.
All of thereported numbers are for the final data subset which we discussin Section 4.1.
We choose to use a subset which did not includecolor labels that had less than 100 occurrences.
In the English-speaking and American-citizenship subset, the rare descriptiontail accounts for 13% of the data?Roughly one third of thetail data is unique descriptions.
If the tail represents real world110TOP 1 TOP 5 TOP 10LUX 39.55% 69.80% 80.46%HM 39.40% 71.89% 82.53%GM 39.05% 69.25% 79.99%Table 1: Decision-based results.
The percentage of cor-rect responses of 544,764 test-set data points are shown.
?LL ?LLV AIC PerpLUX 1.13*107 2.05*106 4.13*106 13.61HM 1.13*107 2.09*106 4.82*106 14.41GM 1.34*107 2.08*106 4.17*106 14.14Table 2: Likelihood-based evaluation results: negativelog likelihood of the data, negative log likelihood oflabels given points, number of parameters, Akaike In-formation Criterion and perplexity of labels given colorvalues.
Parameter counts for AIC are 15751 for LUX,315669 for HM and 5803 for GM.GM?s constrained frameworks for modeling choices.However, the differences in the table, though nu-merically small, are significant (by Binomial test)at p < .02 or less.
In particular, the fact that LUXwins TOP 1 hints that its representations enable bet-ter generalization than HM or GM.
The success ofHM at TOP 5 and TOP 10, meanwhile, suggeststhat some qualitative aspects of people?s use of colorwords do escape the strong assumptions of LUX andGM?a point we return to below.At the same time, we draw a general lesson fromthe overall patterns of results in Table 1.
Languageusers must be quite uncertain about how speakerswill describe colors.
Speakers do not seem to choosethe most likely color label in a majority of responses;their behavior shows a long tail.
These results are inline with the probabilistic models of meaning andspeaker choice we have developed.Table 2 summarizes the likelihood based metrics.GM?s estimates don?t fit the distribution of the testdata as a whole: GM is a good model of what labelsspeakers give but not a good model of the points thatget particular labels.
By contrast, LUX tops out ev-ery row in the table.
HM is flexible enough in prin-ciple to mirror LUX?s predictions; HM must suffercircumstances, our model is only applicable 87% of the time,and thus the performance metrics should be scaled down.
Wedo not explicitly report the scaled numbers.from sparse data, given its vast number of parame-ters.
By contrast, LUX is able to capture the dis-tributions of speaker responses in deeper and moreflexible ways by using semantics as an abstraction.Our analysis of patterns of error in LUX sug-gests that LUX would best improved by more faith-ful models of linguistic meaning, rather than moreelaborate models of subjects?
choices or more pow-erful learning methods.
For one thing, neither LUXnor the simple prototype model captures ambiguity,which sometimes arises in Munroe?s data.
An exam-ple is the color label melon, which has a multimodaldistribution in the reddish-orange and green areas ofcolor space shown in Figure 5?most likely corre-sponding to people thinking about the distinct col-ors of the flesh of watermelon, cantaloupe and hon-eydew.
Interestingly, our model captures the morecommon usage.A different modeling challenge is illustrated bythe behavior of greenish in Figure 6.
Greenish seemsto be an exception to the general assumption thatcolor terms label convex categories.
Actually, green-ish seems to fit the boundary of green?the areas thatare not definitely green but not definitely not green.
(Linguists often appeal to such concepts in the liter-ature on vagueness.)
This is not a convex area so,not surprisingly, our model finds a poor match.
Ad-ditional research is needed to understand when it?sappropriate to give meanings more complex repre-sentations and how they can be learned.6 Discussion and ConclusionNatural language color descriptions provide an ex-pressive, precise but open-ended vocabulary to char-acterize real-world objects.
This paper documentsHue0.00.20.40.60.81.0Probability?HueMelonMelon dataFigure 5: For the Hue dimension, the data for ?melon?
isplotted against the LUX model?s ?
curve.111Hue?Lower ?Upper0.00.20.40.60.81.0Probability ?Lower ?Upper0.0000.0010.0020.0030.0040.005?HueGreenishGreenish dataFigure 6: For the Hue dimension, the data for ?greenish?is plotted against the LUX model?s ?
curve.and releases Lexicon of Uncertain Color Standards(LUX), which provides semantic representations of829 English color labels, derived from a large cor-pus of attested descriptions.
Our evaluation showsthat LUX provides a precise description of speak-ers?
free-text labels of color patches.
Our expec-tation therefore is that LUX will serve as a usefulresource for building systems for situated languageunderstanding and generation that need to describecolors to English-speaking users.Our work in LUX has built closely on linguis-tic approaches to color meaning and psychologicalapproaches to modeling experimental subjects.
Be-cause LUX bridges linguistic theory, psychologi-cal data, and system building, LUX also affords aunique set of resources for future research at the in-tersection of semantics and pragmatics of dialogue.For example, our work explains subjects?
deci-sions as a straightforward reflection of their com-municative goals in a probabilistic setting.
Ourmeasures of availability and applicability can beseen as offering computational interpretations of theGricean Maxims of Manner and Quality (Grice,1975).
However, these particular interpretationsdon?t give rise to implicatures on our model?largely because our Rational Observer is so inclusiveand variable in the descriptions it offers.
To showthis, we can analyze what an idealized hearer learnsabout an underlying color x when the speaker uses acolor term k: this is P (x|ksaid).
The model predic-tions are formalized in Eq.
11.P (x|ksaid) = P (x|ksaid, ktrue)= P (ksaid, ktrue|x)P (x)P (ksaid, ktrue)= P (ksaid|ktrue)P (ktrue|x)P (x)P (ksaid|ktrue)P (ktrue)= ?kP (ktrue|x)P (x)?kP (ktrue)= P (x|ktrue)(11)We apply Bayes?s rule, exploiting our model as-sumption that the speaker says k only when thespeaker first judges that k is true.
Our model alsotells us that, given that k is true, the speaker?s choiceof whether to say k depends only on the availabil-ity ?k of the term k. Simplifying, we find that thepragmatic posterior?what we think the speaker waslooking at when she said this word?coincides withthe semantic posterior?what we think the word istrue of.
Intuitively, the hearer knows that the term istrue because the speaker has used the word, indepen-dent of the color x the speaker is describing.
Sim-ilarly, in our model of speaker choice, the speakerdoes not take x into account in choosing one of theapplicable words to say (one way the speaker coulddo this, for example, would be to prefer terms thatwere more informative about the target color x).
In-stead, the speaker simply samples from the candi-dates.
That?s why the speaker?s choice reveals onlywhat the semantics says about x.Technically, this makes semantics a Nash equi-librium, where the information the hearer recov-ers from an utterance is exactly the informationthe speaker intends to express?in keeping with alongstanding tradition in the philosophy of language(Lewis, 1969; Cumming, 2013).
By contrast, re-searchers such as Smith et al.
(2013) adopt broadlysimilar formal assumptions but predict asymme-tries where sophisticated listeners can second-guessnaive speakers?
choices and recover ?extra?
infor-mation that the speaker has revealed incidentallyand unintentionally.
The difference between this ap-proach and ours eventually leads to a difference inthe priors over utterances, but it?s best explainedthrough the different utilities that motivate speak-ers?
different choices in the first place.
Smith et al.
(2013) assume speakers want to be informative; we112assume they want to fit in.
The empirical successof our approach on Munroe?s data motivates a largerproject to elicit data that can explicitly probe sub-jects?
communicative goals in relation to semanticcoordination.Meanwhile, our work formalizes probabilistictheories of vagueness with new scale and preci-sion.
These naturally suggest that we test predictionsabout the dynamics of conversation drawn from thesemantic literature on vagueness.
For example, inhearing a description for an object, we come to knowmore about the standards governing the applicabilityof the description.
This is outlined by Barker (2002)as having a meta-semantic effect on the commonground among interlocutors.
For example, hearinga yellow-green object called yellowish green shouldmake objects in the same color range more likelyto be referred to as yellowish green.
We could useLUX straightforwardly to represent such conceptualpacts (Brennan and Clark, 1996) via a posterior overthreshold parameters.
It?s natural to look for empir-ical evidence to assess the effectiveness of such rep-resentations of dependent context.A particularly important case involves descrip-tive material that distinguishes a target referent fromsalient alternatives, as in the understanding or gen-eration of referring expressions (Krahmer and vanDeemter, 2012).
Following Kyburg and Morreau(2000), we could represent this using LUX via a pos-terior over the threshold parameters that fit the targetbut exclude its alternatives.
Again, our model as-sociates such goals with quantitative measures thatfuture research can explore empirically.
Meo et al.
(2014) present an initial exploration of this idea.These open questions complement the key advan-tage that makes uncertainty about meaning crucial tothe success of the model and experiments we havereported here.
Many kinds of language use seem tobe highly variable, and approaches to grounded se-mantics need ways to make room for this variabil-ity both in the semantic representations they learnand the algorithms that induce these representationsfrom language data.
We have argued that uncertaintyabout meaning is a powerful new tool to do this.
Welook forward to future work addressing uncertaintyin grounded meanings in a wide range of continu-ous domains?generalizing from color to quantity,scales, space and time?and pursuing a wide rangeof reasoning efforts, to corroborate our results andto leverage them in grounded language use.AcknowledgmentsThis work was supported in part by NSF DGE-0549115.
This work has benefited from discus-sion and feedback from the reviewers of TACL, Ma-neesh Agrawala, David DeVault, Jason Eisner, TarekEl-Gaaly, Katrin Erk, Vicky Froyen, Joshua Gang,Pernille Hemmer, Alex Lascarides, and Tim Meo.ReferencesHirotugu Akaike.
1974.
A new look at the statisticalmodel identification.
IEEE Transactions on AutomaticControl, 19(6):716?723.John R. Anderson.
1991.
The adaptive nature of humancategorization.
Psychological Review, 98(3):409.Jacob Andreas and Dan Klein.
2014.
Grounding lan-guage with points and paths in continuous spaces.
InProceedings of the Eighteenth Conference on Com-putational Natural Language Learning, pages 58?67,June.Chris Barker.
2002.
The dynamics of vagueness.
Lin-guistics and Philosophy, 25(1):1?36.Brent Berlin.
1991.
Basic Color Terms: Their Univer-sality and Evolution.
Univ of California Press.Susan E. Brennan and Herbert H. Clark.
1996.
Concep-tual pacts and lexical choice in conversation.
Journalof Experimental Psychology: Learning, Memory andCognition, 22(6):1482?1493.Stephen P. Brooks and Andrew Gelman.
1998.
Gen-eral methods for monitoring convergence of iterativesimulations.
Journal of Computational and GraphicalStatistics, 7(4):434?455.Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer,Stephen A. Della Pietra, and Jennifer C. Lai.
1992.
Anestimate of an upper bound for the entropy of English.Computational Linguistics, 18(1):31?40.Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran.
2012.
Distributional semantics in tech-nicolor.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics, pages136?145.Stanley F. Chen and Joshua Goodman.
1996.
An empiri-cal study of smoothing techniques for language model-ing.
In Proceedings of the 34th annual meeting on As-sociation for Computational Linguistics, pages 310?318.David L. Chen and Raymond J. Mooney.
2008.
Learningto sportscast: a test of grounded language acquisition.113In ICML ?08: Proceedings of the 25th internationalconference on Machine learning, pages 128?135.Siddhartha Chib and Edward Greenberg.
1995.
Un-derstanding the Metropolis?Hastings algorithm.
TheAmerican Statistician, 49(4):327?335.Jason Chuang, Maureen Stone, and Pat Hanrahan.
2008.A probabilistic model of the categorical associationbetween colors.
In Color Imaging Conference, pages6?11.Sam Cumming.
2013.
Coordination and content.Philosophers?
Imprint, 13(4):1?16.Colin R. Dawson, Jeremy Wright, Antons Rebguns,Marco Valenzuela Esca?rcega, Daniel Fried, andPaul R. Cohen.
2013.
A generative probabilis-tic framework for learning spatial language.
In2013 IEEE Third Joint International Conference onDevelopment and Learning and Epigenetic Robotics(ICDL), pages 1?8.
IEEE.David DeVault, Iris Oved, and Matthew Stone.
2006.
So-cietal grounding is essential to meaningful languageuse.
In Proceedings of the Twenty-first National Con-ference on Artificial Intelligence, pages 747?754.Mark D. Fairchild.
2013.
Color Appearance Models.The Wiley-IS&T Series in Imaging Science and Tech-nology.
Wiley.Delia Graff Fara.
2000.
Shifting sands: An interest-relative theory of vagueness.
Philosophical Topics,28(1):45?81.Ali Farhadi, Ian Endres, Derek Hoiem, and DavidForsyth.
2009.
Describing objects by their attributes.2009 IEEE Conference on Computer Vision and Pat-tern Recognition, pages 1778?1785, June.George W. Furnas, Thomas K. Landauer, Louis M.Gomez, and Susan T. Dumais.
1987.
The vocabularyproblem in human-system communication.
Communi-cations of the ACM, 30(11):964?971.Peter Ga?rdenfors.
2000.
Conceptual Spaces.
MIT Press.Andrew Gelman, Gareth O. Roberts, and Walter R. Gilks.1996.
Efficient Metropolis jumping rules.
In J. M.Bernardo, J. O. Berger, A. P. Dawid, and A. F. Smith,editors, Bayesian Statistics 5, pages 599?607.
OxfordUniversity Press.Herbert P. Grice.
1975.
Logic and conversation.
InP.
Cole and J. Morgan, editors, Syntax and SemanticsIII: Speech Acts, pages 41?58.
Academic Press.Stevan Harnad.
1990.
The symbol grounding problem.Physica D: Nonlinear Phenomena, 42(1?3):335?346.Jeffrey Heer and Maureen Stone.
2012.
Color namingmodels for color selection, image editing and palettedesign.
In Proceedings of the SIGCHI Conference onHuman Factors in Computing Systems, pages 1007?1016.John F. Hughes, Andries van Dam, Morgan McGuire,David F. Sklar, James D. Foley, Steven K. Feiner, andKurt Akeley.
2013.
Computer Graphics: Principlesand Practice (3rd Edition).
Addison-Wesley Profes-sional.Gerhard Ja?ger.
2010.
Natural color categories are con-vex sets.
In Maria Aloni, Harald Bastiaanse, Tikitude Jager, and Katrin Schulz, editors, Logic, Languageand Meaning - 17th Amsterdam Colloquium, Amster-dam, The Netherlands, December 16-18, 2009, Re-vised Selected Papers, volume 6042 of Lecture Notesin Computer Science, pages 11?20.
Springer.Fred Jelinek, Robert L. Mercer, Lalit R. Bahl, andJames K. Baker.
1977.
Perplexity?a measure of thedifficulty of speech recognition tasks.
The Journal ofthe Acoustical Society of America, 62:S63.Paul Kay, Brent Berlin, Luisa Maffi, William R. Merri-field, and Richard Cook.
2009.
The World Color Sur-vey.
CSLI.Emiel Krahmer and Kees van Deemter.
2012.
Compu-tational generation of referring expressions: A survey.Computational Linguistics, 38(1):173?218.Jayant Krishnamurthy and Thomas Kollar.
2013.
Jointlylearning to parse and perceive: Connecting natural lan-guage to the physical world.
Transactions of the Asso-ciation for Computational Linguistics, 1(2):193?206.Alice Kyburg and Michael Morreau.
2000.
Fittingwords: Vague words in context.
Linguistics and Phi-losophy, 23(6):577?597.Johan Maurice Gisele Lammens.
1994.
A computationalmodel of color perception and color naming.
Ph.D.thesis, SUNY Buffalo.Staffan Larsson.
2013.
Formal semantics for percep-tual classification.
Journal of Logic and Computa-tion.
Advance online publication.
doi: 10.1093/log-com/ext059.Daniel Lassiter.
2009.
Vagueness as probabilistic lin-guistic knowledge.
In Rick Nouwen, Robert vanRooij, Uli Sauerland, and Hans-Christian Schmitz,editors, Vagueness in Communication - InternationalWorkshop, ViC 2009, held as part of ESSLLI 2009,Bordeaux, France, July 20-24, 2009.
Revised SelectedPapers, volume 6517 of Lecture Notes in ComputerScience, pages 127?150.
Springer.David K. Lewis.
1969.
Convention: A PhilosophicalStudy.
Harvard University Press, Cambridge, MA.Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettle-moyer, Liefeng Bo, and Dieter Fox.
2012.
A jointmodel of language and perception for grounded at-tribute learning.
In Proceedings of the 29th Interna-tional Conference on Machine Learning (ICML-12),pages 1671?1678.114Nikolaos Mavridis and Deb Roy.
2006.
Groundedsituation models for robots: Where words and per-cepts meet.
In Intelligent Robots and Systems, 2006IEEE/RSJ International Conference on, pages 4690?4697.
IEEE.Timothy Meo, Brian McMahan, and Matthew Stone.2014.
Generating and resolving vague color refer-ences.
In SEMDIAL 2014: THE 18th Workshop on theSemantics and Pragmatics of Dialogue, pages 107?115.Nicholas Metropolis, Arianna W. Rosenbluth, Mar-shall N. Rosenbluth, Augusta H. Teller, and EdwardTeller.
1953.
Equation of state calculations byfast computing machines.
The Journal of ChemicalPhysics, 21(6):1087?1092.Randall Munroe.
2010.
Color survey results.
On-line at http://blog.xkcd.com/2010/05/03/color-survey-results/.Kimele Persaud and Pernille Hemmer.
2014.
The in-fluence of knowledge and expectations for color onepisodic memory.
In P Bello, M Guarini, M Mc-Shane, and B Scassellati, editors, Proceedings of the36th Annual Conference of the Cognitive Science So-ciety, pages 1162?1167.Terry Regier, Paul Kay, and Richard S. Cook.
2005.
Fo-cal colors are universal after all.
Proceedings of theNational Academy of Sciences, 102:8386?8391.Terry Regier, Paul Kay, and Naveen Khetarpal.
2007.Color naming reflects optimal partitions of colorspace.
Proceedings of the National Academy of Sci-ences, 104:1436?1441.Carina Silberer, Vittorio Ferrari, and Mirella Lapata.2013.
Models of Semantic Representation with VisualAttributes.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics,pages 572?582.Nathaniel J. Smith, Noah D. Goodman, and Michael C.Frank.
2013.
Learning and using language via recur-sive pragmatic reasoning about other agents.
In Ad-vances in Neural Information Processing Systems 26,pages 3039?3047.Stefanie Tellex, Thomas Kollar, and Steven Dickerson.2011a.
Approaching the symbol grounding problemwith probabilistic graphical models.
AI magazine,32(4):64?76.Stefanie Tellex, Thomas Kollar, Steven Dickerson,Matthew R Walter, Ashis Gopal Banerjee, Seth JTeller, and Nicholas Roy.
2011b.
Understanding nat-ural language commands for robotic navigation andmobile manipulation.
In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, pages1507?1514.Terry Winograd.
1970.
Procedures as a representationfor data in a computer program for understanding nat-ural language.
Ph.D. thesis, MIT.Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-enmaier.
2014.
From image descriptions to visualdenotations: New similarity metrics for semantic in-ference over event descriptions.
Transactions of theAssociation for Computational Linguistics, 2:67?78.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logic pro-gramming.
In Proceedings of the National Conferenceon Artificial Intelligence, pages 1050?1055.Luke S. Zettlemoyer and Michael Collins.
2005.
Learn-ing to map sentences to logical form: Structured clas-sification with probabilistic categorial grammars.
InUAI ?05, Proceedings of the 21st Conference in Un-certainty in Artificial Intelligence, pages 658?666.115116
