Complete Syntactic Analysis Based on Multi-level ChunkingZhiPeng Jiang and Yu Zhao and Yi Guan andChao Li and Sheng LiSchool of Computer Science and Technology,Harbin Institute of Technology,150001, Harbin, Chinaxyf-3456@163.com; woshizhaoy@gmail.comguanyi@hit.edu.cn; beyondlee2008@yahoo.cnlisheng@hit.edu.cnAbstractThis paper describes a complete syntacticanalysis system based on multi-levelchunking.
On the basis of the correct se-quences of Chinese words provided byCLP2010, the system firstly has a Part-of-speech (POS) tagging with ConditionalRandom Fields (CRFs), and then does thebase chunking and complex chunking withMaximum Entropy (ME), and finally gene-rates a complete syntactic analysis tree.The system took part in the Complete Sen-tence Parsing Track of the Task 2 ChineseParsing in CLP2010, achieved the F-1measure of 63.25% on the overall analysis,ranked the sixth; POS accuracy rate of89.62%, ranked the third.1 IntroductionChunk is a group of adjacent words which belongto the same s-projection set in a sentence, whosesyntactic structure is actually a tree (Abney, 1991),but apart from the root node, all other nodes areleaf nodes.
Complete syntactic analysis requires aseries of analyzing processes, eventually to get afull parsing tree.
Parsing by chunks is proved to befeasible (Abney, 1994).The concept of chunking was first proposed byAbney in 1991, who defined chunks in terms ofmajor heads, and parsed by chunks in 1994 (Ab-ney, 1994).
An additional chunk tag set {B, I, O}was added to chunking (Ramshaw and  Marcus,1995), which limited dependencies between ele-ments in a chunk, changed chunking into a ques-tion of sequenced tags, to promote the develop-ment of chunking.
Chunking algorithm was ex-tended to the bottom-up parser, which is trainedand tested on the Wall Street Journal (WSJ) part ofthe Penn Treebank (Marcus, Santorini and Mar-cinkiewicz 1993), and achieved a performance of80.49% F-measure, the results show that it per-formed better than a standard probabilistic con-text-free grammar, and can improve performanceby adding the information of parent node (Sang,2000).On Chinese parsing, Maximum Entropy Modelwas first used to have a POS tagging and chunking,and then a full parsing tree was generated (Fung,2004), training and testing in the Penn ChineseTreebank, which achieved 79.56% F-measure.
Theparsing process was divided into POS tagging,base chunking and complex chunking, having aPOS tagging and chunking on a given sentence,and then looping the process of complex chunkingup to identify the root node (Li and Zhou, 2009).This parsing method is the basis of this paper.
Inaddition, we have the existing Chinese chunkingsystem in laboratory, which ranked first in Task 2:Chinese Base Chunking of CIPS-ParsEval-2009,so we try to apply chunking to complete syntacticanalysis in CLP2010, to achieve better results.We will describe the POS tagging based onCRFs in Section 2, including CRFs, feature tem-plate selection and empirical results.
Multi-levelchunking based on ME will be expounded in Sec-tion 3, including ME, MEMM, base chunking andcomplex chunking.
Finally, we will summarize ourwork in Section 4.2 POS Tagging Based on CRFs2.1 Conditional Random FieldsX is a random variable over data sequences to belabeled, and Y is a random variable over corres-ponding label sequences.
All components Yi of Yare assumed to range over a finite label alphabet.For example, X might range over natural languagesentences and Y range over part-of-speech tags ofthose sentences, a finite label alphabet is the set ofpossible part-of-speech tags (Lafferty and McCal-lum and Pereira, 2001).
CRFs is represented by thelocal feature vector f and the corresponding weightvector, f is divided into the state feature s (y, x, i)and transfer feature t (y, y', x, i), where y and y' arepossible POS tags, x is the current input sentence, iis the position of current term (Jiang and Guan andWang, 2006).
Formalized as follows:s (y, x, i) = s (yi, x, i)                      (1)?
??
?1, , , 1, ,0 1i it y y x i it y x ii??
???
??
??
(2)By the local feature of the formula (1) and (2),the global features of x and y:?
?
?
?, , ,iF y x f y x i??
(3)At this point of (X, Y), the conditional probabil-ity distribution of CRFs:?
?
?
??
??
?exp ,| F Y Xp Y X Z X?
????
(4)where ?
?
?
??
?exp ,yZ x F y x?
??
??
is a fac-tor for normalizing.
For the input sentence x, thebest sequence of POS tagging:?
?arg max |yy p y x??
?2.2 Feature Template SelectionWe use the template as a baseline which is takenby Yang (2009) in CIPS-ParsEval-2009, directlytesting the performance, whose accuracy was93.52%.
On this basis, we adjust the feature tem-plate through the experiment, and improve thetagging accuracy of unknown words by introduc-ing rules, in the same corpus for training and test-ing, accuracy is to 93.89%.
Adjusted feature tem-plate is shown in Table 1, in which the term pre isthe first character in current word, suf is the lastcharacter of current word, num is the number ofcharacters of current word, pos-1 is the tagging re-sults of the previous word.Table 1: feature templatefeature templatew2,w1,w0,w-1,w-2,w+1w0,w0w-1,pre0, pre0w0,suf0,w0suf0,num,pos-12.3 Empirical Results and AnalysisWe divide the training data provided by CLP2010into five folds, the first four of which are train cor-pus, the last one is test corpus, on which we usethe CRF++ toolkit for training and testing.
Tag-ging results with different features are shown intable 2.Table 2: tagging results with different featuresModel Explain AccuracyCRF baseline 93.52%CRF1 add w-1, pos-1 93.58%CRF2 add num 93.66%CRF3 add num, w-1, pos-1 93.68%CRF4 add num, rules 93.80%CRF5 add num, w-1, pos-1, rules 93.89%Tagging results show that the number of charac-ter and POS information can be added to improvethe accuracy of tagging, but in CLP2010, the tag-ging accuracy is only 89.62%, on the one hand itmay be caused by differences of corpus, on theother hand it may be due to that we don?t use allthe features of CRFs but remove the featureswhich appear one time in order to reduce the train-ing time.3 Multi-level Chunking Based on ME3.1 Maximum Entropy Models and Maxi-mum Entropy Markov ModelsMaximum entropy model is mainly used to esti-mate the unknown probability distribution whoseentropy is the maximum under some existing con-ditions.
Suppose h is the observations of context, tis tag, the conditional probability p (t | h) can beexpressed as:exp( ( , ))( | ) ( )ii i t hP t h Z hf??
?where fi is the feature of model,( ) exp( ( , ))i it iZ h t hf???
?is a factor for nor-malizing.i?
is weigh of feature fi, training is theprocess of seeking the value ofi?
.Maximum entropy Markov model is the seria-lized form of Maximum entropy model (McCal-lum and Freitag and Pereira, 2000), for example,transition probabilities and emission probabilitiesare merged into a single conditional probabilityfunction1( | , )i iP t t h?
in binary Maximum entropyMarkov model,1( | , )i iP t t h?
is turned to ( | )p t h  tobe solved by adding features which can expresspreviously tagging information (Li and Sun andGuan, 2009).3.2 Base ChunkingFollowing the method of multi-level chunking, wefirst do the base chunking on the sentences whichare through the POS tagging, then loop the processof complex chunking until they can?t be merged.We use the existing Chinese base chunking systemto do base chunking in laboratory, which marksboundaries and composition information of chunkwith MEMM, and achieved 93.196% F-measure inTask 2: Chinese Base Chunking of CIPS-ParsEval-2009.
The input and output of base chunking areas follows:Input??
?/nS ?
?/a  ?
?/n ?/v ?
?/nR  ?
?/n ?/p ?
?/n ?/uJDE ?
?/n ?/wD ??/n?
?/vN ?/f ?/wP ?
?/d  ?
?/v ?/wP ??/d?
?/v ?/c ?
?/d  ?
?/v ?/uJDE ?
?/v ?
?/a  ?
?/n ?
?/n ?/uJDE ?
?/n  ?
?/n ?/wEOutput??
?/nS [np ?
?/a  ?
?/n ] ?/v [np ?
?/nR  ?
?/n ] ?/p ?
?/n ?/uJDE [np ?
?/n  ?/wD ?
?/n ] ?
?/vN ?/f ?/wP [vp ?
?/d  ?
?/v ] ?/wP [vp ?
?/d  ?
?/v ] ?/c [vp?
?/d  ?
?/v ] ?/uJDE ?
?/v [np ?
?/a  ?
?/n ] ?
?/n ?/uJDE [np ?
?/n  ?
?/n ] ?/wE3.3 Complex ChunkingWe take the sentences which are through POS tag-ging and base chunking as input, using Li?s tag-ging method and feature template.
Categories ofcomplex chunk include xx_Start, xx_Middle,xx_End and Other, where xx is a category of arbi-trary chunk.
The process of complex chunking isshown as follows:Step 1: input the sentences which are through POStagging and base chunking, for example:?
?/nS  [np ?
?/a  ?
?/n  ] ?/uJDE  [np ?
?/vN  ?
?/vN  ] ?/c  [np ?
?/n  ?
?/n  ]Step 2: if there are some category tags in the sen-tence, then turn a series of tags to brackets, forinstance, if continuous cells are marked asxx_Start, xx_Middle, ..., xx_Middle, xx_End, thenthe combination of continuous cells is a complexchunk xx;Step 3: determine the head words with the set ofrules, and compress the sentence:?
?/nS  [np ?
?/n  ] ?/uJDE  [np ?
?/vN  ] ?/c  [np ?
?/n  ]Step 4: if the sentence can be merged, mark thesentence with ME, then return step 2, else theanalysis process ends:??
/nS@np_Start  [np ??
/n  ]@np_End ?/uJDE@Other  [np ??
/vN  ]@np_Start ?/c@np_Middle  [np ?
?/n  ]@np_EndAt last, the output is:[np [np ?
?/nS  [np ?
?/a  ?
?/n  ] ] ?/uJDE[np [np ?
?/vN  ?
?/vN  ] ?/c  [np ?
?/n  ?
?/n  ] ] ]Following the above method, we first use theViterbi decoding, but in the decoding process weencountered two problems:1.
Similar to the label xx_Start, whose back is onlyxx_Middle or xx_End, so the probability ofxx_Start label turning to Other is 0, But, if onlyusing ME to predict, the probability may not be 0.2.
Viterbi decoding can?t solve that all the labels ofpredicted results are Other, if all labels are Other,they can?t be merged, this result doesn?t makesense.Solution:For the first question, we add the initial transfermatrix and the end transfer matrix in decodingprocess, that is, the corresponding xx_Middle orxx_End of xx_Start is seted to 1 in the transfermatrix, the others are marked as 0, matrix multip-lication is taken during the state transition.
It caneffectively avoid errors caused by probability toimprove accuracy.To rule out the second question, we use heuris-tic search approach to decode, and exclude allOther labels with the above matrix.
In addition, wedefined another ME classifier to do some pruningin the decoding process, the features of ME clas-sifier are POS, the head word, the POS of headword.
The pseudo-code of Heuristic search is:While searching priority queue is not emptyTake the node with the greatest priority in thequeue;If the node?s depth = length of the chunkingresultsSearching is over, reverse the search-ing path to get searching results;ElseCompute the probability of all candi-date children nodes according tothe current probability;Record searching path;Press it into the priority queue;In addition, we found that some punctuation atthe end of a sentence can?t be merged, probablydue to sparseness of data, according to that thetone punctuation (period, exclamation mark, ques-tion mark) at the end of the sentence can be addedto implement a complete sentence (zj) (Zhou,2004), we carried out a separate deal with this sit-uation, directly add punctuation at the end of thesentence, to form a sentence.In training data provided by CLP2010 in sub-task: Complete Sentence Parsing, the head wordsaren?t marked.
We can?t use the statistical methodto determine the head words, but only by rules.
Wetake Li?s rule set as baseline, but the rule set wasused to supplement the statistical methods, sosome head words don?t appear in the rule set, re-sulting in many head words are marked as NULL,for this situation, we add some rules through expe-riment, Table 3 lists some additional rules.Table 3: increasing part of rulesparent  head wordsvp vp, vB, vSB, vM, vJY, vC, vap a, b, dmp qN, qV, qC, qdj vp, dj, ap, v, fjdlc vpmbar m, mp3.4 Empirical Results and AnalysisWe take the corpus which are through correct POStagging and base chunking for training and testing,it is divided into five folds, the first four as train-ing corpus, the last one as testing corpus, using theexisting ME toolkit to train and test model in la-boratory.
Table 4 shows the results on Viterbi de-coding and Heuristic Search method, where headwords are determined by rules.Table 4: results with different decodingDecoding Accuracy Recall FmeasureViterbi  84.87% 84.47% 84.67%HeuristicSearch85.62% 85.19% 85.40%The system participated in the Complete Sen-tence Parsing of CLP2010, results are shown inTable 5 below.
Because we can?t determine thehead words by statistical method on the corpusprovided by CLP2010, resulting in the accuracydecreasing, creating a great impact on results.Table 5: the track resultsTrainingmodeModel use F-measure POSAccuracyClosed Single 63.25% 89.62%4 ConclusionsIn this paper, we use CRFs to have a POS tagging,and increase the tagging accuracy by adjusting thefeature template; multi-level chunking is appliedto complete syntactic analysis, we do the basechunking with MEMM to recognize boundariesand components, and make the complex chunkingwith ME to generate a full parsing tree; on decod-ing, we add transfer matrix to improve perfor-mance, and remove some features with a ME clas-sifier to reduce training time.As the training data are temporarily changed,our system?s training on the Event DescriptionSub-sentence Analysis of CLP2010 isn?t com-pleted, and head words are marked in the trainingcorpus of this task, so our next step will be tocomplete training and testing of this task, comparethe existing evaluation results, and use ME clas-sifier to determine head words, analyze impact ofhead words on system.
On the POS tagging, wewill retain all features to train and compare tag-ging results.AcknowledgementWe would like to thank XingJun Xu and BenYangLi for their valuable advice to our work in Com-plete Sentence Parsing of CLP2010.
We also thankJunHui Li, XiaoRui Yang and HaiLong Cao forpaving the way for our work.ReferencesS.
Abney (1991) Parsing by Chunks.
Kluwer AcademicPublishers, Dordrecht, 257-278Lance A. Ramshaw, Mitchell P. Marcus (1995) TextChunking Using Transformation-Based Learning.
InProceeding of the Third ACL Workshop on VeryLarge Corpora, USA, 87-88Erik F. Tjong Kim Sang (2001) Transforming a Chunk-er to a Parser.
Computational Linguistics in theNetherlands 2000, 6-8YongSheng Yang, BenFeng Chen (2004) A Maximum-Entropy Chinese Parser Augmented by Transforma-tion-Based Learning.
ACM Transactions on AsianLanguage Information Processing, 4-8John Lafferty, Andrew McCallum, and Fernando Perei-ra (2001) Conditional Random Fields: ProbabilisticModels for Segmenting and Labeling Sequence Data.Proceedings of the Eighteenth International Confe-rence on Machine Learning, 282-289Junhui Li, Guodong Zhou (2009) Soochow UniversityReport for the 1st China Workshop on SyntacticParsing.
CIPS-ParsEval-2009, 5-8Wei Jiang, Yi Guan, and Xiaolong Wang (2006) Condi-tional Random Fields Based POS Tagging.ComputerEngineering and Applications, 14-15Xiaorui Yang, Bingquan Liu, Chengjie Sun, and LeiLin (2009) InsunPOS: a CRF-based POS TaggingSystem.
CIPS-ParsEval-2009, 4-6A.
McCallum, D. Freitag, and F. Pereira (2000) Maxi-mum Entropy Markov Models for Information Ex-traction and Segmentation.
Proceedings of ICML-2000, Stanford University, USA, 591-598Chao Li, Jian Sun, Yi Guan, Xingjun Xu, Lei Hou, andSheng Li (2009) Chinese Chunking With MaximumEntropy Models.
CIPS-ParsEval-2009, 2-4Qiang Zhou (2004) Annotation Scheme for ChineseTreebank.
Journal of Chinese Information Processing,4-5
