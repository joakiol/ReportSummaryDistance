A Memory-Based Approach for Semantic Role LabelingBeata KouchnirDepartment of Computational LinguisticsUniversity of Tu?bingenWilhelmstrasse 19, 72074 Tu?bingen, Germanykouchnir@sfs.uni-tuebingen.de1 IntroductionThis paper presents a system for Semantic Role Label-ing (SRL) for the CoNLL 2004 shared task (Carreras andMa`rquez, 2004).
The task is divided into two sub-tasks,recognition and labeling.
These are performed indepen-dently with different feature representations.
Both mod-ules are based on the principle of memory-based learning.For the first module, we use the IOB2 format to deter-mine whether a chunk belongs to an argument or not.
Fur-thermore, we test two different strategies for extractingarguments from the classifier output.
The second modulelabels the extracted arguments with one of the 30 seman-tic roles.2 Memory-Based LearningThe concept of Memory-Based Learning (MBL) (Lin andVitter, 1994) is to classify unseen (test) instances basedon their similarity to known (training) instances.
In prac-tice, this is done by storing all training in memory with-out abstraction and computing the similarity between newand old examples based on a distance metric.
New in-stances are then assigned the most frequent class within aset of k most similar examples (k-nearest neighbors).Memory-based learning algorithms have proven to beeffective for several NLP tasks, including named entityrecognition (Hendrickx and van den Bosch, 2003), clauseidentification (Tjong Kim Sang, 2001) and most rele-vantly, grammatical relation finding (Buchholz, 2002).As testing all possible distance metrics in combinationwith different values for k is not feasible, we have limitedthe experiment to the Overlap and Modified Value Dif-ference (MVDM) metrics.
The values for k tested eachmetric were 1, 3, 5, 7, and 9.
Even values were omittedin order to avoid ties.The Overlap metric computes the distance betweentwo instances by adding up the differences between thefeatures.
For symbolic features, each mismatch has avalue of 1.
MVDM, however, allows different degreesof similarity by examining co-occurrence of feature val-ues with target classes.
While this concept seems moresuitable for the underlying task, it is only reliable whenused with large amounts of data.
For a more detailed de-scription of the distance metrics, see (Daelemans et al,2003).TiMBL1 (Daelemans et al, 2003), the MBL imple-mentation used in this experiment, is freely availablefrom the ILK research group at Tilburg University.3 The Recognition ModuleThis module identifies the arguments of a proposition,without assigning a label.
For this task we use the IOB2format, where B marks an element at the beginning ofan argument, I an element inside an argument and O anelement that does not belong to an argument.As all argument boundaries, except for those within thetarget verb chunks, coincide with base chunk boundaries,the data is processed by words only within the target verbchunk, and by chunks otherwise.The recognition module uses the following features:?
Head word and POS of the focus element, wherethe head of a multi-word chunk is its last words.?
Chunk type: one of the 12 chunks types, withoutthe B- or I- prefix.?
Clause information: whether the element is at thebeginning, at the end or inside a clause.?
Directionality: whether the focus element comesbefore the target verb, after the target verb, or co-incides with the target verb.?
Distance: numerical distance (1 .. n) between thefocus element and the target verb.1http://ilk.kub.nl/software.htmlMetric / k B I OOverlap k=1 87.27 69.34 80.49MVDM k=1 85.96 74.22 83.83Overlap k=3 87.91 71.96 82.61MVDM k=3 87.68 75.35 85.12Overlap k=5 88.37 73.43 83.47MVDM k=5 89.21 76.70 86.52Overlap k=7 88.56 73.41 83.54MVDM k=7 89.31 77.43 86.83Overlap k=9 88.69 73.61 84.04MVDM k=9 89.39 77.38 86.77Table 1: Results for different distance metrics and valuesof k?
Adjacency: whether the focus element is adjacent tothe verb chunk or not, or it is within the verb chunk.?
The target verb and voice: the voice is passive ifthe target verb is a past participle preceded by a formof to be, and active otherwise.?
Context: in addition, the features head word, partof speech, chunk type and adjacency of the threechunks each to the left and right of the focus chunkare used as context information.Testing each feature separately showed the direction-ality and adjacency features to be most useful.
Omittingone feature at a time showed to decrease performance forevery omitted feature.
Therefore, all of the above featureswere used in the final system.The best TiMBL parameter setting for this task wasdetermined to be the Modified Value Difference metricpaired with a set of seven nearest neighbors.
As we an-ticipated, the nature of the task requires a more subtledifferentiation than the Overlap metric can provide.
Fur-thermore, the size of the training set is apparently suffi-cient to take full advantage of MVDM.
The results forboth metrics and all values of k are summarized in Table1.
It is interesting to observe the effect of the k value foreach class.
Although the results for the I- and O-classesdecrease after k=7, those for the B-class do not.
However,since the overall results are best for k=7, this values waschosen for the final system.For all metric/k combination, the results for the I classare much lower than for the other two.
The most com-mon error is the assignment of the O class to I-elements,or vice versa.
This performance distribution implies thatwhile the beginning of most arguments is recognized cor-rectly, their span is not, which results in many ?broken-up?
arguments.To filter out the actual arguments, we try a strict anda lenient approach.
For the latter, any sequence of ele-ments that is not labeled as O is considered an argument(i.e.
also those not starting with a B-element).
Althoughthis approach slightly reduces the number of missed ar-guments, it also vastly overgenerates, which ultimatelydecreases performance.
The former approach recognizesas arguments only those sequences beginning with a B-element.
Since B is the class most reliably predicted bythe classifier, this approach yields better overall perfor-mance.4 The Labeling ModuleThis module assigns one of the 30 semantic role labels tothe arguments extracted by the recognition module.
Here,we used only ten features, of which four are ?recycled?from the previous module:?
Word, POS and chunk sequence: the head wordsof all the chunks in the argument, their respectiveparts of speech and chunk types.
As TiMBL onlyallows feature vectors of a fixed length, each of thesequences represents one value.?
Clause information: as an element sequences canbe a whole clause we added this value to the begin-ning, end and inside values described in Section 3.?
Length: the length in chunks of the argument.?
Directionality and adjacency: same as in Section3.?
The target verb and voice: same as in Section 3.?
Prop Bank roleset of the target verb: as an analysisof the training data showed that about 86% of theverbs were used in their first sense, and many times,the rolesets for the first two senses are identical, weonly considered the roleset of first sense.Just as for the recognition module, the directionalityand adjacency features had the highest information gain.The POS sequence and length features showed no effect,and their omission even slightly improved performance.Therefore, the final system uses only eight features.To test the performance of this module independentlyfrom the first, it was evaluated on the gold-standard ar-guments (i.e.
recognition score of 100).
While MVDMonce again outperforms the Overlap metric, the optimalvalue for k in this setting is one.
The former supports theassumption that for feature values such as words, or wordsequences, some values are more similar than others.
Thelatter suggest that the size of the nearest neighbor set (1vs.
7) should be somewhat proportional to the length ofthe feature vector (8 vs. 45).The results for each semantic role are summarized inTable 2.
It can be seen that arguments with very restrictedsurface patterns (e.g.
AM-DIS, AM-MOD, AM-NEG)Precision Recall F?=1Overall 75.71% 74.60% 75.15A0 82.35% 83.41% 82.88A1 80.69% 82.14% 81.40A2 61.89% 64.68% 63.25A3 36.18% 36.91% 36.54A4 58.39% 63.95% 61.04A5 33.33% 50.00% 40.00AM 0.00% 0.00% 0.00AM-ADV 41.89% 35.23% 38.27AM-CAU 16.67% 9.43% 12.05AM-DIR 40.91% 30.00% 34.62AM-DIS 84.04% 87.75% 85.85AM-EXT 48.72% 38.78% 43.18AM-LOC 55.06% 42.61% 48.04AM-MNR 55.81% 35.93% 43.72AM-MOD 89.90% 96.14% 92.92AM-NEG 95.52% 97.71% 96.60AM-PNC 55.32% 26.00% 35.37AM-PRD 25.00% 33.33% 28.57AM-REC 0.00% 0.00% 0.00AM-TMP 70.06% 64.43% 67.12R-A0 82.63% 85.19% 83.89R-A1 67.90% 74.32% 70.97R-A2 72.22% 76.47% 74.29R-A3 0.00% 0.00% 0.00R-AM-LOC 100.00% 50.00% 66.67R-AM-MNR 0.00% 0.00% 0.00R-AM-TMP 44.44% 66.67% 53.33V 99.84% 99.84% 99.84Table 2: Results for the labeling module with perfect ar-gument spansare fairly easy to predict.
However, it must be noted thatgiven the correct span, the complex (and most frequentlyoccurring) arguments A0 and A1 can be also predictedwith very high accuracy.
On the down side, the accuracyfor most adjuncts is rather low, even though their surfacepatterns are thought to be somewhat restricted (e.g.
AM-LOC, AM-TMP, AM-MNR, AM-EXT).5 EvaluationTables 3 and 4 show the final results for the develop-ment and test set, respectively.
Although each moduleperforms fairly well separately, their combined results aresuboptimal.
This is probably due to the fact that the label-ing module is trained with gold standard arguments, andis not able to deal with noise induced by the recognitionmodule.
The argument type whose results suffer the mostis A1, because it usually spans over several chunks, and isdifficult to retrieve correctly by the recognition module.Improvements to the system could be made on the syn-Precision Recall F?=1Overall 44.93% 63.12% 52.50A0 59.19% 80.31% 68.15A1 48.03% 63.53% 54.70A2 24.40% 44.55% 31.53A3 15.92% 30.87% 21.00A4 33.06% 55.10% 41.33A5 25.00% 25.00% 25.00AM 0.00% 0.00% 0.00AM-ADV 18.77% 29.55% 22.96AM-CAU 3.57% 7.55% 4.85AM-DIR 11.01% 20.00% 14.20AM-DIS 51.75% 86.76% 64.84AM-EXT 31.15% 38.78% 34.55AM-LOC 17.26% 25.22% 20.49AM-MNR 27.69% 30.84% 29.18AM-MOD 82.93% 96.14% 89.05AM-NEG 92.65% 96.18% 94.38AM-PNC 16.35% 17.00% 16.67AM-PRD 0.00% 0.00% 0.00AM-REC 0.00% 0.00% 0.00AM-TMP 31.09% 47.43% 37.56R-A0 79.21% 87.04% 82.94R-A1 54.21% 78.38% 64.09R-A2 68.42% 76.47% 72.22R-A3 0.00% 0.00% 0.00R-AM-LOC 44.44% 100.00% 61.54R-AM-MNR 0.00% 0.00% 0.00R-AM-TMP 60.00% 100.00% 75.00V 98.14% 98.26% 98.20Table 3: Results for the development settactic, lexical, as well as semantic levels.
Firstly, it is cru-cial to improve the performance of the recognition mod-ule on I-elements.
This could either be done by usinga head-lexicalized parser, or, on a lower level, by a pre-processing module that resolves prepositional phrase at-tachment.
Performance for adjuncts such as AM-LOCor AM-TMP could be improved, by using gazetteers oftrigger words (e.g.
Tuesday) or morphemes (e.g.
-day).Furthermore, one could use a semantic database such asWordNet to cluster words.
Last but not least, more advan-tage could be taken from the information in Prop Bank,so different representations of the rolesets should be ex-plored.ReferencesSabine Buchholz.
2002.
Memory-based grammatical re-lation finding.
Ph.D. thesis, Tilburg University.Xavier Carreras and Llu?
?s Ma`rquez.
2004.
IntroductionPrecision Recall F?=1Overall 56.86% 49.95% 53.18A0 68.12% 63.05% 65.49A1 55.79% 53.22% 54.48A2 30.95% 30.95% 30.95A3 21.77% 18.00% 19.71A4 30.56% 44.00% 36.07A5 0.00% 0.00% 0.00AA 0.00% 0.00% 0.00AM-ADV 23.91% 10.75% 14.83AM-CAU 0.00% 0.00% 0.00AM-DIR 28.89% 26.00% 27.37AM-DIS 53.30% 53.05% 53.18AM-EXT 15.00% 21.43% 17.65AM-LOC 21.78% 9.65% 13.37AM-MNR 45.19% 23.92% 31.28AM-MOD 91.18% 91.99% 91.58AM-NEG 90.77% 92.91% 91.83AM-PNC 26.09% 7.06% 11.11AM-PRD 0.00% 0.00% 0.00AM-TMP 47.49% 31.73% 38.04R-A0 82.61% 71.70% 76.77R-A1 64.91% 52.86% 58.27R-A2 50.00% 44.44% 47.06R-A3 0.00% 0.00% 0.00R-AM-LOC 0.00% 0.00% 0.00R-AM-MNR 0.00% 0.00% 0.00R-AM-PNC 0.00% 0.00% 0.00R-AM-TMP 66.67% 14.29% 23.53V 97.77% 97.82% 97.79Table 4: Results for the test setto the CoNLL-2004 shared task: Semantic role label-ing.
In Proceedings of ConNLL-2004.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2003.
TiMBL: Tilburg memorybased learner, version 5.0, reference guide.
Technicalreport, ILK.Iris Hendrickx and Antal van den Bosch.
2003.
Memory-based one-step named-entity recognition: Effects ofseed list features, classifier stacking, and unannotateddata.
In Proceedings of CoNLL-2003, pages 176?179.Jyh-Han Lin and Jeffrey Scott Vitter.
1994.
A theory formemory-based learning.
Machine Learning, 17:1?26.Erik Tjong Kim Sang.
2001.
Memory-based clause iden-tification.
In Proceedings of CoNLL-2001, pages 67?69.
