Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 21?26,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Multitask Objective to Inject Lexical Contrastinto Distributional SemanticsNghia The Pham Angeliki Lazaridou Marco BaroniCenter for Mind/Brain SciencesUniversity of Trento{thenghia.pham|angeliki.lazaridou|marco.baroni}@unitn.itAbstractDistributional semantic models have trou-ble distinguishing strongly contrastingwords (such as antonyms) from highlycompatible ones (such as synonyms), be-cause both kinds tend to occur in similarcontexts in corpora.
We introduce the mul-titask Lexical Contrast Model (mLCM),an extension of the effective Skip-grammethod that optimizes semantic vectorson the joint tasks of predicting corpuscontexts and making the representationsof WordNet synonyms closer than thatof matching WordNet antonyms.
mLCMoutperforms Skip-gram both on generalsemantic tasks and on synonym/antonymdiscrimination, even when no direct lex-ical contrast information about the testwords is provided during training.
mLCMalso shows promising results on the taskof learning a compositional negation oper-ator mapping adjectives to their antonyms.1 IntroductionDistributional semantic models (DSMs) extractvectors representing word meaning by relying onthe distributional hypothesis, that is, the idea thatwords that are related in meaning will tend to oc-cur in similar contexts (Turney and Pantel, 2010).While extensive work has shown that contextualsimilarity is an excellent proxy to semantic simi-larity, a big problem for DSMs is that both wordswith very compatible meanings (e.g., near syn-onyms) and words with strongly contrasting mean-ings (e.g., antonyms) tend to occur in the samecontexts.
Indeed, Mohammad et al (2013) haveshown that synonyms and antonyms are indistin-guishable in terms of their average degree of dis-tributional similarity.This is problematic for the application of DSMsto reasoning tasks such as entailment detection(black is very close to both dark and white in dis-tributional semantic space, but it implies the for-mer while contradicting the latter).
Beyond word-level relations, the same difficulties make it chal-lenging for compositional extensions of DSMsto capture the fundamental phenomenon of nega-tion at the phrasal and sentential levels (the dis-tributional vectors for good and not good arenearly identical) (Hermann et al, 2013; Prellerand Sadrzadeh, 2011).Mohammad and colleagues concluded thatDSMs alone cannot detect semantic contrast, andproposed an approach that couples them with otherresources.
Pure-DSM solutions include isolatingcontexts that are expected to be more discrimina-tive of contrast, tuning the similarity measure tomake it more sensitive to contrast or training a su-pervised contrast classifier on DSM vectors (Adeland Sch?utze, 2014; Santus et al, 2014; Schulte imWalde and K?oper, 2013; Turney, 2008).
We pro-pose instead to induce word vectors using a mul-titask cost function combining a traditional DSMcontext-prediction objective with a term forcingwords to be closer to their WordNet synonymsthan to their antonyms.
In this way, we make themodel aware that contrasting words such as hotand cold, while still semantically related, shouldnot be nearest neighbours in the space.In a similar spirit, Yih et al (2012) devise aDSM in which the embeddings of the antonymsof a word are pushed to be the vectors that arefarthest away from its representation.
While theirmodel is able to correctly pick the antonym of atarget item from a list of candidates (since it isthe most dissimilar element in the list), we con-jecture that their radical strategy produces embed-dings with poor performance on general semantictasks.1Our method has instead a beneficial global1Indeed, by simulating their strategy, we were able to in-ject lexical contrast into word embeddings, but performanceon a general semantic relatedness task decreased dramati-21effect on semantic vectors, leading to state-of-the-art results in a challenging similarity task, and en-abling better learning of a compositional negationfunction.Our work is also closely related to Faruqui et al(2015), who propose an algorithm to adapt pre-trained DSM representations using semantic re-sources such as WordNet.
This post-processingapproach, while extremely effective, has the dis-advantage that changes only affect words that arepresent in the resource, without propagating tothe whole lexicon.
Other recent work has insteadadopted multitask objectives similar to ours in or-der to directly plug in knowledge from structuredresources at DSM induction time (Fried and Duh,2015; Xu et al, 2014; Yu and Dredze, 2014).
Ourmain novelties with respect to these proposals arethe focus on capturing semantic contrast, and ex-plicitly testing the hypothesis that the multitaskobjective is also beneficial to words that are not di-rectly exposed to WordNet evidence during train-ing.22 The multitask Lexical Contrast ModelSkip-gram model The multitask Lexical Con-trast Model (mLCM) extends the Skip-grammodel (Mikolov et al, 2013).
Given an inputtext corpus, Skip-gram optimizes word vectorson the task of approximating, for each word, theprobability of other words to occur in its context.More specifically, its objective function is:1TT?t=1???
?c?j?c,j 6=0log p(wt+j|wt)??
(1)where w1, w2, ..., wTis the training corpus,consisting of a list of target words wt, for whichwe want to learn the vector representations (andserving as contexts of each other), and c is thewindow size determining the span of contextwords to be considered.
p(wt+j|wt), the proba-bility of a context word given the target word iscomputed using softmax:p(wt+j|wt) =ev?wt+jTvwt?Ww?=1ev?w?Tvwt(2)cally, with a 25% drop in terms of Spearman correlation.2After submitting this work, we became aware of Ono etal.
(2015), that implement very similar ideas.
However, onemajor difference between their work and ours is that theirstrategy is in the same direction of (Yih et al, 2012), whichmight result in poor performance on general semantic tasks.where vwand v?ware respectively the target andcontext vector representations of word w, and Wis the number of words in the vocabulary.
To avoidthe O(|W |) time complexity of the normalizationterm in Equation (2), Mikolov et al (2013) useeither hierarchical softmax or negative sampling.Here, we adopt the negative sampling method.Injecting lexical contrast information Weaccount for lexical contrast by implementing a2-task strategy, combining the Skip-gram contextprediction objective with a new term:1TT?t=1(Jskipgram(wt) + Jlc(wt)) (3)The lexical contrast objective Jlc(wt) tries to en-force the constraint that contrasting pairs shouldhave lower similarity than compatible ones withina max-margin framework.
Our formulation is in-spired by Lazaridou et al (2015), who use a sim-ilar multitask strategy to induce multimodal em-beddings.
Given a target word w, with sets ofantonyms A(w) and synonyms S(w), the max-margin objective for lexical contrast is:??s?S(w),a?A(w)max(0,??
cos(vw, vs)+ cos(vw, va)) (4)where ?
is the margin and cos(x, y) stands forcosine similarity between vectors x and y. Notethat, by equation (3), the Jlc(wt) term is evalu-ated each time a word is encountered in the corpus.We extract antonym and synonym sets from Word-Net (Miller, 1995).
If a word wtis not associatedto synonym/antonym information in WordNet, weset Jlc(wt) = 0.3 Experimental setupWe compare the performance of mLCM againstSkip-gram.
Both models?
parameters are esti-mated by backpropagation of error via stochasticgradient descent.
Our text corpus is a Wikipedia32009 dump comprising approximately 800M to-kens and 200K distinct word types.4Other hyper-parameters, selected without tuning, include: vec-tor size (300), window size (5), negative sam-ples (10), sub-sampling to disfavor frequent words(10?3).
For mLCM, we use 7500 antonym pairs3https://en.wikipedia.org4We only consider words that occur more than 50 times inthe corpus22MEN SimLexSkip-gram 0.73 0.39mLCM 0.74 0.52Table 1: Relatedness/similarity tasksand 15000 synonym pairs; on average, 2.5 pairsper word and 9000 words are covered.Both models are evaluated in four tasks:two lexical tasks testing the general quality ofthe learned embeddings and one focusing onantonymy, and a negation task which verifies thepositive influence of lexical contrast in a composi-tional setting.4 Lexical tasks4.1 Relatedness and similarityIn classic semantic relatedness/similarity tasks,the models provide cosine scores between pairs ofword vectors that are then compared to human rat-ings for the same pairs.
Performance is evaluatedby Spearman correlation between system and hu-man scores.
For general relatedness, we use theMEN dataset of Bruni et al (2014), which con-sists of 3,000 word pairs comprising 656 nouns,57 adjectives and 38 verbs.
The SimLex datasetfrom Hill et al (2014b), comprising 999 wordpairs (666 noun, 222 verb and 111 adjective pairs)was explicitly built to test a tighter notion of strict?semantic?
similarity.Table 1 reports model performance.
On MEN,mLCM outperforms Skip-gram by a small margin,which shows that the new information, at the veryleast, does not have any negative effect on gen-eral semantic relatedness.
On the other hand, lex-ical contrast information has a strong positive ef-fect on measuring strict semantic similarity, lead-ing mLCM to achieve state-of-the-art SimLex per-formance (Hill et al, 2014a).4.2 Distinguishing antonyms and synonymsHaving shown that capturing lexical contrast in-formation results in higher-quality representationsfor general purposes, we focus next on the spe-cific task of distinguishing contrasting words fromhighly compatible ones.
We use the adjective partof dataset of Santus et al (2014), that contains 262antonym and 364 synonym pairs.
We compute co-sine similarity of all pairs and use the area underthe ROC curve (AUC) to measure model perfor-mance.
Moreover, we directly test mLCM?s abil-AUCSkip-gram 0.62mLCM 0.78mLCM-propagate 0.66Table 2: Synonym vs antonym taskity to propagate lexical contrast across the vocab-ulary by retraining it without using WordNet in-formation for any of the words in the dataset, i.e.the words in the dataset are removed from the syn-onym or antonym sets of all the adjectives used intraining (mLCM-propagate in the results table).The results, in Table 2, show that mLCM cansuccessfully learn to distinguish contrasting wordsfrom synonyms.
The performance of the mLCMmodel trained without explicit contrast informa-tion about the dataset words proves moreover thatlexical contrast information is indeed propagatedthrough the lexical network.4.3 Vector space structureTo further investigate the effect of lexical con-trast information, we perform a qualitative anal-ysis of how it affects the space structure.
We pick20 scalar adjectives denoting spatial or weight-related aspects of objects and living beings, where10 indicate the presence of the relevant propertyto a great degree (big, long, heavy.
.
.
), whereasthe remaining 10 suggest that the property ispresent in little amounts (little, short, light.
.
.
).We project the 300-dimensional vectors of theseadjectives onto a 2-dimensional plane using thet-SNE toolkit,5which attempts to preserve thestructure of the original high-dimensional wordneighborhoods.
Figure 1 shows that, in Skip-gram space, pairs at the extreme of the same scale(light vs heavy, narrow vs wide, fat vs skinny) arevery close to each other compared to other words;whereas for mLCM the extremes are farther apartfrom each other, as expected.
Moreover, the ad-jectives at the two ends of the scales are groupedtogether.
This is a very nice property, since manyadjectives in one group will tend to characterizethe same objects.
Within the two clusters, wordsthat are more similar (e.g., wide and broad) arestill closer to each other, just as we would expectthem to be.5http://lvdmaaten.github.io/tsne/23-2000 -1500 -1000 -500 0 500 1000 1500 2000-1000-500050010001500largebigsmallhugegiantlongpetitetinybroadlittlefatthickthinshortminiaturedeepshallownarrowwidetallskinnylightheavylarge-groupsmall-group(a) Skip-gram space-800 -600 -400 -200 0 200 400 600 800 1000-1500-1000-500050010001500largebigsmallhugegiantlongpetitetinybroadlittlefatthickthinshortminiaturedeepshallownarrowwidetallskinnylightheavylarge-groupsmall-group(b) mLCM spaceFigure 1: Arrangement of some scalar adjectives in Skip-gram vs mLCM spaces5 Learning NegationHaving shown that injecting lexical contrast in-formation into word embeddings is beneficial forlexical tasks, we further explore if it can alsohelp composition.
Since mLCM makes contrast-ing and compatible words more distinguishablefrom each other, we conjecture that it would beeasier for compositional DSMs to capture negationin mLCM space.
We perform a proof-of-conceptexperiment where we represent not as a functionthat is trained to map an adjective to its antonym(good to bad).
That is, by adopting the frame-work of Baroni et al (2014), we take not to bea matrix that, when multiplied with an adjective-representing vector, returns the vector of an adjec-tive with the opposite meaning.
We realize thatthis is capturing only a tiny fraction of the linguis-tic uses of negation, but it is at least a concretestarting point.First, we select a list of adjectives and antonymsfrom WordNet; for each adjective, we only pickthe antonym of its first sense.
This yields a to-tal of around 4,000 antonym pairs.
Then, we in-duce the not matrix with least-squares regressionon training pairs.
Finally, we assess the learnednegation function by applying it to an adjectiveand computing accuracy in the task of retrievingthe correct antonym as nearest neighbour of thenot-composed vector, searching across all Word-Net adjectives (10K items).
The results in Table 3are obtained by using 10-fold cross-validation onthe 4,000 pairs.
We see that mLCM outperformsSkip-gram by a large margin.Figure 2 shows heatmaps of the weight matriceslearnt for not by the two models.
Intriguingly, formLCM, the not matrix has negative values on thediagonal, that is, it will tend to flip the values intrain testSkip-gram 0.44 0.02mLCM 0.87 0.27Table 3: Average accuracy in retrieving antonymas nearest neighbour when applying the not com-position function to 4,000 adjectives.Skip-Gram-0.100.10.20.3-0.2mLCMFigure 2: Heatmaps of not-composition matrices.the input vector, not unlike what arithmetic nega-tion would do.
On the other hand, the Skip-gram-based not matrix is remarkably identity-like, withlarge positive values concentrated on the diagonal.Thus, under this approach, an adjective will be al-most identical to its antonym, which explains whyit fails completely on the test set data: the nearestneighbour of not-X will typically be X itself.6 ConclusionGiven the promise shown by mLCM in the ex-periments reported here, we plan to test it nexton a range of linguistically interesting phenomenathat are challenging for DSMs and where lexicalcontrast information might help.
These includemodeling a broader range of negation types (deSwart, 2010), capturing lexical and phrasal infer-ence (Levy et al, 2015), deriving adjectival scales(Kim and de Marneffe, 2013) and distinguishingsemantic similarity from referential compatibility24(Kruszewski and Baroni, 2015).7 AcknowledgmentsThis research was supported by the ERC 2011Starting Independent Research Grant n. 283554(COMPOSES).ReferencesHeike Adel and Hinrich Sch?utze.
2014.
Using minedcoreference chains as a resource for a semantic task.In Proceedings of EMNLP, pages 1447?1452, Doha,Qatar.Marco Baroni, Raffaella Bernardi, and Roberto Zam-parelli.
2014.
Frege in space: A program for com-positional distributional semantics.
Linguistic Is-sues in Language Technology, 9(6):5?110.Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Ar-tificial Intelligence Research, 49:1?47.Henriette de Swart.
2010.
Expression and Interpreta-tion of Negation: an OT Typology.
Springer, Dor-drecht, Netherlands.Manaal Faruqui, Jesse Dodge, Sujay Jauhar, ChrisDyer, Ed Hovy, and Noah Smith.
2015.
Retrofittingword vectors to semantic lexicons.
In Proceedingsof NAACL, Denver, CO.
In press.Daniel Fried and Kevin Duh.
2015.
Incorporat-ing both distributional and relational semantics inword representations.
In Proceedings of ICLRWorkshop Track, San Diego, CA.
Published on-line: http://www.iclr.cc/doku.php?id=iclr2015:main#accepted_papers.Karl Moritz Hermann, Edward Grefenstette, and PhilBlunsom.
2013.
?Not not bad?
is not ?bad?
: A dis-tributional account of negation.
In Proceedings ofACL Workshop on Continuous Vector Space Mod-els and their Compositionality, pages 74?82, Sofia,Bulgaria.Felix Hill, KyungHyun Cho, Sebastien Jean, ColineDevin, and Yoshua Bengio.
2014a.
Not all neu-ral embeddings are born equal.
arXiv preprintarXiv:1410.0718.Felix Hill, Roi Reichart, and Anna Korhonen.
2014b.Simlex-999: Evaluating semantic models with(genuine) similarity estimation.
arXiv preprintarXiv:1408.3456.Joo-Kyung Kim and Marie-Catherine de Marneffe.2013.
Deriving adjectival scales from continu-ous space word representations.
In Proceedings ofEMNLP, pages 1625?1630, Seattle, WA.Germ?an Kruszewski and Marco Baroni.
2015.
So sim-ilar and yet incompatible: Toward automated identi-fication of semantically compatible words.
In Pro-ceedings of NAACL, pages 64?969, Denver, CO.Angeliki Lazaridou, Nghia The Pham, and Marco Ba-roni.
2015.
Combining language and vision witha multimodal skip-gram model.
In Proceedings ofNAACL, pages 153?163, Denver, CO.Omer Levy, Steffen Remus, Chris Biemann, , and IdoDagan.
2015.
Do supervised distributional methodsreally learn lexical inference relations?
In Proceed-ings of NAACL, Denver, CO.
In press.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL,pages 746?751, Atlanta, Georgia.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-ter Turney.
2013.
Computing lexical contrast.
Com-putational Linguistics, 39(3):555?590.Masataka Ono, Makoto Miwa, and Yutaka Sasaki.2015.
Word embedding-based antonym detectionusing thesauri and distributional information.
InProceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 984?989, Denver, Colorado, May?June.
As-sociation for Computational Linguistics.Anne Preller and Mehrnoosh Sadrzadeh.
2011.
Bellstates and negative sentences in the distributedmodel of meaning.
Electr.
Notes Theor.
Comput.Sci., 270(2):141?153.Enrico Santus, Qin Lu, Alessandro Lenci, and Chu-RenHuang.
2014.
Taking antonymy mask off in vectorspace.
In Proceedings of PACLIC, pages 135?144,Phuket,Thailand.Sabine Schulte im Walde and Maximilian K?oper.
2013.Pattern-based distinction of paradigmatic relationsfor German nouns, verbs, adjectives.
In Proceed-ings of GSCL, pages 184?198, Darmstadt, Germany.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Peter Turney.
2008.
A uniform approach to analogies,synonyms, antonyms and associations.
In Proceed-ings of COLING, pages 905?912, Manchester, UK.Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, GangWang, Xiaoguang Liu, and Tie-Yan Liu.
2014.
RC-NET: A general framework for incorporating knowl-edge into word representations.
In Proceedings ofCIKM, pages 1219?1228, Shanghai, China.25Wen-tau Yih, Geoffrey Zweig, and John Platt.
2012.Polarity inducing latent semantic analysis.
In Pro-ceedings of EMNLP-CONLL, pages 1212?1222.Mo Yu and Mark Dredze.
2014.
Improving lexical em-beddings with semantic knowledge.
In Proceedingsof ACL, pages 545?550, Baltimore, MD.26
