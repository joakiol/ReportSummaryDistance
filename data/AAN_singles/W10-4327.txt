Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 147?156,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsDiscourse indicators for content selection in summarizationAnnie Louis, Aravind Joshi, Ani NenkovaUniversity of PennsylvaniaPhiladelphia, PA 19104, USA{lannie,joshi,nenkova}@seas.upenn.eduAbstractWe present analyses aimed at elicitingwhich specific aspects of discourse pro-vide the strongest indication for text im-portance.
In the context of content selec-tion for single document summarization ofnews, we examine the benefits of both thegraph structure of text provided by dis-course relations and the semantic senseof these relations.
We find that structureinformation is the most robust indicatorof importance.
Semantic sense only pro-vides constraints on content selection butis not indicative of important content by it-self.
However, sense features complementstructure information and lead to improvedperformance.
Further, both types of dis-course information prove complementaryto non-discourse features.
While our re-sults establish the usefulness of discoursefeatures, we also find that lexical overlapprovides a simple and cheap alternativeto discourse for computing text structurewith comparable performance for the taskof content selection.1 IntroductionDiscourse relations such as cause, contrast orelaboration are considered critical for text inter-pretation, as they signal in what way parts of a textrelate to each other to form a coherent whole.
Forthis reason, the discourse structure of a text can beseen as an intermediate representation, over whichan automatic summarizer can perform computa-tions in order to identify important spans of textto include in a summary (Ono et al, 1994; Marcu,1998; Wolf and Gibson, 2004).
In our work, westudy the content selection performance of differ-ent types of discourse-based features.Discourse relations interconnect units of a textand discourse formalisms have proposed differentresulting structures for the full text, i.e.
tree (Mannand Thompson, 1988) and graph (Wolf and Gib-son, 2005).
This structure is one source of in-formation from discourse which can be used tocompute the importance of text units.
The seman-tics of the discourse relations between sentencescould be another indicator of content importance.For example, text units connected by ?cause?
and?contrast?
relationships might be more importantcontent for summaries compared to those convey-ing ?elaboration?.
While previous work have fo-cused on developing content selection methodsbased upon individual frameworks (Marcu, 1998;Wolf and Gibson, 2004; Uzda et al, 2008), little isknown about which aspects of discourse are actu-ally correlated with content selection power.In our work, we separate out structural and se-mantic features and examine their usefulness.
Wealso investigate whether simpler intermediate rep-resentations can be used in lieu of discourse.
Moreparsimonious, easy to compute representations oftext have been proposed for summarization.
Forexample, a text can be reduced to a set of highlydescriptive topical words, the presence of whichis used to signal importance for content selection(Lin and Hovy, 2002; Conroy et al, 2006).
Sim-ilarly, a graph representation of the text can becomputed, in which vertices represent sentences,and the nodes are connected when the sentencesare similar in terms of word overlap; properties ofthe graph would then determine the importance ofthe nodes (Erkan and Radev, 2004; Mihalcea andTarau, 2005) and guide content selection.We compare the utility of discourse features forsingle-document text summarization from threeframeworks: Rhetorical Structure Theory (Mannand Thompson, 1988), Graph Bank (Wolf andGibson, 2005), and Penn Discourse Treebank(PDTB) (Prasad et al, 2008).
We present a de-tailed analysis of the predictive power of differenttypes of discourse features for content selection147and compare discourse-based selection to simplernon-discourse methods.2 DataWe use a collection of Wall Street Journal (WSJ)articles manually annotated for discourse infor-mation according to three discourse frameworks.The Rhetorical Structure Theory (RST) and GraphBank (GB) corpora are relatively small comparedto the Penn Discourse Treebank (PDTB) annota-tions that cover the 1 million word WSJ part of thePenn Treebank corpus (Marcus et al, 1994).
Ourevaluation requires gold standard summaries writ-ten by humans, so we perform our experiments ona subset of the overlapping documents for whichwe also have human summaries available.2.1 RST corpusRST (Mann and Thompson, 1988) proposes thatcoherent text can be represented as a tree formedby the combination of text units via discourse re-lations.
The RST corpus developed by Carlson etal.
(2001) contains discourse tree annotations for385 WSJ articles from the Penn Treebank corpus.The smallest annotation units in the RST corpusare sub-sentential clauses, also called elementarydiscourse units (EDUs).
Adjacent EDUs combinethrough rhetorical relations into larger spans suchas sentences.
The larger units recursively partici-pate in relations with others, yielding one hierar-chical tree structure covering the entire text.The discourse units participating in a RST re-lation are assigned either nucleus or satellite sta-tus; a nucleus is considered to be more central,or important, in the text than a satellite.
Rela-tions composed of one nucleus and one satelliteare called mononuclear relations.
On the otherhand, in multinuclear relations, two or more textunits participate, and all are considered equallyimportant.
The RST corpus is annotated with 53mononuclear and 25 multinuclear relations.
Rela-tions that convey similar meaning are grouped, re-sulting in 16 classes of relations: Cause, Comparison,Condition, Contrast, Attribution, Background, Elaboration,Enablement, Evaluation, Explanation, Joint, Manner-Means,Topic-Comment, Summary, Temporal and Topic-Change.2.2 Graph Bank corpusSometimes, texts cannot be described in a treestructure as hypothesized by the RST.
For exam-ple, crossing dependencies and nodes with multi-ple parents appear frequently in texts and do notallow a tree structure to be built (Lee et al, 2008).To address this problem, general graph representa-tion was proposed by Wolf and Gibson (2005) asa more realistic model of discourse structure.Graph annotations of discourse are available for135 documents (105 from AP Newswire and 30from the WSJ) as part of the Graph Bank cor-pus (Wolf and Gibson, 2005).
Clauses are the ba-sic discourse segments in this annotation.
Theseunits are represented as the nodes in a graph, andare linked with one another through 11 differ-ent rhetorical relations: Cause-effect, Condition, Vio-lated expectation, Elaboration, Example, Generalization, At-tribution, Temporal sequence, Similarity, Contrast and Same.The edge between two nodes representing a rela-tion is directed in the case of asymmetric relationssuch as Cause and Condition and undirected forsymmetric relations like Similarity and Contrast.2.3 Penn Discourse TreebankThe Penn Discourse Treebank (PDTB) (Prasad etal., 2008) is theory-neutral and does not makeany assumptions about the form of the overall dis-course structure of text.
Instead, this approach fo-cuses on local and lexically-triggered discourse re-lations.
Annotators identify explicit signals suchas discourse connectives: ?but?, ?because?, ?while?and mark the text spans which they relate.
Therelations between these spans are called explicitrelations.
In addition, adjacent sentences in a dis-course are also semantically related even in the ab-sence of explicit markers.
In the PDTB, these arecalled implicit relations and are annotated betweenadjacent sentences in the same paragraph.For both implicit and explicit relations, sensesare assigned from a hierarchy containing fourtop-level categories: Comparison (contrast, prag-matic contrast, concession, pragmatic concession), Contin-gency (cause, pragmatic cause, condition, pragmatic con-dition) , Expansion (conjunction, instantiation, restate-ment, alternative, exception, list) and Temporal (asyn-chronous, synchronous).
The top level senses are di-vided into types and subtypes that represent morefine grained senses?the second level senses arelisted in parentheses above.PDTB also provides annotations for the textspans of the two arguments (referred to Arg1 andArg2) involved in a relation.
In explicit relations,the argument syntactically bound to the discourseconnective is called Arg2.
The other argument is148referred to as Arg1.
For implicit relations, the ar-gument occurring first in the text is named Arg1,the one appearing later is called Arg2.2.4 Human summariesHuman summaries are available for some of theWSJ articles.
These summaries are extractive: hu-man judges identified and extracted important textunits from the source articles and used them assuch to compose the summary.The RST corpus contains summaries for 150documents.
Two annotators selected the most im-portant EDUs from these documents and createdsummaries that contain about square root of thenumber of EDUs in the source document.
Forconvenience, we adopt sentences as the commonunit for comparison across all frameworks.
So,we mapped the summary EDUs to the sentenceswhich contain them.
Two variable length sum-maries for each document were obtained in thisway.
In some documents, it was not possible toalign EDUs automatically with gold standard sen-tence boundaries given by the Penn Treebank andthese were not used in our work.
We performour experiments on the remaining 124 document-summary pairs.
These documents consisted of4,765 sentences in total, of which 1,152 were la-beled as important sentences because they con-tained EDUs selected by at least one annotator.The Graph Bank corpus also contains humansummaries.
However, only 15 are for documentsfor which RST and PDTB annotations are alsoavailable.
These summaries were created by fif-teen human annotators who ranked the sentencesin each document on a scale from 1 (low impor-tance) to 7 (very important for a summary).
Foreach document, we ordered the sentences accord-ing to the average rank from the annotators, andcreated a summary of 100 words using the topranked sentences.
The number of summary (im-portant) sentences is 67, out of a total of 308 sen-tences from the 15 documents.3 Features for content selectionIn this section, we describe two sets of discoursefeatures?structural and semantic.
The structurefeatures are derived from RST trees and do notinvolve specific relations.
Rather they computethe importance of a segment as a function of itsposition in the global structure of the entire text.On the other hand, semantic features indicate thesense of a relation between two sentences anddo not involve structure information.
We com-pute these from the PDTB annotations.
To un-derstand the benefits of discourse information, wealso study the performance of some non-discoursefeatures standardly used in summarization.3.1 Structural features: RST-basedPrior work in text summarization has developedcontent selection methods using properties of theRST tree: the nucleus-satellite distinction, notionsof salience and the level of an EDU in the tree.In early work, Ono et al (1994) suggesteda penalty score for every EDU based on theirnucleus-satellite status.
Since satellites of rela-tions are considered less important than the corre-sponding nuclei, spans that appear as satellites canbe assigned a lower score than the nucleus spans.This intuition is implemented by Ono et al (1994)as a penalty value for each EDU, defined as thenumber of satellite nodes found on the path fromthe root of the tree to that EDU.
Figure 1 showsthe RST tree (Carlson et al, 2002) for the follow-ing sentence which contains four EDUs.1.
[Mr. Watkins said] 2.
[volume on Interprovincial?s sys-tem is down about 2% since January] 3.
[and is expected tofall further,] 4.
[making expansion unnecessary until perhapsthe mid-1990s.
]The spans of individual EDUs are representedat the leaves of the tree.
At the root of the tree, thespan covers the entire text.
The path from EDU 1to the root contains one satellite node.
It is there-fore assigned a penalty of 1.
Paths to the root fromall other EDUs involve only nucleus nodes andsubsequently these EDUs do not incur any penalty.Figure 1: RST tree for the example sentence inSection 3.1.Marcu (1998) proposed another method to uti-lize the nucleus-satellite distinction, rewarding nu-cleus status instead of penalizing satellite.
He putforward the idea of a promotion set, consisting of149salient/important units of a text span.
The nu-cleus is the more salient unit in the full span ofa mononuclear relation.
In a multinuclear relation,all the nuclei are salient units of the larger span.For example, in Figure 1, EDUs 2 and 3 partici-pate in a multinuclear (List) relation.
As a result,both EDUs 2 and 3 appear in the promotion set oftheir combined span.
The salient units (promotionset) of each text span are shown above the horizon-tal line which represents the span.
At the leaves,salient units are the EDUs themselves.For the purpose of identifying important con-tent, units in the promotion sets of nodes close tothe root are hypothesized to be more importantthan those at lower levels.
The highest promo-tion of an EDU occurs at the node closest to theroot which contains that EDU in its promotion set.The depth of the tree from the highest promotionis assigned as the score for that EDU.
Hence, thecloser to the root an EDU is promoted, the betterits score.
Since EDUs 2, 3 and 4 are promoted allthe way up to the root of the tree, the score as-signed to them is equal to 4, the total depth of thetree.
EDU 1 receives a depth score of 3.However, notice that EDUs 2 and 3 are pro-moted to the root from a greater depth than EDU4 but all three receive the same depth score.
Butan EDU promoted successively over multiple lev-els should be more important than one which ispromoted fewer times.
In order to make this dis-tinction, a promotion score was also introduced byMarcu (1998) which is a measure of the numberof levels over which an EDU is promoted.
Now,EDUs 2 and 3 receive a promotion score of threewhile the score of EDU 4 is only two.For our experiments, we use the nucleus-satellite penalty, depth and promotion based scoresas features.
Because all these scores depend on thelength of the document, another set of the samefeatures normalized by number of words in thedocument are also included.
The penalty/score fora sentence is computed as the maximum of thepenalties/scores of its constituent EDUs.3.2 Semantic features: PDTB-basedThese features represent sentences purely in termsof the relations which they participate in.
For eachsentence, we use the PDTB annotations to encodethe sense of the relation expressed by the sentenceand the type of realization (explicit or implicit).For example, the sentence below expresses aContingency relation.In addition, its machines are easier to operate, so cus-tomers require less assistance from software.For such sentences that contain both the argu-ments of a relation ie., expresses the relation byitself, we set the feature ?expresses relation?.
Forthe above sentence, the binary feature ?expressesContingency relation?
would be true.Alternatively, sentences participating in multi-sentential relations will have one of the followingfeatures on: ?contains Arg1 of relation?
or ?con-tains Arg2 of relation?.
Therefore, for the follow-ing sentences in an Expansion relation, we recordthe feature ?contains Arg1 of Expansion relation?for sentence (1) and for sentence (2), ?containsArg2 of Expansion relation?.
(1) Wednesday?s dominant issue was Yasuda &Marine In-surance, which continued to surge on rumors of speculativebuying.
(2) It ended the day up 80 yen to 1880 yen.We combine the implicit/explicit type distinc-tion of the relations with the other features de-scribed so far, doubling the number of features.We also added features that use the second levelsense of a relation.
So, the relevant features forsentence (1) above would be ?contains Arg1 ofImplicit Expansion relation?
as well as ?containsArg1 of Implicit Restatement relation?
(Restate-ment is a type of Expansion relation (Section 2.3)).In addition, we include features measuring thenumber of relations shared by a sentence (implicit,explicit and total) and the distance between argu-ments of explicit relations (the distance of Arg1when the sentence contains Arg2).3.3 Non-discourse featuresWe use standard non-discourse features used insummarization: length of the sentence, whetherthe sentence is paragraph initial or the first sen-tence of a document, and its offsets from docu-ment beginning as well as paragraph beginningand end (Edmundson, 1969).
We also include theaverage, sum and product probabilities of the con-tent words appearing in sentences (Nenkova et al,2006) and the number of topic signature words inthe sentence (Lin and Hovy, 2000).4 Predictive power of featuresWe used the human summaries from the RST cor-pus to study which features strongly correlate withthe important sentences selected by humans.
Forbinary features such as ?does the sentence con-150tain a Contingency relation?, a chi-square test wascomputed to measure the association between afeature and sentence class (in summary or not insummary).
For real-valued features, comparisonbetween important and unimportant/non-summarysentences was done using a two-sided t-test.
Thesignificant features from our different classes arereported in the Appendix?Tables 5, 6 and 7.
Abrief summary of the results is provided below.Significant features that have higher values forsentences selected in a summary are:Structural: depth score and promotion score?both normal-ized and unnormalized.Semantic-PDTB-level11: contains Arg1 of Explicit Expan-sion, contains Arg1 of Implicit Contingency, contains Arg1of Implicit Expansion, distance of other argumentNon-discourse: length, is the first sentence in the article, isthe first sentence in the paragraph, offset from paragraph end,number of topic signature terms present, average probabilityof content words, sum of probabilities of content wordsSignificant features that have higher values forsentences not selected in a summary are:Structural: Ono penalty?normalized and unnormalized.Semantic-PDTB-level1: expresses Explicit Expansion, ex-presses Explicit Contingency, contains Arg2 of Implicit Tem-poral relation, contains Arg2 of Implicit Contingency, con-tains Arg2 of Implicit Expansion, contains Arg2 of ImplicitComparison, number of shared implicit relations, total sharedrelationsNon-discourse: offset from paragraph beginning, offsetfrom article beginning, sentence probability based on contentwords.All the structural features prove to be strong in-dicators for content selection.
RST depth and pro-motion scores are higher for important sentences.Unimportant sentences have high penalties.On the other hand, note that most of the sig-nificant sense features are descriptive of the ma-jority class of sentences?those not important ornot selected to appear in the summary (refer Ta-ble 7).
For example, the second arguments ofall the first level implicit PDTB relations are notpreferred in human summaries.
Most of the sec-ond level sense features also serve as indicatorsfor what content should not be included in a sum-mary.
Such features can be used to derive con-straints on what content is not important, but thereare only few indicators associated with importantsentences.
Overall, out of the 25 first and second1Features based on the PDTB level 1 senses.
The signif-icant features based on the level 2 senses are reported in theappendix.level sense features which turned out to be signifi-cantly related to a sentence class, only 8 are thoseindicative of important content.Another compelling observation is that highlycognitively salient discourse relations such asContrast and Cause are not indicative of importantsentences.
Of the features that indicate the occur-rence of a particular relation in a sentence, onlytwo are significant, but they are predictive of non-important sentences.
These are ?expresses Ex-plicit Expansion?
(also subtypes Conjunction andList) and ?expresses Explicit Contingency?.An additional noteworthy fact is the differencesbetween implicit and explicit relations that holdacross sentences.
For implicit relations, the testsshow a strong indication that the second argumentsof Implicit Contingency or Expansion would notbe included in a summary, their first argumentshowever are often important and likely to appearin a summary.
At the same time, for explicit rela-tions, there is no regularity for any of the relationsof which of the two arguments is more important.All the non-discourse features turned out highlysignificant (Table 6).
Longer sentences, those inthe beginning of an article or its paragraphs andsentences containing frequent content words arepreferred in human summaries.5 Classification performanceWe now test the strengths and complementary be-havior of these features in a classification task topredict important sentences from input texts.5.1 Comparison of feature classesTable 1 gives the overall accuracy, as well as pre-cision and recall for the important/summary sen-tences.
Features classes were combined using lo-gistic regression.
The reported results are from 10-fold cross-validation runs on sentences from the124 WSJ articles for which human summaries areavailable in the RST corpus.
For the classifier us-ing sense information from the PDTB, all the fea-tures described in Section 3.2 were used.The best class of features turn out to be thestructure-based ones.
They outperform both non-discourse (ND) and sense features by a large mar-gin.
F-measure for the RST-based classifier is33.50%.
The semantic type of relations, on theother hand, gives no indication of content impor-tance obtaining an F-score of only 9%.
Non-discourse features provide an F-score of 19%,151which is much better than the semantic class butstill less than structural discourse features.The structure and semantic features are com-plementary to each other.
The performance ofthe classifier is substantially improved when bothtypes of features are used (line 6 in Table 1).
TheF-score for the combined classifier is 40%, whichamounts to 7% absolute improvement over thestructure-only classifier.Discourse information is also complementaryto non-discourse.
Adding discourse structureor sense features to non-discourse (ND) featuresleads to better classification decisions (lines 4, 5in Table 1).
Particularly notable is the improve-ment when sense and non-discourse features arecombined?over 10% better F-score than the classi-fier using only non-discourse features.
The overallbest classifier is the combination of discourse?structure as well as sense?and non-discourse fea-tures.
Here, recall for important sentences is 34%and the precision of predictions is 62%.We also evaluated the features using ROUGE(Lin and Hovy, 2003; Lin, 2004).
ROUGE com-putes ngram overlaps between human referencesummaries and a given system summary.
Thismeasure allows us to compare the human sum-maries and classifier predictions at word levelrather than using full sentence matches.To perform ROUGE evaluation, summaries forour different classes of features were obtained asfollows.
Important sentences for each documentwere predicted using a logistic regression classi-fier trained on all other documents.
When thenumber of sentences predicted to be importantwas not sufficient to meet the required summarylength, sentences predicted with lowest confidenceto be non-important were selected.
All summarieswere truncated to 100 words.
Stemming was used,and stop words were excluded from the calcula-tion.
Both human extracts were used as references.The results from this evaluation are shown inTable 2.
They closely mirror the results obtainedusing precision and recall.
The sense features per-form worse than the structural and non-discoursefeatures.
The best set of features is the one com-bining structure, sense and non-discourse features,with ROUGE-1 score (unigram overlap) of 0.479.Overall, combining types of features considerablyimproves results in all cases.
However, unlikein the precision and recall evaluation, structuraland non-discourse features perform very similarly.Features used Acc P R Fstructural 78.11 63.38 22.77 33.50semantic 75.53 44.31 5.04 9.05non-discourse (ND) 77.25 67.48 11.02 18.95ND + semantic 77.38 59.38 20.62 30.61ND + structural 78.51 63.49 26.05 36.94semantic + structural 77.94 58.39 30.47 40.04structural + semantic + ND 78.93 61.85 34.42 44.23Table 1: Accuracy (Acc) and Precision (P), Recall(R) and F-score (F) of important sentences.Features ROUGE Features ROUGEstructural + semantic + ND 0.479 ND 0.432structural + ND 0.468 LEAD 0.411structural + semantic 0.453 semantic 0.369semantic + ND 0.444 TS 0.338structural 0.433Table 2: ROUGE-1 recall scoresTheir ROUGE-1 recall scores are 0.433 and 0.432respectively.
The top ranked sentences by bothsets of features appear to contain similar content.We also evaluated sentences chosen by twobaseline summarizers.
The first, LEAD, includessentences from the beginning of the article up tothe word limit.
This simple method is a very com-petitive baseline for single document summariza-tion.
The second baseline ranks sentences basedon the proportion of topic signature (TS) wordscontained in the sentences (Conroy et al, 2006).This approach leads to very good results in identi-fying important content for multi-document sum-maries where there is more redundancy, but it isthe worst when measured by ROUGE-1 on thissingle document task.
Structure and non-discoursefeatures outperform both these baselines.5.2 Tree vs. graph discourse structureWolf and Gibson (2004) showed that the GraphBank annotations of texts can be used for sum-marization with results superior to that based onRST trees.
In order to derive the importance ofsentences from the graph representation, they usethe PageRank algorithm (Page et al, 1998).
Thesescores, similar to RST features, are based only onthe link structure; the semantic type of the relationlinking the sentences is not used.
In Table 3, wereport the performance of structural features fromRST and Graph Bank on the 15 documents withoverlapping annotations from the two frameworks.As discussed by Wolf and Gibson (2004), wefind that the Graph Bank discourse representation(GB) leads to better sentence choices than usingRST trees.
The F-score is 48% for the GB clas-152Features Acc P R F ROUGERST-struct.
81.61 63.00 31.56 42.05 0.569GB-struct.
82.58 62.50 39.16 48.15 0.508Table 3: Tree vs graph-based discourse featuressifier and 42% for the RST classifier.
The betterperformance of GB method comes from higher re-call scores compared to RST.
Their precision val-ues are comparable.
But, in terms of ngram-basedROUGE scores, the results from RST (0.569)turn out slightly better than GB (0.508).
Over-all, discourse features based on structure turn outas strong indicators of sentence importance andwe find both tree and graph representations to beequally useful for this purpose.6 Lexical approximation to discoursestructureIn prior work on summarization, graph models oftext have been proposed that do not rely on dis-course.
Rather, lexical similarity between sen-tences is used to induce graph structure (Erkanand Radev, 2004; Mihalcea and Tarau, 2005).PageRank-based computation of sentence impor-tance have been used on these models with goodresults.
Now, we would like to see if the discoursegraphs from the Graph Bank (GB) corpus wouldbe more helpful for determining content impor-tance than the general text graph based on lexi-cal similarity (LEX).
We perform this comparisonon the 15 documents that we used in the previoussection for evaluating tree versus graph structures.We used cosine similarity to link sentences in thelexical graph.
Links with similarity less than 0.1were removed to filter out weak relationships.The classification results are shown in Table 4.The similarity graph representation is even morehelpful than RST or GB: the F-score is 53% com-pared to 42% for RST and 48% for GB.
The mostsignificant improvement from the lexical graph isin terms of precision 75% which is more than 10%higher compared to RST and GB features.
UsingROUGE as the evaluation metric, the lexical sim-ilarity graph, LEX (0.557), gives comparable per-formance with both GB (0.508) and RST (0.569)representations (refer Table 3).
Therefore, for usein content selection, lexical overlap informationappears to be a good proxy for building text struc-ture in place of discourse relations.Features Acc P R F ROUGELEX-struct.
83.23 75.17 41.14 53.18 0.557Table 4: Performance of lexrank summarizer7 DiscussionWe have analyzed the contribution of differenttypes of discourse features?structural and seman-tic.
Our results provide strong evidence that dis-course structure is the most useful aspect.
Bothtree and graph representations of discourse can beused to compute the importance of text units withvery good results.
On the other hand, sense in-formation from discourse does not provide strongindicators of good content but some constraintsas to which content should not be included ina summary.
These sense features complementstructure information leading to improved perfor-mance.
Further, both these types of discourse fea-tures are complementary to standardly used non-discourse features for content selection.However, building automatic parsers for dis-course information has proven to be a hard taskoverall (Marcu, 2000; Soricut and Marcu, 2003;Wellner et al, 2006; Sporleder and Lascarides,2008; Pitler et al, 2009) and the state of cur-rent parsers might limit the benefits obtainablefrom discourse.
Moreover, discourse-based struc-ture is only as useful for content selection as sim-pler text structure built using lexical similarity.Even with gold standard annotations, the perfor-mance of structural features based on the RSTand Graph Bank representations is not better thanthat obtained from automatically computed lexicalgraphs.
So, even if robust discourse parsers existto use these features on other test sets, it is notlikely that discourse features would provide betterperformance than lexical similarity.
Therefore, forcontent selection in summarization, current sys-tems can make use of simple lexical structures toobtain similar performance as discourse features.But it should be remembered that summaryquality does not depend on content selection per-formance alone.
Systems should also produce lin-guistically well formed summaries and currentlysystems perform poorly on this aspect.
To addressthis problem, discourse information is vital.
Themost comprehensive study of text quality of au-tomatically produced summaries was performedby Otterbacher et al (2002).
A collection of 15automatically produced summaries was manuallyedited in order to correct any problems.
The study153found that discourse and temporal ordering prob-lems account for 34% and 22% respectively of allthe required revisions.
Therefore, we suspect thatfor building summarization systems, most benefitsfrom discourse can be obtained with regard to textquality compared to the task of content selection.We plan to focus on this aspect of discourse usefor our future work.ReferencesL.
Carlson, D. Marcu, and M. E. Okurowski.
2001.Building a discourse-tagged corpus in the frame-work of rhetorical structure theory.
In Proceedingsof SIGdial, pages 1?10.L.
Carlson, D. Marcu, andM.
E. Okurowski.
2002.
Rstdiscourse treebank.
Corpus number LDC 2002T07,Linguistic Data Consortium, Philadelphia.J.
Conroy, J. Schlesinger, and D. O?Leary.
2006.Topic-focused multi-document summarization usingan approximate oracle score.
In Proceedings ofACL.H.P.
Edmundson.
1969.
New methods in automaticextracting.
Journal of the ACM, 16(2):264?285.G.
Erkan and D. Radev.
2004.
Lexrank: Graph-basedcentrality as salience in text summarization.
Journalof Artificial Intelligence Research (JAIR).A.
Lee, R. Prasad, A. Joshi, and B. Webber.
2008.
De-partures from Tree Structures in Discourse: SharedArguments in the Penn Discourse Treebank.
In Pro-ceedings of the Constraints in Discourse Workshop.C.
Lin and E. Hovy.
2000.
The automated acquisitionof topic signatures for text summarization.
In Pro-ceedings of COLING, pages 495?501.C.
Lin and E. Hovy.
2002.
Manual and automaticevaluation of summaries.
In Proceedings of the ACLWorkshop on Automatic Summarization.C.
Lin and E. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
InProceedings of HLT-NAACL.C.
Lin.
2004.
ROUGE: a package for automatic eval-uation of summaries.
In Proceedings of ACL TextSummarization Workshop.W.C.
Mann and S.A. Thompson.
1988.
Rhetoricalstructure theory: Towards a functional theory of textorganization.
Text, 8.D.
Marcu.
1998.
To build text summaries of high qual-ity, nuclearity is not sufficient.
In Working Notesof the the AAAI-98 Spring Symposium on IntelligentText Summarization, pages 1?8.D.
Marcu.
2000.
The rhetorical parsing of unrestrictedtexts: A surface-based approach.
ComputationalLinguistics, 26(3):395?448.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1994.
Building a large annotated corpus of en-glish: The penn treebank.
Computational Linguis-tics, 19(2):313?330.R.
Mihalcea and P. Tarau.
2005.
An algorithm forlanguage independent single and multiple documentsummarization.
In Proceedings of IJCNLP.A.
Nenkova, L. Vanderwende, and K. McKeown.2006.
A compositional context sensitive multi-document summarizer: exploring the factors that in-fluence summarization.
In Proceedings of SIGIR.K.
Ono, K. Sumita, and S. Miike.
1994.
Abstract gen-eration based on rhetorical structure extraction.
InProceedings of COLING, pages 344?348.J.C.
Otterbacher, D.R.
Radev, and A. Luo.
2002.
Revi-sions that improve cohesion in multi-document sum-maries: a preliminary study.
In Proceedings of ACLText Summarization Workshop, pages 27?36.L.
Page, S. Brin, R. Motwani, and T. Winograd.
1998.The pagerank citation ranking: Bringing order tothe web.
Technical report, Stanford Digital LibraryTechnologies Project.E.
Pitler, A. Louis, and A. Nenkova.
2009.
Automaticsense prediction for implicit discourse relations intext.
In Proceedings of ACL-IJCNLP, pages 683?691.R.
Prasad, N. Dinesh, A. Lee, E. Miltsakaki,L.
Robaldo, A. Joshi, and B. Webber.
2008.
Thepenn discourse treebank 2.0.
In Proceedings ofLREC.R.
Soricut and D. Marcu.
2003.
Sentence level dis-course parsing using syntactic and lexical informa-tion.
In Proceedings of HLT-NAACL.C.
Sporleder and A. Lascarides.
2008.
Using automat-ically labelled examples to classify rhetorical rela-tions: An assessment.
Natural Language Engineer-ing, 14:369?416.V.R.
Uzda, T.A.S.
Pardo, and M.G.
Nunes.
2008.Evaluation of automatic text summarization meth-ods based on rhetorical structure theory.
IntelligentSystems Design and Applications, 2:389?394.B.
Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,and R.
Saur??.
2006.
Classification of discourse co-herence relations: An exploratory study using mul-tiple knowledge sources.
In Proceedings of SIGdial,pages 117?125.F.
Wolf and E. Gibson.
2004.
Paragraph-, word-, andcoherence-based approaches to sentence ranking: Acomparison of algorithm and human performance.In Proceedings of ACL, pages 383?390.F.
Wolf and E. Gibson.
2005.
Representing discoursecoherence: A corpus-based study.
ComputationalLinguistics, 31(2):249?288.154Appendix: Feature analysisThis appendix provides the results from statistical tests for identifying predictive features from the dif-ferent classes (RST-based structural features?Table 5, Non-discourse features?Table 6 and PDTB-basedsense features?Table 7).For real-valued features, we performed a two sided t-test between the corresponding feature valuesfor important versus non-important sentences.
For features which turned out significant in each set, thevalue of the test statistic and significance levels are reported in the tables.For binary features, we report results from a chi-square test to measure how indicative a feature isfor the class of important or non-important sentences.
For results from the chi-square test, a (+/-) signis enclosed within parentheses for each significant feature to indicate whether the observed number oftimes the feature was true in important sentences is greater (+) than the expected value (indication thatthis feature is frequently associated with important sentences).
When the observed frequency is less thanthe expected value, a (-) sign is appended.RST Features t-stat p-valueOno penalty -21.31 2.2e-16Depth score 16.75 2.2e-16Promotion score 16.00 2.2e-16Normalized penalty -11.24 2.2e-16Normalized depth score 17.24 2.2e-16Normalized promotion score 14.36 2.2e-16Table 5: Significant RST-based featuresNon-discourse features t-stat p-valueSentence length 3.14 0.0017Average probability of content words 9.32 2.2e-16Sum probability of content words 11.83 2.2e-16Product probability of content words -5.09 3.8e-07Number of topic signature terms 9.47 2.2e-16Offset from article beginning -12.54 2.2e-16Offset from paragraph beginning -28.81 2.2e-16Offset from paragraph end 7.26 5.8e-13?2 p-valueFirst sentence?
224.63 (+) 2.2e-16Paragraph initial?
655.82 (+) 2.2e-16Table 6: Significant non-discourse features155PDTB features t-stat p-valueNo.
of implicit relations involved -9.13 2.2e-16Total relations involved -6.95 4.9e-12Distance of Arg1 3.99 6.6e-05Based on level 1 senses?2 p-valueExpresses explicit Expansion 12.96 (-) 0.0003Expresses explicit Contingency 7.35 (-) 0.0067Arg1 explicit Expansion 12.87 (+) 0.0003Arg1 implicit Contingency 13.84 (+) 0.0002Arg1 implicit Expansion 29.10 (+) 6.8e-08Arg2 implicit Temporal 4.58 (-) 0.0323Arg2 implicit Contingency 60.28 (-) 8.2e-15Arg2 implicit Expansion 134.60 (-) 2.2e-16Arg2 implicit Comparison 27.59 (-) 1.5e-07Based on level 2 senses?2 p-valueExpresses explicit Conjunction 8.60 (-) 0.0034Expresses explicit List 4.41 (-) 0.0358Arg1 explicit Conjunction 10.35 (+) 0.0013Arg1 implicit Conjunction 5.26 (+) 0.0218Arg1 implicit Instantiation 18.94 (+) 1.4e-05Arg1 implicit Restatement 15.35 (+) 8.9-05Arg1 implicit Cause 12.78 (+) 0.0004Arg1 implicit List 5.89 (-) 0.0153Arg2 explicit Asynchronous 4.23 (-) 0.0398Arg2 explicit Instantiation 10.92 (-) 0.0009Arg2 implicit Conjunction 51.57 (-) 6.9e-13Arg2 implicit Instantiation 12.08 (-) 0.0005Arg2 implicit Restatement 28.24 (-) 1.1e-07Arg2 implicit Cause 58.62 (-) 1.9e-14Arg2 implicit Contrast 30.08 (-) 4.2e-08Arg2 implicit List 12.31 (-) 1.9e-14Table 7: Significant PDTB-based features156
