Development  and Evaluationof a Broad-Coverage Probabi l ist ic Grammar ofEngl ish-Language Computer  ManualsEzra Black John Lafferty Salim Roukos<black I j laf f  \] roukos>*watson, ibm.
tomIBM Thomas J. Watson Research Center, P.O.
Box 704, Yorktown Heights, New York 10598ABSTRACTWe present an approach to grammar development wherethe task is decomposed into two separate subtasks.
The firsttask is hnguistic, with the goal of producing aset of rules thathave a large coverage (in the sense that the correct parse isamong the proposed parses) on a bhnd test set of sentences.The second task is statistical, with the goal of developing amodel of the grammar which assigns maximum probabilityfor the correct parse.
We give parsing results on text fromcomputer manuals.1.
Introduct ionMany language understanding systems and machinetranslation systems rely on a parser of English as the firststep in processing an input sentence.
The general impres-sion may be that parsers with broad coverage of Englishare readily available.
In an effort to gauge the state of theart in parsing, the authors conducted an experiment inSummer 1990 in which 35 sentences, all of length 13 wordsor less, were selected randomly from a several-million-word corpus of Associated Press news wire.
The sentenceswere parsed by four of the major large-coverage parsersfor general English.
1 Each of the authors, working sep-arately, scored 140 parses for correctness of constituentboundaries, constituent labels, and part-of-speech labels.All that was required of parses was accuracy in delim-iting and identifying obvious constituents such as nounphrases, prepositional phrases, and clauses, along with atleast rough correctness in assigning part-of-speech labels,e.g.
a noun could not be labelled as a verb.
The tallies ofeach evaluator were compared, and were identical or veryclose in all cases.
The best-performing parser was correctfor 60% of the sentences and the the remaining parserswere below 40%.
More recently, in early 1992, the cre-ator of another well-known system performed self-scoringon a similar task and reported 30% of input sentences ashaving been correctly parsed.
On the basis of the pre-ceeding evidence it seems that the current state of thet At least one of the par t ies  involved insisted that no per for -mance  results be made public.
Such reticence is widespread andunderstandable.
However, it is nonetheless important that perfor-mance norms be established for the field.
Some progress has beenmade in this direction \[3, 4\].art is far from being able to produce a robust parser ofgeneral English.In order to break through this bottleneck and beginmaking steady and quantifiable progress toward the goalof developing a highly accurate parser for general En-glish, organization of the grammar-development processalong scientific lines and the introduction of stochasticmodelling techniques are necessary, in our view.
We haveinitiated a research program on these principles, whichwe describe in what follows.
An account of our overallmethod of attacking the problem is presented in Section2.
The grammar involved is discussed in Section 3.
Sec-tion 4 is concerned with the statistical modelling methodswe employ.
Finally, in Section 5, we present our experi-mental results to date.2.
ApproachOur approach to grammar development consists of thefollowing 4 elements:?
Selection of application domain.?
Development of a manually-bracketed corpus (tree-bank) of the domain.?
Creation of a grammar with a large coverage of ablind test set of treebanked text.Statistical modeling with the goal that the cor-rect parse be assigned maximum probability by thestochastic grammar.We now discuss each of these elements in more detail.App l i ca t ion  domain :  It would be a good first steptoward our goal of covering general English to demon-strate that we can develop a parser that has a high pars-ing accuracy for sentences in, say, any book listed inBooks In Print concerning needlework; or in any whole-sale footwear catalog; or in any physics journal.
The se-lected domain of focus should allow the acquisition ofa naturally-occuring large corpus (at least a few millionwords) to allow for realistic evaluation of performance and185Fa Adverbial PhraseFc Comparative PhraseFn Nominal ClauseFr Relative ClauseG Possessive PhraseJ Adjectival PhraseN Noun PhraseNn Nominal ProxyNr Temporal Noun PhraseNv Adverbial Noun PhraseP Prepositional PhraseS Full SentenceSi Sentential InterrupterTg Present Participial ClauseTi Infinitival ClauseTn Past Participial ClauseV Verb PhraseNULL OtherTable 1: Lancaster constituent labelsadequate amounts of data to characterize the domain sothat new test data does not surprise system developerswith a new set of phenomena hitherto unaccounted for inthe grammar.We selected the domain of computer manuals.
Be-sides the possible practical advantages to being able toassign valid parses to the sentences in computer manu-als, reasons for focusing on this domain include the verybroad but not unrestricted range of sentence types andthe availability of large corpora of computer manuals.
Weamassed a corpus of 40 million words, consisting of severalhundred computer manuals.
Our approach in attackingthe goal of developing a grammar for computer manualsis one of successive approximation.
As a first approxima-tion to the goal, we restrict ourselves to sentences of wordlength 7 - 17, drawn from a vocabulary consisting of the3000 most frequent words (i.e.
fully inflected forms, notlcmmas) in a 600,000-word subsection of our corpus.
Ap-proximately 80% of the words in the 40-million-word cor-pus are included in the 3000-word vocabulary.
We haveavailable to us about 2 million words of sentences com-pletely covered by the 3000-word vocabulary.
A lexiconfor this 3000-word vocabulary was completed in about 2months.T reebank:  A sizeable sample of this corpus is hand-parsed ("treebanked").
By definition, the hand parse("treebank parse") for any given sentence is consideredAT1CSTCSWJJNN1PPH1PPYRRVBDZVVCVVGSingular Article (a, every)that as Conjunctionwhether as ConjunctionGeneral Adjective (free, subsequent)Singular Common Noun (character, site)the Pronoun "it"the Pronoun "you"General Adverb (exactly, manually)"was"Imperative form of Verb (attempt, proceed)-ing form of Verb (containing, powering)Table 2: Sample of Lancaster part-of-speech labelsits "correct parse" and is used to judge the grammar'sparse.
To fulfill this role, treebank parses are constructedas "skeleton parses," i.e.
so that all obvious decisionsare made as to part-of-speech labels, constituent bound-aries and constituent labels, but no decisions are madewhich are problematic, ontroversial, or of which the tree-bankers are unsure.
Hence the term "skeleton parse":clearly not all constituents will always figure in a tree-bank parse, but the essential ones always will.
In practice,these are quite detailed parses in most cases.
The 18 con-stituent labels 2 used in the Lancaster treebank are listedand defined in Table 1.
A sampling of the approximately200 part-of-speech tags used is provided in Table 2.To date, roughly 420,000 words (about 35,000 sen-tences) of the computer manuals material have been tree-banked by a team at the University of Lancaster, Eng-land, under Professors Geoffrey Leech and Roger Gar-side.
Figure 1 shows two sample parses selected at ran-dom from the Lancaster Treebank.The treebank is divided into a training subcorpus anda test subcorpus.
The grammar developer is able to in-spect the training dataset at will, but can never see thetest dataset.
This latter restriction is, we feel, crucial formaking progress in grammar development.
The purposeof a grammar is to correctly analyze previously unseensentences.
It is only by setting it to this task that itstrue accuracy can be ascertained.
The value of a largebracketed training corpus is that it allows the grammar-ian to obtain quickly a very large 3 set of sentences that2Actually there are 18 x 3 = 54 labels, as each label L has vari-ants LA: for a first conjunct, and L-{- for second and later conjuncts,of type L: e.g.
\[N\[Ng~ the cause NSz\] and \[Nq- the appropr iate act ionN-k\]N\].3 We discovered that  the grammar 's  coverage (to be defined later)of the training set increased quickly to above 98% as soon as thegrammar ian identif ied the problem sentences.
So we have been186IN It_PPH1 N\]\[V indicates_VVZ\[Fn \[Fn&whether_CSW\[N a_AT1 call_NN1 N\]\[V completed_VVD successfully_RR V\]Fn&\]or_CC\[Fn+ if_CSWIN some_DD error_NN1 N\]@\[V was_VBDZ detected_VVN V\]@\[Fr that_CST\[V caused_VVD\[N the_AT call_NNl N\]\[Ti to_TO fail_VVI Wi\]V\]Fr\]Fn+\]Fn\]V\]._.\[Fa If_CS\[N you_PPY N\]IV were_VBDR using_VVG\[N a_AT1 shared_JJ folder_NN1 N\]V\]Fa\], - ,IV include_VVCIN the_AT following_JJ N\]V\]:_:Figure 1: Two sample bracketed sentences from Lan-caster Treebank.the grammar fails to parse.
We currently have about25,000 sentences for training.The point of the treebank parses is to constitute a"strong filter," that is to eliminate incorrect parses, onthe set of parses proposed by a grammar for a given sen-tence.
A candidate parse is considered to be "accept-able" or "correct" if it is consistent with the treebankparse.
We define two notions of consistency: structure-consistent and label-consistent.
The span of a consitituentis the string of words which it dominates, denoted by apair of indices (i, j)  where i is the index of the leftmostword and j is the index of the rightmost word.
We saythat a constituent A with span (i, j)  in a candidate parsefor a sentence is structure-consistent with the treebankparse for the same sentence in case there is no constituentin the treebank parse having span (i', j ')  satisfyingi' < i < j '  < jori < i' < j < j ' .In other words, there can be no "crossings" of the spanof A with the span of any treebank non-terminal.
Agrammar parse is structure-consistent wi h the treebankparse if all of its constituents are structure-consistent wi hthe treebank parse.continuously increasing the training set as more data is treebanked.The notion of label-consistent requires in addition tostructure-consistency that the grammar constituent nameis equivalent 4 to the treebank non-terminal label.The following example will serve to illustrate our con-sistency criteria.
We compare a "treebank parse":\[NT1 \[NT2 wl_pl w2_p2 NT2\] \[NT3 w3_p3 w4_p4w5_p5 NT3\]NT1\]with a set of "candidate parses":\[NT1 \[NT2 wl_pl w2_p2 NT2\] \[NT3 w3_p3 \[NT4w4_p4 w5_p5 NT4\]NT3\]NTI\]\[NT1 \[NT2 wl_p6 w2_p2 NT2\] \[NT5 w3_p9 w4_p4w5_p5 NT5\]NTI\]\[NTI wl_pl \[NT6 b_p2 w3_p15 NT6\]\[NT7 w4_p4w5_p5 NTT\]NTI\]For the structure-consistent criterion, the first and sec-ond candidate parses are correct, even though the firstone has a more detailed constituent spanning (4, 5).
Thethird is incorrect since the constituent NT6  is a case ofa crossing bracket.
For the label-consistent criterion, thefirst candidate parse is the only correct parse, because ithas all of the bracket labels and parts-of-speech of thetreebank parse.
The second candidate parse is incorrect,since two of its part-of-speech labels and one of its bracketlabels differ from those of the treebank parse.Grammar  writing and  statistical estimation:The task of developing the requisite system is factoredinto two parts: a linguistic task and a statistical task.The linguistic task is to achieve perfect or near-perfect coverage of the test set.
By this we meanthat among the n parses provided by the parser foreach sentence of the test dataset, there must be atleast one which is consistent with the treebank ill-ter.
s To eliminate trivial solutions to this task, thegrammarian must hold constant over the course ofdevelopment the geometric mean of the number ofparses per word, or equivalently the total number ofparses for the entire test corpus.The statistical task is to supply a stochastic modelfor probabilistically training the grammar  such thatthe parse selected as the most likely one is a correctparse.
64See Section 4 for the definition of a many-to-many mapping  be-tween grammar  and trcebank non-terminals for determining equiv-Mence of non-termlnals.SWe propose this sense of the term coverage as a replacement forthe sense in current use, viz.
simply supplying one or more parses,correct or not, for some portion of a given set of sentences.6Clcarly the grammar ian can contribute to this task by, amongother things, not just holding the average number  of parses con-"I 87The above decomposition i to two tasks should lead tobetter broad-coverage rammars.
In the first task, thegrammarian can increase coverage since he can examineexamples of specific uncovered sentences.
In the secondtask, that of selecting a parse from the many parses pro-posed by a grammar, can best be done by maximum like-lihood estimation constrained by a large treebank.
Theuse of a large treebank allows the development ofsophisti-cated statistical models that should outperform the tra-ditional approach of using human intuition to developparse preference strategies.
We describe in this paper amodel based on probabilistic ontext-free grammars es-t imated with a constrained version of the Inside-Outsidealgorithm (see Section 4)that can be used for picking aparse for a sentence.
In \[2\], we desrcibe a more sophisti-cated stochastic grammar that achieves even higher pars-ing accuracy.3.
GrammarOur grammar is a feature-based context-free phrasestructure grammar employing traditional syntactic ate-gories.
Each of its roughly 700 "rules" is actually a ruletemplate, compressing a family of related productions viaunification.
7 Boolean conditions on values of variablesoccurring within these rule templates erve to limit theirambit where necessary.
To illustrate, the rule templatebelow sf2 : V1 ~ f2 : V1 f2 : V1f3 : V2 f3 : V3 f3 : V2where(V2 = dig \[h) & (V3 # ~)imposes agreement of the children with reference to fea-ture f2, and percolates this value to the parent.
Accept-able values for feature f3 are restricted to three (d,g,h) forthe second child (and the parent), and include all possi-ble values for feature f3 ezeept k, for the first child.
Notethat the variable value is also allowed in all cases men-tioned (V1,V2,V3).
If the set of licit values for feature f3is (d,e,f,g,h,i,j,k,1}, and that for feature f2 is {r,s}, then,allowing for the possibility of variables remaining as such,the rule template above represents 3*4*9 = 108 differentrules.
If the condition were removed, the rule templatewould stand for 3"10"10 = 300 different rules.stunt, but  in fact steadily reducing it.
The importance of thiscontr ibut ion will u lt imately depend on the power of the statisti-cal models developed after a reasonable amount  of effort.Unification is to be understood in this paper in a very l imitedsense, which is precisely stated in Section 4.
Our grammar  is nota unif ication grammar  in the sense which is most often used in thel iterature.awhere fl,f2,f3 are features; a,b,c are feature values; andV1,V2,V3 are variables over feature valuesWhile a non-terminal in the above grammar is a fea-ture vector, we group multiple non-terminals into oneclass which we call a mnemonic,  and which is representedby the least-specified non-terminal of the class.
A samplemnemonic is N2PLACE (Noun Phrase of semantic ate-gory Place).
This mnemonic omprises all non-terminalsthat unify with:I pos :n  \]barnum : twodetai ls : placeincluding, for instance, Noun Phrases of Place with nodeterminer, Noun Phrases of Place with various sortsof determiner, and coordinate Noun Phrases of Place.Mnemonics are the "working nonterminals" of the gram-mar; our parse trees are labelled in terms of them.
Aproduction specified in terms of mnemonics (a mnemonicproduction) is actually a family of productions, in just thesame way that a mnemonic is a family of non-terminals.Mnemonics and mnemonic productions play key roles inthe stochastic modelling of the grammar (see below).
Arecent version of the grammar has some 13,000 mnemon-ics, of which about 4000 participated in full parses ona run of this grammar on 3800 sentences of averageword length 12.
On this run, 440 of the 700 rule tem-plates contributed to full parses, with the result that the4000 mnemonics utilized combined to form approximately60,000 different mnemonic productions.
The grammarhas 21 features whose range of values is 2 - 99, with amedian of 8 and an average of 18.
Three of these featuresare listed below, with the function of each:det_posdegreenoun_pronounDeterminer SubtypeDegree of ComparisonNominal SubtypeTable 3: Sample Grammatical FeaturesTo handle the huge number of linguistic distinctionsrequired for real-world text input, the grammarian usesmany of the combinations of the feature set.
A samplerule (in simplified form) illustrates this:pos : jbarnum : onedetai ls : V1degree : V3pos : jbarnum : zerodetai ls : V1degree : V3This rule says that a lexical adjective parses up to an ad-jective phrase.
The logically primary use of the feature"details" is to more fully specify conjunctions and phrases188involving them.
Typical values, for coordinating conjunc-tions, are "or" and "but"; for subordinating conjunctionsand associated adverb phrases, they include e.g.
"that"and "so."
But for content words and phrases (more pre-cisely, for nominal, adjectival and adverbial words andphrases), the feature, being otherwise otiose, carries thesemantic ategory of the head.The mnemonic names incorporate "semantic" cate-gories of phrasal heads, in addition to various sorts ofsyntactic information (e.g.
syntactic data concerning theembedded clause, in the case of "that-clauses").
The "se-mantics" is a subclassification of content words that isdesigned specifically for the manuals domain.
To provideexamples of these categories, and also to show a case inwhich the semantics ucceeded in correctly biasing theprobabilities of the trained grammar, we contrast (simpli-fied) parses by an identical grammar, trained on the samedata (see below), with the one difference that semanticswas eliminated from the mnemonics of the grammar thatproduced the first parse below.\[SC\[V1 Enter \[N2\[N2 the name \[P1 of the systemP1\]N2\]\[SD you \[V1 want \[V2 to \[V1 connect \[P1 toP 1\]V1\]V2\]V1\]SD\]N2\]V1\]SC\].\[SCSEND-ABS-UNIT\[V1SEND-ABS-UNITEnter \[N2ABS-UNIT the name \[P1SYSTEMOF of\[N2SYSTEM the system \[SDORGANIZE-PERSONyou \[V1ORGANIZE want \[V2ORGANIZE to con-nect \[P1WO to P1\]V2\]V1\]SD\]N2\]P1\]N2\]V1\]SC\].What is interesting here is that the structural parse isdifferent in the two cases.
The first case, which doesnot match the treebank parse 9 parses the sentence in thesame way as one would understand the sentence, "En-ter the chapter of the manual you want to begin with.
"In the second case, the semantics were able to bias thestatistical model in favor of the correct parse, i.e.
onewhich does match the treebank parse.
As an experiment,the sentence was submitted to the second grammar witha variety of different verbs in place of the original verb"connect", to make sure that it is actually the semanitcclass of the verb in question, and not some other factor,that accounts for the improvement.
Whenever verbs weresubstituted that were licit syntatically but not semanti-cally (e.g.
adjust, comment, lead) the parse was as in thefirst case above.
Of course other verbs of the class "OR-GANIZE" were associated with the correct parse, andverbs that did were not even permitted syntactically oc-casioned the incorrect parse.We employ a lexical preprocessor to mark multiword9\[V Enter \[N the name \[P of \[N the system \[Fr\[N you \]\[V want\[Wl to connect \[P to \]\]\]\]\]\]\]\].189units as well as to license unusual part-of-speech assign-ments, or even force labellings, given a particular context.For example, in the context: "How to:", the word "How"can be labelled once and for all as a General Wh-Adverb,rather than a Wh-Adverb of Degree (as in, "How tallhe is getting!").
Three sample entries from our lexiconfollow: "Full-screen" is labelled as an adjective whichfull-screen JSCREEN-PTB*Hidden VALTERN*1983 NRSG* M-C-*Table 4: Sample lexical entriesusually bears an attributive function, with the semanticclass "Screen-Part".
"Hidden" is categorized as a pastparticiple of semantic class "Alter".
"1983" can be atemporal noun (viz.
a year) or else a number.
Notethat all of these classifications were made on the basis ofthe examination of concordances over a several-hundred-thousand-word sample of manuals data.
Possible uses notencountered were in general not included in our lexicon.Our approach to grammar development, syntacticalas well as lexical, is frequency-based.
In the case of syn-tax, this means that, at any given time, we devote ourattention to the most frequently-occurring constructionwhich we fail to handle, and not the most "theoreticallyinteresting" such construction.4.
S ta t i s t i ca l  T ra in ing  and  Eva luat ionIn this section we will give a brief description of theprocedures that we have adopted for parsing and traininga probabilistic model for our grammar.
In parsing withthe above grammar,  it is necessary to have an efficientway of determining if, for example, a particular featurebundle A = (AI ,  A2 , .
.
.
,AN)  can be the parent of agiven production, some of whose features are expressedas variables.
As mentioned previously, we use the termunification to denote this matching procedure, and it isdefined precisely in figure 2.In practice, the unification operations are carried outvery efficiently by representing bundles of features as bit-strings, and realizing unification in terms of logical bitoperations in the programming language PL.8 which issimilar to C. We have developed our own tools to translatethe rule templates and conditions into PL.8 programs.A second operation that is required is to partit ionthe set of nonterminals, which is potentially extremelylarge, into a set of equivalence classes, or mnemonics, asmentioned earlier.
In fact, it is useful to have a tree,which hierarchically organizes the space of possible fea-UNIFY(A, B):do for each feature fi f  not  FEATURE_UNIFY(A/,  B / )then  re turn  FALSEre turn  TRUEFEATURE_UNIFY(a, b):i f  a -- b then  re turn  TRUEelse i f  a is variable or b is variablethen  re turn  TRUEre turn  FALSEFigure 2ture bundles into increasingly detailed levels of semanticand syntactic information.
Each node of the tree is it-self represented by a feature bundle, with the root beingthe feature bundle all of whose features are variable, andwith a decreasing number of variable features occuring asa branch is traced from root to leaf.
To find the mnemonic.A4(A) assigned to an arbitrary feature bundle A, we findthe node in the mnemonic tree which corresponds to thesmallest mnemonic that contains (subsumes) the featurebundle A as indicated in Fugure 3..A4(A):n = root_of_mnemonic_treere turn  SEARCH_SUBTREE(n, A)SEARCH_SUBTREE(n, A)do for each child m of ni f  Mnemonic(m) contains Athen  re turn  SEARCH_SUBTREE(m, A)re turn  Mnemonic(n)Figure 3Unconst ra ined  t ra in ing :  Since our grammar hasan extremely large number of non-terminals, we first de-scribe how we adapt the well-known Inside-Outside algo-rithm to estimate the parameters of a stochastic ontext-free grammar that approximates the above context-freegrammar.
We begin by describing the case, which wc callunconstrained training, of maximizing the likelihood of anunbrackctcd corpus.
We will later describe the modifica-tions necessary to train with the constraint of a bracketedcorpus.To describe the training procedure we have used, wewill assume familiarity with both the CKY algorithm\[?\] and the Inside-Outside algorithm \[?\], which we haveadapted to the problem of training our grammar.
Themain computations of the Inside-Outside algorithm areindexed using the CKY procedure which is a bottom-upchart parsing algorithm.
To summarize the main points190in our adaptation of these algorithms, let us assume thatthe grammar is in Chomsky normal form.
The generalcase involves only straight-forward modifications.
Pro-ceeding in a bottom-up fashion, then, we suppose thatwe have two nonterminals (bundles of features) B andC, and we find all nonterminals A for which A -~ B Cis a production in the grammar.
This is accomplishedby using the unfication operation and checking that therelevent Boolean conditions are satisfied for the nonter-minals A, B, and C.Having found such a nonterminal, the usual Inside-Outside algorithm requires a recursive update of theInside probabilities IA( i , j )  and outside probabilitiesOA(i , j) that A spans (i, j).
These updates involve theprobability parameterPrA(A  ---* B C).In the case of our feature-based grammar, however, thenumber of such parameters would be extremely large(the grammar  can have on the order of few billion non-terminals).
We thus organize productions into the equiv-alence classes induced by the mncmomic  classes on thenon-terminals.
The update then uses mnemonic produc-tions for the stochastic grammar using the parameterPrM(A)(J~4(B) --) A4(C) A4(C)).Of course, for lexical productions A --) w we use thecorresponding probabilityPr~(A)(jVI(A ) -~ w)in the event that we are rewriting not a pair of nontermi-nals, but a word w.Thus, probabilities are expressed in terms of the setof mnemonics (that is, by the nodes in the mnemonictree), rather that in terms of the actual nonterminals ofthe grammar.
It is in this manner that we can obtainefficient and reliable estimates of our parameters.
Sincethe grammar  is very detailed, the mnemonic map JUt canbe increasingly refined so that a greater number of lin-guistic phenomena are caputured in the probabilities.
Inprinciple, this could be carried out automatically to de-termine the opt imum level of detail to be incorporatedinto the model, and different paramcterizations could besmoothed together.
To date, however, we have only con-tructed mnemonic  maps by hand, and have thus experi-mented with only a small number of paramcterizations.Constra ined training: The Inside-Outside algo-rithm is a special case of the general EM algorithm, andas such, succssive iteration is guaranteed to converge toa set of parameters which locally maximize the likelihoodof generating the training corpus.
We have found it use-ful to employ the trccbank to supervise the training ofthese parameters.
Intuitively, the idea is to modify thealgorithm to locally maximize the likelihood of generat-ing the training corpus using parses which are "similar"to the treebank parses.
This is accomplished by onlycollecting statistics over those parses which are consis-tent with the treebank parses, in a manner which we willnow describe.
The notion of label-consistent is definedby a (many-to-many) mapping from the mnemonics ofthe feature-based grammar to the nonterminal labels ofthe treebank grammar.
For example, our grammar main-tains a fairly large number of semantic lasses of singularnouns, and it is natural to stipulate that each of themis label-consistent with the nonterminal NI~I denoting ageneric singular noun in the treebank.
Of course, to ex-haustively specify such a mapping would be rather timeconsuming.
In practice, the mapping is implemented byorganizing the nonterminals hierarchically into a tree, andsearching for consistency in a recursive fashion.The simple modification of the CKY algorithm whichtakes into account he treebank parse is, then, the follow-ing.
Given a pair of nonterminals B and C in the CKYchart, if the span of the parent is not structure-consistentthen this occurence of B C cannot be used in the parseand we continue to the next pair.
If, on the other hand, itis structure-consistent then we find all candidate parentsA for which A ~ B C is a production of the grammar,but include only those that are label-consistent with thetreebank nonterminal (if any) in that position.
The prob-abilities are updated in exactly the same manner as forthe standard Inside-Outside algorithm.
The procedurethat we have described is called constrained training, andit significantly improves the effectiveness of the parser,providing a dramatic reduction in computational require-ments for parameter estimation as well as a modest im-provement in parsing accuracy.Sample mappings from the terminals and non-terminals of our grammar to those of the Lancaster tree-bank are provided in Table 5.
For ease of understanding,we use the version of our grammar in which the semanticsare eliminated from the mnemonics (see above).
Categorynames from our grammar are shown first, and the Lan-caster categories to which they map are shown second:The first case above is straightforward: ourprepositional-phrase category maps to Lancaster's.
Inthe second case, we break down the category RelativeClause more finely than Lancaster does, by specifyingthe syntax of the embedded clause (e.g.
FRV2: "thatopened the adapter").
The third case relates to rela-tive clauses lacking prefatory particles, such as: "the rowyou are specifying"; we would call "you are specifying"an SD (Declarative Sentence), while Lancaster calls it anFr (Relative Clause).
Our practice of distinguishing con-stituents which function as interrupters from the sameconstituents tout court accounts for the fourth case; thecategory in question is Infinitival Clause.
Finally, we gen-erate attributive adjectives (JB) directly from past par-ticiples (VVN) by rule, whereas Lancaster opts to labelas adjectives (J J) those past participles o functioning.5.
Experimental ResultsWe report results below for two test sets.
One (TestSet A) is drawn from the 600,000-word subsection of ourcorpus of computer manuals text which we referred toabove.
The other (Test Set B) is drawn from our full 40-million-word computer manuals corpus.
Due to a moreor less constant error rate of 2.5% in the treebank parsesthemselves, there is a corresponding built-in margin of er-ror in our scores.
For each of the two test sets, results arepresented first for the linguistic task: making sure that acorrect parse is present in the set of parses the grammarproposes for each sentence of the test set.
Second, resultsare presented for the statistical task, which is to ensurethat the parse which is selected as most likely, for eachsentence of the test set, is a correct parse.Number of Sentences 935Average Sentence Length 12Range of Sentence Lengths 7-17Correct Parse Present 96%Correct Parse Most Likely 73%Table 6: Results for Test Set AP1 PFRV2 FrSD FrIANYTI  TiJBVVN* :lJTable 5: Sample of grammatical  category mappingsNumber of Sentences 1105Average Sentence Length 12Range of Sentence Lengths 7-17Correct Parse Present 95%Correct Parse Most Likely 75%Table 7: Results for Test Set B191Recall (see above) that the geometric mean of thenumber of parses per word, or equivalently the total num-ber of parses for the entire test set, must be held con-stant over the course of the grammar's development, toeliminate trivial solutions to the coverage task.
In theroughly year-long period since we began work on the com-puter manuals task, this average has been held steady atroughly 1.35 parses per word.
What this works out to is arange of from 8 parses for a 7-word sentence, through 34parses for a 12-word sentence, to 144 parses for a 17-wordsentence.
In addition, during this development period,performance on the task of picking the most likely parsewent from 58% to 73% on Test Set A.
Periodic results onTest Set A for the task of providing at least one correctparse for each sentence are displayed in Table 8.We present additional experimental results to showthat our grammar is completely separable from its accom-panying "semantics".
Note that semantic ategories arenot "written into" the grammar; i.e., with a few minorexceptions, no rules refer to them.
They simply perco-late up from the lexical items to the non-terminal level,and contribute information to the mnemonic productionswhich constitute the parameters of the statistical trainingmodel.An example was given in Section 3 of a case in whichthe version of our grammar that includes semantics out-performed the version of the same grammar without se-mantics.
The effect of the semantic information in thatparticular case was apprently to bias the trained grammartowards choosing a correct parse as most likely.
However,we did not quantify this effect when we presented the ex-ample.
This is the purpose of the experimental resultsshown in Table 9.
Test B was used to test our currentgrammar, first with and then without semantic ategoriesin the mnemonics.It follows from the fact that the semantics are notwritten into the grammar that the coverage figure is thesame with and without semantics.
Perhaps surprising,however, is the slight degree of improvement due to thesemantics on the task of picking the most likely parse:only 2 percentage points.
The more detailed parametriza-January 1991 91%April 1991 92%August 1991 94%December 1991 96%April 1992 96%Table 8: Periodic Results for Test Set A: Sentences WithAt Least 1 Correct ParseNumber of Sentences 1105Average Sentence Length 12Range of Sentence Lengths 7-17Correct Parse Present (In Both Cases) 95%Correct Parse Most Likely (With Semantics) 75%Correct Parse Most Likely (No Semantics) 73%Table 9: Test Subcorpus B With and Without Semanticstion with semantic categories, which has about 13,000mnemonics achieved only a modest improvement in pars-ing accuracy over the parametrization without semantics,which has about 4,600 mnemonics.6.
Future  ResearchOur future research divides naturally into two efforts.Our linguistic research will be directed toward first pars-ing sentences of any length with the 3000-word vocabu-lary, and then expanding the 3000-word vocabulary to anunlimited vocabulary.
Our statistical research will focuson efforts to improve our probabilistic models along thelines of the new approach presented in \[2\].Re ferences1.
Baker, J., Trainable grammars for speech recognition.
InSpeech Communication papers presented at the 97-thMeeting of the Acostical Society of America, MIT, Can-bridge, MA, June 1979.2.
Black, E., Jelinek, F., Lafferty, J., Magerman, D., Mer-cer, R., and Itoukos, S., Towards History-based Gram-mars: Using Richer Models for Probabilistic Parsing.Proceedings of Fifth DARPA Speech and Natural Lan-guage Workshop, Harriman, NY, February 1992.3.
Black, E., Abney, S., Fhckenger, D., Gdaniec, C., Grish-man, R., Harrison, P., Hindle, D., Ingria, R., Jelinek, F.,Klavans, J., Liberman, M., Marcus, M., Roukos, S., San-torini, B., and Strzalkowsld, T.. A Procedure for Quan-titatively Comparing the Syntactic Coverage of EnglishGrammars.
Proceedings of Fourth DARPA Speech andNatural Language Workshop, pp.
306-311, 1991.4.
Harrison, P., Abney, S., Black, E., Fhckenger, D.,Gdaniee, C., Grishman, R., Hindle, D., Ingria, It., Mar-cus, M., Santorini, B., and Strzalkowski, T.. Evaluat-ing Syntax Performance of Parser/Grammars ofEnglish.Proceedings of Natural Language Processing SystemsEvaluation Workshop, Berkeley, California, 1991.5.
Hopcraft, J. E. and Ullman, Jeffrey D. Introduction toAutomata Theory, Languages, and Computation, Read-ing, MA: Addison-Wesley, 1979.6.
Jehnek, F., Lafferty, J. D., and Mercer, R. L. Basic Meth-ods of Probabilistic Context-Free Grammars.
Computa-tional Linguistics, to appear.192
