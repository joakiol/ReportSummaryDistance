Trigger-Pair Predictors in Parsing and TaggingEzra Black, Andrew Finch, Hideki KashiokaATR In terpret ing  Te lecommunicat ionsLaborator ies2-2 Hikar idai ,  Seika-choSoraku-gun,  Kyoto ,  Japan  619-02{black ,finch ,kashioka}?atr.
itl.
co. jpAbst rac tIn this article, we apply to natural anguageparsing and tagging the device of trigger-pair predictors, previously employed exclu-sively within the field of language mod-elling for speech recognition.
Given thetask of predicting the correct rule to as-sociate with a parse-tree node, or the cor-rect tag to associate with a word of text,and assuming a particular class of pars-ing or tagging model, we quantify the in-formation gain realized by taking accountof rule or tag trigger-pair predictors, i.e.pairs consisting of a "triggering" rule ortag which has already occurred in the docu-ment being processed, together with a spe-cific "triggered" rule or tag whose proba-bility of occurrence within the current sen-tence we wish to estimate.
This informa-tion gain is shown to be substantial.
Fur-ther, by utilizing trigger pairs taken fromthe same general sort of document as is be-ing processed (e.g.
same subject matter orsame discourse type)--as opposed to pre-dictors derived from a comprehensive g n-eral set of English texts--we can signifi-cantly increase this information gain.1 In t roduct ionI fa person or device wished to predict which wordsor grammatical constructions were about to occur insome document, intuitively one of the most helpfulthings to know would seem to be which words andconstructions occurred within the last half-dozen ordozen sentences of the document.
Other things be-ing equal, a text that has so far been larded with,say, mountaineering terms, is a good bet to continuefeaturing them.
An author with the habit of endingsentences with adverbial clauses of confirmation, e.g.
"as we all know", will probably keep up that habitas the discourse progresses.Within the field of language modelling for speechrecognition, maintaining a cache of words that haveoccurred so far within a document, and using thisinformation to alter probabilities of occurrence ofparticular choices for the word being predicted, hasproved a winning strategy (Kuhn et al, 1990).
Mod-els using trigger pairs of words, i.e.
pairs consist-ing of a "triggering" word which has already oc-curred in the document being processed, plus a spe-cific "triggered" word whose probability of occur-rence as the next word of the document needs tobe estimated, have yielded perplexity 1 reductionsof 29-38% over the baseline trigram model, for a5-million-word Wall Street Journal training corpus(Rosenfeld, 1996).This paper introduces the idea of using trigger-pair techniques to assist in the prediction of ruleand tag occurrences, within the context of natural-language parsing and tagging.
Given the task ofpredicting the correct rule to associate with a parse-tree node, or the correct ag to associate with a wordof text, and assuming a particular class of parsingor tagging model, we quantify the information gainrealized by taking account of rule or tag trigger-pairpredictors, i.e.
pairs consisting of a "triggering" ruleor tag which has already occurred in the documentbeing processed, plus a specific "triggered" rule ortag whose probability of occurrence within the cur-rent sentence we wish to estimate.In what follows, Section 2 provides a basicoverview of trigger-pair models.
Section 3 de-scribes the experiments we have performed, whichto a large extent parallel successful modelling ex-periments within the field of language modelling forspeech recognition.
In the first experiment, we inves-tigate the use of trigger pairs to predict both rulesand tags over our full corpus of around a millionwords.
The subsequent experiments investigate the\]See Section 2.131additional information gains accruing from trigger-pair modelling when we know what sort of documentis being parsed or tagged.
We present our exper-imental results in Section 4, and discuss them inSection 5.
In Section 6, we present some exampletrigger pairs; and we conclude, with a glance at pro-jected future research, in Section 7.2 BackgroundTrigger-pair modelling research as been pursuedwithin the field of language modelling for speechrecognition over the last decade (Beeferman et al,1997; Della Pietra et al, 1992; Kupiec, 1989; Lau,1994; Lau et al, 1993; Rosenfeld, 1996).Fundamentally, the idea is a simple one: if youhave recently seen a word in a document, then it ismore likely to occur again, or, more generally, theprior occurrence of a word in a document affects theprobability of occurrence of itself and other words.More formally, from an information-theoreticviewpoint, we can interpret he process as the rela-tionship between two dependent random variables.Let the outcome (from the alphabet of outcomesAy)  of a random variable Y be observed and usedto predict a random variable X (with alphabet .Ax).The probability distribution of X, in our case, is de-pendent on the outcome of Y.The average amount of information ecessary tospecify an outcome of X (measured in bits) is calledits entropy H(X)  and can also be viewed as a mea-sure of the average ambiguity of its outcome: 2H(X)  =- y~-P(z ) log~P(x)  (1)x6.AxThe mutual information between X and Y is ameasure of entropy (ambiguity) reduction of X fromthe observation of the outcome of Y.
This is theentropy of X minus its a posteriori entropy, havingobserved the outcome of Y.I (X ;Y )  = H(X) -  H(X IY  )= ~ ~\]  P(x,y) log~ P(x,y) (2)xe.~ x ye.4v P( x)P(y)The dependency information between a word andits history may be captured by the trigger pair.
3A trigger pair is an ordered pair of words t andw.
Knowledge that the trigger word t has occurredwithin some window of words in the history, changes2A more intuitive view of entropy is provided throughperplexity (Jelinek et al, 1977) which is a measure of thenumber of choices, on average, there are for a randomvariable.
It is defined to be: 2 H(x).3For a thorough description of trigger-based mod-elling, see (Rosenfeld, 1996).the probability estimate that word w will occur sub-sequently.Selection of these triggers can be performed bycalculating the average mutual information betweenword pairs over a training corpus.
In this case, thealphabet Ax = {w,~}, the presence or absence ofword w; similarly, Ay = {t,t}, the presence or ab-sence of the triggering word in the history.This is a measure of the effect that the knowl-edge of the occurrence of the triggering word t hason the occurence of word w, in terms of the entropy(and therefore perplexity) reduction it will provide.Clearly, in the absence of other context (i.e.
in thecase of the a priori distribition of X), this infor-mation will be additional.
However, once ~elatedcontextual information is included (for example bybuilding a trigram model, or, using other triggers forthe same word), this is no longer strictly true.Once the trigger pairs are chosen, they may beused to form constraint functions to be used ina maximum-entropy model, alongside other con-straints.
Models of this form are extremely versa-tile, allowing the combination of short- and long-range information.
To construct such a model, onetransforms the trigger pairs into constraint functionsf(t, w):1 i f t  E history andf (t ,  w) = next word = w (3)0 otherwiseThe expected values of these functions are thenused to constrain the model, usually in combinationof with other constraints uch as similar functionsembodying uni-, bi- and trigram probability esti-mates.
(Beeferman et al, 1997) models more accuratelythe effect of distance between triggering and trig-gered word, showing that for non-self-triggers, 4 thetriggering effect decays exponentially with distance.For self-triggers, 5 the effect is the same except hatthe triggering effect is lessened within a short rangeof the word.
Using a model of these distance ffects,they are able to improve the performance ofa triggermodel.We are unaware of any work on the use of triggerpairs in parsing or tagging.
In fact, we have notfound any previous research in which extrasententialdata of any sort are applied to the problem of parsingor tagging.3 The Experiments3.1 Experimental DesignIn order to investigate the utility of using long-range trigger information in tagging and parsing4i.e.
words which trigger words other than themselves5i.e.
words which trigger themselves132tasks, we adopt the simple mutual-information ap-proach used in (Rosenfeld, 1996).
We carry overinto the domain of tags and rules an experiment fromRosenfeld's paper the details of which we outline be-low.The idea is to measure the information con-tributed (in bits, or, equivalently in terms of per-plexity reduction) by using the triggers.
Using thistechnique requires pecial care to ensure that infor-mation "added" by the triggers is indeed additionalinformation.For this reason, in all our experiments we use theunigram model as our base model and we allow onlyone trigger for each tag (or rule) token.
6 We derivethese unigram probabilities from the training cor-pus and then calculate the total mutual informationgained by using the trigger pairs, again with respectto the training corpus.When using trigger pairs, one usually restricts thetrigger to occur within a certain window defined byits distance to the triggered token.
In our experi-ments, the window starts at the sentence prior tothat containing the token and extends back W (thewindow size) sentences.
The choice to use sentencesas the unit of distance is motivated by our intentionto incorporate triggers of this form into a probabilis-tie treebank-based parser and tagger, such as (Blacket al, 1998; Black et al, 1997; Brill, 1994; Collins,1996; Jelinek et al, 1994; Magerman, 1995; Ratna-parkhi, 1997).
All such parsers and taggers of whichwe are aware use only intrasentential information inpredicting parses or tags, and we wish to removethis information, as far as possible, from our results7 The window was not allowed to cross a docu-ment boundary.
The perplexity of the task beforetaking the trigger-pair information into account fortags was 224.0 and for rules was 57.0.The characteristics of the training corpus we em-ploy are given in Table 1.
The corpus, a subset sof the ATR/Lancaster General-English Treebank(Black et al, 1996), consists of a sequence of sen-tences which have been tagged and parsed by hu-man experts in terms of the ATR English Gram-mar; a broad-coverage rammar of English with ahigh level of analytic detail (Black et al, 1996; Blacket al, 1997).
For instance, the tagset is both seman-?By rule assignment, we mean the task of assigninga rule-name to a node in a parse tree, given that theconstituent boundaries have already been defined.7This is not completely possible, since correlations,even if slight, will exist between intra- and extrasenten-tial informationSspecifically, a roughly-900,000-word subset of thefull ATR/Lancaster General-English Treebank (about1.05 million words), from which all 150,000 words wereexcluded that were treebanked by the two least accurateATR/Lancaster treebankers (expected hand-parsing er-ror rate 32%, versus less than 10% overall for the threeremaining treebankers)1868 documents80299 sentences904431 words (tag instances)1622664 constituents (rule instances)1873 tags utilized907 rules utilized11.3 words per sentence, on averageTable 1: Characteristics of Training Set (Subset ofATR/Laneaster General-English Treebank)tic and syntactic, and includes around 2000 differenttags, which classify nouns, verbs, adjectives and ad-verbs via over 100 semantic ategories.
As examplesof the level of syntactic detail, exhaustive syntacticand semantic analysis is performed on all nominalcompounds; and the full range of attachment sitesis available within the Grammar for sentential andphrasal modifiers, and are used precisely in the Tree-bank.
The Treebank actually consists of a set of doc-uments, from a variety of sources.
Crucially for ourexperiments ( ee below), the idea 9 informing the se-lection of (the roughly 2000) documents for inclusionin the Treebank was to pack into it the maximumdegree of document variation along many differentscales--document length, subject area, style, pointof view, etc.--but without establishing a single, pre-determined classification of the included documents.In the first experiment, we examine the effective-ness of using trigger pairs over the entire trainingcorpus.
At the same time we investigate the ef-fect of varying the window size.
In additional ex-periments, we observe the effect of partitioning ourtraining dataset into a few relatively homogeneoussubsets, on the hypothesis that this will decreaseperplexity.
It seems reasonable that in different extvarieties, different sets of trigger pairs will be useful,and that tokens which do not have effective triggerswithin one text variety may have them in another) ?To investigate the utility of partitioning thedataset, we construct a separate set of trigger pairsfor each class.
These triggers are only active for theirrespective class and are independent of each other.Their total mutual information is compared to thatderived in exactly the same way from a random par-tition of our corpus into the same number of classes,each comprised of the same number of documents.Our training data partitions naturally into foursubsets, shown in Table 2 as Partitioning 1("Source").
Partitioning 2, "List Structure", putsall documents which contain at least some HTML-like "List" markup (e.g.
LI (=List Item)) 11 in one9see (Black et al, 1996)1?Related work in topic-specific trigram modelling(Lau, 1994) has led to a reduction in perplexity.11All documents in our training set are marked up inHTML-like annotation.133subset, and all other documents in the other sub-set.
By merging Partitionings 1 and 2 we obtainPartitioning 3, "Source Plus List Structure".
Parti-tioning 4 is "Source Plus Document Type", and con-tains 9 subsets, e.g.
"Letters; diaries" (subset 8) and"Novels; stories; fables" (subset 7).
With 13 subsets, ~ePartitioning 5, "Source Plus Domain" includes e.g.'
.~"Social Sciences" (subset 9) and Recreation (subset1).
Partitionings 4 and 5 were effected by actualinspection of each document, or at least of its titleand/or summary, by one of the authors.
The reason P-we included Source within most partitionings wasto determine the extent to which information gainswere additive, a24 Experimental Results4.1 Window SizeFigure 1 shows the effect of varying the windowsize from 1 to 500 for both rule and tag tokens.
Theoptimal window size for tags was approximately 12sentences (about 135 words) and for rules it was ap-proximately 6 sentences (about 68 words).
Thesevalues were used for all subsequent experiments.
Itis interesting to note that the curves are of simi-lar shape for both rules and tags and that the op-timal value is not the largest window size.
Relatedeffects for words are reported in (Lau, 1994; Beefer-man et al, 1997).
In the latter paper, an exponentialmodel of distance is used to penalize large distancesbetween triggering word and triggered word.
Thevariable window used here can be seen as a simplealternative to this.One explanation for this effect in our data is, inthe case of tags, that topic changes occur in docu-ments.
In the case of rules, the effect would seemto indicate a short span of relatively intense stylisticcarryover in text.
For instance, it may be much moreimportant, in predicting rules typical of list struc-ture, to know that similar rules occurred a few sen-tences ago, than to know that they occurred ozensof sentences back in the document.4.2 Class-Specific TriggersTable 3 shows the improvement in perplexity overthe base (unigram) tag and rule models for both therandomly-split and the hand-partitioned trainingsets.
In every case, the meaningful split yielded sig-nificantly more information than the random split.
(Of course, the results for randomly-split trainingsets are roughly the same as for the unpartitionedtraining set (Figure 1)).12For instance, compare the results for Partitionings1, 2, and 3 in this regard.0.350.30.250.20.150.10.05 ' '0 5O 100tagsrules ......""""-----.
...............................................................................t t I t t t t150 200 250 300 350 400 450 500Window size (sentences)Figure 1: Mutual information gain varying windowsize5 DiscussionThe main result of this paper is to show thatanalogous to the case of words in language mod-elling, a significant amount of extrasentential infor-mation can be extracted from the long-range his-tory of a document, using trigger pairs for tags andrules.
Although some redundancy of information isinevitable, we have taken care to exclude as muchinformation as possible that is already available to(intrasentential-data-based, i. .
all known) parsersand taggers.Quantitatively, the studies of (Rosenfeld, 1996)yielded a total mutual information gain of 0.38 bits,using Wall Street Journal data, with one trigger perword.
In a parallel experiment, using the same tech-nique, but on the ATR/Lancaster corpus, the totalmutual information of the triggers for lags was 0.41bits.
This figure increases to 0.52 bits when tags fur-ther away than 135 tags (the approximate equivalentin words to the optimal window size in sentences) areexcluded from the history.
For the remainder of ourexperiments, we do not use as part of the historythe tags/rules from the sentence containing the to-ken to be predicted.
This is motivated by our wishto exclude the intrasentential information which isalready available to parsers and taggers.In the case of tags, using the optimal window size,the gain was 0.31 bits, and for rules the informationgain was 0.12 bits.
Although these figures are notas large as for the case where intrasentential infor-mation is incorporated, they are sufficiently close toencourage us to exploit this information in our mod-els.For the case of words, the evidence shows thattriggers derived in the same manner as the trig-gers in our experiments, can provide a substantialamount of new information when used in combina-tion with sophisticated language models.
For ex-ample, (Rosenfeld, 1996) used a maximum-entropy134Part.
1: SourceClass Name Sents1: Assoc.
Press, WSJ 88512: Canadian Hansards 50023: General English 231054: Travel-domain dialgs 43341Part.
2: List StructureClass Name Sents1: Contains lists 141472: Contains no lists 66152Part.
3: Source + List StructureClass Name Sents1: Assoc.
Press, WSJ 88512: Canadian Hansards 50023: Contains lists (Gem) 119984: Contains no lists (Gen.) 111175: Travel-domain dialogues 43341Part.
4: Source + Doc TypeClass Name Sents1: Legislative 5626(incl.
Srce.2)2: Transcripts 44287(incl.
Srce.4)3: News 8614(incl.
most Srce.1)4: Polemical essays 51605: Reports; FAQs; 11440listings6: Idiom examples 6667: Novels; stories; 741fables8: Letters; diaries 19979: Legal cases; 1768constitutionsPart.
5: Source + DomainClass Name Sents1: Recreation 35452: Business 20553: Science, Techn.
40184: Humanities 22245: Daily Living 8966: Health, Education 16497: Government, Polit.
17688: Travel 26679: Social Sciences 361710: Idiom examp, sents 66611: Canadian Hansards 500212: Assoc.
Press, WSJ 885113: Travel dialgs 43341Table 2: Training Set PartitionsPartitioning1: Source2: List Structure3: Source Plus List Structure4: Source Plus Document Type5: Source Plus DomainMeaningful partition28.40%20.39%28.74%30.11%31.55%Perplexity reduction for tagsI Random16.66%18.71%17.12%18.15%19.39%Perplexity reduction for rulesMeaningful partition15.44%10.55%15.61%16.20%16.60%Random6.30%7.46%6.50%6.82%7.34%Table 3: Perplexity reduction using class-specific triggers to predict tags and rules# Triggering Tag1 NP1LOCNM2 JJSYSTEM3 IIDESPITE4 PN1PERSON.
, .6 IIAT(SF)7 IIFROM(SF)8 NNUNUMTriggered TagNP 1STATENMNP1ORGCFYETLEBUT22MPRICEMPHONE22MZIPNN1MONEYI.e.
Words Like These:Hill, County, Bay, Lakenational, federal, politicaldespiteeveryone, one, anybody, .
.
,at (sent.-final, +/--":")from (sent.-final, + / - " : " )25%, 12", 9.4m3Trigger Words Like These:Utah, Maine, AlaskaParty, Council, Departmentyet (conjunction)(not) only, (not) just$452,983,000, $10,000, $19.95913-3434 (follows area code)22314-1698 (postal zipcode)profit, price, costTable 4: Selected Tag Trigger-Pairs, ATR/Lancaster General-English Treebank#lalb2a2b3a3b4a4b5a5bA Construction Like This:Interrupter Phrase -> * Or -Example: * , -VP -> Verb+Interrupter Phrase+Obj/ComplExample: starring--surprise, surprise--menNoun Phrase -> Simple Noun Phrase+NumExample: Lows around 50Verb Phrase -> Adverb Phrase+Verb PhraseExample: just need to understand itQuestion -> Be+NP+Object/ComplementExample: Is it possible?Triggers A Construction Like This:Sentence -> Interrupter P+Phrasal (Non-S)Example: * DIG.
AM/FM TUNERInterrupter Phrase -> ,+Interrupter+,Example: , according to participants,Num-> Num +PrepP with Numerical ObjExample: (Snow level) 6000 to 7000Auxiliary VP -> Model/Auxilliary Verb+NotExample: do notQuoted Phrasal -> "+Phrasal Constit+"Example: "Mutual funds are back.
"Table 5: Selected Rule Trigger-Pairs, ATR/Lancaster General-English Treebank135ForTriggering TagVVNSENDNP1LOCNMtraining-set documentTriggered Tag I.e.
Words Like These: Trigger Words Like These:NP1STATENM shipped, distributed Utah, Maine, AlaskaNP1STATENM Hill, County, Bay, Lake Utah, Maine, Alaskaclass Recreation (1) vs. for unpartitioned training set (2)3 \[ VVOALTER4 J JPHYS-ATTFor training-set document~\[  NN1TIMENP1POSTFRMNMFor training-set documentNN2SUBSTANCE inhibit, affect, modify tumors, drugs, agentsNN2SUBSTANCE fragile, brown, choppy pines, apples, chemicalsclass Health And Education (3) vs. for unparlitioned training set (4)NN2MONEY period, future, decade T expenses, fees, taxesNN2MONEY Inc., Associates, Co. | loans, damages, chargesclass Business (5) vs. for unpartitioned training set (6)7 8 \ [DD1DDQ \]For training-set documentDDQ O II this th t an?ther each I whic wic hichclass Travel Dialogues (7) vs..for unpariitioned training set (8)Table 6: Selected Tag Trigger-Pairs, ATR/Lancaster General-English Treebank: Contrasting Trigger-PairsArising From Partitioned vs. Unpartitioned Training Setsmodel trained on 5 million words, with only trigger,uni-, hi- and trigram constraints, to measure thetest-set perpexity reduction with respect o a "com-pact" backoff trigram model, a well-respected modelin the language-modelling field.
When the top sixtriggers for each word were used, test-set perplex-ity was reduced by 25%.
Furthermore, when a moresophisticated version of this model 13 was applied inconjunction with the SPHINX II speech recognitionsystem (Huang et al, 1993), a 10-14% reduction inword error rate resulted (Rosenfeld, 1996).
We seeno reason why this effect should not carry over to tagand rule tokens, and are optimistic that long-rangetrigger information can be used in both parsing andtagging to improve performance.For words (Rosenfeld, 1996), self-triggers--wordswhich triggered themselves--were the most frequentkind of triggers (68% of all word triggers were self-triggers).
This is also the case for tags and rules.
Fortags, 76.8% were self-triggers, and for rules, 96.5%were self-triggers.
As in the case of words, the setof self-triggers provides the most useful predictiveinformation.6 Some ExamplesWe will now explicate a few of the example trig-ger pairs in Tables 4-6.
Table 4 Item 5, for instance,captures the common practice of using a sequence ofpoints, e.g ........... , to separate ach item of a (price)list and the price of that item.
Items 6 and 7 aresimilar cases (e.g.
"contact~call (someone) at:" +phone number; "available from:" + source, typicallyincluding address, hence zipcode).
These correla-tions typically occur within listings, and, cruciallya3trained on 38 million words, and also employingdistance-2 N-gram constraints, a unigram cache and aconditional bigram cache (this model reduced perplexityover the baseline trigram model by 32%)for their usefulness as triggers, typically occur manyat a time.When triggers are drawn from a relatively homo-geneous et of documents, correlations emerge whichseem to reflect the character of the text type in-volved.
So in Table 6 Item 5, the proverbial equa-tion of time and money emerges as more central toBusiness and Commerce texts than the different butequally sensible linkup, within our overall trainingset, between business corporations and money.Turning to rule triggers, Table 5 Item 1 is moreor less a syntactic analog of the tag examples Ta-ble 4 Items 5-7, just discussed.
What seems to becaptured is that a particular style of listing things,e.g.
* + listed item, characterizes a document as awhole (if it contains lists); further, listed items arenot always of the same phrasal type, but are proneto vary syntactically.
The same document that con-tains the list item "* DIG.
AM/FM TUNER",  forinstance, which is based on a Noun Phrase, soon af-terwards includes ':* WEATHER PROOF" and "*ULTRA COMPACT", which are based on AdjectivePhrases.Finally, as in the case of the tag trigger examplesof Table 6, text-type-particular correlations emergewhen rule triggers are drawn from a relatively ho-mogeneous set of documents.
A trigger pair of con-structions pecific to Class 1 of the Source partition-ing, which contains only Associated Press newswireand Wall Street Journal articles, is the following: Asentence containing both a quoted remark and anattribution of that remark to a particular source,triggers a sentence containing simply a quoted re-mark, without attribution.
(E.g.
"The King was introuble," Wall wrote, triggers "This increased theKing's bitterness.".)
This correlation is essentiallyabsent in other text types.1367 Conc lus ionIn this paper, we have shown that, as in the case ofwords, there is a substantial amount of informationoutside the sentence which could be used to sup-plement agging and parsing models.
We have alsoshown that knowledge of the type of document beingprocessed greatly increases the usefulness of triggers.If this information is known, or can be predicted ac-curately from the history of a given document beingprocessed, then model interpolation techniques (Je-linek et al, 1980) could be employed, we anticipate,to exploit this to useful effect.Future research will concentrate on incorporatingtrigger-pair information, and extrasentential infor-mation more generally, into more sophisticated mod-els of parsing and tagging.
An obvious first extentionto this work, for the case of tags, will be, following(Rosenfeld, 1996), to incorporate the triggers into amaximum-entropy model using trigger pairs in ad-dition to unigram, bigram and trigram constraints.Later we intend to incorporate trigger informationinto a probabilistic English parser/tagger which isable to ask complex, detailed questions about thecontents of a sentence.
From the results presentedhere we are optimistic that the additional, extrasen-tential information provided by trigger pairs willbenefit such parsing and tagging systems.ReferencesD.
Beeferman, A. Berger, and J. Lafferty.
1997.
AModel of Lexical Attraction and Repulsion.
InProceedings of the ACL-EACL'97 Joint Confer-ence, Madrid.E.
Black, S. Eubank, H. Kashioka, J. Saia.
1998.Reinventing Part-of-Speech Tagging.
Journal ofNatural Language Processing (Japan), 5:1.E.
Black, S. Eubank, H. Kashioka.
1997.
Probabilis-tic Parsing of Unrestricted English Text, With AHighly-Detailed Grammar.
In Proceedings, FifthWorkshop on Very Large Corpora, Beijing/HongKong.E.
Black, S. Eubank, H. Kashioka, R. Garside, G.Leech, and D. Magerman.
1996.
Beyond skeletonparsing: producing a comprehensive large-scalegeneral-English treebank with full grammaticalanalysis.
In Proceedings of the 16th Annual Con-ference on Computational Linguistics, pages 107-112, Copenhagen.E.
Brill.
1994.
Some Advances in Transformation-Based Part of Speech Tagging.
In Proceedingsof the Twelfth National Conference on ArtificialIntelligence, pages 722-727, Seattle, Washington.American Association for Artificial Intelligence.M.
Collins.
1996.
A new statistical parser basedon bigram lexical dependencies.
In Proceedings ofthe 34th Annual Meeting of the Association forComputational Languistics, Santa Cruz.S.
Della Pietra, V. Della Pietra, R. Mercer, S.Roukos.
1992.
Adaptive language modeling us-ing minimum discriminant information.
Proceed-ings of the International Conference on Acoustics,Speech and Signal Processing, I:633-636.X.
Huang, F. Alleva, H.-W. Hon, M.-Y.
Hwang, K.-F. Lee, and R. Rosenfeld.
1993.
The SPHINX-IIspeech recognition system: an overview.
Com-puter Speech and Language, 2:137-148.F.
Jelinek, R. L. Mercer, L. R. Bahl, J. K. Baker.1977.
Perplexity--a measure of difficulty ofspeech recognition tasks.
In Proceedings of the94th Meeting of the Acoustic Society of America,Miami Beach, FL.F.
Jelinek and R. Mercer.
1980.
Interpolated esti-mation of Markov source parameters from sparsedata.
In Pattern Recognition In Practice, E. S.Gelsema and N. L. Kanal, eds., pages 381-402,Amsterdam: North Holland.F.
Jelinek, J. Lafferty, D. Magerman, R. Mercer, A.Ratnaparkhi, S. Roukos.
1994.
Decision tree pars-ing using a hidden derivation model.
In Proceed-ings of the ARPA Workshop on Human LanguageTechnology, pages 260-265, Plainsboro, New Jer-sey.
Advanced Research Projects Agency.R.
Kuhn, R. De Mort.
1990.
A Cache-BasedNatural Language Model for Speech Recognition.IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 12(6):570-583.J.
Kupiec.
1989.
Probabilistic models of short andlong distance word dependencies in running text.In Proceedings of the DARPA Workshop on Speechand Natural Language, pages 290-295.R.
Lau, R. Rosenfeld, S. Roukos.
1993.
Trigger-based language models: a maximum entropy ap-proach.
Proceedings of the International Confer-ence on Acoustics, Speech and Signal Processing,II:45-48.R.
Lau.
1994.
Adaptive Statistical Language Mod-elling.
Master's Thesis, Massachusetts Instituteof Technology, MA.D.
Magerman.
1995.
Statistical decision-tree mod-els for parsing.
In 33rd Annual Meeting of theAssociation for Computational Linguistics, pages276-283, Cambridge, Massachusetts.
Associationfor Computational Linguistics.A.
Ratnaparkhi.
1997.
A Linear Observed TimeStatistical Parser Based on Maximum EntropyModels.
In Proceedings, Second Conference onEmpirical Methods in Natural Language Process-ing, Providence, RI.R.
Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modelling.
Com-puter Speech and Language, 10:187-228.137
