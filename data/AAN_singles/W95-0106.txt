Trainable Coarse Bilingual Grammarsfor Parallel Text BracketingDekai WuHKUSTDepartment of  Computer ScienceUniversity of  Science & TechnologyClear Water Bay, Hong Kongdekai@cs,  ust.
hkAbstractWe describe two new strategies to automatic bracketing of parallel corpora, with particular appli-cation to languages where prior grammar resources are scarce: (1) coarse bilingual grammars, and (2)unsupervised training of such grammars via EM (expectation-maximization).
Both methods build upon aformalism we recently introduced called stochastic inversion transduction grammars.
The first approachborrows acoarse monolingual grammar into our bilingual formalism, in order to transfer knowledge ofone language's constraints to the task of bracketing the texts in both languages.
The second approachgeneralizes the inside-outside algorithm to adjust he grammar parameters so as to improve the likelihoodof a training corpus.
Preliminary experiments on parallel English-Chinese text are supportive of thesestrategies.1 IntroductionA number of empirical studies have found bracketing to be a useful type of corpus annotation (e.g., Pereira& Schabes 1992; Black et al 1993).
Bracketed corpora have been available for some time in English,and to some extent other European languages, the best-known example being perhaps the Penn Treebank(Marcus 1991).
However, at present bracketed corpora for Chinese are unknown, as is the case for manyother languages.
Moreover, even for better-studied languages, parallel bracketed texts are scarce.The problem of bracketing such corpora is the focus of two new strategies described in this paper.
Thestrategies build upon stochastic inversion transduction grammars (SITGs), a formalism that we have beendeveloping for bilingual anguage modeling.
Numerous experiments have shown parallel bilingual corporato provide a rich source of constraints for statistical analysis (e.g., Brown et al 1990; Gale & Church 1991 ;Gale et al 1992; Church 1993; Brown et al 1993; Dagan et al 1993; Fung & Church 1994; Wu & Xia 1994;Fung & McKeown 1994).
SITGs are a generalization of context-free grammars that have several desirableproperties for parallel corpus analysis; a brief summary of these properties i given in Section 2.Our first strategy is to expropriate a very simple, coarse monolingual grammar of English as the backbonefor a bilingual English-Chinese SITG, which is then used for bracketing parallel text.
The effect of this isto transfer knowledge of English syntactic onstraints (or more precisely, probabilistic preferences) to thebilingual task.
This is discussed in Section 3.Our second strategy is to apply an unsupervised training algorithm to tune the probabilistic parametersof the SITG.
For this purpose we have devised an EM-based algorithm, a bilingual generalization of the69inside-outside method, that iteratively improves the likelihood of the training corpus.
This is discussed inSection 4.It is important to stress at the outset that aparallel bracketed corpus is different from a bracketed parallelcorpus.
The latter is simply a parallel corpus in which both halves have been independently bracketed.
Incontrast, in a parallel bracketed corpus, the bracketed sub-constituents are themselves parallel in the sensethat explicit matching relationships are designated between sub-constituents of each half.
This is a muchmore interesting kind of annotation if it can be accomplished, especially for machine translation applications.2 Stochastic Inversion Transduction GrammarsIn Wu (1995b) we define an inversion tranduction grammar (ITG) formalism for bilingual anguage mod-eling, i.e., modeling of two languages (referred to as L1 and L2) simultaneously.
The description here isnecessarily brief; for further details the reader is referred to Wu (1995a, 1995b).An ITG is a context-free grammar that generates output on two separate streams, together with a matchingthat associates the corresponding tokens and constituents of each stream.
The formalism also differs fromstandard context-free grammars inthat the concatenation peration, which is implicit in any production rule'sright-hand side, is replaced with two kinds of concatenation with either straight or inverted orientation.
Thus,the following are two distinct productions in an ITG:C ~ \[AB\]c (A B)Consider each nonterminal symbol to stand for a pair of matched strings, so that for example (A1, A2)denotes the string-pair generated by A.
The operator \[\] performs the "usual" pairwise concatenation so that\[AB\] yields the string-pair (C1, C2) where C1 = AiB~ and C~ = A2B2.
But the operator () concatenatesconstituents on output stream 1 while reversing them on stream 2, so that C1 = A~B~ but C2 = B2A2.The inverted concatenation perator permits the extra flexibility needed to accommodate many kinds ofword-order variation between source and target languages.
Since inversion is permitted at any level of ruleexpansion, a derivation may intermix productions of either orientation within the parse tree.
More on theordering flexibility will be said later.There are also lexical productions of the form:A --~ x/ywhere x and y am symbols of languages L1 and L2, respectively.
Either or both x and y may take the specialvalue E denoting an empty string, allowing a symbol of either language to have no counterpart in the otherlanguage by being matched to an empty string.
We call x/~ an Ll-singleton and c/y an L2-singleton.Parsing, in the context of ITGs, means to take as input a sentence-pair rather than a sentence, and tooutput a parse tree that imposes a shared hierarchical structuring on both sentences.
For example, Figure 1shows a parse tree for an English-Chinese s ntence translation.
The English is mad in the usual depth-firstleft-to-right order, but for the Chinese, a horizontal line means the right subtree is traversed before the left,so that the following sentence pair is generated:(1) a.
\[\[\[The Authority\]Np \[will \[\[be accountable\]vv \[to \[the \[\[Financial Secretary\]NN \] NN \]iP \]PP \]VP\]vv \]sv ./o \]sb.
\ [ \ [ \ [~/~\ ]NP  \ [N~ \[\[~1 \ [ \ [ \ [~  NNN \]NNN \]Ne \]eP \ [ t~\]VV \]VP \]VP \]SP ./o ISAlternatively, we can show the common structure of the two sentences more compactly using bracket notationwith the aid of the () operator:70S?
JOwi l l /~The/c Author i ty /~to/~be/e accountable/~ the/eFinancial/l~i~ Secretary/~Figure 1: Inversion transducer parse tree.
(2) \[\[\[The/e Author i ty/~t~ \]NP \ [w i l l /~  (\[be/e accountable/~\]vv \[to/~ \[the/e \[\[Financial/liaRSecretary/~J\]NN \] NN \]NP \]PP )VP \]VP \]SP .\[o \]Swhere the horizontal line from Figure 1 corresponds tothe () level of bracketing.A stochastic inversion transduction grammar is an ITG where a probability is associated with eachproduction, subject to the constraint that(ai_\[jk\] + ai~(jk)) + Z bi(x,y) = 1l_<j,k<N l_<x<Wal<y<W2where ai_..~\] = P( i  ~ \[jk\]li), bi(x, y) = P( i  ~ x/yl i) ,  W1 and W2 are the vocabulary sizes of the twolanguages, and N is the number of nonterrninal categories.Under the stochastic formulation, the objective of parsing is to find the maximum-likelihood parse for agiven sentence pair.
A general algorithm for this is given in Wu (1995b).The following convenient theorem isproved in Wu (1995b), which indicates that any ITG can be convertedto a normal form, where all productions are either lexical productions or binary-fanout productions:Theorem 1 For any inversion transduction grammar G, there exists an equivalent inversion transductiongrammar G' in which every production takes one of the following forms:S ---~ e/e A ~ x/e A ~ \[BC\]A ~ x /y  A ~ e/y A ~ (BC)71A \[A A\]A (A A}A ~ ui/vjA ui/Ebej A ~ e/vjfor all i, j English-Chinese l xical translationsfor all i English vocabularyfor all j Chinese vocabularyFigure 2: A simple constituent-matching ITG.The algorithms inthis paper assume that ITGs are in this normal form, with one slight relaxation.
Lexicalproductions of the form A ~ x/y may generate multiple-word sequences, i.e., x and g may each be morethan one word.
This does not affect he generative power, but allows probabilities to be placed on collocationtranslations.
The form is called lexical normal form.ITGs impose two desirable classes of constraints on the space of possible matchings between sentences.Crossing constraints prohibit arrangements where the matchings between subtrees cross each another, unlessthe subtrees' immediate parent constituents are also matched to each other.
Aside from linguistic motivationsstemming from the compositionality principle, this constraint isimportant for computational reasons, to avoidexponential bilingual matching times.
Fanout constraints limit the number of direct sub-constituents of anysingle constituent, i.e., the number of subtrees whose matchings may cross at any level.
We have shownthat ITGs inherently permit nearly free matchings for fanouts up to four, with strong constraints hereaftercreating a rapid falloff in the proportion of matchings permitted (Wu 1995a).
This characteristic gives ITGsjust the right degree of flexibility needed to map syntactic structures interlingually.3 Coarse Bilingual GrammarsBecause the expressiveness of ITGs naturally constrains the space of possible matchings in a highly appro-priate fashion, the possibility arises that the information supplied by a word-translation lexicon alone may beadequately discriminating to match constituents, without language-specific monolingual grammars for thesource and target languages, imply by bringing the ITG constraints obear in tandem with lexical matching.That is, the bilingual SITG parsing algorithm can perform constituent identification and matching using onlya generic, language-independent bracketing rammar.Several earlier experiments (Wu 1995a) tested out variants of this hypothesis, using generic SITGs similarto the one shown in Figure 2, which employs only one nonterminal category.
The first two productionsare sufficient to generate all possible matchings of ITG expressiveness (this follows from the normal formtheorem).
The remaining productions are all lexical.
Productions of the A --+ ui/v~ form list all wordtranslations found in the translation lexicon, and the others list all potential singletons without correspondingtranslations.
Thus, a parser with this grammar can build a bilingual parse tree for any possible ITG matchingon a pair of input sentences.Probabilities on the grammar are placed as follows.
The bii distribution encodes the English-Chinesetranslation lexicon with degrees of probability on each potential word translation.
A small e-constant can bechosen for the probabilities bl, and b,i, so that the optimal matching resorts to these productions only when itis otherwise impossible to match the singletons.
The result is that the maximum-likelihood parser selects theparse tree that best meets the combined lexical translation preferences, asexpressed by the bij probabilities.Performance, as reported in Wu (1995a), was encouraging, with precision on automatically-filtered72Chinese: ~ {~ ~ ~ {~ + ~ iE ~ oEnglish: They are right to do soA\[They /{~A<are/*right/~E~to/** /~do/{~so/*>A./o\]AFigure 3: A problematic sentence pair with a generic bracketing rammar.sentence pairs in the 80% range with the aid of supporting heuristics.
However, there are of course inherentlimitations of any approach that relies entirely on crossing- and fanout-constrained l xical matching.
Inparticular, if the sub-constituents of any constituent appear in the same order in both languages, lexicalmatchings do not provide the discriminative leverage to identify the sub-constituent boundaries.
This appliesto both straight and inverted orientations; an example with inverted orientation is shown in Figure 3.
In suchcases, specific grammatical information about one or both of the languages i needed.Grammatical information is far less easily available for Chinese than for English, however, with respectto part-of-speech lexicons as well as grammars.
The SITG formalism offers another possibility: the genericbracketing rammar can be replaced with a context-free backbone designed for English.It is critical under this approach that the English grammar be reasonably robust.
It should also avoidbeing too specific, since to be effective at bracketing, its structure must accomodate Chinese to a reasonablybroad extent.
For these reasons it is best o employ a simple, coarse grammar, with fallback productions thatsimulate the generic bracketing rammar when the English productions are too inflexible.As before, the lexical productions will constitute the bulk of the rules set.
However, we can now distin-guish between different part-of-speech nonterminals.
Different part-of-speech nonterminals may generatethe same words.
We can accomodate he fact that no Chinese part-of-speech lexicon is available withnoninformative distributions as follows:1.
The conditional distribution over L ~ ui/e productions i estimated from the frequencies for eachEnglish part-of-speech L.2.
The conditional distribution over L ~ ui/v# productions i estimated from the frequencies for theEnglish part-of-speech L uniformly distributed over the set of matching Chinese words.3.
The conditional distribution over L ~ ~/v~ productions i uniformly distributed over the Chinesevocabulary.73SOSOSO ----~SOSOSOSOSOV2V2V2V2V2VlV1V1V1V0PPPPN1N1N1N1NO --+\[so so\] (so so)Is so\] (s so)\[NI v2 I(N1 V2)IN1 Vl\] (N1 Vl)\[N1 VB\] (NI VB)\[NP V2\] (NP V2)\[NP V1\] (NP V1)\[NP VB\] (NP VB)\[V2 PP\] (V2 PP)\[Vl PP\] (Vl PP)\[VB PP\] (VB PP)\[V1 N1\] (V1 N1)IV1 NP\] (V1 NP)\[VBN1\] (VB NI)\[V0 NIl (V0 N1)\[VB NP\] (VB NP)\[V0 NP\] (V0 NP)\[VB V0\] (VB V0)\[INN1\] (INN1)\[IN NP\] (IN NP)\[N1 PP\] (N1 PP)\[NP PP\] (NP PP)\[DT NO\] (DT NO)\[DT NN\] (DT NN)\[NN NG \[ (NN NO)start symbolditransitive verb phrasestransitive verb phrasesverb sequencesprepositional phrasesnoun phrasescomplex nominalsFigure 4: Syntactic productions of a stochastic constituent-matching ITG.Because the grammar is coarse while the lexicon is fine, the approach retains the previous approach's highsensitivity to lexical matching constraints.It is interesting toconstrast this method with the "parse-parse-match" approaches that have been reportedrecently for producing parallel bracketed corpora (Sadler & Vendelmans 1990; Kaji et al 1992; Matsumotoet al 1993; Cranias et al 1994; Gfishman 1994).
"Parse-parse-match" methods first bracket aparallel corpusby parsing each half individually using a monolingual grammar.
1 Heuristic procedures are subsequentlyused to select a matching between the bracketed constituents across sentence-pairs.
These approachescan encounter difficulties with incompatibilities between the monolingual grammars used to parse thetexts.
The grammars will usually be of unrelated origins, not designed to make interlingual matching easy.Furthermore, how to deal with ambiguities presents another serious problem.
Most sentences in the corpuswill have multiple possible parses.
In a pure "parse-parse-match" approach, however, the monolingualparsers must arbitrarily select one bracketing with which to annotate the corpus.
The resulting parse maybe incompatible with the parse chosen for the other half of the sentence-pair, causing amatching error eventhough some alternative parse might in fact been compatible.The coarse bilingual grammar approach proposed here solves these problems by choosing the parsea Of course, this assumes that adequate grammars are available for both languages, contrary toour present assumptions.74S ~ ul/v~S ~ ui/ES ---~ E/v~VB ~ ui / v~VB ~ ui / eVB ~ elvjNN ~ ui/v~NN ~ ui l eNN ~ e /v jNP ~ ui / v~NP ~ u i leNP  ~ ElvjIN ~ u~/vjIN ~ ui / EIN  ~ EIv sDT ~ u i /v jDT ~ ui /eDT ~ e/vjmiscellaneousverbs, auxiliary verbsnouns, adjectivespronounsprepositionsdeterminersFigure 5: Lexical productions of a stochastic constituent-matching ITG.structure for both sentences simultaneously with the interlingual constituent matching criteria.
The weightingof the bracketing constraints and matching constraints i probabilistic.
Even if a sentence pair's translationstruly contain structural mismatches that are beyond syntactic accounts, the soft constraint optimizationpermits graceful degradation i the bilingual parse.
The parser will attempt to match those constituents forwhich a partial decomposition a d matching can be found, parsing the rest largely according to the Englishgrammar backbone.More sophisticated "parse-parse-match" procedures postpone ambiguity resolution until the matchingstage (Kaji et al 1992; Matsumoto et al 1993; Grishman 1994).
This tactic bears closer resemblance toour approach, but still requires ad hoc heuristics to determine exactly how the matching task influences themonolingual parses that are chosen.
On the other hand, the present framework incorporates all these aspectswithin a single probabilistic optimization.Another alternative approach discussed in Wu (1995b) is to first use a monolingual grammar to bracketonly the English half of the text, followed by a SITG parallel bracketing procedure constrained by the Englishbrackets.
However, this hybrid approach is subject o the same incompatibility and ambiguity problems thatarise for pure "parse-parse-match" procedures; thus the proposed coarse bilingual grammar approach issuperior for the same reasons given above.For our experiments, we employed the grammar shown in Figures 4 and 5, with only 50 syntacticproductions and 13 nonterminal categories, including 6part-of-speech ategories.
Each syntactic productionoccurs in both straight and inverted orientations, to model ignorance of the ordering tendencies of thecorresponding Chinese constituents.
The part-of-speech ategories were designed by conflating categoriesin the Brown corpus tagset, under the following general principle: categories should be as broad as possible,while still maintaining reasonable discriminativeness forbracketing structure.
Thus, notice that adjectivesand nouns are conflated, since complex nominal phrases have largely similar parse structures regardless of75the difference between adjective and noun labels.
Similarly, all verbs including auxiliaries are grouped toallow simple tail-recursive compounding.
The S category (not to be confused with the start symbol SO) is aplaceholder for miscellaneous items including punctuation and adverbs, and functions as a fallback categorysimilar to the A nonterminal in the generic bracketing rammars.Probabilities were placed on the syntactic productions uniformly, but all inverted productions wereThey are right to do soSO\[so\[They/~ NPVl\[vo\[are/* VBV0<r ight /~ VBv0\[to/* VBv0\[* /~ VBvO\[do/~ VB* /+~ v0IV0IV0IV0>v0Iv0so/* NP\]vl\]so./o S0\]s0The Author i ty will be accountableto the Financial Secretaryso\[so\[Ni<Ni\[The/* DTAuthor i ty /~ NN\] N1* /~ PP>Nivl\[w i l l /~  VBNi<Ni\[be/* DTaccountab le /~ NN\]NiPP\[to/* INNi\[the/* DTNO \[* /~ NNNO \[F inanc ia l /~  NNSecretary/* NO\]NO\]NO\]Ni\]PP>Ni\]VlIS0./o S0\]SOFigure 6: Sample outputs with a coarse bilingual grammar.76assigned a slightly smaller probability in order to break ties in favor of straight matchings.
Probabilities wereplaced on the lexical productions as discussed above, with the following additional provisions.
The translationlexicon was automatically earned from the HKUST English-Chinese Parallel Bilingual Corpus via statisticalsentence alignment (Wu 1994) and statistical Chinese word and collocation extraction (Fung & Wu 1994;Wu & Fung 1994), followed by an EM word-translation learning procedure (Wu & Xia 1994).
The latterstage gives us the lexical translation probabilities.
The translation lexicon contained approximately 6,500English words and 5,500 Chinese words, and was not manually corrected for this experiment, having about86% translation accuracy.
The English part-of-speech lexicon with relative frequencies was derived fromthe English portion of our corpus as tagged by Brill's (1993) tagger.Our preliminary experiments show improved parsing behavior in general, compared to generic bracketinggrammars.
Examples of the output are shown in Figure 6.
The latter example shows problematic behavior onthe example given earlier in Figure 3 of sentence pairs without sufficient ordering discrimination.
Althoughan attempt is made in this case to fit the English constraints, the main difficulty is that the translation "so/~~"  was missing from the automatically-learned lexicon; also, the simple grammar lacks infinitival clauses.4 An EM Algorithm for Training SITGsAn unavoidable consequence of using more structured, complex grammars--coarse though they may be--isthat the bilingual matching process becomes more sensitive to the syntactic production probabilities thanunder the earlier generic bracketing rammar approaches.
Performance therefore suffers if the probabilitiesare not appropriate, a serious problem given that the syntactic production probabilities above are manually,and arbitrarily, set to be uniform.It therefore becomes desirable to find means to tune the syntactic production probabilities automatically,so as to be optimal with respect to some training data set.
Note that we do not expect he parallel trainingcorpus to be parsed or otherwise syntactically annotated beforehand.
To this end we present an EM(expectation-maximization) algorithm for iteratively improving the syntactic production parameters of aSITG, according to a likelihood criterion.
The method is a generalization f the inside-outside algorithm forSCFG estimation (Baker 1979; Lari & Young 1990).A few notational preliminaries: we will denote the sentence pairs by (E, C) where the English sentenceE -- e l , .
.
.
,  eT and the corresponding Chinese sentence C = c l , .
.
.
, ev are vectors of observed symbols(that is, lexemes or words).
As an abbreviation we write e~..t for the sequence of words e~+~, e~+2,.. .
,  et,and similarly for ~..~.
It will be convenient touse a 4-tuple of the form q = (s, t, u, v) to identify each nodeof the parse tree, where the substrings es..t and c~..~ both derive from the node q. Denote the nonterminallabel on q = (s, t, u, v) by gq or ?stu~, with the convention that ?~t~ = 0 means that e,..t and e~..~ are notderived from a single common onterminal.The inside probabilities, defined as:(1) /3,t,,,(i) = P\[i ~ e,..,/c,~...lg,,,~ : i,(I)\]are computed recursively as follows.1.
Basis(2) /3tt~v(i)(3) o ?= 0=O< t < T ,O< v< VO<s< t < T,O_< u < v< V,(t  - - u)  # o772.
Recurs ion(4) fl,,~,(i)(5 )  t\] ?
(6) 0m\[\] ~i~ ~0 r ia+ o ?
= to'stuvk \] "JC i'Jstuv\ \]: ~ a,_\[jk\] fl, s~,u(j) flstu~(k)I _< j _< Nl<k<Ns<S<tu<U<v(s-~)(t-s)+(u-~,)(v-U)#O= 52i <j_<Nl<k<N~<S<tu<U<v(s-,)(t-s)+(u-~)(~-u)#oai-(jk) fl~su.
(j ) fls.,u( k )Subsequent tothe inside computation, the outside probabilities, defined as:(7) a~( i )  = P\[S ~ eo..fiet..T/co..~,ie,,..v,g~t~ = il~\]are also computed recursively:1.
Bas i s1 i f i=S(8) aO,T,O,v(i) = 0 otherwise(9) a .~,( i )  = 02.
Recurs ionO<t  <T,O< v< V(lO)(11) -\[1 ri~ (Xstuvk \](12)  c~ 0 ~;~ s tuv \  ~ \]\[\] ?
0 ?= ~ astv,( j )  aj-\[ki\] fls,g~(k) + ~ a,s .v ( j )  aj--.\[ik\] flts, v(k)i< j<N i< j<Nl<k<N l<k<N0<S<s t<S<T0<U<u v<U<V( ,-s)(~-u )#o ( s-~ )(u-~ )~o= E C~StuU(j) a j - (k i )~ssvu(k)+ E c~,suv(j)aj--,(ik)Ztsuu(k)l< j<Y l< j<gl<k<N l<k<NO<S<s t<S_<Tv<U<V 0<U<u(,-s)(u-~)~o (s-t)(,~-u)#oThe estimation procedure for adjusting the model parameter set ff is defined in terms of the inside and outsideprobabilities.
We begin by considering for each nonterminal the probability of its use in a derivation of theobserved sentence-pair:78T T V V(13) P\[i used I S ~ E/C,~\ ]  = ,=o,:, ~=0,:~P\[S ~ E/CI?\]T T V VE E E E-.,o (oz.,oo(os :O  t=s  u-~O v=u (14) = P\[S ~ E/CI?\]The probability of using each straight production rule in a derivation of the observed sentence-pair is:(15) P\[i ---~ \[jk\] used I S ~ E/C,  ?\](16) =Similarly for each inverted production rule:(17) P\[i ~ (jk) used l S ~ E/C,  4\] =(18) =T T V V= ~ ~ E ~ P\[i =~ \[jk\] ~ es..t/c~,..v IS :-,.
E/C, ~18=0 t=,S U=0 'V=UT T VE E E ?
?
?
ai-\[jk, c~st,~(i)flss,u(j)Zstu~(k)8=0 t=s  u=0 v=u S=s  U=ue\[s ~ E/tIC\]T T V VS=0 t=$ u=O V=UT T V V t v8=0 t=8 u=0 v=u S=8 U=ue\[s ~ E/ClO\]By definition, the syntactic production probabilities are:(19) a~-~k\] = P\[i ~ \[jk\] used\[ i used, S =~ E/C,  ~\](20) ai_.
(jk) = P\[i ~ (jk) used I i used, S =~ E/C,  ,I)\]Substitution yields a re-estimation procedure for A:T T V V(21) hi--~k\] = ,=0 t .
.
.
.
0o=~s=, u=~,T T V VE E E8=0 t=8 ~=Ov=~T T V V t v8=0 t=8 u=O V=U S=,s V=~ (22) 5~_(jk) =T T V VThe behavior f a typical training run is shown in Figure 7.
The relative movement ofthe log likelihoodis what is important here.
The absolute magnitudes are not meaningful since they are largely determinedby the fixed lexical translation probabilities.
What is significant isthat due to the relatively small numberof parameters being trained, convergence is achieved within two or three iterations.
(The rise in perplexityafterwards i  caused by numerical error on overtrained parameters; we terminate raining as soon as thisoccurs.
)79X100008000600040002000i\i i I i i0 1 2 3 4IterationFigure 7: Perplexity on successive training iterations.5 ConclusionWe have described two new approaches toautomatic bracketing of parallel corpora, which are particularlyapplicable to languages where grammar resources are scarce.
The methods---coarse bilingual grammarsexpropriated from monolingual grammars, with EM parameter stimation--are grounded upon a firm theo-retical model, and preliminary experiments show promising behavior.
The training method oes not requiresyntactically annotated parallel corpora, which are difficult to obtain.
We are presently conducting morequantitative evaluations of the bracketing performance improvement.6 AcknowledgementsI would like to thank Xuanyin Xia and Eva Wai-man Fong for data conversion assistance.ReferencesBAKER, JAMES K. 1979.
Trainable grammars for speech recognition.
In Speech Communication Papers for the 97thMeeting of the Acoustic Society of America, ed.
by D. H. Klatt & J. J. Wolf, 547-550.BLACK, EZRA, ROGER GARSIDE, & GEOFFREY LEECH (eds.).
1993.
Statistically-driven computer grammars of English:The IBM~Lancaster approach.
Amsterdam: Editions Rodopi.BRILL, ERIC, 1993.
A corpus-based approach to language l arning.
University of Pennsylvania dissertation.BROWN, PETER F., JOHN COCKE, STEPHEN A. DELLAPIETRA, VINCENT J. DELLAPIETRA, FREDERICK JELINEK, JOHN D.LAFFERTY, ROBERT L. MERCER, & PAUL S. ROOSSIN.
1990.
A statistical approach to machine translation.Computational Linguistics, 16(2):29-85.BROWN, PETER F., STEPHEN A. DELLAPIETRA, VINCENT J. DELLAPIETRA, & ROBERT L. MERCER.
1993.
Themathematics of statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263-311.CHURCH, KENNETH W. 1993.
Char-align: A program for aligning parallel texts at the character level.
In Proceedingsof the 31st Annual Conference of the Association for Computational Linguistics, 1-8, Columbus, OH.80CRANIAS, LAMBROS, HARRIS PAPAGEORGIOU, d~; STELIOS PEPERIDIS.
1994.
A matching technique in example-basedmachine translation.
In Proceedings of the Fifteenth International Conference on Computational Linguistics,100-104, Kyoto.DAGAN, IDO, KENNETH W. CHURCH, & WILLIAM A. GALE.
1993.
Robust bilingual word alignment for machine aidedtranslation.
In Proceedings of the Workshop on Very Large Corpora, 1-8, Columbus, OH.FUNG, PASCALE & KENNETH W. CHURCH.
1994.
K-vec: A new approach for aligning parallel texts.
In Proceedingsof the Fifteenth International Conference on Computational Linguistics, 1096-1102, Kyoto.FUNG, PASCALE t~z KATHLEEN MCKEOWN.
1994.
Aligning noisy parallel corpora cross language groups: Word pairfeature match!ng by dynamic time warping.
In AMTA-94, Association for Machine Translation in the Americas,81-88, Columbia, Maryland.FUNG, PASCALE & DEKAI Wu.
1994.
Statistical ugmentation f a Chinese machine-readable dictionary.
In Proceedingsof the Second Annual Workshop on Very Large Corpora, 69-85, Kyoto.GALE, WILLIAM m. & KENNETH W. CHURCH.
1991.
A program for aligning sentences in bilingual corpora.
InProceedings of the 29th Annual Conference of the Association for Computational Linguistics, 177-184, Berkeley.GALE, WILLIAM A., KENNETH W. CHURCH, & DAVID YAROWSKY.
1992.
Using bilingual materials to develop wordsense disambiguation methods.
In Fourth International Conference on Theoretical nd Methodological lssues inMachine Translation, 101-112, Montreal.GRISHMAN, RALPH, 1994.
Iterative alignment of syntactic structures for a bilingual corpus.
In Proceedings of theSecond Annual Workshop on Very Large Corpora, 57-68, Kyoto.KAJI, HIROYUKI, YUUKO KIDA, & YASUTSUGU MORIMOTO.
1992.
Learning translation templates from bilingual text.In Proceedings of the Fourteenth International Conference on Computational Linguistics, 672-678, Nantes.LARI, K. & S. J.
YOUNG.
1990.
The estimation of stochastic context-free grammars using the inside-outside algorithm.Computer Speech and Language, 4:35-56.MARCUS, MITCHELL.
1991.
The automatic acquisition of linguistic structure from large corpora: An overview of workat the university of pennsylvania.
In Working Notes from the Spring Symposium on Machine Learning of NaturalLanguage and Ontology, 123-125, Stanford University, Stanford, CA.
AAAI.MATSUMOTO, YUJI, HIROYUKI ISHIMOTO, & TAKEHITO UTSURO.
1993.
Structural matching of parallel texts.
InProceedings of the 31st Annual Conference of the Association for Computational Linguistics, 23-30, Columbus,OH.PEREIRA, FERNANDO & YVES SCHABES.
1992.
Inside-outside r estimation from partially bracketed corpora.
InProceedings of the 30th Annual Conference of the Association for Computational Linguistics, 128-135, Newark,DE.SADLER, VICTOR & RONALD VENDELMANS.
1990.
Pilot implementation f a bilingual knowledge bank.
In Proceedingsof the Thirteenth International Conference on Computational Linguistics, 449-451, Helsinki.WU, DEKAI.
1994.
Aligning a parallel English-Chinese corpus tatistically with lexical criteria.
In Proceedings of the32nd Annual Conference of the Association for Computational Linguistics, 80-87, Las Cruces, New Mexico.WU, DEKAI.
1995a.
Grammarless extraction of phrasal translation examples from parallel texts.
In Proceedings ofthe Sixth International Conference on Theoretical nd Methodological Issues in Machine Translation, Leuven,Belgium.
To appear.WU, DEKAI.
1995b.
Stochastic inversion transduction grammars, with application to segmentation, bracketing, andalignment of parallel corpora.
In Proceedings of lJCAI-95, Fourteenth International Joint Conference on ArtificialIntelligence, Montreal.
To appear.WU, DEKAI & PASCALE FUNG.
1994.
Improving Chinese tokenization with linguistic filters on statistical lexicalacquisition.
In Proceedings of the Fourth Conference on Applied Natural Language Processing, 180-181,Stuttgart.WU, DEKAI & XUANYIN XIA.
1994.
Learning an English-Chinese l xicon from a parallel corpus.
In AMTA-94,Association for Machine Translation in the Americas, 206-213, Columbia, Maryland.81
