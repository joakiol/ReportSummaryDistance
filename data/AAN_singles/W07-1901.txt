Proceedings of the Workshop on Embodied Language Processing, pages 1?8,Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational LinguisticsComparing Rule-based and Data-driven Selection of Facial DisplaysMary Ellen FosterInformatik VI: Robotics and Embedded SystemsTechnische Universit?t M?nchenBoltzmannstra?e 3, 85748 Garching, Germanyfoster@in.tum.deAbstractThe non-verbal behaviour of an embodiedconversational agent is normally based onrecorded human behaviour.
There are twomain ways that the mapping from human be-haviour to agent behaviour has been imple-mented.
In some systems, human behaviouris analysed, and then rules for the agent arecreated based on the results of that analysis;in others, the recorded behaviour is used di-rectly as a resource for decision-making, us-ing data-driven techniques.
In this paper, weimplement both of these methods for select-ing the conversational facial displays of ananimated talking head and compare them intwo user evaluations.
In the first study, par-ticipants were asked for subjective prefer-ences: they tended to prefer the output of thedata-driven strategy, but this trend was notstatistically significant.
In the second study,the data-driven facial displays affected theability of users to perceive user-model tai-loring in synthesised speech, while the rule-based displays did not have any effect.1 IntroductionThere is no longer any question that the productionof language and its accompanying non-verbal be-haviour are tightly linked (e.g., Bavelas and Chovil,2000).
The communicative functions of body lan-guage listed by Bickmore and Cassell (2005) includeconversation initiation and termination, turn-takingand interruption, content elaboration and emphasis,and feedback and error correction; non-verbal be-haviours that can achieve these functions includegaze modification, facial expressions, hand gestures,and posture shifts, among others.When choosing non-verbal behaviours to accom-pany the speech of an embodied conversationalagent (ECA), it is necessary to translate general find-ings from observing human behaviour into concreteselection strategies.
There are two main implemen-tation techniques that have been used for making thisdecision.
In some systems, recorded behaviours areanalysed and rules are created by hand based on theanalysis; in others, recorded human data is used di-rectly in the decision process.
The former techniqueis similar to the classic role of corpora in natural-language generation described by Reiter and Dale(2000), while the latter is more similar to the morerecent data-driven techniques that have been adopted(Belz and Varges, 2005).Researchers that have used rule-based techniquesto create embodied-agent systems include: Poggiand Pelachaud (2000), who concentrated on generat-ing appropriate affective facial displays based on de-scriptions of typical facial expressions of emotion;Cassell et al (2001a), who selected gestures andfacial expressions to accompany text using heuris-tics derived from studies of typical North Ameri-can non-verbal-displays; and Marsi and van Rooden(2007), who generated typical certain and uncertainfacial displays for a talking head in an information-retrieval system.
Researchers that used data-driventechniques include: Stone et al (2004), who cap-tured the motions of an actor performing scriptedoutput and then used that data to create performance1specifications on the fly; Cassell et al (2001b), whoselected posture shifts for an embodied agent basedon recorded human behaviour; and Kipp (2004),who annotated the gesturing behaviour of skilledpublic speakers and derived ?gesture profiles?
to usein the generation process.Using rules derived from the data can produce dis-plays that are easily identifiable and is straightfor-ward to implement.
On the other hand, making di-rect use of the data can produce output that is moresimilar to actual human behaviour by incorporatingnaturalistic variation, although it generally requiresa more complex selection algorithm.
In this paper,we investigate the relative utility of the two imple-mentation strategies for a particular decision: select-ing the conversational facial displays of an animatedtalking head.
We use two methods for comparison:gathering users?
subjective preferences, and measur-ing the impact of both selection strategies on users?ability to perceive user tailoring in speech.In Section 2, we first describe how we recordedand annotated a corpus of facial displays in the do-main of the target generation system.
Section 3 thenpresents the two strategies that were implementedto select facial displays based on this corpus: oneusing a simple rule derived from the most character-istic behaviours in the corpus, and one that made aweighted choice among all of the options found inthe corpus for each context.
The next sections de-scribe two user studies comparing these strategies:in Section 4, we compare users?
subjective prefer-ences, while in Section 5 we measure the impact ofeach strategy on user?s ability to select spoken de-scriptions correctly tailored to a given set of userpreferences.
Finally, in Section 6, we discuss theresults of these two studies, draw some conclusions,and outline potential future work.2 Corpus collection and annotation1The recording scripts for the corpus were createdby the output planner of the COMIC multimodaldialogue system (Foster et al, 2005) and consistedof a total of 444 sentences describing and compar-ing various tile-design options.
The surface form ofeach sentence was created by the OpenCCG surfacerealiser (White, 2006), using a grammar that spec-1Foster (2007) gives more details of the face-display corpus.ified both the words and the intended prosody forthe speech synthesiser.
We attached all of the rele-vant contextual, syntactic, and prosodic informationto each node in the OpenCCG derivation tree, in-cluding the user-model evaluation of the object be-ing described (positive, negative, or neutral), the pre-dicted pitch accent, the clause of the sentence (first,second, or only), and whether the information beingpresented was new to the discourse.The sentences in the script were presented oneat a time to a speaker who was instructed to readeach out loud as expressively as possible into a cam-era directed at his face.
The following facial dis-plays were then annotated on the recordings: eye-brow motions (up or down), eye squinting, and rigidhead motion on all three axes (nodding, leaning, andturning).
Each of these displays was attached tothe node or nodes in the OpenCCG derivation treethat exactly covered the span of words temporallyassociated with the display.
Two coders separatelyprocessed the sentences in the corpus.
Using a ver-sion of the ?
weighted agreement measure proposedby Artstein and Poesio (2005)?which allows fora range of agreement levels?the agreement on thesentences processed by both coders was 0.561.When the distribution of facial displays in thecorpus was analysed, it was found that the singlebiggest influence on the speaker?s behaviour wasthe user-model evaluation of the features being de-scribed.
When he described features of the designthat had positive user-model evaluations, he wasmore likely to turn to the right and to raise his eye-brows (Figure 1(a)); on the other hand, on featureswith negative user-model evaluations, he was morelikely to lean to the left, lower his eyebrows, andsquint his eyes (Figure 1(b)).
The overall most fre-quent display in all contexts was a downward nod onits own.
Other factors that had a significant effect onthe facial displays included the predicted pitch ac-cent, the clause of the sentence (first or second), andthe number of words spanned by a node.3 Selection strategiesBased on the recorded behaviour of the speaker,we implemented two different methods for selectingfacial displays to accompany synthesised speech.Both methods begin with the OpenCCG derivation2(a) Positive (b) NegativeFigure 1: Characteristic facial displays from the corpusAlthough it?s in the family style, the tiles are by Alessi.Original nd=d nd=d nd=d nd=d nd=d,bw=u.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ln=l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Data-driven nd=d nd=d .
.
tn=r .
.Rule-based ln=l,bw=d,sq tn=r,bw=uFigure 2: Face-display schedules for a sample sentencetree for a sentence?that is, a tree in the same for-mat as those that were used for the corpus annota-tion, including all of the contextual features.
Theythen proceed top-down through the derivation tree,considering each node in turn and determining thedisplay combination (if any) to accompany it.The rule-based strategy specifies motions only onnodes corresponding to mentions of specific proper-ties of a tile design: manufacturer and series names,colours, and decorations.
The display combinationis determined by the user-model evaluation of theproperty being described, based on the behavioursof the recorded speaker.
For a positive evaluation,this strategy selects a right turn and brow raise; fora negative evaluation, it selects a left turn, browlower, and eye squint; while for neutral evaluations,it chooses a downward nod.In contrast, the data-driven strategy considers allnodes in the derivation tree.
For each node, it selectsfrom all of the display combinations that occurredon similar nodes in the corpus, weighted by the fre-quency.
As a concrete example, in a hypotheticalcontext where the speaker made no motion 80% ofthe time, nodded 15% of the time, and turned to theright in the other 5%, this strategy would select nomotion with probability 0.8, a nod with probability0.15, and a right turn with probability 0.05.Figure 2 shows a sample sentence from the cor-pus, the original facial displays, and the displays se-lected by each of the strategies.
In the figure, nd=dindicates a downward nod, bw=u and bw=d a browraise and lower, respectively, sq an eye squint, ln=la left lean, and tn=r a right turn.4 Subjective preferencesAs a first comparison of the two implementationstrategies, we gathered users?
subjective preferencesbetween three different types of face-display sched-ules: the displays selected by each of the generationstrategies described in the preceding section, as wellas the original displays annotated in the corpus.4.1 ParticipantsThis experiment was run through the Language Ex-periments Portal,2 a website dedicated to online psy-cholinguistic experiments.
There were a total of 36participants: 20 females and 16 males.
23 of the par-ticipants were between 20 and 29 years old, 9 wereover 30, and 4 were under 20.
21 described them-selves as expert computer users, 14 as intermediateusers, and one as a beginner.
18 were native speak-ers of English, while the others had a range of othernative languages.2http://www.language-experiments.org/3Figure 3: RUTH talking head4.2 MethodologyEach participant saw videos of two possible synthe-sised face-display schedules accompanying a seriesof 18 sentences.
Both videos had the same syn-thesised speech, but each had a different differentfacial-display schedule.
For each pair, the partici-pant was asked to select which of the two versionsthey preferred.
There were three different scheduletypes: the original displays annotated in the corpus,along with the output of both of the selection strate-gies.
Participants made each pairwise comparisonbetween these types six times, three times in each or-der.
All participants saw the same set of sentences,in a random order: the pairwise choices were alsoallocated to sentences randomly.4.3 MaterialsTo create the materials for this experiment, we ran-domly selected 18 sentences from the corpus andgenerated facial displays for each, using both of thestrategies.
The data-driven schedules were gener-ated through 10-fold cross-validation as part of aprevious study (Foster and Oberlander, 2007): thatis, the display counts from 90% of the corpus wereused to select the displays to use for the sentencesin the held-out 10%.
The rule-based schedules weregenerated by running the rule-based procedure fromSection 3 on the same OpenCCG derivation trees.Videos were then created of all of the schedules forall of the sentences, using the Festival speech syn-thesiser (Clark et al, 2004) and the RUTH animatedtalking head (DeCarlo et al, 2004) (Figure 3).Original ?
Rule-based Weighted ?
Rule-based Original ?
Weighted0102030405060708090100110120130 123 120 12393 97 92ComparisonChoice countFigure 4: Subjective-preference results4.4 ResultsThe overall results of this study are shown in Fig-ure 4.
Not all participants responded to all items,so there were a total of 648 responses: 216 compar-ing the original corpus schedules to the rule-basedschedules, 217 for the data-driven vs. rule-basedcomparison, and 215 for the original vs. data-drivencomparison.
To assess the significance of the pref-erences, we use a binomial test, which provides anexact measure of the statistical significance of de-viations from a theoretically expected classificationinto two categories.
This test indicates that therewas a mildly significant preference for the originalschedules over the output of each of the strategies(p< 0.05 in both cases).
While there was also a ten-dency to prefer the output of the data-driven strategyover that of the rule-based strategy, this preferencewas not significant (p?
0.14).
No demographic fac-tor had a significant effect on these results.4.5 DiscussionAlthough there was no significant preference be-tween the output of the two strategies, the generatedschedules were very different.
The rule-based strat-egy used only the three display combinations de-scribed in Section 3 and selected an average of 1.78displays per sentence on the 18 sentences used inthis study, while the data-driven strategy selected 12different display combinations across the sentencesand chose an average of 5.06 displays per sentence.For comparison, the original sentences from the cor-pus used a total of 15 different combinations on the4(1) Here is a family design.
Its tiles are from the Lollipop collection by Agrob Buchtal.
Although the tiles have a blue colourscheme, it does also feature green.
(2) Here is a family design.
As you can see, the tiles have a blue and green colour scheme.
It has floral motifs and artworkon the decorative tiles.Figure 5: Tile-design description tailored to two user models (conflicting concession highlighted)same sentences and had an average of 4.83 displaysper sentence.
In other words, in terms of the rangeof displays, the schedules generated by the data-driven strategy are fairly similar to those in the cor-pus, while those from the rule-based strategy do notresemble the corpus very much at all.In another study (Foster and Oberlander, 2007),the weighted data-driven strategy used here wascompared to a majority strategy that always chosethe highest-probability option in every context.
Inother words, in the hypothetical context mentionedearlier where the top option occurred 80% of thetime, the majority strategy would always choose thatoption.
This strategy scored highly on an automatedcross-validation study; however, human judges verystrongly preferred the output of the weighted strat-egy described in this paper (p < 0.0001).
This con-trasts with the weak preference for the weightedstrategy over the rule-based strategy in the currentexperiment.
The main difference between the out-put of the majority strategy on the one hand, and thatof the two strategies described here on the other, isin the distribution of the face-display combinations:over 90% of the that the majority strategy selecteda display, it used a downward nod on its own, whileboth of the other strategies tended to generate a moreeven distribution of displays across the sentences.This suggests that the distribution of facial displaysis more important than strict corpus similarity fordetermining subjective preferences.The participants in this study generally preferredthe original corpus displays to the output of eitherof the generation strategies.
This suggests that amore sophisticated data-driven implementation thatreproduces the corpus data more faithfully couldbe successful.
For example, the process of select-ing facial displays could be integrated directly intothe OpenCCG realiser?s n-gram-guided search for agood realisation (White, 2006), rather than being runon the output of the realiser as was done here.5 Perception of user tailoring in speechThe results of the preceding experiment indicate thatparticipants mildly preferred the output of the data-driven strategy to that of the rule-based strategy;however, this preference was not statistically signif-icant.
In this second experiment, we compare theface-display schedules generated by both strategiesin a different way: measuring the impact of eachschedule type on users?
ability to detect user-modeltailoring in synthesised speech.Foster andWhite (2005) performed an experimentin which participants were shown a series of pairs ofCOMIC outputs (e.g., Figure 5) and asked to choosewhich was correctly tailored to a given set of userpreferences.
The participants in that study were ableto select the correctly-tailored output only on trialswhere one option contained a concession to a neg-ative preference that the other did not.
For exam-ple, the description in (1) contains the concession Al-though the tiles have a blue colour scheme, as if theuser disliked the colour blue, while (2) has no suchconcession.
Figure 6 shows the results from thatstudy when outputs were presented as speech; theresults for text were nearly identical.
The first pairof bars represent the choices made on trials wherethere was a conflicting concession, while the secondpair show the choices made on trials with no con-flicting concession.
Using a binomial test, the dif-ference for the conflicting-concession trials is sig-nificant at p < 0.0001, while there is no significantdifference for the other trials (p?
0.4).In this experiment, use the same experimental ma-terials, but we use the talking head to present the sys-tem turns.
This experiment allows us to answer twoquestions: whether the addition of a talking head af-fects users?
ability to perceive tailoring in speech,and whether there is a difference between the impactof the two selection strategies.5Orig nal ?
r Rl r ignal  ueu- ubusuduWuhut u0u 0u- sse - 012345 6  753O7ral5Rlr8iFigure 6: Results for speech-only presentation5.1 ParticipantsLike the previous study, this one was also run overthe web.
There were 32 participants: 19 females and13 males.
18 of the participants were between 20and 29 years old, 10 were over 30, and 4 were un-der 20.
15 described themselves as expert computerusers, 15 as intermediate users, and 2 as beginners.30 of the participants were native English speakers.5.2 MethodologyParticipants in this experiment observed an eight-turn dialogue between the system and a user withspecific likes and dislikes.
The user preferenceswere displayed on screen at all times; the user inputwas presented as written text on the screen, while thesystem outputs were played as RUTH videos in re-sponse to the user clicking on a button.
There weretwo versions of each system turn, one tailored to thepreferences of the given user and one to the prefer-ences of another user; the user task was to select thecorrectly tailored version.
The order of presentationwas counterbalanced so that the correctly tailoredversion was the first option in four of the trials andthe second in the other four.
Participants were as-signed in rotation to one of four randomly-generateduser models.
As an additional factor, half of the par-ticipants saw videos with facial displays generatedby the data-driven strategy, while the other half sawvideos generated by the rule-based strategy.5.3 MaterialsThe user models and dialogues were identical tothose used by Foster and White (2005).
For eachsentence in each system turn, we annotated thenodes of the OpenCCG derivation tree with all ofthe necessary information for generation: the user-model evaluation, the pitch accents, the clause ofthe sentence, and the surface string.
We then usedthose annotated trees to create face-display sched-ules using both of the selection strategies, using thefull corpus as context for the data-driven strategy,and prepared RUTH videos of all of the generatedschedules as in the previous study.5.4 ResultsThe results of this study are shown in Figure 7: Fig-ure 7(a) shows the results for the participants usingthe rule-based schedules, while Figure 7(b) showsthe results with the data-driven schedules.
Just asin the speech-only condition, the participants in thisexperiment responded essentially at chance on tri-als where there was no conflicting concession tonegative preferences.
For the trials with a conflict-ing concession, participants using rule-based videosselected the targeted version significantly more of-ten (p < 0.01), while the results for participants us-ing the data-driven videos show no significant trend(p ?
0.49).
None of the demographic factors af-fected these results.To assess the significance of the difference be-tween the two selection strategies, we comparedthe results on the conflicting-concession trials fromeach of the groups to the corresponding results fromthe speech-only experiment, using a ?2 test.
Theresults for the judges using the rule-based videosare very similar to those of the judges using onlyspeech (?2 = 0.21, p = 0.65).
However, there is asignificant difference between the responses of thespeech-only judges and those of the judges using theweighted schedules (?2 = 4.72, p < 0.05).5.5 DiscussionThe materials for this study were identical to thoseused by Foster and White (2005); in fact, the wave-forms for the synthesised speech were identical.However, the participants in this study who sawthe videos generated by the data-driven strategy6Orig nal ?
r Rl r ignal  ue- u- ebubesusedudeeueeWu ehbesb but 0123 4  531O5ral3Rlr6i(a) Rule-based schedulesOrig nal ?
r Rl r ignal  ue- u- ebubesusedudeeuee ee- Wdhbst 0123 4  531O5ral3Rlr6i(b) Data-driven schedulesFigure 7: Results of the perception studywere significantly worse at identifying the correctly-tailored speech than were the participants in the pre-vious study, while the performance of the partic-ipants who saw rule-based videos was essentiallyidentical to that of the speech-only subjects.The schedules selected by the data-driven strat-egy for this evaluation include a variety of facial dis-plays; sometimes these displays are actually the op-posite of what would be selected by the rule-basedstrategy.
For example, the head moves to the rightwhen describing a negative fact in 23 of the 520data-driven schedules, and moves to the left whendescribing a neutral or positive fact in 20 cases.
Adescription includes up to three sentences, and a trialinvolved comparing two descriptions, so a total of 75of the trials (52%) for the data-driven participantsinvolved at least one of these these potentially mis-leading head movements.
Across all of the trials forthe participants using data-driven videos, there were38 conflicting-concession trials with no such headmovement.
The performance on these trials was es-sentially the identical to that on the full set of tri-als: the correctly targeted description was chosen 20times, and the other version 18 times.
So the worseperformance with the data-driven schedules cannotbe attributed solely to the selected facial displaysconflicting with the linguistic content.Another possibility is that the study participantswho used the data-driven schedules were distractedby the expressive motions of the talking head andfailed to pay attention to the content of the speech.This appears to have been the case in the COMICwhole-system evaluation (White et al, 2005), forexample, where the performance of the male par-ticipants on a recall task was significantly worsewhen a more expressive talking head was used.
Onthis study, there was no effect of gender (or any ofthe other demographic factors) on the pattern of re-sponses; however, it could be that a similar effectoccurred in this study for all of the participants.6 Conclusions and future workThe experiments in this paper have compared thetwo main current implementation techniques forchoosing non-verbal behaviour for an embodiedconversational agent: using rules derived from thestudy of human behaviour, and using recorded hu-man behaviour directly in the generation process.The results of the subjective-preference evaluationindicate that participants tended to prefer the out-put generated by the data-driven strategy, althoughthis preference was not significant.
In the secondstudy, videos generated by the data-driven strat-egy significantly decreased participants?
ability todetect correctly-tailored spoken output when com-pared to a speech-only presentation; on the otherhand, videos generated by the rule-based strategydid not have a significant impact on this task.These results indicate that, at least for this cor-pus and this generation task, the choice of gener-ation strategy depends largely on which aspect ofthe system is more important: to create an agent7that users like subjectively, or to ensure that usersfully understand all aspects of the output presentedin speech.
If the former is more important, than animplementation that uses the data directly appearsto be a slightly better option; if the latter is more im-portant, then the rule-based strategy seems superior.On the subjective-preference evaluation, userspreferred the original corpus motions over either ofthe generated versions.
As discussed in Section 4.5,this suggests that there is room for a more sophisti-cated data-driven selection strategy that reproducesthe corpus data more closely.
The output of such ageneration strategy might also have a different effecton the perception task.Both of these studies used the RUTH talking head(Figure 3), which has no body and, while human inappearance, is not particularly realistic.
We used thishead to investigate the the generation of a limited setof facial displays, based on contextual informationincluding the user-model evaluation, the predictedprosody, the clause of the sentence, and the surfacestring.
More information about the relative utilityof different techniques for selecting non-verbal be-haviour for embodied agents can be gathered by ex-perimenting with a wider range of agents and ofnon-verbal behaviours.
Other possible agent typesinclude photorealistic animated agents, agents withfully articulated virtual bodies, and physically em-bodied robot agents.
The possibilities for non-verbalbehaviours include deictic, iconic, and beat gestures,body posture, gaze behaviour, and facial expressionsof various types of affect, while any source of syn-tactic or pragmatic context could be used to helpmake the selection.
Experimenting with other com-binations of agent properties and behaviours can im-prove our knowledge of the relative utility of differ-ent mechanisms for selecting non-verbal behaviour.ReferencesR.
Artstein and M. Poesio.
2005.
Kappa3 = alpha (or beta).Technical Report CSM-437, University of Essex Departmentof Computer Science.J.
B. Bavelas and N. Chovil.
2000.
Visible acts of mean-ing: An integrated message model of language in face-to-face dialogue.
Journal of Language and Social Psychology,19(2):163?194.
doi:10.1177/0261927X00019002001.A.
Belz and S. Varges, editors.
2005.
Corpus Linguistics 2005Workshop on Using Corpora for Natural Language Genera-tion.
http://www.itri.brighton.ac.uk/ucnlg/ucnlg05/.T.
Bickmore and J. Cassell.
2005.
Social dialogue with em-bodied conversational agents.
In J. van Kuppevelt, L. Dy-bkj?r, and N. Bernsen, editors, Advances in Natural, Multi-modal Dialogue Systems.
Kluwer, New York.
doi:10.1007/1-4020-3933-6_2.J.
Cassell, T. Bickmore, H. Vilhj?lmsson, and H. Yan.
2001a.More than just a pretty face: Conversational protocols andthe affordances of embodiment.
Knowledge-Based Systems,14(1?2):55?64.
doi:10.1016/S0950-7051(00)00102-7.J.
Cassell, Y. Nakano, T. W. Bickmore, C. L. Sidner, andC.
Rich.
2001b.
Non-verbal cues for discourse structure.
InProceedings of the 39th Annual Meeting of the Associationfor Computational Linguistics (ACL 2001).
ACL AnthologyP01-1016.R.
A. J. Clark, K. Richmond, and S. King.
2004.
Festival 2 ?build your own general purpose unit selection speech synthe-siser.
In Proceedings of the 5th ISCA Workshop on SpeechSynthesis.D.
DeCarlo, M. Stone, C. Revilla, and J. Venditti.
2004.
Spec-ifying and animating facial signals for discourse in embod-ied conversational agents.
Computer Animation and VirtualWorlds, 15(1):27?38.
doi:10.1002/cav.5.M.
E. Foster.
2007.
Associating facial displays with syntacticconstituents for generation.
In Proceedings of the ACL 2007Workshop on Linguistic Annotation (The LAW).M.
E. Foster and J. Oberlander.
2007.
Corpus-based generationof conversational facial displays.
In submission.M.
E. Foster andM.White.
2005.
Assessing the impact of adap-tive generation in the COMIC multimodal dialogue system.In Proceedings of the IJCAI 2005 Workshop on Knowledgeand Reasoning in Practical Dialogue Systems.M.
E. Foster, M. White, A. Setzer, and R. Catizone.
2005.
Mul-timodal generation in the COMIC dialogue system.
In Pro-ceedings of the ACL 2005 Demo Session.
ACL AnthologyW06-1403.M.
Kipp.
2004.
Gesture Generation by Imitation - From Hu-man Behavior to Computer Character Animation.
Disserta-tion.com.E.
Marsi and F. van Rooden.
2007.
Expressing uncertainty witha talking head.
In Proceedings of the Workshop on Multi-modal Output Generation (MOG 2007).I.
Poggi and C. Pelachaud.
2000.
Performative facial expres-sions in animated faces.
In J. Cassell, J. Sullivan, S. Prevost,and E. Churchill, editors, Embodied Conversational Agents,pages 154?188.
MIT Press.E.
Reiter and R. Dale.
2000.
Building Natural Language Gen-eration Systems.
Cambridge University Press.
doi:10.2277/052102451X.M.
Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Lees, A. Stere,and C. Bregler.
2004.
Speaking with hands: Creatinganimated conversational characters from recordings of hu-man performance.
ACM Transactions on Graphics (TOG),23(3):506?513.
doi:10.1145/1015706.1015753.M.
White.
2006.
Efficient realization of coordinate struc-tures in Combinatory Categorial Grammar.
Research onLanguage and Computation, 4(1):39?75.
doi:10.1007/s11168-006-9010-2.M.
White, M. E. Foster, J. Oberlander, and A.
Brown.
2005.Using facial feedback to enhance turn-taking in a multimodaldialogue system.
In Proceedings of HCI International 2005.8
