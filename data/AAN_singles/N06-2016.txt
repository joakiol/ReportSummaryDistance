Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 61?64,New York, June 2006. c?2006 Association for Computational LinguisticsInvestigating Cross-Language Speech Retrieval for aSpontaneous Conversational Speech CollectionDiana Inkpen, Muath Alzghool Gareth J.F.
Jones Douglas W. OardSchool of Info.
Technology and Eng.
School of Computing College of Info.
Studies/UMIACSUniversity of Ottawa Dublin City University University of MarylandOttawa, Ontario, Canada, K1N 6N5 Dublin 9, Ireland College Park, MD 20742, USA{diana,alzghool}@site.uottawa.ca Gareth.Jones@computing.dcu.ie oard@umd.eduAbstractCross-language retrieval of spontaneousspeech combines the challenges of workingwith noisy automated transcription and lan-guage translation.
The CLEF 2005 Cross-Language Speech Retrieval (CL-SR) taskprovides a standard test collection to inves-tigate these challenges.
We show that wecan improve retrieval performance: by care-ful selection of the term weighting scheme;by decomposing automated transcripts intophonetic substrings to help ameliorate tran-scription errors; and by combining auto-matic transcriptions with manually-assignedmetadata.
We further show that topic trans-lation with online machine translation re-sources yields effective CL-SR.1 IntroductionThe emergence of large collections of digitizedspoken data has encouraged research in speech re-trieval.
Previous studies, notably those at TREC(Garafolo et al 2000), have focused mainly onwell-structured news documents.
In this paper wereport on work carried out for the Cross-LanguageEvaluation Forum (CLEF) 2005 Cross-LanguageSpeech Retrieval (CL-SR) track (White et al 2005).The document collection for the CL-SR task is apart of the oral testimonies collected by the USCShoah Foundation Institute for Visual History andEducation (VHI) for which some Automatic SpeechRecognition (ASR) transcriptions are available(Oard et al, 2004).
The data is conversional spon-taneous speech lacking clear topic boundaries; it isthus a more challenging speech retrieval task thanthose explored previously.
The CLEF data is alsoannotated with a range of automatic and manuallygenerated sets of metadata.
While the complete VHIdataset contains interviews in many languages, theCLEF 2005 CL-SR task focuses on English speech.Cross-language searching is evaluated by makingthe topic statements (from which queries are auto-matically formed) available in several languages.This task raises many interesting research ques-tions; in this paper we explore alternative termweighting methods and content indexing strategies.The remainder of this paper is structured as fol-lows: Section 2 briefly reviews details of the CLEF2005 CL-SR task; Section 3 describes the systemwe used to investigate this task; Section 4 reportsour experimental results; and Section 5 gives con-clusions and details for our ongoing work.2 Task descriptionThe CLEF-2005 CL-SR collection includes 8,104manually-determined topically-coherent segmentsfrom 272 interviews with Holocaust survivors, wit-nesses and rescuers, totaling 589 hours of speech.Two ASR transcripts are available for this data, inthis work we use transcripts provided by IBM Re-search in 2004 for which a mean word error rate of38% was computed on held out data.
Additional,metadata fields for each segment include: two setsof 20 automatically assigned thesaurus terms fromdifferent kNN classifiers (AK1 and AK2), an aver-age of 5 manually-assigned thesaurus terms (MK),and a 3-sentence summary written by a subject mat-ter expert.
A set of 38 training topics and 25 testtopics were generated in English from actual userrequests.
Topics were structured as Title, Descrip-tion and Narrative fields, which correspond roughlyto a 2-3 word Web query, what someone might firstsay to a librarian, and what that librarian might ul-timately understand after a brief reference inter-view.
To support CL-SR experiments the topicswere re-expressed in Czech, German, French, andSpanish by native speakers in a manner reflecting61the way questions would be posed in those lan-guages.
Relevance judgments were manually gener-ated using by augmenting an interactive search-guided procedure and purposive sampling designedto identify additional relevant segments.
See (Oardet al 2004) and (White et al 2005) for details.3 System OverviewOur Information Retrieval (IR) system was builtwith off-the-shelf components.
Topics were trans-lated from French, Spanish, and German into Eng-lish using seven free online machine translation(MT) tools.
Their output was merged in order toallow for variety in lexical choices.
All the transla-tions of a topic Title field were combined in amerged Title field of the translated topics; the sameprocedure was adopted for the Description and Nar-rative fields.
Czech language topics were translatedusing InterTrans, the only web-based MT systemavailable to us for this language pair.
Retrieval wascarried out using the SMART IR system (Buckleyet al 1993) applying its standard stop word list andstemming algorithm.In system development using the training topics wetested SMART with many different term weightingschemes combining collection frequency, documentfrequency and length normalization for the indexedcollection and topics (Salton and Buckley, 1988).
Inthis paper we employ the notation used in SMARTto describe the combined schemes: xxx.xxx.
Thefirst three characters refer to the weighting schemeused to index the document collection and the lastthree characters refer to the weighting scheme usedto index the topic fields.
For example, lpc.atc meansthat lpc was used for documents and atc for queries.lpc would apply log term frequency weighting (l)and probabilistic collection frequency weighting (p)with cosine normalization to the document collec-tion (c).
atc would apply augmented normalizedterm frequency (a), inverse document frequencyweight (t) with cosine normalization (c).One scheme in particular (mpc.ntn) proved tohave much better performance than other combina-tions.
For weighting document terms we used termfrequency normalized by the maximum value (m)and probabilistic collection frequency weighting (p)with cosine normalization (c).
For topics we usednon-normalized term frequency (n) and inversedocument frequency weighting (t) without vectornormalization (n).
This combination worked verywell when all the fields of the query were used; italso worked well with Title plus Description, butslightly less well with the Title field alone.4 Experimental InvestigationIn this section we report results from our experi-mental investigation of the CLEF 2005 CL-SR task.For each set of experiments we report Mean unin-terpolated Average Precision (MAP) computed us-ing the trec_eval script.
The topic fields used areindicated as: T for title only, TD for title + descrip-tion, TDN for title + description + narrative.
Thefirst experiment shows results for different termweighting schemes; we then give cross-languageretrieval results.
For both sets of experiments,?documents?
are represented by combining theASR transcription with the AK1 and AK2 fields.Thus each document representation is generatedcompletely automatically.
Later experiments ex-plore two alternative indexing strategies.4.1 Comparison of Term Weighting SchemesThe CLEF 2005 CL-SR collection is quite small byIR standards, and it is well known that collectionsize matters when selecting term weighting schemes(Salton and Buckley, 1988).
Moreover, the docu-ments in this case are relatively short, averagingabout 500 words (about 4 minutes of speech), andthat factor may affect the optimal choice of weight-ing schemes as well.
We therefore used the trainingtopics to explore the space of available SMARTterm weighting schemes.
Table 1 presents resultsfor various weighting schemes with  English topics.There are 3,600 possible combinations of weightingschemes available: 60 schemes (5 x 4 x 3) fordocuments and 60 for queries.
We tested a total of240 combinations.
In Table 1 we present the resultsfor 15 combinations (the best ones, plus some oth-ers to illustate  the diversity of the results).
mpc.ntnis still the best for the test topic set; but, as shown, afew other weighting schemes achieve similar per-formance.
Some of the weighting schemes performbetter when indexing all the topic fields (TDN),some on TD, and some on title only (T).
npn.ntnwas best for TD and lsn.ntn and lsn.atn are best forT.
The mpc.ntn weighting scheme is used for allother experiments in this section.
We are investi-gating the reasons for the effectiveness of thisweighting scheme in our experiments.62TDN TD T  Weightingscheme Map Map Map1 Mpc.mts 0.2175 0.1651 0.11752 Mpc.nts 0.2175 0.1651 0.11753 Mpc.ntn  0.2176 0.1653 0.11744 npc.ntn 0.2176 0.1653 0.11745 Mpc.mtc 0.2176 0.1653 0.11746 Mpc.ntc 0.2176 0.1653 0.11747 Mpc.mtn 0.2176 0.1653 0.11748 Npn.ntn 0.2116 0.1681 0.11819 lsn.ntn 0.1195 0.1233 0.122710 lsn.atn 0.0919 0.1115 0.122711 asn.ntn 0.0912 0.0923 0.106212 snn.ntn 0.0693 0.0592 0.072913 sps.ntn 0.0349 0.0377 0.038314 nps.ntn 0.0517 0.0416 0.047415 Mtc.atc 0.1138 0.1151 0.1108Table 1.
MAP, 25 English test topics.
Bold=best scores.4.2 Cross-Language ExperimentsTable 2 shows our results for the merged ASR,AK1 and AK2 documents with multi-system topictranslations for French, German and Spanish, andsingle-system Czech translation.
We can see thatSpanish topics perform well compared to monolin-gual English.
However, results for German andCzech are much poorer.
This is perhaps not surpris-ing for the Czech topics where only a single transla-tion is available.
For German, the quality oftranslation was sometimes low and some Germanwords were retained untranslated.
For French, onlyTD topic fields were available.
In this case we cansee that cross-language retrieval effectiveness isalmost identical to monolingual English.
Every re-search team participating in the CLEF 2005 CL-SRtask submitted at least one TD English run, andamong those our mpc.ntn system yielded the bestMAP (Wilcoxon signed rank test for paired sam-ples, p<0.05).
However, as we show in Table 4,manual metadata can yield better retrieval effec-tiveness than automatic description.TopicLanguageSystem Map FieldsEnglish Our system 0.1653 TDEnglish Our system 0.2176 TDNSpanish Our system 0.1863 TDNFrench Our system 0.1685 TDGerman Our system 0.1281 TDNCzech Our system 0.1166 TDNTable 2.
MAP, cross-language, 25 test topicsLanguage Map Fields DescriptionEnglish 0.1276 T PhoneticEnglish 0.2550 TD PhoneticEnglish 0.1245 T Phonetic+TextEnglish 0.2590 TD Phonetic+TextSpanish 0.1395 T PhoneticSpanish 0.2653 TD PhoneticSpanish 0.1443 T Phonetic+TextSpanish 0.2669 TD Phonetic+TextFrench 0.1251 T PhoneticFrench 0.2726 TD PhoneticFrench 0.1254 T Phonetic+TextFrench 0.2833 TD Phonetic+TextGerman 0.1163 T PhoneticGerman 0.2356 TD PhoneticGerman 0.1187 T Phonetic+TextGerman 0.2324 TD Phonetic+TextCzech 0.0776 T PhoneticCzech 0.1647 TD PhoneticCzech 0.0805 T Phonetic+TextCzech 0.1695 TD Phonetic+TextTable 3.
MAP, phonetic 4-grams, 25 test topics.4.3 Results on Phonetic TranscriptionsIn Table 3 we present results for an experimentwhere the text of the collection and topics, withoutstemming, is transformed into a phonetic transcrip-tion.
Consecutive phones are then grouped intooverlapping n-gram sequences (groups of n sounds,n=4 in our case) that we used for indexing.
Thephonetic n-grams were provided by Clarke (2005),using NIST?s text-to-phone tool1.
For example, thephonetic form for the query fragment child survi-vors is: ch_ay_l_d s_ax_r_v ax_r_v_ay r_v_ay_vv_ay_v_ax ay_v_ax_r v_ax_r_z.The phonetic form helps compensate for thespeech recognition errors.
With TD queries, the re-sults improve substantially compared with the textform of the documents and queries (9% relative).Combining phonetic and text forms (by simply in-dexing both phonetic n-grams and text) yields littleadditional improvement.4.4 Manual summaries and keywordsManually prepared transcripts are not availablefor this test collection, so we chose to use manuallyassigned metadata as a reference condition.
To ex-plore the effect of merging automatic and manualfields, Table 4 presents the results combining man-1 http://www.nist.gov/speech/tools/63ual keywords and manual summaries with ASRtranscripts, AK1, and AK2.
Retrieval effectivenessincreased substantially for all topic languages.
TheMAP score improved with 25% relative when add-ing the manual metadata for English TDN.Table 4 also shows comparative results betweenand our results and results reported by the Univer-sity of Maryland at CLEF 2005 using a widely usedIR system (InQuery) that has a standard termweighting algorithm optimized for large collections.For English TD, our system is 6% (relative) betterand for French TD 10% (relative) better.
The Uni-versity of Maryland results with only automatedfields are also lower than the results we report inTable 2 for the same fields.Table 4.
MAP, indexing all fields (MK, summaries,ASR transcripts, AK1 and AK2), 25 test topics.Language System Map FieldsEnglish Our system 0.4647 TDNEnglish Our system 0.3689 TDEnglish InQuery 0.3129 TDEnglish Our system 0.2861 TSpanish Our system 0.3811 TDNFrench Our system 0.3496 TDFrench InQuery 0.2480 TDFrench Our system 0.3496 TDGerman Our system 0.2513 TDNCzech Our system 0.2338 TDN5 Conclusions and Further InvestigationThe system described in this paper obtained the bestresults among the seven teams that participated inthe CLEF 2005 CL-SR track.
We believe that thisresults from our use of the 38 training topics to finda term weighting scheme that is particularly suitablefor this collection.
Relevance judgments are typi-cally not available for training until the second yearof an IR evaluation; using a search-guided processthat does not require system results to be availablebefore judgments can be performed made it possi-ble to accelerate that timetable in this case.
Table 2shows that performance varies markedly with thechoice of weighting scheme.
Indeed, some of theclassic weighting schemes yielded much poorerresults than the one  we ultimately selected.
In thispaper we presented results on the test queries, butwe observed similar effects on the training queries.On combined manual and automatic data, thebest MAP score we obtained for English topics is0.4647.
On automatic data, the best MAP is 0.2176.This difference could result from ASR errors orfrom terms added by human indexers that were notavailable to the ASR system to be recognized.
Infuture work we plan to investigate methods of re-moving or correcting some of the speech recogni-tion errors in the ASR transcripts using semanticcoherence measures.In ongoing further work we are exploring the re-lationship between properties of the collection andthe weighting schemes in order to better understandthe underlying reasons for the demonstrated effec-tiveness of the mpc.ntn weighting scheme.The challenges of CLEF CL-SR task will con-tinue to expand in subsequent years as new collec-tions are introduced (e.g., Czech interviews in2006).
Because manually assigned segment bounda-ries are available only for English interviews, thiswill yield an unknown topic boundary conditionthat is similar to previous experiments with auto-matically transcribed broadcast news the Text Re-trieval Conference (Garafolo et al 2000), but withthe additional caveat that topic boundaries are notknown for the ground truth relevance judgments.ReferencesChris Buckley, Gerard Salton, and James Allan.
1993.Automatic retrieval with locality information usingSMART.
In Proceedings of the First Text REtrievalConference (TREC-1), pages 59?72.Charles L. A. Clarke.
2005.
Waterloo Experiments forthe CLEF05 SDR Track, in Working Notes for theCLEF 2005 Workshop, Vienna, AustriaJohn S. Garofolo, Cedric G.P.
Auzanne and Ellen M.Voorhees.
2000.
The TREC Spoken Document Re-trieval Track: A Success Story.
In Proceedings of theRIAO Conference: Content-Based Multimedia Infor-mation Access, Paris, France, pages 1-20.Douglas W. Oard, Dagobert Soergel, David Doermann,Xiaoli Huang, G. Craig Murray, Jianqiang Wang,Bhuvana Ramabhadran, Martin Franz and SamuelGustman.
2004.
Building an Information RetrievalTest Collection for Spontaneous ConversationalSpeech, in  Proceedings of SIGIR, pages 41-48.Gerard Salton and Chris Buckley.
1988.
Term-weightingapproaches in automatic retrieval.
Information Proc-essing and Management, 24(5):513-523.Ryen W. White, Douglas W. Oard, Gareth J. F. Jones,Dagobert Soergel and Xiaoli Huang.
2005.
Overviewof the CLEF-2005 Cross-Language Speech RetrievalTrack, in Working Notes for the CLEF 2005 Work-shop, Vienna, Austria64
