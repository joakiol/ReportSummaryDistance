Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 18?26,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsDetection of Implicit Citations for Sentiment DetectionAwais Athar Simone TeufelComputer Laboratory, University of Cambridge15 JJ Thomson Avenue, Cambridge CB3 0FD, UK{awais.athar,simone.teufel}@cl.cam.ac.ukAbstractSentiment analysis of citations in scientific pa-pers is a new and interesting problem whichcan open up many exciting new applications inbibliometrics.
Current research assumes thatusing just the citation sentence is enough fordetecting sentiment.
In this paper, we showthat this approach misses much of the exist-ing sentiment.
We present a new corpus inwhich all mentions of a cited paper have beenannotated.
We explore methods to automat-ically identify these mentions and show thatthe inclusion of implicit citations in citationsentiment analysis improves the quality of theoverall sentiment assignment.1 IntroductionThe idea of using citations as a source of informationhas been explored extensively in the field of biblio-metrics, and more recently in the field of compu-tational linguistics.
State-of-the-art citations iden-tification mechanisms focus either on detecting ex-plicit citations i.e.
those that consist of either theauthor names and the year of publication or brack-eted numbers only, or include a small sentence win-dow around the explicit citation as input text (Coun-cill et al, 2008; Radev et al, 2009; Ritchie et al,2008).
The assumption behind this approach is thatall related mentions of the paper would be concen-trated in the immediate vicinity of the anchor text.However, this assumption does not generally holdtrue (Teufel, 2010; Sugiyama et al, 2010).
The phe-nomenon of trying to determine a citations?s cita-tion context has a long tradition in library sciences(O?Connor, 1982), and its connection with corefer-ence has been duely noted (Kim et al, 2006; Kaplanet al, 2009).
Consider Figure 1, which illustrates atypical case.Figure 1: Example of the use of anaphoraWhile the first sentence cites the target paper ex-plicitly using the name of the primary author alongwith the year of publication of the paper, the re-maining sentences mentioning the same paper ap-pear after a gap and contain an indirect and implicitreference to that paper.
These mentions occur twosentences after the formal citation in the form ofanaphoric it and the lexical hook METEOR.
Mostcurrent techniques, with the exception of Qazvinianand Radev (2010), are not able to detect linguisticmentions of citations in such forms.
Ignoring suchmentions and examining only the sentences contain-18ing an explicit citation results in loss of informationabout the cited paper.
While this phenomenon isproblematic for applications like scientific summari-sation (Abu-Jbara and Radev, 2011), it has a particu-lar relevance for citation sentiment detection (Athar,2011).Citation sentiment detection is an attractive task.Availability of citation polarity information can helpresearchers in understanding the evolution of a fieldon the basis of research papers and their critiques.It can also help expert researchers who are in theprocess of preparing opinion based summaries forsurvey papers by providing them with motivationsbehind as well as positive and negative commentsabout different approaches (Qazvinian and Radev,2008).Current work on citation sentiment detectionworks under the assumption that the sentimentpresent in the citation sentence represents the truesentiment of the author towards the cited paper(Athar, 2011; Piao et al, 2007; Pham and Hoffmann,2004).
This assumption is so dominant becausecurrent citation identification methods (Councill etal., 2008; Ritchie et al, 2008; Radev et al, 2009)can readily identify the citation sentence, whereasit is much harder to determine the relevant context.However, this assumption most certainly does nothold true when the citation context spans more thanone sentence.Concerning the sentiment aspect of the citationcontext from Figure 1, we see that the citation sen-tence does not contain any sentiment towards thecited paper, whereas the following sentences act asa critique and list its shortcomings.
It is clear thatcriticism is the intended sentiment, but if the goldstandard is defined by looking at the citation sen-tence in isolation, a significant amount of sentimentexpressed in the text is lost.
Given that overall mostcitations in a text are neutral with respect to sen-timent (Spiegel-Rosing, 1977; Teufel et al, 2006),this makes it even more important to recover whatexplicit sentiment there is in the article, wherever itis to be found.In this paper, we examine methods to extract allopinionated sentences from research papers whichmention a given paper in as many forms as we canidentify, not just as explicit citations.
We presenta new corpus in which all mentions of a cited paperhave been manually annotated, and show that our an-notation treatment increases citation sentiment cov-erage, particularly for negative sentiment.
We thenexplore methods to automatically identify all men-tions of a paper in a supervised manner.
In par-ticular, we consider the recognition of named ap-proaches and acronyms.
Our overall system thenclassifies explicit and implicit mentions according tosentiment.
The results support the claim that includ-ing implicit citations in citation sentiment analysisimproves the quality of the overall sentiment assign-ment.2 Corpus ConstructionWe use the dataset from Athar (2011) as our startingpoint, which consists of 8,736 citations in the ACLAnthology (Bird et al, 2008) that cite a target set of310 ACL Anthology papers.
The citation summarydata from the ACL Anthology Network1 (Radev etal., 2009) is used.
This dataset is rather large, andsince manual annotation of context for each citationis a time consuming task, a subset of 20 target pa-pers (i.e., all citations to these) has been selectedfor annotation.
These 20 papers correspond to ap-proximately 20% of incoming citations in the orig-inal dataset.
They contain a total of 1,555 citationsfrom 854 citing papers.2.1 AnnotationWe use a four-class scheme for annotation.
Everysentence which does not contain any direct or indi-rect mention of the citation is labelled as being ex-cluded (x) from the context.
The rest of the sen-tences are marked either positive (p), negative (n)or objective/neutral (o).
To speed up the annotationprocess, we developed a customised annotation tool.A total of 203,803 sentences have been annotatedfrom 1,034 paper?reference pairs.
Although this an-notation been performed by the first author only,we know from previous work that similar stylesof annotation can achieve acceptable inter-annotatoragreement (Teufel et al, 2006).
An example anno-tation is given in Figure 2, where the first columnshows the line number and the second one showsthe class label for the citation to Smadja (1993).
Itshould be noted that since annotation is always per-1http://www.aclweb.org19formed for a specific citation only, sentences such asthe one at line 32, which carry sentiment but refer toa different citation, are marked as excluded from thecontext.If there are multiple sentiments in the same sen-tence, the sentence has been labelled with the classof the last sentiment mentioned.
In this way, a totalof 3,760 citation sentences have been found in thewhole corpus, i.e.
sentences belonging to class o, nor p, and the rest have been labelled as x.
Table 1compares the number of sentences with only the ex-plicit citations with all explicit and implicit mentionsof those citations.
We can see that including thecitation context increases the subjective sentimentby almost 185%.
The resulting negative sentimentalso increases by more than 325%.
This may be at-tributed to the strategic behaviour of the authors of?sweetening?
the criticism in order to soften its ef-fects among their peers (Hornsey et al, 2008).Figure 2: Example annotation of a citation context.Explicit mentions All mentionso 1, 509 3, 100n 86 368p 146 292Table 1: Distribution of classes.Another view of the annotated data is available inFigure 3a.
This is in the form of interactive HTMLwhere each HTML page represents all the incominglinks to a paper.
Each row represents the citing pa-per and each column square represents a sentence.The rows are sorted by increasing publication date.Black squares are citations with the author name andyear of publication mentioned in the text.
The red,green and gray squares show negative, positive andneutral sentiment respectively.
Pointing the mousecursor at any square gives the text content of the cor-responding sentence, as shown in the Figure 3a.The ACL Id, paper title and authors?
names arealso given at the top of the page.
Similar data for thecorresponding citing paper is made available whenthe mouse cursor is positioned on one of the orangesquares at the start of each row, as shown in the Fig-ure 3b.
Clicking on the checkboxes at the top hidesor shows the corresponding type of squares.
There isalso an option to hide/show a grid so that the squaresare separated and rows are easier to trace.
For ex-ample, Figure 3b shows the grid with the neutral orobjective citations hidden.In the next section, we describe the features set weuse to detect implicit citations from this annotatedcorpus and discuss the results.3 Experiments and ResultsFor the task of detecting all mentions of a citation,we merge the class labels of sentences mentioning acitation in any form (o n p).
To make sure that theeasily detectable explicit citations do not influencethe results, we change the class label of all thosesentences to x which contain the first author?s namewithin a 4-word window of the year of publication.Our dataset is skewed as there are many more ob-jective sentences than subjective ones.
In such sce-narios, average micro-F scores tend to be slightlyhigher as they are a weighted measure.
To avoidthis bias, we also report the macro-F scores.
Fur-thermore, to ensure there is enough data for trainingeach class, we use 10-fold cross-validation (Lewis,1991) in all our experiments.We represent each citation as a feature set in aSupport Vector Machine (SVM) (Cortes and Vapnik,1995) framework.
The corpus is processed usingWEKA (Hall et al, 2008) and the Weka LibSVMlibrary (EL-Manzalawy and Honavar, 2005; Changand Lin, 2001).
For each ith sentence Si, we use thefollowing binary features.?
Si?1 contains the last name of the primary au-thor, followed by the year of publication withina four-word window.20(a) Sentence Text (b) Paper metadataFigure 3: Different views of an annotated paper.This feature is meant to capture the fact thatthe sentence immediately after an explicit cita-tion is more likely to continue talking about thesame work.?
Si contains the last name of the primary au-thor followed by the year of publication withina four-word window.This feature should help in identifying sen-tences containing explicit citations.
Since suchsentences are easier to identify, including themin the evaluation metric would result in a falseboost in the final score.
We have thus excludedall such sentences in our annotation and thisfeature should indicate a negative instance tothe classifier.?
Si contains the last name of the primary au-thor.This feature captures sentences which containa reference to tools and algorithms which havebeen named after their inventors, such as,?One possible direction for future work is tocompare the search-based approach of Collinsand Roark with our DP-based approach.
?It should also capture the mentions of methodsand techniques used in the cited paper e.g.,?We show that our approach outperforms Tur-ney?s approach.??
Si contains an acronym used in an explicit ci-tation.Acronyms are taken to be capitalised wordswhich are extracted from the vicinity of thecited author?s last name using regular expres-sions.
For example, METEOR in Figure 1 is anacronym which is used in place of a formal ci-tation to refer to the original paper in the rest ofthe citing paper.?
Si contains a determiner followed by a worknoun.We use the following determiners D = {the,this, that, those, these, his, her, their, such, pre-vious, other}.
The list of work nouns (tech-nique, method, etc.)
has been taken from Teufel(2010).
This feature extracts a pattern whichhas been found to be useful for extracting cita-tions in previous work (Qazvinian and Radev,2010).
Such phrases usually signal a continua-tion of the topics related to citations in earliersentences.
For example:?Church et al(1989), Wettler & Rapp (1989)and Church & Hanks (1990) describe algo-rithms which do this.
However, the validity ofthese algorithms has not been tested by system-atic comparisons with associations of humansubjects.??
Si starts with a third person pronoun.The feature also tries to capture the topic con-tinuation after a citation.
Sentences startingwith a pronoun (e.g.
they, their, he, she, etc.
)are more likely to describe the subject citationof the previous sentence in detail.
For example:21?Because Daume III (2007) views the adapta-tion as merely augmenting the feature space,each of his features has the same prior meanand variance, regardless of whether it is do-main specific or independent.
He could haveset these parameters differently, but he did not.??
Si starts with a connector.This feature also focuses on detecting the topiccontinuity.
Connectors have been shown tobe effective in other context related works aswell (Hatzivassiloglou and McKeown, 1997;Polanyi and Zaenen, 2006).
A list of 23 con-nectors (e.g.
however, although, moreover, etc.
)has been compiled by examining the high fre-quency connectors from a separate set of papersfrom the same domain.
An example is:?An additional consistent edge of a linear-chain conditional random field (CRF) explicitlymodels the dependencies between distant oc-currences of similar words (Sutton and McCal-lum, 2004; Finkel et al , 2005).
However, thisapproach requires additional time complexityin inference/learning time and it is only suit-able for representing constraints by enforcinglabel consistency.??
Si starts with a (sub)section heading.?
Si?1 starts with a (sub)section heading.?
Si+1 starts with a (sub)section heading.The three features above are a consequence ofmissing information about the paragraph andsection boundaries in the used corpus.
Sincethe text extraction has been done automatically,the section headings are usually found to bemerged with the text of the succeeding sen-tence.
For example, the text below merges theheading of section 4.2 with the next sentence.
?4.2 METEOR vs. SIA SIA is designed to takethe advantage of loose sequence-based metricswithout losing word-level information.
?Start and end of such section boundaries cangive us important information about the scopeof a citation.
In order to exploit this informa-tion, we use regular expressions to detect if thesentences under review contains these mergedsection titles and headings.?
Si contains a citation other than the one underreview.It is more probable for the context of a citationto end when other citations are mentioned ina sentence, which is the motivation behind us-ing this feature, which might contribute to thediscriminating power of the classifier in con-junction with the presence of a citation in theprevious sentence.
For example, in the extractbelow, the scope of the first citation is limitedto the first sentence only.
?Blitzer et al(2006) proposed a structuralcorrespondence learning method for domainadaptation and applied it to part-of-speech tag-ging.
Daume III (2007) proposed a simple fea-ture augmentation method to achieve domainadaptation.??
Si contains a lexical hook.The lexical hooks feature identifies lexical sub-stitutes for the citations.
We obtain these hooksby examining all explicit citation sentences tothe cited paper and selecting the most frequentcapitalized phrase in the vicinity of the author?slast name.
The explicit citations come from allciting papers and not just the paper for whichthe features are being determined.
For exam-ple, the sentences below have been taken fromtwo different papers and cite the same target pa-per (Cutting et al, 1992).
While the acronymHMM will be captured by the feature stated ear-lier, the word Xerox will be missed.E95-1014: ?This text was part-of-speechtagged using the Xerox HMM tagger (Cuttinget al , 1992).
?J97-3003: ?The Xerox tagger (Cutting et al1992) comes with a set of rules that assign anunknown word a set of possible pos-tags (i.e.
,POS-class) on the basis of its ending segment.
?This ?domain level?
feature makes it possibleto extract the commonly used name for a tech-nique which may have been missed by theacronym feature due to long term dependen-cies.
We also extrapolate the acronym for such22phrases, e.g., in the example below, SCL wouldalso be checked along with Structural Corre-spondence Learning.
?The paper compares Structural Correspon-dence Learning (Blitzer et al, 2006) with (var-ious instances of) self-training (Abney, 2007;McClosky et al, 2006) for the adaptation of aparse selection model to Wikipedia domains?We also add n-grams of length 1 to 3 to this lexi-cal feature set and compare the results obtained withan n-gram only baseline in Table 2.
N-grams havebeen shown to perform consistently well in variousNLP tasks (Bergsma et al, 2010).Class Baseline Our Systemx 0.995 0.996o n p 0.358 0.513Avg.
0.990 0.992Avg.
(macro) 0.677 0.754Table 2: Comparison of F -scores for non-explicitcitation detection.By adding the new features listed above, the per-formance of our system increases by almost 8% overthe n-gram baseline for the task of detecting citationmentions.
Using the pairwise Wilcoxon rank-sumtest at 0.05 significance level, we found that the dif-ference between the baseline and our system is sta-tistically significant2.
While the micro-F score ob-tained is quite high, the individual class scores showthat the task is hard and a better solution may requirea deeper analysis of the context.4 Impact on Citation Sentiment DetectionWe explore the effect of this context on citation sen-timent detection.
For a baseline, we use features ofthe state-of-the-art system proposed in our earlierwork (Athar, 2011).
While there we used n-gramand dependency feature on sentences containing ex-plicit citations only, our annotation is not restrictedto such citations and we may have more than one2While this test may not be adequate as the data is highlyskewed, we are reporting the results since there is no obviousalternative for discrete skewed data.
In future, we plan to usethe continuous probability estimates produced by the classifierfor testing significance.sentiment per each explicit citation.
For example,in Figure 2, our 2011 system will be restricted toanalysing sentence 33 only.
However, it is clearfrom our annotation that there is more sentimentpresent in the succeeding sentences which belongsto this explicit citation.
While sentence 34 in Fig-ure 2 is positive towards the cited paper, the nextsentence criticises it.
Thus for this explicit citation,there are three sentences with sentiment and all ofthem are related to the same explicit citation.
Treat-ing these sentences separately will result in an artifi-cial increase in the amount of data because they par-ticipate in the same discourse.
It would also makeit impossible to compare the sentiment annotated inthe previous work with our annotation.To make sure the annotations are comparable,we mark the true citation sentiment to be the lastsentiment mentioned in a 4-sentence context win-dow, as this is pragmatically most likely to be thereal intention (MacRoberts and MacRoberts, 1984).The window length is motivated by recent research(Qazvinian and Radev, 2010) which favours a four-sentence boundary for detecting non-explicit cita-tions.
Analysis of our data shows that more than60% of the subjective citations lie in this window.We include the implicit citations predicted by themethod described in the previous section in the con-text.
The results of the single-sentence baseline sys-tem are compared with this context enhanced systemin Table 3.Class Baseline Our Systemo 0.861 0.887n 0.138 0.621p 0.396 0.554Avg.
0.689 0.807Avg.
(macro) 0.465 0.687Table 3: F -scores for citation sentiment detection.The results show that our system outperforms thebaseline in all evaluation criteria.
Performing thepairwise Wilcoxon rank-sum testat 0.05 significancelevel, we found that the improvement is statisticallysignificant.
The baseline system does not use anycontext and thus misses out on all the sentimentinformation contained within.
While this window-based representation does not capture all the senti-23ment towards a citation perfectly, it is closer to thetruth than a system based on single sentence analysisand is able to detect more sentiment.5 Related WorkWhile different schemes have been proposed forannotating citations according to their function(Spiegel-Rosing, 1977; Nanba and Okumura, 1999;Garzone and Mercer, 2000), the only recent work oncitation sentiment detection using a relatively largecorpus is by Athar (2011).
However, this work doesnot handle citation context.
Other approaches to ci-tation classification include work by Wilbur et al(2006), who annotated a 101 sentence corpus onfocus, polarity, certainty, evidence and directional-ity.
Piao et al (2007) proposed a system to attachsentiment information to the citation links betweenbiomedical papers by using existing semantic lexicalresources and NLP tools.A common approach for sentiment detection is touse a labelled lexicon to score sentences (Hatzivas-siloglou and McKeown, 1997; Turney, 2002; Yu andHatzivassiloglou, 2003).
However, such approacheshave been found to be highly topic dependent (En-gstro?m, 2004; Gamon and Aue, 2005; Blitzer et al,2007), which makes the creation of a general senti-ment classifier a difficult task.Teufel et al (2006) worked on a 2,829 sentence ci-tation corpus using a 12-class classification scheme.While the authors did make use of the context intheir annotation, their focus was on the task of deter-mining the author?s reason for citing a given paper.This task differs from citation sentiment detection,which is in a sense a ?lower level?
of analysis.Some other recent work has focused on the prob-lem of implicit citation extraction (Kaplan et al,2009; Qazvinian and Radev, 2010).
Kaplan et al(2009) explore co-reference chains for citation ex-traction using a combination of co-reference resolu-tion techniques (Soon et al, 2001; Ng and Cardie,2002).
However, the corpus that they use consists ofonly 94 citations to 4 papers and is likely to be toosmall to be representative.For citation extraction, the most relevant work isby Qazvinian and Radev (2010) who proposed aframework of Markov Random Fields to extract onlythe non-explicit citations for a given paper.
Theymodel each sentence as a node in a graph and ex-periment with various window boundaries to cre-ate edges between neighbouring nodes weighted bylexical similarity between nodes.
However, theirdataset consists of only 569 citations from 10 pa-pers and their annotation scheme deals with neitheracronyms nor sentiment.6 DiscussionWhat is the role of citation contexts in the overallstructure of scientific context?
We assume a hier-archical, rhetorical structure not unlike RST (Mannand Thompson, 1987), but much flatter, where theatomic units are textual blocks which carry a cer-tain functional role in the overall scientific argumentfor publication (Teufel, 2010; Hyland, 2000).
Undersuch a general model, citation blocks are certainlya functional unit, and their recognition is a reward-ing task in their own right.
If citation blocks can berecognised along with their sentiment, this is evenmore useful, as it restricts the possibilities for whichrhetorical function the segment plays.
For instance,in the motivation section of a paper, before the pa-per contribution is introduced, we often find nega-tive sentiment assigned to citations, as any indica-tion can serve as a justification for the current paper.In contrast, positive sentiment is more likely to berestricted to the description of an approach whichthe authors include in their solution, or further de-velop.Another aspect concerns which features mighthelp in detecting coherent citation blocks.
We havehere addressed coherence of citation contexts viacertain referring expressions, lexical hooks and alsocoherence-indicating conjunctions (amongst oth-ers).
The reintroduction of citation contexts wasaddressed via lexical hooks.
Much more could bedone to explore this very interesting question.
Amore fine-grained model of coherence might includeproper anaphora resolution (Lee et al, 2011), whichis still an unsolved task for scientific texts, and alsoinclude models of lexical coherence such as lexicalchains (Barzilay and Elhadad, 1997) and entity co-herence (Barzilay and Lapata, 2008).247 ConclusionIn this paper, we focus on automatic detection of ci-tation sentiment using citation context.
We annotatea new large corpus and show that ignoring the cita-tion context would result in loss of a lot of sentiment,specially criticism.
We also report the results of thestate-of-the-art citation sentiment detection systemson this corpus and when using this context-enhancedgold standard definition.ReferencesA.
Abu-Jbara and D. Radev.
2011.
Coherent citation-based summarization of scientific papers.
In Proc.
ofACL.A.
Athar.
2011.
Sentiment analysis of citations usingsentence structure-based features.
In Proc of ACL,page 81.Regina Barzilay and Michael Elhadad.
1997.
Usinglexical chains for text summarization.
In InderjeetMani and Mark T. Maybury, editors, Proceedings ofthe ACL/EACL-97 Workshop on Intelligent ScalableText Summarization.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, (1):1?34.Shane Bergsma, Emily Pitler, and Dekang Lin.
2010.Creating robust supervised classifiers via web-scale n-gram data.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 865?874, Uppsala, Sweden, July.
Associationfor Computational Linguistics.S.
Bird, R. Dale, B.J.
Dorr, B. Gibson, M.T.
Joseph, M.Y.Kan, D. Lee, B. Powley, D.R.
Radev, and Y.F.
Tan.2008.
The acl anthology reference corpus: A ref-erence dataset for bibliographic research in computa-tional linguistics.
In Proc.
of LREC.J.
Blitzer, M. Dredze, and F. Pereira.
2007.
Biographies,bollywood, boom-boxes and blenders: Domain adap-tation for sentiment classification.
In Proc.
of ACL,number 1.C.C.
Chang and C.J.
Lin.
2001.
LIBSVM: a li-brary for support vector machines, 2001.
Softwareavailable at http://www.csie.ntu.edu.tw/cjlin/libsvm.C.
Cortes and V. Vapnik.
1995.
Support-vector networks.Machine learning, 20(3):273?297.I.G.
Councill, C.L.
Giles, and M.Y.
Kan. 2008.
Parscit:An open-source crf reference string parsing package.In Proc.
of LREC, volume 2008.
Citeseer.Y.
EL-Manzalawy and V. Honavar, 2005.
WLSVM:Integrating LibSVM into Weka Environment.
Soft-ware available at http://www.cs.iastate.edu/?yasser/wlsvm.C.
Engstro?m.
2004.
Topic dependence in sentiment clas-sification.
Unpublished MPhil Dissertation.
Univer-sity of Cambridge.M.
Gamon and A. Aue.
2005.
Automatic identifica-tion of sentiment vocabulary: exploiting low associa-tion with known sentiment terms.
In Proc.
of the ACL,pages 57?64.M.
Garzone and R. Mercer.
2000.
Towards an automatedcitation classifier.
Advances in Artificial Intelligence.D.
Hall, D. Jurafsky, and C.D.
Manning.
2008.
Studyingthe history of ideas using topic models.
In EMNLP,pages 363?371.V.
Hatzivassiloglou and K.R.
McKeown.
1997.
Predict-ing the semantic orientation of adjectives.
In Proc.
ofACL, page 181.M.J.
Hornsey, E. Robson, J. Smith, S. Esposo, and R.M.Sutton.
2008.
Sugaring the pill: Assessing rhetori-cal strategies designed to minimize defensive reactionsto group criticism.
Human Communication Research,34(1):70?98.Ken Hyland.
2000.
Disciplinary Discourses; Social In-teraction in Academic Writing.
Longman, Harlow.D.
Kaplan, R. Iida, and T. Tokunaga.
2009.
Automaticextraction of citation contexts for research paper sum-marization: A coreference-chain based approach.
InProc.
of the 2009 Workshop on Text and Citation Anal-ysis for Scholarly Digital Libraries.D.
Kim, P. Webber, et al 2006.
Implicit references tocitations: A study of astronomy papers.H.
Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-deanu, and D. Jurafsky.
2011.
Stanford?s multi-passsieve coreference resolution system at the conll-2011shared task.
ACL HLT 2011.D.D.
Lewis.
1991.
Evaluating text categorization.
InProc.
of Speech and Natural Language Workshop,pages 312?318.M.H.
MacRoberts and B.R.
MacRoberts.
1984.
Thenegational reference: Or the art of dissembling.
So-cial Studies of Science, 14(1):91?94.William C. Mann and Sandra A. Thompson.
1987.Rhetorical Structure Theory: A theory of text organ-isation.
ISI/RS-87-190.
Technical report, InformationSciences Institute, University of Southern California,Marina del Rey, CA.H.
Nanba and M. Okumura.
1999.
Towards multi-papersummarization using reference information.
In IJCAI,volume 16, pages 926?931.
Citeseer.V.
Ng and C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In Proc.
of ACL,pages 104?111.25J.
O?Connor.
1982.
Citing statements: Computer recog-nition and use to improve retrieval.
Information Pro-cessing & Management, 18(3):125?131.S.B.
Pham and A. Hoffmann.
2004.
Extracting positiveattributions from scientific papers.
In Discovery Sci-ence, pages 39?45.
Springer.S.
Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. Mc-Naught.
2007.
Mining opinion polarity relations of ci-tations.
In International Workshop on ComputationalSemantics (IWCS).
Citeseer.L.
Polanyi and A. Zaenen.
2006.
Contextual valenceshifters.
Computing attitude and affect in text: Theoryand applications, pages 1?10.V.
Qazvinian and D.R.
Radev.
2008.
Scientific papersummarization using citation summary networks.
InProceedings of the 22nd International Conference onComputational Linguistics-Volume 1, pages 689?696.Association for Computational Linguistics.V.
Qazvinian and D.R.
Radev.
2010.
Identifying non-explicit citing sentences for citation-based summariza-tion.
In Proc.
of ACL.D.R.
Radev, M.T.
Joseph, B. Gibson, and P. Muthukrish-nan.
2009.
A Bibliometric and Network Analysis ofthe field of Computational Linguistics.
Journal of theAmerican Soc.
for Info.
Sci.
and Tech.A.
Ritchie, S. Robertson, and S. Teufel.
2008.
Com-paring citation contexts for information retrieval.
InProc.
of ACM conference on Information and knowl-edge management, pages 213?222.
ACM.W.M.
Soon, H.T.
Ng, and D.C.Y.
Lim.
2001.
A ma-chine learning approach to coreference resolution ofnoun phrases.
Comp.
Ling., 27(4):521?544.I.
Spiegel-Rosing.
1977.
Science studies: Bibliometricand content analysis.
Social Studies of Science.K.
Sugiyama, T. Kumar, M.Y.
Kan, and R.C.
Tripathi.2010.
Identifying citing sentences in research papersusing supervised learning.
In Information Retrieval &Knowledge Management,(CAMP), 2010 InternationalConference on, pages 67?72.
IEEE.S.
Teufel, A. Siddharthan, and D. Tidhar.
2006.
Auto-matic classification of citation function.
In Proc.
ofEMNLP, pages 103?110.Simone Teufel.
2010.
The Structure of Scientific Arti-cles: Applications to Citation Indexing and Summa-rization.
Stanford: CSLI Publications.P.D.
Turney.
2002.
Thumbs up or thumbs down?
: seman-tic orientation applied to unsupervised classification ofreviews.
In Proc.
of ACL.W.J.
Wilbur, A. Rzhetsky, and H. Shatkay.
2006.
Newdirections in biomedical text annotation: definitions,guidelines and corpus construction.
BMC bioinfor-matics, 7(1):356.H.
Yu and V. Hatzivassiloglou.
2003.
Towards answeringopinion questions: Separating facts from opinions andidentifying the polarity of opinion sentences.
In Proc.of EMNLP, page 136.26
