The Acquis it ion and Appl ication of Context Sensitive Grammar forEnglishRobert F. Simmons and Yeong-Ho Yu @cs.texas.eduAbst rac tDepartment of Computer Sciences, AI  LabUniversity of Texas, Austin Tx 78712A system is described for acquiring a context-sensitive, phrase structure grammar which is applied bya best-path, bottom-up, deterministic parser.
The gram-mar was based on English news stories and a high degreeof success in parsing is reported.
Overall, this researchconcludes that CSG is a computationally and concep-tually tractable approach to the construction of phrasestructure grammar for news story text.
11 Introduct ionAlthough many papers report natural language process-ing systems based in part on syntactic analysis, their au-thors typically do not emphasize the complexity of theparsing and grammar acquisition processes that were in-volved.
The casual reader might suppose that parsing isa well understood, minor aspect in such research.
In fact,parsers for natural language are generally very compli-cated programs with complexity at best of O(n 3) wheren is the number of words in a sentence.
The gram-mars they usually use are technically, "augmented con-text free" where the simplicity of the context-free form isaugmented by feature tests, transformations, and occa-sionally arbitrary programs.
The combination of evenan efficient parser with such intricate grammars maygreatly increase the computational complexity of the sys-tem \[Tomita 1985\].
It is extremely difficult to writesuch grammars and they must frequently be revised tomaintain internal consistency when applied to new texts.In this paper we present an alternative approach usingcontext-sensitive grammar to enable preference parsingand rapid acquisition of CSG from example parsings ofnewspaper stories.Chomsky\[1957\] defined a hierarchy of grammars in-cluding context-free and context-sensitive ones.
For nat-ural language a grammar distinguishes terminal, singleelement constituents uch as parts of speech from non-terminals which are phrase-names such as NP, VP, AD-VPH, or SNT 2 signifying multiple constituents.1 This work was partially supported by the Army Research Officeunder contract DAAG29-84-K-0060.~NounPhrase, VerbPhrase, AdverbialPhrase, SentenceA context-free grammar production is characterizedas a rewrite rule where a non-terminal element as a left-side is rewritten as multiple symbols on the right.Snt -* NP + VPSuch rules may be augmented by constraints to limittheir application to relevant contexts.Snt --* NP + VP / anim(np),agree(nbr(np),nbr(vp))To the right of the slash mark, the constraints are appliedby an interpretive program and even arbitrary code maybe included; in this case the interpreter would recognizethat the NP must be animate and there must be agree-ment in number between the NP and the VP.
Since thisis such a flexible and expressive approach, its many vari-ations have found much use in application to natural an-guage applications and there is a broad literature on Aug-mented Phrase Structure Grammar \[Gazdar et.
al.
1985\],Unification Grammars of various types \[Shieber 1986\],and Augmented Transition Networks \[Allen, J.
1987, Sim-moils 1984\].In context-sensitive grammars, the productions arerestricted to rewrite rules of the form,uXv ---* uYvwhere u and v are context strings of terminals or nonter-minals, and X is a non-terminal and Y is a non-emptystring .
That is, the symbol X may be rewritten as asthe string Y in the context u - .
.v .
More generally, theright-hand side of a context-sensitive rule must containat least as many symbols as the left-hand side.Excepting Joshi's Tree Adjoining Grammars whichare shown to be "mildly context-sensitive," \[Joshi 1987\]context-sensitive grammars found little or no use amongnatural language processing (NLP) researchers until thereoccurrance of interest in Neural Network computa-tion.
One of the first suggestions of their potentialutility came from Sejnowski and Rosenberg's NETtalk\[1988\], where seven-character contexts were largely suf-ficient to map each character of a printed word intoits corresponding phoneme - -  where each character ac-tually maps in various contexts into several differentphonemes.
For accomplishing linguistic case analysesMcClelland and Kawamoto \[1986\] and Miikulainen and122Dyer \[1989\] used the entire context of phrases and sen-tences to map string contexts into case structures.
RobertAllen \[1987\] mapped nine-word sentences of English intoSpanish translations, and Yu and Simmons \[1990\] ac-complished context sensitive translations between Englishand German.
It was apparent that the contexts in whicha word occurred provided information to a neural net-work that was sufficient o select correct word sense andsyntactic structure for otherwise ambiguous usages of lan-guage.An explicit use of context-sensitive grammar was de-veloped by Simmons and Yu \[1990\] to solve the prob-lem of accepting indefinitely long, recursively embeddedstrings of language for training a neural network.
How-ever although the resulting neural network was trainedas a satisfactory grammar, there was a problem of scale-up.
Training the network for even 2000 rules took severaldays, and it was foreseen that the cost of training for10-20 thousand rules would be prohibitive.
This led us toinvestigate the hypothesis that storing a context-sensitivegrammar in a hash-table and accessing it using a scoringfunction to select the rule that best matched a sentencecontext would be a superior approach.In this paper we describe a series of experiments inacquiring context-sensitive grammars (CSG) from news-paper stories, and a deterministic parsing system thatuses a scoring function to select the best matching con-text sensitive rules from a hash-table.
We have accumu-lated 4000 rules from 92 sentences and found the resultingCSG to be remarkably accurate in computing exactly theparse structures that were preferred by the linguist whobased the grammar on his understanding of the text.
Weshow that the resulting grammar generalizes well to newtext and compresses to a fraction of the example trainingrules.2 Context-Sensit ive ParsingThe simplest form of parser applies two operations hiftor reduce to an input string and a stack.
A sequence ofelements on the stack may be reduced - -  rewritten as asingle symbol, or a new element may be shifted from theinput to the stack.
Whenever a reduce occurs, a subtreeof the parse is constructed, ominated by the new symboland placed on the stack.
The input and the stack mayboth be arbitrarily long, but the parser need only consultthe top elements of the stack and of the input.
The parseis complete when the input string is empty and the stackcontains only the root symbol of the parse tree.
Such asimple approach to parsing has been used frequently tointroduce methods of CFG parsing in texts on computeranalysis of natural anguage \[J. Allen 1987\], but it worksequally well with CSG.
In our application to phrase struc-ture analysis, we further constrain the reduce operationto refer to only the top two elements of the stack2.1 Phrase  S t ructure  Ana lys i s  w i th  CFGFor shift/reduce parsing, a phrase structure anMysis takesthe form of a sequence of states, each comprising a condi-tion of the stack and the input string.
The final state inthe parse is an empty input string and a stack containingonly the root symbol, SNT.
In an unambiguous analy-sis, each state is followed by exactly one other; thus eachstate can be viewed as the left-half of a CSG productionwhose right-half is the succeeding state.stacksinpu~ ~ ::?, s~ack,+ l inpu~,+ lNews story sentences, however, may be very long,sometimes exceeding fifty words and the resulting parsestates would make cumbersome rules of varying lengths.To obtain manageable rules we limit the stack and inputparts of the state to five symbols each, forming a ten sym-bol pattern for each state of the parse.
In the example ofFigure 1 we separate the stack and input parts with thesymbol "*", as we illustrate the basic idea on the sentence"The late launch from Alaska delayed interception."
Thesymbol b stands for blank, ax-1; for article, adj for adjec-tive, p for preposition, n for noun, and v for verb.
Thesyntactic lasses are assigned by dictionary lookup.The analysis terminates successfully with an emptyinput string and the single symbol "snt" on the stack.Note that the first four operations can be described asshifts followed by the two reductions, adj n --* np and artnp --, up.
Subsequently the p and n were shifted onto thestack and then reduced to a pp; then the np and pp onthe stack were reduced to an np, followed by the shiftingof v and n, their reduction to vp, and a final reduction ofnp vp ~ snt.The grammar could now be recorded as pairs of suc-cessive states as below:b b b np p*  nvn  bb- - *b  bnpp n*  vn  bbbb b np p n*  v nb  b b--~ b b b np pp*  v nbbbbut some economy can be achieved by summarizing theright-half of a rule as the operations, shift or reduce, thatproduce it from the left-half.
So for the example imme-diately above, we record:hbbnpp*nvnbb- -~(S)bbnp p n*  vn  b b b--* (Rpp)where S shifts and (R pp) replaces the top twoelements of the stack with pp to form the nextstate of the parse,Thus we create a windowed confexf of 10 symbols as theleft half of a rule and an operation as the right half.
Notethat if the stack were limited to the top two elements,and the input to a single element, the rule system wouldreduce to a CFG; thus this CSG embeds a CFG.123The l a te  launch from Alaskaar t  ads n p nde layed in tercept ion .V nb b b b b * ~t  ads n p nb b b b ~t  * adS n p n vb b b ~t  ads * n p n v nb b ~t  ads n * p n v n bb b b ~t  up*  p n v n bbbbbnp*pnvnbbbbnpp*nvnbbbbnppn*vnbbbb b b np pp * v n b b bbbbbnp*vnbbbbbbnpv*nbbbbbbnpvn*bbbbbbbbnp~*bbbbbb b b b snt  * b b b b bFigure 1: Successive Stack/Input States in a Parse2 .2  A lgor i thm for  Sh i f t /Reduce  ParserThe algorithm used by the Shift/Reduce parser is de-scribed in Figure 2.
Essentially, the algorithm shifts el-ements from the input onto the stack under the controlof the CSG productions.
It can be observed that unlikemost grammars which include only rules for reductions,this one has rules for recognizing shifts as well.
The re-ductions always apply to the top two elements of the stackand it is often the case that in one context a pair of stackelements lead to a shift, but in another context he samepair can be reduced.An essential aspect of this algorithm is to consultthe CFG to find the left-half of a rule that matches thesentence context.
The most important part of the rule isthe top two stack elements, but for any such pair theremay be multiple contexts leading to shifts or various re-ductions, so it is the other eight context elements thatdecide which rule is most applicable to the current stateof the parse.
Since many thousands of contexts can exist,an exact match cannot always be expected and there-fore a scoring function must be used to discover the bestmatching rule.C S .S  R -  P .
r se~( Input  ,Csg)Input  is a s t r in  K of  syntact i c  c lassesCs s i s  the  Kiven CSQ product ion  rules.S t , ck  :---~ emptydo  u=f i I ( Input  --.~ empty  ~md Steck  ~ (SNT) )Windowed-context  :---- Append(Top .
f i ve (s tack) ,F i r s t .
f i ve ( input ) )Operat ion  :---- Consu I t .CSG(Window-context ,Csg)i f  F i rs t (Oper~t lon)  = SHIFTthen  S tack  :=  Pnsh(F i r s t ( lnput ) ,S tack)Input  :~-~ Rest ( Input )e lse  S tack  :=  Push(Second(C)perat lon) ,Pop(Pop(Stack) ) )end doThe  funct ions~ Top.f ive and F i r s t .
f i ve ,  re turn  the  l i s t s  o f  top  (or f i rst)  f ive  e lementsof  the  Stack and  the  Input  respect ive ly .
I f  there  Lre not  enough e lements ,  theseprocedures  pad  w i th  b l~nks .
The  funct ion  Append concatenates  two  l i s t s  in to  one,Cnnsu l t -CSG consu l t s  the  given CSO rules to f ind  the  next  operat ion  to t~ke.
Thedeta i l s  o f  th l8  funct ion  a re  the  sub jec t  o f  the  next  sec t ion .
Push  and  Pop .dd  orde le te  one  e lement  to / f rom a s tack  wh i le  F i r s t  and  Second re turn  the  f i r s t  or  seconde lements  of  a f lat ,  respect lve ly .
Res t  Teturns  the  glven llst minus  the  f i r s t  element .Figure 2: Context Sensitive Shift Reduce ParserOne of the exciting aspects of neural network re-search is the ability of a trained NN system to discoverclosest matches from a set of patterns to a given one.
Westudied Sejnowski and Rosenberg's \[1988\] analyses of theweight matrices resulting from training NETtalk.
Theyreported that the weight matrix had maximum weightsrelating the character in the central window to the outputphoneme, with weights for the surrounding context char-acters falling off with distance from the central window.We designed a similar function with maximum weightsbeing assigned to the top two stack elements and weightsdecreasing in both directions with distance from thosepositions.
The scoring function is developed as follows.Let "R be the set of vectors {R1, R2, .
.
.
,  Rn}where R~ is the vector \[rl, r2 , .
.
.
,  rl0\]Let C be the vector \[Cl, c2, .
.
.
,  c10\]Let p(ci, rl) be a matching function whosevalue is 1 if ci = n,  and 0 otherwise.is the entire set of rules, P~ is (the left-half of) a par-ticular rule, and C is the parse context.Then 7~' is the subset of 7~, whereif R~ 6 7~' then #(n4, c4).
P(ns, c5) = 1.The statement above is achieved by accessing the hashtable with the top two elements of the stack, c4, c5 toproduce the set 7~'.We can now define the scoring function for each R~ 61243 10Score = E It(c,, r,) .
i 4- E It(c,, r , ) ( l l  - i)i=1  i=SThe first summation scores the matches between thestack elements of the rule and the current context whilethe second summation scores the matches between theelements in the input string.
If two items of the ruleand context match, the total score is increased by theweight assigned to that position.
The maximum score fora perfect match is 21 according to the above formula.From several experiments, varying the length of vec-tor and the weights, particularly those assigned to blanks,it has been determined that this formula gave the bestperformance among those tested.
More importantly, ithas worked well in the current phrase structure and caseanalysis experiments.3 Experiments with CSGTo support the claim that CSG systems are an improve-ment over Augmented CFG, a number of questions needbe answered.?
Can they be acquired easily??
Do they reduce ambiguity in phrase structure anal-ysis??
How well do CSG rules generalize to new texts??
How large is the CSG that encompasses most of thesyntactic structures in news stories?3 .1  Acqu is i t ion  o f  CSGIt has been shown that our CSG productions are essen-tially a recording of the states from parsing sentences.Thus it was easy to construct a grammar acquisition sys-tem to present he successive states of a sentence to a lin-guist user, accepting and recording the linguist's judge-ments of shift or reduce.
This system has evolved to asophisticated grammar acquisition/editing program thatprompts the user on the basis of the rules best fitting thecurrent sentence context.
It 's lexicon also suggests thechoice of syntactic lass for words in context.
Generallyit reduces the linguistic task of constructing a grammarto the much simpler task of deciding for a given contextwhether to shift input or to rewrite the top elements of thestack as a new constituent.
It reduces a vastly complextask of grammar writing to relatively simple, concretejudgements that can be made easily and reliably.Using the acquisition system, it has been possiblefor linguist users to provide example parses at the rate oftwo or three sentences per hour.
The system collects theresulting states in the form of CSG productions, allowsthe user to edit them, and to use them for examining theresulting phrase structure tree for a sentence.
To obtainthe 4000+ rules examined below required only about fourman-weeks of effort (much of which was initial trainingtime.
)3 .2  Reduced Ambigu i ty  in  Pars ing  ?Over the course of this study six texts were accumulated.The first two were brief disease descriptions from a youthencyclopedia; the remaining four were newspaper texts.Figure 1 characterizes ach article by the number of CSGrules or states, number of sentences, the range of sentencelengths, and the average number of words per sentence.Text  St~teJ I Seateaces 'Wdl/Snt Mn-Wdl/SatHep&t l t / l  236 12 4 -19  10.3Measles 316 I0  4 -25  16.3News-Stor}~ 470 I0  9-51 23.6APWire-Robots  i 005  21 11-53  26.0APW~re-Rocket  1437 25 6 -47  29.2APWire-Shutt le  598 14 12-32  21.9Total  4062 I 93  4 -53  22.8Table 1: Characteristics of the Text CorpusIt can be seen that the news stories were fairly com-plex texts with average sentence lengths ranging from 22to 29 words per sentence.
A total of 92 sentences in over2000 words of text resulted in 4062 CSG productions.It was noted earlier that in each CFG productionthere is an embedded context-free rule and that the pri-mary function of the other eight symbols for parsing is toselect the rule that best applies to the current sentencestate.
When the linguist makes the judgement of shift orreduce, he or she is considering the entire meaning of thesentence to do so, and is therefore specifying a semanti-cally preferred parse.
The parsing system has access onlyto limited syntactic information, five syntactic symbolson the stack, and five input word classes and the parsingalgorithm follows only a single path.
How well does itwork?The CSG was used to parse the entire 92 sentenceswith the algorithm described in Figure 2 augmented withinstrumentation to compare the constituents the parserfound with those the linguist prescribed.
88 of the 92sentences exactly matched the linguist's parse.
The otherfour cases resulted in perfectly reasonable complete parsetrees that differed in minor ways from the linguist's pre-125scription.
As to whether any of the 92 parses are truly"correct", that is a question that linguists could only de-cide after considerable study and discussion.
Our claimis only that the grammars we write provide our own pre-ferred interpretations - -  useful and meaningful segmen-tation of sentences into trees of syntactic onstituents.Figure 3 displays the tree of a sentence as analyzedby the parser using CSG.
It is a very pleasant surprise todiscover that using context sensitive productions, an ele-mentary, deterministic, parsing algorithm is adequate toprovide (almost) perfectly correct, unambiguous analysesfor the entire text studied.Another mission soon scheduled that also would have pri-ority over the shuttle is the first firing of a trident twointercontinental range missile from a submerged subma-rine.h- -v lN  ~,- -pFigure 3: Sentence Parse3.3 Genera l i za t ion  of  CSGOne of the first questions considered was what percent ofnew constituents would be recognized by various accumu-lations of CSG.
We used a system called union-grammarthat would only add a rule to the grammar if the gram-mar did not already predict its operation.
The black lineof Figure 4 shows successive accumulations of 400-rulesegments of the grammar after randomizing the orderingof the rules.
Of the first 400 CS rules 50% were new; andfor an accumulation of 800, only 35% were new.
When2000 rules had been experienced the curve is flattening toan average of 20% new rules.
This curve tells us that ifthe acquisition system uses the current grammar to sug-gest operations to the linguist, it will be correct about 4out of 5 times and so reduce the linguist's efforts accord-ingly.
The curve also suggests that our collection of ruleexamples has about 80% redundancy in that earlier rulescan predict newcomers at that level of accuracy.
On thedown-side, though, it shows that only 80% of the con-stituents of a new sentence will be recognized, and thusthe probability of a correct parse for a sentence never seenbefore is very small.
We experimented with a grammarof 3000 rules to attempt o parse the new shuttle text,but found that only 2 of 14 new sentences were parsedcorrectly.Joo7o!
?!
,oI -  raIoo I ....... t ......... ...... i. .
.
.
.
.~mnlb~ d W  ~Figure 4: Generalization of CSG RulesIf two parsing grammars equally well account for thesame sentences, the one with fewer rules is less redundant,more general, and the one to be preferred.
We used union-grammar to construct he "minimal grammar" with suc-cessive passes through 3430 rules, as shown in Figure2.The first pass found 856 rules would account for the rest.A second pass of the 3430 rules against he 856 extractedby the first pass resulted in the addition of 26 more rules,adding rules that although recognized by earlier rulesfound interference as a result of later ones.
The remaining8 rules discovered in the next pass are apparently identicalpatterns resulting in differing operations - -  contradicto-ries that need to be studied and resolved.
The resultingminimal grammar totaling 895 rules succeeds in parsingthe texts with only occasional minor differences from thelinguist's prescriptions.
We must emphasize that the un,retained rules are not identical but only similar to thosein the minimal grammar.126I Pass I Unretained2574340434223425Retained Total Rules85626853430343034303430Table 2: Four Passes with Minimal Grammar3 .4  Es t imated  S ize  o f  Completed  CSGA question, central to the whole argument for the utilityof CSG, is how many rules will be required to account forthe range of structures found in news story text?
Referagain to Figure 4 to try to estimate when the black line,CS, will intersect he abscissa.
It is apparent hat moredata is needed to make a reliable prediction.Let us consider the gray line, labeled CF that showshow many new context-free rules are accumulated for 400CSG rule increments.
This line rapidly decreases to about5% new CFG rules at the accumulation of 4000 CSG pro-ductions.
We must recall that it is the embedded context-free binary rule that is carrying the most weight in deter-mining a constituent, so let us notice some of the CFGproperties.We allow 64 symbols in our phrase structure analy-sis.
That means, there are 642 possible combinations forthe top two elements of the stack.
For each combination,there are 65 possible operations3: a shift or a reduction toanother symbol.
Among 4000 CSG rules, we studied howmany different CFG rules can be derived by eliminatingthe context.
We found 551 different CFG rules that used421 different left-side pairs of symbols.
This shows thata given context free pair of symbols averages 1.3 differentoperations.Then, as we did with CSG rules, we measured howmany new CFG rules were added in an accumulative fash-ion.
The shaded line of Figure 4 shows the result.
No-tice that the line has descended to about 5% errors at4000 rules.
To make an extrapolation easier, a log-loggraph shows the same data in Figure 5.
From this graph,it can be predicted that, after about 25000 CSG rulesare accumulated, the grammar will encompass an Mmostcomplete CFG component.
Beyond this point, additionalCSG rules will add no new CFG rules, but only fine-tunethe grammar so that it can resolve ambiguities more ef-fectively.Also, it is our belief that, after the CSG reachesthat point, a multi-path, beam-search parser would be3 Actually, there are many fewer than 65 possible operations sincethe stack elements can be reduced only to non-terminal symbols.I 1 !
;,o !1... JIGO 1,000 4,0o0 10,000 2s.ooo 100,000Nbr of  Aaaumuktted RuloeExlrq~lalon, l ie gray Ine, predc~ Ilat 99% of ~ COnlmxt Iree pldrs vdll be achlemcl ~ ~ac~mlUlalon d 2~.000 c~nte~ sensiUve rules.Figure 5: Log-Log Plot of New CFG Rulesable to parse most newswire stories very reliably.
Thisbelief is based on our observation that most failures inparsing new sentences with a single-path parser resultfrom a dead-end sequence; i.e., by making a wrong choicein the middle, the parsing ends up with a state whereno rule is applicable.
The beam-search parser should beable to recover from this failure and produce a reasonableparse.4 D iscuss ion  and Conc lus ionsNeurM network research showed us the power of con-textuM elements for selecting preferred word-sense andparse-structure in context.
But since NN training is stilla laborious, computation-intensive process that does notscale well into tens of thousands of patterns, we chose tostudy context-sensitive grammar in the ordinary contextof sequential parsing with a hash-table representation ofthe grammar, and a scoring function to select the rulemost applicable to a current sentence context.
We findthat context-sensitive, binary phrase structure rules witha context comprising the three preceding stack symbolsand the oncoming five input symbols,stack1-3 binary-rule inputl_5 --~ operationprovide unexpected advantages for acquisition, the com-putation of preferred parsings, and generalization.127A linguist constructs a CSG with the acquisition sys-tem by demonstrating successive states in parsing sen-tences.
The acquisition system presents the state result-ing from each shift/reduce operation that the linguist pre-scribes, and it uses the total grammar so far accumulatedto find the best matching rule and so prompt the linguistfor the next decision.
As a result CSG acquisition is arapid process that requires only that a linguist decide fora given state to reduce the top two elements of the stack,or to shift a new input element onto the stack.
Since thecurrent grammar is about 80% accurate in its predictions,the linguist's task is reduced by the prompts to an alertobservation and occasional correction of the acquisitionsystem's choices.The parser is a bottom-up, determinis-tic, shift/reduce program that finds a best sequence ofparse states for a sentence according to the CSG.
Whenwe instrument the parser to compare the constituents itfinds with those originally prescribed by a linguist, wediscover almost perfect correspondence.
We observe thatthe linguist used judgements based on understanding themeaning of the sentence and that the parser using thecontextual elements of the state and matching rules cansuccessfully reconstruct the linguist's parse, thus provid-ing a purely syntactic approach to preference parsing.The generalization capabilities of the CSG arestrong.
With the accumulation 2-3 thousand examplerules, the system is able to predict correctly 80% of sub-sequent parse states.
When the grammar is compressedby storing only rules that the accumulation does not al-ready correctly predict, we observe a compression from3430 to 895 rules, a ratio of 3.8 to 1.
We extrapolate fromthe accumulation of our present 4000 rules to predict thatabout 25 thousand rule examples hould approach com-pletion of the CF grammar for the syntactic structuresusually found in news stories.
For additional fine tun-ing of the context selection we might suppose we createa total of 40 thousand example rules.
Then if the 3.8/1compression ratio holds for this large a set of rules, wecould expect our final grammar to be reduced from 40 toabout 10 thousand context sensitive rules.In view of the large combinatoric space provided bythe ten symbol parse states - -  it could be as large as 641?- -  our prediction of 25-40 thousand examples as mainlysufficient for news stories seems contra~intuitive.
But ourpresent grammar seems to have accumulated 95% of thebinary context free rules - -  551 of about 4096 possiblebinaries or 13% of the possibility space.
If 551 is in fact95% then the total number of binary rules is about 580or only 14% of the combinatoric space for binary rules.In the compressed grammar, there are only 421 differentleft-side patterns for the 551 rules, and we can notice thateach context-free pair of symbols averages only 1.3 differ-ent operations.
We interpret his to mean that we needonly enough context patterns to distinguish the differentoperations associated with binary combinations ofthe toptwo stack elements; since there are fewer than an averageof two, it appears reasonable to expect that the context-sensitive portion of the grammar will not be excessivelylarge.We conclude,?
Context sensitive grammar is a conceptually andcomputationally tractable approach to unambigu-ous parsing of news stories.?
The context of the CSG rules in conjunction with ascoring formula that selects the rule best matchingthe current sentence context allow a deterministicparser to provide preferred parses reflecting a lin-guist's meaning-based judgements.?
The CSG acquisition system simplifies a linguist'sjudgements and allows rapid accumulation of largegrammars.?
CSG grammar generalizes in a satisfactory fashionand our studies predict that a nearly-complete ac-counting for syntactic phrase structures of news sto-ries can be accomplished with about 25 thousandexample rules.REFERENCESAlien, Robert, "Several Studies on Natural Languageand Back Propagation", Proc.
Int.
Conf.
on NeuralNetworks, San Diego, Calif., 1987.Allen, James, Natural Language Understanding, Ben-jamin Cummings, Menlo Park, Calif., 1987..Chomsky, Noam, Syntactic Structures, Mouton, TheHague, 1957.Gazdar, Gerald, Klein E., Pullum G., and Sag I., Gen-eralized Phrase Structure Grammar, Harvard Univ.Press, Boston, 1985.Joshi, Aravind K., "An Introduction to Tree AdjoiningGrammars."
In Manaster-RamerEd.
),Mathematics of Language, John Benjamins,msterdam, Netherlands, 1985.McClelland, J.L., and Kawamoto, A.H., "Mechanismsof Sentence Processing: Assigning Roles to Con-stituents," In McClelland J. L. and Rumelhart, D.E., Parallel Distributed Processing, Vol.
2.
1986.128Miikkulainen, Risto, and Dyer, M., "A Modular NeuralNetwork Architecture for Sequential Paraphrasingof Script-Based Stories", Artif.
Intell.
Lab., Dept.Comp.
Sci., UCLA, 1989.Shieber, Stuart M., An Introduction to UnificationBased Approaches to Grammar, Chicago Univ.Press, Chicago, 1986.Sejnowski, Terrence J., and Rosenberg, C., "NETtalk:A Parallel Network that Learns to Read Aloud", inAnderson and Rosenfeld (Eds.)
Nearocomputing,MIT Press., Cambridge Mass., 1988.Simmons, Robert F. Computations from the English,Prentice-Hall, Engelwood Cliffs, New Jersey, 1984.Simmons, Robert F. and Yu, Yeong-Ho, "Training aNeural Network to be a Context Sensitive Gram-mar," Proc.
5th Rocky Mountain AI Conf.
LasCruces, N.M., 1990.Tomita, M. Efficient Parsing for Natural Language,Kluwer Academic Publishers, Boston, Ma., 1985.Yu, Yeong-Ho, and Simmons, R.F.
"Descending Epsilonin Back-Propagation: A Technique for Better Gen-eralization," In Press, Proc.
Int.
Jr. Conf.
NeuralNetworks, San Diego, Calif., 1990.129
