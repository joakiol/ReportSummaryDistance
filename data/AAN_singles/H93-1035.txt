Machine TranslationAlex WaibelSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh PA 15213Machine Translation was one of the declared highlightsand focal points of the Human Language TechnologyWorkshop.
Machine Translation, or MT for short, has seena renaissance in recent years, brought about by theavailability of faster and more powerful computing, andseveral decades of advances in speech and languageprocessing.
ARPA now sponsors a machine translationinitiative and companies and governments, domestic andoverseas, view it as a growth area and devote significantefforts to this problem.
Indeed, as nations are growingincreasingly intertwined with each other, and aseconomies, defense, travel, and the media grow increas-ingly internationalized and globalized, handling and over-coming language barriers effectively, becomes an evermore pressing issue.The session at the ARPA workshop attempted todo justiceto the various blossoming avenues that make up machinetranslationa aswe see it today.
First and perhaps foremostis the question of how MT is to be done.
Two approacheshave a~acted considerable scientific interest and debate:the knowledge based and statistical approaches.
Theknowledge based approach is perhaps the more classicalapproach, based on linguistic theory and in its most puristincarnation relying on rule based systems for syntactic andsemantic analysis and generation.
The statistical pproach,in contrast, attempts o achieve solutions to machine trans-lation by finding suitable mappings between two languagesvia statistical nalysis based on large corpora.
Critcs of theformer will argue that a knowledge based approach willlack the ability to make soft decisions, deal with uncer-tainty and ambiguity and cannot learn.
Critics of the latterwill see the lack of structure and simplicity of the statisticalmodel as too simplistic and limited, given the intricaciesand rich structure of language.
The two views were high-lighted and well argued by Ed Hovy and Peter Brown intwo eloquent, enlightening as well as entertaining tutorials.Both cases were well delivered and the discussion thatensued highlighted perhaps the commonalities more thanthe differences.
Indeed, as knowledge based approachesadopt statistical techniques and as proponents pf statsficalMT discover structure in language, the two approaches ap-pear to be growing toward a common middle ground.Learning and handling uncertainty as well as taking advan-tage of structural universals of human languages will guideprogress in years to come.
Along he way, better tools,better principles of evaluation and a better understandingof what the ultimate needs in MT would be will driveadvances.Tools, dictionaries and knowledgebases of various kindsmake up important parts of the Iranslation task (human orby machine).
Short of academic squabbles over the "right"approach, a number of efforts are aiming to improve andexpand these supporting technologies to achieve betterquality translations more effectively and more efficientlyby humans and/or machines.
Acknowledging that accurateautomatic translation of any unrestricted texts may still bea research item for a while, researchers attempt to developtools that help the human translator in "Machine AidedTranslation" (MAT), to do the job more effectively.
Un-like speech recognition, a partial solution here can providesignificant help or save costs.
A paper by Kevin Knightentitled "Building a Large Ontology for Machine Trans-lation" and by Peter Brown et al entitled "But Dictionariesare Data too", address ways by which dictionaries and on-tologies can be automatically or semi-automaticallygenerated and how they can be applied and used in MT.The papers "LINGSTAT: and Interactive, Machine-AidedTranslation System" by Jonathan Yumron et al and "AnMAT Tool and Its Effectiveness" by Robert Frederking etal.
address the question of how tools for generating trans-lated documents semi-automatically can improve ffective-ness of translation.In the light of these different streams of activity it is par-ticularly difficult o define commonly useful and acceptedevaluation procedures.
Since there is no clear definition ofa "correct" translation, it is not a simple matter of countingthe number correct or error rate.
Translation fidelity issubjective in part and is determined by various chemes inwhich panels of judges decide on the naturalness and intel-ligibility of translations.
No doubt, the cost of performingsuch evaluations i considerable and different schemes arebeing discussed.
The paper "Evaluation of Machine Trans-lation" by John White et al addresses this thorny issue andgives evaluation results using current evaluation measuresused under the ARPA MT-program.Finally, two papers on Speech Translation address thequestions that arise, when text is not the input medium, butif an input sentence is spoken in one language and shouldbe translated into another.
Applications for this kind ofMT system abound (telecommunication, media, con-ferences, etc.).
The problem of translation is made harderby the fact that the input to the MT-system is now cor-rupted by syntactic ill-formedness produced by thespeaker, colloquialisms, acoustic noise, and speech recog-nition error.
While a speech translation system may at firstglance combine speech-to-text recognition with text basedmachine translation, its long term viability demands atighter coupling as translation and recognition eed toderive the intended meaning, not a perfect textualtranscription and need to involve conextual cues in a cross-lingual dialog.
Attempts at answering some of these stillopen questions are under way and described in two papers:"Recent Advances in Speech Translation" by MonikaWoszczyna et al and "A Speech to Speech Translation183System built from Standard Components" by MannyRayner et al They describe currently operational speechtranslation systems.In summary, a good number of the outstanding issues inMachine Translation have been touched by the paperspresented atthe workshop.
It is our hope that hey pave theway for a rich ongoing debate between proponents of dif-ferent approaches, applications and deployment considera-tions to our collective benefit.
Indeed, the academic effortsare well warranted by the urgent needs in an increasinglyinternationalized but linguistically splintered world.Session 8: Machine Translation SummaryMachine Translation was one of the declared highlightsand focal points of the Human Language TechnologyWorkshop.
Machine Translation, or MT for short, has seena renaissance in recent years, brought about by theavailability of faster and more powerful computing, andseveral decades of advances in speech and languageprocessing.
ARPA now sponsors a machine translation i -itiative and companies and governments, domestic andoverseas, view it as a growth area and devote significantefforts to this problem.
Indeed, as nations are growingincreasingly intertwined with each other, and aseconomies, defense, travel, and the media grow increas-ingly internationalized and globalized, handling and over-coming language barriers effectively, becomes an evermore pressing issue.
The session at the ARPA workshopattempted todo justice to the various blossoming avenuesthat make up machine translafiona as we see it today.
Firstand perhaps foremost is the question of how MT is to bedone.
Two approaches have attracted considerable scien-tific interest and debate: the knowledge based and statis-tical approaches.
The knowledge based approach is per-haps the more classical approach, based on linguistictheory and in its most purist incarnation relying on rulebased systems for syntactic and semantic analysis andgeneration.
The statistical pproach, in contrast, attempts oachieve solutions to machine translation by finding suitablemappings between two languages via statistical analysisbased on large corpora.
Cntcs of the former will argue thata knowledge based approach will lack the ability to makesoft decisions, deal with uncertainty and ambiguity andcannot learn.
Critics of the latter will see the lack of struc-ture and simplicity of the statistical model as too simplisticand limited, given the intricacies and rich structure of lan-guage.
The two views were highlighted and well argued byEd Hovy and Peter Brown in two eloquent, enlightening aswell as entertaining tutorials.
Both cases were welldelivered and the discussion that ensued highlighted per-haps the commonalities more than the differences.
Indeed,as knowledge based approaches adopt statistical techniquesand as proponents pf statistical MT discover structure inlanguage, the two approaches appear to be growing towarda common middle ground.
Learning and handling uncer-tainty as well as taking advantage of structural universalsof human languages will guide progress in years to come.Along he way, better tools, better principles of evaluationand a better understanding of what the ultimate needs inMT would be will drive advances.
Tools, dictionaries andknowledgebases of various kinds make up important partsof the translation task (human or by machine).
Short ofacademic squabbles over the right approach, a number ofefforts are aiming to improve and expand these supportingtechnologies toachieve better quality translations more ef-fectively and more efficiently by humans and/or machines.Acknowledging that accurate automatic translation of anyunrestricted texts may still be a research item for a while,researchers attempt to develop tools that help the humantranslator in Machine Aided Translation (MAT), to do thejob more effectively.
Unlike speech recognition, a partialsolution here can provide significant help or save costs.
Apaper by Kevin Knight entitled Building a Large Ontologyfor Machine Translation and by Peter Brown et al entitledBut Dictionaries are Data too, address ways by which dic-tionaries and ontologies can be automatically or semi-automatically generated and how they can be applied andused in MT, The papers LINGSTAT: and Interactive,Machine-Aided Translation System by Jonathan Yamron etal.
and An MAT Tool and Its Effectiveness by RobertFrederking et al address the question of how tools forgenerating translated ocuments emi-automatically can~mprove ffectiveness of translation.
In the light of thesedifferent streams of activity it is particularly difficult todefine commonly useful and accepted evaluationprocedures.
Since there is no clear definition of a correcttranslation, it is not a simple matter of counting the numbercorrect or error rate.
Translation fidelity is subjective inpart and is determined by various chemes in which panelsof judges decide on the naturalness and intelligibility oftranslations.
No doubt, the cost of performing such evalua-tions is considerable and different schemes are being dis-cussed.
The paper Evaluation of Machine Translation byJohn White et al addresses this thorny issue and givesevaluation results using current evaluation measures usedunder the ARPA MT-program.
Finally, two papers onSpeech Translation address the questions that arise, whentext is not the input medium, but if an input sentence isspoken in one language and should be translated intoanother.
Applications for this kind of MT system abound(telecommunication, media, conferences, etc.).
Theproblem of translation is made harder by the fact that theinput to the MT-system is now corrupted by syntactic ill-formedness produced by the speaker, colloquialisms,acoustic noise, and speech recognition error.
While aspeech translation system may at first glance combinespeech-to-text recognition with text based machine trans-lation, its long term viability demands a tighter coupling astranslation and recognition eed to derive the intendedmeaning, not a perfect extual transcription and need toinvolve conextual cues in a cross-lingual dialog.
Attemptsat answering some of these still open questions are underway and described in two papers: Recent Advances inSpeech Translation by Monika Woszczyna et al and ASpeech to Speech Translation System built from StandardComponents by Manny Rayner et al They describe cur-rently operational speech translation systems.
In summary,a good number of the outstanding issues in Machine Trans-lation have been touched by the papers presented at theworkshop.
It is our hope that they pave the way for a richongoing debate between proponents of different ap-proaches, applications and deployment considerations toour collective benefit.
Indeed, the academic efforts are wellwarranted by the urgent needs in an increasingly inter-nationalized butlinguistically splintered world.184
