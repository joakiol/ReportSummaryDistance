Proceedings of the ACL 2010 Conference Short Papers, pages 371?376,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAn Active Learning Approach to Finding Related TermsDavid VickreyStanford Universitydvickrey@cs.stanford.eduOscar KipersztokBoeing Research & Technologyoscar.kipersztok@boeing.comDaphne KollerStanford Univeristykoller@cs.stanford.eduAbstractWe present a novel system that helps non-experts find sets of similar words.
Theuser begins by specifying one or more seedwords.
The system then iteratively sug-gests a series of candidate words, whichthe user can either accept or reject.
Cur-rent techniques for this task typically boot-strap a classifier based on a fixed seedset.
In contrast, our system involvesthe user throughout the labeling process,using active learning to intelligently ex-plore the space of similar words.
Inparticular, our system can take advan-tage of negative examples provided by theuser.
Our system combines multiple pre-existing sources of similarity data (a stan-dard thesaurus, WordNet, contextual sim-ilarity), enabling it to capture many typesof similarity groups (?synonyms of crash,?
?types of car,?
etc.).
We evaluate on ahand-labeled evaluation set; our systemimproves over a strong baseline by 36%.1 IntroductionSet expansion is a well-studied NLP problemwhere a machine-learning algorithm is given afixed set of seed words and asked to find additionalmembers of the implied set.
For example, giventhe seed set {?elephant,?
?horse,?
?bat?
}, the al-gorithm is expected to return other mammals.
Pastwork, e.g.
(Roark & Charniak, 1998; Ghahramani& Heller, 2005; Wang & Cohen, 2007; Pantelet al, 2009), generally focuses on semi-automaticacquisition of the remaining members of the set bymining large amounts of unlabeled data.State-of-the-art set expansion systems workwell for well-defined sets of nouns, e.g.
?US Pres-idents,?
particularly when given a large seed set.Set expansions is more difficult with fewer seedwords and for other kinds of sets.
The seed wordsmay have multiple senses and the user may have inmind a variety of attributes that the answer mustmatch.
For example, suppose the seed word is?jaguar?.
First, there is sense ambiguity; we couldbe referring to either a ?large cat?
or a ?car.?
Be-yond this, we might have in mind various more (orless) specific groups: ?Mexican animals,?
?preda-tors,?
?luxury cars,?
?British cars,?
etc.We propose a system which addresses sev-eral shortcomings of many set expansion systems.First, these systems can be difficult to use.
As ex-plored by Vyas et al (2009), non-expert usersproduce seed sets that lead to poor quality expan-sions, for a variety of reasons including ambiguityand lack of coverage.
Even for expert users, con-structing seed sets can be a laborious and time-consuming process.
Second, most set expansionsystems do not use negative examples, which canbe very useful for weeding out other bad answers.Third, many set expansion systems concentrate onnoun classes such as ?US Presidents?
and are noteffective or do not apply to other kinds of sets.Our system works as follows.
The user initiallythinks of at least one seed word belonging to thedesired set.
One at a time, the system presents can-didate words to the user and asks whether the can-didate fits the concept.
The user?s answer is fedback into the system, which takes into account thisnew information and presents a new candidate tothe user.
This continues until the user is satisfiedwith the compiled list of ?Yes?
answers.
Our sys-tem uses both positive and negative examples toguide the search, allowing it to recover from ini-tially poor seed words.
By using multiple sourcesof similarity data, our system captures a variety ofkinds of similarity.
Our system replaces the poten-tially difficult problem of thinking of many seedwords with the easier task of answering yes/noquestions.
The downside is a possibly increasedamount of user interaction (although standard setexpansion requires a non-trivial amount of user in-teraction to build the seed set).There are many practical uses for such a sys-tem.
Building a better, more comprehensive the-saurus/gazetteer is one obvious application.
An-other application is in high-precision query expan-sion, where a human manually builds a list of ex-371pansion terms.
Suppose we are looking for pagesdiscussing ?public safety.?
Then synonyms (ornear-synonyms) of ?safety?
would be useful (e.g.?security?)
but also non-synonyms such as ?pre-cautions?
or ?prevention?
are also likely to returngood results.
In this case, the concept we are inter-ested in is ?Words which imply that safety is beingdiscussed.?
Another interesting direction not pur-sued in this paper is using our system as part ofa more-traditional set expansion system to buildseed sets more quickly.2 Set ExpansionAs input, we are provided with a small set of seedwords s. The desired output is a target set ofwords G, consisting of all words that fit the de-sired concept.
A particular seed set s can belongto many possible goal sets G, so additional infor-mation may be required to do well.Previous work tries to do as much as possibleusing only s. Typically s is assumed to contain atleast 2 words and often many more.
Pantel et al(2009) discusses the issue of seed set size in detail,concluding that 5-20 seed words are often requiredfor good performance.There are several problems with the fixed seedset approach.
It is not always easy to think ofeven a single additional seed word (e.g., the user istrying to find ?German automakers?
and can onlythink of ?Volkswagen?).
Even if the user can thinkof additional seed words, time and effort might besaved by using active learning to find good sug-gestions.
Also, as Vyas et al (2009) show, non-expert users often produce poor-quality seed sets.3 Active Learning SystemAny system for this task relies on informationabout similarity between words.
Our system takesas input a rectangular matrix M .
Each columncorresponds to a particular word.
Each row cor-responds to a unique dimension of similarity; thejth entry in row i mij is a number between 0 and1 indicating the degree to which wj belongs to theith similarity group.
Possible similarity dimen-sions include ?How similar is word wj to the verbjump??
?Is wj a type of cat??
and ?Are the wordswhich appear in the context of wj similar to thosethat appear in the context of boat??
Each row riof M is labeled with a word li.
This may followintuitively from the similarity axis (e.g., ?jump,??cat,?
and ?boat?, respectively), or it can be gen-erated automatically (e.g.
the word wj with thehighest membership mij).Let ?
be a vector of weights, one per row, whichcorrespond to how well each row aligns with thegoal set G. Thus, ?i should be large and positive ifrow i has large entries for positive but not negativeexamples; and it should be large and negative ifrow i has large entries for negative but not positiveexamples.
Suppose that we have already chosenan appropriate weight vector ?.
We wish to rankall possible words (i.e., the columns of M ) so thatthe most promising word gets the highest score.A natural way to generate a score zj for columnj is to take the dot product of ?
with column j,zj =?i ?imij .
This rewards word wj for havinghigh membership in rows with positive ?, and lowmembership in rows with negative ?.Our system uses a ?batch?
approach to activelearning.
At iteration i, it chooses a new ?
basedon all data labeled so far (for the 1st iteration,this data consists of the seed set s).
It thenchooses the column (word) with the highest score(among words not yet labeled) as the candidatewordwi.
The user answers ?Yes?
or ?No,?
indicat-ing whether or not wi belongs to G. wi is addedto the positive set p or the negative set n basedon the user?s answer.
Thus, we have a labeled dataset that grows from iteration to iteration as the userlabels each candidate word.
Unlike set expansion,this procedure generates (and uses) both positiveand negative examples.We explore two options for choosing ?.
Recallthat each row i is associated with a label li.
Thefirst method is to set ?i = 1 if li ?
p (that is, theset of positively labeled words includes label li),?i = ?1 if li ?
n, and ?i = 0 otherwise.
Werefer to this method as ?Untrained?, although it isstill adaptive ?
it takes into account the labeledexamples the user has provided so far.The second method uses a standard machinelearning algorithm, logistic regression.
As be-fore, the final ranking over words is based on thescore zj .
However, zj is passed through the lo-gistic function to produce a score between 0 and1, z?j =11+e?zj.
We can interpret this scoreas the probability that wj is a positive example,P?
(Y |wj).
This leads to the objective functionL(?)
= log(?wj?pP?
(Y |wj)?wj?n(1?P?
(Y |wj))).This objective is convex and can be optimized us-ing standard methods such as L-BFGS (Liu & No-cedal, 1989).
Following standard practice we addan L2 regularization term ?
?T ?2?2 to the objective.This method does not use the row labels li.372Data Word Similar wordsMoby arrive accomplish, achieve, achieve success, advance, appear, approach, arrive at, arrive in, attain,...WordNet factory (plant,-1.9);(arsenal,-2.8);(mill,-2.9);(sweatshop,-4.1);(refinery,-4.2);(winery,-4.5);...DistSim watch (jewerly,.137),(wristwatch,.115),(shoe,0.09),(appliance,0.09),(household appliance,0.089),...Table 1: Examples of unprocessed similarity entries from each data source.4 Data SourcesWe consider three similarity data sources: theMoby thesaurus1, WordNet (Fellbaum, 1998), anddistributional similarity based on a large corpusof text (Lin, 1998).
Table 1 shows similarity listsfrom each.
These sources capture different kindsof similarity information, which increases the rep-resentational power of our system.
For all sources,the similarity of a word with itself is set to 1.0.It is worth noting that our system is not strictlylimited to choosing from pre-existing groups.
Forexample, if we have a list of luxury items, and an-other list of cars, our system can learn weights sothat it prefers items in the intersection, luxury cars.Moby thesaurus consists of a list of word-based thesaurus entries.
Each word wi has a list ofsimilar words simij .
Moby has a total of about 2.5million related word pairs.
Unlike some other the-sauri (such as WordNet and thesaurus.com), en-tries are not broken down by word sense.In the raw format, the similarity relation is notsymmetric; for example, there are many wordsthat occur only in similarity lists but do not havetheir own entries.
We augmented the thesaurus tomake it symmetric: if ?dog?
is in the similarity en-try for ?cat,?
we add ?cat?
to the similarity entryfor ?dog?
(creating an entry for ?dog?
if it does notexist yet).
We then have a row i for every similar-ity entry in the augmented thesaurus; mij is 1 ifwj appears in the similarity list of wi, and 0 other-wise.
The label li of row i is simply word wi.
Un-like some other thesauri (including WordNet andthesaurus.com), the entries are not broken downby word sense or part of speech.
For polysemicwords, there will be a mix of the words similar toeach sense and part of speech.WordNet is a well-known dictionary/thesaurus/ontology often used in NLP applications.
It con-sists of a large number of synsets; a synset is a setof one or more similar word senses.
The synsetsare then connected with hypernym/hyponym links,which represent IS-A relationships.
We focusedon measuring similarity in WordNet using the hy-pernym hierarchy.2.
There are many methods for1Available at icon.shef.ac.uk/Moby/.2A useful similarity metric we did not explore in this pa-per is similarity between WordNet dictionary definitionsconverting this hierarchy into a similarity score;we chose to use the Jiang-Conrath distance (Jiang& Conrath, 1997) because it tends to be more ro-bust to the exact structure of WordNet.
The num-ber of types of similarity in WordNet tends to beless than that captured by Moby, because synsetsin WordNet are (usually) only allowed to have asingle parent.
For example, ?murder?
is classifiedas a type of killing, but not as a type of crime.The Jiang-Conrath distance gives scores forpairs of word senses, not pairs of words.
We han-dle this by adding one row for every word sensewith the right part of speech (rather than for ev-ery word); each row measures the similarity of ev-ery word to a particular word sense.
The label ofeach row is the (undisambiguated) word; multiplerows can have the same label.
For the columns, wedo need to collapse the word senses into words;for each word, we take a maximum across all ofits senses.
For example, to determine how similar(the only sense of) ?factory?
is to the word ?plant,?we compute the similarity of ?factory?
to the ?in-dustrial plant?
sense of ?plant?
and to the ?livingthing?
sense of ?plant?
and take the higher of thetwo (in this case, the former).The Jiang-Conrath distance is a number be-tween??
and 0.
By examination, we determinedthat scores below ?12.0 indicate virtually no sim-ilarity.
We cut off scores below this point andlinearly mapped each score x to the range 0 to1, yielding a final similarity of min(0,x+12)12 .
Thisgreatly sparsified the similarity matrix M .Distributional similarity.
We used DekangLin?s dependency-based thesaurus, available atwww.cs.ualberta.ca/?lindek/downloads.htm.This resource groups words based on the wordsthey co-occur with in normal text.
The wordsmost similar to ?cat?
are ?dog,?
?animal,?
and?monkey,?
presumably because they all ?eat,??walk,?
etc.
Like Moby, similarity entries are notdivided by word sense; usually, only the dominantsense of each word is represented.
This type ofsimilarity is considerably different from the othertwo types, tending to focus less on minor detailsand more on broad patterns.Each similarity entry corresponds to a single373wordwi and is a list of scored similar words simij .The scores vary between 0 and 1, but usually thehighest-scored word in a similarity list gets a scoreof no more than 0.3.
To calibrate these scoreswith the previous two types, we divided all scoresby the score of the highest-scored word in thatlist.
Since each row is normalized individually,the similarity matrix M is not symmetric.
Also,there are separate similarity lists for each of nouns,verbs, and modifiers; we only used the lists match-ing the seed word?s part of speech.5 Experimental SetupGiven a seed set s and a complete target set G, it iseasy to evaluate our system; we say ?Yes?
to any-thing in G, ?No?
to everything else, and see howmany of the candidate words are in G. However,building a complete gold-standard G is in practiceprohibitively difficult; instead, we are only capa-ble of saying whether or not a word belongs to Gwhen presented with that word.To evaluate a particular active learning algo-rithm, we can just run the algorithm manually, andsee how many candidate words we say ?Yes?
to(note that this will not give us an accurate estimateof the recall of our algorithm).
Evaluating severaldifferent algorithms for the same s and G is moredifficult.
We could run each algorithm separately,but there are several problems with this approach.First, we might unconsciously (or consciously)bias the results in favor of our preferred algo-rithms.
Second, it would be fairly difficult to beconsistent across multiple runs.
Third, it would beinefficient, since we would label the same wordsmultiple times for different algorithms.We solved this problem by building a labelingsystem which runs all algorithms that we wish totest in parallel.
At each step, we pick a random al-gorithm and either present its current candidate tothe user or, if that candidate has already been la-beled, we supply that algorithm with the given an-swer.
We do NOT ever give an algorithm a labeledtraining example unless it actually asks for it ?
thisguarantees that the combined system is equivalentto running each algorithm separately.
This pro-cedure has the property that the user cannot tellwhich algorithms presented which words.To evaluate the relative contribution of activelearning, we consider a version of our systemwhere active learning is disabled.
Instead of re-training the system every iteration, we train it onceon the seed set s and keep the weight vector ?
fixedfrom iteration to iteration.We evaluated our algorithms along three axes.First, the method for choosing ?
: Untrained andLogistic (U and L).
Second, the data sources used:each source separately (M for Moby, W for Word-Net, D for distributional similarity), and all threein combination (MWD).
Third, whether activelearning is used (+/-).
Thus, logistic regression us-ing Moby and no active learning is L(M,-).
For lo-gistic regression, we set the regularization penalty?2 to 1, based on qualitative analysis during devel-opment (before seeing the test data).We also compared the performance of ouralgorithms to the popular online thesaurushttp://thesaurus.com.
The entries in thisthesaurus are similar to Moby, except that eachword may have multiple sense-disambiguated en-tries.
For each seed word w, we downloaded thepage for w and extracted a set of synonyms en-tries for that word.
To compare fairly with our al-gorithms, we propose a word-by-word method forexploring the thesaurus, intended to model a userscanning the thesaurus.
This method checks thefirst 3 words from each entry; if none of these arelabeled ?Yes,?
it moves on to the next entry.
Weomit details for lack of space.6 Experimental ResultsWe designed a test set containing different typesof similarity.
Table 2 shows each category, withexamples of specific similarity queries.
For eachtype, we tested on five different queries.
For eachquery, the first author built the seed set by writ-ing down the first three words that came to mind.For most queries this was easy.
However, for thesimilarity type Hard Synonyms, coming up withmore than one seed word was considerably moredifficult.
To build seed sets for these queries, weran our evaluation system using a single seed wordand took the first two positive candidates; this en-sured that we were not biasing our seed set in favorof a particular algorithm or data set.For each query, we ran our evaluation systemuntil each algorithm had suggested 25 candidatewords, for a total of 625 labeled words per algo-rithm.
We measured performance using mean av-erage precision (MAP), which corresponds to areaunder the precision-recall curve.
It gives an over-all assessment across different stopping points.Table 3 shows results for an informative sub-set of the tested algorithms.
There are many con-clusions we can draw.
Thesaurus.Com performspoorly overall; our best system, L(MWD,+),outscores it by 164%.
The next group of al-374Category Name Example Similarity QueriesSimple Groups (SG) car brands, countries, mammals, crimesComplex Groups (CG) luxury car brands, sub-Saharan countriesSynonyms (Syn) syn of {scandal, helicopter, arrogant, slay}Hard Synonyms (HS) syn of {(stock-market) crash, (legal) maneuver}Meronym/Material (M) parts of a car, things made of woodTable 2: Categories and examplesAlgorithm MAPThesaurus.Com .122U(M,-) .176U(W,-) .182U(D,-) .211L(D,-) .236L(D,+) .288U(MWD,-) .233U(MWD,+) .271L(MWD,-) .286L(MWD,+) .322Table 3: Comparison of algorithmsSG CG Syn HS MThesaurus.Com .041 .060 .275 .173 .060L(D,+) .377 .344 .211 .329 .177L(M,-) .102 .118 .393 .279 .119U(W,+) .097 .136 .296 .277 .165U(MWD,+) .194 .153 .438 .357 .213L(MWD,-) .344 .207 .360 .345 .173L(MWD,+) .366 .335 .379 .372 .158Table 4: Results by categorygorithms, U(*,-), add together the similarity en-tries of the seed words for a particular similaritysource.
The best of these uses distributional simi-larity; L(MWD,+) outscores it by 53%.
Combin-ing all similarity types, U(MWD,-) improves by10% over U(D,-).
L(MWD,+) improves over thebest single-source, L(D,+), by a similar margin.Using logistic regression instead of the un-trained weights significantly improves perfor-mance.
For example, L(MWD,+) outscoresU(MWD,+) by 19%.
Using active learning alsosignificantly improves performance: L(MWD,+)outscores L(MWD,-) by 13%.
This shows thatactive learning is useful even when a reasonableamount of initial information is available (threeseed words for each test case).
The gains fromlogistic regression and active learning are cumula-tive; L(MWD,+) outscores U(MWD,-) by 38%.Finally, our best system, L(MWD,+) improvesover L(D,-), the best system using a single datasource and no active learning, by 36%.
We con-sider L(D,-) to be a strong baseline; this compari-son demonstrates the usefulness of the main con-tributions of this paper, the use of multiple datasources and active learning.
L(D,-) is still fairlysophisticated, since it combines information fromthe similarity entries for different words.Table 4 shows the breakdown of results by cat-egory.
For this chart, we chose the best set-ting for each similarity type.
Broadly speaking,the thesauri work reasonably well for synonyms,but poorly for groups.
Meronyms were difficultacross the board.
Neither logistic regression noractive learning always improved performance, butL(MWD,+) performs near the top for every cate-gory.
The complex groups category is particularlyinteresting, because achieving high performanceon this category required using both logistic re-gression and active learning.
This makes sensesince negative evidence is particularly importantfor this category.7 Discussion and Related WorkThe biggest difference between our system andprevious work is the use of active learning, espe-cially in allowing the use of negative examples.Most previous set expansion systems use boot-strapping from a small set of positive examples.Recently, the use of negative examples for set ex-pansion was proposed by Vyas and Pantel (2009),although in a different way.
First, set expansion isrun as normal using a fixed seed set.
Then, humanannotators label a small number of negative exam-ples from the returned results, which are used toweed out other bad answers.
Our method incorpo-rates negative examples at an earlier stage.
Also,we use a logistic regression model to robustly in-corporate negative information, rather than deter-ministically ruling out words and features.Our system is limited by our data sources.
Sup-pose we want actors who appeared in Star Wars.
Ifwe only know that Harrison Ford andMark Hamillare actors, we have little to go on.
There hasbeen a large amount of work on other sources ofword-similarity.
Hughes and Ramage (2007) userandom walks over WordNet, incorporating infor-mation such as meronymy and dictionary glosses.Snow et al (2006) extract hypernyms from freetext.
Wang and Cohen (2007) exploit web-pagestructure, while Pasca and Durme (2008) exam-ine query logs.
We expect that adding these typesof data would significantly improve our system.375ReferencesFellbaum, C.
(Ed.).
(1998).
Wordnet: An elec-tronic lexical database.
MIT Press.Ghahramani, Z., & Heller, K. (2005).
Bayesiansets.
Advances in Neural Information Process-ing Systems (NIPS).Hughes, T., & Ramage, D. (2007).
Lexical se-mantic relatedness with random graph walks.EMNLP-CoNLL.Jiang, J., & Conrath, D. (1997).
Semantic similar-ity based on corpus statistics and lexical taxon-omy.
Proceedings of International Conferenceon Research in Computational Linguistics.Lin, D. (1998).
An information-theoretic defini-tion of similarity.
Proceedings of ICML.Liu, D. C., & Nocedal, J.
(1989).
On the lim-ited memory method for large scale optimiza-tion.
Mathematical Programming B.Pantel, P., Crestan, E., Borkovsky, A., Popescu,A., & Vyas, V. (2009).
Web-scale distributionalsimilarity and entity set expansion.
EMNLP.Pasca, M., & Durme, B. V. (2008).
Weakly-supervised acquisition of open-domain classesand class attributes from web documents andquery logs.
ACL.Roark, B., & Charniak, E. (1998).
Noun-phraseco-occurrence statistics for semiautomatic se-mantic lexicon construction.
ACL-COLING.Snow, R., Jurafsky, D., & Ng, A.
(2006).
Seman-tic taxonomy induction from heterogenous evi-dence.
ACL.Vyas, V., & Pantel, P. (2009).
Semi-automatic en-tity set refinement.
NAACL/HLT.Vyas, V., Pantel, P., & Crestan, E. (2009).
Helpingeditors choose better seed sets for entity expan-sion.
CIKM.Wang, R., & Cohen, W. (2007).
Language-independent set expansion of named entities us-ing the web.
Seventh IEEE International Con-ference on Data Mining.376
