Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 65?75,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsMulti-class Animacy Classification with Semantic FeaturesJohannes BjervaCenter for Language and Cognition GroningenUniversity of GroningenThe Netherlandsj.bjerva@rug.nlAbstractAnimacy is the semantic property of nounsdenoting whether an entity can act, or isperceived as acting, of its own will.
Thisproperty is marked grammatically in var-ious languages, albeit rarely in English.It has recently been highlighted as a rele-vant property for NLP applications such asparsing and anaphora resolution.
In orderfor animacy to be used in conjunction withother semantic features for such applica-tions, appropriate data is necessary.
How-ever, the few corpora which do containanimacy annotation, rarely contain muchother semantic information.
The additionof such an annotation layer to a corpus al-ready containing deep semantic annotationshould therefore be of particular interest.The work presented in this paper containsthree main contributions.
Firstly, we im-prove upon the state of the art in multi-class animacy classification.
Secondly, weuse this classifier to contribute to the anno-tation of an openly available corpus con-taining deep semantic annotation.
Finally,we provide source code, as well as trainedmodels and scripts needed to reproducethe results presented in this paper, or aidin annotation of other texts.11 IntroductionAnimacy is the semantic property of nouns de-noting whether, or to what extent, the referentof that noun is alive, human-like or even cogni-tively sophisticated.
Several ways of characteris-ing the animacy of such referents have been pro-posed in the literature, the most basic distinctionbeing between animate and inanimate entities.
In1https://github.com/bjerva/animacysuch a binary scheme, examples of animate nounsmight include author and dog, while examplesof inanimate nouns might include table and rock.More elaborate schemes tend to represent a hier-archy or continuum typically ranging from HU-MAN ?
NON-HUMAN ?
INANIMATE (cf.
Com-rie (1989)), with other categories in between.In various languages, animacy affects linguis-tic phenomena such as case marking and argumentrealization.
Furthermore, hierarchical restrictionsare often imposed by animacy, e.g.
with subjectstending to be higher in an animacy hierarchy thanobjects (Dahl and Fraurud, 1996).
Even thoughanimacy is rarely overtly marked in English, it stillinfluences the choice of certain grammatical struc-tures, such as the choice of relative pronouns (e.g.who vs. which).The aims of this work are as follows: (i) to im-prove upon the state of the art in multi-class an-imacy classification by comparing and evaluatingdifferent classifiers and features for this task, (ii) toinvestigate whether a corpus of spoken languagecontaining animacy annotation can be used as abasis to annotate animacy in a corpus of writtenlanguage, (iii) to use the resulting classifier as partof the toolchain used to annotate a corpus contain-ing deep semantic annotation.The remainder of this paper is organized as fol-lows: In Section 2 we go through the relevance ofanimacy for Natural Language Processing (NLP)and describe some corpora which contain animacyannotation.
Previous attempts and approaches toanimacy classification are portrayed in Section 3.Section 4 contains an overview of the data usedin this study, as well as details regarding the man-ual annotation of animacy carried out as part ofthis work.
The methods employed and the resultsobtained are presented in Sections 5 and 6.
Thediscussion is given in Section 7.
Finally, Section 8contains conclusions and some suggestions for fu-ture work in multi-class animacy classification.652 Background2.1 Relevance of animacy for NLPAlthough seemingly overlooked in the past, ani-macy has recently been shown to be an impor-tant feature for NLP.
?vrelid & Nivre (2007)found that the accuracy of a dependency parser forSwedish could be improved by incorporating a bi-nary animacy distinction.
Other work has high-lighted animacy as relevant for anaphora and co-reference resolution (Or?asan and Evans, 2007; Leeet al., 2013) and verb argument disambiguation(Dell?Orletta et al., 2005).Furthermore, in English, the choices for dativealternation (Bresnan et al., 2007), between geni-tive constructions (Stefanowitsch, 2003), and be-tween active and passive voice (Rosenbach, 2008)are also affected by the animacy of their con-stituent nouns.
With this in mind, Zaenen et al.
(2004) suggest that animacy, for languages suchas English, is not a matter of grammatical and un-grammatical sentences, but rather of sentences be-ing more and less felicitous.
This highlights anno-tation of animacy as potentially particularly usefulfor applications such as Natural Language Gener-ation.In spite of this, animacy appears to be rarely an-notated in corpora, and thus also rather rarely usedin tools and algorithms for NLP (although somerecent efforts do exist, cf.
Moore et al.
(2013)).Furthermore, the few corpora that do include ani-macy in their annotation do not contain much othersemantic annotation, making them less interestingfor computational semanticists.2.2 Annotation of animacyResources annotated with animacy are few andfar between.
One such resource is the MC160dataset which has recently been labelled for bi-nary animacy (Moore et al., 2013).
The distinc-tion between animate and inanimate was based onwhether or not an entity could ?move under itsown will?.
Although interesting, the size of thisdata set (approximately 8,000 annotated nouns)limits its usefulness, particularly with the methodsused in this paper.Talbanken05 is a corpus of Swedish spoken lan-guage which includes a type of animacy annota-tion (Nivre et al., 2006).
However, this annotationis better described as a distinction between humanand non-human, than between animate and inani-mate (?vrelid, 2009).
Although the work in thispaper focusses on English, a potential applicationof this corpus is discussed at the end of this paper(see Section 8).The NXT Switchboard corpus represents alarger and more interesting resource for our pur-poses (Calhoun et al., 2010).
This spoken lan-guage corpus contains high quality manual anno-tation of animacy for nearly 200,000 noun phrases(Zaenen et al., 2004).
Furthermore, the annota-tion is fairly fine-grained, as a total of ten animacycategories are used (see Table 1), with a few addi-tional tags for mixed animacy and cases in whichannotators were uncertain.
This scheme can bearranged hierarchically, so that the classes Con-crete, Non-concrete, Place and Time are groupedas inanimate, while the remaining classes aregrouped as animate.
The availability of this dataallows us to easily exploit the annotation for a su-pervised learning approach (see Section 5).3 Related workIn this section we will give an overview of previ-ous work in animacy classification, some of whichhas inspired the approach presented in this paper.3.1 Exploiting corpus frequenciesA binary animacy classifier which uses syntacticand morphological features has been previouslydeveloped for Norwegian and Swedish (?vrelid,2005; ?vrelid, 2006; ?vrelid, 2009).
The fea-tures used are based on frequency counts from thedependency-parsed Talbanken05 corpus.
Thesefrequencies are counted per noun lemma, mean-ing that this classifier is not context sensitive.
Inother words, cases of e.g.
polysemy where head isinanimate in the sense of human head, but animatein the sense of head of an organization, are likelyto be problematic.
Intuitively, by taking context orsemantically motivated features into account, suchcases ought to be resolved quite trivially.This classifier performs well, as it reaches anaccuracy for 96.8% for nouns, as compared to abaseline of 90.5% when always picking the mostcommon class (?vrelid, 2009).
Furthermore, it isshown that including the binary distinction fromthis classifier as a feature in dependency parsingcan significantly improve its labelled attachmentscore (?vrelid and Nivre, 2007).A more language-specific system for animacyclassification has also been developed for Japanese(Baker and Brew, 2010).
In this work, vari-66Table 1: Overview of the animacy tag set from Zaenen et al.
(2004) with examples from the GMB.Tag Description ExamplesHUM Human Mr. Calderon said Mexico has become a worldwide leader ...ORG Organization Mr. Calderon said Mexico has become a worldwide leader ...ANI Animal There are only about 1,600 pandas still living in the wild in China.LOC Place There are only about 1,600 pandas still living in the wild in China.NCN Non-concrete There are only about 1,600 pandas still living in the wild in China.CNC Concrete The wind blew so much dust around the field today.TIM Time The wind blew so much dust around the field today.MAC Machine The astronauts attached the robot, called Dextre, to the ...VEH Vehicle Troops fired on the two civilians riding a motorcycle ...ous language-specific heuristics are used to im-prove coverage of, e.g., loanwords from English.The features used are mainly frequency counts ofnouns as subjects or objects of certain verbs.
Thisis then fed to a Bayesian classifier, which yieldsquite good results on both Japanese and English.Taking these works into account, it is clear thatthe use of morphosyntactic features can providerelevant information for the task of animacy clas-sification.
However, both of these approaches usebinary classification schemes.
It is therefore notclear whether acceptably good results could be ob-tained for more elaborate schemes.3.2 Exploiting lexico-semantic resourcesOr?asan & Evans (2007) present an animacy classi-fier which is based on knowledge obtained fromWordNet (Miller, 1995).
In one approach, theybase this on the so-called unique beginners at thetop of the WordNet hierarchy.
The fact that someof these are closely related to animacy is then usedto infer the animacy of their hyponyms.
The inclu-sion of the classifications obtained by this systemfor the task of anaphora resolution is shown to im-prove its results.An animacy classifier based on exploiting syn-onymy relations in addition to hyponymy and hy-peronymy has been described for Basque (de Il-laraza et al., 2002).
In this work, a small set con-sisting of 100 nouns was manually annotated.
Us-ing an electronic dictionary from which semanticrelations could be inferred, they then further auto-matically annotated all common nouns in a 1 mil-lion word corpus.An approach to animacy classification forDutch is presented in Bloem & Bouma (to ap-pear).
This approach exploits a lexical semanticresource, from which word-senses were obtainedand merged per lemma.
This is done, as they pos-tulate that ambiguity in animacy per lemma oughtto be relatively rare.
Each lemma was then as-signed a simplified animacy class depending onits animacy category ?
either human, non-humanor inanimate.
Similarly to Baker & Brew (2010),they also use dependency features obtained froman automatically parsed corpus for Dutch.
Thistype-based approach obtains accuracies in the low90% range, compared to a most frequent classbaseline of about 81%.Based on the three aforementioned works, it isclear that the use of semantic relations obtainedfrom lexico-semantic resources such as WordNetare particularly informative for the classificationof animacy.3.3 Multi-class animacy classificationAn animacy classifier which distinguishes be-tween ten different classes of animacy has beendeveloped by Bowman & Chopra (2012).
Theyuse a simple logistic regression classifier andquite straight-forward bag-of-words and PoS fea-tures, as well as subject, object and PP dependen-cies.
These are obtained from the aforementionedSwitchboard corpus, for which they obtain quitegood results.A quite involved system for animacy classifi-cation based on using an ensemble of voters ispresented by Moore et al.
(2013).
This systemdraws its strengths from the fact that it, ratherthan defining and using a large number of featuresand training one complex classifier, uses more in-terpretable voting models which differ dependingon the class in question.
They distinguish be-tween three categories, namely person, animal and67inanimate.
The voters comprise a variety of sys-tems, based on the n-gram list method of Ji andLin (2009), a WordNet-based approach similar toOr?asan & Evans (2007), and several others.
Theirresults yield animacy detection rates in the mid-90% range, and can therefore be seen as an im-provement upon the state of the art.
However,comparison between animacy classification sys-tems is not all that straight-forward, consideringthe disparity between the data sets and classifica-tion schemes used.These two works show that multi-class animacyclassification can be successfully done both withsyntactic and semantic features.4 DataTwo annotated corpora are used in this work.
Afurther data source is concreteness ratings ob-tained through manual annotation (Brysbaert etal., 2013), and is used as a feature in the classifier.These ratings were obtained for approximately40,000 English words and two-word expressions,through the use of internet crowd-sourcing.
Therating was given on a five-point scale, rangingfrom abstract, or language based, to concrete, orexperience based (Brysbaert et al., 2013).4.1 The NXT Switchboard CorpusFirstly, the classifier is trained and evaluated on theSwitchboard corpus, as this allows for direct com-parison of results to at least one previous approach(i.e.
Bowman & Chopra (2012)).4.1.1 Pre-processing of spoken dataThe fact that the Switchboard corpus consists oftranscribed spoken data presents challenges forsome of the tools used in the feature extractionprocess.
The primary concern identified, apartfrom the differing form of spoken language ascompared to written language, is the presence ofdisfluency markers in the transcribed texts.
As apreprocessing step, all disfluencies were removedusing a simple automated script.
Essentially, thisconsisted of removing all words tagged as interjec-tions (labelled with the tag UH), as this is the tagassigned to disfluencies in the Switchboard cor-pus.
Although interjections generally can be in-formative, the occurrences of interjections withinNPs was restricted to usage as disfluencies.4.2 The Groningen Meaning BankThere are several corpora of reasonable size whichinclude semantic annotation on some level, such asPropBank (Palmer et al., 2005), FrameNet (Bakeret al., 1998), and the Penn Discourse TreeBank(Prasad et al., 2005).
The combination of sev-eral levels of semantic annotation into one formal-ism are not common, however.
Although some ef-forts exist, they tend to lack a level of formallygrounded ?deep?
semantic representation whichcombines these layers.The Groningen Meaning Bank (GMB) containsa substantial collection of English texts with suchdeep semantic annotation (Basile et al., 2012a).One of its goals is to combine semantic phenom-ena into a single formalism, as opposed to deal-ing with single phenomena in isolation.
This pro-vides a better handle on explaining dependenciesbetween various ambiguous linguistic phenomena.Manually annotating a comprehensive corpuswith gold-standard semantic representations is ob-viously a hard and time-consuming task.
There-fore, a sophisticated bootstrapping approach isused.
Existing NLP tools are used to get a rea-sonable approximation of the target annotationsto start with.
Pieces of information coming fromboth experts (linguists) and crowd sourcing meth-ods are then added in to improve the annotation.The addition of animacy annotation is done in thesame manner.
First, the animacy classifier willbe incorporated into this toolchain.
We then cor-rect the tags for a subset of the corpus, whichis also used to evaluate the classifier.
Note thatthe classifier used in the toolchain uses a differentmodel from the conditions where we evaluate onthe Switchboard corpus.
For the GMB, we includetraining data obtained through the crowd-sourcinggame Wordrobe, which uses a subset of the datafrom the GMB (Venhuizen et al., 2013).4.2.1 AnnotationSo as to allow for evaluation of the classifier ona widely used semantically annotated corpus, onepart (p00) of the GMB was semi-manually anno-tated for animacy, although this might lead to abias with potentially overly good results for ourclassifier, if annotators are affected by its out-put.
We use the tagset presented by Zaenen etal.
(2004), which is given in Table 1.
This tagsetwas chosen for the addition of animacy annota-tion to the GMB.
Including this level of annotation68Figure 1: A tagged document in the GMB.in a resource which already contains other seman-tic annotation should prove particularly useful, asthis allows animacy to be used in conjunction withother semantically based features in NLP tools andalgorithms.
This annotation was done using theGMB?s interface for expert annotation (Basile etal., 2012b).
A total of 102 documents, contain-ing approximately 15,000 tokens, were annotatedby an expert annotator, who corrected the tags as-signed by the classifier.
We assign animacy tagsto all nouns and pronouns.
Similarly to our tag-ging convention for named entities, we assign thesame tag to the whole NP, so that wagon driveris tagged with HUM, although wagon in isolationwould be tagged with CNC.
This has the addedadvantage that this is the manner in which NPs areannotated in the Switchboard corpus, making eval-uation and comparison with Bowman & Chopra(2012) somewhat more straight-forward.
An ex-ample of a tagged document can be seen in Fig-ure 1.
Table 2 shows the amount of annotatednouns per class.
In order to verify the integrity ofthis annotation, two other experts annotated a ran-dom selection of ten documents.
Inter-annotatoragreement was calculated using Fleiss?
kappa onthis selection, yielding a score of ?
= .596.Table 2: Annotation statistics for p00 of the GMBHUM NCN CNC TIM ORG LOC ANI VEH MAC1436 2077 79 500 887 512 67 28 05 Method5.1 ClassifiersWe experiment using four different classifiers (seeTable 3).
All classifiers used are obtained fromthe implementations provided by SciKit-learn (Pe-dregosa et al., 2011).
For each type of classifier,we train one classifier for each class in a one-versus-all fashion.
For source code, trained mod-els and scripts to run the experiments in this paper,please see https://github.com/bjerva/animacy.The classifiers are trained on a combination ofthe Switchboard corpus and data gathered fromWordrobe, depending on the experimental condi-tion.
In addition to the features explained below,the classifier exploits named entity tags, in thatthese override the proposed animacy tag where ap-plicable.
That is to say, if a named entity has al-ready been identified and tagged as, e.g., a person,this is reflected in the animacy layer with the HUMtag.Considering that the balance between samplesper class is quite skewed, an attempt was made atplacing lower weights on the samples from the ma-jority classes.
Although this did lead to a marginalincrease in accuracy for the minority classes, over-all accuracy dropped to such an extent that thisweighting was not used for the results presentedin this work.5.2 FeaturesIn this section, an overview of the features used bythe classifiers is given.5.2.1 Bag-of-words featureThe simplest feature used consists of looking ateach lemma in the NP to be classified, and theircorresponding PoS tags.
We also experimentedwith using whole sentences as context for classi-fication, but as this worsened results on our devel-opment data, it was not used for the evaluationslater in the paper.5.2.2 Concreteness ratingsConsidering that two of the categories in ourtag set discriminate between concrete and non-concrete entities, we include concreteness ratings69Table 3: Overview of the classifiers used in the experiments.Classifier Reference Parameter settingsLogistic Regression (MaxEnt) (Berger et al., 1996) `2 regularizationSupport Vector Machine (SVM) (Joachims, 1998) linear kernelStochastic Gradient Descent (SGD) (Tsuruoka et al., 2009) `2 regularization, hinge lossBernoulli Naive Bayes (B-NB) (McCallum et al., 1998) ?as a feature in the classifier (Brysbaert et al.,2013).
In its original form, these ratings are quitefine-grained as they are provided with the averageconcreteness score given by annotators on a scale.We experimented with using different granulari-ties of these scores as a feature.
A simple binarydistinction where anything with a score of c > 2.5being represented as concrete, and c ?
2.5 be-ing represented as non-concrete yielded the bestresults, and is used in the evaluations in this paper.5.2.3 WordNet distancesWe also include a feature based on WordNet dis-tances.
In this work, we use the path distance sim-ilarity measure provided in NLTK (Bird, 2006).In essence, this measure provides a score basedon the shortest path that connects the senses in ahypernym/hyponym taxonomy.
First, we calcu-late the distance to each hypernym of every givenword.
These distances are then summed togetherfor each animacy class.
Taking the most fre-quent hypernym for each animacy class gives usthe following hypernyms: person.n.01, abstrac-tion.n.06, city.n.01, time period.n.01, car.n.01, or-ganization.n.01, artifact.n.01, animal.n.01, ma-chine.n.01, buddy.n.01.
The classifier then useswhichever of these words is closest as its Word-Net feature.5.2.4 Thematic rolesThe use of thematic roles for animacy annotationconstitutes a novel contribution from this work.Intuitively this makes sense, as e.g.
agents tend tobe animate.
Although the GMB contains an anno-tation layer with thematic roles, the Switchboardcorpus does not.
In order to use this feature, wetherefore preprocessed the latter using Boxer (Bos,2008).
We use the protoroles obtained from Boxer,namely agent, theme and patient.
Although auto-matic annotation does not provide 100% accuracy,especially on such a particular data set, this featureproved somewhat useful (see Section 6.1.2).6 Results6.1 Evaluation on the Switchboard corpusWe employ 10-fold cross validation for the evalua-tions on the Switchboard corpus.
All NPs were au-tomatically extracted from the pre-processed cor-pus, put into random order and divided into tenequally-sized folds.
In each of the ten cross valida-tion iterations, one of these folds was left out andused for evaluation.
For the sake of conciseness,averaged results over all classes are given in thecomparisons of Section 6.1.1 and Section 6.1.2,whereas detailed results are only given for the bestperforming classifier.
Note that the training datafrom Wordrobe is not used for the evaluations onthe Switchboard corpus, as this would prohibit fairevaluation with previous work.6.1.1 Classifier evaluationWe first ran experiments to evaluate which of theclassifiers performed the best on this task.
Figure 2shows the average accuracy for each classifier, us-ing 10-fold cross validation on the Switchboardcorpus.
Table 4 contains the per-class results fromthe cross validation performed with the best per-forming classifier, namely the Logistic Regressionclassifier.
The remaining evaluations in this pa-per are all carried out with this classifier.
Aver-age accuracy over the 10 folds was 85.8%.
Thisis well above the baseline of always picking themost common class (HUM), which results in an ac-curacy of 45.3%.
More interestingly, this is some-what higher than the best results for this datasetreported in the literature (84.9% without cross val-idation (Bowman and Chopra, 2012)).6.1.2 Feature evaluationUsing the best performing classifier, we ran exper-iments to evaluate how different features affect theresults.
These experiments were also performedusing 10-fold cross validation on the Switchboardcorpus.
Table 5 shows scores from using only one70MaxEntSGD SVMB-NB40506070809085.88284.280.8Figure 2: Accuracy of the classifiers, using 10-fold cross validation on the Switchboard corpus.The dashed line represents the most frequent classbaseline.feature in addition to the lemma and PoS of thehead of the NP to be classified.
Although none ofthe features in isolation add much to the perfor-mance of the classifier, some marginal gains canbe observed.Table 5: Comparison of the effect of including sin-gle features, from cross validation on the Switch-board corpus.
All conditions consist of the fea-ture named in the condition column in addition toLemma+PoS.Condition Precision Recall F-scoreLemma+PoS 0.846 0.850 0.848Bag of Words 0.851 0.856 0.853Concreteness 0.847 0.851 0.849WordNet 0.849 0.855 0.852Thematic Roles 0.847 0.851 0.849All features 0.851 0.857 0.8546.1.3 Performance on unknown wordsFor a task such as animacy classification, wheremany words can be reliably classified based solelyon their lemma and PoS tag, it is particularly in-teresting to investigate performance on unknownwords.
As in all other conditions, this was evalu-ated using 10-fold cross validation on the Switch-board corpus.
It should come as no surprise thatthe results are substantially below those for knownwords, for every single class.
The average accu-racy for this condition was 59.2%, which can becompared to the most frequent class (NCN) base-line at 43.0%.6.2 Evaluation on the GMBSince one of the purposes of the development ofthis classifier was to include it in the tools used inthe tagging of the GMB, we also present the firstresults in the literature for the animacy annotationof this corpus.
Due to the limited size of the por-tion of this corpus for which animacy tags havebeen manually corrected, no cross-validation wasperformed.
However, due to the high differencesin the training data from the Switchboard corpus,and the evaluation data in the GMB, the resultscould be seen as a lower bound for this classifieron this data set.
Table 4 contains the results fromthis evaluation.
The accuracy on this dataset was79.4%, which can be compared to a most frequentclass baseline of 37.2%.6.3 Excluding pronounsThe discrepancy between the results obtainedfrom the Switchboard corpus and the GMB doescall for some investigation.
Considering that theSwitchboard corpus consists of spoken language,it contains a relatively large amount of personalpronouns compared to, e.g., news text.
Taking intoaccount that these pronouns are rarely ambiguousas far as animacy is concerned, it seems feasiblethat this may be why the results for the Switch-board corpus are better than those of the GMB.To evaluate this, a separate experiment was runin which all pronouns were excluded.
As a largeamount of pronouns are tagged as HUM, the F-scores for this class dropped by 8% and 5% forthe Switchboard corpus and GMB respectively.For the GMB, results for other classes remainedfairly stable, most likely due to there not beingmany pronouns present which affect the remain-ing classes.
For the Switchboard corpus, however,an increase in F-score was observed for severalclasses.
This might be explained by that the ex-clusion of pronouns lowered the classifier?s pre-existing bias for the HUM class, as the numberof annotated examples was lowered from approxi-mately 85,000 to 15,000.Animacy classification of pronouns can be con-sidered trivial, as there is little or no ambiguity ofthat the referent of e.g.
he is HUM.
Even so, pro-nouns were included in the main results provided71Table 4: Results from 10-fold cross validation on the Switchboard corpus and evaluation on the GMB.Switchboard GMBClass Count Precision Recall F-score Count Precision Recall F-scoreHUM 82596 0.91 0.97 0.94 1436 0.82 0.79 0.80NCN 62740 0.82 0.94 0.88 2077 0.76 0.88 0.82CNC 12425 0.75 0.43 0.55 79 0.48 0.13 0.20TIM 7179 0.88 0.85 0.87 500 0.77 0.95 0.85ORG 6847 0.71 0.26 0.38 887 0.85 0.68 0.75LOC 5592 0.71 0.66 0.69 512 0.89 0.71 0.79ANI 2362 0.89 0.36 0.51 67 0.63 0.22 0.33VEH 1840 0.89 0.45 0.59 28 1.00 0.39 0.56MAC 694 0.80 0.34 0.47 - - - -MIX 34 0.00 0.00 0.00 - - - -here, as this is the standard manner of reportingresults in prior work.6.4 Summary of resultsTable 6 contains a brief overview of the most es-sential results from this work.
For the Switchboardcorpus, this constitutes the current best results inthe literature.
As for the GMB, this constitutes thefirst results in the literature for animacy classifica-tion.Table 6: Main results from all conditions.
B&C(2012) refers to Bowman & Chopra (2012).Corpus Condition AccuracySwitchboardB&C (2012) 0.849Unknown words 0.592Known words 0.860All words 0.858GMBUnknown words 0.764Known words 0.831All words 0.7947 DiscussionThe work presented in this paper constitutes a mi-nor improvement to the previously best results formulti-class animacy classification on the Switch-board corpus (Bowman and Chopra, 2012).
Ad-ditionally, we also present the first results in theliterature for animacy classification on the GMB,allowing for future research to use this work as apoint of comparison.
It is, however, important tonote that the results obtained for the GMB in thispaper are prone to bias, as the annotation proce-dure was done in a semi-automatic fashion.
If an-notators were affected by the output of the clas-sifier, this is likely to have improved the resultspresented here.A striking factor when observing the results, isthe high discrepancy in performance between theGMB and the Switchboard corpus.
This is, how-ever, not all that surprising.
Considering that theSwitchboard corpus consists of spoken language,and the GMB contains written language, one caneasily draw the conclusion that the domain dif-ferences pose a substantial obstacle.
This can,for instance, be seen in the differing vocabulary.In the cross-validation conditions for the Switch-board corpus, approximately 1% of the words tobe classified in each fold are unknown to the clas-sifier.
As for the GMB, approximately 10% ofthe words are unknown.
As mentioned in Sec-tion 6.1.2, the lemma of the head noun in an NPis a very strong feature, which naturally can not beused in the case of unknown words.
As seen in Ta-ble 6, performance on known words in the GMBis not far away from that of known words in theSwitchboard corpus.Although a fairly good selection of classifierswere tested in this work, there is room for im-provement in this area.
The fact that the LogisticRegression classifier outperformed all other clas-sifiers is likely to have been caused by that notenough effort was put into parameter selection forthe other classifiers.
More sophisticated classi-fiers, such as Artificial Neural Networks, ought to72at the very least replicate the results achieved here.Quite likely, results should even improve, seeingthat the added computational power of ANNs al-lows us to capture more interesting/deeper statisti-cal patterns, if they exist in the data.The features used in this paper mainly revolvedaround semantically oriented ones, such as seman-tic relations from WordNet, thematic roles and, ar-guably, concreteness ratings.
Better results couldmost likely be achieved if one also incorporatedmore syntactically oriented features, such as fre-quency counts from a dependency parsed corpus,as done by e.g.
Bowman & Chopra (2012) and?vrelid (2009).
Other options include the use ofmore linguistically motivated features, such as ex-ploiting relative pronouns (i.e.
who vs. which).8 Conclusions and future workAt the beginning of this paper, we set out threeaims.
Firstly, we wanted to improve upon thestate of the art in multi-class animacy classifica-tion.
A conclusive statement to that effect is hardto make, considering that comparison was onlymade directly to one previous work.
However, asour performance compared to this work was some-what higher, this work certainly marks some sortof improvement.
Secondly, we aimed at investi-gating whether a corpus of spoken language con-taining animacy annotation could be used to anno-tate a corpus of written language.
As our resultsfor the GMB are well above the baseline, we con-clude that this is indeed feasible, in spite of thedisparities between language form and vocabulary.Lastly, we aimed at using the resulting classifier asa part of the toolchain used to annotate the GMB.This goal has also been met.As for future work, the fact that animacy ismarked explicitly in many languages presents agolden opportunity to alleviate the annotation ofthis semantic property for languages in which itis not explicitly marked.
By identifying thesemarkers, the annotation of animacy in such a lan-guage should be relatively trivial through the useof parallel texts.
Alternatively, one could lookat using existing annotated corpora, such as Tal-banken05 (Nivre et al., 2006), as a source of an-notation.
One could then look at transferring thisannotation to a second language.
Although intu-itively promising, this approach has some poten-tial issues, as animacy is not represented univer-sally across languages.
For instance, fluid contain-ers (e.g.
cups, spoons) represent a class of nounswhich are considered grammatically animate inAlgonquian (Quinn, 2001).
Annotating such itemsas animate in English would most likely not beconsidered correct, neither by native speakers norby most experts.
Nevertheless, if a sufficientlylarge amount of languages have some manner ofconsensus as to where a given entity is in an ani-macy hierarchy, this problem ought to be solvableby simply hand-picking such languages.73ReferencesKirk Baker and Chris Brew.
2010.
Multilingual an-imacy classification by sparse logistic regression.OSUWPL, 59:52?75.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In 36th An-nual Meeting of the Association for ComputationalLinguistics and 17th International Conference onComputational Linguistics.
Proceedings of the Con-ference, pages 86?90, Universit?e de Montr?eal, Mon-treal, Quebec, Canada.Valerio Basile, Johan Bos, Kilian Evang, and NoortjeVenhuizen.
2012a.
Developing a large semanticallyannotated corpus.
In Proceedings of the Eighth In-ternational Conference on Language Resources andEvaluation (LREC 2012), pages 3196?3200, Istan-bul, Turkey.Valerio Basile, Johan Bos, Kilian Evang, and NoortjeVenhuizen.
2012b.
A platform for collaborative se-mantic annotation.
In Proceedings of the Demon-strations at the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL), pages 92?96, Avignon, France.Adam L Berger, Vincent J Della Pietra, and StephenA Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Computa-tional linguistics, 22(1):39?71.Steven Bird.
2006.
NLTK: the natural languagetoolkit.
In Proceedings of the COLING/ACL on In-teractive presentation sessions, pages 69?72.
Asso-ciation for Computational Linguistics.Jelke Bloem and Gosse Bouma.
to appear.
Automaticanimacy classification for dutch.
ComputationalLinguistics in the Netherlands Journal, 3, 12/2013.Johan Bos.
2008.
Wide-Coverage Semantic Analy-sis with Boxer.
In J. Bos and R. Delmonte, editors,Semantics in Text Processing.
STEP 2008 Confer-ence Proceedings, volume 1 of Research in Compu-tational Semantics, pages 277?286.
College Publi-cations.Samuel R Bowman and Harshit Chopra.
2012.
Au-tomatic animacy classification.
In Proceedings ofthe 2012 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies: Student ResearchWorkshop, pages 7?10.
Association for Computa-tional Linguistics.Joan Bresnan, Anna Cueni, Tatiana Nikitina, R HaraldBaayen, et al.
2007.
Predicting the dative alterna-tion.
Cognitive foundations of interpretation, pages69?94.Marc Brysbaert, Amy Beth Warriner, and Victor Ku-perman.
2013.
Concreteness ratings for 40 thou-sand generally known english word lemmas.
Behav-ior Research Methods, pages 1?8.Sasha Calhoun, Jean Carletta, Jason M Brenier, NeilMayo, Dan Jurafsky, Mark Steedman, and DavidBeaver.
2010.
The nxt-format switchboard corpus:a rich resource for investigating the syntax, seman-tics, pragmatics and prosody of dialogue.
LanguageResources and Evaluation, 44(4):387?419.Bernard Comrie.
1989.
Language universals and lin-guistic typology: Syntax and morphology.
Univer-sity of Chicago press.
?Osten Dahl and Kari Fraurud.
1996.
Animacy in gram-mar and discourse.
PRAGMATICS AND BEYONDNEW SERIES, pages 47?64.Arantza D?
?az de Illaraza, Aingeru Mayor, and KepaSarasola.
2002.
Semiautomatic labelling of seman-tic features.
In Proceedings of the 19th InternationalConference on Computational Linguistics.Felice Dell?Orletta, Alessandro Lenci, Simonetta Mon-temagni, and Vito Pirrelli.
2005.
Climbing the pathto grammar: A maximum entropy model of sub-ject/object learning.
In Proceedings of the Work-shop on Psychocomputational Models of HumanLanguage Acquisition, pages 72?81.
Association forComputational Linguistics.Heng Ji and Dekang Lin.
2009.
Gender and animacyknowledge discovery from web-scale n-grams forunsupervised person mention detection.
In PACLIC,pages 220?229.Thorsten Joachims.
1998.
Text categorization withsupport vector machines: Learning with many rel-evant features.
Springer.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolutionbased on entity-centric, precision-ranked rules.Andrew McCallum, Kamal Nigam, et al.
1998.
Acomparison of event models for naive bayes textclassification.
In AAAI-98 workshop on learning fortext categorization, volume 752, pages 41?48.
Cite-seer.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Joshua L. Moore, Christopher J.C. Burges, Erin Ren-shaw, and Yih Wen-tau.
2013.
Animacy detec-tion with voting models.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing, pages 55?60.
Association forComputational Linguistics.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006.Talbanken05: A swedish treebank with phrase struc-ture and dependency annotation.
In Proceedings ofthe fifth International Conference on Language Re-sources and Evaluation (LREC), pages 1392?1395.74Constantin Or?asan and Richard Evans.
2007.
Np ani-macy identification for anaphora resolution.
J. Artif.Intell.
Res.
(JAIR), 29:79?103.Lilja ?vrelid and Joakim Nivre.
2007.
When word or-der and part-of-speech tags are not enough?Swedishdependency parsing with rich linguistic features.In Proceedings of the International Conference onRecent Advances in Natural Language Processing(RANLP), pages 447?451.Lilja ?vrelid.
2005.
Animacy classification based onmorphosyntactic corpus frequencies: some experi-ments with norwegian nouns.
In Proc.
of the Work-shop on Exploring Syntactically Annotated Corpora.Lilja ?vrelid.
2006.
Towards robust animacy clas-sification using morphosyntactic distributional fea-tures.
In Proceedings of the Eleventh Conference ofthe European Chapter of the Association for Com-putational Linguistics: Student Research Workshop,pages 47?54.
Association for Computational Lin-guistics.Lilja ?vrelid.
2009.
Empirical evaluations of animacyannotation.
In Proceedings of the 12th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 630?638.
Associationfor Computational Linguistics.Martha Palmer, Paul Kingsbury, and Daniel Gildea.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, AlanLee, Eleni Miltsakaki, and Bonnie Webber.
2005.The Penn Discourse TreeBank as a resource for nat-ural language generation.
In Proc.
of the CorpusLinguistics Workshop on Using Corpora for NaturalLanguage Generation, pages 25?32.Conor Quinn.
2001.
A preliminary survey of animacycategories in penobscot.
In Papers of the 32nd.
Al-gonquian Conference, pages 395?426.Anette Rosenbach.
2008.
Animacy and grammaticalvariation?findings from English genitive variation.Lingua, 118(2):151?171.Anatol Stefanowitsch.
2003.
Constructional semanticsas a limit to grammatical alternation: The two gen-itives of English.
TOPICS IN ENGLISH LINGUIS-TICS, 43:413?444.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic gradient descent trainingfor l1-regularized log-linear models with cumulativepenalty.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP, volume 1, pages 477?485.Association for Computational Linguistics.Noortje J. Venhuizen, Valerio Basile, Kilian Evang, andJohan Bos.
2013.
Gamification for word senselabeling.
Proc.
10th International Conference onComputational Semantics (IWCS-2013), pages 397?403.Annie Zaenen, Jean Carletta, Gregory Garretson,Joan Bresnan, Andrew Koontz-Garboden, TatianaNikitina, M Catherine O?Connor, and Tom Wasow.2004.
Animacy encoding in english: why and how.In Proceedings of the 2004 ACL Workshop on Dis-course Annotation, pages 118?125.
Association forComputational Linguistics.75
