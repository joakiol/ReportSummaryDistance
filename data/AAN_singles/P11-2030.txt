Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 170?175,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsWord Alignment via Submodular Maximization over MatroidsHui LinDept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195, USAhlin@ee.washington.eduJeff BilmesDept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195, USAbilmes@ee.washington.eduAbstractWe cast the word alignment problem as max-imizing a submodular function under matroidconstraints.
Our framework is able to expresscomplex interactions between alignment com-ponents while remaining computationally ef-ficient, thanks to the power and generality ofsubmodular functions.
We show that submod-ularity naturally arises when modeling wordfertility.
Experiments on the English-FrenchHansards alignment task show that our ap-proach achieves lower alignment error ratescompared to conventional matching based ap-proaches.1 IntroductionWord alignment is a key component in most statisti-cal machine translation systems.
While classical ap-proaches for word alignment are based on generativemodels (e.g., IBM models (Brown et al, 1993) andHMM (Vogel et al, 1996)), word alignment can alsobe viewed as a matching problem, where each wordpair is associated with a score reflecting the desirabil-ity of aligning that pair, and the alignment is then thehighest scored matching under some constraints.Several matching-based approaches have beenproposed in the past.
Melamed (2000) introducesthe competitive linking algorithm which greedilyconstructs matchings under the one-to-one mappingassumption.
In (Matusov et al, 2004), matchingsare found using an algorithm for constructinga maximum weighted bipartite graph matching(Schrijver, 2003), where word pair scores come fromalignment posteriors of generative models.
Similarly,Taskar et al (2005) cast word alignment as amaximum weighted matching problem and propose aframework for learning word pair scores as a functionof arbitrary features of that pair.
These approaches,however, have two potentially substantial limitations:words have fertility of at most one, and interactionsbetween alignment decisions are not representable.Lacoste-Julien et al (2006) address this issue byformulating the alignment problem as a quadraticassignment problem, and off-the-shelf integer linearprogramming (ILP) solvers are used to solve to op-timization problem.
While efficient for some medianscale problems, ILP-based approaches are limitedsince when modeling more sophisticated interactions,the number of variables (and/or constraints) requiredgrows polynomially, or even exponentially, makingthe resultant optimization impractical to solve.In this paper, we treat the word alignment problemas maximizing a submodular function subject tomatroid constraints (to be defined in Section 2).Submodular objective functions can representcomplex interactions among alignment decisions,and essentially extend the modular (linear) objectivesused in the aforementioned approaches.
While ourextensions add expressive power, they do not resultin a heavy computational burden.
This is becausemaximizing a monotone submodular function undera matroid constraint can be solved efficiently usinga simple greedy algorithm.
The greedy algorithm,moreover, is a constant factor approximationalgorithm that guarantees a near-optimal solution.In this paper, we moreover show that submodularitynaturally arises in word alignment problems whenmodeling word fertility (see Section 4).
Experimentresults on the English-French Hansards alignmenttask show that our approach achieves lower align-ment error rates compared to the maximum weightedmatching approach, while being at least 50 times170faster than an ILP-based approach.2 BackgroundMatroids and submodularity both play importantroles in combinatorial optimization.
We briefly in-troduce them here, referring the reader to (Schrijver,2003) for details.Matroids are combinatorial structures that general-ize the notion of linear independence in matrices.
Apair (V, I) is called a matroid if V is a finite groundset and I is a nonempty collection of subsets of Vthat are independent.
In particular, I must satisfy (i)if X ?
Y and Y ?
I then X ?
I, (ii) if X,Y ?
Iand |X| < |Y | thenX?
{e} ?
I for some e ?
Y \X .We typically refer to a matroid by listing its groundset and its family of independent sets:M = (V, I).A set function f : 2V ?
R is called submodu-lar (Edmonds, 1970) if it satisfies the property ofdiminishing returns: for any X ?
Y ?
V \ v, a sub-modular function f must satisfy f(X+v)?f(X) ?f(Y + v)?
f(Y ).
That is, the incremental ?value?of v decreases as the context in which v is consideredgrows from X to Y .
If this is satisfied everywherewith equality, then the function f is called modu-lar.
A set function f is monotone nondecreasing if?X ?
Y , f(X) ?
f(Y ).
As shorthand, in this pa-per, monotone nondecreasing submodular functionswill simply be referred to as monotone submodular.Historically, submodular functions have their rootsin economics, game theory, combinatorial optimiza-tion, and operations research.
More recently, submod-ular functions have started receiving attention in themachine learning and computer vision community(Kempe et al, 2003; Narasimhan and Bilmes, 2004;Narasimhan and Bilmes, 2005; Krause and Guestrin,2005; Narasimhan and Bilmes, 2007; Krause et al,2008; Kolmogorov and Zabin, 2004; Jegelka andBilmes, 2011) and have recently been introducedto natural language processing for the task of docu-ment summarization (Lin and Bilmes, 2010; Lin andBilmes, 2011).3 ApproachWe are given a source language (English) string eI1 =e1, ?
?
?
, ei, ?
?
?
, eI and a target language (French)string fJ1 = f1, ?
?
?
, fj , ?
?
?
, fJ that have to bealigned.
Define the word positions in the Englishstring as set E , {1, ?
?
?
, I} and positions in theFrench string as set F , {1, ?
?
?
, J}.
An alignmentA between the two word strings can then be seen asa subset of the Cartesian product of the word posi-tions, i.e., A ?
{(i, j) : i ?
E, j ?
F} , V, andV = E ?
F is the ground set.
For convenience, werefer to element (i, j) ?
A as an edge that connects iand j in alignment A.Restricting the fertility of word fj to be at most kjis mathematically equivalent to having |A ?
PEj | ?kj , whereA ?
V is an alignment and PEj = E?
{j}.Intuitively, PEj is the set of all possible edges in theground set that connect to j, and the cardinality ofthe intersection between A and PEj indicates howmany edges in A are connected to j.
Similarly, wecan impose constraints on the fertility of Englishwords by constraining the alignment A to satisfy|A ?
PFi | ?
ki for i ?
E where PFi = {i} ?
F .Note that either of {PEj : j ?
F} or {PFi : i ?
E}constitute a partition of V .
Therefore, alignments Athat satisfy |A ?
PEj | ?
kj ,?j ?
F , are independentin the partition matroidME = (V, IE) withIE = {A ?
V : ?j ?
F, |A ?
PEj | ?
kj},and alignmentsA that satisfy |A?PFi | ?
ki, ?i ?
E,are independent in matroidMF = (V, IF ) withIF = {A ?
V : ?i ?
E, |A ?
PFi | ?
ki}.Suppose we have a set function f : 2V ?
R+ thatmeasures quality (or scores) of an alignment A ?
V ,then when also considering fertility constraints, wecan treat the word alignment problem as maximizinga set function subject to matroid constraint:Problem 1. maxA?V f(A), subject to: A ?
I,where I is the set of independent sets of a matroid (orit might be the set of independent sets simultaneouslyin two matroids, as we shall see later).Independence in partition matroids generalizesthe typical matching constraints for word alignment,where each word aligns to at most one word (kj =1,?j) in the other sentence (Matusov et al, 2004;Taskar et al, 2005).
Our matroid generalizations pro-vide flexibility in modeling fertility, and also strate-gies for solving the word alignment problem effi-ciently and near-optimally.
In particular, when fis monotone submodular, near-optimal solutions forProblem 1 can be efficiently guaranteed.171For example, in (Fisher et al, 1978), a simplegreedy algorithm for monotone submodular functionmaximization with a matroid constraint is shownto have a constant approximation factor.
Precisely,the greedy algorithm finds a solution A such thatf(A) ?
1m+1f(A?)
whereA?
is the optimal solutionand m is number of matroid constraints.
When thereis only one matroid constraint, we get an approxima-tion factor 12 .
Constant factor approximation algo-rithms are particularly attractive since the quality ofthe solution does not depend on the size of the prob-lem, so even very large size problems do well.
It isalso important to note that this is a worst case bound,and in most cases the quality of the solution obtainedwill be much better than this bound suggests.Vondra?k (2008) shows a continuous greedy al-gorithm followed by pipage rounding with approx-imation factor 1 ?
1/e (?
0.63) for maximizinga monotone submodular function subject to a ma-troid constraint.
Lee et al (2009) improve the 1m+1 -approximation result in (Fisher et al, 1978) by show-ing a local-search algorithm has approximation guar-antee of 1m+ for the problem of maximizing a mono-tone submodular function subject to m matroid con-straints (m ?
2 and  > 0).
In this paper, however,we use the simple greedy algorithm for the sake ofefficiency.
We outline our greedy algorithm for Prob-lem 1 in Algorithm 1, which is slightly different fromthe one in (Fisher et al, 1978) as in line 4 of Al-gorithm 1, we have an additional requirement on asuch that the increment of adding a is strictly greaterthan zero.
This additional requirement is to main-tain a higher precision word alignment solution.
Thetheoretical guarantee still holds as f is monotone ?i.e., Algorithm 1 is a 12 -approximation algorithm forProblem 1 (only one matroid constraint) when f ismonotone submodular.Algorithm 1: A greedy algorithm for Problem 1.input : A = ?, N = V .begin1while N 6= ?
do2a?
argmaxe?N f(A ?
{e})?
f(A);3if A ?
{a} ?
I and f(A ?
{a})?
f(A) > 04thenA?
A ?
{a}5N ?
N \ {a}.6end7Algorithm 1 requires O(|V |2) evaluations of f .
Inpractice, the argmax in Algorithm 1 can be efficientimplemented with priority queue when f is submod-ular (Minoux, 1978), which brings the complexitydown to O(|V | log |V |) oracle function calls.4 Submodular FertilityWe begin this section by demonstrating that submod-ularity arises naturally when modeling word fertility.To do so, we borrow an example of fertility from(Melamed, 2000).
Suppose a trained model estimatess(e1, f1) = .05, s(e1, f2) = .02 and s(e2, f2) = .01,where s(ei, fj) represents the score of aligning ei andfj .
To find the correct alignment (e1, f1) and (e2, f2),the competitive linking algorithm in (Melamed, 2000)poses a one-to-one assumption to prevent choosing(e1, f2) over (e2, f2).
The one-to-one assumption,however, limits the algorithm?s capability of handlingmodels with fertility larger than one.
Alternatively,we argue that the reason of choosing (e2, f2) ratherthan (e1, f2) is that the benefit of aligning e1 and f2diminishes after e1 is already aligned with f1 ?
thisis exactly the property of diminishing returns, andtherefore, it is natural to use submodular functions tomodel alignment scores.To illustrate this further, we use another realexample taken from the trial set of English-FrenchHansards data.
The scores estimated from the datafor aligning word pairs (the, le), (the, de) and (of,de) are 0.68, 0.60 and 0.44 respectively.
Givenan English-French sentence pair: ?I have stressedthe CDC as an example of creative, aggressiveeffective public ownership?
and ?je le ai cite?
commeexemple de proprie?te?
publique cre?atrice, dynamiqueet efficace?, an algorithm that allows word fertilitylarger than 1 might choose alignment (the, de) over(of, de) since 0.68 + 0.60 > 0.68 + 0.44, regardlessthe fact that the is already aligned with le.
Now ifwe use a submodular function to model the score ofaligning an English word to a set of French words,we might obtain the correct alignments (the, le) and(of, de) by incorporating the diminishing returnsproperty (i.e., the score gain of (the, de), which is0.60 out of context, could diminish to something lessthan 0.44 when evaluated in the context of (the, le)).Formally, for each i in E, we define a mapping172?i : 2V ?
2F with?i(A) = {j ?
F |(i, j) ?
A}, (1)i.e., ?i(A) is the set of positions in F that are alignedwith position i in alignment A.We use function fi : 2F ?
R+ to represent thebenefit of aligning position i ?
E to a set of positionsin F .
Given score si,j of aligning i and j, we couldhave, for S ?
F ,fi(S) =???j?Ssi,j??
?, (2)where 0 < ?
?
1, i.e., we impose a concave functionover a modular function, which produces a submod-ular function.
The value of ?
determines the ratethat the marginal benefit diminishes when aligninga word to more than one words in the other string.Summing over alignment scores in all positions inE, we obtain the total score of an alignment A:f(A) =?i?Efi(?i(A)), (3)which is again, monotone submodular.
By diminish-ing the marginal benefits of aligning a word to morethan one words in the other string, f(A) encouragesthe common case of low fertility while allowing fer-tility larger than one.
For instance in the aforemen-tioned example, when ?
= 12 , the score for aligningboth le and de to the is?0.68 + 0.60 ?
1.13, whilethe score of aligning the to le and of to de is?0.68 +?0.44 ?
1.49, leading to the correct alignment.5 ExperimentsWe evaluated our approaches using the English-French Hansards data from the 2003 NAACL sharedtask (Mihalcea and Pedersen, 2003).
This corpus con-sists of 1.1M automatically aligned sentences, andcomes with a test set of 447 sentences, which havebeen hand-aligned and are marked with both ?sure?and ?possible?
alignments (Och and Ney, 2003).
Us-ing these alignments, alignment error rate (AER) iscalculated as:AER(A,S, P ) = 1?|A ?
S|+ |A ?
P ||A|+ |S|(4)where S is the set of sure gold pairs, and P is theset of possible gold pairs.
We followed the workin (Taskar et al, 2005) and split the original test setinto 347 test examples, and 100 training examplesfor parameters tuning.In general, the score of aligning i to j can bemodeled as a function of arbitrary features.
Althoughparameter learning in our framework would beanother interesting topic to study, we focus herein onthe inference problem.
Therefore, only one feature(Eq.
5) was used in our experiments in order for nofeature weight learning to be required.
In particular,we estimated the score of aligning i to j assi,j =p(fj |ei) ?
p(i|j, I)?j?
?F p(fj?
|ei) ?
p(i|j?, I), (5)where the translation probability p(fj |ei) andalignment probability p(i|j, I) were obtained fromIBM model 2 trained on the 1.1M sentences.
TheIBM 2 models gives an AER of 21.0% with Frenchas the target, in line with the numbers reported inOch and Ney (2003) and Lacoste-Julien et al (2006).We tested two types of partition matroid con-straints.
The first is a global matroid constraint:A ?
{A?
?
V : ?j ?
F, |A?
?
PEj | ?
b}, (6)which restricts fertility of all words on F side to be atmost b.
This constraint is denoted as FertF (A) ?
bin Table 1 for simplicity.
The second type, denotedas FertF (A) ?
kj , is word-dependent:A ?
{A?
?
V : ?j ?
F, |A?
?
PEj | ?
kj}, (7)where the fertility of word on j is restricted to beat most kj .
Here kj = max{b : pb(f) ?
?, b ?
{0, 1, .
.
.
, 5}}, where ?
is a threshold and pb(f) isthe probability that French word f was aligned to atmost b English words based on the IBM 2 alignment.As mentioned in Section 3, matroid constraintsgeneralize the matching constraint.
In particular,when using two matroid constraints, FertE(A) ?
1and FertF (A) ?
1, we have the matching constraintwhere fertility for both English and French wordsare restricted to be at most one.
Our setup 1 (see Ta-ble 1) uses these two constraints along with a modularobjective function, which is equivalent to the max-imum weighted bipartite matching problem.
Using173Table 1: AER resultsID Objective function Constraint AER(%)1 FertF (A) ?
1, FertE(A) ?
1 21.02 FertF (A) ?
1 23.13modular: f(A) =Pi?EPj?
?i(A)si,jFertF (A) ?
kj 22.14 FertF (A) ?
1 19.85submodular: f(A) =Pi?E?Pj??i(A)si,j?
?FertF (A) ?
kj 18.6Generative model (IBM 2, E?F) 21.0Maximum weighted bipartite matching 20.9Matching with negative penalty on fertility (ILP) 19.3greedy algorithm to solve this problem, we get AER21.0% (setup 1 in Table 1) ?
no significant differencecompared to the AER (20.9%) achieved by the ex-act solution (maximum weighted bipartite matchingapproach), illustrating that greedy solutions are near-optimal.
Note that the bipartite matching approachdoes not improve performance over IBM 2 model,presumably because only one feature was used here.When allowing fertility of English words to bemore than one, we see a significant AER reductionusing a submodular objective (setup 4 and 5) insteadof a modular objective (setup 2 and 3), which verifiesour claim that submodularity lends itself to modelingthe marginal benefit of growing fertility.
In setup2 and 4, while allowing larger fertility for Englishwords, we restrict the fertility of French words tobe most one.
To allow higher fertility for Frenchwords, one possible approach is to use constraintFertF (A) ?
2, in which all French words areallowed to have fertility up to 2.
This approach, how-ever, results in a significant increase of false positivealignments since all French words tend to collectas many matches as permitted.
This issue could bealleviated by introducing a symmetric version ofthe objective function in Eq.
3 such that marginalbenefit of higher fertility of French words are alsocompressed.
Alternatively, we use the second typeof matroid constraint in which fertility upper boundsof French words are word-dependent instead ofglobal.
With ?
= .8, about 10 percent of the Frenchwords have kj equal to 2 or greater.
By using theword-dependent matroid constraint (setup 3 and 5),AERs are reduced compared to those using globalmatroid constraints.
In particular, 18.6% AER isachieved by setup 5, which significantly outperformsthe maximum weighted bipartite matching approach.We also compare our method with model ofLacoste-Julien et al (2006) which also allows fer-tility larger than one by penalizing different levels offertility.
We used si,j as an edge feature and pb(f) asa node feature together with two additional features:a bias feature and the bucketed frequency of the wordtype.
The same procedures for training and decodingas in (Lacoste-Julien et al, 2006) were performedwhere MOSEK was used as the ILP solver.
As shownin Table 1, performance of setup 5 outperforms thismodel and moreover, our approach is at least 50 timesfaster: it took our approach only about half a secondto align all the 347 test set sentence pairs whereasusing the ILP-based approach took about 40 seconds.6 DiscussionWe have presented a novel framework where wordalignment is framed as submodular maximizationsubject to matroid constraints.
Our frameworkextends previous matching-based frameworksin two respects: submodular objective functionsgeneralize modular (linear) objective functions, andmatroid constraints generalize matching constraints.Moreover, such generalizations do not incur aprohibitive computational price since submodularmaximization over matroids can be efficiently solvedwith performance guarantees.
As it is possible toleverage richer forms of submodular functions thatmodel higher order interactions, we believe that thefull potential of our approach has yet to be explored.Our approach might lead to novel approaches formachine translation as well.AcknowledgmentWe thank Simon Lacoste-Julien for sharing his codeand features from (Lacoste-Julien et al, 2006), andthe anonymous reviewers for their comments.
Thiswork was supported by NSF award 0905341.174ReferencesP.F.
Brown, V.J.D.
Pietra, S.A.D.
Pietra, and R.L.
Mercer.1993.
The mathematics of statistical machine transla-tion: Parameter estimation.
Computational linguistics,19(2):263?311.J.
Edmonds, 1970.
Combinatorial Structures and their Ap-plications, chapter Submodular functions, matroids andcertain polyhedra, pages 69?87.
Gordon and Breach.ML Fisher, GL Nemhauser, and LA Wolsey.
1978.
Ananalysis of approximations for maximizing submodularset functions?II.
Polyhedral combinatorics, pages73?87.S.
Jegelka and J.
A. Bilmes.
2011.
Submodularity beyondsubmodular energies: coupling edges in graph cuts.In Computer Vision and Pattern Recognition (CVPR),Colorado Springs, CO, June.D.
Kempe, J. Kleinberg, and E. Tardos.
2003.
Maximiz-ing the spread of influence through a social network.In Proceedings of the 9th Conference on SIGKDD In-ternational Conference on Knowledge Discovery andData Mining (KDD).V.
Kolmogorov and R. Zabin.
2004.
What energy func-tions can be minimized via graph cuts?
IEEE Trans-actions on Pattern Analysis and Machine Intelligence,26(2):147?159.A.
Krause and C. Guestrin.
2005.
Near-optimal nonmy-opic value of information in graphical models.
In Proc.of Uncertainty in AI.A.
Krause, H.B.
McMahan, C. Guestrin, and A. Gupta.2008.
Robust submodular observation selection.
Jour-nal of Machine Learning Research, 9:2761?2801.S.
Lacoste-Julien, B. Taskar, D. Klein, and M.I.
Jordan.2006.
Word alignment via quadratic assignment.
InProceedings of the main conference on Human Lan-guage Technology Conference of the North AmericanChapter of the Association of Computational Linguis-tics, pages 112?119.
Association for ComputationalLinguistics.J.
Lee, M. Sviridenko, and J. Vondra?k.
2009.
Submodularmaximization over multiple matroids via generalizedexchange properties.
Approximation, Randomization,and Combinatorial Optimization.
Algorithms and Tech-niques, pages 244?257.H.
Lin and J. Bilmes.
2010.
Multi-document summariza-tion via budgeted maximization of submodular func-tions.
In North American chapter of the Associationfor Computational Linguistics/Human Language Tech-nology Conference (NAACL/HLT-2010), Los Angeles,CA, June.H.
Lin and J. Bilmes.
2011.
A class of submodular func-tions for document summarization.
In The 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies (ACL-HLT), Port-land, OR, June.E.
Matusov, R. Zens, and H. Ney.
2004.
Symmetricword alignments for statistical machine translation.
InProceedings of the 20th international conference onComputational Linguistics, page 219.
Association forComputational Linguistics.I.D.
Melamed.
2000.
Models of translational equivalenceamong words.
Computational Linguistics, 26(2):221?249.R.
Mihalcea and T. Pedersen.
2003.
An evaluation exer-cise for word alignment.
In Proceedings of the HLT-NAACL 2003 Workshop on Building and using paralleltexts: data driven machine translation and beyond-Volume 3, pages 1?10.
Association for ComputationalLinguistics.M.
Minoux.
1978.
Accelerated greedy algorithms formaximizing submodular set functions.
OptimizationTechniques, pages 234?243.Mukund Narasimhan and Jeff Bilmes.
2004.
PAC-learning bounded tree-width graphical models.
In Un-certainty in Artificial Intelligence: Proceedings of theTwentieth Conference (UAI-2004).
Morgan KaufmannPublishers, July.M.
Narasimhan and J. Bilmes.
2005.
A submodular-supermodular procedure with applications to discrimi-native structure learning.
In Proc.
Conf.
Uncertainty inArtifical Intelligence, Edinburgh, Scotland, July.
Mor-gan Kaufmann Publishers.M.
Narasimhan and J. Bilmes.
2007.
Local search forbalanced submodular clusterings.
In Twentieth Inter-national Joint Conference on Artificial Intelligence (IJ-CAI07), Hyderabad, India, January.F.J.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
Computationallinguistics, 29(1):19?51.A.
Schrijver.
2003.
Combinatorial optimization: polyhe-dra and efficiency.
Springer Verlag.B.
Taskar, S. Lacoste-Julien, and D. Klein.
2005.
A dis-criminative matching approach to word alignment.
InProceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, pages 73?80.
Association for Com-putational Linguistics.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based word alignment in statistical translation.
InProceedings of the 16th conference on Computationallinguistics-Volume 2, pages 836?841.
Association forComputational Linguistics.J.
Vondra?k.
2008.
Optimal approximation for the sub-modular welfare problem in the value oracle model.
InProceedings of the 40th annual ACM symposium onTheory of computing, pages 67?74.
ACM.175
