Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645?650,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsJoint Relational Embeddings for Knowledge-based Question AnsweringMin-Chul Yang?Nan Duan?Ming Zhou?Hae-Chang Rim??Dept.
of Computer & Radio Comms.
Engineering, Korea University, Seoul, South Korea?Microsoft Research Asia, Beijing, Chinamcyang@nlp.korea.ac.kr{nanduan, mingzhou}@microsoft.comrim@nlp.korea.ac.krAbstractTransforming a natural language (NL)question into a corresponding logical form(LF) is central to the knowledge-basedquestion answering (KB-QA) task.
Un-like most previous methods that achievethis goal based on mappings between lex-icalized phrases and logical predicates,this paper goes one step further and pro-poses a novel embedding-based approachthat maps NL-questions into LFs for KB-QA by leveraging semantic associationsbetween lexical representations and KB-properties in the latent space.
Experimen-tal results demonstrate that our proposedmethod outperforms three KB-QA base-line methods on two publicly released QAdata sets.1 IntroductionKnowledge-based question answering (KB-QA)involves answering questions posed in naturallanguage (NL) using existing knowledge bases(KBs).
As most KBs are structured databases,how to transform the input question into its corre-sponding structured query for KB (KB-query) asa logical form (LF), also known as semantic pars-ing, is the central task for KB-QA systems.
Pre-vious works (Mooney, 2007; Liang et al., 2011;Cai and Yates, 2013; Fader et al., 2013; Berant etal., 2013; Bao et al., 2014) usually leveraged map-pings between NL phrases and logical predicatesas lexical triggers to perform transformation tasksin semantic parsing, but they had to deal with twolimitations: (i) as the meaning of a logical pred-icate often has different natural language expres-sion (NLE) forms, the lexical triggers extracted fora predicate may at times are limited in size; (ii)entities detected by the named entity recognition(NER) component will be used to compose thelogical forms together with the logical predicates,so their types should be consistent with the pred-icates as well.
However, most NER componentsused in existing KB-QA systems are independentfrom the NLE-to-predicate mapping procedure.We present a novel embedding-based KB-QAmethod that takes all the aforementioned lim-itations into account, and maps NLE-to-entityand NLE-to-predicate simultaneously using sim-ple vector operations for structured query con-struction.
First, low-dimensional embeddings ofn-grams, entity types, and predicates are jointlylearned from an existing knowledge base and fromentries <entitysubj, NL relation phrase, entityobj>that are mined from NL texts labeled as KB-properties with weak supervision.
Each such en-try corresponds to an NL expression of a triple<entitysubj, predicate, entityobj> in the KB.
Theseembeddings are used to measure the semantic as-sociations between lexical phrases and two prop-erties of the KB, entity type and logical predicate.Next, given an NL-question, all possible struc-tured queries as candidate LFs are generated andthen they are ranked by the similarity between theembeddings of observed features (n-grams) in theNL-question and the embeddings of logical fea-tures in the structured queries.
Last, answers areretrieved from the KB using the selected LFs.The contributions of this work are two-fold: (1)as a smoothing technique, the low-dimensionalembeddings can alleviate the coverage issues oflexical triggers; (2) our joint approach integratesentity span selection and predicate mapping tasksfor KB-QA.
For this we built independent entityembeddings as the additional component, solvingthe entity disambiguation problem.2 Related WorkSupervised semantic parsers (Zelle and Mooney,1996; Zettlemoyer and Collins, 2005; Mooney,2007) heavily rely on the <sentence, semantic an-645notation> pairs for lexical trigger extraction andmodel training.
Due to the data annotation re-quirement, such methods are usually restricted tospecific domains, and struggle with the coverageissue caused by the limited size of lexical triggers.Studies on weakly supervised semantic parsershave tried to reduce the amount of human supervi-sion by using question-answer pairs (Liang et al.,2011) or distant supervision (Krishnamurthy andMitchell, 2012) instead of full semantic annota-tions.
Still, for KB-QA, the question of how toleverage KB-properties and analyze the questionstructures remains.Bordes et al.
(2012) and Weston et al.
(2013) de-signed embedding models that connect free textswith KBs using the relational learning method(Weston et al., 2010).
Their inputs are oftenstatement sentences which include subject and ob-ject entities for a given predicate, whereas NL-questions lack either a subject or object entity thatis the potential answer.
Hence, we can only usethe information of a subject or object entity, whichleads to a different training instance generationprocedure and a different training criterion.Recently, researchers have developed open do-main systems based on large scale KBs such asFREEBASE1(Cai and Yates, 2013; Fader et al.,2013; Berant et al., 2013; Kwiatkowski et al.,2013; Bao et al., 2014; Berant and Liang, 2014;Yao and Van Durme, 2014).
Their semanticparsers for Open QA are unified formal and scal-able: they enable the NL-question to be mappedinto the appropriate logical form.
Our method ob-tains similar logical forms, but using only low-dimensional embeddings of n-grams, entity types,and predicates learned from texts and KB.3 Setup3.1 Relational Components for KB-QAOur method learns semantic mappings betweenNLEs and the KB2based on the paired relation-ships of the following three components: C de-notes a set of bag-of-words (or n-grams) as contextfeatures (c) for NLEs that are the lexical represen-tations of a logical predicate (p) in KB; T denotesa set of entity types (t) in KB and each type can beused as the abstract expression of a subject entity1http://www.freebase.com2For this paper, we used a large scale knowledge base thatcontains 2.3B entities, 5.5K predicates, and 18B assertions.A 16-machine cluster was used to host and serve the wholedata.
(s) that occurs in the input question; P denotes aset of logical predicates (p) in KB, each of whichis the canonical form of different NLEs sharing anidentical meaning (bag-of-words; c).Based on the components defined above, thepaired relationships are described as follows: T -P can investigate the relationship between sub-ject entity and logical predicate, as object entityis always missing in KB-QA; C-T can scruti-nize subject entity?s attributes for the entity spanselection such as its positional information andrelevant entity types to the given context, whichmay solve the entity disambiguation problem inKB-QA; C-P can leverage the semantic overlapbetween question contexts (n-gram features) andlogical predicates, which is important for mappingNL-questions to their corresponding predicates.3.2 NLE-KB Pair ExtractionThis section describes how we extract the semanticassociated pairs of NLE-entries and KB-triples tolearn the relational embeddings (Section 4.1).<Relation Mention, Predicate> Pair (MP)Each relation mention denotes a lexical phraseof an existing KB-predicate.
Following informa-tion extraction methods, such as PATTY (Nakas-hole et al., 2012), we extracted the <relationmention, logical predicate> pairs from EnglishWIKIPEDIA3, which is closely connected to ourKB, as follows: Given a KB-triple <entitysubj,logical predicate, entityobj>, we extracted NLE-entries <entitysubj, relation mention, entityobj>where relation mention is the shortest path be-tween entitysubjand entityobjin the dependencytree of sentences.
The assumption is that any re-lation mention (m) in the NLE-entry containingsuch entity pairs that occurred in the KB-triple islikely to express the predicate (p) of that triple.With obtaining high-qualityMP pairs, we keptonly relation mentions that were highly associatedwith a predicate measured by the scoring function:S(m, p) = PMI(em; ep) + PMI(um;up) (1)where exis the set of total pairs of both-sideentities of entry x (m or p) and uxis the setof unique (distinct) pairs of both-side entities ofentry x.
In this case, the both-side entities in-dicate entitysubjand entityobj.
For a frequency-based probability, PMI(x; y) = logP (x,y)P (x)P (y)3http://en.wikipedia.org/646(Church and Hanks, 1990) can be re-written asPMI(x; y) = log|x?y|?C|x|?|y|, where C denotes thetotal number of items shown in the corpus.
Thefunction is partially derived from the support score(Gerber and Ngonga Ngomo, 2011), but we fo-cus on the correlation of shared entity pairs be-tween relation mentions and predicates using thePMI computation.<Question Pattern, Predicate> Pair (QP)Since WIKIPEDIA articles have no information toleverage interrogative features which highly de-pend on the object entity (answer), it is difficult todistinguish some questions that are composed ofonly different 5W1H words, e.g., {When|Where}was Barack Obama born?
Hence, we used themethod of collecting question patterns with humanlabeled predicates that are restricted by the set ofpredicates used inMP (Bao et al., 2014).4 Embedding-based KB-QAOur task is as follows.
First, our model learns thesemantic associations of C-T , C-P , and T -P (Sec-tion 3.1) based on NLE-KB pairs (Section 3.2),and then predicts the semantic-related KB-querywhich can directly find the answer to a given NL-question.For our feature space, given an NLE-KB pair,the NLE (relation mention in MP or questionpattern in QP) is decomposed into n-gram fea-tures: C = {c | c is a segment of NLE}, andthe KB-properties are represented by entity typet of entitysubjand predicate p. Then we can ob-tain a training triplet w = [C, t, p].
Each feature(c ?
C, t ?
T , p ?
P) is encoded in the distributedrepresentation which is n-dimensional embeddingvectors (En): ?x, xencode?
E(x) ?
En.All n-gram features (C) for an NLE are mergedinto one embedding vector to help speed up thelearning process: E(C) =?c?CE(c)/|C|.
Thisfeature representation is inspired by previous workin embedding-based relation extraction (Weston etal., 2013), but differs in the following ways: (1)entity information is represented on a separate em-bedding, but its positional information remains assymbol ?entity?
; (2) when the vectors are com-bined, we use the average of each index to normal-ize features.For our joint relational approach, we focus onthe set of paired relationships R = {C-t, C-p, t-p} that can be semantically leveraged.
Formally,these features are embedded into the same latentspace (En) and their semantic similarities can becomputed by a dot product operation:Sim(a, b) = Sim(rab) = E(a)?E(b) (2)where rabdenotes a paired relationship a-b (or (a,b)) in the above set R. We believe that our joint re-lational learning can smooth the surface (lexical)features for semantic parsing using the aligned en-tity and predicate.4.1 Joint Relational Embedding LearningOur ranking-based relational learning is based ona ranking loss (Weston et al., 2010) that supportsthe idea that the similarity scores of observed pairsin the training set (positive instances) should belarger than those of any other pairs (negative in-stances):?i, ?y?6= yi, Sim(xi, yi) > 1+Sim(xi, y?)
(3)More precisely, for each triplet wi= [Ci, ti, pi]obtained from an NLE-KB pair, the relationshipsRi= {Ci-ti, Ci-pi, ti-pi} are trained under thesoft ranking criterion, which conducts StochasticGradient Descent (SGD).
We thus aim to minimizethe following:?i,?y?6= yi,max(0, 1?Sim(xi, yi)+Sim(xi, y?
))(4)Our learning strategy is as follows.
First, we ini-tialize embedding space Enby randomly givingmean 0 and standard deviation 1/n to each vec-tor.
Then for each training triplet wi, we select thenegative pairs against positive pairs (Ci-ti, Ci-pi,and ti-pi) in the triplet.
Last, we make a stochasticgradient step to minimize Equation 4 and updateEnat each step.4.2 KB-QA using Embedding ModelsOur goal for KB-QA is to translate a given NL-question to a KB-query with the form <subjectentity, predicate, ?>, where ?
denotes the an-swer entity we are looking for.
The decoding pro-cess consists of two stages.
The first stage in-volves generating all possible KB-queries (Kq) foran NL-question q.
We first extract n-gram fea-tures (Cq) from the NL-question q.
Then for aKB-query kq, we find all available entity types(tq) of the identified subject entities (sq) usingthe dictionary-based entity detection on the NL-question q (all of spans can be candidate entities),and assign all items of predicate set (P) as the can-didate predicates (pq).
Like the training triplets,647q where is the city of david?
?k(q) [The City of David, contained by, ?
]Cqn-grams of ?where is ?entity?
?
?tqlocationpqcontained byTable 1: The corresponding KB-query?k(q) for aNL-question q and its decoding triplet wq.we also represent the above features as the tripletform wqi= [Cqi, tqi, pqi] which is directly linked toa KB-query kqi= [sqi, pqi, ?].
The second stageinvolves ranking candidate KB-queries based onthe similarity scores between the following pairedrelationships from the triplet wqi: Rqi= {Cqi-tqi,Cqi-pqi, tqi-pqi}.
Unlike in the training step, the sim-ilarities of Cqi-tqiand Cqi-pqiare computed by sum-mation of all pairwise elements (each context em-bedding E(c), not E(C), with each paired E(t) orE(p)) for a more precise measurement.
Since sim-ilarites of Rqare calculated on different scales, wenormalize each value using Z-score (Z(x) =x???
)(Kreyszig, 1979).
The final score is measured by:Simq2k(q, kq) =?r?RqZ(Sim(r)) (5)Then, given any NL-question q, we can predict thecorresponding KB-query?k(q):?k(q) = argmaxk?KqSimq2k(q, k) (6)Last, we can retrieve an answer from the KB usinga structured query?k(q).
Table 1 shows an exampleof our decoding process.Multi-related Question Some questions in-clude two-subject entities, both of which are cru-cial to understanding the question.
For the ques-tion who plays gandalf in the lord of the rings?Gandalf (character) and The Lord Of TheRings (film) are explicit entities that should bejoined to a pair of the two entities (implicit entity).More precisely, the two entities can be combinedinto one concatenated entity (character-in-film)using our manual rule, which compares the possi-ble pairs of entity types in the question with thelist of pre-defined entity type pairs that can bemerged into a concatenated entity.
Our solutionenables a multi-related question to be transformedto a single-related question which can be directlytranslated to a KB-query.
Then, the two entity# Entries AccuracyMP pairs 291,585 89%QP pairs 4,764 98%Table 2: Statistics of NLE-KB pairsmentions are replaced with the symbol ?entity?
(who play ?entity?
in ?entity?
?).
We re-gard the result of this transformation as one of thecandidate KB-queries in the decoding step.5 ExperimentsExperimental Setting We first performed pre-processing, including lowercase transformation,lemmatization and tokenization, on NLE-KB pairsand evaluation data.
We used 71,310 n-grams(uni-, bi-, tri-), 990 entity types, and 660 predi-cates as relational components shown in Section3.1.
The sum of these three numbers (72,960)equals the size of the embeddings we are goingto learn.
In Table 2, we evaluated the quality ofNLE-KB pairs (MP and QP) described in Sec-tion 3.2.
We can see that the quality ofQP pairs isgood, mainly due to human efforts.
Also, we ob-tained MP pairs that have an acceptable qualityusing threshold 3.0 for Equation 1, which lever-ages the redundancy information in the large-scaledata (WIKIPEDIA).
For our embedding learning,we set the embedding dimension n to 100, thelearning rate (?)
for SGD to 0.0001, and the it-eration number to 30.
To make the decodingprocedure computable, we kept only the popularKB-entity in the dictionary to map different entitymentions into a KB-entity.We used two publicly released data sets for QAevaluations: Free917 (Cai and Yates, 2013) in-cludes the annotated lambda calculus forms foreach question, and covers 81 domains and 635Freebase relations; WebQ.
(Berant et al., 2013)provides 5,810 question-answer pairs that are builtby collecting common questions from Web-querylogs and by manually labeling answers.
We usedthe previous three approaches (Cai and Yates,2013; Berant et al., 2013; Bao et al., 2014) as ourbaselines.Experimental Results Table 3 reports the over-all performances of our proposed KB-QA methodon the two evaluation data sets and compares themwith those of the three baselines.
Note that wedid not re-implement the baseline systems, but justborrowed the evaluation results reported in their648Methods Free917 WebQ.Cai and Yates (2013) 59.00% N/ABerant et al.
(2013) 62.00% 31.40%Bao et al.
(2014) N/A 37.50%Our method 71.38% 41.34%Table 3: Accuracy on the evaluation dataMethods Free917 WebQ.Our method 71.38% 41.34%w/o T -P 70.65% 40.55%w/o C-T 67.03% 38.44%w/o C-P 31.16% 19.24%Table 4: Ablation of the relationship typespapers.
Although the KB used by our system ismuch larger than FREEBASE, we still think thatthe experimental results are directly comparablebecause we disallow all the entities that are not in-cluded in FREEBASE.Table 3 shows that our method outperforms thebaselines on both Free917 and WebQ.
data sets.We think that using the low-dimensional embed-dings of n-grams rather than the lexical triggersgreatly improves the coverage issue.
Unlike theprevious methods which perform entity disam-biguation and predicate prediction separately, ourmethod jointly performs these two tasks.
Moreprecisely, we consider the relationships C-T andC-P simultaneously to rank candidate KB-queries.In Table 1, the most independent NER in KB-QAsystems may detect David as the subject entity,but our joint approach can predict the appropriatesubject entity The City of David by leveragingnot only the relationships with other componentsbut also other relationships at once.
The syntax-based (grammar formalism) approaches such asCombinatory Categorial Grammar (CCG) may ex-perience errors if a question has grammatical er-rors.
However, our bag-of-words model-based ap-proach can handle any question as long as thequestion contains keywords that can help in un-derstanding it.Table 4 shows the contributions of the relation-ships (R) between relational components C, T ,and P .
For each row, we remove the similarityfrom each of the relationship types described inSection 3.1.
We can see that the C-P relationshipplays a crucial role in translating NL-questions toKB-queries, while the other two relationships areslightly helpful.Result Analysis Since the majority of questionsin WebQ.
tend to be more natural and diverse, ourmethod cannot find the correct answers to manyquestions.
The errors can be caused by any ofthe following reasons.
First, some NLEs cannotbe easily linked to existing KB-predicates, mak-ing it difficult to find the answer entity.
Second,some entities can be mentioned in several differentways, e.g., nickname (shaq?Shaquille O?neal)and family name (hitler?Adolf Hitler).
Third, interms of KB coverage issues, we cannot detect theentities that are unpopular.
Last, feature represen-tation for a question can fail when the questionconsists of rare n-grams.The two training sets shown in Section 3.2 arecomplementary: QP pairs provide more oppor-tunities for us to learn the semantic associationsbetween interrogative words and predicates.
Suchresources are especially important for understand-ing NL-questions, as most of them start with such5W1H words; on the other hand, MP pairs en-rich the semantic associations between context in-formation (n-gram features) and predicates.6 ConclusionIn this paper, we propose a novel method thattransforms NL-questions into their correspondinglogical forms using joint relational embeddings.We also built a simple and robust KB-QA systembased on only the learned embeddings.
Such em-beddings learn the semantic associations betweennatural language statements and KB-propertiesfrom NLE-KB pairs that are automatically ex-tracted from English WIKIPEDIA using KB-tripleswith weak supervision.
Then, we generate all pos-sible structured queries derived from latent logicalfeatures of the given NL-question, and rank thembased on the similarity scores between those re-lational attributes.
The experimental results showthat our method outperforms the latest three KB-QA baseline systems.
For our future work, we willbuild concept-level context embeddings by lever-aging latent meanings of NLEs rather than theirsurface n-grams with the aligned logical featureson KB.Acknowledgement This research was sup-ported by the Next-Generation InformationComputing Development Program through theNational Research Foundation of Korea (NRF)funded by the Ministry of Science, ICT & FuturePlanning (NRF-2012M3C4A7033344).649ReferencesJunwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.2014.
Knowledge-based question answering as ma-chine translation.
Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics, pages 967?976.
Association for Computa-tional Linguistics.Jonathan Berant and Percy Liang.
2014.
Seman-tic parsing via paraphrasing.
Proceedings of the52nd Annual Meeting of the Association for Com-putational Linguistics, pages 1415?1425.
Associa-tion for Computational Linguistics.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 1533?1544, Seattle, Wash-ington, USA, October.
Association for Computa-tional Linguistics.Antoine Bordes, Xavier Glorot, Jason Weston, andYoshua Bengio.
2012.
Joint learning of wordsand meaning representations for open-text seman-tic parsing.
In In Proceedings of 15th InternationalConference on Artificial Intelligence and Statistics.Qingqing Cai and Alexander Yates.
2013.
Large-scalesemantic parsing via schema matching and lexiconextension.
In Association for Computational Lin-guistics (ACL), pages 423?433.
The Association forComputer Linguistics.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Comput.
Linguist., 16(1):22?29, March.Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.2013.
Paraphrase-driven learning for open questionanswering.
In Association for Computational Lin-guistics (ACL), pages 1608?1618.
The Associationfor Computer Linguistics.Daniel Gerber and Axel-Cyrille Ngonga Ngomo.
2011.Bootstrapping the linked data web.
In 1st Workshopon Web Scale Knowledge Extraction @ ISWC 2011.E.
Kreyszig.
1979.
Advanced Engineering Mathemat-ics.
Wiley.Jayant Krishnamurthy and Tom M. Mitchell.
2012.Weakly supervised training of semantic parsers.
InProceedings of the 2012 Joint Conference on Em-pirical Methods in Natural Language Processingand Computational Natural Language Learning,EMNLP-CoNLL ?12, pages 754?765, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1545?1556, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Percy Liang, Michael I. Jordan, and Dan Klein.
2011.Learning dependency-based compositional seman-tics.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies - Volume 1, HLT ?11,pages 590?599, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.RaymondJ.
Mooney.
2007.
Learning for semanticparsing.
In Alexander Gelbukh, editor, Computa-tional Linguistics and Intelligent Text Processing,volume 4394 of Lecture Notes in Computer Science,pages 311?324.
Springer Berlin Heidelberg.Ndapandula Nakashole, Gerhard Weikum, and FabianSuchanek.
2012.
Patty: A taxonomy of relationalpatterns with semantic types.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?12,pages 1135?1145, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Jason Weston, Samy Bengio, and Nicolas Usunier.2010.
Large scale image annotation: Learning torank with joint word-image embeddings.
MachineLearning, 81(1):21?35, October.Jason Weston, Antoine Bordes, Oksana Yakhnenko,and Nicolas Usunier.
2013.
Connecting languageand knowledge bases with embedding models for re-lation extraction.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1366?1371, Seattle, Washington,USA, October.
Association for Computational Lin-guistics.Xuchen Yao and Benjamin Van Durme.
2014.
Infor-mation extraction over structured data: Question an-swering with freebase.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics, pages 956?966, Baltimore, Mary-land, June.
Association for Computational Linguis-tics.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proceedings of the Thirteenth Na-tional Conference on Artificial Intelligence - Volume2, AAAI?96, pages 1050?1055.
AAAI Press.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In UAI, pages 658?666.
AUAI Press.650
