L inguist ic  St ructure as Compos i t ion  and Per turbat ionCarl de MarckenMIT AI Laboratory, NE43-769545 Technology SquareCambridge, MA, 02139, USAcgdemarc@ai.mit.eduAbstractThis paper discusses the problem of learn-ing language from unprocessed text andspeech signals, concentrating on the prob-lem of learning a lexicon.
In particular, itargues for a representation of language inwhich linguistic parameters like words arebuilt by perturbing a composition of exist-ing parameters.
The power of the represen-tation is demonstrated by several examplesin text segmentation and compression, ac-quisition of a lexicon from raw speech, andthe acquisition of mappings between textand artificial representations of meaning.1 MotivationLanguage is a robust and necessarily redundantcommunication mechanism.
Its redundancies com-monly manifest hemselves as predictable patternsin speech and text signals, and it is largely thesepatterns that enable text and speech compression.Naturally, many patterns in text and speech re-flect interesting properties of language.
For ex-ample, the is both an unusually frequent sequenceof letters and an English word.
This suggests us-ing compression as a means of acquiring under-lying properties of language from surface signals.The general methodology of language-learning-by-compression is not new.
Some notable early propo-nents included Chomsky (1955), Solomonoff (1960)and Harris (1968), and compression has been usedas the basis for a wide variety of computer programsthat attack unsupervised learning in language; see(Olivier, 1968; Wolff, 1982; Ellison, 1992; Stolcke,1994; Chen, 1995; Cartwright and Brent, 1994)among others.1.1 Pat terns  and  LanguageUnfortunately, while surface patterns often reflectinteresting linguistic mechanisms and parameters,they do not always do so.
Three classes of exam-ples serve to illustrate this.1.1.1 Ext ra l lngu ls t l c  Pat ternsThe sequence it was a dark and stormy night isa pattern in the sense it occurs in text far morefrequently than the frequencies of its letters wouldsuggest, but that does not make it a lexical or gram-matical primitive: it is the product of a complexmixture of linguistic and extra-linguistic processes.Such patterns can be indistinguishable from desiredones.
For example, in the Brown corpus (Francis andKucera, 1982) scratching her nose occurs 5 times,a corpus-specific diosyncrasy.
This phrase has thesame structure as the idiom kicking the bucket.
It isdifficult to imagine any induction algorithm learn-ing kicking the bucket from this corpus without also(mistakenly) learning scratching her nose.1.1.2 The  Def in i t ion  o f  In teres t ingThis discussion presumes there is a set of desiredpatterns to extract from input signals.
What is thisset?
For example, is kicking the bucket a proper lexi-cal unit?
The answer depends on factors external tothe unsupervised learning framework.
For the pur-poses of machine translation or information retrievalthis sequence is an important idiom, but with re-spect to speech recognition it is unremarkable.
Sim-ilar questions could be asked of subword units likesyllables.
Plainly, the answers depends on the learn-ing context, and not on the signal itself.1.1.3 The  Def in i t ion  o f  Pat ternAny statistical definition of pattern depends onan underlying model.
For instance, the sequence thedog occurs much more frequently than one wouldexpect given an independence assumption about let-ters.
But for a model with knowledge of syntax andword frequencies, there is nothing remarkable aboutthe phrase.
Since all existing models have flaws, pat-terns will always be learned that are artifacts of im-perfections in the learning algorithm.These examples eem to imply that unsupervisedinduction will never converge to ideal grammars andlexicons.
While there is truth to this, the rest of thispaper describes a representation of language thatbypasses many of the apparent difficulties.335\[national football league\]~O~\]  \[football\] \[league\] / \  / ' .
.tnation\] \[al\] \[foot\] \[ball\] \ [a \ ]  \[gue\],1/ / \ I -  ~ \ /\[n\] \[t\] \[b\] \[1\] \[g\]Figure I: A compositional representation.Code Length Components000 --- Co( 2 Co, c,001 = c,h.
3 c,,c~,ce010 = cln 2 ca, c,0110 ----- c .
.
.
.
4 C.,Co, Cm,Cc0111 = C .
.
.
.
.
nh.
3 C .
.
.
.
.
CoS, C, he10000 ---- .
.
.
.
.
.
.
.
.Figure 2: A coding of the first few words of a hypo-thetical exicon.
The first two columns can be codedsuccinctly, leaving the cost of pointers to componentwords as the dominant cost of both the lexicon andthe representation f the input.2 A Compos i t iona l  Representat ionThe examples in sections 1.1.1 and 1.1.2 seem toimply that any unsupervised language learning pro-gram that returns only one segmentation of the in-put is bound to make many mistakes.
And sec-tion 1.1.3 implies that the decisions about linguisticunits must be made relative to their representations.Both problems can be solved if linguistic units (fornow, words in the lexicon) are built by compositionof other units.
For example, kicking the bucket mightbe built by composing kicking, the and bucket.
1 Ofcourse, if a word is merely the composition of itsparts, there is nothing interesting about it and noreason to include it in the lexicon.
So the motiva-tion for including a word in the lexicon must be thatit function differently from its parts.
Thus a word isa perturbation of a composition.In the case of kicking the bucket he perturbation isone of both meaning and frequency.
For scratchingher nose the perturbation may just be frequency.
~This is a very natural representation from the view-point of language.
It correctly predicts that bothphrases inherit their sound and syntax from theircomponent words.
At the same time it leaves openthe possibility that idiosyncratic information will beattached to the whole, as with the meaning of kick-ing the bucket.
This structure is very much like theclass hierarchy of a modern programming language.It is not the same thing as a context-free grammar,since each word does not act in the same way as thedefault composition of its components.Figure 1 illustrates a recursive decomposition (un-der concatenation) of the phrase national footballleague.
The phrase is broken into three words, eachof which are also decomposed in the lexicon.
Thisprocess bottoms out in the terminal characters.
Thisis a real decomposition achieved by a program de-scribed in section 4.
Not shown are the perturba-1A simple composition operator is concatenation, butin section 6 a more interesting one is discussed.~Naturally, an unsupervised learning algorithm withno access to meaning will not treat them differently.tions (in this case merely frequency changes) thatdistinguish each word from its parts.
This generalframework extends to other perturbations.
For ex-ample, the word wanna is naturally thought of asa composition of want and to with a sound change.And in speech the three different words to, two andtoo may well inherit the sound of a common ancestorwhile introducing new syntactic and semantic prop-erties.2.1 Cod ingOf course, for this representation to be more thanan intuition both the composition and perturbationoperators must be exactly specified.
In particular,a code must be designed that enables a word (or asentence) to be expressed in terms of its parts.
As asimple example, suppose that the composition oper-ator is concatenation, that terminals are characters,and that the only perturbation operator-is the abil-ity to express the frequency of a word independentlyof the frequency of its parts.
Then to code either asentence of the input or a (nonterminal) word in thelexicon, the number of component words in the rep-resentation must be written, followed by a code foreach component word.
Naturally, each word in thelexicon must be associated with its code, and undera near-optimal coding scheme like a Huffman code,the code length will be related to the frequency ofthe word.
Thus, associating a word with a code sub-stitutes for writing down the frequency of a word.Furthermore, if words are written down in order ofdecreasing frequency, a Huffman code for a largelexicon can be specified using a negligible numberof bits.
This and the near-negligible cost of writ-ing down word lengths will not be discussed further.Figure 2 presents a portion of an encoding of a hy-pothetical lexicon.2.2 MDLGiven a coding scheme and a particular lexicon (anda parsing algorithm) it is in theory possible to calcu-late the minimum length encoding of a given input.336Part of the encoding will be devoted to the lexicon,the rest to representing the input in terms of thelexicon.
The lexicon that minimizes the combineddescription length of the lexicon and the input max-imally compresses the input.
In the sense of Rissa-nen's minimum description-length (MDL) principle(Rissanen, 1978; Rissanen, 1989) this lexicon is thetheory that best explains the data, and one can hopethat the patterns in the lexicon reflect the underly-ing mechanisms and parameters of the language thatgenerated the input.2.3 P roper t ies  o f  the RepresentationRepresenting words in the lexicon as perturbationsof compositions has a number of desirable properties.?
The choice of composition and perturbation op-erators captures a particular detailed theory oflanguage.
They can be used, for instance, toreference sophisticated phonological and mor-phological mechanisms.?
The length of the description of a word is a mea-sure of its linguistic plausibility, and can serveas a buffer against learning unnatural coinci-dences.?
Coincidences like scratching her nose do not ex-clude desired structure, since they are furtherbroken down into components that they inheritproperties from.?
Structure is shared: the words blackbird andblackberry can share the common substructureassociated with black, such as its sound andmeaning.
As a consequence, data is pooled forestimation, and representations are compact.?
Common irregular forms are compiled out.
Forexample, if wang is represented in terms of go(presumably to save the cost of unnecessarilyreproducing syntactic and semantic properties)the complex sound change need only be repre-sented once, not every time went is used.?
Since parameters (words) have compact repre-sentations, they are cheap from a descriptionlength standpoint, and many can be includedin the lexicon.
This allows learning algorithmsto fit detailed statistical properties of the data.This coding scheme is very similar to that found inpopular dictionary-based compression schemes likeLZ78 (Ziv and Lempel, 1978).
It is capable of com-pressing a sequence of identical characters of lengthn to size O(log n).
However, in contrast o compres-sion schemes like LZ78 that use deterministic rulesto add parameters to the dictionary (and do not ar-rive at linguistically plausible parameters), it is pos-sible ta perform more sophisticated searches in thisrepresentation.Start with lexicon of terminals.IterateIterate (EM)Parse input and words using current lexicon.Use word counts to update frequencies.Add words to the lexicon.Iterate (EM)Parse input and words using current lexicon.Use word counts to update frequencies.Delete words from the lexicon.Figure 3: An iterative search algorithm.
Two it-erations of the inner loops are usually sufficient forconvergence, and for the tests described in this pa-per after 10 iterations of the outer loop there is littlechange in the lexicon in terms of either compressionperformance or structure.3 A Search  AlgorithmSince the class of possible lexicons is infinite, theminimization of description length is necessarilyheuristic.
Given a fixed lexicon, the expectation-maximization algorithm (Dempster et al, 1977) canbe used to arrive at a (locally) optimal set of fre-quencies and codelengths for the words in the lex-icon.
For composition by concatenation, the algo-rithm reduces to the special case of the Baum-Welchprocedure (Baum et al, 1970) discussed in (Deligneand Bimbot, 1995).
In general, however, the parsingand reestimation involved in EM can be consider-ably more complicated.
To update the structure ofthe lexicon, words can be added or deleted from itif this is predicted to reduce the description lengthof the input.
This algorithm is summarized in fig-ure 3.
33.1 Add ing  and De le t ing  WordsFor words to be added to the lexicon, two things areneeded.
The first is a means of hypothesizing candi-date new words.
The second is a means of evaluat-ing candidates.
One reasonable means of generatingcandidates is to look at pairs (or triples) of wordsthat are composed in the parses of words and sen-tences of the input.
Since words are built by com-posing other words and act like their composition, anew word can be created from such a pair and substi-tuted in place of the pair wherever the pair appears.For example, if water and melon are frequently com-posed, then a good candidate for a new word is watero melon = watermelon, where o is the concatenation3For the composition operators and test sets we havelooked at, using single (Viterbi) parses produces almostexactly the same results (in terms of both compressionand lexical structure) as summing probabilities over mul-tiple parses.337operator.
In order to evaluate whether the additionof such a new word is likely to reduce the descriptionlength of the input, it is necessary to record duringthe EM step the extra statistics of how many timesthe composed pairs occur in parses.The effect on description length of adding a newword can not be exactly computed.
Its additionwill not only affect other words, but may also causeother words to be added or deleted.
Furthermore, itis more computationally efficient o add and deletemany words simultaneously, and this complicatesthe estimation of the change in description length.Fortunately, simple approximations of the changeare adequate.
For example, if Viterbi analyses arebeing used then the new word watermelon will com-pletely take the place of all compositions of waterand melon.
This reduces the counts of water andmelon accordingly, though they are each used oncein the representation f watermelon.
If it is assumedthat no other word counts change, these assumptionsallow one to predict the counts and probabilities ofall words after the change.
Since the codelengthof a word w with probability p(w) is approximately- l og  p(~), the total estimated change in descriptionlength of adding a new word W to a lexicon/; iszx -c'(W) logp'(w) + d.l.
(changes) +Z + c( 0)logp( o))where c(w) is the count of the word w, primes indi-cated counts and probabilities after the change andd.l.
(changes) represents the cost of writing down theperturbations involved in the representation of W.If A < 0 the word is predicted to reduce the totaldescription length and is added to the lexicon.
Sim-ilar heuristics can be used to estimate the benefit ofdeleting words.
43.2 Search  Proper t iesA significant source of problems in traditional gram-mar induction techniques i  local minima (de Mar-cken, 1995a; Pereira and Schabes, 1992; Carroll andCharniak, 1992).
The search algorithm describedabove avoids many of these problems.
The reasonis that hidden structure is largely a "compile-time"phenomena.
During parsing all that is importantabout a word is its surface form and codelength.
Theinternal representation does not matter.
Therefore,the internal representation is free to reorganize atany time; it has been decoupled.
This allows struc-ture to be built bottom up or for structure to emergeinside already existing parameters.
Furthermore,since parameters (words) encode surface patterns, it4See (de Mareken, 1995b) for more detailed discus-sion of these estimations.
The actual formulas used inthe tests presented in this paper are slightly more com-plicated than presented here.is relatively easy to determine when they are useful,and their use is limited.
They usually do not havecompeting roles, in contrast, for instance, to hiddennodes in neural networks.
And since there are nofixed number of parameters, when words do start tohave multiple disparate uses, they can be split withcommon substructure shared.
Finally, since add anddelete cycles can compensate for initial mistakes, in-exact heuristics can be used for adding and deletingwords.4 Concatenat ion  Resu l tsThe simplest reasonable instantiation of thecomposition-and-perturbation framework is with theconcatenation operator and frequency perturbation.This instantiation is easily tested on problems of textsegmentation and compression.
Given a text docu-ment, the search algorithm can be used to learn alexicon that minimizes its description length.
Fortesting purposes, spaces will be removed from inputtext and true words will be defined to be minimalsequences bordered by spaces in the original input).The search algorithm parses the input as it com-presses it, and can therefore output a segmentationof the input in terms of words drawn from the lex-icon.
These words are themselves decomposed inthe lexicon, and can be considered to form a treethat terminates in the characters of the sentence.This tree can have no more than O(n) nodes for asentence with n characters, though there are O(n 2)possible "true words" in the input sentence; thus,the tree contains considerable information.
Definerecall to be the percentage of true words that oc-cur at some level of the segmentation-tree.
Definecrossing-bracket to be the percentage of true wordsthat violate the segmentation-tree structure, sThe search algorithm was applied to two texts,a lowercase version of the million-word Brown cor-pus with spaces and punctuation removed, and 4million characters of Chinese news articles in a two-byte/character format.
In the case of the Chinese,which contains no inherent separators like spaces,segmentation performance is measured relative toanother computer segmentation program that hadaccess to a (human-created) lexicon.
The algorithmwas given the raw encoding and had to deduce theinternal two-byte structure.
In the case of the Browncorpus, word recall was 90.5% and crossing-bracketswas 1.7%.
For the Chinese word recall was 96.9%and crossing-brackets was 1.3%.
In the case of bothEnglish and Chinese, most of the unfound wordswere words that occurred only once in the corpus.Thus, the algorithm has done an extremely good jobof learning words and properly using them to seg-ment the input.
Furthermore, the crossing-bracket5The true word moon in the input \[the/\[moon\] is acrossing-bracket violation of them in the segmentationtree \[\[th~mJfoI\[on\]\].338Kank Word0 \[s\]1 \[the\]2 \[and\]3 \[a\]4 \[o~\]5 \[in\]6 \[to\]500 \[students\]501 \[material \ ]502 \[tun\]503 \[words\]504 \[period\]505 \[class\]506 \[question\]5000 \[ ling\] \[them\] \]5001 \[ \[mort\] \[k\]5002 \[ \[re\] \[lax\] \]5003 \[\[rig\] \[id\]\]5004 \[\[connect\] \[ed\]\]5005 \[\[i\]Ek\]\]5006 \[\[hu\] \ [ t \ ] \ ]26000 \[ \ [ploural \]  \[blood\] \[supply\] \]26001 \[ \[anordinary\] \[happy\] \[family\] \]26002 \[\[f\] leas\] \[ibility\] \[of\]\]26003 \[ \[lunar\] \[brightness\] \[distribut ion\] \]26004 \[ \[primarily\] \[diff\] \[using\] \]26005 \[\[sodium\] \[tri\] \[polyphosphate\]\]26006 \[\[charcoal\] \[broil\] ted\]\]Figure 4: Sections of the lexicon learned from theBrown corpus, ranked by frequency.
The words inthe less-frequent half are listed with their first-leveldecomposition.
Word 5000 causes crossing-bracketviolations, and words 26002 and 26006 have internalstructure that causes recall violations.measure indicates that the algorithm has made veryfew clear mistakes.
Of course, the hierarchical lexicalrepresentation does not make a commitment to whatlevels are "true words" and which are not; about5 times more internal nodes exist than true words.Experiments in section 5 demonstrate that for mostapplications this is not only not a problem, but de-sirable.
Figure 4 displays ome of the lexicon learnedfrom the Brown corpus.The algorithm was also run as a compressoron a lower-case version of the Brown corpus withspaces and punctuation left in.
All bits neces-sary for exactly reproducing the input were counted.Compression performance is 2.12 bits/char, signifi-cantly lower than popular algorithms like gzip (2.95bits/char).
This is the best text compression resulton this corpus that we are aware of, and should notbe confused with lower figures that do not includethe cost of parameters.
Furthermore, because thecompressed text is stored in terms of linguistic unitslike words, it can be searched, indexed, and parsedwithout decompression.5 Learning Mean ingsUnsupervised learning algorithms are rarely used inisolation.
The goal of this work has been to ex-plain how linguistic units like words can be learned,so that other processes can make use of theseunits.
In this section a means of learning the map-pings between words and artificial representationsof meanings is described.
The composition-and-perturbation encompasses this application eatly.Imagine that text utterances are paired with rep-resentations of meaning, s and that the goal is to findthe minimum-length description of both the text andthe meaning.
If  there is mutual information betweenthe meaning and text portions of the input, then bet-ter compression is achieved if the two streams arecompressed simultaneously.
If a text word can havesome associated meaning, then writing down thatword to account for some portion of text also ac-counts for some portion of the meaning of that text.The remaining meaning can be written down moresuccinctly.
Thus, there is an incentive to associatemeaning with sound, although of course the associ-ation pays a price in the description of the lexicon.Although it is obviously a naive simplification,many of the interesting properties of the composi-tional representation surface even when meaningsare treating as sets of arbitrary symbols.
A word isnow both a character sequence and a set of symbols.The composition operator concatenates the charac-ters and unions the meaning symbols.
Of course,there must be some way to alter the default meaningof a word.
One way to do this is to explicitly writeout any symbols that are present in the word's mean-ing but not in its components, or vice versa.
Thus,the word red {RED} might be represented as r o e od+RED.
Given an existing word berry {BERRY},the red berry cranberry {RED BERRY} can be rep-resented c o r o a o n o berry {BERRY}+RED.5.1 Resu l tsTo test the algorithm's ability to infer word mean-ings, 10,000 utterances from an unsegmented textualdatabase of mothers' speech to children were pairedwith representations of meaning, constructed by as-signing a unique symbol to each root word in the vo-cabulary.
For example, the sentence and wha~ is hepainting a plc~ure o f f  is paired with the unorderedmeaning AND WHAT BE HE PA INT  A P IC -TURE OF.
In the first experiment, the algorithmreceived these pairs with no noise or ambiguity, us-ing an encoding of meaning symbols such that eachsymbol's length was 10 bits.
After 8 iterations oftraining without meaning and then a further 8 it-erations with, the text sequences were parsed againwithout access to the true meaning.
The meaningsSThis framework is easily extended to handle multi-ple ambiguous meanings (with and without priors) andnoise, but these extensions will not be discussed here.339of the resulting word sequences were compared withthe true meanings.
Symbol accuracy was 98.9%, re-call was 93.6%.
Used to differentiate the true mean-ing from the meanings of the previous 20 sentences,the program selected correctly 89.1% of the time, orranked the true meaning tied for first 10.8% of thetime.A second test was performed in which the algo-rithm received three possible meanings for each ut-terance, the true one and also the meaning of thetwo surrounding utterances.
A uniform prior wasused.
Symbol accuracy was again 98.9%, recall was75.3%.The final lexicon includes extended phrases, butmeanings tend to filter down to the proper level.For instance, although the words duck, ducks, theducks and duekdrink all exist and contain the mean-ing DUCK, the symbol is only written into the de-scription of duck.
All others inherit it.
Similar re-sults hold for similar experiments on the Brown cor-pus.
For example, scratching her nose inherits itsmeaning completely from its parts, while kicking thebucke~ does not.
This is exactly the result arguedfor in the motivation section of this paper, and illus-trates why occasional extra words in the lexicon arenot a problem for most applications.6 Other  App l i ca t ions  and  Cur rentWorkWe have performed other experiments using this rep-resentation and search algorithm, on tasks in unsu-pervised learning from speech and grammar induc-tion.Figure 5 contains a small portion of a lexiconlearned from 55,000 utterances of continuous speechby multiple speakers.
The utterances are taken fromdictated Wall Street :Journal articles.
The concate-nation operators was used with phonemes as termi-nals.
A second layer was added to the frameworkto map from phonemes to speech; these extensionsare described in more detail in (de Marcken, 1995b).The sound model of each phoneme was learned sep-arately using supervised training on different, seg-mented speech.
Although the phoneme model is ex-tremely poor, many words are recognizable, and thisis the first significant lexicon learned directly fromspoken speech without supervision.If the composition operator makes use of context,then the representation extends naturally to a morepowerful form of context-free grammars, where com-position is tree-insertion.
In particular, if each wordis associated with a part-of-speech, and parts ofspeech are permissible terminals in the lexicon, then"words" become production rules.
For example, aword might be VP ~ take off NP and representedin terms of the composition of VP ---* V P NP, V ---*~ake and P ---* off.
Furthermore, VP --* V P NP maybe represented in terms of VP ---* V PP and PP ---*P~ank w rep(w)5392 \[wvrmr\] \[\[w3r\]mr\]5393 \[Oauzn\] \[O\[auzn\]\]5394 \[tahld\] \[\[tah\]Id\]5395 \[~ktld\] \[~k\[tld\]\]5396 \[Anitn\] \[An\[itn\]\]5397 \[m?1i~ndalrz\] \[\[m?liindalr\]z\]8948 \[aldiiz\] \[\[al\]di~z\]8949 \[s\]krti\] \[s~k\[rti\]\]8950 \[130taim \] \[\[130\]\[talm\]\]8951 \[s?kgIn\] \[\[s?k\] \[gln\]\]8952 \[wAnpA\] \[\[wAn\]PAl8953 \[vend~r\] \[v\[~n\]\[d~r\]\]8954 \[ollmlnei\] \[e\[lImln\]\[ei\]\]8955 \[m~lii~\] \[\[m~l\]i\[i0\]\]8956 \[b?1iindal\] \[b~\[liindal\]\]9164 \[gouldm~nsmks\] \[\[goul\] d [rr~n\]s \[a~ks\]\]9165 \[kmp~utr\] \[\[kmp\] \[~ut\]r\]9166 \[gavrmin\] \[ga\[vrmin\]\]9167 \[oublzohuou\] \[\[oubl\]\[~.ohuou\]\]9168 \[ministrei~in\] \[\[min\]i\[strei~in\]\]9169 \[tj?rtn\] \[\[tj?\]r \[in\]\]9170 \[hAblhahwou\] \[\[hAbl\]\[h~hwou\]\]9171 \[shmp~iO\] \[S\[hmp\] \[6iO\] \]9172 \[prplou ,l\] \[\[prJ\[plou\] .l\]9173 \[bouskgi\] \[\[bou\]\[skg\]i\]9174 \[kg?d\]il\] \[\[kg?\]\[dji\]l\]9175 \[gouldmaiinz\] \[\[goul\]d\[maiinz\]\]9176 \[k~rpreiUd\] \[\[brpr\] \[eitld\]\]Figure 5: Some words from a lexicon learned from55,000 utterances of continuous, dictated Wall Street:Journal articles.
Although many words are seem-ingly random, words representing million dollars,Goldman-Sachs, thousand, etc.
are learned.
Further-more, as word 8950 (loTzg time) shows, they are oftenproperly decomposed into components.P NP.
In this way syntactic structure merges in theinternal representation f words.
This sort of gram-mar offers significant advantages over context-freegrammars in that non-independent rule expansionscan be accounted for.
We are currently looking atvarious methods for automatically acquiring parts ofspeech; in initial experiments some of the first suchclasses learned are the class of vowels, of consonants,and of verb endings.7 Conc lus ionsNo previous unsupervised language-learning proce-dure has produced structures that match so closelywith linguistic intuitions.
We take this as a vindi-cation of the perturbation-of-compositions represen-tation.
Its ability to capture the statistical and lin-guistic idiosyncrasies of large structures without sac-340rificing the obvious regularities within them makes ita valuable tool for a wide variety of induction prob-lems.ReferencesLeonard E. Baum, Ted Petrie, George Soules, and Nor-man Weiss.
1970.
A maximization technique occur-ing in the statistical analysis of probabaJistic functionsin markov chains.
Annals of Mathematical Statistics,41:164-171.Glenn Carroll and Eugene Charniak.
1992.
Learn-ing probaballstic dependency grammars from labelledtext.
In Working Notes, Fall Symposium Series,AAAL pages 25-31.Timothy Andrew Cartwright and Michael R. Brent.1994.
Segmenting speech without a lexicon: Evidencefor a bootstrapping model of lexical acquisition.
InProc.
of the 16th Annual Meeting of the Cognitive Sci-ence Society, Hillsdale, New Jersey.Stanley F. Chen.
1995.
Bayesian grammar induction forlanguage modeling.
In Proe.
$2nd Annual Meeting ofthe Association for Computational Linguistics, pages228-235, Cambridge, Massachusetts.Noam A. Chomsky.
1955.
The Logical Structure of Lin-guistic Theory.
Plenum Press, New York.Carl de Marcken.
1995a.
Lexical heads, phrase structureand the induction of grammar.
In Third Workshop onVery Large Corpora, Cambridge, Massachusetts.Carl de Marcken.
1995b.
The unsupervised acquisitionof a lexicon from continuous speech.
Memo A.I.
Memo1558, MIT Artificial Intelligence Lab., Cambridge,Massachusetts.Sabine Deligne and Frederic Bimbot.
1995.
Languagemodeling by variable length sequences: Theoreticalformulation and evaluation of multigrams.
In Proceed-ings of the International Conference on Speech andSignal Processing, volume 1, pages 169-172.A.
P. Dempster, N. M. Liard, and D. B. Rubin.
1977.Maximum lildihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society,B(39):I-38.T.
Mark Ellison.
1992.
The Machine Learning of Phono-logical Structure.
Ph.D. thesis, University of WesternAustralia.W.
N. Francis and H. Kucera.
1982.
Frequency analysisof English usage: lexicon and grammar.
Houghton-Mifflin, Boston.Zellig Harris.
1968.
Mathematical Structure of Lan-guage.
Wiley, New York.Donald Cort Olivier.
1968.
Stochastic Grammars andLanguage Acquisition Mechanisms.
Ph.D. thesis, Har-vard University, Cambridge, Massachusetts.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed corpora.In Proc.
$9th Annual Meeting of the Association forComputational Linguistics, pages 128-135, Berkeley,California.Jorma Rissanen.
1978.
Modeling by shortest data de-scription.
Automatica, 14:465-471.Jorma Rissanen.
1989.
Stochastic Complexity in Statis-tical Inquiry.
World Scientific, Singapore.R.
J. Solomonoff.
1960.
The mechanization of linguis-tic learning.
In Proceedings of the 2nd InternationalConference on Cybernetics, pages 180-193.Andreas Stolcke.
1994.
Bayesian Learning of Proba-balistic Language Models.
Ph.D. thesis, University ofCalifornia at Berkeley, Berkeley, CA.J.
Gerald Wolff.
1982.
Language acquisition, data com-pression and generalization.
Language and Communi-cation, 2(1):57-89.J.
Ziv and A. Lempel.
1978.
Compression of individualsequences by variable rate coding.
I ggg  Transactionson Information Theory, 24:530-538.341
