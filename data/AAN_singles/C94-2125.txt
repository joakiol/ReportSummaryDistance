ALGORITHM FOR AUTOMATIC INTERPRETATION OF NOUN SEQUENCESLucy VanderwendeMicrosoft Researchlucyv@ microsoft.cornABSTRACTThis paper describes an algorithm forautomatically interpreting noun sequences inunrestricted text.
This system uses broad-coverage semantic information which has beenacquired automatically by analyzing thedefinitions ira an on-line dictionary.
Previously,computational studies of noun sequences madeuse of hand-coded semantic information, and theyapplied the analysis rules sequentially.
Incontrast, the task of analyzing noun sequences inunrestricted text strongly favors an algorithmaccording to which the rules are applied inparallel and the best interpretation is determinedby weights associated with rule applications.1.
INTRODUCTIONThe inte~opretation of noun sequences(henceforth NSs, and also known as nouncompounds or complex nominals) has long been atopic of research in natural language processing(NLP) (Finin, 1980; Sparck Jones, 1983;Leonard, 1984; Isabelle, 1984; Lehnert, 1988; andRiloff, 1989).
The challenge in analyzing NSsderives from the semantic nature of the problem:their interpretation is, at best, only partiallyrecoverable from a syntactic or a morphologicalanalysis of NSs.
To arrive at an interpretation ofplum sauce which specifies that plum is theIngredient of sauce, or of knowledgerepresentation, specifying that knowledge is theObject of representation, requires semanticinformation for both the first noun (the modifier)and the second noun (the head).In this paper, we are concerned withinterpreting NSs which are composed of twonouns, ira absence of the context in which the NSappears; this scope is similar to most of thestudies mentioned above.
The algorithm forinterpreting a sequence of two nouns is intendedto be basic to the algorithm for interpretingsequences of more than two nouns: each pair ofNSs will be interpreted in turn, and the bestinterpretation forms a constituent which canmodify, or be modified by, another noun or NS(see also Finin, 1980).
There is no doubt thatcontext, both intra- and inter-sentential, plays arole in determining the correct interpretation of aNS, since the most plausible interpretation inisolation might not be the most plausible incontext.
It is, however, a premise of the presentsystem that, whatever the context is, theinterpretation of a NS is always available in thelist of possible interpretations.
A NS that isah'eady listed in an on-line dictionary needs nointerpretation because the meaning can be derivedfrom its definition.In the studies of NSs mentioned above, thesystems tbr interpreting NSs have relied on hand-coded semantic information, which is limited to aspecific domain by the sheer effort involved increating such a semantic knowledge base.
Thelevel of detail made possible by hand-coding hasled to the development of two main algorithmsfor the automatic interpretation of NSs: conceptdependent and sequential rule application.The concept dependent algorithm (Finin,1980) requires each lexical item to contain anindex to the rule(s) which should be applied whenthat item is part of a NS; it has the advantage thatonly those rules are applied for which theconditions are met and each noun potentiallysuggests a unique interpretation.
Whenever theresult of the analysis is a set of possibleinterpretations, the most plausible one isdetermined on the basis of the weight which isassociated with a role fitting procedure.
Thedisadvantage of this approach is that this level oflexical information cannot be acquiredautomatically, and so this approach cannot beused to process unrestricted text.The algorithm for sequential rule application(Leonard, 1984) focuses on the process ofdetermining which interpretation is the mostplausible; the fixed set of rules are applied in afixed order and the first rule for which theconditions are met results in the most plausibleinterpretation.
This algorithm has the advantagethat no weights are associated with the rules.
Thedisadvantage of this approach is that the degree towhich the rules are satisfied cannot be expressed,and so, in some cases, the most plausible782interpretation of an NS will not be produced.Also, section 4 will show that this algorithm issuitable only when the sense of each noun is agiven, a situation which is not true for processingunrestricted text.This paper introdt.ces an algorithm which isspecifically designed for analyzing NSs inunrestricted text.
The task of processingunrestricted text has two consequences: firstly,hand-coded semantic information, and therefore aconcept dependent algorithm, is no longerfeasible; and secondly, the intended sense of eachnoun cau no longer be taken as a given.
\]'healgorithm described here, therefore, relies onsemantic information which has been extractedautomatically fi'om an on-line dictionary (seeMontemagni and Vanderwende, 1992; l)ohm etal., 1993).
This algorithm manipulates a set ofgeneral rules, each of which has an associatedweight, and a general procedure for matchingwords.
The result of this algorithm is an orderedset of interpretations and partial scnse-disambiguation of the nouns by taking note ofwhich noun senses were most relevant in each ofthe possible interpretations.We will begin by reviewing thechtssification schema for NSs described inVanderwende (1993) and the type of general ruleswhich this algorithm is designed to handle.
Thematching procedure will be described; byintroducing a separate matching procedure, therules in Vanderwende (1993) can be organized insuch it way as to make the algorithm moreefficient.
We will then show the algorithm t'orrule application in delail.
This algorithm differsfiom I,conard (1984) by applying all of the rulesbefore determining which interpretation is themost plausible (effectively, a parallel ruleapplication), rather than determining the bestinterpretations by the order in which the rules ateapplied (a serial rule application).
In section 4, wewill provide examples which illustrate that aparallel algorithm is required when processingunrestricted, uudisambiguated text.
Finally, lheresults of applying this algorithm to a trainingand a test corpus of NSs will be presented, alongwith a discussion of these results and fnrtherdirections fox" research in NS analysis.1.1 NS interpretations'Fable 1 shows a classil'ication schema forNSs (Vande,wende, 1993) which accounts formost of the NS classes studied previously intheoretical linguistics (Downing, 1977; Jespersen,1954; Lees, 1960; and Levi, 1978).
The relationwhich holds between the nouns in a NS hasconventionally been given names such as Purposeor Location.
The classification schema that isused in this system has been formulated as wh-questions.
A NS 'can be classified according towhich wh-question the modifier (filwt noun) bestanswers' (Vanderwende, 1993).
Deciding how aNS should be classified is not at all clear and weneed criteria for judging whether a NS has beenclassified appropriately.
The formulation of NSclasses its wh-questions i intended to provide atleast one criterion for judging NS classification;other criteria are provided in Vanderwende(1993).Table I.
Classification schema for NSsRelation Conventional Name ExampleWho/what?Whom/what?Where?When?Whose?__ .
What is it part of?What are its .parts?What kind of?How?What for?Made of what?What does it cause'?What causes it?SubjectObjectLocativeTimePossessiveWhole-PartPart-WholeEquativcInstrtnnentPurposeMaterialCausesCaused-bypress reportaccident reportfield mousenight attackfamily estateduck lootdaisy chainflounder fishparaffin cookerbird sanctuarL_alligator shoedisease germ ___drug death7831.2 General rules for NS analysisEach general rule call be considered to be adescription of the configuration of semantic andsyntactic attributes which provide evidence for aparticular NS interpretation, i.e., a NSclassification.
Exactly how these rules are appliedis the topic of this paper.
Typically, the generalrules correspond in a many-to-one relation to thenumber of classes in the classification schemabecause more than one combination of semanticattributes can identify the NS as a member of aparticular class.
This is illustrated in Table 2,which presents two of the rules tbr establishing a'What for?'
interpretation.The first rule (H1) tests whether thedefinition of the head contains a PURPOSE orINSTRUMENT-FOR attribute which matches(i.a., has the same lemlna as) the modifier.
Whenthis rule is applied to the NS bird sanctuary, therule finds that a PURPOSE attribute has beenidentified automatically in the definition of thehead: sanctuary (L n,3) 'an area for birds or otherkinds of animals where they may not be huntedand their animal enemies are controlled'.
(Allexamples are from the Longman Dictionary ofContemporary English, Longman Group, 1978.
)The values of this PURPOSE attribute are birdand animal.
The rule HI verifies that thedefinition of sanctuary contains a PURPOSEattribute, and that one of its values, namely bird,has the same lemma as the modifier, the firstnoun.The second rule (H2) tests a differentconfiguration, namely, whether the definition ofthe head contains a LOCATION-OF attributewhich matches the modifier; the example birdcage will be presented in section 2.These rules are in a notation modified fromVanderwende (1993, pp.
166-7).
Firstly, the ruleshave been divided into those that test attributes onthe head, as rules HI and H2 do, and those thattest attributes on the modifier.
Secondly,associated with each rule is a weight.
Unlike inVanderwende (1993), this rule weight is only partof the final score of a rule application; the finalscore of a rule application is composed of boththe rule weight and the weight returned from thematching procedure, which will be described inthe next section.2.
THE MATCHING PROCEDUREMatching is a general procedure whichreturns a weight to reflect how closely related twowords are, in this case how related the value of anattribute is to a given lemma.
The weight returnedby the matching procedure is added to the weightof the rule to arrive at the score of the rule as awhole.
Ill the best case, the matching procedurefinds that the lemma is the same as the value oftile attribute being tested.
We saw above that illthe NS bird sanctuary, the lnodifier bird has thesame lemma as the value of a PURPOSE attributewhich can be identified in the definition of thehead, sanctualy.
The weight associated with snchan exact match is 0.5.
Applying rule H1 ill Table2 to the NS bird sanctuary has an overall score of1; the match weight 0.5 added to the rule weight0.5.When an exact match cannot be foundbetween the lemma and the attribute value, thematching procedure can investigate a match givensemantic information for each of the senses of thelemma.
(Only in the worst case would this beequivalent to applying each rule to eachcombination of modifier and head senses.)
Ofcourse the HYPERNYM attribute will be usefulto find a match.
Applying rule HI to the NS owlsanctuary, a match is found between thePURPOSE attribute in the definition of sanctuaryand the modifier owl, because the definition ofowl (L n,l): 'any of several types of night birdwith large eyes, supposed to be very wise',identifies bird (one of the values of sanctual:v'sPURPOSE attribute) as the HYPERNYM of owl.Whenever the HYPERNYM attribute is used, theweight returned by the matching procedure isonly 0.4.Table 2.
Rules for a 'What for?'
interpretationSENS classWhat for?Rulename.HIModifier.attributesHead attributesmatchIExamplematch PURP()SE, water heater,INSTRUMENT- bird sanctuaryFORLOCATION-OF bird cage,~ e  cam_pWeight'0.5784Other semantic attributes are also relewmtfor l'inding a match.
Fig.
I shows graphically howthe attribute HAS-PART can be used to establisha match.
One of the 'Who~What?"
rules testswhether any of the verbal senses of the head has aBY-MEANS-OF attribute which lnatches themodifier.
In the verb definition o1' scratch (I, v, I):'to rub and tear or mark (a surface) withsomething pointed or rough, as with claws orfingernails', a P,Y-MI';ANS-OF attribute can beidenlified with claw and.fingernail as its values,neither of which match the modifier norm cat.Now the ma|ching procedure investigates thesenses of cat attempting to find a match.
Thedefinition of eat (L n, 1): 'a small animal with softfur and sharp teeth and claws (nails) .... ' klentil'iesclaw (one o1' scratch's 13 Y-MF, ANS-OFaltributes) as one of the wflues of ItAS-PART,thus establishing the match shown in Fig.
I. Theweight associated with a match using \[tAS-PART, PART-()F, ltAS-MATERIAL, orMATERIAL-OF is 0.3.HAS-PAre >>K ?
/~ /  BY-MEANS-OFQ cat -) (-scratchFig.
1.
'Who/what?'
interpretation for cat scratchwith cat (/, n, I) and scratch (l, v, 1)lqg.
2 shows how also the attrilmtes IIAS-OBJECT and HAS--SUfLIECT can be used; thistype of match is required when a rule calls for amatch between a lemma (which is a noun) and anattribute which typically has a verb as its value,since we can expect no link between a noun and averb according to hypernymy or any part relation.In the definition of cage (l, n, I): 'a framework ofwires or bars m which animals or birds may hckept or carried', a IX)CATION-.OF attribute canbe identified, with as its value the veflm keep andcarry and a nested HAS-()BJI~;CI" attribute, withanimal and bird as its wflue; it is the HAS-()BJECT attribute which can match the modifiernoun bird.
A match using the HAS-OBJECT orIlAS-SUBJI';CT attribute carries a weight of 0.2.f .
~oo.,, )~/~ ?
I OCATION-OFFig.
2.
'What for?'
interpretation for bird cagewith cage (L n, l)Even when alternate matches are beinginvestigated, such as a match using \[I\[AS-OBJECT, the senses of the lemma can still beexamined.
In this way, a 'What for?
'interpretation can also be determined for the NScanat:v cage, shown in Fig.
3; the weight for thistype of link is O.
1.IIAS-OBJECTC '~'~;) < <_ ,oo,, >/\[~YPE,~NYM "~ LOCATION-OFFig.
2.
'What for?'
interpretation for canary cagewith canary (L n, 1 ) and cage (L n, 1)In Vanderwende (1993), the rules themselvesspecified how to find the indirect matchesdescribed above.
By separating the matchinginformation from the information relevant to eachrole, the matching can be applied moreconsistently; but equally important, the rolesspecify only those semantic attributes thatindicate a specific interprelation.3.
ALGORITHM FOR APPLYING RULI,;SThe algoritlm\] controls how the set ofgeneral rules will be applied in order to interpretNSs in unrestricted text.
Given that a separateprocedure for matching exists, the rules arenaturally formulated as conditions, in the form ofa semantic attribute(s) to be satisfied, on eitherthe modifier or head, but not necessarily on bothat the same time.
This allows the rules lo bedivided into groups: modifier-based, head-based,and deverbal-head based.
NSs with a deverbalhead require additional conditions in the rules; ifdeverbal-head based rules were applied on parwith the headqmsed rules, the deverbal-head ruleswouM apply far too often, leading to spuriousinterpretations, because in English nouns andverbs are often homographs.785The algorithm for interpreting NSs has foursteps:1. apply the head-based rules to each ofthe noun senses of the head and the lemma ofthe modifier2.
apply the modifier-based rules to eachof the noun senses of the modifier and thelemma of the head3.
if no interpretation has received aweight above a certain threshold, then applythe deverbal-head rules to each of the verbsenses of the head and the lemma of themodifier4.
order the possible interpretations bycomparing the weights assigned by the ruleapplications and return the list in order oflikelihoodThe semantic attributes which are found inthe head-based conditions are: LOCATED-AT,PART-OF, HAS-PART, HYPERNYM, BY-MEANS-OF, PURPOSE, INSTRUMENT-FOR,LOCATION-OF, TIME, MADE-OF, ROLE,CAUSES and CAUSED-BY.
The semanticattributes which are found in the modifier-basedconditions are: SUBJECT-OF, OBJECT-OF,LOCATION-OF, TIME-OF, HAS-PART, PART-OF, HYPERNYM, MATERIAL-OF, CAUSESand CAUSED-BY.
The semantic attributes in thedeverbal-head based conditions are: HAS-SUBJECT, BY-MEANS-OF, and HAS-OBJECT.In Vanderwende (1993), it was suggestedthat each rule is applied to each combination ofhead sense and modifier sense.
If the modifier hasthree noun senses and the head has four nounsenses, then each of the 34 general rules wouldapply to each of the (3x4) possible combinations,for a total of 408 rules applications.
With thecurrent algorithm, if the modifier has three nounsenses and the head has four noun senses, thenfirst the eleven modifier oles apply (3xl 1), thenthe sixteen head rules apply (4xl6), and if thehead can be analyzed as a deverbal noun, thenalso the seven deverbal-head rules apply (4x7),for a total of 125 rule applications.
Only after allof the rules have applied are the possibleinterpretations ordered according to their scores.It may seem that we have made the task ofinterpreting NSs artificially difficult by takinginto consideration each noun sense in themodifier and head; one might argue that it isreasonable to assume that these nouns could besense-disambiguated b fore NS analysis.
We arenot aware of any study which describes ense-disambiguation of the nouns in a NS.
On thecontrary, Braden-Harder (1992) suggests that theresults of disambiguation can be improved whenrelations such as verb-object, purpose, andlocation, are available; these relations are theresult of our NS analysis, not the input.4.
PARALLEL VERSUS SERIAL RULEAPPLICATIONAs we have seen above, the overall score foreach possible interpretation is a combination ofthe weight of a rule and the weight returned bythe matching procedure.
A rule with a relativelyhigh weight may have a low score overall if thematch weight is very low, and a role with arelatively low weight could have a high overallscore if the match weight is particularly high.
It istherefore impossible to order the rules a prioriaccording to their weight.In Leonard (1984), the most plausibleinterpretation is determined by the order in whichthe rules are applied.
By ordering the 'search for amaterial modifier' ahead of the 'search for arelated verb', the interpretations of both silver penand ink pen will be the same, given that bothsilver and ink are materials.
In fact, only silverpen is correctly analyzed by the 'search for amaterial modifier' rule, while the correctinterpretation of ink pen would have used the'search for a related verb'.The problem with rule ordering iscompounded when more than one sense of eachnoun is considered.
In Leonard's lexicon, pen\[l\]is the writing implement and pen\[2\] is theenclosure for keeping animals in.
By ordering a'search for a related verb' ahead of a 'search for alocative', the interpretation of the NS bull pen isincorrect: 'a pen\[1\] that a bull or bulls writessomething with'.
Less likely is the correctlocative interpretation 'apen\[2 \]Jbr or containinga bull or bulls'.In our system, the most likely interpretationsof bull pen are ordered correctly because, for thelocative interpretation, we find meaningfulmatches in the definitions of bull and pen: thedefinition of pen (L n,l): 'a small piece of landenclosed by a fence, used esp.
for keepinganimals in', identifies a PURPOSE attribute, withthe verb keep and a nested HAS-OBJECT animalas its values.
The HAS-OBJECT animal can bematched with the modifier lemma bull, becauseone of the HYPERNYMs of bull (L n,2) isanimal.
For the related verb interpretation,786however, we find no match between the typicalsubjects the verb related to pen, namely write,and the modifier bull; a 'Who/What?
'interpretation is only possible because bull is ananimate, and, by default, animates can be thesubject of a verb.We must conclude that what is important isthe degree to which there is a match between thevalues of these attribules and the lemma, and notmerely the presence or absence of semanticattributes.
Only after all of the rttles have beenapplied can the most plausible interprelation bedetermined.5.
TEST, RESULTS AND I)ISCUSSIONThe results that arc under discussion wereobtained on the basis of semantic informationwhich was automatically extracted from LongmanI)ictionary of Contemporary English (I~I)OCE) asdescribed in Montemagni and Vanderwende(1992) 1 .
The semantic information has not beenaltered in any way fl'onl its automatically derivedform, and so there are still errors: for the 94,()00attribute clusters extracted fl'om nearly 75,000single noun and verb definitions in L1)()CE, weestimate the accuracy to be 78%, with a margin oferror of +/- 5% (see Richardson et al, 1993).A training corpus of 100 NSs was collected\[rein lhc examples of NSs in lhe 1)reviousliterature, to ensure that all known classes of NSsare handled in this system.
These results wereexpected to be good because these NSs were usedto develop the rules and their weights.
Thesystem successfully identified the most likelyinterpretation for 79 of the 100 NSs (79%).
Of theremaining 21 NSs, tile most plausibleinterpretation was alnong the possibleinterpretations 8 times, (8 %), and nointerpretation at all was given for 4 NSs (4 %).The test corpus consisted of 97 NSs from thetagged version of the Brown corpus (I;rancis andKucera, 1989), to ensure the adequacy ofapplying this approach to unrestricted test; theresults for an expanded test corpus will bereported in Vanderwende (in preparation).
Tbesystem currently identified successfully II1e mostlikely inlerpretation lor 51 of the 97 NSs (52%).
()1' the remaining 46 NSs, the most likelyinterpretation was presented second for 21 NSsIAlthough 1,1)OCE includes omc semanticinformation i  the form of box codes and subjectcodes, these were not used in this system.
Thisapproach isdesigned t() work with semanticinformation from any dictionary.
(22 %); when first and second interpretationswere considered, the system was successfulapproximately 74% of the time.
A wrong or nointerpretation was given for 25 NSs (26 %1).
Uponexamination of these results, several areas forimprovement arc suggested.
First is to improvelhe semantic information: Dolan et al (1993)describes a network of semantic information,given not only the definition of the Icxical entrybut also all of the other definitions which have alabeled relation to that entry.Secondly, while the NS classificationproposed in Vanderwende (1993) proves to beadequate for analyzing NSs in tmrestrictcd text,an additional 'What about?'
class, suggested inLevi (1978), may be justified.
In the currentclassification schema, NSs such as cigare.tte warand history confi, rence have been considered'Whom/what?'
NSs given tim verbs that areassociated with the head, fight and eonJer/talkabout respectively.
In unrestricted text, similarNSs are quite fi'equent, for example universitypolicy, prevention program, care plan, but thedefinitions of the heads do not always specify arelated verb.
The bead definitions for policy,program and plan, however, do allow a IIAS-TOPIC semantic feature to be identified, and thisIIAS-TOPIC can be used to establish a 'Whatabout?'
interpretation.Applying this algorithm to previouslytmseen text also produced a very promisingresult: the verbs that are associated with nouns intheir definitions (i.e., role nominals in lqnin,1980) are being used often and correctly toproduce NS interpretations.
While some rules hadbeen developed to handle obvious cases in thetraining corpus, how often the conditions on theserules would be met could not be predicted.
Infact, such NS interpretations are frequent.
Forexample, the NS wine cellar is analyzed as a'What for?'
relation with a high score, and thesystem provides as a paraphrase: cellar w,~ich isfor storing wine, given the definition of cellar (1,n, 1): 'an undergrotmd room, ttsu.
used for storinggoods; basement'.
This result is promising fortwo reasons: first, by analyzing the definitions(and later also the example sentences) in an on--line dictionary, we now have access to a non-handcoded source of semantic information whichincludes the verbs and their lelation to the notms,essential for determining role nominals.
Second,the related verbs are used to construct theparaphrases of a NS, and doing so makes ageneral interpretation such as 'What lot?'
more787specific, e.g., a service office is not an office forservice, but an office for ~ service, and avegetable market is not a market for vegetables,but a market for buying and selling vegetables.Enhancing the general interpretations with therelated verb(s) approximates at least in partDowning's observation that the types of relationsthat can hold between the nouns in a NS arepossibly infinite (Downing, 1977).6.
CONCLUSIONSOur goal is to create a system for analyzingNSs automatically on the basis of semanticinformation extracted automatically from on-lineresources; this is a strong requirement for NLP asit moves its focus away from technicalsublanguages towards the processing ofunrestricted text.
We have shown that processingNSs in unrestricted text strongly favors analgorithm which is comprised of a set of generalrules and a general procedure for matching twowords, each of which have associated weights.This algorithm must apply all of the rules beforethe most plausible NS interpretation can bedetermined.Several directions for further esearch can bepursued within this approach: a methodology forautomatically assigning the weights associatedwith the rules and the matching procedure,following Richardson (in preparation), and amethodology for incorporating the context of theNS into the analysis of NS interpretations.
We arealso pursuing the acquisition of semanticreformation which is not already available, alongthe same lines as extracting informationautomatically from on-line dictionaries.Acknowledgements: I would like to extend mythanks to the members of the Microsoft NLPgroup: George Heidorn, Karen Jensen, BillDolan, Joseph Pentheroudakis, Diana Peterson,and Stephen Richardson.REFERENCESBraden-Harder, L.C.
( 1991).
"SenseDisambiguation using on-line dictionaries: anapproach using linguistic structure and multiplesources of information".
New York University,NY.Dolan, W.B., L. Vanderwende and S.D.Richardson (1993).
Automatically derivingstructured knowledge bases from on-linedictionaries.
Proceedings of the First ConJerenceof the Pacific Association for ComputationalLinguistics, at Simon Fraser University,Vancouver, BC., pp.5-14.Downing, P. (1977).
On the creation and useof English compound nouns.
Language 53,pp.810-842.Finin, T.W.
(1980).
"The semanticinterpretation f compound nominals".
Universityof Illinois at Urbana-Champaign.
UniversityMicrofihns International.Isabelle, P. (1984).
Another look at nominalcompounds.
Proceedings of the 22nd AnnualMeeting of the Association for ComputationalLinguistics, COLING-84, Stanford, CA.
pp.509-516.Jespcrsen, O.
(1954).
A modern Englishgrammar on historical principles, VI.
GeorgeAllen & Unwin Ltd., London, 1909-49; reprinted1954.Lees, R.B.
(1960).
The grammar of Englishnominalizations.
Indiana University,Bloomington, Indiana, (Fourth printing 1966).Lehnert, W. (1988).
The analysis of nominalcompounds, In U. Eco, M. Santambrogio, and P.Violi Ed.
,Meaning and Mental Representations,VS 44/45, Bompiani, Milan.Leonard, R. (1984).
The interpretation ofEnglish noun sequences on the computer.
North-Holland Linguistic Studies, Elsevier, theNetherlands.Levi, J.N.
(1978).
The syntax and semanticsof complex nominals.
Academic Press, NewYork.Montemagni, S. and L. Vanderwende(1992).
Structural Patterns vs. string patterns forextracting semantic information fromdictionaries.
Proceedings of COLlNG92, Nantes,France, pp.546-552.Richardson, S.D., L. Vanderwende and W.B.Dolan (1993).
Combining dictionary-basedmethods for natural language analysis.Proceedings of TMl~93, Kyoto, Japan.
pp.69-79.Riloff, E. (1989).
"Understanding gerund-noun pairs".
University of Massachusetts, MA.Sparck Jones, K. (1983).
So what aboutparsing compound nouns?
In K. Sparck Jones andY.
Wilks Lkl., Autotnatic Natural LanguageParsing, Ellis Horwood, Chichester, England,pp.
164-8.Vanderwende, L. 1993.
SENS: the systemfor evaluating noun sequences.
In K. Jensen, G.E.Heidorn and S.D.
Richardson Ed., NaturalLanguage Processing: the PLNLP approach,Kluwer Academic Publishers, pp.
161-73.Vanderwende, L. (in prep.).
"The analysis ofnoun sequences using semantic informationextracted from on-line dictionaries".
GeorgetownUniversity, Washington, D.C.788
