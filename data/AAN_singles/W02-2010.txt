Named Entity Recognition as a House of Cards: Classifier StackingRadu FlorianDepartment of Computer Science and Center for Language and Speech ProcessingJohns Hopkins University3400 N. Charles St., Baltimore, MD 21218, USArflorian@cs.jhu.edu1 IntroductionThis paper presents a classifier stacking-based ap-proach to the named entity recognition task (NERhenceforth).
Transformation-based learning (Brill,1995), Snow (sparse network of winnows (Mu?ozet al, 1999)) and a forward-backward algorithm arestacked (the output of one classifier is passed as in-put to the next classifier), yielding considerable im-provement in performance.
In addition, in agree-ment with other studies on the same problem, theenhancement of the feature space (in the form ofcapitalization information) is shown to be especiallybeneficial to this task.2 Computational ApproachesAll approaches to the NER task presented in thispaper, except the one presented in Section 3, use theIOB chunk tagging method (Tjong Kim Sang andVeenstra, 1999) for identifying the named entities.2.1 Feature Space and BaselinesA careful selection of the feature space is a veryimportant part of classifier design.
The algorithmspresented in this paper are using only informa-tion that can be extracted directly from the train-ing data: the words, their capitalization informa-tion and the chunk tags.
While they can defi-nitely incorporate additional information (such aslists of countries/cities/regions, organizations, peo-ple names, etc.
), due to the short exposition space,we decided to restrict them to this feature space.Table 2 presents the results obtained by runningoff-the-shelf part-of-speech/text chunking classi-fiers; all of them use just word information, albeitin different ways.
The leader of the pack is the MX-POST tagger (Ratnaparkhi, 1996).
The measure ofchoice for the NER task is F-measure, the harmonicmean of precision and recall:   , usu-ally computed with   .As observed by participants in the MUC-6 and -7tasks (Bikel et al, 1997; Borthwick, 1999; Miller et1: Capitalization information2: Presence indictionaryfirst_cap, all_caps, all_lower,number, punct, otherupper, lower,both, noneTable 1: Capitalization informational., 1998), an important feature for the NER task isinformation relative to word capitalization.
In anapproach similar to Zhou and Su (2002), we ex-tracted for each word a 2-byte code, as summarizedin Table 1.
The first byte specifies the capitaliza-tion of the word (first letter capital, etc), while thesecond specifies whether the word is present in thedictionary in lower case, upper case, both or neitherforms.
These two codes are extracted in order to of-fer both a way of backing-off in sparse data cases(unknown words) and a way of encouraging gen-eralization.
Table 2 shows the performance of thefnTBL (Ngai and Florian, 2001) and Snow systemswhen using the capitalization information, both sys-tems displaying considerably better performance.2.2 Transformation-Based LearningTransformation-based learning (TBL henceforth) isan error-driven machine learning technique whichworks by first assigning an initial classification tothe data, and then automatically proposing, evalu-ating and selecting the transformations that max-imally decrease the number of errors.
Each suchtransformation, or rule, consists of a predicate anda target.
In our implementation of TBL ?
fnTBL ?predicates consist of a conjunction of atomic pred-icates, such as feature identity (e.g.
), membership in a set (e.g.
B ORG   ), etc.TBL has some attractive qualities that make itsuitable for the language-related tasks: it can au-tomatically integrate heterogenous types of knowl-edge, without the need for explicit modeling (simi-lar to Snow, Maximum Entropy, decision trees, etc);it is error?driven, therefore directly minimizes theMethod Accuracy without capitalization informationTnT 94.78% 66.72MXPOST 95.02% 69.04Snow 94.27% 65.94fnTBL 94.92% 68.06with capitalization informationSnow (extended templates) 95.15% 71.36fnTBL 95.57% 71.88fnTBL+Snow 95.36% 73.49Table 2: Comparative results for different methods on theSpanish development dataultimate evaluation measure: the error rate; and ithas an inherently dynamic behavior1.
TBL has beenpreviously applied to the English NER task (Ab-erdeen et al, 1995), with good results.The fnTBL-based NER system is designed in thesame way as Brill?s POS tagger (Brill, 1995), con-sisting of a morphological stage, where unknownwords?
chunks are guessed based on their morpho-logical and capitalization representation, followedby a contextual stage, in which the full interactionbetween the words?
features is leveraged for learn-ing.
The feature templates used are based on a com-bination of word, chunk and capitalization informa-tion of words in a 7-word window around the targetword.
The entire template list (133 templates) willbe made available from the author?s web page afterthe conclusion of the shared task.2.3 SnowSnow ?
Sparse Network of Winnows ?
is an archi-tecture for error-driven machine learning, consistingof a sparse network of linear separator units overa common predefined or incrementally learned fea-ture space.
The system assigns weights to each fea-ture, and iteratively updates these weights in sucha way that the misclassification error is minimized.For more details on Snow?s architecture, please re-fer to Mu?oz et al (1999).Table 2 presents the results obtained by Snow onthe NER task, when using the same methodologyfrom Mu?oz et al (1999), with the their templates2and with the same templates as fnTBL.1The quality of chunk tags evolves as the algorithm pro-gresses; there is no mismatch between the quality of the sur-rounding chunks during training and testing.2In this experiment, we used the feature patterns describedin Mu?oz et al (1999): a combination of up to 2 words in a3-word window around the target word and a combination ofup to 4 chunks in a 7-word window around the target word.
Allthroughout the paper, Snow?s default parameters were used.7070.57171.57272.57373.50 2 4 6 8 10Iteration NumberF?measureFigure 1: Performance of applying Snow to TBL?s out-put, plotted against iteration number2.4 Stacking ClassifiersBoth the fnTBL and the Snow methods havestrengths and weaknesses: fnTBL?s strength is represented by its dynamicmodeling of chunk tags ?
by starting in a sim-ple state and using complex feature interac-tions, it is able to reach a reasonable end-state.Its weakness consists in its acute myopia: theoptimization is done greedily for the local con-text, and the feature interaction is observedonly in the order in which the rules are se-lected. Snow?s strength consists in its ability to modelinteractions between the all features associatedwith a sample.
However, in order to obtaingood results, the system needs reliable contex-tual information.
Since the approach is not dy-namic by nature, good initial chunk classifica-tions are needed.One way to address both weaknesses is to com-bine the two approaches through stacking, by ap-plying Snow on fnTBL?s output.
This allows Snowto have access to reasonably reliable contextual in-formation, and also allows the output of fnTBLto be corrected for multiple feature interaction.This stacking approach has an intuitive interpreta-tion: first, the corpus is dynamically labeled us-ing the most important features through fnTBLrules (coarse-grained optimization), and then is fine-grained tuned through a few full-feature-interactioniterations of Snow.Table 2 contrasts stacking Snow and fnTBL withrunning either fnTBL or Snow in isolation - an im-provement of 1.6 F-measure points is obtained whenstacking is applied.
Interestingly, as shown in Fig-ure 1, the relation between performance and Snow-iteration number is not linear: the system initiallytakes a hit as it moves out of the local fnTBL maxi-mum, but then proceeds to increase its performance,Method Accuracy Spanish 98.42% 90.26Dutch 98.54% 88.03Table 3: Unlabeled chunking results obtained by fnTBLon the development setsfinally converging after 10 iterations to a F-measurevalue of 73.49.3 Breaking-Up the TaskMu?oz et al (1999) examine a different method ofchunking, called Open/Close (O/C) method: 2 clas-sifiers are used, one predicting open brackets andone predicting closed brackets.
A final optimiza-tion stage pairs open and closed brackets through aglobal search.We propose here a method that is similar inspirit to the O/C method, and also to Carreras andM?rquez (2001), Ar?valo et al (2002):1.
In the first stage, detect only the entity bound-aries, without identifying their type, using thefnTBL system3;2.
Using a forward-backward type algorithm (FBhenceforth), determine the most probable typeof each entity detected in the first step.This method has some enticing properties: Detecting only the entity boundaries is a sim-pler problem, as different entity types sharecommon features; Table 3 shows the perfor-mance obtained by the fnTBL system ?
the per-formance is sensibly higher than the one shownin Table 2; The FB algorithm allows for a global searchfor the optimum, which is beneficial since bothfnTBL and Snow perform only local optimiza-tions; The FB algorithm has access to both entity-internal and external contextual features (asfirst described in McDonald (1996)); further-more, since the chunks are collapsed, the localarea is also larger in span.The input to the FB algorithm consists of a seriesof chunks      , each spanning a sequence ofwords                   3For this task, Snow does not bring any improvement to thefnTBL?s output.Method Spanish DutchFB performance 76.49 73.30FB on perfect chunk breaks 83.52 81.30Table 4: Forward-Backward results (F-measure) on thedevelopment setsFor each marked entity , the goal is to determineits most likely type:4      (1)where    represents theentity-external/contextual probability, andis the entity-internalprobability.
These probabilities are computedusing the standard Markov assumption of inde-pendence, and the forward-backward algorithm5.Both internal and external models are using 5-gramlanguage models, smoothed using the modifieddiscount method of Chen and Goodman (1998).In the case of unseen words, backoff to the cap-italization tag is performed: if is unknown,    	  .
Finally, theprobability is assumed to beexponentially distributed.Table 4 shows the results obtained by stackingthe FB algorithm on top of fnTBL.
Comparingthe results with the ones in Table 2, one can ob-serve that the global search does improve the perfor-mance by 3 F-measure points when compared withfnTBL+Snow and 5 points when compared with thefnTBL system.
Also presented in Table 4 is the per-formance of the algorithm on perfect boundaries;more than 6 F-measure points can be gained byimproving the boundary detection alone.
Table 5presents the detailed performance of the FB algo-rithm on all four data sets, broken by entity type.A quick analysis of the results revealed that mosterrors were made on the unknown words, both in4We use the notation     .5It is notable here that the best entity type for a chunk iscomputed by selecting the best entity in all combinations ofthe other entity assignments in the sentence.
This choice ismade because it reflects better the scoring method, and makesthe algorithm more similar to the HMM?s forward-backwardalgorithm (Jelinek, 1997, chapter 13) rather than the Viterbialgorithm.Spanish and Dutch: the accuracy on known words is97.4%/98.9% (Spanish/Dutch), while the accuracyon unknown words is 83.4%/85.1%.
This suggeststhat lists of entities have the potential of being ex-tremely beneficial for the algorithm.4 ConclusionIn conclusion, we have presented a classifier stack-ing method which uses transformation-based learn-ing to obtain a course-grained initial entity anno-tation, then applies Snow to improve the classi-fication on samples where there is strong featureinteraction and, finally, uses a forward-backwardalgorithm to compute a global-best entity typeassignment.
By using the pipelined processing,this method improves the performance substan-tially when compared with the original algorithms(fnTBL, Snow+fnTBL).5 AcknowledgementsThe author would like to thank Richard Wicen-towski for providing additional language resources(such as lemmatization information), even if theywere ultimately not used in the research, DavidYarowsky for his support and advice during thisresearch, and Cristina Nita-Rotaru for useful com-ments.
This work was supported by NSF grant IIS-9985033 and ONR/MURI contract N00014-01-1-0685.ReferencesJ.
Aberdeen, D. Day, L. Hirschman, P. Robinson, andM.
Vilain.
1995.
Mitre: Description of the Alembicsystem used for MUC-6.
In Proceedings of MUC-6,pages 141?155.M.
Ar?valo, X. Carreras, L. M?rquez, M. A.
Mart?,L.
Padr?, and M. J. Sim?n.
2002.
A proposalfor wide-coverage Spanish named entity recognition.Technical Report LSI-02-30-R, Universitat Polit?c-nica de Catalunya.D.
M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.1997.
Nymble: a high-performance learning name-finder.
In Proceedings of ANLP-97, pages 194?201.A.
Borthwick.
1999.
A Maximum Entropy Approach toNamed Entity Recognition.
Ph.D. thesis, New YorkUniversity.E.
Brill.
1995.
Transformation-based error-driven learn-ing and natural language processing: A case studyin part of speech tagging.
Computational Linguistics,21(4):543?565.X.
Carreras and L. M?rquez.
2001.
Boosting trees forclause splitting.
In Proceedings of CoNNL?01.Spanish devel precision recall LOC 70.44% 83.45% 76.39MISC 53.20% 63.60% 57.93ORG 78.35% 73.00% 75.58PER 86.28% 84.37% 85.31overall 75.41% 77.60% 76.49Spanish test precision recall LOC 82.06% 79.34% 80.68MISC 59.71% 61.47% 60.58ORG 78.51% 78.29% 78.40PER 82.94% 89.93% 86.29overall 78.70% 79.40% 79.05Dutch deve precision recall LOC 81.15% 74.16% 77.50MISC 72.02% 74.53% 73.25ORG 79.92% 60.97% 69.17PER 66.18% 84.04% 74.05overall 73.09% 73.51% 73.30Dutch test precision recall LOC 86.69% 77.69% 81.94MISC 75.21% 68.80% 71.86ORG 74.68% 66.59% 70.40PER 69.39% 86.05% 76.83overall 75.10% 74.89% 74.99Table 5: Results on the development and test sets inSpanish and DutchS.
Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Tech-nical Report TR-10-98, Harvard University.F.
Jelinek, 1997.
Information Extraction From SpeechAnd Text.
MIT Press.D.
McDonald, 1996.
Corpus Processing for LexicalAquisition, chapter Internal and External Evidencein the Identification and Semantic Categorization ofProper Names, pages 21?39.
MIT Press.S.
Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwarz,R.
Stone, and R. Weischedel.
1998.
Bbn: Descriptionof the SIFT system as used for MUC-7.
In MUC-7.M.
Mu?oz, V. Punyakanok, D. Roth, and D. Zimak.1999.
A learning approach to shallow parsing.
Tech-nical Report 2087, Urbana, Illinois.G.
Ngai and R. Florian.
2001.
Transformation-basedlearning in the fast lane.
In Proceedings of NAACL?01,pages 40?47.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart of speech tagging.
In Proceedings EMNLP?96,Philadelphia.E.
F. Tjong Kim Sang and J. Veenstra.
1999.
Represent-ing text chunks.
In Proceedings of EACL?99.G.
D. Zhou and J. Su.
2002.
Named entity recognitionusing a HMM-based chunk tagger.
In Proceedings ofACL?02.
