Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1308?1318,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSemantic Parsing with Relaxed Hybrid TreesWei LuInformation Systems Technology and DesignSingapore University of Technology and Designluwei@sutd.edu.sgAbstractWe propose a novel model for parsingnatural language sentences into their for-mal semantic representations.
The modelis able to perform integrated lexicon ac-quisition and semantic parsing, mappingeach atomic element in a complete seman-tic representation to a contiguous wordsequence in the input sentence in a re-cursive manner, where certain overlap-pings amongst such word sequences areallowed.
It defines distributions over thenovel relaxed hybrid tree structures whichjointly represent both sentences and se-mantics.
Such structures allow tractabledynamic programming algorithms to bedeveloped for efficient learning and decod-ing.
Trained under a discriminative set-ting, our model is able to incorporate a richset of features where certain unboundedlong-distance dependencies can be cap-tured in a principled manner.
We demon-strate through experiments that by exploit-ing a large collection of simple features,our model is shown to be competitive toprevious works and achieves state-of-the-art performance on standard benchmarkdata across four different languages.
Thesystem and code can be downloaded fromhttp://statnlp.org/research/sp/.1 IntroductionSemantic parsing, the task of transforming natu-ral language sentences into formal representationsof their underlying semantics, is one of the clas-sic goals for natural language processing and ar-tificial intelligence.
This area of research recentlyhas received a significant amount of attention.
Var-ious models have been proposed over the past fewyears (Zettlemoyer and Collins, 2005; Kate andQUERY : answer(RIVER)RIVER : exclude(RIVER, RIVER)RIVER : traverse(STATE)STATE : stateid(STATENAME)STATENAME : (?tn?
)RIVER : river(all)What rivers do not run through Tennessee ?Figure 1: An example tree-structured semanticrepresentation (above) and its corresponding nat-ural language sentence.Mooney, 2006; Wong and Mooney, 2006; Lu etal., 2008; Jones et al., 2012).Following previous research efforts, we performsemantic parsing under a setting where the seman-tics for complete sentences are provided as train-ing data, but detailed word-level semantic infor-mation is not explicitly given during the trainingphase.
As one example, consider the followingnatural language sentence paired with its corre-sponding semantic representation:What rivers do not run through Tennessee ?answer(exclude(river(all), traverse(stateid(?tn?
))))The training data consists of a set of sentencespaired with semantic representations.
Our goal isto learn from such pairs a model, which can beeffectively used for parsing novel sentences intotheir semantic representations.Certain assumptions about the semantics aretypically made.
One common assumption is thatthe semantics can be represented as certain re-cursive structures such as trees, which consist ofatomic semantic units as tree nodes.
For exam-ple, the above semantics can be converted into anequivalent tree structure as illustrated in Figure 1.We will provide more details about such tree struc-tured semantic representations in Section 2.1.1308Currently, most state-of-the-art approaches thatdeal with such tree structured semantic represen-tations either cast the semantic parsing problem asa statistical string-to-string transformation prob-lem (Wong and Mooney, 2006), which ignoresthe potentially useful structural information of thetree, or employ latent-variable models to cap-ture the correspondences between words and treenodes using a generative approach (Lu et al., 2008;Jones et al., 2012).
While generative models canbe used to flexibly model the correspondences be-tween individual words and semantic nodes of thetree, such an approach is limited to modeling localdependencies and is unable to flexibly incorporatea large set of potentially useful features.In this work, we propose a novel model forparsing natural language into tree structured se-mantic representations.
Specifically, we proposea novel relaxed hybrid tree representation whichjointly encodes both natural language sentencesand semantics; such representations can be effec-tively learned with a latent-variable discriminativemodel where long-distance dependencies can becaptured.
We present dynamic programming al-gorithms for efficient learning and decoding.
Witha large collection of simple features, our modelreports state-of-the-art results on benchmark dataannotated with four different languages.Furthermore, although we focus our discussionson semantic parsing in this work, our proposedmodel is a general.
Essentially our model is a dis-criminative string-to-tree model which recursivelymaps overlapping contiguous word sequences totree nodes at different levels, where efficient dy-namic programming algorithms can be used.
Sucha model may find applications in other areas ofnatural language processing, such as statisticalmachine translation and information extraction.2 Background2.1 SemanticsVarious semantic formalisms have been consid-ered for semantic parsing.
Examples include thetree-structured semantic representations (Wongand Mooney, 2006), the lambda calculus expres-sions (Zettlemoyer and Collins, 2005; Wong andMooney, 2007), and dependency-based compo-sitional semantic representations (Liang et al.,2013).
In this work, we specifically focus on thetree-structured representations for semantics.Each semantic representation consists of se-mantic units as its tree nodes, where each semanticunit is of the following form:ma?
?a: p?(?b?)
(1)Here mais used to denote a complete seman-tic unit, which consists of its semantic type ?a, itsfunction symbol p?, as well as an argument list ?b?
(we assume there are at most two arguments foreach semantic unit).
In other words, each seman-tic unit can be regarded as a function which takesin other semantics of specific types as arguments,and returns new semantics of a particular type.
Forexample, in Figure 1, the semantic unit at the roothas a type QUERY, a function name answer, anda single argument type RIVER.2.2 Joint RepresentationsSemantic parsing models transform sentences intotheir corresponding semantics.
It is therefore es-sential to make proper assumptions about jointrepresentations for language and semantics thatcapture how individual words and atomic seman-tic units connect to each other.
Typically, differ-ent existing models employ different assumptionsfor establishing such connections, leading to verydifferent definitions of joint representations.
Wesurvey in this section various representations pro-posed by previous works.The WASP semantic parser (Wong and Mooney,2006) essentially casts the semantic parsing prob-lem as a string-to-string transformation problemby employing a statistical phrase-based machinetranslation approach with synchronous gram-mars (Chiang, 2007).
Therefore, one can think ofthe joint representation for both language and se-mantics as a synchronous derivation tree consist-ing of those derivation steps for transforming sen-tences into target semantic representation strings.While this joint representation is flexible, allow-ing blocks of semantic structures to map to wordsequences, it does not fully exploit the structuralinformation (tree) as conveyed by the semantics.The KRISP semantic parser (Kate and Mooney,2006) makes use of Support Vector Machines withstring kernels (Lodhi et al., 2002) to recursivelymap contiguous word sequences into semanticunits to construct a tree structure.
Our relaxedhybrid tree structures also allow input word se-quences to map to semantic units in a recursivemanner.
One key distinction, as we will see, is thatour structure distinguishes words which are imme-1309diately associated with a particular semantic unit,from words which are remotely associated.The SCISSOR model (Ge and Mooney, 2005)performs integrated semantic and syntactic pars-ing.
The model parses natural language sentencesinto semantically augmented parse trees whosenodes consist of both semantic and syntactic labelsand then builds semantic representations based onsuch augmented trees.
Such a joint representationconveys more information, but requires language-specific syntactic analysis.The hybrid tree model (Lu et al., 2008) is basedon the assumption that there exists an underlyinggenerative process which jointly produces both thesentence and the semantic tree in a top-down re-cursive manner.
The generative process results ina hybrid tree structure which consists of words asleaves and semantic units as nodes.
An examplehybrid tree structure is shown in Figure 2 (a).
Sucha representation allows each semantic unit to mapto a possibly discontiguous sequence of words.The model was shown to be effective empirically,but it implicitly assumes that both the sentence andsemantics exhibit certain degree of structural sim-ilarity that allows the hybrid tree structures to beconstructed.UBL (Kwiatkowski et al., 2010) is a semanticparser based on restricted higher-order unificationwith CCG (Steedman, 1996).
The model can beused to handle both tree structured semantic rep-resentations and lambda calculus expressions, andassumes there exist CCG derivations as joint rep-resentations in which each semantic unit is associ-ated with a contiguous word sequence where over-lappings amongst word sequences are not allowed.Jones et al.
(2012) recently proposed a frame-work that performs semantic parsing with treetransducers.
The model learns representations thatare similar to the hybrid tree structures using agenerative process under a Bayesian setting.
Thus,their representations also potentially present simi-lar issues as the ones mentioned above.Besides these supervised approaches, recentlythere are also several works that take alternativelearning approaches to (mostly task-dependent)semantic parsing.
Poon and Domingos (2009) pro-posed a model for unsupervised semantic pars-ing that transforms dependency trees into seman-tic representations using Markov logic (Richard-son and Domingos, 2006).
Clarke et al.
(2010)proposed a model that learns a semantic parserSymbol Descriptionn A complete natural language sentencem A complete semantic representationh A complete latent joint representation (or, arelaxed hybrid tree for our work)H(n,m) A complete set of latent joint representationsthat contain the (n,m) pair exactlyn, naA contiguous sequence of wordsw, wkA natural language wordm,maA semantic unith, haA node in the relaxed hybrid tree?
The feature vector?kThe k-th feature?
The weight vector (model parameters)?kThe weight for the k-th feature ?kTable 1: Notation Tablefor answering questions without relying on seman-tic annotations.
Goldwasser et al.
(2011) tookan unsupervised approach for semantic parsingbased on self-training driven by confidence esti-mation.
Liang et al.
(2013) proposed a model forlearning the dependency-based compositional se-mantics (DCS) which can be used for optimiz-ing the end-task performance.
Artzi and Zettle-moyer (2013) proposed a model for mapping in-structions to actions with weak supervision.3 ApproachWe discuss our approach to semantic parsing inthis section.
The notation that we use in this paperis summarized in Table 1.3.1 ModelIn standard supervised syntactic parsing, one typi-cally has access to a complete syntactic parse treefor each sentence in the training phase, which ex-actly tells the correct associations between wordsand syntactic labels.
In our problem, however,each sentence is only paired with a complete se-mantic representation where the correct associa-tions between words and semantic units are un-available.
We thus need to model such informationwith latent variables.For a given n-m pair (where n is a completenatural language sentence, and m is a completesemantic representation), we assume there exists alatent joint representation h that consists of both nandmwhich tells the correct associations betweenwords and semantic units in such a pair.
We useH(n,m)1to denote the set of all such possible1We will give a concrete definition of H(n,m) used forthis work, which is the complete set of all possible relaxedhybrid tree structures for the n-m pair, when we discuss ourown joint representations later in Section 3.2.1310mambw10mdw9w8mcw6w7w4w5w1w2w3(a)ma(w1w2w3) w4w5w6w7w8w9w10mb(w4w5) w6w7(w8) w9(w10)md(w9)mc(w6w7)(b)Figure 2: Two different ways of jointly representing sentences and their semantics.
The hybrid treerepresentation of Lu et al.
(2008) (left), and our novel relaxed hybrid tree representation (right).
Inour representation, a word w can be either immediately associated with its parent m (the words whichappear inside the parenthesis), or remotely associated with m (the words that do not appear inside theparenthesis, and will also appear under a subtree rooted by one of m?s children).latent joint representations that contain both n andm exactly.Given the joint representations, to model howthe data is generated, one can either take a gener-ative approach which models the joint probabilitydistribution over (n,m,h) tuples, or a discrimina-tive approach which models the distribution over(m,h) tuples given the observation n. Followingseveral previous research efforts (Zettlemoyer andCollins, 2005; Kwiatkowski et al., 2010; Liang etal., 2013), in this work we define a discriminativemodel using a log-linear approach:P (m,h|n; ?)
=e???(n,m,h)?m?,h??H(n,m?)e???(n,m?,h?
)(2)Here ?
(n,m,h) is a function defined over thetuple (n,m,h) that returns a vector consisting ofcounts of features associated with the tuple, and ?is a vector consisting of feature weights, which arethe parameters of the model.In practice, we are only given the n-m pairs butthe latent structures are not observed.
We there-fore consider the following marginal probability:P (m|n; ?)
=?h?H(n,m)P (m,h|n; ?)=?h?H(n,m)e???(n,m,h)?m?,h??H(n,m?)e???(n,m?,h?
)(3)The above probability is defined for a particularn-m pair.
The complete log-likelihood objectivefor the training set is:L(?)
=?ilogP (mi|ni; ?)?
?||?||2=?ilog?h?H(ni,mi)P (mi,h|ni; ?)?
?||?||2(4)where (ni,mi) refers to the i-th instance in thetraining set.
Note that here we introduce the ad-ditional regularization term ??
?
||?||2to controlover-fitting, where ?
is a positive scalar.Our goal is to maximize this objective functionby tuning the model parameters ?.
Let?s assume?
= ?
?1, ?2, .
.
.
, ?N?, where N is the total num-ber of features (or the total number of parameters).Differentiating with respect to ?k, the weight as-sociated with the k-th feature ?k, yields:?L(?)?
?k=?i?hEP (h|ni,mi;?)[?k(ni,mi,h)]?
?i?m,hEP (m,h|ni;?)[?k(ni,m,h)]?
2?
?k(5)where ?k(n,m,h) refers to the number of occur-rences for the k-th feature in the tuple (n,m,h).Given the objective value (4) and gradients (5),standard methods such as stochastic gradient de-scent or L-BFGS (Liu and Nocedal, 1989) can beemployed to optimize the objective function.
Wewill discuss the computation of the objective func-tion and gradients next.3.2 Relaxed Hybrid TreesTo allow tractable computation of the values forthe objective function (4) and the gradients (5),1311. .
.NUM : count(STATE)STATE : state(STATE)statesSTATE : next to(CITY)borders how many.
.
.(a).
.
.NUM : count(STATE).
.
.
borders how many states (.
.
.
)STATE : state(STATE).
.
.
borders how many (states)STATE : next to(CITY).
.
.
(borders how many).
.
.
(b)Figure 3: An example hybrid tree and an example relaxed hybrid tree representation.
When the correctlatent structure can not be found, the dependency between the words ?how many?
and the semantic unit?NUM : count(STATE)?
can not be captured if the hybrid tree is used, whereas with our relaxed hybridtree representation, such a dependency can still be captured.certain restrictions on the latent structures (h) willneed to be imposed.
We define in this section theset of all valid latent structures H(n,m) for the(n,m) pair so that some efficient dynamic pro-gramming algorithms can be deployed.We introduce our novel relaxed hybrid tree rep-resentations which jointly encode both natural lan-guage sentences and the tree-structured semantics.A relaxed hybrid tree h defined over (n,m) is atree whose nodes are (n,m) pairs, where each n isa contiguous sequence of words from n, and eachm is a semantic unit (a tree node) from m. Forany two nodes ha?
(na,ma) and hb?
(nb,mb)that appear in the relaxed hybrid tree h, if hais theparent of hbin h, then mamust also be the parentof mbin m, and namust contain nb.
If the low-est common ancestor of haand hbin h is neitherhanor hb, then naand nbdo not share any com-mon word.
Note that words that appear at differentpositions in n are regarded as different words, re-gardless of their string forms.Figure 2 (b) shows an example relaxed hybridtree structure that we consider.
Assume we wouldlike to jointly represent both the natural languagesentence n ?
w1w2.
.
.
w10and its correspondingsemantic representation m ?
ma(mb(mc,md)).In the given example, the semantic unit mamapsto the complete sentence,mbmaps to the sequencew4w5.
.
.
w10, mcmaps to w6w7, and mdmaps tow9.
Certain words such as w4and w10that ap-pear directly below the semantic unit mbbut donot map to any of mb?s child semantic units arehighlighted with parentheses ?
()?, indicating theyare immediately associated with mb.
These wordsplay unique roles in the sub-tree rooted by mbandare expected to be semantically closely related tomb.
Note that each word is immediately associ-ated with exactly one semantic unit.As a comparison, we also show an example hy-brid tree representation (Lu et al., 2008) in Fig-ure 2 (a) that has similar words-semantics cor-respondences.
Different from our representation,the hybrid tree representation assumes each natu-ral language word only maps to a single seman-tic unit (which is its immediate parent), and eachsemantic unit maps to a possibly discontiguoussequence of words.
We believe that such a rep-resentation is overly restrictive, which might ex-hibit problems in cases where natural languagesentences are highly non-isomorphic to their se-mantic tree structures.
Under our relaxed hybridtree representations, words that are immediatelyassociated with a particular semantic unit now canalso be remotely associated with all its parent se-mantic units as well.
Essentially, our representa-tion allows us to capture certain unbounded depen-dencies ?
for any word, as long as it appears be-low a certain semantic unit (in the relaxed hybridtree), we can always capture the dependency be-tween the two, regardless of which actual seman-tic unit that word is immediately associated with.Such an important relaxation allows some long-distance dependencies to be captured, which canpotentially alleviate the sentence-semantics non-isomorphism issue reported in several earlier se-mantic parsing works (Kate and Mooney, 2006;1312Wong and Mooney, 2007).To better illustrate the differences, we show aconcrete example in Figure 3, where the correctlatent structure showing the correspondences be-tween words and semantic units can not be foundwith the hybrid tree model.
As a result, the hy-brid tree model will fail to capture the correct de-pendency between the words ?how many?
and thesemantic unit ?NUM : count(STATE)?.
On theother hand, with our relaxed hybrid tree represen-tation, such a dependency can still be captured,since these words will still be (remotely) associ-ated with the semantic unit.Such a relaxed hybrid tree representation, whenfurther constrained with the word association pat-terns that we will introduce next, allows both theobjective function (4) and the gradients of (5) tobe computed through the dynamic programmingalgorithms to be presented in Section 4.3.3 Word Association PatternsAs we have mentioned above, in the relaxed hy-brid tree structures, each word w under a certainsemantic unit m can either appear directly belowm only (immediately associated with m), or canalso appear in a subtree rooted by one ofm?s childsemantic unit (remotely associated with m).We allow several different ways for word asso-ciations and define the allowable patterns for se-mantic units with different number of argumentsin Table 2.
Such patterns are defined so that ourmodel is amendable to dynamic programming al-gorithms to be discussed in Sec 4.
In this table,w refers to a contiguous sequence of natural lan-guage words that are immediately associated withthe current semantic unit, while X and Y refersto a sequence of natural language words that thefirst and second child semantic unit will map to,respectively.For example, in Figure 2 (b), the word sequencedirectly below the semantic unit mafollows thepattern wX (since the word sequence w1w2w3isimmediately associated with ma, and the remain-ing words are remotely associated with ma), andthe word sequence below mbfollows wXwYw2.The word association patterns are similar tothose hybrid patterns used in hybrid trees.
Onekey difference is that we disallow the unary pat-2This is based on the assumption that mcand mdare thefirst and second child of mbin the semantic representation,respectively.
If mdis the first child in the semantic represen-tation andmcis the second, the pattern should be wYwXw.#Args Word Association Patterns0 w1 wX, Xw, wXw2XY, YX, wXY, wYX, XwY, YwXXYw, YXw, wXwY, wYwXwXYw, wYXw, XwYw, YwXwwXwYw, wYwXwTable 2: The complete list of word association pat-terns.
Here #Args means the number of argumentsfor a semantic unit.tern X.
The reason is, when computing the parti-tion function in Equation 3, inclusion of pattern Xwill result in relaxed hybrid trees consisting of aninfinite number of nodes.
However, this issue doesnot come up in the original hybrid tree models dueto their generative setting, where the training pro-cess does not involve such a partition function.3.4 FeaturesThe features are defined over the (n,m,h) tuples.In practice, we define features at each level of therelaxed hybrid tree structure h. In other words,features are defined over (n,m) tuples where n isa contiguous sequence of natural language words(immediately or remotely) associated with the se-mantic unit m (recall that h contains both n andm, and each level of h simply consists of a seman-tic unit and a contiguous sequence of words).
Eachfeature over (n,m) is then further decomposedas a product between two indicator feature func-tions, defined over the natural language words (n)and semantic unit (m) respectively: ?
(n,m) =?i(n) ?
?o(m).
For each ?i(n) we define twotypes of features: the local features, which are de-fined over immediately associated words only, andthe span features, which are defined over all (im-mediately or remotely) associated words to cap-ture long range dependencies.The local features include word unigrams andbigrams, the word association patterns, as well ascharacter-level features3which perform implicitmorphological analysis.
The span features includeword unigrams, bigrams, as well as trigrams.
Al-though our model allows certain more sophisti-cated features to be exploited, such as word POSfeatures, word similarity features based on theWordNet (Pedersen et al., 2004), we deliberatelychoose to only include these simple features so3For each word, we used all its prefixes (not necessarilylinguistically meaningful ones) whose length are greater than2 as features, for all languages.1313as to make a fair comparison with previous workswhich also did not make use of external resources.For the features defined on m (i.e., ?o(m)), we in-clude only the string form of m, as well as m?sfunction name as features.Finally, we also define features over m only.Such features are defined over semantic unit pairssuch as (ma,mb) where mais the parent node ofmbas in m. They include: 1) concatenation of thestring forms ofmaandmb, 2) concatenation of thestring form of maand mb?s type, and 3) concate-nation of the function names of maand mb.4 AlgorithmsIn this section we describe the efficient algorithmsused for learning and decoding.
The algorithmsare inspired by the inside-outside style algorithmsused for the generative hybrid tree models (Luet al., 2008), but are different in the followingways: 1) we need to handle features, includinglong-distance features, 2) we need to additionallyhandle the computation of the partition function ofEquation (3).4.1 LearningThe training process involves the computation ofthe objective function (4) as well as the gradientterms (5).The objective function (4) (excluding the regu-larization term which can be trivially computed) isequivalent to the following:L(?)
=?ilog?h?H(ni,mi)e???(ni,mi,h)??ilog?m?,h??H(ni,m?)e???(ni,m?,h?
)(6)In the first term,?h?H(ni,mi)e???
(ni,mi,h)isin fact the sum of the scores (as defined by ?
and?)
associated with all such latent structures thatcontain both miand niexactly.
The second termis the sum of the scores associated with all the la-tent structures that contain niexactly.
We focusour discussions on the computation of the first partfirst.We usem(p)wi.
.
.
wjto denote the combined scoreof all such latent relaxed hybrid tree structures thatcontain both the semantic tree rooted bym and thenatural language word sequence wi.
.
.
wjthatforms the word association pattern p with respectto m. For example, the score of the relaxed hybridtree in Figure 2 (b) is contained byma(wX)w1.
.
.
w10(here p = wX because only w1w2w3are imme-diately associated with ma).We give an illustrative example that shows howthese scores can be computed efficiently using dy-namic programming.
Consider the following casewhen m has at least one child semantic unit:m(wXw)wi.
.
.
wj=m(w)wi?m(wXw)wi+1.
.
.
wj+m(w)wi?m(Xw)wi+1.
.
.
wjHere the symbol ?
means extract and compute,a process that involves 1) extraction of additionalfeatures when the two structures on the right-handside are put together (for example, the local bi-gram feature ?wiwi+1?
can be extracted in theabove case), and 2) computation of the score forthe new structure when the two structures fromboth sides of ?
are combined, based on the scoresof these structures and newly extracted features.The above equation holds because for any re-laxed hybrid tree contained by the left-hand side,the left-most word wiis always immediately as-sociated with m. The termm(wXw)wi+1.
.
.
wjis present-ing a similar but smaller structure to the term onthe left-hand side.
The other termm(wX)wi.
.
.
wjcanalso be computed based on similar equations.
Inother words, such terms can be computed fromeven smaller similar terms in a recursive manner.A bottom-up dynamic programming algorithm isused for computing such terms.When the semantic unit m has two child nodes,similar equations can also be established.
Here wegive an illustrative example:m(wXwYw)wi.
.
.
wj=?j?1k=im(wX)wi.
.
.
wk?m(wYw)wk+1.
.
.
wjFinally, we have the following equation:mwi.
.
.
wj=?pm(p)wi.
.
.
wjThe left-hand side simply means the combinedscore for all such relaxed hybrid trees that have(n,m) as the root, where n ?
wi.
.
.
wj.
Once thecomputation for a certain (n,m) pair is done, we1314can move up to process such pairs that involvem?sparent node.The above process essentially computes the in-side score associated with the (n,m) pair, whichgives the sum of the scores of all such (incomplete)relaxed hybrid trees that can be constructed with(n,m) as the root.
Similar to (Lu et al., 2008), wecan also define and compute the outside scores for(n,m) (the combined score of such incomplete re-laxed hybrid trees that contain (n,m) as one of itsleave nodes) in an analogous manner, where thecomputation of the gradient functions can be effi-ciently integrated in this process.Computation of the second part of the objectivefunction (6) involves dynamic programming overa packed forest representation rather than a singletree, which requires an extension to the algorithmdescribed in (Lu et al., 2008).
The resulting al-gorithm is similar to the one used in (Lu and Ng,2011), which has been used for language gener-ation from packed forest representations of typed?-calculus expressions.4.2 DecodingThe decoding phase involves finding the optimalsemantic tree m?given a new input sentence n:m?= arg maxmP (m|n) (7)This in fact is equivalent to finding the follow-ing optimal semantic tree m?
:m?= arg maxm?h?H(n,m)e???
(n,m,h)(8)Unfortunately, the summation operation insidethe arg max prevents us from employing a simi-lar version of the dynamic programming algorithmwe developed for learning in Section 4.1.
To over-come this difficulty, we instead find the optimalsemantic tree using the following equation:m?= arg maxm,h?H(n,m)e???
(n,m,h)(9)We essentially replace the?operation by themax operation inside the arg max.
In other words,we first find the best latent relaxed hybrid tree h?that contains the input sentence n, and next we ex-tract the optimal semantic tree m?from h?.This decoding algorithm is similar to the dy-namic programming algorithm used for comput-ing the inside score for a given natural languagesentence n (i.e., the algorithm for computing thesecond term of Equation (6)).
The difference hereis, at each intermediate step, instead of computingthe combined score for all possible relaxed hybridtree structures (i.e., performing sum), we find thesingle-best relaxed hybrid tree structure (i.e., per-forming max).5 ExperimentsWe present evaluations on the standard GeoQuerydataset which is publicly available.
This datasethas been used for evaluations in various seman-tic parsing works (Wong and Mooney, 2006; Kateand Mooney, 2006; Lu et al., 2008; Jones et al.,2012).
It consists of 880 natural language sen-tences paired with their corresponding formal se-mantic representations.
Each semantic represen-tation is a tree structured representation derivedfrom a Prolog query that can be used to interactwith a database of U.S. geography facts for retriev-ing answers.
The original dataset was fully anno-tated in English, and recently Jones et al.
(2012)released a new version of this dataset with threeadditional language annotations (German, Greekand Thai).
For all the experiments, we used theidentical experimental setup as described in Joneset al.
(2012).
Specifically, we trained on 600 in-stances, and evaluated on the remaining 280.We note that there exist two different versions ofthe GeoQuery dataset annotated with completelydifferent semantic representations.
Besides theversion that we use in this work, which is an-notated with tree structured semantic representa-tions, the other version is annotated with lambdacalculus expressions (Zettlemoyer and Collins,2005).
Results obtained from these two versionsare not comparable.4Like many previous works,we focus on tree structured semantic representa-tions for evaluations in this work since our modelis designed for handling the class of semantic rep-resentations with recursive tree structures.We used the standard evaluation criteria forjudging the correctness of the outputs.
Specifi-cally, our system constructs Prolog queries fromthe output parses, and uses such queries to retrieveanswers from the GeoQuery database.
An outputis considered correct if and only if it retrieves the4Kwiatkowski et al.
(2010) showed in Table 3 of theirwork that the version with tree-structured representations ap-peared to be more challenging ?
their semantic parser?s per-formance on this version was substantially lower than that onthe lambda calculus version.1315SystemEnglish Thai German GreekAcc.
F1 Acc.
F1 Acc.
F1 Acc.
F1WASP 71.1 77.7 71.4 75.0 65.7 74.9 70.7 78.6HYBRIDTREE+ 76.8 81.0 73.6 76.7 62.1 68.5 69.3 74.6UBL-S 82.1 82.1 66.4 66.4 75.0 75.0 73.6 73.7TREETRANS 79.3 79.3 78.2 78.2 74.6 74.6 75.4 75.4RHT (all features) 83.6 83.6 79.3 79.3 74.3 74.3 78.2 78.2Table 3: Performance on the benchmark data, using four different languages as inputs.
RHT: relaxedhybrid tree (this work).same answers as the gold standard (Jones et al.,2012).
We report accuracy scores ?
the percentageof inputs with correct answers, and F1 measures ?the harmonic mean of precision (the proportion ofcorrect answers out of inputs with an answer) andrecall (the proportion of correct answers out of allinputs).
By adopting such an evaluation methodwe will be able to directly compare our model?sperformance against those of the previous works.The evaluations were conducted under such asetting in order to make comparisons to previousworks.
We would like to stress that our modelis designed for general-purpose semantic parsingthat is not only natural language-independent, butalso task-independent.
We thus distinguish ourwork from several previous works in the literaturewhich focused on semantic parsing under other as-sumptions.
Specifically, for example, works suchas (Liang et al., 2013; Poon and Domingos, 2009;Clarke et al., 2010) essentially performed seman-tic parsing under different settings where the goalwas to optimize the performance of certain down-stream NLP tasks such as answering questions,and different semantic formalisms and language-specific features were usually involved.For all our experiments, we used the L-BFGSalgorithm for learning the feature weights, wherefeature weights were all initialized to zeros and theregularization hyper-parameter ?
was set to 0.01.We set the maximum number of L-BFGS steps to100.
When all the features are considered, ourmodel creates over 2 million features for each lan-guage on the dataset (English: 2.1M, Thai: 2.3M,German: 2.7M, Greek: 2.6M).
Our model re-quires (on average) a per-instance learning time of0.428 seconds and a per-instance decoding time of0.235 seconds, on an Intel machine with a 2.2 GHzCPU.
Our implementation is in Java.
Here theper-instance learning time refers to the time spenton computing the instance-level log-likelihood aswell as the expected feature counts (needed for thegradients).Table 3 shows the evaluation results of our sys-tem as well as those of several other comparableprevious works which share the same experimen-tal setup as ours.
UBL-S is the system presentedin Kwiatkowski et al.
(2010) which performs se-mantic parsing with the CCG based on mappingbetween graphs, and is the only non-tree basedtop-performing system.
Their system, similar toours, also uses a discriminative log-linear modelwhere two types of features are defined.
WASP isa model based on statistical phrase-based machinetranslation as we have described earlier.
The hy-brid tree model (HYBRIDTREE+) performs learn-ing using a generative process which is augmentedwith an additional discriminative-reranking stage,where certain global features are incorporated (Luet al., 2008).
The Bayesian tree transducer model(TREETRANS) learns under a Bayesian genera-tive framework, using hyper-parameters manuallytuned on the German training data.We can observe from Table 3 that the semanticparser based on relaxed hybrid tree gives compet-itive performance when all the features (describedin Sec 3.4) are used.
It significantly outperformsthe hybrid tree model that is augmented with a dis-criminative reranking step.
The model reports thebest accuracy and F1 scores on English and Thaiand best accuracy score on Greek.
The scoreson German are lower than those of UBL-S andTREETRANS, mainly because the span featuresappear not to be effective for this language, as wewill discuss next.We report in Table 4 the test set performancewhen certain types of features are excluded fromour system.
Such results can help us understandthe effectiveness of features of different types.
Aswe can see from the table, in general, all fea-tures play essential roles, though their effective-1316SystemEnglish Thai German GreekAcc.
F1 Acc.
F1 Acc.
F1 Acc.
F1RHT (all features) 83.6 83.6 79.3 79.3 74.3 74.3 78.2 78.2RHT (no local features) 81.4 81.4 78.2 78.2 74.3 74.3 75.7 75.7RHT (no span features) 81.1 81.1 77.9 77.9 78.2 78.2 78.9 78.9RHT (no char features) 79.6 79.6 82.1 82.1 73.6 73.6 76.1 76.1Table 4: Results when certain types of features (local features, span features and character-level features)are excluded.ness vary across different languages.
The localfeatures, which capture local dependencies, areof particular importance.
Performance on threelanguages (English, Thai, and Greek) will dropwhen such features are excluded.
Character-levelfeatures are very helpful for the three Europeanlanguages (English, German, and Greek), but ap-pear to be harmful for Thai.
This indicates thecharacter-level features that we propose do notperform effective morphological analysis for thisAsian language.5The span features, which areable to capture certain long-distance dependen-cies, also play important roles.
Specifically, ifsuch features are excluded, our model?s perfor-mance on three languages (Greek, English, Thai)will drop.
Such features do not appear to be help-ful for Thai and appear to be harmful for Ger-man.
Clearly, such long-distance features are notcontributing useful information to the model whenthese two languages are considered.
This is espe-cially the case for German, where we believe suchfeatures are contributing substantial noisy infor-mation to the model.
What underlying language-specific, syntactic properties are generally caus-ing these gaps in the performances?
We believethis is an important question that needs to be ad-dressed in future research.
As we have mentioned,to make an appropriate comparison with previ-ous works, only simple features are used.
We be-lieve that our system?s performance can be furtherimproved when additional informative language-specific features can be extracted from effectivelanguage tools and incorporated into our system.6 ConclusionsIn this work, we present a new discriminativemodel for semantic parsing which extends the hy-5The character-level features that we introduced are in-deed very general.
We have conducted several additional ex-periments, which show that our model?s performance for eachlanguage can be further improved when certain language-specific character-level features are introduced.brid tree model.
Such an extension is similar to theextension of the generative syntactic parser basedon probabilistic context-free grammars (PCFG) tothe feature-based CRF parser (Finkel et al., 2008),but is slightly more complex due to latent struc-tures.
Developed on top of our novel relaxed hy-brid tree representations, our model allows cer-tain long-distance dependencies to be captured.We also present efficient algorithms for learn-ing and decoding.
Experiments on benchmarkdata show that our model is competitive to previ-ous works and achieves the state-of-the-art perfor-mance across several different languages.Future works include development of efficientalgorithms for feature-based semantic parsingwith alternative loss functions (Zhou et al., 2013),development of feature-based language generationmodels (Lu et al., 2009; Lu and Ng, 2011) andmultilingual semantic parsers (Jie and Lu, 2014),as well as the development of efficient semanticparsing algorithms for optimizing the performanceof certain downstream NLP tasks with less super-vision (Clarke et al., 2010; Liang et al., 2013).Being able to efficiently exploit features definedover individual words, our model also opens up thepossibility for us to exploit alternative representa-tions of words for learning (Turian et al., 2010), orto perform joint learning of both distributional andlogical semantics (Lewis and Steedman, 2013).Furthermore, as a general string-to-tree structuredprediction model, this work may find applicationsin other areas within NLP.The system and code can be downloaded fromhttp://statnlp.org/research/sp/.AcknoledgmentsThe author would like to thank the anonymous re-viewers for their helpful comments.
This workwas supported by SUTD grant SRG ISTD 2013064.1317ReferencesYoav Artzi and Luke Zettlemoyer.
2013.
Weakly su-pervised learning of semantic parsers for mappinginstructions to actions.
Transactions of the Associa-tion for Computational Linguistics, 1(1):49?62.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Comput.
Linguist., 33(2):201?228, June.James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing fromthe world?s response.
In Proc.
of CONLL ?10, pages18?27.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, condi-tional random field parsing.
In Proc.
of ACL/HLT,pages 959?967.Ruifang Ge and Raymond J. Mooney.
2005.
A statis-tical semantic parser that integrates syntax and se-mantics.
In Proc.
of CONLL ?05, pages 9?16.Dan Goldwasser, Roi Reichart, James Clarke, and DanRoth.
2011.
Confidence driven unsupervised se-mantic parsing.
In Proc.
of ACL ?11, pages 1486?1495.Zhanming Jie and Wei Lu.
2014.
Multilingual se-mantic parsing: Parsing multiple languages into se-mantic representations.
In Proc.
of COLING, pages1291?1301.Bevan Keeley Jones, Mark Johnson, and Sharon Gold-water.
2012.
Semantic parsing with bayesian treetransducers.
In Proc.
of ACL ?12, pages 488?496.Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
InProc.
of COLING/ACL, pages 913?920.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing proba-bilistic ccg grammars from logical form with higher-order unification.
In Proc.
EMNLP?10, pages 1223?1233.Mike Lewis and Mark Steedman.
2013.
Combineddistributional and logical semantics.
Transactionsof the Association for Computational Linguistics,1:179?192.Percy Liang, Michael I Jordan, and Dan Klein.
2013.Learning dependency-based compositional seman-tics.
Computational Linguistics, 39(2):389?446.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory bfgs method for large scale optimization.
Math.Program., 45(3):503?528, December.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, and Chris Watkins.
2002.
Textclassification using string kernels.
The Journal ofMachine Learning Research, 2:419?444.Wei Lu and Hwee Tou Ng.
2011.
A probabilis-tic forest-to-string model for language generationfrom typed lambda calculus expressions.
In Proc.of EMNLP ?11, pages 1611?1622.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.Zettlemoyer.
2008.
A generative model for pars-ing natural language to meaning representations.
InProc.
of EMNLP ?08, pages 783?792.Wei Lu, Hwee Tou Ng, and Wee Sun Lee.
2009.
Nat-ural language generation with tree conditional ran-dom fields.
In Proc.
of EMNLP, pages 400?409.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet:: Similarity: measuring the re-latedness of concepts.
In Proc.
of HLT-NAACL ?04(Demonstration), pages 38?41.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proc.
of EMNLP ?09,pages 1?10.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine learning, 62(1-2):107?136.Mark Steedman.
1996.
Surface structure and interpre-tation.
MIT press.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proc.
of ACL ?10,pages 384?394.Yuk Wah Wong and Raymond J. Mooney.
2006.Learning for semantic parsing with statistical ma-chine translation.
In Proc.
of HLT/NAACL ?06,pages 439?446.Yuk Wah Wong and Raymond J Mooney.
2007.Learning synchronous grammars for semantic pars-ing with lambda calculus.
In Proc.
of ACL ?07.Luke S Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proc.
of UAI ?05.Junsheng Zhou, Juhong Xu, and Weiguang Qu.
2013.Efficient latent structural perceptron with hybridtrees for semantic parsing.
In IJCAI, pages 2246?2252.1318
