Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1744?1753,Beijing, China, July 26-31, 2015. c?2015 Association for Computational LinguisticsGated Recursive Neural Network for Chinese Word SegmentationXinchi Chen, Xipeng Qiu?, Chenxi Zhu, Xuanjing HuangShanghai Key Laboratory of Intelligent Information Processing, Fudan UniversitySchool of Computer Science, Fudan University825 Zhangheng Road, Shanghai, China{xinchichen13,xpqiu,czhu13,xjhuang}@fudan.edu.cnAbstractRecently, neural network models for natu-ral language processing tasks have been in-creasingly focused on for their ability of al-leviating the burden of manual feature en-gineering.
However, the previous neuralmodels cannot extract the complicated fea-ture compositions as the traditional meth-ods with discrete features.
In this paper,we propose a gated recursive neural net-work (GRNN) for Chinese word segmen-tation, which contains reset and updategates to incorporate the complicated com-binations of the context characters.
SinceGRNN is relative deep, we also use asupervised layer-wise training method toavoid the problem of gradient diffusion.Experiments on the benchmark datasetsshow that our model outperforms the pre-vious neural network models as well as thestate-of-the-art methods.1 IntroductionUnlike English and other western languages, Chi-nese do not delimit words by white-space.
There-fore, word segmentation is a preliminary and im-portant pre-process for Chinese language process-ing.
Most previous systems address this problemby treating this task as a sequence labeling prob-lem and have achieved great success.
Due to thenature of supervised learning, the performance ofthese models is greatly affected by the design offeatures.
These features are explicitly representedby the different combinations of context charac-ters, which are based on linguistic intuition and sta-tistical information.
However, the number of fea-tures could be so large that the result models aretoo large to use in practice and prone to overfit ontraining corpus.
?Corresponding author.Rainy?
?
Day?
Ground?
?
Accumulated water?
?M E SBFigure 1: Illustration of our model for Chineseword segmentation.
The solid nodes indicate theactive neurons, while the hollow ones indicate thesuppressed neurons.
Specifically, the links denotethe information flow, where the solid edges de-note the acceptation of the combinations while thedashed edges means rejection of that.
As shown inthe right figure, we receive a score vector for tag-ging target character ???
by incorporating all thecombination information.Recently, neural network models have been in-creasingly focused on for their ability to minimizethe effort in feature engineering.
Collobert et al(2011) developed a general neural network archi-tecture for sequence labeling tasks.
Following thiswork, many methods (Zheng et al, 2013; Pei etal., 2014; Qi et al, 2014) applied the neural net-work to Chinese word segmentation and achieveda performance that approaches the state-of-the-artmethods.However, these neural models just concatenatethe embeddings of the context characters, and feedthem into neural network.
Since the concatena-tion operation is relatively simple, it is difficult tomodel the complicated features as the traditionaldiscrete feature based models.
Although the com-plicated interactions of inputs can be modeled bythe deep neural network, the previous neural modelshows that the deep model cannot outperform theone with a single non-linear model.
Therefore, the1744neural model only captures the interactions by thesimple transition matrix and the single non-lineartransformation .
These dense features extracted viathese simple interactions are not nearly as good asthe substantial discrete features in the traditionalmethods.In this paper, we propose a gated recursive neu-ral network (GRNN) to model the complicatedcombinations of characters, and apply it to Chi-nese word segmentation task.
Inspired by the suc-cess of gated recurrent neural network (Chung etal., 2014), we introduce two kinds of gates to con-trol the combinations in recursive structure.
Wealso use the layer-wise training method to avoidthe problem of gradient diffusion, and the dropoutstrategy to avoid the overfitting problem.Figure 1 gives an illustration of how our ap-proach models the complicated combinations ofthe context characters.
Given a sentence ??(Rainy)?
(Day)??
(Ground)??
(Accumu-lated water)?, the target character is ???.
Thissentence is very complicated because each consec-utive two characters can be combined as a word.To predict the label of the target character ???
un-der the given context, GRNN detects the combina-tions recursively from the bottom layer to the top.Then, we receive a score vector of tags by incorpo-rating all the combination information in network.The contributions of this paper can be summa-rized as follows:?
We propose a novel GRNN architecture tomodel the complicated combinations of thecontext characters.
GRNN can select and pre-serve the useful combinations via reset andupdate gates.
These combinations play a sim-ilar role in the feature engineering of the tra-ditional methods with discrete features.?
We evaluate the performance of Chineseword segmentation on PKU, MSRA andCTB6 benchmark datasets which are com-monly used for evaluation of Chinese wordsegmentation.
Experiment results show thatour model outperforms other neural networkmodels, and achieves state-of-the-art perfor-mance.2 Neural Model for Chinese WordSegmentationChinese word segmentation task is usually re-garded as a character-based sequence labelingInput WindowCharacters Ci-2 Ci-1 Ci+1 Ci+2CiLookup Tableggggggggggggggg345261gggd-1dFeaturesLinearW1  ?
?+b1 g g gNumber of Hidden UnitsSigmoidg(?)
g g gNumber of Hidden UnitsLinearW2  ?
?+b2 Number of tagsg g gTag Inferencef(t|1) f(t|2) f(t|i) f(t|n-1) f(t|n)AijConcatenateBEMSFigure 2: General architecture of neural model forChinese word segmentation.problem.
Each character is labeled as one of {B,M, E, S} to indicate the segmentation.
{B, M, E}represent Begin, Middle, End of a multi-charactersegmentation respectively, and S represents a Sin-gle character segmentation.The general neural network architecture for Chi-nese word segmentation task is usually character-ized by three specialized layers: (1) a characterembedding layer; (2) a series of classical neuralnetwork layers and (3) tag inference layer.
A il-lustration is shown in Figure 2.The most common tagging approach is based ona local window.
The window approach assumesthat the tag of a character largely depends on itsneighboring characters.Firstly, we have a character set C of size |C|.Then each character c ?
C is mapped into an d-dimensional embedding space as c ?
Rd by alookup tableM ?
Rd?|C|.For each character ciin a given sentence c1:n,the context characters ci?w1:i+w2are mappedto their corresponding character embeddings asci?w1:i+w2, where w1and w2are left and rightcontext lengths respectively.
Specifically, the un-known characters and characters exceeding the1745sentence boundaries are mapped to special sym-bols, ?unknown?, ?start?
and ?end?
respectively.In addition, w1and w2satisfy the constraint w1+w2+ 1 = w, where w is the window size of themodel.
As an illustration in Figure 2, w1, w2andw are set to 2, 2 and 5 respectively.The embeddings of all the context characters arethen concatenated into a single vector ai?
RH1 asinput of the neural network, where H1= w ?
d isthe size of Layer 1.
And aiis then fed into a con-ventional neural network layer which performs alinear transformation followed by an element-wiseactivation function g, such as tanh.hi= g(W1ai+ b1), (1)where W1?
RH2?H1 , b1?
RH2 , hi?
RH2 .
H2is the number of hidden units in Layer 2.
Here, w,H1andH2are hyper-parameters chosen on devel-opment set.Then, a similar linear transformation is per-formed without non-linear function followed:f(t|ci?w1:i+w2) = W2hi+ b2, (2)where W2?
R|T |?H2 , b2?
R|T | and T is theset of 4 possible tags.
Each dimension of vectorf(t|ci?w1:i+w2) ?
R|T | is the score of the corre-sponding tag.To model the tag dependency, a transition scoreAijis introduced to measure the probability ofjumping from tag i ?
T to tag j ?
T (Collobert etal., 2011).3 Gated Recursive Neural Network forChinese Word SegmentationTo model the complicated feature combinations,we propose a novel gated recursive neural network(GRNN) architecture for Chinese word segmenta-tion task (see Figure 3).3.1 Recursive Neural NetworkA recursive neural network (RNN) is a kind ofdeep neural network created by applying the sameset of weights recursively over a given struc-ture(such as parsing tree) in topological order (Pol-lack, 1990; Socher et al, 2013a).In the simplest case, children nodes are com-bined into their parent node using a weight matrixW that is shared across the whole network, fol-lowed by a non-linear function g(?).
Specifically,if hLand hRare d-dimensional vector representa-tions of left and right children nodes respectively,E M B S??
??
??
??
??
??
??
??
??
??
??
??
????
?????
?ci- 2 ci- 1 ci ci+1 ci+2??
??
??
?
?Linear xiyi =  Ws  ?
xi +  bs ConcatenateyiFigure 3: Architecture of Gated Recursive NeuralNetwork for Chinese word segmentation.their parent node hPwill be a d-dimensional vec-tor as well, calculated as:hP= g(W[hLhR]), (3)where W ?
Rd?2d and g is a non-linear functionas mentioned above.3.2 Gated Recursive Neural NetworkThe RNN need a topological structure to model asequence, such as a syntactic tree.
In this paper, weuse a directed acyclic graph (DAG), as showing inFigure 3, to model the combinations of the inputcharacters, in which two consecutive nodes in thelower layer are combined into a single node in theupper layer via the operation as Eq.
(3).In fact, the DAG structure can model the com-binations of characters by continuously mixing theinformation from the bottom layer to the top layer.Each neuron can be regarded as a complicated fea-ture composition of its governed characters, simi-lar to the discrete feature basedmodels.
The differ-ence between them is that the neural one automat-ically learns the complicated combinations whilethe conventional one need manually design them.1746When the children nodes combine into their parentnode, the combination information of two childrennodes is also merged and preserved by their parentnode.Although the mechanism above seem to workwell, it can not sufficiently model the complicatedcombination features for its simplicity in practice.Inspired by the success of the gated recurrentneural network (Cho et al, 2014b; Chung et al,2014), we propose a gated recursive neural net-work (GRNN) by introducing two kinds of gates,namely ?reset gate?
and ?update gate?.
Specifi-cally, there are two reset gates, rLand rR, par-tially reading the information from left child andright child respectively.
And the update gates zN,zLand zRdecide what to preserve when combin-ing the children?s information.
Intuitively, thesegates seems to decide how to update and exploitthe combination information.In the case of word segmentation, for each char-acter ciof a given sentence c1:n, we first repre-sent each context character cjinto its correspond-ing embedding cj, where i ?
w1?
j ?
i + w2and the definitions of w1and w2are as same asmentioned above.Then, the embeddings are sent to the first layerof GRNN as inputs, whose outputs are recursivelyapplied to upper layers until it outputs a singlefixed-length vector.The outputs of the different neurons can be re-garded as the different feature compositions.
Afterconcatenating the outputs of all neurons in the net-work, we get a new big vector xi.
Next, we receivethe tag score vector yifor character cjby a lineartransformation of xi:yi= Ws?
xi+ bs, (4)where bs?
R|T |, Ws?
R|T |?Q.
Q = q ?
d is di-mensionality of the concatenated vector xi, whereq is the number of nodes in the network.3.3 Gated Recursive UnitGRNNconsists of theminimal structures, gated re-cursive units, as showing in Figure 4.By assuming that the window size is w, we willhave recursion layer l ?
[1, w].
At each recursionlayer l, the activation of the j-th hidden node h(l)j?Rd is computed ash(l)j={zN?
?hlj+ zL?
hl?1j?1+ zR?
hl?1j, l > 1,cj, l = 1,(5)Gate zGate r L Gate rRh j - 1( l - 1 ) h j( l - 1 )h j^ ( l )h j( l )Figure 4: Our proposed gated recursive unit.where zN, zLand zR?
Rd are update gatesfor new activation ?hlj, left child node hl?1j?1andright child node hl?1jrespectively, and?
indicateselement-wise multiplication.The update gates can be formalized as:z =??zNzLzR??=??1/Z1/Z1/Z???
exp(U????hljhl?1j?1hl?1j???
),(6)where U ?
R3d?3d is the coefficient of updategates, and Z ?
Rd is the vector of the normal-ization coefficients,Zk=3?i=1[exp(U????hljhl?1j?1hl?1j???)]d?
(i?1)+k, (7)where 1 ?
k ?
d.Intuitively, three update gates are constrainedby:???????????
[zN]k+ [zL]k+ [zR]k= 1, 1 ?
k ?
d;[zN]k?
0, 1 ?
k ?
d;[zL]k?
0, 1 ?
k ?
d;[zR]k?
0, 1 ?
k ?
d.(8)The new activation ?hljis computed as:?hlj= tanh(W?h[rL?
hl?1j?1rR?
hl?1j]), (9)where W?h ?
Rd?2d, rL?
Rd, rR?
Rd.
rLandrRare the reset gates for left child node hl?1j?1andright child node hl?1jrespectively, which can be1747formalized as:[rLrR]= ?
(G[hl?1j?1hl?1j]), (10)(11)where G ?
R2d?2d is the coefficient of two resetgates and ?
indicates the sigmoid function.Intuiativly, the reset gates control how to selectthe output information of the left and right chil-dren, which results to the current new activation?h.By the update gates, the activation of a parentneuron can be regarded as a choice among the thecurrent new activation ?h, the left child, and theright child.
This choice allows the overall structureto change adaptively with respect to the inputs.This gating mechanism is effective to model thecombinations of the characters.3.4 InferenceIn Chinese word segmentation task, it is usually toemploy the Viterbi algorithm to inference the tagsequence t1:nfor a given input sentence c1:n.In order to model the tag dependencies, theprevious neural network models (Collobert et al,2011; Zheng et al, 2013; Pei et al, 2014) intro-duce a transition matrix A, and each entry Aijisthe score of the transformation from tag i ?
T totag j ?
T .Thus, the sentence-level score can be formu-lated as follows:s(c1:n, t1:n, ?)
=n?i=1(Ati?1ti+ f?
(ti|ci?w1:i+w2)),(12)where f?
(ti|ci?w1:i+w2) is the score for choosingtag tifor the i-th character by our proposed GRNN(Eq.
(4)).
The parameter set of our model is ?
=(M,Ws,bs,W?h,U,G,A).4 Training4.1 Layer-wise TrainingDeep neural network with multiple hidden layersis very difficult to train for its problem of gradientdiffusion and risk of overfitting.Following (Hinton and Salakhutdinov, 2006),we employ the layer-wise training strategy to avoidproblems of overfitting and gradient vanishing.The main idea of layer-wise training is to train thenetwork with adding the layers one by one.
Specif-ically, we first train the neural network with thefirst hidden layer only.
Then, we train at the net-work with two hidden layers after training at firstlayer is done and so on until we reach the top hid-den layer.
When getting convergency of the net-work with layers 1 to l , we preserve the currentparameters as initial values of that in training thenetwork with layers 1 to l + 1.4.2 Max-Margin CriterionWe use the Max-Margin criterion (Taskar et al,2005) to train our model.
Intuitively, the Max-Margin criterion provides an alternative to prob-abilistic, likelihood based estimation methods byconcentrating directly on the robustness of the de-cision boundary of a model.
We use Y (xi) to de-note the set of all possible tag sequences for a givensentence xiand the correct tag sequence for xiisyi.
The parameter set of our model is ?.
We firstdefine a structured margin loss ?
(yi, y?)
for pre-dicting a tag sequence y?
for a given correct tag se-quence yi:?
(yi, y?)
=n?j?1{yi,j?= y?j}, (13)where n is the length of sentence xiand ?
is a dis-count parameter.
The loss is proportional to thenumber of characters with an incorrect tag in thepredicted tag sequence.
For a given training in-stance (xi, yi), we search for the tag sequence withthe highest score:y?= argmaxy?
?Y (x)s(xi, y?, ?
), (14)where the tag sequence is found and scored bythe proposed model via the function s(?)
in Eq.(12).
The object of Max-Margin training is thatthe tag sequence with highest score is the correctone: y?
= yiand its score will be larger up to amargin to other possible tag sequences y?
?
Y (xi):s(x, yi, ?)
?
s(x, y?, ?)
+ ?
(yi, y?).
(15)This leads to the regularized objective function form training examples:J(?)
=1mm?i=1li(?)
+?2??
?22, (16)li(?)
= maxy?
?Y (xi)(s(xi, y?, ?)+?
(yi, y?
))?s(xi, yi, ?).
(17)1748By minimizing this object, the score of the correcttag sequence yiis increased and score of the high-est scoring incorrect tag sequence y?
is decreased.The objective function is not differentiable due tothe hinge loss.
We use a generalization of gradientdescent called subgradient method (Ratliff et al,2007) which computes a gradient-like direction.Following (Socher et al, 2013a), we minimizethe objective by the diagonal variant of AdaGrad(Duchi et al, 2011) with minibatchs.
The parame-ter update for the i-th parameter ?t,iat time step tis as follows:?t,i= ?t?1,i???
?t?=1g2?,igt,i, (18)where ?
is the initial learning rate and g??
R|?i|is the subgradient at time step ?
for parameter ?i.5 ExperimentsWe evaluate our model on two different kinds oftexts: newswire texts and micro-blog texts.
Forevaluation, we use the standard Bakeoff scoringprogram to calculate precision, recall, F1-score.5.1 Word Segmentation on Newswire Texts5.1.1 DatasetsWe use three popular datasets, PKU, MSRA andCTB6, to evaluate our model on newswire texts.The PKU and MSRA data are provided by thesecond International Chinese Word SegmentationBakeoff (Emerson, 2005), and CTB6 is fromChinese TreeBank 6.0 (LDC2007T36) (Xue etal., 2005), which is a segmented, part-of-speechtagged, and fully bracketed corpus in the con-stituency formalism.
These datasets are commonlyused by previous state-of-the-art models and neu-ral network models.
In addition, we use the first90% sentences of the training data as training setand the rest 10% sentences as development set forPKU and MSRA datasets, and we divide the train-ing, development and test sets according to (Yangand Xue, 2012) for the CTB6 dataset.All datasets are preprocessed by replacing theChinese idioms and the continuous English char-acters and digits with a unique flag.5.1.2 Hyper-parametersWe set the hyper-parameters of the model as listin Table 1 via experiments on development set.In addition, we set the batch size to 20.
And weWindow size k = 5Character embedding size d = 50Initial learning rate ?
= 0.3Margin loss discount ?
= 0.2Regularization ?
= 10?4Dropout rate on input layer p = 20%Table 1: Hyper-parameter settings.0 10 20 30 408890929496epochesF-value(%) 1 layer2 layers3 layers4 layers5 layerslayer-wiseFigure 5: Performance of different models with orwithout layer-wise training strategy on PKUdevel-opment set.find that it is a good balance between model per-formance and efficiency to set character embed-ding size d = 50.
In fact, the larger embeddingsize leads to higher cost of computational resource,while lower dimensionality of the character em-bedding seems to underfit according to the experi-ment results.Deep neural networks contain multiple non-linear hidden layers are always hard to train for itis easy to overfit.
Several methods have been usedin neural models to avoid overfitting, such as earlystop and weight regularization.
Dropout (Srivas-tava et al, 2014) is also one of the popular strate-gies to avoid overfitting when training the deepneural networks.
Hence, we utilize the dropoutstrategy in this work.
Specifically, dropout is totemporarily remove the neuron away with a fixedprobability p independently, along with the incom-ing and outgoing connections of it.
As a result,we find dropout on the input layer with probabilityp = 20% is a good tradeoff between model effi-ciency and performance.1749models without layer-wise with layer-wiseP R F P R FGRNN (1 layer) 90.7 89.6 90.2 - - -GRNN (2 layers) 96.0 95.6 95.8 96.0 95.6 95.8GRNN (3 layers) 95.9 95.4 95.7 96.0 95.7 95.9GRNN (4 layers) 95.6 95.2 95.4 96.1 95.7 95.9GRNN (5 layers) 95.3 94.7 95.0 96.1 95.7 95.9Table 2: Performance of different models with or without layer-wise training strategy on PKU test set.5.1.3 Layer-wise TrainingWe first investigate the effects of the layer-wisetraining strategy.
Since we set the size of contextwindow to five, there are five recursive layers inour architecture.
And we train the networks withthe different numbers of recursion layers.
Due tothe limit of space, we just give the results on PKUdataset.Figure 5 gives the convergence speeds of thefive models with different numbers of layers andthe model with layer-wise training strategy on de-velopment set of PKU dataset.
The model withone layer just use the neurons of the lowest layerin final linear score function.
Since there are nonon-linear layer, its seems to underfit and performpoorly.
The model with two layers just use theneurons in the lowest two layers, and so on.
Themodel with five layers use all the neurons in thenetwork.
As we can see, the layer-wise trainingstrategy lead to the fastest convergence and thebest performance.Table 2 shows the performances on PKU testset.
The performance of the model with layer-wisetraining strategy is always better than that with-out layer-wise training strategy.
With the increaseof the number of layers, the performance also in-creases and reaches the stable high performanceuntil getting to the top layer.5.1.4 ResultsWe first compare our model with the previous neu-ral approaches on PKU,MSRA and CTB6 datasetsas showing in Table 3.
The character embed-dings of the models are random initialized.
Theperformance of word segmentation is significantlyboosted by exploiting the gated recursive archi-tecture, which can better model the combinationsof the context characters than the previous neuralmodels.Previous works have proven it will greatly im-prove the performance to exploit the pre-trainedcharacter embeddings instead of that with randominitialization.
Thus, we pre-train the embeddingson a huge unlabeled data, the Chinese Wikipediacorpus, with word2vec toolkit (Mikolov et al,2013).
By using these obtained character embed-dings, our model receives better performance andstill outperforms the previous neural models withpre-trained character embeddings.
The detailed re-sults are shown in Table 4 (1st to 3rd rows).Inspired by (Pei et al, 2014), we utilize the bi-gram feature embeddings in our model as well.The concept of feature embedding is quite similarto that of character embedding mentioned above.Specifically, each context feature is represented asa single vector called feature embedding.
In thispaper, we only use the simply bigram feature em-beddings initialized by the average of two embed-dings of consecutive characters element-wisely.Although the model of Pei et al (2014) greatlybenefits from the bigram feature embeddings, ourmodel just obtains a small improvement with them.This difference indicates that our model has wellmodeled the combinations of the characters and donot needmuch help of the feature engineering.
Thedetailed results are shown in Table 4 (4-th and 6-throws).Table 5 shows the comparisons of our modelwith the state-of-the-art systems on F-value.
Themodel proposed by Zhang and Clark (2007) isa word-based segmentation method, which ex-ploit features of complete words, while remainsof the list are all character-based word segmenters,whose features are mostly extracted from the con-text characters.
Moreover, some systems (such asSun and Xu (2011) and Zhang et al (2013)) alsoexploit kinds of extra information such as the un-labeled data or other knowledge.
Although ourmodel only uses simple bigram features, it outper-forms the previous state-of-the-art methods whichuse more complex features.1750models PKU MSRA CTB6P R F P R F P R F(Zheng et al, 2013) 92.8 92.0 92.4 92.9 93.6 93.3 94.0* 93.1* 93.6*(Pei et al, 2014) 93.7 93.4 93.5 94.6 94.2 94.4 94.4* 93.4* 93.9*GRNN 96.0 95.7 95.9 96.3 96.1 96.2 95.4 95.2 95.3Table 3: Performances on PKU,MSRA and CTB6 test sets with random initialized character embeddings.models PKU MSRA CTB6P R F P R F P R F+Pre-train(Zheng et al, 2013) 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7*(Pei et al, 2014) 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0*GRNN 96.3 95.9 96.1 96.2 96.3 96.2 95.8 95.4 95.6+bigramGRNN 96.6 96.2 96.4 97.5 97.3 97.4 95.9 95.7 95.8+Pre-train+bigram(Pei et al, 2014) - 95.2 - - 97.2 - - - -GRNN 96.5 96.3 96.4 97.4 97.8 97.6 95.8 95.7 95.8Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em-beddings.models PKU MSRA CTB6(Tseng et al, 2005) 95.0 96.4 -(Zhang and Clark, 2007) 95.1 97.2 -(Sun and Xu, 2011) - - 95.7(Zhang et al, 2013) 96.1 97.4 -This work 96.4 97.6 95.8Table 5: Comparison of GRNN with the state-of-the-art methods on PKU, MSRA and CTB6 testsets.5.2 Word Segmentation on Micro-blog Texts5.2.1 DatasetWeuse the NLPCC 2015 dataset1 (Qiu et al, 2015)to evaluate our model on micro-blog texts.
TheNLPCC 2015 data are provided by the shared taskin the 4th CCF Conference on Natural LanguageProcessing & Chinese Computing (NLPCC 2015):Chinese Word Segmentation and POS Tagging formicro-blog Text.
Different with the popular usednewswire dataset, the NLPCC 2015 dataset is col-lected from Sina Weibo2, which consists of therelatively informal texts from micro-blog with thevarious topics, such as finance, sports, entertain-ment, and so on.
The information of the dataset is1http://nlp.fudan.edu.cn/nlpcc2015/2http://www.weibo.com/shown in Table 6.To train our model, we also use the first 90%sentences of the training data as training set andthe rest 10% sentences as development set.Here, we use the default setting of CRF++toolkit with the feature templates as shown in Ta-ble 7.
The same feature templates are also used forFNLP.5.2.2 ResultsSince the NLPCC 2015 dataset is a new releaseddataset, we compare our model with the two popu-lar open source toolkits for sequence labeling task:FNLP3 (Qiu et al, 2013) and CRF++4.
Our modeluses pre-trained and bigram character embeddings.Table 8 shows the comparisons of our modelwith the other systems on NLPCC 2015 dataset.6 Related WorkChinese word segmentation has been studied withconsiderable efforts in the NLP community.
Themost popular word segmentation method is basedon sequence labeling (Xue, 2003).
Recently, re-searchers have tended to explore neural network3https://github.com/xpqiu/fnlp/4http://taku910.github.io/crfpp/*The result is from our own implementation of the corre-sponding method.1751Dataset Sents Words Chars Word Types Char Types OOV RateTraining 10,000 215,027 347,984 28,208 39,71 -Test 5,000 106,327 171,652 18,696 3,538 7.25%Total 15,000 322,410 520,555 35,277 4,243 -Table 6: Statistical information of NLPCC 2015 dataset.unigram feature c?2, c?1, c0, c+1, c+2bigram feature c?1?
c0, c0?
c+1trigram feature c?2?c?1?c0, c?1?c0?c+1,c0?
c+1?
c+2Table 7: Templates of CRF++ and FNLP.models P R FCRF++ 93.3 93.2 93.3FNLP 94.1 93.9 94.0This work 94.7 94.8 94.8Table 8: Performances on NLPCC 2015 dataset.based approaches (Collobert et al, 2011) to re-duce efforts of the feature engineering (Zheng etal., 2013; Qi et al, 2014).
However, the featuresof all these methods are the concatenation of theembeddings of the context characters.Pei et al (2014) also used neural tensor model(Socher et al, 2013b) to capture the complicatedinteractions between tags and context characters.But the interactions depend on the number of thetensor slices, which cannot be too large due to themodel complexity.
The experiments also showthat the model of (Pei et al, 2014) greatly bene-fits from the further bigram feature embeddings,which shows that their model cannot even handlethe interactions of the consecutive characters.
Dif-ferent with them, our model just has a small im-provement with the bigram feature embeddings,which indicates that our approach has well mod-eled the complicated combinations of the contextcharacters, and does not need much help of furtherfeature engineering.More recently, Cho et al (2014a) also proposeda gated recursive convolutional neural network inmachine translation task to solve the problem ofvarying lengths of sentences.
However, their ap-proach only models the update gate, which can nottell whether the information is from the currentstate or from sub notes in update stage without re-set gate.
Instead, our approach models two kindsof gates, reset gate and update gate, by incorporat-ing which we can better model the combinationsof context characters via selection function of re-set gate and collection function of update gate.7 ConclusionIn this paper, we propose a gated recursive neu-ral network (GRNN) to explicitly model the com-binations of the characters for Chinese word seg-mentation task.
Each neuron in GRNN can be re-garded as a different combination of the input char-acters.
Thus, the whole GRNN has an ability tosimulate the design of the sophisticated features intraditional methods.
Experiments show that ourproposed model outperforms the state-of-the-artmethods on three popular benchmark datasets.Despite Chineseword segmentation being a spe-cific case, our model can be easily generalized andapplied to other sequence labeling tasks.
In futurework, we would like to investigate our proposedGRNN on other sequence labeling tasks.AcknowledgmentsWe would like to thank the anonymous review-ers for their valuable comments.
This workwas partially funded by the National Natural Sci-ence Foundation of China (61472088, 61473092),the National High Technology Research and De-velopment Program of China (2015AA015408),Shanghai Science and Technology DevelopmentFunds (14ZR1403200), Shanghai Leading Aca-demic Discipline Project (B114).ReferencesKyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014a.
On the proper-ties of neural machine translation: Encoder?decoderapproaches.
In Proceedings of Workshop on Syntax,Semantics and Structure in Statistical Translation.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Fethi Bougares, Holger Schwenk, and YoshuaBengio.
2014b.
Learning phrase representationsusing rnn encoder-decoder for statistical machinetranslation.
In Proceedings of EMNLP.1752Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,and Yoshua Bengio.
2014.
Empirical evaluation ofgated recurrent neural networks on sequence model-ing.
arXiv preprint arXiv:1412.3555.Ronan Collobert, Jason Weston, L?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Research,12:2493?2537.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal ofMachineLearning Research, 12:2121?2159.T.
Emerson.
2005.
The second international Chi-nese word segmentation bakeoff.
In Proceedings ofthe Fourth SIGHANWorkshop on Chinese LanguageProcessing, pages 123?133.
Jeju Island, Korea.Geoffrey E Hinton and Ruslan R Salakhutdinov.
2006.Reducing the dimensionality of data with neural net-works.
Science, 313(5786):504?507.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Wenzhe Pei, Tao Ge, and Chang Baobao.
2014.
Max-margin tensor neural network for chinese word seg-mentation.
In Proceedings of ACL.Jordan B Pollack.
1990.
Recursive distributed repre-sentations.
Artificial Intelligence, 46(1):77?105.Yanjun Qi, Sujatha G Das, Ronan Collobert, and JasonWeston.
2014.
Deep learning for character-basedinformation extraction.
In Advances in InformationRetrieval, pages 668?674.
Springer.Xipeng Qiu, Qi Zhang, and Xuanjing Huang.
2013.FudanNLP: A toolkit for Chinese natural languageprocessing.
In Proceedings of Annual Meeting of theAssociation for Computational Linguistics.Xipeng Qiu, Peng Qian, Liusong Yin, and Xuan-jing Huang.
2015.
Overview of the NLPCC2015 shared task: Chinese word segmentation andPOS tagging for micro-blog texts.
arXiv preprintarXiv:1505.07599.Nathan D Ratliff, J Andrew Bagnell, and Martin AZinkevich.
2007.
(online) subgradient methodsfor structured prediction.
In Eleventh InternationalConference on Artificial Intelligence and Statistics(AIStats).Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013a.
Parsing with compo-sitional vector grammars.
In In Proceedings of theACL conference.
Citeseer.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013b.
Reasoning with neural ten-sor networks for knowledge base completion.
In Ad-vances in Neural Information Processing Systems.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Weiwei Sun and Jia Xu.
2011.
Enhancing Chineseword segmentation using unlabeled data.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 970?979.
As-sociation for Computational Linguistics.Ben Taskar, Vassil Chatalbashev, Daphne Koller, andCarlos Guestrin.
2005.
Learning structured pre-diction models: A large margin approach.
In Pro-ceedings of the international conference on Machinelearning.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proceedings of the fourth SIGHANworkshop on Chinese language Processing, volume171.Naiwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese TreeBank: Phrasestructure annotation of a large corpus.
Natural lan-guage engineering, 11(2):207?238.N.
Xue.
2003.
Chinese word segmentation as charac-ter tagging.
Computational Linguistics and ChineseLanguage Processing, 8(1):29?48.Yaqin Yang and Nianwen Xue.
2012.
Chinese commadisambiguation for discourse analysis.
In Proceed-ings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics: Long Papers-Volume 1, pages 786?794.
Association for Compu-tational Linguistics.Yue Zhang and Stephen Clark.
2007.
Chinese segmen-tation with a word-based perceptron algorithm.
InACL.Longkai Zhang, Houfeng Wang, Xu Sun, and MairgupMansur.
2013.
Exploring representations from un-labeled data with co-training for Chinese word seg-mentation.
In Proceedings of the 2013 Conferenceon EmpiricalMethods in Natural Language Process-ing.Xiaoqing Zheng, Hanyang Chen, and TianyuXu.
2013.Deep learning for chinese word segmentation andpos tagging.
In EMNLP, pages 647?657.1753
