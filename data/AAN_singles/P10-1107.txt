Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1048?1057,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsA Statistical Model for Lost Language DeciphermentBenjamin Snyder and Regina BarzilayCSAILMassachusetts Institute of Technology{bsnyder,regina}@csail.mit.eduKevin KnightISIUniversity of Southern Californiaknight@isi.eduAbstractIn this paper we propose a method for theautomatic decipherment of lost languages.Given a non-parallel corpus in a known re-lated language, our model produces bothalphabetic mappings and translations ofwords into their corresponding cognates.We employ a non-parametric Bayesianframework to simultaneously capture bothlow-level character mappings and high-level morphemic correspondences.
Thisformulation enables us to encode some ofthe linguistic intuitions that have guidedhuman decipherers.
When applied tothe ancient Semitic language Ugaritic, themodel correctly maps 29 of 30 letters totheir Hebrew counterparts, and deducesthe correct Hebrew cognate for 60% ofthe Ugaritic words which have cognates inHebrew.1 IntroductionDozens of lost languages have been decipheredby humans in the last two centuries.
In eachcase, the decipherment has been considered a ma-jor intellectual breakthrough, often the culmina-tion of decades of scholarly efforts.
Computershave played no role in the decipherment any ofthese languages.
In fact, skeptics argue that com-puters do not possess the ?logic and intuition?
re-quired to unravel the mysteries of ancient scripts.1In this paper, we demonstrate that at least some ofthis logic and intuition can be successfully mod-eled, allowing computational tools to be used inthe decipherment process.1?Successful archaeological decipherment has turned outto require a synthesis of logic and intuition .
.
.
that comput-ers do not (and presumably cannot) possess.?
A. Robinson,?Lost Languages: The Enigma of the World?s UndecipheredScripts?
(2002)Our definition of the computational decipher-ment task closely follows the setup typically facedby human decipherers (Robinson, 2002).
Our in-put consists of texts in a lost language and a corpusof non-parallel data in a known related language.The decipherment itself involves two related sub-tasks: (i) finding the mapping between alphabetsof the known and lost languages, and (ii) translat-ing words in the lost language into correspondingcognates of the known language.While there is no single formula that human de-cipherers have employed, manual efforts have fo-cused on several guiding principles.
A commonstarting point is to compare letter and word fre-quencies between the lost and known languages.In the presence of cognates the correct mappingbetween the languages will reveal similarities infrequency, both at the character and lexical level.In addition, morphological analysis plays a cru-cial role here, as highly frequent morpheme cor-respondences can be particularly revealing.
Infact, these three strands of analysis (character fre-quency, morphology, and lexical frequency) areintertwined throughout the human deciphermentprocess.
Partial knowledge of each drives discov-ery in the others.We capture these intuitions in a generativeBayesian model.
This model assumes that eachword in the lost language is composed of mor-phemes which were generated with latent coun-terparts in the known language.
We model bilin-gual morpheme pairs as arising through a seriesof Dirichlet processes.
This allows us to assignprobabilities based both on character-level corre-spondences (using a character-edit base distribu-tion) as well as higher-level morpheme correspon-dences.
In addition, our model carries out an im-plicit morphological analysis of the lost language,utilizing the known morphological structure of therelated language.
This model structure allows usto capture the interplay between the character-1048and morpheme-level correspondences that humanshave used in the manual decipherment process.In addition, we introduce a novel techniquefor imposing structural sparsity constraints oncharacter-level mappings.
We assume that an ac-curate alphabetic mapping between related lan-guages will be sparse in the following way: eachletter will map to a very limited subset of lettersin the other language.
We capture this intuitionby adapting the so-called ?spike and slab?
prior tothe Dirichlet-multinomial setting.
For each pairof characters in the two languages, we posit anindicator variable which controls the prior likeli-hood of character substitutions.
We define a jointprior over these indicator variables which encour-ages sparse settings.We applied our model to a corpus of Ugaritic,an ancient Semitic language discovered in 1928.Ugaritic was manually deciphered in 1932, us-ing knowledge of Hebrew, a related language.We compare our method against the only existingdecipherment baseline, an HMM-based charactersubstitution cipher (Knight and Yamada, 1999;Knight et al, 2006).
The baseline correctly mapsthe majority of letters ?
22 out of 30 ?
to theircorrect Hebrew counterparts, but only correctlytranslates 29% of all cognates.
In comparison, ourmethod yields correct mappings for 29 of 30 let-ters, and correctly translates 60.4% of all cognates.2 Related WorkOur work on decipherment has connections tothree lines of work in statistical NLP.
First, ourwork relates to research on cognate identifica-tion (Lowe and Mazaudon, 1994; Guy, 1994;Kondrak, 2001; Bouchard et al, 2007; Kondrak,2009).
These methods typically rely on informa-tion that is unknown in a typical deciphering sce-nario (while being readily available for living lan-guages).
For instance, some methods employ ahand-coded similarity function (Kondrak, 2001),while others assume knowledge of the phoneticmapping or require parallel cognate pairs to learna similarity function (Bouchard et al, 2007).A second related line of work is lexicon in-duction from non-parallel corpora.
While thisresearch has similar goals, it typically builds oninformation or resources unavailable for ancienttexts, such as comparable corpora, a seed lexi-con, and cognate information (Fung and McKe-own, 1997; Rapp, 1999; Koehn and Knight, 2002;Haghighi et al, 2008).
Moreover, distributionalmethods that rely on co-occurrence analysis oper-ate over large corpora, which are typically unavail-able for a lost language.Finally, Knight and Yamada (1999) and Knightet al (2006) describe a computational HMM-based method for deciphering an unknown scriptthat represents a known spoken language.
Thismethod ?makes the text speak?
by gleaningcharacter-to-sound mappings from non-parallelcharacter and sound sequences.
It does not relatewords in different languages, thus it cannot encodedeciphering constraints similar to the ones consid-ered in this paper.
More importantly, this methodhad not been applied to archaeological data.
Whilelost languages are gaining increasing interest inthe NLP community (Knight and Sproat, 2009),there have been no successful attempts of their au-tomatic decipherment.3 Background on UgariticManual Decipherment of Ugaritic Ugaritictablets were first found in Syria in 1929 (Smith,1955; Watson and Wyatt, 1999).
At the time, thecuneiform writing on the tablets was of an un-known type.
Charles Virolleaud, who lead the ini-tial decipherment effort, recognized that the scriptwas likely alphabetic, since the inscribed wordsconsisted of only thirty distinct symbols.
The lo-cation of the tablets discovery further suggestedthat Ugaritic was likely to have been a Semiticlanguage from the Western branch, with proper-ties similar to Hebrew and Aramaic.
This real-ization was crucial for deciphering the Ugariticscript.
In fact, German cryptographer and Semiticscholar Hans Bauer decoded the first two Ugariticletters?mem and lambda?by mapping them toHebrew letters with similar occurrence patternsin prefixes and suffixes.
Bootstrapping from thisfinding, Bauer found words in the tablets that werelikely to serve as cognates to Hebrew words?e.g., the Ugaritic word for king matches its He-brew equivalent.
Through this process a fewmore letters were decoded, but the Ugaritic textswere still unreadable.
What made the final deci-pherment possible was a sheer stroke of luck?Bauer guessed that a word inscribed on an ax dis-covered in the Ras Shamra excavations was theUgaritic word for ax.
Bauer?s guess was cor-rect, though he selected the wrong phonetic se-quence.
Edouard Dhorme, another cryptographer1049and Semitic scholar, later corrected the reading,expanding a set of translated words.
Discoveriesof additional tablets allowed Bauer, Dhorme andVirolleaud to revise their hypothesis, successfullycompleting the decipherment.Linguistic Features of Ugaritic Ugariticshares many features with other ancient Semiticlanguages, following the same word order, gender,number, and case structure (Hetzron, 1997).
It is amorphologically rich language, with triliteral rootsand many prefixes and suffixes.At the same time, it exhibits a number of fea-tures that distinguish it from Hebrew.
Ugaritic hasa bigger phonemic inventory than Hebrew, yield-ing a bigger alphabet ?
30 letters vs. 22 in He-brew.
Another distinguishing feature of Ugariticis that vowels are only written with glottal stopswhile in Hebrew many long vowels are written us-ing homorganic consonants.
Ugaritic also does nothave articles, while Hebrew nouns and adjectivestake definite articles which are realized as prefixes.These differences result in significant divergencebetween Hebrew and Ugaritic cognates, therebycomplicating the decipherment process.4 Problem FormulationWe are given a corpus in a lost language and a non-parallel corpus in a related language from the samelanguage family.
Our primary goal is to translatewords in the unknown language by mapping themto cognates in the known language.
As part of thisprocess, we induce a lower-level mapping betweenthe letters of the two alphabets, capturing the reg-ular phonetic correspondences found in cognates.We make several assumptions about the writ-ing system of the lost language.
First, we assumethat the writing system is alphabetic in nature.
Ingeneral, this assumption can be easily validated bycounting the number of symbols found in the writ-ten record.
Next, we assume that the corpus hasbeen transcribed into electronic format, where thegraphemes present in the physical text have beenunambiguously identified.
Finally, we assume thatwords are explicitly separated in the text, either bywhite space or a special symbol.We also make a mild assumption about the mor-phology of the lost language.
We posit that eachword consists of a stem, prefix, and suffix, wherethe latter two may be omitted.
This assumptioncaptures a wide range of human languages and avariety of morphological systems.
While the cor-rect morphological analysis of words in the lostlanguage must be learned, we assume that the in-ventory and frequencies of prefixes and suffixes inthe known language are given.In summary, the observed input to the modelconsists of two elements: (i) a list of unanalyzedword types derived from a corpus in the lost lan-guage, and (ii) a morphologically analyzed lexiconin a known related language derived from a sepa-rate corpus, in our case non-parallel.5 Model5.1 IntuitionsOur goal is to incorporate the logic and intuitionused by human decipherers in an unsupervised sta-tistical model.
To make these intuitions concrete,consider the following toy example, consisting ofa lost language much like English, but written us-ing numerals:?
15234 (asked)?
1525 (asks)?
4352 (desk)Analyzing the undeciphered corpus, we might firstnotice a pair of endings, -34, and -5, which bothoccur after the initial sequence 152- (and may like-wise occur at the end of a variety of words inthe corpus).
If we know this lost language to beclosely related to English, we can surmise thatthese two endings correspond to the English ver-bal suffixes -ed and -s. Using this knowledge,we can hypothesize the following character corre-spondences: (3 = e), (4 = d), (5 = s).
We now knowthat (4252 = des2) and we can use our knowl-edge of the English lexicon to hypothesize that thisword is desk, thereby learning the correspondence(2 = k).
Finally, we can use similar reasoning toreveal that the initial character sequence 152- cor-responds to the English verb ask.As this example illustrates, human deci-pherment efforts proceed by discovering bothcharacter-level and morpheme-level correspon-dences.
This interplay implicitly relies on amorphological analysis of words in the lost lan-guage, while utilizing knowledge of the knownlanguage?s lexicon and morphology.One final intuition our model should capture isthe sparsity of the alphabetic correspondence be-tween related languages.
We know from compar-ative linguistics that the correct mapping will pre-1050serve regular phonetic relationships between thetwo languages (as exemplified by cognates).
As aresult, each character in one language will map toa small number of characters in the other language(typically one, but sometimes two or three).
Byincorporating this structural sparsity intuition, wecan allow the model to focus on on a smaller set oflinguistically valid hypotheses.Below we give an overview of our model, whichis designed to capture these linguistic intuitions.5.2 Model StructureOur model posits that every observed word in thelost language is composed of a sequence of mor-phemes (prefix, stem, suffix).
Furthermore weposit that each morpheme was probabilisticallygenerated jointly with a latent counterpart in theknown language.Our goal is to find those counterparts that lead tohigh frequency correspondences both at the char-acter and morpheme level.
The technical chal-lenge is that each level of correspondence (char-acter and morpheme) can completely describe theobserved data.
A probabilistic mechanism basedsimply on one leaves no room for the other to playa role.
We resolve this tension by employing anon-parametric Bayesian model: the distributionsover bilingual morpheme pairs assign probabil-ity based on recurrent patterns at the morphemelevel.
These distributions are themselves drawnfrom a prior probabilistic process which favorsdistributions with consistent character-level corre-spondences.We now give a formal description of the model(see Figure 1 for a graphical overview).
There arefour basic layers in the generative process:1.
Structural sparsity: draw a set of indicatorvariables ??
corresponding to character-editoperations.2.
Character-edit distribution: draw a basedistribution G0 parameterized by weights oncharacter-edit operations.3.
Morpheme-pair distributions: draw a setof distributions on bilingual morpheme pairsGstm, Gpre|stm, Gsuf |stm.4.
Word generation: draw pairs of cognatesin the lost and known language, as well aswords in the lost language with no cognatecounterpart.G0wordG stmustmhstmu pr ehpreu sufh su fstm stmGsuf |stmGpre|stm!v!
?Figure 1: Plate diagram of the deciphermentmodel.
The structural sparsity indicator variables??
determine the values of the base distribution hy-perparameters v?.
The base distribution G0 de-fines probabilities over string-pairs based solely oncharacter-level edits.
The morpheme-pair distri-butions Gstm, Gpre|stm, Gsuf |stm directly assignprobabilities to highly frequent morpheme pairs.We now go through each step in more detail.Structural Sparsity The first step of the genera-tive process provides a control on the sparsity ofedit-operation probabilities, encoding the linguis-tic intuition that the correct character-level map-pings should be sparse.
The set of edit opera-tions includes character substitutions, insertions,and deletions, as well as a special end sym-bol: {(u, h), (?, h), (u, ?
), END} (where u and hrange over characters in the lost and known lan-guages, respectively).
For each edit operation eweposit a corresponding indicator variable ?e.
Theset of character substitutions with indicators set toone, {(u, h) : ?
(u,h) = 1}) conveys the set ofphonetically valid correspondences.
We define ajoint prior over these variables to encourage sparsecharacter mappings.
This prior can be viewed as adistribution over binary matrices and is defined toencourage rows and columns to sum to low integervalues (typically 1).
More precisely, for each char-acter u in the lost language, we count the numberof mappings c(u) =?h ?(u,h).
We then definea set of features which count how many of thesecharacters map to i other characters beyond somebudget bi: fi = max (0, |{u : c(u) = i}| ?
bi).Likewise, we define corresponding features f ?i andbudgets b?i for the characters h in the known lan-1051guage.
The prior over ??
is then defined asP (??)
=exp(f?
?
w?
+ f?
?
?
w?
)Z(1)where the feature weight vector w?
is set to encour-age sparse mappings, and Z is a correspondingnormalizing constant, which we never need com-pute.
We set w?
so that each character must map toat least one other character, and so that mappingsto more than one other character are discouraged 2Character-edit Distribution The next step inthe generative process is drawing a base distri-bution G0 over character edit sequences (each ofwhich yields a bilingual pair of morphemes).
Thisdistribution is parameterized by a set of weights ?
?on edit operations, where the weights over substi-tutions, insertions, and deletions each individuallysum to one.
In addition, G0 provides a fixed dis-tribution q over the number of insertions and dele-tions occurring in any single edit sequence.
Prob-abilities over edit sequences (and consequently onbilingual morpheme pairs) are then defined ac-cording to G0 as:P (e?)
=?i?ei ?
q (#ins(e?),#del(e?
))We observe that the average Ugaritic word is overtwo letters longer than the average Hebrew word.Thus, occurrences of Hebrew character insertionsare a priori likely, and Ugaritic character deletionsare very unlikely.
In our experiments, we set qto disallow Ugaritic deletions, and to allow oneHebrew insertion per morpheme (with probability0.4).The prior on the base distribution G0 is aDirichlet distribution with hyperparameters v?, i.e.,??
?
Dirichlet(v?).
Each value ve thus corre-sponds to a character edit operation e. Crucially,the value of each ve depends deterministically onits corresponding indicator variable:ve ={1 if ?e = 0,K if ?e = 1.where K is some constant value > 1.3 The overalleffect is that when ?e = 0, the marginal prior den-sity of the corresponding edit weight ?e spikes at2We set w0 = ?
?, w1 = 0, w2 = ?50, w>2 = ?
?,with budgets b?2 = 7, b?3 = 1 (otherwise zero), reflecting theknowledge that there are eight more Ugaritic than Hebrewletters.3Set to 50 in our experiments.0.
When ?e = 1, the corresponding marginal priordensity remains relatively flat and unconstrained.See (Ishwaran and Rao, 2005) for a similar appli-cation of ?spike-and-slab?
priors in the regressionscenario.Morpheme-pair Distributions Next we draw aseries of distributions which directly assign prob-ability to morpheme pairs.
The previously drawnbase distribution G0 along with a fixed concentra-tion parameter ?
define a Dirichlet process (An-toniak, 1974): DP (G0, ?
), which provides prob-abilities over morpheme-pair distributions.
Theresulting distributions are likely to be skewed infavor of a few frequently occurring morpheme-pairs, while remaining sensitive to the character-level probabilities of the base distribution.Our model distinguishes between three types ofmorphemes: prefixes, stems, and suffixes.
As aresult, we model each morpheme type as arisingfrom distinct Dirichlet processes, that share a sin-gle base distribution:Gstm ?
DP (G0, ?stm)Gpre|stm ?
DP (G0, ?pre)Gsuf |stm ?
DP (G0, ?suf )We model prefix and suffix distributions as con-ditionally dependent on the part-of-speech of thestem morpheme-pair.
This choice capture the lin-guistic fact that different parts-of-speech bear dis-tinct affix frequencies.
Thus, while we draw a sin-gle distribution Gstm, we maintain separate distri-butions Gpre|stm and Gsuf |stm for each possiblestem part-of-speech.Word Generation Once the morpheme-pairdistributions have been drawn, actual word pairsmay now be generated.
First the model draws aboolean variable ci to determine whether word i inthe lost language has a cognate in the known lan-guage, according to some prior P (ci).
If ci = 1,then a cognate word pair (u, h) is produced:(ustm, hstm) ?
Gstm(upre, hpre) ?
Gpre|stm(usuf , hsuf ) ?
Gsuf |stmu = upreustmusufh = hprehstmhsufOtherwise, a lone word u is generated, accordinga uniform character-level language model.1052In summary, this model structure captures bothcharacter and lexical level correspondences, whileutilizing morphological knowledge of the knownlanguage.
An additional feature of this multi-layered model structure is that each distributionover morpheme pairs is derived from the singlecharacter-level base distribution G0.
As a re-sult, any character-level mappings learned fromone type of morphological correspondence will bepropagated to all other morpheme distributions.Finally, the character-level mappings discoveredby the model are encouraged to obey linguisticallymotivated structural sparsity constraints.6 InferenceFor each word ui in our undeciphered lan-guage we predict a morphological segmentation(upreustmusuf )i and corresponding cognate in theknown language (hprehstmhsuf )i.
Ideally wewould like to predict the analysis with highestmarginal probability under our model given theobserved undeciphered corpus and related lan-guage lexicon.
In order to do so, we need tointegrate out all the other latent variables in ourmodel.
As these integrals are intractable to com-pute exactly, we resort to the standardMonte Carloapproximation.
We collect samples of the vari-ables over which we wish to marginalize but forwhich we cannot compute closed-form integrals.We then approximate the marginal probabilitiesfor undeciphered word ui by summing over all thesamples, and predicting the analysis with highestprobability.In our sampling algorithm, we avoid sam-pling the base distribution G0 and the derivedmorpheme-pair distributions (Gstm etc.
), insteadusing analytical closed forms.
We explicitly sam-ple the sparsity indicator variables ?
?, the cognateindicator variables ci, and latent word analyses(segmentations and Hebrew counterparts).
To doso tractably, we use Gibbs sampling to draw eachlatent variable conditioned on our current sampleof the others.
Although the samples are no longerindependent, they form a Markov chain whose sta-tionary distribution is the true joint distribution de-fined by the model (Geman and Geman, 1984).6.1 Sampling Word AnalysesFor each undeciphered word, we need to samplea morphological segmentation (upre, ustm, usuf )ialong with latent morphemes in the known lan-guage (hpre, hstm, hsuf )i.
More precisely, weneed to sample three character-edit sequencese?pre, e?stm, e?suf which together yield the observedword ui.We break this into two sampling steps.
Firstwe sample the morphological segmentation of ui,along with the part-of-speech pos of the latentstem cognate.
To do so, we enumerate each pos-sible segmentation and part-of-speech and calcu-late its joint conditional probability (for notationalclarity, we leave implicit the conditioning on theother samples in the corpus):P (upre, ustm, usuf , pos) =?e?stmP (e?stm)?e?preP (e?pre|pos)?e?sufP (e?suf |pos)(2)where the summations over character-edit se-quences are restricted to those which yield the seg-mentation (upre, ustm, usuf ) and a latent cognatewith part-of-speech pos.For a particular stem edit-sequence e?stm, wecompute its conditional probability in closed formaccording to a Chinese Restaurant Process (An-toniak, 1974).
To do so, we use counts fromthe other sampled word analyses: countstm(e?stm)gives the number of times that the entire edit-sequence e?stm has been observed:P (e?stm) ?countstm(e?stm) + ?
?i p(ei)n + ?where n is the number of other word analyses sam-pled, and ?
is a fixed concentration parameter.
Theproduct?i p(ei) gives the probability of e?stm ac-cording to the base distribution G0.
Since theparameters of G0 are left unsampled, we use themarginalized form:p(e) = ve + count(e)?e?
ve?
+ k(3)where count(e) is the number of times thatcharacter-edit e appears in distinct edit-sequences(across prefixes, stems, and suffixes), and k is thesum of these counts across all character-edits.
Re-call that ve is a hyperparameter for the Dirichletprior on G0 and depends on the value of the corre-sponding indicator variable ?e.Once the segmentation (upre, ustm, usuf ) andpart-of-speech pos have been sampled, we pro-ceed to sample the actual edit-sequences (and thus1053latent morphemes counterparts).
Now, instead ofsumming over the values in Equation 2, we insteadsample from them.6.2 Sampling Sparsity IndicatorsRecall that each sparsity indicator ?e determinesthe value of the corresponding hyperparameter veof the Dirichlet prior for the character-edit basedistributionG0.
In addition, we have an unnormal-ized joint prior P (??)
= g(??
)Z which encourages asparse setting of these variables.
To sample a par-ticular ?e, we consider the set ??
in which ?e = 0and ???
in which ?e = 1.
We then compute:P (??)
?
g(??)
?
v[count(e)]e?e?
v[k]e?where k is the sum of counts for all edit opera-tions, and the notation a[b] indicates the ascendingfactorial.
Likewise, we can compute a probabilityfor ???
with corresponding values v?e.6.3 Sampling Cognate IndicatorsFinally, for each word ui, we sample a correspond-ing indicator variable ci.
To do so, we calcu-late Equation 2 for all possible segmentations andparts-of-speech and sum the resulting values to ob-tain the conditional likelihood P (ui|ci = 1).
Wealso calculate P (ui|ci = 0) using a uniform uni-gram character-level language model (and thus de-pends only on the number of characters in ui).
Wethen sample from among the two values:P (ui|ci = 1) ?
P (ci = 1)P (ui|ci = 0) ?
P (ci = 0)6.4 High-level ResamplingBesides the individual sampling steps detailedabove, we also consider several larger samplingmoves in order to speed convergence.
For exam-ple, for each type of edit-sequence e?
which hasbeen sampled (and may now occur many timesthroughout the data), we consider a single jointmove to another edit-sequence e??
(both of whichyield the same lost language morpheme u).
Thedetails are much the same as above, and as beforethe set of possible edit-sequences is limited by thestring u and the known language lexicon.We also resample groups of the sparsity indica-tor variables ??
in tandem, to allow a more rapid ex-ploration of the probability space.
For each char-acter u, we block sample the entire set {?
(u,h)}h,and likewise for each character h.6.5 Implementation DetailsMany of the steps detailed above involve the con-sideration of all possible edit-sequences consis-tent with (i) a particular undeciphered word ui and(ii) the entire lexicon of words in the known lan-guage (or some subset of words with a particu-lar part-of-speech).
In particular, we need to bothsample from and sum over this space of possibil-ities repeatedly.
Doing so by simple enumerationwould needlessly repeat many sub-computations.Instead we use finite-state acceptors to compactlyrepresent both the entire Hebrew lexicon as wellas potential Hebrew word forms for each Ugariticword.
By intersecting two such FSAs and mini-mizing the result we can efficiently represent allpotential Hebrew words for a particular Ugariticword.
We weight the edges in the FSA accordingto the base distribution probabilities (in Equation 3above).
Although these intersected acceptors haveto be constantly reweighted to reflect changingprobabilities, their topologies need only be com-puted once.
One weighted correctly, marginalsand samples can be computed using dynamic pro-gramming.Even with a large number of sampling rounds, itis difficult to fully explore the latent variable spacefor complex unsupervised models.
Thus a cleverinitialization is usually required to start the sam-pler in a high probability region.
We initialize ourmodel with the results of the HMM-based baseline(see section 8), and rule out character substitutionswith probability < 0.05 according to the baseline.7 Experiments7.1 Corpus and AnnotationsWe apply our model to the ancient Ugaritic lan-guage (see Section 3 for background).
Our un-deciphered corpus consists of an electronic tran-scription of the Ugaritic tablets (Cunchillos et al,2002).
This corpus contains 7,386 unique wordtypes.
As our known language corpus, we use theHebrew Bible, which is both geographically andtemporally close to Ugaritic.
To extract a Hebrewmorphological lexicon we assume the existenceof manual morphological and part-of-speech an-notations (Groves and Lowery, 2006).
We divideHebrew stems into four main part-of-speech cat-egories each with a distinct affix profile: Noun,Verb, Pronoun, and Particle.
For each part-of-speech category, we determine the set of allowableaffixes using the annotated Bible corpus.1054Words Morphemestype token type tokenBaseline 28.82% 46.00% N/A N/AOur Model 60.42% 66.71% 75.07% 81.25%No Sparsity 46.08% 54.01% 69.48% 76.10%Table 1: Accuracy of cognate translations, mea-sured with respect to complete word-forms andmorphemes, for the HMM-based substitution ci-pher baseline, our complete model, and our modelwithout the structural sparsity priors.
Note that thebaseline does not provide per-morpheme results,as it does not predict morpheme boundaries.To evaluate the output of our model, we anno-tated the words in the Ugaritic lexicon with thecorresponding Hebrew cognates found in the stan-dard reference dictionary (del Olo Lete and San-mart?
?n, 2004).
In addition, manual morphologicalsegmentation was carried out with the guidance ofa standard Ugaritic grammar (Schniedewind andHunt, 2007).
Although Ugaritic is an inflectionalrather than agglutinative language, in its writtenform (which lacks vowels) words can easily besegmented (e.g.
wyplt.n becomes wy-plt.-n).Overall, we identified Hebrew cognates for2,155 word forms, covering almost 1/3 of theUgaritic vocabulary.48 Evaluation Tasks and ResultsWe evaluate our model on four separate decipher-ment tasks: (i) Learning alphabetic mappings,(ii) translating cognates, (iii) identifying cognates,and (iv) morphological segmentation.As a baseline for the first three of these tasks(learning alphabetic mappings and translating andidentifying cognates), we adapt the HMM-basedmethod of Knight et al (2006) for learning let-ter substitution ciphers.
In its original setting, thismodel was used to map written texts to spoken lan-guage, under the assumption that each characterwas emitted from a hidden phonemic state.
In ouradaptation, we assume instead that each Ugariticcharacter was generated by a hidden Hebrew let-ter.
Hebrew character trigram transition probabili-ties are estimated using the Hebrew Bible, and He-brew to Ugaritic character emission probabilitiesare learned using EM.
Finally, the highest prob-4We are confident that a large majority of Ugaritic wordswith known Hebrew cognates were thus identified.
Theremaining Ugaritic words include many personal and geo-graphic names, words with cognates in other Semitic lan-guages, and words whose etymology is uncertain.ability sequence of latent Hebrew letters is pre-dicted for each Ugaritic word-form, using Viterbidecoding.Alphabetic Mapping The first essential step to-wards successful decipherment is recovering themapping between the symbols of the lost languageand the alphabet of a known language.
As a goldstandard for this comparison, we use the well-established relationship between the Ugaritic andHebrew alphabets (Hetzron, 1997).
This mappingis not one-to-one but is generally quite sparse.
Ofthe 30 Ugaritic symbols, 28 map predominantlyto a single Hebrew letter, and the remaining twomap to two different letters.
As the Hebrew alpha-bet contains only 22 letters, six map to two dis-tinct Ugaritic letters and two map to three distinctUgaritic letters.We recover our model?s predicted alphabeticmappings by simply examining the sampled val-ues of the binary indicator variables ?u,h for eachUgaritic-Hebrew letter pair (u, h).
Due to ourstructural sparsity prior P (??
), the predicted map-pings are sparse: each Ugaritic letter maps to onlya single Hebrew letter, and most Hebrew lettersmap to only a single Ugaritic letter.
To recoveralphabetic mappings from the HMM substitutioncipher baseline, we predict the Hebrew letter hwhich maximizes the model?s probability P (h|u),for each Ugaritic letter u.To evaluate these mappings, we simply countthe number of Ugaritic letters that are correctlymapped to one of their Hebrew reflexes.
By thismeasure, the baseline recovers correct mappingsfor 22 out of 30 Ugaritic characters (73.3%).
Ourmodel recovers correct mappings for all but one(very low frequency) Ugaritic characters, yielding96.67% accuracy.Cognate Decipherment We compare the deci-pherment accuracy for Ugaritic words that havecorresponding Hebrew cognates.
We evaluateour model?s predictions on each distinct Ugariticword-form at both the type and token level.
AsTable 1 shows, our method correctly translatesover 60% of all distinct Ugaritic word-forms withHebrew cognates and over 71% of the individ-ual morphemes that compose them, outperform-ing the baseline by significant margins.
Accu-racy improves when the frequency of the word-forms is taken into account (token-level evalua-tion), indicating that the model is able to deci-pher frequent words more accurately than infre-10550 0.2 0.4 0.6 0.8 1False positive rate00.20.40.60.81Truepositive rateOur ModelBaselineRandomFigure 2: ROC curve for cognate identification.quent words.
We also measure the average Leven-shtein distance between predicted and actual cog-nate word-forms.
On average, our model?s pre-dictions lie 0.52 edit operations from the true cog-nate, whereas the baseline?s predictions average adistance of 1.26 edit operations.Finally, we evaluated the performance of ourmodel when the structural sparsity constraints arenot used.
As Table 1 shows, performance degradessignificantly in the absence of these priors, indi-cating the importance of modeling the sparsity ofcharacter mappings.Cognate identification We evaluate ourmodel?s ability to identify cognates using thesampled indicator variables ci.
As before, wecompare our performance against the HMMsubstitution cipher baseline.
To produce baselinecognate identification predictions, we calculatethe probability of each latent Hebrew letter se-quence predicted by the HMM, and compare it toa uniform character-level Ugaritic language model(as done by our model, to avoid automaticallyassigning higher cognate probability to shorterUgaritic words).
For both our model and thebaseline, we can vary the threshold for cognateidentification by raising or lowering the cognateprior P (ci).
As the prior is set higher, we detectmore true cognates, but the false positive rateincreases as well.Figure 2 shows the ROC curve obtained byvarying this prior both for our model and the base-line.
At all operating points, our model outper-forms the baseline, and both models always pre-dict better than chance.
In practice for our model,we use a high cognate prior, thus only ruling outprecision recall f-measureMorfessor 88.87% 67.48% 76.71%Our Model 86.62% 90.53% 88.53%Table 2: Morphological segmentation accuracy fora standard unsupervised baseline and our model.those Ugaritic word-forms which are very unlikelyto have Hebrew cognates.Morphological segmentation Finally, we eval-uate the accuracy of our model?s morphologicalsegmentation for Ugaritic words.
As a baselinefor this comparison, we use Morfessor Categories-MAP (Creutz and Lagus, 2007).
As Table 2shows, our model provides a significant boost inperformance, especially for recall.
This result isconsistent with previous work showing that mor-phological annotations can be projected to newlanguages lacking annotation (Yarowsky et al,2000; Snyder and Barzilay, 2008), but generalizesthose results to the case where parallel data is un-available.9 Conclusion and Future WorkIn this paper we proposed a method for the au-tomatic decipherment of lost languages.
The keystrength of our model lies in its ability to incorpo-rate a range of linguistic intuitions in a statisticalframework.We hope to address several issues in futurework.
Our model fails to take into accountthe known frequency of Hebrew words and mor-phemes.
In fact, the most common error is incor-rectly translating the masculine plural suffix (-m)as the third person plural possessive suffix (-m)rather than the correct and much more commonplural suffix (-ym).
Also, even with the correct al-phabetic mapping, many words can only be deci-phered by examining their literary context.
Ourmodel currently operates purely on the vocabularylevel and thus fails to take this contextual infor-mation into account.
Finally, we intend to exploreour model?s predictive power when the family ofthe lost language is unknown.55The authors acknowledge the support of the NSF (CA-REER grant IIS-0448168, grant IIS-0835445, and grant IIS-0835652) and the Microsoft Research New Faculty Fellow-ship.
Thanks to Michael Collins, Tommi Jaakkola, andthe MIT NLP group for their suggestions and comments.Any opinions, findings, conclusions, or recommendations ex-pressed in this paper are those of the authors, and do not nec-essarily reflect the views of the funding organizations.1056ReferencesC.
E. Antoniak.
1974.
Mixtures of Dirichlet pro-cesses with applications to bayesian nonparametricproblems.
The Annals of Statistics, 2:1152?1174,November.Alexandre Bouchard, Percy Liang, Thomas Griffiths,and Dan Klein.
2007.
A probabilistic approach todiachronic phonology.
In Proceedings of EMNLP,pages 887?896.Mathias Creutz and Krista Lagus.
2007.
Unsuper-vised models for morpheme segmentation and mor-phology learning.
ACM Transactions on Speech andLanguage Processing, 4(1).Jesus-Luis Cunchillos, Juan-Pablo Vita, and Jose-A?ngel Zamora.
2002.
Ugaritic data bank.
CD-ROM.Gregoria del Olo Lete and Joaqu?
?n Sanmart??n.
2004.A Dictionary of the Ugaritic Language in the Alpha-betic Tradition.
Number 67 in Handbook of OrientalStudies.
Section 1 The Near and Middle East.
Brill.Pascale Fung and Kathleen McKeown.
1997.
Find-ing terminology translations from non-parallel cor-pora.
In Proceedings of the Annual Workshop onVery Large Corpora, pages 192?202.S.
Geman and D. Geman.
1984.
Stochastic relaxation,gibbs distributions and the bayesian restoration ofimages.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 12:609?628.Alan Groves and Kirk Lowery, editors.
2006.
TheWestminster Hebrew Bible Morphology Database.Westminster Hebrew Institute, Philadelphia, PA,USA.Jacques B. M. Guy.
1994.
An algorithm for identifyingcognates in bilingual wordlists and its applicabilityto machine translation.
Journal of Quantitative Lin-guistics, 1(1):35?42.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of theACL/HLT, pages 771?779.Robert Hetzron, editor.
1997.
The Semitic Languages.Routledge.H.
Ishwaran and J.S.
Rao.
2005.
Spike and slab vari-able selection: frequentist and Bayesian strategies.The Annals of Statistics, 33(2):730?773.Kevin Knight and Richard Sproat.
2009.
Writing sys-tems, transliteration and decipherment.
NAACL Tu-torial.K.
Knight and K. Yamada.
1999.
A computa-tional approach to deciphering unknown scripts.
InACL Workshop on Unsupervised Learning in Natu-ral Language Processing.Kevin Knight, Anish Nair, Nishit Rathod, and KenjiYamada.
2006.
Unsupervised analysis for deci-pherment problems.
In Proceedings of the COL-ING/ACL, pages 499?506.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
InProceedings of the ACL-02 workshop on Unsuper-vised lexical acquisition, pages 9?16.Grzegorz Kondrak.
2001.
Identifying cognates byphonetic and semantic similarity.
In Proceeding ofNAACL, pages 1?8.Grzegorz Kondrak.
2009.
Identification of cognatesand recurrent sound correspondences in word lists.Traitement Automatique des Langues, 50(2):201?235.John B. Lowe and Martine Mazaudon.
1994.
The re-construction engine: a computer implementation ofthe comparative method.
Computational Linguis-tics, 20(3):381?417.Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated english and germancorpora.
In Proceedings of the ACL, pages 519?526.Andrew Robinson.
2002.
Lost Languages: TheEnigma of the World?s Undeciphered Scripts.McGraw-Hill.William M. Schniedewind and Joel H. Hunt.
2007.
APrimer on Ugaritic: Language, Culture and Litera-ture.
Cambridge University Press.Mark S. Smith, editor.
1955.
Untold Stories: The Bibleand Ugaritic Studies in the Twentieth Century.
Hen-drickson Publishers.Benjamin Snyder and Regina Barzilay.
2008.
Cross-lingual propagation for morphological analysis.
InProceedings of the AAAI, pages 848?854.Wilfred Watson and Nicolas Wyatt, editors.
1999.Handbook of Ugaritic Studies.
Brill.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2000.
Inducing multilingual text analysistools via robust projection across aligned corpora.In Proceedings of HLT, pages 161?168.1057
