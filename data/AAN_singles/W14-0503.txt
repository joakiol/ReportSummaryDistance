Proc.
of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 9?13,Gothenburg, Sweden, April 26 2014. c?2014 Association for Computational LinguisticsBayesian inference as a cross-linguistic word segmentation strategy:Always learning useful thingsLawrence Phillips and Lisa PearlDepartment of Cognitive SciencesUniversity of California, Irvine{lawphill, lpearl}@uci.eduAbstractStatistical learning has been proposed as one of theearliest strategies infants could use to segmentwords out of their native language because it doesnot rely on language-specific cues that must bederived from existing knowledge of the words inthe language.
Statistical word segmentationstrategies using Bayesian inference have beenshown to be quite successful for English(Goldwater et al.
2009), even when cognitivelyinspired processing constraints are integrated intothe inference process (Pearl et al.
2011, Phillips &Pearl 2012).
Here we test this kind of strategy onchild-directed speech from seven languages toevaluate its effectiveness cross-linguistically, withthe idea that a viable strategy should succeed ineach case.
We demonstrate that Bayesian inferenceis indeed a viable cross-linguistic strategy,provided the goal is to identify useful units of thelanguage, which can range from sub-wordmorphology to whole words to meaningful wordcombinations.1 IntroductionWord segmentation is one of the first taskschildren must complete when learning theirnative language, and infants are able to identifywords in fluent speech by around 7.5 months(Jusczyk & Aslin 1995; Echols et al.
1997;Jusczyk et al., 1993)).
Proposals for learningstrategies that can accomplish this (Saffran et al.1996) have centered on language-independent cuesthat are not derived from existing knowledge ofwords.
Bayesian inference is a statistical strategyoperating over transitional probability that has beenshown to be successful for identifying words inEnglish, whether the salient perceptual units arephonemes (Goldwater et al.
2009 [GGJ], Pearl et al.2011 [PGS]) or syllables (Phillips & Pearl 2012[P&P]), and whether the inference process isoptimal (GGJ, PGS) or constrained by cognitivelimitations that children may share (PGS, P&P).
Itmay, however, be the case that these strategies workwell for English, but not other languages (Fourtassiet al.
2013).
Therefore, we evaluate this samelearning strategy on seven languages with differentlinguistic profiles: English, German, Spanish, Italian,Farsi, Hungarian, and Japanese.
If Bayesianinference is a viable strategy for word segmentation,it should succeed on all languages.
While someattempts have been made to evaluate Bayesian wordsegmentation strategies on languages other thanEnglish (e.g., Sesotho: Johnson 2008, Blanchard etal.
2010), this is the first evaluation on a significantrange of languages that we are aware of.We assume the relevant perceptual units aresyllables, following previous modeling work(Swingly 2005, Gambell & Yang 2006, Lignos &Yang 2010, Phillips & Pearl 2012) that draws fromexperimental evidence that infants younger than 7.5months are able to perceive syllables but notphonemes (Werker & Tees 1984, Juszyck & Derrah1987, Eimas 1999).
We demonstrate that Bayesianword segmentation is a successful cross-linguisticlearning strategy, provided we define success in amore practical way than previous wordsegmentation studies have done.
We consider asegmentation strategy successful if it identifies unitsuseful for subsequent language acquisitionprocesses (e.g., meaning learning, structurelearning).
Thus, not only is the orthographic goldstandard typically used in word segmentation tasksacceptable, but also productive morphology andcoherent chunks made up of multiple words.
Thisserves as a general methodological contributionabout the definition of segmentation success,especially when considering that the meaningfulunits across the world?s languages may vary.2 The Bayesian learning strategyBayesian models are well suited to questions oflanguage acquisition because they distinguishbetween the learner?s pre-existing beliefs (prior)9and how the learner evaluates incoming data(likelihood), using Bayes?
theorem:( | )   ( | ) ( )The Bayesian learners we evaluate are theoptimal learners of GGJ and the constrainedlearners of PGS.
All learners are based on thesame underlying models from GGJ.
The first ofthese models assumes independence betweenwords (a unigram assumption) while the secondassumes that a word depends on the word beforeit (a bigram assumption).
To encode theseassumptions into the model, GGJ use a DirichletProcess (Ferguson, 1973), which supposes thatthe observed sequence of words w1 ?
wn isgenerated sequentially using a probabilisticgenerative process.
In the unigram case, theidentity of the ith word is chosen according to:(    |       )( )    ( )(1)where ni-1(w) is the number of times w appears inthe previous i ?
1 words, ?
is a free parameter ofthe model, and P0 is a base distributionspecifying the probability that a novel word willconsist of the perceptual units x1 ?
xm:(       )  ?
(  )(2)In the bigram case, a hierarchical DirichletProcess (Teh et al.
2006) is used.
This modeladditionally tracks the frequencies of two-wordsequences and is defined as:(    |)()    ( )(  )(3)(    )( )    ( )(4)where ni-1(w?,w) is the number of times thebigram (w?,w) has occurred in the first i ?
1words, bi-1(w) is the number of times w hasoccurred as the second word of a bigram, bi-1 isthe total number of bigrams, and ?
and ?
are freemodel parameters.11Parameters for the unigram and bigram models underlyingall learners were chosen to maximize the performance of theBatchOpt learner, discussed below.
English: ?=1, ?=1,?=90; German: ?=1, ?=1, ?=100; Spanish: ?=1, ?=200,?=50; Italian: ?=1, ?=20, ?=200; Farsi: ?=1, ?=200, ?=500;Hungarian: ?=1, ?=300, ?=500; Japanese: ?=1, ?=300,?=100In both the unigram and bigram case, themodel implicitly incorporates preferences forsmaller lexicons by preferring words that appearfrequently (due to (1) and (3)) and preferringshorter words in the lexicon (due to (2) and (4)).The BatchOpt learner for this model is takenfrom GGJ and uses Gibbs sampling (Geman &Geman 1984) to run over the entire input in asingle batch, sampling every potential wordboundary 20,000 times.
We consider this learner?optimal?
in that it is unconstrained by cognitiveconsiderations.
We also evaluate the constrainedlearners developed by PGS that incorporateprocessing and memory constraints into thelearning process.The OnlineOpt learner incorporates a basicprocessing limitation: linguistic processingoccurs online rather than in batch after a periodof data collection.
Thus, the OnlineOpt learnerprocesses one utterance at a time, rather thanprocessing the entire input at once.
This learneruses the Viterbi algorithm to converge on thelocal optimal word segmentation for the currentutterance, conditioned on all utterances seen sofar.The OnlineSubOpt learner is similar to theOnlineOpt learner in processing utterancesincrementally, but is motivated by the idea thatinfants are not optimal decision-makers.
Infantsmay not always select the best segmentation, andinstead sample segmentations based on theirperceived probabilities.
The OnlineSubOptlearners will often choose the best segmentationbut will occasionally choose less likelyalternatives, based on the probability associatedwith each segmentation.
The Forward algorithmis used to compute the likelihood of all possiblesegmentations and then a segmentation is chosenbased on the resulting distribution.The OnlineMem learner also processes dataincrementally, but uses a Decayed Markov ChainMonte Carlo algorithm (Marthi et al.
2002) toimplement a kind of limited short-term memory.This learner is similar to the original GGJ ideal(BatchOpt) learner in that it uses something likeGibbs sampling.
However, the OnlineMemlearner does not sample all potential boundaries;instead, it samples some number s of previousboundaries using the decay function b-d to selectthe boundary to sample; b is the number ofpotential boundary locations between theboundary under consideration bc and the end of10the current utterance while d is the decay rate.Thus, the further bc is from the end of the currentutterance, the less likely it is to be sampled.Larger values of d indicate a stricter memoryconstraint.
All our results here use a set, non-optimized value for d of 1.5, which was chosento implement a heavy memory constraint (e.g.,90% of samples come from the current utterance,while 96% are in the current or previousutterances).
Having sampled a set of boundaries2,the learner can then update its beliefs about thoseboundaries and subsequently update its lexicon.3 Cross-linguistic inputWe evaluate the Bayesian learner on inputderived from child-directed speech corpora inseven languages: English, German, Spanish,Italian, Farsi, Hungarian and Japanese.
Allcorpora were taken from the CHILDES database(MacWhinney, 2000).
When corpora wereavailable only in orthographic form, they werefirst converted into the appropriate phonemicform.
Afterwards, the corpora were syllabified.Where possible, we utilized adult syllabificationjudgments.
All other words were syllabifiedusing the Maximum-Onset principle, whichstates that the beginning of a syllable should beas large as possible, without violating thelanguage?s phonotactic constraints.Our corpora vary in a number of importantways.
Although we attempt to limit our corporato early child-directed speech, some of ourcorpora contain speech directed to children asold as age five (e.g.
Farsi).
Many of our corporado, however, consist entirely of early child-directed speech (e.g., English, Japanese).Similarly, the same amount of data is not alwayseasily available for each language.
Our shortestcorpus (German) consists of 9,378 utterances,while the longest (Farsi) consists of 31,657.The languages themselves also contain manydifferences that potentially affect syllable-basedword segmentation.
While our English andHungarian corpora contain 2,330 and 3,029unique syllables, respectively, Japanese andSpanish contain only 526 and 524, respectively.Some languages may be easier to segment thanothers based on distributional factors.
Fourtassi2  All OnlineMem learners sample s=20,000 boundariesper utterance.
For a syllable-based learner, this works out toapproximately 74% less processing than the BatchOptlearner (P&P).et al.
(2013) show, for example, that English hasless ambiguous segmentation than Japanese.
Inaddition, the languages also have differences intheir syntax and morphology.
For example,Hungarian and Japanese are both agglutinativelanguages that have more regular morphologicalsystems, while English, German, Spanish, Italianand Farsi are all fusional languages to varyingdegrees.
If a language has regular morphology,an infant might reasonably segment outmorphemes rather than words.
This highlightsthe need for a more flexible metric ofsegmentation performance: A segmentationstrategy which identifies units useful for laterlinguistic analysis should not be penalized.4 Learning results & discussionWe analyze our results in terms of word token F-scores, which is the harmonic mean of tokenprecision and recall, where precision is theprobability that a word segmented by the modelis a true word (# identified true / # identified) andrecall measures the probability that any trueword was correctly identified (# identified true /total # true).
F-scores range from 0 to 100, withhigher values indicating better performance.Performance on all languages is presented inTable 1.
An error analysis was conducted wherewe systematically counted the following?reasonable errors?
as successful segmentation:(i) Mis-segmentations resulting in real words.For example, the word ?alright?
might beoversegmented as ?all right?, resulting in twoactual English words.
Most languages showerrors of this type, with more occurring for thebigram model, with the least in English(BatchOpt: 4.52%) and most in Spanish(BatchOpt: 23.97%).
We restrict these errors towords which occur minimally ten times in thecorpus in order to avoid accepting errors in thecorpora or nonsense syllables as real words.
(ii) Productive morphology.
Given the syllabicnature of our corpora, only syllabic morphologycan be identified.
Languages like English,Spanish and Italian have relatively few errorsthat produce morphemes (e.g., BatchOpt: 0.13%,0.05%, and 1.13% respectively), while Japanese,with more syllabic morphology has many sucherrors (e.g., BatchOpt: 4.69%).11English German Spanish Italian Farsi Hungarian JapaneseUnigramBatchOpt 55.70 73.43 64.28 70.48 72.48 64.01 69.11OnlineOpt 60.71 58.41 74.98 65.05 75.66 56.77 71.56OnlineSubOpt 65.76 70.95 77.15 66.48 74.89 60.21 71.73OnlineMem 58.68 73.85 67.78 66.77 67.31 60.07 70.49BigramBatchOpt 80.19 84.15 80.34 79.36 76.01 70.87 73.11OnlineOpt 78.09 82.08 82.71 75.78 79.23 69.67 73.36OnlineSubOpt 80.44 82.03 80.75 73.59 67.54 65.48 66.14OnlineMem 89.58 88.83 83.27 74.08 73.98 69.48 73.24Table 1.
Token F-scores (presented as percents, from 0 to 100) for each learner across every language.Higher Token F-scores indicate better performance.
(iii) Common sequences of function words.For example, a learner might identify ?is that a?as a single word, ?isthata?.
These errors tend tobe more common for unigram learners thanbigram learners, which makes sense from astatistical standpoint since the unigram learneris unable to account for commonly occurringsequences of words and must do so by positingthe collocation as a single word.
Still, functionword sequence errors are relatively uncommonin every language except German (e.g.,BatchOpt: 21.73%)Table 2 presents common examples of eachtype of acceptable error in English.True Word(s) Model OutputReal words  something some   thingalright all   rightMorphology  going go   ingreally rea   llyFunctionwordyou   can youcanare   you areyouTable 2.
Example reasonable errors of eachtype from English that result in real words,morphology, or function word collocations.Generally speaking, the bigram learners tendto outperform the unigram learners, suggestingthat the knowledge that words depend onprevious words continues to be a useful one (asGGJ, PGS, and P&P found for English),though this difference may be small for somelanguages (e.g., Farsi, Japanese).
Overall,performance for English and German is veryhigh (best score: ~90%), while for otherlanguages the learners tend to fare less well(best score: 70-83%), though still quite good.These results match previous work whichindicated that English is particularly easy tosegment compared to other languages (Johnson2008; Blanchard et al.
2010; Fourtassi et al.2013)Importantly, the goal of early wordsegmentation is not for the infant to entirelysolve word segmentation, but to get the wordsegmentation process started.
Given this goal,Bayesian word segmentation seems effectivefor all these languages.
Moreover, because ourlearners are looking for useful units, which canbe realized in different ways across languages,they can identify foundational aspects of alanguage that are both smaller and larger thanorthographic words.5 ConclusionWe have demonstrated that Bayesian wordsegmentation performs quite well as an initiallearning strategy for many different languages,so long as the learner is measured by howuseful the units are that it identifies.
This notonly supports Bayesian word segmentation asa viable cross-linguistic strategy, but alsosuggests that a useful methodological norm forword segmentation research should be howwell it identifies units that can scaffold futurelanguage acquisition.
By taking into accountreasonable errors that identify such units, webring our model evaluation into alignment withthe actual goal of early word segmentation.12ReferencesBlanchard, D., Heinz, J., & Golinkoff, R. 2010.Modeling the contribution of phonotactic cues tothe problem of word segmentation.
Journal of childlanguage, 37(3), 487.Echols, C.H., Crowhurst, M.J. & Childers, J.B. 1997.The perception of rhythmic units in speech byinfants and adults.
Journal of Memory andLanguage, 36, 202-225.Eimas, P.D.
1999.
Segmental and syllabicrepresentations in the perception of speech byyoung infants.
Journal of the Acoustical Society ofAmerica, 105(3), 1901-1911.Fourtassi, A., B?rschinger, B., Johnson, M., &Dupoux, E. 2013.
Whyisenglishsoeasytosegment?Proceedings of the Fourth Annual Workshop onCognitive Modeling and Computational Linguistics,1-10.Gambell, T. & Yang, C. 2006.
Word Segmentation:Quick but not dirty.
Manuscript.
New Haven: YaleUniversityGeman S. & Geman D. 1984.
Stochastic Relaxation,Gibbs Distributions, and the Bayesian Restorationof Images.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 6, 721-741.Goldwater, S., Griffiths, T. & Johnson, M. 2009.
ABayesian framework for word segmentation:Exploring the effects of context.
Cognition 112(1),21-54.Johnson, M. 2008.
Unsupervised word segmentationfor Sesotho using adaptor grammars.
Proceedingsof the Tenth Meeting of the ACL Special InterestGroup on Computational Morphology andPhonology, 20-27.Jusczyk, P.W.
& Derrah, C. 1987.
Representation ofspeech sounds by young infants.
DevelopmentalPsychology, 23(5), 648-654.Jusczyk, P.W., Cutler, A.
& Redanz, N.J. 1993.Infants?
preference for the predominant stresspattern of English words.
Child Development,64(3), 675-687.Jusczyk, P.W.
& Aslin, R.N.
1995.
Infants?
detectionof the sound patterns of words in fluent speech.Cognitive Psychology, 29, 1-23.Lignos, C. & Yang, C. 2010.
Recession segmentation:Simpler online word segmentation using limitedresources.
Proceedings of the FourteenthConference on Computational Natural LanguageLearning, 88-97.MacWhinney, B.
2000.
The CHILDES project: Toolsfor analyzing talk.
Mahwah, NJ: LawrenceErlbaum Associates.Marthi, B., Pasula, H., Russell, S. & Peres, Y., et al.2002.
Decayed MCMC filtering.
In Proceedings of18th UAI, 319-326.Pearl, L., Goldwater, S., & Steyvers, M. 2011.
OnlineLearning Mechanisms for Bayesian Models ofWord Segmentation, Research on Language andComputation, special issue on computationalmodels of language acquisition.Phillips, L. & Pearl, L. 2012.
?Less is more?
inBayesian word segmentation: When cognitivelyplausible learners outperform the ideal.
InProceedings of the 34th Annual Conference of theCognitive Science Society.Saffran, J.R., Aslin, R.N.
& Newport, E.L. 1996.Statistical learning by 8-Month-Old Infants.Science, 274, 1926-1928.Swingley, D. 2005.
Statistical clustering and thecontents of the infant vocabulary.
CognitivePsychology, 50, 86-132.Teh, Y., Jordan, M., Beal, M., & Blei, D. 2006.Heirarchical Dirichlet processes.
Journal of theAmerican Statistical Association, 101(476), 1566-1581.Werker, J.F.
& Tees, R.C.
1984.
Cross-languagespeech perception: Evidence for perceptualreorganization during the first year of life.
InfantBehavior & Development, 7, 49-63.13
