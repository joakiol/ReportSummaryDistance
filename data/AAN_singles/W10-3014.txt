Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 100?105,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsLearning to Detect Hedges and their Scope Using CRFQi Zhao, Chengjie Sun, Bingquan Liu, Yong ChengHarbin Institute of Technology, HITHarbin, PR China{qzhao, cjsun, liubq, ycheng}@insun.hit.edu.cnAbstractDetecting speculative assertions is essentialto distinguish the facts from uncertaininformation for biomedical text.
This paperdescribes a system to detect hedge cues andtheir scope using CRF model.
HCDic featureis presented to improve the system perfor-mance of detecting hedge cues on BioScopecorpus.
The feature can make use of cross-domain resources.1 IntroductionGeorge Lakoff (1972) first introduced linguistichedges which indicate that speakers do not backup their opinions with facts.
Later other linguistsfollowed the social functions of hedges closely.Interestingly, Robin Lakoff (1975) introducesthat hedges might be one of the ?women?slanguage features?
as they have higher frequencyin women?s languages than in men?s.In the natural language processing domain,hedges are very important, too.
Along with therapid development of computational andbiological technology, information extractionfrom huge amount of biomedical resourcebecomes more and more important.
While theuncertain information can be a noisy factorsometimes, affecting the performance ofinformation extraction.
Biomedical articles arerich in speculative, while 17.70% of thesentences in the abstracts section of theBioScope corpus and 19.44% of the sentences inthe full papers section contain hedge cues(Vincze et al, 2008).
In order to distinguish factsfrom uncertain information, detecting speculativeassertions is essential in biomedical text.Hedge detection is paid attention to in thebiomedical NLP field.
Some researchers regardthe problem as a text classification problem (asentence is speculative or not) using simplemachine learning techniques.
Light et al (2004)use substring matching to annotate speculation inbiomedical text.
Medlock and Briscoe (2007)create a hedging dataset and use an SVMclassifier and get to a recall/precision Break-Even Point (BEP) of 0.76.
They report that thePOS feature performs badly, while lemmafeature works well.
Szarvas (2008) extends thework of Medlock and Briscoe with featureselection, and further improves the result to aBEP of 0.85 by using an external dictionary.Szarvas concludes that scientific articles containmultiword hedging cues more commonly, andthe portability of hedge classifiers is limited.Halil Kilicoglu and Sabine Bergler (2008)propose an algorithm to weight hedge cues,which are used to evaluate the speculativestrength of sentences.
Roser Morante and WalterDaelemans (2009) introduce a metalearningapproach to process the scope of negation, andthey identify the hedge cues and their scope witha CRF classifier based on the original work.They extract a hedge cues dictionary as well, butdo not combine it with the CRF model.In the CoNLL-2010 shared task (Farkas et al,2010), there are two subtasks for worldwideparticipants to choose:?
Task 1: learning to detect sentencescontain-ing uncertainty.?
Task 2: learning to resolve the in-sentence scope of hedge cues.This paper describes a system using CRFmodel for the task, which is partly based onRoser Morante and Walter Daelemans?
work.2 Hedges in the training dataset ofBioScope and Wikipedia CorpusTwo training datasets, the BioScope and Wiki-pedia corpus are provided in the CoNLL-2010shared task.
BioScope consists of two parts, fullarticles and abstracts collected from biomedicalpapers.
The latter is analyzed for having largerscale and more information of hedges.In Table 1, the percentage of the speculativesentences in the abstracts section of BioScopecorpus is the same as Vincze et al (2008)reported.
We can estimate 1.28 cue words persentence, meaning that each sentence usually justhas one hedge cue.
The statistics in Table 1 also100indicate that a hedge cue appears 26.7 times onaverage.Dataset ITEM #Sentences 11871Certain sentences 9770Uncertainsentences2101(17.7%)Hedge cues 2694cues# per sentence 1.28Different hedgecues143AbstractsofBioScopeMax length of thecues4Sentences 11111Certain sentences 8627Uncertainsentences2484(22.4%)weasel cues 3133Different weaselcues1984WikipediaMax length of thecues13 wordsTable 1: Statistics about the abstracts section ofthe BioScope corpus and Wikipedia corpus.We extract all the hedge cues from theabstracts section of BioScope corpus, getting 143different hedge cues and 101 cues with ignoringmorphological changes.
The maximum length ofthe cues is 4, with 1.44 words per hedge cue.This suggests that most hedge cues happen to bea single word.
We assume that hedge cues set isa limited one in BioScope corpus.
Most hedgecues could be identified if the known dataset ofhedge cues is large enough.
The cue wordscollected from the BioScope corpus play animportant role in the speculative sentencesdetection.In contrast to the biomedical abstracts, theweasel cues on Wikipedia corpus make a littledifference.
Most weasel cues consist of morethan one word, and usually appear once.
Thisleads to different results in our test.A hedge cue word may appear in the non-speculative sentences.
Occurrences of the fourtypical words in speculative and non-speculativesentences are counted.As shown in Table 2, the cue words can bedivided into two classes generally.
The hedgecue words ?feel?
and ?suggesting?, which aregrouped as one class, only act as hedge cues withnever appearing in the non-speculative sentences.While ?may?
and ?or?
appear both in thespeculative and non-speculative sentences, whichare regard as the other one.
Moreover, we treatthe words ?may?
and ?or?
in the same classdifferently, while ?may?
is more likely to be ahedge cue than ?or?.
The treatment is alsounequal between ?feel?
and ?suggesting?.
In thetraining datasets, the non-S#/S# ratio can give aweight to distinguish the words in each class.After all, we can divide the hedge cues into 4groups.word S# non-S#feel 1 0suggesting 150 0may 516 1or 118 6218Table 2: Statistics of cue words.
(S# short for theoccurrence times in speculative sentences, non-S# for the count in non-speculative ones)3 MethodsConditional random fields (CRF) model wasfirstly introduced by Lafferty et al (2001).
CRFmodel can avoid the label bias problem ofHMMs and other learning approaches.
It wasapplied to solve sequence-labeling problems, andhas shown good performance in NER task.
Weconsider hedge cues detection as some kind ofsequence-labeling problem, and the model willcontribute to a good result.We use CRF++ (version 0.51) to implementthe CRF model.
Cheng Yong, one of our teammembers has evaluated the several widespreadused CRF tool kits, and he points out thatCRF++ has better precision and recall but longertraining time.
Fortunately, the training time costof BioScope corpus is acceptable.
In our system,all the data training and testing processing stepcan be completed within 8 minutes (Intel Xeon2.0GHz CPU, 6GB RAM).
It is likely due to thesmall scale of the training dataset and the limitedtypes of the annotation.To identify sentences in the biomedical textsthat contain unreliable or uncertain information(CoNLL-2010 shared task1), we start with hedgecues detection:?
If one or more than one hedge cues aredetected in the sentence, then it will beannotated ?uncertain??
If not, the sentence will be tagged as?certain?.1013.1 Detecting hedge cuesThe BioScope corpus annotation guidelines 1show that most typical instances of keywords canbe grouped into 4 types as Auxiliaries, Verbs ofhedging or verbs with speculative content,Adjectives or adverbs, and Conjunctions.
So thePOS (part-of-speech) is thought to be the featurereasonably.
Lemma feature of the word andchunk features are also considered to improvesystem performance.
Chunk features may help tothe recognition of biomedical entity boundaries.GENIA Tagger (Tsuruoka et al, 2005) is em-ployed to obtain part-of-speech (POS) features,chunk features and lemma features.
It works wellfor biomedical documents.In the biomedical abstracts section of Bio-Scope corpus, the hedge cues are collected into adictionary (HCDic, short for the Hedge CuesDictionary).
As mentioned in section 2, onehedge cue appears 26.7 times on average, and weassume the set of hedge cues is limited.
TheHCDic consist of 143 different hedge cuesextracted from the abstracts.
The dictionary(HCDic) extracted from the corpus is veryvaluable for the system.
We can focus onwhether the word such as ?or?
listed in table 2 isa hedge cue or not.
The cue words in HCDic aredivided into 4 different levels with the non-S#/S#ratio.The four types are described as ?L?, ?H?,?FL?
and ?FH?.
?L?
shows low confidence ofthe cue word being a hedge cue, while ?H?indicates high confidence about it.
The prefix ?F?for ?FL?/?FH?
shows false negatives mayhappen to the cue word in HCDic.
The thresholdfor the non-S#/S# ratio to distinguish ?FL?
typefrom ?FH?
is set 1.0.
As the non-S#/S# ratio of?L?
and ?H?
is always zero, we set the hedge cuewhose S# is more than 5 as ?H?
type as shown intable 3.
The four types are added into the HCDicalong with the hedge cues,In our experiment, HCDic types of wordsequence are tagged as follows:?
If words are found in HCDic usingmaximum matching method, label themwith their types in HCDic.
For hedges ofmulti-word, label them with BI schemewhich will be described later.?
If not, tag the words as ?O?
type.1http://www.inf.u-szeged.hu/rgai/bioscopeThe processing assigns each token of asentence with an HCDic type.
The BIO types foreach token are involved as features for the CRF.The HCDic can be expanded to a larger scale.Hedge cues extracted from different corpora canbe added into HCDic, and regular expression ofhedge cues can be used, too.
This will be helpfulto the usage of cross-domain resources.word S# non-S# typefeel 1 0 Lsuggesting 150 0 Hmay 516 1 FHor 118 6218 FLTable 3: Types of the HCDic words.
(S# andnon-S# have the same meaning as in Table 2)The features F (F stands for all the Features)including unigram, bigram, and trigram types isused for CRF as follows:F(n)(n=-2,-1,0,+1,+2)F(n-1)F(n)(n=-1,0,+1,+2)F(n-2)F(n-1)F(n) (n=0,+1,+2)Where F(0) is the current feature, F(-1) is theprevious one, F(1) is the following one, etc.We regard each word in a sentence as a tokenand each token is tagged with a cue-label.
TheBIO scheme is used for tagging multiword hedgecues, such as ?whether or not?
in our HCDic.where B-cue (tag for ?whether?)
represents thatthe token is the start of a hedge cue, I-cue (tagfor ?or?, ?not?)
stands for the inside of a hedgecue, and O (tag for the other words in thesentence) indicates that the token does notbelong to any hedge cue.We also have the method tested on Wikipediacorpus with a preprocessing of the HCDic.Section 2 reports that most weasel cues inWikipedia corpus are multiword, and usuallyappear once.
Different from our assumption inBioScope corpus, the set of weasel cues seemsnumerous.
The HCDic of Wikipedia would benot so valuable if it tags few tokens for a newgiven text.
To prevent these from happening, apreprocessing of the HCDic is taken.Most of the hedge cues in Wikipedia corpusaccord with the structure of ?adjective + noun?e.g.
?many persons?.
Although most cue wordsappear just once, the adjective usually happens tobe the same, and we call them core words.Therefore, the hedge cue dictionary (HCDic) canbe simplified with the core words.
It helps to102reduce the scale of the hedges cues from 1984cues down to 170.
Then, we process theWikipedia text the same way as the BioScopecorpus.3.2 Detecting scope of hedge cuesThis phase (for CoNLL-2010 shared task 2) isbased on Roser Morante and Walter Daelemans?scope detection system.CRF model is applied in this part, too.
Theword, POS, lemma, chunk and HCDic tags arealso applied to be the features as in the step ofhedge cues detection.
In section 3.1, we canobtain the hedge cues in a sentence.
The scoperelies on its cue vary much.
We make the BIOschema of detected hedge cues to be theimportant features of this part.
Besides, thesentences tagged as ?certain?
type are neglectedin this step.Here is an example of golden standard ofscope label.<sentence id="S5.149"> We <xcope id="X5.149.3"><cue ref="X5.149.3" type= "specula-tion">propose </cue> that IL-10-producing Th1 cells<xcope id="X5.149.2"> <cue ref="X5.149.2"type= "speculation" >may</cue> be the essentialregulators of acute infection-induced inflammation</xcope> and that such ?self-regulating?
Th1 cells<xcope id= "X5.149.1"> <cue ref= "X5.149.1"type= "speculation" >may</cue> be essential forthe infection to be cleared without inducingimmune-mediated pathology </xcope> </xcope>.As shown, each scope is a block with abeginning and an end, and we refer to thebeginning of scope as scope head (<xcope?>),and the end of the scope as scope tail(</xcope>).The types of the scope are labeled as:1.
Label the token next to scope head as?xcope-H?
( e.g.
propose, may )2.
Tag the token before scope tail as ?xcope-T?(e.g.
pathology for both scopes)3.
The other words tag ?O?
, including thewords inside the scope and out of it.
Thisis very different from the BIO scheme.The template for each feature is the same as insection 3.1.Following are our rules to form the scope of ahedge:1.
Most hedge cues have only one scope tag,meaning there is one-to-one relationshipbetween hedge cue and its scope.2.
The scope labels may be nested.3.
The scope head of the cue words appearsnearest before hedge cue.4.
The scope tail appears far from the cueword.5.
The most frequent head/tail positions of thescope are shown in Table 4.a) The scope head usually is just beforethe cue words.b) The scope tail appears in the end of thesentence frequently.Scopes of hedge cues in BioScope corpusshould be found for the shared task.
The trainingdataset of abstract part is analyzed for its largerscaleitem Following stringswith high frequency %1scopehead<cue...>(cue words) 0.861?.?
(sentence end) 0.695</xcope>(another scope tail) 0.1442scopetail?,?
?;?
?:?
0.078Table 4: Statistics of the strings nearby the scopehead and tail.
Item 1 shows the word followscope head, and item 2 shows the frequent wordsnext to the scope tail.We analyze the words around the scope headand the scope tail.
The item 1 in Table 4 showsthat 86.1% of the following words of the scopehead are hedge cues.
Other following words notlisted are less than 1%, according to ourstatistics.
The item 2 lists the strings with highfrequency next to the scope tail as well.
The first2 words in item 2 can be combined sometimes,so the percentage of scope tail at the end of thesentence can be more than 80%.
The stringsahead of scope head and tail not listed are alsocounted, but they do not give such valuableinformation as the two items listed in Table 4.Therefore, when the CRF model gives lowconfidence, we just set the most probablepositions of scope head and tail.For the one-to-one relationship between hedgecues and their scopes, we make rules to insureeach cue has only one scope, including the scopehead and scope tail.103Rule 1: if more than one scope heads or tailsare predicted, we get rid of the farther head ornearer tail.Rule 2: if none of scope head or tail is pre-dicted, the head is set to the word just before thecue words; the tail is set at the end of thesentence.Rule 3: if one scope head and one tail arepredicted, we consider them the result of scopedetection.4 ResultsOur experiments are based on the CoNLL-2010shared task?s datasets, including BioScope andWikipedia corpus.
All the experiments forBioScope use abstracts and full papers fortraining data and the provided evaluation fortesting.We employ CRF model to detect the hedgecues in the BioScope.
The experiments arecarried out on different feature sets: wordssequence with the chunk feature only, lemmafeature only and POS feature only.
The effect ofthe HCDic feature is also evaluated.Features prec.
recall F-scoreChunk only 0.7236 0.6275 0.6721Lemma only 0.7278 0.6103 0.6639POS only 0.7320 0.6208 0.6718WithoutHCDic0.7150 0.6447 0.6781ALL 0.7671 0.7393 0.7529Table 5: Results at hedge cue-levelAs described in section 1 of this paper, thefeature of POS may be not so significant as thelemma, but we do not agree with this point ofview for given POS feature's better performancein F-score (in Table 5).
The interesting cue-levelresult does not go into for time limitations.
TheF-score of the three features, chunk, lemma andPOS are approximately equal.
When all of thethree features are used for CRF model, theperformance is not improved so significantly.The recall rate is a bit low in the experimentwithout HCDic features.
As shown in Table 5,the feature of HCDic is effective to get a betterscore both in precision rate and in recall rate.
Asour assumption, hedges in the evaluation datasetare limited, too.
Most of them along with somenon-hedges can be tagged with HCDic.
Then thetag could contribute to a good recall.
It also helpsthe classifier to focus on whether the words with?L?, ?FL?, and ?FH?
are hedge cues or not,which will be good for a better precision.With detected hedge cues, we can get senten-ces containing uncertainty for the shared task 1.A sentence is tagged as ?uncertain?
type if anyhedge cue is found in it.precision recall F-scoreWithoutHCDic 0.8965 0.7898 0.8398ALL  0.8344 0.8481 0.8412Table 6: Evaluation result of task 1Statistics in Table 6 show that even poorperformance in cue-level test can get asatisfactory F-score of speculative sentencesdetection as well.
It seems that hedges detectionat cue-level is not proportionate to the sentence-level.
Think about instance of more than onecues in a sentence such as the example of goldenstandard in section 3.2, the sentence will betagged even if only one hedge cue has beenidentified (lower recall at cue-level).
Moreover,in the speculative sentence with one hedge cue,false positives (lower precision at cue-level) canalso lead to the correct result at sentence-level.The method is also tested on Wikipedia corpus,using provided training dataset and evaluationdata.
The method has a bad performance in ourclose test.
The results are listed in Table 7.As talked in section 2, hedges in Wikipediacorpus are very different from in BioScopecorpus.
Besides, the string matching method forsimplified HCDic is not so effective.
The useful-ness of HCDic is not so significant for a goodrecall in Wikipedia corpus.dataset precision recall F-scoreWikipedia 0.7075 0.2001 0.3120BioScope 0.7671 0.7393 0.7529Table 7: Results of weasel/hedge detection inWikipedia and BioScope corpus.In CoNLL-2010 shared task 2, the evaluationresult shows our precision, recall and F-score are34.8%, 41% and 37.6%.
The performance ofidentifying the scope relies on the cue-leveldetection.
Therefore, the false positive and falsenegatives of hedge cues can lead to recognitionerrors.
The result shows that our lexical-levelmethod for the semantic problem is limited.
Forthe time constraints, we do not probe deeply.1045 ConclusionsThis paper presents an approach for extractingthe hedge cues and their scopes in BioScopecorpus using two CRF models for CoNLL-2010shared task.
In the first task, the HCDic feature isproposed to improve the system performances,getting better performance (84.1% in F-score)than the baseline.
The HCDic feature is alsohelpful to make use of cross-domain resources.The comparison of our methods based onbetween BioScope and Wikipedia corpus isgiven, which shows that ours are good at hedgecues detection in BioScope corpus but short atthe in Wikipedia corpus.
To detect the scope ofhedge cues, we make rules to post process thetext.
For future work, we will look forward toconstructing regulations for the HCDic toimprove our system.ReferencesRich?rd Farkas, Veronika Vincze, Gy?rgy M?ra,J?nos Csirik, and Gy?rgy Szarvas.
2010.
TheCoNLL-2010 Shared Task: Learning to DetectHedges and their Scope in Natural Language Text.In Proceedings of the Fourteenth Conferenceon Computational Natural Language Learning(CoNLL-2010): Shared Task, pages 1?12,Uppsala, Sweden, July.
Association forComputational Linguistics.Halil Kilicoglu, and Sabine Bergler.
2008.Recognizing speculative language in biomedicalresearch articles: a linguistically motivatedperspective.
BMC Bioinformatics, 9(Suppl11):S10.John Lafferty, Andrew K. McCallum, and FernandoPereira.
2001.
Conditional random fields: prob-abilistic models for segmenting and labelingsequence data.
In ICML, pages 282?289.George Lakoff.
1972.
Hedges: a study in meaningcriteria and the logic of fuzzy concepts.
ChicagoLinguistics Society Papers, 8:183?228.Marc Light, Xin Y. Qiu, and Padmini Srinivasan.2004 The language of bioscience:facts,speculations, and statements in between.
InBioLINK 2004: Linking Biological Literature,Ontologies and Databases, pages 17?24.Ben Medlock, and Ted Briscoe.
2007.
Weaklysupervised learning for hedge classification inscientific literature.
In Proceedings of ACL2007, pages 992?999.Roser Morante, and Walter Daelemans.
2009.Learning the scope of hedge cues in biomedicaltexts.
In Proceedings of the BioNLP 2009Workshop, pages 28-36, Boulder, Colorado, June2009.
Association for Computational Linguistics.Roser Morante, and Walter Daelemans.
2009.
Ametalearning approach to processing the scope ofnegation.
In Proceedings of CoNLL-2009.Boulder, Colorado.Gy?rgy Szarvas.
2008.
Hedge classification inbiomedical texts with a weakly supervisedselection of keywords.
In Proceedings of ACL2008, pages 281?289, Columbus, Ohio, USA.ACL.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robustpart-of-speech tagger for biomedical text.
In:Advances in Informatics, PCI 2005, pages 382?392.Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas,Gy?rgy M?ra, and J?nos Csirik.
2008.
TheBioScope corpus: biomedical texts annotated foruncertainty, negation and their scopes.
BMCBioinformatics, 9(Suppl 11):S9.105
