Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 46?55,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Corpus-Based Approach for the Prediction of Language Impairment inMonolingual English and Spanish-English Bilingual ChildrenKeyur Gabani and Melissa Sherman and Thamar Solorio and Yang LiuDepartment of Computer ScienceThe University of Texas at Dallaskeyur,mesh,tsolorio,yangl@hlt.utdallas.eduLisa M. Bedore and Elizabeth D. Pen?aDepartment of Communication Sciences and DisordersThe University of Texas at Austinlbedore,lizp@mail.utexas.eduAbstractIn this paper we explore a learning-based ap-proach to the problem of predicting languageimpairment in children.
We analyzed sponta-neous narratives of children and extracted fea-tures measuring different aspects of languageincluding morphology, speech fluency, lan-guage productivity and vocabulary.
Then, weevaluated a learning-based approach and com-pared its predictive accuracy against a methodbased on language models.
Empirical re-sults on monolingual English-speaking chil-dren and bilingual Spanish-English speakingchildren show the learning-based approach isa promising direction for automatic languageassessment.1 IntroductionThe question of how best to identify children withlanguage disorders is a topic of ongoing debate.One common assessment approach is based on cut-off scores from standardized, norm-referenced lan-guage assessment tasks.
Children scoring at thelower end of the distribution, typically more than1.25 or 1.5 Standard Deviations (SD) below themean, are identified as having language impair-ment (Tomblin et al, 1997).
This cutoff-basedapproach has several well-documented weaknessesthat may result in both over- and under-identificationof children as language impaired (Plante and Vance,1994).
Recent studies have suggested considerableoverlap between children with language impairmentand their typically developing cohorts on many ofthese tasks (e.g., (Pen?a et al, 2006b; Spaulding etal., 2006)).
In addition, scores and cutoffs on stan-dardized tests depend on the distribution of scoresfrom the particular samples used in normalizing themeasure.
Thus, the validity of the measure for chil-dren whose demographic and other socioeconomiccharacteristics are not well represented in the test?snormative sample is a serious concern.
Finally, mostnorm-referenced tests of language ability rely heav-ily on exposure to mainstream language and expe-riences, and have been found to be biased againstchildren from families with low parental educationand socioeconomic status, as well as children fromdifferent ethnic backgrounds (Campbell et al, 1997;Dollaghan and Campbell, 1998).This paper aims to develop a reliable and auto-matic method for identifying the language status ofchildren.
We propose the use of different lexico-syntactic features, typically used in computationallinguistics, in combination with features inspiredby current assessment practices in the field of lan-guage disorders to train Machine Learning (ML) al-gorithms.
The two main contributions of this pa-per are: 1) It is one step towards developing a re-liable and automatic approach for language statusprediction in English-speaking children; 2) It pro-vides evidence showing that the same approach canbe adapted to predict language status in Spanish-English bilingual children.2 Related Work2.1 Monolingual English-Speaking ChildrenSeveral hypotheses exist that try to explain the gram-matical deficits of children with Language Impair-46ment (LI).
Young children normally go through astage where they use non-finite forms of verbs ingrammatical contexts where finite forms are re-quired (Wexler, 1994).
This is referred as the op-tional infinitive stage.
The Extended Optional Infini-tive (EOI) theory (Rice and Wexler, 1996) suggeststhat children with LI exhibit the use of a ?young?grammar for an extended period of time, wheretense, person, and number agreement markers areomitted.In contrast to the EOI theory, the surface accounttheory (Leonard et al, 1997) assumes that chil-dren with LI have reduced processing capabilities.This deficit affects the perception of low stress mor-phemes, such as -ed, -s, be and do, resulting in aninconsistent use of these verb morphemes.Spontaneous narratives are considered as one ofthe most ecologically valid ways to measure com-municative competence (Botting, 2002).
They rep-resent various aspects involved in children?s every-day communication.
Typical measures for sponta-neous language samples include Mean Length ofUtterance (MLU) in words, Number of DifferentWords (NDW), and errors in grammatical morphol-ogy.
Assessment approaches compare children?sperformance on these measures against expectedperformance.
As mentioned in Section 1, these cut-off based methods raise questions concerning accu-racy and bias.
Manually analyzing the narratives isalso a very time consuming task.
After transcribingthe sample, clinicians need to code for the differ-ent clinical markers and other morphosyntactic in-formation.
This can take up to several hours for eachchild making it infeasible to analyze a large numberof samples.2.2 Bilingual Spanish-English SpeakingChildrenBilingual children face even more identificationchallenges due to their dual language acquisition.They can be mistakenly labeled as LI due to: 1) theinadequate use of translations of assessment tools;2) an over reliance on features specific to English; 3)a lack of appropriate expectations about how the lan-guages of a bilingual child should develop (Bedoreand Pen?a, 2008); 4) or the use of standardizedtests where the normal distribution used to comparelanguage performance is composed of monolingualchildren (Restrepo and Gutie?rrez-Clellen, 2001).Spanish speaking children with LI show differ-ent clinical markers than English speaking childrenwith LI.
As mentioned above, English speakers haveproblems with verb morphology.
In contrast, Span-ish speakers have been found to have problems withnoun morphology, in particular in the use of articlesand clitics (Restrepo and Gutie?rrez-Clellen, 2001;Jacobson and Schwartz, 2002; Bedore and Leonard,2005).
Bedore and Leonard (2005) also found dif-ferences in the error patterns of Spanish and relatedlanguages such as Italian.
Spanish-speakers tend toboth omit and substitute articles and clitics, whilethe dominant errors for Italian-speakers are omis-sions.3 Our ApproachWe use language models (LMs) in our initial inves-tigation, and later explore more complex ML algo-rithms to improve the results.
Our ultimate goal isto discover a highly accurate ML method that can beused to assist clinicians in the task of LI identifica-tion in children.3.1 Language Models for Predicting LanguageImpairmentLMs are statistical models used to estimate the prob-ability of a given sequence of words.
They have beenexplored previously for clinical purposes.
Roark etal.
(2007) proposed cross entropy of LMs trainedon Part-of-Speech (POS) sequences as a measure ofsyntactic complexity with the aim of determiningmild cognitive impairment in adults.
Solorio andLiu (2008) evaluated LMs on a small data set in apreliminary trial on LI prediction.The intuition behind using LMs is that they canidentify atypical grammatical patterns and help dis-criminate the population with potential LI fromthe Typically Developing (TD) one.
We use LMstrained on POS tags rather than on words.
UsingPOS tags can address the data sparsity issue in LMs,and place less emphasis on the vocabulary and moreemphasis on the syntactic patterns.We trained two separate LMs using POS tagsfrom the transcripts of TD and LI children, respec-tively.
The language status of a child is predictedusing the following criterion:47d(s) ={ LI if (PPTD(s) > PPLI(s))TD otherwisewhere s represents a transcript from a child, andPPTD(s) and PPLI(s) are the perplexity valuesfrom the TD and LI LMs, respectively.
We used theSRI Language Modeling Toolkit (Stolcke, 2002) fortraining the LMs and calculating perplexities.3.2 Machine Learning for Predicting LanguageImpairmentAlthough LMs have been used successfully on dif-ferent human language processing tasks, they aretypically trained and tested on language sampleslarger than what is usually collected by clinicians forthe purpose of diagnosing a child with potential LI.Clinicians make use of additional information be-yond children?s speech, such as parent and teacherquestionnaires and test scores on different languageassessment tasks.
Therefore in addition to usingLMs for children language status prediction, we ex-plore a machine learning classification approach thatcan incorporate more information for better predic-tion.
We aim to identify effective features for thistask and expect this information will help cliniciansin their assessment.We consider various ML algorithms for the clas-sification task, including Naive Bayes, ArtificialNeural Networks (ANNs), Support Vector Ma-chines (SVM), and Boosting with Decision Stumps.Weka (Witten and Frank, 1999) was used in our ex-periments due to its known reliability and the avail-ability of a large number of algorithms.
Below weprovide a comprehensive list of features that we ex-plored for both English and Spanish-English tran-scripts.
We group these features according to theaspect of language they focus on.
Features specificto Spanish are discussed in Section 5.2.1.
Language productivity(a) Mean Length of Utterance (MLU) inwordsDue to a general deficit of language abil-ity, children with LI have been found toproduce language samples with a shorterMLU in words because they producegrammatically simpler sentences whencompared to their TD peers.
(b) Total number of wordsThis measure is widely used when build-ing language profiles of children for diag-nostic and treatment purposes.
(c) Degree of supportIn spontaneous samples of children?sspeech, it has been pointed out that chil-dren with potential LI need more encour-agement from the investigator (Wetherellet al, 2007) than their TD peers.
A sup-port prompt can be a question like ?Whathappened next??
We count the number ofutterances, or turns, of the investigator in-terviewing the child for this feature.2.
Morphosyntactic skills(a) Ratio of number of raw verbs to the totalnumber of verbsAs mentioned previously, children with LIomit tense markers in verbs more oftenthan their TD cohorts.
For example:...the boy look into the hole but didn?tfind...Hence, we include the ratio of the numberof raw verbs to the total number of verbsas a feature.
(b) Subject-verb agreementResearch has shown that English-speakingchildren with LI have difficulties mark-ing subject-verb agreement (Clahsen andHansen, 1997; Schu?tze and Wexler, 1996).An illustration of subject-verb disagree-ment is the following:...and he were looking behind the rocksAs a way of capturing this informationin the machine learning setting, we con-sider various bigrams of POS tags: nounand verb, noun and auxiliary verb, pro-noun and verb, and pronoun and auxiliaryverb.
These features are included in a bag-of-words fashion using individual counts.Also, we allow a window between thesepairs to capture agreement between sub-48ject and verb that may have modifiers inbetween.
(c) Number of different POS tagsThis feature is the total number of differ-ent POS tags in each transcript.3.
Vocabulary knowledgeWe use the Number of Different Words (NDW)to represent vocabulary knowledge of a child.Although such measures can be biased againstchildren from different backgrounds, we expectthis possible negative effect to decrease as a re-sult of having a richer pool of features.4.
Speech fluencyRepetitions, revisions, and filled pauses havebeen considered indicators of language learn-ing difficulties (Thordardottir and Weismer,2002; Wetherell et al, 2007).
In this workwe include as features (a) the number of fillers,such as uh, um, er; and (b) the number of disflu-encies (abandoned words) found in each tran-script.5.
Perplexities from LMsAs mentioned in Section 3.1 we trained LMs oforder 1, 2, and 3 on POS tags extracted fromTD and LI children.
We use the perplexity val-ues from these models as features.
Addition-ally, differences in perplexity values from LIand TD LMs for different orders are used asfeatures.6.
Standard scoresA standard score, known as a z-score, is the dif-ference between an observation and the meanrelative to the standard deviation.
For this fea-ture group, we first find separate distributionsfor the MLU in words, NDW and total num-ber of utterances for the TD and LI populations.Then, for each transcript, we compute the stan-dard scores based on each of these six distribu-tions.
This represents how well the child is per-forming relative to the TD and LI populations.Note that a cross validation setup was used toobtain the distribution for the TD and LI chil-dren for training.
This is also required for theLM features above.4 Experiments with Monolingual Children4.1 The Monolingual English Data SetOur target population for this work is children withan age range of 3 to 6 years old.
However, currentlywe do not have any monolingual data sets readilyavailable to test our approach in this age range.
Inthe field of communication disorders data sharingis not a common practice due to the sensitive con-tent of the material in the language samples of chil-dren, and also due to the large amount of effort andtime it takes researchers to collect, transcribe, andcode the data before they can begin their analysis.To evaluate our approach we used a dataset fromCHILDES (MacWhinney, 2000) that includes nar-ratives from English-speaking adolescents with andwithout LI with ages ranging between 13 and 16years old.
Even though the age range is outside therange we are interested in, we believe that this dataset can still be helpful in exploring the feasibility ofour approach as a first step.This data set contains 99 TD adolescents and 19adolescents who met the LI profile at one point inthe duration of the study.
There are transcripts fromeach child for two tasks: a story telling and a spon-taneous personal narrative.
The first task is a pictureprompted story telling task using the wordless pic-ture book, ?Frog, Where Are You??
(Mayer, 1969).In this story telling task children first look at thestory book ?to develop a story in memory?
and thenare asked to narrate the story.
This type of elicitationtask encourages the use of past tense constructions,providing plenty of opportunities for extracting clin-ical markers.
In the spontaneous personal narrativetask, the child is asked to talk about a person who an-noys him/her the most and describe the most annoy-ing features of that person.
This kind of spontaneouspersonal narrative encourages the participant for theuse of third person singular forms (-s).
Detailed in-formation of this data set can be found in (Wetherellet al, 2007).We processed the transcripts using the CLANtoolkit (MacWhinney, 2000).
MOR and POST fromCLAN are used for morphological analysis and POStagging of the children?s speech.
We decided to usethese analyzers since they are customized for chil-dren?s speech.49Story telling Personal narrativeMethod P (%) R (%) F1 (%) P (%) R (%) F1 (%)Baseline 28.57 10.53 15.38 33.33 15.79 21.431-gram LMs 41.03 84.21 55.17 34.21 68.42 45.612-gram LMs 75.00 47.37 58.06 55.56 26.32 35.713-gram LMs 80.00 21.05 33.33 87.50 36.84 51.85Table 1: Evaluation of language models on the monolingual English data set.Story telling Personal narrativeAlgorithm P (%) R (%) F1 (%) P (%) R (%) F1 (%)Naive Bayes 38.71 63.16 48.00 34.78 42.11 38.10Bayesian Network 58.33 73.68 65.12 28.57 42.11 34.04SVM 76.47 68.42 72.22 47.06 42.11 44.44ANNs 62.50 52.63 57.14 50.00 47.37 48.65Boosting 70.59 63.16 66.67 69.23 47.37 56.25Table 2: Evaluation of machine learning algorithms on the monolingual English data set.4.2 Results with MonolingualEnglish-Speaking ChildrenThe performance measures we use are: precision(P), recall (R), and F-measure (F1).
Here the LI cat-egory is the positive class and the TD category is thenegative class.Table 1 shows the results of leave-one-out-cross-validation (LOOCV) obtained from the LM ap-proach for the story telling and spontaneous personalnarrative tasks.
It also shows results from a base-line method that predicts language status by usingstandard scores on measures that have been asso-ciated with LI in children (Dollaghan, 2004).
Thethree measures we used for the baseline are: MLUin words, NDW, and total number of utterances pro-duced.
To compute this baseline we estimate themean and standard deviation of these measures us-ing LOOCV with the TD population as our norma-tive sample.
The baseline predicts that a child hasLI if the child scores more than 1.25 SD below themean on at least two out of the three measures.Although LMs yield different results for the storytelling and personal narrative tasks, they both pro-vide consistently better results than the baseline.
Forthe story telling task the best results, in terms of theF1 measure, are achieved by a bigram LM (F1 =58.06%) while for the personal narrative the highestF1 measure (51.85%) is from the trigram LM.
If weconsider precision, both tasks have the same increas-ing pattern when increasing LM orders.
However forrecall that is not the case.
In the story telling task,recall decreases at the expense of higher precision,but for the personal narrative task, the trigram LMreaches a better trade-off between precision and re-call, which yields a high F1 measure.
We also evalu-ated 4-gram LMs, but results did not improve, mostlikely because we do not have enough data to trainhigher order LMs.The results for different ML algorithms are shownin Table 2, obtained by using all features describedin Section 3.2.
The feature based approach us-ing ML algorithms outperformed using only LMson both tasks.
For the story telling task, SVMwith a linear kernel achieves the best results (F1 =72.22%), while Boosting with Decision Stumps pro-vides the best performance (F1 = 56.25%) for thepersonal narrative task.4.3 Feature and Error AnalysisThe ML results shown above use the entire featureset described in Subsection 3.2.
The next questionwe ask is the effectiveness of different features forthis task.
The datasets we are using in our evalua-tion are very small, especially considering the num-ber of positive instances.
This prevents us from hav-ing a separate subset of the data for parameter tun-ing or feature selection.
Therefore, we performedadditional experiments to evaluate the usefulness ofindividual features.
Figure 1 shows the F1 measures500204060801001 2 3 4 5 6F-measure(%)FeaturesStory TellingPersonal NarrativeFigure 1: Discriminating power of different groups offeatures.
The numbers on the x-axis correspond to thefeature groups in Section 3.2.when using different feature groups.
The numberson the x-axis correspond to the feature groups de-scribed in Section 3.2.
The F1 measure value foreach of the features is the highest value obtained byrunning different ML algorithms for classification.We noticed that for the story telling task, usingperplexity values from LMs (group 5) as a featurein the ML setting outperforms the LM threshold ap-proach by a large margin.
It seems that having theperplexity values as well as the perplexity differ-ences from all the LMs of different orders in the MLalgorithm provides a better estimation of the targetconcept.Only the standard scores (group 6) yield a higherF1 measure for the personal narrative task than thestory telling one.
The majority of the features (5out of 6 groups) provide higher F1 measures for thestory telling task, which explains the significantlybetter results on this task over the personal narrativein our learning approach.
This is consistent with pre-vious work contrasting narrative genre stating thatthe restrictive setting of a story retell is more reveal-ing of language difficulties than spontaneous narra-tives, where the subjects have more control on thecontent and style (Wetherell et al, 2007).We also performed some error analysis for someof the transcripts that were consistently misidenti-fied by different ML algorithms.
In the story tellingtask, we find that some LI transcripts are misclassi-fied as TD because they (1) have fewer fillers, dis-fluencies, and degree of support; (2) are similar tothe TD transcripts, which is depicted by the perplex-ity values for these transcripts; or (3) contain higherMLU in words as compared to their LI peers.
Someof the reasons for classifying transcripts in the TDcategory as LI are shorter MLU in words as com-pared to other TD peers, large number of fillers, andexcessive repetitions of words and phrases unlike theother TD children.
These factors are consistent withthe effective features that we found from Figure 1.For the personal narrative task, standard scores(group 6) and language productivity (group 1) havean important role in classification, as shown in Fig-ure 1.
The TD transcripts that are misidentified havelower standard scores and MLU in words than thoseof their TD peers.We believe that another source of noise in thetranscripts comes from the POS tags themselves.For instance, we found that many verbs in presenttense for third person singular are tagged as pluralnouns, which results in a failure to capture subject-verb agreement.Lastly, according to the dataset description, chil-dren in the LI category met the LI criteria at onestage in their lifetime and some of these childrenalso had, or were receiving, some educational sup-port in the school environment at the time of datacollection.
This support for children with LI ismeant to improve their performance on languagerelated tasks, making the automatic classificationproblem more complicated.
This also raises thequestion about the reference label (TD or LI) foreach child in the data set we used.
The details aboutwhich children received interventions are not speci-fied in the dataset description.5 Experiments with Bilingual ChildrenIn this section we generalize the approach to aSpanish-English bilingual population.
In adaptingthe approach to our bilingual population we face twochallenges: first, what shows to be promising fora monolingual and highly heterogeneous populationmay not be as successful in a bilingual setting wherewe expect to have a large variability of exposure toeach language; second, there is a large differencein the mean age of the monolingual setting and thatof our bilingual one.
This age difference will resultin different speech patterns.
Younger children pro-51duce more ill-formed sentences since they are stillin a language acquisition phase.
Lastly, the clini-cal markers in adolescents are geared towards prob-lems at the pragmatic and discourse levels, while atyounger ages they focus more on syntax and mor-phology.For dealing with the first challenge we are extract-ing language-specific features and hope that by look-ing at both languages we can reach a good discrim-ination performance.
For the second challenge, ourfeature engineering approach has been focused onyounger children from the beginning.
We are aimingto capture the type of morphosyntactic patterns thatcan identify LI in young children.
In addition, thesamples in the bilingual population are story retells,and our feature setting showed to be a good matchfor this task.
Therefore, we expect our approach tocapture relevant classification patterns, even in thepresence of noisy utterances.5.1 The Bilingual Data SetThe transcripts for the bilingual LI task come froman on-going longitudinal study of language impair-ment in Spanish-English speaking children (Pen?a etal., 2006a).
The children in this study were enrolledin kindergarten with a mean age of about 70 months.Of the 59 children, 6 were identified as having apossible LI by an expert in communication disor-ders, while 53 were identified as TD.
Six of the TDchildren were excluded due to missing information,yielding a total of 47 TD children.Each child told a series of stories based on MercerMayer?s wordless picture books (Mayer, 1969).
Twostories were told in English and two were told inSpanish, for a total of four transcripts per child.
Thebooks used for English were ?A Boy, A Dog, andA Frog?
and ?Frog, Where Are You??
The booksused for Spanish retelling were ?Frog on His Own?and ?Frog Goes to Dinner.?
The transcripts for eachseparate language were combined, yielding one in-stance per language for each child.An interesting aspect of the bilingual data is thatthe children mix languages in their narratives.
Thisphenomenon is called code-switching.
At the begin-ning of a retelling session, the interviewer encour-ages the child to speak the target language if he/sheis not doing so.
Once the child begins speaking thecorrect language, any code-switching thereafter isnot corrected by the interviewer.
Due to this, the En-glish transcripts contain Spanish utterances and viceversa.
We believe that words in the non-target lan-guage help contribute to a more accurate languagedevelopment profile.
Therefore, in our work we de-cided to keep these code-switched elements.
A com-bined lexicon approach was used to tag the mixed-language fragments.
If a word does not appear in thetarget language lexicon, we apply the POS tag fromthe non-target language.5.2 Spanish-Specific FeaturesMany structural differences exist between Spanish,a Romance language, and English, a Germanic lan-guage.
Spanish is morphologically richer than En-glish.
It contains a larger number of different verbconjugations and it uses a two gender system fornouns, adjectives, determiners, and participles.
ASpanish-speaking child with LI will have difficultieswith different grammatical elements, such as articlesand clitics, than an English-speaking child (Bedoreand Pen?a, 2008).
These differences indicate that theSpanish feature set will need to be tailored towardsthe Spanish language.To account for Spanish-specific patterns we in-cluded new POS bigrams as features.
To capturethe use of correct and incorrect gender and num-ber marking morphology, we added noun-adjective,determiner-noun, and number-noun bigrams to thelist of morphosyntactic features.5.3 Results on Bilingual ChildrenResults are shown for the baseline and LM thresholdapproach for the bilingual data set in Table 3.
Thebaseline is computed from the same measures as themonolingual dataset (MLU in words, NDW, and to-tal utterances).Compared to Table 1, the values in Table 3are generally lower than on the monolingual storytelling task.
In this inherently difficult task, the bilin-gual transcripts are more disfluent than the monolin-gual ones.
This could be due to the age of the chil-dren or their bilingual status.
Recent studies on psy-cholinguistics and language production have shownthat bilingual speakers have both languages activeat speech production time (Kroll et al, 2008) andit is possible that this may cause interference, espe-cially in children still in the phase of language acqui-52English SpanishMethod P (%) R (%) F1 (%) P (%) R (%) F1 (%)Baseline 20.00 16.66 18.18 16.66 16.66 16.661-gram LMs 40.00 33.33 36.36 17.64 50.00 26.082-gram LMs 50.00 33.33 40.00 33.33 16.66 22.223-gram LMs 100.00 33.33 50.00 0.00 0.00 -Table 3: Evaluation of language models on Bilingual Spanish-English data set.sition.
In addition, the LMs in the monolingual taskwere trained using more instances per class, possiblyyielding better results.There are some different patterns between usingthe English and Spanish transcripts.
In English,the unigram models provide the least discriminativevalue, and the bigram and trigram models improvediscrimination.
We also evaluated higher order n-grams, but did not obtain any further improvement.We found that the classification accuracy of the LMapproach was influenced by two children with LIwho were consistently marked as LI due to a greaterperplexity value from the TD LM.
A further analysisshows that these children spoke mostly Spanish onthe ?English?
tasks yielding larger perplexities fromthe TD LM, which was trained from mostly English.In contrast, the LI LM was created with transcriptscontaining more Spanish than the TD one, and thustest transcripts with a lot of Spanish do not inflateperplexity values that much.For Spanish, unigram LMs provide some discrim-inative usefulness, and then the bigram performancedecreases while the trigram model provides no dis-criminative value.
One reason for this may be thatthe Spanish LMs have a larger vocabulary.
In theSpanish LMs, there are 2/3 more POS tags than inthe English LM.
This size difference dramaticallyincreases the possible bigrams and trigrams, there-fore increasing the number of parameters to esti-mate.
In addition, we are using an ?off the shelf?POS tagger (provided by CLAN) and this may addnoise in the feature extraction process.
Since we donot have gold standard annotations for these tran-scripts, we cannot measure the POS tagging accu-racy.
A rough estimate based on manually revis-ing one transcript in each language showed a POStagging accuracy of 90% for English and 84% forSpanish.
Most of the POS tagger errors involveverbs, nouns and pronouns.
Thus while the accu-0204060801001 2 3 4 5 6F-measure(%)FeaturesEnglishSpanishCombinedFigure 2: Discriminating power of different groups offeatures for the bilingual population.
The numbers on thex-axis correspond to the feature groups in Section 3.2.racy might not seem that low, it can still have a ma-jor impact on our approach since it involves the POScategories that are more relevant for this task.Table 4 shows the results from various ML algo-rithms.
In addition to predicting the language statuswith the English and Spanish samples separately, wealso combined the English and Spanish transcriptstogether for each child, and used all the featuresfrom both languages in order to allow a predictionbased on both samples.
The best F1 measure for thistask (60%) is achieved by using the Naive Bayes al-gorithm with the combined Spanish-English featureset.
This is an improvement over both the separateEnglish and Spanish trials.
The Naive Bayes algo-rithm provided the best discrimination for the En-glish (54%) and Combined data sets and Boostingand SVM provided the best discrimination for theSpanish set (18%).5.4 Feature AnalysisSimilar to the monolingual dataset, we performedadditional experiments exploring the contributionof different groups of features.
We tested the six53English Spanish CombinedAlgorithm P (%) R (%) F1 (%) P (%) R (%) F1 (%) P (%) R (%) F1 (%)ANNs 66.66 33.33 44.44 0.00 0.00 - 100.00 16.66 28.57SVM 14.28 16.66 15.38 20.00 16.66 18.18 66.66 33.33 44.44Naive Bayes 60.00 50.00 54.54 0.00 0.00 - 75.00 50.00 60.00Logistic Regression 25.00 16.66 20.00 - 0.00 - 50.00 33.33 40.00Boosting 50.00 33.33 40.00 20.00 16.66 18.18 66.66 33.33 44.44Table 4: Evaluation of machine learning algorithms on the Bilingual Spanish-English data set.groups of features described in Section 3.2 sepa-rately.
Overall, the combined LM perplexity val-ues (group 5) provided the best discriminative value(F1 = 66%).
The LM perplexity values performedthe best for English.
It even outperformed using allthe features in the ML algorithm, suggesting somefeature selection is needed for this task.The morpohsyntactic skills (group 2) provided thebest discriminative value for the Spanish languagefeatures, and performed better than the completefeature set for Spanish.
Within group 2, we evalu-ated different POS bigrams for the Spanish and En-glish sets and observed that most of the bigram com-binations by themselves are usually weak predictorsof language status.
In the Spanish set, out of all ofthe lexical combinations, only the determiner-noun,noun-verb, and pronoun-verb categories providedsome discriminative value.
The determiner-nouncategory captured the correct and incorrect gendermarking between the two POS tags.
The noun-verband pronoun-verb categories covered the correct andincorrect usage of subject-verb combinations.
In-terestingly enough, the pronoun-verb category per-formed well by itself, yielding an F1 measure of54%.
There are also some differences in the frequen-cies of bigram features in the English and Spanishdata sets.
For example, there is no noun-auxiliaryPOS pattern in Spanish, and the pronoun-auxiliarybigram appears less frequently in Spanish than inEnglish because in Spanish the use of personal pro-nouns is not mandatory since the verb inflection willdisambiguate the subject of the sentence.The vocabulary knowledge feature (group 3) didnot provide any discriminative value for any of thelanguage tasks.
This may be because bilingual chil-dren receive less input for each language than amonolingual child learning one language, or due tothe varied vocabulary acquisition rate in our bilin-gual population.6 Conclusions and Future WorkIn this paper we present results on the use of LMsand ML techniques trained on features representingdifferent aspects of language gathered from spon-taneous speech samples for the task of assistingclinicians in determining language status in chil-dren.
First, we evaluate our approach on a monolin-gual English-speaking population.
Next, we showthat this ML approach can be successfully adaptedto a bilingual Spanish-English population.
ML al-gorithms provide greater discriminative power thanonly using a threshold approach with LMs.Our current efforts are devoted to improving pre-diction accuracy by refining our feature set.
We areworking on creating a gold standard corpus of chil-dren?s transcripts annotated with POS tags.
Thisdata set will help us improve accuracy on our POS-based features.
We are also exploring the use ofsocio-demographic features such as the educationallevel of parents, the gender of children, and enroll-ment status on free lunch programs.AcknowledgmentsThis work was supported by NSF grant 0812134,and by grant 5 UL1 RR024982 from NCRR, a com-ponent of NIH.
We also thank the three NAACL re-viewers for insightful comments on the submittedversion of this paper.ReferencesLisa M. Bedore and Laurence B. Leonard.
2005.
Verbinflections and noun phrase morphology in the sponta-neous speech of Spanish-speaking children with spe-cific language impairment.
Applied Psycholinguistics,26(2):195?225.54Lisa M. Bedore and Elizabeth D. Pen?a.
2008.
Assess-ment of bilingual children for identification of lan-guage impairment: Current findings and implicationsfor practice.
International Journal of Bilingual Edu-cation and Bilingualism, 11(1):1?29.Nicola Botting.
2002.
Narrative as a tool for the assess-ment of linguistic and pragmatic impairments.
ChildLanguage Teaching and Therapy, 18(1):1?21.Thomas Campbell, Chris Dollaghan, Herbert Needle-man, and Janine Janosky.
1997.
Reducing bias in lan-guage assessment: Processing-dependent measures.Journal of Speech, Language, and Hearing Research,40(3):519?525.Harald Clahsen and Detlef Hansen.
1997.
The grammat-ical agreement deficit in specific language impairment:Evidence from therapy experiments.
In Myrna Gop-nik, editor, The Inheritance and Innateness of Gram-mar, chapter 7.
Oxford University Press, New York.Christine A. Dollaghan and Thomas F. Campbell.
1998.Nonword repetition and child language impairment.Journal of Speech, Language, and Hearing Research,41(5):1136?1146.Christine A. Dollaghan.
2004.
Taxometric analyses ofspecific language impairment in 3- and 4-year-old chil-dren.
Journal of Speech, Language, and Hearing Re-search, 47(2):464?475.Peggy F. Jacobson and Richard G. Schwartz.
2002.Morphology in incipient bilingual Spanish-speakingpreschool children with specific language impairment.Applied Psycholinguistics, 23(1):23?41.Judith F. Kroll, Chip Gerfen, and Paola E. Dussias.
2008.Laboratory designs and paradigms: Words, sounds,sentences.
In L. Wei and M. G. Moyer, editors, TheBlackwell Guide to Research Methods in Bilingualismand Multilingualism, chapter 7.
Blackwell Pub.Laurence B. Leonard, Julia A. Eyer, Lisa M. Bedore,and Bernard G. Grela.
1997.
Three accounts ofthe grammatical morpheme difficulties of English-speaking children with specific language impairment.Journal of Speech, Language, and Hearing Research,40(4):741?753.Brian MacWhinney.
2000.
The CHILDES project: Toolsfor analyzing talk.
Lawrence Erlbaum, Mahwah, NJ.Mercer Mayer.
1969.
Frog, where are you?
Dial Press.Elizabeth D. Pen?a, Lisa M. Bedore, Ronald B. Gillam,and Thomas Bohman.
2006a.
Diagnostic markersof language impairment in bilingual children.
Grantawarded by the NIDCD, NIH.Elizabeth D. Pen?a, Tammie J. Spaulding, and ElenaPlante.
2006b.
The composition of normative groupsand diagnostic decision making: Shooting ourselvesin the foot.
American Journal of Speech-LanguagePathology, 15(3):247?254.Elena Plante and Rebecca Vance.
1994.
Selectionof preschool language tests: A data-based approach.Language, Speech, and Hearing Services in Schools,25(1):15?24.Mar?
?a Adelaida Restrepo and Vera F. Gutie?rrez-Clellen.2001.
Article use in Spanish-speaking children withspecific language impairment.
Journal of Child Lan-guage, 28(2):433?452.Mabel L. Rice and Kenneth Wexler.
1996.
Toward tenseas a clinical marker of specific language impairmentin English-speaking children.
Journal of Speech andHearing Research, 39(6):1239?1257.Brian Roark, Margaret Mitchell, and Kristy Holling-shead.
2007.
Syntactic complexity measures for de-tecting mild cognitive impairment.
In Proceedings ofthe Workshop on BioNLP 2007, pages 1?8.
ACL.Carson T. Schu?tze and Kenneth Wexler.
1996.
Subjectcase licensing and English root infinitives.
In Proceed-ings of the 20th Annual Boston University Conferenceon Language Development.
Cascadilla Press.Thamar Solorio and Yang Liu.
2008.
Using languagemodels to identify language impairment in Spanish-English bilingual children.
In Proceedings of theWorkshop on BioNLP 2008, pages 116?117.
ACL.Tammie J. Spaulding, Elena Plante, and Kimberly A.Farinella.
2006.
Eligibility criteria for language im-pairment: Is the low end of normal always appropri-ate?
Language, Speech, and Hearing Services inSchools, 37(1):61?72.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,volume 2, pages 901?904.Elin T. Thordardottir and Susan Ellis Weismer.
2002.Content mazes and filled pauses on narrative languagesamples of children with specific language impair-ment.
Brain and Cognition, 48(2-3):587?592.J.
Bruce Tomblin, Nancy L. Records, Paula Buckwal-ter, Xuyang Zhang, Elaine Smith, and Marlea O?Brien.1997.
Prevalence of specific language impairment inkindergarten children.
Journal of Speech, Language,and Hearing Research, 40(6):1245?1260.Danielle Wetherell, Nicola Botting, and Gina Conti-Ramsden.
2007.
Narrative in adolescent specificlanguage impairment (SLI): a comparison with peersacross two different narrative genres.
InternationalJournal of Language and Communication Disorders,42:583?605(23).Kenneth Wexler.
1994.
Optional infinitives.
In DavidLightfoot and Norbert Hornstein, editors, Verb Move-ment.
Cambridge University Press.Ian H. Witten and Eibe Frank.
1999.
Data Mining: Prac-tical Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann.55
