JEAN PIERRE PM~LETCOMPUTATIONAL LINGUISTICS AND LINGUISTIC THEORYThe present paper is an attempt o justify and explain the directionof present research in Ottawa (Carleton University and also Universityof Ottawa) on Computational manipulation of speech.
Our actual real-izations are not necessarily original; rather, we are trying to make useof the findings of other workers, assembling them, however, in a dif-ferent way, on the basis of a novel conception of linguistic organization.1, DESIDERATA FOR COMPUTATIONAL LINGUISTICSThe main problem facing computational linguists, when they wantto make use of linguistic theory, is the divergence of aims.
Basically,computational linguists attempt o design devices which will automat-ically manipulate instances of speech (henceforth discourses) in specifiedways, to obtain particular results.
Linguistic theories, on the other hand,are concerned with the overall structure of a language, and considerwhat happens in particular cases as the interaction between the overalllinguistic competence (general structural knowledge) and many unde-termined factors of individual performance.
Hence the computationallinguist is left to find for himself pragmatic solutions to his particularproblems.
We examine first what sort of theory the computationallinguist could make good use of.1.1.
It is possible to distinguish between formal and functionaltheories according to whether the main criteria for classification anddescription of units are based on their formal properties alone or relyon the role they play in conveying meaning.
It would appear, given thegoals of computational linguistics, that functional theories might bemore useful, since they give us direct access to the meat: meaning.
How-ever, most of them are not precise enough to be used in a computerenvironment.
The best attempt at using one of them was made by T.358 JEAN" PIERRE PAILLETWINOG~.AD (1972).
It is significant that Winograd had to reformulatemuch of systemic grammar in order to be able to use it.1.2.
Accordingly, most of the work in computational linguisticsrelies on formal theories of language, starting with distributional lin-guistics.
The problem here is reversed.
A good formal theory will pro-vide interesting possibilities for manipulating discourses, but gives noreason for doing so.
More explicitly, the schemas of manipulation soughtby computational linguists must have some interesting functional prop-erty: semantic invariance, logically oriented semantic relations, orthe like.
Hence the necessity for a theory presenting a well definedsemantic aspect, as well as allowing manipulations of the expressionaspect of discourses.1.3.
There are some such theories, among which most conspicuousare generative semantics and Lamb's stratificational grammar.
R.B~NNICK (1969) and S. LAMB (1973) respectively, have argued for ap-plying these theories to computational work on language.
While widelydifferent in most respects, these two theories hare two interrelatedfeatures.
First, they are theories of langue, that is, they try to accountfor wellformedness and systematic interrelationships within a set ofobjects, called sentences of the language.
Second, as a consequenceof this feature, they establish a systematic, deterministic relation be-tween expression and content.
In the case of generative semantics, thissystematic relation is oriented from content o expression; the under-standing of a particular sentence must then be viewed as the ~ undoing ~>of transformations, which often yields ambiguity.
In the case of strati-ficational grammar, which is basically non-oriented, the understandingof a sentence appears as the activation of a static network of connectionsfrom the expression end.
At the content end, this yields a complex ofactivated lines which, hopefully, represents the meaning of the sen-tence.
There is here an interesting distinction, not explicit in genera-tive semantics, between the static set of connections and their dynamicactivation in a particular speech act.
However, both formulations relyheavily on wellformedness.
Any speech act which is not perfect mustbe blocked somewhere in the encoding or decoding.
This is normal ifthe only interaction between speaker and hearer is to take place throughwellformed speech expressions.
On the other hand, such a requirementis known to be unnatural in the human environment, where wellform-edness is patently non-essential.
Thus, if realizable, generative seman-COMPUTATIONAL LINGUISTICS AND LINGUISTIC THEORY 359tics or stratificational grammar are at best an artificial simplification ofhuman language, for computing purposes.
Even thus, we know thata particular sentence may have different ~ meanings ~) in different lin-guistic or non linguistic contexts; therefore, it is illusory to look fora purely linguistic mapping between content and expression.
We labeldirect hose theories which posit such a mapping, and mediate those whichallow for (and, ideally, specify the conditions of) the insertion of extra-linguistic information in the speaking or understanding processes.What is needed, thus, for computational linguistics, is a formalized,semantically specific, mediate theory of language.2.
SYNTAX AND THE DICTIONARY2.1.
Among other things, the adoption of a mediate theory oflanguage forces on us an interesting consequence: it is no longer possi-ble to derive the wellformedness of expression from that of content, orconversely.
Each has to be specified in its own terms.
This in turn re-quires the creation of an independent metalanguage for content, a taskstarted, with a limited scope, by symbolic logic.
It also changes theoutlook of syntax.
In most formal theories of syntax, a large apparatusis devoted to selectional restrictions.
Inasmuch as these are the man-ifestations of semantic wellformedness requirements, to be handledanyhow in the semantic description, we can now reduce syntactic de-scription to the specification of order structures and ~ strict subcategori-zation ~.
As a consequence, we may ignore, in syntax, such notions asthat of transformation, which characterizes selectional invariance.2.2.
In order to see what kind of device might be used for suchsimplified syntactic description, a detour through morphology is inorder.In a recent paper, M. HALLE (1973) offers suggestions for the treat-ment of morphology in a generative grammar.
1 1~.
BINNICK (1973) haswritten a penetrating review of Halle's paper; within the generativeframework, he points out a very important feature of Halle's proposal:the confusion between irregularity in expression and irregularity in1 The concern with morphology is rather new in generative grammar, but has longbeen manifested by other linguists, e.g.G.
Gtr~LAUM~ (1971), or M. A. K. HaLLmAY(1961), in a functional approach.360 JEAN PIERRE PAILLETcontent.
According to Binnick, the two types of structures hould becarefully distinguished (which is precisely what a mediate theory doesbetter than a direct heory).
On the other hand, there are some regular-ities in morphology, which involve a systematic orrespondence b -tween expression and content: for instance, in English, it is always pos-sible to nominalize a verb.
Whenever a "strong nominalisation" (asin the arrival of the prime minister) is not available, it is possible to usea"  gerund" (as in the second coming).
It seems that the regular formationsare less acceptable when a lexical possibility exists; they are thereforeto be considered as "otherwise" solutions.
We have here a suggestionthat the description of the lexicon must make extensive use of disjunc-tive ordering (this, incidentally, is built into the structure of strati-ficational theory).The dictionary is "needed anyhow" either to list irregularities inmorphology, or to account for the link between expression and con-tent aspects of morphemes, or probably for both.
This dictionary ap-pears as a list of arbitrary associations between some content structuresand some expression structures: nothing new, on this point, since Saus-sure.
We may borrow an argument from the practice of generativegrammarians, and argue that, since the dictionary is needed anyhow,we might as well use it for syntax as well.
This will also remind oneof F. Dr SAtrssu~ (1966), who thought hat syntax could not entirelybe part of langue: only "stereotyped expressions " would.
Translate:some recurrent syntagmatic patterns of morpheme classes are part ofthe linguistic tool box, just as morphemes are.
The rest is up to thespeaker.2.3.
Such a view would have two interesting consequences.
Thefirst one, from the computational point of view, is that whatever de-vice is needed for handling the dictionary it could do duty to handle syn-tagms as well.
The second, from the theoretical point of view, is thata theory of syntactic illformedness would be naturally statable, by anal-ogy with morphology.
To take an example from Halle's paper, arriv-ation is wellformed although non occurring, because of the specific prop-erties of-ation and arrive.
Morphological wellformedness can be stat-ed in terms of contiguity relationships.
On this model, we might wantto state syntactic onditions in terms of contiguity relationships aswell: that is precisely what string structure theory does.
A string struc-ture description (A. K. Josm, 1970; z. s. HARRtS, 1965) starts with avocabulary of strings, each of which has occurrence properties, tat-COMPUTATIONAL LINGUISTICS AND LINGUISTIC THEORY 361able in terms of which strings or parts of strings they may or mustbe contiguous with.2.4.
One frequent objection to assimilating morphology and syn-tax can be dealt with here.
It is often argued that the types of recur-siveness found in syntactic structures do not appear in morphology.
Thiswould be the reason why a finite state treatment, which is feasible formorphology, is not adaptable to syntax.
This argument reflects a biasedview of morphology, influenced mainly by indoeuropean languages.As a matter of fact, Eskimo morphology, for instance, exhibitsinstances of full recursiveness.
Any solution to the problem of describ-ing Eskimo words will be a likely solution for describing Englishsyntax, and conversely.
In his axiomatic presentation f the string struc-ture of English, Harris offers such a solution, based on the fact thatonly very few types of syntagms are recursive: for English syntax,they are those labeled c, Y~, and A by Harris.
One can then describeEnglish order structures by a finite state diagram, where some of thetransitions are labeled by one of these three symbols.
These transitionsimply a (recursive) call to the description of the corresponding syntagm.The systems developed by W. A.
WOODS (1970), J. THORPE (1968),and D. G. BOBROW (1969) exploit a similar trick.
The only theoreticalrequirement is to keep track of the depth of recursion when run-ning through the diagram.
Practically, the depth of recursion will belimited by the capacity of the processor; this turns out to be an advan-tage when one tries to model human performance, which is notoriouslylimited in this respect.The transition diagrams used in this type of treatment are conspic-uously similar to the organization of dictionaries for computer use.Even if no other reason existed, this similarity should prompt the com-putational linguist to explore such a possibility.2.5.
Such a scheme would not look very new to stratificationalists.They could argue, as pointed out earlier, that their theory distinguishesbetween the description of regularities and idiosyncracies on the onehand, and the use made of them on the other.
They might even arguethat what I call dictionary is nothing but the set of static relationshipsrepresented in a stratificational network.
However, there are two basicdifferences to be dealt with.
One is that I argued earlier that ira compu-tational device is to model linguistic performance adequately, it mustbe insensitive to noise, either in the form of occultation of the signal362 J~AN PIERI~ VAILL~T(e.g.
unrecognizable characters in a printed form, or morphemes ofunknown classification) or in the form of"  mistakes " (e.g.
occurrencesof ill-formedness).
Thus recognition of a message cannot rely on the"activation" of a stratificational network, which would be blocked inboth cases.
T. 1~.
HOrM^N~ (1971) has proposed a device which hecalls Vigilant Memory, and which is capable of " recognizing" mor-phemes in a string, in the presence of noise.
It performs very well inthe case of unrecognizable or substituted characters, lightly less wellin the case of insertions or deletions, and still less well, but adequately,it seems, in the case of metatheses, which amount o a double substi-tution.
We are now working on the task of combining the possibilitiesof the vigilant memory with the economy of conventional dictionarylookup procedures.
One important point is that the output of a vigilantmemory is a decision on the identity of some form, given a possiblyfaulty input.
This output can be input to another vigilant memory,which will recognize other forms in terms of the previous ones (e.g.words or syntagms in terms of morphemes).
One thus obtains the el-egance of formulation of stratificational networks with a useful insen-sitiveness to noise.2.6.
The other important difference between our proposal andstratificational grammar stems from the fact that the latter is a direct,structural theory: it offers no way of representing an isolated constructof expression or content independently of the general network of rela-tionships describing the overall structure of the language.
We propose,on the contrary, to have two sets ofwellformedness characterizations, onefor expression and one for content.
The link between the two is tobe seen as a collection of procedures, called by the various recognizableforms of expression, which build forms of content according to the well-formedness chemas of content.
To put it another way, units of expres-sion (morphemes, syntagms, entences, etc.)
do not have, or carry, orcorrespond to, a particular meaning, but induce certain computationswhose result is some meaning structure.
This view, which Winogradalso seems to hold, is at the center of our Integrative Semantics (J. P.PAILt~T, T. l~.
HOrMANN, 1972).3.
INTEGRATIVE SEMANTICS3.1.
This proposal for a semantic theory started as an effort todevelop an adequate notation for semantic structures, which should,COMPUTATIONAL LINGUISTICS AND LINGUISTIC THEORY 363among other things, be free from syntax-induced biases (J. p. PAILLET,1973).
By combining the insights of logicians like Frege and Tarski,and of linguists like Hjelmslev and Tesni~re, we come to distinguishfirst semantic forms from semantic processes.3.2.
Semantic processes are of several kinds.
The simplest kind tounderstand is the object of logical proof theory.
It consists in manipulat-ing semantic forms to obtain other semantic forms systematically re-lated to the original ones.
Our cognitive activity, however, does notconsist merely in manipulation of abstract forms.
We find two otherkinds of semantic processes, also represented in Winograd's system.Interpretation processes have long been studied by logicians.
Theyconsist in putting the abstract forms which we hold into correspond-ence with a portion of some universe.
Evaluation processes consistin reaching decisions as to the response which is appropriate in a partic-ular case.
An important point to keep in mind is that such processesare not limited to speech activity.
They are ever present in our con-scious life.
We can look upon speech as a particularly effective wayof nudging someone's cognitive activity in a certain direction.
Thatthis particularly effective tool need not always be successful is apparentin all cases where the hearer fails to understand, misunderstands, etc.3.3.
In a human language, expression is contrained by wellform-edness conditions which limit the possible use and arrengement of mor-phemes.
Consequently, expression is organized in such units as phrasesand sentences, which need not correspond to "complete thoughts ",or, more specifically, to complete structures, of content.
If a new formof content is to be transmitted toa hearer, it will be imparted piecemeal,by the use of separate (usually successive) sentences.
The process ofbuilding a new structure of content, which, for obvious reasons, wecall integration, has to be directed by specific devices (morphemes orconstructions) called integration functors.
Similarly, there are devicesto direct interpretation, such as the deictics, which make a descriptiondefinite, often with recourse to non-linguistic information.
Finallythere are also evaluation functors, which direct the evaluation of theappropriate response.
These functors could not be said to have mean-ing independently of the precise conditions of use.
Their action musttherefore be described in terms of the computational procedures whichthey call for in the hearer.
The simplicity argument suggests that wetreat lexical items in the same way.364 JEAN PIERRE PA~LLET4.
A MODEL OF THE HEARER'S PERFORMANCE4.1.
It is apparent from the foregoing that our proposal is basicallyhearer-oriented.
We believe, indeed, that the speaker must have recours-ed to a set of heuristics, taking into account his knowledge (or imaginedknowledge) of the hearer, his status with respect o him, and manyother variables.
In most tasks of computational linguistics, however,the full complexity of human interactions is fortunately not present.There are a number of simplifying assumptions one can make as tosubject matter, vocabulary, style, etc., so that the task of giving expres-sion to given semantic structures is not overwhelmingly complex.However, from the point of view of linguistic theory, it is much easierto formalize the understanding of speech (phonological perceptionexcluded) than the production of sensible discourses.4.2.
A hearer is primarily a cognitive system capable of building,holding and manipulating semantic forms.
These forms consist ofindividual objects, descriptions for and relations between these objects,and various interacting modalities uch as quantifiers, moods, aspects,and the like.
The manipulation of semantic forms may yield other se-mantic forms as well as judgements of contradiction, consistency, im-plication, and the like (note that these judgments are not limited tolinguistically induced semantic forms: optical illusions are examples ofvisual paradoxes, i.e.
contradiction between two cognitive forms vis-ually extracted from the same object).4.3.
It is conceivable, although not shown, that the cognitive pro-cedures used for building semantic forms in non linguistic situations arethe same as those called forth by lexical items and integrative functors:this is a restatement of Whorf's hypothesis (B. L. WHom~, 1965).
Inany case, lexical items and integration functors are seen as subroutinenames (to use computer terminology) whose perception usually trig-gers the corresponding building procedure.
Similarly, integration ftmc-tors call forth procedures for interrelating parts of semantic forms whichare in the building or already built.4.4.
A hearer is also able to relate semantic forms to a universe ofinterpretation.
This may be part of what we traditionally call attention.In this case, the interpretation functors may be said to direct the heater'sCOMPUTATIONAL LINGUISTICS AND LINGUISTIC THEORY 365attention.
Their action can also be represented by procedures in whichthe best possible referent for a given description is computed.
Some in-terpretation functors direct the selection of an appropriate universe ofreference.
Similarly performatives and other devices are calls for proce-dures of evaluation, in preparation for an appropriate r sponse.4.5.
Naturally, the various kinds of procedures called by elementsof expression i  a discourse are often insufficient in themselves to pro-duce all the results mentioned.
Part of the speaker's communicativeability consists in leaving out much of the information eeded, strictosensu, to build, interpret and evaluate semantic forms, and in selectingonly what is absolutely necessary to lead the hearer in the proper di-rection: hence the term "nudging " used in 3.2.
The rest of the infor-mation, left out by the speaker, has to be provided by the hearer, eitherthrough autonomous manipulation of his semantic forms, or throughrecourse to his store of previous information: his "knowledge of theworld ".
In order to implement this last aspect, R.. QUILLIAN'S (1969)concept of a semantic network is very attractive, and is probably pref-erable to Lamb's version, cognitive networks, which do not to myknowledge incorporate any notion of semantic proximity.4.6.
If syntax is indeed reducible, in its "order structure" aspect,to a set of patterns recognizable by a device like a Vigilant Memory,we have an explanation of why syntactic illformedness i  not more de-structive.
The only thing which counts, from the heater's point ofview, is to be able to recognize some pattern present in his syntacticdictionary, which will then call some building or integrating procedure.Illformedness becomes destructive only when the error-correcting ca-pabilities of the vigilant memory are overwhelmed.
On the other hand,it is always possible, even when recognition occurs, to detect syntacticillformedness through the workings of the memory.REFERENCESt~.
BINNICK, Generative Semantics andAutomatic Translation COLING, Stock-holm, 1969.IL.
BINIqlCK, Epilegomena to a Theory ofWord Formation, unpublished, 1973.D.
G. BOBROW, J.
B. FRASm~, An augmen-ted state transition etwork analysis pro-cedure, in Proc.
Int.
Joint Conf.
on Art.int., Washington, D.C., 1969.G.
GUIL~AUM~, Le cons de linguistique,vols.
1 & 2, Qu6bec, 1971.M.
H~LE, Prolegomena to a Theory ofWord Formation, in ~Linguistic In-quiry ~), IV (1973) 1.M.
A. K. HAtUDAY, Categories of thetheory of grammar, in ~ Word ~, XVII(1961).Z.
S. HAaaIS, String Analysis of sentencestructure, The Hague, 1965.T.
IL.
Ho~raAN~, Lexical redundancy inSpeech Perception: Vigilant Memory, inVII International Congress of PhoneticSciences, Montreal, 1971.A.
K. JosHx, String adjunct grammars, inTDAP no.
75, Philadelphia, 1970.S.
LArca3, Stratificational Linguistics as abasis for Machine translation, in MAKKAI,LOCKWOOD (eds.
), Readings in Stratifi-cational Linguistics, University of Ala-bama Press, 1973.J.
P. PAIl.LET, Prdrequis pour une thdoriesemantique, in ,Cahier de linguisti-que ~, no.
2, Montr&l, 1973.J.
P. PaILL~T, T. tL.
HorMA~, IntegrativeSemantics 1, SILG Monograph No.
2,Carleton University, Ottawa, 1972.g.. QmLLIAN, The teachable LanguageComprehender, in ~ Communications ofthe ACM~), (1969).F.
DE Saussum~, Cours de linguistique gd-n&ale, Paris, 1966, pp.
170-171.J.
THORPE, P. ~BRATLEY, H. DEWAa, TheSyntactic analysis of English by Machine,in D. MIcHte.
(ed.
), Machine Intelli-gence 3, New York, 1968.B.L.
WHOR~, Selected writings, Cam-bridge (Mass.
), 1965.,T.
WINOGaAD, Understanding NaturarLanguage, New York & London, 1972-W. A.
WOoDs, Transition networks folnatural language analysis, in * Commu.nications of the ACM *, XIII (1970) 10.
