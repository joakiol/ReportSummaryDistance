Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 213?216,Sydney, July 2006. c?2006 Association for Computational LinguisticsChinese Named Entity Recognition with a Multi-Phase ModelZhou JunshengState Key Laboratory for Novel Software Tech-nology, Nanjing University,  ChinaDeptartment of Computer Science, NanjingNormal University, Chinazhoujs@nlp.nju.edu.cnHe LiangState Key Laboratory for Novel SoftwareTechnology, Nanjing University, Chinahel@nlp.nju.edu.cnDai XinyuState Key Laboratory for Novel Software Tech-nology, Nanjing University,  Chinadxy@nlp.nju.edu.cnChen JiajunState Key Laboratory for Novel SoftwareTechnology, Nanjing University, Chinachenjj@nlp.nju.edu.cnAbstractChinese named entity recognition is oneof the difficult and challenging tasks ofNLP.
In this paper, we present a Chinesenamed entity recognition system using amulti-phase model.
First, we segment thetext with a character-level CRF model.Then we apply three word-level CRFmodels to the labeling person names, lo-cation names and organization names inthe segmentation results, respectively.Our systems participated in the NER testson open and closed tracks of  MicrosoftResearch (MSRA).
The actual evaluationresults show that our system performswell on both the open tracks and closedtracks.1 IntroductionNamed entity recognition (NER) is a fundamen-tal component for many NLP applications, suchas Information extraction, text Summarization,machine translation and so forth.
In recent years,much attention has been focused on the problemof recognition of Chinese named entities.
Theproblem of Chinese named entity recognition isdifficult and challenging, In addition to the chal-lenging difficulties existing in the counterpartproblem in English, this problem also exhibitsthe following more difficulties: (1) In a Chinesedocument, the names do not have ?boundary to-kens?
such as the capitalized initial letters for aperson name in an English document.
(2) Thereis no space between words in Chinese text, so wehave to segment the text before NER is per-formed.In this paper, we report a Chinese named en-tity recognition system using a multi-phasemodel which includes a basic segmentationphase and three named entity recognition phases.In our system, the implementations of basic seg-mentation components and named entity recogni-tion component are both based on conditionalrandom fields (CRFs) (Lafferty et al, 2001).
Atlast, we apply the rule method to recognize somesimple and short location names and organizationnames in the text.
We will describe each of thesephases in more details below.2 Chinese NER with multi-level models2.1 Recognition ProcessThe input to the recognition algorithm is Chinesecharacter sequence that is not segmented and theoutput is recognized entity names.
The process ofrecognition of Chinese NER is illustrated in fig-ure 1.
First, we segment the text with a character-level CRF model.
After basic segmentation, asmall number of named entities in the text, suchas ????
?, ???????????
and so on,which are segmented as a single word.
Thesesimple single-word entities will be labeled withsome rules in the last phase.
However, a greatnumber of named entities in the text, such as ????????????
?, ???????
?, arenot yet segmented as a single word.
Then, differ-ent from (Andrew et al 2003), we apply threetrained CRFs models with carefully designed andselected features to label person names, locationnames and organization names in the segmenta-tion results, respectively.
At last phase, we applysome rules to tag some names not recognized byCRFs models, and adjust part of the organizationnames recognized by CRFs models.2132.2 Word segmentationWe implemented the basic segmentation compo-nent with linear chain structure CRFs.
CRFs areundirected graphical models that encode a condi-tional probability distribution using a given set offeatures.
In the special case in which the desig-nated output nodes of the graphical model arelinked by edges in a linear chain, CRFs make afirst-order Markov independence assumptionamong output nodes, and thus correspond to fi-nite state machines (FSMs).
CRFs define theconditional probability of a state sequence givenan input sequence as?????
?= ?
?= =?
?TtKkttkkotossfZosP1 11 ),,,(exp1)|( ?Where ),,,( 1 tossf ttk ?
is an arbitrary featurefunction over its arguments, and ?k is a learnedweight for each feature function.Based on CRFs model, we cast the segmenta-tion problem as a sequence tagging problem.
Dif-ferent from (Peng et al, 2004), we represent thepositions of a hanzi (Chinese character) with fourdifferent tags: B for a hanzi that starts a word, Ifor a hanzi that continues the word, F for a hanzithat ends the word, S for a hanzi that occurs as asingle-character word.
The basic segmentation isa process of labeling each hanzi with a tag giventhe features derived from its surrounding context.The features used in our experiment can be bro-ken into two categories: character features andword features.
The character features are instan-tiations of the following templates, similar tothose described in (Ng and Jin, 2004), C refers toa Chinese hanzi.
(a) Cn (n = ?2,?1,0,1,2 )(b) CnCn+1( n = ?2,?1,0,1)(c) C?1C1(d) Pu(C0 )In addition to the character features, we cameup with another type word context feature whichwas found very useful in our experiments.
Thefeature captures the relationship between thehanzi and the word which contains the hanzi.
Fora two-hanzi word, for example, the first hanzi???
within the word ????
will have the fea-ture WC0=TWO_F set to 1, the second hanzi???
within the same word ????
will have thefeature WC0=TWO_L set to 1.
For the three-hanzi word, for example, the first hanzi ??
?within a word ?????
will have the featureWC0=TRI_F set to 1, the second hanzi ??
?within the same word ?????
will have thefeature WC0=TRI_M set to 1, and the last hanzi???
within the same word ?????
will havethe feature WC0=TRI_L set to 1.
Similarly, thefeature can be extended to a four-hanzi word.2.3 Named entity tagging with CRFsAfter basic segmentation, we use three word-level CRFs models to label person names, loca-tion names and organization names, respectively.The important factor in applying CRFs model toname entity recognition is how to select theproper features set.
Most of entity names do nothave any common structural characteristics ex-cept for containing some feature words,  such as???
?, ???
?, ???
, ???
and so on.
In addi-tion, for person names, most names include acommon surname, e.g.
??
?, ???.
But as aproper noun, the occurrence of an entity namehas the specific context.
In this section, we onlypresent our approach to organization name rec-ognition.
For example, the context information oforganization name mainly includes the boundarywords and some title words (e.g.
??????
).By analyzing a large amount of entity name cor-pora, we find that the indicative intensity of dif-ferent boundary words vary greatly.
So we dividethe left and right boundary words into twoclasses according to the indicative intensity.
Ac-cordingly we construct the four boundary wordslexicons.
To solve the problem of the selectionand classification of boundary words, we makeuse of mutual Information I(x, y).
If there is agenuine association between x and y, then I(x, y)>>0.
If there is no interesting relationship be-Unsegmented textWord SegmentationPerson Names RecognitionRecognized Named EntitiesProcessing with RulesLocation Names RecognitionOrganization Names RecognitionFig1.
Chinese NER process214tween x and y, then I(x, y)?0.
If x and y are incomplementary distribution, then I(x, y) << 0.By using mutual information, we compute theassociation between boundary word and the typeof organization name, then select and classify theboundary words.
Some example boundary wordsfor organization names are listed in table 1.Table 1.
The classified boundary words for ORG namesBased on the consideration given in precedingsection, we constructed a set of atomic featurepatterns, listed in table 2.
Additionally, we de-fined a set of conjunctive feature patterns, whichcould form effective feature conjunctions to ex-press complicated contextual information.Table 2.
Atomic feature patterns for ORG namesAtomic pattern Meaning of patternCurWord Current wordLocationName Check if current word is a locationnamePersonName Check if current word is a personnameKnownORG Check if current word is a knownorganization nameORGFeatureScanFeatureWord_8Check if current word is a featureword of ORG nameCheck if there exist a feature wordamong eight words behind thecurrent wordLeftBoundary1_-2LeftBoundary2_-2Check if there exist a first-class orsecond-class left boundary wordamong two words before the cur-rent wordRightBoundary1_+2RightBoundary2_+2Check if there exist a first-class orsecond-class right boundary wordamong two words behind the cur-rent word2.4 Processing with rulesThere exists some single-word named entitiesthat aren?t tagged by CRFs models.
We recog-nize these single-word named entities with somerules.
We first construct two known locationnames and organization names dictionaries andtwo feature words lists for location names andorganization names.
In closed track, we collectknown location names and organization namesonly from training corpus.
The recognition proc-ess is described below.
For each word in the text,we first check whether it is a known location ororganization names according to the known loca-tion names and organization names dictionaries.If it isn?t a known name, then we further checkwhether it is a known word.
If it is not a knownword also, we next check whether the word endswith a feature word of location or organizationnames.
If it is, we label it as a location or organi-zation name.In addition, we introduce some rules to adjustorganization names recognized by CRF modelbased on the labeling specification of MRSAcorpus.
For example, the string ???????????
?
is recognized as an organizationname, but the string should be divided into twonames: a location name (?????)
and a or-ganization name (?????????
), accordingto label specification, so we add some rules toadjust it.3 Experimental resultsWe participated in the three GB tracks in thethird international Chinese language processingbakeoff: NER msra-closed, NER msra-open andWS msra-open.
In the closed track, we con-structed all dictionaries only with the words ap-pearing in the training corpus.
In the closed track,we didn?t use the same feature characters lists forlocation names and organization names as in theopen tracks and we collected the feature charac-ters from the training data in the closed track.
Weconstructed feature characters lists for locationnames and organization names by the followingapproach.
First, we extract all suffix string for alllocation names and organization names in thetraining data and count the occurrence of thesesuffix strings in all location names and organiza-tion names.
Second, we check every suffix stringto judge whether it is a known word.
If a suffixstring is not a known word, we discard it.
Finally,in the remaining suffix words, we select the fre-quently used suffix words as the feature charac-ters whose counts are greater than the threshold.We set different thresholds for single-characterfeature words and multi-character feature words.Similar approaches were taken to the collectionof common Chinese surnames in the closed track.While making training data for segmentationmodel, we adopted different tagging methods fororganization names in the closed track and in theopen track.
In the closed track, we regard everyorganization name, such as ?????????
?, as a single word.
But, in the open track, wesegment a long organization name into severalwords.
For example, the organization name ?
?Type Class ExamplesFirst-class ??
?6.0006?Left boundaryword Second-class ??
?3.1161?First -class ??
?5.4531?Right boundaryword Second-class  ???2.0135?215????????
would be divided into threewords: ????
?, ????
and ?????.
Thedifferent tagging methods at segmentation phasewould bring different effect to organizationnames recognition.
The size of training data usedin the open tracks is same as the closed tracks.We have not employed any additional trainingdata in the open tracks.
Table 3 shows the per-formance of our systems for NER in the bakeoff.Table 3: Named entity recognition outcomeTrack P  R F  Per-F Loc-F Org-FNER msraclosed88.94  84.20 86.51 90.09 85.45 83.10NER msraopen90.76 89.22 89.99 92.61 90.99 83.97For the separate word segmentation task(WS),the above NER task is performed first.
Then weadded several additional processing steps on theresult of named entity recognition.
As we allknow, disambiguation problem is one of the keyissue in Chinese words segmentation.
In this task,some ambiguities were resolved through a rule-set which was automatically constructed basedon error driven learning theory.
The pre-constructed rule-set stored many pseudo-ambiguity strings and gave their correct segmen-tations.
After analyzing the result of our NERbased on CRFs model, we noticed that it presentsa high recall on out-of-vocabulary.
But at thesame time, some characters and words werewrongly combined as new words which causedthe losing of the precision of OOV and the recallof IV.
To this phenomenon, we adopted an un-conditional rule, that if a word, except recog-nized name entity, was detected as a new wordand its length was more than 6 (Chinese Charac-ters), and it should be segmented as several in-vocabulary words based on the combination ofFMM and BMM methods.
Table 4 shows theresult of our systems for word segmentation inthe bakeoff.Table 4: Word segmentation outcomeTrack P R F OOV-R IV-RWS msraopen 0.975 0.976 0.975 0.811 0.9814 ConclusionWe have presented our Chinese named entityrecognition system with a multi-phase model andits result for Msra_open and mrsa_closed tracks.Our open and closed GB track experiments showthat its performance is competitive.
We will tryto select more useful feature functions into theexisting segmentation model and named entityrecognition model in future work.ReferenceAitao Chen.
2003.
Chinese Word Segmentation UsingMinimal Linguistic Knowledge.
In Proceedings ofthe Second SIGHAN Workshop on Chinese Lan-guage Processing.Andrew McCallum, Wei Li.
2003.
Early Results forNamed Entity Recognition with Conditional Ran-dom Fields, Feature Induction and Web-EnhancedLexicons.
Proceedings of the Seventh CoNLL con-ference, Edmonton,J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proc.ICML 01.Ng, Hwee Tou and Jin Kiat Low.
2004.
Chinese Part-of-Speech Taging: One-at-a-Time or All at Once?Word-based or Character based?
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, Spain.Peng, Fuchun, Fangfang Feng, and AndrewMcCallum.
2004.
Chinese Segmentation and NewWord Detection using Conditional Random Fields .In Proceedings of the Twentith International Con-ference on Computaional Linguistics.216
