Proceedings of the 43rd Annual Meeting of the ACL, pages 515?522,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Phonotactic Language Model for Spoken Language IdentificationHaizhou Li and Bin MaInstitute for Infocomm ResearchSingapore 119613{hli,mabin}@i2r.a-star.edu.sgAbstractWe have established a phonotactic lan-guage model as the solution to spokenlanguage identification (LID).
In thisframework, we define a single set ofacoustic tokens to represent the acousticactivities in the world?s spoken languages.A voice tokenizer converts a spokendocument into a text-like document ofacoustic tokens.
Thus a spoken documentcan be represented by a count vector ofacoustic tokens and token n-grams in thevector space.
We apply latent semanticanalysis to the vectors, in the same waythat it is applied in information retrieval,in order to capture salient phonotacticspresent in spoken documents.
The vectorspace modeling of spoken utterances con-stitutes a paradigm shift in LID technol-ogy and has proven to be very successful.It presents a 12.4% error rate reductionover one of the best reported results onthe 1996 NIST Language RecognitionEvaluation database.1 IntroductionSpoken language and written language are similarin many ways.
Therefore, much of the research inspoken language identification, LID, has been in-spired by text-categorization methodology.
Bothtext and voice are generated from language de-pendent vocabulary.
For example, both can be seenas stochastic time-sequences corrupted by a chan-nel noise.
The n-gram language model hasachieved equal amounts of success in both tasks,e.g.
n-character slice for text categorization by lan-guage (Cavnar and Trenkle, 1994) and Phone Rec-ognition followed by n-gram Language Modeling,or PRLM (Zissman, 1996) .Orthographic forms of language, ranging fromLatin alphabet to Cyrillic script to Chinese charac-ters, are far more unique to the language than theirphonetic counterparts.
From the speech productionpoint of view, thousands of spoken languages fromall over the world are phonetically articulated us-ing only a few hundred distinctive sounds or pho-nemes (Hieronymus, 1994).
In other words,common sounds are shared considerably acrossdifferent spoken languages.
In addition, spokendocuments1, in the form of digitized wave files, arefar less structured than written documents and needto be treated with techniques that go beyond thebounds of written language.
All of this makes theidentification of spoken language based on pho-netic units much more challenging than the identi-fication of written language.
In fact, the challengeof LID is inter-disciplinary, involving digital signalprocessing, speech recognition and natural lan-guage processing.In general, a LID system usually has three fun-damental components as follows:1) A voice tokenizer which segments incomingvoice feature frames and associates the seg-ments with acoustic or phonetic labels, calledtokens;2) A statistical language model which captureslanguage dependent phonetic and phonotacticinformation from the sequences of tokens;3) A language classifier which identifies the lan-guage based on discriminatory characteristicsof acoustic score from the voice tokenizer andphonotactic score from the language model.In this paper, we present a novel solution to thethree problems, focusing on the second and thirdproblems from a computational linguistic perspec-tive.
The paper is organized as follows: In Section2, we summarize relevant existing approaches tothe LID task.
We highlight the shortcomings ofexisting approaches and our attempts to address the1 A spoken utterance is regarded as a spoken document in thispaper.515issues.
In Section 3 we propose the bag-of-soundsparadigm to turn the LID task into a typical textcategorization problem.
In Section 4, we study theeffects of different settings in experiments on the1996 NIST Language Recognition Evaluation(LRE) database2.
In Section 5, we conclude ourstudy and discuss future work.2 Related WorkFormal evaluations conducted by the National In-stitute of Science and Technology (NIST) in recentyears demonstrated that the most successful ap-proach to LID used the phonotactic content of thevoice signal to discriminate between a set of lan-guages (Singer et al, 2003).
We briefly discussprevious work cast in the formalism mentionedabove: tokenization, statistical language modeling,and language identification.
A typical LID systemis illustrated in Figure 1 (Zissman, 1996), wherelanguage dependent voice tokenizers (VT) and lan-guage models (LM) are deployed in the ParallelPRLM architecture, or P-PRLM.Figure 1.
L monolingual phoneme recognitionfront-ends are used in parallel to tokenize the inpututterance, which is analyzed by LMs to predict thespoken language2.1 Voice TokenizationA voice tokenizer is a speech recognizer thatconverts a spoken document into a sequence oftokens.
As illustrated in Figure 2, a token can be ofdifferent sizes, ranging from a speech featureframe, to a phoneme, to a lexical word.
A token isdefined to describe a distinct acoustic/phoneticactivity.
In early research, low level spectral2 http://www.nist.gov/speech/tests/frames, which are assumed to be independent ofeach other, were used as a set of prototypical spec-tra for each language (Sugiyama, 1991).
By adopt-ing hidden Markov models, people moved beyondlow-level spectral analysis towards modeling aframe sequence into a larger unit such as a pho-neme and even a lexical word.Since the lexical word is language specific, thephoneme becomes the natural choice when build-ing a language-independent voice tokenizationfront-end.
Previous studies show that parallel lan-guage-dependent phoneme tokenizers effectivelyserve as the tokenization front-ends with P-PRLMbeing the typical example.
However, a language-independent phoneme set has not been exploredyet experimentally.
In this paper, we would like toexplore the potential of voice tokenization using aunified phoneme set.Figure 2 Tokenization at different resolutions2.2 n-gram Language ModelWith the sequence of tokens, we are able to es-timate an n-gram language model (LM) from thestatistics.
It is generally agreed that phonotactics,i.e.
the rules governing the phone/phonemes se-quences admissible in a language, carry more lan-guage discriminative information than thephonemes themselves.
An n-gram LM over thetokens describes well n-local phonotactics amongneighboring tokens.
While some systems modelthe phonotactics at the frame level (Torres-Carrasquillo et al, 2002), others have proposed P-PRLM.
The latter has become one of the mostpromising solutions so far (Zissman, 1996).A variety of cues can be used by humans andmachines to distinguish one language from another.These cues include phonology, prosody, morphol-ogy, and syntax in the context of an utterance.VT-1: ChineseVT-2: EnglishVT-L: FrenchLM-L: FrenchLM-1 ?
LM-LLM-L: FrenchLM-1 ?
LM-LLM-L: FrenchLM-1 ?
LM-Llanguage classifierspoken utterancehypothesizedlanguagewordphonemeframe516However, global phonotactic cues at the level ofutterance or spoken document remains unexploredin previous work.
In this paper, we pay special at-tention to it.
A spoken language always contains aset of high frequency function words, prefixes, andsuffixes, which are realized as phonetic token sub-strings in the spoken document.
Individually, thosesubstrings may be shared across languages.
How-ever, the pattern of their co-occurrences discrimi-nates one language from another.Perceptual experiments have shown (Mut-husamy, 1994) that with adequate training, humanlisteners?
language identification ability increaseswhen given longer excerpts of speech.
Experi-ments have also shown that increased exposure toeach language and longer training sessions im-prove listeners?
language identification perform-ance.
Although it is not entirely clear how humanlisteners make use of the high-order phonotac-tic/prosodic cues present in longer spans of a spo-ken document, strong evidence shows thatphonotactics over larger context provides valuableLID cues beyond n-gram, which will be furtherattested by our experiments in Section 4.2.3 Language ClassifierThe task of a language classifier is to makegood use of the LID cues that are encoded in themodel l?
to hypothesize from among L lan-guages, ?
, as the one that is actually spoken in aspoken document O.
The LID modell?l?
in P-PRLM refers to extracted information from acous-tic model and n-gram LM for language l.  We haveand { ,AM } LLMl l l?
?
?=  ( 1,..., )l l?
??
= .
A maxi-mum-likelihood classifier can be formulated asfollows:( ) (?
arg max ( / )arg max / , /llAM LMl ll Tl P OP O T P T??
?????
??=?
?
))(1)The exact computation in Eq.
(1) involves sum-ming over all possible decoding of token se-quences T given O.
In many implementations,it is approximated by the maximum over all se-quences in the sum by finding the most likely to-ken sequence, , for each language l, using theViterbi algorithm:?
?l?T( ) (?
?
?arg max[ / , / ]AM LMl l l lll P O T P T?
????
(2)Intuitively, individual sounds are heavily sharedamong different spoken languages due to the com-mon speech production mechanism of humans.Thus, the acoustic score has little language dis-criminative ability.
Many experiments (Yan andBarnard, 1995; Zissman, 1996) have further at-tested that the n-gram LM score provides morelanguage discriminative information than theiracoustic counterparts.
In Figure 1, the decoding ofvoice tokenization is governed by the acousticmodel AMl?
to arrive at an acoustic score ( )?/ , AMl lP O T ?
and a token sequence .
The n-gram LM derives the n-local phonotactic scorel?T( )?
/ LMl lP T ?
from the language model LMl?
.Clearly, the n-gram LM suffers the major short-coming of having not exploited the global phono-tactics in the larger context of a spoken utterance.Speech recognition researchers have so far chosento only use n-gram local statistics for primarilypragmatic reasons, as this n-gram is easier to attain.In this work, a language independent voice tokeni-zation front-end is proposed, that uses a unifiedacoustic model  AM?
instead of multiple languagedependent acoustic models AMl?
.
The n-gramLM LMl?
is generalized to model both local andglobal phonotactics.3 Bag-of-Sounds ParadigmThe bag-of-sounds concept is analogous to thebag-of-words paradigm originally formulated inthe context of information retrieval (IR) and textcategorization (TC) (Salton 1971; Berry et al,1995; Chu-Caroll and Carpenter, 1999).
One focusof IR is to extract informative features for docu-ment representation.
The bag-of-words paradigmrepresents a document as a vector of counts.
It isbelieved that it is not just the words, but also theco-occurrence of words that distinguish semanticdomains of text documents.Similarly, it is generally believed in LID that, al-though the sounds of different spoken languagesoverlap considerably, the phonotactics differenti-ates one language from another.
Therefore, one caneasily draw the analogy between an acoustic tokenin bag-of-sounds and a word in bag-of-words.Unlike words in a text document, the phonotacticinformation that distinguishes spoken languages is517concealed in the sound waves of spoken languages.After transcribing a spoken document into a textlike document of tokens, many IR or TC tech-niques can then be readily applied.It is beyond the scope of this paper to discusswhat would be a good voice tokenizer.
We adoptphoneme size language-independent acoustic to-kens to form a unified acoustic vocabulary in ourvoice tokenizer.
Readers are referred to (Ma et al,2005) for details of acoustic modeling.3.1 Vector Space ModelingIn human languages, some words invariably occurmore frequently than others.
One of the mostcommon ways of expressing this idea is known asZipf?s Law (Zipf, 1949).
This law states that thereis always a set of words which dominates most ofthe other words of the language in terms of theirfrequency of use.
This is true both of written wordsand of spoken words.
The short-term, or local pho-notactics, is devised to describe Zipf?s Law.The local phonotactic constraints can be typi-cally described by the token n-grams, or phonemen-grams as in (Ng et al, 2000), which representsshort-term statistics such as lexical constraints.Suppose that we have a token sequence, t1 t2 t3 t4.We derive the unigram statistics from the tokensequence itself.
We derive the bigram statisticsfrom t1(t2) t2(t3) t3(t4) t4(#) where the token vo-cabulary is expanded over the token?s right context.Similarly, we derive the trigram statistics from thet1(#,t2) t2(t1,t3) t3(t2,t4) t4(t3,#) to account for leftand right contexts.
The # sign is a place holder forfree context.
In the interest of manageability, wepropose to use up to token trigram.
In this way, foran acoustic system of Y  tokens, we have poten-tially bigram and Y trigram in the vocabulary.
2Y 3Meanwhile, motivated by the ideas of havingboth short-term and long-term phonotactic statis-tics, we propose to derive global phonotactics in-formation to account for long-term phonotactics:The global phonotactic constraint is the high-order statistics of n-grams.
It represents documentlevel long-term phonotactics such as co-occurrences of n-grams.
By representing a spokendocument as a count vector of n-grams, also calledbag-of-sounds vector, it is possible to explore therelations and higher-order statistics among the di-verse n-grams through latent semantic analysis(LSA).It is often advantageous to weight the rawcounts to refine the contribution of each n-gram toLID.
We begin by normalizing the vectors repre-senting the spoken document by making each vec-tor of unit length.
Our second weighting is basedon the notion that an n-gram that only occurs in afew languages is more discriminative than an n-gram that occurs in nearly every document.
We usethe inverse-document frequency (idf) weightingscheme (Spark Jones, 1972), in which a word isweighted inversely to the number of documents inwhich it occurs, by means of( ) log / ( )idf w D d w=  , where w is a word in thevocabulary of W token n-grams.
D is the total num-ber of documents in the training corpus from L lan-guages.
Since each language has at least onedocument in the training corpus, we have D L?
.is the number of documents containing theword w. Letting be the count of word w indocument d, we have the weighted count as( )d w,w dc2 1/ 2, , ,1( ) /( )w d w d w dw Wc c idf w c ???
??
= ?
?
(3)and a vector to representdocument d. A corpus is then represented by aterm-document matrix1, 2, ,{ , ,..., }Td d d W dc c c c?
?
?=1 2{ , ,..., }DH c c c= of W D?
.3.2 Latent Semantic AnalysisThe fundamental idea in LSA is to reduce thedimension of a document vector, W to Q, whereQ W<< and Q D<<  , by projecting the probleminto the space spanned by the rows of the closestrank-Q matrix to H in the Frobenius norm (Deer-wester et al 1990).
Through singular value de-composition (SVD) of H, we construct a modifiedmatrix HQ from the Q-largest singular values:TQ Q Q QH U S V=                         (4)QU is a W Q?
left singular matrix with rows,1wu w W?
?
QS; is a Q Q?
diagonal matrix of Q-largest singular values of H; is QV D Q?
right sin-gular matrix with rows , 1 .
dv d D?
?With the SVD, we project the D document vec-tors in H into a reduced space  , referred to asQ-space in the rest of this paper.
A test documentof unknown language ID is mapped to apseudo-document in the Q-space by matrixQVpcpv QU5181Tp p p Qc v c U S??
= Q   (5)After SVD, it is straightforward to arrive at anatural metric for the closeness between two spo-ken documents  and in Q-space instead oftheir original W-dimensional space  and .iv jvic jc( , ) cos( , )|| || || ||Ti ji j i ji jv vg c c v vv v??
= ?
(6)( , )i jg c c  indicates the similarity between two vec-tors, which can be transformed to a distance meas-ure .
1( , ) cos ( , )i j i jk c c g c c?=In the forced-choice classification, a test docu-ment, supposedly monolingual, is classified intoone of the L languages.
Note that the test documentis unknown to the H matrix.
We assume consis-tency between the test document?s intrinsic phono-tactic pattern and one of the D patterns, that isextracted from the training data and is presented inthe H matrix, so that the SVD matrices still applyto the test document, and Eq.
(5) still holds for di-mension reduction.3.3 Bag-of-Sounds Language ClassifierThe bag-of-sounds phonotactic LM benefits fromseveral properties of vector space modeling andLSA.1) It allows for representing a spoken documentas a vector of n-gram features, such as unigram,bigram, trigram, and the mixture of them;2) It provides a well-defined distance metric formeasurement of phonotactic distance betweenspoken documents;3) It processes spoken documents in a lower di-mensional Q-space, that makes the bag-of-sounds phonotactic language modeling, LMl?
,and classification computationally manageable.Suppose we have only one prototypical vectorand its projection in the Q-space to representlanguage l. Applying LSA to the term-documentmatrixlc lv:H W L?
, a minimum distance classifier isformulated:?
arg min ( , )p lll k v?
?= v    (7)In Eq.
(7), is the Q-space projection of , a testdocument.pv pcApparently, it is very restrictive for each lan-guage to have just one prototypical vector, alsoreferred to as a centroid.
The pattern of languagedistribution is inherently multi-modal, so it isunlikely well fitted by a single vector.
One solutionto this problem is to span the language space withmultiple vectors.
Applying LSA to a term-document matrix :H W L??
, where L L as-suming each language l is represented by a set ofM vectors,M?
= ?l?
, a new classifier, using k-nearestneighboring rule (Duda and Hart, 1973) , is formu-lated, named k-nearest classifier (KNC):?
arg min ( , )lp ll ll k????
?
?= v v?
(8)where l?
is the set of k-nearest-neighbor to  and  pvl l?
?
?
.Among many ways to derive the M centroid vec-tors, here is one option.
Suppose that we have a setof training documents Dl for language l , as subsetof corpus ?
,  and .
To derivethe M vectors, we choose to carry out vector quan-tization (VQ) to partition DlD ??
1Ll lD=?
= ?ll  into M cells Dl,m in theQ-space such that 1 ,Mm l mD D=?
=  using similaritymetric Eq.(6).
All the documents in each cell,l mD can then be merged to form a super-document,which is further projected into a Q-space vector.
This results in M prototypical centroids.
Using KNC, a test vector iscompared with M vectors to arrive at the k-nearestneighbors for each language, which can be compu-tationally expensive when M is large.,l mv, ( 1,...l m l )M?
?v m =Alternatively, one can account for multi-modaldistribution through finite mixture model.
A mix-ture model is to represent the M discrete compo-nents with soft combination.
To extend the KNCinto a statistical framework, it is necessary to mapour distance metric Eq.
(6) into a probability meas-ure.
One way is for the distance measure to inducea family of exponential distributions with pertinentmarginality constraints.
In practice, what we needis a reasonable probability distribution, whichsums to one, to act as a lookup table for the dis-tance measure.
We here choose to use the empiri-cal multivariate distribution constructed byallocating the total probability mass in proportionto the distances observed with the training data.
Inshort, this reduces the task to a histogram normali-zation.
In this way, we map the distanceto a conditional probability distribution( , )i jk c c( | )i jp v v519subject to .
Now that we are in theprobability domain, techniques such as mixturesmoothing can be readily applied to model a lan-guage class with finer fitting.| |1( | ) 1i ji p v v?= =?Let?s re-visit the task of L language forced-choice classification.
Similar to KNC, suppose wehave M centroids  in the Q-space for each language l. Each centroid representsa class.
The class conditional probability can bedescribed as a linear combination of,  ( 1,... )l m lv m??
= M,( | )i l mp v v :,1( | ) ( ) ( | )MLMi l l m i l mm,p v p v p v?==?
v)(9)the probability ,( l mp v , functionally serves as amixture weight of ,( | )i l mp v v .
Together with a setof centroids , ,  ( 1,...l m lv m )??
= ,( | )i l mM p v v)and,( l mp v  define a mixture modelLMl?
.
,( | )i l mp v vis estimated by histogram normalization and,( l m )p v is estimated under the maximum likelihoodcriteria, , ,( ) /l m m l lp v C= C  , where C  is totalnumber of documents in Dll, of which C docu-ments fall into the cell m.,m lAn Expectation-Maximization iterative processcan be devised for training of LMl?
to maximize thelikelihood Eq.
(9) over the entire training corpus:| |1 1( | ) ( | )lDLLMd ll dp p v ?= =?
?
=??
(10)Using the phonotactic LM score ( )?
/ LMl lP T forclassification, with T  being represented by thebag-of-sounds vector v ,  Eq.
(2) can be reformu-lated as Eq.
(11),  named mixture-model classifier(MMC):?l?p, ,1?
arg max ( | )arg max ( ) ( | )LMp llMl m p l ml ml p vp v p v v?????
=== ?
(11)To establish fair comparison with P-PRLM, asshown in Figure 3, we devise our bag-of-soundsclassifier to solely use the LM score ( )?
/ LMl lP T ?
for classification decision whereas theacoustic score ( )?/ , AMl lP O may potentially helpas reported in (Singer et al, 2003).T ?Figure 3.
A bag-of-sounds classifier.
A unifiedfront-end followed by L parallel bag-of-soundsphonotactic LMs.4 ExperimentsThis section will experimentally analyze the per-formance of the proposed bag-of-sounds frame-work using the 1996 NIST Language RecognitionEvaluation (LRE) data.
The database was intendedto establish a baseline of performance capabilityfor language recognition of conversational tele-phone speech.
The database contains recordedspeech of 12 languages: Arabic, English, Farsi,French, German, Hindi, Japanese, Korean, Manda-rin, Spanish, Tamil and Vietnamese.
We use thetraining set and development set from LDC Call-Friend corpus3 as the training data.
Each conversa-tion is segmented into overlapping sessions ofabout 30 seconds each, resulting in about 12,000sessions for each language.
The evaluation set con-sists of 1,492 30-sec sessions, each distributedamong the various languages of interest.
We treat a30-sec session as a spoken document in both train-ing and testing.
We report error rates (ER) of the1,492 test trials.4.1 Effect of Acoustic VocabularyThe choice of n-gram affects the performance ofLID systems.
Here we would like to see how a bet-ter choice of acoustic vocabulary can help converta spoken document into a phonotactically dis-criminative space.
There are two parameters thatdetermine the acoustic vocabulary: the choice ofacoustic token, and the choice of n-grams.
In thispaper, the former concerns the size of an acousticsystem Y in the unified front-end.
It is studied inmore details in (Ma et al, 2005).
We set Y to 32 in3 See http://www.ldc.upenn.edu/.
The overlap between 1996NIST evaluation data and CallFriend database has been re-moved from training data as suggested in the 2003 NIST LREwebsite http://www.nist.gov/speech/tests/index.htmLMl?
LM-L:  FrenchUnified VT1LM?
LM-1: Chinese2LM?
LM-2: EnglishLanguage Classifierspoken utteranceHypothesized languageAM?520this experiment; the latter decides what features tobe included in the vector space.
The vector spacemodeling allows for multiple heterogeneous fea-tures in one vector.
We introduce three types ofacoustic vocabulary (AV) with mixture of tokenunigram, bigram, and trigram:a) AV1: 32 broad class phonemes as unigram,selected from 12 languages, also referred to asP-ASM as detailed in (Ma et al, 2005)b) AV2: AV1 augmented by 32  bigrams ofAV1, amounting to 1,056 tokens32?c) AV3: AV2 augmented by 32  tri-grams of AV1, amounting to 33,824 tokens32 32?
?AV1 AV2 AV3ER % 46.1 32.8 28.3Table 1.
Effect of acoustic vocabulary (KNC)We carry out experiments with KNC classifierof 4,800 centroids.
Applying k-nearest-neighboringrule, k is empirically set to 3.
The error rates arereported in Table 1 for the experiments over thethree AV types.
It is found that high-order token n-grams improve LID performance.
This reaffirmsmany previous findings that n-gram phonotacticsserves as a valuable cue in LID.4.2 Effect of Model SizeAs discussed in KNC, one would expect to im-prove the phonotactic model by using more cen-troids.
Let?s examine how the number of centroidvectors M affects the performance of KNC.
We setthe acoustic system size Y to 128, k-nearest to 3,and only use token bigrams in the bag-of-soundsvector.
In Table 2, it is not surprising to find thatthe performance improves as M increases.
How-ever, it is not practical to have large M be-cause comparisons need to take place ineach test trial.L L M?
= ?#M 1,200 2,400 4,800 12,000ER % 17.0 15.7 15.4 14.8Table 2.
Effect of number of centroids (KNC)To reduce computation, MMC attempts to useless number of mixtures M to represent the phono-tactic space.
With the smoothing effect of the mix-ture model, we expect to use less computation toachieve similar performance as KNC.
In the ex-periment reported in Table 3, we find that MMC(M=1,024) achieves 14.9% error rate, which al-most equalizes the best result in the KNC experi-ment (M=12,000) with much less computation.#M 4 16 64 256 1,024ER % 29.6 26.4 19.7 16.0 14.9Table 3.
Effect of number of mixtures (MMC)4.3 DiscussionThe bag-of-sounds approach has achieved equalsuccess in both 1996 and 2003 NIST LRE data-bases.
As more results are published on the 1996NIST LRE database, we choose it as the platformof comparison.
In Table 4, we report the perform-ance across different approaches in terms of errorrate for a quick comparison.
MMC presents a12.4% ER reduction over the best reported result4(Torres-Carrasquillo et al, 2002).It is interesting to note that the bag-of-soundsclassifier outperforms its P-PRLM counterpart by awide margin (14.9% vs 22.0%).
This is attributedto the global phonotactic features in LMl?
.
Theperformance gain in (Torres-Carrasquillo et al,2002; Singer et al, 2003) was obtained mainly byfusing scores from several classifiers, namelyGMM, P-PRLM and SVM, to benefit from bothacoustic and language model scores.
Noting thatthe bag-of-sounds classifier in this work solely re-lies on the LM score, it is believed that fusing withscores from other classifiers will further boost theLID performance.ER %P-PRLM5 22.0P-PRLM + GMM acoustic5 19.5P-PRLM + GMM acoustic +GMM tokenizer517.0Bag-of-sounds classifier (MMC) 14.9Table 4.
Benchmark of different approachesBesides the error rate reduction, the bag-of-sounds approach also simplifies the on-line com-puting procedure over its P-PRLM counterpart.
Itwould be interesting to estimate the on-line com-putational need of MMC.
The cost incurred hastwo main components: 1) the construction of the4 Previous results are also reported in DCF, DET, and equalerror rate (EER).
Comprehensive benchmarking for bag-of-sounds phonotactic LM will be reported soon.5 Results extracted from (Torres-Carrasquillo et al, 2002)521pseudo document vector, as done via Eq.
(5); 2)vector comparisons.
The computingcost is estimated to be  per test trial(Bellegarda, 2000).
For typical values of Q, thisamounts to less than 0.05 Mflops.
While this ismore expensive than the usual table look-up inconventional n-gram LM, the performance im-provement is able to justify the relatively modestcomputing overhead.L L M?
= ?2( )QO5 ConclusionWe have proposed a phonotactic LM approach toLID problem.
The concept of bag-of-sounds is in-troduced, for the first time, to model phonotacticspresent in a spoken language over a larger context.With bag-of-sounds phonotactic LM, a spokendocument can be treated as a text-like document ofacoustic tokens.
This way, the well-establishedLSA technique can be readily applied.
This novelapproach not only suggests a paradigm shift in LID,but also brings 12.4% error rate reduction over oneof the best reported results on the 1996 NIST LREdata.
It has proven to be very successful.We would like to extend this approach to otherspoken document categorization tasks.
In monolin-gual spoken document categorization, we suggestthat the semantic domain can be characterized bylatent phonotactic features.
Thus it is straightfor-ward to extend the proposed bag-of-sounds frame-work to spoken document categorization.AcknowledgementThe authors are grateful to Dr. Alvin F. Martin ofthe NIST Speech Group for his advice when pre-paring the 1996 NIST LRE experiments, to Dr G.M.
White and Ms Y. Chen of Institute for Info-comm Research for insightful discussions.ReferencesJerome R. Bellegarda.
2000.
Exploiting latent semanticinformation in statistical language modeling, In Proc.of the IEEE, 88(8):1279-1296.M.
W. Berry, S.T.
Dumais and G.W.
O?Brien.
1995.Using Linear Algebra for intelligent information re-trieval, SIAM Review, 37(4):573-595.William B. Cavnar, and John M. Trenkle.
1994.
N-Gram-Based Text Categorization, In Proc.
of 3rdAnnual Symposium on Document Analysis and In-formation Retrieval, pp.
161-169.Jennifer Chu-Carroll, and Bob Carpenter.
1999.
Vector-based Natural Language Call Routing, Computa-tional Linguistics, 25(3):361-388.S.
Deerwester, S. Dumais, G. Furnas, T. Landauer, andR.
Harshman, 1990, Indexing by latent semanticanalysis, Journal of the American Society for Infor-matin Science, 41(6):391-407Richard O. Duda and Peter E. Hart.
1973.
Pattern Clas-sification and scene analysis.
John Wiley & SonsJames L. Hieronymus.
1994.
ASCII Phonetic Symbolsfor the World?s Languages: Worldbet.
Technical Re-port AT&T Bell Labs.Spark Jones, K. 1972.
A statistical interpretation ofterm specificity and its application in retrieval, Jour-nal of Documentation, 28:11-20Bin Ma, Haizhou Li and Chin-Hui Lee, 2005.
An Acous-tic Segment Modeling Approach to Automatic Lan-guage Identification, submitted to Interspeech 2005Yeshwant K. Muthusamy, Neena Jain, and Ronald A.Cole.
1994.
Perceptual benchmarks for automaticlanguage identification, In Proc.
of ICASSPCorinna Ng , Ross Wilkinson , Justin Zobel, 2000., Speech Communication, 32(1-2):61-77Ex-periments in spoken document retrieval using pho-neme n-gramsG.
Salton, 1971.
The SMART Retrieval System, Pren-tice-Hall, Englewood Cliffs, NJ, 1971E.
Singer, P.A.
Torres-Carrasquillo, T.P.
Gleason, W.M.Campbell and D.A.
Reynolds.
2003.
Acoustic, Pho-netic and Discriminative Approaches to Automaticlanguage recognition, In Proc.
of EurospeechMasahide Sugiyama.
1991.
Automatic language recog-nition using acoustic features, In Proc.
of ICASSP.Pedro A. Torres-Carrasquillo, Douglas A. Reynolds,and J.R. Deller.
Jr. 2002.
Language identification us-ing Gaussian Mixture model tokenization, in Proc.
ofICASSP.Yonghong Yan, and Etienne Barnard.
1995.
An ap-proach to automatic language identification based onlanguage dependent phone recognition, In Proc.
ofICASSP.George K. Zipf.
1949.
Human Behavior and the Princi-pal of Least effort, an introduction to human ecology.Addison-Wesley, Reading, Mass.Marc A. Zissman.
1996.
Comparison of four ap-proaches to automatic language identification oftelephone speech, IEEE Trans.
on Speech and AudioProcessing, 4(1):31-44.522
