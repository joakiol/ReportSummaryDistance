Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 35?40,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsRating Computer-Generated Questions with Mechanical TurkMichael HeilmanLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAmheilman@cs.cmu.eduNoah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAnasmith@cs.cmu.eduAbstractWe use Amazon Mechanical Turk to ratecomputer-generated reading comprehensionquestions about Wikipedia articles.
Suchapplication-specific ratings can be used totrain statistical rankers to improve systems?final output, or to evaluate technologies thatgenerate natural language.
We discuss thequestion rating scheme we developed, assessthe quality of the ratings that we gatheredthrough Amazon Mechanical Turk, and showevidence that these ratings can be used to im-prove question generation.1 IntroductionThis paper discusses the use of Amazon Mechani-cal Turk (MTurk) to rate computer-generated read-ing comprehension questions about Wikipedia arti-cles.We have developed a question generation sys-tem (Heilman and Smith, 2009; Heilman andSmith, 2010) that uses the overgenerate-and-rankparadigm (Langkilde and Knight, 1998).
In thethe overgenerate-and-rank approach, many system-generated outputs are ranked in order to select higherquality outputs.
While the approach has had con-siderable success in natural language generation(Langkilde and Knight, 1998; Walker et al, 2001),it often requires human labels on system output forthe purpose of learning to rank.
We employ MTurkto reduce the time and cost of acquiring these labels.For many problems, large labeled datasets do notexist.
One alternative is to build rule-based sys-tems, but it is often difficult and time-consumingto accurately encode relevant linguistic knowledgein rules.
Another alternative, unsupervised or semi-supervised learning, usually requires clever formu-lations of bias that guide the learning process (Car-roll and Charniak, 1992; Yarowsky, 1995); suchintuitions are not always available.
Thus, small,application-specific labeled datasets, which can becheaply constructed using MTurk, may provide con-siderable benefits by enabling the use of supervisedlearning.In addition to using MTurk ratings to train alearned ranking component, we could also useMTurk ratings to evaluate the final top-ranked out-put of our system.
More generally, MTurk can be auseful evaluation tool for systems that output natu-ral language (e.g., systems for natural language gen-eration, summarization, translation).
For example,Callison-Burch (2009) used MTurk to evaluate ma-chine translations.
MTurk facilitates the efficientmeasurement and understanding of errors made bysuch technologies, and could be used to complementautomatic evaluation metrics such as BLEU (Pap-ineni et al, 2002) and ROUGE (Lin, 2004).It is true that, for our task, MTurk workersannotate computer-generated rather than human-generated natural language.
Thus, the data willnot be as generally useful as other types of anno-tations, such as parse trees, which could be used tobuild general purpose syntactic parsers.
However,for the reasons described above, we believe the useof MTurk to rate computer-generated output can beuseful for the training, development, and evaluationof language technologies.The remainder of the paper is organized as fol-lows: ?2 and ?3 briefly describe the question gener-ation system and corpora used in our experiments.
?4 provides the details of our rating scheme.
?5 dis-cusses the quantity, cost, speed, and quality of theratings we gathered.
?6 presents preliminary experi-ments showing that theMTurk ratings improve ques-tion ranking.
Finally, in ?7, we conclude.352 Question Generation SystemWe use MTurk to improve and evaluate a systemfor automatic question generation (QG).
In our QGapproach, hand-crafted rules transform declarativesentences from an input text into a large set of ques-tions (i.e., hundreds per page).
This rule system iscomplemented by a statistical ranker, which ranksquestions according to their quality.
Currently, wefocus on basic linguistic issues and the goal of pro-ducing acceptable questions?that is, questions thatare grammatical, make sense, and are not vague.
Webelieve an educator could select and revise outputfrom the system in order to produce a final set ofhigh-quality, challenging questions.Our system is described by Heilman and Smith(2010).
In that work, we employed a differ-ent scheme involving binary judgments of questionquality according to various factors such as gram-maticality, vagueness, and others.
We also employeduniversity students as novice annotators.
For thetraining dataset, only one human rated each ques-tion.
See Heilman and Smith (2009) for more de-tails.13 CorporaIn our experiments, we generated questions from60 articles sampled from the ?featured?
articles inthe English Wikipedia2 that have between 250 and2,000 word tokens.
This collection provides expos-itory texts written at an adult reading level from avariety of domains, which roughly approximates theprose that a secondary or post-secondary level stu-dent would encounter.
By choosing from the fea-tured articles, we intended to select well-edited ar-ticles about topics of general interest.
We then ran-domly selected 20 questions from each of 60 articlesfor labeling with MTurk.31We also generated some questions using a technique thatreplaces pronouns and underspecified noun phrases with an-tecedent mentions identified by a coreference resolver.
We willnot provide details about this component here because they arenot relevant to our use of MTurk to rate questions.
A forthcom-ing paper will describe these additions.2The English Wikipedia data were downloaded on Decem-ber 16, 2008 from http://en.wikipedia.org3Five questions were later eliminated from this set due tominor implementation changes, the details of which are unin-teresting.
The final set contained 1,195 questions.Rating Details1 Bad The question has major prob-lems.2 Unacceptable The question definitely has aminor problem.3 Borderline The question might have aproblem, but I?m not sure.4 Acceptable The question does not haveproblems.5 Good The question is as good as onethat a human teacher mightwrite for a reading quiz.Table 1: The five-point question rating scale.4 Rating SchemeThis section describes the rating scheme we de-veloped for evaluating the quality of computer-generated questions on MTurk.Questions were presented independently as sin-gle human intelligence tasks (HITs).
At the top ofthe page, raters were given the instructions shownin Figure 1 along with 7 examples of good and badquestions with their appropriate ratings.
Below theinstructions and examples was an excerpt from thesource text consisting of up to 5 sentences of con-text, ending with the primary sentence that the ques-tion was generated from.
The question to be ratedthen followed.Below each question was the five-point ratingscale shown in Table 1.
Workers were required toselect a single rating by clicking a radio button.
Atthe bottom of the page, the entire source article textwas given, in case the worker felt it was necessaryto refer back to more context.We paid 5 cents per rating,4 and each question wasrated by five workers.
With the 10% commissioncharge by Amazon, each question cost 27.5 cents.The final rating value was computed by takingthe arithmetic mean of the ratings.
Table 2 providessome examples of questions and their mean ratings.4.1 Monitoring Turker RatingsDuring some pilot tests, we found that it was par-ticularly important to set some qualification criteriafor workers.
Specifically, we only allowed workers4Given the average time spent per HIT, the pay rate can beextrapolated to $5?10 per hour.36Figure 1: A screenshot of the instructions given to workers.who had completed at least 50 previously acceptedHITs.
We also required that at least 95% of workers?previous submissions had been accepted.We also submitted HITs in batches of 100 to 500so that we could more closely monitor the process.In addition, we performed a limited amount ofsemi-automated monitoring of the ratings, and re-jected work from workers who were clearly ran-domly clicking on answers or not following the rat-ing scheme properly.
We tried to err on the side ofaccepting bad work.
After all ratings for a batchof questions were received, we calculated for eachworker the number of ratings submitted, the aver-age time spent on each question, the average rating,and the correlation of the worker?s rating with themean of the other 4 ratings.
We used a combinationof these statistics to identify extremely bad workers(e.g., ones who had negative correlations with otherworkers and spent less than 10 seconds per ques-tion).
If some of the ratings for a question wererejected, then the HIT was ?extended?
in order toreceive 5 ratings.5 Quantity, Cost, Speed, and QualityThis section discusses the quantity and quality of thequestion ratings we received from MTurk.5.1 Quantity and Cost of RatingsWe received 5 ratings each for 1,200 questions, cost-ing a total of $330.
178 workers participated.
Work-ers submitted 33.9 ratings on average (s.d.
= 58.0).The distribution of ratings per worker was highlyskewed, such that a handful of workers submitted100 or more ratings (max = 395).
The ratings fromthese who submitted more than 100 ratings seemedto be slightly lower in quality but still acceptable.The median number of ratings per worker was 11.5.2 Speed of RatingsRatings were received very quickly once the HITswere submitted.
Figure 2 shows the cumulativenumber of ratings received for a batch of questions,37Source Text Excerpt Question RatingMD 36 serves as the main road through the Georges CreekValley, a region which is historically known for coal mining,and has been designated by MDSHA as part of the Coal Her-itage Scenic Byway.Which part has MD 36 been desig-nated by MDSHA as?1.4He worked further on the story with the Soviet author IsaacBabel, but no material was ever published or released fromtheir collaboration, and the production of Bezhin Meadowcame to an end.What did the production of BezhinMeadow come to?2.0The design was lethal, successful and much imitated, andremains one of the definitive weapons of World War II.Does the design remain one of thedefinitive weapons of World War II?2.8Francium was discovered by Marguerite Perey in France(from which the element takes its name) in 1939.Where was Francium discovered byMarguerite Perey in 1939?3.8Lazare Ponticelli was the longest-surviving officially recog-nized veteran.
.
.
Although he attempted to remain with hisFrench regiment, he eventually enlisted in.
.
.Did Lazare Ponticelli attempt to re-main with his French regiment?4.4Table 2: Example computer-generated questions, along with their mean ratings from Mechanical Turk.1000150020002500Cumulative # Ratings05001000150020002500010203040506070Minutes Elapsed05001000150020002500010203040506070Minutes ElapsedFigure 2: The cumulative number of ratings submitted byMTurk workers over time, for a batch of 497 questionsposted simultaneously (there are 5 ratings per question).indicating that more than 1,000 ratings were re-ceived per hour.5.3 Quality of RatingsWe evaluated inter-rater agreement by having thefirst author and an independent judge rate a randomsample of 40 questions from 4 articles.
The indepen-dent judge was a computational linguist.
The Pear-son correlation coefficient between the first author?sratings and the mean ratings from MTurk work-ers was r = 0.79, which is fairly strong thoughnot ideal.
The correlation between the independentjudge?s ratings and the MTurk workers was r =0.74.
These fairly strong positive correlations be-tween the MTurk ratings and the two human judgesprovide evidence that the rating scheme is consis-tent and well-defined.
The results also agree withSnow et al (2008), who found that aggregating la-bels from 3 to 7 workers often provides expert lev-els of agreement.
Interestingly, the agreement be-tween the two human raters was somewhat lower(r = 0.65), suggesting that aggregated labels from acrowd of MTurk workers can be more reliable thanindividual humans.56 Using Labeled Data to Improve QuestionRankingIn this section, we provide some preliminary resultsto demonstrate that MTurk ratings can be used forlearning to rank QG output.First, we briefly characterize the quality of un-ranked output.
Figure 3 shows a histogram of themean MTurk ratings for the 1,195 questions, show-ing that only a relatively small fraction of the ques-tions created by the overgenerating steps of our sys-tem are acceptable: 12.9% when using 3.5 as thethreshold for acceptability.However, ranking can lead to substantially higherlevels of quality in the top-ranked questions, which5We also converted the ratings into binary values based onwhether they exceeded a threshold of 3.5.
After this conversionto a nominal scale, we computed a Cohen?s ?
of 0.54, whichindicates ?moderate?
agreement (Landis and Koch, 1977).385%10%15%20%25%Percent of Questions0%5%10%15%20%25%MeanRating RangeFigure 3: The distribution of the 1,195 question ratings.might be presented first in a user interface.
There-fore, we investigated how many MTurk-rated ques-tions are needed to train an effective statistical ques-tion ranker.
Our ranking model is essentially thesame as the one used by Heilman and Smith (2010).Rather than logistic regression, which we used pre-viously, here we use a linear regression with `2 reg-ularization to account for the ordinal scale of the av-eraged question ratings.
We set the regularizationparameter through cross-validation with the trainingdata.The regression includes all of the features de-scribed by Heilman and Smith (2010).
It includesfeatures for sentence lengths, whether the questionincludes various WH words, whether certain syntac-tic transformations performed during QG, whethernegation words are present in questions, how manytimes various parts of speech appeared, and others.It also includes some additional coreference featuresfor parts of speech and lengths of noun phrase men-tions and their antecedents.6 In all, the ranker in-cludes 326 features.For our experiments, we set aside a randomly cho-sen 200 of the 1,195 rated questions as a test set.We then trained statistical rankers on randomly sam-pled subsets of the remaining questions, from sizeN = 50 up to N = 995.
For each value of N ,we used the ranker trained on that amount of datato rank the 200 test questions.
We then computed6Since these additional coreference features are not immedi-ately relevant to this work, we will not describe them fully here.A forthcoming paper will describe them in more detail.0.40.50.6Acceptability ofRanked Fifth0.20.30.40.50.6 02505007501000Top-Training SetSize0.20.30.40.50.6 02505007501000Training SetSizeFigure 4: A graph of the acceptability of top-ranked ques-tions when datasets of increasing size are used to train astatistical question ranker.
Error bars show 95% confi-dence intervals computed from the 10 runs of the sam-pling process.the percentage of the top fifth of the ranked test setquestions with a mean rating above 3.5.
For eachN less than 995, we repeated the entire sampling,training, and ranking process 10 times and averagedthe results.
(We used the same 200 question test setthroughout the process.
)Figure 4 presents the results, with the acceptabil-ity of unranked questions (23%) included at N = 0for comparison.
We see that ranking more than dou-bles the acceptability of the top-ranked questions,consistent with findings from Heilman and Smith(2010).
It appears that ranking performance im-proves as more training data are used.
When 650 ex-amples were used, 49% of the top-ranked questionswere acceptable.
Ranking performance appears tolevel off somewhat when more than 650 training ex-amples are used.
However, we speculate that if themodel included more fine-grained features, the valueof additional labeled data might increase.77 ConclusionIn this paper, we used MTurk to gather quality rat-ings for computer-generated questions.
We pre-7To directly compare the ranker?s predictions to the correla-tions presented in ?5.3, we computed a correlation coefficientbetween the test set ratings from MTurk and the ratings pre-dicted by the ranker when it was trained on all 995 training ex-amples.
The coefficient was r = 0.36, which is statistically sig-nificant (p < .001) but suggests that there is substantial roomfor improvement in the ranking model.39sented a question rating scheme, and found high lev-els of inter-rater agreement (r ?
0.74) between rat-ings from reliable humans and ratings from MTurk.We also showed that ratings can be gathered fromMTurk quickly (more than 1,000 per hour) andcheaply (less than 30 cents per question).While ratings of computer-generated language arenot as generally useful as, for example, annotationsof the syntactic structure of human-generated lan-guage, many research paradigms involving the auto-matic generation of language may be able to benefitfrom using MTurk to quickly and cheaply evaluateongoing work.
Also, we demonstrated that such rat-ings can be used in an overgenerate-and-rank strat-egy to greatly improve the quality of a system?s top-ranked output.ReferencesC.
Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In Proc.
of EMNLP.G.
Carroll and E. Charniak.
1992.
Two experiments onlearning probabilistic dependency grammars from cor-pora.
Technical report, Brown University.M.
Heilman and N. A. Smith.
2009.
Question gener-ation via overgenerating transformations and ranking.Technical Report CMU-LTI-09-013, Language Tech-nologies Institute, Carnegie Mellon University.M.
Heilman and N. A. Smith.
2010.
Good question!statistical ranking for question generation.
In Proc.
ofNAACL-HLT.J.
R. Landis and G. G. Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33.I.
Langkilde and Kevin Knight.
1998.
Generation thatexploits corpus-based statistical knowledge.
In Proc.of ACL.C.
Lin.
2004.
ROUGE: a package for automatic eval-uation of summaries.
In Proc.
of Workshop on TextSummarization.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.R.
Snow, B. O?Connor, D. Jurafsky, and A. Ng.
2008.Cheap and fast ?
but is it good?
evaluating non-expertannotations for natural language tasks.
In Proc.
ofEMNLP.M.
A. Walker, O. Rambow, and M. Rogati.
2001.
Spot:a trainable sentence planner.
In Proc.
of NAACL.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proc.
ofACL.40
