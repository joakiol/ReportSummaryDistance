BYBLOS SPEECH RECOGNITION BENCHMARK RESULTSF.
Kubala, S. Austin, C. Barry, J. Makhoul, P. Placeway, R. SchwartzBBN Systems and TechnologiesCambridge MA 02138ABSTRACTThis paper presents speech recognition test results from the BBNBYBLOS system on the Feb 91 DARPA benchmarks in both theResource Management (RM) and the Air Travel Information System(ATIS) domains.
In the RM test, we report on speaker-independent(SI) recognition performance for the standard training condition using109 speakers and for our recently proposed SI model made from only12 training speakers.
Surprisingly, the 12-speaker model performs aswell as the one made from 109 speakers.
Also within the RM do-main, we demonstrate that state-of-the-art SI models perform poorlyfor speakers with strong dialects.
But we show that this degradationcan be overcome by using speaker adaptation from multiple-referencespeakers.
For the ATIS benchmarks, we ran a new system conligu-ration which first produced a rank-ordered list of the N--best word-sequence hypotheses.
The list of hypotheses was then reordered usingmore detailed acoustic and language models.
In the ATIS bench-marks, we report SI recognition results on two conditions.
The firstis a baseline condition using only training data available from NISTon CD-ROM and a word-based statistical hi-gram grammar developedat MIT/Lincoln.
In the second condition, we added training data fromspeakers collected at BBN and used a 4-gram class grammar.
Thesechanges reduced the word error ate by 25%.INTRODUCTIONThis paper will present new test results for running the BBNBYBLOS system on the speech recognition benchmarks in both theResource Management (RM) and the Air Travel Information System(ATIS) domains.During this reporting period we have conceentrated on speaker-independent recognition conditions.
However, we will also report anew result demonstrating the need and usefulness of speaker adap-tation in order to be able to recognize the speech of speakers withdifferent dialects than those found in the training data.For the RM corpus, we report on three conditions:1.
The common SI-109 training condition that has been widelyreported in the past.2.
The new SI-12 training paradigm that we introduced at theprevious DARPA workshop.3.
Adaptation to the dialect of the speakerThe ATIS domain presents a new type of speech recognition prob-lem in several respects.
First of all, and most importantly, the speechwas collected uring simulations of actual use of the ATIS system.The speakers were completely uncoached, and therefore, the range ofspeech phenomena goes far beyond that of the carefully controlledread-speech onditions that exist in the RM corpus.
We will describeour recent efforts to deal with these new problems.Since understanding s the ultimate goal of the ATIS domain, weuse a rank ordered list of the N-best speech recognition hypothesesas the interface to the natural language component of BBN's spokenlanguage system.
Below, we desoribe a new procedure which allowsthe system to use powerful but eomputationally prohibitive acousticmodels and statistical grammars to reorder the hypotheses in the N-best lisLFor the ATIS corpus, we report on two conditions:1.
A baseline control condition using a standard training set, lex-icon, and M-gram grammar.2.
An augmented condition using additional training, acoustic mod-els for nun-speech p enomena, nd a 4-gram class grammar.In the next section, we describe the main features of the baselineByblos system used in both RM and ATIS tests.
Next, the RM resultsare presented.
For the ATIS domain, we first describe the speech cor-pus used.
Then we describe the informal baseline training conditionwhich was developed to provide informative controlled experimentsfor this domain.
Next, we explain how the Byblos system was modi-fied for this evaluation.
Finally, we describe our augmented conditionand present comparative r sults.BYBLOS SYSTEM DESCRIPTIOPNThe BBN BYBLOS system had the following notable character-istics for the Feb 91 evaluation:?
Speech was represented by45 spectral features: 14 cepstra ndtheir 1st and 2nd diffe~nces, plus normalized energy and its1st and 2rid difference.?
The HMM observation densities were modeled by tied Gaussianmatures.
The mixture components were defined by K-meansclustering and remained fixed during training.?
Context-dependent phonetic HMMs were constructed from tri-phone, left-diphone, right-diphone, and phoneme contexts andincluded cross-word-boundary contexts.
The individual contextmodels were trained jointly in forward-backward.77?
Cooocurrence smoothing matrices were estimated from the tri-phone contexts only and then were applied to all HMM obser-vation densities.?
C~nder-dependent models were used for SI recognition.
Eachtest sentence was decoded on both models and the final answerwas chosen automatically by picking the one with the higherprobability.?
For all SI results other than the 109 RM condition, SI modelswere created by combining multiple, independently-trained andsmoothed, speaker-dependent (SD) models.
For the SI 109condition, however, the training data was simply pooled beforetraining.This baseline system is the same for both RM and ATIS results re-ported below.RESOURCE MANAGEMENT RESULTSSI Recognition with 109 Training SpeakersAt the June 90 DARPA workshop, we reported our first resulton the standard 109 SI training condition of 6.5% word error on theFeb 89 SI test set using the word-pair grammar.
When we retestwith the current system the en~r ate is reduced to 4.2%.
For thesetwo tests, the system differed in three ways.
First, we augmentedthe signal representation with vectors of second--order differences forboth the cepstral and energy features.
Secondly, the discrete observa-tion densities of the phonetic HMMs were generalized tomixtures ofGaussians that were tied across all states of all models, as in \[2\], \[5\].The VQ input to the trainer preserved the 5 most probable mixturecomponents for each speech frame.
In the earlier system, only theinput to the decoder was modeled as a mixture of Ganssians.
To date,we have not found any improvement for re-estimating the parametersof the mixture components within forward--backward.
Finally, in thenewer system, we trained separate codebooks and HMMs for maleand female talkers and selected the appropriate model automatically.We observed a small additive improvement for each of these threechanges to the system.For the current Feb 91 evaluation, we ran our latest system onthe standard 109 SI training condition with both no-grammar and theword--pair-grammar.
Results for these runs are shown in the firsttwo rows of table 1 below.# Training Spkrs I Grammar % Word Err % Sent Err109i None 18.8 69109 Word-Pair 3.8 2112 Word-Pair J 3.8 23Table 1: Resource Manageanent SI recognition results-- Feb 91 Test Set.SI Recognition with 12 Training SpeakersSince it is often difficult and expensive to obtain speech fromhundreds of speakers for a new domain, we recently proposed \[71creating SI models from a much smaller number of speakers butusing more speech from each speaker.
To test the proposal, we ranan experiment using 600 utterances from each of the 12 speakers inthe SD segment of the RM corpus.12 speakers could hardly be expected to cover all speaker typesin the general population (including both genders), so we anticipatedthat smoothing would be needed to make the models robust for newspeakers.
Our usual technique for smoothing across the bins of thediscrete densities, triphone cooceurrence smoothing \[10\], has proven?
to be an effective method for dealing with the widely varying amountsof training data for the detailed context-dependent models in the By-blos system.
This technique estimates the probability of any pair ofdiscrete spectra ceooecurring in a density by counting over all the den-sities of the triphone HMMs.
These probabilities are organized intoa set of phoneme--dependent co fusion matrices which are then usedto smooth all the densities in the system.The data from each training speaker is kept separate throughforward--backward (Baum-Welch) training and cooccurrenee smooth-ing.
The individually trained SD HMMs are then combined by av-eraging their parameters.
We have found that this approach leadsto better SI performance from a small number of training speakersthan the usual practice of pooling of all the data prior to training andsmoothing.In table 1, we show that he model made from 12 training speakersperforms as well as the standard 109 speakers on the Feb 91 SI test set.This is better than we had expected based on our previous experiencewith the Feb 89 SI test set.
To get a better estimate of the relativeperformance of the two approaches, we tested the current system onthree evaluation test sets (Feb 89, Oct 89, Feb 91).
Averaging theresults for these 30 test speakers, the SI 109 model achieved 3.9%word error while the SI 12 model got 4.5%.
This is a very smalldegradation for nearly a 10-fold decrease in the number of trainingspeakers.Adaptation to DialectWe found that our current state-of-the-art (SI) models performpoorly when a test speaker's characteristics differ markedly fromthose of the Ixaining speakers.
The speaker differences which causepoor recognition are not well understood, but outliers of this sortare not a rare phenomenom.
Our SI models have difficulty withthe RM test speaker RKM, for instance, a native speaker of Englishwith an African-American dialect.
Moreover, non-native speakersof American English nearly always suffer significantly degraded SIperformance.The problem was made obvious in a pilot experiment we per-formed recently.
As noted above, our baseline SI performance iscurrently about 4% word error using the standard 109 training speak-ers and word-pair grammar.
But when we tested four non-nativespeakers of American English under the same conditions, the worderror rates ranged from 22% to 45%, as shown in table 2.Speaker GenderJMMSSAVSNative Years of SI 109 AdaptLanguage English % Wd Err % Wdmale Arabic >25male Cantonese 5male British Eng.female Hebrew >15 ~Average \]Table 2: SI and SA results for non-native speakers of English.~tedErr27.6 5.245.4 10.731.7 5.422.2 4.731.7 6.5The table also shows the native language of each speaker andthe number of years that each has spoken English as their tnSmarylanguage.
Even though they vary widely in their experience with78English (and in their subjective intelligibility), each of them sufferedseverely degraded SI performance.
Even native speakers of BritishEnglish are subject to this degradation, asthe result from speaker SAdemonstrates.
Furthermore, the result from speaker JM shows thatthis problem does not disappear even after many years of fluency ina second language.We then tried to adapt he training models to the new dialects byestimating a probabilistic speOral mapping between each of the train-ing speakers and the test speaker as described in \[7\].
The resultingset of adapted models are combined to obtain a single adapted modelfor the test speaker.
In this experiment, we used the 12 SD speakersfrom the RM corpus as training speakers.
Each test speaker provided40 utterances for adaptation and 50 additional ones for testing.
Theword error rates after adaptation are also shown in table 2.
The over-all average word error rate after speaker adaptation is 5 times betterthan SI recognition for these speakerslThe success of speaker adaptation i restoring most of the per-formance degradation is quite surprising iven that no examples ofthese dialects are included in the training data Fmlbermore, onlyspectral and phonetic differences are modeled by our speaker adap-tation procedure.
No lexical variations were modeled irectly; weused a single speaker-independent phonetic dictionary of AmericanEnglish pronunciations.
These results how that systematic spectraland phonetic differences between speakers can account for most ofthe differences in the speech of native and non-native speakers of alanguage.THE ATIS CORPUSCorpus Descr ipt ionThe ATIS corpus consists of severai different ypes of speechdata, collected in different ways.
First, there are approximately 900utterances that were collected uring a "Wizard" simulation of anactual ATIS system.
The subjects were trying to perform aparticulartask using the system.
This data was collected from 31 speakers.The data from five of the speakers was used for the test of the naturallanguage systems prior to the June 1990 meeting.
These have sincebeen designated asthe development test speech.
Thus, there remained774 spontaneous training sentences from 26 speakers.In addition to the spontaneous sentences, everal of the subjectsread cleaned up versions of their spontaneous queries.
Finally, 10subjects each read 300 sentences during a single 1-hour session.
Thefirst 100 sentences were read by all the subjects.
The next 200 werechosen at random from a list of 2800 sentences constructed by BBNand SRI, by generalizing from previously recorded sentences fromseveral sites.
The average total duration of the 300 sentences wasabout 18.5 minutes per speaker (counting only the parts of the utter-ances containing speech).The 774 sentences from a total of about 30 speakers i clearly notsufficient for creating a powerful speaker-independent speech model.Collecting speech from an additional 70 speakers would require alarge additional effort.
Therefore, the additional 3000 sentences readby the 10 speakers provided the most efficient source of speech forestimating a speaker-independent model.ATIS Training Speech CharacteristicsThe subjects were instructed topush a button (push-to-talk) beforespeaking.
However, frequently, they pushed the button quite a whilebefore they began speaking.
In many cases, they breathed irectlyon the microphone windscreen, which was apparently placed irectlyin front of the mouth and nose.
Therefore, many files contain longperiods of silance with interspersed noises.
In fact, only 55% ofthe total duration of the training data contains peech (for both theread and spontaneous data).
In addition, some subjects paused in themiddle of a sentence for several seconds while thinking about whatto say next.
Others made side comments to themselves or otherswhile the microphone was live.
All of these effets are included inthe speech data, thus making it much more difficult to recognize thanpreviously distributed da~a~In the RaM corpus, there was a concerted effort to use subjectsfrom several dialectal regions.
In addition, since the speakers werereading, they tended toward standard General American.
Thus, themodels generated from this speech were reasonably robust for nativespeakers of American English.
In contrast, he ATIS corpus consistedprimarily of speakers from the South (26 of 31 speakers were labeledSouth Midland or Southern).In order to estimate speech models, we need an accurate tran-scription of what is contained in each speech file.
This transcriptionusually is in the form of the string of words contained in each sen-tence.
However, since this was spontaneous speech, there were oftennonspeech sounds and long periods of silence included among thewords.
Most of these ffects were marked for the spontaneous speech.Unfortunately, the transcriptions distributed with the read speech didnot follow the usual conventions for the stxing of words.
A signif-icant amount of work was required to correct hese inconsistencies.This work was undertaken by BBN and NIST, and was thoroughlychecked by Lincoln.
When all the corrections had been made, theywere redistributed to the community.Definition of Common BaselineThe new ATIS task presents everal new problems for speechrecognition.
Therefore, itwill be essential to try many new algorithmsfor dealing with it.
These experiments wiLl deal with a wide varietyof topics, including the makeup of the training data, the vocabulary,and the grammar.
However, it is just as important with this domain,as it was with the RM domain, that we use well-founded controlledexperiments across the different sites.
Without a baseline, meaningfulcomparisons between techniques cannot be made.
It is necessary inorder for researchers atother sites to be able to determine whethera new technique has actually made a significant improvment overprevious techniques.Since no common evaluation condition has been specified by thespeech performance evaluation committee, BBN and IV\[IT/Lincolnhave defined, promoted, and distributed an ATIS control conditionto provide a common reference baseline.
This baseline conditionconsists of a common lexicon, training set, and statistical grammar.In order to provide auseful baseline condition, all of the standardizeddata should represent a reasonable approximation to current state-of-the-art conditions.
BBN and Lincoln undertook to define such abaseline condition, under the severe constraints of limited availabledata and time.Training Set We defined as the baseline training set, all of thespeech that had been distributed by NIST on CD-ROM excepting thespontaneous speech spoken by the speakers used for the June 1990test of ATIS.
This test consisted of 138 sentences spoken by a total of5 speakers, Cod, bf, bm, bp, bw).
While an additional 435 sentences79that been recorded at SRI were made available on tapes at a laterdate, we felt that the small amount of additional speech and the latedate did not warrant including the speech in the baseline conditon.Of course the data was available to all who wanted to use it in anyother experimental or evaluation condition.Vocabulary One of the variables in designing a real speech sys-tem is to specify the recognition vocabulary.
Given that we do notknow what words will be included in the test speech, we have to makeour best attempt to include those words that would seem reasonablylikely.
Of course, ff we include too many words, the perplexity ofthe grammar will inerease, and the recognition error rate will in-crease.
We felt that, for a baseline condition, the vocabulary mustbe kept fixed, since we wanted to avoid random differences betweensites due to correctly guessing which words would occur in the test.We decided, at BBN to define a standard vocabulary based on thetranscriptions of all of the designated training data.
Thus, all of thewords included in the Standard Normal Orthographic Representation(SNOR) were included in the dictionary.
We made sure that manyfixed sets of words, such as the days, months, and numbers werecomplete.
In addition, we filled out many open class word categoriesbased on the specific ATIS database that was being used.
This in-eluded plurals and possessive forms of words wherever appropriate.This included names of airlines, plane types, fare codes, credit cards,etc.
When we did this completely, the result was a vocabulary of over1700 words, most of which seemed quite unlikely.
Therefore, we ap-plied an additional constraint on new open class words.
We added tothe vocabulary only the names of all of the airlines and plane types,etc., that served the 10 cities whose flights were included the currentdatabase.
In total, we added about 350 words to the vocabulary actu-ally used in the training speech.
This brought the baseline vocabularyup to 1067 words.
The result, when measured on the developmentset, was that the number of words in the test that were not in thevocabulary was decreased from 13 to 4.Grammar While the definition of the grammar to be used inspeech recognition is certainly a topic of research, it is necessaryto have a baseline grammar with which any new grammars may becompared.
It is also essential that this standard grammar be rela-tively easy for most sites to implement, in order that this not be animpediment to the use of the baseline condition.
Therefore, Lincolnestimated the parameters of a statistical bigram grammar using theback-off technique developed by IBM \[6\].
The derivation of thisgrammar is described in more detail in \[9\].
The transcriptions usedto estimate this grammar included those of all of the speech in thedesignated training set (SNOR transcriptions only) and also used the435 transcriptions for the new SRI set.
The parameters of this modelwere distributed in simple text format so that all sites could use iteasily.
The grammar has a test set perplexity of 17 when measuredon the development test set.
Thus, it provided a low upper bound forcomparison with any new language models.BBN SPEECH TECHNIQUES USED FOR ATISIn this section we describe the techniques that we used in theATIS speech recognition evaluation.
In particular, we only discussthose techniques that differed from those used for RM.
The techniquesinclude:1.
Speech/silence d tection.2.
N-Best recognition and rescoring with detailed models.3.
Optimization.Each of these techniques will be described below.Speech/Si lence Detect ionAs described in a previous section, both the training and testspeech contained large regions of silence mixed with extraneous noises.While the HMM training and recognition algorithms are capable ofdealing with a certain amount of silence and background noise, theynot very good at dealing with large periods of silence with occa-sional noise.
Therefore, we applied a speech end-point detector as apreprocess to the training and recognition programs.
We found thatthis improved the ability of the training algorithms to concentrate onmodeling the speech, and of the recognition programs to recognizesentences.N-Best RecognitionSince the purpose of the speech recognition is to understand thesentences, we needed to integrate it with the natural anguage (NL)component of the BBN HARC spoken language understanding sys-tem.
For this we use the N-Best recognition paradigm \[3\].
The basicsteps aae enumerated bdow:1.
Find N-Best hypotheses using non-cross-word models and bi-gram grammar2.
For each hypothesis:(a) rescore acoustics with cross-word models(b) score word stxing with a more powerful statistical gram-mar3.
Combine scores and reorder hypotheses4.
Report highest scoring answer as speech recognition result5.
Feed ordered list to NLFor efficiency, we use the Word-Dependent N-Best algorithm \[11 \].In addition to providing an efficient and convenient interface betweenspeech and NL, the N-Best paradigm also provides an efficient meansfor applying more expensive speech knowledge sources.
For example,while the use of cross-word triphone models reduces the error rate bya substantial factor, it  greatly increases the storage and computationof recognition.
In addition, a trigram or higher order language modelwould immensely increase the storage and computa~on of a recog-nition algorithm.
However, given the N-Best hypotheses obtainedusing non-cross-word triphone models, and a bigrarn grammar, eachhypothesis can be reseored with any knowledge source desired.
Then,the resulting hypotheses can be reordered.
The top scoring answer isthen the speech ~eognition result.
The entire list is then sent to theNL component, which chooses the highest answer that it can interpret.By using the N-Best paradigm we have found it efficient o ap-ply more expensive knowledge sources (as a post process) than wecould have considered previously.
Other examples of such knowledgesources include: Stochastic Segment Models \[8\] or Segment NeuralNetworks \[ 1 \].Opt imizat ionWe usually run the recognition several times on development testdata in order to find the optimal values for a few system parameters,such as the insertion penalty, and the relative weight for the gram-mar and acoustic scores.
This is a very slow and inexact process.However, given the N-Best paradigm, it is a simple matter to find80the values that maximize recognition accuracy.
Briefly, we generateseveral hypotheses for each utterance.
For each hypothesis, we factorthe total score into a weighted combination of the acoustic score(s),the language model score, and the insertion penalty.
Then, we searchfor the values of the the weights that optimize some measure of cor-rectness over a corpus.
This technique is described more fully in\[81.ATIS BBN AUGMENTED CONDITIONWe decided to consider three different conditions beyond thosespecified in the common baseline condition.
These include:1.
Use of additional training speech2.
Inclusion of explicit nonspeech models3.
More powerful statistical grammarsAdditional t ra in ing  speechOne of the easiest ways to improve the accuracy of a recognitionsystem is to train it on a Larger amount of speech, from a representativesample of the population that will use it.
Since there was clearly nottime to record speech from a very large number of speakers, wedecided to record a large amount of speech from a smaller numberof speakers.
We had shown previously \[7\] that this training paradigmresults in similar accuracy, with a smaller data collection effort (sincethe effort is largely proportional to the number of speakers rather thanthe total amount of speech.
)We collected over 660 sentences from each of 15 speakers.
Fivewere male and ten were female.
Due to the lack of time, most ofthe speakers were from the northeast.
However, we made an effortto include 4 female speakers from the Southeast and South Midlandregions.
We found that, once started, the subjects were able to collectabout 300 sentences per hour comfortably.Nonspeech  ModelsOne of the new problems in this speech data is that there werenonspeech sounds.
Some were vocal sounds (e.g.
"UH", "MIVI', cte.
),while some were nonvocal (e.g.
laughter, coughing, paper rostling,telephone tings, etc.).
The only frequent nonspeech sound was "UH",with 57 occurrences in the training corpus.
All the rest occurred oniy1 to 5 times.
We created a separate "word" for each such event.
Eachconsisted of it's own special phoneme or two phonemes.
All of themwere included in the same language model class within the statisticallanguage model.While several of the nonspeech events were correctly detected inthe development test speech, we found that the false alarm rate (i.e.typically recognizing a short word like "a" as "UH") was about equalto the detection rate.
Thus, there was no real gain for using nonspeechmodels in our development experiments.Stat ist ical  Language Mode lsIn this section, we discuss the use of statistical language modelsthat have been estimated from very limited amounts of text.
Weargue that it is clearly necessary to group words into classes to avoidrobustness problems.
However, the optimal number of classes eemsto be higher than expected.Since there is essentially no additional cost for using complexlanguage models within the N-Best paradigm, we decided to use a4-gram statistical class grammar.
'Dmt is, the probability of the nextword depends the classes of the three previous words.Need for Class Grarnma~'s An important area of research that hasnot received much attention is how to create powerful and robuststatistical language models from a very limited amount of domain-dependent training data.
We would eertainly like to be able to usemore powerful language models than a simple word-based bigrammodel.Currently, the most powerful "fair" grammars used within the pro-gram have been statistical bigram class grammars.
These grammars,which use padded maximum likeliD~od estimates of class pairs, al-low all words with some probability, and share the statistics for wordsthat are within the same domain-dependent class.
One issue of im-portance in defining a class grammar is the optimal number of classesinto which words should be grouped.
With more classes we can bet-ter distinguish between words, but with fewer classes there is morestatistical sharing between words making the grammar more robust.We compared the perplexity with three different grammars for theRM task with 100 classes, 548 classes, and 1000 classes respectively.In the first, words were grouped mainly on syntactic grounds, withadditional classes for the short very common words.
In the second,we grouped into classes only those words that obviously belongedtogether.
(That is, we had elt~.~es for shipnames, months, digits, etc.
)Thus, most of the classes eoteained only one word.
In the third gram-mar, there was a separate class for every word, thus resulting in aword-bigram grammar.
We used the backing off algorithm to smooththe probabilities for unseen bigrarns.
The Perplexities of the threegrammars measured on training data and on independent sentencesam given in the table below.Number of Classesi 100 \[ 548 1000Training 79 20 14Test L 95 42 49Table 3: Perplexity for three bigram class grammars measured on the train-in 8 and test set.As shown in table 3, perplexity on the training set decreases asthe number of classes increases, which is to be expected.
What isinteresting is the perplexity on the test set.
Of the three grammars,the 548-class grammar results in the lowest test set perplexity.
(Inter-estingly, the 548-class grammar is easier to specify than the 100-classgrammar.)
The increased perplexity for the 1000-class grammar isdue to insufficient training data.The effective difference between the 548- and 1000-class gram-mars was larger than implied by the average perplexity.
The standarddeviation of the word entropy was one half bit higher, which resultedin a doubling in the standard eviation of the perplexity.
To explain,the word bigram grammar frequently has unseen word pairs with verylow probabih'ty, while this effect is greatly reduced in the class gram-mar.
Thus, as expected, the class grammar is much more robust.Initial recognition e:-periments also seem to indicate a factor of twodifference in error rate between a class bigram grammar and a wordbigram grammar of the same perplexity.
These effects are likely tobe even larger when we use higher order n-gram models.81ATIS RECOGNIT ION RESULTSThe table below contains the recognition results for the ATIS cor-pus for both the development test set and the evaluation set.
The firstline shows the recognition results for the development test set consist-ing of 138 sentences spoken by five speakers (bd, bf, bin, bp, bw).
Allspeech data from these five speakers was left out of the training.
Thedevelopment results are given for the "augmented" enndition only.Next, we give the results for the ewAuation test set.
The first tworesults are the baseline condition and our augmented condition.
Wealso give results separately for the subset of 148 sentences that weredesignated as Class A (unambiguous, context-independent queries)for the NL evaluation.To review the two basic conditions, the baseline condition usedthe standard vocabulary, training set, and grammar throughout.
Theaugmented condition used more training data, a 4-gram class gram-mar, and a nonspeech model.ConditionAugmented; ail DevBaseline; all 200Augmented; all 200Baseline; ClassAI Augmented; ClassATable 4:Corr} Sub Del Ins Word Err Sent Err92.2 6.1 1.6 1.6 9.4 46.280.2 16.2 3.6 6.1 25.8 73.584.2 12.6 3.2 4.7 20.5 60.582.5 i 14.5 3.0 5.3 22.8 67~'6'87.6 9.9 \] 2.6 3.7 16.1 54.5ATIS speech recognition results.The first clear result is that the error rates for the evaluation testset are more than twice those of the development test set.
In addition,the perplexity of the evaluation test set is significantly higher than forthe development set (26 instead of 17 for the standard word-basedbigram grammar, and 22 instead of 13 for the 4-gram class grammar).Thus, we surmise that the evaluation data is somehow significantlydifferent han both the training data and the development test set.Next, it is clear that the Class A subset of the sentences presentsfewer problems for the recognition.
This is also indicated in theperplexities we computed for the two subsets.Finally, we see that, for both the full set of 200 sentences andthe Class A subset of 148, the augmented enndition has about 20%-30% fewer word errors than the baseline condition.
We are currentlyattempting to understand the causes of this improvement by morecareful comparison to the baseline.
The augmented condition wasrerun after including the training data from the held-out developmenttest speakers (about 900 utterances), but this made no difference.We suspect, therefore, that very little gain was also derived fromthe additional training speech collected at BBN (which suffers fromboth environmental nd dialectal differences).
We have also retestedwith a class higram grammar instead of the 4-gram, and again, therewas no change in performance.
This behavior may be explained bythe large difference between the evaluation test and the training.
Itis interesting, then, that the higher order grammar did not degradein the presence of such a difference.
This result also indicates thatsmoothing a word-based bigram by class definitions is important fortraining statistical grammars from small training corpora.
We have notretested without he nonspeech models, but their eon~hation appearssmall from a preliminary review of the recognition errors made.
Thetwo worst test speakers were also the ones that tended to producenumerous pause fillers (e.g.
"UH", "ulvr') as well as many otherdisfluencies.
Clearly, better nonspeech modeling will be essential ifwe continue to evaluate on this kind of data.CONCLUSIONSWe have reported several new benchmark speech recognition re-sults for both the RM corpus and the new ATIS corpus.
On RM, usingthe standard 109 speaker training set and the word-pair grammar, theword error rate for the BYBLOS system was 3.8%.
Surprisingly ournew SI paradigm, using only 12 training speakers, achieved the sameresnitl In addition, we have demonstrated that SI performance isgenerally very bad for speakers with s-~ong dialects.
But we haveachieved a 5-fold reduction in error rate for these speakers by usingspeaker adaptation from only 40 training utterances.For the ATIS corpus we developed several new techniques basedon the N-Best paradigm.
These have allowed us to use cross-wordtriphone models and a 4-gram statistical grammar efficiently in therecognition.
We have improved performance over a baseline conditionby 20%-30% by using additional training, models of nonspeech, anda 4-gram class grammar.
Our preliminary conclusion is that most ofthis gain is due to the smoothing of the grammar by classes.
Thespontaneous speech effects that appear in this corpus clearly presenta new set of difficult problems, since the error rates are about 4 timeshigher than for the RM corpus.AcknowledgementThis work was supported by the Defense Advanced ResearchProjects Agency and monitored by the Office of Naval Research underContract No.
N00014-89-C-0008.REFERENCES\[I\] S. Austin, J. Makhoul, R. Schwartz and G. Zavaliagkos, "ContinuousSpeech Recognition Using Segmental Neural Nets," this proceedings.\[2\] Bellegarda, J., D. Nahamoo, "Tied Mixture Continuous ParameterModeling for Speech Recognition," IEEE Transactions on Acoustics,Speech, and Signal Processing, Dee.
1990, ~Vol.
38, No.
12.\[3\] Chow, Y-L. and R.M.
Schwartz, "The N-Best Algorithm: An EfficientProcedure for Finding Top N Senumce Hypotheses," Proceedings oftheDARPA Speech and Natural Language Workshop, Morgan KaufmannPublishers, Inc., Oct. 1989.\[4\] Feng, M., F. Kubala, th Schwartz, J. Makhoul, "Improved SpeakerAdaptation Using Text Dependent Spectral Mappings," IEEE ICASSP-88, paper $3.9.\[5\] Huang, X., K. Lee, H. Hen, "On Semi--Continuous Hidden MarkerModeling," IEEE ICASSP-90, Apr.
1990, paper S13.3.\[6\] Katz, S., "Estimation of Probabiliities from Sparse Data for the Lan-guage Model Component of a Speech Recognizer', IEEE Transactionson Acoustics, Speech, and Signal Processing, Mar.
1987, Vol.
35, No.3.\[71 Kubala, F., R. Schwartz, "A New Paradigm for Speaker-IndependentTraining and Speaker Adaptation," Proceedings of the DARPA Speechand Natural Language Workshop, Morgan Kaufinann Publishers, Inc.,Jun.
1990, pp.
306-310.\[8\] Ostendoff, M., Kannan, A., Austin, S., Kimball, O., Schwartz, R.,mid J.R. Rohlicek.
"Integration of Diverse Recognition MethodologiesThrough Reevaluation of N-Best Sentence Hypotheses" this proceed-Ings.\[9\] D. B. Paul, "New Results with the Lincoln Tied-Mixture HMM CSRSystem," this proceedings.\[10\] Schwartz, R., O. Kimball, F. Kubala, M. Feng, Y. Chow, C. Barry,J.
Makhoul, "Robust Smoothing Methods for Discrete Hiddeaa MarkovModels," IEEE ICASSP-89, May 1989, paper $10b.9.\[11\] Schwartz, R.M., end S.A. Austin, "Efficient, High-Performance Al-gorithms for N-Best Search," Proceedings of the DARPA Speech andNatural Language Workshop, Morgan Kanffmann Publishers, Inc., Inn.1990.82
