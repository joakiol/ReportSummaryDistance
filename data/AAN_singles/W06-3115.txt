Proceedings of the Workshop on Statistical Machine Translation, pages 122?125,New York City, June 2006. c?2006 Association for Computational LinguisticsNTT System Description for the WMT2006 Shared TaskTaro Watanabe Hajime Tsukada Hideki IsozakiNTT Communication Science Laboratories2-4 Hikaridai, Seika-cho, Soraku-gun,Kyoto, Japan 619-0237{taro,tsukada,isozaki}@kecl.ntt.co.jpAbstractWe present two translation systems ex-perimented for the shared-task of ?Work-shop on Statistical Machine Translation,?a phrase-based model and a hierarchicalphrase-based model.
The former uses aphrasal unit for translation, whereas thelatter is conceptualized as a synchronous-CFG in which phrases are hierarchicallycombined using non-terminals.
Experi-ments showed that the hierarchical phrase-based model performed very comparableto the phrase-based model.
We also reporta phrase/rule extraction technique differ-entiating tokenization of corpora.1 IntroductionWe contrasted two translation methods for theWorkshop on Statistical Machine Translation(WMT2006) shared-task.
One is a phrase-basedtranslation in which a phrasal unit is employedfor translation (Koehn et al, 2003).
The other isa hierarchical phrase-based translation in whichtranslation is realized as a set of paired productionrules (Chiang, 2005).
Section 2 discusses those twomodels and details extraction algorithms, decodingalgorithms and feature functions.We also explored three types of corpus pre-processing in Section 3.
As expected, differenttokenization would lead to different word align-ments which, in turn, resulted in the divergenceof the extracted phrase/rule size.
In our method,phrase/rule translation pairs extracted from threedistinctly word-aligned corpora are aggregated intoone large phrase/rule translation table.
The experi-ments and the final translation results are presentedin Section 4.2 Translation ModelsWe used a log-linear approach (Och and Ney,2002) in which a foreign language sentence f J1 =f1, f2, ... fJ is translated into another language, i.e.English, eI1 = e1, e2, ..., eI by seeking a maximumlikelihood solution ofe?I1 = argmaxeI1Pr(eI1| f J1 ) (1)= argmaxeI1exp(?Mm=1 ?mhm(eI1, f J1 ))?e?
I?1exp(?Mm=1 ?mhm(e?
I?1 , f J1 ))(2)In this framework, the posterior probabilityPr(eI1| f J1 ) is directly maximized using a log-linearcombination of feature functions hm(eI1, f J1 ), suchas a ngram language model or a translation model.When decoding, the denominator is dropped since itdepends only on f J1 .
Feature function scaling factors?m are optimized based on a maximum likelihoodapproach (Och and Ney, 2002) or on a direct errorminimization approach (Och, 2003).
This modelingallows the integration of various feature functionsdepending on the scenario of how a translation isconstituted.In a phrase-based statistical translation (Koehnet al, 2003), a bilingual text is decomposed as Kphrase translation pairs (e?1, ?fa?1), (e?2, ?fa?2 ), ...: The in-put foreign sentence is segmented into phrases ?f K1 ,122mapped into corresponding English e?K1 , then, re-ordered to form the output English sentence accord-ing to a phrase alignment index mapping a?.In a hierarchical phrase-based translation (Chi-ang, 2005), translation is modeled after a weightedsynchronous-CFG consisting of production ruleswhose right-hand side is paired (Aho and Ullman,1969):X ?
?
?, ?,?
?where X is a non-terminal, ?
and ?
are strings of ter-minals and non-terminals.
?
is a one-to-one corre-spondence for the non-terminals appeared in ?
and?.
Starting from an initial non-terminal, each rulerewrites non-terminals in ?
and ?
that are associatedwith ?.2.1 Phrase/Rule ExtractionThe phrase extraction algorithm is based on thosepresented by Koehn et al (2003).
First, many-to-many word alignments are induced by runninga one-to-many word alignment model, such asGIZA++ (Och and Ney, 2003), in both directionsand by combining the results based on a heuristic(Och and Ney, 2004).
Second, phrase translationpairs are extracted from the word aligned corpus(Koehn et al, 2003).
The method exhaustively ex-tracts phrase pairs ( f j+mj , ei+ni ) from a sentence pair( f J1 , eI1) that do not violate the word alignment con-straints a.In the hierarchical phrase-based model, produc-tion rules are accumulated by computing ?holes?
forextracted contiguous phrases (Chiang, 2005):1.
A phrase pair ( ?f , e?)
constitutes a rule:X ??
?f , e??2.
A rule X ?
?
?, ??
and a phrase pair ( ?f , e?)
s.t.?
= ??
?f???
and ?
= ??e????
constitutes a rule:X ????
X k ??
?, ??
X k ???
?2.2 DecodingThe decoder for the phrase-based model is a left-to-right generation decoder with a beam search strategysynchronized with the cardinality of already trans-lated foreign words.
The decoding process is verysimilar to those described in (Koehn et al, 2003):It starts from an initial empty hypothesis.
From anexisting hypothesis, new hypothesis is generated byconsuming a phrase translation pair that covers un-translated foreign word positions.
The score for thenewly generated hypothesis is updated by combin-ing the scores of feature functions described in Sec-tion 2.3.
The English side of the phrase is simplyconcatenated to form a new prefix of English sen-tence.In the hierarchical phrase-based model, decodingis realized as an Earley-style top-down parser on theforeign language side with a beam search strategysynchronized with the cardinality of already trans-lated foreign words (Watanabe et al, 2006).
The ma-jor difference to the phrase-based model?s decoder isthe handling of non-terminals, or holes, in each rule.2.3 Feature FunctionsOur phrase-based model uses a standard pharaohfeature functions listed as follows (Koehn et al,2003):?
Relative-count based phrase translation proba-bilities in both directions.?
Lexically weighted feature functions in both di-rections.?
The supplied trigram language model.?
Distortion model that counts the number ofwords skipped.?
The number of words in English-side and thenumber of phrases that constitute translation.For details, please refer to Koehn et al (2003).In addition, we added three feature functions torestrict reorderings and to represent globalized in-sertion/deletion of words:?
Lexicalized reordering feature function scoreswhether a phrase translation pair is monotoni-cally translated or not (Och et al, 2004):hlex(a?K1 | ?f K1 , e?K1 ) = logK?k=1pr(?k | ?fa?k , e?k) (3)where ?k = 1 iff a?k ?
a?k?1 = 1 otherwise ?k = 0.?
Deletion feature function penalizes words thatdo not constitute a translation according to a123Table 1: Number of word alignment by different preprocessings.de-en es-en fr-en en-de en-es en-frlower 17,660,187 17,221,890 16,176,075 17,596,764 17,237,723 16,220,520stem 17,110,890 16,601,306 15,635,900 17,052,808 16,597,274 15,658,940prefix4 16,975,398 16,540,767 15,610,319 16,936,710 16,530,810 15,613,755intersection 12,203,979 12,677,192 11,645,404 12,218,997 12,688,773 11,653,242union 23,186,379 21,709,212 20,760,539 23,066,052 21,698,267 20,789,570Table 2: Number of phrases extracted from differently preprocessed corpora.de-en es-en fr-en en-de en-es en-frlower 37,711,217 61,161,868 56,025,918 38,142,663 60,619,435 55,198,497stem 46,550,101 75,610,696 68,210,968 46,749,195 75,473,313 67,733,045prefix4 53,429,522 78,193,818 70,514,377 53,647,033 78,223,236 70,378,947merged 80,260,191 111,153,303 103,523,206 80,666,414 110,787,982 102,940,840lexicon model t( f |e) (Bender et al, 2004):hdel(eI1, f J1 ) =J?j=1[max0?i?It( f j|ei) < ?del](4)The deletion model simply counts the numberof words whose lexicon model probability islower than a threshold ?del.
Likewise, we alsoadded an insertion model hins(eI1, f J1 ) that pe-nalizes the spuriously inserted English wordsusing a lexicon model t(e| f ).For the hierarchical phrase-based model, we em-ployed the same feature set except for the distortionmodel and the lexicalized reordering model.3 Phrase Extraction from Different WordAlignmentWe prepared three kinds of corpora differentiatedby tokenization methods.
First, the simplest pre-processing is lower-casing (lower).
Second, corporawere transformed by a Porter?s algorithm based mul-tilingual stemmer (stem) 1.
Third, mixed-cased cor-pora were truncated to the prefix of four letters ofeach word (prefix4).
For each differently tokenizedcorpus, we computed word alignments by a HMMtranslation model (Och and Ney, 2003) and by aword alignment refinement heuristic of ?grow-diag-final?
(Koehn et al, 2003).
Different preprocessingyields quite divergent alignment points as illustratedin Table 1.
The table also shows the numbers forthe intersection and union of three alignment anno-tations.The (hierarchical) phrase translation pairs are ex-tracted from three distinctly word aligned corpora.1We used the Snowball stemmer from http://snowball.tartarus.orgIn this process, each word is recovered into its lower-cased form.
The associated counts are aggregatedto constitute relative count-based feature functions.Table 2 summarizes the size of phrase tables in-duced from the corpora.
The number of rules ex-tracted for the hierarchical phrase-based model wasroughly twice as large as those for the phrase-basedmodel.
Fewer word alignments resulted in largerphrase translation table size as observed in the ?pre-fix4?
corpus.
The size is further increased by ouraggregation step (merged).Different induction/refinement algorithms or pre-processings of a corpus bias word alignment.
Wefound that some word alignments were consistenteven with different preprocessings, though we couldnot justify whether such alignments would matchagainst human intuition.
If we could trust suchconsistently aligned words, reliable (hierarchical)phrase translation pairs would be extracted, which,in turn, would result in better estimates for relativecount-based feature functions.
At the same time, dif-ferently biased word alignment annotations suggestalternative phrase translation pairs that is useful forincreasing the coverage of translations.4 ResultsTable 3 shows the open test translation results on2005 and 2006 test set (the development-test set andthe final test set) 2.
We used the merged (hierar-chical) phrase tables for decoding.
Feature functionscaling factors were optimized on BLEU score us-ing the supplied development set that is identical tothe 2005?s development set.
We observed that our2We did not differetiated in-domain or out-of-domain for2006 test set.124Table 3: Open test on the 2005/2006 test sets (BLEU [%]).de-en es-en fr-en en-de en-es en-frtest2005 Phrase 25.72 30.97 30.97 18.08 30.48 32.14Rule 25.14 30.11 30.31 17.96 27.96 31.042005?s best 24.77 30.95 30.27test2006 Phrase 23.16 29.90 27.89 15.79 29.54 29.19Rule 22.74 28.80 27.28 15.99 26.56 27.86results are very comparable to the last year?s best re-sults in test2005.
Also found that our hierarchicalphrase-based translation (Rule) performed slightlyinferior to the phrase-based translation (Phrase) inboth test sets.
The hierarchically combined phrasesseem to be too flexible to represent the relationshipof similar language pairs.
Note that our hierarchicalphrase-based model performed better in the English-to-German translation task.
Those language pair re-quires rather distorted reordering, which could berepresented by hierarchically combined phrases.We also conducted additional studies on howdifferently aligned corpora might affect the trans-lation quality on Spanish-to-English task for the2005 test set.
Using our phrase-based model,the BLEU scores for lower/stem/prefix4 were30.90/30.89/30.76, respectively.
The differences oftranslation qualities were statistically significant atthe 95% confidence level.
Our phrase translationpairs aggregated from all the differently prepro-cessed corpora improved the translation quality.5 ConclusionWe presented two translation models, a phrase-based model and a hierarchical phrase-based model.The former performed as well as the last year?s bestsystem, whereas the latter performed comparable toour phrase-based model.
We are going to experi-ment new feature functions to restrict the too flexiblereordering represented by our hierarchical phrase-based model.We also investigated different word alignment an-notations, first using lower-cased corpus, secondperforming stemming, and third retaining only 4-letter prefix.
Differently preprocessed corpora re-sulted in quite divergent word alignment.
Largephrase/rule translation tables were accumulatedfrom three distinctly aligned corpora, which in turn,increased the translation quality.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1969.
Syntaxdirected translations and the pushdown assembler.
J.Comput.
Syst.
Sci., 3(1):37?56.Oliver Bender, Richard Zens, Evgeny Matusov, and Her-mann Ney.
2004.
Alignment templates: the RWTHSMT system?.
In Proc.
of IWSLT 2004, pages 79?84,Kyoto, Japan.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of ACL2005, pages 263?270, Ann Arbor, Michigan, June.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of NAACL 2003, pages 48?54, Edmonton, Canada.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proc.
of ACL 2002, pages295?302.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51, March.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Comput.
Linguist., 30(4):417?449.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Shankar Fraser, Alex and Kumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
Asmorgasbord of features for statistical machine transla-tion.
In HLT-NAACL 2004: Main Proceedings, pages161?168, Boston, Massachusetts, USA, May 2 - May7.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL 2003,pages 160?167.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proc.
of COLING-ACL2006 (to appear), Sydney, Australia, July.125
