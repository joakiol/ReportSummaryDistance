PARSING WITH FLEXIBILITY~ DYNAMIC STRATEGIES~ AND IDIOMS IN MINDOl iv ie ro  S tockIstituto per la Ricerca Scientifica e Tecnologica,  38050 Povo,  Trento,  ItalyOne desirable aspect of a syntactic parser is being meaningful (i.e., contributing to incrementalinterpretation) during the process of parsing and not only at the end of it.
This becomes even moreimportant when dealing with flexible word order languages, where the number of alternatives in parsingmay grow dangerously.
One such parser is WEDNESDAY 2.
It is a lexicon-based parser, relying on thechart mechanism combined with a particular kind of unification, guided by the so-called Principle of theGood Clerk.
The paradigm is a multiprocessing one and the experimentation with dynamic syntacticstrategies (what heuristics may sensibly guide the process, for instance within a particular sublanguage)is a relevant task here.
An interactive integrated environment built around the parser is described.Flexible idiom processing is one of the best features of WEDNESDAY 2.
Idioms are treated asdecomposable parts of speech and the treatment of idiom recognition does not differ from the "literal"process until a threshold of activation of a flexible pattern is crossed.
At that point a new, idiomaticprocess is added.1 INTRODUCTIONWhile natural anguage parsing has come to be a veryserious and respectable field (almost as much as thetheory of compilers, according to Martin Kay), a lot ofnew realizations that appear as variations of consoli-dated approaches continue to emerge in an unorderedway.
Although all this causes a lot of redundance andnoise, we believe that it is important to maintain thisexperimental ttitude, provided that it is complementedby a correct acknowledgment of the state of the art.
Inour own work we have followed a set of ideas in thebelief that, at this stage, it was better to see themrealized in a "closed" world (one in which we do notadopt an external formalism, but rather one in whichthings are conceived and realized autonomously) thatstill could lead to a convergence with other more widelyaccepted currents of lexicon privileging parsing, such aslexical-functional grammar (LFG).The best machines that actually parse NL (humanbeings) are able to dynamically disambiguate a sentenceand to make sense of partial parsings, and they takeadvantage of this ability in their normal activities.Machines that do not do that: a) are bound to haveproblems in communicating with the machines citedabove; and b) will give limited support o our under-standing of those machines.
While these statementsmay be readily endorsed by all those working with an"integrated" or "semantics driven" approach, thingsare less clear when we consider esearch in which theemphasis i on the role of a strong syntactic omponentthat, per se, may lead only to a shallow semanticrepresentation.
Although we belong to this latter sub-community, we nonetheless believe that parsing as suchmust deal with the above statements.In recent years work on a number of languages otherthan English, the renewed interest in nontransforma-tional grammars, and the taking into account of theexperience in parsing have influenced the appearance ofnew linguistic theories, such as lexical-functional gram-mar (Kaplan and Bresnan 1982), generalized phrasestructure grammar (GPSG) (Gazdar and Pullum 1982),functional unification grammar (FUG) (Kay 1979), defi-nite clause grammar (DCG) (Pereira and Warren 1980),and tree adjoining grammar (TAG) (Joshi and Levy1982).The appearance of these theories has likewise meanta strong interest in the development and refinement ofsome techniques that are well suited to processing dataexpressed within the abstract formalisms characterizingall these approaches, namely feature structures.
Inparticular, unification has become the basic idea(Shieber 1986), even if with a large number of variationson the formal character (for instance, one may compareProlog-based unification for DCG, specialized unifica-tion for particular data types in FUG, equation resolu-tion for LFG), and on overall strategies.Copyright 1989 by the Association for Computational Linguistics.
Permission tocopy without fee all or part of this material isgranted providedthat he copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, or to republish, requires a fee and/or specific permission.0362-613X/89/01001-18503.00Computational Linguistics, Volume 15, Number 1, March 1989 1Oliviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in MindOften unification is combined with tabular methodsderived from techniques used for parsing programminglanguages.
These methods are naturally suited for pars-ing nondeterministically context-free languages, allow-ing for the drastic reduction of the complexity of anondeterministic algorithm to acceptable polynomialfigures (Aho and Ullman 1972).
Given the presenttendency to emphasize the reducibility of natural an-guage to tractable formal languages (see, for instance,Joshi and Levy 1982), and the high level of ambiguitytypical of natural language, this is a fairly naturalchoice.
The most relevant of these techniques fornatural anguage is chart parsing (Kay 1980, Kaplan1973).
See also another interesting and efficient ech-nique in Tomita (1985).
Shieber et al (1983) and Kart-tunen (1986) introduce a neutral tool, usable with anumber of different formalisms all based on chart andunification.
It is worth noting that while in some of thereferred works there is an explicit interest in the humandynamics of parsing, for most of them the effort isdevoted to building clean and efficient mechanisms thatare able to associate syntactic descriptions or mappingsof structures to input sentences.Our parser was developed with two purposes inmind: 1) understanding more fully the nature of lan-guage understanding and trying out our ideas concern-ing it; and 2) having an adequate tool for processing theItalian language.
With regard to point 1, our view ofsyntax is twofold, namely as:a. a set of specifications and constraints that, inparsing, guide the search for, and the linking togetherof, pieces of semantic representation, at the sametime delivering an overall functional description.This is expressed basically in the lexicon.b.
imposing restrictions on the spaces of search forpieces to be linked together, possibly taking intoaccount general criteria of linear precedence.As far as point 2 is concerned, Italian is a freer wordorder language than English (e.g., subject-verb-object isonly the most likely order in simple declarative sen-tences--the other five permutations of subject verb andobject may occur as well, even in written Italian) and ithas a richer morphology.
Therefore the role of syntaxspecified in (b) above is somewhat reduced, whileconstraints and specifications due to the lexicon, of thekind specified in (a) above are richer.
Of course, also,morphological analysis is an inescapable aspect thatmust be combined with syntax.
In particular, in Italian,you can find words like rifacendogliene, which standsfor "while making some (of them) for him again.
"The lexicon in our system includes a large quantity ofinformation, represented in a particular form of featurestructure, one in which alternative subcategorizationsare presented in a compact way.
There is a simplecentralized component that deals with distributionalaspects of language.
In our approach we can state all thevariations of word ordering: from obligatory positionsof a constituent in the string, to an obligatory position inrelation to another constituent, to a preferred position,all the way down to simply admittable positions.Parsing is the process of building an internal repre-sentation of the sentence, while disambiguating in localconditions of uncertainty.
In this sense we follow anon-deterministic approach that results in a particularrealization of the idea of chart parsing, such that theprocess goes bottom-up but with strict top-down con-firmation.
The concept of "sleeping edge" guaranteesthat all needed edges are introduced, whatever the orderof the operations, but also that not too many superflu-ous edges are introduced.
Chart parsing is combinedwith dynamic unification.
The main idea is that unifica-tion :is not carried on in a second phase, after chartparsing has yielded a constituent structure (as in thebasic LFG implementation), but also that it must not beflatly incremental (as in PROLOG-based systems).
Ourstrategy, embodied in the so-called Principle of the GoodClerk, is that unification is (asymmetrically) carried outafter the main element of the constituent has beenanalyzed.
This does not mean that unification here doesnot maintain its highly desirable qualities, such as orderindependence and monotonicity.
This is true also here,only that the application of unification is not continu-ous; our strategy seems reasonable and advantageousespecially with languages in which considerable free-dom is allowed in ordering the constituents.
We wouldlike to emphasize the fact that while the fundamentalrole of a particular word class for a given constituent(often called the "head") is a well-established conceptin many linguistic theories, a similarly privileged rolewithin the process of parsing is not common.
The parseris also designed for online interaction with a semanticcontext and therefore any particular data structure thatit builds is a shallow semantic representation, i.e., alogical form of the analyzed portions of the sentence.This approach is also a good starting point for recog-nizing idioms.
Most parsers are either devoted to idiomrecognition, do not treat idioms at all, or else have avery special separate device for that purpose.
In thepresent work instead, we propose a view of the idiomrecognition process as completely integrated with theparser, so that, in particular, idiom recognition can takeadvantage of the flexibility of the approach withoutredundancy.
We think this is important, because idiomsare on a continuum with literal language and share the"normal" characteristics, including the flexibility, ofthe particular language involved.
The treatment of id-iom recognition does not differ from the literal processuntil a threshold of activation of a flexible pattern iscrossed.
At that point a new idiomatic process is added.Another salient point is that within our approach,given the fact that the algorithm is of a multiprocessingtype, one can consider what strategies may be intro-duced in order to select dynamically the tasks that havebetter chances to lead to the (preferable) final analysis.These heuristics can also rely on measures of likelihoodassociated with each partial analysis.
An integrated2 Computational Linguistics, Volume 15, Number 1, March 1989Oliviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in Mindinteractive environment has been built up to experimentwith these ideas.
Besides aspects present in otherenvironments, e.g., D-PATR and the LFG workbench,our environment provides scope for the cognitive sci-entist, or the application-minded one dealing with arestricted sublanguage, to explore and define processingbehaviour.In this paper, after an introductory example, we firstoutline the characteristics of linguistic knowledge forour parser and give an overview of the parser itself.Following that we describe the particular kind of unifi-cation used.
The way non-determinism and disambigu-ation work is then discussed.
This is followed by a briefdescription of our treatment of long-distance d penden-cies.
After that comes a description of syntax-basedidiom recognition, in our view one of the most relevantfeatures of our work.
A description of the environmentbuilt around the parser and of experiments carried on indefining dynamic strategies concludes the paper.1.1 AN INTRODUCTORY EXAMPLEWe shall begin with an informal example to give theflavour of a basic part of our approach.
Let us assumethat the Italian sentence to be parsed is: Un itinerariomolto noioso ai visitatori ha prescritto la guida, lit-erally, "A very boring itinerary to the visitors hasdescribed the guide."
The sentence is topicalized, re-sulting in a quite common construction.
The represen-tations for the first noun phrase (NP) and the preposi-tional phrase (PP) are built up as parsing proceeds.
Thebuilding includes both a functional and a semanticrepresentation f the constituents, drawn from informa-tion explicited in the lexical entries.
At the level of thesubject (S) constituent being built, no commitment istaken for what will be the subject, even when theauxiliary ha is dealt with.
The verb prescritto insteadcauses the merging of information available so far.Therefore some of the functions pecified with the verbwill find their values (notably the oblique object and thedirect object functions).
The constituent un itinerariomolto noioso fails to be chosen as a subject, by virtue ofthe result of the interaction of the parser with a (sepa-rate, and not described in this paper) semantic ompo-nent that is questioned at the moment of building a newsemantic representation.
I  fact here the meaning of theproposed subject would not be compatible with theagent role for the verb; therefore the only possibilitybecomes the goal role.
Note that interaction with se-mantics is possible because we have partial representa-tions available at this point.
After that, the analysis isguided by the context hat has been built so far.
Therecould be two possible constituents whose constructionmay begin with the word la (this word is ambiguous--one reading is an article, the other eading aclitic): an Sand an NP.
But these bottom-up roposals are checkedand, as only an NP is expected, only that interpretationis carried out, and is ultimately unified with the previousrepresentation to produce a complete representation fthe sentence, with la guida as subject.With an algorithm that yields constituent structuresat an initial stage and performs unification at a laterstage, things would have been different.
The dynamicinteraction with semantics-based disambiguation, whenthe object is chosen, the selection of the correct inter-pretation of la guida later in the process would not havebeen straightforward; moreover, positional flexibilitycould have been achieved only at further cost and,finally, there would have been more redundancy.
Anapproach based on strictly incremental unificationwould have needed instead either a lot of redundancy atthe level of descriptive patterns or a useless tentativecommitment to binding the first NP to informationcarried by the auxiliary.2 ENCODING LINGUISTIC KNOWLEDGEWe shall now begin describing our parser, calledWEDNESDAY 2 and implemented in Interlisp-D on aXerox AI-Processor.
In the first place we shall give abrief description of how linguistic knowledge is en-coded.WEDNESDAY 2 is based on the assumption thatmost linguistic knowledge can be conveniently distrib-uted through the lexicon.
The lexicon includes in acompact form a lot of information that is normallystored in a grammar.
This information i cludes eman-tics, syntactic "static" specifications such as category,mood and tense, case marking, the specification of therelation between semantic objects and syntactic ob-jects, such as grammatical functions, but also somemore detailed specifications of constraints in bindingthese objects to other objects that can occur in thesentence.
These specifications describe in disjunctiveform sets of features that include different aspects:beside the usual ones (syntactic ategory, generic fea-tures, such as gender, number, person, case marking),relative positions of the entities in the string combinedwith a measure of likelihood of these positions, cospe-cification of co-occurring phenomena (such as the equi-np in a subordinate clause), and measures of likelihoodof the obligatoriness of the actual finding of one suchentity to be unified with.
A characteristic aspect oflexical representation in WEDNESDAY 2 is that se-mantic objects are part of the value of linguistic func-tions.The only centralized component of the system, in theform of a simple (recursive) transition etwork, dealswith aspects of distribution of constituents.
The treat-ment of rather free word order languages becomes quitenatural within this approach, and, in particular, thecentralized network then becomes very simple.
Theimportant thing to note is that the network does notsupport operations of structure building (like in anATN: Woods 1970) nor free annotations, includingoccurrence of equations (such as in LFG rules).
It is theComputational Linguistics, Volume 15, Number 1, March 1989 3Oliviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in Mindlexicon that bears feature sets: the network merely hasthe role of restricting the space where unification can becarried on.Each entry in the lexicon may include:1.
A semantic representation f the meaning of theword, called sere-units.
The format of the latter isindependent of that of other pieces of informationand can be of several different kinds.
In our actualimplementation weuse semantic networks.
They canbe seen as a set of propositions that use semanticpredicates and arguments.
Of course, every formathas its own operational modality.
For instance, uni-fication in a semantic network is carried on as a nodemerging operation: two or more nodes in one or morenet shreds collapse into a single node, producing anew net configuration, that nonetheless subsumesthe previous ones.
So the basic principle of unifica-tion is obeyed.2.
Syntactic data such as:a. Specification of linguistic (grammatical) functionswith indication of objects in the semantic represen-tation as semantic values.
In our implementations theobjects are nodes of the semantic network.
Arbitraryfeatures and a case marking may be specified, indisjunctive form, in relation to the syntactic aspectconnected to that object.
A relevant aspect in thisrepresentation is therefore that grammatical func-tions are the point of contact between semantics andsyntax.b.
A particular linguistic function that indicates anode (in the semantic net) that would be referred toas the Main node of the first syntactic onstituentthat includes the word.
(Eventually every constitu-ent, even of higher level, is provided with a Main, inthe course of parsing.)
It is worth noting that theMain constitutes a concept that bears some similaritywith the concept of "head" in some linguistic theo-ries.
The most relevant difference, and the one thatjustifies the use of a different term, is that the valueof the Main is an object in the semantic representa-tion, integrated with syntactic specifications.c.
categoryd.
mood and tense.Syntactic data here are supplemented byother dynamicsyntactic information, kept separate from them for thesake of clarity.
The latter consists of sets of constraintson variables but a good metaphor would be to viewthem as impulses to connect fragments of semanticinformation, guided by syntactic onstraints.
Yet, im-pulses are in a declarative format and are actuallyinterpreted by the parsing algorithm.
Impulses pecifycharacteristics of an entity to be identified in a givensearch space.
They have alternatives (for instance, theword "tell" has an impulse to merge its object nodewith the Main of either an NP or a subordinate clause).An alternative includes: a contextual condition of appli-cability, a category, features, case marking, side effects(through which, for example, coreference between sub-(sem-units (nl(p-prescribe n2 n3 n4)))(likeliradix 0.6)(cat v)(verbtense (ind past))(main nl)(lingfunctions ( ubj n2)(obj n3) (a-obj n4))(uni (subj)(must 0.5)((t np 0.9 ( nu sing) nom)))(uni (obj)(must)((t np 0.3 nil acc)(t s/sub 0.3 nil)(t s/prepinf 0.1 nil di (sideuni a-obj subj))(uni (a-obj)(must 0.8)((t np 0.2 nil a)))Figure 1.ject of a subordinate clause and a function of the mainclause can be indicated).
Impulses may also be directedto a different search space than the normal one.
Fur-thermore, there can be also impulses that do not involveoperations on the semantic representation: they setmarkings or features for linguistic functions or for theMain.Measures of likelihood can also be specified a. forone alternative to be considered; b. for which relativeposition the entity sought should be in with respect othe present word; c. for the overall necessity of findingthe entity.
These measures processed together withother ones will give a quantitative account of thelikelihood of an analysis and will dynamically play aheuristic role.For the sake of example, a lexical entry, correspond-ing to the word prescrisse, a verb in the past tense(encountered in Section 1.1, in the form of a participle),is shown in Figure 1.Sem-units pecify the semantics; here they consist ofa single proposition.
As we use a semantic networkformat, the n's are called nodes.
Besides the threearguments of the predicate, there is a node, nl, thatstands for the instantiation of the involved predicate.Next we have the likelihood of the word reading (in ourcase drawn from frequency figures).
Category and verbtense are self-explanatory.
The Main specification re-fers to the instantiation node of the only proposition.Three grammatical functions are indicated, each speci-fying a particular argument of the predicate.
Next, wefind unification specifications (impulses).
Each indi-cates a set of constraints: the subject function requires,with an obligatoriness of0.5, the Main of a noun phrase,which, with likelihood 0.9, has to occur to the left of thepresent word.
The number is singular and the casemarking nominative.
The initial t indicates that nofurther condition must be obeyed.
The object function4 Computational Linguistics, Volume 15, Number 1, March 1989Oiiviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in Mindnecessarily requires one of the alternatives: a nounphrase, a simple subordinate clause, or an infinitiveclause.
This last alternative specifies that the infinitiveoccurs with likelihood 0.1 before the present word (andwith likelihood 0.9 after it) and must be marked with adi case.
Moreover the subject of the infinitive is to bemade coreferent with the oblique object of the presentclause, at the same time as object unification.We have briefly referred to the only centralized atastructures in WEDNESDAY 2 that handle the distribu-tion of constituents and, in operating terms, the restric-tion of the space where unification can take place.
Wesay that this part is concerned with opening, closing andmaintaining search spaces, where entities sought byimpulses are possibly found.
It is realized with verysimple space management transition networks (SMTNs),in which $EXP, a distinguished symbol on an arc,indicates that only the occurrence of something ex-pected by the preceding context (i.e., for which animpulse was set up) may allow the transition.
SMTNscan impose generalized linear precedence on labeledsubstrings.
They encode information that could beexpressed by rewriting rules, factorizing fragments hatare common to different rules more perspicaciously.
Inparticular for Italian, which has a substantially freerword order language than English, such networks arevery simple.
Only locally do they impose an ordering(for instance, prepositions come before what theymark).
More often they only exclude intraposition of aconstituent ot belonging to the current space.
It shouldbe noted also that a set of acceptable verbal moods andtenses may be specified on entry points in the SMTNs.For instance, a (main) sentence needs a verb in a finitetense.It is worth noting that SMTNs are converted into aninternal format hat includes also a table of first transi-tion cross references, i.e., for each space type T, F(T),the set of initial states that allow for transition on T, or,recursively on a space type subsumed by a state in F(T).For example, F(Det) = {NP,S}, at least.
This is in thesame spirit with the concept of reachability discussed inKay (1980).3 THE BASIC PARSERWEDNESDAY 2 is a parser based on an integration ofchart and unification.
The parser uses linguistic knowl-edge of the type outlined in Section 2.A morphological nalyzer (Stock, Castelfranchi, andCecconi 1986) is actually attached to it and unifies dataincluded in stems and affixes.
Furthermore, tobe moreprecise, stems are put in hierarchies of prototypes: theresult is that linguistic data are much less redundantthan appeared in our introductory description.
Theprocess leans heavily on the idea of chart (Kay 1980,Kaplan 1973).
Chart parsing is a very general conceptfor non-deterministic parsing (see, for instance, Thomp-son 1981 and Ritchie and Thompson 1984), historicallyLEXICONmorphologicalprocessorSMTNssentence unification.~ ~ :tarchuSpgCes'ohe.al,ng se,ecti.g q IFigure 2.
WEDNESDAY 2.inspired by Earley's (1970) work on CFL parsing.
Weshall briefly review the main concepts here.A chart is a directed graph that represents he state ofthe analysis.
Given the input string, the junctures be-tween words are called vertices and are represented asnodes in the chart.
Each vertex has an arbitrary numberof arcs, called edges, entering and leaving it.
An edge istherefore a link between two vertices.
In the classicchart definition there are two possible types of edge:inactive or active.
An inactive edge stands for a recog-nized constituent ( he edge spans the words that areincluded).
An active edge represents a partially recog-nized constituent.
For an inactive edge there is aspecification of the category of the constituent.
In thecase of an active edge, a rewriting rule in the grammarand a position on the right hand side of that rule areprovided, thus indicating what is still in order to com-plete the recognition of the constituent.
If the rule is R:Co ---> C~.
.
.
C, there are specifications R and i, with 0< i < n, where i is the position on the right hand side ofrule R. A word in the string is itself represented as aninactive edge connecting two adjoining vertices.
Anempty active edge is an active edge that spans no wordsand is therefore represented in the chart as a link cyclingover one vertex.
It means, at least in top-down parsing,a prediction of the application of a rule.It is important to note that edges are only added tothe chart, never emoved.
A new edge may be added inthe following ways:1.
Given an active edge A spanning from Va to V b andan inactive dge I spanning from Vb to V e, where Arefers to rule R and to position i, and the category ofI is just Ci + ~, i + 1-th symbol of the right hand sideof R, then a new edge E can be added to the chart,which will span from Va to Vc, and, if Ci + ~ was thelast symbol in R, E will be an inactive edge withcategory equal to the symbol on the left hand side ofR; if not, it will be an active edge with rule R andposition i + 1.2.
Empty active edges are placed at particular pointsin the chart, according to the general strategy used.
IfComputational Linguistics, Volume 15, Number 1, March 1989 5Oliviero Stock Parsing with Flexibility, Dynamic SttZategies, and Idioms in Mindthe parser is a top-down one, when, given an activeedge with rule R and position i, that has reached thevertex V, there is a rule R' with left hand side equalto Ci + 1, i + 1-th symbol of the right hand side of R,an empty active edge is introduced on the vertex V,with rule R', provided that one such edge is notalready present on that vertex.
If the parser is abottom-up one, when, given an inactive edge withcategory C that has reached the vertex V, there is arule R' that has C as the first symbol of its right handside, an empty active edge is introduced on thevertex V, with rule R', provided that one such edge isnot already present on that vertex.The whole process of parsing aims at getting one ormore (if the sentence is ambiguous) inactive edges tospan the whole string, with category S the distinguishedinitial symbol in the grammar.One of the great advantages of chart parsing is that,whatever the strategy adopted, work is never dupli-cated.
For example, if, after backtracking, the analysisis continued by extending a different active edge and avertex is reached from where an inactive edge starts,and if the edge addition rule 1 can be applied, then theanalysis takes advantage of previously done partialanalysis.
Another advantage is that the mechanism isperfectly suited for both bottom-up and top-down pars-ing, depending only on the form of the addition rule 2.Another point worth mentioning is that the inputrelation with other levels of analysis is coherent: lexicalambiguity results in the very simple fact that more thanone inactive edge is introduced for one ambiguousword.
Furthermore, the basic top-down and bottom-upcontrol schemata can be sophisticated in a number ofways, resulting in better performance (smaller numberof unnecessary edges introduced in the chart).
Wiren(1987) compares a number of proposals.In WEDNESDAY 2 the chart is the basic structure inwhich search spaces are defined?
An active edge defines?
an operational environment for unification activity?Some notable overall aspects in the WEDNESDAY 2chart are:Instead of referring to a set of rewriting rules, edgesrefer to positions that are states in SMTNs.
The caseof the $EXP arc will be discussed in further detail inSection 4.Parsing goes basically bottom-up, with top-down con-firmation, with an improvement of the so-called leftcorner technique, a version of which is referred to inWiren (1987).
When a word edge with category C isadded to the chart, its first left cross references F(C)are fetched and, for each of them, an edge is intro-duced in the chart.
These particular edges are calledsleeping edges.
A sleeping edge S at a vertex V s is"awakened," i.e., causes the introduction of a nor-mal active edge iff there is an active edge arriving atV s that may be extended with an edge with thecategory of S. If they are not awakened, sleepingedges play no role at all in the process.An agenda is provided.
Tasks can be added to theagenda nd at every moment a scheduling functioncan decide the order in which tasks should be per-formed, in a multiprogramming fashion.
The schedul-ing function can very easily implement depth-firstcontrol and breadth-first control, but any kind ofcontrol can in principle be included.
Tasks are ofseveral kinds, including lexical tasks, extension tasks,insertion tasks, and virtual tasks.
A lexical task spec-ifies a possible reading of a word to be introduced inthe chart as an inactive edge.
An extension taskspecifies an active edge and an inactive dge that canextend it (together with some more information).Insertion tasks will be explained in Section 4.1.
Avirtual task consists in extending an active edge withan edge displaced to another point of the sentence,according to the mechanism treating long-distancedependencies, which is explained in Section 5.4 UNIFICATIONLet us now focus on the actual problem of unification.The sentence is La luna splende ("The moon shines").We consider an active edge with category S, that spansthe fragment la luna, and an inactive dge with categoryV, that spans the fragment splende.
Unification isperformed simultaneously with the application of thebasic edge extension rule.
The first frame in Figure 3ashows part of the situation in the preexisting active Sedge, before the V edge contribution (shown in thedotted frame).
In the first frame it is emphasized thatthere is a candidate for unification.
PI01.1 denotes thefirst argument of proposition P101.
An unsatisfied im-pulse is emphasized in the dotted frame.
Two alterna-tive sets of constraints are actually indicated.
The framein Figure 3b shows the situation within the new S edgeafter execution.Let us informally describe the aspect of an edge inWEDNESDAY 2.
An edge is composed of a. a struc-tural aspect, that includes a provenience vertex, adestination vertex, a category (corresponding to a spacetype) and a state in an SMTN; b. a set of syntacticspecifications of various kinds; c. a measure of likeli-hood for this analysis, resulting from the likelihood ofpartial interpretations and the likelihood of choicesperformed at the present level; and d. the semanticrepresentation f the fragment, in the form of a semanticnetwork.The syntactic specifications include, among others,the Main, other functions, sets of constraints to besatisfied (impulses), and candidates for satisfying thesesets of constraints.
An explicit list of candidates isuseful because the unification activity is not continuous,and therefore a memory of pending situations i needed.Candidates emerge from included edges: a typical caseis that a Main of an included edge may be taken as a6 , Computational Linguistics, Volume 15, Number 1, March 1989Oliviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in MindCAT:Scandidates:\[(P105.1)label NPmark:nominative or accusativefeatures:ge f mnu sing?
.
\ ]NET: PlO5:(moonx 110)MAIN:PI01LINGFUNCTIONS:SUBJECT:(PI01.1)features:nu singimpulses:UNISUBJECT  .
.1) label:NPmark:nominativefeatures:nu sing2) label:S/Infinitivemark:nominativeNE'F: PlOl:(shine?
102)(a)CAT':SMAIN:P101LINGFUNCTIONS:SUBJECT:(PI01.
I)label:NPmark:nominativefeatures:ge f mnu singimpulses:NILcandidates:NILNE'F: P101 :(shine ?
110)PlO5:(moonx 110)(b)Figure 3.
WEDNESDAY 2 Unification?candidate in the present edge, if the SMTN arc transitedfor including the edge specifies it.
A candidate iscomposed of a complex data structure, including syn-tactic features and a reference to a node in the semanticnetwork.In the actual implementation there are specializededge slots for different kinds of impulses, in relation totheir range: if they modify a structure in a differentsearch space, or if they "fill an argument" for thepresent frame, etc.
Introducing a new edge also meansthat these data structures are set consistently.The contents of a word edge are established andinstantiated by the lexicon (or, more exactly, by themorphological processor and the lexicon)?
When a wordedge W is used to extend a given edge A, to produce theedge A' based on the configuration of A, then (withoutconsidering the further processing described below) itssyntactic and semantic contents are inserted in thecorresponding slots of A'.
When any other inactive dgeI is used to extend an active edge A, then (withoutconsidering the further processing described below)some of its contents are inserted in A' in this way: if I"plays an argumental role" in that space (informationthat comes from the SMTN arc transited for acceptingI), then the value of its Main is placed among thepending candidates of A'.
The likelihood figure of A' isthe result of the application of a numeric function to thelikelihoods of A' and I.
The semantic net shred of I iscopied to A'.What we shall call the Principle of the Good Clerkgoverns the unification activity:Before the arrival of the Main (the "Boss" )  theprocessor adopts a lazy attitude, when the Mainarrives all possible (unification) activity is done,when the Main is already there an (unification) actionis performed as soon as a new event occurs.Accordingly, before the Main arrives, an extension ofan edge causes very little more than the copying actionsindicated above.
The only check is for mood and tenseto be in accordance with what was possibly expected.On the arrival of the Main, all the present impulsesthat either have only a syntactic aspect (like the casemarking introduced by a preposition) or that are im-ported from subsumed edges (impulses that bind modi-fiers, in the linguistic sense) must be satisfactorilycarried on and, for each candidate an expectationmatching its characteristics must be found.
If all thisdoes not happen then the new edge A' supposed to beadded to the chart is not added: the situation is recog-nized as a failure.After the arrival of the Main, each new candidatemust find an impulse to merge with, and each incomingimpulse must find satisfaction.
Again, if all this does nothappen, the new edge A' will not be added to the chart.The example in Section 1 informally shows the effect ofthe principle.What are the effects of unifying?
The value of thelinguistic function gets more precise with a reference tothe filler node, a category (if not yet specified), a markas prescribed by the considered alternative in the im-pulse and features of the two contractors merged to-gether.
If, for one feature, there is a set of values for oneor both contractors, then the intersection of the valuesComputational Linguistics, Volume 15, Number 1, March 1989 7Oliviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in Mindis considered as the value.
Of course, if the intersectionis empty, and/or if any of the characteristics do notmatch, the merger is precluded, and unification fails.An implementation note: changes are always per-formed in the active edge, but nonetheless, using spe-cialized data types only selected ata need to be copiedfrom underlying edges.The actual semantic unification happens in the se-mantic network, represented as a set of labelled prop-ositions.
With every entity x (variable or propositionlabel), Rx, a list reference pairs (proposition and argu-ment number), is associated.
In this way, merging xwith y causes the substitution of all the occurrences ofx, inferred from Rx, with y and the assignment toRx ofthe union (without repetitions) of Rx and Ry.
Shouldthere be any specified side effects of a merging type (seeSection 2), they are carried out in the same way.In languages with a variable order or, for instance, ifsubject gapping is admitted, possible mergings are notunivocally determined by structure or position.
There-fore there may actually be more than one completemerging combination for a tentative new edge A'.
In thiscase more new edges must be added to the chart, onefor each alternative unification.
This establishes thesecond kind of task mentioned before---an insertiontask, with self-explanatory execution behavior.By way of example, let us consider the simplesentence: I!
saggio arna la ballerina, i.e., "The wiseman loves the ballerina".
When the "loves" edge istaken care of in the S active edge that has alreadyincluded the NP "the wise man", it happens that twocases must be considered: in one case "the wise man"is the subject, in the other case it is a topicalized object.This is specified in the lexicon with the word, "loves",and different likelihoods are associated with the twopossibilities.
Two different edges are therefore pro-posed: the proposals are in the form of insertion tasksspecifying the measure of likelihood of the new pro-posed edges.
Up to that point a single path had beenfollowed, with no commitment before the verb wasanalyzed.
Only as a consequence of the verb introduc-tion are the two hypotheses set up, with all the possiblecommitments involved in each of them and resulting instronger constraints in the following analysis.4.1 NON-DETERMINISM AND DISAMBIGUATIONVarious kinds of ambiguities can be present in thelinguistic knowledge: a word may be semantically am-biguous; an impulse may have a number of alternatives;a case marking may be ambiguous.
On top of this theremay be idiomatic interpretations competing with theliteral ones (see Section 6).
And then, of course, theSMTNs express an infinite set of configurations.
Whena particular set of nondeterministic unifications are tobe carried out, an insertion task is introduced in theagenda.
The execution of the task leads to the inclusionof a new edge in the chart.
Dynamically, apart from thegeneral behavior of the parser, there are some particularrestriction,; on its nondeterministic behavior whichbring syntactic dynamic disambiguation to bear.1.
The $EXP arc allows for a transition only if thecotffiguration i the active edge includes an impulseto link up with the Main of the proposed inactiveedge.2.
The sleeping edge mechanism prevents paces notappropriate to the left context from being estab-lished.3.
A search space can be closed only if no impulsespecified as having to be satisfied remains.
In otherwords, if in a state with an outgoing EXIT arc, anactive edge can lead to the establishment of aninactive dge only if there are no obligatory impulsesleft.4.
A proposed new edge A' with a verb tense notmatching the expected values causes a failure, i.e.,A' will not be introduced in the chart.5.
As set out in greater detail in the previous para-graph, failure is caused by inadequate mergings, withrelation to the presence, absence, or ongoing intro-duction of the Main.Making a comparison with the criteria established forLFG for functional compatibility of an f-structure(Kaplan and Bresnan 1982), the following can be said ofthe dynamics outlined here.
Incompleteness recognitionperforms as specified in (3), and furthermore, there is anearlier check when the Main arrives, in case there wereobligatory impulses to be satisfied at that point (e.g., anargument that must occur before the Main).
Incoherenceis avoided completely after the Main has arrived bymeans of the $EXP arc mechanism.
Before this, it isrecognized as specified in (5) above, and causes animmediate failure.
Inconsistency is detected as indicatedin (4) and (5).
As far as (5) is concerned, though, theattitude is to activate impulses when the right premisesare present and to look for the right thing and not tocheck if what was done is consistent.One hitherto fundamental, but only partially imple-mented, aspect is a mechanism for semantic disambig-uation that would interact with WEDNESDAY 2.
Infact, some aspects of the parser have been designedprecisely in view of such interaction.5 LONG-DISTANCE DEPENDENCIESSentences like "The programmer with the brother ofwhose colleague John said that Mary danced has left"require that something positioned far away in the sen-tence (in this example, "with the brother of whosecolleague") plays the role of filler for a certain gap (inthis case bound to "dance").
Constructions ofthis typeare called long-distance dependencies.
It is generallydifficult to relate them back to a context-free mecha-nism unless some particular device, such as metarulesin GPSG, is introduced.
In LFG, double arrows areused to denote this and are combined with particularrewriting rules in the grammar that derive c-structures.8 Computational Linguistics, Volume 15, Number 1, March 1989Oliviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in MindThese rules have the empty element, e, on the right-hand side, and therefore gaps are foreseen by thegrammar writer.We take a different approach.
First, we do not wantthe parsing process to be overwhelmed by the appear-ance of spurious constituents, where a rule that pre-scribes the introduction of the empty symbol was ap-plied.
Of course, the use of empty rewriting rules isattractive to the linguist, because he/she can then give aclear description of the structures.
But in parsing wehave a problem of the explosion of possible constitu-ents.
Second, we want the lexicon to control also theproblems and idiosyncrasies involved in limiting theallowed dependencies.
To illustrate this point, the solu-tions given by Government Binding theory and bylexical-functional grammar are worth recalling.
Theformer approach considers wh-movements as con-strained by 'among other thing' the principle of subja-cency, which is a universal structural principle thatimposes bounding categories.
The latter approach al-lows imposition of hounding nodes, which is a moreflexible concept (see Bresnan 1982).We shall give here a very concise description of howrelative clauses are dealt with in WEDNESDAY 2.
Letus begin with lexical data.
Entries like wh-words carrythe information of being the focus of a relative clause,complemented by two kinds of constraints: a. con-straints on the path that may be followed upwardsbefore reaching the top-level space of the relativeclause.
The effect of this in the above example is toallow precisely the following structure: \[S'\[PP with \[NPthe brother\[pp Of\[Np whose colleague\]\]\]\] .
.
.
.
\] andcause the main node of the NP "the programmer" tobereferenced as the second argument of the relation"colleague"; b. constraints on the path that may befollowed downwards from the implied S' space.
Theeffect of this is that the impulse bound to "with" will beallowed to operate in the above example, modifyingeither "said", or "danced", while irregular sentenceswill be detected.We have so far discussed the effects before mention-ing the other data implied in the matter: SMTNs.
Let usintroduce virtual arcs to the network.
This is a conceptthat may sound reminiscent of ATNs (Woods 1970).Actually, the role is similar but the concept is quitedifferent.
In ATNs a virt arc (a.k.a.
retrieve) specifiesthe particular kind of phrase that should be retrievedfrom an implicit register called hold.
Of course, thatparticular kind of phrase (for instance, NP or PP) isspecified in the central network and it becomes quiteunnatural to deal with word idiosyncrasies.
The gram-mar writer must describe all kinds of complex gappingsthat can occur with different words.
WEDNESDAY 2'sphilosophy reverses the procedure.
A virtual arc hasnothing specified on it: it can be transited if there is animpulse or a candidate that, although displaced, hasbeen made available by the parser's mechanism, andnow is compatible with the situation determined by theComputational Linguistics, Volume 15, Number 1, March 1989EXIT$EXP PPADVPFigure 4.
Virtual Arc in an SMTN.configuration of the present particular words.
The vir-tual arc is transited if unification can be carried out.
Asan example of a fragment of SMTN that includes avirtual arc, see Figure 4.The cycling arc specifies that transitions can be madeon whatever is expected by the left context, plusmodifiers such as PPs or adverbials.
The virtual arcindicates that a proposed isplaced element compatiblewith present functional expectations may be used.
Notethat virtual arcs are basically inserted in the SMTNsafter the Main has been taken care of and thereforeunification is immediate.
The implementation f thisaspect of the parser equires the alteration of the basicchart with the inclusion of topicalized edges, i.e., edgesthat have a displaced candidate or impulse in evidence,which, by obeying the constraints discussed above, mayfind its place within that particular edge.
If this happensand the parser manages to build an inactive dge, it willbe an inactive edge that satisfies the topic.
If theinactive dge, and all the inactive dges ubsumed by it,do not make use of the topic, the topic simply does notappear at all in the edge.
Of course, a necessarycondition for the introduction of an inactive top-leveltopicalized edge (e,g., an edge corresponding to thesearch space S') is that it satisfies the topic.In our example, given that in the fragment \[s,\[ppwiththe brother of whose colleague\]\[sJohn said that\[sMarydanced\]\] .
.
.
.
.
the inactive edge of \[sMary danced\]satisfies the topic, also \[s John said that\[sMary danced\]\]satisfies the topic, and therefore an S' inactive dge canbe happily introduced in the chart.
The mechanismdiscussed above guarantees one aspect hat we havestressed throughout our work: the dynamic unificationof partial interpretations.
Furthermore, it seems to workquite well with Italian.6 IDIOMSIdioms are a pervasive phenomenon i natural lan-guages.
Linguists have proposed ifferent accounts foridioms, which are derived from two basic points ofview: one point of view considers idioms as the basicunits of language, with holistic characteristics, perhapsincluding words as a particular case; in the other pointOiiviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in Mindof view the emphasis is instead laid on the fact thatidioms are made up of normal parts of speech that playa definite role in the complete idiom.
An explicit state-ment in this approach is the Principle of Decomposit ion-ality (Wasow, Sag, and Nunberg 1982): "When anexpression admits analysis as morphologically or syn-tactically complex, assume as an operating hypothesisthat the sense of the expression arises from the compo-sition of the senses of its constituent parts".
Thesyntactic onsequence is that idioms are not a differentthing from "normal" forms.We hold the latter view.
We are aware of the fact thatthe flexibility of an idiom depends on how recognizableits metaphorical origin is.
Within flexible word orderlanguages the flexibility of idioms seems to be evenmore closely linked to the strengths of particular syn-tactic constructions.Let us now briefly discuss some computational p-proaches to idiom understanding.
Applied computa-tional systems must necessarily have a capacity foranalyzing idioms.
In some systems there is a preproces-sor delegated to the recognition of idiomatic forms.
Thispreprocessor replaces the group of words that make forone idiom with the word or the words that convey themeaning involved.
In ATN systems, specially if ori-ented towards aparticular domain, there are sometimesinstead sequences of particular arcs inserted in thenetwork, which, if transited, lead to the recognition of aparticular idiom (e.g., PLANES, Waltz 1978).
LIFER(Hendrix 1977), one of the most successful appliedsystems, was based on a semantic grammar, and withinthis mechanism idiom recognition was easy to imple-ment, without considering flexibility.
Of course, there isno intention in any of these systems to give an accountof human processing.
PHRAN (Wilensky and Arens1980) is a system in which maximum emphasis i placedon the role of idioms.
A particularly interesting aspect isthat the system had also a twin generator, calledPHRED (Jacobs 1985), that reversed the process, asboth are based on the same linguistic representation.Idiom recognition, following Fillmore's (1979) view, isconsidered the basic resource all the way down toreplace the concept of grammar-based parsing.
PHRANis based on a data base of patterns (including singlewords, at the same level) and proceeds deterministi-cally, at least in its basic implementation, applying thetwo principles "when in doubt choose the more specificpattern" and "choose the longest pattern".
The limitsof PHRAN lie in the capacity to generate variousalternative interpretations in case of ambiguity (thoughit must be reported that a nondeterministic implemen-tation was also eventually realized) and in running therisk of having an excessive spread of nonterminalsymbols if the data base of idioms is large.
A recentwork on idioms with a similar perspective and withparticular attention to the problem of learning idioms isDyer and Zernik (1986).The approach we have followed is different.
The10goals pursued in our work must be stated explicitly: 1.to yield a cognitive model of idiom processing; and 2. tointegrate idioms in our lexical data merely as furtherinformation concerning words (as in a traditional dictio-nary).
Idiom understanding is based on normal syntacticanalysis with word-driven recognition in the back-ground.
When a certain threshold is crossed by theweight of a particular idiom, the latter starts a process ofits own, that may eventually ead to a solution.
Some ofthe questions we have dealt with are: How are idioms tobe specified?
When are they recognized?
What happenswhen they are recognized?
And what happens after-wards?Note that a morphological nalyzer, WED-MORPH(Stock et al 1986), linked to WEDNESDAY 2, plays asubstantial role, especially if the language is Italian.6.1 SPECIFICATION OF IDIOMS IN THE LEXICONIdioms are introduced in the lexicon as further specifi-cations of words, just as in a normal dictionary.
Theymay be of two types: a. canned phrases, that justbehave as several-word entries in the lexicon (there isnothing particularly interesting in that, so we shall notgo into detail here); b. flexible idioms; these idioms aredescribed in the lexicon bound to the particular wordrepresenting the "thread" of that idiom; in WEDNES-DAY 2 terms, this is the word that bears the Main of theimmediate constituent including the idiom.
Thus if wehave an idiom like "to build castles in the air", it will bedescribed along with the verb "to build".After the normal word specifications, the word mayinclude a list of idiomatic entries.
Figure 5 shows a BNFspecification of idioms in the lexicon.
The symbol +stands for "at least one occurrence of what precedes".Each idiom is described in two sections: the first onedescribes the elements that characterize that idiom,expressed coherently with the normal characterizationof the word; the second one describes the interpreta-tion, i.e., which substitutions hould be performedwhen the idiom is recognized.Let us briefly describe Figure 5.
The lexical formindicates whether passivization (which, in our theory,as in LFG, is treated in the lexicon) is admitted in theidiomatic reading.
The idiom-stats, describing configu-rations of the components of an idiom, are based onthe basic impulses included in the word.
In otherwords constituents of an idiom are described as partic-ular fillers of linguistic functions or particular modifiers.For example "build castles in the air", when "build" isin an active form, has "castles" as a further descriptionof the filler of the OBJ function and the string "in theair" as a further specification of a particular modifierthat may be attached to the Main node.
MORESPECI-FIC, the further specification ofan impulse to set a fillerfor a function includes: a reference to one of thepossible alternative types of fillers specified in thenormal impulse, a specification that describes the frag-ment hat is to play this particular role in the idiom, andComputational Linguistics, Volume 15, Number 1, March 1989Oiiviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in Mind<idioms>:: =(IDIOMS<idiomentry> +)<idiomentry>:: = (<lexicalform> <idiom-stat> + SUBSTITUTIONS <idiomsubst> +)<lexicalform>:: =T/(NOT-PASSIVE)<idiom-stat >:: = (MORESPECIFIC <lingfunc ><alternnum> < fragment spec > <weight>)/(CHANGEIMPULSE<lingfunc > <alternative> + <fragmentspec> <weight >)/(IDMODIFIER <fragmentspec> <weight>)/(REMOVEIMPULSE<Iingfunc>)<alternative> :: = (<test> <fillertype > < beforelh ><features> <mark> <sideffect >< fragment spec >)< fragmentspec>:: = (WORD<word>)/(FIXWORDS <wordseq>)/(FIRSTWORDS <wordseq>)/(MORPHWORD<wordroot >)/(SEM (<concept> +)<prep>)/(EQSUBJ)<idiomsubst>:: = (SEM-UNITS<sem-unit> +)/(MAIN<node>)/(BINDINGS(<lingfunc > <node>) +)/(NEWBINDINGS(<node> <iingfunc path>)+)Figure 5.the weight that this component has in the overallrecognition of the idiom.
IDMODIFIER is a specifica-tion of a modifier, including the description of thefragment and the weight of this component.CHANGEIMPULSE and REMOVEIMPULSE allownormal syntactic behavior to be modified.
The formerspecifies a new alternative for a filler for an existingfunction, including the description of the componentand its weight (for instance, the new alternative may bea partial NP instead of a complete NP, as in "takecare"), or a NP marked differently from usual).
Thelatter specifies that a certain impulse, specified for theword, is to be considered to have been removed for thisidiom description.There are a number of possible fragment specifica-tions, including string patterns, semantic patterns, mor-phological variations, coreferences,tetc.Substitutions include the semantics of the idiom,which are supposed to take the place of the literalsemantics, plus the specification ofthe new Main and ofthe bindings for the functions.
New bindings may beincluded to specify new semantic linkings not present inthe literal meaning (e.g., "take care of <someone>", ifthe meaning is "to attend to <someone>", then"<someone>" must become an argument of "attend").6.2 IDIOM PROCESSINGIdiom processing works in WEDNESDAY 2 by beingintegrated in the nondeterministic, multiprocessing-based behavior of the parser.
As the normal (literal)analysis proceeds and partial representations are built,impulses are monitored in the background, and a checkmade for possible idiomatic fragments.
Monitoring iscarried out only for fragments of idioms not in contrastwith the present configuration.
A dynamic activationtable is introduced with the occurrence of a word thathas some associated idiom specification.
The occur-rence of an expected fragment of an idiom in the tableraises the level of activation of that idiom, in proportionto the relative weight of the fragment.
If the configura-tion of the sentence contrasts with one fragment, henthe relative idiom is removed from the table.
So all thenormal processing continues, including the possiblenondeterministic choices, the establishing of new pro-cesses, etc.
The activation tables are included in theedges of the chart.When the activation level of a particular idiomcrosses a fixed threshold, a new process is introduced,dedicated to that particular idiom.
In that process onlythat particular idiomatic, interpretation is considered.So, in the first place, an edge is introduced in whichsubstitutions are carried out; the process will proceedwith the idiomatic representation.
Note that the processbegins at that precise point, with all the previous literalanalysis acquired by the idiomatic analysis.
The originalprocess goes on as well (unless the fragment that causedthe new process is nonsyntactic and peculiar to thatidiom alone); the idiom is merely removed from theactive idiom table.
At this point there are two workingprocesses and it is up to the (external) schedulingfunction to decide priorities.
Relevant points are: a. theidiomatic process may still result in failure: furtheranalysis may not confirm what has been hypothesizedas an idiom; and b. a different idiomatic process maystart from the literal process at a later stage, when itsown activation level crosses the threshold.
Overall, thisyields all the analyses, literal and idiomatic, with like-lihoods for the different interpretations.
But it alsoprovides a reasonable model of how humans processidioms.
Some psycholinguistic experiments have givensupport o this view, which seems also compatible withthe model presented by Swinney and Cutler (1978).Here we have disregarded the situation in which apossible idiomatic form occurs and its role in disambig-uating.
The whole parsing mechanism in WEDNES-DAY 2 is based on dynamic unification, i.e., at everystep in the parsing process a partial interpretation isprovided; dynamic hoices are performed by schedulingthe agenda on the basis of the relation between partialinterpretation a d the context.Computational Linguistics, Volume 15, Number 1, March 1989 11Oliviero Stock Parsing ,with Flexibility, Dynamic Strategies, and Idioms in Mind(sem-units(nl(p-take n2 n3)))(likeliradix 0.8)(main nl)(lingfunctions (subj n2)(obj n3))(cat v)(uni(subj)(must 0.7)((t np 0.9 nil nom)))(uni (obj)(must)((t np 0.3 nil acc)))(idioms((t(morespecific (obj) 1 (fixwords il toro) 8)(idmodifier (fixwords per le corna) I0)substitutions(sem-units (ml(p-confront m2 m3))(m4 (p-situation m3))(m5 (p-difficult m3)))(main ml)(bindings (subj m2))\]Figure 6.6.3 AN EXAMPLEAs an example, let us consider the Italian idiom pren-dere il toro per le coma (literally: "to take the bull bythe horns"; idiomatically: "to confront a difficult situ-ation").
The verb prendere ("to take") in the lexiconincludes some descriptions of idioms.
Figure 6 showsthe representation of prendere in the lexicon, but in-cludes only information relevant o our example.
Thestem representation will be unified with other informa-tion and constraints coming from the affixes involved ina particular form of the verb.
The first portion of therepresentation is devoted to the literal interpretation ofthe word and includes the semantic representation, thelikelihood of that reading, and functional information,including the specification of impulses for unification.The numbers are likelihoods of the presence of anargument or of a relative position of an argument.
Thesecowd portion, after idioms, includes the idioms in-volving prendere.
In Figure 6 only one such idiom isspecified.
It is indicated that the idiom can also occur ina passive form (the first t specification) and the specifi-cation of the expected fragments i given.
The numbershere are the weights of the fragments (the threshold isfixed at 10).
The substitutions include the new semanticrepresentation, with the specification of the main nodeand of the binding of the subject.
Note that the surfacefunctional representation will not be destroyed after thesubstitutions, only the semantic (logical) representationwill be recomputed, imposing its own bindings.As mentioned, Italian allows great flexibility.
Let theinput sentence be L'informatico prese per le corna lacapra (literally: "The computer scientist ook by thehorns the goat").
When prese ("took") is analyzed, itsidiom activation table is inserted.
When the modifier perle coma ("by the horns") shows up, the activation ofthe idiom referred to above crosses the threshold (thesum of the two weights goes up to 12).
A new processstarts at this point, with the new interpretation unifiedwith the previous interpretation of the subject.
Also,semantic specifications coming from the suffixes arereused in the new partial interpretation.
The processmerely departs from the literal process--no backtrack-ing is performed.
At this point we have two processesgoing on: an idiomatic process, where the interpretationis already "The computer scientist is facing a difficultsituation" and a literal process, where, in the back-ground, yet other active idioms monitor the events.
InFigure 7 the two semantic representations are shown inthe form of semantic networks (they correspond to twosets of propositions).When the last NP, la capra ("the goat"), is recog-nized, the idiomatic process fails (it needed "the bull"as object).
The literal process yields its analysis, butC11105 C21103 C101132 C201130 C31139 C41140p SCIENTIST/~ 2~O~MPU TER p AT ~ 2 C ~ 1 0 1 1 ~ ~ F I C U IXIIlI~(a)C21125 C11123 CI 1118 C101114 C201117 CI 1105 C21103P-TAKE X11106 X2110812Figure 7.Computational Linguistics, Volume 15, Number 1, March 1989O|iviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in Mindanother idiom crosses the threshold, starts its processwith the substitutions, and immediately concludes pos-itively.
This latter, unlikely, idiomatic interpretation isthat the computer scientist confused the goat and thehorns.7 THE EXPLORATIVE ENVIRONMENTComputer-based nvironments for the linguist are con-ceived as sophisticated workbenches, built on AI work-stations around a specific parser, where the linguist cantry out his/her ideas about a grammar for a certainnatural language.
In doing so, he/she can take advantageof rich and easy-to-use graphic interfaces that "know"about linguistics.
Of course, behind all this lies the ideathat cooperation with linguists will provide better re-sults in NLP.
To substantiate this assumption it may berecalled that some of the most interesting recent ideason syntax have been developed by means of jointcontributions from linguists and computational lin-guists.Instances of the tools introduced above are the LFGenvironment, which was probably the first of its kind,an environment built by Ron Kaplan for iexical-func-tional grammars, D-PATR, built by Lauri Karttunen(Karttunen 1986) and conceived as an environment thatwould suit linguists of a number of different schools allcommitted to a view of parsing as a process that makesuse of a unification algorithm.The environment we have developed has a somewhatdifferent purpose.
Besides a number of tools for enter-ing data in graphic mode and inspecting resulting struc-tures, it provides a means for experimenting with strat-egies in the course of the parsing process.
We think thatthis can be a valuable tool for gaining insight into thecognitive aspects of language processing as well as fortailoring the behaviour of the processor when used witha particular sublanguage.In this way an attempt can be made to answer basicquestions in the course of a nondeterministic approach:what heuristics to apply when facing a certain choicepoint, what to do when facing a failure point, i.e., whichof the pending processes to activate, taking account ofinformation resulting from the failure?
Of course thiskind of environment makes sense only because theparser it works on has some characteristics that make ita psychologically interesting realization.Psychologically motivated parsers may be put inthree main categories.
First, those that embody a strongclaim on the specification of the general control struc-ture of the human parsing mechanism.
The authorsusually consider the level of basic control of the systemas the level they are simulating and are not concernedwith more particular heuristics.
An instance of this classof parsers in Marcus's parser (Marcus 1979), based onthe claim that, basically, parsing is a deterministicprocess: only sentences that we perceive as surprising(the so-called "garden paths") actually imply back-Computational Linguistics, Volume 15, Number 1, March 1989tracking.
Connectionist parsers are also instances ofthis category.
The second category refers to generallinguistic performance notions uch as the Lexical Pref-erence Principle and the Final Argument Principle(Fodor, Bresnan, and Kaplan 1982).
It includes theoriesof processing like the one expressed by Wanner andMaratsos for ATNs in the mid-'70s.
In this category thearguments are at the level of general structural prefer-ence analysis.
A third category tends, at every stage ofthe parsing process, to consider the full complexity ofthe data and the hypothetical partial internal represen-tation of the sentence, including, at least in principle,interaction with knowledge of the world, aspects ofmemory, and particular task-oriented behavior.
Worthmentioning here are Church and Patil (1982) and Bartonet al (1987), who attempt to put some order in the chaosof complexity and computational load.Our parser lies between the second and the third ofthe above categories.
The parser is seen as a non-deterministic apparatus that disambiguates and gives a"shallow" interpretation a d an incremental functionalrepresentation f each processed fragment of the sen-tence, and allows choices to be made amOng alterna-tives in a cognitively meaningful way.
We shall brieflydescribe the environment and, by way of example,illustrate its behavior by analyzing oscillating sen-tences, i.e., sentences in which one first perceives afragment in one way, then changes one's mind and takesit in a different way, then, as further input comes in,goes back to the previous pattern (and possibly continu-ing like this till the end of the sentence).7.1 OVERVIEW OF THE ENVIRONMENTWEDNESDAY 2 and its environment are implementedon a Xerox Lisp Machine.
The coverage of WEDNES-DAY 2 with the present Italian data includes subordi-nates, complex relative clauses, interrogatives, and soon, all this possibly mixed with idioms.
The currentlexicon is about 1,500 stems, resulting in about 30,000forms, but it is very easy to extend it, with the toolsbuilt around the parser.
An effort is currently beingmade to revisit linguistic specifications in the lexicon.Moreover, we have been reversing the process for thepurpose of generation (Caraceni and Stock 1987).
Wethink that, beside the interest of generation per se andthe usefulness of having a generator based on the sameprinciples and data of the parser, this is, paradoxically,the real test for our parser.
The results, so far, havebeen very encouraging, including the fact that it was nothard to make good the deficiencies we detected in ourparser when dealing with problems the other wayround.The environment is composed of a series of special-ized tools, each one based on one or more windows(Figure 8).Using a mouse the user selects a desired behaviorfrom menus attached to the windows.
We have thefollowing windows:13Oliviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in Mind~11 ?
F=~ .-11VERTEX:  1 :.,r ' .
-I :,a : .
-  :1 .
: , :  .. l ~ , ,~ ' r \ [  :" 't :  1.~ ?.'
: .~1,~x\[ .? "
're, 1VERTEX:  2 c , ,I ~.~ ~'.
F F'EI:'I N~" ~C'I 1(I  PREPM~E~ re,  )I ~.
P~E~" 'tO 315 S /PREP lNF  S ' r~r l :  ~ /PREP I~a 1,~ $/PFtE I :  INF  !
?~TE '  :...'~F'EI~ \] |A 11 PI:',M~EP.
S ' r~r \ [  I : ' t : , l l~gk  'r(PF' sx~rE '  PF TC' ~.~-1 $/ l : l tEP  1 r; ~:?'
?-~.
PF  .'
M ~ r,;;"?
?-.
: 8 COMPL?
~.o~.t I !
r .CE~.=\ [  $ TC ~ .
"'r ~a.~ "?
a. T IK ~/~: '0$TNk lN\ [ .
r l~ .T l~ l~ '  rEReTE laSE  ( IN \ [ '  CONC'~la  C11"1 .
?,me,~ N ILt l l l i f l  I N | Lk, INGI~UNC T I 0 I'4 |e .
-0B , l  (C11719 .
3 )mien.
N i tee*lu~t~ NILOB~l ( C11" t1~ ~.
)m*~ NIL~e *lu~es N I L$UI~3 ~ C l1" r19  1)~ '*~ N ILftt*t~rel NIL/~-O~ 2 bllten#1, .4|PF  M~.
.
.
.
.
, ,, , ..IIITli'i r?.
~-4  pr  N~: ".hfC'r3,"'f:'-.?
~.'fO"t.
'Z3 C~.Cz'IT'2~r?.
?
$.C, NE NF' ~ "~-~ .Z"VERTEX:  3 oml~ SUE3 *~een*t, vel P- IP  i kSTl 51 $ / INF  TO" ~ Nr  P 'SP  P 'AT"  ~"117 ' r9  c loo ,  r~Tl | 91E VINF~'INF TO.TO'4 4 ' i  .
.
_ _ .~ .
.
___ .
.
~ .
.1  ~ ' - - - -40 S INF  !T~E ~.
'PO~TMalN.:|.
- c  ~, ~,,?
.
.
.
.
r. r r ,  cT .
,  , .
,  .m 191P' t l  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,, , .
.
.
.  "
?
? "
?
: ' ; ' :  : :',';'i'" " '  ' .  '
(  ' ':'.. ,",  '.'
'.'.'
:.:.
: :.:.:.:.'.'.:+:.:.'+:'..:.
: : '.:.:.:+:::':':.
:':':: '.
':" ;';';'; : :'": : : '"  ""'~ ' .
"=rf-~lr,l~itl,~f~Q ffc, r'f-, n,~l~le ~,~' t,~a~L~ to  t"lo(~e ~{~ pF~El~rd/tt II~j ~F: i L 'r' i.,:11,= .
.
.
.
~1 F ?
r :iii:i!iii!!ii:.ii!ii!ii:!:iii!
!i~ii:iiii:~il I i i:::i i;:iiii!
'!iiii !
:.~' ;;'re  a c . '
?
d u ~ e l ?
, t~'~ ?
f o o .
,  n c~ .
'a  t u ?
.- .~n  .
.
.
.
.
.
- : : :=  = =~, '~" - - -  .- :?~i!i!iiiii~iiii!iii!i ?.i !
i !!
i  : i ' !
i i i i  i 51!
i i !
i  ~i i :  : : i  : : : : : :  : : : !
: :  !
:~::~ i : : .. =~ e= r.~ ~ ,  ~ r ,  " i!
I , vEto -  ~ iiii~i~i;~!iiiiiiiiiii!ii:::~i:iiiiii::;ili:::~ii!ii:ii!ii:::  !!
::::  i ::!
::::: ~i: i ; :~ : !ili i ::::!
: ; ii!
:i:i:ili::: ?
::"'~='~P~; I VERRn-PP-DI-ma ; f ~, ".
::iiiii.il - I I  ~ - - ' - - - i i !
!!!i:1;t;1~11!
!i:iI / \ '.
i :  :::!\]:i:i:i: - ~ i i : : i : i i  I ~ 1  i ' :?
.
?
~ ~E~,~,~,  - - - - _ .
-o .
1 , ..:::::: :::: .
.
: .
: : .
.
l~t t t t t t l  .
.?
= -r - ~  \] I VERBS-8U6-083 :!
:::::::::::::" ~ ' : : : : : :  I~1: : ' : '~ .
~ .
: : : : : : : : : : : : :  .
: : .
: :  .
.
.
.
: .
; .
.
.
.
.
: :  .
.
.
: : : : :  : .
.
.
.
.
.
.
.
.::.:::,.
',~ ,.~ L"  "T" E ,  p ~n ~ ~ ~', n r',~ !1 ' i:i:~:::i:i:i:i:i:i:!:!:!;!
: :: '.:!:!:!:!:::!
:i:ii:iL: :':!.i:i:i:iii: iiiiii!!!!!'.~!:ililjii!iii:!
!ii!i!;:i.i:i.!
:Figure 8.?
the main WEDNESDAY 2 window, in which thesentence is entered.
Menus attached to this windowspecify different modalities (including "through" and"stepping", "all parsings" or "one parsing") and anumber of facilities;?
a window where one can view, enter, and modifytransition networks graphically (Figure 9).?
a window where one can view, enter and modify thelexicon.
As a word reading is a complex object forWEDNESDAY 2, entering a new word can be greatlyfacilitated by a set of subwindows, each specialized inone aspect of the word, "knowing" what it may belike and facilitating editing.The lexicon is a lexicon of stems: a morphologicalanalyzer and a lexicon manager are integrated in thesystem.
Let us briefly describe this point.
A lexicalisttheory such as ours requires that a large quantity ofinformation be included in the lexicon.
This informationhas different origins: some comes from the stem and14some from the affixes.
All the information must be putinto a coherent data structure, through a particularlyconstrained unification-based process.
Furthermore,we must emphasize the fact that, just as in LFG,phenomena such as passivization are treated in thelexicon (the Subject and Object functions and the re-lated impulses attached to the active form are rear-ranged).
This is something that the morphological ana-lyzer must deal with.
The internal behavior of themorphological analyzer is beyond the scope of thepresent paper.
We shall instead briefly discuss thelexicon manager, the role of which will be emphasizedhere.The lexicon manager deals with the complex processof entering data, maintaining, and preprocessing thelexicon.
One notable aspect is that we have arranged thelexicon on a hierachical basis according to inheritance,so that properties of a particular word can be inheritedfrom a word class and a word class can inherit aspectsfrom another class.
One consequence of this is that weComputational Linguistics, Volume 15, Number 1, March 1989Oiiviero Stock Pars ing with Flexibi l ity, Dynamic Strategies, and Idioms in MindTr~o: ,* t ions  f rom noO~.
14P.
AIZ'J tO no@- ~ I4P 11)rb.
)lt,:,=~.eeJ Ul' lcler th f f  fo l lowi r l~l  '~ ;)10.1~5;-111. o .
"-..\ n  ",', I',, ' 3U IT  E ' IOAnCI ~hr inkFigure 9.can introduce a graphic aspect (Figure I0) and the usercan browse through the lattice (the lexicon appears as atree of classes where one has specialized editors at eachlevel).
What is even more relevant is the fact that onecan factorize knowledge that is in the lexicon, so that ifone particular phenomenon needs to be treated iffer-ently, the change of information is immediate for thewords concerned.
Of course, this also means that thereis a space gain: the same information does not need tobe duplicated--complete word data are reconstructedwhen required.There is also a modality by which one can enter thesyntactic aspects of a word through examples, ~ laTEAM (Grosz 1984).?
a window showing the present configuration of thechart;?
a window that allows zooming into one edge, showingseveral aspects of the edge, including its structuralaspects, its likelihood, the functional aspect, thespecification of unrealized impulses, etc.?
a window graphically displaying the semantic inter-pretation of an edge as a semantic net or, if oneprefers (this is usually the case when the net is toocomplex), in logical format;?
a window where one can manipulate the agenda(Figure I 1).Attached to this window we have a menu including aset of functions that the tasks included in the agenda tobe manipulated: "move before", "move after","delete", "switch", "undo", etc.
One just points tothe two particular tasks one wishes to operate on withthe mouse and then to the menu entry, thus obtainingthe desired effect.
The same effect could be obtained byapplying a different scheduling function: the tasks willbe picked up in the order here prescribed by manualscheduling.
This tool, when the parser is in the steppingmodality, provides a very easy way of altering thedefault behavior of the system and of trying out newstrategies.
The manual scheduling mode is supple-mented by a series of counters that provide control overthe penetrance of these strategies.
(The penetrance ofanondeterministic algorithm is the ratio between theLe4~ SmmVI~I.~Oi IW-OIJ \]/VIOII~I-OIJ /HI - I I F -4 iD  V I I I I3 -1~I L41t,11NIL.k=l\[WJ X3 NILX2 NILR I J  ~1 HK.Test Gap kfm'MikelihoodFeatures Mark S idc t fec t~(A-Oh..1)Olu~!
.a)((T PP/l l~K I NIL a(obJ)(Nu~!
.~)((T NP .J.
NIL NIL NI(su~J)(xusr .s)((T NP .3 NIL NIL N|Figure 10.Computational Linguistics, Volume 15, Number 1, March 1989 15Oliviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in MindTT  A: 13 I: 41 FllrW Lfl:0.O40500006 FlEW Tt4.$:~;,' .IU$ TE ?ITLT ,,,erte?
: 5 ?~t; N LH: 0,7LT  vertex: 4 cJ.t: PI~EPM.~,.F,I,~.&.RTLH; 1.0TT  A: 13 I: 16 NEW LH;0,04S;00001 FlEW TH-~:S / JUST I rX ITTT  A: 11 |: 10 NEW LH: 1,0FlEW TN.~: PP /PREPMAhKLT  vertex: 1 c&t: V LH: 0,5LT  verte?
: 1 c=t: V LH: 0,2GOFigure 11.Move .BeforeMove.Af terSwitchDeleteUndoUndo .Allsteps that lead to the solution and the overall stepscarried out in trying to obtain the solution.
Of coursethis measure lies between 0 and 1).Dynamically, it is attempted to find sensible strate-gies, by interacting with the agenda.
When, after for-malizable heuristics have been tried out, they can beintroduced permanently into the system through agivenspecialized function.
This is the only place where someknowledge of Lisp and of the internal structure ofWEDNESDAY 2 is required.7.2 AN EXAMPLE OF EXPLORATION: OSCILLATINGSENTENCESWe shall now briefly discuss a processing examplethat we have been able to understand using the environ-ment described above.
The following example is a goodinstance of flexibility and parsing problems present inItalian:A Napoli preferisco Roma a Milano.The complete sentence reads "while in Naples I preferRome to Milan".
The problem arises during the parsingprocess with the fact that the " to"  argument of"prefer" in Italian may occur before the verb, and thelocative preposition " in"  is a, the same word as themarking preposition corresponding to "to".The reader/hearer first takes a Napoli as an adverbiallocation, then, as the verb preferisco is perceived, aNapoli is clearly reinterpreted as an argument of theverb (in the sense of surprise).
As the sentence proceedsafter the object Roma, the new word a causes things tochange again and we go back with a feeling of surpriseto the first hypothesis.
When this second reconsider-ation takes place, we feel the surprise, but this does notcause us to reconsider the sentence, we only go back toadding further to an hypothesis we were already work-ing at.
It should also be noted that the surprise seems tobe caused not by a heavy computational load, but by a16sudden readjustment of the weights of the hypotheses.In a sense it is a matter of memory, rather thancomputation.We have succeeded in getting WEDNESDAY 2 toperform naturally in such situations, taking advantageof the environment.
The following simple heuristicswere found: a. try solutions that satisfy the impulses (ifthere are alternatives consider likelihoods); b. maintainviscosity (prefer the path you are already following);and c. follow the alternative that yields the edge withthe greatest likelihood chosen among edges of compa-rable length.The likelihood of an edge depends on: 1. the likeli-hood of the included edges; 2. the level of obligatorinessof the filled impulses; 3. the likelihood of a particularrelative position of an argument in the string; 4. thelikelihood of that transition in the network, given theprevious transition.The critical points in the sentence are the following(note that we distinguish between a PP and a "markedNP" possible argument of a verb, where the prepositionhas no associated semantics):i.
At the beginning: only the PP edge is expanded,(not the one including amarked NP) because of staticpreference for the former expressed in the lexiconand in the transition etwork.ii.
After the verb is detected: on the one hand there isan edge that, if extended, would not satisfy anobligatory impulse, on the other hand, one thatpossibly would.
The marked NP alternative is chosenbecause of (a) of the above heuristics.iii.
After the object Roma: when the preposition acomes in, the edge that can extend the sentence witha PP on the one hand, and on the other hand a cyclingactive edge that is a promising satisfaction for animpulse are compared.
Since this relative position ofthe argument is so favorable for the particular verbpreferisco (.9 to .
I for this position compared to thepreceding one), the parser proceeds with the alterna-tive view, taking a Napoli, as a modifier, and so on,after eentering the working hypothesis.
The object isalready there, analyzed for the other reading anddoes not need to be reanalyzed.
So a Milano is takenas the filler for the impulse and the analysis isconcluded properly.CONCLUSIONSWe have focused on some important features of aparser, e.g., able to perform unification while workingon structures, able to analyze idiomatic forms, provid-ing a good testbed for ideas about dynamic strategies.The parser we have introduced is based on linguisticknowledge distributed fundamentally through the lexi-con and uses a chart with unification activity developedwhen new edges are established.
Especially if wordorder is flexible, it is important to avoid redundancy anddetect useless hypotheses atan early stage.
Here, chartComputational Linguistics, Volume 15, Number 1, March 1989Oiiviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in Mindparsing is based on a bottom up with left contextconfirmation strategy and unification is based on thePrinciple of the Good Clerk.
Also, we have proposed aview of the idiom recognition process that is completelyintegrated with the parser, so that, in particular, idiomrecognition can take advantage of the flexibility of theapproach without redundancy.
We think that this isimportant, because idioms are on a continuum withliteral language, and, share the "normal" characteris-tics, including the flexibility, of the particular languageinvolved.
Moreover, given that the algorithm is of amultiprocessing type, one can consider what strategiescan be introduced in order to select dynamically thetasks that have best chances of leading to the (prefer-able) final analysis.
These heuristics can also takeadvantage of measures of likelihood associated witheach partial analysis.
An integrated interactive nviron-ment has been built to experiment with these ideas,which can also be used to define sublanguages andselect strategies for particular applications.ACKNOWLEDGMENTSThe author wishes to thank the following people who advised him,probably without much hope, at different stages of this work: C.Cacciari, C. Castelfranchi, R. Kaplan, M. Kay.
Special thanks to F.Cecconi for his contribution to the implementation of the environ-ment.
Particular thanks go to the anonymous referees that havehelped (hopefully) in making a readable paper out of an unreadableone .Most of the work was carried out while the author was working forthe Italian National Research Council, at the lstituto di Psicologia inRome, and reaped particular benefit from the resources of theArtificial Intelligence Strategic Project, which, in spite of its name,was a purely civilian, basic research initiative.REFERENCESAho, A.V.
and Ullman, J.D.
1972 The Theory of Parsing, Translation,and Compiling.
Vol.
I: Parsing.
Prentice-Hall, Englewood Cliffs,NJ.Barton, E.; Berwick, R.; and Ristad, E. 1987 Computational Com-plexity and Natural Language.
MIT Press, Cambridge, MA.Bresnan, J.
(ed.)
1982 The Mental Representation f GrammaticalRelations.
MIT Press, Cambridge, MA.Caraceni, R. and Stock, O.
1987 Reversing a Lexically Based Parserfor Generation.
Applied Artificial Intelligence.
An InternationalJournal 1:149-174.Church, K. and Patil, R. 1982 Coping with syntactic ambiguity or howto put the block in the box on the table.
American Journal ofComputational Linguistics 8: 139-149.Dyer, M. and Zernik, U.
1986 Encoding and Acquiring Meaning forFigurative Phrases.
In Proceedings of the 24th Meeting of theAssociation for Computational Linguistics, New York, NY.Earley, J.
1970 An Efficient Context-free Parsing Algorithm.
Com-munications of the Association for Computing Machinery 13(2):94-102.Ferrari, G. and Stock, O.
1980 Strategy Selection for an ATNSyntactic Parser.
In Proceedings of the 18th Meeting of theAssociation for Computational Linguistics, Philadelphia, PA.Fillmore, C. 1979 Innocence: a Second Idealization for Linguistics.
InProceedings of the Fifth Annual Meeting of the Berkeley Linguis-tics Society, University of California at Berkeley: 63-76.Ford, M.; Bresnan, J.; and Kaplan, R. 1982 A Competence-BasedTheory of Syntactic Closure.
In Bresnan, J.
(ed.)
The MentalComputational Linguistics, Volume 15, Number 1, March 1989Representation fGrammatical Relations.
MIT Press, Cambridge,MA.Gazdar, G. and Pullum, G.K. 1982 Generalized Phrase StructureGrammar: A Theoretical Synopsis.
Indiana University LinguisticsClub, Bloomington, IN.Grosz, B.
1983 TEAM, a Transportable Natural Language InterfaceSystem.
In Proceedings of the Conference on Applied NaturalLanguage Processing, Santa Monica, CA.Hayes, P.J.
and Carbonell, J.G.
1981 Multi-Strategy Parsing and itsRole in Robust Man-Machine Communication.
Computer ScienceDepartment, Carnegie-Mellon University, Pittsburgh, PA.Hendrix, G.G.
1977 LIFER: a Natural Language Interface Facility.SIGART Newsletter 61.Jacobs, P. 1985 PHRED: A Generator for Natural Language Inter-faces.
Computational Linguistics 11(4): 219--242.Joshi, A., and Levy, L. 1982 Phrase Structure Trees Bear More FruitsThan You Would Have Thought.
American Journal of Computa-tional Linguistics 8:1-11.Kaplan, R. 1973 Augmented Transition Networks as PsychologicalModels of Sentence Comprehension.
Artificial Intelligence 3: 77-100.Kaplan, R. 1973 A General Syntactic Processor.
In Rustin, R.
(ed.
)Natural Language Processing.
Prentice-Hall, Englewood Cliffs,NJ.Kaplan, R. and Bresnan, J.
1982 Lexical-Functional Grammar: AFormal System for Grammatical Representation.
In Bresnan, J.(ed.)
The Mental Representation fGrammatical Relations.
MITPress, Cambridge, MA: 173-281.Kaplan, R., Maxwell, J., and Zaenen, A.
1987 Functional Uncer-tainty.
CSLI Monthly 2(4): 1--6.Karttunen, L. 1986 D-PATR: A Development Environment for Uni-fication-Based Grammars, Report No.
CSLI-86-61.
Center for theStudy Of Language and Information.
Palo Alto, CA.Kay, M. 1979 Functional Grammar.
In Proceedings of the FifthMeeting of the Berkeley Linguistic Society, Berkeley, CA: 142-158.Kay, M. 1980 Algorithm Schemata nd Data Structures in SyntacticProcessing.
Xerox, Palo Alto Research Center, Palo Alto, CA.Kay, M. 1985 Parsing in Functional Unification Grammar.
In Dowty,D.
; Karttunen, L.; and Zwicky, A.
Natural Language Parsing:Psychological, Computational, and Theoretical Perspectives.Cambridge University Press, Cambridge, England.Marcus, M. 1979 An Overview of a Theory of Syntactic Recognitionfor Natural Language (AI memo 531).
MIT Artificial IntelligenceLaboratory, Cambridge, MA.Pereira, F. and Warren, D. 1980 Definite Clause Grammars forLanguage Analysis.
A Survey of the Formalism and a Comparisonwith Augmented Transition Networks.
Artificial Intelligence 13:231-278.Ritchie, G. and Thompson, H. 1984 Implementing Natural LanguageParsers.
In O'Shea, T. and Eisenstadt, M.
(eds.)
Artificial Intelli-gence: Tools, Techniques, and Applications.
Harper and Row.New York, NY.Shieber, S.M.
1986 An Introduction to Unification-Based Approachesto Grammar.
CSLI Lecture Notes Series No.
4, University ofChicago Press, Chicago, IL.Shieber, S.M.
; Uzkoreit, H.; Pereira, F.; Robinson, J.; and Tyson, M.1983 The Formalism and Implementation fPATR II.
In Grosz B.and Stickel M.
(eds.)
Research on Interactive Acquisition and Useof Knowledge, SRI Report 1894, SRI International, Menlo Park,CA.Small, S. 1980 Word Expert Parsing: a Theory of Distributed Word-Based Natural Language Understanding.
Technical Report TR-954 NSG-7253.
University of Maryland, Baltimore, MD.Stock, O.
1986 Dynamic Unification in Lexically Based Parsing.
InProceedings of the Seventh European Conference on ArtificialIntelligence; Brighton, England: 212-221.17Oliviero Stock Parsing with Flexibility, Dynamic Strategies, and Idioms in MindStock, O.; Cecconi, F., and Castelfranchi, C. 1986 Analisi Morfolo-gica Integrata in un Parser a Conoscenze Linguistiche Distribuite,In Atti del Convegno AICA-86, Palermo, Italy.Swinney, D.A., and Cutler, A.
1978 The Access and Processing ofIdiomatic Expressions.
Journal of Verbal Learning and VerbalBehaviour 18, 523-534.Thompson, H.S.
1981 Chart Parsing and Rule Schemata inGPSG.
InProceedings of the 19th Annual Meeting of the Association forComputational Linguistics, Alexandria, VA.Tomita, M. 1985 An Efficient Context-Free Parsing Algorithm forNatural Languages.
In Proceedings of the Ninth InternationalJoint Conference on Artificial Intelligence, Los Angeles, CA.Waltz, D. 1978 An English Language Question Answering System fora Large Relational Database.
Communications ofthe Associationfor Computing Machinery 21(7).Wanner, E. and Maratsos, M. 1978 An ATN Approach to Compre-hension.
In Halle, M.; Bresnan, J.; and Miller, G.A.
(eds.
):Linguistic Theory and Psychological Reality.
MIT Press, Cam-bridge, MA.Wasow, 1~.
; Sag, I.; and Nunberg, G. 1982 Idioms: an Interim Report.Preprints of the International Congress of Linguistics, Tokyo,Japart: 87-96.Wilensky, R. and Arens, Y.
1980 PHRAN--A Knowledge-BasedApproach to Natural Language Analysis.
ERL Memorandum No.UCB/ERL M80/34.
University of California at Berkeley, CA.Wiren, M. 1987 A Comparison of Rule Invocation Strategies inContext-free Chart Parsing.
In Proceedings of the Third Confer-ence of the European Chapter of the Association for Computa-tional Linguistics, Copenhagen, Denmark.Woods, W. 1970 Transition Network Grammars for Natural LanguageAnalysis.
Communications of the Association for ComputingMachinery 13.NOTEA recent alternative treatment of long-distance dependencieswithin the LFG approach, which gives away with structure-basedoperations, is Kaplan et al (1987).18 Computational Linguistics, Volume 15, Number 1, March 1989
