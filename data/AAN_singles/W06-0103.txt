Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 17?24,Sydney, July 2006. c?2006 Association for Computational LinguisticsMining Atomic Chinese Abbreviation Pairs: A Probabilistic Model forSingle Character Word RecoveryJing-Shin ChangDepartment of Computer Science &Information EngineeringNational Chi-Nan UniversityPuli, Nantou, Taiwan, ROC.jshin@csie.ncnu.edu.twWei-Lun TengDepartment of Computer Science &Information EngineeringNational Chi-Nan UniversityPuli, Nantou, Taiwan, ROC.S3321512@ncnu.edu.twAbstractAn HMM-based Single CharacterRecovery (SCR) Model is proposed inthis paper to extract a large set of?atomic abbreviation pairs?from a largetext corpus.
By an ?atomic abbreviationpair,?it refers to an abbreviated wordand its root word (i.e., unabbreviatedform) in which the abbreviation is asingle Chinese character.This task is interesting since theabbreviation process for Chinesecompound words seems to be?compositional?
; in other words, onecan often decode an abbreviated word,such as ????
(Taiwan University),character-by-character back to its rootform.
With a large atomic abbreviationdictionary, one may be able to recovermultiple-character abbreviations moreeasily.With only a few training iterations, theacquisition accuracy of the proposedSCR model achieves 62% and 50 %precision for training set and test set,respectively, from the ASWSC-2001corpus.1 IntroductionChinese abbreviations are widely used in themodern Chinese texts.
They are a special formof unknown words, which cannot beexhaustively enumerated in an ordinarydictionary.
Many of them originated fromimportant lexical units such as named entities.However, the sources for Chinese abbreviationsare not solely from the noun class, but from mostmajor categories, including verbs, adjectivesadverbs and others.
No matter what lexical orsyntactic structure a string of characters could be,one can almost always find a way to abbreviateit into a shorter form.
Therefore, it may benecessary to handle them beyond a class-basedmodel.
Furthermore, abbreviated words aresemantically ambiguous.
For example, ???
?can be the abbreviation for??????or??????
; on the opposite direction, multiplechoices for abbreviating a word are also possible.For instance,?????
?may be abbreviated as???
?, ???
?or ?????.
This results indifficulty for correct Chinese processing andapplications, including word segmentation,information retrieval, query expansion, lexicaltranslation and much more.
An abbreviationmodel or a large abbreviation lexicon istherefore highly desirable for Chinese languageprocessing.Since the smallest possible Chinese lexicalunit into which other words can be abbreviatedis a single character, identifying the set ofmulti-character words which can be abbreviatedinto a single character is especially interesting.Actually, the abbreviation of a compound wordcan often be acquired by the principle ofcomposition.
In other words, one can decomposea compound word into its constituents and thenconcatenate their single character equivalents toform its abbreviated form.
The reverse processto predict the unabbreviated form from anabbreviation shares the same compositionalproperty.The Chinese abbreviation problem can beregarded as an error recovery problem in whichthe suspect root words are the ?errors?
to berecovered from a set of candidates.
Such aproblem can be mapped to an HMM-basedgeneration model for both abbreviationidentification and root word recovery; it can also17be integrated as part of a unified wordsegmentation model when the input extends to acomplete sentence.
As such, we can find themost likely root words, by finding thosecandidates that maximizes the likelihood of thewhole text.
An abbreviation lexicon, whichconsists of the root-abbreviation pairs, can thusbe constructed automatically.In a preliminary study (Chang and Lai, 2004),some probabilistic models had been developedto handle this problem by applying the models toa parallel corpus of compound words and theirabbreviations, without knowing the context ofthe abbreviation pairs.
In this work, the sameframework is extended and a method is proposedto automatically acquire a large abbreviationlexicon for indivisual characters from web textsor large corpora, instead of building abbreviationmodels based on aligned abbreviation pairs ofshort compound words.
Unlike the previous task,which trains the abbreviation model parametersfrom a list of known abbreviation pairs, thecurrent work aims at extracting abbreviationpairs from a corpus of free text, in which thelocations of prospective abbreviations and fullforms are unknown and the correspondencebetween them is not known either.In particular, a Single Character Recovery(SCR) Model is exploited in the current work toextract ?atomic abbreviation pairs?from a largetext corpus.
With only a few training iterations,the acquisition accuracy achieves 62% and 50 %precision for training set and test set from theASWSC-2001 corpus.1.1 Chinese Abbreviation ProblemsThe modern Chinese language is a highlyabbreviated one due to the mixed uses of ancientsingle character words as well as modernmulti-character words and compound words.The abbreviated form and root form are usedinterchangeably everywhere in the currentChinese articles.
Some news articles maycontain as high as 20% of sentences that havesuspect abbreviated words in them (Lai, 2003).Since abbreviations cannot be enumerated in adictionary, it forms a special class of unknownwords, many of which originate from namedentities.
Many other open class words are alsoabbreviatable.
This particular class thusintroduces complication for Chinese languageprocessing, including the fundamental wordsegmentation process (Chiang et al, 1992; Linet al, 1993; Chang and Su, 1997) and manyword-based applications.
For instance, akeyword-based information retrieval system mayrequires the two forms, such as ?????and???????
(?Academia Sinica?
), in order notto miss any relevant documents.
The Chineseword segmentation process is also significantlydegraded by the existence of unknown words(Chiang et al, 1992), including unknownabbreviations.There are some heuristics for Chineseabbreviations.
Such heuristics, however, caneasily break (Sproat, 2002).
Unlike Englishabbreviations, the abbreviation process of theChinese language is a very special wordformation process.
Almost all characters in allpositions of a word can be omitted when usedfor forming an abbreviation of a compound word.For instance, it seems that, by commonheuristics, ?most?Chinese abbreviations couldbe derived by keeping the first characters of theconstituent words of a compound word, such astransforming ?????
?into ???
?, ?????
?into ???
?and ????(?)?????into????.
Unfortunately, it is not always the case.For example, we can transform ??????into???
?, ?????
?into ???
?, and, for verylong compounds like ???????
?into ?????
(Sproat, 2002).
Therefore, it is verydifficult to predict the possible surface forms ofChinese abbreviations and to guess their base(non-abbreviated) forms heuristically.P(bit|n) Score ExamplesP(10|2) 0.87 (?|??),(?|??
)P(101|3) 0.44 (??|???),(??|???
)P(1010|4) 0.56 (??|????),(??|????
)P(10101|5) 0.66 (???|?????),(???|?????
)P(101001|6) 0.51 (???|??????),(???|??????
)P(1010001|7) 0.55 (???|???????),(???|???????
)P(10101010|8) 0.21 (????|????????
),( ????|????????
)Table 1.
High Frequency Abbreviation Patterns[by P(bit|n)] (Chang and Lai, 2004)18The high frequency abbreviation patternsrevealed in (Chang and Lai, 2004) further breakthe heuristics quantitatively.
Table 1 lists thedistribution of the most frequent abbreviationpatterns for word of length 2~8 characters.The table indicates which characters will bedeleted from the root of a particular length (n)with a bit ?0?
; on the other hand, a bit ?1?
meansthat the respective character will be retained.This table does support some general heuristicsfor native Chinese speaker quantitatively.
Forinstance, there are strong supports that the firstcharacter in a two-character word will beretained in most cases, and the first and the thirdcharacters in a 4-character word will be retainedin 56% of the cases.
However, the table alsoshows that around 50% of the cases cannot beuniquely determined by character positionsimply by consulting the word length of theun-abbreviated form.
This does suggest thenecessity of either an abbreviation model or alarge abbreviation lexicon for resolving this kindof unknown words and named entities.There are also a large percentage (312/1547)of ?tough?
abbreviation paterns (Changand Lai,2004), which are considered ?tough?in the sensethat they violate some simple assumptions, andthus cannot be modeled in a simple way.
Forinstance, some tough words will actually berecursively abbreviated into shorter and shorterlexical forms; and others may change the wordorder (as in abbreviating ?????????as????
?instead of ?????.).
As a result, theabbreviation process is much more complicatedthan a native Chinese speaker might think.1.2 Atomic Abbreviation PairsSince the abbreviated words are createdcontinuously through the abbreviation of new(mostly compound) words, it is nearlyimpossible to construct a complete abbreviationlexicon.
In spite of the difficulty, it is interestingto note that the abbreviation process for Chinesecompound words seems to be ?compositional?.In other words, one can often decode anabbreviated word, such as ????(?TaiwanUniversity?
), character-by-character back to itsroot form ?????
?by observing that ??
?can be an abbreviation of ???
?and ??
?canbe an abbreviation of ????and?????
?isa frequently observed character sequence.Since character is the smallest lexical unit forChinese, no further abbreviation into smallerunits is possible.
We therefore use ?atomicabbreviation pair?to refer to an abbreviatedword and its root word (i.e., unabbreviated form)in which the abbreviation is a single Chinesecharacter.On the other hand, abbreviations ofmulti-character compound words may besynthesized from single characters in the?atomic abbreviation pairs?.
If we are able toidentify all such ?atomic abbreviation pairs?,where the abbreviation is a single character, andconstruct such an atomic abbreviation lexicon,then resolving multiple character abbreviationproblems, either by heuristics or by otherabbreviation models, might become easier.Furthermore, many ancient Chinese articlesare composed mostly of single-character words.Depending on the percentage of suchsingle-character words in a modern Chinesearticle, the article will resemble to an ancientChinese article in proportional to such apercentage.
As another application, an effectivesingle character recovery model may thereforebe transferred into an auxiliary translationsystem from ancient Chinese articles into theirmodern versions.
This is, of course, an overlybold claim since lexical translation is not theonly factor for such an application.
However, itmay be consider as a possible direction forlexical translation when constructing anancient-to-modern article translation system.Also, when a model for recovering atomictranslation pair is applied to the?single characterregions?of a word segmented corpus, it is likelyto recover unknown abbreviated words that arepreviously word-segmented incorrectly intoindividual characters.An HMM-based Single Character Recovery(SCR) Model is therefore proposed in this paperto extract a large set of ?atomic abbreviationpairs?from a large text corpus.1.3 Previous WorksCurrently, only a few quantitative approaches(Huang et al, 1994a; Huang et al, 1994b) areavailable in predicting the presence of anabbreviation.
There are essentially no prior artsfor automatically extracting atomic abbreviationpairs.
Since such formulations regard the wordsegmentation process and abbreviation19identification as two independent processes, theyprobably cannot optimize the identificationprocess jointly with the word segmentationprocess, and thus may lose some usefulcontextual information.
Some class-basedsegmentation models (Sun et al, 2002; Gao etal., 2003) well integrate the identification ofsome regular non-lexicalized units (such asnamed entities).
However, the abbreviationprocess can be applied to almost all word forms(or classes of words).
Therefore, this particularword formation process may have to be handledas a separate layer in the segmentation process.To resolve the Chinese abbreviationproblems and integrate its identification into theword segmentation process, (Chang and Lai,2004) proposes to regard the abbreviationproblem in the word segmentation process as an?eror recovery?
problem in which the suspectroot words are the ?erors?
to be recovered froma set of candidates according to some generationprobability criteria.
This idea implies that anHMM-based model for identifying Chineseabbreviations could be effective in eitheridentifying the existence of an abbreviation orthe recovery of the root words from anabbreviation.Since the parameters of an HMM-like modelcan usually be trained in an unsupervisedmanner, and the ?output probabilities?
known tothe HMM framework will indicate the likelihoodfor an abbreviation to be generated from a rootcandidate, such a formulation can easily beadapted to collect highly probableroot-abbreviation pairs.
As a side effect of usingHMM-based formulation, we expect that a largeabbreviation dictionary could be derivedautomatically from a large corpus or from webdocuments through the training process of theunified word segmentation model.In this work, we therefore explore thepossibility of using the theories in (Chang andLai, 2004) as a framework for constructing alarge abbreviation lexicon consisting of allChinese characters and their potential roots.
Inthe following section, the HMM models asoutlined in (Chang and Lai, 2004) is reviewedfirst.
We then described how to use thisframework to construct an abbreviation lexiconautomatically.
In particular, a Single CharacterRecovery (SCR) Model is exploited forextracting possible root (un-abbreviated) formsfor all Chinese characters.2 Chinese Abbreviation Models2.1 Unified Word Segmentation Model forAbbreviation RecoveryTo resolve the abbreviation recovery problem,one can identify some root candidates forsuspect abbreviations (probably from a largeabbreviation dictionary if available or from anordinary dictionary with some educated guesses),and then confirm the most probable root byconsulting local context.
This process isidentical to the operation of many errorcorrection models, which generate the candidatecorrections according to a reversed wordformation process, then justify the bestcandidate.Such an analogy indicates that we may use anHMM model (Rabiner and Juang, 1993), whichis good at finding the best unseen state sequence,for root word recovery.
There will be a directmap between the two paradigms if we regard theobserved input character sequence as our?observation sequence?, and regard the unseenword candidates as the underlying ?statesequence?.To integrate the abbreviation process into theword segmentation model, firstly we can regardthe segmentation model as finding the bestunderlying words mm www ,,11 ??
(whichinclude only base/root forms), given the surfacestring of characters nn ccc ,,11 ??
(which maycontain abbreviated forms of compound words.
)The segmentation process is then equivalent tofinding the best (un-abbreviated) wordsequence *w?
such that:?
??
?
?
??
?
?
?????????????iinmmnmmnmmcwmiiiiicwwmmncwwnmcwwwwPwcPwPwcPcwPw??
?,11:111:11:||maxarg|maxarg|maxarg*111111111Equation 1.
Unified Word SegmentationModel for Abbreviation Recovery20where ic?refers to the surface form of iw ,which could be in an abbreviated ornon-abbreviated root form of iw .
The lastequality assumes that the generation of anabbreviation is independent of context, and thelanguage model is a word-based bigram model.If no abbreviated words appears in real text,such that all surface forms are identical to their?root?
forms, we will have ?
?| 1i iP c w ??
,1,i m??
, and Equation 1 is simply a wordbigram model for word segmentation (Chiang etal., 1992).
In the presence of abbreviations,however, the generation probability ?
?ii wcP |?can no longer be ignored, since the probability?
?ii wcP |?
is not always 1 or 0.As an example, if two consecutive ic?are??
?and ??
?then their roots, iw , could be ???
?plus ????
(Taiwan University) or ???
?plus ?????
(Taiwan Major League).
In thiscase, the parameters in P(??|??)
x P(?|??)
x P(?|??)
and P(???|??)
x P(?|??)
x P(?|???)
wil indicate how likely ????
is an abbreviation, and which of the abovetwo compounds is the root form.Notice that, this equation is equivalent to anHMM (Hidden Markov Model) (Rabiner andJuang, 1993) normally used to find the best?state?
sequence given the observation symbols.The parameters ?
?1| ?ii wwP and ?
?ii wcP |?represent the transition probability and the(word-wise) output probability of an HMM,respectively; and, the formulations for ?
?mwP 1and ?
?mn wcP 11 | are the respective ?languagemodel?
of the Chinese language and the?generation model?
for the abbreviated words(i.e., the ?abbreviation model?in the currentcontext).
The ?state?
sequence in this case ischaracterized by the hidden root formsmm www ,,11 ??
; and, the ?observationsymbols?
are characterized bymnn ccccc ????
,,,, 111 ??
, where the surfaceform ???
?ieibi cc ?
?is a chunk of charactersbeginning at the b(i)-th character and ending atthe e(i)-th character.The word-wise transition probability?
?1| ?ii wwP in the language model is used toprovide contextual constraints among root wordsso that the underlying word sequence forms alegal sentence with high probability.Notice that, after applying the wordsegmentation model Equation 1 to the wordlattice, some of the above candidates may bepreferred and others be discarded, by consultingthe neighboring words and their transitionprobabilities.
This makes the abbreviation modeljointly optimized in the word segmentationprocess, instead of being optimized independentof context.2.2 Simplified Abbreviation ModelsSometimes, it is not desirable to use ageneration probability that is based on theroot-abbreviation pairs, since the number ofparameters will be huge and estimation error dueto data sparseness might be high.
Therefore, it isdesirable to simplify the abbreviation probabilityby using some simpler features in the model.
Forinstance, many 4-character compound words areabbreviated as 2-character abbreviations (such asin the case for the <???
?, ?
?> pair.)
Itwas also known that many such 4-characterwords are abbreviated by preserving the first andthe third characters, which can be represented bya ?1010?
bit patern, where the ?1?
or ?0?meansto preserve or delete the respective character.Therefore, a reasonable simplification for theabbreviation model is to introduce the length andthe positional bit pattern as additional features,resulting in the following augmented model forthe abbreviation probability.?
?
1 11 1| ( , , | , )( | ) ( | ) ( | )m nm nP c w P c bit m r nP c r P bit n P m n??
?
?
?11: surface characters.
: root word characters.where : length of surface characters.
: length of root word characters.bit: bit pattern of abbreviationmncrmn????????
?Equation 2.
Abbreviation Probability usingAbbreviation Pattern and Length Features.All these three terms can be combined freelyto produce as many as 7 sub-models for the21abbreviation model.
Note, the first term?
?nm rc 11 |Pr plays the same role as the oldernotation of ?
?wc |Pr ?
.
To use the simple lengthand position features, this term can be unused inthe above augmented abbreviation model.3 The Single Character Recovery (SCR)ModelAs mentioned earlier, many multiplecharacter words are frequently abbreviated into asingle Chinese character.
Compound wordsconsisting of a couple of such multiple characterwords are then abbreviated by concatenating allthe single character abbreviations.
This meansthat those N-to-1 abbreviation patterns may formthe basis for the underlying Chinese abbreviationprocess.
The other M-to-N abbreviation patternsmight simply be a composition of such basicN-to-1 abbreviations.
The N-to-1 abbreviationpatterns can thus be regarded as the atomicabbreviation pairs.Therefore, it is interesting to apply theabbreviation recovery model to acquire all basicN-to-1 abbreviation patterns, in the first place,so that abbreviations of multi-character wordscan be detected and predicted more easily.Such a task can be highly simplified if eachcharacter in a text corpus is regarded as anabbreviated word whose root form is to berecovered.
In other words, the surface form ic?in Equation 1 is reduced to a single character.The abbreviation recovery model based on thisassumption will be referred to as the SCRModel.The root candidates for each single characterwill form a word lattice, and each path of thelattice will represent a non-abbreviated wordsequence.
The underlying word sequence that ismost likely to produce the input charactersequence will then be identified as the best wordsequence.
Once the best word sequence isidentified, the model parameters can bere-estimated.
And the best word sequence isidentified again.
Such process is repeated untilthe best sequence no more changes.
In addition,the corresponding <root, abbreviation> pairs willbe extracted as atomic abbreviation pairs, whereall the abbreviations are one character in size.While it is overly simplified to use this SCRmodel for conducting a general abbreviationenhanced word segmentation process (since notall single characters are abbreviated words), thesingle character assumption might still be usefulfor extracting roots of real single-characterabbreviations.
The reason is that one only care touse the contextual constraints around a truesingle character abbreviation for matching itsroot form against the local context in order toconfirm that the suspect root did conform to itsneighbors (with a high language model score).An alternative to use a two-stage recoverystrategy, where the first stage applies a baselineword segmentation model to identify mostnormal words and a second stage to identify andrecover incorrectly segmented single characters,is currently under investigation with other modesof operations.
For the present, the SCR model istested first as a baseline.The HMM-based recovery models enables usto estimate the model parameters using anunsupervised training method that is directlyported from the Baum-Welch re-estimationformula (Rabiner and Juang, 1993) or a genericEM algorithm (Dempster et al, 1977).
Uponconvergence, we should be able to acquire alarge corpus of atomic abbreviation pairs fromthe text corpus.If a word-segmented corpus is available, wecan also use such re-estimation methods forHMM parameter training in an unsupervisedmanner, but with initial word transitionprobabilities estimated in a supervised mannerfrom the seed.The initial candidate <root, abbreviation>pairs are generated by assuming that allword-segmented words in the training corpus arepotential roots for each of its single-characterconstituents.
For example, if we have ????and???
?as two word-segmented tokens, thenthe abbreviation pairs <?, ?
?>, <?, ?
?>,<?, ?
?> and <?, ?
?> will be generated.Furthermore, each single character by default isits own abbreviation.To estimate the abbreviation probabilities,each abbreviation pair is associated with afrequency count of the root in the wordsegmentation corpus.
This means that eachsingle-character abbreviation candidate is22equally weighted.
The equal weighting strategymay not be absolutely true (Chang and Lai,2004).
In fact, the character position and wordlength features may be helpful as mentioned inEquation 2.
The initial probabilities aretherefore weighted differently according to theposition of the character and the length of theroot.
The weighting factors are directly acquiredfrom a previous work in (Chang and Lai, 2004).Before the initial probabilities are estimated,Good-Turning smoothing (Katz, 1987) is appliedto the raw frequency counts of the abbreviationpairs.4 ExperimentsTo evaluate the SCR Model, the AcademiaSinica Word Segmentation Corpus (dated July,2001), ASWSC-2001 (CKIP, 2001), is adoptedfor parameter estimation and performanceevaluation.
Among the 94 files in this balancedcorpus, 83 of them (13,086KB) are randomlyselected as the training set and 11 of them(162KB) are used as the test set.Table 3 shows some examples of atomicabbreviation pairs acquired from the trainingcorpus.
The examples here partially justify thepossibility to use the SCR model for acquiringatomic abbreviation pairs from a large corpus.The iterative training process convergesquickly after 3~4 iterations.
The numbers ofunique abbreviation patterns for the training andtest sets are 20,250 and 3,513, respectively.Since the numbers of patterns are large, a roughestimate on the acquisition accuracy rates isconducted by randomly sampling 100 samples ofthe <root, abbreviation> pairs.
The pattern isthen manually examined to see if the root iscorrectly recovered.
The precision is estimatedas 50% accuracy for the test set, and 62% for thetraining set on convergence.
Although a largersample may be necessary to get a better estimate,the preliminary result is encouraging.
Figures1~4 demonstrate the curve of convergence forthe iterative training process in terms of patternnumber and accuracy.SCR Model Training Set ??pattern?
?2030720252 20250 202502022020240202602028020300203201 2 3 4???
?Figure 1.
Numbers of abbreviation patterns ineach iteration.
(Training Set)SCR Model Training Set??
?29%45%62% 62%0%20%40%60%80%1 2 3 4???
?Figure 2.
Acquisition accuracy for each iteration.
(Training Set)SCR Model Test Set ??pattern?
?3521351835153513351235133514351535163517351835193520352135220 0.5 1 1.5 2 2.5 3 3.5 4 4.5???
?Figure 3.
Numbers of abbreviation patterns ineach iteration.
(Test Set)Abbr : Root Example Abbr : Root Example?:??
??
?:??
???:??
??
?:??
???:??
??
?:??
???:??
??
?:??
???:??
??
?:??
????:??
??
?:???
???:??
??
?:??
???:??
??
?:??
????:??
??
?:??
???:??
??
?:??
????:??
??
?:??
????:??
??
?:??
???:??
???
?:???
???:??
??
?:??
???:??
??
?:??
?
?Table 3.
Examples of atomic abbreviation pairs.23SCR Model Test Set ??
?20% 22%33%50%0%10%20%30%40%50%60%1 2 3 4???
?Figure 4.
Acquisition accuracy for each iteration.
(Test Set)5 Concluding RemarksChinese abbreviations, which form a specialkind of unknown words, are widely seen in themodern Chinese texts.
This results in difficultyfor correct Chinese processing.
In this work, wehad applied a unified word segmentation modeldeveloped in a previous works (Chang and Lai,2004), which was able to handle the kind of?erors?
introduced by the abbreviation process.An iterative training process is developed toautomatically acquire an abbreviation dictionaryfor single-character abbreviations from largecorpora.
In particular, a Single CharacterRecovery (SCR) Model is exploited.
With only afew training iterations, the acquisition accuracyachieves 62% and 50 % precision for training setand test set from the ASWSC-2001 corpus.
Forsystems that choose to lexicalize such lexiconentities, the automatically constructedabbreviation dictionary will be an invaluableresource to the systems.
And, the proposedrecovery model looks encouraging.ReferencesChang, Jing-Shin and Keh-Yih Su, 1997.
?AnUnsupervised Iterative Method for Chinese NewLexicon Extraction?, International Journal ofComputational Linguistics and Chinese LanguageProcessing (CLCLP), 2(2): 97-148.Chang, Jing-Shin and Yu-Tso Lai, 2004.
?APreliminary Study on Probabilistic Models forChinese Abbreviations.?
Proceedings of the ThirdSIGHAN Workshop on Chinese LanguageLearning, pages 9-16, ACL-2004, Barcelona,Spain.Chiang, Tung-Hui, Jing-Shin Chang, Ming-Yu Linand Keh-Yih Su, 1992.
?Statistical Models forWord Segmentation and Unknown WordResolution,?Proceedings of ROCLING-V, pages123-146, Taipei, Taiwan, ROC.CKIP 2001, Academia Sinica Word SegmentationCorpus, ASWSC-2001, (??????????
),Chinese Knowledge Information Processing Group,Acdemia Sinica, Tiapei, Taiwan, ROC.Dempster, A. P., N. M. Laird, and D. B. Rubin, 1977.?Maximum Likelihood from Incomplete Data viathe EM Algorithm?, Journal of the RoyalStatistical Society, 39 (b): 1-38.Gao, Jianfeng, Mu Li, Chang-Ning Huang, 2003.?Improved Source-Channel Models for ChineseWord Segmentation,?
Proc.
ACL 2003, pages272-279.Huang, Chu-Ren, Kathleen Ahrens, and Keh-JiannChen, 1994a.
?A data-driven approach topsychological reality of the mental lexicon: Twostudies on Chinese corpus linguistics.?
InLanguage and its Psychobiological Bases, Taipei.Huang, Chu-Ren, Wei-Mei Hong, and Keh-JiannChen, 1994b.
?Suoxie: An information basedlexical rule of abbreviation.?
In Proceedings of theSecond Pacific Asia Conference on Formal andComputational Linguistics II, pages 49?52, Japan.Katz, Slava M., 1987.
?Estimation of Probabilitiesfrom Sparse Data for the Language ModelComponent of a Speech Recognizer,?IEEE Trans.ASSP-35 (3).Lai, Yu-Tso, 2003.
A Probabilistic Model forChinese Abbreviations, Master Thesis, NationalChi-Nan University, ROC.Lin, Ming-Yu, Tung-Hui Chiang and Keh-Yih Su,1993.
?A Preliminary Study on Unknown WordProblem in Chinese Word Segmentation,?Proceedings of ROCLING VI, pages 119-142.Rabiner, L., and B.-H., Juang, 1993.
Fundamentals ofSpeech Recognition, Prentice-Hall.Sun, Jian, Jianfeng Gao, Lei Zhang, Ming Zhou andChang-Ning Huang, 2002.
?Chinese named entityidentification using class-based language model,?Proc.
of COLING 2002, Taipei, ROC.Sproat, Richard, 2002.
?Corpus-Based Methods inChinese Morphology?, Pre-conference Tutorials,COLING-2002, Taipei, Taiwan, ROC.24
