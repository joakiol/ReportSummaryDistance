Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 189?196,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsExploiting Machine-Transcribed Dialog Corpus to Improve Multiple DialogStates Tracking MethodsSungjin Lee1,2 and Maxine Eskenazi11Language Technologies Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania2Computer Science and Engineering, Pohang University of Science and Technology, South Korea{sungjin.lee, max}@cs.cmu.edu1, junion@postech.ac.kr2AbstractThis paper proposes the use of unsuper-vised approaches to improve components ofpartition-based belief tracking systems.
Theproposed method adopts a dynamic Bayesiannetwork to learn the user action model directlyfrom a machine-transcribed dialog corpus.
Italso addresses confidence score calibration toimprove the observation model in a unsuper-vised manner using dialog-level grounding in-formation.
To verify the effectiveness of theproposed method, we applied it to the Let?s Godomain (Raux et al, 2005).
Overall systemperformance for several comparative modelswere measured.
The results show that the pro-posed method can learn an effective user ac-tion model without human intervention.
Inaddition, the calibrated confidence score wasverified by demonstrating the positive influ-ence on the user action model learning processand on overall system performance.1 IntroductionWith present Automatic Speech Recognition (ASR)and Spoken Language Understanding (SLU) errors,it is impossible to directly observe the true user goaland action.
It is crucial, therefore, to efficiently inferthis true state from erroneous observations over mul-tiple dialog turns.
The Partially Observable MarkovDecision Process (POMDP) framework has offereda well-founded theory for this purpose (Hendersonet al, 2008; Thomson and Young, 2010a; Williamsand Young, 2007; Young et al, 2010).
Severalapproximate methods have also emerged to tacklethe vast complexity of representing and maintainingbelief states, e.g., partition-based approaches (Ga-sic and Young, 2011; Williams, 2010; Young etal., 2010) and Bayesian network (BN)-based meth-ods (Raux and Ma, 2011; Thomson and Young,2010a).
The partition-based approaches attempt togroup user goals into a small number of partitionsand split a partition only when a distinction is re-quired by observations.
This property endows itwith the high scalability that is suitable for fairlycomplex domains.
However, the parameter learn-ing procedures for the partition-based methods isstill limited to hand-crafting or the use of a sim-ple maximum likelihood estimation (Keizer et al,2008; Roy et al, 2000; Thomson and Young, 2010a;Williams, 2008).
In contrast, several unsupervisedmethods which do not require human transcriptionand annotation have been recently proposed to learnBN-based models (Jurcicek et al, 2010; Syed andWilliams, 2008; Thomson et al, 2010b).
In this pa-per we describe an unsupervised process that can beapplied to the partition-based methods.
We adopt adynamic Bayesian network to learn the user actionmodel which defines the likelihood of user actionsfor a given context.
In addition, we propose a simpleconfidence score calibration method to improve theobservation model which represents the probabilityof an observation given the true user action.This paper is structured as follows.
Section 2 de-scribes previous research and the novelty of our ap-proach.
Section 3 and Section 4 elaborate on ourproposed unsupervised approach.
Section 5 explainsthe experimental setup.
Section 6 presents and dis-cusses the results.
Finally, Section 7 concludes witha brief summary and suggestions for future research.1892 Background and Related WorkIn order to reduce the complexity of the belief statesover the POMDP states, the following factorizationof the belief state has been commonly applied to thebelief update procedure (Williams et al, 2005):b(gt,ut,ht)?
p(ot|ut)?
??
?observation model?ht?1p(ht|ht?1,ut, st)?
??
?dialog history modelp(ut|gt, st,ht?1)?
??
?user action model?gt?1p(gt|gt?1, st?1)?
??
?user goal model?ut?1b(gt?1,ut?1,ht?1)(1)where gt, st,ut,ht,ot represents the user goal, thesystem action, the user action, the dialog history,and the observed user action for each time slice, re-spectively.
The user goal model describes how theuser goal evolves.
In the partition-based approaches,this model is further approximated by assuming thatthe user does not change their mind during the dia-log (Young et al, 2010):?gt?1p(gt|gt?1, st?1) = p(pt|pt?1) (2)where pt is a partition from the current turn.
The di-alog history model indicates how the dialog historychanges and can be set deterministically by simplediscourse rules, for example:p(ht = Informed|ht?1,ut, st) ={1 if ht?1 = Informed or ut = Inform(?
),0 otherwise.
(3)The user action model defines how likely user ac-tions are.
By employing partitions, this can be ap-proximated by the bigram model of system and useraction at the predicate level, and the matching func-tion (Keizer et al, 2008):p(ut|gt, st,ht?1)?
p(T (ut)|T (st)) ?
M(ut,pt, st)(4)where T (?)
denotes the predicate of the actionand M(?)
indicates whether or not the user actionmatches the partition and system action.
However,it turned out that the bigram user action model didnot provide an additional gain over the improve-ment achieved by the matching function accordingto (Keizer et al, 2008).
This might indicate thatit is necessary to incorporate more historical infor-mation.
To make use of historical information inan unsupervised manner, the Expectation Maximiza-tion algorithm was adopted to obtain maximum like-lihood estimates (Syed and Williams, 2008).
Butthese methods still require a small amount of tran-scribed data to learn the observation confusability,and they suffer from overfitting as a general prop-erty of maximum likelihood.
To address this prob-lem, we propose a Bayesian learning method, whichrequires no transcribed data.The observation model represents the probabilityof an observation given the true user action.
Theobservation model is usually approximated with theconfidence score computed from the ASR and SLUresults:p(ot|ut) ?
p(ut|ot) (5)It is therefore of vital importance that we obtain themost accurate confidence score as possible.
We pro-pose an efficient method that can improve the confi-dence score by calibrating it using grounding infor-mation.3 User Action ModelTo learn the user action model, a dynamic Bayesiannetwork is adopted with several conditional inde-pendence assumptions similar to Equation 1.
Thisgives rise to the graphical structure shown in Fig-ure 1.
As mentioned in Section 2, the user ac-tion model deals with actions at the predicate level1.This abstract-level handling enables the user actionmodel to employ exact inference algorithms such asthe junction tree algorithm (Lauritzen and Spiegel-halter, 1988) for more efficient reasoning over thegraphical structure.1To keep the notation uncluttered, we will omit T (?
).190Figure 1: The graphical structure of the dynamicBayesian network for the user action model.
The shadeditems are observable and the transparent ones are latent.The joint distribution for this model is given byp(S,H,U,O|?
)= p(h0|pi)?tp(ut|st,ht?1,?)?
p(ht|ht?1,ut,?
)p(ot|ut, ?
)(6)where a capital letter stands for the set ofcorresponding random variables, e.g., U ={u1, .
.
.
,uN}, and ?
= {pi,?,?, ?}
denotes theset of parameters governing the model2.Unlike previous research which learns ?
usingmaximum likelihood estimation, we use a determin-istic function that yields a fraction of an observedconfidence score in accordance with the degree ofagreement between ut and ot:p(ot|ut) = CS(ot) ?
( |ot ?
ut||ot ?
ut|)+  (7)where CS(?)
returns the confidence score of the as-sociated observation.
As mentioned above, pi and?
are deterministically set by simple discourse rules(Equation 3).
This only leaves the user action model?
to be learned.
In a Bayesian model, any unknownparameter is given a prior distribution and is ab-sorbed into the set of latent variables, thus it is notfeasible to directly evaluate the posterior distributionof the latent variables and the expectations with re-spect to this distribution.
Therefore a determinis-tic approximation, called mean field theory (Parisi,1988), is applied.In mean field theory, the family of posterior distri-butions of the latent variables is assumed to be par-titioned into disjoint groups:q(Z) =M?i=1qi(Zi) (8)2Here, a uniform prior distribution is assigned on Swhere Z = {z1, .
.
.
, zN} denotes all latent variablesincluding parameters and Zi is a disjoint group.Amongst all distributions q(Z) having the form ofEquation 8, we then seek the member of this familyfor which the divergence from the true posterior dis-tribution is minimized.
To achieve this, the follow-ing optimization with respect to each of the qi(Zi)factors is to be performed in turn (Bishop, 2006):ln q?j (Zj) = Ei 6=j[ln(X,Z)]+ const (9)where X = {x1, .
.
.
,xN} denotes all observed vari-ables and Ei 6=j means an expectation with respect tothe q distributions over all groups Zi for i 6= j.Now we apply the mean field theory to the usermodel.
Before doing so, we need to introduce theprior over the parameter ?
which is a product ofDirichlet distributions3.p(?)
=?kDir(?k|?0k)=?kC(?0k)?l?
?0k?1k,l(10)where k represents the joint configuration of all ofthe parents and C(?0k) is the normalization constantfor the Dirichlet distribution.
Note that for symme-try we have chosen the same parameter ?0k for eachof the components.Next we approximate the posterior distribution,q(H,U,?)
using a factorized form, q(H,U)q(?
).Then we first apply Equation 9 to find an expressionfor the optimal factor q?(?
):3Note that priors over parameters for deterministic distribu-tions (e.i., pi,?,and ?)
are not necessary.191ln q?(?)
= EH,U[ln p(S,H,U,O,?
)]+ const= EH,U[?tln p(ut|st,ht?1,?
)]+ ln p(?)
+ const=?t?i,j,k(EH,U[?i,j,k]ln?i,j,k)+?i,j,k(?oi,j,k ?
1) ln?i,j,k + const=?i,j,k((EH,U[ni,j,k] + (?oi,j,k ?
1))?
ln?i,j,k)+ const(11)where ?
(?, ?)
denotes Kronecker delta and ?i,j,k de-notes ?
(st, i)?
(ht?1, j) ?
(ut, k).
ni,j,k is the num-ber of times where , st = i,ht?1 = j, and ut = k.This leads to a product of Dirichlet distributions bytaking the exponential of both sides of the equation:q?(?)
=?i,jDir(?i,j |?i,j),?i,j,k = ?0i,j,k + EH,U[ni,j,k](12)To evaluate the quantity EH,U[ni,j,k], Equation 9needs to be applied once again to obtain an op-timal approximation of the posterior distributionq?
(H,U).ln q?
(H,U) = E?
[ln p(S,H,U,O,?
)]+ const= E?
[?tln p(ut|st,ht?1,?
)+ ln p(ht|ht?1,ut)+ ln p(ot|ut)]+ const=?t(E?
[ln p(ut|st,ht?1,?
)]+ ln p(ht|ht?1,ut)+ ln p(ot|ut))+ const(13)where E?
[ln p(ut|st,ht?1,?)]
can be obtained us-ing Equation 12 and properties of the Dirichlet dis-tribution:E?
[ln p(ut|st,ht?1,?)]=?i,j,k?i,j,kE?[ln?i,j,k]=?i,j,k?i,j,k(?(?i,j,k)?
?(?
?i,j))(14)where ?(?)
is the digamma function with ?
?i,j =?k ?i,j,k.
Because computing EH,U[ni,j,k] isequivalent to summing each of the marginal poste-rior probabilities q?
(ht?1,ut) with the same con-figuration of conditioning variables, this can bedone efficiently by using the junction tree algorithm.Note that the expression on the right-hand side forboth q?(?)
and q?
(H,U) depends on expectationscomputed with respect to the other factors.
Wewill therefore seek a consistent solution by cyclingthrough the factors and replacing each in turn with arevised estimate.4 Confidence Score CalibrationAs shown in Section 2, we can obtain a better obser-vation model by improving confidence score accu-racy.
Since the confidence score is usually computedusing the ASR and SLU results, it can be enhancedby adding dialog-level information.
Basically, theconfidence score represents how likely it is that therecognized input is correct.
This means that a well-calibrated confidence score should satisfy that prop-erty such that:p(ut = a|ot = a) '?k ?
(uk, a)?
(ok, a)?k ?
(ok, a)(15)However, the empirical distribution on the right sideof this equation often does not well match the con-fidence score measure on the left side.
If a largecorpus with highly accurate annotation was used, astraightforward remedy for this problem would be toconstruct a mapping function from the given confi-dence score measure to the empirical distribution.This leads us to propose an unsupervised methodthat estimates the empirical distribution and con-structs the mapping function which is fast enoughto run in real time.
Note that we will not construct192Figure 2: Illustrations of confidence score calibration for the representative concepts in the Let?s Go domaina mapping function for each instance, but ratherfor each concept, since the former could cause se-vere data sparseness.
In order to estimate the em-pirical distribution in an unsupervised manner, weexploit grounding information4 as true labels.
Wefirst parse dialog logs to look for the grounding in-formation that the users have provided.
Each timewe encounter grounding information that includesthe constraints used in the backend queries, this isadded to the list.
If two actions contradict each other,the later action overwrites the earlier one.
Then,for each observation in the data, we determine itscorrectness by comparing it with the grounding in-formation.
Next, we gather two sets of confidencescores with respect to correctness, on which we ap-ply a Gaussian kernel-based density estimation.
Af-4Specifically, we used explicitly confirmed information bythe system for this studyter that, we scale the two estimated densities by theirtotal number of elements to see how the ratio of cor-rect ones over the sum of correct and incorrect onesvaries according to the confidence score.
The ratiocomputed above will be the calibrated score:c?
= dc(c)dc(c) + dinc(c)(16)where c?
indicates the calibrated confidence scoreand c is the input confidence score.
dc(?)
denotesthe scaled density for the correct set and dinc(?)
isthe scaled density for the incorrect set.Note that this approach tends to yield a moreconservative confidence score since correct user ac-tions can exist, even though they may not matchthe grounding information.
Finally, in order to effi-ciently obtain the calibrated score for a given confi-dence score, we employ the sparse Bayesian regres-sion (Tipping, 2001) with the Gaussian kernel.
By193virtue of the sparse representation, we only need toconsider a few so-called relevance vectors to com-pute the score:y(x) =?xn?RVwnk(x,xn) + b (17)where RV denotes the set of relevance vectors,|RV |  |{xn}|.
k(?, ?)
represents a kernel functionand b is a bias parameter.
Figure 2 shows the afore-mentioned process for several representative con-cepts in the Let?s Go domain.5 Experimental SetupTo verify the proposed method, three months of datafrom the Let?s Go domain were used to train theuser action model and the observation model.
Thetraining data consists of 2,718 dialogs and 23,044turns in total.
To evaluate the user action model,we compared overall system performance with threedifferent configurations: 1) the uniform distribution,2) the user action model without historical infor-mation5 which is comparable to the bigram modelof (Keizer et al, 2008), 3) the user action model withhistorical information included.
For system perfor-mance evaluation, we used a user simulator (Lee andEskenazi, 2012) which provides a large number ofdialogs with statistically similar conditions.
Also,the simulated user enables us to examine how per-formance changes over a variety of error levels.
Thissimulated user supports four error levels and eachmodel was evaluated by generating 2,000 dialogs ateach error level.
System performance was measuredin terms of average dialog success rate.
A dialog isconsidered to be successful if the system providesthe bus schedule information that satisfies the usergoal.To measure the effectiveness of the calibrationmethod, we conducted two experiments.
First, weapplied the calibration method to parameter learn-ing for the user action model by using the calibratedconfidence score in Equation 7.
We compared thelog-likelihood of two models, one with calibrationand the other without calibration.
Second, we com-pared overall system performance with four differ-ent settings: 1) the user action model with histori-5This model was constructed by marginalizing out the his-torical variables.cal information and the observation model with cal-ibration, 2) the user action model with historical in-formation and the observation model without cali-bration, 3) the user action model without historicalinformation and the observation model with calibra-tion, 4) the user action model without historical in-formation and the observation model without cali-bration.6 ResultsThe effect of parameter learning of the user actionmodel on average dialog success rate is shown inFigure 3.
While, in the previous study, the bigrammodel unexpectedly did not show a significant ef-fect, our result here indicates that our comparablemodel, i.e.
the model with historical information ex-cluded, significantly outperformed the baseline uni-form model.
The difference could be attributed tothe fact that the previous study did not take tran-scription errors into consideration, whereas our ap-proach handles the problem by treating the true useraction as hidden.
However, we cannot directly com-pare this result with the previous study since the tar-get domains are different.
The model with historicalinformation included also consistently surpassed theuniform model.
Interestingly, there is a noticeabletrend: the model without historical information per-forms better as the error level increases.
This resultmay indicate that the simpler model is more robustFigure 3: The effect of parameter learning of each useraction model on overall system performance.
The errorbar represents standard error.194Figure 4: The effect of confidence score calibration onthe log-likelihood of the user action model during thetraining process.Figure 5: The effect of confidence score calibration forthe observation model on overall system performance.The error bar shows standard error.to error.
Although average dialog success rates be-came almost zero at error level four, this result is anatural consequence of the fact that the majority ofthe dialogs in this corpus are failed dialogs.Figure 4 shows the effect of confidence scorecalibration on the log-likelihood of the user actionmodel during the training process.
To take into ac-count the fact that different confidence scores resultin different log-likelihoods regardless of the qual-ity of the confidence score, we shifted both log-likelihoods to zero at the beginning.
This modifica-tion more clearly shows how the quality of the confi-dence score influences the log-likelihood maximiza-tion process.
The result shows that the calibratedconfidence score gives greater log-likelihood gains,which implies that the user action model can betterdescribe the distribution of the data.The effect of confidence score calibration for theobservation model on average dialog success rate ispresented in Figure 5.
For both the user action modelwith historical information included and excluded,the application of the confidence score calibrationconsistently improved overall system performance.This result implies the possibility of automaticallyimproving confidence scores in a modularized man-ner without introducing a dependence on the under-lying methods of ASR and SLU.7 ConclusionIn this paper, we have presented novel unsupervisedapproaches for learning the user action model andimproving the observation model that constitute thepartition-based belief tracking method.
Our pro-posed method can learn a user action model directlyfrom a machine-transcribed spoken dialog corpus.The enhanced system performance shows the effec-tiveness of the learned model in spite of the lack ofhuman intervention.
Also, we have addressed con-fidence score calibration in a unsupervised fashionusing dialog-level grounding information.
The pro-posed method was verified by showing the positiveinfluence on the user action model learning processand the overall system performance evaluation.
Thismethod may take us a step closer to being able toautomatically update our models while the system islive.
Although the proposed method does not dealwith N-best ASR results, the extension to supportN-best results will be one of our future directions,as soon as the Let?s Go system uses N-best ASR re-sults.AcknowledgmentsThis work was supported by the second Brain Korea21 project.ReferencesC.
Bishop, 2006.
Pattern Recognition and MachineLearning.
Springer.195M.
Gasic and S. Young, 2011.
Effective handlingof dialogue state in the hidden information statePOMDP-based dialogue manager.
ACM Transactionson Speech and Language Processing, 7(3).J.
Henderson, O.
Lemon, K. Georgila, 2008.
Hybrid Re-inforcement / Supervised Learning of Dialogue Poli-cies from Fixed Datasets.
Computational Linguistics,34(4):487-511.F.
Jurcicek, B. Thomson and S. Young, 2011.
Natu-ral Actor and Belief Critic: Reinforcement algorithmfor learning parameters of dialogue systems modelledas POMDPs.
ACM Transactions on Speech and Lan-guage Processing, 7(3).S.
Keizer, M. Gasic, F. Mairesse, B. Thomson, K. Yu, S.Young, 2008.
Modelling User Behaviour in the HIS-POMDP Dialogue Manager.
In Proceedings of SLT.S.
Lauritzen and D. J. Spiegelhalter, 1988.
Local Com-putation and Probabilities on Graphical Structures andtheir Applications to Expert Systems.
Journal ofRoyal Statistical Society, 50(2):157?224.S.
Lee and M. Eskenazi, 2012.
An Unsuper-vised Approach to User Simulation: toward Self-Improving Dialog Systems.
In Proceedings of SIG-DIAL.
http://infinitive.lti.cs.cmu.edu:9090.G.
Parisi, 1988.
Statistical Field Theory.
Addison-Wesley.A.
Raux, B. Langner, D. Bohus, A. W Black, and M.Eskenazi, 2005.
Let?s Go Public!
Taking a SpokenDialog System to the Real World.
In Proceedings ofInterspeech.A.
Raux and Y. Ma, 2011.
Efficient Probabilistic Track-ing of User Goal and Dialog History for Spoken Dia-log Systems.
In Proceedings of Interspeech.N.
Roy, J. Pineau, and S. Thrun, 2000.
Spoken dia-logue management using probabilistic reasoning.
InProceedings of ACL.U.
Syed and J. Williams, 2008.
Using automaticallytranscribed dialogs to learn user models in a spokendialog system.
In Proceedings of ACL.B.
Thomson and S. Young, 2010.
Bayesian updateof dialogue state: A POMDP framework for spokendialogue systems.
Computer Speech & Language,24(4):562-588.B.
Thomson, F. Jurccek, M. Gasic, S. Keizer, F. Mairesse,K.
Yu, S. Young, 2010.
Parameter learning forPOMDP spoken dialogue models.
In Proceedings ofSLT.M.
Tipping, 2001.
Sparse Bayesian Learning andthe Relevance Vector Machine.
Journal of MachineLearning Research, 1:211?244.J.
Williams, P. Poupart, and S. Young, 2005.
FactoredPartially Observable Markov Decision Processes forDialogue Management.
In Proceedings of Knowledgeand Reasoning in Practical Dialogue Systems.J.
Williams and S. Young, 2007.
Partially observableMarkov decision processes for spoken dialog systems.Computer Speech & Language, 21(2):393-422.J.
Williams, 2008.
Exploiting the ASR N-best by track-ing multiple dialog state hypotheses.
In Proceedingsof Interspeech.J.
Williams, 2010.
Incremental partition recombinationfor efficient tracking of multiple dialog states.
In Pro-ceedings of ICASSP.S.
Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-mann, B. Thomson and K. Yu, 2010.
The HiddenInformation State Model: a practical framework forPOMDP-based spoken dialogue management.
Com-puter Speech and Language, 24(2):150?174.196
