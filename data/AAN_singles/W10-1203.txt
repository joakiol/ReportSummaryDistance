Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 19?26,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsQuery-based Text Normalization Selection Models for Enhanced RetrievalAccuracySi-Chi Chin Rhonda DeCook W. Nick Street David EichmannThe University of IowaIowa City, USA.
{si-chi-chin, rhonda-decook, nick-street, david-eichmann}@uiowa.eduAbstractText normalization transforms words into abase form so that terms from common equiv-alent classes match.
Traditionally, informa-tion retrieval systems employ stemming tech-niques to remove derivational affixes.
Deplu-ralization, the transformation of plurals intosingular forms, is also used as a low-level textnormalization technique to preserve more pre-cise lexical semantics of text.Experiment results suggest that the choice oftext normalization technique should be madeindividually on each topic to enhance informa-tion retrieval accuracy.
This paper proposes ahybrid approach, constructing a query-basedselection model to select the appropriate textnormalization technique (stemming, deplural-ization, or not doing any text normalization).The selection model utilized ambiguity prop-erties extracted from queries to train a com-posite of Support Vector Regression (SVR)models to predict a text normalization tech-nique that yields the highest Mean AveragePrecision (MAP).
Based on our study, sucha selection model holds promise in improvingretrieval accuracy.1 IntroductionStemming removes derivational affixes of termstherefore allowing terms from common equivalenceclasses to be clustered.
However, stemming also in-troduces noise by mapping words of different con-cepts or meanings into one base form, thus impedingword-sense disambiguation.
Depluralization, theconversion of plural word forms to singular form,preserves more precise semantics of text than stem-ming (Krovetz, 2000).Empirical research has demonstrated the ambiva-lent effect of stemming on text retrieval perfor-mance.
Hull (1996) conducted a comprehensivecase study on the effects of four stemmer tech-niques and the removal of plural ?s?
1 on retrievalperformance.
Hull suggested that the adoption ofstemming is beneficial but plural removal is as wellcompetitive when the size of documents is small.Prior research (Manning and Schtze, 1999; Mc-Namee et al, 2008) indicated that traditional stem-ming, though still benefiting some queries, wouldnot necessarily enhance the average retrieval perfor-mance.
In addition, stemming was considered one ofthe technique failures undermining retrieval perfor-mance in the TREC 2004 Robust Track (Voorhees,2006).
Prior research also noted the semantic differ-ences between plurals and singulars.
Riloff (1995)indicated that plural and singular nouns are distinctbecause plural nouns usually pertain to the ?generaltypes of incidents,?
while singular nouns often per-tain to ?a specific incident.
?Nevertheless, prior research has not closely ex-amined the effect of the change of the semanticscaused by different level of text normalization tech-niques.
In our work, we conducted extensive exper-iments on the TREC 2004 Robust track collection toevaluate the effect of stemming and depluralizationon information retrieval.
In addition, we quantifythe ambiguity of a query, extracting five ambiguityproperties from queries.
The extracted ambiguityproperties are used to construct query-based selec-tion model, a composite of multiple Support Vector1In our work, we not only removed the plural ?s?
or ?es?but also changed irregular plural forms such as ?children?
to itssingular form ?child?.19Regression models, to determine the most appropri-ate text normalization technique for a given query.To our knowledge, our work is the first study to con-struct a query-based selection model, using ambigu-ity properties extracted from provided queries to se-lect an optimal text normalization technique for eachquery.The remainder of this paper is organized as fol-lows.
In section 2 we describe our experimentalsetups and dataset.
Section 3 describes and ana-lyzes experiment results of different text normaliza-tion techniques on the dataset.
We discuss five am-biguity properties and validate each property in sec-tion 4.
In section 5 we describe the framework andthe prediction results of the proposed query-basedselection model.
Finally, we summarize our findingsand discuss future work in section 6.2 Experiment SetupThe experiment utilizes the queries and relevancejudgment results from the TREC 2004 RobustTrack to evaluate the effect of three text normal-ization techniques ?
raw text, depluralized text, andstemmed text.
The TREC 2004 Robust Track used adocument set of approximately 528,000 documentscomprising 1,904 MB of text.
In total, 249 querytopics were used in TREC Robust 2004.Figure 1 illustrates the setup of the experiment.The collection is parsed with a SAX parser andstored in a Postgres database.
Lucene is then used togenerate three indices: indices of raw text, deplural-ized text, and stemmed text.
The Postgres databasestores each document of the collection, the querytopics of the TREC 2004 Robust Track, and resultsof experiments.
The ambiguity properties for eachquery is also computed in the Postgres system.
Wequery Lucene indices to obtain the top 1,000 relevantresults and compute Mean Average Precision (MAP)with the trec eval program to evaluate performance.We use R to analyze performance scores, generatedescriptive charts, conduct non-parametric statisti-cal tests, and perform a paired t-test.
We use Weka(Hall et al, 2009) to construct query-based selectionmodel that incorporates multiple Support Vector Re-gression (SVR) models.2.1 Query ModelsThe TREC 2004 Robust Track provides 249 querytopics; each includes a title, a short description,and a narrative (usually one-paragraph).
We se-lected three basic query models as a modest base-line to demonstrate the effect of different text nor-malization techniques.
Our future work will exploitother ranking models such as BM25 and LMIR.
Thethree query models used in the experiment are: (1)boolean search with the title words of topics con-catenated with logical AND (e.g.
hydrogen ANDfuel AND automobiles); (2) boolean search with thetitle words of topics concatenated with logical OR(e.g.
hydrogen OR fuel OR automobiles); (3) co-sine similarity with the title words of topics.
LuceneMoreLikeThis (MLT) class supports both booleanand cosine similarity query methods for the exper-iment.
Figure 2 shows how query topics are pro-cessed before interrogating the indices.
Originalqueries are first depluralized or stemmed, furtherprocessed according to each query model, and fi-nally run against the depluralized and stemmed in-dices.
The experiment runs unprocessed raw queriesagainst the index of raw text, depluralized queriesagainst the index of depluralized text, and stemmedqueries against the index of stemmed text.3 Experiment ResultsTable 3 and Figure 3 summarize the results for thefull set of topics.
Each row in Table 3 represents aquery model combined with a given text normaliza-tion technique as described in section 2.1.For each query model and text normalizationtechnique, we present the MAP value computedacross all relevant topics.
We also provide the p-value for comparing MAP between each normaliza-tion technique and the baseline (i.e.
non-normalized(raw) queries).
The p-value is generated from thepairwise Wilcoxon signed rank sum test.
Figure 3describes the distribution of MAP across the threetext normalization techniques and three query mod-els.
The distributions are skewed and many out-liers are observed.
In general, boolean OR and MLTquery models perform similarly and stemming hasthe highest median MAP value across all three querymodels.
The results from Table 3 for the combinedtopic set show that depluralization and stemmingperform significantly better than the raw baseline.However, the performance difference between de-pluralization and stemming is not significant exceptfor the AND boolean query model.
In general, thedifferences of MAP among three text normalization20XMLParserPostgresDBMSLuceneRLexiconAnalysisLexiconAnalysisLuceneIndex FilesIndex FilesIndex FilesXMLDocumentsTREC Robust Track 2004AmbiguityCalculatorTrec EvalDocumentProcessDBMSLegendDataQueryTopicsSearchResultsPerformanceScoreGraphsGraphsGraphsWilcoxon TestsPaired T-testWekaSVMRegressionAmbiguityMeasuresTextFigure 1: Flow chart of experiment setupFinal QueryText NormalizationQuery TopicLexicon AnalysisQuery ModelLuceneMoreLikeThisOR booleanANDbooleanhydrogenfuelautomobilehydrogenfuelautomobilhydrogen fuelautomobilesStemmerDepluralizerDepluralizedIndexStemmedIndexhydrogenANDfuelANDautomobileIndexhydrogenORfuelORautomobilehydrogenautomobile^0.939fuel^0.631Figure 2: Using the query ?hydrogen fuel automobiles?as an example, the depluralized query becomes ?hydro-gen fuel automobile?
and the stemmed query becomes?hydrogen fuel automobil.?
Final boolean queries for de-pluralized topic become ?hydrogen AND fuel AND au-tomobile?
and ?hydrogen OR fuel OR automobil.?
More-LikeThis (MLT) is the Lucene class used for cosine sim-ilarity retrieval.
A term vector score appends each wordin the topic.techniques are within 2%.To visualize the relative performances amongthree text normalization techniques, we standardizedthe three MAP values for a single topic (one fromeach text normalization technique) to have mean 0and standard deviation of 1.
The result provides a 3-value pattern emphasizing the ordering of the MAPsacross the text normalization techniques, rather thanthe raw MAP values themselves.
We then usedthe K-medoids algorithm (Kaufman and Rousseeuw,1990) to cluster the transformed data, applying Eu-clidean distance as the distance measure.
Figure 4is an example of 9 constructed clusters based on theMAP scores of the MLT query model.
In a clus-ter, a line represents the standardized MAP valueof a topic on each text normalization technique.Given the small differences in aggregate MAP per-formance, it is interesting to note that the clustersdemonstrate variable patterns, indicating that sometopics performed better as a depluralized query thana stemmed query.The cluster analysis suggests that the choice oftext normalization technique should be made indi-vidually on each topic.
As we choose an appropri-ate text normalization technique for a given topic,we would further enhance retrieval performance.
In21Query ModelAverage Precisionand or mlt0.00.20.40.60.81.01.2Mean Average Precision vs. Query Model0.00.20.40.60.81.01.20.00.20.40.60.81.01.20.00.20.40.60.81.01.20.00.20.40.60.81.01.20.00.20.40.60.81.01.20.00.20.40.60.81.01.20.00.20.40.60.81.01.20.00.20.40.60.81.01.20.00.20.40.60.81.01.2rawdepluralstemFigure 3: Profile plot of MAPthe next section, we address the issue of inconsis-tent performance by constructing regression modelsto predict the mean average precision of each queryfrom the ambiguity measures, and choose an appro-priate normalization method based on these predic-tions.4 Ambiguity PropertiesResearch has affirmed the negative impact of queryambiguity on an information retrieval system.
Asstemming clusters terms of different concepts, itshould increase query ambiguity.
To quantify thequery ambiguity potentially caused by stemming,we compute five ambiguity properties for eachquery: 1) the product of the number of senses, re-ferred as the sense product; 2) the product of thenumber of words mapped to one base form (e.g.
astem), referred as the word product; 3) the ratio ofthe sense product of depluralized query to which ofstemmed query, referred as the deplural-stem ratio;4) the sum of the inverse document frequency foreach word in a query, referred as the idf-sum; 5) thelength of a query.4.1 Sense ProductSense product measures the extent of query ambigu-ity after stemming.
We first find all words mapped toa given stem and, for each word, we then count thenumber of senses found in WordNet.
To computeFigure 4: Example of relative performance similaritiesamong text normalization techniques.
The cluster analy-sis uses the MAP scores of the MLT query modelCombined Topic SetRun MAP p-valueAND Raw 0.1213 N/AAND Dep 0.1324 5.598e-06*AND Stem 0.1550 ?
1.599e-07*OR Raw 0.1851 N/AOR Dep 0.1922 0.03035*OR Stem 0.2069 0.01123*MLT Raw 0.1893 N/AMLT Dep 0.1959 0.04837*MLT Stem 0.2093 0.009955*Table 1: Paired Wilcoxon signed-ranked test on MeanAverage Precision (MAP), utilizing raw query as thebaseline.
Significant differences between query modelsare labeled with *.
Results labeled with ?
indicate sig-nificant differences between depluralized queries and astemmed queries.22the number of senses for a given stem, we accumu-late the number of senses of each word mapped tothe stem.
The sense product is then the multiply-ing of the number of senses for each stemmed queryterm, computed as:sense product =n?i=1m?j=1Sj (1)Sj denotes the number of senses for each wordmapped to a stem i.
We have m words mapped to astem i and have n stems in a query.
As the senseproduct increases, the query ambiguity increases.Figure 5 illustrates the computation of the senseproduct for the query ?organic soil enhancement.
?The term ?organic?
has the stem organ, which is astem for 9 different words.
The accumulated num-ber of senses for ?organ?
is 39.
With the same ap-proach, we obtain 7 senses for 1 ?soil?
and 7 sensesfor ?enhanc.?
Therefore, multiplication 39, 7, and 7gives us the sense product value 1911.4.2 Word ProductWord product is an alternative measure of query am-biguity after stemming.
To compute the word prod-uct, we multiply the number of words mapped toeach stem of a given query, which is formulated as:word product =n?i=1Wi (2)Wi denotes the number of words mapped to astem i, and n is the number of stems in a query.We assume that the query ambiguity increases as theword product increases.
Consider the query ?organicsoil enhancement?
in Figure 5.
We find 9 wordsmapped to the stem ?organ?
; 3 words mapped to thestem ?soil?
; 5 words mapped to the stem ?enhance-ment?.
Therefore the word product for the query is105, the product of 9, 3, and 5.4.3 Deplural-Stem RatioDeplural-stem ratio is a variation of sense product.
Ittakes the ratio of the sense product of a depluralizedquery to the stemmed query.
As the deplural-stemratio increases, the query ambiguity after stemmingincreases.
In the example illustrated in Figure 5, thedeplural-stem ratio is the sense product of the deplu-ralized query ?organic soil enhancement?
divided bythe sense product of the stemmed query ?organ soilenhanc?.
The deplural-stem ratio is computed as:deplural-stem ratio =?ni=1?mj=1 Smj?ni=1?mj=1 Dj(3)4.4 Idf-sumThe idf-sum is the sum of the inverse document fre-quency (IDF) of each word in the query.
The IDF ofa given word measures the importance of the wordin the document collection.
Queries with high valuesof IDF are more likely to return relevant documentsfrom the collection.
For example, the term ?ZX-Turbo,?
describing a series of racing cars, has a highIDF and occurs only once in the entire TREC 2004Robust Track collection.
Therefore, searching thecollection with the term ?ZX-Turb?
will return theonly relevant document in the collection and achievehigh precision and recall.
The idf-sum is computedas:idf sum =n?i=1IDFi (4)IDFi denotes the idf of each query term i andn is the number of words in a query.
We assumethat the query ambiguity decreases as the idf-sum in-creases.
For the query ?organic soil enhancement?,the IDF for each term is 5.97082 (organic), 5.18994(soil), and 4.86996 (enhancement).
The idf-sum ofthe query is 16.0307.4.5 Query LengthThe length of the query is the number of words in aquery.4.6 Feature ValidationWe performed simple linear regression on each fea-ture as the first step to exclude ineffectual features.Table 2 demonstrates example results of simple lin-ear regression from the MLT query model, using theMAP of stemmed queries as the dependent variable.We take the logarithm of the sense product and wordproduct and the square root of the deplural-stem ra-tio (ds ratio) to mitigate skewness of the data.
Weincluded all five ambiguity properties to constructa query-based selection model as they demonstratestatistical significance in prediction.23organic soil enhancementorganic soil enhancementorgan soil enhancorganicorganismorganizerorganizationorganizeorganorganizedorganicallysoilsoiledsoilingorgans876633321511enhanciveenhanceenhancedenhancementenhancer2211139 7 739x7x7=191119115.970825.18994 4.8699616.0307tokennnumber of sensesn IDFLegend539nnumber of words1059x3x5=105Figure 5: Example of ambiguity indicators on the query?organic soil enhancement?Figure 6 demonstrates the distribution of ambigu-ity properties against the actual best text normaliza-tion technique.
It is noted that stemming is the actualbest method when a query has lower sense product,or lower word product, or a higher idf-sum.
It im-plies that stemming is less likely to be the actual bestmethod as a query is ambiguous.
The results demon-strate the potential of utilizing ambiguity measuresto select the actual best text normalization technique.5 Query-based Selection ModelThe cluster analysis in Section 3 suggests that thechoice of text normalization technique should bemade individually on each topic.
The retrieval per-formance would be enhanced as we choose an ap-propriate text normalization technique for a giventopic.
Given the five ambiguity properties describedin Section 4, we constructed Support Vector Regres-sion (SVR) (Smola and Schlkopf, 2004) models tochoose between stemming, depluralization, and notdoing any text normalization for different queries.Regression models aim to discover the relationshipbetween two random variables x and y.
In our work,independent variable x is a vector of the five prop-erties described in section 4: x = (sense product,word product, deplural-stem ratio, Idf-sum, length),and dependent variable y is the MAP score of agiven topic.
SVR has been successfully appliedfor many time series and function estimation prob-lems.
We utilized training data to construct multipleSVR models for each of nine combinations of querymodels (AND, OR, and MLT) and text normaliza-tion techniques (raw, depluralized, and stemmedqueries).
For example, the regression model for anMLT query model using stemmed queries is:Map MLT stem = 0.0853?
0.0849 ?
length+ 0.6286 ?
sense prod+ 0.0171 ?
word prod?
0.0774 ?
gap ds+ 0.4189 ?
idf sumFor a given query model, MLT, for example, weutilized training data to construct three SVR modelseach to predict the MAP scores of raw queries, de-pluralized queries, and stemmed queries in the testset.
We then compared the predicted MAP score of aquery and selected the text normalization techniquewith the highest predicted score.
Figure 5 illustratesour experiment framework on the query-based se-lection model.
We used five-fold cross-validation toevaluate the performance of the selection model.
Foreach iteration (fold) we used the 4 out of the 5 parti-tions as training data, constructing SVR models andusing the remaining fifth partition for testing.
We ac-cumulated all testing results and computed one over-all MAP score for evaluation.
Table 3 shows the re-sults of the five-fold cross-validation performed on249 query topics provided by the TREC 2004 Ro-bust Track.
We utilized a paired t-test to determinethe performance difference between the query-basedselection model (hybrid model) and other three textnormalization techniques.
The results in Table 3shows that the query-based selection model attainedthe highest MAP score and achieved significant im-provement.6 Conclusion and Future workThis paper evaluates the performance of stemmingand depluralization on the TREC 2004 Robust trackcollection.
We assume that the depluralization, as24Feature Coefficient R-square P-valuelength -0.06811 0.07864 5.768e-05*log(sense prod) -0.034692 0.1482 1.819e-08*log(word prod) -0.04557 0.09426 9.78e-06*sqrt(ds ratio) -0.021657 0.03738 0.006088*idf sum 0.008498 0.04165 0.003747*Table 2: Results of simple linear regression on the MAP of stemmed queries in MLT query model.dep raw stem0246810Sense Product(log) vs. Actual Best MethodActual Best Lexical Analysislog(sense_prod)dep raw stem012345Word Product(log) vs. Actual Best MethodActual Best Lexical Analysislog(word_prod)dep raw stem5101520253035IDF-sum vs. Actual Best MethodActual Best Lexical Analysisidf_sumFigure 6: Boxplots of the distribution of three ambiguity properties in each actual best text normalization technique23415Divide all querytopics into fivegroupsTrainingMAP score fromraw queriesMAP score fromdepluralized queriesMAP score fromstemmed queries2345SVR forraw queries2345SVR fordepluralizedqueries2345SVR forstemmedqueriesBuilding SupportVector Regression(SVR) modelsTesting andpredicting MeanAverage Precision(MAP)111Select the textnormalizationtechnique withhighest predictedMAP scoreFigure 7: Five-fold cross validation on query-based selection model (hybrid model)25Raw Dep Stem HybridAND MAP 0.1213 0.1324 0.1550 0.2094p-value <2.2e-16* <2.2e-16* <2.2e-16*OR MAP 0.1851 0.1922 0.2069 0.2131p-value 1.286e-05* 0.0003815* 0.09MLT MAP 0.1893 0.1959 0.2093 0.2132p-value 3.979e-05* 0.000939* 0.09677Table 3: Paired T-test was performed to examine the differences of each text normalization techniques (raw, deplu-ralizer, and stemmer) and query-based selection model (hybrid model).
Significant differences between models arelabeled with *.a low-level text-normalization technique, introducesless ambiguity than stemming and preserves moreprecise semantics of text.
The experimental re-sults demonstrate variable patterns, indicating thatsome topics performed better as a depluralized querythan as a stemmed query.
From Figure 4 in Sec-tion 3, we conclude that the choice of text nor-malization technique should be made individuallyon each topic.
An effective query-based selectionmodel would enhance information retrieval perfor-mance.
The query-based selection model utilizesSupport Vector Regression (SVR) models to predictthe mean average precision (MAP) of each queryfrom the ambiguity measures, and to choose an ap-propriate normalization technique based on thesepredictions.
The selection is lightweight, requiringonly analysis of the topic title itself against infor-mation readily available regarding the corpus (e.g,term idf values).
We extracted 5 measures to quan-tify the ambiguity of a query: 1) sense product; 2)word product; 3) deplural-stem ratio; 4) idf-sum;5) length of a query.
The constructed query-basedselection model demonstrate positive results on en-hanced performance.
The experiments reported hereshow that, even when the improvement is modest(1%), the selection model competes well with tra-ditional approaches.
To improve the model, futurework may first explore and introduce more powerfulfeatures to the models, considering properties suchas part of speech of text.
Second, future work mayexplore the effect of noise and outliers in the datato improve the accuracy of the model.
Finally, ad-ditional data mining techniques may be adopted infuture work to further improve the prediction.ReferencesM.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H.
Witten.
2009.
The WEKA Data Min-ing Software: An Update.
SIGKDD Explorations,1:10?18.David A.
Hull.
1996.
Stemming algorithms: A casestudy for detailed evaluation.
Journal of the AmericanSociety for Information Science, 47(1):70?84.L.
Kaufman and P.J.
Rousseeuw.
1990.
Finding groupsin data.
An introduction to cluster analysis.
Wiley,New York.Robert Krovetz.
2000.
Viewing morphology as an in-ference process.
Artificial Intelligence, 118(1-2):277?294, April.Christopher D Manning and Hinrich Schtze.
1999.
Foun-dations of Statistical Natural Language Processing.MIT Press, Cambridge, Mass.Paul McNamee, Charles Nicholas, and James Mayfield.2008.
Don?t have a stemmer?
: be un+concern+ed.In Proceedings of the 31st International ACM SIGIRConference on Research and Development in Informa-tion retrieval, pages 813?814, Singapore, Singapore.ACM.Ellen Riloff.
1995.
Little words can make a big differ-ence for text classification.
In Proceedings of the 18thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, pages 130?136, Seattle, Washington, United States.
ACM.Alex J. Smola and Bernhard Schlkopf.
2004.
A tutorialon support vector regression.
Statistics and Comput-ing, 14(3):199?222.E.
M. Voorhees.
2006.
The TREC 2005 robust track.In ACM SIGIR Forum, volume 40, pages 41?48.
ACMNew York, NY, USA.26
