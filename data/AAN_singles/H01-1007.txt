Architecture and Design Considerations in NESPOLE!
:a Speech Translation System for E-commerce ApplicationsAlon Lavie,Chad Langley,Alex WaibelCarnegie Mellon UniversityPittsburgh, PA, USAalavie@cs.cmu.eduFabio Pianesi,Gianni Lazzari,Paolo ColettiITC-irstTrento, ItalyLoredana Taddei,Franco BalducciAETHRAAncona, Italy1.
INTRODUCTIONNESPOLE!
1 is a speech-to-speech machine translation researchproject funded jointly by the European Commission and the USNSF.
The main goal of the NESPOLE!
project is to advance thestate-of-the-art of speech-to-speech translation in a real-world set-ting of common users involved in e-commerce applications.
Theproject is a collaboration between three European research labs(IRST in Trento Italy, ISL at University of Karlsruhe in Germany,CLIPS at UJF in Grenoble France), a US research group (ISL atCarnegie Mellon in Pittsburgh) and two industrial partners (APT- the Trentino provincial tourism bureau, and Aethra - an Italiantele-communications commercial company).
The speech-to-speechtranslation approach taken by the project builds upon previous workthat the research partners conducted within the context of the C-STAR consortium (see http://www.c-star.org).
The pro-totype system developed in NESPOLE!
is intended to provide ef-fective multi-lingual speech-to-speech communication between allpairs of four languages (Italian, German, French and English) withinbroad, but yet restricted domains.
The first showcase currently un-der development is in the domain of tourism and travel information.The NESPOLE!
speech translation system is designed to be anintegral part of advanced e-commerce technology of the next gener-ation.
We envision a technological scenario in which multi-modal(speech, video and gesture) interaction plays a significant role, inaddition to the passive browsing of pre-designed web pages as iscommon in e-commerce today.
The interaction between client andprovider will need to support online communication with agents(both real and artificial) on the provider side.
The language barrierthen becomes a significant obstacle for such online communica-tion between the two parties, when they do not speak a commonlanguage.
Within the tourism and travel domain, one can imaginea scenario in which users (the clients) are planning a recreationaltrip and are searching for specific detailed information about the1NESPOLE!
- NEgotiating through SPOken Lan-guage in E-commerce.
See the project website athttp://nespole.itc.it/.regions they wish to visit.
Initial general information is obtainedfrom a web site of a tourism information provider.
When moredetailed or special information is required, the customer has theoption of opening an online video-conferencing connection with ahuman agent of the tourism information provider.
Speech transla-tion is integrated within the video-conference connection; the twoparties each speak in their native language and hear the synthesizedtranslation of the speech of the other participant.
Text translation(in the form of subtitles) can also be provided.
Some multi-modalcommunication between the parties is also available.
The provideragent can send web pages to the display of the customer, and bothsides can annotate and refer to pictures and diagrams presented ona shared whiteboard application.In this paper we describe the design considerations behind the ar-chitecture that we have developed for the NESPOLE!
speech trans-lation system in the scenario described above.
In order to make thedeveloped prototype as realistic as possible for use by a commonuser, we assume only minimal hardware and software is availableon the customer side.
This does include a PC-type video camera,commercially available internet video-conferencing software (suchas Microsoft Netmeeting), standard audio and video hardware anda standard web browser.
However, no speech recognition and/ortranslation software is assumed to reside locally on the PC of thecustomer.
This implies a server-type architecture in which speechrecognition and translation are accomplished via interaction witha dedicated server.
The extent to which this server is centralizedor distributed is one of the major design considerations taken intoaccount in our system.2.
NESPOLE!
INTERLINGUA-BASEDTRANSLATION APPROACHOur translation approach builds upon previous work that we haveconducted within the context of the C-STAR consortium.
We usean interlingua-based approach with a relatively shallow task-orientedinterlingua representation [2] [1], that was initially designed for theC-STAR consortium and has been significantly extended for theNESPOLE!
project.
Interlingual machine translation is convenientwhen more than two languages are involved because it does not re-quire each language to be connected by a set of transfer rules toeach other language in each direction [3].
Adding a new languagethat has all-ways translation with existing languages requires onlywriting one analyzer that maps utterances into the interlingua andone generator that maps interlingua representations into sentences.The interlingua approach also allows each partner group to imple-ment an analyzer and generator for its home language only.
A fur-Figure 1: General Architecture of NESPOLE!
Systemther advantage is that it supports a paraphrase generation back intothe language of the speaker.
This provides the user with some con-trol in case the analysis of an utterance failed to produce a correctinterlingua.
The following are three examples of utterances taggedwith their corresponding interlingua representation:Thank you very muchc:thankAnd we?ll see you on February twelfth.a:closing (time=(february, md12))On the twelfth we have a single and a doubleavailable.a:give-information+availability+room(room-type=(single & double),time=(md12))3.
NESPOLE!
SYSTEM ARCHITECTUREDESIGNSeveral main considerations were taken into account in the de-sign of the NESPOLE!
Human Language Technology (HLT) serverarchitecture: (1) The desire to cleanly separate the actual HLTsystem from the communication channel between the two parties,which makes use of the speech translation capabilities provided bythe HLT system; (2) The desire to allow each research site to in-dependently develop its language specific analysis and generationmodules, and to allow each site to easily integrate new and im-proved components into the global NESPOLE!
HLT system; and(3) The desire of the research partners to build to whatever ex-tent possible upon software components previously developed inthe context of the C-STAR consortium.
We will discuss the ex-tent to which the designed architecture achieves these goals afterpresenting an overview of the architecture itself.Figure 1 shows the general architecture of the current NESPOLE!system.
Communication between the client and agent is facilitatedby a dedicated module - the Mediator.
This module is designed tocontrol the video-conferencing connection between the client andthe agent, and to integrate the speech translation services into thecommunication.
The mediator handles audio and video data as-sociated with the video-conferencing application and binary dataassociated with a shared whiteboard application.
Standard H.323data formats are used for these three types of data transfer.
Speech-to-speech translation of the utterances captured by the mediator isaccomplished through communication with the NESPOLE!
globalHLT server.
This is accomplished via socket connections withlanguage-specific HLT servers.
The communication between themediator and each HLT server consists mainly of linear PCM au-dio packets (some text and control messages are also supported andare described later in this section).Communication with MediatorSpeechRecognizerModuleParser/AnalysisIFtextAnalysis ChainSpeechSynthsizerGenerationModuleIFtext.GenerationChainCommunication with CommSwitchaudio audioLanguage X HLT ServerFigure 2: Architecture of NESPOLE!
Language-specific HLT ServersThe global NESPOLE!
HLT server comprises four separate lang-uage-specific servers.
Additional language-specific HLT serverscan easily be integrated in the future.
The internal architectureof each language-specific HLT server is shown in figure 2.
Eachlanguage-specific HLT server consists of an analysis chain and ageneration chain.
The analysis chain receives an audio stream cor-responding to a single utterance and performs speech recognitionfollowed by parsing and analysis of the input utterance into the in-terlingua representation (IF).
The interlingua is then transmitted toa central HLT communication switch (the CS), that forwards it tothe HLT servers for the other languagesas appropriate.
IF messagesreceived from the central communication switch are processed bythe generation chain.
A generation module first generates text inthe target language from the IF.
The text utterance is then sent toa speech synthesis module that produces an audio stream for theutterance.
The audio is then communicated externally to the me-diator, in order to be integrated back into the video-conferencingstream between the two parties.The mediator can, in principle, support multiple one-to-one com-munication sessions between client and agent.
However, the de-sign supports multiple mediators, which, for example, could eachbe dedicated to a different provider application.
Communicationwith the mediator is initiated by the client by an explicit actionvia the web browser.
This opens a communication channel to themediator, which contacts the agent station, establishes the video-conferencing connection between client and agent, and starts thewhiteboard application.
The specific pair of languages for a dia-logue is determined in advance from the web page from which theclient initiates the communication.
The mediator then establishes asocket communication channel with the two appropriate languagespecific HLT servers.
Communication between the two languagespecific HLT servers, in the form of IF messages, is facilitated bythe NESPOLE!
global communication switch (the CS).
The lan-guage specific HLT servers may in fact be physically distributedover the internet.
Each language specific HLT server is set to ser-vice analysis requests coming from the mediator side, and genera-tion requests arriving from the CS.Some further functionality beyond that described above is alsosupported.
As described earlier, the ability to produce a textualparaphrase of an input utterance and to display it back to the orig-inal speaker provides useful user control in the case of translationfailures.
This is supported in our system in the following way.
Inaddition to the translated audio, each HLT server also forwards thegenerated text in the output language to the mediator, which thendisplays the text on a dedicated application window on the PC ofthe target user.
Additionally, at the end of the processing of an in-put utterance by the analysis chain of an HLT server, the resultingIF is passed internally to the generation chain, which produces atext generation from the IF.
The result is a textual paraphrase of theinput utterance in the source language.
This text is then sent backto the mediator, which forwards it to the party from which the ut-terance originated.
The paraphrase is then displayed to the originalspeaker in the dedicated application window.
If the paraphrase iswrong, it is likely that the produced IF was incorrect, and thus thetranslation would also be wrong.
The user may then use a buttonon the application interface to signal that the last displayed para-phrase was wrong.
This action triggers a message that is forwardedby the mediator to the other party, indicating that the last displayedtranslation should be ignored.
Further functionality is planned tosupport synchronization between multi-modal events on the white-board and their corresponding speech actions.
As these are in verypreliminary stages of planning we do not describe them here.4.
DISCUSSION AND CONCLUSIONSWe believe that the architectural design described above has sev-eral strengths and advantages.
The clean separation of the HLTserver dedicated to the speech translation services from the exter-nal communication modules between the two parties allows the re-search partners to develop the HLT modules with a large degreeof independence.
Furthermore, this separation will allow us in thefuture to explore other types of mediators for different types of ap-plications.
One such application being proposed for developmentwithin the C-STAR consortium is a speech-to-speech translationservice over mobile phones.
The HLT server architecture describedhere would be able to generally support such alternative externalcommunication modalities as well.The physical distribution of the individual language specific HLTservers allows each site to independently develop, integrate andtest its own analysis and generation modules.
The organization ofeach language specific HLT server as an independent module al-lows each of the research sites to develop its unique approaches toanalysis and generation, while adhering to a simple communicationprotocol between the HLT servers and externally with the mediator.This allowed the research partners to ?jump-start?
the project withanalysis and generation modules previously developed for the C-STAR consortium, and incrementally develop these modules overtime.
Furthermore, the global NESPOLE!
communication switch(the CS) supports testing of analysis and generation among the fourlanguages in isolation from the external parts of the system.
Cur-rently, requests for analysis of a textual utterance can be transmittedto the HLT servers via the CS, with the resulting IF sent (via the CS)to all HLT servers for generation.
This gives us great flexibility indeveloping and testing our translation system.
The functionality ofthe CS was originally developed for our previous C-STAR project,and was reused with little modification.Support for additional languages is also very easy to incorpo-rate into the system by adding new language-specific HLT servers.Any new language specific HLT server needs only to adhere to thecommunication protocols with both the global NESPOLE!
commu-nication switch (the CS) and the external mediator.
The C-STARconsortium plans to use the general architecture described here forits next phase of collaboration, with support for at least three asianlanguages (Japanese, Korean and Chinese) in addition to the lan-guages currently covered by the NESPOLE!
project.The first prototype of the NESPOLE!
speech translation systemis currently in advanced stages of full integration.
A showcasedemonstration of the prototype system to the European Commis-sion is currently scheduled for late April 2001.5.
ACKNOWLEDGMENTSThe research work reported here was supported in part by theNational Science Foundation under Grant number 9982227.
Anyopinions, findings and conclusions or recomendations expressed inthis material are those of the author(s) and do not necessarily reflectthe views of the National Science Foundation (NSF).6.
REFERENCES[1] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,T.
Watanabe, and M. Woszczyna.
Evaluation of a PracticalInterlingua for Task-Oriented Dialogue.
In Workshop onApplied Interlinguas: Practical Applications of InterlingualApproaches to NLP, Seattle, 2000.
[2] L. Levin, D. Gates, A. Lavie, and A. Waibel.
An InterlinguaBased on Domain Actions for Machine Translation ofTask-Oriented Dialogues.
In Proceedings of the InternationalConference on Spoken Language Processing (ICSLP?98),pages Vol.
4, 1155?1158, Sydney, Australia, 1998.
[3] S. Nirenburg, J. Carbonell, M. Tomita, and K. Goodman.Machine Translation: A Knowledge-Based Approach.
MorganKaufmann, San Mateo, California, 1992.
