The Entropy Rate Principle as a Predictor of Processing Effort: AnEvaluation against Eye-tracking DataFrank KellerSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UKkeller@inf.ed.ac.ukAbstractThis paper provides evidence for Genzel and Char-niak?s (2002) entropy rate principle, which predictsthat the entropy of a sentence increases with its po-sition in the text.
We show that this principle holdsfor individual sentences (not just for averages), butwe also find that the entropy rate effect is partlyan artifact of sentence length, which also correlateswith sentence position.
Secondly, we evaluate a setof predictions that the entropy rate principle makesfor human language processing; using a corpus ofeye-tracking data, we show that entropy and pro-cessing effort are correlated, and that processing ef-fort is constant throughout a text.1 IntroductionGenzel and Charniak (2002, 2003) introduce the en-tropy rate principle, which states that speakers pro-duce language whose entropy rate is on averageconstant.
The motivation for this comes from in-formation theory: the most efficient way of trans-mitting information through a noisy channel is at aconstant rate.
If human communication has evolvedto be optimal in this sense, then we would expecthumans to produce text and speech with approx-imately constant entropy.
There is some evidencethat this is true for speech (Aylett, 1999).For text, the entropy rate principle predicts thatthe entropy of an individual sentence increases withits position in the text, if entropy is measured out ofcontext.
Genzel and Charniak (2002) show that thisprediction is true for the Wall Street Journal corpus,for both function words and for content words.
Theyestimate entropy either using a language model orusing a probabilistic parser; the effect can be ob-served in both cases.
Genzel and Charniak (2003)extend this results in several ways: they show thatthe effect holds for different genres (but the effectsize varies across genres), and also applies withinparagraphs, not only within whole texts.
Further-more, they show that the effect can also be ob-tained for language other than English (Russian andSpanish).
The entropy rate principle also predictsthat a language model that takes context into ac-count should yield lower entropy estimates com-pared to an out of context language model.
Genzeland Charniak (2002) show that this prediction holdsfor caching language models such as the ones pro-posed by Kuhn and de Mori (1990).The aim of the present paper is to shed furtherlight on the entropy rate effect discovered by Gen-zel and Charniak (2002, 2003) (henceforth G&C)by providing new evidence in two areas.In Experiment 1, we replicate G&C?s entropyrate effect and investigate the source of the effect.The results show that the correlation coefficientsthat G&C report are inflated by averaging over sen-tences with the same position, and by restrictingthe range of the sentence position considered.
Oncethese restrictions are removed the effect is smaller,but still significant.
We also show that the effect isto a large extend due to a confound with sentencelength: longer sentences tend to occur later in thetext.
However, we are able to demonstrate that theentropy rate effect still holds once this confound hasbeen removed.In Experiment 2, we test the psycholinguistic pre-dictions of the entropy rate principle.
This experi-ment uses a subset of the British National Corpusas training data and tests on the Embra corpus, a setof newspaper articles annotated with eye-movementdata.
We find that there is a correlation betweenthe entropy of a sentence and the processing ef-fort it causes, as measured by reading times in eye-tracking data.
We also show that there is no corre-lation between processing effort and sentence posi-tion, which indicates that processing effort in con-text is constant through a text, which is one of theassumptions underlying the entropy rate principle.2 Predictions for Human LanguageProcessingLet us examine the psycholinguistic predictions ofG&C?s entropy rate principle in more detail.
Weneed to distinguish two types of predictions: in-context predictions and out-of-context predictions.The principle states that the entropy rate in a textis constant, i.e., that speakers produce sentences sothat on average, all sentences in a text have the sameentropy.
In other words, communication is optimalin the sense that all sentences in the text are equallyeasy to understand, as they all have the same en-tropy.This constancy principle is claimed to hold forconnected text: all sentences in a text should beequally easy to process if they are presented in con-text.
If we take reading time as a measure of pro-cessing effort, then the principle predicts that thereshould be no significant correlation between sen-tence position and reading time in context.
We willtest this prediction in Experiment 2 using an eye-tracking corpus consisting of connected text.The entropy rate principle also makes the follow-ing prediction: if the entropy of a sentence is mea-sured out of context (i.e., without taking the preced-ing sentences into account), then entropy will in-crease with sentence position.
This prediction wastested extensively by G&C, whose results will bereplicated in Experiment 1.
With respect to process-ing difficulty, the entropy rate principle also predictsthat processing difficulty out of context (i.e., if iso-lated sentences are presented to experimental sub-jects) should increase with sentence position.
Wecould not test this prediction, as we only had in-context reading time data available for the presentstudy.However, there is another important predictionthat can be derived from the entropy rate principle:sentences with a higher entropy should have higherreading times.
This is an important precondition forthe entropy rate principle, whose claims about therelationship between entropy and sentence positionare only meaningful if entropy and processing effortare correlated.
If there was no such correlation, thenthere would be no reason to assume that the out-of-context entropy of a sentence increases with sen-tence position.
G&C explicitly refer to this relation-ship i.e., they assume that a sentence that is moreinformative is harder to process (Genzel and Char-niak, 2003, p. 65).
Experiment 1 will try to demon-strate the validity of this important prerequisite ofthe entropy rate principle.3 Experiment 1: Entropy Rate andSentence LengthThe main aim of this experiment was to replicateG&C?s entropy rate effect.
A second aim was totest the generality of their result by determining ifthe relationship between sentence position and en-tropy also holds for individual sentences (rather thanfor averages over sentences of a given position, astested by G&C).
We also investigated the effect oftwo parameters that G&C did not explore: the cut-off for article position (G&C only deal with sen-tences up to position 25), and the size of the n-gramused for estimating sentence probability.
Finally, weinclude sentence length as a baseline that entropy-based models should be evaluated against.3.1 Method3.1.1 MaterialsThis experiment used the same corpus as Genzeland Charniak (2002), viz., the Wall Street Journalpart of the Penn Treebank, divided into a trainingset (section 0?20) and a test set (sections 21?24).Each article was treated as a separate text, and sen-tence positions were computed by counting the sen-tences from the beginning of the text.
The trainingset contained 42,075 sentences, the test set 7,133sentences.
The sentence positions in the test set var-ied between one and 149.3.1.2 ProcedureThe per-word entropy was computed using an n-gram language model, as proposed by G&C:1?H(X) = ?
1|X | ?xi?X logP(xi|xi?
(n?1) .
.
.xi?1)(1)Here, ?H(X) is the estimate of the per-word en-tropy of the sentence X , consisting of the words xi,and n is the size of the n-gram.
The n-gram proba-bilities were computed using the CMU-Cambridgelanguage modeling toolkit (Clarkson and Rosen-feld, 1997), with the following parameters: vocab-ulary size 50,000; smoothing by absolute discount-ing; sentence beginning and sentence end as contextcues (default values were used for all other parame-ters).G&C use n = 3, i.e., a trigram model.
Weexperimented with this parameter and used n =1, .
.
.
,5.
For n = 1, equation (1) reduces to ?H(X) =?
1|X | ?xi?X logP(xi), i.e., a model based on wordfrequency.The experiment also includes a simple model thatdoes not take any probabilistic information into ac-count, but simply uses the sentence length |X | topredict sentence position.
This model will serve asthe baseline.1Note that the original definition given by Genzel and Char-niak (2002, 2003) does not include the minus sign.
However,all their graphs display entropy as a positive quantity, hence weconclude that this is the definition they are using.0 20 40 60 80Sentence position77.588.599.5Entropy[bits]Figure 1: Experiment 1: correlation of sentence en-tropy and sentence position (bins, 3-grams, cut-off 76)We also vary another parameter: c, the cut-offfor the position.
Genzel and Charniak (2002) usec = 25, i.e., only sentences with a position of 25or lower are considered.
In Genzel and Charniak(2003), an even smaller cut-off of c = 10 is used.This severely restricts the generality of the resultsobtained.
We will therefore report results not onlyfor c = 25, but also for c = 76.
This cut-off has beenset so that there are at least 10 items in the test setfor each position.
Furthermore, we also repeated theexperiment without a cut-off for sentence length.3.2 ResultsTable 1 shows the results for the replication of Gen-zel and Charniak?s (2002) entropy rate effect.
Theresults at the top of the table were obtained usingbinning, i.e., we computed the mean entropy of allsentences of a given position, and then correlatedthese mean entropies with the sentence positions.The parameters n (n-gram size) and c (cut-off value)were varied as indicated in the previous section.The bottom of Table 1 gives the correlation co-efficients computed on the raw data, i.e., withoutbinning: here, we correlated the entropy of a givensentence directly with its position.
The graphs inFigure 1 and Figure 2 illustrate the relationship be-tween position and entropy and between positionand length, respectively.3.3 Discussion3.3.1 Entropy Rate and Sentence LengthThe results displayed in Table 1 confirm G&C?smain finding, i.e., that entropy increases with sen-tence length.
For a cut-off of c = 25 (as used byG&C), a maximum correlation of 0.6480 is ob-tained (for the 4-gram model).
The correlations forthe other n-gram models are lower.
All correlations0 20 40 60 80Sentence position152025SentencelengthFigure 2: Experiment 1: correlation of sentencelength and sentence position (bins, cut-off 76)are significant (with the exception of the unigrammodel).
However, we also find that a substantial cor-relation of ?0.4607 is obtained even for the base-line model: there is a negative correlation betweensentence length and sentence position, i.e., longersentences tend to occur earlier in the text.
This find-ing potentially undermines the entropy rate effect,as it raises the possibility that this effect is simplyan effect of sentence length, rather than of sentenceentropy.
Note that the correlation coefficient for thenone of the n-gram models is significantly higherthan the baseline (significance was computed on theabsolute values of the correlation coefficients).The second finding concerns the questionwhether the entropy rate effect generalizes to sen-tences with a position of greater than 25.
The re-sults in Table 1 show that the effect generalizes toa cut-off of c = 76 (recall that this value was cho-sen so that each position is represented at least tentimes in the test data).
Again, we find a significantcorrelation between entropy and sentence positionfor all values of n. This is illustrated in Figure 1.However, none of the n-gram models is able to beatthe baseline of simple sentence position; in fact,now all models (with the exception of the unigrammodel) perform significantly worse than the base-line.
The correlation obtained by the baseline modelis graphed in Figure 2.Finally, we tried to generalize the entropy rate ef-fect to sentences with arbitrary position (no cut-off).Here, we find that there is no significant positivecorrelation between entropy and position for any ofthe n-gram models.
Only sentence length yields areliable correlation, though it is smaller than if acut-off is applied.
This result is perhaps not surpris-ing, as a lot of the data is very sparse: for positionsbetween 77 and 149, less than ten data points arec = 25 c = 76 c = ?Binned data r p r p r pEntropy 1-gram 0.0593?
0.7784 0.3583 0.0015 ?0.3486?
0.0000Entropy 2-gram 0.4916 0.0126 0.2849?
0.0126 0.0723 0.3808Entropy 3-gram 0.6387 0.0006 0.2427?
0.0346 0.1350 0.1006Entropy 4-gram 0.6480 0.0005 0.2378?
0.0386 0.1354 0.0996Entropy 5-gram 0.6326 0.0007 0.2281?
0.0475 0.1311 0.1111Sentence length ?0.4607 0.0205 ?0.4943 0.0000 ?0.1676 0.0410c = 25 c = 76 c = ?Raw data r p r p r pEntropy 1-gram ?0.0023?
0.8781 0.0598?
0.0000 0.0301?
0.0110Entropy 2-gram 0.0414 0.0056 0.0755?
0.0000 0.0615?
0.0000Entropy 3-gram 0.0598 0.0001 0.0814?
0.0000 0.0706?
0.0000Entropy 4-gram 0.0625 0.0000 0.0830?
0.0000 0.0712?
0.0000Entropy 5-gram 0.0600 0.0001 0.0812?
0.0000 0.0695?
0.0000Sentence length ?0.0635 0.0000 ?0.1099 0.0000 ?0.1038 0.0000Table 1: Results of Experiment 1: correlation of sentence entropy and sentence position, on binned data; c:cut-off, r: correlation coefficient, p: significance level, ?
: correlation significantly different from baseline(sentence length)available per position.
Based on data this sparse, noreliable correlation coefficients can be expected.Let us now turn to Table 1, which displays theresults that were obtained by computing correlationcoefficients on the raw data, i.e., without computingthe mean entropy for all sentences with the same po-sition.
We find that for all parameter settings a sig-nificant correlation between sentence entropy andsentence position is obtained (with the exception ofn = 1, c = 25).
The correlation coefficients are sig-nificantly lower than the ones obtained using bin-ning, the highest coefficient is 0.0830.
This meansthat a small but reliable entropy effect can be ob-served even on the raw data, i.e., for individual sen-tences rather than for bins of sentences with thesame position.However, the results in Table 1 also confirm ourfindings regarding the baseline model (simple sen-tence length): in all cases the correlation coefficientachieved for the baseline is higher than the oneachieved by the entropy models, in some cases evensignificantly so.3.3.2 Disconfounding Entropy and SentenceLengthTaken together, the results in Table 1 seem to indi-cate that the entropy rate effect reported by G&C isnot really an effect of entropy, but just an effect ofsentence length.
The effect seems to be due to thefact that G&C compute entropy rate by dividing theentropy of a sentence by its length: sentence lengthis correlated with sentence position, hence entropyrate will be correlated with position as well.It is therefore necessary to conduct additionalanalyses that remove the confound of sentencelength.
This can be achieved by computing partialcorrelations; the partial correlation coefficient be-tween a factor 1 and a factor 2 expresses the degreeof association between the factors that is left oncethe influence of a third factor has been removedfrom both factors.
For example, we can compute thecorrelation of position and entropy, with sentencelength partialled out.
This will tell us use the amountof association between position and entropy that isleft once the influence of length has been removedfrom both position and entropy.Table 2 shows the results of partial correlationanalyses for length and entropy.
Note that theseresults were obtained using total entropy, not per-word entropy, i.e., the normalizing term 1|X | wasdropped from (1).
The partial correlations are onlyreported for the trigram model.The results indicate that entropy is a signifi-cant predictor sentence position, even once sentencelength has been partialled out.
This result holds forboth the binned data and the raw data, and for allcut-offs (with the exception of c = 76 for the binneddata).
Note however, that entropy is always a worsepredictor than sentence length; the absolute value ofthe correlation coefficient is always lower.
This in-dicates that the entropy rate effect is a much weakereffect than the results presented by G&C suggest.4 Entropy Rate Effect and ProcessingEffortThe previous experiment confirmed the validity ofthe entropy rate effect: it demonstrated a signifi-c = 25 c = 76 c = ?Binned data r p r p r pEntropy 3-gram 0.6708 0.0000 0.1473 0.2067 0.1703 0.0383Sentence length ?0.7435 0.0000 ?0.3020 0.0084 ?0.2131 0.0093Raw data r p r p r pEntropy 3-gram 0.0784 0.0000 0.0929 0.0000 0.0810 0.0000Sentence length ?0.0983 0.0000 ?0.1311 0.0000 ?0.1176 0.0000Table 2: Results of Experiment 1: correlation of entropy and sentence length with sentence position, withthe other factor partialled outcant correlation between sentence entropy and sen-tence position, even when sentence length, whichwas shown to be a confounding factor, was con-trolled for.
The effect, however, was smaller thanclaimed by G&C, in particular when applied to in-dividual sentences, as opposed to means obtainedfor sentences at the same position.In the present experiment, we will test a crucialaspect of the entropy rate principle, viz., that en-tropy should correlate with processing effort.
Wewill test this using a corpus of newspaper text thatis annotated with eye-tracking data.
Eye-trackingmeasures of reading time are generally thought toreflect the amount of cognitive effort that is requiredfor the processing of a given word or sentence.A second prediction of the entropy rate princi-ple is that sentences with higher position should beharder to process than sentences with lower posi-tion.
This relationship should hold out of context,but not in context (see Section 2).4.1 Method4.1.1 MaterialsAs a test corpus, we used the Embra corpus (Mc-Donald and Shillcock, 2003).
This corpus consistsof 10 articles from Scottish and UK national broad-sheet newspapers.
The excerpts cover a wide rangeof topics; they are slightly edited to make them com-patible with eye-tracking.2 The length of the articlesvaries between 97 and 405 words, the total size ofthe corpus is 2,262 words (125 sentences).
Twenty-three native speakers of English read all 10 arti-cles while their eye-movements were recorded us-ing a Dual-Purkinke Image eye-tracker.
To makesure that subjects read the texts carefully, compre-hension questions were also administered.
For de-tails on method used to create the Embra corpus,see McDonald and Shillcock (2003).The training and development sets for this exper-iment were compiled so as to match the test corpusin terms of genre.
This was achieved by selecting2This includes, e.g., the removal of quotation marks andbrackets, which can disrupt the eye-movement record.all files from the British National Corpus (Burnard,1995) that originate from UK national or regionalbroadsheet newspapers.
This subset of the BNC wasdivided into a 90% training set and a 10% develop-ment set.
This resulted in a training set consisting of6,729,104 words (30,284 sentences), and a develop-ment set consisting of 746,717 words (34,269 sen-tences).
The development set will be used to test ifthe entropy rate effect holds on this new corpus.The sentence positions in the test set varied be-tween one and 24, in the development, they variedbetween one and 206.4.1.2 ProcedureTo compute per-word entropy, we trained n-gram models on the training set using the CMU-Cambridge language modeling toolkit, with thesame parameters as in Experiment 1.
Again, n wasvaried from 1 to 5.
We determined the correlationbetween per-word entropy and sentence position forboth the development set (derived from the BNC)and for the test set (the Embra corpus).Then, we investigated the predictions of G&C?sentropy rate principle by correlating the positionand entropy of a sentence with its reading time inthe Embra corpus.The reading measure used was total reading time,i.e., the total time it takes a subject to read a sen-tence; this includes second fixations and re-fixationsof words.
We also experimented with other readingmeasures such as gaze duration, first fixation time,second fixation time, regression duration, and skip-ping probability.
However, the results obtained withthese measures were similar to the ones obtainedwith total reading time, and will not be reportedhere.Total reading time is trivially correlated with sen-tence length (longer sentences taker longer to read).Hence we normalized total reading time by sen-tence length, i.e., by multiplying with the factor 1|X | ,also used in the computation of per-word entropy.It is also well-known that reading time is corre-lated with two other factors: word length and wordfrequency; shorter and more frequent words takec = 25 c = 76 c = ?Binned data r p r p r pEntropy 1-gram ?0.5495 0.0044 ?0.2510 0.0287 0.0232 0.7419Entropy 2-gram 0.0602 0.7751 0.4249 0.0001 0.0392 0.5773Entropy 3-gram 0.4523 0.0232 0.5395 0.0000 0.1238 0.0776Entropy 4-gram 0.4828 0.0145 0.5676 0.0000 0.1229 0.0800Entropy 5-gram 0.4834 0.0144 0.5723 0.0000 0.1223 0.0813Sentence length ?0.8584 0.0000 ?0.2947 0.0098 ?0.2161 0.0019Raw data r p r p r pEntropy 1-gram ?0.0636 0.0000 ?0.0543 0.0000 0.0364 0.0000Entropy 2-gram ?0.0069 0.2783 0.0435 0.0000 0.0477 0.0000Entropy 3-gram 0.0162 0.0103 0.0659 0.0000 0.0687 0.0000Entropy 4-gram 0.0193 0.0022 0.0691 0.0000 0.0711 0.0000Entropy 5-gram 0.0192 0.0024 0.0685 0.0000 0.0707 0.0000Sentence length ?0.0747 0.0000 ?0.1027 0.0000 ?0.0913 0.0000Table 3: Results of Experiment 2: correlation of sentence entropy and sentence position on the BNCless time to read (Just and Carpenter, 1980).
Weremoved these confounding factors by conductingmultiple regression analyses involving word length,word frequency, and the predictor variable (entropyor sentence position).
The aim was to establish ifthere is a significant effect of entropy or sentencelength, even when the other factors are controlledfor.
Word frequency was estimated using the uni-gram model trained on the training corpus.In the eye-tracking literature, it is generally rec-ommended to run regression analyses on the readingtimes collected from individual subjects.
In otherwords, it is not good practice to compute regressionson average reading times, as this fails take between-subject variation in reading behavior into account,and leads to inflated correlation coefficients.
Wetherefore followed the recommendations of Lorchand Myers (1990) for computing regressions with-out averaging over subjects (see also McDonald andShillcock (2003) for details on this procedure).4.2 ResultsTable 3 shows the results of the correlation analyseson the development set.
These results were obtainedafter excluding all sentences at positions 1 and 2.In the newspaper texts in the BNC, these positionshave a special function: position 1 contains the title,and position 2 contains the name of the author.
Thefirst sentence of the text is therefore on position 3(unlike in the Penn Treebank, in which no title orauthor information is included and texts start at po-sition 1).We then conducted the same correlation analyseson the test set, i.e., on the Embra eye-tracking cor-pus.
The results are tabulated in Table 4.
Note we setno threshold for sentence position in the test set, asBinned data Raw datar p r p1-gram ?0.6505 0.0006 ?0.2087 0.01952-gram ?0.3471 0.0965 ?0.1498 0.09543-gram ?0.5512 0.0052 ?0.1674 0.06204-gram ?0.5824 0.0028 ?0.1932 0.03085-gram ?0.5750 0.0033 ?0.1919 0.0320Length 0.3902 0.0594 0.0885 0.3264Table 4: Results of Experiment 2: correlation of sen-tence entropy and sentence position on the Embracorpusr pEntropy 2-gram 0.1551 0.007Entropy 3-gram 0.1646 0.000Entropy 4-gram 0.1650 0.000Entropy 5-gram 0.1648 0.000Sentence position ?0.0266 0.564Table 5: Results of Experiment 2: correlation ofreading times with sentence entropy and sentencepositionthe maximum article length in this corpus was only24 sentences.Finally, we investigated if the total reading timesin the Embra corpus are correlated with sentence po-sition and entropy.
We computed regression analysisthat partialled out word length, word frequency, andsubject effects as recommended by Lorch and My-ers (1990).
All variables other than position werenormalized by sentence length.
Table 5 lists the re-sulting correlation coefficients.
Note that no binningwas carried out here.
Figure 3 plots one of the cor-relations for illustration.0 200 400 600 800 1000Reading time [ms]468101214Entropy[bits]Figure 3: Experiment 2: correlation of reading timesand sentence entropy (3-grams)4.3 DiscussionThe results in Table 3 confirm that the results ob-tained on the Penn Treebank also hold for the news-paper part of the BNC.
The top half of the table liststhe correlation coefficients for the binned data.
Wefind a significant correlation between sentence po-sition and entropy for the cut-off values 25 and 76.In both cases, there is also a significant correlationwith sentence length; this correlation is particularlyhigh (?0.8584) for c = 25.
The entropy rate effectdoes not seem to hold if there is no cut-off; here,we fail to find a significant correlation (though thecorrelation with length is again significant).
This isprobably explained by the fact that the BNC test setcontains sentences with a maximum position of 206,and data for these high sentence positions is verysparse.The lower half of Table 3 confirms another resultfrom Experiment 1: there is generally a low, but sig-nificant correlation between entropy and position,even if the correlation is computed for individualsentences rather than for bins of sentences with thesame position.
Furthermore, we find that sentencelength is again a significant predictor of sentenceposition, even on the raw data.
This is in line withthe results of Experiment 1.Table 4 lists the results obtained on the test set(i.e., the Embra corpus).
Note that no cut-off wasapplied here, as the maximum sentence position inthis set is only 24.
Both on the binned data andon the raw data, we find significant correlations be-tween sentence position and both entropy and sen-tence length.
However, compared to the results onthe BNC, the signs of the correlations are inverted:there is a significant negative correlation betweenposition and entropy, and a significant positive cor-relation between position and length.
It seems thatthe Embra corpus is peculiar in that longer sen-tences appear later in the text, rather than earlier.This is at odds with what we found on the PennTreebank and on the BNC.
Note that the positivecorrelation of position and length explains the neg-ative correlation of position and entropy: length en-ters into the entropy calculation as 1|X | , hence a high|X | will lead to low entropy, and vice versa.We have no immediate explanation for the inver-sion of the relationship between position and lengthin the Embra corpus; it might be an idiosyncrasyof this corpus (note that the texts were specificallypicked for eye-tracking, and are unlikely to be a ran-dom sample; they are also shorter than usual news-paper texts).
Note in particular that the Embra cor-pus is not a subset of the BNC (although it was sam-pled from UK broadsheet newspapers, and henceshould be similar to our development and trainingcorpora).Let us now turn to Table 5, which lists the re-sults of the analyses correlating the total readingtime for a sentence with its position and its entropy(derived from n-grams with n = 2, .
.
.
,5).
Note thatthese correlation analyses were conducted by par-tialling out word length and word frequency, whichare well-know to correlate with reading times.
Wefind that even once these factors have been con-trolled, there is still a significant positive correlationbetween entropy and reading time: sentences withhigher entropy are harder to process and hence havehigher reading times.
This is illustrated in Figure 3for one of the correlations.
As we argued in Sec-tion 2, this relationship between entropy and pro-cessing effort is a crucial prerequisite of the entropyrate principle.
The increase of entropy with sen-tence position observed by G&C (and in our Exper-iment 1) only makes sense if increased entropy cor-responds to increased processing difficulty (e.g., toincreased reading time).
Note that this result is com-patible with previous research by McDonald andShillcock (2003), who demonstrate a correlation be-tween reading time measures and bigram probabil-ity (though their analysis is on the word level, noton the sentence level).The second main finding in Table 5 is that there isno significant correlation between sentence positionand reading time.
As we argued in Section 2, thisis predicted by the entropy rate principle: the opti-mal way to send information is at a constant rate.In other words, speakers should produce sentenceswith constant informativeness, which means that ifcontext is taken into account, all sentences shouldbe equally difficult to process, no matter which po-sition they are at.
This manifests itself in the absenceof a correlation between position and reading timein the eye-tracking corpus.5 ConclusionsThis paper made a contribution to the understandingof the entropy rate principle, first proposed by Gen-zel and Charniak (2002).
This principle predicts thatthe position of a sentence in a text should correlatewith its entropy, defined as the sentence probabil-ity normalized by sentence length.
In Experiment 1,we replicated the entropy rate effect reported byGenzel and Charniak (2002, 2003) and showed thatit generalizes to a larger range of sentence posi-tions and also holds for individual sentences, notjust averaged over all sentences with the same posi-tion.
However, we also found that a simple baselinemodel based on sentence length achieves a corre-lation with sentence position.
In many cases, therewas no significant difference between the entropyrate model and the baseline.
This raises the possibil-ity that the entropy rate effect is simply an artifactof the way entropy rate is computed, which involvessentence length as a normalizing factor.
However,using partial correlation analysis, we were able toshow that entropy is a significant predictor of sen-tence position, even when sentence length is con-trolled.In Experiment 2, we tested a number of importantpredictions of the entropy rate principle for humansentence processing.
First, we replicated the entropyrate effect on a different corpus, a subset of the BNCrestricted to newspaper text.
We found essentiallythe same pattern as in Experiment 1.
Using a cor-pus of eye-tracking data, we showed that entropyis correlated with processing difficulty, as measuredby reading times in the eye-movement record.
Thisconfirms an important assumption that underlies theentropy rate principle.
As the eye-tracking corpuswe used was a corpus of connected sentences, it en-abled us to also test another prediction of the en-tropy rate principle: in context, all sentences shouldbe equally difficult to process, as speakers gener-ate sentences with constant informativeness.
Thismeans that no correlation between sentence positionand reading times was expected, which is what wefound.Another important prediction of the entropy rateprinciple remains to be evaluated in future work: forout-of-context sentences, there should be a correla-tion between sentence position and processing ef-fort.
This prediction can be tested by obtaining read-ing times for sentences sampled from a corpus andread by experimental subjects in isolation.ReferencesAylett, Matthew.
1999.
Stochastic suprasegmen-tals: Relationships between redundancy, prosodicstructure and syllabic duration.
In Proceedings ofthe 14th International Congress of Phonetic Sci-ences.
San Francisco.Burnard, Lou.
1995.
Users Guide for the BritishNational Corpus.
British National Corpus Con-sortium, Oxford University Computing Service.Clarkson, Philip R. and Ronald Rosenfeld.
1997.Statistical language modeling using the CMU-Cambridge toolkit.
In Proceedings of Eu-rospeech.
ESCA, Rhodes, Greece, pages 2707?2710.Genzel, Dmitriy and Eugene Charniak.
2002.
En-tropy rate constancy in text.
In Proceedings of the40th Annual Meeting of the Association for Com-putational Linguistics.
Philadelphia, pages 199?206.Genzel, Dmitriy and Eugene Charniak.
2003.
Vari-ation of entropy and parse trees of sentences asa function of the sentence number.
In MichaelCollins and Mark Steedman, editors, Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing.
Sapporo, pages 65?72.Just, Marcel A. and Patricia A. Carpenter.
1980.
Atheory of reading: From eye fixations to compre-hension.
Psychological Review 87:329?354.Kuhn, Roland and Renato de Mori.
1990.
A cache-based natural language model for speech repro-duction.
IEEE Transactions on Pattern Analysisand Machine Intelligence 12(6):570?583.Lorch, Robert F. and Jerome L. Myers.
1990.
Re-gression analyses of repeated measures data incognitive research.
Journal of ExperimentalPsychology: Learning, Memory, and Cognition16(1):149?157.McDonald, Scott A. and Richard C. Shillcock.2003.
Low-level predictive inference in reading:The influence of transitional probabilities on eyemovements.
Vision Research 43:1735?1751.
