Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 901?911,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsConvolutional Neural Network for Paraphrase IdentificationWenpeng Yin and Hinrich Sch?utzeCenter for Information and Language ProcessingUniversity of Munich, Germanywenpeng@cis.uni-muenchen.deAbstractWe present a new deep learning architectureBi-CNN-MI for paraphrase identification (PI).Based on the insight that PI requires compar-ing two sentences on multiple levels of granu-larity, we learn multigranular sentence repre-sentations using convolutional neural network(CNN) and model interaction features at eachlevel.
These features are then the input to alogistic classifier for PI.
All parameters of themodel (for embeddings, convolution and clas-sification) are directly optimized for PI.
To ad-dress the lack of training data, we pretrain thenetwork in a novel way using a language mod-eling task.
Results on the MSRP corpus sur-pass that of previous NN competitors.1 IntroductionIn this paper, we address the problem of paraphraseidentification.
It is usually formalized as a binaryclassification task: for two sentences (S1, S2), deter-mine whether they roughly have the same meaning.Inspired by recent successes of deep neuralnetworks (NNs) in fields like computer vision(Neverova et al, 2014), speech recognition (Denget al, 2013) and natural language processing (Col-lobert and Weston, 2008), we adopt a deep learningapproach to paraphrase identification in this paper.The key observation that motivates our NN archi-tecture is that the identification of a paraphrase rela-tionship between S1and S2requires an analysis atmultiple levels of granularity.
(A1) ?Detroit manufacturers have raised vehicleprices by ten percent.?
?
(A2) ?GM, Ford andChrysler have raised car prices by five percent.
?Example A1/A2 shows that paraphrase identifica-tion requires comparison at the word level.
A1 can-not be a paraphrase of A2 because the numbers ?ten?and ?five?
are different.
(B1) ?Mary gave birth to a son in 2000.?
?
(B2)?He is 14 years old and his mother is Mary.
?PI for B1/B2 can only succeed at the sentencelevel since B1/B2 express the same meaning usingvery different means.Most work on paraphrase identification has fo-cused on only one level of granularity: either on low-level features (e.g., Madnani et al (2012)) or on thesentence level (e.g., ARC-I, Hu et al (2014)).An exception is the RAE model (Socher et al,2011).
It computes representations on all levels ofa parse tree: each node ?
including nodes corre-sponding to words, phrases and the entire sentence?
is represented as a vector.
RAE then computesa n1?
n2comparison matrix of the two trees de-rived from S1and S2respectively, where n1, n2arethe number of nodes and each comparison is the Eu-clidean distance between two vectors.
This is thenthe basis for paraphrase classification.RAE (Socher et al, 2011) is one of three prior NNarchitectures that we draw on to design our system.It embodies the key insight that paraphrase identi-fication involves analysis of information at multiplelevels of granularity.
However, relying on parsinghas limitations for noisy text and for other applica-tions in which highly accurate parsers are not avail-able.
We extend the basic idea of RAE by explor-ing stacked convolution layers which on one handuse sliding windows to split sentences into flexiblephrases, furthermore, higher layers are able to ex-901tract more abstract features of longer-range phrasesby combining phrases in lower layers.A representative way of doing this in deep learn-ing is the work by Kalchbrenner et al (2014), thesecond prior NN architecture that we draw on.
Theyuse convolution to learn representations at multiplelevels (Collobert and Weston, 2008).
The motiva-tion for convolution is that natural language con-sists of long sequences in which many short sub-sequences contribute in a stable way to the struc-ture and meaning of the long sequence regardlessof the position of the subsequence within the longsequence.
Thus, it is advantageous to learn con-volutional filters that detect a particular feature re-gardless of position.
Kalchbrenner et al (2014)?s ar-chitecture extends this idea in two important ways.First, k-max pooling extracts the k top values froma sequence of convolutional filter applications andguarantees a fixed length output.
Second, they stackseveral levels of convolutional filters, thus achievingmultigranularity.
We incorporate this architecture asthe part that analyzes an individual sentence.The third prior NN architecture we draw on isARC proposed by Hu et al (2014) who also attemptto exploit convolution for paraphrase identification.Their key insight is that we want to be able to di-rectly optimize the entire system for the task we areaddressing, i.e., for paraphrase identification.
Hu etal.
(2014) do this by adopting a Siamese architec-ture: their NN consists of two shared-weight sen-tence analysis NNs that feed into a binary classi-fier that is directly trained on labeled sentence pairs.As we will show below, this is superior to separat-ing the two steps: first learning sentence represen-tations, then training binary classification for fixed,learned sentence representations as Bromley et al(1993), Socher et al (2011) and many others do.We can now give an overview of our NN architec-ture (Figure 1).
We call it Bi-CNN-MI: ?Bi-CNN?stands for double CNNs used in Siamese frame-work, ?MI?
for multigranular interaction features.Bi-CNN-MI has three parts: (i) the sentence anal-ysis network CNN-SM, (ii) the sentence interactionmodel CNN-IM and (iii) a logistic regression on topof the network that performs paraphrase identifica-tion.
We now describe these three parts in detail.
(i) Following Kalchbrenner et al (2014), we de-sign CNN-SM, a convolutional sentence analysisNN that computes representations at four differentlevels: word, short ngram, long ngram and sentence.This multigranularity is important because para-phrase identification benefits from analyzing sen-tences at multiple levels.
(ii) Following Socher et al (2011), CNN-IM, theinteraction model, computes interaction features ass1?
s2matrices, where siis the number of items ofa certain granularity in Si.
In contrast to Socher etal.
(2011), CNN-IM computes these features at fixedlevels and only for comparable units; e.g., we do notcompare single words with entire sentences.
(iii) Following Hu et al (2014), we integrate twocopies of CNN-SM into a Siamese architecture thatallows to optimize all parameters of the NN for para-phrase identification.
In our case, these parametersinclude parameters for word embedding, for convo-lution filters, and for the classification of paraphrasecandidate pairs.
In contrast to Hu et al (2014), theinputs to the final paraphrase candidate pair classifi-cation layer are interaction feature matrices at mul-tiple levels ?
as opposed to single-level features thatdo not directly compare an element of S1with a po-tentially corresponding element of S2.There is one other problem we have to address toget good performance.
Training sets for paraphraseidentification are small in comparison with the highcomplexity of the task.
Training a complex networklike Bi-CNN-MI with a large number of parameterson a small training set is not promising due to sparse-ness and likely overfitting.In order to make full use of the training data, wepropose a new unsupervised training scheme CNN-LM (CNN Language Model) to pretrain the largestpart of the model, the sentence analysis networkCNN-SM.
The key innovation is that we use a lan-guage modeling task in a setup similar to autoen-coding for pretraining (see below for details).
Thismeans that embedding and convolutional parameterscan be pretrained on very large corpora since no hu-man labels are required for pretraining.We will show below that this pretraining is criticalfor getting good performance in the paraphrase task.However, the general design principle of this type ofunsupervised pretraining should be widely applica-ble given that next-word prediction training is possi-ble in many NLP applications.
Thus, this new wayof unsupervised pretraining could be an important902contribution of the paper independent of paraphraseidentification.Section 2 discusses related work.
Sections 3 and4 introduce the sentence model CNN-SM and thesentence interaction model CNN-IM.
Section 5 de-scribes the training regime.
The experiments arepresented in Section 6.
Section 7 concludes.2 Related workBi-CNN-MI is closely related to NN models for sen-tence representations and for text matching.A pioneering work using CNN to model sentencesis (Collobert and Weston, 2008).
They conductedconvolutions on sliding windows of a sentence andfinally use max pooling to form a sentence represen-tation.
Kalchbrenner et al (2014) introduce k-maxpooling and stacking of several CNNs as discussedin Section 1.Lu and Li (2013) developed a deep NN to matchshort texts, where interactions between componentswithin the two objects were considered.
These inter-actions were obtained via LDA (Blei et al, 2003).A two-dimensional interaction space is formed, thenthose local decisions will be sent to the correspond-ing neurons in upper layers to get rounds of fusion,finally logistic regression in the output layer pro-duces the final matching score.
Drawbacks of thisapproach are that LDA parameters are not optimizedfor the paraphrase task and that the interactions areformed on the level of single words only.Gao et al (2014) model interestingness betweentwo documents with deep NNs.
They map source-target document pairs to feature vectors in a latentspace in such a way that the distance between thesource document and its corresponding interestingtarget in that space is minimized.
Interestingnessis more like topic relevance, based mainly on theaggregate meaning of lots of keywords.
Addition-ally, their model is a document-level model and isnot multigranular.Madnani et al (2012) treated paraphrase relation-ship as a kind of mutual translation, hence combinedeight kinds of machine translation metrics includingBLEU (Papineni et al, 2002), NIST (Doddington,2002), TER (Snover et al, 2006), TERp (Snoveret al, 2009), METEOR (Denkowski and Lavie,2010), SEPIA (Habash and Elkholy, 2008), BAD-GER (Parker, 2008) and MAXSIM (Chan and Ng,2008).
These features are not multigranular; ratherthey are low-level only; high-level features ?
e.g., arepresentation of the entire sentence ?
are not con-sidered.Bach et al (2014) claimed that elementary dis-course units obtained by segmenting sentences playan important role in paraphrasing.
Their conclu-sion also endorses Socher et al (2011)?s and ourwork, for both take similarities between componentphrases into account.We discussed Socher et al (2011)?s RAE and Huet al (2014)?s ARC-I in Section 1.
In addition tosimilarity matrices there are two other important as-pects of (Socher et al, 2011).
First, the similaritymatrices are converted to a fixed size feature vectorby dynamic pooling.
We adopt this approach in Bi-CNN-MI; see Section 4.2 for details.Second, (Socher et al, 2011) is partially based onparsing as is some other work on paraphrase iden-tification (e.g., Wan et al (2006), Ji and Eisenstein(2013)).
Parsing is a potentially powerful tool foridentifying the important meaning units of a sen-tence, which can then be the basis for determiningmeaning equivalence.
However, reliance on parsingmakes these approaches less flexible.
For example,there are no high-quality parsers available for somedomains and some languages.
Our approach is inprinciple applicable for any domain and language.It is also unclear how we would identify compara-ble units in the parse trees of S1and S2if the parsetrees have different height and the sentences differ-ent lengths.
A key property of Bi-CNN-MI is that itis designed to produce units at fixed levels and onlyunits at the same level are compared with each other.3 Convolution sentence model CNN-SMOur network Bi-CNN-MI for paraphrase detection(Figure 1) consists of four parts.
On the left andon the right there are two multilayer NNs with sevenlayers, from ?initialized word embeddings: sentence1/2?
to ?k-max pooling?.
The weights of these twoNNs are shared.
This part of Bi-CNN-MI is basedon (Kalchbrenner et al, 2014) and we refer to it asconvolutional sentence model CNN-SM.Between the two CNN-SMs there is the interac-tion model CNN-IM, consisting of four feature ma-903Figure 1: The paraphrase identification architecture Bi-CNN-MI904trices (unigram, short ngram, long ngram, sentence).CNN-IM feeds into a logistic classifier that performsparaphrase detection.
See Sections 4 and 5 for thesetwo parts of Bi-CNN-MI.3.1 Wide convolutionWe use Kalchbrenner et al (2014)?s wide one-dimensional convolution.
Denoting the number oftokens of Sias |Si|, we convolve weight matrixM ?
Rd?mover sentence representation matrix S ?Rd?|Si|and generate a matrix C ?
Rd?
(|Si|+m?1)each column of which is the representation of an m-gram.
d is the dimension of word (and also ngram)embeddings.
m is filter width.Our motivation for using convolution is that af-ter training, a convolutional filter corresponds to afeature detector that learns to recognize a class ofm-grams that is useful for paraphrase detection.3.2 AveragingAfter convolution, to build simple relations acrossrows, each odd row and the row behind im-mediately are averaged, generating matrix A ?Rd2?(|Si|+m?1).
Namely:A = (Codd+Ceven)/2 (1)where Codd, Cevendenote the odd and even rows ofC, respectively.
Finally, this convolution layer willoutput matrix B whose jth column is defined thus:B:,j= tanh(A:,j+ bT) 0 ?
j < (|Si|+m?
1)(2)b is a bias vector with dimension d/2, same for eachcolumn.3.3 Dynamic k-max poolingWe use Kalchbrenner et al (2014)?s dynamic k-max pooling to extract features for variable-lengthsentences.
It extracts kdytop values from each di-mension after the first layer of averaging and ktop=4 top values after the top layer of averaging.
We setkdy= max(ktop, |Si|/2 + 1) (3)Thus, kdydepends on the length of Si.The sequence of layers in (Kalchbrenner et al,2014) is convolution, folding, k-max pooling, tanh.We experimented with this sequence and found thatafter k-max pooling many tanh units had an inputclose to 1, in the nondynamic range of the function(since the input is the addition of several values).This makes learning difficult.
We therefore changedthe sequence to convolution, averaging, tanh, k-maxpooling.
This makes it less likely that tanh units willbe saturated.We have described convolution, averaging and k-max pooling.
We can stack several blocks of thesethree layers to form deep architectures, as the twoblocks (marked ?first block?
and ?top block?)
in Fig-ure 1.4 Convolution interaction model CNN-IMAfter the introduction in the previous section of theCNN-SM part of our architecture for processing anindividual sentence, we now turn to the CNN-IM in-teraction model that computes the four feature ma-trices in Figure 1 to assess the interactions betweenthe two sentences.4.1 Feature matricesOne key innovation of our approach is multigranu-larity: we compute similarity between the two para-phrase candidates on multiple levels.
Specifically,we compute similarity at four levels in this paper:unigram, short ngram, long ngram and sentence.
Weuse notation l ?
{u, sn, ln, s} to refer to the four lev-els, and use Si,lto denote the matrix representingsentence Siat level l. For level l, we compute fea-ture matrices?Flas follows:?Fijl= exp(?||S1,l:,i?
S2,l:,j||22?)
(4)where ||S1,l:,i?
S2,l:,j||2is the Euclidean distance be-tween the representations of the ith item of S1andthe jth item of S2on level l. We set ?
= 2 (cf.
Wuet al (2013)).We do not use cosine because the magnitude ofthe activations of hidden units is important, not justthe overall direction; e.g., if S1,l:,iand S2,l:,jpoint inthe same direction, but activations are much largerin S2,l:,j, then the two vectors are very dissimilar.The lowest level feature matrix (l = u) is the un-igram similarity matrix?Fu.
It has size |S1| ?
|S2|.The feature entry?Fuijis the similarity between theith word of S1and the jth word of S2where each905word is represented by a d-dimensional word em-bedding (d = 100 in our experiments).The next level feature matrix is the short ngramsimilarity matrix?Fsn.
It has size (|S1|+msn?
1)?
(|S2|+msn?1) wheremsn= 3 is the filter width inthis convolution layer and |Si|+msn?1 is the num-ber of short ngrams in Si.
The feature entry?Fsnijisthe similarity between two d/2-dimensional vectorsrepresenting two short ngrams from S1and S2.We use multiple feature maps to improve the sys-tem performance.
Different feature maps are ex-pected to extract different kinds of sentence features,and can be implemented in the same convolutionlayer in parallel.
Specifically, we use fsn= 6 fea-ture maps on this level following Kalchbrenner et al(2014).
Thus, we actually compute six feature ma-trices?Fsn,i(i = 1, ?
?
?
, fsn), one for each pair offeature maps that share convolution weights whilederived from S1and S2respectively.
(Figure 1 onlyshows one of those six matrices.
)The next level feature matrix is the long ngramsimilarity matrix?Fln.
It has size (kdy,1+mln?
1)?
(kdy,2+ mln?
1) where kdy,i(Equation 3) is thek value in dynamic k-max pooling for sentence i,kdy,i+ mln?
1 is the number of long ngrams inSiand mln= 5 is the filter width in this convolu-tion layer.
The feature entry?Flnijis the similaritybetween two d/4-dimensional vectors representingtwo long ngrams from S1and S2.We use fln= 14 feature maps on this level fol-lowing Kalchbrenner et al (2014).
Thus, we com-pute 14 feature matrices?Fln,i(i = 1, ?
?
?
, fln), in away analogous to the fsn= 6 feature maps?Fsn,i.The last feature matrix is the sentence similaritymatrix?Fs.
It has size ktop?
ktopwhere ktop= 4is the parameter in k-max pooling at the last maxpooling layer.
The feature entry?Fsijis the similaritybetween two d/4-dimensional vectors computed bymax pooling from S1and S2.For l = s, there are also fln= 14 feature matrices?Fs,i(i = 1, ?
?
?
, fln), analogous to the fln= 14feature matrices?Fln,i.A general design principle of the architecture isthat we compute each interaction feature matrix be-tween two feature maps that share the same convo-lution weights.
Two feature maps learned with thesame filter will contain the same kinds of featuresderived from the input.4.2 Dynamic pooling of feature matricesAs sentence lengths vary, feature matrices?Flhavedifferent sizes, which makes it impossible to usethem directly as input of the last layer.That means we need to map?Fl?
Rr?cinto amatrix Flof fixed size r??
c?
(l ?
{u, sn, ln, s};r?, c?are parameters and are the same for all sen-tence pairs while r, c depend on |S1| and |S2|).
Dy-namic pooling divides?Flinto r?
?c?nonoverlapping(dynamic) pools and copies the maximum value ineach dynamic pool to Fl.
Our method is similar to(Socher et al, 2011), but preserves locality better.
?Flcan be split into equal regions only if r (resp.c) is divisible by r?(resp.
c?).
Otherwise, for r > r?and if r mod r?= b, the dynamic pools in the firstr?
?b splits each have?rr?
?rows while the remainingb splits each have?rr?
?+ 1 rows.
In Figure 2, a r ?c = 4 ?
5 matrix (left) is split into r??
c?= 3 ?
3dynamic pools (middle): each row is split into [1, 1,2] and each column is split into [1, 2, 2].If r < r?, we first repeat all rows until no fewerthan r?rows remain.
Then first r?rows are keptand split into r?dynamic pools.
The same princi-ple applies to the partitioning of columns.
In Fig-ure 2 (right), the areas with dashed lines and dottedlines are repeated parts for rows and columns, re-spectively; each cell is its own dynamic pool.5 Training5.1 Supervised trainingDynamic pooling gives us fixed size interaction fea-ture matrices for sentence, ngram and unigram lev-els.
As shown in Figure 1, the concatenation of thesefeatures (Fs, Fln, Fsnand Fu) is the input to a logis-tic regression layer for paraphrase classification.
Wehave now described all three parts of Bi-CNN-MI:CNN-SM, CNN-IM and logistic regression.Bi-CNN-MI with all its parameters ?
includ-ing word embeddings and convolution weights ?
istrained on MSRP.
We initialize embeddings withthose provided by Turian et al (2010)1(based onCollobert and Weston (2008)).
For layer sn, we havefsn= 6 feature maps and set filter width msn= 3.For layer ln, we have fln= 14 feature maps and setfilter width mln= 5 and ktop= 4.
Dynamic pooling1metaoptimize.com/projects/wordreprs/906Figure 2: Partition methods in dynamic pooling.
Original matrix with size 4?
5 is mapped into matrix with size 3?
3and matrix with size 6?
7, respectively.
Each dynamic pool is distinguished by a border of empty white space aroundit.Figure 3: Unsupervised architecture: CNN-LMsizes are 10?
10, 10?
10, 6?
6, 2?
2 for unigram,short ngram, long ngram and sentence, respectively.For training, we employ mini-batch of size 70, L2regularization with weight 5 ?
10?4and Adagrad(Duchi et al, 2011).5.2 Unsupervised pretrainingOne of the key contributions of this paper is the ar-chitecture CNN-LM shown in Figure 3.
CNN-LMis used to pretrain the convolutional filters on unla-beled data.
This addresses sparseness and limitedtraining data for paraphrase identification.The convolution sentence model CNN-SM (Sec-tion 3) is part of CNN-LM (?CNN-SM?
in Figure 3).The input to CNN-SM is the entire sentence (?thecat sat on the mat?
); its output (?sentence represen-tation?
in the leftmost rectangle in Figure 3 and thetwo grids labeled ?sentence representation?
in thetop layer of the top block in Figure 1) is concate-nated with a history consisting of the embeddingsof the h = 3 preceding words (?the?, ?cat?, ?sat?)
asthe input of a fully connected layer to generate a pre-dicted representation for the next word (?on?).
Weemploy noise-contrastive estimation (NCE) (Mnihand Teh, 2012; Mnih and Kavukcuoglu, 2013) tocompute the cost: the model learns to discriminatebetween true next words and noise words.
NCE al-lows us to fit unnormalized models making the train-ing time effectively independent of the vocabularysize.In experiments, CNN-LM is trained on unlabeledMSRP data and an additional 100,000 sentencesfrom English Gigaword (Graff et al, 2003).
In prin-ciple, sentences from any source, not just EnglishGigaword, can be used to train this model.
In NCE,20 noise words are sampled for each true example.So training has two parts: unsupervised, CNN-LM (Figure 3) and supervised, Bi-CNN-MI (Fig-ure 1).
In the first phase, the unsupervised trainingphase, we adopt a language modeling approach be-cause it does not require human labels and can uselarge corpora to pretrain word embeddings and con-volution weights.
The goal is to learn sentence fea-tures that are unbiased and reflect useful attributes ofthe input sentence.
More importantly, pretraining isuseful to relieve overfitting, which is a severe prob-lem when building deep NNs on small corpora likeMSRP (cf.
Hu et al (2014)).In the second phase, the supervised trainingphase, pretrained word embeddings and convolution907weights are tuned for optimal performance on PI.In CNN-LM, we have combined several architec-tural elements to pretrain a high-quality sentenceanalysis NN despite the lack of training data.
(i)Similar to PV-DM (Le and Mikolov, 2014), we in-tegrate global context (CNN-SM) and local context(the history of size h) into one model ?
althoughour global context consists only of a sentence, notof a paragraph or document.
(ii) Similar to workon autoencoding (Vincent et al, 2010), the outputthat is to be predicted is part of the input.
Au-toencoding is a successful approach to learning rep-resentations and we adapt it here to pretrain goodsentence representations.
(iii) A second successfulapproach to learning embeddings is neural networklanguage modeling (Bengio et al, 2003; Mikolov,2012).
Again, we adopt this by including in CNN-LM an ngram language modeling part to predict thenext word.
The great advantage of this type of em-bedding learning is that no labels are needed.
(iv)CNN-LM only adds one hidden layer over CNN-SM.
It keeps simple architecture like PV-DM (Leand Mikolov, 2014), CBOW (Mikolov et al, 2013)and LBL (Mnih and Teh, 2012), enabling the CNN-SM as main training target.In summary, the key contribution of CNN-LM isthat we pretrain convolutional filters.
Architecturalelements from the literature are combined to supporteffective pretraining of convolutional filters.6 Experiments6.1 Data set and evaluation metricsWe use the Microsoft Research Paraphrase Corpus(MSRP) (Dolan et al, 2004; Das and Smith, 2009).The training set contains 2753 true and 1323 falseparaphrase pairs; the test set contains 1147 and 578pairs, respectively.
For each triple (label, S1, S2) inthe training set we also add (label, S2, S1) to makebest use of the training data; these additions arenonredundant because the interaction feature matri-ces (Section 4.1) are asymmetric.
Systems are eval-uated by accuracy and F1.6.2 Paraphrase detection systemsSince we want to show that Bi-CNN-MI performsbetter than previous NN work, we compare withthree NN approaches: NLM, ARC and RAE (Ta-ble 1).2We also include the majority baseline(?baseline?)
and MT (Madnani et al, 2012).
RAE(Socher et al, 2011) and MT were discussed in Sec-tions 1 and 2.
We now briefly describe the otherprior work.Blacoe and Lapata (2012) compute the vectorrepresentation of a sentence from the neural lan-guage model (NLM) embeddings (computed basedon (Collobert and Weston, 2008)) of the words ofthe sentence as the sum of the word embeddings(NLM+), as the element-wise multiplication of theword embeddings (NLM), or by means of therecursive autoencoder (NLM RAE, Socher et al(2011)).
The representations of the two paraphrasecandidates are then concatenated as input to an SVMclassifier.
See Blacoe and Lapata (2012) for details.The ARC model (Hu et al, 2014) is a convolu-tional architecture similar to (Collobert and Weston,2008).
ARC-I is a Siamese architecture in whichtwo shared-weight convolutional sentence modelsare trained on the binary paraphrase detection task.Hu et al (2014) find that ARC-I is suboptimal inthat it defers the interaction between S1and S2tothe very end of processing: only after the vectorsrepresenting S1and S2have been computed does aninteraction occur.
To remedy this problem, they pro-pose ARC-II in which the Siamese architecture isreplaced by a multilayer NN that processes a singlerepresentation produced by interleaving S1and S2.We also evaluate Bi-CNN-MI?, an NN identicalto Bi-CNN-MI, except that it is not pretrained in un-supervised training.6.3 ResultsTable 1 shows that Bi-CNN-MI outperforms allother systems.
The comparison with Bi-CNN-MI?indicates that this is partly due to one major in-novation we introduced: unsupervised pretraining.Bi-CNN-MI?, the model without unsupervised pre-training, performs badly.
Thus, unsupervised train-ing is helpful to pretrain parameters in paraphrase2A reviewer suggests an additional experiment to directlyevaluate the importance of multigranularity: a ?system that putsall unigrams, short ngrams, long ngrams, and sentence repre-sentations into one interaction matrix.?
This would indeed bean interesting baseline, but there is no obvious way to conductthis experiment since vectors from different levels are not com-parable; e.g., they have different dimensionality.908method acc F1baseline 66.5 79.9NLM+ 69.0 80.1NLM 67.8 79.3NLM RAE 70.3 81.3ARC-I 69.6 80.3ARC-II 69.9 80.9RAE 76.7 83.6MT 77.4 84.1Bi-CNN-MI?
72.5 81.4Bi-CNN-MI 78.1 84.4Table 1: Performance of different systems on MSRPfeatures used acc F11 no features 66.5 79.92 + u: unigram 68.4 79.73 + sn: short ngram 75.3 82.84 + ln: long ngram 76.2 83.15 + s: sentence 73.4 82.36 ?
u: unigram 77.8 84.37 ?
sn: short ngram 76.3 83.58 ?
ln: long ngram 75.6 83.29 ?
s: sentence 77.6 84.210 all features 78.1 84.4Table 2: Analysis of impact of the four feature classes.Line 1: majority baseline.
Line 10: Bi-CNN-MI resultfrom Table 1.
Lines 2?5: Bi-CNN-MI when only onefeature class is used.
Line 6?9: ablation experiment: oneach line one feature class is removed.detection, especially when the training set is small.RAE also uses pretraining, but not as effectively asBi-CNN-MI as Table 1 indicates.
Hu et al (2014)also suggest that training complex NNs only withsupervised training runs the risk of overfitting on thesmall MSRP corpus.Table 2 looks at the relative importance of the fourfeature matrices shown in Figure 1.
(The unsuper-vised part of the training regime is not changed forthis experiment.)
The results indicate that levels snand ln are most informative: F1scores are highestif only these two levels are used (lines 3&4: 82.8,83.1) and performance drops most when they are re-moved (lines 7&8: 83.5, 83.2).Unigrams contribute little to overall performance(lines 2&6), probably because the paraphrases in thecorpus typically do not involve individual words (re-placing one word by its synonym); rather, the para-phrase relationship involves larger context, whichcan only be judged by the higher-level features.Just using the sentence matrix by itself performswell (line 5), but less well than using only levels snor ln (lines 3&4).
Most prior NN work on PI hastaken the sentence-level approach.
Our results indi-cate that combining this with the more fine-grainedcomparison on the ngram-level is superior.Removing the sentence matrix results in a smalldrop in performance (line 9).
The reason is that sen-tence representations are computed by k-max pool-ing from level ln.
Thus, we can roughly view thesentence-level feature matrix Fsas a subset of Fln.Adding (Madnani et al, 2012)?s MT metrics asinput to the Bi-CNN-MI logistic regression furtherimproves performance: accuracy of 78.4 and F1of84.6.7 Conclusion and future workWe presented the deep learning architecture Bi-CNN-MI for paraphrase identification (PI).
Basedon the insight that PI requires comparing two sen-tences on multiple levels of granularity, we learnmultigranular sentence representations using convo-lution and compute interaction feature matrices ateach level.
These matrices are then the input to alogistic classifier for PI.
All parameters of the model(for embeddings, convolution and classification) aredirectly optimized for PI.
To address the lack oftraining data, we pretrain the network in a novel wayfor a language modeling task.
Results on MSRP arestate of the art.In the future, we plan to apply Bi-CNN-MI to sen-tence matching, question answering and other tasks.AcknowledgmentsWe are grateful to Thang Vu, Irina Sergienya andSebastian Ebert for comments on earlier versionsof this paper.
This work was supported by Baidu(through a Baidu scholarship awarded to Wen-peng Yin) and by Deutsche Forschungsgemeinschaft(grant DFG SCHU 2246/8-2, SPP 1335).909ReferencesNgo Xuan Bach, Nguyen Le Minh, and Akira Shi-mazu.
2014.
Exploiting discourse information toidentify paraphrases.
Expert Systems with Applica-tions, 41(6):2832?2841.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155.William Blacoe and Mirella Lapata.
2012.
A compari-son of vector-based representations for semantic com-position.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 546?556.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Jane Bromley, James W Bentz, L?eon Bottou, IsabelleGuyon, Yann LeCun, Cliff Moore, Eduard S?ackinger,and Roopak Shah.
1993.
Signature verification usinga ?siamese?
time delay neural network.
InternationalJournal of Pattern Recognition and Artificial Intelli-gence, 7(04):669?688.Yee Seng Chan and Hwee Tou Ng.
2008.
Maxsim:A maximum similarity metric for machine translationevaluation.
In ACL, pages 55?62.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof the 25th international conference on Machine learn-ing, pages 160?167.Dipanjan Das and Noah A Smith.
2009.
Paraphrase iden-tification as probabilistic quasi-synchronous recogni-tion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP: Volume 1-Volume 1, pages 468?476.Li Deng, Geoffrey Hinton, and Brian Kingsbury.
2013.New types of deep neural network learning for speechrecognition and related applications: An overview.
InAcoustics, Speech and Signal Processing (ICASSP),2013 IEEE International Conference on, pages 8599?8603.Michael Denkowski and Alon Lavie.
2010.
Extendingthe meteor machine translation evaluation metric to thephrase level.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 250?253.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the second interna-tional conference on Human Language TechnologyResearch, pages 138?145.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:Exploiting massively parallel news sources.
In Pro-ceedings of the 20th international conference on Com-putational Linguistics, page 350.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.Jianfeng Gao, Patrick Pantel, Michael Gamon, XiaodongHe, Li Deng, and Yelong Shen.
2014.
Modeling inter-estingness with deep neural networks.
In Proceedingsof the 2013 Conference on Empirical Methods in Nat-ural Language Processing.David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.2003.
English gigaword.
Linguistic Data Consortium,Philadelphia.Nizar Habash and Ahmed Elkholy.
2008.
Sepia: sur-face span extension to syntactic dependency precision-based mt evaluation.
In Proceedings of the NIST met-rics for machine translation workshop at the associ-ation for machine translation in the Americas confer-ence, AMTA-2008.
Waikiki, HI.Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen.2014.
Convolutional neural network architectures formatching natural language sentences.
In Advances inNeural Information Processing Systems.Yangfeng Ji and Jacob Eisenstein.
2013.
Discriminativeimprovements to distributional sentence similarity.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics.Quoc V Le and Tomas Mikolov.
2014.
Distributed rep-resentations of sentences and documents.
In Proceed-ings of the 31th international conference on Machinelearning.Zhengdong Lu and Hang Li.
2013.
A deep architecturefor matching short texts.
In Advances in Neural Infor-mation Processing Systems, pages 1367?1375.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining machine translation metrics forparaphrase identification.
In Proceedings of the 2012NAACL-HLT, pages 182?190.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In Proceedings of Workshop atICLR.910Tom?a?s Mikolov.
2012.
Statistical language modelsbased on neural networks.
Ph.D. thesis, Ph.
D. the-sis, Brno University of Technology.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastive es-timation.
In Advances in Neural Information Process-ing Systems, pages 2265?2273.Andriy Mnih and Yee Whye Teh.
2012.
A fast and sim-ple algorithm for training neural probabilistic languagemodels.
In Proceedings of the 29th International Con-ference on Machine Learning, pages 1751?1758.N Neverova, C Wolf, GW Taylor, and F Nebout.
2014.Multi-scale deep learning for gesture detection and lo-calization.
In European Conference on Computer Vi-sion (ECCV) 2014 ChaLearn Workshop.
Zurich.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proceedings of the 40thACL, pages 311?318.Steven Parker.
2008.
Badger: A new machine translationmetric.
Metrics for Machine Translation Challenge.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of association for machine translationin the Americas, pages 223?231.Matthew G Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Ter-plus: paraphrase, se-mantic, and alignment enhancements to translationedit rate.
Machine Translation, 23(2-3):117?127.Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-pher D Manning, and Andrew Y Ng.
2011.
Dynamicpooling and unfolding recursive autoencoders for para-phrase detection.
In Advances in Neural InformationProcessing Systems, pages 801?809.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 384?394.Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, YoshuaBengio, and Pierre-Antoine Manzagol.
2010.
Stackeddenoising autoencoders: Learning useful representa-tions in a deep network with a local denoising cri-terion.
The Journal of Machine Learning Research,11:3371?3408.Stephen Wan, Mark Dras, Robert Dale, and C?ecile Paris.2006.
Using dependency-based features to take the?para-farce?
out of paraphrase.
In Proceedings of theAustralasian Language Technology Workshop, volume2006.Pengcheng Wu, Steven CH Hoi, Hao Xia, Peilin Zhao,Dayong Wang, and Chunyan Miao.
2013.
Onlinemultimodal deep similarity learning with applicationto image retrieval.
In Proceedings of the 21st ACM in-ternational conference on Multimedia, pages 153?162.ACM.911
