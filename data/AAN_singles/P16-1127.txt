Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1341?1350,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSequence-based Structured Prediction for Semantic ParsingChunyang Xiao and Marc DymetmanXerox Research Centre Europe6-8 Chemin de MaupertuisMeylan, 38240, Francechunyang.xiao, marc.dymetman@xrce.xerox.comClaire GardentCNRS, LORIA, UMR 7503Vandoeuvre-l`es-Nancy,F-54500, Franceclaire.gardent@loria.frAbstractWe propose an approach for semanticparsing that uses a recurrent neural net-work to map a natural language questioninto a logical form representation of aKB query.
Building on recent work by(Wang et al, 2015), the interpretable log-ical forms, which are structured objectsobeying certain constraints, are enumer-ated by an underlying grammar and arepaired with their canonical realizations.In order to use sequence prediction, weneed to sequentialize these logical forms.We compare three sequentializations: adirect linearization of the logical form, alinearization of the associated canonicalrealization, and a sequence consisting ofderivation steps relative to the underlyinggrammar.
We also show how grammati-cal constraints on the derivation sequencecan easily be integrated inside the RNN-based sequential predictor.
Our experi-ments show important improvements overprevious results for the same dataset, andalso demonstrate the advantage of incor-porating the grammatical constraints.1 IntroductionLearning to map natural language utterances (NL)to logical forms (LF), a process known as seman-tic parsing, has received a lot of attention recently,in particular in the context of building Question-Answering systems (Kwiatkowski et al, 2013;Berant et al, 2013; Berant and Liang, 2014).
Inthis paper, we focus on such a task where the NLquestion may be semantically complex, leading toa logical form query with a fair amount of compo-sitionality, in a spirit close to (Pasupat and Liang,2015).Given the recently shown effectiveness ofRNNs (Recurrent Neural Networks), in particu-lar Long Short Term Memory (LSTM) networks(Hochreiter and Schmidhuber, 1997), for perform-ing sequence prediction in NLP applications suchas machine translation (Sutskever et al, 2014) andnatural language generation (Wen et al, 2015),we try to exploit similar techniques for our task.However we observe that, contrary to those appli-cations which try to predict intrinsically sequen-tial objects (texts), our task involves producing astructured object, namely a logical form that istree-like by nature and also has to respect cer-tain a priori constraints in order to be interpretableagainst the knowledge base.In our case, building on the work ?Building aSemantic Parser Overnight?
(Wang et al, 2015),which we will refer to as SPO, the LFs are gener-ated by a grammar which is known a priori, and itis this grammar that makes explicit the structuralconstraints that have to be satisfied by the LFs.The SPO grammar, along with generating logi-cal forms, generates so-called ?canonical forms?
(CF), which are direct textual realizations of theLF that, although they are not ?natural?
English,transparently convey the meaning of the LF (seeFig.
1 for an example).Based on this grammar, we explore three differ-ent ways of representing the LF structure througha sequence of items.
The first one (LF Prediction,or LFP), and simplest, consists in just linearizingthe LF tree into a sequence of individual tokens;the second one (CFP) represents the LF through itsassociated CF, which is itself a sequence of words;and finally the third one (DSP) represents the LFthrough a derivation sequence (DS), namely thesequence of grammar rules that were chosen toproduce this LF.We then predict the LF via LSTM-based modelsthat take as input the NL question and map it into1341NL: article published in 1950 CF: article whose publication date is 1950 LF: get[[lambda,s,[filter,s,pubDate,=,1950]],article] DT: s0(np0 (np1 (typenp0), cp0 (relnp0, entitynp0)) DS: s0 np0 np1 typenp0 cp0 relnp0 entitynp0Figure 1: Example of natural language utterance (NL) from the SPO dataset and associated representa-tions considered in this work.
CF: canonical form, LF: logical form, DT: derivation tree, DS: derivationsequence.one of the three sequentializations.
In the threecases, the LSTM predictor cannot on its own en-sure the grammaticality of the predicted sequence,so that some sequences do not lead to well-formedLFs.
However, in the DSP case (in contrast toLFP and CFP), it is easy to integrate inside theLSTM predictor local constraints which guaranteethat only grammatical sequences will be produced.In summary, the contribution of our paper istwofold.
Firstly, we propose to use sequence pre-diction for semantic parsing.
Our experimentalresults show some significant improvements overprevious systems.
Secondly, we propose to predictderivation sequences taking into account gram-matical constraints and we show that the modelperforms better than sequence prediction modelsnot exploiting this knowledge.
These results areobtained without employing any reranking or lin-guistic features such as POS tags, edit distance,paraphrase features, etc., which makes the pro-posed methodology even more promising.2 Background on SPOThe SPO paper (Wang et al, 2015) proposes anapproach for quickly developing semantic parsersfor new knowledge bases and domains when notraining data initially exists.
In this approach,a small underlying grammar is used to generatecanonical forms and pair them with logical forms.Crowdsourcing is then used to paraphrase each ofthese canonical forms into several natural utter-ances.
The crowdsourcing thus creates a dataset(SPO dataset in the sequel) consisting of (NL, CF,LF) tuples where NL is a natural language ques-tion with CF and LF the canonical and the logicalform associated with this question.SPO learns a semantic parser on this datasetby firstly learning a log-linear similarity modelbased on a number of features (word matches,ppdb matches, matches between semantic typesand POSs, etc.)
between NL and the correspond-ing (CF, LF) pair.
At decoding time, SPO parsesa natural utterance NL by searching among thederivations of the grammar for one for which theprojected (CF, LF) is most similar to the NL basedon this log-linear model.
The search is based ona so-called ?floating parser?
(Pasupat and Liang,2015), a modification of a standard chart-parser,which is able to guide the search based on the sim-ilarity features.In contrast, our approach does not search amongthe derivations for the one that maximizes a matchwith the NL, but instead directly tries to predict adecision sequence that can be mapped to the LF.The SPO system together with its dataset werereleased to the public1and our work exploits thisrelease.3 Approach3.1 Grammars and Derivationss0: s(S) ?
np(S).np0: np(get[CP,NP]) ?
np(NP), cp(CP).np1: np(NP) ?
typenp(NP).cp0: cp([lambda,s,[filter,s,RELNP,=,ENTNP]]) ?
[whose], relnp(RELNP), [is], entitynp(ENTNP)....typenp0: typenp(article) ?
[article].relnp0: relnp(pubDate) ?
[publication, date]entitynp0: entitynp(1950) ?
[1950]....Figure 2: Some general rules (top) and domain-specific rules (bottom) in DCG format.The core grammatical resource released by SPOis a generic grammar connecting logical formswith canonical form realizations.
They also pro-vide seven domain-specific lexica that can be usedin combination with the generic grammar to obtaindomain-specific grammars which generate (LF,CF) pairs in each domain, in such a way thatLF can then be used to query the correspondingknowledge base.
While SPO also released a set of1https://github.com/percyliang/sempre1342s0np0np1 cp0typenp0  relnp0 entitynp0Figure 3: A derivation tree.
Its leftmost derivationsequence is [s0, np0, np1, typenp0, cp0, relnp0,entitynp0].typenp0 articlearticlerelnp0 publication datepubDateentitynp0 19501950cp0 whose publication date is 1950[lambda,s,[filter,s,pubDate,=,1950]np1 articlearticlenp0 article whose publication date is 1950get[[lambda,s,[filter,s,pubDate,=,1950]],article]s0 article whose publication date is 1950get[[lambda,s,[filter,s,pubDate,=,1950]],article]Figure 4: Projection of the derivation tree nodesinto (i) a canonical form and (ii) a logical form.Java-based parsers and generators for these gram-mars, for our own purposes we found it conve-nient to translate the grammars into the formalismof Definite Clause Grammars (Pereira and War-ren, 1980), a classical unification-based extensionof CFGs, which ?
through a standard Prolog in-terpreter such as SWIPL2?
provide direct sup-port for jointly generating textual realizations andlogical forms and also for parsing text into logi-cal forms; we found this translation process to berather straightforward and we were able to coverall of the SPO grammars.Figure 2 lists a few DCG rules, general rulesfirst, then lexical rules, for the SPO ?publications?domain.
Nonterminals are indicated in bold, ter-minals in italics.
We provide each rule with aunique identifier (e.g.
s0, np0, ...), which is ob-tained by concatenating the name of its head non-terminal with a position number relative to therules that may expand this nonterminal; we canthen consider that the nonterminal (e.g.
np) is the?type?
of all its expanding rules (e.g.
np0, np1,...).According to standard DCG notation, upper-2http://www.swi-prolog.org/case items S, NP, CP, RELNP, ENTNP de-note unification variables that become instantiatedduring processing.
In our case unificaion vari-ables range over logical forms and each nonter-minal has a single argument denoting a partiallyinstantiated associated logical form.
For instance,in the cp0 rule, relnp is associated with the log-ical form RELNP, entitynp with the logicalform ENTNP, and the LHS nonterminal cp is thenassociated with the logical form [lambda, s,[filter, s, RELNP, =, ENTNP]].3In Figure 3, we display a derivation treeDT (or simply derivation) relative to this gram-mar, where each node is labelled with a ruleidentifier.
This tree projects on the one handonto the canonical form article whose publica-tion date is 1950, on the other hand onto thelogical form get[[lambda,s,[filter,s,pubDate,=,1950]],article].Figure 4 shows how these projections are ob-tained by bottom-up composition.
For instance,the textual projection of node cp0 is obtained fromthe textual representations of nodes relnp0 and en-titynp0, according to the RHS of the rule cp0,while its logical form projection is obtained by in-stantiation of the variables RELNP and ENTNP re-spectively to the LFs associated with relnp0 andentitynp0.Relative to these projections, one may note afundamental difference between derivation treesDT and their projections CF and LF: while thewell-formedness of DT can simply be assessedlocally by checking that each node expansion isvalid according to the grammar, there is in princi-ple no such easy, local, checking possible for thecanonical or the logical form; in fact, in order tocheck the validity of a proposed CF (resp.
LF),one needs to search for some DT that projects ontothis CF (resp LF).
The first process, of course, isknown as ?parsing?, the second process as ?gener-ation?.
While parsing has polynomial complexityfor grammars with a context-free backbone suchas the ones considered here, deciding whether alogical form is well-formed or not could in princi-ple be undecidable for certain forms of LF compo-sition.43This logical form is written here in DCG list notation; inthe more ?Lispian?
format used by SPO, it would be written(lambda s (filter s RELNP = ENTNP)).4The term ?projection?
is borrowed from the notion ofbimorphism in formal language theory (Shieber, 2014) andrefers in particular to the fact that the overall logical form isconstructed by bottom-up composition of logical forms asso-1343To be able to leverage sequence prediction mod-els, we can associate with each derivation tree DTits leftmost derivation sequence DS, which corre-sponds to a preorder traversal of the tree.
For thetree of Figure 3, this sequence is [s0, np0, np1,typenp0, cp0, relnp0, entitynp0].
When the gram-mar is known (in fact, as soon as the CFG coreof the grammar is known), two properties of theDS hold (we omit the easy algorithms underlyingthese properties; they involve using a prefix of theDS for constructing a partial derivation tree in atop-down fashion):1. knowing the DS uniquely identifies thederivation tree.2.
knowing a prefix of the DS (for instance [s0,np0, np1, typenp0]) completely determinesthe type of the next item (here, this type iscp).The first property implies that if we are able to pre-dict DS, we are also able to predict DT, and there-fore also LF and CF.
The second property impliesthat the sequential prediction of DS is stronglyconstrained by a priori knowledge of the underly-ing grammar: instead of having to select the nextitem among all the possible rules in the grammar,we only have to select among those rules that areheaded by a specific nonterminal.
Under a simplecondition on the grammar (namely that there areno ?unproductive?
rules, rules that can never pro-duce an output5), following such constrained se-lection for the next rule guarantees that the deriva-tion sequence will always lead to a valid derivationtree.At this point, a theoretical observation shouldbe made: there is no finite-state mechanism on thesequence of rule-names that can control whetherthe next rule-name is valid or not.6The relevanceof that observation for us is that the RNNs that weuse are basically finite-state devices (with a hugenumber of states, but still finite-state), and there-fore we do not expect them in principle to be ableciated with lower nodes in the derivation tree.
In our DCGgrammars, this composition actually involves more complexoperations (such as ?beta-reduction?)
than the simple copy-ings illustrated in the small excerpt of Fig.
2.5The general grammar ensures a good coverage of possi-ble logical and canonical forms.
However, when this generalgrammar is used in particular domains, some rules are not rel-evant any more (i.e.
become ?unproductive?
), but these canbe easily eliminated at compile time.6This is easy to see by considering a CFG generating thenon finite-state language anbn.to always produce valid derivation sequences un-less they can exploit the underlying grammar forconstraining the next choice.3.2 Sequence prediction modelsIn all these models, we start from a natural utter-ance NL and we predict a sequence of target items,according to a common sequence prediction archi-tecture that will be described in section 3.3.3.2.1 Predicting logical form (LFP model)The most direct approach is to directly pre-dict a linearization of the logical form fromNL, the input question.
While an LF suchas that of Figure 1 is really a structured ob-ject respecting certain implicit constraints (bal-anced parentheses, consistency of the variablesbound by lambda expressions, and more gener-ally, conformity with the underlying grammar),the linearization treats it simply as a sequenceof tokens: get [ [ lambda s [ filters pubDate = 1950 ] ] article ].
Attraining time, the LFP model only sees such se-quences, and at test time, the next token in thetarget sequence is then predicted without takinginto account any structural constraints.
The train-ing regime is the standard one attempting to mini-mize the cross-entropy of the model relative to thelogical forms in the training set.3.2.2 Predicting derivation sequence (DSP-Xmodels)Rather than predicting LF directly, we can chooseto predict a derivation sequence DS, that is, a se-quence of rule-names, and then project it onto LF.We consider three variants of this model.DSP This basic derivation sequence predictionmodel is trained on pairs (NL, DS) with the stan-dard training regime.
At test time, it is possible forthis model to predict ill-formed sequences, whichdo not correspond to grammatical derivation trees,and therefore do not project onto any logical form.DSP-C This is a Constrained variant of DSPwhere we use the underlying grammar to constrainthe next rule-name.
We train this model exactly asthe previous one, but at test time, when samplingthe next rule-name inside the RNN, we reject anyrule that is not a possible continuation.DSP-CL This last model is also constrained, butuses a different training regime, with ConstrainedLoss.
In the standard learning regime (used for the1344Target sequence DS CF LFLength 10.5 11.8 47.0Vocabulary Size 106.0 55.8 59.9Table 1: Characteristics of different target se-quences.two previous models), the incremental loss whenpredicting the next item ytof the sequence is com-puted as ?
log p(yt), where p(yt) is the probabil-ity of ytaccording to the RNN model, normalized(through the computation of a softmax) over allthe potential values of yt(namely, here, all therules in the grammar).
By contrast, in the CLlearning regime, the incremental loss is computedas ?
log p?
(yt), where p?
(yt) is normalized onlyover the values of ytthat are possible continu-ations once the grammar-induced constraints aretaken into account, ignoring whatever weights theRNN predictor may (wrongly) believe should beput on impossible continuations.
In other words,the DSP-CL model incorporates the prior knowl-edge about well-formed derivation sequences thatwe have thanks to the grammar.
It computes theactual cross-entropy loss according to the under-lying generative process of the model that is usedonce the constraints are taken into account.3.2.3 Predicting canonical form (CFP model)The last possibility we explore is to predict thesequence of words in the canonical form CF, andthen use our grammar to parse this CF into its cor-responding LF, which we then execute against theknowledge base.7Table 1 provides length and vocabulary-sizestatistics for the LFP, DSP and CFP tasks.We see that, typically, for the different domains,DS is a shorter sequence than LF or CF, but its vo-cabulary size (i.e.
number of rules) is larger thanthat of LF or CF.
However DS is unique in allow-ing us to easily validate grammatical constraints.We also note that the CF is less lengthy than the7Although the general intention of SPO is to unambigu-ously reflect the logical form through the canonical form(which is the basis on which Turkers provide their para-phrases), we do encounter some cases where, although theCF is well-formed and therefore parsable by the grammar,several parses are actually possible, some of which do notcorrespond to queries for which the KB can return an answer.In these cases, we return the first parse whose logical formdoes return an answer.
Such situations could be eliminatedby refining the SPO grammar to a moderate extent, but wedid not pursue this.article?
?whose publication??,?
??,???
??,??
?LSTM encoding for the prefix of a sequence of items??
?
?whose publication dateFigure 5: Our neural network model which isshared between all the systems.
An MLP encodesthe sentence in unigrams and bigrams and pro-duces ub.
An LSTM encodes the prefix of thepredicted sequence generating ul,tfor each stept.
The two representations are then fed into a fi-nal MLP to predict the next choice of the targetsequence.LF, which uses a number of non ?word-like?
sym-bols such as parentheses, lambda variables, andthe like.3.3 Sequence prediction architecture3.3.1 Neural network modelThe goal of our neural network is to estimate theconditional probability p(y1, .
.
.
, yT?|x1, .
.
.
, xT)where (x1, .
.
.
, xT) is a natural language questionand (y1, .
.
.
, yT?)
is a target sequence (linearizedLF, CF or derivation sequence).
In all three cases,we use the same neural network model, which weexplain in this subsection.Suppose that the content of the NL is capturedin a real-valued vector ub, while the prefix of thetarget sequence up to time t is captured in anotherreal-valued vector ul,t.
Now, the probability of thetarget sequence given the input question can be es-timated as:p(y1, .
.
.
yT?|x1, .
.
.
, xT) =T?
?t=1p(yt|ub, y1, .
.
.
yt?1)=T?
?t=1p(yt|ub, ul,t?1)In all our systems, the ubcapturing the contentof the NL is calculated from the concatenation of avector u1reading the sentence based on unigramsand another vector u2reading the sentence basedon bigrams.
Mathematically, u1= tanh(W1v1)where v1is the 1-hot unigram encoding of the NL1345and u2= tanh(W2v2) where v2is its 1-hot bi-gram encoding.
Then ub= tanh(Wu), where uis the concatenation of u1and u2.
W1, W2and Ware among the parameters to be learnt.
For regular-ization purposes, a dropout procedure (Srivastavaet al, 2014) is applied to u1and u2.The prefix of the target sequence up to time tis modelled with the vector ul,tgenerated by thelatest hidden state of an LSTM (Hochreiter andSchmidhuber, 1997); LSTM is appropriate here inorder to capture the long distance dependencies in-side the target sequence.
The vector ul,tis thenconcatenated with ub(forming ublin the equationbelow) before passing through a two-layer MLP(Multi-Layer Perceptron) for the final prediction:p(yt+1|ul,t, ub) = softmax(W?2tanh(W?1ubl))Using deep structures such as this MLP for RNNprediction has been shown to be beneficial in pre-vious work (Pascanu et al, 2013).The overall network architecture is summarizedin Figure 5.
We train the whole network to min-imize the cross entropy between the predicted se-quence of items and the reference sequence.This network architecture can easily supportother representations for the input sentence thanunigrams and bigrams, as long as they are real-valued vectors of fixed length.
We can just con-catenate them with u1and u2and generate ubas previously.
In fact, in initial experiments, wedid concatenate an additional representation whichreads the sentence through an LSTM, but the per-formance was not improved.3.3.2 Decoding the target sequenceWe implemented a uniform-cost search algorithm(Russell and Norvig, 2003) to decode the best de-cision sequence as the sequence with the highestprobability.
The algorithm finishes in a reasonabletime for two reasons: 1) as indicated by Table 1,the vocabulary size of each domain is relativelysmall, and 2) we found that our model predicts rel-atively peaked distributions.
Of course, it wouldalso be easy to use a beam-search procedure, forsituations where these conditions would not hold.4 Experiments4.1 SetupWe conduct our experiments on the SPO dataset.To test the overall performance of a semanticparser, the SPO dataset contains seven domains fo-cusing on different linguistic phenomena such asmulti-arity relations, sublexical compositionalityetc.
The utterances in each domain are annotatedboth with logical forms (LFs) and canonical forms(CFs).
The number of such utterances vary from800 to 4000 depending on the domain.
The size oftraining data is indeed small but as the target vo-cabulary is always in the domain, thus very smallas well, it is actually possible to learn a reasonablesemantic parser.In the SPO dataset, the natural utterances weresplit randomly into 80%-20% for training and test,and we use the same sets.
We perform an addi-tional 80%-20% random split on the SPO train-ing data and keep the 20% as development setto choose certain hyperparameters of our model.Once the hyperparameters are chosen, we retrainon the whole training data before testing.For LFP experiments, we directly tokenize theLF, as explained earlier, and for CFP experimentswe directly use the CF.
For DSP experiments(DSP, DSP-C, DSP-CL) where our training dataconsist of (NL, DS) pairs, the derivation sequencesare obtained by parsing each canonical form usingthe DCG grammar of section 3.We compare our different systems to SPO.While we only use unigram and bigram featureson the NL, SPO uses a number of features of dif-ferent kinds: linguistic features on NL such asPOS tags, lexical features computing the similaritybetween words in NL and words in CF, semanticfeatures on types and denotations, and also fea-tures based on PPDB (Ganitkevitch et al, 2013).At test time, like SPO, we evaluate our systemon the proportion of questions for which the sys-tem is able to find the correct answer in the knowl-edge base.4.2 Implementation detailsWe choose the embedding vectors u1for unigramsand u2for bigrams to have 50 dimensions.
Thevector ubrepresenting the sentence content has200 dimensions.
The word embedding layer has100 dimensions, which is also the case of the hid-den layer of the LSTM ul,t.
Thus ublwhich is theconcatenation of uband ul,thas 300 dimensionsand we fix the next layer to ublto have 100 dimen-sions.
The model is implemented in Keras8on topof Theano (Bergstra et al, 2010).
For all the exper-8https://github.com/fchollet/keras1346iments, we train our models using rmsprop (Tiele-man and Hinton., 2012) as the backpropagation al-gorithm9.
We use our development set to selectthe number of training epochs, the dropout factorover unigrams representation and the dropout fac-tor over bigrams representation, by employing agrid search over these hyperparameters: epochs in{20, 40, 60}, unigrams dropout in {0.05, 0.1} andbigrams dropout in {0.1, 0.2, 0.3}.4.3 Experimental results4.3.1 Results on test dataTable 2 shows the test results of SPO and of ourdifferent systems over the seven domains.It can be seen that all of our sequence-based sys-tems are performing better than SPO by a largemargin on these tests.
When averaging over theseven domains, our ?worst?
system DSP scores at64.7% compared to SPO at 57.1%.We note that these positive results hold despitethe fact that DSP has the handicap that it maygenerate ungrammatical sequences relative to theunderlying grammar, which do not lead to inter-pretable LFs.
The LFP and CFP models, withhigher performance than DSP, also may generateungrammatical sequences.The best results overall are obtained by theDSP-C system, which does take into account thegrammatical constraints.
This model performsnot only considerably better than its DSP base-line (72.7% over 64.7%), but also better than themodels LFP and CFP.
Somewhat contrary to ourexpectations, the DSP-CL model, which exploitsconstraints not only during decoding, but also dur-ing training, performs somewhat worse than theDSP-C, which only exploits them during decod-ing.We note that, for all the sequence based models,we strictly base our results on the performance ofthe first sequence predicted by the model.
It wouldprobably be possible to improve them further byreranking n-best sequence lists using a set of fea-tures similar to those used by SPO.4.4 Analysis of results4.4.1 Grammatical errorsWe just observed that CFP and LFP perform wellon test data although the sequences generated are9All the hyperparameters of rmsprop as well as options forinitializing the neural network are left at their default valuesin Keras.Basketball Publication HousingLFP 6.6 3.7 1.6CFP 1.8 1.9 2.2DSP 9.5 11.8 5.8DSP-C(L) 0.0 0.0 0.0Table 3: Grammatical error rate of different sys-tems on test.not guaranteed to be grammatical.
We analysedthe percentage of grammatical errors made bythese models and also by DSP for three domains,which we report in Table 3.10The table shows that LFP and especially CFPmake few grammatical errors while DSP makesthem more frequently.
For DSP-C and DSP-CL,the error rate is always 0 since by construction,the derivations must be well-formed.
Note that asDSP is not constrained by prior knowledge aboutthe grammar, the grammatical error rate can behigh ?
even higher than CFP or LFP because DSPtypically has to choose among more symbols, seeTable 1.4.4.2 Difference between DSP-C and DSP-CLWe observed that the DSP-CL model performssomewhat worse than DSP-C in our experiments.While we were a bit surprised by that behav-ior, given that the DSP-CL has strong theoreti-cal motivations, let us note that the two modelsare quite different.
To stress the difference, sup-pose that, for a certain prediction step, only tworules are considered as possible by the grammar,among the many rules of the grammar.
Supposethat the LSTM gives probabilities 0.004 and 0.006respectively to these two rules, the rest of themass being on the ungrammatical rules.
Whilethe DSP-C model associates respective losses of?
log 0.004,?
log 0.006 with the two rules, theDSP-CL model normalizes the probabilites first,resulting in smaller losses ?
log 0.4,?
log 0.6.As we choose the best complete sequence dur-ing decoding, it means that DSP-C will be morelikely to prefer to follow a different path in sucha case, in order not to incur a loss of at least?
log 0.006.
Intuitively, this means that DSP-C will prefer paths where the LSTM on its own10Our DCG permits to compute this error rate directly forcanonical forms and derivation sequences.
For logical forms,we made an estimation by executing them against the knowl-edge base and eliminating the cases where the errors are notdue to the ungrammaticality of the logical form.1347Basketball Social Publication Blocks Calendar Housing Restaurants AvgSPO 46.3 48.2 59.0 41.9 74.4 54.0 75.9 57.1LFP 73.1 70.2 72.0 55.4 71.4 61.9 76.5 68.6CFP 80.3 79.5 70.2 54.1 73.2 63.5 71.1 70.3DSP 71.6 67.5 64.0 53.9 64.3 55.0 76.8 64.7DSP-C 80.5 80.0 75.8 55.6 75.0 61.9 80.1 72.7DSP-CL 80.6 77.6 70.2 53.1 75.0 59.3 74.4 70.0Table 2: Test results over different domains on SPO dataset.
The numbers reported correspond to the pro-portion of cases in which the predicted LF is interpretable against the KB and returns the correct answer.LFP = Logical Form Prediction, CFP = Canonical Form Prediction, DSP = Derivation Sequence Predic-tion, DSP-C = Derivation Sequence constrained using grammatical knowledge, DSP-CL = DerivationSequence using a loss function constrained by grammatical knowledge.gives small probability to ungrammatical choices,a property not shared by DSP-CL.
However, amore complete understanding of the differencewill need more investigation.5 Related Work and DiscussionIn recent work on developing semantic parsersfor open-domain and domain-specific question an-swering, various methods have been proposed tohandle the mismatch between natural languagequestions and knowledge base representations in-cluding, graph matching, paraphrasing and em-beddings techniques.Reddy et al (2014) exploits a weak supervisionsignal to learn a mapping between the logical formassociated by a CCG based semantic parser withthe input question and the appropriate logical formin Freebase (Bollacker et al, 2008).Paraphrase-based approaches (Fader et al,2013; Berant and Liang, 2014) generate variantsof the input question using a simple hand-writtengrammar and then rank these using a paraphrasemodel.
That is, in their setting, the logical formassigned to the input question is that of the gen-erated sentence which is most similar to the inputquestion.Finally, Bordes et al (2014b; 2014a) learn asimilarity function between a natural languagequestion and the knowledge base formula encod-ing its answer.We depart from these approaches in that welearn a direct mapping between natural languagequestions and their corresponding logical form orequivalently, their corresponding derivation andcanonical form.
This simple, very direct ap-proach to semantic parsing eschews the needfor complex feature engineering and large exter-nal resources required by such paraphrase-basedapproaches as (Fader et al, 2013; Berant andLiang, 2014).
It is conceptually simpler than thetwo steps, graph matching approach proposed byReddy et al (2014).
And it can capture much morecomplex semantic representations than Bordes etal.
(2014b; 2014a)?s embeddings based method.11At a more abstract level, our approach differsfrom previous work in that it exploits the fact thatlogical forms are structured objects whose shape isdetermined by an underlying grammar.
Using thepower of RNN as sequence predictors, we learn topredict, from more or less explicit representationsof this underlying grammar, equivalent but differ-ent representations of a sentence content namely,its canonical form, its logical form and its deriva-tion sequence.We observe that the best results are obtained byusing the derivation sequence, when also exploit-ing the underlying grammatical constraints.
How-ever the results obtained by predicting directly thelinearization of the logical form or canonical formare not far behind; we show that often, the pre-dicted linearizations actually satisfy the underly-ing grammar.
This observation can be related tothe results obtained by Vinyals et al (2014), whouse an RNN-based model to map a sentence tothe linearization of its parse tree,12and find thatin most cases, the predicted sequence produceswell-balanced parentheses.
It would be interest-11In (Bordes et al, 2014b; Bordes et al, 2014a), the logicalforms denoting the question answers involve only few RDFtriples consisting of a subject, a property and an object i.e., abinary relation and its arguments.12Note a crucial difference with our approach.
While intheir case the underlying (?syntactic?)
grammar is only par-tially and implicitly represented by a set of parse annotations,in our case the explicit (?semantic?)
grammar is known a pri-ori and can be exploited as such.1348ing to see if our observation would be maintainedfor more complex LFs than the ones we tested on,where it might be more difficult for the RNN topredict not only the parentheses, but also the de-pendencies between several lambda variables in-side the overall structure of the LF.6 Conclusion and Future WorkWe propose a sequence-based approach for thetask of semantic parsing.
We encode the targetlogical form, a structured object, through threetypes of sequences: direct linearization of the log-ical form, canonical form, derivation sequence inan underlying grammar.
In all cases, we obtaincompetitive results with previously reported ex-periments.
The most effective model is one usingderivation sequences and taking into account thegrammatical constraints.In order to encode the underlying derivationtree, we chose to use a leftmost derivation se-quence.
But there are other possible choices thatmight make the encoding even more easily learn-able by the LSTM, and we would like to explorethose in future work.In order to improve performance, other promis-ing directions would involve adding re-rerankingtechniques and extending our neural networkswith attention models in the spirit of (Bahdanauet al, 2015).AcknowledgementsWe would like to thank Guillaume Bouchard forhis advice on a previous version of this work, aswell as the anonymous reviewers for their con-structive feedback.
The authors gratefully ac-knowledge the support of the Association Na-tionale de la Recherche Technique (ANRT), Con-vention Industrielle de Formation par la Recherche(CIFRE) No.
2014/0476 .ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In International Con-ference on Learning Representations (ICLR).J.
Berant and P. Liang.
2014.
Semantic parsing viaparaphrasing.
In Annual Meeting for the Associationfor Computational Linguistics (ACL).J.
Berant, A. Chou, R. Frostig, and P. Liang.
2013.Semantic parsing on freebase from question-answerpairs.
In Empirical Methods in Natural LanguageProcessing (EMNLP).James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, and Yoshua Bengio.
2010.Theano: a CPU and GPU math expression compiler.In Proceedings of the Python for Scientific Comput-ing Conference (SciPy), jun.
Oral.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In SIGMOD Conference, pages1247?1250.Antoine Bordes, Sumit Chopra, and Jason Weston.2014a.
Question answering with subgraph embed-dings.
In Empirical Methods in Natural LanguageProcessing (EMNLP), pages 615?620.Antoine Bordes, Jason Weston, and Nicolas Usunier.2014b.
Open question answering with weakly su-pervised embedding models.
European Conferenceon Machine Learning and Principles and Practiceof Knowledge Discovery (ECML-PKDD).Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2013.
Paraphrase-driven learning for open questionanswering.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 1608?1618.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
Ppdb: The paraphrasedatabase.
In North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (NAACL-HLT), pages 758?764.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural Computation,9(8):1735?1780.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, andLuke S. Zettlemoyer.
2013.
Scaling semanticparsers with on-the-fly ontology matching.
In Em-pirical Methods in Natural Language Processing,(EMNLP), pages 1545?1556.Razvan Pascanu, C?aglar G?ulc?ehre, Kyunghyun Cho,and Yoshua Bengio.
2013.
How to construct deeprecurrent neural networks.
CoRR, abs/1312.6026.1349Panupong Pasupat and Percy Liang.
2015.
Compo-sitional semantic parsing on semi-structured tables.In Annual Meeting of the Association for Compu-tational Linguistics and the 7th International JointConference on Natural Language Processing of theAsian Federation of Natural Language Processing,ACL Volume 1: Long Papers.Fernando C.N.
Pereira and David H.D.
Warren.
1980.Definite clause grammars for language analysis asurvey of the formalism and a comparison with aug-mented transition networks.
Artificial Intelligence,13:231 ?
278.Siva Reddy, Mirella Lapata, and Mark Steedman.2014.
Large-scale semantic parsing withoutquestion-answer pairs.
Transactions of the Associ-ation for Computational Linguistics, 2:377?392.Stuart J. Russell and Peter Norvig.
2003.
Artificial In-telligence: A Modern Approach.
Pearson Education,2 edition.Stuart M. Shieber.
2014.
Bimorphisms and syn-chronous grammars.
J.
Language Modelling,2(1):51?104.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
Journal of Machine Learning Re-search (JMLR), 15(1):1929?1958.Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Process-ing Systems (NIPS), pages 3104?3112.T.
Tieleman and G. E. Hinton.
2012.
Lecture 6.5-rmsprop: Divide the gradient by a running averageof its recent magnitude.Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey E. Hinton.
2014.Grammar as a foreign language.
Neural InformationProcessing Systems (NIPS).Yushi Wang, Jonathan Berant, and Percy Liang.
2015.Building a semantic parser overnight.
In AnnualMeeting of the Association for Computational Lin-guistics and the 7th International Joint Conferenceon Natural Language Processing of the Asian Feder-ation of Natural Language Processing, ACL Volume1: Long Papers, pages 1332?1342.Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-hao Su, David Vandyke, and Steve J.
Young.
2015.Semantically conditioned lstm-based natural lan-guage generation for spoken dialogue systems.
InEmpirical Methods in Natural Language Processing(EMNLP).1350
