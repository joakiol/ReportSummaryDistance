Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 2?7,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsRobust Cross-Domain Sentiment Analysis for Low-Resource LanguagesJakob Elming Dirk Hovy Barbara PlankCentre for Language TechnologyUniversity of Copenhagenzmk867@hum.ku.dk,{dirk,bplank}@cst.dkAbstractWhile various approaches to domain adap-tation exist, the majority of them requiresknowledge of the target domain, and ad-ditional data, preferably labeled.
For alanguage like English, it is often feasibleto match most of those conditions, but inlow-resource languages, it presents a prob-lem.
We explore the situation when nei-ther data nor other information about thetarget domain is available.
We use twosamples of Danish, a low-resource lan-guage, from the consumer review domain(film vs. company reviews) in a sentimentanalysis task.
We observe dramatic perfor-mance drops when moving from one do-main to the other.
We then introduce asimple offline method that makes modelsmore robust towards unseen domains, andobserve relative improvements of morethan 50%.1 IntroductionSentiment analysis, the task of determining thepolarity of a text, is a valuable tool for gather-ing information from the vast amount of opin-ionated text produced today.
It is actively usedin reputation management and consumer assess-ment (Amig?o et al., 2012; Amig?o et al., 2013).While supervised approaches achieve reasonableperformance (Mohammad et al., 2013), they aretypically highly domain-dependent.
In fact, mov-ing from one (source) domain to a different (tar-get) domain will often lead to severe performancedrops (Blitzer et al., 2007; Daum?e et al., 2010).This is mainly due to the models overfitting thesource (training) data, both in terms of its la-bel and word distribution.
The task of overcom-ing this tendency is known as domain adaptation(DA) (Blitzer et al., 2007; Daum?e et al., 2010).There are three different approaches to DA: inSupervised DA, labeled training data for the targetdomain exists, in Unsupervised DA, data for thetarget domain exists, but it is unlabeled.
A third,less investigated scenario is Blind DA: the targetdomain is not known at all in advance.
Super-vised DA effectively counteracts domain-bias byincluding labeled data from the target domain dur-ing training, thus preventing overfitting to both thelabel and the word distribution of the source.
Un-supervised methods usually rely either on externaldata, in the form of gazetteers, dictionaries, or onunlabeled data from the target domain.
While theydo not prevent overfitting to the source domain?slabel distribution, the additional data acts as a reg-ularizer by introducing a larger vocabulary.However, both cases presuppose that we alreadyknow the target domain and have data from it.
Inmany real-world settings, these conditions are notmet, especially when dealing with low-resourcelanguages.
We thus need to regularize our modelsindependent of the possible target domains.
Ef-fectively, this means that we need to prevent ourmodels from memorizing the observed label distri-bution, and from putting too much weight on fea-tures that are predictive in the source domain, butmight not even be present in the target domain.In this paper, we investigate sentiment analysisfor Danish, a low-resource language, and thereforeapproach it as a Blind DA problem.
We performexperiments on two types of domains, namely re-views for movies and companies.
The challengelies in the fact that the label distribution (posi-tive, negative, neutral) changes dramatically whenmoving from one domain to the other, and manyhighly predictive words in the company domain(e.g., ?reliable?)
are unlikely to carry over to themovie domain, and vice versa.
To the best of ourknowledge, this is the first study to perform senti-ment analysis for Danish, a low-resource languagewhere relevant resources like polarity dictionaries2are hard to come by.We present a simple offline-learning version in-spired by previous work on corruptions (S?gaard,2013), which also addresses the sparsity of avail-able training data.
Our method introduces a rela-tive improvement on out-of-domain performanceby up to 54%.2 Robust LearningThe main idea behind robust learning is to steer themodel away from overfitting the source domain.Overfitting can occur either by1.
putting too much weight on certain features(which might not be present in the target do-main), or2.
over-using certain labels (since the label dis-tribution on the target domain might differ).One approach that has been proven to re-duce overfitting is data corruption, also known asdropout training (S?gaard and Johannsen, 2012;S?gaard, 2013), which is a way of regularizingthe model by randomly leaving out features.
In-tuitively, this approach can be viewed as coercingthe learning algorithm to rely on more general, butless consistent features.
Rather than learning tomainly trust the features that are highly predictivefor the given training data, the algorithm is encour-aged to use the less predictive features, since thehighly predictive features might be deleted by thecorruption.
Most prior work on dropout regular-ization (S?gaard and Johannsen, 2012; Wang andManning, 2012; S?gaard, 2013) has used onlinecorruptions, i.e., the specific dropout function isintegrated into the learning objective and thus tiedto the specific learner.
Here, we propose a simpleapproximation, i.e., a wrapper function that cor-rupts instances in an off-line fashion based on theweights learned from a base model.
The advan-tage is that it can be used for any learning func-tion, thereby abstracting away from the underlyinglearner.2.1 Our approachOur off-line feature corruption algorithm works asfollows:1. train an uncorrupted (base) model,2.
create k copies of the training data instances,3.
corrupt copies based on the feature weights ofthe base model and an exponential function(described below), and4.
train a new model on the corrupted trainingdata.The advantages of this algorithm compared toonline corruption are1.
it is a wrapper method, so it becomes veryeasy to move to a different learning algo-rithm, and2.
corruption is done based on knowledge froma full, uncorrupted model, which provides abetter picture of the overfitting.This comes, however, at the cost of longer trainingtimes, but in a low-resource language training timeis less of an issue.Specifically, multiple copies of the training dataare used in the corrupted training stage.
This re-sults in each data point appearing in different, cor-rupted versions, as visualized in Figure 1.
Thecopying process retains more of the information inthe training data, since it is unlikely that the samefeature is deleted in each copy.
In our experiments,we used k=5.
Larger values of k resulted in longertraining times without improving performance.1 11 1Original1 1 11 1 111111Corrupted Copies?!?!
?111Figure 1: Example of an original feature vectorand its multiple corrupted copies.We experiment with a random and a biasedcorruption approach.
The first approach (S?gaardand Johannsen, 2012) does not utilize the featureweight information from the base model, but ran-domly deletes 10% of the features.
We use thisapproach to test whether an effect is merely theresult of deleting features.The biased approach, on the other hand, tar-gets the most predictive features in the base modelfor deletion.
We use a function that increasesthe probability of deleting a feature exponentially30?25?50?75?100?-??0.33?
-??0.23?
-??0.13?
-??0.03?
0.07?
0.17?
0.27?
0.37?%?Feature?weight?Figure 2: The corruption function conditioning theprobability of deleting a feature in a positive in-stance on its weight in the Scope baseline model.with its model weight.
That is, a highly predic-tive feature (with a high weight in the model) willbe more likely to be deleted.
A feature with alow weight, on the other hand, has a much lowerchance of being deleted.
Figure 2 visualizes theexponential corruption function used.
The func-tion assigns the lowest weighted feature of themodel zero likelihood of deletion, and the highestweighted feature a 0.9 likelihood of deletion.
Inorder to mainly corrupt the highly predictive fea-tures, the exponential function is shifted to an areawith a steeper gradient.
That is, instead of scal-ing to the exponential function between 0 and 1, itis scaled to the area between -3 and 2 (parametersset experimentally on the development set).
Thecorruption probability pcorof deleting a feature fgiven a category c is defined aspcor(f |c) =exp(w(f |c)?wmin(c)wmax(c)?wmin(c)?5?3)?exp(?3)exp(2)?exp(?3)?
0.9(1)with w(f |c) being the weight of f given the in-stance category c in the model, and wmin(c) andwmax(c) being the lowest and highest weights ofthe model respectively for category c.3 ExperimentsOur experiments use Danish reviews from two do-mains: movies and companies.
The specificationsof the data sets are listed in Table 1 and Figure 3.The two data sets differ considerably in data sizeand label distribution.DOMAIN SPLIT REVIEWS WORDSScope Train 8,718 749,952Dev 1,198 107,351Test 2,454 210,367Total 12,370 1,067,670Trustpilot Train 170,137 7,180,160Dev 23,958 1,000,443Test 48,252 2,040,956Total 242,347 10,221,559Table 1: Overview of data set and split sizes innumber of reviews and number of words.3.1 Data preparationThe movie reviews are downloaded from a Dan-ish movie website, www.scope.dk.
They con-tain reviews of 829 movies, each rated on a scalefrom 1 to 6 stars.
The company reviews aredownloaded from a Danish consumer review web-site, www.trustpilot.dk.
They consist of re-views of 19k companies, each rated between 1 and5 stars.Similar to prior work on sentiment analy-sis (Blitzer et al., 2007), the star ratings are binnedinto the three standard categories; positive, neu-tral, and negative.
For the Scope data, a 6 star rat-ing is considered positive, a 3 or 4 rating neutral,and a 1 star rating negative.
2 and 5 star ratings areexcluded to retain more distinct categories.
For theTrustpilot data, 5 star reviews are categorized aspositive, 3 stars as neutral, and 1 star as negative.Similar to Scope data, 2 and 4 stars are excluded.0%25%50%75%100%scope trustpilot84.85%27.36%5.40%60.85%9.75%11.79%negative neutral positiveFigure 3: Label distribution in the two data sets.Apart from the difference in size, the two datasets also differ in the distribution of categories (seeFigure 3).
This means that a majority label base-line estimated from one would perform horribly on4- N-gram presence for token lengths 1 to 4- Skip-grams (n-gram with one middle word replaced by *) presence for token lengths 3 to 4- Character n-gram presence for entire document string for token lengths 1 to 5- Brown clusters (Brown et al., 1992; Liang, 2005) estimated on the source training data- Number of words with only upper case characters- Number of contiguous sequences of question marks, exclamation marks, or both- Presence of question mark or exclamation mark in last word- Number of words with characters repeated more than two times e.g.
?sooooo?- Number of negated contexts using algorithm described in the text- Most positive, most negative, or same amount of polar words according to a sentiment lexiconTable 2: Feature set description.the other domain.
For instance, the majority base-line on Scope (assigning neutral to all instances)achieves a 5% accuracy on Trustpilot data.
Sim-ilarly, the Trustpilot majority baseline obtains anaccuracy of 27% on Scope data by always assign-ing positive.We choose not to balance the data sets, in keep-ing with the blind DA setup.
Knowing the targetlabel distribution can help greatly, but we can as-sume no prior knowledge about that.
In fact, thedifference in label distribution is one of the ma-jor challenges when predicting on out-of-domaindata.3.2 FeaturesThe features we use (described in Table 2) areinspired by the top performing system from theSemEval-2013 task on Twitter sentiment analy-sis (Mohammad et al., 2013).One main difference is that Mohammad et al.
(2013) had several high-quality sentiment lexiconsat their disposal, shown to be effective.
Workingwith a low-resource language, we only have ac-cess to a single lexicon created by an MA student(containing 2248 positive and 4736 negative wordforms).
Our lexicon features are therefore simpler,i.e., based on whether words are considered pos-itive or negative in the lexicon, as opposed to thescore-based features in Mohammad et al.
(2013).We adopted the simple negation scope reso-lution algorithm directly from Mohammad et al.(2013).
Anything appearing in-between a negationtoken1and the first following punctuation mark isconsidered a negated context.
This works well forEnglish, but Danish has different sentence adver-bial placement, so the negation may also appear1We use the following negation markers: ikke, ingen, in-tet, ingenting, aldrig, hverken.
n?ppe.after the negated constituent.
This simple algo-rithm is therefore less likely to be beneficial in aDanish system.
We plan to extend the system forbetter negation handling in future work.3.3 CorruptionThe corruption happens at the feature-instancelevel.
When we refer to the deletion of a featurein the following, it does not mean the deletion ofthis feature throughout the training data, but thedeletion of a single instance in a feature vector (cf.Figure 1).Corrupting the Scope data deleted 9.24% of allfeature instances in the training data.
Most fea-tures are deleted from positive instances (16.7%of all features) and least from the majority neutralinstances (6.5% of all features).
Only 9.4% of theminority class negative are deleted.For Trustpilot, the corruption deleted 11.73%of the feature instances.
The pattern is the samehere, though more extreme.
The majority positiveclass has the fewest features removed (2.2%), theminority class neutral has 22.8% of its featuresdeleted, and the negative class has an overwhelm-ing 35.6% of its features deleted.The fact that the corruption function does nottake the weight distribution of the individual la-bels into account, and therefore corrupts the dataof some labels much more than others, does proveto be a problem.
We will get back to this in theresults section.4 ResultsTable 3 shows the results of the experiments.
Wereport both accuracy and the average f-score forpositive and negative instances (AF).AF is the official SemEval-2013 metric (Nakovet al., 2013).
It offers a more detailed insight into5In-domain Out-of-domainSystem Dev set Test set Dev set Test setAcc.
AF Acc.
AF Acc.
AF Acc.
AFScope baseline 84.2 75.6 82.4 72.1 35.5 43.3 36.0 44.3Scope random corrupt 83.1 72.9 82.7 72.8 35.7 43.9 36.2 44.5Scope biased corrupt 82.7 72.6 81.5 70.6 55.5 48.6 55.5 44.9Trustpilot baseline 94.8 91.8 94.3 91.2 39.9 45.0 39.9 46.2Trustpilot random corrupt 94.8 91.7 94.4 91.4 39.8 45.6 40.0 46.0Trustpilot biased corrupt 93.7 89.0 93.4 89.5 43.6 45.7 43.4 44.7Table 3: Evaluation on development and test sets measured in accuracy (Acc.)
and the average f-scorefor positive and negative instances (AF).the model?s performance on the two ?extreme?classes, but it is highly skewed, since it ignores theneutral label.
As we have seen in our data, thiscan make up the majority of the instances.
Ac-curacy has the advantage that it provides a clearpicture of how often the system makes a correctprediction, but can be harder to interpret when thedata sets are highly skewed in favor of one class.The results show that randomly corrupting thedata (cf.
S?gaard and Johannsen (2012), Sec.
5)does not have much influence on the model.
Per-formance on in- and out-of-domain data is similarto the baseline system.
This indicates that we cannot just delete any features to help domain adapta-tion.The biased corruption model, on the other hand,makes informed choices about deleting features.As expected, this leads to a drop on in-domaindata, since we are underfitting the model.
Con-sidering that the algorithm is targeting the mostimportant features for this particular domain, thedrop is relatively small, though.
The percentageof features deleted is roughly the same as the 10%for the random system (see section 3.3).With the exception of AF on Trustpilot test,our biased corruption approach always increasesout-of-domain performance.
The increase is es-pecially notable when the model is trained on thesmall domain, Scope.
On both test and develop-ment, the corruption approach increases accuracymore than 50%.
On the AF measure, the increaseis smaller, which indicates that most of the in-crease stems from the neutral category.
On thetest set, the f-score for positive labels increasesfrom 49.1% to 71.2%, neutral increases from13.5% to 18.4%, but negative decreases from39.4% to 27.5%.
The fact that f-score decreases onnegative indicates that the corruption algorithmis too aggressive for this category.
We previouslysaw that this was the category where 35% of thefeatures are deleted.The lower degree of overfitting in the corruptedmodel is also reflected in the overall label distri-bution.
For the Scope system, the training datahas a negative/neutral/positive distribution (in per-centages) of 27/61/12.
The baseline predictionson the Trustpilot data has a very similar distribu-tion of 30/63/7, while the corrupted system resultsin a very different distribution of 52/35/13, whichis more similar to the Trustpilot gold distributionof 85/5/10.
The KL divergence between the base-line system and the Trustpilot data is 1.26, whilefor the corrupted system it is 0.46.5 Related WorkThere is a large body of prior work on sen-timent analysis (Pang and Lee, 2008), rangingfrom work on well-edited newswire data usingthe MPQA corpus (Wilson et al., 2005), to Ama-zon reviews (Blitzer et al., 2007), blogs (Kessleret al., 2010) and user-generated content such astweets (Mohammad et al., 2013).
All of thesestudies worked with English, while this study ?
tothe best of our knowledge ?
is the first to presentresults for Danish.As far as we are aware of, the only related workon Danish is Hardt and Wulff (2012).
In their ex-ploratory paper, they investigate whether user pop-ulations differ systematically in the way they ex-press sentiment, finding that positive ratings arefar more common in U.S. reviews than in Danishones.
However, their paper focuses on a quantita-tive analysis and a single domain (movie reviews),while we build an actual sentiment classificationsystem that performs well across domains.Data corruption has been used for other NLP6tasks (S?gaard and Johannsen, 2012; S?gaard,2013).
Our random removal setup is basi-cally an offline version of the approach presentedin (S?gaard and Johannsen, 2012).
Their onlinealgorithm removes a random subset of the featuresin each iteration and was successfully applied tocross-domain experiments on part-of-speech tag-ging and document classification.
S?gaard (2013)presents a follow-up online approach that takesthe weights of the current model into considera-tion, regularizing the most predictive features.
Ourbiased approach is inspired by this, but has the ad-vantage that it abstracts away from the underlyinglearner.6 Discussion and Future WorkWe investigate cross-domain sentiment analysisfor a low-resource language, Danish.
We observethat performance drops precipitously when train-ing on one domain and evaluating on the other.
Wepresented a robust offline-learning approach thatdeletes features proportionate to their predictive-ness.
Applied to blind domain adaptation, this cor-ruption method prevents overfitting to the sourcedomain, and results in relative improvements ofmore than 50%.In the future, we plan to experiment with in-tegrating the weight distribution of a label intothe corruption function in order to prevent over-corrupting of certain labels.AcknowledgmentsWe would like to thank Daniel Hardt for host-ing the Copenhagen Sentiment Analysis Work-shop and making the data sets available.
The lasttwo authors are supported by the ERC StartingGrant LOWLANDS No.
313695.ReferencesEnrique Amig?o, Adolfo Corujo, Julio Gonzalo, EdgarMeij, and Maarten de Rijke.
2012.
Overview ofRepLab 2012: Evaluating Online Reputation Man-agement Systems.
In CLEF.Enrique Amig?o, Jorge Carrillo de Albornoz, IrinaChugur, Adolfo Corujo, Julio Gonzalo, TamaraMart?
?n, Edgar Meij, Maarten de Rijke, and Dami-ano Spina.
2013.
Overview of RepLab 2013: Eval-uating Online Reputation Monitoring Systems.
InCLEF.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In ACL.P.F.
Brown, P.V.
Desouza, R.L.
Mercer, V.J.
DellaPi-etra, and J.C. Lai.
1992.
Class-based n-gram mod-els of natural language.
Computational linguistics,18(4):467?479.Hal Daum?e, Abhishek Kumar, and Avishek Saha.2010.
Frustratingly easy semi-supervised domainadaptation.
In ACL Workshop on Domain Adapta-tion for NLP.Daniel Hardt and Julie Wulff.
2012.
What is the mean-ing of 5 *?s?
An investigation of the expression andrating of sentiment.
In Proceedings of KONVENS2012.Jason S. Kessler, Miriam Eckert, Lyndsie Clark, andNicolas Nicolov.
2010.
The 2010 ICWSM JDPAsentiment corpus for the automotive domain.
InICWSM-DWC.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Ph.D. thesis, Massachusetts Instituteof Technology.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets.
InSemEval-2013.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysis intwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM).Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Anders S?gaard and Anders Johannsen.
2012.
Ro-bust learning in random subspaces: Equipping nlpfor oov effects.
In COLING.Anders S?gaard.
2013.
Part-of-speech tagging withantagonistic adversaries.
In ACL.Sida Wang and Christopher D Manning.
2012.
Fastdropout training for logistic regression.
In NIPSworkshop on log-linear models.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In EMNLP.7
