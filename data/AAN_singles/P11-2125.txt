Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710?714,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAn Ensemble Model that Combines Syntactic and Semantic Clusteringfor Discriminative Dependency ParsingGholamreza HaffariFaculty of Information TechnologyMonash UniversityMelbourne, Australiareza@monash.eduMarzieh Razavi and Anoop SarkarSchool of Computing ScienceSimon Fraser UniversityVancouver, Canada{mrazavi,anoop}@cs.sfu.caAbstractWe combine multiple word representationsbased on semantic clusters extracted from the(Brown et al, 1992) algorithm and syntac-tic clusters obtained from the Berkeley parser(Petrov et al, 2006) in order to improve dis-criminative dependency parsing in the MST-Parser framework (McDonald et al, 2005).We also provide an ensemble method for com-bining diverse cluster-based models.
The twocontributions together significantly improvesunlabeled dependency accuracy from 90.82%to 92.13%.1 IntroductionA simple method for using unlabeled data indiscriminative dependency parsing was providedin (Koo et al, 2008) which involved clustering thelabeled and unlabeled data and then each word in thedependency treebank was assigned a cluster identi-fier.
These identifiers were used to augment the fea-ture representation of the edge-factored or second-order features, and this extended feature set wasused to discriminatively train a dependency parser.The use of clusters leads to the question ofhow to integrate various types of clusters (possiblyfrom different clustering algorithms) in discrimina-tive dependency parsing.
Clusters obtained from the(Brown et al, 1992) clustering algorithm are typi-cally viewed as ?semantic?, e.g.
one cluster mightcontain plan, letter, request, memo, .
.
.
while an-other may contain people, customers, employees,students, .
.
.. Another clustering view that is more?syntactic?
in nature comes from the use of state-splitting in PCFGs.
For instance, we could ex-tract a syntactic cluster loss, time, profit, earnings,performance, rating, .
.
.
: all head words of nounphrases corresponding to cluster of direct objects ofverbs like improve.
In this paper, we obtain syn-tactic clusters from the Berkeley parser (Petrov etal., 2006).
This paper makes two contributions: 1)We combine together multiple word representationsbased on semantic and syntactic clusters in order toimprove discriminative dependency parsing in theMSTParser framework (McDonald et al, 2005), and2) We provide an ensemble method for combiningdiverse clustering algorithms that is the discrimina-tive parsing analog to the generative product of ex-perts model for parsing described in (Petrov, 2010).These two contributions combined significantly im-proves unlabeled dependency accuracy: 90.82% to92.13% on Sec.
23 of the Penn Treebank, and wesee consistent improvements across all our test sets.2 Dependency ParsingA dependency tree represents the syntactic structureof a sentence with a directed graph (Figure 1), wherenodes correspond to the words, and arcs indicatehead-modifier pairs (Mel?c?uk, 1987).
Graph-baseddependency parsing searches for the highest-scoringtree according to a part-factored scoring function.
Inthe first-order parsing models, the parts are individ-ual head-modifier arcs in the dependency tree (Mc-Donald et al, 2005).
In the higher-order models, theparts consist of arcs together with some context, e.g.the parent or the sister arcs (McDonald and Pereira,2006; Carreras, 2007; Koo and Collins, 2010).
Witha linear scoring function, the parse for a sentence sis:PARSE(s) = arg maxt?T (s)?r?tw ?
f(s, r) (1)where T (s) is the space of dependency trees for s,and f(s, r) is the feature vector for the part r whichis linearly combined using the model parameter wto give the part score.
The above argmax searchfor non-projective dependency parsing is accom-710root ForIN-1PP-20111JapanNNP-19NP-100110,,-0,-00010theDT-15DT-151101trendNN-23NP-181010improvesVBZ-1S-140101accessNN-13NP-240011toTO-0TO-00011AmericanJJ-31JJ-310110marketsNNS-25NP-91011Figure 1: Dependency tree with cluster identifiers obtained from the split non-terminals from the Berkeley parser output.
The firstrow under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row isthe first 4 bits (to save space in this figure) of the (Brown et al, 1992) clusters.plished using minimum spanning tree algorithms(West, 2001) or approximate inference algorithms(Smith and Eisner, 2008; Koo et al, 2010).
The(Eisner, 1996) algorithm is typically used for pro-jective parsing.
The model parameters are trainedusing a discriminative learning algorithm, e.g.
av-eraged perceptron (Collins, 2002) or MIRA (Cram-mer and Singer, 2003).
In this paper, we work withboth first-order and second-order models, we trainthe models using MIRA, and we use the (Eisner,1996) algorithm for inference.The baseline features capture information aboutthe lexical items and their part of speech (POS) tags(as defined in (McDonald et al, 2005)).
In this work,following (Koo et al, 2008), we use word clusteridentifiers as the source of an additional set of fea-tures.
The reader is directed to (Koo et al, 2008)for the list of cluster-based feature templates.
Theclusters inject long distance syntactic or semantic in-formation into the model (in contrast with the useof POS tags in the baseline) and help alleviate thesparse data problem for complex features that in-clude n-grams.3 The Ensemble ModelA word can have different syntactic or semanticcluster representations, each of which may lead to adifferent parsing model.
We use ensemble learning(Dietterich, 2002) in order to combine a collectionof diverse and accurate models into a more powerfulmodel.
In this paper, we construct the base modelsbased on different syntactic/semantic clusters usedin the features in each model.
Our ensemble parsingmodel is a linear combination of the base models:PARSE(s) = arg maxt?T (s)?k?k?r?twk ?
fk(s, r) (2)where ?k is the weight of the kth base model, andeach base model has its own feature mapping fk(.
)based on its cluster annotation.
Each expert pars-ing model in the ensemble contains all of the base-line and the cluster-based feature templates; there-fore, the experts have in common (at least) the base-line features.
The only difference between individ-ual parsing models is the assigned cluster labels, andhence some of the cluster-based features.
In a fu-ture work, we plan to take the union of all of thefeature sets and train a joint discriminative parsingmodel.
The ensemble approach seems more scal-able though, since we can incrementally add a largenumber of clustering algorithms into the ensemble.4 Syntactic and Semantic ClusteringIn our ensemble model we use three different clus-tering methods to obtain three types of word rep-resentations that can help alleviate sparse data in adependency parser.
Our first word representation isexactly the same as the one used in (Koo et al, 2008)where words are clustered using the Brown algo-rithm (Brown et al, 1992).
Our two other clusteringsare extracted from the split non-terminals obtainedfrom the PCFG-based Berkeley parser (Petrov et al,2006).
Split non-terminals from the Berkeley parseroutput are converted into cluster identifiers in twodifferent ways: 1) the split POS tags for each wordare used as an alternate word representation.
Wecall this representation Syn-Low, and 2) head per-colation rules are used to label each non-terminal inthe parse such that each non-terminal has a uniquedaughter labeled as head.
Each word is assigned acluster identifier which is defined as the parent splitnon-terminal of that word if it is not marked as head,else if the parent is marked as head we recursivelycheck its parent until we reach the unique split non-terminal that is not marked as head.
This recursionterminates at the start symbol TOP.
We call this rep-resentation Syn-High.
We only use cluster identi-fiers from the Berkeley parser, rather than dependen-cies, or any other information.711First order featuresSec Baseline BrownSyn-LowSyn-High Ensemble00 89.61 90.39 90.01 89.97 90.8234.68 36.97 34.42 34.94 37.9601 90.44 91.48 90.89 90.76 91.8436.36 38.62 35.66 36.56 39.6723 90.02 91.13 90.46 90.35 91.3034.13 39.64 36.95 35.00 39.4324 88.84 90.06 89.44 89.40 90.3330.85 34.49 32.49 31.22 34.05Second order featuresSec Baseline BrownSyn-LowSyn-High Ensemble00 90.34 90.98 90.89 90.59 91.4138.02 41.04 38.80 39.16 40.9301 91.48 92.13 91.95 91.72 92.5141.48 43.84 42.24 41.28 45.0523 90.82 91.84 91.31 91.21 92.1339.18 43.66 40.84 39.97 44.2824 89.87 90.61 90.28 90.31 91.1835.53 37.99 37.32 35.61 39.55Table 1: For each test section and model, the number in thefirst/second row is the unlabeled-accuracy/unlabeled-complete-correct.
See the text for more explanation.
(TOP(S-14(PP-2 (IN-1 For)(NP-10 (NNP-19 Japan)))(,-0 ,)(NP-18 (DT-15 the) (NN-23 trend))(VP-6 (VBZ-1 improves)(NP-24 (NN-13 access))(PP-14 (TO-0 to)(NP-9 (JJ-31 American)(NNS-25 markets))))))For the Berkeley parser output shown above, theresulting word representations and dependency treeis shown in Fig.
1.
If we group all the head-words inthe training data that project up to split non-terminalNP-24 then we get a cluster: loss, time, profit, earn-ings, performance, rating, .
.
.
which are head wordsof the noun phrases that appear as direct object ofverbs like improve.5 Experimental ResultsThe experiments were done on the English PennTreebank, using standard head-percolation rules(Yamada and Matsumoto, 2003) to convert thephrase structure into dependency trees.
We split theTreebank into a training set (Sections 2-21), a devel-Verb Noun Pronoun Adverb Adjective Adpos.
Conjunc.0.040.060.080.100.120.14BaselineBrownSyn?LowSyn?HighEnsemble(a)1 3 5 7 9 11 13 +150.800.850.900.95Dependency lengthFscore!!!!!!!!!!!
!
!!!!!!!!!!!!!!!!!!!!!!!!!
!
!
!!!!!!!
!BaselineBrownSyn?LowSyn?HighEnsemble(b)Figure 2: (a) Error rate of the head attachment for differenttypes of modifier categories.
(b) F-score for each dependencylength.opment set (Section 22), and test sets (Sections 0,1, 23, and 24).
All our experimental settings matchprevious work (Yamada and Matsumoto, 2003; Mc-Donald et al, 2005; Koo et al, 2008).
POS tags forthe development and test data were assigned by MX-POST (Ratnaparkhi, 1996), where the tagger wastrained on the entire training corpus.
To generatepart of speech tags for the training data, we used 20-way jackknifing, i.e.
we tagged each fold with thetagger trained on the other 19 folds.
We set modelweights ?k in Eqn (2) to one for all experiments.Syntactic State-Splitting The sentence-specificword clusters are derived from the parse trees using712Berkeley parser1, which generates phrase-structureparse trees with split syntactic categories.
To gen-erate parse trees for development and test data, theparser is trained on the entire training data to learna PCFG with latent annotations using split-mergeoperations for 5 iterations.
To generate parse treesfor the training data, we used 20-way jackknifing aswith the tagger.Word Clusterings from Brown Algorithm Theword clusters were derived using Percy Liang?s im-plementation of the (Brown et al, 1992) algorithmon the BLLIP corpus (Charniak et al, 2000) whichcontains ?43M words of Wall Street Journal text.2This produces a hierarchical clustering over thewords which is then sliced at a certain height to ob-tain the clusters.
In our experiments we use the clus-ters obtained in (Koo et al, 2008)3, but were unableto match the accuracy reported there, perhaps due toadditional features used in their implementation notdescribed in the paper.4Results Table 1 presents our results for eachmodel on each test set.
In this table, the baseline(first column) does not use any cluster-based fea-tures, the next three models use cluster-based fea-tures using different clustering algorithms, and thelast column is our ensemble model which is the lin-ear combination of the three cluster-based models.As Table 1 shows, the ensemble model has out-performed the baseline and individual models in al-most all cases.
Among the individual models, themodel with Brown semantic clusters clearly outper-forms the baseline, but the two models with syntac-tic clusters perform almost the same as the baseline.The ensemble model outperforms all of the individ-ual models and does so very consistently across bothfirst-order and second-order dependency models.Error Analysis To better understand the contri-bution of each model to the ensemble, we take acloser look at the parsing errors for each model andthe ensemble.
For each dependent to head depen-1code.google.com/p/berkeleyparser2Sentences of the Penn Treebank were excluded from thetext used for the clustering.3people.csail.mit.edu/maestro/papers/bllip-clusters.gz4Terry Koo was kind enough to share the source code for the(Koo et al, 2008) paper with us, and we plan to incorporate allthe features in our future work.dency, Fig.
2(a) shows the error rate for each depen-dent grouped by a coarse POS tag (c.f.
(McDonaldand Nivre, 2007)).
For most POS categories, theBrown cluster model is the best individual model,but for Adjectives it is Syn-High, and for Pronounsit is Syn-Low that is the best.
But the ensemble al-ways does the best in every grammatical category.Fig.
2(b) shows the F-score of the different modelsfor various dependency lengths, where the length ofa dependency from word wi to word wj is equal to|i ?
j|.
We see that different models are experts ondifferent lengths (Syn-Low on 8, Syn-High on 9),while the ensemble model can always combine theirexpertise and do better at each length.6 Comparison to Related WorkSeveral ensemble models have been proposed fordependency parsing (Sagae and Lavie, 2006; Hall etal., 2007; Nivre and McDonald, 2008; Attardi andDell?Orletta, 2009; Surdeanu and Manning, 2010).Essentially, all of these approaches combine dif-ferent dependency parsing systems, i.e.
transition-based and graph-based.
Although graph-based mod-els are globally trained and can use exact inferencealgorithms, their features are defined over a lim-ited history of parsing decisions.
Since transition-based parsing models have the opposite character-istics, the idea is to combine these two types ofmodels to exploit their complementary strengths.The base parsing models are either independentlytrained (Sagae and Lavie, 2006; Hall et al, 2007;Attardi and Dell?Orletta, 2009; Surdeanu and Man-ning, 2010), or their training is integrated, e.g.
usingstacking (Nivre and McDonald, 2008; Attardi andDell?Orletta, 2009; Surdeanu and Manning, 2010).Our work is distinguished from the aforemen-tioned works in two dimensions.
Firstly, we com-bine various graph-based models, constructed usingdifferent syntactic/semantic clusters.
Secondly, wedo exact inference on the shared hypothesis space ofthe base models.
This is in contrast to previous workwhich combine the best parse trees suggested by theindividual base-models to generate a final parse tree,i.e.
a two-phase inference scheme.7 ConclusionWe presented an ensemble of different dependencyparsing models, each model corresponding to a dif-713ferent syntactic/semantic word clustering annota-tion.
The ensemble obtains consistent improve-ments in unlabeled dependency parsing, e.g.
from90.82% to 92.13% for Sec.
23 of the Penn Tree-bank.
Our error analysis has revealed that each syn-tactic/semantic parsing model is an expert in cap-turing different dependency lengths, and the ensem-ble model can always combine their expertise anddo better at each dependency length.
We can in-crementally add a large number models using dif-ferent clustering algorithms, and our preliminary re-sults show increased improvement in accuracy whenmore models are added into the ensemble.AcknowledgementsThis research was partially supported by NSERC,Canada (RGPIN: 264905).
We would like to thankTerry Koo for his help with the cluster-based fea-tures for dependency parsing and Ryan McDonaldfor the MSTParser source code which we modifiedand used for the experiments in this paper.ReferencesG.
Attardi and F. Dell?Orletta.
2009.
Reverse revisionand linear tree combination for dependency parsing.In Proc.
of NAACL-HLT.P.
F. Brown, P. V. deSouza, R. L. Mercer, T. J. Watson,V.
J. Della Pietra, and J. C. Lai.
1992.
Class-basedn-gram models of natural language.
ComputationalLinguistics, 18(4).X.
Carreras.
2007.
Experiments with a higher-order pro-jective dependency parser.
In Proc.
of EMNLP-CoNLLShared Task.E.
Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.2000.
BLLIP 1987-89 WSJ Corpus Release 1, LDCNo.
LDC2000T43, Linguistic Data Consortium.M.
Collins.
2002.
Discriminative training methods forhidden markov models: theory and experiments withperceptron algorithms.
In Proc.
of EMNLP.K.
Crammer and Y.
Singer.
2003.
Ultraconservativeonline algorithms for multiclass problems.
J. Mach.Learn.
Res., 3:951?991.T.
Dietterich.
2002.
Ensemble learning.
In The Hand-book of Brain Theory and Neural Networks, SecondEdition.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: an exploration.
In COLING.J.
Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,M.
Nilsson, and M. Saers.
2007.
Single malt orblended?
a study in multilingual parser optimization.In Proc.
of CoNLL Shared Task.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In Proc.
of ACL.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In Proc.
of ACL/HLT.T.
Koo, A.
Rush, M. Collins, T. Jaakkola, and D. Son-tag.
2010.
Dual decomposition for parsing with non-projective head automata.
In Proc.
of EMNLP.R.
McDonald and J. Nivre.
2007.
Characterizing theerrors of data-driven dependency parsing models.
InProc.
of EMNLP-CONLL.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Proc.of EACL.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Proc.of ACL.I.
Mel?c?uk.
1987.
Dependency syntax: theory and prac-tice.
State University of New York Press.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InProc.
of ACL.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In Proc.
COLING-ACL.S.
Petrov.
2010.
Products of random latent variablegrammars.
In Proc.
of NAACL-HLT.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proc.
of EMNLP.K.
Sagae and A. Lavie.
2006.
Parser combination byreparsing.
In Proc.
of NAACL-HLT.D.
A. Smith and J. Eisner.
2008.
Dependency parsing bybelief propagation.
In Proc.
of EMNLP.M.
Surdeanu and C. Manning.
2010.
Ensemble modelsfor dependency parsing: Cheap and good?
In Proc.
ofNAACL.D.
West.
2001.
Introduction to Graph Theory.
PrenticeHall, 2nd editoin.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Proc.of IWPT.714
