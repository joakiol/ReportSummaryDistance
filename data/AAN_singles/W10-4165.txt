Triplet-Based Chinese Word Sense InductionZhao LiuSchool of Computer ScienceFudan UniversityShanghai, ChinaZLiu.fd@gmail.comXipeng QiuSchool of Computer ScienceFudan UniversityShanghai, Chinaxpqiu@fudan.edu.cnXuanjing HuangSchool of Computer ScienceFudan UniversityShanghai, Chinaxjhuang@fudan.edu.cnAbstractThis paper describes the implementa-tion of our system at CLP 2010 bake-off of Chinese word sense induction.We first extract the triplets for the tar-get word in each sentence, then usethe intersection of all related words ofthese triplets from the Internet.
Weuse the related word to construct fea-ture vectors for the sentence.
At lastwe discriminate the word senses byclustering the sentences.
Our systemachieved 77.88% F-score under the of-ficial evaluation.1 IntroductionThe goal of the CLP 2010 bake-off of Chi-nese word sense induction is to automati-cally discriminate the senses of Chinese targetwords by the use of only un-annotated data.The use of word senses instead of wordforms has been shown to improve perfor-mance in information retrieval, informationextraction and machine translation.
WordSense Disambiguation generally requires theuse of large-scale manually annotated lexicalresources.
Word Sense Induction can over-come this limitation, and it has become oneof the most important topics in current com-putational linguistics research.In this paper we introduce a method tosolve the problem of Chinese word sense in-duction.For this task, Firstly we constructedtriplets containing the target word in everyinstance, then searched the intersection of allthe three words from the Internet with websearching engine and constructed feature vec-tors.Then we clustered the vectors with thesIB clustering algorithm and at last discrimi-nated the word senses.This paper is organized as following: firstlywe introduce the related works.
Then we talkabout the methods in features selection andclustering.
The method of evaluation and theresult of our system is following.
At last wediscuss the improvement and the weakness ofour system.2 Related WorksSense induction is typically treated as aclustering problem, by considering their co-occurring contexts, the instances of a targetword are partitioned into classes.
Previousmethods have used the first or second or-der co-occurrence (Pedersen and Bruce, 1997;Sch?tze, 1998), parts of speech, and local col-locations (Niu et al, 2007).
The size of con-text window is also various, it can be as smallas only two words before and after the targetwords.
It may be the sentence where the tar-get word is in.
Or it will be 20 surroundingwords on either side of the target words andeven more words.After every instance of the target word isrepresented as a feature vector, it will be theinput of the clustering methods.
Many clus-tering methods have been used in the taskof word sense induction.
For example, k-means and agglomerative clustering (Sch?tze,1998).
sIB (Sequential Information Bottle-neck) a variation of Information Bottleneck isapplied in (Niu et al, 2007).
In (Dorow andWiddows, 2003) Graph-based clustering algo-rithm is employed that in a graph a node rep-resents a noun and two nodes have an edge be-tween them if they co-occur in list more thana given number of times.
A generative modelbased on LDA is proposed in (Brody and La-pata, 2009).In our method, we use the triplets (Bordag,2006) and their intersections from the Internetto construct the feature vectors then sIB isused as the clustering method.3 Feature SelectionOur method select the features of the wordssimilar to (Bordag, 2006) is also using thetriplets.
In Chinese there are no natural sep-arators between the words as English, so thefirst step in Chinese language processing is of-ten the Chinese word segmentation.
In oursystem we use the FudanNLP toolkit1 to splitthe words.At the first stage, we split the instance ofthe target word and filter out the numbers,English words and stop words from it.
So weget a sequence of the words.
Then we selecttwo words before the target and another twowords after it.
If there are no words before orafter then leave it empty.
After that we enu-merate two words from the selected four wordsto construct a triplets together with the targetwords.
So we can get several triplets for everyinstance of the target.
Because the faulty ofChinese word segmentation and some specialtarget word for example a single Chinese char-acter as a word, there are some errors findingthe position of the target words.
If the wordis a single Chinese character and the toolkitcombine it with other Chinese characters tobe a word, we will use that word as the tar-get instead of the character to construct thetriplets.The second stage is obtaining corpus fromthe Internet.
For every triplet we search thethree words sequence in it with a pair of dou-ble quotation marks in Baidu web searchingengine2.
It gives the snippets of the webs1http://code.google.com/p/fudannlp/2http://www.baidu.comwhich have all the three words in it.
We se-lect the first 50 snippets of each triplets.
Ifthe number of the snippets is less than 50,we will ignore that triplet.
For some rarewords the snippets searched from the Internetfor all the triplets of the instance is less than50.
In that situation we will search the targetword and another one co-occurring word inthe searching engine to achieve enough snip-pets as features.
After searching the tripletswe select the first three triplets (or doublets)with largest amount of the webs searched bythe searching engine.
For every instance thereare three or less triplets (or doublets) and wehave obtained many snippets for them.
Aftersegmenting and filtering these snippets we usethe bag of words from them as the feature forthis instance.The last stage of feature selection is to con-struct the feature vector for every instancescontaining the target word.
In the previousstage we get a bag of words for each instance.For all the instances of one target word wemake a statistic of the frequence of each wordin the bags.
In our system we select the wordswhose frequence is more than 50 as the dimen-sions for the feature vectors.
From the testswe find that when this thread varies from 50 to120 the result of our system is nearly the same,but outside that bound the result will becomerather bad.
So we use 50 as the thread.
Af-ter constructing the dimension of that targetword, we can get a feature vector for each in-stance that at each dimension the number isthe frequence of that word occurs in that po-sition.We obtain the feature vectors for the targetwords by employing these three stage.
Thefollowing work is clustering these vector to getthe classes of the word senses.4 The Clustering AlgorithmThere are many classical clustering methodssuch as k-means, EM and so on.
In (Niu etal., 2007) they applied the sIB (Slonim et al,2002) clustering algorithm at SemEval-2007for task 2 and it achieved a quite good result.And at first this algorithm is also introducedfor the unsupervised document classificationproblem.
So we use the sIB algorithm for clus-tering the feature vectors in our system.Unlike the situation in (Niu et al, 2007),the number of the sense classes is provided inCLP2010 task 4.
So we can apply the sIBalgorithm directly without the sense numberestimation procedure in that paper.
sIB algo-rithm is a variant of the information bottle-neck method.Let d represent a document, and w repre-sent a feature word, d ?
D,w ?
F .
Giventhe joint distribution p(d,w), the documentclustering problem is formulated as looking fora compact representation T for D, which re-serves as much information as possible aboutF .
T is the document clustering solution.
Forsolving this optimization problem, sIB algo-rithm was proposed in (Slonim et al, 2002),which found a local maximum of I(T, F ) by:given a initial partition T, iteratively draw-ing a d ?
D out of its cluster t(d), t ?
T ,and merging it into tnew such that tnew =argmaxt?Td(d, t).
d(d, t) is the change ofI(T, F ) due to merging d into cluster tnew,which is given byd(d, t) = (p(d)+p(t))JS(p(w|d), p(w|t)).
(1)JS(p, q) is the Jensen-Shannon divergence,which is defined asJS(p, q) = pipDKL(p||p) + piqDKL(q||p), (2)DKL(p||p) =?yp log pp, (3)DKL(q||p) =?yq log qp, (4){p, q} ?
{p(w|d), p(w|t)}, (5){pip, piq} ?
{p(d)p(d) + p(t), p(t)p(d) + p(t)}, (6)p = pipp(w|d) + piqp(w|t).
(7)In our system we use the sIB algorithm inthe Weka 3.5.8 cluster package to cluster thefeature vectors obtained in the previous sec-tion.
The detailed description of the sIB algo-rithm in weka can refer to the website 3.
Andthe parameters for this Weka class is that: thenumber of clusters is the number of senses pro-vided by the task, the random number seed iszero and the other parameters like maximumnumber of iteration and so on is set as default.5 CLP 2010 Bake-Off of ChineseWord Sense Induction5.1 Evaluation MeasureThe evaluation measure is described as fol-lowing:We consider the gold standard as a solu-tion to the clustering problem.
All examplestagged with a given sense in the gold standardform a class.
For the system output, the clus-ters are formed by instances assigned to thesame sense tag (the sense tag with the highestweight for that instance).
We will compareclusters output by the system with the classesin the gold standard and compute F-score asusual.
F-score is computed with the formulabelow.Suppose Cr is a class of the gold standard,and Si is a cluster of the system generated,then1.
F ?
score(Cr, Si) = 2?P?RP+R2.
P =the number of correctly labeled ex-amples for a cluster/total cluster size3.
R =the number of correctly labeled ex-amples for a cluster/total class sizeThen for a given class Cr,FScore(Cr) = maxSi(F ?
score(Cr, Si)) (8)ThenFScore =c?r=1nrnFScore(Cr) (9)3http://pacific.mpi-cbg.de/javadoc/Weka/clusterers/sIB.htmlWhere c is total number of classes, nr is thesize of class Cr , and n is the total size.5.2 DataSetThe data set includes 100 ambiguous Chi-nese words and for every word it provided 50instances.
Besides that they also provided asample test set of 2500 examples of 50 targetwords with the answers to illustrate the dataformat.Besides the sIB algorithm we also apply thek-means and EM algorithm to cluster the fea-ture vectors.
These algorithms are separatelyusing the simpleKMeans class and the EMclass in the Weka 3.5.8 cluster package.
Ex-cept the number of clusters set as the givennumber of senses and number of seeds set aszero, all other parameters are set as default.For the given sample test set with answers theresult is illustrated in the Table 1 below.algorithm F-scorek-means 0.7025EM 0.7286sIB 0.8132Table 1: Results of three clustering algorithmsFrom Table 1 we can see the sIB clusteringalgorithm improves the result of the Chineseword sense induction evidently.In the real test data test containing 100 am-biguous Chinese words, our system achieves aF-score 0.7788 ranking 6th among the 18 sys-tems submitted.
The best F-score of these 18systems is 0.7933 and the average of them is0.7128.5.3 DiscussionIn our system we only use the local collo-cations and the co-occurrences of the targetwords.
But the words distance for the targetword in the same sentence and the parts ofspeech of the neighboring word together withthe target word is also important in this task.In our experiment we used the parts ofspeech for the target word and each word be-fore and after it achieved by the Chinese wordsegmentation system as part of the featuresvectors for clustering.
With a proper weighton each POS dimension in the feature vectors,the F score for some word in the given sampletest set with answers improved evidently.
Forexample the Chinese word ???
?, the F scoreof it was developed from 0.5983 to 0.7573.
Butbecause of the fault of the segmentation sys-tem and other reasons F score of other wordsfell and the speed of the system was ratherslower than before, we gave up this improve-ment finally.Without the words distance for the targetword in the same sentence the feature vectorsmaybe lack some information useful.
So if wecan calculate the correlation between the tar-get word and other words, we will use theseword sufficiently.
However because of quan-tity of the Internet corpus is unknown, wedidn?t find the proper method to weigh thecorrelation.From the previous section we find that theF score for the real test data test is lower thanthat for the sample test set.
It is mainly be-cause there are more single Chinese charac-ters (as words) in the real test data set.
Oursystem does not process these characters spe-cially.
For most of the Chinese characters wecan?t judge their correct senses only from thecontext where they appear.
Their meaningalways depends on the collocations with theother Chinese characters with which they be-come a Chinese word.
However our systemdiscriminates the senses of them only refer-ring to the context of them, it can?t judge themeaning of these Chinese characters properly.Maybe the best way is to search them in thedictionary.However our system does not always have avery poor performance for any single Chinesecharacter (as a word).
The result is quite goodfor some Chinese characters.
For example theChinese character ???
which has three mean-ing: valley, millet and a family name, the pre-cision (P) of our system is 0.760.
But for mostof single Chinese characters such as ???
and??
?, it is so bad that the result in the sampletest worked rather better than the real test.In Chinese the former character ???
tendsto express a complete meaning and the othercharacters in the word which they combine of-ten modify it such as the characters ???
and???
in the word ????
and ????.
So thischaracter can have a relatively high correla-tion with the words around and our systemcan deal with such characters like it.
Unfor-tunately most characters need other charac-ters to represent a complete meaning as thethe latter ???
and ???
so they almost haveno correlation with the words around but withthose characters in the word in which they oc-cur.
But our system only uses the context fea-tures and even doesn?t do any special processabout these single Chinese characters.
There-fore our system can not address those char-acters appropriately and we need to find aproper method to solve it, using a dictionarymay be a choice.This method works better for nouns and ad-jectives (in the sample test data set there areonly 4 adjectives), but for verbs F score fallsa little, illustrated in the Table 2 below.POS F-scorenouns 0.8473adjectives 0.8543verbs 0.7921Table 2: Results of each POS in the sampletest data setOnly using the local collocations in our sys-tem the F score is achieve above 80% (in thesample test), it demonstrates to some extentthe information of collocations is so importantthat we should pay more attention to it.6 ConclusionThe triplet-based Chinese word sense induc-tion method is fitted to the task of Chineseword sense induction and obtain rather goodresult.
But for some single characters wordand some verbs, this method is not appropri-ate enough.
In the future work, we will im-prove the method with more reasonable tripletselection strategies.AcknowledgmentsThis work was (partially) funded by 863Program (No.
2009AA01A346), 973 Pro-gram (No.
2010CB327906), and Shanghai Sci-ence and Technology Development Funds (No.08511500302).ReferencesBordag, S. 2006.
Word sense induction: Triplet-based clustering and automatic evaluation.
Pro-ceedings of EACL-06.
Trento.Brody, S. and M. Lapata.
2009.
Bayesian wordsense induction.
In Proceedings of the 12th Con-ference of the European Chapter of the Associa-tion for Computational Linguistics, pages 103?111.
Association for Computational Linguistics.Dorow, B. and D. Widdows.
2003.
Discoveringcorpus-specific word senses.
In Proceedings ofthe tenth conference on European chapter ofthe Association for Computational Linguistics-Volume 2, pages 79?82.
Association for Compu-tational Linguistics.Niu, Z.Y., D.H. Ji, and C.L.
Tan.
2007.
I2r: Threesystems for word sense discrimination, chineseword sense disambiguation, and english wordsense disambiguation.
In Proceedings of the 4thInternational Workshop on Semantic Evalua-tions, pages 177?182.
Association for Compu-tational Linguistics.Pedersen, T. and R. Bruce.
1997.
Distinguishingword senses in untagged text.
In Proceedings ofthe Second Conference on Empirical Methods inNatural Language Processing, volume 2, pages197?207.Sch?tze, H. 1998.
Automatic word sense discrim-ination.
Computational Linguistics, 24(1):97?123.Slonim, N., N. Friedman, and N. Tishby.
2002.Unsupervised document classification using se-quential information maximization.
In Proceed-ings of the 25th annual international ACM SI-GIR conference on Research and developmentin information retrieval, pages 129?136.
ACM.
