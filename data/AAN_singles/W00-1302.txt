What's yours and what's mine: Determining IntellectualAttribution in Scientific TextS imone Teufe l  tComputer  Science DepartmentColumbia Universityt eu fe l?cs ,  co lumbia ,  eduMarc  MoensHCRC Language Technology GroupUniversity of Ed inburghMarc .
Moens?ed.
ac.
ukAbst rac tWe believe that identifying the structure of scien-tific argumentation in articles can help in taskssuch as automatic summarization or the auto-mated construction of citation indexes.
One par-ticularly important aspect of this structure is thequestion of who a given scientific statement is at-tributed to: other researchers, the field in general,or the authors themselves.We present he algorithm and a systematic eval-uation of a system which can recognize the mostsalient textual properties that contribute to theglobal argumentative structure of a text.
In thispaper we concentrate on two particular features,namely the occurrences ofprototypical gents andtheir actions in scientific text.1 In t roduct ionWhen writing an article, one does not normallygo straight to presenting the innovative scien-tific claim.
Insteacl, one establishes other, well-known scientific facts first, which are contributedby other researchers.
Attribution of ownership of-ten happens explicitly, by phrases uch as "Chom-sky (1965) claims that".
The question of intel-lectual attribution is important for researchers:not understanding the argumentative status ofpart of the text is a common problem for non-experts reading highly specific texts aimed at ex-perts (Rowley, 1982).
In particular, after readingan article, researchers need to know who holds the"knowledge claim" for a certain fact that intereststhem.We propose that segmentation according to in-tellectual ownership can be done automatically,and that such a segmentation has advantages forvarious hallow text understanding tasks.
At theheart of our classification scheme is the followingtrisection:* BACKGROUND (generally known work)* OWN,  new work and.
specific OTHER work.The advantages of a segmentation at a rhetori-cal level is that rhetorics is conveniently constanttThis work was done while the first author was at theHCRC Language T chnology Group, Edinburgh.BACKGROUND:Researchers in knowledge representa-tion agree that one of the hard problems ofunderstanding narrative is the representationof temporal information.
Certain facts of nat-ural language make it hard to capture tempo-ral information \[...\]OTHER WORK:Recently, Researcher-4 has suggested thefollowing solution to this problem \[...\].WEAKNESS/CONTRAST:But this solution cannot be used to inter-pret the following Japanese examples: \[...\]OWN CONTRIBUT ION:We propose a solution which circumventsthis p row while retaining the explanatorypower of Researcher-4's approach.Figure h Fictional introduction sectionacross different articles.
Subject matter, on thecontrary, is not constant, nor are writing style andother factors.We work with a corpus of scientific pa-pers (80 computational linguistics conference ar-ticles (ACL, EACL, COLING or ANLP), de-posited on the CMP_LG archive between 1994and 1996).
This is a difficult test bed due tothe large variation with respect o different fac-tors: subdomain (theoretical linguistics, statisti-cal NLP, logic programming, computational psy-cholinguistics), types of research (implementa-tion, review, evaluation, empirical vs. theoreti-cal research), writing style (formal vs. informal)and presentational styles (fixed section structureof type Introduction-Method-Results-Conclusionvs.
more idiosyncratic, problem-structured presen-tation).One thing, however, is constant across all arti-cles: the argumentative aim of every single articleis to show that the given work is a contribution toscience (Swales, 1990; Myers, 1992; Hyland, 1998).Theories of scientific argumentation in research ar-ticles stress that authors follow well-predictablestages of argumentation, as in the fictional intro-duction in figure 1.9Are the scientific statements expressedin this sentence attributed to theauthors, the general field, or specific othern work / Other WorkDoes this sentence contain materialthat describes the specific aimof the paper?Does this sentence makereference to the externalstructure of the paper?I SACKCRO D ID.~s it describe.a negative aspectof me omer worK, or a contzastor comparison of the own work to it?I CONTRAST I Does this sentence mentionthe other work as basis ofor support for own work?Figure 2: Annotation Scheme for Argumentative ZonesOur hypothesis i that a segmentation based onregularities of scientific argumentation and on at-tribution of intellectual ownership is one of themost stable and generalizable dimensions whichcontribute to the structure of scientific texts.
Inthe next section we will describe an annotationscheme which we designed for capturing these ef-fects.
Its categories are based on Swales' (1990)CARS model.1.1 The schemeAs our corpus contains many statements talkingabout relations between own and other work, wedecided to add two classes ("zones") for express-ing relations to the core set of OWN, OTHERand BACKGROUND, namely contrastive statements(CONTRAST;  comparable to Swales' (1990) move2A/B) and statements of intellectual ancestry(BAsis; Swales' move 2D).
The label OTHER isthus reserved for neutral descriptions of otherwork.
OWN segments are further subdivided tomark explicit aim statements (AIM; Swales' move3.1A/B), and explicit section previews (TEXTUAL;Swales' move 3.3).
All other statements about theown work are classified as OwN.
Each of the sevencategory covers one sentence.Our classification, which is a further develop-ment of the scheme in Teufel and Moens (1999),can be described procedurally as a decision tree(Figure 2), where five questions are asked abouteach sentence, concerning intellectual attribution,author stance and continuation vs. contrast.
Fig-ure 3 gives typical example sentences for each zone.The intellectual-attribution distinction we makeis comparable with Wiebe's (1994) distinction intosubjective and objective statements.
Subjectivityis a property which is related to the attribution ofauthorship as well as to author stance, but it isjust one of the dimensions we consider.1.2 Use o f  Argumentat ive  ZonesWhich practical use would segmenting a paper intoargumentative zones have?Firstly, rhetorical information as encoded inthese zones should prove useful for summariza-tion.
Sentence extracts, still the main type ofsummarization around, are notoriously context-insensitive.
Context in the form of argumentativerelations of segments to the overall paper couldprovide a skeleton by which to tailor sentence x-tracts to user expertise (as certain users or certaintasks do not require certain types of information).A system which uses such rhetorical zones to pro-duce task-tailored extracts for medical articles, al-beit on the basis of manually-segmented xts, isgiven by Wellons and Purcell (1999).Another hard task is sentence xtraction fromlong texts, e.g.
scientific journal articles of 20pages of length, with a high compression.
Thistask is hard because one has to make decisionsabout how the extracted sentences relate to eachother and how they relate to the overall messageof the text, before one can further compress them.Rhetorical context of the kind described above isvery likely to make these decisions easier.Secondly, it should also help improve citationindexes, e.g.
automatically derived ones likeLawrence et al's (1999) and Nanba and Oku-mura's (1999).
Citation indexes help organize sci-entific online literature by linking cited (outgoing)and citing (incoming) articles with a given text.But these indexes are mainly "quantitative", list-ing other works without further qualifying whethera reference to another work is there to extend the10AIM "We have proposed a method of clustering words based on large corpus data.
"TEXTUAL "Section $ describes three unification-based parsers which are... "OWN "We also compare with the English language and draw some conclusions on the benefitsof our approach.
"BACKGROUND "Part-of-speech tagging is the process of assigning rammatical categories to individualwords in a corpus.
"CONTRAST "However, no method for extracting the relationships from superficial inguistic ex-pressions was described in their paper.
"BASIS "Our disambiauation method is based on the similaritu of context vectors, which wasOTHERC :g yoriginated by Wilks et al 1990.
""Strzalkowski's Essential Arguments Approach (EAA) is a top-down approach to gen-eration... "Figure 3: Examples for Argumentative Zonesearlier work, correct it, point out a weakness init, or just provide it as general background.
This"qualitative" information could be directly con-tributed by our argumentative zones.In this paper, we will describe the algorithm ofan argumentative zoner.
The main focus of thepaper is the description of two features which areparticularly useful for attribution determination:prototypical gents and actions.2 Human Annotat ion  o fArgumentat ive  ZonesWe have previously evaluated the scheme mpiri-cally by extensive experiments with three subjects,over a range of 48 articles (Teufel et al, 1999).We measured stability (the degree to which thesame annotator will produce an annotation after6 weeks) and reproducibility (the degree to whichtwo unrelated annotators will produce the sameannotation), using the Kappa coefficient K (Siegeland Castellan, 1988; Carletta, 1996), which con-trols agreement P(A) for chance agreement P(E):K = P{A)-P(E)1-P(Z)Kappa is 0 for if agreement is only as would beexpected by chance annotation following the samedistribution as the observed istribution, and 1 forperfect agreement.
Values of Kappa surpassing.8 are typically accepted as showing a very highlevel of agreement (Krippendorff, 1980; Landis andKoch, 1977).Our experiments show that humans can distin-guish own, other specific and other general workwith high stability (K=.83, .79, .81; N=1248; k=2,where K stands for the Kappa coefficient, N forthe number of items (sentences) annotated and kfor the number of annotators) and reproducibil-ity (K=.78, N=4031, k=3), corresponding to 94%,93%, 93% (stability) and 93% (reproducibility)agreement.The full distinction into all seven categories ofthe annotation scheme is slightly less stable andreproducible (stability: K=.82, .81, .76; N=1220;k=2 (equiv.
to 93%, 92%, 90% agreement); repro-ducibility: K=.71, N=4261, k=3 (equiv.
to 87%agreement)), but still in the range of what is gener-ally accepted as reliable annotation.
We concludefrom this that humans can distinguish attributionand full argumentative zones, if trained.
Humanannotation is used as trMning material in our sta-tistical classifier.3 Automat ic  Argumentat iveZon ingAs our task is not defined by topic coherencelike the related tasks of Morris and Hirst (1991),Hearst (1997), Kan et al (1998) and Reynar(1999), we predict hat keyword-based techniquesfor automatic argumentative zoning will not workwell (cf.
the results using text categorization asdescribed later).
We decided to perform machinelearning, based on sentential features like the onesused by sentence xtraction.
Argumentative zoneshave properties which help us determine them onthe surface:?
Zones appear in typical positions in the article(Myers, 1992); we model this with a set oflocation features.?
Linguistic features like tense and voice cor-relate with zones (Biber (1995) and Riley(1991) show correlation for similar zones like"method" and "introduction").
We modelthis with syntactic features.?
Zones tend to follow particular other zones(Swales, 1990); we model this with an ngrammodel operating over sentences.?
Beginnings of attribution zones are linguisti-cally marked by meta-discourse like "Otherresearchers claim that" (Swales, 1990; Hy-land, 1998); we model this with a specializedagents and actions recognizer, and by recog-nizing formal citations.?
Statements without explicit attribution areinterpreted as being of the same attributionas previous entences in the same segment ofattribution; we model this with a modifiedagent feature which keeps track of previouslyrecognized agents.113.1 Recognizing Agents and ActionsPaice (1981) introduces grammars for patternmatching of indicator phrases, e.g.
"theaim/purpose of this paper/article/study" and "weconclude/propose".
Such phrases can be usefulindicators of overall importance.
However, forour task, more flexible meta-diiscourse expressionsneed to be determined.
The ,description of a re-search tradition, or the stateraent that the workdescribed in the paper is the continuation ofsomeother work, cover a wide range of syntactic andlexical expressions and are too hard to find for amechanism like simple pattern matching.Agent Type ExampleUS-AGENTTHEM_AGENTGENERAL_AGENTUS_PREVIOUS.
AGENTOUR_AIM_AGENTREF_US_AGENTREF._AGENTTHEM_PRONOUN_AGENTAIM_I:LEF_AGENTGAP_AGENTPROBLEM_AGENTSOLUTION_AGENTTEXTSTRUCTURE_AGENTwehis approachtraditional methodsthe approach given inX (99)the point o\] this studythia paperthe papertheyits goalnone of these papersthese drawbacksa way out o\] thisdilemmathe concluding chap-terFigure 4: Agent Lexicon: 168 Patterns, 13 ClassesWe suggest hat the robust recognition of pro-totypical agents and actions is one way out of thisdilemma.
The agents we propose to recognize de-scribe fixed role-players in the argumentation.
IFigure 1, prototypical agents are given in bold-face ("Researchers in knowledge representation,"Researcher-4" and "we").
We also propose pro-totypical actions frequently occurring in scientificdiscourse (shown underlined in Figure 1): the re-searchers "agree", Researcher-4 "suggested" some-thing, the solution "cannot be used".We will now describe an algorithm which rec-ognizes and classifies agents and actions.
Weuse a manually created lexicon for patterns foragents, and a manually clustered verb lexicon forthe verbs.
Figure 4 lists the agent types we dis-tinguish.
The main three types are US_aGENT,THEM-AGENT and GENERAL.AGENT.
A fourthtype is US.PREVIOUS_AGENT (the authors, but ina previous paper).Additional agent types include non-personalagents like aims, problems, solutions, absence ofsolution, or textual segments.
There are fourequivalence classes of agents with ambiguousreference ("this system"), namely REF_US_AGENT,THEM-PRONOUN_AGENT, AIM.-REF-AGENT,REF_AGENT.
The total of 168 patterns in thelexicon expands to many more as we use a replacemechanism (@WORK_NOUN is expanded to"paper, article, study, chapter" etc).For verbs, we use a manually created the ac-tion lexicon summarized in Figure 6.
The verbclasses are based on semantic oncepts uch assimilarity, contrast, competition, presentation, ar-gumentation and textual structure.
For ex-ample, PRESENTATION..ACTIONS include commu-nication verbs like "present", "report", "state"(Myers, 1992; Thompson and Yiyun, 1991), RE-SEARCH_ACTIONS include "analyze", "conduct"and "observe", and ARGUMENTATION_ACTIONS"argue", "disagree", "object to".
Domain-specificactions are contained in the classes indicatinga problem ( ".fail", "degrade", "overestimate"),and solution-contributing actions (" "circumvent',solve", "mitigate").The main reason for using a hand-crafted, genre--specific lexicon instead of a general resource suchas WordNet or Levin's (1993) classes (as used inKlavans and Kan (1998)), was to avoid polysemyproblems without having to perform word sensedisambiguation.
Verbs in our texts often have aspecialized meaning in the domain of scientific ar-gumentation, which our lexicon readily encodes.We did notice some ambiguity problems (e.g.
"fol-low" can mean following another approach, or itcan mean follow in a sense having nothing to dowith presentation of research, e.g.
following anarc in an algorithm).
In a wider domain, however,ambiguity would be a much bigger problem.Processing of the articles includes transforma-tion from I~TEX into XML format, recognitionof formal citations and author names in runningtext, tokenization, sentence separation and POS-tagging.
The pipeline uses the TTT software pro-vided by the HCRC Language Technology Group(Grover et al, 1999).
The algorithm for deter-mining agents in subject positions (or By-PPs inpassive sentences) is based on a finite automatonwhich uses POS-input; cf.
Figure 5.In the case that more than one finite verb isfound in a sentence, the first finite verb which hasagents and/or actions in the sentences i used asa value for that sentence.4 Eva luat ionWe carried out two evaluations.
Evaluation Atests whether all patterns were recognized as in-tended by the algorithm, and whether patternswere found that should not have been recognized.Evaluation B tests how well agent and actionrecognition helps us perform argumentative zon-ing automatically.4.1 Evaluation A: Cor rectnessWe first manually evaluated the error level of thePOS-Tagging of finite verbs, as our algorithm cru-cially relies on finite verbs.
In a random sample of100 sentences from our corpus (containing a totalof 184 finite verbs), the tagger showed a recall of121.
Start from the first finite verb in the sentence.2.
Check right context of the finite verb for verbal forms of interest which might make up morecomplex tenses.
Remain within the assumed clause boundaries; do not cross commas or otherfinite verbs.
Once the main verb of that construction (the "semantic" verb) has been found,a simple morphological nalysis determines its lemma; the tense and voice of the constructionfollow from the succession of auxiliary verbs encountered.3.
Look up the lemma of semantic verb in Action Lexicon; return the associated Action Class ifsuccessful.
Else return Action 0.4.
Determine if one of the 32 fixed negation words contained in the lexicon (e.g.
"not, don't,neither") is present within a fixed window of 6 to the right of the finite verb.5.
Search for the agent either as a by-PP to the right, or as a subject-NP to the left, depending onthe voice of the construction as determined in step 2.
Remain within assumed clause boundaries.6.
If one of the Agent Patterns matches within that area in the sentence, return the Agent Type.Else return Agent 0.7.
Repeat Steps 1-6 until there are no more finite verbs left.Figure 5: Algorithm for Agent and Action DetectionAction Type Example Action Type ExampleAFFECTARGUMENTATIONAWARENESSBETTER_SOLUTIONCHANGECOMPARISONCONTINUATIONCONTRASTFUTURE_INTERESTINTERESTwe hope to improve our resultswe argue against a model ofwe are not aware of attemptsour system outperforms .
.
.we extend <CITE /> 's  algo-rithmwe tested our system against.. .we follow <REF/> .
.
.our approach differs from .
.
.we intend to improve .
.
.we are concerned with .
.
.NEEDPRESENTATIONPROBLEMRESEARCHSIMILARSOLUTIONTEXTSTRUCTUREUSECOPULAPOSSESSIONthis approach, however, lacks...we present here a method for.
.
.this approach fai ls .
.
.we collected our data f rom.
.
.our approach resembles that ofwe solve this problem by.
.
.the paper is organize&..we employ <REF/> 's method...our goal ~ to .
.
.we have three goals...Figure 6: Action Lexicon: 366 Verbs, 20 Classes95% and a precision of 93%.We found that for the 174 correctly determinedfinite verbs (out of the total 184), the heuristics fornegation worked without any errors (100% accu-racy).
The correct semantic verb was determinedin 96% percent of all cases; errors are mostly dueto misrecognition of clause boundaries.
ActionType lookup was fully correct, even in the caseof phrasal verbs and longer idiomatic expressions("have to" is a NEED..ACTION; "be inspired by" isa, CONTINUE_ACTION).
There were 7 voice errors,2 of which were due to POS-tagging errors (pastparticiple misrecognized).
The remaining 5 voiceerrors correspond to a 98% accuracy.
Figure 7gives an example for a voice error (underlined) inthe output of the action/agent determination.Correctness of Agent Type determination wastested on a random sample of 100 sentences con-taining at least one agent, resulting in 111 agents.No agent pattern that should have been identi-fied was missed (100% recall).
Of the 111 agents,105 cases were completely correct: the agent pat-tern covered the complete grammatical subject orby-PP intended (precision of 95%).
There was onecomplete rror, caused by a POS-tagging error.
In5 of the 111 agents, the pattern covered only partAt the point where John <ACTIONTENSE=Pi~SENT VOICE=ACTIVEMODAL=NOMODAL NEGATION=0ACT IONTYPE=0> knows </ACTION> the truthhas been  <FINITE TENSE=PRESENT_PERFECTVOICE=PASSIVE  MODAL=NOMODAL NEGA-T ION=0 ACTIONTYPE=0> processed</ACTION> , a complete clause will havebeen <ACTION TENSE=FUTURE.PERFECTVOICE=ACTIVE MODAL=NOMODAL NEGA-TION=0 ACTIONTYPE=0> bu i l t  </ACTION>Figure 7: Sample Output of Action Detectionof a subject NP (typically the NP in a postmodify-ing PP), as in the phrase "the problem with theseapproaches" which was classified as REF_AGENT.These cases (counted as errors) indeed constituteno grave errors, as they still give an indicationwhich type of agents the nominal phrase is associ-ated with.134.2 Evaluation B: Usefulness forArgumentat ive ZoningWe evaluated the usefulness of the Agent and Ac-tion features by measuring if they improve theclassification results of our stochastic classifier forargumentative zones.We use 14 features given in figure 8, some ofwhich are adapted from sentence xtraction tech-niques (Paice, 1990; Kupiec et eL1., 1995; Teufel andMoens, 1999)..2.3.4.5.6.7.8.9.10.11.12.13.14.Absolute location of sentence in documentRelative location of sentence in sectionLocation of a sentence in paragraphPresence of citationsLocation of citationsType of citations (self citation or not)Type of headlinePresence of tf/idf key wordsPresence of title wordsSentence lengthPresence of modal auxiliariesTense of the finite verbVoice of the finite verbPresence of Formulaic ExpressionsFigure 8: Other features usedAll features except Citation Location andCitation Type proved helpful for classification.Two different statistical models were used: a NaiveBayesian model as in Kupiec et al's (1995) exper-iment, cf.
Figure 9, and an ngram model over sen-tences, cf.
Figure 10.
Learning is supervised andtraining examples are provided by our previous hu-man annotation.
Classification preceeds sentenceby sentence.
The ngram model combines evidencefrom the context (Cm-1, Cm-2) and from I senten-tiai features (F,~,o...Fmj-t), assuming that thosetwo factors are independent ofeach other.
It usesthe same likelihood estimation as the Naive Bayes,but maximises a context-sensitive prior using theViterbi algorithm.
We received best results forn=2, i.e.
a bigram model.The results of stochastic lassification (pre-sented in figure 11) were compiled with a 10-foldcross-validation our 80-paper corpus, contain-ing a total of 12422 sentences (classified items).As the first baseline, we use a standard text cat-egorization method for classification (where eachsentence is considered as a document*) Baseline 1has an accuracy of 69%, which is low consideringthat the most frequent category (OWN) also coy-errs 69% of all sentences.
Worse still, the classifierclassifies almost all sentences as OWN and OTHERsegments (the most frequent categories).
Recall onthe rare categories but important categories AIM,TEXTUAL, CONTRAST and BASIS is zero or verylow.
Text classification is therefore not a solution.
*We used the Rainbow implementation of a Naive Bayestf/idf method, 10-fold cross-validation.Baseline 2, the most frequent category (OWN),is a particularly bad baseline: its recall on all cate-gories except OWN is zero.
We cannot see this badperformance in the percentage accuracy values,but only in the Kappa values (measured againstone human annotator, i.e.
k=2).
As Kappa takesperformance on rare categories into account more,it is a more intuitive measure for our task.In figure 11, NB refers to the Naive Bayes model,and NB+ to the Naive Bayes model augmentedwith the ngram model.
We can see that thestochastic models obtain substantial improvementover the baselines, particularly with respect to pre-cision and recall of the rare categories, raising re-call considerably in all cases, while keeping preci-sion at the same level as Baseline 1 or improvingit (exception: precision for BASIS drops; precisionfor AIM is insignificantly lower).If we look at the contribution of single features(reported for the Naive Bayes system in figure 12),we see that Agent and Action features improvethe overall performance of the system by .02 and.04 Kappa points respectively (.36 to .38/.40).This is a good performance for single features.Agent is a strong feature beating both baselines.Taken by itself, its performance at K=.08 is stillweaker than some other features in the pool, e.g.the Headline feature (K=.19), the C i tat ion fea-ture (K=.I8) and the Absolute Location Fea-ture (K=.17).
(Figure 12 reports classification re-sults only for the stronger features, i.e.
those whoare better than Baseline 2).
The Action feature,if considered on its own, is rather weak: it showsa slightly better Kappa value than Baseline 2, butdoes not even reach the level of random agreement(K=0).
Nevertheless, if taken together with theother features, it still improves results.Building on the idea that intellectual attribu-tion is a segment-based phenomena, we improvedthe Agent feature by including history (featureSAgent).
The assumption is that in unmarked sen-tences the agent of the previous attribution isstillactive.
Wiebe (1994) also reports segment-basedagenthood as one of the most successful features.SAgent alone achieved a classification success ofK=.21, which makes SAgent the best single fea-tures available in the entire feature pool.
Inclusionof SAgent to the final model improved results toK=.43 (bigram model).Figure 12 also shows that different features arebetter at disambiguating certain categories.
TheFormulaic feature, which is not very strong onits own, is the most diverse, as it contributes tothe disambiguation f six categories directly.
BothAgent and Action features disambiguate cate-,gories which many of the other 12 features cannotdisambiguate ( .g.
CONTRAST), and SAgent addi-tionally contributes towards the determination fBACKGROUND zones (along with the Fo~ula icand the Absolute Location feature).14P(CIFo, ..., F,~_,) ~ P(C) Nj~---?l P(FyIC)n- -1I ' I j=o P (F j )P(CIFo .... , F.-i ):P(C):P(FjIC):P(FA:Probability that a sentence has target category C, given its feature values F0, .
.
.
,F .
- i ;(OveraU) probability of category C);Probability of feature-value pair Fj, given that the sentence is of target category C;Probability of feature value Fj;Figure 9: Naive Bayesian ClassifierI--I F CP(CmlFm,o,.
.,F~,~-i,C0,.
.
,6~-1) ~ P(V,~lCm-l,C~-2) l-I~=?P( ~,~1 ,~)? "
l - -1  FI~=o P(Fm,~)m:l:P( C,~IF~,o, .
.
.
, F,~,~-t, Co , .
.
.
,  C,~-l ):P (C ,~IC~- , ,C~-2) :P(F,~j\[C,~):P(F~,j):index of sentence (ruth sentence in text)number of features consideredtarget category associated with sentence at index mProbability that sentence rn has target category Cm, given itsfeature values Fro,o, .
.
.
,  Fmj-1 and given its context Co, ...C,~-1;Probability that sentence rn has target category C, given the cat-egories of the two previous entences;Probability of feature-value pair Fj occu~ing within target cate-gory C at position m;Probability of feature value Fmj;Figure 10: Bigram Model5 Discuss ionThe result for automatic lassification is in agree-ment with our previous experimental results forhuman classification: humans, too, recognize thecategories AIM and TEXTUAL most robustly (cf.Figure 11).
AIM and TEXTUAL sentences, tatingknowledge claims and organizing the text respec-tively, are conventionalized to a high degree.
Thesystem's results for AIM sentences, for instance,compares favourably to similar sentence xtractionexperiments (cf.
Kupiec et al's (1995) results of42%/42% recall and precision for extracting "rel-evant" sentences from scientific articles).
BASISand CONTRAST sentences have a less prototypicalsyntactic realization, and they also occur at lesspredictable places in the document.
Therefore, itis far more difficult for both machine and humanto recognize such sentences.While the system does well for AIM and TEX-TUAL sentences, and provides ubstantial improve-ment over both baselines, the difference to humanperformance is still quite large (cf.
figure 11).
Weattribute most of this difference to the modest sizeof our training corpus: 80 papers are not much formachine learning of such high-level features.
It ispossible that a more sophisticated model, in com-bination with more training material, would im-prove results significantly.
However, when we ranthem on our data as it is now, different other sta-tistical models, e.g.
Ripper (Cohen, 1996) and aMaximum Entropy model, all showed similar nu-merical results.Another factor which decreases results are in-consistencies in the training data: we discoveredthat 4% of the sentences with the same featureswere classified differently by the human annota-tion.
This points to the fact that our set of fea-tures could be made more distinctive.
In mostof these cases, there were linguistic expressionspresent, such as subtle signs of criticism, whichhumans correctly identified, but for which the fea-tures are too coarse.
Therefore, the addition of"deeper" features to the pool, which model the se-mantics of the meta-discourse hallowly, seemeda promising avenue.
We consider the automaticand robust recognition of agents and actions, aspresented here, to be the first incarnations of suchfeatures.6 Conc lus ionsArgumentative zoning is the task of breaking atext containing a scientific argument into linearzones of the same argumentative status, or zonesof the same intellectual attribution.
We plan touse argumentative zoning as a first step for IR andshallow document understanding tasks like sum-marization.
In contrast o hierarchical segmenta-tion (e.g.
Marcu's (1997) work, which is based onRST (Mann and Thompson, 1987)), this type ofsegmentation aims at capturing the argumentativestatus of a piece of text in respect o the overallargumentative act of the paper.
It does not deter-15I Method Acc.
K Precision/recall per category (in %) I(~) AIM CONTR.
TXT.
OWN BACKG.
BASIS OTHERI Human Performance 87 .71 72/56 50/55 79/79 94/92 68/75 82/34 74/83 \]I NB+ (best results) 71 .43 40/53 33/20 62/57 85/85 30/58 28/31 50/38 II NB (best results) 7'2 .41 42/60 34/22 61/60 82/90 40/43 27/41 53/29 I.I BasoL 1: Text catog 69 13 44/9 32/42 58/14 77/90 20/5 47/12 31/16 II Basel.
2: Most freq.
cat.
69 -.12 0/0 0/0 0/0 69/100 0/0 0/0 0/0 IFigure 11: Accuracy, Kappa, Precision and Recall of Human and Automatic Processing, in comparisonto baselinesFeatures used Acc.
K Precision/recallper category(in%)(Naive Bayes System) (%) AIM CONTR.
TXT.
OWN BACKG.
BASIS OTHERAction alone 68 -.II 0/0 43/1 0/0 68/99 0/0 0/0 0/0Agent alone 67 .08 0/0 0/0 0/0 71/93 0/0 0/0 36/23Shgent alone 70 .21 0/0 17/0 0/0 74/94 53/16 0/0 46/33Abs.
Locationalone 70 .17 0/0 0/0 0/0 74/97  40/36 0/0 28/9Headlinesalone 69 .19 0/0 0/0 0/0 75/95 0/0 0/0 29/25CitaCionalone 70 .18 0/0 0/0 0/0 73/96 0/0 0/0 43/30Citat2on Type alone 70 .13 0/0 0/0 0/0 72/98 0/0 0/0 43/24Citation Locat.
alone 70 .13 0/0 0/0 0/0 72/97 0/0 0/0 43/24Foz~mlaicalone 70 .07 40/2 45/2 75/39 71/98 0/0 40/1 47/1312 other features 71 .36 37/53 32/17 54/47 81/91 39/41 22/32 45/2212 fea.+hction 71 .38 38/57 34/22 58/59 81/91 39/40 25/38 48/2212fea.+hgent 72 .40 40/57 35/18 59/51 82/91 39/43 25/34 52/2912fea.+SAgent 73 .40 39/57 33/19 61/51 81/91 42/43 25/33 52/2912 ~a.+Action+hgent 71 .43 40/53 33/20 62/57 85/85 30/58 28/31 50/3812 fea.+Action+Shgen~ 73 .41 41/59 34/22 62/61 82/91 41/42 27/39 51/29Figure 12: Accuracy, Kappa,individual featuresPrecision and Recall of Automatic Processing (Naive Bayes system), permine the rhetorical structure within zones.
Sub-zone structure is most likely related to domain-specific rhetorical relations which are not directlyrelevant to the discourse-level relations we wish torecognize.We have presented a fully implemented proto-type for argumentative zoning.
Its main inno-vation are two new features: prototypical agentsand actions - -  semi-shallow representations of theoverall scientific argumentation f the article.
Foragent and action recognition, we use syntacticheuristics and two extensive libraries of patterns.Processing is robust and very low in error.
Weevaluated the system without and with the agentand action features and found that the features im-prove results for automatic argumentative zoningconsiderably.
History-aware agents are the bestsingle feature in a large, extensively tested featurepool.ReferencesBiber, Douglas.
1995.
Dimensions of Register Varia-tion: A Cross-linguistic Comparison.
Cambridge,England: Cambridge University Press.Carletta, Jean.
1996.
Assessing agreement on classi-fication tasks: The kappa statistic.
ComputationalLinguistics 22(2): 249-.-254.Cohen, William W. 1996.
Learning trees and ruleswith set-valued features.
In Proceedings ofAAAL96.Grocer, Claire, Andrei Mikheev, and Colin Mathe-son.
1999.
LT TTT Version 1.0: Text Tokenisa-tion Software.
Technical report, Human Commu-nication Research Centre, University of Edinburgh.ht tp  : / /~w.
ltg.
ed.
ac.
uk/software/ttt/.Hearst, Marti A.
1997.
TextTiling: Segmenting textinto multi-paragraph subtopic passages.
Computa-tional Linguistics 23(1): 33---64.Hyland, Ken.
1998.
Persuasion and context: The prag-matics of academic metadiscourse.
Journal o\] Prag-matics 30(4): 437-455.Kan, Min-Yen, Judith L. Klavans, and Kathleen R.McKeown.
1998.
Linear Segmentation and SegmentSignificance.
In Proceedings o~ the Sixth Workshopon Very Large Corpora (COLIN G/ACL-98), 197-205.Klavans, Judith L., and Min-Yen Kan. 1998.
Roleof verbs in document analysis.
In Proceedingsof 36th Annual Meeting o\] the Association /orComputational Linguistics and the 17th Interna-tional Conference on Computational Linguistics(,4 CL/COLING-gS), 68O--686.Krippendorff, Klaus.
1980.
Content Analysis: An In-troduction to its Methodology.
Beverly Hills, CA:Sage Publications.Kupiee, Julian, Jan O. Pedersen, and Franeine Chela.161995.
A trainable document summarizer.
In Pro-ceedings of the 18th Annual International Confer-ence on Research and Development in InformationRetrieval (SIGIR-95), 68--73.Landis, J.R., and G.G.
Koch.
1977.
The Measurementof Observer Agreement for Categorical Data.
Bio-metrics 33: 159-174.Lawrence, Steve, C. Lee Giles, and Ku_t Bollaeker.1999.
Digital libraries and autonomous citation in-dexing.
IEEE Computer 32(6): 67-71.Levin, Beth.
1993.
English Verb Classes and Alterna-tions.
Chicago, IL: University of Chicago Press.Mann, William C., and Sandra A. Thompson.
1987.Rhetorical Structure Theory: Description and Con-struction of text structures.
In Gerard Kempen,ed., Natural Language Generation: New Results inArtificial Intelligence, Psychology, and Linguistics,85-95.
Dordrecht, NL: Marinus Nijhoff Publishers.Marcu, Daniel.
1997.
From Discourse Structures toText Summaries.
In Inderjeet Mani and Mark T.Maybury, eds., Proceedings of the ACL/EACL-97Workshop on Intelligent Scalable Text Summariza-tion, 82-88.Morris, Jane, and Graeme Hirst.
1991.
Lexical cohe-sion computed by thesau.ral relations as an indicatorof the structure of text.
Computational Linguistics17: 21-48.Myers, Greg.
1992.
In this paper we report...---speechacts and scientific facts.
Journal of Pragmatics17(4): 295-313.:Nanba, I:Iidetsugu, and Manabu Okumura.
1999.
To-wards multi-paper summarization using referencein.formation.
In Proceedings of IJCAI-99, 926-931. http://galaga, jaist, ac.
jp: 8000/'nanba/study/papers .html.Paice, Chris D. 1981.
The automatic generation ofliterary abstracts: an approach based on the iden-tification of self-indicating phrases.
In Robert Nor-man Oddy, Stephen E. Robertson, Cornelis Joostvan Pdjsbergen, and P. W. Williams, eds., Infor-mation Retrieval Research, 172-191.
London, UK:Butterworth.Paice, Chris D. 1990.
Constructing literature abstractsby computer: techniques and prospects.
Informa-tion Processing and Management 26: 171-186.Reynar, Jeffrey C. 1999.
Statistical models for topicsegmentation.
In Proceedings of the 37th AnnualMeeting of the Association for Computational Lin-guistics (A CL-99), 357-364.Riley, Kathryn.
1991.
Passive voice and rhetorical rolein scientific writing.
Journal of Technical Writingand Communication 21(3): 239--257.Rowley, Jennifer.
1982.
Abstracting and Indexing.London, UK: Bingley.Siegel, Sidney, and N. John Jr. CasteUan.
1988.
Non-parametric Statistics for the Behavioral Sciences.Berkeley, CA: McGraw-Hill, 2nd edn.Swales, John.
1990.
Genre Analysis: English in Aca-demic and Research Settings.
Chapter 7: Researcharticles in English, 110-.-176.
Cambridge, UK: Cam-bridge University Press.Teufel, Simone, Jean Carletta, and Marc Moens.
1999.An annotation scheme for discourse-level argumen-tation in research articles.
In Proceedings of the 8thMeeting of the European Chapter of the Associationfor Computational Linguistics (EA CL-99), 110-117.Teufel, Simone, and Marc Moens.
1999.
Argumenta-tive classification of extracted sentences as a firststep towards flexible abstracting.
In Inderjeet Maniand Mark T. Maybury, eds., Advances in Auto-matic Text Summarization, 155-171.
Cambridge,MA: MIT Press.Thompson, Geoff, and Ye Yiyun.
1991.
Evaluation inthe reporting verbs used in academic papers.
Ap-plied Linguistics 12(4): 365-382.Wellons, M. E., and G. P. Purcell.
1999.
Task-specificextracts for using the medical iterature.
In Pro-ceedings of the American Medical Informatics Sym-posium, 1004-1008.Wiebe, Janyce.
1994.
Tracking point of view in narra-tive.
Computational Linguistics 20(2): 223-287.17
