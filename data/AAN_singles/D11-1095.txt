Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023?1033,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsHierarchical Verb Clustering Using Graph FactorizationLin Sun and Anna KorhonenUniversity of Cambridge, Computer Laboratory15 JJ Thomson Avenue, Cambridge CB3 0GD, UKls418,alk23@cl.cam.ac.ukAbstractMost previous research on verb clustering hasfocussed on acquiring flat classifications fromcorpus data, although many manually builtclassifications are taxonomic in nature.
AlsoNatural Language Processing (NLP) applica-tions benefit from taxonomic classificationsbecause they vary in terms of the granularitythey require from a classification.
We intro-duce a new clustering method called Hierar-chical Graph Factorization Clustering (HGFC)and extend it so that it is optimal for the task.Our results show that HGFC outperforms thefrequently used agglomerative clustering on ahierarchical test set extracted from VerbNet,and that it yields state-of-the-art performancealso on a flat test set.
We demonstrate how themethod can be used to acquire novel classifi-cations as well as to extend existing ones onthe basis of some prior knowledge about theclassification.1 IntroductionA variety of verb classifications have been built tosupport NLP tasks.
These include syntactic and se-mantic classifications, as well as ones which in-tegrate aspects of both (Grishman et al, 1994;Miller, 1995; Baker et al, 1998; Palmer et al, 2005;Kipper, 2005; Hovy et al, 2006).
Classificationswhich integrate a wide range of linguistic proper-ties can be particularly useful for tasks sufferingfrom data sparseness.
One such classification isthe taxonomy of English verbs proposed by Levin(1993) which is based on shared (morpho-)syntacticand semantic properties of verbs.
Levin?s taxon-omy or its extended version in VerbNet (Kipper,2005) has proved helpful for various NLP applica-tion tasks, including e.g.
parsing, word sense disam-biguation, semantic role labeling, information ex-traction, question-answering, and machine transla-tion (Swier and Stevenson, 2004; Dang, 2004; Shiand Mihalcea, 2005; Zapirain et al, 2008).Because verbs change their meaning and be-haviour across domains, it is important to be ableto tune existing classifications as well to build novelones in a cost-effective manner, when required.
Inrecent years, a variety of approaches have been pro-posed for automatic induction of Levin style classesfrom corpus data which could be used for this pur-pose (Schulte im Walde, 2006; Joanis et al, 2008;Sun et al, 2008; Li and Brew, 2008; Korhonenet al, 2008; O?
Se?aghdha and Copestake, 2008; Vla-chos et al, 2009).
The best of such approacheshave yielded promising results.
However, they havemostly focussed on acquiring and evaluating flatclassifications.
Levin?s classification is not flat, buttaxonomic in nature, which is practical for NLP pur-poses since applications differ in terms of the gran-ularity they require from a classification.In this paper, we experiment with hierarchicalLevin-style clustering.
We adopt as our baselinemethod a well-known hierarchical method ?
ag-glomerative clustering (AGG) ?
which has been pre-viously used to acquire flat Levin-style classifica-tions (Stevenson and Joanis, 2003) as well as hierar-chical verb classifications not based on Levin (Fer-rer, 2004; Schulte im Walde, 2008).
The method hasalso been popular in the related task of noun clus-1023tering (Ushioda, 1996; Matsuo et al, 2006; Bassiouand Kotropoulos, 2011).We introduce then a new method called Hierar-chical Graph Factorization Clustering (HGFC) (Yuet al, 2006).
This graph-based, probabilistic cluster-ing algorithm has some clear advantages over AGG(e.g.
it delays the decision on a verb?s cluster mem-bership at any level until a full graph is available,minimising the problem of error propagation) and ithas been shown to perform better than several otherhierarchical clustering methods in recent compar-isons (Yu et al, 2006).
The method has been appliedto the identification of social network communities(Lin et al, 2008), but has not been used (to the bestof our knowledge) in NLP before.We modify HGFC with a new tree extraction al-gorithm which ensures a more consistent result, andwe propose two novel extensions to it.
The first is amethod for automatically determining the tree struc-ture (i.e.
number of clusters to be produced for eachlevel of the hierarchy).
This avoids the need to pre-determine the number of clusters manually.
The sec-ond is addition of soft constraints to guide the clus-tering performance (Vlachos et al, 2009).
This isuseful for situations where a partial (e.g.
a flat) verbclassification is available and the goal is to extend it.Adopting a set of lexical and syntactic featureswhich have performed well in previous works, wecompare the performance of the two methods on testsets extracted from Levin and VerbNet.
When eval-uated on a flat clustering task, HGFC outperformsAGG and performs very similarly with the best flatclustering method reported on the same test set (Sunand Korhonen, 2009).
When evaluated on a hierar-chical task, HGFC performs considerably better thanAGG at all levels of gold standard classification.
Theconstrained version of HGFC performs the best, asexpected, demonstrating the usefulness of soft con-straints for extending partial classifications.Our qualitative analysis shows that HGFC is ca-pable of detecting novel information not included inour gold standards.
The unconstrained version canbe used to acquire novel classifications from scratchwhile the constrained version can be used to extendexisting ones with additional class members, classesand levels of hierarchy.2 Target classification and test setsThe taxonomy of Levin (1993) groups English verbs(e.g.
break, fracture, rip) into classes (e.g.
45.1Break verbs) on the basis of their shared mean-ing components and (morpho-)syntactic behaviour,defined in terms of diathesis alternations (e.g.
thecausative/inchoative alternation, where an NP framealternates with an intransitive frame: Tony broke thewindow ?
The window broke).
It classifies over3000 verbs in 57 top level classes, some of whichdivide further into subclasses.
The extended versionof the taxonomy in VerbNet (Kipper, 2005) classifies5757 verbs.
Its 5 level taxonomy includes 101 toplevel and 369 subclasses.
We used three gold stan-dards (and corresponding test sets) extracted fromthese resources in our experiments:T1: The first gold standard is a flat gold standardwhich includes 13 classes appearing in Levin?s orig-inal taxonomy (Stevenson and Joanis, 2003).
We in-cluded this small gold standard in our experimentsso that we could compare the flat version of ourmethod against previously published methods.
Fol-lowing Stevenson and Joanis (2003), we selected 20verbs from each class which occur at least 100 timesin our corpus.
This gave us 260 verbs in total.T2: The second gold standard is a large, hi-erarchical gold standard which we extracted fromVerbNet as follows: 1) We removed all the verbsthat have less than 1000 occurrences in our cor-pus.
2) In order to minimise the problem of pol-ysemy, we assigned each verb to the class which,according to VerbNet, corresponds to its predomi-nant sense in WordNet (Miller, 1995).
3) In orderto minimise the sparse data problem with very fine-grained classes, we converted the resulting classifi-cation into a 3-level representation so that the classesat the 4th and 5th level were combined.
For exam-ple, the sub-classes of Declare verbs (numbered as29.4.1.1.
{1,2,3}) were combined into 29.4.1.
4) Theclasses that have fewer than 5 members were dis-carded.
The total number of verb senses in the re-sulting gold standard is 1750, which is 33.2% of theverbs in VerbNet.
T2 has 51 top level, 117 secondlevel, and 133 third level classes.T3: The third gold standard is a subset of T2where singular classes (top level classes which donot divide into subclasses) are removed.
This gold1024standard was constructed to enable proper evalua-tion of the constrained version of HGFC (introducedin the following section) where we want to com-pare the impact of constraints across several levelsof classification.
T3 provides classification of 357verbs into 11 top level, 14 second level, and 32 thirdlevel classes.For each verb appearing in T1-T3, we extractedall the occurrences (up to 10,000) from the BritishNational Corpus (Leech, 1992) and North AmericanNews Text Corpus (Graff, 1995).3 Method3.1 Features and feature extractionPrevious works on Levin style verb classificationhave investigated optimal features for this task(Stevenson and Joanis, 2003; Li and Brew, 2008;Sun and Korhonen, 2009)).
We adopt for our exper-iments a set of features which have performed wellin recent verb clustering works:A: Subcategorization frames (SCFs) and their rela-tive frequencies with individual verbs.B: A with SCFs parameterized for prepositions.C: B with SCFs parameterized for subjects appear-ing in grammatical relations associated with theverb in parsed data.D: B with SCFs parameterized for objects appear-ing in grammatical relations associated with theverb in parsed data.These features are purely syntactic.
Althoughsemantic features ?
verb selectional preferences ?proved the best (when used in combination with syn-tactic features) in the recent work of Sun and Ko-rhonen (2009), we left such features for future workbecause we noticed that different levels of classifi-cation are likely to require semantic features at dif-ferent granularities.We extracted the syntactic features using the sys-tem of Preiss et al (2007).
The system tags, lemma-tizes and parses corpus data using the RASP (RobustAccurate Statistical Parsing toolkit (Briscoe et al,2006)), and on the basis of the resulting grammat-ical relations, assigns each occurrence of a verb asa member of one of the 168 verbal SCFs.
We pa-rameterized the SCFs as described above using theinformation provided by the system.3.2 ClusteringWe introduce the agglomerative clustering (AGG)and Hierarchical Graph Factorization Clustering(HGFC) methods in the following two subsec-tions, respectively.
The subsequent two subsectionspresent our extensions to HGFC: (i) automaticallydetermining the cluster structure and (ii) adding softconstraints to guide clustering performance.3.2.1 Agglomerative clusteringAGG is a method which treats each verb as asingleton cluster and then successively merges twoclosest clusters until all the clusters have beenmerged into one.
We used the SciPy?s imple-mentation (Oliphant, 2007) of the algorithm.
Thecluster distance is measured using linkage criteria.We experimented with four commonly used link-age criteria: Single, Average, Complete and Ward?s(Ward Jr., 1963).
Ward?s criterion performed thebest and was used in all the experiments in this pa-per.
It measures the increase in variance after twoclusters are merged.
The output of AGG tends tohave excessive number of levels.
Cut-based meth-ods (Wu and Leahy, 1993; Shi and Malik, 2000) arefrequently applied to extract a simplified view.
Wefollowed previous verb clustering works and cut theAGG hierarchy manually.AGG suffers from two problems.
The first is er-ror propagation.
When a verb is misclassified at alower level, the error propagates to all the upper lev-els.
The second is local pairwise merging, i.e.
thefact that only two clusters can be combined at anylevel.
For example, in order to group clusters rep-resenting Levin classes 9.1, 9.2 and 9.3 into a sin-gle cluster representing class 9, the method has toproduce intermediate clusters, e.g.
9.
{1,2} and 9.3.Such clusters do not always have a semantic inter-pretation.
Although they can be removed using acut-based method, this requires a pre-defined cut-offvalue which is difficult to set (Stevenson and Joanis,2003).
In addition, a significant amount of informa-tion is lost in pair-wise clustering.
In the above ex-ample, only the clusters 9.
{1,2} and 9.3 are consid-ered, while alternative clusters 9.
{1,3} and 9.2 areignored.
Ideally, information about all the possibleintermediate clusters should be aggregated, but thisis intractable in practice.10253.2.2 Hierarchical Graph FactorizationClusteringOur new method HGFC derives a probabilistic bi-partite graph from the similarity matrix (Yu et al,2006).
The local and global clustering structures arelearned via the random walk properties of the graph.The method does not suffer from the above prob-lems with AGG.
Firstly, there is no error propagationbecause the decision on a verb?s membership at anylevel is delayed until the full bipartite graph is avail-able and until a tree structure can be extracted fromit by aggregating probabilistic information from allthe levels.
Secondly, the bipartite graph enables theconstruction of a hierarchical structure without anyintermediate classes.
For example, we can groupclasses 9.
{1,2,3} directly into class 9.We use HGFC with the distributional similaritymeasure Jensen-Shannon Divergence (djs(v, v?
)).Given a set of verbs, V = {vn}Nn=1, wecompute a similarity matrix W where Wij =exp(?djs(v1, v2)).
W can be encoded by a undi-rected graph G (Figure 1(a)), where the verbs aremapped to vertices and the Wij is the edge weightbetween vertices i and j.The graph G and the cluster structure can be rep-resented by a bipartite graph K(V,U).
V are thevertices onG.
U = {up}mp=1 represent the hiddenmclusters.
For example, looking at Figure 1(b), V onG can be grouped into three clusters u1, u2 and u3.The matrix B denotes the n ?m adjacency matrix,with bip being the connection weight between thevertex vi and the cluster up.
Thus, B represents theconnections between clusters at an upper and lowerlevel of clustering.
A flat clustering algorithm canbe induced by computing B.The bipartite graph K also induces a similarity(W ?)
between vi and vj : w?ij =?mp=1bipbjp?p =(B?
?1BT )ij where ?
= diag(?1, ..., ?m).
There-fore,B can be found by approximating the similaritymatrix W of G using W ?
derived from K. Given adistance function ?
between two similarity matrices,B approximates W by minimizing the cost function?(W,B?
?1BT ).
The coupling between B and ?
isremoved by setting H = B??1:minH,??
(W,H?HT ), s.t.n?i=1hip = 1 (1)We use the divergence distance: ?
(X,Y ) =?ij(xij logxijyij ?xij+yij).
Yu et al (2006) showedthat this cost function is non-increasing under theupdate rule:h?ip ?
hip?jwij(H?HT )ij?phjp s.t.
?ih?ip = 1 (2)?
?p ?
?p?jwij(H?HT )ijhiphjp s.t.?p?
?p =?ijwij (3)wij can be interpreted as the probability of the di-rect transition between vi and vj : wij = p(vi, vj),when?ij wij = 1. bip can be interpreted as:p(up, uq) = p(up)p(up|uq) =n?i=1bipbiqdi= (BTD?1B)pq (4)D = diag(d1, ..., dn) where di =m?p=0bipp(up, uq) is the similarity between the clusters.
Ittakes into account of a weighted average of contri-butions from all the data.
This is different from thelinkage method where only the data from two clus-ters are considered.Given the cluster similarity p(up, uq), we can con-struct a new graphG1 (Figure 1(d)) with the clustersU as vertices.
The cluster algorithm can be appliedagain (Figure 1(e)).
This process can go on itera-tively, leading to a hierarchical graph.Algorithm 1 HGFC algorithm (Yu et al, 2006)Require: N verbs V , number of clusters ml for L levelsCompute the similarity matrix W0 from VBuild the graph G0 from W0 , and m0 ?
nfor l = 1, 2 to L doFactorizeGl?1 to obtain bipartite graph Kl with theadjacency matrix Bl (eq.
1, 2 and 3)Build a graph Gl with similarity matrix Wl =BTl D?1l Bl according to equation 4end forreturn BL, BL?1...B1Additional steps need to be performed in order toextract a tree from the hierarchical graph.
Yu et al(2006) performs the extraction via a propagation ofprobabilities from the bottom level clusters.
For averb vi, the probability of assigning it to cluster v(l)pat level l is given by:1026v1v6v2v4v3v5v7 v8v9(a)v1v7v6v9v8v2v3v4v5u1u2u3(b)u3u1u2v1v6v2v4v3v5v7 v8v9(c)u1 u2u3(d)v1v7v6v9v8v2v3v4v5u1u2u3q1q2(e)Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusterson G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1p(v(l)p |vi) =?Vl?1...?V1p(v(l)p |v(l?1))...p(v(1)|vi)= (D(?1)1 B1D?12 B2D?13 B3...D?1l Bl)ip (5)This method might not extract a consistent treestructure, because the cluster membership at thelower level does not constrain the upper level mem-bership.
This prevented us from extracting a Levinstyle hierarchical classification in our initial experi-ments.
For example, where two verbs were groupedtogether at a lower level, they could belong to sepa-rate clusters at an upper level.
We therefore proposea new tree extraction algorithm (Algorithm 2).The new algorithm starts from the top level bipar-tite graph, and generates consistent labels for eachlevel by taking into account of the tree constraintsset at upper levels.Algorithm 2 Tree extraction algorithm for HGFCRequire: Given N , (Bl,ml) on each level for L levelsOn the top level L, collect the labels TL (eq.
5)Define C to be a (mL?1 ?mL) zero matrix, Cij ?
1,where i, j = arg maxi,j{BLij}for l = L?
1 to 1 dofor i = 1 to N doCompute p(vlp|vi) for each cluster p (eq.
5)tli = argmaxp{p(vlp|vi)|p = 1...ml, Cptl+1i 6= 0}end forRedefine C to be a (ml?1?ml) zero matrix, Cij ?1, where i, j = arg maxi,j{Blij}end forreturn Tree consistent labels TL, TL?1...T 13.2.3 Automatically determining the number ofclusters for HGFCHGFC needs the number of levels and clusters ateach level as input.
However, this information is notalways available (e.g.
when the goal is to actuallylearn this information automatically).
We thereforepropose a method for inferring the cluster structurefrom data.
As shown in figure 1, a similarity ma-trix W models one-hop transitions that follow thelinks from vertices to neighbors.
A walker can alsogo to other vertices via multi-hop transitions.
Ac-cording to the chain rule of the Markov process, themulti-hop transitions indicate a decaying similarityfunction on the graph (Yu et al, 2006).
After t tran-sitions, the similarity matrix (Wt) becomes:Wt = Wt?1D?10 W0Yu et al (2006) proved the correspondence be-tween the HGFC levels (l) and the random walk time:t = 2l?1.
So the vertices at level l induce a sim-ilarity matrix of verbs after t-hop transitions.
Thedecaying similarity function captures the differentscales of clustering structure in the data (Azran andGhahramani, 2006b).
The upper levels would havea smaller number of clusters which represent a moreglobal structure.
After several levels, all the verbsare expected to be grouped into one cluster.
Thenumber of levels and clusters at each level can thusbe learned automatically.We therefore propose a method that uses the de-caying similarity function to learn the hierarchicalclustering structure.
One simple modification to al-gorithm 1 is to set the number of clusters at level l1027(ml) to be ml?1 ?
1. m is denoted as the numberof clusters that have at least one member accordingto eq.
5.
We start by treating each verb as a clusterat the bottom level.
The algorithm stops when allthe data points are merged into one cluster.
The in-creasingly decaying similarity causes many clustersto have 0 members especially at lower levels, whichare pruned in the tree extraction.3.2.4 Adding constraints to HGFCThe basic version of HGFC makes no prior as-sumptions about the classification.
It is usefulfor learning novel verb classifications from scratch.However, when wishing to extend an existing clas-sification (e.g.
VerbNet) it may be desirable to guidethe clustering performance on the basis of informa-tion that is already known.
We propose a constrainedversion of HGFC which makes uses of labels at thebottom level to learn upper level classifications.
Wedo this by adding soft constraints to clustering, fol-lowing Vlachos et al (2009).We modify the similarity matrix W as follows: Iftwo verbs have different labels (li 6= lj), the simi-larity between them is decreased by a factor a, anda < 1.
We set a to 0.5 in the experiments.
The re-sulting tree is generally consistent with the originalclassification.
The influence of the underlying data(domain or features) is reduced according to a.4 Experimental evaluationWe applied the clustering methods introduced insection 3 to the test sets described in section 2 andevaluated them both quantitatively and qualitatively,as described in the subsequent sections.4.1 Evaluation methodsWe used class based accuracy (ACC) and adjustedrand index (Radj) to evaluate the results on the flattest set T1 (see section 2 for details of T1-T3).ACC is the proportion of members of dominantclusters DOM-CLUSTi within all classes ci.ACC =?Ci=1 verbs in DOM-CLUSTinumber of verbsThe formula of Radj is (Hubert and Arabie, 1985):Radj =?i,j(nij2)?
?i(ni?2)?j(n?j2)/(n2)12 [?i(ni?2)+?j(n?j2)]?
?i(ni?2)?j(n?j2)/(n2)where nij is the size of the intersection betweenclass i and cluster j.We used normalized mutual information (NMI)and F-Score (F) to evaluate hierarchical clusteringresults on T2 and T3.
NMI measures the amount ofstatistical information shared by two random vari-ables representing the clustering result and the gold-standard labels.
Given random variables A and B:NMI(A,B) = I(A;B)[H(A) +H(B)]/2I(A,B) =?k?j|(vk ?
cj |N logN |vk ?
cj ||vk||cj |where |vk ?
cj | is the number of shared member-ship between cluster vk and gold-standard class cj .The normalized variant of mutual information (MI)enables the comparison of clustering with differentcluster numbers (Manning et al, 2008).F is the harmonic mean of precision (P) and re-call (R).
P is calculated using modified purity ?
aglobal measure which evaluates the mean precisionof clusters.
Each cluster is associated with its preva-lent class.
The number of verbs in a cluster K thattake this class is denoted by nprevalent(K).mPUR =?nprevalent(ki)>2nprevalent(ki)number of verbsR is calculated using ACC.F = 2 ?mPUR ?
ACCmPUR + ACCF is not suitable for comparing results with dif-ferent cluster numbers (Rosenberg and Hirschberg,2007).
Therefore, we only report NMI when thenumber of classes in clustering and gold-standard issubstantially different.Finally, we supplemented quantitative evaluationwith qualitative evaluation of clusters produced bydifferent methods.4.2 Quantitative evaluationWe first evaluated AGG and the basic (uncon-strained) HGFC on the small flat test set T1.
Themain purpose of this evaluation was to compare theresults of our methods against previously publishedresults on the same test set.
The number of clus-ters (K) and levels (L) were inferred automaticallyfor HGFC as described in section 3.2.3.
However, to1028make the results comparable with previously pub-lished ones, we cut the resulting hierarchy at thelevel of closest match (12 clusters) to the K (13) inthe gold-standard.
For AGG, we cut the hierarchy at13 clusters.Method ACC RadjHGFC 41.2 17.4AGG (reproduced) 32.7 9.9AGG (Stevenson and Joanis (2003) 31.0 9.0Table 1: Comparison against Stevenson and Joanis(2003)?s result on T1 (using similar features).Table 1 shows our results and the results ofStevenson and Joanis (2003) on T1 when employingAGG using Ward as the linkage criterion.
In this ex-periment, we used the same feature set as Stevensonand Joanis (2003) (set B, see section 3.1) and weretherefore able to reproduce their AGG result with adifference smaller than 2%.
When using this simplefeature set, HGFC outperforms the best performingAGG clearly: 8.5% in ACC and 7.3% in Radj .We also compared HGFC against the best reportedclustering method on T1 to date ?
that of spectralclustering by Sun and Korhonen (2009).
We usedthe feature sets C and D which are similar to thefeatures (SCF parameterized by lexical prefences) intheir experiments.
HGFC obtains F of 49.93% on T1which is 5% lower than the result of Sun and Ko-rhonen (2009).
The difference comes from the treeconsistency requirement.
When the HGFC is forcedto produce a flat clustering (a one level tree only), itachieves the F of 52.55% which is very close to theperformance of spectral clustering.We then evaluated our methods on the hierarchi-cal test sets T2 and T3.
In the first set of experi-ments, we pre-defined the tree structure for HGFCby setting L to 3 and K at each level to be the Kin the hierarchical gold standard.
The hierarchy pro-duced by AGG was cut into 3 levels according to Ksin the gold standard.
This enabled direct evaluationof the results against the 3 level gold standards usingboth NMI and F.The results are reported in tables 2 and 3.
In thesetables, Nc is the number of clusters in HGFC cluster-ing while Nl is the number of classes in the goldstandard (the two do not always correspond per-fectly because a few clusters have zero members).Nc NlHGFCunconstrainedAGGNMI F NMI F130 133 57.31 36.65 54.22 32.62114 117 54.67 37.96 51.35 32.4450 51 37.75 40.00 32.61 32.78Table 2: Performance on T2 using a pre-defined treestructure.Nc NlHGFCunconstrainedHGFCconstrainedAGGNMI F NMI F NMI F31 32 51.65 42.01 91.47 92.07 49.70 40.3015 14 42.75 47.70 82.16 82.80 39.19 43.6911 11 38.91 51.17 71.69 75.00 34.88 44.80Table 3: Performance on T3 using a pre-defined treestructure.Table 2 compares the results of the unconstrainedversion of HGFC against those of AGG on our largesttest set T2.
As with T1, HGFC outperforms AGGclearly.
The benefit can now be seen at 3 differentlevels of hierarchy.
On average, the HGFC outper-forms AGG 3.5% in NMI and 4.8% in F. The dif-ference between the methods becomes clearer whenmoving towards the upper levels of the hierarchy.Table 3 shows the results of both unconstrainedand constrained versions of HGFC and those ofAGG on the test set T3 (where singular classes areremoved to enable proper evaluation of the con-strained method).
The results are generally gener-ally better on this test set than on T2 ?
which is to beexpected since T3 is a refined subset of T21.Recall that the constrained version of HGFC learnsthe upper levels of classification on the basis of softconstraints set at the bottom level, as described ear-lier in section 3.2.4.
As a consequence, NMI and Fare both greater than 90% at the bottom level andthe results at the top level are notably lower becausethe impact of the constraints degrades the furtheraway one moves from the bottom level.
Yet, the rela-tively high result across all levels shows that the con-strained version of HGFC can be employed a usefulmethod to extend the hierarchical structure of knownclassifications.1NMI is higher on T2, however, because NMI has a higherbaseline for larger number of clusters (Vinh et al, 2009).
NMIis not ideal for comparing the results of T2 and T3.1029T2 T3Nc Nl HGFC Nc Nl HGFC148 133 53.26 64 32 54.9197 117 49.85 35 32 50.8346 51 33.55 20 14 44.0219 51 25.80 10 14 34.419 51 19.17 6 11 32.273 51 13.06Table 4: NMI of unconstrained HGFC when trees for T2and T3 are inferred automatically.Finally, Table 4 shows the results for the uncon-strained HGFC on T2 and and T3 when the tree struc-ture is not pre-defined but inferred automatically asdescribed in section 3.2.3.
6 levels are learned forT2 and 5 for T3.
The number of clusters producedranges from 3 to 148 for T2 and from 6 to 64 forT3.
We can see that the automatically detected clus-ter numbers distribute evenly across different levels.The scale of the clustering structure is more com-plete here than in the gold standards.In the table, Nc indicates the number of clustersin the inferred tree, while Nl indicates the closestmatch to the number of classes in the gold stan-dard.
This evaluation is not fully reliable becausethe match between the gold standard and the cluster-ing is poor at some levels of hierarchy.
However, itis encouraging to see that the results do not drop dra-matically until the match between the two is reallypoor.4.3 Qualitative evaluationTo gain a better insight into the performance ofHGFC, we conducted further qualitative analysis ofthe clusters the two versions of this method pro-duced for T3.
We focussed on the top level of 11clusters (in the evaluation against the hierarchicalgold standard, see table 3) as the impact of soft con-straints is the weakest for the constrained method atthis level.As expected, the constrained HGFC kept many in-dividual verbs belonging to same Verbnet subclasstogether (e.g.
verbs enjoy, hate, disdain, regret, love,despise, detest, dislike, fear for the class 31.2.1) sothat most clusters simply group lower level classesand their members together.
Three nearly clean clus-ters were produced which only include sub-classesof the same class (e.g.
31.2.0 and 31.2.1 which bothbelong to 31.2 Admire verbs).
However, the remain-ing 8 clusters group together sub-classes (and theirmembers) belonging to unrelated parent classes.
In-terestingly, 6 of these make both syntactic and se-mantic sense.
For example, several such 37.7 Sayverbs and 29.5 Conjencture verbs are found togetherwhich share the meaning of communication andwhich take similar sentential complements.In contrast, none of the clusters produced bythe unconstrained HGFC represent a single VerbNetclass.
The majority represent a high number ofclasses and fewer members per class.
Yet many ofthe clusters make syntactic and semantic sense.
Agood example is a cluster which includes memberverbs from 9.7 Spray/Load verbs, 21.2 Carve verbs,51.3.1 Roll verbs, and 10.4 Wipe verbs.
The verbsincluded in this cluster share the meaning of specifictype of motion and show similar syntactic behaviour.Thorough Levin style investigation of especiallythe unconstrained method would require looking atshared diathesis alternations between cluster mem-bers.
We left this for future work.
However,the analysis we conducted confirmed that the con-strained method could indeed be used for extend-ing known classifications, while the unconstrainedmethod is more suitable for acquiring novel classi-fications from scratch.
The errors in clusters pro-duced by both methods were mostly due to syntacticidiosyncracy and the lack of semantic information inclustering.
We plan to address the latter problem inour future work.5 Discussion and conclusionWe have introduced a new graph-based method ?HGFC ?
to hierarchical verb clustering which avoidssome of the problems (e.g.
error propagation, pair-wise cluster merging) reported with the frequentlyused AGG method.
We modified HGFC so that it canbe used to automatically determine the tree struc-ture for clustering, and proposed two extensions toit which make it even more suitable for our task.
Thefirst involves automatically determining the numberof clusters to be produced, which is useful whenthis is not known in advance.
The second involvesadding soft constraints to guide the clustering per-formance, which is useful when aiming to extendexisting classification.1030The results reported in the previous section arepromising.
On a flat test set (T1), the unconstrainedversion of HGFC outperforms AGG and performsvery similarly with the best current flat clusteringmethod (spectral clustering) evaluated on the samedataset.
On the hierarchical test sets (T2 and T3),the unconstrained and constrained versions of HGFCoutperform AGG clearly at all levels of classification.The constrained version of HGFC detects the missinghierarchy from the existing gold standards with highaccuracy.
When the number of clusters and levelsis learned automatically, the unconstrained methodproduces a multi-level hierarchy.
Our evaluationagainst a 3-level gold standard shows that such a hi-erarchy is fairly accurate.
Finally, the results fromour qualitative evaluation show that both constrainedand unconstrained versions of HGFC are capable oflearning valuable novel information not included inthe gold standards.The previous work on Levin style verb classifica-tion has mostly focussed on flat classifications us-ing methods suitable for flat clustering (Schulte imWalde, 2006; Joanis et al, 2008; Sun et al, 2008; Liand Brew, 2008; Korhonen et al, 2008; O?
Se?aghdhaand Copestake, 2008; Vlachos et al, 2009).
How-ever, some works have employed hierarchical clus-tering as a method to infer flat clustering.For example, Schulte im Walde and Brew (2002)employed AGG to initialize the KMeans clusteringfor German verbs.
This gave better results thanrandom initialization.
Stevenson and Joanis (2003)used AGG for flat clustering on T1.
They cut the hi-erarchy at the number of classes in the gold standardand found that it is difficult to automatically deter-mine a good cut-off.
Our evaluation in the previoussection shows that HGFC outperforms their imple-mentation of AGG.AGG was also used by Ferrer (2004) who per-formed hierarchical clustering of 514 Spanish verbs.The results were evaluated against a hierarchicalgold standard resembling that of Levin?s classifi-cation in English (Va?zquez et al, 2000).
Radj of0.07 was reported for a 15-way classification whichis comparable to the result of Stevenson and Joanis(2003).Hierarchical clustering has also been performedfor the related task of semantic verb classification.For example, Basili et al (1993) identified the prob-lems of AGG, and applied a conceptual clustering al-gorithm (Fisher, 1987) to Italian verbs.
They usedsemi-automatically acquired semantic roles and theconcept types as features.
No quantitative resultswere reported.
The qualitative evaluation shows thatthe resulting clusters are very fine-grained.Schulte im Walde (2008) performed hierarchicalclustering of German verbs using human verb asso-ciation as features and AGG as a method.
They fo-cussed on two small collections of 56 and 104 verbsand evaluated the result against flat gold standardextracted from GermaNet (Kunze and Lemnitzer,2002) and German FrameNet (Erk et al, 2003), re-spectively.
They reported F of 62.69% for the 56verbs, and F of 34.68% for the 104 verbs.In the future, we plan to extend this research linein several directions.
First, we will try to deter-mine optimal features for different levels of clus-tering.
For example, the general syntactic features(e.g.
SCF) may perform the best at top levels of a hi-erarchy while more specific or refined features (e.g.SCF+pp) may be optimal at lower levels.
We alsoplan to investigate incorporating semantic features,like verb selectional preferences, in our feature set.It is likely that different levels of clustering requiremore or less specific selectional preferences.
Oneway to obtain the latter is hierarchical clustering ofrelevant noun data.In addition, we plan to apply the unconstrainedHGFC to specific domains to investigate its capabil-ity to learn novel, previously unknown classifica-tions.
As for the constrained version of HGFC, wewill conduct a larger scale experiment on the Verb-Net data to investigate what kind of upper level hi-erarchy it can propose for this resource (which cur-rently has over 100 top level classes).Finally, we plan to compare HGFC to other hier-archical clustering methods that are relatively newto NLP but have proved promising in other fields,including Bayesian Hierarchical Clustering (Hellerand Ghahramani, 2005; Teh et al, 2008) and themethod of Azran and Ghahramani (2006a) based onspectral clustering.6 AcknowledgementOur work was funded by the Royal Society Uni-versity Research Fellowship (AK), the DorothyHodgkin Postgraduate Award (LS), the EPSRC1031grants EP/F030061/1 and EP/G051070/1 (UK) andthe EU FP7 project ?PANACEA?.ReferencesArik Azran and Zoubin Ghahramani.
A new approachto data driven clustering.
In Proceedings of the 23rdinternational conference on Machine learning, ICML?06, pages 57?64, New York, NY, USA, 2006a.
ISBN1-59593-383-2.Arik Azran and Zoubin Ghahramani.
Spectral methodsfor automatic multiscale data clustering.
In Proceed-ings of the 2006 IEEE Computer Society Conferenceon Computer Vision and Pattern Recognition-Volume1, pages 190?197.
IEEE Computer Society Washing-ton, DC, USA, 2006b.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.The berkeley framenet project.
In In COLING-ACL,pages 86?90, 1998.Roberto.
Basili, Maria Teresa Pazienza, and Paola Ve-lardi.
Hierarchical clustering of verbs.
In Proceedingsof the Workshop on Acquisition of Lexical Knowledgefrom Text, 1993.Nikoletta Bassiou and Constantine Kotropoulos.
Longdistance bigram models applied to word clustering.Pattern Recogn., 44:145?158, January 2011.
ISSN0031-3203.Ted Briscoe, John Carroll, and Rebecca Watson.
Thesecond release of the rasp system.
In Proceedingsof the COLING/ACL on Interactive presentation ses-sions, 2006.Hoa Trang Dang.
Investigations into the Role of LexicalSemantics in Word Sense Disambiguation.
PhD thesis,CIS, University of Pennsylvania, 2004.Katrin Erk, Andrea Kowalski, Sebastian Pado?, and Man-fred Pinkal.
Towards a resource for lexical semantics:a large german corpus with extensive semantic anno-tation.
In Proceedings of the 41st Annual Meeting onAssociation for Computational Linguistics - Volume1, ACL ?03, pages 537?544, Stroudsburg, PA, USA,2003.
Association for Computational Linguistics.Eva Esteve Ferrer.
Towards a semantic classificationof spanish verbs based on subcategorisation informa-tion.
In Proceedings of the ACL 2004 workshop onStudent research, ACLstudent ?04, Stroudsburg, PA,USA, 2004.
Association for Computational Linguis-tics.Douglas H. Fisher.
Knowledge acquisition via incremen-tal conceptual clustering.
Machine Learning, 2:139?172, 1987.
ISSN 0885-6125.David Graff.
North american news text corpus.
LinguisticData Consortium, 1995.Ralph Grishman, Catherine Macleod, and Adam Meyers.Comlex syntax: Building a computational lexicon.
InCOLING, pages 268?272, 1994.Katherine A. Heller and Zoubin Ghahramani.
Bayesianhierarchical clustering.
In Proceedings of the 22ndinternational conference on Machine learning, pages297?304.
ACM, 2005.
ISBN 1595931805.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
Ontonotes: the90% solution.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, Com-panion Volume: Short Papers, NAACL-Short ?06,pages 57?60, Stroudsburg, PA, USA, 2006.
Associa-tion for Computational Linguistics.Lawrence Hubert and Phipps Arabie.
Comparing par-titions.
Journal of Classification, 2:193?218, 1985.ISSN 0176-4268.Eric Joanis, Suzanne Stevenson, and David James.
Ageneral feature space for automatic verb classification.Natural Language Engineering, 14(3):337?367, 2008.Karin Kipper.
VerbNet: A broad-coverage, comprehen-sive verb lexicon.
2005.Anna Korhonen, Yuval Krymolowski, and Nigel Collier.The Choice of Features for Classification of Verbs inBiomedical Texts.
In Proceedings of COLING, 2008.Claudia Kunze and Lothar Lemnitzer.
GermaNet-representation, visualization, application.
In Proceed-ings of LREC, 2002.Geoffrey Leech.
100 million words of english: thebritish national corpus.
Language Research, 28(1):1?13, 1992.Beth.
Levin.
English verb classes and alternations: Apreliminary investigation.
Chicago, IL, 1993.Jianguo Li and Chris Brew.
Which Are the Best Featuresfor Automatic Verb Classification.
In Proceedings ofACL, 2008.Yu-Ru Lin, Yun Chi, Shenghuo Zhu, Hari Sundaram, andBelle L. Tseng.
Facetnet: a framework for analyz-ing communities and their evolutions in dynamic net-works.
In Proceeding of the 17th international confer-ence on World Wide Web, pages 685?694, New York,NY, USA, 2008.
ACM.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schtze.
Introduction to Information Retrieval.Cambridge University Press, New York, NY, USA,2008.
ISBN 0521865719, 9780521865715.Yutaka Matsuo, Takeshi Sakaki, Ko?ki Uchiyama, andMitsuru Ishizuka.
Graph-based word clustering usinga web search engine.
In Proceedings of the EMNLP,pages 542?550, 2006.1032George A. Miller.
WordNet: a lexical database for En-glish.
Communications of the ACM, 38(11):39?41,1995.Travis E. Oliphant.
Python for scientific computing.Computing in Science and Engineering, 9:10?20,2007.
ISSN 1521-9615.Diarmuid O?
Se?aghdha and Ann Copestake.
Semanticclassification with distributional kernels.
In Proceed-ings of COLING, 2008.Martha Palmer, Daniel Gildea, and Paul Kingsbury.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106, 2005.Judita Preiss, Ted Briscoe, and Anna Korhonen.
A sys-tem for large-scale acquisition of verbal, nominal andadjectival subcategorization frames from corpora.
InProceedings of ACL, pages 912?919, 2007.Andrew Rosenberg and Julia Hirschberg.
V-measure: Aconditional entropy-based external cluster evaluationmeasure.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,2007.Sabine Schulte im Walde.
Experiments on the automaticinduction of german semantic verb classes.
Computa-tional Linguistics, 32(2), 2006.Sabine Schulte im Walde.
Human associations and thechoice of features for semantic verb classification.Research on Language and Computation, 6:79?111,2008.
ISSN 1570-7075.Sabine Schulte im Walde and Chris Brew.
Inducing ger-man semantic verb classes from purely syntactic sub-categorisation information.
In Proceedings of ACL,pages 223?230, 2002.Jianbo Shi and Jitendra Malik.
Normalized cuts and im-age segmentation.
IEEE Transactions on pattern anal-ysis and machine intelligence, 22(8):888?905, 2000.Lei Shi and Rada Mihalcea.
Putting pieces together:Combining FrameNet, VerbNet and WordNet for ro-bust semantic parsing.
In Proceedings of CICLING,2005.Suzanne Stevenson and Eric Joanis.
Semi-supervisedverb class discovery using noisy features.
In Proceed-ings of HLT-NAACL 2003, pages 71?78, 2003.Lin Sun and Anna Korhonen.
Improving verb clusteringwith automatically acquired selectional preferences.
InProceedings of the EMNLP 2009, 2009.Lin Sun, Anna Korhonen, and Yuval Krymolowski.
Verbclass discovery from rich syntactic data.
Lecture Notesin Computer Science, 4919:16, 2008.Robert Swier and Suzanne Stevenson.
Unsupervisedsemantic role labelling.
In Proceedings of EMNLP,pages 95?102, 2004.Yee Whye Teh, Hal Daume?
III, and Daniel Roy.
Bayesianagglomerative clustering with coalescents.
In Ad-vances in Neural Information Processing Systems, vol-ume 20, 2008.Akira Ushioda.
Hierarchical clustering of words.
InProceedings of the 16th conference on Computationallinguistics-Volume 2, pages 1159?1162.
Associationfor Computational Linguistics, 1996.Gloria Va?zquez, Ana Ferna?ndez-Montraveta, andM.
Anto`nia Mart??.
Clasificacio?n verbal:(alternanciasde dia?tesis).
Universitat de Lleida, 2000.
ISBN8484090671.Nguyen Xuan Vinh, Julien Epps, and James Bailey.
Infor-mation theoretic measures for clusterings comparison:is a correction for chance necessary?
In ICML ?09:Proceedings of the 26th Annual International Confer-ence on Machine Learning, pages 1073?1080, NewYork, NY, USA, 2009.
ACM.
ISBN 978-1-60558-516-1.Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-mani.
Unsupervised and constrained dirichlet processmixture models for verb clustering.
In Proceedings ofthe Workshop on Geometrical Models of Natural Lan-guage Semantics, pages 74?82, 2009.Joe H. Ward Jr. Hierarchical grouping to optimize an ob-jective function.
Journal of the American statistical as-sociation, 58(301):236?244, 1963.
ISSN 0162-1459.Zhenyu Wu and Richard Leahy.
An optimal graph the-oretic approach to data clustering: Theory and its ap-plication to image segmentation.
IEEE transactionson pattern analysis and machine intelligence, pages1101?1113, 1993.
ISSN 0162-8828.Kai Yu, Shipeng Yu, and Volker Tresp.
Soft clustering ongraphs.
Advances in Neural Information ProcessingSystems, 18:1553, 2006.Ben?at Zapirain, Eneko Agirre, and Llu?
?s Ma`rquez.
Ro-bustness and generalization of role sets: PropBank vs.VerbNet.
In Proceedings of ACL-08: HLT, pages 550?558, 2008.1033
