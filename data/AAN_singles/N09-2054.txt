Proceedings of NAACL HLT 2009: Short Papers, pages 213?216,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsImproving A Simple Bigram HMM Part-of-Speech Tagger by LatentAnnotation and Self-TrainingZhongqiang Huang?, Vladimir Eidelman?, Mary Harper??
?Laboratory for Computational Linguistics and Information ProcessingInstitute for Advanced Computer StudiesUniversity of Maryland, College Park?Human Language Technology Center of ExcellenceJohns Hopkins University{zqhuang,vlad,mharper}@umiacs.umd.eduAbstractIn this paper, we describe and evaluate a bi-gram part-of-speech (POS) tagger that useslatent annotations and then investigate usingadditional genre-matched unlabeled data forself-training the tagger.
The use of latentannotations substantially improves the per-formance of a baseline HMM bigram tag-ger, outperforming a trigram HMM taggerwith sophisticated smoothing.
The perfor-mance of the latent tagger is further enhancedby self-training with a large set of unlabeleddata, even in situations where standard bigramor trigram taggers do not benefit from self-training when trained on greater amounts oflabeled training data.
Our best model obtainsa state-of-the-art Chinese tagging accuracy of94.78% when evaluated on a representativetest set of the Penn Chinese Treebank 6.0.1 IntroductionPart-of-speech (POS) tagging, the process of as-signing every word in a sentence with a POS tag(e.g., NN (normal noun) or JJ (adjective)), is pre-requisite for many advanced natural language pro-cessing tasks.
Building upon the large body of re-search to improve tagging performance for variouslanguages using various models (e.g., (Thede andHarper, 1999; Brants, 2000; Tseng et al, 2005b;Huang et al, 2007)) and the recent work on PCFGgrammars with latent annotations (Matsuzaki et al,2005; Petrov et al, 2006), we will investigate the useof fine-grained latent annotations for Chinese POStagging.
While state-of-the-art tagging systems haveachieved accuracies above 97% in English, ChinesePOS tagging (Tseng et al, 2005b; Huang et al,2007) has proven to be more challenging, and it isthe focus of this study.The value of the latent variable approach for tag-ging is that it can learn more fine grained tags to bet-ter model the training data.
Liang and Klein (2008)analyzed the errors of unsupervised learning usingEM and found that both estimation and optimiza-tion errors decrease as the amount of unlabeled dataincreases.
In our case, the learning of latent anno-tations through EM may also benefit from a largeset of automatically labeled data to improve taggingperformance.
Semi-supervised, self-labeled data hasbeen effectively used to train acoustic models forspeech recognition (Ma and Schwartz, 2008); how-ever, early investigations of self-training on POStagging have mixed outcomes.
Clark et al (2003)reported positive results with little labeled trainingdata but negative results when the amount of labeledtraining data increases.
Wang et al (2007) reportedthat self-training improves a trigram tagger?s accu-racy, but this tagger was trained with only a smallamount of in-domain labeled data.In this paper, we will investigate whether theperformance of a simple bigram HMM tagger canbe improved by introducing latent annotations andwhether self-training can further improve its perfor-mance.
To the best of our knowledge, this is the firstattempt to use latent annotations with self-trainingto enhance the performance of a POS tagger.2 ModelPOS tagging using a hidden Markov model can beconsidered as an instance of Bayesian inference,213wherein we observe a sequence of words and needto assign them the most likely sequence of POS tags.If ti1 denotes the tag sequence t1, ?
?
?
, ti, and wi1denotes the word sequence w1, ?
?
?
, wi, given thefirst-order Markov assumption of a bigram tagger,the best tag sequence ?
(wn1 ) for sentence wn1 can becomputed efficiently as1:?
(wn1 ) = argmaxtn1 p(tn1 |wn1 )?
argmaxtn1?ip(ti|ti?1)p(wi|ti)with a set of transition parameters {p(b|a)}, for tran-siting to tag b from tag a, and a set of emissionparameters {p(w|a)}, for generating word w fromtag a.
A simple HMM tagger is trained by pullingcounts from labeled data and normalizing to get theconditional probabilities.It is well know that the independence assumptionof a bigram tagger is too strong in many cases.
Acommon practice for weakening the independenceassumption is to use a second-order Markov as-sumption, i.e., a trigram tagger.
This is similar toexplicitly annotating each POS tag with the preced-ing tag.
Rather than explicit annotation, we coulduse latent annotations to split the POS tags, sim-ilarly to the introduction of latent annotations toPCFG grammars (Matsuzaki et al, 2005; Petrovet al, 2006).
For example, the NR tag may besplit into NR-1 and NR-2, and correspondingly thePOS tag sequence of ?Mr./NR Smith/NR saw/VVMs./NR Smith/NR?
could be refined as: ?Mr./NR-2Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1?.The objective of training a bigram tagger with la-tent annotations is to find the transition and emissionprobabilities associated with the latent tags such thatthe likelihood of the training data is maximized.
Un-like training a standard bigram tagger where the POStags are observed, in the latent case, the latent tagsare not observable, and so a variant of EM algorithmis used to estimate the parameters.Given a sentence wn1 and its tag sequence tn1 , con-sider the i-th word wi and its latent tag ax ?
a = ti(which means ax is a latent tag of tag a, the i-th tagin the sequence) and the (i + 1)-th word wi+1 andits latent tag by ?
b = ti+1, the forward, ?i+1(by) =p(wi+11 , by), and backward, ?i(ax) = p(wni+1|ax),probabilities can be computed recursively:?i+1(by) =?x?i(ax)p(by|ax)p(wi+1|by)1We assume that symbols exist implicitly for boundary con-ditions.
?i(ax) =?yp(by|ax)p(wi+1|by)?j+1(by)In the E step, the posterior probabilities of co-occurrence events can be computed as:p(ax, by|w) ?
?i(ax)p(by|ax)?i+1(by)p(ax, wi|w) ?
?i(ax)?i(ax)In the M step, the above posterior probabilities areused as weighted observations to update the transi-tion and emission probabilities2:p(by|ax) = c(ax, by)/?byc(ax, by)p(w|ax) = c(ax, w)/?wc(ax, w)A hierarchical split-and-merge method, similarto (Petrov et al, 2006), is used to gradually increasethe number of latent annotations while allocatingthem adaptively to places where they would pro-duce the greatest increase in training likelihood (e.g.,we observe heavy splitting in categories such as NN(normal noun) and VV (verb), that cover a wide vari-ety of words, but only minimal splitting in categorieslike IJ (interjection) and ON (onomatopoeia)).Whereas tag transition occurrences are frequent,allowing extensive optimization using EM, word-tagco-occurrences are sparser and more likely to suf-fer from over-fitting.
To handle this problem, wemap all words with frequency less than threshold3?
to symbol unk and for each latent tag accumu-late the word tag statistics of these rare words tocr(ax, unk) = ?w:c(w)<?
c(ax, w).
These statisticsare redistributed among the rare words (w : c(w) <?)
to compute their emission probabilities:c(ax, w) = cr(ax, unk) ?
c(a,w)/cr(a, unk)p(w|ax) = c(ax, w)/?wc(ax, w)The impact of this rare word handling method willbe investigated in Section 3.A character-based unknown word model, similarto the one described in (Huang et al, 2007), is usedto handle unknown Chinese words during tagging.A decoding method similar to the max-rule-productmethod in (Petrov and Klein, 2007) is used to tagsentences using our model.3 ExperimentsThe Penn Chinese Treebank 6.0 (CTB6) (Xue et al,2005) is used as the labeled data in our study.
CTB62c(?)
represents the count of the event.3The value of ?
is tuned on the development set.214contains news articles, which are used as the primarysource of labeled data in our experiments, as well asbroadcast news transcriptions.
Since the news ar-ticles were collected during different time periodsfrom different sources with a diversity of topics, inorder to obtain a representative split of train-test-development sets, we divide them into blocks of 10files in sorted order and for each block use the firstfile for development, the second for test, and the re-maining for training.
The broadcast news data ex-hibits many of the characteristics of newswire text(it contains many nonverbal expressions, e.g., num-bers and symbols, and is fully punctuated) and so isalso included in the training data set.
We also uti-lize a greater number of unlabeled sentences in theself-training experiments.
They are selected fromsimilar sources to the newswire articles, and arenormalized (Zhang and Kahn, 2008) and word seg-mented (Tseng et al, 2005a).
See Table 1 for a sum-mary of the data used.Train Dev Test Unlabeledsentences 24,416 1904 1975 210,000words 678,811 51,229 52,861 6,254,947Table 1: The number of sentences and words in the data.50 100 150 200 250 300 350 40091.59292.59393.59494.5Number of latent annotationsTokenaccuracy (%)Bigram+LA:1Bigram+LA:2TrigramFigure 1: The learning curves of the bigram tagger withlatent annotations on the development set.Figure 1 plots the learning curves of two bigramtaggers with latent annotations (Bigram+LA:2 hasthe special handling of rare words as described inSection 2 while Bigram+LA:1 does not) and com-pares its performance with a state-of-the-art trigramHMM tagger (Huang et al, 2007) that uses trigramtransition and emission models together with bidi-rectional decoding.
Both bigram taggers initiallyhave much lower tagging accuracy than the trigramtagger, due to its strong but invalid independence as-sumption.
As the number of latent annotations in-creases, the bigram taggers are able to learn morefrom the context based on the latent annotations,and their performance improves significantly, out-performing the trigram tagger.
The performancegap between the two bigram taggers suggests thatover-fitting occurs in the word emission model whenmore latent annotations are available for optimiza-tion; sharing the statistics among rare words alle-viates some of the sparseness while supporting themodeling of deeper dependencies among more fre-quent events.
In the later experiments, we use Bi-gram+LA to denote the Bigram+LA:2 tagger.Figure 2 compares the self-training capability ofthree models (the bigram tagger w/ or w/o latentannotations, and the aforementioned trigram tagger)using different sizes of labeled training data and thefull set of unlabeled data.
For each model, a tag-ger is first trained on the allocated labeled trainingdata and is then used to tag the unlabeled data.
Anew tagger is then trained on the combination4 ofthe allocated labeled training data and the newly au-tomatically labeled data.0.1 0.2 0.4 0.6 0.8 189909192939495Fraction of CTB6 training dataTokenaccuracy (%)Bigram+LA+STBigram+LATrigram+STTrigramBigram+STBigramFigure 2: The performance of three taggers evaluated onthe development set, before and after self-training withdifferent sizes of labeled training data.There are two interesting observations that distin-guish the bigram tagger with latent annotations fromthe other two taggers.
First, although all of the tag-gers improve as more labeled training data is avail-able, the performance gap between the bigram tag-ger with latent annotations and the other two taggersalso increases.
This is because more latent annota-tions can be used to take advantage of the additionaltraining data to learn deeper dependencies.Second, the bigram tagger with latent annotationsbenefits much more from self-training, although it4We always balance the size of manually and automaticallylabeled data through duplication (for the trigram tagger) or pos-terior weighting (for the bigram tagger w/ or w/o latent annota-tions), as this provides superior performance.215already has the highest performance among the threetaggers before self-training.
The bigram taggerwithout latent annotations benefits little from self-training.
Except for a slight improvement whenthere is a small amount of labeled training, self-training slightly hurts tagging performance as theamount of labeled data increases.
The trigram tag-ger benefits from self-training initially but eventu-ally has a similar pattern to the bigram tagger whentrained on the full labeled set.
The performanceof the latent bigram tagger improves consistentlywith self-training.
Although the gain decreases formodels trained on larger training sets, since strongermodels are harder to improve, self-training still con-tributes significantly to model accuracy.The final tagging performance on the test set isreported in Table 2.
All of the improvements arestatistically significant (p < 0.005).Tagger Token Accuracy (%)Bigram 92.25Trigram 93.99Bigram+LA 94.53Bigram+LA+ST 94.78Table 2: The performance of the taggers on the test set.It is worth mentioning that we initially added la-tent annotations to a trigram tagger, rather than a bi-gram tagger, to build from a stronger starting point;however, this did not work well.
A trigram tagger re-quires sophisticated smoothing to handle data spar-sity, and introducing latent annotations exacerbatesthe sparsity problem, especially for trigram wordemissions.
The uniform extension of a bigram tag-ger to a trigram tagger ignores whether the use of ad-ditional context is helpful and supported by enoughdata, nor is it able to use a longer context.
In con-trast, the bigram tagger with latent annotations isable to learn different granularities for tags based onthe training data.4 ConclusionIn this paper, we showed that the accuracy of a sim-ple bigram HMM tagger can be substantially im-proved by introducing latent annotations togetherwith proper handling of rare words.
We also showedthat this tagger is able to benefit from self-training,despite the fact that other models, such as bigram ortrigram HMM taggers, do not.In the future work, we will investigate automaticdata selection methods to choose materials that aremost suitable for self-training and evaluate the effectof the amount of automatically labeled data.AcknowledgmentsThis work was supported by NSF IIS-0703859and DARPA HR0011-06-C-0023 and HR0011-06-2-001.
Any opinions, findings and/or recommenda-tions expressed in this paper are those of the authorsand do not necessarily reflect the views of the fund-ing agencies.ReferencesT.
Brants.
2000.
TnT a statistical part-of-speech tagger.In ANLP.S.
Clark, J. R. Curran, and M. Osborne.
2003.
Bootstrap-ping pos taggers using unlabelled data.
In CoNLL.Z.
Huang, M. Harper, and W. Wang.
2007.
Mandarinpart-of-speech tagging and discriminative reranking.EMNLP.P.
Liang and D. Klein.
2008.
Analyzing the errors ofunsupervised learning.
In ACL.J.
Ma and R. Schwartz.
2008.
Factors that affect unsu-pervised training of acoustic models.
In Interspeech.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with latent annotations.
In ACL.
Associationfor Computational Linguistics.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In HLT-NAACL.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In ACL.S.
M. Thede and M. P. Harper.
1999.
A second-orderhidden markov model for part-of-speech tagging.
InACL.H.
Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-ning.
2005a.
A conditional random field word seg-menter.
In SIGHAN Workshop on Chinese LanguageProcessing.H.
Tseng, D. Jurafsky, and C. Manning.
2005b.
Morpho-logical features help pos tagging of unknown wordsacross language varieties.
In SIGHAN Workshop onChinese Language Processing.W.
Wang, Z. Huang, and M. Harper.
2007.
Semi-supervised learning for part-of-speech tagging of Man-darin transcribed speech.
In ICASSP.N.
Xue, F. Xia, F. Chiou, and M. Palmer.
2005.
Thepenn chinese treebank: Phrase structure annotation ofa large corpus.
Natural Language Engineering.B.
Zhang and J. G. Kahn.
2008.
Evaluation of decaturtext normalizer for language model training.
Technicalreport, University of Washington.216
