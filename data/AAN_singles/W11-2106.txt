Proceedings of the 6th Workshop on Statistical Machine Translation, pages 78?84,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsTESLA at WMT 2011: Translation Evaluation and Tunable MetricDaniel Dahlmeier1 and Chang Liu2 and Hwee Tou Ng1,21NUS Graduate School for Integrative Sciences and Engineering2Department of Computer Science, National University of Singapore{danielhe,liuchan1,nght}@comp.nus.edu.sgAbstractThis paper describes the submission from theNational University of Singapore to the WMT2011 Shared Evaluation Task and the TunableMetric Task.
Our entry is TESLA in three dif-ferent configurations: TESLA-M, TESLA-F,and the new TESLA-B.1 IntroductionTESLA (Translation Evaluation of Sentences withLinear-programming-based Analysis) was first pro-posed in Liu et al (2010).
The simplest variant,TESLA-M (M stands for minimal), is based on N-gram matching, and utilizes light-weight linguis-tic analysis including lemmatization, part-of-speechtagging, and WordNet synonym relations.
TESLA-B (B stands for basic) additionally takes advan-tage of bilingual phrase tables to model phrase syn-onyms.
It is a new configuration proposed in this pa-per.
The most sophisticated configuration TESLA-F(F stands for full) additionally uses language mod-els and a ranking support vector machine instead ofsimple averaging.
TESLA-F was called TESLA inLiu et al (2010).
In this paper, we rationalize thenaming convention by using TESLA to refer to thewhole family of metrics.The rest of this paper is organized as follows.
Sec-tions 2 to 4 describe the TESLA variants TESLA-M,TESLA-B, and TESLA-F, respectively.
Section 5describes MT tuning with TESLA.
Section 6 showsexperimental results for the evaluation and the tun-able metric task.
The last section concludes the pa-per.2 TESLA-MThe version of TESLA-M used in WMT 2011 is ex-actly the same as in Liu et al (2010).
The descrip-tion is reproduced here for completeness.We consider the task of evaluating machine trans-lation systems in the direction of translating a sourcelanguage to a target language.
We are given a refer-ence translation produced by a professional humantranslator and a machine-produced system transla-tion.
At the highest level, TESLA-M is the arith-metic average of F-measures between bags of N-grams (BNGs).
A BNG is a multiset of weightedN-grams.
Mathematically, a BNG B consists of tu-ples (bi, bWi ), where each bi is an N-gram and bWi isa positive real number representing the weight of bi.In the simplest case, a BNG contains every N-gramin a translated sentence, and the weights are just thecounts of the respective N-grams.
However, to em-phasize the content words over the function words,we discount the weight of an N-gram by a factor of0.1 for every function word in the N-gram.
We de-cide whether a word is a function word based on itsPOS tag.In TESLA-M, the BNGs are extracted in the targetlanguage, so we call them bags of target languageN-grams (BTNGs).2.1 Similarity functionsTo match two BNGs, we first need a similarity mea-sure between N-grams.
In this section, we definethe similarity measures used in our experiments.
Weadopt the similarity measure from MaxSim (Chanand Ng, 2008; Chan and Ng, 2009) as sms.
For uni-grams x and y,78?
If lemma(x) = lemma(y), then sms = 1.?
Otherwise, leta = I(synsets(x) overlap with synsets(y))b = I(POS(x) = POS(y))where I(?)
is the indicator function, then sms =(a + b)/2.The synsets are obtained by querying WordNet(Fellbaum, 1998).
For languages other than English,a synonym dictionary is used instead.We define two other similarity functions betweenunigrams:slem(x, y) = I(lemma(x) = lemma(y))spos(x, y) = I(POS(x) = POS(y))All the three unigram similarity functions generalizeto N-grams in the same way.
For two N-grams x =x1,2,...,n and y = y1,2,...,n,s(x, y) ={0 if ?i, s(xi, yi) = 01n?ni=1 s(xi, yi) otherwise2.2 Matching two BNGsNow we describe the procedure of matching twoBNGs.
We take as input BNGs X and Y and a sim-ilarity measure s. The i-th entry in X is xi and hasweight xWi (analogously for yj and yWj ).Intuitively, we wish to align the entries of the twoBNGs in a way that maximizes the overall similar-ity.
An example matching problem for bigrams isshown in Figure 1a, where the weight of each nodeis shown, along with the hypothetical similarity foreach edge.
Edges with a similarity of zero are notshown.
Note that for each function word, we dis-count the weight by a factor of ten.
The solution tothe matching problem is shown in Figure 1b, and theoverall similarity is 0.5 ?
0.01 + 0.8 ?
0.1 + 0.8 ?0.1 = 0.165.Mathematically, we formulate this as a (real-valued) linear programming problem1.
The vari-ables are the allocated weights for the edgesw(xi, yj) ?i, j1While integer linear programming is NP-complete, real-valued linear programming can be solved efficiently.w=1.0 w=0.1 w=0.1 w=0.1w=0.01 w=0.1 w=0.1s=0.1 s=0.8s=0.5 s=0.8Good morning morning , , sir sir .Hello , , Querrien Querrien .s=0.4(a) The matching problemw=1.0 w=0.1 w=0.1 w=0.1w=0.01 w=0.1 w=0.1w=0.1w=0.01 w=0.1s885Go8d m r o8d m r Gn nGimd imdG.g,HH8Gn nGel,ddm, el,ddm, G.(b) The solutionFigure 1: A BNG matching problemWe maximize?i,js(xi, yj)w(xi, yj)subject tow(xi, yj) ?
0 ?i, j?jw(xi, yj) ?
xWi ?i?iw(xi, yj) ?
yWj ?jThe value of the objective function is the overallsimilarity S. Assuming X is the reference and Yis the system translation, we havePrecision =S?j yWjRecall =S?i xWiThe F-measure is derived from the precision and therecall:F =Precision ?
Recall?
?
Precision + (1 ?
?)
?
RecallIn this work, we set ?
= 0.8, following MaxSim.The value gives more importance to the recall thanthe precision.79If the similarity function is binary-valued andtransitive, such as slem and spos, then wecan use a much simpler and faster greedymatching procedure: the best match is simply?g min(?xi=gxWi ,?yi=gyWi ).2.3 ScoringThe TESLA-M sentence-level score for a referenceand a system translation is the arithmetic average ofthe BTNG F-measures for unigrams, bigrams, andtrigrams based on similarity functions sms and spos.We thus have 3 ?
2 = 6 BTNG F-measures forTESLA-M.We can compute a system-level score for a ma-chine translation system by averaging its sentence-level scores over the complete test set.3 TESLA-BTESLA-B uses the average of two types of F-measures: (1) BTNG F-measures as in TESLA-Mand (2) F-measures between bags of N-grams in oneor more pivot languages, called bags of pivot lan-guage N-grams (BPNGs), The rest of this section fo-cuses on the generation of the BPNGs.
Their match-ing is done in the same way as described for BTNGsin the previous section.3.1 Phrase level semantic representationGiven a sentence-aligned bitext between the targetlanguage and a pivot language, we can align thetext at the word level using well known tools suchas GIZA++ (Och and Ney, 2003) or the Berkeleyaligner (Liang et al, 2006; Haghighi et al, 2009).We observe that the distribution of alignedphrases in a pivot language can serve as a seman-tic representation of a target language phrase.
Thatis, if two target language phrases are often alignedto the same pivot language phrase, then they can beinferred to be similar in meaning.
Similar observa-tions have been made by previous researchers (Ban-nard and Callison-Burch, 2005; Callison-Burch etal., 2006; Snover et al, 2009).We note here two differences from WordNet syn-onyms: (1) the relationship is not restricted to theword level only, and (2) the relationship is not bi-nary.
The degree of similarity can be measured bythe percentage of overlap between the semantic rep-resentations.3.2 Segmenting a sentence into phrasesTo extend the concept of this semantic representa-tion of phrases to sentences, we segment a sentencein the target language into phrases.
Given a phrasetable, we can approximate the probability of a phrasep by:Pr(p) =N(p)?p?
N(p?
)(1)where N(?)
is the count of a phrase in the phrasetable.
We then define the likelihood of seg-menting a sentence S into a sequence of phrases(p1, p2, .
.
.
, pn) by:Pr(p1, p2, .
.
.
, pn|S) =1Z(S)n?i=1Pr(pi) (2)where Z(S) is a normalizing constant.
The segmen-tation of S that maximizes the probability can be de-termined efficiently using a dynamic programmingalgorithm.
The formula has a strong preference forlonger phrases, as every Pr(p) is a small fraction.To deal with out-of-vocabulary (OOV) words, weallow any single word w to be considered a phrase,and if N(w) = 0, we set N(w) = 0.5 instead.3.3 BPNGs as sentence level semanticrepresentationSimply merging the phrase-level semantic represen-tation is insufficient to produce a sensible sentence-level semantic representation.
As an example, weconsider two target language (English) sentencessegmented as follows:1.
||| Hello , ||| Querrien ||| .
|||2.
||| Good morning , sir .
|||A naive comparison of the bags of aligned pivot lan-guage (French) phrases would likely conclude thatthe two sentences are completely unrelated, as thebags of aligned phrases are likely to be completelydisjoint.
We tackle this problem by constructinga confusion network representation of the alignedphrases, as shown in Figures 2 and 3.
A confusionnetwork is a compact representation of a potentiallyexponentially large number of weighted and likelymalformed French sentences.
We can collect the N-gram statistics of this ensemble of French sentences80w=1.=0s858G8odmrn0i858G8odg,0HsseH18G8gdo d8G8gdoFigure 2: A confusion network as a semantic repre-sentationw=1.=0s858G=1od 0s8m8r8nmiFigure 3: A degenerate confusion network as a se-mantic representationefficiently from the confusion network representa-tion.
For example, the trigram Bonjour , Querrien 2would receive a weight of 0.9 ?
1.0 = 0.9 in Fig-ure 2.
As with BTNGs, we discount the weight of anN-gram by a factor of 0.1 for every function word inthe N-gram, so as to place more emphasis on thecontent words.The collection of all such N-grams and their cor-responding weights forms the BPNG of a sentence.The reference and system BPNGs are then matchedusing the algorithm outlined in Section 2.2.3.4 ScoringThe TESLA-B sentence-level score is a linear com-bination of (1) BTNG F-measures for unigrams,bigrams, and trigrams based on similarity func-tions sms and spos, and (2) BPNG F-measures forunigrams, bigrams, and trigrams based on sim-ilarity functions slem and spos.
We thus have3 ?
2 F-measures from the BTNGs and 3 ?
2 ?#pivot languages F-measures from the BPNGs.
Weaverage the BTNG and BPNG scores to obtainsBTNG and sBPNG, respectively.
The sentence-level TESLA-B score is then defined as 12(sBTNG +sBPNG).
The two-step averaging process preventsthe BPNG scores from overwhelming the BTNGscores, especially when we have many pivot lan-guages.
The system-level TESLA-B score is thearithmetic average of the sentence-level TESLA-Bscores.2Note that an N-gram can span more than one segment.4 TESLA-FUnlike the simple arithmetic averages used inTESLA-M and TESLA-B, TESLA-F uses a gen-eral linear combination of three types of scores: (1)BTNG F-measures as in TESLA-M and TESLA-B,(2) BPNG F-measures as in TESLA-B, and (3) nor-malized language model scores of the system trans-lation, defined as 1n logP , where n is the length ofthe translation, and P the language model probabil-ity.
The method of training the linear model dependson the development data.
In the case of WMT, thedevelopment data is in the form of manual rankings,so we train SVM rank (Joachims, 2006) on these in-stances to build the linear model.
In other scenarios,some form of regression can be more appropriate.The BTNG and BPNG scores are the same asused in TESLA-B.
In the WMT campaigns, we usetwo language models, one generated from the Eu-roparl dataset and one from the news-train dataset.We thus have 3 ?
2 features from the BTNGs,3 ?
2 ?
#pivot languages features from the BPNGs,and 2 features from the language models.
Again, wecan compute system-level scores by averaging thesentence-level scores.4.1 Scaling of TESLA-F ScoresWhile machine translation evaluation is concernedonly with the relative order of the different trans-lations but not with the absolute scores, there arepractical advantages in normalizing the evaluationscores to a range between 0 and 1.
For TESLA-Mand TESLA-B, this is already the case, since everyF-measure has a range of [0, 1] and so do their av-erages.
In contrast, the SVM rank -produced modeltypically gives scores very close to zero.To remedy that, we note that we have the free-dom to scale and shift the linear SVM model with-out changing the metric.
We observe that the F-measures have a range of [0, 1], and studying thedata reveals that [?15, 0] is a good approximation ofthe range for normalized language model scores, forall languages involved in the WMT campaign.
Sincewe know the range of values of an F-measure feature(between 0 and 1) and assuming that the range ofthe normalized LM score is between ?15 and 0, wecan find the maximum and minimum possible scoregiven the weights.
Then we linearly scale the range81of scores from [min, max] to [0, 1].
We provide anoption of scaling TESLA-F scores in the new releaseof TESLA.5 MT tuning with TESLAAll variants of TESLA can be used for automaticMT tuning using Z-MERT (Zaidan, 2009).
Z-MERT?s modular design makes it easy to integrate anew metric.
As TESLA already computes scores atthe sentence level, integrating TESLA into Z-MERTwas straightforward.
First, we created a ?streaming?version of each TESLA metric which reads trans-lation candidates from standard input and prints thesentence-level scores to standard output.
This allowsZ-MERT to easily query the metric for sentence-level scores during MT tuning.
Second, we wrotea Java wrapper that calls the TESLA code from Z-MERT.
The resulting metric can be used for MERTtuning in the standard fashion.
All that a user hasto do is to change the metric in the Z-MERT config-uration file to TESLA.
All the necessary code forZ-MERT tuning is included in the new release ofTESLA.6 Experiments6.1 Evaluation TaskWe evaluate TESLA using the publicly availabledata from WMT 2009 for into-English and out-of-English translation.
The pivot language phrasetables and language models are built using theWMT 2009 training data.
The SVM rank model forTESLA-F is trained on manual rankings from WMT2008.
The results for TESLA-M and TESLA-F havepreviously been reported in Liu et al (2010)3.
Weadd results for the new variant TESLA-B here.Tables 1 and 2 show the sentence-level consis-tency and system-level Spearman?s rank correlation,respectively for into-English translation.
For com-parison, we include results for some of the best per-forming metrics in WMT 2009.
Tables 3 and 4 showthe same results for out-of-English translation.
Wedo not include the English-Czech language pair inour experiments, as we unfortunately do not havegood linguistic resources for the Czech language.3The English-Spanish system correlation differs from ourprevious result after fixing a minor mistake in the languagemodel.cz-en fr-en de-en es-en hu-en OverallTESLA-M 0.60 0.61 0.61 0.59 0.63 0.61TESLA-B 0.63 0.64 0.63 0.62 0.63 0.63TESLA-F 0.63 0.65 0.64 0.62 0.66 0.63ulc 0.63 0.64 0.64 0.61 0.60 0.63maxsim 0.60 0.63 0.63 0.61 0.62 0.62meteor-0.6 0.47 0.51 0.52 0.49 0.48 0.50Table 1: Into-English sentence-level consistency onWMT 2009 datacz-en fr-en de-en es-en hu-en AvgTESLA-M 1.00 0.86 0.85 0.99 0.66 0.87TESLA-B 1.00 0.92 0.67 0.95 0.83 0.87TESLA-F 1.00 0.92 0.68 0.94 0.94 0.90ulc 1.00 0.92 0.78 0.86 0.60 0.83maxsim 0.70 0.91 0.76 0.98 0.66 0.80meteor-0.6 0.70 0.93 0.56 0.87 0.54 0.72Table 2: Into-English system-level Spearman?s rankcorrelation on WMT 2009 dataThe new TESLA-B metric proves to be competi-tive to its siblings and is often on par with the moresophisticated TESLA-F metric.
The exception isthe English-German language pair, where TESLA-B has very low system-level correlation.
We havetwo possible explanations for this.
First, the system-level correlation is computed on a very small samplesize (the ranked list of MT systems).
This makes thesystem-level correlation score more volatile com-pared to the sentence-level consistency score whichis computed on thousands of sentence pairs.
Sec-ond, German has a relatively free word order whichpotentially makes word alignment and phrase tableextraction more noisy.
Interestingly, all participatingmetrics in WMT 2009 had low system-level correla-tion for the English-German language pair.en-fr en-de en-es OverallTESLA-M 0.64 0.59 0.59 0.60TESLA-B 0.65 0.59 0.60 0.61TESLA-F 0.68 0.57 0.60 0.61wpF 0.66 0.60 0.61 0.61wpbleu 0.60 0.47 0.49 0.51Table 3: Out-of-English sentence-level consistencyon WMT 2009 data82en-fr en-de en-es AvgTESLA-M 0.93 0.86 0.79 0.86TESLA-B 0.91 0.05 0.63 0.53TESLA-F 0.85 0.78 0.67 0.77wpF 0.90 -0.06 0.58 0.47wpbleu 0.92 0.07 0.63 0.54Table 4: Out-of-English system-level Spearman?srank correlation on WMT 2009 data6.2 Tunable Metric TaskThe goal of the new tunable metric task is to exploreMT tuning with metrics other than BLEU (Papineniet al, 2002).
To allow for a fair comparison, theWMT organizers provided participants with a com-plete Joshua MT system for an Urdu-English trans-lation task.
We tuned models for each variant ofTESLA, using Z-MERT in the default configurationprovided by the organizers.
There are four referencetranslations for each Urdu source sentence.
The sizeof the N-best list is set to 300.For our own experiments, we randomly split thedevelopment set into a development portion (781sentences) and a held-out test portion (200 sen-tences).
We run the same Z-MERT tuning processfor each TESLA variant on this reduced develop-ment set and evaluate the resulting models on theheld out test set.
We include a model trained withBLEU as an additional reference point.
The resultsare shown in Table 5.
We observe that the modeltrained with TESLA-F achieves the best resultswhen evaluated with any of the TESLA metrics, al-though the differences between the scores are small.We found that TESLA produces slightly longertranslations than BLEU: 22.4 words (TESLA-M),21.7 words (TESLA-B), and 22.5 words (TESLA-F), versus 18.7 words (BLEU).
The average refer-ence length is 19.8 words.The official evaluation for the tunable metric taskis performed using manual rankings.
The score ofa system is calculated as the percentage of timesthe system is judged to be either better or equal(score1) or strictly better (score2) compared to eachother system in pairwise comparisons.
Althoughwe submit results for all TESLA variants, only ourprimary submission TESLA-F is included in themanual evaluation.
The results for TESLA-F aremixed.
When evaluated with score1, TESLA-F isTune\Test BLEU TESLA-M TESLA-B TESLA-FBLEU 0.2715 0.3756 0.3129 0.3920TESLA-M 0.2279 0.4056 0.3279 0.3981TESLA-B 0.2370 0.4001 0.3257 0.3977TESLA-F 0.2432 0.4076 0.3299 0.4007Table 5: Automatic evaluation scores on held outtest portion for the tunable metric task.
The best re-sult in each column is printed in bold.ranked 7th out of 8 participating systems, but whenevaluated with score2, TESLA-F is ranked secondbest.
These findings differ from previous resultsthat we reported in Liu et al (2011) where MTsystems tuned with TESLA-M and TESLA-F con-sistently outperform two other systems tuned withBLEU and TER for translations from French, Ger-man, and Spanish into English on the WMT 2010news data set.
A manual inspection of the referencesin the tunable metric task shows that the translationsare of lower quality compared to the news data setsused in WMT.
As the SVM model in TESLA-F istrained with rankings from WMT 2008, it is possiblethat the model is less robust when applied to Urdu-English translations.
This could explain the mixedperformance of TESLA-F in the tunable metric task.7 ConclusionWe introduce TESLA-B, a new variant of theTESLA machine translation metric and present ex-perimental results for all TESLA variants in the set-ting of the WMT evaluation task and tunable met-ric task.
All TESLA variants are integrated into Z-MERT for automatic machine translation tuning.AcknowledgmentsThis research was done for CSIDM Project No.CSIDM-200804 partially funded by a grant fromthe National Research Foundation (NRF) adminis-tered by the Media Development Authority (MDA)of Singapore.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics.83Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine translationusing paraphrases.
In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the Association for Computational Linguis-tics.Yee Seng Chan and Hwee Tou Ng.
2008.
MaxSim:A maximum similarity metric for machine translationevaluation.
In Proceedings of the 46th Annual Meetingof the Association for Computational Linguistics.Yee Seng Chan and Hwee Tou Ng.
2009.
MaxSim: Per-formance and effects of translation fluency.
MachineTranslation, 23(2?3):157?168.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
The MIT Press.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with supervisedITG models.
In Proceedings of 47th Annual Meetingof the Association for Computational Linguistics andthe 4th IJCNLP of the AFNLP.Thorsten Joachims.
2006.
Training linear SVMs in lin-ear time.
In Proceedings of the ACM Conference onKnowledge Discovery and Data Mining.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.2010.
TESLA: Translation evaluation of sentenceswith linear-programming-based analysis.
In Proceed-ings of the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2011.Better evaluation metrics lead to better machine trans-lation.
In Proceedings of the 2011 Conference on Em-pirical Methods in Natural Language Processing.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
Exploring different human judgments with atunable MT metric.
In Proceedings of of the FourthWorkshop on Statistical Machine Translation.Omar Zaidan.
2009.
Z-MERT: A fully configurable opensource tool for minimum error rate training of machinetranslation systems.
The Prague Bulletin of Mathe-matical Linguistics, 91:79?88.84
