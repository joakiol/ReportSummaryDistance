Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105?113,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsEU-BRIDGE MT: Combined Machine Translation?Markus Freitag,?Stephan Peitz,?Joern Wuebker,?Hermann Ney,?Matthias Huck,?Rico Sennrich,?Nadir Durrani,?Maria Nadejde,?Philip Williams,?Philipp Koehn,?Teresa Herrmann,?Eunah Cho,?Alex Waibel?RWTH Aachen University, Aachen, Germany?University of Edinburgh, Edinburgh, Scotland?Karlsruhe Institute of Technology, Karlsruhe, Germany?{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de?{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk?v1rsennr@staffmail.ed.ac.uk?maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk?
{teresa.herrmann,eunah.cho,alex.waibel}@kit.eduAbstractThis paper describes one of the col-laborative efforts within EU-BRIDGE tofurther advance the state of the art inmachine translation between two Euro-pean language pairs, German?Englishand English?German.
Three researchinstitutes involved in the EU-BRIDGEproject combined their individual machinetranslation systems and participated with ajoint setup in the shared translation task ofthe evaluation campaign at the ACL 2014Eighth Workshop on Statistical MachineTranslation (WMT 2014).We combined up to nine different machinetranslation engines via system combina-tion.
RWTH Aachen University, the Uni-versity of Edinburgh, and Karlsruhe In-stitute of Technology developed severalindividual systems which serve as sys-tem combination input.
We devoted spe-cial attention to building syntax-based sys-tems and combining them with the phrase-based ones.
The joint setups yield em-pirical gains of up to 1.6 points in BLEUand 1.0 points in TER on the WMT news-test2013 test set compared to the best sin-gle systems.1 IntroductionEU-BRIDGE1is a European research projectwhich is aimed at developing innovative speechtranslation technology.
This paper describes a1http://www.eu-bridge.eujoint WMT submission of three EU-BRIDGEproject partners.
RWTH Aachen University(RWTH), the University of Edinburgh (UEDIN)and Karlsruhe Institute of Technology (KIT) allprovided several individual systems which werecombined by means of the RWTH Aachen systemcombination approach (Freitag et al., 2014).
Asdistinguished from our EU-BRIDGE joint submis-sion to the IWSLT 2013 evaluation campaign (Fre-itag et al., 2013), we particularly focused on trans-lation of news text (instead of talks) for WMT.
Be-sides, we put an emphasis on engineering syntax-based systems in order to combine them with ourmore established phrase-based engines.
We builtcombined system setups for translation from Ger-man to English as well as from English to Ger-man.
This paper gives some insight into the tech-nology behind the system combination frameworkand the combined engines which have been usedto produce the joint EU-BRIDGE submission tothe WMT 2014 translation task.The remainder of the paper is structured as fol-lows: We first describe the individual systems byRWTH Aachen University (Section 2), the Uni-versity of Edinburgh (Section 3), and KarlsruheInstitute of Technology (Section 4).
We thenpresent the techniques for machine translation sys-tem combination in Section 5.
Experimental re-sults are given in Section 6.
We finally concludethe paper with Section 7.2 RWTH Aachen UniversityRWTH (Peitz et al., 2014) employs both thephrase-based (RWTH scss) and the hierarchical(RWTH hiero) decoder implemented in RWTH?spublicly available translation toolkit Jane (Vilar105et al., 2010; Wuebker et al., 2012).
The modelweights of all systems have been tuned with stan-dard Minimum Error Rate Training (Och, 2003)on a concatenation of the newstest2011 and news-test2012 sets.
RWTH used BLEU as optimiza-tion objective.
Both for language model estima-tion and querying at decoding, the KenLM toolkit(Heafield et al., 2013) is used.
All RWTH sys-tems include the standard set of models providedby Jane.
Both systems have been augmented witha hierarchical orientation model (Galley and Man-ning, 2008; Huck et al., 2013) and a cluster lan-guage model (Wuebker et al., 2013).
The phrase-based system (RWTH scss) has been further im-proved by maximum expected BLEU training sim-ilar to (He and Deng, 2012).
The latter has beenperformed on a selection from the News Commen-tary, Europarl and Common Crawl corpora basedon language and translation model cross-entropies(Mansour et al., 2011).3 University of EdinburghUEDIN contributed phrase-based and syntax-based systems to both the German?English andthe English?German joint submission.3.1 Phrase-based SystemsUEDIN?s phrase-based systems (Durrani et al.,2014) have been trained using the Moses toolkit(Koehn et al., 2007), replicating the settings de-scribed in (Durrani et al., 2013b).
The featuresinclude: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++align-ments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield,2011) used at runtime, a lexically-driven 5-gramoperation sequence model (OSM) (Durrani et al.,2013a), msd-bidirectional-fe lexicalized reorder-ing, sparse lexical and domain features (Hasleret al., 2012), a distortion limit of 6, a maxi-mum phrase length of 5, 100-best translation op-tions, Minimum Bayes Risk decoding (Kumar andByrne, 2004), cube pruning (Huang and Chiang,2007), with a stack size of 1000 during tuning and5000 during testing and the no-reordering-over-punctuation heuristic.
UEDIN uses POS and mor-phological target sequence models built on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models as additional factorsin phrase translation models (Koehn and Hoang,2007).
UEDIN has furthermore built OSM mod-els over POS and morph sequences followingDurrani et al.
(2013c).
The English?Germansystem additionally comprises a target-side LMover automatically built word classes (Birch etal., 2013).
UEDIN has applied syntactic pre-reordering (Collins et al., 2005) and compoundsplitting (Koehn and Knight, 2003) of the sourceside for the German?English system.
The sys-tems have been tuned on a very large tuning setconsisting of the test sets from 2008-2012, witha total of 13,071 sentences.
UEDIN used news-test2013 as held-out test set.
On top of UEDINphrase-based 1 system, UEDIN phrase-based 2augments word classes as additional factor andlearns an interpolated target sequence model overcluster IDs.
Furthermore, it learns OSM modelsover POS, morph and word classes.3.2 Syntax-based SystemsUEDIN?s syntax-based systems (Williams et al.,2014) follow the GHKM syntax approach as pro-posed by Galley, Hopkins, Knight, and Marcu(Galley et al., 2004).
The open source Mosesimplementation has been employed to extractGHKM rules (Williams and Koehn, 2012).
Com-posed rules (Galley et al., 2006) are extracted inaddition to minimal rules, but only up to the fol-lowing limits: at most twenty tree nodes per rule,a maximum depth of five, and a maximum size offive.
Singleton hierarchical rules are dropped.The features for the syntax-based systems com-prise Good-Turing-smoothed phrase translationprobabilities, lexical translation probabilities inboth directions, word and phrase penalty, a rulerareness penalty, a monolingual PCFG probability,and a 5-gram language model.
UEDIN has usedthe SRILM toolkit (Stolcke, 2002) to train the lan-guage model and relies on KenLM for languagemodel scoring during decoding.
Model weightsare optimized to maximize BLEU.
2000 sentencesfrom the newstest2008-2012 sets have been se-lected as a development set.
The selected sen-tences obtained high sentence-level BLEU scoreswhen being translated with a baseline phrase-based system, and each contain less than 30 wordsfor more rapid tuning.
Decoding for the syntax-based systems is carried out with cube pruningusing Moses?
hierarchical decoder (Hoang et al.,2009).UEDIN?s German?English syntax-based setupis a string-to-tree system with compound splitting106on the German source-language side and syntacticannotation from the Berkeley Parser (Petrov et al.,2006) on the English target-language side.For English?German, UEDIN has trained var-ious string-to-tree GHKM syntax systems whichdiffer with respect to the syntactic annotation.
Atree-to-string system and a string-to-string system(with rules that are not syntactically decorated)have been trained as well.
The English?GermanUEDIN GHKM system names in Table 3 denote:UEDIN GHKM S2T (ParZu): A string-to-treesystem trained with target-side syntactic an-notation obtained with ParZu (Sennrich etal., 2013).
It uses a modified syntactic labelset, target-side compound splitting, and addi-tional syntactic constraints.UEDIN GHKM S2T (BitPar): A string-to-treesystem trained with target-side syntacticannotation obtained with BitPar (Schmid,2004).UEDIN GHKM S2T (Stanford): A string-to-tree system trained with target-side syntacticannotation obtained with the German Stan-ford Parser (Rafferty and Manning, 2008a).UEDIN GHKM S2T (Berkeley): A string-to-tree system trained with target-side syntacticannotation obtained with the German Berke-ley Parser (Petrov and Klein, 2007; Petrovand Klein, 2008).UEDIN GHKM T2S (Berkeley): A tree-to-string system trained with source-side syn-tactic annotation obtained with the EnglishBerkeley Parser (Petrov et al., 2006).UEDIN GHKM S2S (Berkeley): A string-to-string system.
The extraction is GHKM-based with syntactic target-side annotationfrom the German Berkeley Parser, but westrip off the syntactic labels.
The final gram-mar contains rules with a single generic non-terminal instead of syntactic ones, plus rulesthat have been added from plain phrase-basedextraction (Huck et al., 2014).4 Karlsruhe Institute of TechnologyThe KIT translations (Herrmann et al., 2014) aregenerated by an in-house phrase-based transla-tions system (Vogel, 2003).
The provided NewsCommentary, Europarl, and Common Crawl par-allel corpora are used for training the translationmodel.
The monolingual part of those parallelcorpora, the News Shuffle corpus for both direc-tions and additionally the Gigaword corpus forGerman?English are used as monolingual train-ing data for the different language models.
Opti-mization is done with Minimum Error Rate Train-ing as described in (Venugopal et al., 2005), usingnewstest2012 and newstest2013 as developmentand test data respectively.Compound splitting (Koehn and Knight, 2003)is performed on the source side of the corpus forGerman?English translation before training.
Inorder to improve the quality of the web-crawledCommon Crawl corpus, noisy sentence pairs arefiltered out using an SVM classifier as describedby Mediani et al.
(2011).The word alignment for German?English isgenerated using the GIZA++toolkit (Och and Ney,2003).
For English?German, KIT uses discrimi-native word alignment (Niehues and Vogel, 2008).Phrase extraction and scoring is done using theMoses toolkit (Koehn et al., 2007).
Phrase pairprobabilities are computed using modified Kneser-Ney smoothing as in (Foster et al., 2006).In both systems KIT applies short-range re-orderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009)based on POS tags (Schmid, 1994) to performsource sentence reordering according to the targetlanguage word order.
The long-range reorderingrules are applied to the training corpus to createreordering lattices to extract the phrases for thetranslation model.
In addition, a tree-based re-ordering model (Herrmann et al., 2013) trainedon syntactic parse trees (Rafferty and Manning,2008b; Klein and Manning, 2003) as well as a lex-icalized reordering model (Koehn et al., 2005) areapplied.Language models are trained with the SRILMtoolkit (Stolcke, 2002) and use modified Kneser-Ney smoothing.
Both systems utilize a lan-guage model based on automatically learnedword classes using the MKCLS algorithm (Och,1999).
The English?German system compriseslanguage models based on fine-grained part-of-speech tags (Schmid and Laws, 2008).
In addi-tion, a bilingual language model (Niehues et al.,2011) is used as well as a discriminative word lex-icon (Mauser et al., 2009) using source context toguide the word choices in the target sentence.107In total, the English?German system uses thefollowing language models: two 4-gram word-based language models trained on the parallel dataand the filtered Common Crawl data separately,two 5-gram POS-based language models trainedon the same data as the word-based language mod-els, and a 4-gram cluster-based language modeltrained on 1,000 MKCLS word classes.The German?English system uses a 4-gramword-based language model trained on all mono-lingual data and an additional language modeltrained on automatically selected data (Moore andLewis, 2010).
Again, a 4-gram cluster-basedlanguage model trained on 1000 MKCLS wordclasses is applied.5 System CombinationSystem combination is used to produce consen-sus translations from multiple hypotheses whichare outputs of different translation engines.
Theconsensus translations can be better in terms oftranslation quality than any of the individual hy-potheses.
To combine the engines of the projectpartners for the EU-BRIDGE joint setups, we ap-ply a system combination implementation that hasbeen developed at RWTH Aachen University.The implementation of RWTH?s approach tomachine translation system combination is de-scribed in (Freitag et al., 2014).
This approachincludes an enhanced alignment and reorderingframework.
Alignments between the system out-puts are learned using METEOR (Banerjee andLavie, 2005).
A confusion network is then builtusing one of the hypotheses as ?primary?
hypoth-esis.
We do not make a hard decision on whichof the hypotheses to use for that, but instead com-bine all possible confusion networks into a singlelattice.
Majority voting on the generated latticeis performed using the prior probabilities for eachsystem as well as other statistical models, e.g.
aspecial n-gram language model which is learnedon the input hypotheses.
Scaling factors of themodels are optimized using the Minimum ErrorRate Training algorithm.
The translation with thebest total score within the lattice is selected as con-sensus translation.6 ResultsIn this section, we present our experimental resultson the two translation tasks, German?Englishand English?German.
The weights of the in-dividual system engines have been optimized ondifferent test sets which partially or fully includenewstest2011 or newstest2012.
System combina-tion weights are either optimized on newstest2011or newstest2012.
We kept newstest2013 as an un-seen test set which has not been used for tuningthe system combination or any of the individualsystems.6.1 German?EnglishThe automatic scores of all individual systemsas well as of our final system combination sub-mission are given in Table 1.
KIT, UEDIN andRWTH are each providing one individual phrase-based system output.
RWTH (hiero) and UEDIN(GHKM) are providing additional systems basedon the hierarchical translation model and a string-to-tree syntax model.
The pairwise differenceof the single system performances is up to 1.3points in BLEU and 2.5 points in TER.
ForGerman?English, our system combination pa-rameters are optimized on newstest2012.
Systemcombination gives us a gain of 1.6 points in BLEUand 1.0 points in TER for newstest2013 comparedto the best single system.In Table 2 the pairwise BLEU scores for all in-dividual systems as well as for the system combi-nation output are given.
The pairwise BLEU scoreof both RWTH systems (taking one as hypothesisand the other one as reference) is the highest for allpairs of individual system outputs.
A high BLEUscore means similar hypotheses.
The syntax-basedsystem of UEDIN and RWTH scss differ mostly,which can be observed from the fact of the low-est pairwise BLEU score.
Furthermore, we cansee that better performing individual systems havehigher BLEU scores when evaluating against thesystem combination output.In Figure 1 system combination output is com-pared to the best single system KIT.
We distributethe sentence-level BLEU scores of all sentences ofnewstest2013.
To allow for sentence-wise evalu-ation, all bi-, tri-, and four-gram counts are ini-tialized with 1 instead of 0.
Many sentences havebeen improved by system combination.
Neverthe-less, some sentences fall off in quality comparedto the individual system output of KIT.6.2 English?GermanThe results of all English?German system setupsare given in Table 3.
For the English?Germantranslation task, only UEDIN and KIT are con-108system newstest2011 newstest2012 newstest2013BLEU TER BLEU TER BLEU TERKIT 25.0 57.6 25.2 57.4 27.5 54.4UEDIN 23.9 59.2 24.7 58.3 27.4 55.0RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9syscom 25.6 57.1 26.4 56.5 29.1 53.4Table 1: Results for the German?English translation task.
The system combination is tuned on news-test2012, newstest2013 is used as held-out test set for all individual systems and system combination.Bold font indicates system combination results that are significantly better than the best single systemwith p < 0.05.KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscomKIT 59.07 57.60 57.91 55.62 77.68UEDIN 59.17 56.96 57.84 59.89 72.89RWTH scss 57.64 56.90 64.94 53.10 71.16RWTH hiero 57.98 57.80 64.97 55.73 70.87UEDIN S2T 55.75 59.95 53.19 55.82 65.35syscom 77.76 72.83 71.17 70.85 65.24Table 2: Cross BLEU scores for the German?English newstest2013 test set.
(Pairwise BLEU scores:each entry is taking the horizontal system as hypothesis and the other one as reference.
)system newstest2011 newstest2012 newstest2013BLEU TER BLEU TER BLEU TERUEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3KIT 17.1 67.0 17.8 64.8 20.2 62.2syscom 18.4 65.0 18.7 63.4 21.3 60.6Table 3: Results for the English?German translation task.
The system combination is tuned on news-test2011, newstest2013 is used as held-out test set for all individual systems and system combination.Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better thanthe best single system with p< 0.05.
Italic font indicates system combination results that are significantlybetter than the best single system with p < 0.1.tributing individual systems.
KIT is providing aphrase-based system output, UEDIN is providingtwo phrase-based system outputs and six syntax-based ones (GHKM).
For English?German, oursystem combination parameters are optimized onnewstest2011.
Combining all nine different sys-tem outputs yields an improvement of 0.5 pointsin BLEU and 1.7 points in TER over the best sin-gle system performance.In Table 4 the cross BLEU scores for allEnglish?German systems are given.
The individ-ual system of KIT and the syntax-based ParZu sys-tem of UEDIN have the lowest BLEU score whenscored against each other.
Both approaches arequite different and both are coming from differ-ent institutes.
In contrast, both phrase-based sys-tems pbt 1 and pbt 2 from UEDIN are very sim-ilar and hence have a high pairwise BLEU score.109pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscompbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27Table 4: Cross BLEU scores for the German?English newstest2013 test set.
(Pairwise BLEU scores:each entry is taking the horizontal system as reference and the other one as hypothesis.
)0501001502002503003504000  20  40  60  80  100amountsentencessBLEUbettersameworseFigure 1: Sentence distribution for theGerman?English newstest2013 test set compar-ing system combination output against the bestindividual system.As for the German?English translation direction,the best performing individual system outputs arealso having the highest BLEU scores when evalu-ated against the final system combination output.In Figure 2 system combination output is com-pared to the best single system pbt 2.
We distributethe sentence-level BLEU scores of all sentencesof newstest2013.
Many sentences have been im-proved by system combination.
But there is stillroom for improvement as some sentences are stillbetter in terms of sentence-level BLEU in the indi-vidual best system pbt 2.7 ConclusionWe achieved significantly better translation perfor-mance with gains of up to +1.6 points in BLEUand -1.0 points in TER by combining up to ninedifferent machine translation systems.
Three dif-ferent research institutes (RWTH Aachen Univer-sity, University of Edinburgh, Karlsruhe Instituteof Technology) provided machine translation en-gines based on different approaches like phrase-0501001502002503003504000  20  40  60  80  100amountsentencessBLEUbettersameworseFigure 2: Sentence distribution for theEnglish?German newstest2013 test set compar-ing system combination output against the bestindividual system.based, hierarchical phrase-based, and syntax-based.
For English?German, we included sixdifferent syntax-based systems, which were com-bined to our final combined translation.
The au-tomatic scores of all submitted system outputs forthe actual 2014 evaluation set are presented on theWMT submission page.2Our joint submission isthe best submission in terms of BLEU and TER forboth translation directions German?English andEnglish?German without adding any new data.AcknowledgementsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7/2007-2013) under grantagreement no287658.Rico Sennrich has received funding from theSwiss National Science Foundation under grantP2ZHP1 148717.2http://matrix.statmt.org/110ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In 43rdAnnual Meeting of the Assoc.
for ComputationalLinguistics: Proc.
Workshop on Intrinsic and Extrin-sic Evaluation Measures for MT and/or Summariza-tion, pages 65?72, Ann Arbor, MI, USA, June.Alexandra Birch, Nadir Durrani, and Philipp Koehn.2013.
Edinburgh SLT and MT System Descriptionfor the IWSLT 2013 Evaluation.
In Proceedingsof the 10th International Workshop on Spoken Lan-guage Translation, pages 40?48, Heidelberg, Ger-many, December.Maximilian Bisani and Hermann Ney.
2004.
BootstrapEstimates for Confidence Intervals in ASR Perfor-mance Evaluation.
In IEEE International Confer-ence on Acoustics, Speech, and Signal Processing,volume 1, pages 409?412, Montr?eal, Canada, May.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause Restructuring for Statistical Ma-chine Translation.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL?05), pages 531?540, Ann Arbor,Michigan, June.Nadir Durrani, Alexander Fraser, Helmut Schmid,Hieu Hoang, and Philipp Koehn.
2013a.
CanMarkov Models Over Minimal Translation UnitsHelp Phrase-Based SMT?
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, Sofia, Bulgaria, August.Nadir Durrani, Barry Haddow, Kenneth Heafield, andPhilipp Koehn.
2013b.
Edinburgh?s Machine Trans-lation Systems for European Language Pairs.
InProceedings of the Eighth Workshop on StatisticalMachine Translation, Sofia, Bulgaria, August.Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-san Sajjad, and Richard Farkas.
2013c.
Munich-Edinburgh-Stuttgart Submissions of OSM Systemsat WMT13.
In Proceedings of the Eighth Workshopon Statistical Machine Translation, Sofia, Bulgaria.Nadir Durrani, Barry Haddow, Philipp Koehn, andKenneth Heafield.
2014.
Edinburgh?s Phrase-basedMachine Translation Systems for WMT-14.
In Pro-ceedings of the ACL 2014 Ninth Workshop on Sta-tistical Machine Translation, Baltimore, MD, USA,June.George Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable Smoothing for Statistical Ma-chine Translation.
In EMNLP, pages 53?61.M.
Freitag, S. Peitz, J. Wuebker, H. Ney, N. Dur-rani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues,M.
Mediani, T. Herrmann, A. Waibel, N. Bertoldi,M.
Cettolo, and M. Federico.
2013.
EU-BRIDGEMT: Text Translation of Talks in the EU-BRIDGEProject.
In International Workshop on Spoken Lan-guage Translation, Heidelberg, Germany, Decem-ber.Markus Freitag, Matthias Huck, and Hermann Ney.2014.
Jane: Open Source Machine Translation Sys-tem Combination.
In Conference of the EuropeanChapter of the Association for Computational Lin-guistics, Gothenburg, Sweden, April.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase Reorder-ing Model.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 847?855, Honolulu, HI, USA, Octo-ber.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
of the Human Language Technology Conf./ North American Chapter of the Assoc.
for Compu-tational Linguistics (HLT-NAACL), pages 273?280,Boston, MA, USA, May.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable Inference and Trainingof Context-Rich Syntactic Translation Models.
InProc.
of the 21st International Conf.
on Computa-tional Linguistics and 44th Annual Meeting of theAssoc.
for Computational Linguistics, pages 961?968, Sydney, Australia, July.Eva Hasler, Barry Haddow, and Philipp Koehn.
2012.Sparse Lexicalised features and Topic Adaptationfor SMT.
In Proceedings of the seventh Interna-tional Workshop on Spoken Language Translation(IWSLT), pages 268?275.Xiaodong He and Li Deng.
2012.
Maximum ExpectedBLEU Training of Phrase and Lexicon TranslationModels.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics (ACL), pages 292?301, Jeju, Republic of Korea,July.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modi-fied Kneser-Ney language model estimation.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics, pages 690?696,Sofia, Bulgaria, August.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 187?197, Edinburgh, Scotland, UK, July.Teresa Herrmann, Jan Niehues, and Alex Waibel.2013.
Combining Word Reordering Methods ondifferent Linguistic Abstraction Levels for Statisti-cal Machine Translation.
In Proceedings of the Sev-enth Workshop on Syntax, Semantics and Structurein Statistical Translation, Atlanta, GA, USA, June.111Teresa Herrmann, Mohammed Mediani, Eunah Cho,Thanh-Le Ha, Jan Niehues, Isabel Slawik, YuqiZhang, and Alex Waibel.
2014.
The Karlsruhe In-stitute of Technology Translation Systems for theWMT 2014.
In Proceedings of the ACL 2014 NinthWorkshop on Statistical Machine Translation, Balti-more, MD, USA, June.Hieu Hoang, Philipp Koehn, and Adam Lopez.
2009.A Unified Framework for Phrase-Based, Hierarchi-cal, and Syntax-Based Statistical Machine Transla-tion.
pages 152?159, Tokyo, Japan, December.Liang Huang and David Chiang.
2007.
Forest Rescor-ing: Faster Decoding with Integrated LanguageModels.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 144?151, Prague, Czech Republic, June.Matthias Huck, Joern Wuebker, Felix Rietig, and Her-mann Ney.
2013.
A Phrase Orientation Modelfor Hierarchical Machine Translation.
In ACL 2013Eighth Workshop on Statistical Machine Transla-tion, pages 452?463, Sofia, Bulgaria, August.Matthias Huck, Hieu Hoang, and Philipp Koehn.2014.
Augmenting String-to-Tree and Tree-to-String Translation with Non-Syntactic Phrases.
InProceedings of the ACL 2014 Ninth Workshop onStatistical Machine Translation, Baltimore, MD,USA, June.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of ACL2003.Philipp Koehn and Hieu Hoang.
2007.
Factored Trans-lation Models.
In EMNLP-CoNLL, pages 868?876,Prague, Czech Republic, June.Philipp Koehn and Kevin Knight.
2003.
EmpiricalMethods for Compound Splitting.
In EACL, Bu-dapest, Hungary.Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,Chris Callison-Burch, Miles Osborne, and DavidTalbot.
2005.
Edinburgh System Description forthe 2005 IWSLT Speech Translation Evaluation.
InProceedings of the International Workshop on Spo-ken Language Translation (IWSLT), Pittsburgh, PA,USA.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open Source Toolkitfor Statistical Machine Translation.
In Proceedingsof the 45th Annual Meeting of the ACL on InteractivePoster and Demonstration Sessions, pages 177?180,Prague, Czech Republic, June.Shankar Kumar and William Byrne.
2004.
Mini-mum Bayes-Risk Decoding for Statistical MachineTranslation.
In Proc.
Human Language Technol-ogy Conf.
/ North American Chapter of the Associa-tion for Computational Linguistics Annual Meeting(HLT-NAACL), pages 169?176, Boston, MA, USA,May.Saab Mansour, Joern Wuebker, and Hermann Ney.2011.
Combining Translation and Language ModelScoring for Domain-Specific Data Filtering.
In Pro-ceedings of the International Workshop on SpokenLanguage Translation (IWSLT), pages 222?229, SanFrancisco, CA, USA, December.Arne Mauser, Sa?sa Hasan, and Hermann Ney.
2009.Extending Statistical Machine Translation with Dis-criminative and Trigger-Based Lexicon Models.
InConference on Empirical Methods in Natural Lan-guage Processing, pages 210?217, Singapore, Au-gust.Mohammed Mediani, Eunah Cho, Jan Niehues, TeresaHerrmann, and Alex Waibel.
2011.
The KITEnglish-French Translation systems for IWSLT2011.
In Proceedings of the Eight Interna-tional Workshop on Spoken Language Translation(IWSLT), San Francisco, CA, USA.Robert C. Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In Pro-ceedings of the ACL 2010 Conference Short Papers,pages 220?224, Uppsala, Sweden, July.Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
InFourth Workshop on Statistical Machine Translation(WMT 2009), Athens, Greece.Jan Niehues and Stephan Vogel.
2008.
DiscriminativeWord Alignment via Alignment Matrix Modeling.In Proceedings of Third ACL Workshop on Statisti-cal Machine Translation, Columbus, USA.Jan Niehues, Teresa Herrmann, Stephan Vogel, andAlex Waibel.
2011.
Wider Context by Using Bilin-gual Language Models in Machine Translation.
InSixth Workshop on Statistical Machine Translation(WMT 2011), Edinburgh, UK.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
1999.
An Efficient Method for De-termining Bilingual Word Classes.
In EACL?99.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proc.
of the41th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 160?167, Sapporo,Japan, July.Stephan Peitz, Joern Wuebker, Markus Freitag, andHermann Ney.
2014.
The RWTH Aachen German-English Machine Translation System for WMT2014.
In Proceedings of the ACL 2014 Ninth Work-shop on Statistical Machine Translation, Baltimore,MD, USA, June.112Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized Parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404?411, Rochester, New York, April.Slav Petrov and Dan Klein.
2008.
Parsing Germanwith Latent Variable Grammars.
In Proceedings ofthe Workshop on Parsing German at ACL ?08, pages33?39, Columbus, OH, USA, June.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, and In-terpretable Tree Annotation.
In Proc.
of the 21st In-ternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Assoc.
forComputational Linguistics, pages 433?440, Sydney,Australia, July.Anna N. Rafferty and Christopher D. Manning.
2008a.Parsing Three German Treebanks: Lexicalized andUnlexicalized Baselines.
In Proceedings of theWorkshop on Parsing German at ACL ?08, pages 40?46, Columbus, OH, USA, June.Anna N. Rafferty and Christopher D. Manning.
2008b.Parsing Three German Treebanks: Lexicalized andUnlexicalized Baselines.
In Proceedings of theWorkshop on Parsing German.Kay Rottmann and Stephan Vogel.
2007.
Word Re-ordering in Statistical Machine Translation with aPOS-Based Distortion Model.
In Proceedings ofthe 11th International Conference on Theoreticaland Methodological Issues in Machine Translation(TMI), Sk?ovde, Sweden.Helmut Schmid and Florian Laws.
2008.
Estimationof Conditional Probabilities with Decision Trees andan Application to Fine-Grained POS Tagging.
InCOLING 2008, Manchester, UK.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In InternationalConference on New Methods in Language Process-ing, Manchester, UK.Helmut Schmid.
2004.
Efficient Parsing of HighlyAmbiguous Context-Free Grammars with Bit Vec-tors.
In Proc.
of the Int.
Conf.
on ComputationalLinguistics (COLING), Geneva, Switzerland, Au-gust.Rico Sennrich, Martin Volk, and Gerold Schneider.2013.
Exploiting Synergies Between Open Re-sources for German Dependency Parsing, POS-tagging, and Morphological Analysis.
In Proceed-ings of the International Conference Recent Ad-vances in Natural Language Processing 2013, pages601?609, Hissar, Bulgaria.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.on Speech and Language Processing (ICSLP), vol-ume 2, pages 901?904, Denver, CO, USA, Septem-ber.Ashish Venugopal, Andreas Zollman, and Alex Waibel.2005.
Training and Evaluation Error MinimizationRules for Statistical Machine Translation.
In Work-shop on Data-drive Machine Translation and Be-yond (WPT-05), Ann Arbor, Michigan, USA.David Vilar, Daniel Stein, Matthias Huck, and Her-mann Ney.
2010.
Jane: Open Source Hierarchi-cal Translation, Extended with Reordering and Lex-icon Models.
In ACL 2010 Joint Fifth Workshop onStatistical Machine Translation and Metrics MATR,pages 262?270, Uppsala, Sweden, July.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In International Conference on NaturalLanguage Processing and Knowledge Engineering,Beijing, China.Philip Williams and Philipp Koehn.
2012.
GHKMRule Extraction and Scope-3 Parsing in Moses.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation (WMT), pages 388?394,Montr?eal, Canada, June.Philip Williams, Rico Sennrich, Maria Nadejde,Matthias Huck, Eva Hasler, and Philipp Koehn.2014.
Edinburgh?s Syntax-Based Systems atWMT 2014.
In Proceedings of the ACL 2014 NinthWorkshop on Statistical Machine Translation, Balti-more, MD, USA, June.Joern Wuebker, Matthias Huck, Stephan Peitz, MalteNuhn, Markus Freitag, Jan-Thorsten Peter, SaabMansour, and Hermann Ney.
2012.
Jane 2:Open Source Phrase-based and Hierarchical Statisti-cal Machine Translation.
In COLING ?12: The 24thInt.
Conf.
on Computational Linguistics, pages 483?491, Mumbai, India, December.Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-mann Ney.
2013.
Improving Statistical MachineTranslation with Word Class Models.
In Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1377?1381, Seattle, WA, USA, Oc-tober.113
