Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1774?1782,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsInducing Word and Part-of-Speech withPitman-Yor Hidden Semi-Markov ModelsKei Uchiumi Hiroshi TsukaharaDenso IT Laboratory, Inc.Shibuya Cross Tower 28F2-15-1 Shibuya, Tokyo, Japan{kuchiumi,htsukahara}@d-itlab.co.jpDaichi MochihashiThe Institute of Statistical Mathematics10-3 Midori-cho, Tachikawa cityTokyo, Japandaichi@ism.ac.jpAbstractWe propose a nonparametric Bayesianmodel for joint unsupervised word seg-mentation and part-of-speech taggingfrom raw strings.
Extending a previousmodel for word segmentation, our modelis called a Pitman-Yor Hidden Semi-Markov Model (PYHSMM) and consid-ered as a method to build a class n-gramlanguage model directly from strings,while integrating character and word levelinformation.
Experimental results on stan-dard datasets on Japanese, Chinese andThai revealed it outperforms previous re-sults to yield the state-of-the-art accura-cies.
This model will also serve to analyzea structure of a language whose words arenot identified a priori.1 IntroductionMorphological analysis is a staple of natural lan-guage processing for broad languages.
Especiallyfor some East Asian languages such as Japanese,Chinese or Thai, word boundaries are not explic-itly written, thus morphological analysis is a cru-cial first step for further processing.
Note thatalso in Latin and old English, scripts were orig-inally written with no word indications (scriptacontinua), but people felt no difficulty readingthem.
Here, morphological analysis means wordsegmentation and part-of-speech (POS) tagging.For this purpose, supervised methods have of-ten been employed for training.
However, totrain such supervised classifiers, we have to pre-pare a large amount of training data with cor-rect annotations, in this case, word segmentationand POS tags.
Creating and maintaining thesedata is not only costly but also very difficult, be-cause generally there are no clear criteria for ei-ther ?correct?
segmentation or POS tags.
In fact,since there are different standards for Chineseword segmentation, widely used SIGHAN Bake-off dataset (Emerson, 2005) consists of multipleparts employing different annotation schemes.Lately, this situation has become increasinglyimportant because there are strong demands forprocessing huge amounts of text in consumer gen-erated media such as Twitter, Weibo or Facebook(Figure 1).
They contain a plethora of colloquialexpressions and newly coined words, includingsentiment expressions such as emoticons that can-not be covered by fixed supervised data.To automatically recognize such linguistic phe-nomena beyond small ?correct?
supervised data,we have to extract linguistic knowledge from thestatistics of strings themselves in an unsupervisedfashion.
Needless to say, such methods will alsocontribute to analyzing speech transcripts, classictexts, or even unknown languages.
From a scien-tific point of view, it is worth while to find ?words?and their part-of-speech purely from a collectionof strings without any preconceived assumptions.To achieve that goal, there have been two kindsof approaches: heuristic methods and statisti-cal generative models.
Heuristic methods arebased on basic observations such that word bound-aries will often occur at the place where predic-tive entropy of characters is large (i.e.
the nextcharacter cannot be predicted without assuming????????????????????
(?????)?????????????????????????????????????????????????????(*???)?????????????????????????????????????????????
VTR ????????
?Figure 1: Sample of Japanese Twitter text thatis difficult to analyze by ordinary supervised seg-mentation.
It contains a lot of novel words, emoti-cons, and colloquial expressions.1774the next word).
By formulating such ideas assearch or MDL problems of given coding length1,word boundaries are found in an algorithmic fash-ion (Zhikov et al, 2010; Magistry and Sagot,2013).
However, such methods have difficulty in-corporating higher-order statistics beyond simpleheuristics, such as word transitions, word spellingformation, or word length distribution.
Moreover,they usually depends on tuning parameters likethresholds that cannot be learned without humanintervention.In contrast, statistical models are ready to in-corporate all such phenomena within a consistentstatistical generative model of a string, and oftenprove to work better than heuristic methods (Gold-water et al, 2006; Mochihashi et al, 2009).
Infact, the statistical methods often include the cri-teria of heuristic methods at least in a conceptuallevel, which is noted in (Mochihashi et al, 2009)and also explained later in this paper.
In a statisti-cal model, each word segmentation w of a strings is regarded as a hidden stochastic variable, andthe unsupervised learning of word segmentation isformulated as a maximization of a probability ofw given s:argmaxwp(w|s) .
(1)This means that we want the most ?natural?
seg-mentation w that have a high probability in a lan-guage model p(w|s).Lately, Chen et al (2014) proposed an interme-diate model between heuristic and statistical mod-els as a product of character and word HMMs.However, these two models do not have informa-tion shared between the models, which is not thecase with generative models.So far, these approaches only find word seg-mentation, leaving part-of-speech information be-hind.
These two problems are not actually in-dependent but interrelated, because knowing thepart-of-speech of some infrequent or unknownword will give contextual clues to word segmen-tation, and vice versa.
For example, in Japanese??????
?can be segmented into not only???/?/??/?
(plum/too/peach/too), but also into???/??/??
(plum/peach/peach), which is ungrammati-cal.
However, we could exclude the latter case1For example, Zhikov et al (2010) defined a codinglength using character n-grams plus MDL penalty.
Sincethis can be interpreted as a crude ?likelihood?
and a prior,its essence is similar but driven by a quite simplistic model.Character HPYLMWord HPYLMFigure 2: NPYLM represented in a hierarchicalChinese restaurant process.
Here, a character ?-gram HPYLM is embedded in a word n-gramHPYLM and learned jointly during inference.if we leverage knowledge that a state sequenceN/P/N/P is much more plausible in Japanese thanN/N/N from the part-of-speech information.
Sirtsand Alum?ae (2012) treats a similar problem ofPOS induction with unsupervised morphologicalsegmentation, but they know the words in advanceand only consider segmentation within a word.For this objective, we attempt to maximize thejoint probability of words and tags:argmaxw,zp(w, z|s) ?
p(w, z, s) (2)From the expression above, this amounts tobuilding a generative model of a string s withwordsw and tags z along with an associated infer-ence procedure.
We solve this problem by extend-ing previous generative model of word segmenta-tion.
Note that heuristic methods are never able tomodel the hidden tags, and only statistical genera-tive models can accommodate this objective.This paper is organized as follows.
In Sec-tion 2, we briefly introduce NPYLM (Mochihashiet al, 2009) on which our extension is based.
Sec-tion 3 extends it to include hidden states to yield ahidden semi-Markov models (Murphy, 2002), andwe describe its inference procedure in Section 4.We conduct experiments on some East Asian lan-guages in Section 5.
Section 6 discusses implica-tions of our model and related work, and Section 7concludes the paper.2 Nested Pitman-Yor Language ModelOur joint model of words and states is anextension of the Nested Pitman-Yor LanguageModel (Mochihashi et al, 2009) of a string, whichin turn is an extension of a Bayesian n-gram lan-guage model called Hierarchical Pitman-Yor Lan-guage Model (HPYLM) (Teh, 2006).1775HPYLM is a nonparametric Bayesian model ofn-gram distribution based on the Pitman-Yor pro-cess (Pitman and Yor, 1997) that generates a dis-crete distribution G as G ?
PY(G0, d, ?).
Here,d is a discount factor, ?parent?
distribution G0iscalled a base measure and ?
controls how similarG is to G0in expectation.
In HPYLM, n-gramdistribution Gn= {p(wt|wt?1?
?
?wt?
(n?1))} isassumed to be generated from the Pitman-Yor pro-cessGn?
PY(Gn?1, dn, ?n) , (3)where the base measure Gn?1is an (n?1)-gramdistribution generated recursively in accordancewith (3).
Note that there are different Gnfor eachn-gram history h= wt?1?
?
?wt?(n?1).
When wereach the unigram G1and need to use a base mea-sure G0, i.e.
prior probabilities of words, HPYLMusually uses a uniform distribution over the lexi-con.However, in the case of unsupervised word seg-mentation, every sequence of characters could bea word, thus the size of the lexicon is unbounded.Moreover, prior probability of forming a wordshould not be uniform over all sequences of char-acters: for example, English words rarely beginwith ?gme?
but tend to end with ?-ent?
like in seg-ment.
To model this property, NPYLM assumesthat word prior G0is generated from characterHPYLM to model a well-formedness of w. Inpractice, to avoid dependency on n in the charac-ter model, we used an ?-gram VPYLM (Mochi-hashi and Sumita, 2008) in this research.
Finally,NPYLM gives an n-gram probability of word wgiven a history h recursively by integrating outGn,p(w|h) =c(w|h)?d?thw?+c(h)+?+d?th ??+c(h)p(w|h?)
,(4)where h?is the shorter history of (n?1)-grams.c(w|h), c(h) =?wc(w|h) are n-gram counts ofw appearing after h, and thw, th ?=?wthwareassociated latent variables explained below.
Incase the history h is already empty at the unigram,p(w|h?)
= p0(w) is computed from the character?-grams for the word w=c1?
?
?
ck:p0(w) = p(c1?
?
?
ck) (5)=?ki=1p(ci|ci?1?
?
?
c1) .
(6)In practice, we further corrected (6) so that a wordlength follows a mixture of Poisson distributions.For details, see (Mochihashi et al, 2009).When we know word segmentation w of thedata, the probability above can be computed byadding each n-gram count of w given h to themodel, i.e.
increment c(w|h) in accordance witha hierarchical Chinese restaurant process associ-ated with HPYLM (Figure 2).
When each n-gramcount called a customer is inferred to be actuallygenerated from (n?1)-grams, we send its proxycustomer for smoothing to the parent restaurantand increment thw, and this process will recurse.Notice that if a word w is never seen in w, itsproxy customer is eventually sent to the parentrestaurant of unigrams.
In that case2, w is decom-posed to its character sequence c1?
?
?
ckand this isadded to the character HPYLM in the same way,making it a little ?clever?
about possible wordspellings.Inference Because we do not know word seg-mentation w beforehand, we begin with a trivialsegmentation in which every sentence is a singleword3.
Then, we iteratively refine it by samplinga new word segmentation w(s) of a sentence sin a Markov Chain Monte Carlo (MCMC) frame-work using a dynamic programming, as is donewith PCFG by (Johnson et al, 2007) shown in Fig-ure 3 where we omit MH steps for computationalreasons.
Further note that every hyperparameterdn, ?nof NPYLM can be sampled from the poste-rior in a Bayesian fashion, as opposed to heuristicmethods that rely on a development set for tuning.For details, see Teh (2006).3 Pitman-Yor Hidden Semi-MarkovModelsNPYLM is a complete generative model of astring, that is, a hierarchical Bayesian n-gram lan-Input: a collection of strings SAdd initial segmentation w(s) to ?for j = 1 ?
?
?
J dofor s in randperm (S) doRemove customers of w(s) from ?Sample w(s) according to p(w|s,?
)Add customers of w(s) to ?end forSample hyperparameters of ?end forFigure 3: MCMC inference of NPYLM ?.2To be precise, this occurs whenever thwis incrementedin the unigram restaurant.3Note that a child first memorizes what his mother says asa single word and gradually learns the lexicon.1776zt?1ztzt+1wt?1wtwt+1?
??
?Observation s?
?
??
?
??
?
??
?
?Figure 4: Graphical model of PYHSMM in a bi-gram case.
White nodes are latent variables, andthe shaded node is the observation.
We only ob-serve a string s that is a concatenation of hiddenwords w1?
?
?wT.guage model combining words and characters.
Itcan also be viewed as a way to build a Bayesianword n-gram language model directly from a se-quence of characters, without knowing ?words?
apriori.One possible drawback of it is a lack of part-of-speech: as described in the introduction, grammat-ical states will contribute much to word segmenta-tion.
Also, from a computational linguistics pointof view, it is desirable to induce not only wordsfrom strings but also their part-of-speech purelyfrom the usage statistics (imagine applying it to anunknown language or colloquial expressions).
Inclassical terms, it amounts to building a class n-gram language model where both class and wordsare unknown to us.
Is this really possible?Yes, we can say it is possible.
The idea is sim-ple: we augment the latent states to include a hid-den part-of-speech ztfor each word wt, whichis again unknown as displayed in Figure 4.
As-suming wtis generated from zt?-th NPYLM, wecan draw a generative model of a string s as fol-lows:z0=BOS; s=?
(an empty string).for t = 1 ?
?
?T doDraw zt?
p(zt|zt?1) ,Draw wt?
p(wt|w1?
?
?wt?1, zt) ,Append wtto s .end forHere, z0= BOS and zT+1= EOS are distin-guished states for beginning and end of a sentence,respectively.
For the transition probability of hid-den states, we put a HPY process prior as (Blun-som and Cohn, 2011):p(zt|zt?1) ?
HPY(d, ?)
(7)with the final base measure being a uniform dis-tribution over the states.
The word boundaries are!"#!!!"#$"#%&'()*+,-.&-)/01-0.2*3'45#!
!!
"!
#!
$!
"!!!!!"!
"#!
#$!
$"!$%&'()*+,-.((((/!
0"#(1+'*2((((((((3!#$"!"#$!!"#!!!
"!4"#!516*(-!Figure 5: Graphical representation of samplingwords and POSs.
Each cell corresponds to an in-side probability ?[t][k][z].
Note each cell is notalways connected to adjacent cells, because of anoverlap of substrings associated with each cell.known in (Blunsom and Cohn, 2011), but in ourcase it is also learned from data at the same time.Note that because wtdepends on already gener-ated words w1?
?
?wt?1, our model is consideredas an autoregressive HMM rather than a vanillaHMM, as shown in Figure 4 (wt?1?
wtdepen-dency).Since segment models like NPYLM have seg-ment lengths as hidden states, they are called semi-Markov models (Murphy, 2002).
In contrast, ourmodel also has hidden part-of-speech, thus wecall it a Pitman-Yor Hidden Semi-Markov model(PYHSMM).4Note that this is considered as agenerative counterpart of a discriminative modelknown as a hidden semi-Markov CRF (Sarawagiand Cohen, 2005).4 InferenceInference of PYHSMM proceeds in almost thesame way as NPYLM in Figure 3: For each sen-tence, first remove the customers associated withthe old segmentation similarly to adding them.
Af-ter sampling a new segmentation and states, themodel is updated by adding new customers in ac-cordance with the new segmentation and hiddenstates.4.1 Sampling words and statesTo sample words and states (part-of-speech)jointly, we first compute inside probabilities for-ward from BOS to EOS and sample backwardsfrom EOS according to the Forward filtering-Backward sampling algorithm (Scott, 2002).
This4Lately, Johnson et al (2013) proposed a nonparamet-ric Bayesian hidden semi-Markov models for general statespaces.
However, it depends on a separate distribution for astate duration, thus is clealy different from ours for a naturallanguage.1777can be regarded as a ?stochastic Viterbi?
algorithmthat has the advantage of not being trapped in localminima, since it is a valid move of a Gibbs samplerin a Bayesian model.For a word bigram case for simplicity, insidevariable ?
[t][k][z] is a probability that a substringc1?
?
?
ctof a string s= c1?
?
?
cNis generated withits last k characters being a word, generated fromstate z as shown in Figure 5.
From the definitionof PYHSMM, this can be computed recursively asfollows:?
[t][k][z] =L?j=1K?y=1p(ctt?k|ct?kt?k?j+1, z)p(z|y)?
[t?k][j][y] .
(8)Here, ctsis a substring cs?
?
?
ctand L (?
t) is themaximum length of a word, and K is the numberof hidden states.5In Figure 5, each cell represents ?
[t][k][z] anda single path connecting from EOS to BOS cor-responds to a word sequence w and its state se-quence z.
Note that each cell is not always con-nected to adjacent cells (we omit the arrows), be-cause the length-k substring associated with eachcell already subsumes that of neighborhood cells.Once w and z are sampled, each wtis added tozt?-th NPYLM to update its statistics.4.2 Efficient computation by the NegativeBinomial generalized linear modelInference algorithm of PYHSMM has a computa-tional complexity of O(K2L2N), where N is alength of the string to analyze.
To reduce com-putations it is effective to put a small L of maxi-mum word length, but it might also ignore occa-sionally long words.
Since these long words areoften predictable from some character level infor-mation including suffixes or character types, in aType FeatureciCharacter at time t?i (0?
i?1)tiCharacter type at time t?i (0?
i?4)cont # of the same character types before tch # of times character types changedwithin 8 characters before tTable 1: Features used for the Negative Binomialgeneralized linear model for maximum wordlength prediction.5For computational reasons, we do not pursue using aDirichlet process to yield an infinite HMM (Van Gael et al,2009), but it is straightforward to extend our PYHSMM toiHMM.semi-supervised setting we employ a Negative Bi-nomial generalized linear model (GLM) for set-ting Ltadaptively for each character position t inthe corpus.Specifically, we model the word length ?
by aNegative Binomial distribution (Cook, 2009):?
?
NB(?|r, p) =?(r+?)?
(r) ?!p?(1?
p)r. (9)This counts the number of failures of Bernoullidraws with probability (1?p) before r?th success.For our model, note that Negative Binomial is ob-tained from a Poisson distribution Po(?)
whoseparameter ?
again follows a Gamma distributionGa(r, b) and integrated out:p(?|r, b) =?Po(?|?
)Ga(?|r, b)d?
(10)=?(r+?)?
(r) ?!(b1+b)?(11+b)r.
(11)This construction exactly mirrors the Poisson-Gamma word length distribution in (Mochihashiet al, 2009) with sampled ?.
Therefore, our Neg-ative Binomial is basically a continuous analogueof the word length distribution in NPYLM.6Since r > 0 and 0?
p?
1, we employ an expo-nential and sigmoidal linear regressionr = exp(wTrf), p = ?
(wTpf) (12)where ?
(x) is a sigmoid function and wr,wpareweight vectors to learn.
f is a feature vector com-puted from the substring c1?
?
?
ct, including f0?1for a bias term.
Table 1 shows the features weused for this Negative Binomial GLM.
Since Neg-ative Binomial GLM is not convex in wrand wp,we endow a Normal prior N(0, ?2I) for them andused a random walk MCMC for inference.Predicting LtOnce the model is obtained, wecan set Ltadaptively as the time where the cu-mulative probability of ?
exceeds some threshold?
(we used ?
= 0.99).
Table 2 shows the preci-sion of predicting maximum word length learnedfrom 10,000 sentences from each set: it measureswhether the correct word boundary in test data isincluded in the predicted Lt.Overall it performs very well with high preci-sion, and works better for longer words that cannotbe accommodated with a fixed maximum length.6Because NPYLM employs a mixture of Poisson distri-butions for each character type of a substring, this correspon-dence is not exact.1778Lang Dataset Training TestJaKyoto corpus 37,400 1,000BCCWJ OC 20,000 1,000ZhSIGHAN MSR 86,924 3,985SIGHAN CITYU 53,019 1,492SIGHAN PKU 19,056 1,945Th InterBEST Novel 1,000 1,000Table 3: Datasets used for evaluation.
Abbrevi-ations: Ja=Japanese, Zh=Chinese, Th=Thai lan-guage.Figure 6 shows the distribution of predicted max-imum lengths for Japanese.
Although we used?
= 0.99, it is rather parsimonious but accuratethat makes the computation faster.Because this cumulative Negative Binomialprediction is language independent, we believe itmight be beneficial for other natural language pro-cessing tasks that require some maximum lengthswithin which to process the data.5 ExperimentsTo validate our model, we conducted experimentson several corpora of East Asian languages withno word boundaries.Datasets For East Asian languages, we usedstandard datasets in Japanese, Chinese and Thaias shown in Table 3.
The Kyoto corpus is acollection of sentences from Japanese newspaper(Kurohashi and Nagao, 1998) with both word seg-mentation and part-of-speech annotations.
BC-CWJ (Balanced Corpus of Contemporary Writ-ten Japanese) is a balanced corpus of writtenJapanese (Maekawa, 2007) from the NationalInstitute of Japanese Language and Linguistics,also with both word segmentation and part-of-speech annotations from slightly different crite-ria.
For experiments on colloquial texts, we useda random subset of ?OC?
register from this cor-pus that is comprised of Yahoo!Japan Answersfrom users.
For Chinese, experiments are con-ducted on standard datasets of SIGHAN Bakeoff2005 (Emerson, 2005); for comparison we usedMSR and PKU datasets for simplified Chinese,and the CITYU dataset for traditional Chinese.SIGHAN datasets have word boundaries only, andwe conformed to original training/test splits pro-vided with the data.
InterBEST is a dataset inThai used in the InterBEST 2009 word segmen-tation contest (Kosawat, 2009).
For contrastivepurposes, we used a ?Novel?
subset of it with arandom sampling without replacement for trainingand test data.
Accuracies are measured in tokenF -measures computed as follows:F =2PRP+R, (13)P =# of correct words# of words in output, (14)R =# of correct words# of words in gold standard.
(15)Unsupervised word segmentation In Table 4,we show the accuracies of unsupervised word seg-mentation with previous figures.
We used bi-gram PYHSMM and set L = 4 for Chinese, L =5, 8, 10, 21 for Japanese with different types ofcontiguous characters, and L = 6 for Thai.
Thenumber of hidden states are K =10 (Chinese andThai), K=20 (Kyoto) and K=30 (BCCWJ).We can see that our PYHSMM outperforms onall the datasets.
Huang and Zhao (2007) reportsthat the maximum possible accuracy in unsuper-vised Chinese word segmentation is 84.8%, de-rived through the inconsistency between differentsegmentation standards of the SIGHAN dataset.Our PYHSMM performs nearer to this best possi-ble accuracy, leveraging both word and characterknowledge in a consistent Bayesian fashion.
Fur-ther note that in Thai, quite high performance isachieved with a very small data compared to pre-vious work.Unsupervised part-of-speech induction Asstated above, Kyoto, BCCWJ and Weibo datasetsDataset Kyoto BCCWJ MSR CITYU BESTPrecision (All) 99.9 99.9 99.6 99.9 99.0Precision (?5) 96.7 98.4 73.6 87.0 91.7Maximum length 15 48 23 12 21Table 2: Precision of maximum word length prediction witha Negative Binomial generalized linear model (in percent).?
5 are figures for word length ?
5.
Final row is the maxi-mum length of a word found in each dataset.020004000600080001000012000140002  4  6  8  10  12  14  16FrequencyLFigure 6: Distribution of predicted maxi-mum word lengths on the Kyoto corpus.1779Dataset PYHSMM NPY BE HMM2Kyoto 71.5 62.1 71.3 NABCCWJ 70.5 NA NA NAMSR 82.9 80.2 78.2 81.7CITYU 82.6?82.4 78.7 NAPKU 81.6 NA 80.8 81.1BEST 82.1 NA 82.1 NATable 4: Accuracies of unsupervised word seg-mentation.
BE is a Branching Entropy method ofZhikov et al (2010), and HMM2is a product ofword and character HMMs of Chen et al (2014).
?is the accuracy decoded with L=3: it becomes81.7 with L=4 as MSR and PKU.have part-of-speech annotations as well.
For thesedata, we also evaluated the precision of part-of-speech induction on the output of unsupervisedword segmentation above.
Note that the precisionis measured only over correct word segmentationthat the system has output.
Table 5 shows theprecisions; to the best of our knowledge, thereare no previous work on joint unsupervised learn-ing of words and tags, thus we only comparedwith Bayesian HMM (Goldwater and Griffiths,2007) on both NPYLM segmentation and goldsegmentation.
In this evaluation, we associatedeach tag of supervised data with a latent state thatcooccurred most frequently with that tag.
Wecan see that the precision of joint POS tagging isbetter than NPYLM+HMM, and even better thanHMM that is run over the gold segmentation.For colloquial Chinese, we also conducted anexperiment on the Leiden Weibo Corpus (LWC), acorpus of Chinese equivalent of Twitter7.
We usedrandom 20,000 sentences from this corpus, and re-sults are shown in Figure 7.
In many cases plausi-ble words are found, and assigned to syntacticallyconsistent states.
States that are not shown hereare either just not used or consists of a mixture ofdifferent syntactic categories.
Guiding our modelto induce more accurate latent states is a commonproblem to all unsupervised part-of-speech induc-tion, but we show some semi-supervised resultsnext.Dataset PYHSMM NPY+HMM HMMKyoto 57.4 53.8 49.5BCCWJ 50.2 44.1 44.2LWC 33.0 30.9 32.9Table 5: Precision of POS tagging on correctlysegmented words.7http://lwc.daanvanesch.nl/Semi-supervised experiments Because ourPYHSMM is a generative model, it is easilyamenable to semi-supervised segmentation andtagging.
We used random 10,000 sentences fromsupervised data on Kyoto, BCCWJ, and LWCdatasets along with unsupervised datasets inTable 3.Results are shown in Table 6: segmentation ac-curacies came close to 90% but do not go be-yond.
By inspecting the segmentation and POSthat PYHSMM has output, we found that this isnot necessarily a fault of our model, but it camefrom the often inconsistet or incorrect tagging ofthe dataset.
In many cases PYHSMM found more?natural?
segmentations, but it does not alwaysconform to the gold annotations.
On the otherhand, it often oversegments emotional expressions(sequence of the same character, for example) andthis is one of the major sources of errors.Finally, we note that our proposed model for un-supervised learning is most effective for the lan-guage which we do not know its syntactic behaviorbut only know raw strings as its data.
In Figure 8,we show an excerpt of results to model a Japaneselocal dialect (Mikawa-ben around Nagoya district)collected from a specific Twitter.
Even from thesurface appearance of characters, we can see thatsimilar words are assigned to the same state in-cluding some emoticons (states 9,29,32), and infact we can identify a state of postpositions spe-cific to that dialect (state 3).
Notice that thewords themselves are not trivial before this anal-ysis.
There are also some name of local places(state 41) and general Japanese postpositions (2)or nouns (11,18,25,27,31).
Because of the spar-sity promoting prior (7) over the hidden states, ac-tually used states are sparse and the results can beconsidered quite satisfactory.6 DiscussionThe characteristics of NPYLM is a Baysian inte-gration of character and word level information,which is related to (Blunsom and Cohn, 2011) andthe adaptor idea of (Goldwater et al, 2011).
ThisDataset Seg POSKyoto 92.1 87.1BCCWJ 89.4 83.1LWC 88.5 86.9Table 6: Semi-supervised segmentation and POStagging accuracies.
POS is measured by precision.1780z=1 z=3 z=10 z=11 z=18?
227?
182?
86??
65?
62?
53?
44?
41?
31??
30?
3309?
1901?
482?
226?
110?
93?
69??
56??
47??
43?
13440?
5989?
5224?
3237?
1504?
1206?
1190?
900?
861?
742??
207?
201?
199??
192?
192?
177?
167?
165?
154?
146?
68?
60?
59?
55?
53?
51?
49?
49?
45?
391Figure 7: Some interesting words and states induced from Weibo corpus (K =20).
Numbers representfrequencies that each word is generated from that class.
Although not perfect, emphatic (z = 1), end-of-sentence expressions (z=3), and locative words (z=18) are learned from tweets.
Distinction is farmore clear in the semi-supervised experiments (not shown here).z Induced words2 ?????????
?3 ?????????????????
?9 (*??
*) ?(?-?
; (?_?
;) (??
;; ?(??
;;10 ?
?
??
?
?
?????
???
?11 ????????????????
?13 ?????????????
?18 ?????????????????
?19 ???????????????????
?20 ?????????????????
?24 ???????????????
?25 ??????????????
?26 ????????????????????
?27 ????
&?????????
?29 ( ?
(; ( ??
??(*?
??
??
(*?_?
*)30 ??????????????????
?31 ?????????????????
?32 ( ?*\(?
?
(?
(?
?*\(?
?
(?_?
(*?34 ?????????????????????
?35 ???????????????
?36 ??????????????????
?41 ???????????????????
?Figure 8: Unsupervised analysis of a Japanese lo-cal dialect by PYHSMM.
(K=50)is different from (and misunderstood in) a jointmodel of Chen et al (2014), where word and char-acter HMMs are just multiplied.
There are no in-formation shared from the model structure, andin fact it depends on a BIO-like heuristic taggingscheme in the character HMM.In the present paper, we extended it to includea hidden state for each word.
Therefore, it mightbe interesting to introduce a hidden state also foreach character.
Unlike western languages, thereare many kinds of Chinese characters that workquite differently, and Japanese uses several distinctkinds of characters, such as a Chinese character,Hiragana, Katakana, whose mixture would consti-tute a single word.
Therefore, statistical modelingof different types of characters is an important re-search venue for the future.NPYLM has already applied and extended tospeech recognition (Neubig et al, 2010), statisti-cal machine translation (Nguyen et al, 2010), oreven robotics (Nakamura et al, 2014).
For allthese research area, we believe PYHSMM wouldbe beneficial for their extension.7 ConclusionIn this paper, we proposed a Pitman-Yor HiddenSemi-Markov model for joint unsupervised wordsegmentation and part-of-speech tagging on a rawsequence of characters.
It can also be viewed asa way to build a class n-gram language model di-rectly on strings, without any ?word?
informationa priori.We applied our PYHSMM on several standarddatasets on Japanese, Chinese and Thai, and it out-performed previous figures to yield the state-of-the-art results, as well as automatically inducedword categories.
It is especially beneficial for col-loquial text, local languages or speech transcripts,where not only words themselves are unknown buttheir syntactic behavior is a focus of interest.In order to adapt to human standards given insupervised data, it is important to conduct a semi-supervised learning with discriminative classifiers.Since semi-supervised learning requires genera-tive models in advance, our proposed Bayesiangenerative model will also lay foundations to suchan extension.ReferencesPhil Blunsom and Trevor Cohn.
2011.
A HierarchicalPitman-Yor Process HMM for Unsupervised Part ofSpeech Induction.
In ACL 2011, pages 865?874.1781Miaohong Chen, Baobao Chang, and Wenzhe Pei.2014.
A Joint Model for Unsupervised ChineseWord Segmentation.
In EMNLP 2014, pages 854?863.John D. Cook.
2009.
Notes on the Negative Bi-nomial Distribution.
http://www.johndcook.com/negative binomial.pdf.Tom Emerson.
2005.
The Second International Chi-nese Word Segmentation Bakeoff.
In Proceedingsof the Fourth SIGHAN Workshop on Chinese Lan-guage Processing.Sharon Goldwater and Tom Griffiths.
2007.
A FullyBayesian Approach to Unsupervised Part-of-SpeechTagging.
In Proceedings of ACL 2007, pages 744?751.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2006.
Contextual Dependencies in Un-supervised Word Segmentation.
In Proceedings ofACL/COLING 2006, pages 673?680.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2011.
Producing Power-Law Distribu-tions and Damping Word Frequencies with Two-Stage LanguageModels.
Journal of Machine Learn-ing Research, 12:2335?2382.Chang-Ning Huang and Hai Zhao.
2007.
Chineseword segmentation: A decade review.
Journal ofChinese Information Processing, 21(3):8?20.Matthew J. Johnson and Alan S. Willsky.
2013.Bayesian Nonparametric Hidden Semi-MarkovModels.
Journal of Machine Learning Research,14:673?701.Mark Johnson, Thomas L. Griffiths, and Sharon Gold-water.
2007.
Bayesian Inference for PCFGs viaMarkov Chain Monte Carlo.
In Proceedings ofHLT/NAACL 2007, pages 139?146.Krit Kosawat.
2009.
InterBEST 2009: Thai WordSegmentation Workshop.
In Proceedings of 2009Eighth International Symposium on Natural Lan-guage Processing (SNLP2009), Thailand.Sadao Kurohashi and Makoto Nagao.
1998.
Buildinga Japanese Parsed Corpus while Improving the Pars-ing System.
In Proceedings of LREC 1998, pages719?724.
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus.html.Kikuo Maekawa.
2007.
Kotonoha and BCCWJ: De-velopment of a Balanced Corpus of ContemporaryWritten Japanese.
In Corpora and Language Re-search: Proceedings of the First International Con-ference on Korean Language, Literature, and Cul-ture, pages 158?177.Pierre Magistry and Beno?
?t Sagot.
2013.
Can MDLImprove Unsupervised Chinese Word Segmenta-tion?
In Proceedings of the Seventh SIGHAN Work-shop on Chinese Language Processing, pages 2?10.Daichi Mochihashi and Eiichiro Sumita.
2008.
The In-finite Markov Model.
In Advances in Neural Infor-mation Processing Systems 20 (NIPS 2007), pages1017?1024.Daichi Mochihashi, Takeshi Yamada, and NaonoriUeda.
2009.
Bayesian Unsupervised Word Seg-mentation with Nested Pitman-Yor Language Mod-eling.
In Proceedings of ACL-IJCNLP 2009, pages100?108.Kevin Murphy.
2002.
Hidden semi-Markov models(segment models).
http://www.cs.ubc.ca/?murphyk/Papers/segment.pdf.Tomoaki Nakamura, Takayuki Nagai, Kotaro Fu-nakoshi, Shogo Nagasaka, Tadahiro Taniguchi, andNaoto Iwahashi.
2014.
Mutual Learning of anObject Concept and Language Model Based onMLDA and NPYLM.
In 2014 IEEE/RSJ Interna-tional Conference on Intelligent Robots and Systems(IROS?14), pages 600?607.Graham Neubig, Masato Mimura, Shinsuke Mori, andTatsuya Kawahara.
2010.
Learning a LanguageModel from Continuous Speech.
In Proc.
of INTER-SPEECH 2010.ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.2010.
Nonparametric Word Segmentation for Ma-chine Translation.
In COLING 2010, pages 815?823.Jim Pitman and Marc Yor.
1997.
The Two-ParameterPoisson-Dirichlet Distribution Derived from a Sta-ble Subordinator.
Annals of Probability, 25(2):855?900.Sunita Sarawagi and William W. Cohen.
2005.
Semi-Markov Conditional Random Fields for InformationExtraction.
In Advances in Neural Information Pro-cessing Systems 17 (NIPS 2004), pages 1185?1192.Steven L. Scott.
2002.
Bayesian Methods for HiddenMarkov Models.
Journal of the American StatisticalAssociation, 97:337?351.Kairit Sirts and Tanel Alum?ae.
2012.
A Hierarchi-cal Dirichlet Process Model for Joint Part-of-Speechand Morphology Induction.
In NAACL 2012, pages407?416.Yee Whye Teh.
2006.
A Bayesian Interpretation of In-terpolated Kneser-Ney.
Technical Report TRA2/06,School of Computing, NUS.Jurgen Van Gael, Andreas Vlachos, and ZoubinGhahramani.
2009.
The infinite HMM for unsu-pervised PoS tagging.
In EMNLP 2009, pages 678?687.Valentin Zhikov, Hiroya Takamura, and Manabu Oku-mura.
2010.
An Efficient Algorithm for Unsuper-vised Word Segmentation with Branching Entropyand MDL.
In EMNLP 2010, pages 832?842.1782
