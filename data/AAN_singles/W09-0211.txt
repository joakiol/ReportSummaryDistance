Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 83?90,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsA Non-negative Tensor Factorization Model forSelectional Preference InductionTim Van de CruysUniversity of GroningenThe Netherlandst.van.de.cruys@rug.nlAbstractDistributional similarity methods haveproven to be a valuable tool for the in-duction of semantic similarity.
Up tillnow, most algorithms use two-way co-occurrence data to compute the mean-ing of words.
Co-occurrence frequencies,however, need not be pairwise.
One caneasily imagine situations where it is desir-able to investigate co-occurrence frequen-cies of three modes and beyond.
This pa-per will investigate a tensor factorizationmethod called non-negative tensor factor-ization to build a model of three-way co-occurrences.
The approach is applied tothe problem of selectional preference in-duction, and automatically evaluated in apseudo-disambiguation task.
The resultsshow that non-negative tensor factoriza-tion is a promising tool for NLP.1 IntroductionDistributional similarity methods have proven tobe a valuable tool for the induction of semanticsimilarity.
The aggregate of a word?s contexts gen-erally provides enough information to compute itsmeaning, viz.
its semantic similarity or related-ness to other words.Up till now, most algorithms use two-way co-occurrence data to compute the meaning of words.A word?s meaning might for example be computedby looking at:?
the various documents that the word appearsin (words ?
documents);?
a bag of words context window around theword (words ?
context words);?
the dependency relations that the word ap-pears with (words ?
dependency relations).The extracted data ?
representing the co-occurrence frequencies of two different entities?
is encoded in a matrix.
Co-occurrence fre-quencies, however, need not be pairwise.
Onecan easily imagine situations where it is desirableto investigate co-occurrence frequencies of threemodes and beyond.
In an information retrievalcontext, one such situation might be the investiga-tion of words ?
documents ?
authors.
In an NLPcontext, one might want to investigatewords?
de-pendency relations ?
bag of word context words,or verbs ?
subjects ?
direct objects.Note that it is not possible to investigate thethree-way co-occurrences in a matrix represen-tation form.
It is possible to capture the co-occurrence frequencies of a verb with its sub-jects and its direct objects, but one cannot cap-ture the co-occurrence frequencies of the verb ap-pearing with the subject and the direct object atthe same time.
When the actual three-way co-occurrence data is ?matricized?, valuable informa-tion is thrown-away.
To be able to capture the mu-tual dependencies among the three modes, we willmake use of a generalized tensor representation.Two-way co-occurrence models (such as la-tent semantic analysis) have often been augmentedwith some form of dimensionality reduction in or-der to counter noise and overcome data sparseness.We will also make use of a dimensionality reduc-tion algorithm appropriate for tensor representa-tions.2 Previous Work2.1 Selectional Preferences & VerbClusteringSelectional preferences have been a popular re-search subject in the NLP community.
One ofthe first to automatically induce selectional pref-erences from corpora was Resnik (1996).
Resnikgeneralizes among nouns by using WordNet noun83synsets as clusters.
He then calculates the se-lectional preference strength of a specific verb ina particular relation by computing the Kullback-Leibler divergence between the cluster distribu-tion of the verb and the aggregate cluster distri-bution.
The selectional association is then thecontribution of the cluster to the verb?s prefer-ence strength.
The model?s generalization reliesentirely on WordNet; there is no generalizationamong the verbs.The research in this paper is related to previouswork on clustering.
Pereira et al (1993) use aninformation-theoretic based clustering approach,clustering nouns according to their distribution asdirect objects among verbs.
Their model is a one-sided clustering model: only the direct objects areclustered, there is no clustering among the verbs.Rooth et al (1999) use an EM-based cluster-ing technique to induce a clustering based on theco-occurrence frequencies of verbs with their sub-jects and direct objects.
As opposed to the methodof Pereira et al (1993), their model is two-sided:the verbs as well as the subjects/direct objects areclustered.
We will use a similar model for evalua-tion purposes.Recent approaches using distributional similar-ity methods for the induction of selectional pref-erences are the ones by Erk (2007), Bhagat et al(2007) and Basili et al (2007).This research differs from the approaches men-tioned above by its use of multi-way data: wherethe approaches above limit themselves to two-wayco-occurrences, this research will focus on co-occurrences for multi-way data.2.2 Factorization Algorithms2.2.1 Two-way FactorizationsOne of the best known factorization algorithmsis principal component analysis (PCA, Pearson(1901)).
PCA transforms the data into a new co-ordinate system, yielding the best possible fit in aleast square sense given a limited number of di-mensions.
Singular value decomposition (SVD)is the generalization of the eigenvalue decompo-sition used in PCA (Wall et al, 2003).In information retrieval, singular value decom-position has been applied in latent semantic analy-sis (LSA, Landauer and Dumais (1997), Landaueret al (1998)).
In LSA, a term-document matrixis created, containing the frequency of each wordin a specific document.
This matrix is then de-composed into three other matrices with SVD.
Themost important dimensions that come out of theSVD allegedly represent ?latent semantic dimen-sions?, according to which nouns and documentscan be represented more efficiently.LSA has been criticized for a number of rea-sons, one of them being the fact that the factor-ization contains negative numbers.
It is not clearwhat negativity on a semantic scale should des-ignate.
Subsequent methods such as probabilisticlatent semantic analysis (PLSA, Hofmann (1999))and non-negative matrix factorization (NMF, Leeand Seung (2000)) remedy these problems, andindeed get much more clear-cut semantic dimen-sions.2.2.2 Three-way FactorizationsTo be able to cope with three-way data, sev-eral algorithms have been developed as multilin-ear generalizations of the SVD.
In statistics, three-way component analysis has been extensively in-vestigated (for an overview, see Kiers and vanMechelen (2001)).
The two most popular methodsare parallel factor analysis (PARAFAC, Harshman(1970), Carroll and Chang (1970)) and three-modeprincipal component analysis (3MPCA, Tucker(1966)), also called higher order singular valuedecomposition (HOSVD, De Lathauwer et al(2000)).
Three-way factorizations have been ap-plied in various domains, such as psychometryand image recognition (Vasilescu and Terzopou-los, 2002).
In information retrieval, three-way fac-torizations have been applied to the problem oflink analysis (Kolda and Bader, 2006).One last important method dealing with multi-way data is non-negative tensor factorization(NTF, Shashua and Hazan (2005)).
NTF is a gener-alization of non-negative matrix factorization, andcan be considered an extension of the PARAFACmodel with the constraint of non-negativity (cfr.infra).One of the few papers that has investigated theapplication of tensor factorization for NLP is Tur-ney (2007), in which a three-mode tensor is usedto compute the semantic similarity of words.
Themethod achieves 83.75% accuracy on the TOEFLsynonym questions.843 Methodology3.1 TensorsDistributional similarity methods usually repre-sent co-occurrence data in the form of a matrix.This form is perfectly suited to represent two-wayco-occurrence data, but for co-occurrence data be-yond two modes, we need a more general repre-sentation.
The generalization of a matrix is calleda tensor.
A tensor is able to encode co-occurrencedata of any n modes.
Figure 1 shows a graphi-cal comparison of a matrix and a tensor with threemodes ?
although a tensor can easily be general-ized to more than three modes.Figure 1: Matrix representation vs. tensor repre-sentation3.2 Non-negative Tensor FactorizationIn order to create a succinct and generalized modelof the extracted data, a statistical dimensional-ity reduction technique called non-negative tensorfactorization (NTF) is applied to the data.
The NTFmodel is similar to the PARAFAC analysis ?
popu-lar in areas such as psychology and bio-chemistry?
with the constraint that all data needs to be non-negative (i.e.
?
0).Parallel factor analysis (PARAFAC) is a multi-linear analogue of the singular value decomposi-tion (SVD) used in latent semantic analysis.
Thekey idea is to minimize the sum of squares be-tween the original tensor and the factorized modelof the tensor.
For the three mode case of a tensorT ?
RD1?D2?D3 this gives equation 1, where k isthe number of dimensions in the factorized modeland ?
denotes the outer product.minxi?RD1,yi?RD2,zi?RD3?
T ?k?i=1xi ?
yi ?
zi ?2F (1)With non-negative tensor factorization, the non-negativity constraint is enforced, yielding a modellike the one in equation 2:minxi?RD1?0,yi?RD2?0,zi?RD3?0?
T ?k?i=1xi ?
yi ?
zi ?2F (2)The algorithm results in three matrices, indicat-ing the loadings of each mode on the factorizeddimensions.
The model is represented graphicallyin figure 2, visualizing the fact that the PARAFACdecomposition consists of the summation over theouter products of n (in this case three) vectors.Figure 2: Graphical representation of the NTF asthe sum of outer productsComputationally, the non-negative tensor fac-torization model is fitted by applying an alternat-ing least-squares algorithm.
In each iteration, twoof the modes are fixed and the third one is fittedin a least squares sense.
This process is repeateduntil convergence.13.3 Applied to Language DataThe model can straightforwardly be applied to lan-guage data.
In this part, we describe the fac-torization of verbs ?
subjects ?
direct objectsco-occurrences, but the example can easily besubstituted with other co-occurrence information.Moreover, the model need not be restricted to 3modes; it is very well possible to go to 4 modesand beyond ?
as long as the computations remainfeasible.The NTF decomposition for the verbs ?
sub-jects?
direct objects co-occurrences into the threeloadings matrices is represented graphically in fig-ure 3.
By applying the NTF model to three-way(s,v,o) co-occurrences, we want to extract a gen-eralized selectional preference model, and eventu-ally even induce some kind of frame semantics (inthe broad sense of the word).In the resulting factorization, each verb, subjectand direct object gets a loading value for each fac-tor dimension in the corresponding loadings ma-trix.
The original value for a particular (s,v,o)1The algorithm has been implemented in MATLAB, usingthe Tensor Toolbox for sparse tensor calculations (Bader andKolda, 2007).85Figure 3: Graphical representation of the NTF forlanguage datatriple xsvo can then be reconstructed with equa-tion 3.xsvo =k?i=1ssivviooi (3)To reconstruct the selectional preference valuefor the triple (man,bite,dog), for example, welook up the subject vector for man, the verb vectorfor bite and the direct object vector for dog.
Then,for each dimension i in the model, we multiply theith value of the three vectors.
The sum of thesevalues is the final preference value.4 Results4.1 SetupThe approach described in the previous section hasbeen applied to Dutch, using the Twente NieuwsCorpus (Ordelman, 2002), a 500M words corpusof Dutch newspaper texts.
The corpus has beenparsed with the Dutch dependency parser Alpino(van Noord, 2006), and three-way co-occurrencesof verbs with their respective subject and directobject relations have been extracted.
As dimen-sion sizes, the 1K most frequent verbs were used,together with the 10K most frequent subjects and10K most frequent direct objects, yielding a ten-sor of 1K ?
10K ?
10K.
The resulting tensor isvery sparse, with only 0.0002% of the values be-ing non-zero.The tensor has been adapted with a straight-forward extension of pointwise mutual informa-tion (Church and Hanks, 1990) for three-way co-occurrences, following equation 4.
Negative val-ues are set to zero.22This is not just an ad hoc conversion to enforce non-negativity.
Negative values indicate a smaller co-occurrenceprobability than the expected number of co-occurrences.
Set-ting those values to zero proves beneficial for similarity cal-culations (see e.g.
Bullinaria and Levy (2007)).MI3(x,y,z) = logp(x,y,z)p(x)p(y)p(z)(4)The resulting matrix has been factorized into kdimensions (varying between 50 and 300) with theNTF algorithm described in section 3.2.4.2 ExamplesTable 1, 2 and 3 show example dimensions thathave been found by the algorithm with k = 100.Each example gives the top 10 subjects, verbsand direct objects for a particular dimension, to-gether with the score for that particular dimension.Table 1 shows the induction of a ?police action?frame, with police authorities as subjects, policeactions as verbs and patients of the police actionsas direct objects.In table 2, a legislation dimension is induced,with legislative bodies as subjects3, legislative ac-tions as verbs, and mostly law (proposals) as directobjects.
Note that some direct objects (e.g.
?min-ister?)
also designate persons that can be the objectof a legislative act.Table 3, finally, is clearly an exhibition dimen-sion, with verbs describing actions of display andtrade that art institutions (subjects) can do withworks of art (objects).These are not the only sensible dimensions thathave been found by the algorithm.
A quick qual-itative evaluation indicates that about 44 dimen-sions contain similar, framelike semantics.
In an-other 43 dimensions, the semantics are less clear-cut (single verbs account for one dimension, ordifferent senses of a verb get mixed up).
13 dimen-sions are not so much based on semantic character-istics, but rather on syntax (e.g.
fixed expressionsand pronomina).4.3 EvaluationThe results of the NTF model have been quantita-tively evaluated in a pseudo-disambiguation task,similar to the one used by Rooth et al (1999).
It isused to evaluate the generalization capabilities ofthe algorithm.
The task is to judge which subject(s or s?)
and direct object (o or o?)
is more likelyfor a particular verb v, where (s,v,o) is a combi-nation drawn from the corpus, and s?
and o?
are asubject and direct object randomly drawn from thecorpus.
A triple is considered correct if the algo-rithm prefers both s and o over their counterparts3Note that VVD, D66, PvdA and CDA are Dutch politicalparties.86subjects sus verbs vs objects ob jspolitie ?police?
.99 houd aan ?arrest?
.64 verdachte ?suspect?
.16agent ?policeman?
.07 arresteer ?arrest?
.63 man ?man?
.16autoriteit ?authority?
.05 pak op ?run in?
.41 betoger ?demonstrator?
.14Justitie ?Justice?
.05 schiet dood ?shoot?
.08 relschopper ?rioter?
.13recherche ?detective force?
.04 verdenk ?suspect?
.07 raddraaiers ?instigator?
.13marechaussee ?military police?
.04 tref aan ?find?
.06 overvaller ?raider?
.13justitie ?justice?
.04 achterhaal ?overtake?
.05 Roemeen ?Romanian?
.13arrestatieteam ?special squad?
.03 verwijder ?remove?
.05 actievoerder ?campaigner?
.13leger ?army?
.03 zoek ?search?
.04 hooligan ?hooligan?
.13douane ?customs?
.02 spoor op ?track?
.03 Algerijn ?Algerian?
.13Table 1: Top 10 subjects, verbs and direct objects for the ?police action?
dimensionsubjects sus verbs vs objects ob jsmeerderheid ?majority?
.33 steun ?support?
.83 motie ?motion?
.63VVD .28 dien in ?submit?
.44 voorstel ?proposal?
.53D66 .25 neem aan ?pass?
.23 plan ?plan?
.28Kamermeerderheid ?Chamber majority?
.25 wijs af ?reject?
.17 wetsvoorstel ?bill?
.19fractie ?party?
.24 verwerp ?reject?
.14 hem ?him?
.18PvdA .23 vind ?think?
.08 kabinet ?cabinet?
.16CDA .23 aanvaard ?accepts?
.05 minister ?minister?
.16Tweede Kamer ?Second Chamber?
.21 behandel ?treat?
.05 beleid ?policy?
.13partij ?party?
.20 doe ?do?
.04 kandidatuur ?candidature?
.11Kamer ?Chamber?
.20 keur goed ?pass?
.03 amendement ?amendment?
.09Table 2: Top 10 subjects, verbs and direct objects for the ?legislation?
dimensions?
and o?
(so the (s,v,o) triple ?
that appears in thetest corpus ?
is preferred over the triples (s?,v,o?
),(s?,v,o) and (s,v,o?)).
Table 4 shows three exam-ples from the pseudo-disambiguation task.s v o s?
o?jongere drink bier coalitie aandeel?youngster?
?drink?
?beer?
?coalition?
?share?werkgever riskeer boete doel kopzorg?employer?
?risk?
?fine?
?goal?
?worry?directeur zwaai scepter informateur vodka?manager?
?sway?
?sceptre?
?informer?
?wodka?Table 4: Three examples from the pseudo-disambiguation evaluation task?s test setFour different models have been evaluated.
Thefirst two models are tensor factorization models.The first model is the NTF model, as describedin section 3.2.
The second model is the originalPARAFAC model, without the non-negativity con-straints.The other two models are matrix factorizationmodels.
The third model is the non-negative ma-trix factorization (NMF) model, and the fourthmodel is the singular value decomposition (SVD).For these models, a matrix has been constructedthat contains the pairwise co-occurrence frequen-cies of verbs by subjects as well as direct objects.This gives a matrix of 1K verbs by 10K subjects+ 10K direct objects (1K ?
20K).
The matrix hasbeen adapted with pointwise mutual information.The models have been evaluated with 10-foldcross-validation.
The corpus contains 298,540 dif-ferent (s,v,o) co-occurrences.
Those have beenrandomly divided into 10 equal parts.
So in eachfold, 268,686 co-occurrences have been used fortraining, and 29,854 have been used for testing.The accuracy results of the evaluation are given intable 5.The results clearly indicate that the NTF modeloutperforms all the other models.
The modelachieves the best result with 300 dimensions, butthe differences between the different NTF modelsare not very large ?
all attaining scores around90%.87subjects sus verbs vs objects ob jstentoonstelling ?exhibition?
.50 toon ?display?
.72 schilderij ?painting?
.47expositie ?exposition?
.49 omvat ?cover?
.63 werk ?work?
.46galerie ?gallery?
.36 bevat ?contain?
.18 tekening ?drawing?
.36collectie ?collection?
.29 presenteer ?present?
.17 foto ?picture?
.33museum ?museum?
.27 laat ?let?
.07 sculptuur ?sculpture?
.25oeuvre ?oeuvre?
.22 koop ?buy?
.07 aquarel ?aquarelle?
.20Kunsthal .19 bezit ?own?
.06 object ?object?
.19kunstenaar ?artist?
.15 zie ?see?
.05 beeld ?statue?
.12dat ?that?
.12 koop aan ?acquire?
.05 overzicht ?overview?
.12hij ?he?
.10 in huis heb ?own?
.04 portret ?portrait?
.11Table 3: Top 10 subjects, verbs and direct objects for the ?exhibition?
dimensiondimensions50 (%) 100 (%) 300 (%)NTF 89.52 ?
0.18 90.43 ?
0.14 90.89 ?
0.16PARAFAC 85.57 ?
0.25 83.58 ?
0.59 80.12 ?
0.76NMF 81.79 ?
0.15 78.83 ?
0.40 75.74 ?
0.63SVD 69.60 ?
0.41 62.84 ?
1.30 45.22 ?
1.01Table 5: Results of the 10-fold cross-validation forthe NTF, PARAFAC, NMF and SVD model for 50,100 and 300 dimensions (averages and standarddeviation)The PARAFAC results indicate the fitness of ten-sor factorization for the induction of three-way se-lectional preferences.
Even without the constraintof non-negativity, the model outperforms the ma-trix factorization models, reaching a score of about85%.
The model deteriorates when more dimen-sions are used.Both matrix factorization models performworse than their tensor factorization counterparts.The NMF still scores reasonably well, indicatingthe positive effect of the non-negativity constraint.The simple SVD model performs worst, reaching ascore of about 70% with 50 dimensions.5 Conclusion and Future WorkThis paper has presented a novel method thatis able to investigate three-way co-occurrences.Other distributional methods deal almost exclu-sively with pairwise co-occurrences.
The abilityto keep track of multi-way co-occurrences opensup new possibilities and brings about interestingresults.
The method uses a factorization model ?non-negative tensor factorization ?
that is suitablefor three way data.
The model is able to generalizeamong the data and overcome data sparseness.The method has been applied to the problemof selectional preference induction.
The resultsindicate that the algorithm is able to induce se-lectional preferences, leading to a broad kindof frame semantics.
The quantitative evaluationshows that use of three-way data is clearly benefi-cial for the induction of three-way selectional pref-erences.
The tensor models outperform the sim-ple matrix models in the pseudo-disambiguationtask.
The results also indicate the positive ef-fect of the non-negativity constraint: both mod-els with non-negative constraints outperform theirnon-constrained counterparts.The results as well as the evaluation indicatethat the method presented here is a promising toolfor the investigation of NLP topics, although moreresearch and thorough evaluation are desirable.There is quite some room for future work.
Firstof all, we want to further investigate the useful-ness of the method for selectional preference in-duction.
This includes a deeper quantitative eval-uation and a comparison to other methods for se-lectional preference induction.
We also want toinclude other dependency relations in our model,apart from subjects and direct objects.Secondly, there is room for improvement andfurther research with regard to the tensor factor-ization model.
The model presented here min-imizes the sum of squared distance.
This is,however, not the only objective function possi-ble.
Another possibility is the minimization of theKullback-Leibler divergence.
Minimizing the sumof squared distance assumes normally distributeddata, and language phenomena are rarely normallydistributed.
Other objective functions ?
such as theminimization of the Kullback-Leibler divergence?
might be able to capture the language structures88much more adequately.
We specifically want tostress this second line of future research as one ofthe most promising and exciting ones.Finally, the model presented here is notonly suitable for selectional preference induction.There are many problems in NLP that involvethree-way co-occurrences.
In future work, wewant to apply the NTF model presented here toother problems in NLP, the most important one be-ing word sense discrimination.AcknowledgementsBrett Bader kindly provided his implementation ofnon-negative tensor factorization for sparse ma-trices, from which this research has substantiallybenefited.
The three anonymous reviewers pro-vided fruitful comments and remarks, which con-siderably improved the quality of this paper.ReferencesBrett W. Bader and Tamara G. Kolda.
2006.
EfficientMATLAB computations with sparse and factoredtensors.
Technical Report SAND2006-7592, SandiaNational Laboratories, Albuquerque, NM and Liver-more, CA, December.Brett W. Bader and Tamara G. Kolda.
2007.
Mat-lab tensor toolbox version 2.2. http://csmr.ca.sandia.gov/?tgkolda/TensorToolbox/, Jan-uary.Roberto Basili, Diego De Cao, Paolo Marocco, andMarco Pennacchiotti.
2007.
Learning selectionalpreferences for entailment or paraphrasing rules.
InProceedings of RANLP 2007, Borovets, Bulgaria.Rahul Bhagat, Patrick Pantel, and Eduard Hovy.
2007.Ledir: An unsupervised algorithm for learning di-rectionality of inference rules.
In Proceedings ofConference on Empirical Methods in Natural Lan-guage Processing (EMNLP-07), pages 161?170,Prague, Czech Republic.John A. Bullinaria and Joseph P. Levy.
2007.
Ex-tracting semantic representations from word co-occurrence statistics: A computational study.
Be-havior Research Methods, 39:510?526.J.
D. Carroll and J.-J.
Chang.
1970.
Analysis of in-dividual differences in multidimensional scaling viaan n-way generalization of ?eckart-young?
decom-position.
Psychometrika, 35:283?319.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information & lexicogra-phy.
Computational Linguistics, 16(1):22?29.Lieven De Lathauwer, Bart De Moor, and Joos Vande-walle.
2000.
A multilinear singular value decompo-sition.
SIAM Journal on Matrix Analysis and Appli-cations, 21(4):1253?1278.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In Proceedings of ACL2007, Prague, Czech Republic.R.A.
Harshman.
1970.
Foundations of the parafac pro-cedure: models and conditions for an ?explanatory?multi-mode factor analysis.
In UCLA Working Pa-pers in Phonetics, volume 16, pages 1?84, Los An-geles.
University of California.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In Proc.
of Uncertainty in Artificial Intelli-gence, UAI?99, Stockholm.H.A.L Kiers and I. van Mechelen.
2001.
Three-waycomponent analysis: Principles and illustrative ap-plication.
Psychological Methods, 6:84?110.Tamara Kolda and Brett Bader.
2006.
The TOPHITSmodel for higher-order web link analysis.
In Work-shop on Link Analysis, Counterterrorism and Secu-rity.Thomas Landauer and Susan Dumais.
1997.
A so-lution to Plato?s problem: The Latent SemanticAnalysis theory of the acquisition, induction, andrepresentation of knowledge.
Psychology Review,104:211?240.Thomas Landauer, Peter Foltz, and Darrell Laham.1998.
An Introduction to Latent Semantic Analysis.Discourse Processes, 25:295?284.Daniel D. Lee and H. Sebastian Seung.
2000.
Al-gorithms for non-negative matrix factorization.
InNIPS, pages 556?562.R.J.F.
Ordelman.
2002.
Twente Nieuws Corpus(TwNC), August.
Parlevink Language TechnologyGroup.
University of Twente.K.
Pearson.
1901.
On lines and planes of closest fit tosystems of points in space.
Philosophical Magazine,2(6):559?572.Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of English words.
In31st Annual Meeting of the ACL, pages 183?190.Philip Resnik.
1996.
Selectional Constraints: AnInformation-Theoretic Model and its ComputationalRealization.
Cognition, 61:127?159, November.Mats Rooth, Stefan Riezler, Detlef Prescher, GlennCarroll, and Franz Beil.
1999.
Inducing a semanti-cally annotated lexicon via em-based clustering.
In37th Annual Meeting of the ACL.Amnon Shashua and Tamir Hazan.
2005.
Non-negative tensor factorization with applications tostatistics and computer vision.
In ICML ?05: Pro-ceedings of the 22nd international conference on89Machine learning, pages 792?799, New York, NY,USA.
ACM.L.R.
Tucker.
1966.
Some mathematical notes on three-mode factor analysis.
Psychometrika, 31:279?311.Peter D. Turney.
2007.
Empirical evaluation of fourtensor decomposition algorithms.
Technical ReportERB-1152, National Research Council, Institute forInformation Technology.Gertjan van Noord.
2006.
At Last Parsing Is NowOperational.
In Piet Mertens, Cedrick Fairon, AnneDister, and Patrick Watrin, editors, TALN06.
VerbumEx Machina.
Actes de la 13e conference sur le traite-ment automatique des langues naturelles, pages 20?42, Leuven.M.
Alex O. Vasilescu and Demetri Terzopoulos.
2002.Multilinear analysis of image ensembles: Tensor-faces.
In ECCV, pages 447?460.Michael E. Wall, Andreas Rechtsteiner, and Luis M.Rocha, 2003.
Singular Value Decomposition andPrincipal Component Analysis, chapter 5, pages 91?109.
Kluwel, Norwell, MA, Mar.90
