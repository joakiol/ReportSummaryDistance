Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 734?742,Beijing, August 2010Exploiting Structured Ontology to Organize Scattered Online OpinionsYue Lu, Huizhong Duan, Hongning Wang, ChengXiang ZhaiDepartment of Computer ScienceUniversity of Illinois at Urbana-Champaign{yuelu2,duan9,wang296,czhai}@illinois.eduAbstractWe study the problem of integrating scat-tered online opinions.
For this purpose,we propose to exploit structured ontologyto obtain well-formed relevant aspects toa topic and use them to organize scatteredopinions to generate a structured sum-mary.
Particularly, we focus on two mainchallenges in implementing this idea, (1)how to select the most useful aspects froma large number of aspects in the ontologyand (2) how to order the selected aspectsto optimize the readability of the struc-tured summary.
We propose and exploreseveral methods for solving these chal-lenges.
Experimental results on two dif-ferent data sets (US Presidents and DigitalCameras) show that the proposed methodsare effective for selecting aspects that canrepresent the major opinions and for gen-erating coherent ordering of aspects.1 IntroductionThe explosive growth of online opinions raises in-teresting challenges for opinion integration andsummarization.
It is especially interesting to in-tegrate and summarize scattered opinions in blogarticles and forums as they tend to represent thegeneral opinions of a large number of people andget refreshed quickly as people dynamically gen-erate new content, making them valuable for un-derstanding the current views of a topic.However, opinions in blogs and forums areusually fragmental, scattered around, and buriedamong other off-topic content, so it is quite chal-lenging to organize them in a meaningful way.Traditional text summarization techniques gener-ate an unstructured list of sentences as a sum-mary, which cannot reveal representative opinionson different aspects of a topic or effectively facil-itate navigation into the huge opinion space.
Toaddress this limitation, recent work has shown theusefulness of generating a structured summary ofopinions, in which related opinions are groupedinto topical aspects with explicit labeling of all theaspects.
A major challenge in producing such astructured summary is how to generate these as-pects for an arbitrary topic (e.g., products, politi-cal figures, policies, etc.).
Intuitively, the aspectsshould be concise phrases that can both be easilyinterpreted in the context of the topic under con-sideration and capture the major opinions.
How-ever, where can we find such phrases and whichphrases should we select as aspects?
Furthermore,once we selected aspects, how should we orderthem to improve the readability of a structuredsummary?
One way to generate aspects is to clus-ter all the opinion sentences and then identify rep-resentative phrases in each cluster.
Although as-pects selected in this way can effectively capturethe major opinions, a major limitation is that it isgenerally hard to ensure that the selected phrasesare well connected with the given topic (Chen andDumais, 2000).In this paper, we propose a novel approachto generating aspects by leveraging the ontolo-gies with structured information that are availableonline, such as open domain knowledge base inFreebase1.
Such kind of ontology data is not insmall scale by any measure.
For example, Free-base alone contains more than 10 million topics,3000 types, and 30,000 properties; moreover, it isconstantly growing as people collaboratively con-tribute.
Freebase provides different properties fordifferent types of topics such as personal infor-mation for a ?US President?
and product featuresfor a ?Digital Camera?.
Since this kind of re-sources can provide related entities/relations for a1http://www.freebase.com734wide range of topics , our general idea is to lever-age them as guidance for more informed organi-zation of scattered online opinions, and in partic-ular, to select the most important properties of atopic from such structured ontology as aspects togenerate a structured opinion summary.
A signif-icant advantage of this approach to aspect genera-tion is that the selected aspects are guaranteed tobe very well connected with the topic, but it alsoraises an additional challenge in selecting the as-pects to best capture the major opinions from alarge number of aspects provided for each topic inthe ontology.
Different from some existing workon exploiting ontologies, e.g., (Sauper and Barzi-lay, 2009), which relies on training data, we focuson exploring unsupervised approaches, which canbe applied to a larger scope of topics.Specifically, given a topic with entries in an on-tology and a collection of scattered online opin-ions about the topic, our goal is to generate astructured summary where representative majoropinions are organized with well aligned aspectsand in an order easy for human to follow.
Wepropose the following general approach: First, re-trieval techniques are employed to align opinionsto relevant aspects.
Second, a subset of most inter-esting aspects are selected.
Third, we will furtherorder the selected aspects to present them in a rea-sonable order.
Finally, for the opinions uncoveredby the selected aspects from the ontology, we usea phrase ranking method to suggest new aspects toadd to the ontology for increasing its coverage.Implementing the second and third steps in-volves special challenges.
In particular, withoutany training data, it is unclear how we shouldshow the most interesting aspects in ontology withmajor opinions aligned and which presentationorder of aspects is natural and intuitive for hu-man.
Solving these two challenges is the mainfocus of this paper.
We propose three meth-ods for aspect selection, i.e., size-based, opinioncoverage-based, and conditional entropy-basedmethods, and two methods for aspect ordering,i.e., ontology-ordering and coherence ordering.We evaluate our methods on two different types oftopics: US Presidents and Digital Cameras.
Qual-itative results demonstrate the utility of integratingopinions based on structured ontology as well asthe generalizability of proposed methods.
Quan-titative evaluation is also conducted to show theeffectiveness of our methods.Note that we use the term ?opinion?
to broadlyrefer to any discussion in opinionated sourcessuch as blogs and reviews.
This allows us to for-mulate and solve the problem in a general way.Indeed, the main goal of our work is to extractand organize the major opinions about a topic thatare buried in many scattered opinionated sourcesrather than perform deeper understanding of opin-ions (e.g., distinguishing positive from negativeopinions), which can be done by using any exist-ing sentiment analysis technique as an orthogonalpost-processing step after applying our method.2 Related WorkAspect summarization, i.e., structured opinionsummarization over topical aspects, has attractedmuch attention recently.
Existing work iden-tifies aspects using frequent-pattern/association-rule mining, e.g.
(Liu et al, 2005; Popescu andEtzioni, 2005), sentence clustering, e.g.
(Ga-mon et al, 2005; Leouski and Croft, 1996), ortopic modeling, e.g.
(Mei et al, 2006; Titov andMcDonald, 2008).
After that, meaningful andprominent phrases need to be selected to repre-sent the aspects, e.g.
(Zhao and He, 2006; Meiet al, 2007).
However, these methods suffer fromthe problem of producing trivial aspects.
Conse-quently, some of the aspects generated are verydifficult to interpret (Chen and Dumais, 2000).
Inthis paper, we propose a different kind of approachthat is to use aspects provided by ontology whichare known to be relevant and easy to interpret.Ontology is used in (Carenini et al, 2005) butonly for mapping product features.
The closestwork to ours are (Lu and Zhai, 2008; Sauper andBarzilay, 2009); both try to use well-written arti-cles for summarization.
However, (Lu and Zhai,2008) assumes the well-written article is struc-tured with explicit or implicit aspect information,which does not always hold in practice, while(Sauper and Barzilay, 2009) needs a relativelylarge amount of training data in the given domain.In comparison, our work only needs the ontologyinformation for the given topic which is much eas-ier to obtain from resources such as Freebase.7353 MethodsGiven (1) an input topic T , (2) a large number ofaspects/properties A = {A1, ..., Am} from an on-tology that are related to T , and (3) a huge col-lection of scattered opinion sentences about thetopic DT = {s1, .
.
.
, sn}, our goal is to gener-ate a structured organization of opinions that areboth aligned well with the interesting aspects andrepresentative of major opinions about the topic.The envisioned structured organization consistsof a sequence of selected aspects from ontol-ogy ordered to optimize readability and a set ofsentences matching each selected aspect.
Oncewe obtain a set of sentences in each aspect, wecan easily apply a standard text summarizationmethod to further summarize these sentences, thusthe unique challenges related to our main idea ofexploiting ontology are the following, which arealso the main focus of our study:Aspect Selection: How can we select a subset ofaspects A?
?
A to capture the major opinions inour opinion set DT ?Aspect Ordering: How can we order a subset ofselected aspects A?
so as to present them in an or-der pi(A?)
that is most natural with respect to hu-man perception?New Aspects Suggestion: Can we exploit theopinions in DT to suggest new aspects to be addedto the ontology?3.1 Aspect SelectionIn order to align the scattered opinions to themost relevant aspects, we first use each aspect la-bel Ai ?
A as a query to retrieve a set of rel-evant opinions in the collection Si ?
DT witha standard language modeling approach, i.e., theKL-divergence retrieval model (Zhai and Lafferty,2001).
Up to 1000 opinion sentences are retrievedfor each aspect; each opinion sentence can be po-tentially aligned to several aspects.
In this way,scattered online discussion are linked to the mostrelevant aspects in the ontology, which enables auser to use aspects as ?semantic bridges?
to navi-gate into the opinion space..However, there are usually a lot of candidateaspects in an ontology, and only some are heav-ily commented in online discussions, so showingall the aspects is not only unnecessary, but alsooverwhelming for users.
To solve this problem,we propose to utilize the aligned opinions to fur-ther select a subset of the most interesting aspectsA?
?
A with size k. Several approaches are pos-sible for this subset selection problem.Size-based: Intuitively, the selected subset A?should reflect the major opinions.
So a straightfor-ward method is to order the aspects Ai by the sizeof the aligned opinion sentences Si, i.e., the num-ber of relevant opinion sentences, and then selectthe top k ones.Opinion Coverage-based: The previous methoddoes not consider possible redundancy among theaspects.
A better approach is to select the subsetthat covers as many distinct opinion sentences aspossible.
This can be formulated as a maximumcoverage problem, for which a greedy algorithmis known to be a good approximation: we selectone aspect at a time that is aligned with the largestnumber of uncovered sentences.Conditional Entropy-based: Aspects from a struc-tured ontology are generally quite meaningful, butthey are not designed specifically for organizingthe opinions in our data set.
Thus, they do notnecessarily correspond well to the natural clus-ters in scattered opinions.
To obtain aspects thatare aligned well with the natural clusters in scat-tered opinions, we can first cluster DT into lclusters C = {C1, .
.
.
, Cl} using K-means withTF ?
IDF as features, and then choose the sub-set of aspects that minimize Conditional Entropyof the cluster label given the aspect:A?
= argminH(C|A?)
= argmin???
?Ai?A?,Ci?Cp(Ai, Ci) logp(Ai, Ci)p(Ai)?
?This Conditional Entropy measures the uncer-tainty about the cluster label of a sentence giventhe knowledge of its aspect.
Intuitively, if the as-pects are aligned well with the clusters, we wouldbe able to predict well the cluster label of a sen-tence if we know its aspect, thus there would beless uncertainty about the cluster label.
In theextreme case when the cluster label can be com-pletely determined by the aspect, the conditionalentropy would reach its minimum (i.e., 0).
Intu-itively, the conditional entropy-based method es-sentially selects the most appropriate aspects from736Algorithm 1 Greedy Algorithm forConditional Entropy Based Aspect SelectionInput: A = {A1, ..., Am}Output: k-sized A?
?
A1: A?
= {?mi=1Ai}2: for j=1 to k do3: bestH = ?
; bestA = A04: for each Ai in A do5: tempA?
= {Ai, A?
\Ai}6: if H(C|tempA?)
< bestH then7: bestH = H(C|tempA?
)8: bestA = Ai9: A?
= {bestA,A?
\ bestA}10: output A?the ontology to label clusters of opinions.The exact solution of this combinatorial optimiza-tion problem is NP-complete, so we employ apolynomial time greedy algorithm to approximateit: in the i-th iteration, we select the aspect thatcan minimize the conditional entropy given theprevious i ?
1 selected aspects.
Pseudo code isgiven in Algorithm 1.3.2 Aspect OrderingIn order to present the selected aspects to usersin a most natural way, it is important to obtain acoherent order of them, i.e., generating an orderconsistent with human perception.
To achieve thisgoal, our idea is to use human written articles onthe topic to learn how to organize the aspects au-tomatically.
Specifically, we would order aspectsso that the relative order of the sentences in all theaspects would be as consistent with their order inthe original online discussions as possible.Formally, the input is a subset of selected as-pects A?
; each Ai ?
A?
is aligned with a set ofrelevant opinion sentences Si = {Si,1, Si,2, ...}.We define a coherence measurement function oversentence pairs Co(Si,k, Sj,l), which is set to 1 iffSi,k appears before Sj,l in the same article.
Other-wise, it is set to 0.
Then a coherence measurementfunction over an aspect pair can be calculated asCo(Ai, Aj) =?Si,k?Si,Sj,l?Sj Co(Si,k, Sj,l)|Si||Sj |As an output, we would like to find a permutationp?i(A?)
that maximizes the coherence of all pair-wise aspects, i.e.,p?i(A?)
= arg maxpi(A?
)?Ai,Aj?A?,Ai?AjCo(Ai, Aj)Algorithm 2 Greedy Algorithm forCoherence Based Aspect OrderingInput: AOutput: pi(A)1: for each Ai, Aj in A do2: calculate Co(Ai, Aj)3: for p = 1 to len = A.size() do4: Max = A[1]5: for each aspect Ai in A do6: Ai.coherence = 07: for each aspect Aj in pi(A) do8: Ai.coherence+ = Co(Aj , Ai)9: for each aspect Aj in A, j 6= i do10: Ai.coherence+ = Co(Ai, Aj)11: if Ai.coherence > Max.coherence then12: Max = Ai13: remove Max from A; add Max to pi(A)14: output pi(A)where Ai ?
Aj means that Ai is before Aj .
Itis easy to prove that the problem is NP-complete.Therefore, we resort to greedy algorithms to findapproximations of the solution.
Particularly weview the problem as a ranking problem.
The al-gorithm proceeds by finding at each ranking po-sition an aspect that can maximize the coherencemeasurement, starting from the top of the rank list.The detailed algorithm is given in Algorithm 2.3.3 New Aspects SuggestionFinally, if the opinions cover more aspects than inthe ontology, we also want to identify informativephrases to label such extra aspects; such phrasescan also be used to further augment the ontologywith new aspects.This problem is similar to existing work on gen-erating labels for clusters (Zeng et al, 2004) ortopic models (Mei et al, 2007).
Here we employa simple but representative technique to demon-strate the feasibility of discovering interesting newaspects for augmenting the ontology.
We first ex-tract named entities from scattered opinions DTusing Stanford Named Entity Recognizer (Finkelet al, 2005).
After that, we rank the phrases bypointwise Mutual Information (MI):MI(T, ph) = log P (T, ph)P (T )P (ph)where T is the given topic and ph refers to a candi-date entity phrase.
P (T, ph) is proportional to thenumber of opinion sentences they co-occur; P (T )or P (ph) are proportional to the number of timesT or ph appears.
A higher MI value indicates a737Statistics Category 1 Category 2US president Digital CameraNumber of Topics 36 110Number of Aspects 65?26 32?4Number of Opinions 1001?1542 170?249Table 1: Statistics of Data Setsstronger association.
We can then suggest the topranked entity phrases that are not in the selectedaspects as new aspects.4 Experiments4.1 Data SetsTo examine the generalizability of our methods,we test on two very different categories of top-ics: US Presidents and Digital Cameras.2 For theontology, we leverage Freebase, downloading thestructured ontology for each topic.
For the opin-ion corpus, we use blog data for US Presidents andcustomer reviews for Digital Cameras.
The blogentries for US Presidents were collected by usingGoogle Blog Search3 with the name of a presidentas the query.
Customer reviews for Digital Cam-eras were crawled from CNET4.
The basic statis-tics of our data sets is shown in Table 1.
For all thedata collections, Porter stemmer (Porter, 1997) isapplied and stop words are removed.4.2 Sample ResultsWe first show sample results of automatic orga-nization of online opinions.
We use the opin-ion coverage-based algorithm to select 10 aspects(10-20 aspects were found to be optimal in (Ka?ki,2005)) and then apply the coherence-based aspectordering method.
The number of clusters is set sothat there are on average 15 opinions per cluster.Opinion Organization: Table 2 and Table 3present sample results for President Ronald Rea-gan and Sony Cybershot DSC-W200 camera re-spectively5.
We can see that (1) although Freebaseaspects provide objective and accurate informa-tion about the given topics, extracted opinion sen-tences offer additional subjective information; (2)aligning scattered opinion sentences to most rel-evant aspects in the ontology helps digestion and2We have made our data sets available at http://timan.cs.uiuc.edu/downloads.html .3http://blogsearch.google.com4http://www.cnet.com5Due to space limit, we only show the first few aspects asoutput by our methods.navigation; and (3) the support number, which isthe number of opinion sentences aligned to an as-pect, can show the popularity of the aspect in theonline discussions.Adaptability of Aspect Selection: Being un-supervised is a significant advantage of our meth-ods over most existing work.
It provides flexibil-ity of applying the methods in different domainswithout the requirement of training data, benefit-ing from both the ontology based template guid-ance as well as data-driven approaches.
As a re-sult, we can generate different results for differ-ent topics even in the same domain.
In Table 4,we show the top three selected and ordered as-pects for Abraham Lincoln and Richard Nixon.Although they belong to the same category, differ-ent aspects are picked up due to the differences inonline opinions.
People talk a lot about Lincoln?srole in American Civil War and his famous quo-tation, but when talking about Nixon, people fo-cus on ending the Vietnam war and the Watergatescandal.
?Date of birth?
and ?Government posi-tion?
are ranked first because people tend to starttalking from these aspects, which is more naturalthan starting from aspects like ?Place of death?.Baseline Comparison: We also show below theaspects for Lincoln generated by a representativeapproach using clustering method (e.g.
(Gamon etal., 2005)).
i.e., we label the largest clusters by se-lecting phrases with top mutual information.
Wecan see that although some phrases make sense,not all are well connected with the given topic;using aspects in ontology circumvents this prob-lem.
This example confirms the finding in pre-vious work that the popular existing clustering-based approach to aspects generation cannot gen-erate meaningful labels (Chen and Dumais, 2000).VincentNew Salem State Historic SiteUSS Abraham LincolnMartin Luther King JrGettysburgJohn F.New Aspect Discovery: Finally, in Table 5 weshow some phrases ranked among top 10 usingthe method described in Section 3.3.
They revealadditional aspects covered in online discussionsand serve as candidate new aspects to be added toFreebase.
Interestingly, John Wilkes Booth, whoassassinated President Lincoln, is not explicitly738FreeBase Aspects Supt Representative Opinion SentencesAppointees: 897 Martin Feldstein, whose criticism of Reagan era deficits has not been forgotten.- Martin Feldstein Reagan?s first National Security advisor was quoted as declaring...- Chief Economic AdvisorGovernment Positions Held: 967 1981 Jan 20, Ronald Reagan was sworn in as president as 52 American hostages- President of the United States boarded a plane in Tehran and headed toward freedom.- Jan 20, 1981 to Jan 20, 1989 40th president of the US Ronald Reagan broke the so called ?20 year curse?...Vice president: 847 8 years, 1981-1988 George H. W. Bush as vice president under Ronald Reagan...- George H. W. Bush ...exception to the rule was in 1976, when George H W Bush beat Ronald.Table 2: Opinion Organization Result for President Ronald ReaganFreeBase Aspects Supt Representative Opinion SentencesFormat: 13 Quality pictures in a compact package.- Compact ... amazing is that this is such a small and compact unit but packs so much power.Supported Storage Types: 11 This camera can use Memory Stick Pro Duo up to 8 GB- Memory Stick Duo Using a universal storage card and cable (c?mon Sony)Sensor type: 10 I think the larger ccd makes a difference.- CCD but remember this is a small CCD in a compact point-and-shoot.Digital zoom: 47 once the digital :smart?
zoom kicks in you get another 3x of zoom-2?
I would like a higher optical zoom, the W200 does a great digital zoom translation...Table 3: Opinion Organization Result for Sony Cybershot DSC-W200 Cameralisted in Freebase, but we can find it in people?sonline discussion using mutual information.4.3 Evaluation of Aspect SelectionMeasures: Aspect selection is a new challenge,so there is no standard way to evaluate it.
It is alsovery hard for human to read all of the aspects andopinions and then select a gold standard subset.Therefore, we opt to use indirect measures captur-ing different characteristics of the aspect selectionproblem (1) Aspect Coverage (AC): we first as-sign each aspect Ai to the cluster Cj that has themost overlapping sentences with Ai, approximat-ing the cluster that would come into mind whena reader sees Ai.
Then AC is defined as the per-centage of the clusters covered by at least one as-pect.
(2) Aspect Precision (AP ): for each cov-ered cluster Ci, AP measures the Jaccard similar-ity between Ci as a set of opinions and the unionof all aspects assigned to Ci.
(3) Average AspectPrecision (AAP ): defines averaged AP for allclusters where an uncovered Ci has a zero AP ;it essentially combines AC and AP .
We also re-port Sentence Coverage (SC), i.e., how many dis-tinct opinion sentences can be covered by the se-lected aspects and Conditional Entropy (H), i.e.,how well the selected aspects align with the nat-ural clusters in the opinions; a smaller H valueindicates a better alignment.Results: We summarize the evaluation results inMeasures SC H AC AP AAPPRESIDENTSRandom 503 1.9069 0.5140 0.0933 0.1223Size-based 500 1.9656 0.3108 0.1508 0.0949Opin Cover 746 1.8852 0.5463 0.0913 0.1316Cond Ent.
479 1.7687 0.5770 0.0856 0.1552CAMERASRandom 55 1.6389 0.6554 0.0871 0.1271Size-based 70 1.6463 0.6071 0.1077 0.1340Opin Cover 82 1.5866 0.6998 0.0914 0.1564Cond Ent.
70 1.5598 0.7497 0.0789 0.1574Table 6: Evaluation Results for Aspect SelectionTable 6.
In addition to the three methods de-scribed in Section 3.1, we also include one base-line of averaging 10 runs of random selection.
Thebest performance by each measure on each dataset is highlighted in bold font.
Not surprisingly,opinion coverage-based approach has the bestsentence coverage (SC) performance and condi-tional entropy-based greedy algorithm achievesthe lowest H .
Size-based approach is best in as-pect precision but at the cost of lowest aspect cov-erage.
The trade-off between AP and AC is com-parable to that between precision and recall asin information retrieval while AAP summarizesthe combination of these two.
The greedy algo-rithm based on conditional entropy outperformsall other approaches in AC and also in AAP , sug-gesting that it can provide a good balance betweenAP and AC.739Supt Richard-Nixon Supt Abraham-Lincoln50 Date of birth: 419 Government Positions Held:- Jan 9, 1913 - United States Representative Mar 4,1847-Mar 3,1849108 Tracks Recorded: 558 Military Commands:- 23-73 Broadcast: End of the Vietnam War - American Civil War - United States of America120 Works Written About This Topic: 810 Quotations: - Nearly all men can stand adversity, but if- Watergate you want to test a man?s character, give him power.Table 4: Comparison of Aspect Selection for Two Presidents (aligned opinions are omitted here)Suggested Phrases Supporting Opinion SentencesAbraham Lincoln Presidential Library CDB projects include the Abraham Lincoln Presidential Library and MuseumAbraham Lincoln Memorial ..., eventually arriving at Abraham Lincoln Memorial.John Wilkes Booth John Wilkes Booth shoots President Abraham Lincoln at Ford?s Theatre ...Table 5: New Phrases for Abraham Lincoln4.4 Evaluation of Aspect OrderingHuman Annotation: In order to quantitativelyevaluate the effectiveness of aspect ordering, weconduct user studies to establish gold standard or-dering.
Three users were each given k selected as-pects and asked to perform two tasks for each USPresident: (1) identify clusters of aspects that aremore natural to be presented together (cluster con-straints) and (2) identify aspect pairs where oneaspect is preferred to appear before the other fromthe viewpoint of readability.
(order constraints).We did not ask them to provide a full order ofthe k aspects, because we suspect that there areusually more than one ?perfect?
order.
Instead,identifying partial orders or constraints is easierfor human to perform, thus provides more robustgold standard.Human Agreement: After obtaining the humanannotation results, we first study human consen-sus on the ordering task.
For both types of humanidentified constraints, we convert them into pair-wise relations of aspects, e.g., ?Ai and Aj shouldbe presented together?
or ?Ai should be displayedbefore Aj?.
Then we calculate the agreement per-centage among the three users.
In Table 7, we cansee that only a very small percentage of pair-wisepartial orders (15.92% of the cluster constraintsand none of the order constraints) are agreed byall the three users, though the agreement of clus-tering is much higher than that of ordering.
Thisindicates that ordering the aspects is a subjectiveand difficult task.Measures: Given the human generated gold stan-dard of partial constraints, we use the follow-ing measures to evaluate the automatically gen-AgreedBy Cluster Constraint Order Constraint1 37.14% 89.22%2 46.95% 10.78%3 15.92% 0.00%Table 7: Human Agreement on Orderingerated full ordering of aspects: (1) Cluster Pre-cision (prc): for all the aspect pairs placed inthe same cluster by human, we calculate the per-centage of them that are also placed together inthe system output.
(2) Cluster Penalty (pc): foreach aspect pair placed in the same cluster by hu-man, we give a linear penalty proportional to thenumber of aspects in between the pair that thesystem places; pc can be interpreted as the aver-age number of aspects between aspect pairs thatshould be presented together in the case of mis-ordering.
Smaller penalty corresponds to betterordering performance.
(3) Order Precision (pro):the percentage of correctly predicted aspect pairscompared with human specified order.Results: In Table 8, we report the orderingperformance based on two selection algorithms:opinion coverage-based and conditional entropy-based.
Different selection algorithms provide dif-ferent subsets of aspects for the ordering algo-rithms to operate on.
For comparison with ourcoherence-based ordering algorithm, we include arandom baseline and Freebase ontology ordering.Note that Freebase order is a very strong baselinebecause it is edited by human even though the pur-pose was not for organizing opinions.
To take intoaccount the variation of human annotation, we usefour versions of gold standard: three are from theindividual annotators and one from the union oftheir annotation.
We did not include the gold stan-740Selection Gold Cluster Precision (prc) Cluster Penalty (pc) Order Precision (pro)Algo STD Random Freebase Coherence Random Freebase Coherence Random Freebase CoherenceOpin Cover 1 0.3290 0.9547 0.9505 1.8798 0.1547 0.1068 0.4804 0.7059 0.4510Opin Cover 2 0.3266 0.9293 0.8838 1.7944 0.3283 0.1818 0.4600 0.4000 0.4000Opin Cover 3 0.2038 0.4550 0.4417 2.5208 1.3628 1.7994 0.5202 0.4561 0.5263Opin Cover union 0.3234 0.7859 0.7237 1.8378 0.6346 0.4609 0.4678 0.4635 0.4526Cond Entropy 1 0.2540 0.9355 0.8978 2.0656 0.2957 0.2016 0.5106 0.7111 0.5444Cond Entropy 2 0.2535 0.7758 0.8323 2.1790 0.7530 0.5222 0.4759 0.6759 0.5093Cond Entropy 3 0.2523 0.4030 0.5545 2.3079 2.1328 1.1611 0.5294 0.7143 0.8175Cond Entropy union 0.3067 0.7268 0.7488 1.9735 1.0720 0.7196 0.5006 0.6500 0.6833Table 8: Evaluation Results on Aspect Orderingdard that is the intersection of three annotators be-cause that would leave us with too little overlap.We have several observations: (1) In general, re-sults show large variations when using differentversions of gold standard, indicating the subjec-tive nature of the ordering task.
(2) Coherence-based ordering shows similar performance toFreebase order-based in cluster precision (prc),but when we take into consideration the distance-based penalty (pc) of separating aspects pairs inthe same cluster, coherence-based ordering is al-most always significantly better except in onecase.
This shows that our method can effectivelylearn the coherence of aspects based on how theiraligned opinion sentences are presented in onlinediscussions.
(3) Order precision (pro) can hardlydistinguish different ordering algorithm.
This in-dicates that people vary a lot in their preferencesas which aspects should be presented first.
How-ever, in cases when the random baseline outper-forms others the margin is fairly small, whileFreebase order and coherence-based order have amuch larger margin of improvement when show-ing superior performance.5 Conclusions and Future WorkA major challenge in automatic integration ofscattered online opinions is how to organize allthe diverse opinions in a meaningful way for anygiven topic.
In this paper, we propose to solve thischallenge by exploiting related aspects in struc-tured ontology which are guaranteed to be mean-ingful and well connected to the topic.
We pro-posed three different methods for selecting a sub-set of aspects from the ontology that can bestcapture the major opinions, including size-based,opinion coverage-based, and conditional entropy-based methods.
We also explored two ways toorder aspects, i.e., ontology-order and coherenceoptimization.
In addition, we also proposed ap-propriate measures for quantitative evaluation ofboth aspect selection and ordering.Experimental evaluation on two data sets (USPresident and Digital Cameras) shows that by ex-ploiting structured ontology, we can generate in-teresting aspects to organize scattered opinions.The conditional entropy method is shown to bemost effective for aspect selection, and the coher-ence optimization method is more effective thanontology-order in optimizing the coherence of theaspect ordering, though ontology-order also ap-pears to perform reasonably well.
In addition, byextracting salient phrases from the major opinionsthat cannot be covered well by any aspect in anexisting ontology, we can also discover interest-ing new aspects to extend the existing ontology.Complementary with most existing summariza-tion work, this work proposes a new direction ofusing structured information to organize and sum-marize unstructured opinions, opening up manyinteresting future research directions.
For in-stance, in order to focus on studying aspect selec-tion and ordering, we have not tried to optimizesentences matching with aspects in the ontology;it would be very interesting to further study howto accurately retrieve sentences matching each as-pect.
Another promising future work is to orga-nize opinions using both structured ontology in-formation and well-written overview articles.AcknowledgmentWe thank the anonymous reviewers for their use-ful comments.
This paper is based upon work sup-ported in part by an IBM Faculty Award, an AlfredP.
Sloan Research Fellowship, an AFOSR MURIGrant FA9550-08-1-0265, and by the NationalScience Foundation under grants IIS-0347933,IIS-0713581, IIS-0713571, and CNS-0834709.741ReferencesCarenini, Giuseppe, Raymond T. Ng, and Ed Zwart.2005.
Extracting knowledge from evaluative text.In K-CAP ?05: Proceedings of the 3rd internationalconference on Knowledge capture, pages 11?18,New York, NY, USA.
ACM.Chen, Hao and Susan Dumais.
2000.
Bringing or-der to the web: automatically categorizing searchresults.
In CHI ?00: Proceedings of the SIGCHIconference on Human factors in computing systems,pages 145?152, New York, NY, USA.
ACM.Finkel, Jenny Rose, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In ACL ?05: Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics, pages 363?370, Morristown, NJ, USA.Association for Computational Linguistics.Gamon, Michael, Anthony Aue, Simon Corston-Oliver, and Eric K. Ringger.
2005.
Pulse: Min-ing customer opinions from free text.
In Famili,A.
Fazel, Joost N. Kok, Jose?
Mar?
?a Pen?a, ArnoSiebes, and A. J. Feelders, editors, IDA, volume3646 of Lecture Notes in Computer Science, pages121?132.
Springer.Ka?ki, Mika.
2005.
Optimizing the number of searchresult categories.
In CHI ?05: CHI ?05 extendedabstracts on Human factors in computing systems,pages 1517?1520, New York, NY, USA.
ACM.Leouski, Anton V. and W. Bruce Croft.
1996.
An eval-uation of techniques for clustering search results.Technical report.Liu, Bing, Minqing Hu, and Junsheng Cheng.
2005.Opinion observer: analyzing and comparing opin-ions on the web.
In WWW ?05: Proceedings of the14th international conference on World Wide Web,pages 342?351, New York, NY, USA.
ACM.Lu, Yue and Chengxiang Zhai.
2008.
Opinion in-tegration through semi-supervised topic modeling.In Huai, Jinpeng, Robin Chen, Hsiao-Wuen Hon,Yunhao Liu, Wei-Ying Ma, Andrew Tomkins, andXiaodong Zhang, editors, WWW, pages 121?130.ACM.Mei, Qiaozhu, Chao Liu, Hang Su, and ChengXiangZhai.
2006.
A probabilistic approach to spatiotem-poral theme pattern mining on weblogs.
In WWW?06: Proceedings of the 15th international confer-ence on World Wide Web, pages 533?542.Mei, Qiaozhu, Xuehua Shen, and ChengXiang Zhai.2007.
Automatic labeling of multinomial topicmodels.
In Berkhin, Pavel, Rich Caruana, and Xin-dong Wu, editors, KDD, pages 490?499.
ACM.Pang, Bo and Lillian Lee.
2007.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1-2):1?135.Popescu, Ana-Maria and Oren Etzioni.
2005.
Ex-tracting product features and opinions from reviews.In HLT ?05, pages 339?346, Morristown, NJ, USA.Association for Computational Linguistics.Porter, M. F. 1997.
An algorithm for suffix stripping.pages 313?316.Sauper, Christina and Regina Barzilay.
2009.
Auto-matically generating wikipedia articles: A structure-aware approach.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 208?216,Suntec, Singapore, August.
Association for Compu-tational Linguistics.Titov, Ivan and Ryan McDonald.
2008.
Modelingonline reviews with multi-grain topic models.
InWWW ?08: Proceeding of the 17th internationalconference on World Wide Web, pages 111?120,New York, NY, USA.
ACM.Zeng, Hua-Jun, Qi-Cai He, Zheng Chen, Wei-YingMa, and Jinwen Ma.
2004.
Learning to clusterweb search results.
In SIGIR ?04: Proceedingsof the 27th annual international ACM SIGIR con-ference on Research and development in informa-tion retrieval, pages 210?217, New York, NY, USA.ACM.Zhai, Chengxiang and John Lafferty.
2001.
Model-based feedback in the language modeling approachto information retrieval.
In Proceedings of CIKM2001, pages 403?410.Zhao, Jing and Jing He.
2006.
Learning to generatelabels for organizing search results from a domain-specified corpus.
In WI ?06: Proceedings of the2006 IEEE/WIC/ACM International Conference onWeb Intelligence, pages 390?396, Washington, DC,USA.
IEEE Computer Society.742
