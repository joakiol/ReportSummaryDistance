Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1232?1240,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAccurate Semantic Class Classifier for Coreference ResolutionZhiheng Huang1, Guangping Zeng1,2, Weiqun Xu3, and Asli Celikyilmaz11EECS Department, University of California at Berkeley,CA 94720, USA{zhiheng,gpzeng,asli}@eecs.berkeley.edu2Computer Science Department, School of Information Engineering,University of Science and Technology Beijing, China3ThinkIT, Institute of Acoustics, Chinese Academy of Sciences,Beijing, 100190, Chinaxuweiqun@hccl.ioa.ac.cnAbstractThere have been considerable attemptsto incorporate semantic knowledge intocoreference resolution systems: differentknowledge sources such as WordNet andWikipedia have been used to boost the per-formance.
In this paper, we propose newways to extract WordNet feature.
Thisfeature, along with other features such asnamed entity feature, can be used to buildan accurate semantic class (SC) classifier.In addition, we analyze the SC classifica-tion errors and propose to use relaxed SCagreement features.
The proposed accu-rate SC classifier and the relaxation of SCagreement features on ACE2 coreferenceevaluation can boost our baseline systemby 10.4% and 9.7% using MUC score andanaphor accuracy respectively.1 IntroductionCoreference resolution is used to determine whichnoun phrases (including pronouns, proper names,and common nouns) refer to the same entities indocuments.
Much work on coreference resolutionis based on (Soon et al, 2001), which built a de-cision tree classifier to label pairs of mentions ascoreferent or not.
Recent work aims to improvethe performance from two aspects: new modelsand new features.
The former cast the pair wisemention classifications into various forms such asthe best path in a Bell tree (Luo et al, 2004), thebest graph cut (Nicolae and Nicolae, 2006), in-teger linear programming (Denis and Baldridge,2007) and graph partition based conditional model(McCallum and Wellner, 2004).
The latter de-velop and investigate new linguistic features forthe problem.
For instance, WordNet (Poesio et al,2004), Wikipedia (Ponzetto and Strube, 2006), se-mantic neighbor words (Ng, 2007a), and patternbased features (Yang and Su, 2007) have been ex-tensively studied.Deeper linguistic knowledge is required to en-able the coreference resolution to reach a higherlevel of performance (Kehler et al, 2004).
An im-portant type of semantic knowledge that has beenemployed in coreference resolution system is thesemantic class (SC) of an NP, which can be usedto filter out the coreference between semanticallyincompatible NPs.
However, the difficulty is toaccurately compute the semantic class features.
Inthis paper, we show that the WordNet may not beefficiently employed in the traditional way suchas (Soon et al, 2001; Ng, 2007a; Ponzetto andStrube, 2006) to compute the semantic class fea-tures.
We introduce new ways to use the WordNetand the experiments show its effectiveness in de-termining the semantic classes for noun phrases.In addition, we analyze the classification errors ofthe SC classifier and propose to use relaxed SCagreement features.
With these proposed featuresand other standard syntactic features (which arecommonly employed in existing coreference sys-tems), our coreference resolution system can ob-tain an increase of 10.4% for MUC score and 9.7%for anaphor accuracy from the baseline in ACE2evaluation.2 Related WorkWordNet (Fellbaum, 1998) as an important knowl-edge source has been widely employed in previ-ous coreference resolution work.
For example,Harabagiu et al (2001) have used WordNet rela-tions such as synonym and is-a to mine the pat-terns of WordNet paths for pairs of antecedentsand anaphors.
Due to the nature of the rule basedcoreference system (in contrast to machine learn-ing based), the weights of relations may not beaccurately estimated.
Vieira and Poesio (2000)and Markert and Nissim (2005) have used Word-Net synonym and hyponym etc.
to determine ifan anaphor semantically relates to one previousNP.
Ponzetto and Strube (2006) have used Word-Net semantic similarity and relatedness scores be-tween antecedents and candidate anaphors.
Their1232work is different to this work in the following: 1)Their work involves various relations such as hy-ponyms and meronyms while ours only makes useof hypernyms; and 2) Their work focuses on in-vestigating if two NPs have particular WordNet re-lations or not, while ours focuses on using Word-Net hypernyms for their SC classification and thentesting their SC compatibility.
In doing so, we candirectly model the accuracy of semantic class clas-sification and test its impact on coreference reso-lution.While the SC of a proper name is computedfairly accurately using a named entity (NE) recog-nizer, many coreference resolution systems sim-ply assign to a common noun the first (i.e., mostfrequent) WordNet synset as its SC (Soon et al,2001; Markert and Nissim, 2005).
This heuris-tics, apparently, did not lead to good performance.The best reported ACE2 coreference resolutionsystem (Ng, 2007a; Ng, 2007b) has proposed anaccurate SC classifier which used heterogeneoussemantic knowledge sources.
WordNet is justone of the several knowledge sources which havebeen utilized.
However, the WordNet based fea-tures is not informative compared to other featuressuch as the semantic neighbor feature.
Similarly,Ponzetto and Strube (2006) have discovered thatthe WordNet feature is no more informative thanthe community-generated Wikipedia feature.
Inthis paper, we focus on the investigation of vari-ous usages of WordNet for the SC classificationtask.
The work which is directly comparable toours would be (Ng, 2007a; Ng, 2007b).Other similar work includes the mention detec-tion (MD) task (Florian et al, 2006) and jointprobabilistic model of coreference (Daume?
III andMarcu, 2005).
The MD task identifies the bound-ary of a mention, its mention type (e.g., pronoun,name), and its semantic type (e.g., person, orga-nization).
Unlike them, we do not perform theboundary detection, as we make use of the nounphrases directly from the noun phrase chunker andNE recognizer.
The joint probabilistic model mod-els the MD and coreference simultaneously, whileour work focuses on them separately.3 Semantic Class ClassificationIn this section, we describe how we compile thetraining corpus and extract features using Word-Net.
We report our results on the ACE coreferencecorpus due to that it has been commonly used andit was annotated SCs of six types.1 As in (Ng,1Person, organization, gpe, location and facility are ex-plicitly annotated.
The rest noun phrases are other type.2007a), we first train a classifier to predict the SCof an NP.
This SC information is used later in thecoreference resolution stage.
For example, the au-dience is classified as SC of person, and it thusshould not be coreferent with the security industry,which is usually classified as organization.
Thistask is by no means trivial.
First, while the classi-fication of Tom Hanks being SC of person can beaccurately achieved by an NE recognizer, the as-sociation of audience and person requires seman-tic language source such as WordNet.
Second, thesame noun phrase can be annotated with differentSCs under different context.
For example, the au-thorities is usually annotated as person, but it issometimes as organization.
Even worse, the samenoun phrases are sometimes annotated with one ofthe five explicitly annotated classes while some-times are not annotated at all (thus falling into theother SC).
For example, people is annotated asperson SC explicitly 20 times and is not annotatedat all 21 times in the ACE2 testset.
This inconsis-tent annotation adversely affects the performanceof an SC classifier.
And this in turn would causeerrors during coreference stage.
In section 4.3, weshow how to relax the strict SC agreement featureto address this.3.1 Training instance creationWe use ACE Phase 2 Coreference corpus to trainthe SC classifier.
Each noun phrase which is iden-tified by the noun phrase chunker or NE recognizeris used to create a training instance.
Each instanceis represented by a set of lexical, syntactic and se-mantic features, as described below.
If the NP un-der consideration is annotated as one of the fiveACE SCs in the corpus, then the classification ofthe associated training instance is the ACE SC ofthe NP.
Otherwise, the instance is labeled as other.ACE 2 corpus has a training set and a test setwhich comprise of 422 and 97 texts respectively.We divide the training set into a new training and adevelopment set: the former consists of 90% ran-domly generated and stratified original training in-stances and the latter consists of the rest 10% in-stances.
The test set remains the same as in ACE2corpus.
The size of each dataset and its SC dis-tributions are shown in Table 1.
Note that thetraining and development datasets have exactly thesame distributions of SCs due to the stratificationprocedure.
That is, each class has the same pro-portion in training and development datasets.
Wetune the feature parameters against developmentset and report performance on both developmentset and test set.1233Table 1: Distributions of SCs in ACE2 corpus.Size PER ORG GPE FAC LOC OTHTrain 55629 20.29 7.30 8.42 0.61 0.55 62.80Dev 6181 20.29 7.30 8.42 0.61 0.55 62.80Test 15360 20.48 7.57 6.90 0.85 0.41 63.793.2 Lexical featuresEach instance is represented as a bag of featuresand is fed into a classifier in training stage.
Wepresent four binary lexical feature sets as follows.Word unigrams and bigrams: An N-gram isa sub-sequence of N words from a given nounphrase.
Unigram forms the bag of words feature,and bigram forms the pairs of words feature, andso forth.
We have considered word unigram andbigram features in our experiments.First and last words: This feature extracts thefirst and last words of an NP.
For example, the firstword the and the last word store are extracted fromthe NP the main store.
This feature does not onlycoarsely models the influence of the first word, forexample, a or the, but also models the head word,since the head word usually is the last word in theNP.Head word: We use Collins style rules(Collins, 1999) to extract the head words for givenNPs.
These features should be most informativeif the training corpus is large enough.2 For exam-ple, the head word company of the NP the com-pany immediately determines its SC being organi-zation.
However, due to the sparseness of trainingdata, its potential importance is adversely affected.3.3 Semantic featuresNE feature is extracted from Stanford named en-tity recognizer (NER) (Finkel et al, 2005).
Threetypes of named entities: person, location and or-ganization can be recognized for a given NP.
Thisfeature is primarily useful for SC classification ofproper nouns.WordNet is a large English lexicon in which se-mantically related words are connected via cogni-tive synonyms (synsets).
The WordNet is a use-ful tool for word semantics analysis and has beenwidely used in natural language processing appli-cations.
In WordNet, synsets are organized into hi-erarchies with hypernym/hyponym relationships:Y is a hypernym of X if every X is a (kind of) Y(X is called a hyponym of Y in this case).The WordNet is employed in (Ng, 2007a) asfollowing to create the WN CLASS feature.
Foreach keyword w as shown in the right column of2It, however, is mostly useful for nominal noun phrase andnot for the pronoun and proper noun phrases.Table 2, if the head noun of a given NP is a hy-ponym of w in WordNet,3 then the word w be-comes a feature for such NP.
It is explained thatthese keywords are correlated with the ACE SCsand they are obtained via experimentation withWordNet and the ACE SCs of the NPs in the ACEtraining data.
However, it is likely that these hand-crafted keywords have poor coverage for generalcases.
As a result, it may not make full use ofWordNet semantic knowledge.
This will be shownin our individual feature contribution experimentin Section 3.5.Table 2: List of keywords used in WordNet seman-tic feature in (Ng, 2007a).ACE SC KeywordsPER personORG social groupFAC establishment, construction, building,facility, workplaceGPE country, province, government, town, city,administration, society, island, communityLOC dry land, region, landmass, body of watergeographical area, geological formationThere are other ways of using WordNet for se-mantic feature extraction.
For example, Ponzettoand Strube (2006) have employed WordNet sim-ilarity measure for coreference resolution.
Thedifference is that they created the feature di-rectly at the coreference resolution stage, ie, us-ing the WordNet similarity between the antecedentand anaphor to determine if they are coreferent,while we focus on using this feature to classifyan NP into a particular SC.
For comparison, weimplemented a WordNet similarity based feature(WN SIM) as follows: for a given NP head wordand a key word as listed in Table 2, the WordNetsimilarity package (Seco et al, 2004) models thelength of path traveling from the head word to thekey word over the WordNet network.
It then com-putes the semantic similarity based on the path.For example, the similarity between company andsocial group is 0.77, while the similarity betweencompany and person is 0.59.
The key word whichreceives the highest similarity to the head word ismarked as a feature.The WN CLASS feature may suffer from thecoverage problem and the WN SIM feature isheavily dependent on the definition of similaritymetric which may turn out to be inappropriate forcoreference resolution task.
To make better use ofWordNet knowledge, we attempt to directly intro-duce hypernyms for the NP head words (we denote3Only the first synset of the NP is used.1234it as WN HYP feature).
The most similar workto ours is (Daume?
III and Marcu, 2005), in whichtwo most common synsets from WordNet for allwords in an NP and their hypernyms are extractedas features.
We avoid augmenting the hypernymsfor non-head words in the NP to prevent introduc-ing noisy information, which may potentially cor-rupt the hypernym feature space.Considering a WordNet hypernym structure asshown in Fig.
1 for the word company, its firstsynset (an institution created to conduct business)has a unique id of 08058098 and can also be rep-resented by a set of description words (companyin this case).
Its third synset (the state of beingwith someone) has an id of 13929588 and descrip-tion words of company, companionship, fellow-ship, society.
Each synset can be extended by itshypernym synsets.
For example, the direct hyper-nym of the first synset is the synset of 08053576which can be described as institution, establish-ment.
The augmentation of hypernyms for NPhead words can introduce useful information, butcan also bring noise if the head word or the synsetof head word are not correctly identified.
For anoptimal use of WordNet hypernyms, four ques-tions shall be addressed: 1) how many depths arerequired to tradeoff the generality (thus more in-formative) and the specificity (thus less noisy)?
2)which synset of the given word is needed to beaugmented?
3) which representation (synset id orsynset word) is better?
and 4) is it helpful to en-code the hypernym depth into the hypernym fea-ture?4 These four questions provide the guidelineto search the optimal use of WordNet.
We will de-sign experiments in Section 3.5 to determine theoptimal configuration of WN HYP feature.state(08008335)(13931145)(13928668)(08053576)(07950920)companydepth 2depth 3depth 4depth 1social groupinstitution,establishmentorganization,organisation(00024720)friendship,friendlyrelationshipfellowship,society(13929588)(08058098)company, companionship,companyrelationshipFigure 1: WordNet hypernym hierarchy for theword company.4For example, we encode the synset 08053576 as08053576-1, with the last digit 1 indicating the depth of hy-pernym with regard to the entry word company.3.4 Learning algorithmMaximum entropy (ME) models (Berger et al,1996; Manning and Klein, 2003), also known aslog-linear and exponential learning models, hasbeen adopted in the SC classification task.
Max-imum entropy models can integrate features frommany heterogeneous information sources for clas-sification.
Each feature corresponds to a constrainton the model.
Given a training set of (C,D),where C is a set of class labels and D is a setof feature represented data points, the maximumentropy model attempts to maximize the log like-lihoodlogP (C|D,?)
=?(c,d)?
(C,D)logexp?i?ifi(c, d)?c?exp?j?jfi(c, d)(1)where fi(c, d) are feature indicator functions and?iare the parameters to be estimated.
We use MEmodels for both SC classification and mention pairclassification.3.5 SC classification evaluationWe design three experiments to test the accuracyof our classifiers.
The first experiment evalu-ates the individual contribution of different fea-ture sets to SC classification accuracy.
In par-ticular, a ME model is trained on the 55,629training instances using the following feature setsseparately: 1) unigram, 2) bigram, 3) first-lastword, 4) head word (HW), 5) named entities(NE), 6) HW+WN CLASS, 7) HW+WN SIM,and 8) variants of HW+WN HYP.
Note thatHW+WN CLASS is the semantic feature used in(Ng, 2007a), HW+WN SIM is the semantic fea-ture using WordNet similarity measure (Seco etal., 2004), and variants of HW+WN HYP are thework proposed in this paper.
We combine headword and the semantic features due to the fact thatWordNet features are dependent on head wordsand they could be treated as units.
In the secondexperiment, features are fed into the ME modelincrementally until all features have been used.5Finally, we perform the feature ablation experi-ments.
That is, we remove one feature at a timefrom the entire feature set and test the accuracyloss.
The SC classification performance is mea-sured by accuracy, i.e., the proportion of the cor-rectly classified instances among all test instances.Individual feature contribution Table 3 showsthe SC classification accuracy of all NPs (all)and non-pronoun NPs (non-PN) on the develop-ment and test datasets using individual feature5The optimal of HW+WN HYP configuration is used.1235sets.
Among all the lexical features, unigram fea-Table 3: SC classification accuracy of ME usingindividual feature sets for development and testACE2 datasets.Feature type dev testall non-PN all non-PNunigram 81.3 81.6 72.4 71.9bigram 32.5 36.4 26.3 28.4first-last word 80.1 80.2 71.6 71.0HW 78.2 78.0 68.3 67.1NE 74.0 82.8 73.1 81.9HW+WN CLASS 79.5 79.4 70.3 69.5HW+WN SIM 81.2 81.4 73.8 73.6HW+WN HYP (1) 82.6 83.1 74.8 74.7HW+WN HYP (3) 82.8 83.4 75.2 75.2HW+WN HYP (6) 83.1 83.7 75.6 75.7HW+WN HYP (9) 83.0 83.6 75.7 75.7HW+WN HYP (?)
83.1 83.7 75.8 75.9HW+WN HYP (6) 82.8 83.3 75.6 75.7word formHW+WN HYP (6) 82.9 83.5 75.4 75.4depth encodedHW+WN HYP (6) 83.0 83.6 76.4 76.6first synsetture performs the best (81.3%) for all NPs over thedevelopment dataset.
The bigram feature performspoorly due to the sparsity problem: NPs usuallyconsist of one to three words.
The first-last wordfeature effectively models the prefix words (suchas a and the) and the head words and thus obtains areasonably high accuracy of 80.1%.
As mentionedbefore, the head word feature may suffer from thesparsity and it results in the accuracy of 78.2%.We also list the accuracies for non-pronoun NPSC classification, which are slightly different com-pared to all NP SC classification except for bi-gram, in which the accuracy has increased 3.9%.Although Stanford NER performs well onnamed entity recognition task, it results in ac-curacy of 74.0% for all NP SC classification,due to its inability to deal with pronouns suchas he and common nouns such as the govern-ment.
The removal of pronouns significantlyboosts its accuracy to 82.8%.
The introduc-tion of semantic feature HW+WN CLASS booststhe performance to 79.5% compared to the headword alone of 78.2%.
This conforms to (Ng,2007a) that only small gain can be achieved us-ing WN CLASS feature.
The HW+WN SIMfeature outperforms HW+WN CLASS and theaccuracy reaches 81.2%.
For the variants ofHW+WN HYP, we first search the optimal depth.This is performed by using all synsets for NP headword, encoding the feature using synset id (ratherthan synset word), and no hypernym depth is en-coded in the features.
We try various depths of1, 3, 6, 9 and ?, with ?
signifies that no depthconstraint is imposed.
The optimal depth of 6 isobtained with the accuracy of 83.1% over the de-velopment dataset.
We then fix the depth of 6 to tryusing synset word as features, using synset id withdepth encoded as features, and using first synsetonly.
The results show that the optimum is to en-code the features using hypernym synset id with-out hypernym depth information and all synsetsare considered for hypernym extraction.
This isslightly different from the previous finding (Soonet al, 2001; Lin, 1998b) that a coreference res-olution system employing only the first WordNetsynset performs slightly better than that employ-ing more than one synset.6 The best result reachesthe accuracy of 83.1%.
Although the best seman-tic feature only outperforms the best lexical fea-ture by 1.8% on the development dataset, its gainin the test dataset is more significant (3.2%, from72.4% to 75.6%).Incremental feature contribution Once weuse the training and development datasets to findthe optimal configuration of HW+WN HYP se-mantic feature, we use all lexical features and theoptimal HW+WN HYP feature incrementally totrain an ME model over the combination of train-ing and development datasets.
Table 4 showsthe SC classification accuracy of all NPs (all)and non-pronoun NPs (non-PN) on the train-ing+development (we refer it as training hereafter)and test datasets.Table 4: SC classification accuracy of ME usingincremental feature sets for training and test ACE2datasets.Feature type train testall non-PN all non-PNHW 87.8 89.0 68.6 67.6+WN HYP 87.8 89.0 75.7 75.8+unigram 91.5 93.3 77.7 78.1+bigram 93.1 95.2 78.7 79.2+first-last word 93.2 95.3 78.8 79.3+NE 93.4 95.6 83.1 84.4Ng 2007a - 85.0 - 83.3Note that the significant higher accuracies intraining compared to test are due to the overfit-ting problem.
The interesting evaluation thus re-mains on the test data.
As can be seen, the in-clusion of more features results in higher perfor-mance.
This is more obvious in the test datasetthan in the training dataset.
The inclusion of the6In fact, the accuracy of the test data supports their claims.The accuracy using the first synset compared to using allsynsets results in the accuracy increase from 75.6% to 76.4%for all NPs over the test dataset.1236optimized WN HYP feature (ie, using all synsets?hypernyms up to 6 depth and with synset id encod-ing) results in 7.1% increase for all NP SC classifi-cation over test data.
This shows the effectivenessof the WN HYP features to overcome the sparsityof head word feature.
The unigram, bigram andfirst-last word features offer reasonable accuracygain, and the final inclusion of NE boosts the over-all performance to 83.1% for all NP and 84.4% fornon pronoun NPs over test data.
This result canbe directly compared to the SC classification ac-curacy as reported in (Ng, 2007a), in which thehighest accuracy is 83.3% for non pronoun NPs.7The large difference between the highest trainingaccuracies is due to that our classifier is trained di-rectly on the ACE2 training dataset, while their SCclassifier was trained on BBN Entity Type Corpus(Weischedel and Brunstein, 2005), which is fivetimes larger than the ACE2 corpus used by us.
Inaddition to WordNet, they have adopted multipleknowledge sources which include BBN?s Identi-Finder (this is equivalent to the Stanford NER inour work), BLLIP corpus and Reuters Corpus,8and dependency based thesaurus (Lin, 1998a).
Itis remarkable that our SC classifier can achieveeven higher accuracy only using WordNet hyper-nym and NE features.
It is worth noting that thesmall accuracy gain is indeed hard to achieve con-sidering that the test data size is large (15360).Feature ablation experiment We now performthe feature ablation experiments to further deter-mine the importance of individual features.
We re-move one feature at a time from the entire featureset.
Table 5 shows the SC classification accuracyof all NPs (all) and non-pronoun NPs (non-PN) onthe training and test datasets respectively.Table 5: SC classification accuracy of ME by re-moving one feature at a time for training and testACE2 datasets.Feature type train testall non-PN all non-PNoverall 93.4 95.6 83.1 84.4-HW 93.4 95.5 82.9 84.2-WN HYP 93.4 95.5 82.6 83.8-HW+WN HYP 93.4 95.5 82.3 83.5-unigram 93.4 95.5 82.9 84.2-bigram 92.5 94.5 82.7 84.0-first-last word 93.4 95.5 82.9 84.1-NE 93.2 95.3 78.8 79.3Again, the significant higher accuracies in train-ing compared to test are due to overfitting.
The re-7All NP accuracy was not reported as they excluded thepronouns in creating their training and test data.8They use these corpus to extract patterns to induce SC ofcommon nouns.moval of NE feature results in the largest accuracyloss of 4.3% (from 83.1% to 78.8%) for all nounson test data.
It follows WN HYP (0.5% loss) andthe bigram (0.4%).
If we treat HW+WN HYP asone feature, the removal of it results in accuracyloss of 0.8% for all nouns on test data.
The un-igram, first-last word and head word each resultsin the loss of 0.2%.
The reason that the removalof NE results in a much significant loss is due tothe fact that the NE feature is quite different fromother features.
Its strength is to distinguish SCs forproper names, while other features are more sim-ilar (their targets are common nouns).
The pro-posed use of HW+WN HYP can bring 0.8% gainon top of other features, higher than other informa-tive lexical features including unigram and first-last word.3.6 Error analysisA closer look at the errors produced by our SCclassifier reveals that the second probable label isvery likely to be the actual labels if the first proba-ble one is wrong.
In fact, if we allow the classifierto predict two most probable labels and the clas-sification is judged to be true if the actual label isone of the two predictions, then the classificationaccuracy increases from 83.1% to 96.4%.
Thisis because that the same noun phrases are some-times annotated with one of the five explicitly an-notated classes while sometimes are not annotatedat all (thus falling into the other SC).
Again forthe example of people.
It is annotated as personSC 20 times and is not annotated at all 21 times.Given the same feature set for this instance, thebest the classifier can do is to classify it to othersemantic class.
To address this annotation incon-sistency issue, we relax the SC agreement featurefrom the strict match in designing coreference res-olution features.
For example, if the first probableSC of an NP matches the second probable SC ofanother NP, we still give some partial match credit.4 Application to Coreference ResolutionWe can now incorporate the NP SC classifier intoour ME based coreference resolution system.
Thissection examines how our WordNet hypernym fea-tures help improve the coreference resolution per-formance.4.1 Experimental setupWe use the ACE-2 (version 1.0) coreference cor-pus.
Each raw text in this corpus was prepro-cessed automatically by a pipeline of NLP com-ponents, including sentence boundary detection,1237POS-tagging and text chunking.
The statistics ofcorpus and mention extraction are shown in Table6, where g-mention is the automatically extractedmentions which contain the annotated (gold) men-tions.
The recalls of gold mentions are 95.88%and 95.93% for training and test data respectively.Table 6: Statistics for corpus and extracted men-tions.text# mention# g-mention# gold# recall(%)train 422 61810 22990 23977 95.88test 97 15360 5561 5797 95.93Our coreference system uses Maximum En-tropy model to determine whether two NPs arecoreferent.
As in (Soon et al, 2001; Ponzetto andStrube, 2006), we generate training instances asfollows: a positive instance is created for eachanaphoric NP, NPj, and its closest antecedent,NPi; and a negative instance is created for NPjpaired with each of the intervening NPs, NPi+1,NPi+2, ..., NPj?1.
Each instance is representedby syntactic or semantic features described as fol-lows.
All training data are used to train a maxi-mum entropy model.
In the test stage,we select theclosest preceding NP that is classified as corefer-ent with NPjas the antecedent of NPj.
If no suchNP exists, no antecedent is selected for NPj.Unlike other natural language processing taskssuch as information extraction which have de factoevaluation metrics, it is an open question whichevaluation is the most suitable one.
The evalu-ation becomes more complicated when automat-ically extracted mentions (in contrast to the goldmentions) are used.
To facilitate the comparisonwith previous work, we report performance us-ing two different scoring metrics: the commonly-used MUC scorer (Vilain et al, 1995) and the ac-curacy of the anaphoric references (Ponzetto andStrube, 2006).
An anaphoric reference is correctlyresolved if it and its closest antecedent are in thesame coreference chain in the resulting partition.4.2 Baseline featuresWe briefly review the baseline features used inthis paper as follows.
More detailed informationand implementations can be found at (Soon et al,2001; Versley et al, 2008).
For example, theALIAS feature takes values of true or false.
Thevalue of true means that the antecedent and theanaphor refer to the same entity (date, person, or-ganization or location).
The ALIAS feature de-tection works differently depending on the namedentity type.
For date, the day, month, and yearvalues are extracted and compared.
For person,the last words of the noun phrases are compared.For organization names, the alias detection checksfor acronym match such as IBM and InternationalBusiness Machines Corp.Lexical features STRING MATCH: true ifNPiand NPjhave the same spelling after remov-ing article and demonstrative pronouns, false oth-erwise.
ALIAS: true if NPjis the alias of NPi.Grammatical features I PRONOUN: true ifNPiis a pronoun; J PRONOUN: true if NPjis pronoun; J REFL PRONOUN: true if NPjisreflexive pronoun; J PERS PRONOUN: true ifNPjis personal pronoun; J POSS PRONOUN:true if NPjis possessive pronoun; J PN: trueif NPjis proper noun; J DEF: true if NPjstarts with the; J DEM: true if NPjstarts withthis, that, these or those; J DEM NOMINAL:true if NPjis a demonstrative nominal noun;J DEM PRONOUN: true if NPjis a demonstra-tive pronoun; PROPER NAME: true if both NPiand NPjare proper names; NUMBER: true ifNPiand NPjagree in number; GENDER: trueif NPiand NPjagree in gender; APPOSITIVE:true if NPiand NPjare appositions.Distance feature DISTANCE: how many sen-tences NPiand NPjare apart.Semantic feature SEMCLASS: This feature isimplemented from (Soon et al, 2001).
Its possiblevalues are true, false, or unknown.
First the fol-lowing semantic classes are defined: female, male,person, organization, location, date, time, money,percent, and object.
Each of these defined seman-tic classes is then mapped to a WordNet synset.Then the semantic class determination module de-termines the semantic class for every NP as thefirst synset of the head noun of the NP.
If suchsynset is a hyponym of defined semantic class,then such semantic class is assigned to the NP.Otherwise, unknown class is assigned.
Finally, theagreement of semantic classes of NPiand NPjisunknown if either assigned class is unknown; trueif their assigned class are the same, false other-wise.
Notice that the WordNet use in (Ng, 2007a)and this feature apply in the same principle exceptthat 1) the former is used in SC classification whilethe latter is used directly for coreference resolu-tion, and 2) they have different semantic class cat-egories.4.3 Proposed WordNet agreement featuresFor each instance which consists of NPiand NPj,we apply our SC classifier to label them, say liandljrespectively.
We then use these two induced la-1238bels to propose the SC agreement feature for NPiand NPj.
In particular, SC STRICT is true if liand ljare the same and they are not of other type,false otherwise; SC COARSE is true if both liandljare not of other type; In addition, we proposetwo other SC agreement features to cope with theSC classification errors.
SC RELAX1 is true if thefirst probable of NPi, li1, is not other type and isthe same as the second probable of Nj, lj2, or vicevisa.
SC RELAX2 is true if the second probableof NPi, li2, is not other type and is the same as thesecond probable label of NPj, lj2.
The purpose inusing SC RELAX1 and SC RELAX2 features isto relax the strict SC agreement feature in the hopethat partial SC match is useful for coreference res-olution.4.4 Coreference resultsTable 7 shows the MUC score for ACE2 corpusand its three partitions: bnews, npaper, and nwireusing baseline and the proposed semantic features.It also shows the accuracy of resolving anaphorsfor all nouns in ACE2 corpus.
SC STRICT isthe configuration that uses the baseline featureswith the SEMCLASS (Soon et al, 2001) replacedby SC STRICT, and SC COARSE, SC RELAX1,and SC RELAX2 are incrementally included intothe SC STRICT feature set.As can be seen, the SC STRICT significantlyboosts the performance: it improves the MUCF score and anaphor accuracy of baseline from57.7% to 65.7% and 37.7% to 46.3% respectively.It is remarkable that the new use of WordNet canobtain such significant gain in both MUC scoreand anaphor accuracy.
The large improvementof the precision from 58.1% to 73.3% for allNPs shows that the SC STRICT feature can ef-fectively filter out the semantic incompatible pairsof antecedents and anaphors.
In accordance withour hypothesis, the relaxation of strict SC agree-ment by including SC COARSE, SC RELAX1and SC RELAX2 help improve the performancefurther, which is reflected by both MUC score andanaphor accuracy.
For example, compared to thebaseline, the use of all proposed four SC agree-ment features results in the maximal accuracy gainof 9.7% (from 37.7% to 47.4%) and the use ofSC STRICT, SC COARSE, and SC RELAX1 re-sults in the maximal MUC score gain of 10.4%(from 57.7% to 68.1%).Our best MUC score is 68.1% which outper-forms the MUC score of 64.6% as reported in(Ng, 2007a) by 3.5%, while our best accuracyof anaphor is 47.4%, which is 4.1% less thanthe accuracy of 51.5% in (Ng, 2007a).
Notethat, unlike (Ng, 2007a) which performed exten-sive experiments using different machine learn-ing algorithms, alternative use of features (eitherconstraint or normal features), and heterogeneousknowledge sources, this paper simply uses onelearning classifier (ME model) and only employsWordNet and Stanford NER semantic sources.The different MUC and accuracy scores reflectthe non-trivial cases of evaluating coreference sys-tems.
While we leave out the discussion of whichevaluation is more appropriate, we focus on show-ing that the proposed SC classifier can bring sig-nificant boost from the baseline using both MUCand accuracy metrics.5 ConclusionWe have showed that the traditional use of Word-Net in coreference resolution may not effectivelyexploit the WordNet semantic knowledge.
We pro-posed new ways to extract WordNet feature.
Thisfeature, along with other features such as namedentity feature, can be used to build an accurate se-mantic class (SC) classifier.
In addition, we ana-lyzed the classification errors of the SC classifierand relaxed SC agreement features to cope withpart of the classification errors.
The proposed ac-curate SC classifier and the relaxation of SC agree-ment features can boost our baseline coreferenceresolution system by 10.4% and 9.7% using MUCscore and anaphor accuracy respectively.AcknowledgmentsWe wish to thank Yannick Versley for his sup-port with BART coreference resolution system andthe three anonymous reviewers for their invaluablecomments.
This research was supported by BritishTelecom grant CT1080028046 and BISC Programof UC Berkeley.ReferencesA.
L. Berger, S. A. D. Pietra, and V. J. D. Pietra 1996.A maximum entropy approach to natural languageprocessing.
Computational Linguistics, 22(1):39?71.M.
Collins 1999.
Head-driven statistical models fornatural language parsing.
PhD thesis, University ofPennsylvania.H.
Daume?
III and D. Marcu.
2005.
A large-scaleexploration of effective global features for a jointentity detection and tracking model.
In Proc.
ofHLT/EMNLP, pages 97-104.1239Table 7: MUC score and accuracy of baseline and proposed SC agreement features for ACE2 dataset.MUC score AccuracyAll bnews npaper nwire AllSystem R P F R P F R P F R P Fbaseline 57.4 58.1 57.7 56.6 55.4 56.0 59.3 60.4 59.9 56.2 58.6 57.3 37.7Ng 2007a 59.5 70.6 64.6 - - - - - - - - - 51.5SC STRICT 59.6 73.3 65.7 61.6 72.8 66.7 60.3 74.9 66.8 56.8 72.1 63.5 46.3+ SC COARSE 59.2 76.7 66.8 61.0 76.7 67.9 59.8 77.2 67.4 56.6 76.2 64.9 45.9+ SC RELAX1 59.8 79.0 68.1 61.3 79.8 69.3 60.9 80.3 69.3 57.2 76.7 65.5 47.2+ SC RELAX2 60.2 77.7 67.8 61.5 78.2 68.9 61.4 78.9 69.1 57.5 75.7 65.4 47.4P.
Denis and J. Baldridge.
2007.
Joint determinationof anaphoricity and coreference resolution using in-teger programming.
In HLT-NAACL.C.
Fellbaum.
1998.
An electronic lexical database.The MIT press.J.
Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into informationextraction systems by Gibbs sampling.
In Proc.
ofACL, pages 363-370.R.
Florian, H. Jing, N. Kambhatla, and I. Zitouni.2006.
Factorizing complex models: a case study inmention detection.
In Proc.
of COLING/ACL, pages473-480.S.
M. Harabagiu, R. C. Bunescu, and S. J. Maiorano.2001.
Text and knowledge mining for coreferenceresolution.
In Proc.
of NAACL, pages 55-62.A.
Kehler, D. Appelt, L. Taylor, and A. Simma.
2004.The (non)utility of predicate-argument frequenciesfor pronoun interpretation.
In Proc.
of HLT/NAACL,pages 289-296.D.
Lin.
1998a.
Automatic retrieval and clusteringof similar words.
In Proc.
of COLING/ACL, pages768-774.D.
Lin.
1998b.
Using collocation statistics in informa-tion extraction.
In Proc.
of MUC-7.X.
Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S.Roukos.
2004.
A mention synchronous coreferenceresolution algorithm based on the Bell tree.
In Proc.of the ACL.C.
Manning and D. Klein.
2003.
Optimization,Maxent Models, and Conditional Estimation with-out Magic.
Tutorial at HLT-NAACL 2003 and ACL2003.K.
Markert and M. Nissim.
2005.
Comparing knowl-edge sources for nominal anaphora resolution.
Com-putational Linguistics, 31(3):367-401.A.
McCallum and B. Wellner.
2004.
Conditional mod-els of identity uncertainty with application to nouncoreference.
In Proc.
of the NIPS.V.
Ng.
2007a.
Semantic Class Induction and Corefer-ence Resolution.
In Proc.
of the ACL.V.
Ng.
2007b.
Shallow Semantics for CoreferenceResolution.
In Proc.
of the IJCAI.C.
Nicolae and G. Nicolae 2006.
BESTCUT: A GraphAlgorithm for Coreference Resolution.
In Proc.
ofthe EMNLP.M.
Poesio, R. Mehta, A. Maroudas, and J. Hitzeman.2004.
Learning to resolve bridging references.
InProc.
of the ACL.S.
P. Ponzetto and M. Strube.
2006.
Exploiting se-mantic role labeling, WordNet and Wikipedia forcoreference resolution.
In Proc.
of the HLT/NAACL,pages 192-199.N.
Seco, T. Veale, and J. Hayes.
2004.
An IntrinsicInformation Content Metric for Semantic Similarityin WordNet.
Proc.
of the European Conference ofArtificial Intelligence.W.
M. Soon, H. T. Ng, and D. C. Y. Lim.
A machinelearning approach to coreference resolution of nounphrases.
Computation Linguistics, 27(4):521-544.Y.
Versley, S. P. Ponzetto, M. Poesio, V. Eidelman, A.Jern, J. Smith, X. Yang, and A. Moschitti.
2008.BART: a modular toolkit for coreference resolution.ACL 2008 System demo.R.
Vieira and M. Poesio.
2000.
An empirically-basedsystem for processing definite descriptions.
Compu-tational Linguistics, 26(4):539-593.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.Hirschman.
1995.
A model-theoretic coreferencescoreing scheme.
In Proc.
of MUC-6, pages 45-52.R.
Weischedel and A. Brunstein.
2005.
BBN pronouncoreference and entity type corpus.
Linguistic DataConsortium.X.
Yang and J. Su.
2007.
Coreference resolution us-ing semantic relatedness information from automat-ically discovered pattens.
In Proc.
of the ACL.1240
