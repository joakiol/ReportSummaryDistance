SESSION 9: SPEECH IIIFrancis KubalaBBN Systems and Techno log ies10 Mou l ton  StreetCambr idge MA 02138This session consisted of five papers whose contentsspanned a broad range of topics in speech recognition.
Theydealt with problems in the basic areas of acoustic modeling,statistical language modeling, and recognition searchtechniques, as well as adaptation of both the acoustic andlanguage models to new data.
All papers included experimentaltest results on well-known data sets and conditions wherepossible.The first paper, presented by Jean-Lue Gauvain, formulatedthe training of mixture multivariate Gaussian HMM densities asa Bayesian learning problem.
This formalism provides aunified framework for several basic problems in speechrecognit ion-- in i t ia l  training of the HMM parameters,incremental retraining (adaptation), and parameter smoothing.Experimentally, this approach as reduced the SI recognitionword error rate by about 10%, compared to AT&T's usualsegmental K-means training algorithm, on a large test set of 34speakers.
Since these both were essentially Viterbi trainingprocedures (estimated from only the single best state sequence),it would be interesting to compare the Bayesian formulation tothe commonly used Baurn-Welch ML training algorithm.
In aspeaker adaptation experiment, using 2 minutes of supervisedadaptation data, a 32% reduction in error rate was reported onfour test speakers.
It should be noted, however, that nearly allof that gain was achieved by the two female speakers.
It is notclear that this improvement would remain if (two) gender-dependent SI models were used as the baseline.In the second paper, from CMU, Xuedong Huang presentedthree diverse techniques for supervised speaker adaptation--codebook adaptation, model interpolation and speakernormalization.
The codebook adaptation procedure, whichexploited the semi-continuous (tied-mixture) structure of theHMM observation densities in the CMU system, lead to a 15%error reduction.
The second technique interpolated the baselineSI model with a speaker-specific one.
To make the proceduremore robust to sparse training, the HMM densities wereclustered to a total of 500.
Together, these procedures reducedthe error by about 25% using 40 adaptation utterances from fourtest speakers.
Interestingly, performance continued to improveas more adaptation data was used, and with 300 utterances itexceeded speaker-dependent performance with 600 utterances.In the normalization experiment, a multi-layer perceptron(MLP) was proposed to estimate a spectral mapping betweentwo speakers.
The procedure was evaluated by comparing cross-speaker ecognition (train on one speaker, test on another) tocross-speaker with normalization.
It appears that genderdifference was the dominant effect in the control experiment,however, affecting two of the three test speakers.The third paper was presented by Doug Paul fromMIT/Lincoln.
He reported on his experiences with backoff N-gram language models and a stack decoder.
Backoff N-grammodels have been used as a standard 'control' grammar in therecent ATIS evaluation, largely due to Paul's effort.
In asummary study of bigram grammars at several sites, he foundthat, for the same test set perplexity, class-based N-grammodels outperformed word-based ones.
During the discussion,Fred Jelinek announced that the interpolated N-gram is nowfavored at IBM over the backoff model when the training issparse.
At the last DARPA workshop, Paul proposed animplementation of a stack decoder as a standard interfacebetween speech and natural language.
At that time, the decoderhad only been tested under synthetic onditions.
In this paper,he reports that the algorithm often fails when stochasticlanguage models and real speech data are used.Michael Riley, from AT&T, presented the next paper on theproblem of finding the optimal word sequence, given a sequence(or more generally, a lattice) of phoneme labels and durations.Decision trees were used to estimate the label and durationlikelihoods directly from automatically abeled training data.On a standard DARPA test set, with the word-pair grammar, thisapproach yielded 17% word error, even though the phoneticrecognition rate was near 80%.
Moreover, there was no gainfor the duration modeling.
It should be noted also, that thebottom-up lexical access problem, as posed here, is usuallyavoided by most systems employing HMMs, by constrainingthe acoustic search from the outset o the phoneme sequencesfound in a pre-defmed lexicon.The last paper was given by Salim Roukos from IBM on adynamic (adaptive) language model.
Here, the static parametersof a trigram language model were updated from a cache of N-grams computed from a fixed number of the most recentlyobserved words.
The IBM TANGORA isolated-word ecognizer,with a 20K word office correspondence vocabulary, was used asa testbed.
Five test speakers dictated 5000 words from 14documents.
The recognition word error was reduced by about10% averaged over the test documents which varied from 100 to800 words in length.
It was observed that there was a verysmall improvement for using a trigram cache over a unigramcache even though perplexity predicted a larger difference.
Theinterested reader should review a similar cache-based approachto adapting the language model, by De Mori and Kuhn, that waspresented in session 7.271
