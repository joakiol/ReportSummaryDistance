Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694?704,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsUnsupervised Parse Selection for HPSGRebecca Dridan and Timothy BaldwinDept.
of Computer Science and Software EngineeringUniversity of Melbourne, Australiardridan@csse.unimelb.edu.au, tb@ldwin.netAbstractParser disambiguation with precision gram-mars generally takes place via statistical rank-ing of the parse yield of the grammar usinga supervised parse selection model.
In thestandard process, the parse selection model istrained over a hand-disambiguated treebank,meaning that without a significant investmentof effort to produce the treebank, parse selec-tion is not possible.
Furthermore, as treebank-ing is generally streamlined with parse selec-tion models, creating the initial treebank with-out a model requires more resources than sub-sequent treebanks.
In this work, we show that,by taking advantage of the constrained natureof these HPSG grammars, we can learn a dis-criminative parse selection model from rawtext in a purely unsupervised fashion.
This al-lows us to bootstrap the treebanking processand provide better parsers faster, and with lessresources.1 IntroductionParsing with precision grammars is generally a two-stage process: (1) the full parse yield of the preci-sion grammar is calculated for a given item, oftenin the form of a packed forest for efficiency (Oepenand Carroll, 2000; Zhang et al, 2007); and (2) theindividual analyses in the parse forest are ranked us-ing a statistical model (?parse selection?).
In the do-main of treebank parsing, the Charniak and Johnson(2005) reranking parser adopts an analogous strat-egy, except that ranking and pruning are incorpo-rated into the first stage, and the second stage isbased on only the top-ranked parses from the firststage.
For both styles of parsing, however, parse se-lection is based on a statistical model learned from apre-existing treebank associated with the grammar.Our interest in this paper is in completely remov-ing this requirement of parse selection on explicitlytreebanked data, ie the development of fully unsu-pervised parse selection models.The particular style of precision grammar we ex-periment with in this paper is HPSG (Pollard andSag, 1994), in the form of the DELPH-IN suiteof grammars (http://www.delph-in.net/).One of the main focuses of the DELPH-IN collab-oration effort is multilinguality.
To this end, theGrammar Matrix project (Bender et al, 2002) hasbeen developed which, through a set of question-naires, allows grammar engineers to quickly pro-duce a core grammar for a language of their choice.Bender (2008) showed that by using and expandingon this core grammar, she was able to produce abroad-coverage precision grammar of Wambaya ina very short amount of time.
However, the Gram-mar Matrix can only help with the first stage of pars-ing.
The statistical model used in the second stageof parsing (ie parse selection) requires a treebank tolearn the features, but as we explain in Section 2, thetreebanks are created by parsing, preferably with astatistical model.
In this work, we look at methodsfor bootstrapping the production of these statisticalmodels without having an annotated treebank.
Sincemany of the languages that people are building newgrammars for are under-resourced, we can?t dependon having any external information or NLP tools,and so the methods we examine are purely unsuper-vised, using nothing more than the grammars them-694selves and raw text.
We find that, not only can weproduce models that are suitable for kick-starting thetreebanking process, but the accuracy of these mod-els is comparable to parsers trained on gold standarddata (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in appli-cations (Miyao et al, 2008).2 The problemThe current method of training a parse selectionmodel uses the [incr tsdb()] treebanking mechanism(Oepen, 2001) and works well for updating modelsfor mature grammars, although even for these gram-mars, building a new model for a different domainrequires a time-consuming initial treebanking effort.The treebanks used with DELPH-IN grammars aredynamic treebanks (Oepen et al, 2004) created byparsing text and having an annotator select the cor-rect analysis (or discard all of them).
The annotationprocess involves making binary decisions based onso-called parse discriminants (Carter, 1997).
When-ever the grammar is changed, the treebank can bequickly updated by re-parsing and re-applying theold annotation decisions.
This treebanking processnot only produces gold standard trees, but also a setof non-gold trees which provides the negative train-ing data necessary for a discriminative maximum en-tropy model.The standard process for creating a parse selectionmodel is:1. parse the training set, recording up to 500highest-ranking parses for each sentence;2. treebank the training set;3. extract features from the gold and non-goldparses;4. learn feature weights using the TADM toolkit.1(Malouf, 2002)The useful training data from this process is theparses from those sentences for which: more thanone parse was found; and at least one parse has beenannotated as correct.
That is, there needs to be bothgold and non-gold trees for any sentence to be usedin training the discriminative model.1http://tadm.sourceforge.net/There are two issues with this process for newgrammars.
Firstly, treebanking takes many person-hours, and is hence both time-consuming and ex-pensive.
Complicating that is the second issue: N -best parsing requires a statistical model.
While it ispossible to parse exhaustively with no model, pars-ing is much slower, since the unpacking of resultsis time-consuming.
Selective unpacking (Zhang etal., 2007) speeds this up a great deal, but requiresa parse selection model.
Treebanking is also muchslower when the parser must be run exhaustively,since there are usually many more analyses to man-ually discard.This work hopes to alleviate both problems.
Byproducing a statistical model without requiring hu-man treebanking, we can have a working and effi-cient parser with less human effort.
Even if the top-1 parses this parser produces are not as accurate asthose trained on gold standard data, this model canbe used to produce the N -best analyses for the tree-banker.
Since our models are much better than ran-dom selection, we can afford to reduce N and stillhave a reasonable likelihood that the correct parseis in that top N , making the job of the treebankermuch faster, and potentially leading to even betterparse selection accuracy based on semi-supervisedor fully-supervised parse selection.3 Data and evaluationOur ultimate goal is to use these methods for under-resourced languages but, since there are no pre-existing treebanks for these languages, we have nomeans to measure which method produces the bestresults.
Hence, in this work, we experiment withlanguages and grammars where we have gold stan-dard data, in order to be able to evaluate the qual-ity of the parse selection models.
Since we havegold-standard trained models to compare with, thisenables us to fully explore how these unsupervisedmethods work, and show which methods are worthtrying in the more time-consuming and resource-intensive future experiments on other languages.
Itis worth reinforcing that the gold-standard data isused for evaluation only, except in calculating thesupervised parse selection accuracy as an upper-bound.The English Resource Grammar (ERG:695Language Sentences Average Averagewords parsesJapanese 6769 10.5 49.6English 4855 9.0 59.5Table 1: Initial model training data, showing the averageword length per sentence, and also the ambiguity mea-sured as the average number of parses found per sentence.Flickinger (2002)) is an HPSG-based grammarof English that has been under development formany person years.
In order to examine thecross-lingual applicability of our methods, we alsouse Jacy, an HPSG-based grammar of Japanese(Siegel and Bender, 2002).
In both cases, we usegrammar versions from the ?Barcelona?
release,from mid-2009.3.1 Training DataBoth of our grammars come with statistical models,and the parsed data and gold standard annotationsused to create these models are freely available.
Aswe are trying to simulate a fully unsupervised setup,we didn?t want any influence from these earlier mod-els.
Hence, in our experiments we used the parseddata from those sentences that received less than 500parses and ignored any ranking, thus annulling theeffects of the statistical model.
This led to a re-duced data set, both in the number of sentences, andin the fact that the more ambiguous sentences werediscarded, but it allows clean comparison betweendifferent methods, without incorporating external in-formation.
The details of our training sets are shownin Table 1,2 indicating that the sentence lengths arerelatively short, and hence the ambiguity (measuredas average parses per sentence) is low for both ourgrammars.
The ambiguity figures also suggest thatthe Japanese grammar is more constrained (less am-biguous) than the English grammar, since there are,on average, more parses per sentence for English,even with a lower average sentence length.3.2 Test DataThe test data sets used throughout our experimentsare described in Table 2.
The tc-006 data set is from2Any sentences that do not have both gold and non-goldanalyses (ie, had no correct parse, only one parse, or none) arenot included in these figures.Test Set Language Sentences Average Averagewords parsestc-006 Japanese 904 10.7 383.9jhpstgt English 748 12.8 4115.1catb English 534 17.6 9427.3Table 2: Test data, showing the average word length persentence, and also the ambiguity measured as the averagenumber of parses found per sentence.
Note that the ambi-guity figures for the English test sets are under-estimates,since some of the longer sentences timed out before giv-ing an analysis count.the same Tanaka Corpus (Tanaka, 2001) which wasused for the Japanese training data.
There is a widervariety of treebanked data available for the Englishgrammar than for the Japanese.
We use the jhp-stgt data set, which consists of text from Norwegiantourism brochures, from the same LOGON corpusas the English training data (Oepen et al, 2004).
Inorder to have some idea of domain effects, we alsouse the catb data set, the text of an essay on open-source development.3 We see here that the sentencesare longer, particularly for the English data.
Also,since we are not artificially limiting the parse am-biguity by ignoring those with 500 or more parses,the ambiguity is much higher.
This ambiguity figuregives some indication of the difficulty of the parseselection task.
Again we see that the English sen-tences are more ambiguous, much more in this case,making the parse selection task difficult.
In fact,the English ambiguity figures are an under-estimate,since some of the longer sentences timed out beforeproducing a parse count.
This ambiguity can be afunction of the sentence length or the language it-self, but also of the grammar.
A more detailed andinformative grammar makes more distinctions, notall of which are relevant for every analysis.3.3 EvaluationThe exact match metric is the most common accu-racy metric used in work with the DELPH-IN toolset, and refers to the percentage of sentences forwhich the top parse matched the gold parse in everyway.
This is akin to the sentence accuracy that is oc-casionally reported in the parsing literature, except3The Cathedral and the Bazaar, by Eric Raymond.Available from: http://catb.org/esr/writings/cathedral-bazaar/696that it also includes fine-grained syntactico-semanticfeatures that are not often present in other parsingframeworks.
Exact match is a useful metric for parseselection evaluation, but it is very blunt-edged, andgives no way of evaluating how close the top parsewas to the gold standard.
Since these are very de-tailed analyses, it is possible to get one detail wrongand still have a useful analysis.
Hence, in additionto exact match, we also use the EDMNA evalua-tion defined by Dridan (2009).
This is a predicate?argument style evaluation, based on the semanticoutput of the parser (MRS: Minimal Recursion Se-mantics (Copestake et al, 2005)).
This metric isbroadly comparable to the predicate?argument de-pendencies of CCGBank (Hockenmaier and Steed-man, 2007) or of the ENJU grammar (Miyao andTsujii, 2008), and also somewhat similar to thegrammatical relations (GR) of the Briscoe and Car-roll (2006) version of DepBank.
The EDMNA met-ric matches triples consisting of predicate names andthe argument type that connects them.44 Initial ExperimentsAll of our experiments are based on the same basicprocess: (1) for each sentence in the training datadescribed in Section 3.1, label a subset of analysesas correct and the remainder as incorrect; (2) traina model using the same features and learner as inthe standard process of Section 2; (3) parse the testdata using that model; and (4) evaluate the accuracyof the top analyses.
The differences lay in how the?correct?
analyses are selected each time.
Each ofthe following sections detail different methods fornominating which of the (up to 500) analyses fromthe training data should be considered pseudo-goldfor training the parse selection model.4.1 Upperbound and baseline modelsAs a first step we evaluated each data set using anupperbound and a baseline model.
The upperboundmodel in this case is the model trained with goldstandard annotations.
These accuracy figures areslightly lower than others found in the literature forthis data, since, to allow for comparison, we lim-ited the training data to the sets described in Table 1.4The full EDM metric also includes features such as tenseand aspect, but this is less comparable to the other metrics men-tioned.Test Set Exact EDMMatch Precision Recall F-scoretc-006 72.90 0.961 0.957 0.959jhpstgt 48.07 0.912 0.908 0.910catb 22.29 0.838 0.839 0.839Table 3: Accuracy of the gold standard-based parse se-lection model.Test Set Exact EDMMatch Precision Recall F-scoretc-006 17.26 0.779 0.839 0.807jhpstgt 12.48 0.720 0.748 0.734catb 8.30 0.646 0.698 0.671Table 4: Accuracy of the baseline model, trained on ran-domly selected pseudo-gold analyses.By throwing out those sentences with more than 500parses, we exclude much of the data that is used inthe standard model and so our exact match figuresare slightly lower than might be expected.For the baseline model, we used random selectionto select our gold analyses.
For this experiment, werandomly assigned one parse from each sentence inthe training data to be correct (and the remainder ofanalyses as incorrect), and then used that ?gold stan-dard?
to train the model.
Results for the upperboundand baseline models are shown in Tables 3 and 4.As expected, the results for Japanese are muchhigher, since the lower ambiguity makes this an eas-ier task.
The catb test set results suffer, not onlyfrom being longer, more ambiguous sentences, butalso because it is completely out of the domain ofthe training data.The exact match results from the random baselineare approximately what one might expect, given therespective ambiguity levels in Table 2.
The EDMfigures are perhaps higher than might be expectedgiven random selection from the entire parse forest.This results from using a precision grammar, withan inbuilt notion of grammaticality, hence constrain-ing the parser to only produce somewhat reasonableparses, and creating a reasonably high baseline forour parse selection experiments.We also tried a separate baseline, eliminating theparse selection model altogether, and using randomselection directly to select the top analysis.
The ex-act match and EDM precision results were slightlylower than using random selection to train a model,697which may be due to the learner giving weight tofeatures that are common across the training data,but the differences weren?t significant.
Recall wassignificantly lower when using random selection di-rectly, due to the time outs caused by running with-out a model.
For this reason, we use the randomselection-based model results as our baseline for theother unsupervised parse selection models, notingthat correctly identifying approximately three quar-ters of the dependencies in the jhpstgt set, and over80% when using the Japanese grammar, is a fairlyhigh baseline.4.2 First attemptsAs a first approach to unsupervised parse selection,we looked at two heuristics to designate some num-ber of the analyses as ?gold?
for training.
Both ofthese heuristics looked independently at the parsesof each sentence, rather than calculating any num-bers across the whole training set.The first method builds on the observation fromthe random selection-based model baseline exper-iment that just giving weight to common featurescould improve parser accuracy.
In this case, welooked at the edges of the parsing chart.
For eachsentence, we counted the number of times an edgewas present in an analysis, and used that number(normalised by the total number of times any edgewas used) as the edge weight.
We then calculatedan analysis score by summing the edge weights ofall the edges in that analysis, and dividing by thenumber of edges, to give an average edge weight foran analysis.
All analyses that had the best analysisscore for a sentence were designated ?gold?.
Since itwas possible for multiple analyses to have the samescore, there could be multiple gold analyses for anyone sentence.
If all the analyses had the same score,this sentence could not be used as part of the train-ing data.
This method has the effect of selecting theparse(s) most like all the others, by some definitionsthe centroid of the parse forest.
This has some rela-tionship to the partial training method described byClark and Curran (2006), where the most frequentdependencies where used to train a model for theC&C CCG parser.
In that case, however, the de-pendencies were extracted only from analyses thatmatched the gold standard supertag sequence, ratherthan the whole parse forest.Test Set Exact Match F-scoreEdges Branching Edges Branchingtc-006 17.48 21.35 0.815 0.822jhpstgt 15.27 17.53 0.766 0.780catb 9.36 10.86 0.713 0.712Table 5: Accuracy for each test set, measured both as per-centage of sentences that exactly matched the gold stan-dard, and f-score over elementary dependencies.The second heuristic we tried is one often used asa baseline method: degree of right (or left) branch-ing.
In this instance, we calculated the degree ofbranching as the number of right branches in a parsedivided by the number of left branches (and viceversa for Japanese, a predominantly left-branchinglanguage).
In the same way as above, we designatedall parses with the best branching score as ?gold?.Again, this is not fully discriminatory, and it wascommon to get multiple gold trees for a given sen-tence.Table 5 shows the results for these two methods.All the results show an improvement over the base-line, with all but the F-score for the Edges methodof tc-006 being at a level of statistical significance.5The only statistically significant difference betweenthe Edges and Branching methods is over the jhp-stgt data set.
While improvement over random isencouraging, the results were still uninspiring andso we moved on to slightly more complex methods,described in the next section.5 Supertagging ExperimentsThe term supertags was first used by Bangalore andJoshi (1999) to describe fine-grained part of speechtags which include some structural or dependencyinformation.
In that original work, the supertagswere LTAG (Schabes and Joshi, 1991) elementarytrees, and they were used for the purpose of speed-ing up parsing by restricting the allowable leaf types.Subsequent work involving supertags has mostly fo-cussed on this efficiency goal, but they can also beused to inform parse selection.
Dalrymple (2006)and Blunsom (2007) both look at how discrimina-tory a tag sequence is in filtering a parse forest.
This5All statistical significance tests in these experiments use thecomputationally-intensive randomisation test described in Yeh(2000), with p < 0.05.698work has shown that tag sequences can be success-fully used to restrict the set of parses produced, butgenerally are not discriminatory enough to distin-guish a single best parse.
Toutanova et al (2002)present a similar exploration but also go on to in-clude probabilities from a HMM model into theparse selection model as features.
There has alsobeen some work on using lexical probabilities fordomain adaptation of a model (Hara et al, 2007;Rimell and Clark, 2008).
In Dridan (2009), tag se-quences from a supertagger are used together withother factors to re-rank the top 500 parses from thesame parser and English grammar we use in this re-search, and achieve some improvement in the rank-ing where tagger accuracy is sufficiently high.
Weuse a similar method, one level removed, in that weuse the tag sequences to select the ?gold?
parse(s)that are then used to train a model, as in the previoussections.5.1 Gold SupertagsIn order to test the viability of this method, we firstexperimented using gold standard tags, extractedfrom the gold standard parses.
Supertags come inmany forms, depending on both the grammar for-malism and the implementation.
For this work, weuse HPSG lexical types (lextypes), the native wordclasses in the grammars.
These lextypes encode partof speech and subcategorisation information, as wellas some more idiosyncratic features of words, suchas restrictions on preposition forms, mass/count dis-tinctions and comparative versus superlative formsof adjectives.
As a few examples from the En-glish grammar, v np le represents a basic transi-tive verb, while n pp c-of le represents a countnoun that optionally takes a prepositional phrasecomplement headed by of.
The full definition of alextype consists of a many-featured AVM (attributevalue matrix), but the type names have been de-liberately chosen to represent the main features ofeach type.
In the Dridan (2009) work, parse rankingshowed some improvement when morphological in-formation was added to the tags.
Hence, we alsolook at more fine-grained tags constructed by con-catenating appropriate morphological rules onto thelextypes, as in v np le:past verb orule (ie asimple transitive verb with past tense).We used these tags by extracting the tag sequenceTest Set Exact Match F-scorelextype +morph lextype +morphtc-006 40.49 41.37 0.903 0.903jhpstgt 32.93 32.93 0.862 0.858catb 20.41 19.85 0.798 0.794Table 6: Accuracy using gold tag sequence compatibilityto select the ?gold?
parse(s).from the leaf types of all the parses in the forest,marking as ?gold?
any parse that had the same se-quence as the gold standard parse and then trainingthe models as before.
Table 6 shows the results fromparsing with models based on both the basic lextypeand the lextype with morphology.
The results arepromising.
They still fall well below training purelyon gold standard data (at least for the in-domainsets), since the tag sequences are not fully discrimi-natory and hence noise can creep in, but accuracy issignificantly better than the heuristic methods triedearlier.
This suggested that, at least with a reason-ably accurate tagger, this was a viable strategy fortraining a model.
With no significant difference be-tween the basic and +morph versions of the tag set,we decided to use the basic lextypes as tags, sincea smaller tag set should be easier to tag with.
How-ever, we first had to train a tagger, without using anygold standard data.5.2 Unsupervised SupertaggingResearch into unsupervised part-of-speech taggingwith a tag dictionary (sometimes called weakly su-pervised POS tagging) has been going on for manyyears (cf Merialdo (1994), Brill (1995)), but gener-ally using a fairly small tag set.
The only work weknow of on unsupervised tagging for the more com-plex supertags is from Baldridge (2008), and morerecently, Ravi et al (2010a).
In this work, the con-straining nature of the (CCG) grammar is used tomitigate the problem of having a much more am-biguous tag set.
Our method has a similar under-lying idea, but the implementation differs both inthe way we extract the word-to-tag mappings, andalso how we extract and use the information fromthe grammar to initialise the tagger model.We chose to use a simple first-order HiddenMarkov Model (HMM) tagger, using the implemen-699tation of Dekang Lin,6 which re-estimates probabil-ities, given an initial model, using the Baum-Welchvariant of the Expectation-Maximisation (EM) algo-rithm.
One possibility for an initial model was to ex-tract the word-to-lextype mappings from the gram-mar lexicon as Baldridge does, and make all startingprobabilities uniform.
However, our lexicon mapsbetween lextypes and lemmas, rather than inflectedword forms, which is what we?d be tagging.
Thatis to say, from the lexicon we could learn that thelemma walk can be tagged as v pp* dir le, butwe could not directly extract the fact that thereforewalked should also receive that tag.7 For this rea-son, we decided it would be simplest to initialiseour probability estimates using the output of theparser, feeding in only those tag sequences whichare compatible with analyses in the parse forest forthat item.
This method takes advantage of the factthat, because the grammars are heavily constrained,the parse forest only contains viable tag sequences.Since parsing without a model is slow, we restrictedthe training set to those sentences shorter than aspecific word length (12 for English and 15 forJapanese, since that was the less ambiguous gram-mar and hence faster).Table 7 shows how much parsed data this gave us.From this parsed data we extracted tag-to-word andtag-to-tag frequency counts from all parses for allsentences, and used these frequencies to produce theemission and transition probabilities, respectively.The emission probabilities were taken directly fromthe normalised frequency counts, but for the tran-sition probabilities we allow for all possible transi-tions, and add one to all counts before normalising.This model we call our initial counts model.
TheEM trained model is then produced by starting withthis initial model and running the Baum-Welch al-gorithm using raw text sentences from the trainingcorpus.5.3 Supertagging-based parse selection modelsWe use both the initial counts and EM trainedmodels to tag the training data from Table 1 andthen compared this with the extracted tag sequences6Available from http://webdocs.cs.ualberta.ca/?lindek/hmm.htm7Morphological processing occurs before lexicon lookup inthe PET parser.Japanese EnglishParsed Sentences 9760 3770Average Length 9.63 6.36Average Parses 80.77 96.29Raw Sentences 13500 9410Raw Total Words 146053 151906Table 7: Training data for the HMM tagger (both theparsed data from which the initial probabilities were de-rived, and the raw data which was used to estimated theEM trained models).Test SetExact Match F-scoreInitial EM Initial EMcounts trained counts trainedtc-006 32.85 40.38 0.888 0.898jhpstgt 26.29 24.04 0.831 0.827catb 14.61 14.42 0.782 0.783Table 8: Accuracy using tag sequences from a HMM tag-ger to select the ?correct?
parse(s).
The initial countsmodel was based on using counts from a parse forestto approximate the emission and transition probabilities.The EM trained model used the BaumWelch algorithm toestimate the probabilities, starting from the initial countsstate.used in the gold tag experiment.
Since we couldno longer assume that our tag sequence would bepresent within the extracted tag sequences, we usedthe percentage of tokens from a parse whose lextypematched our tagged sequence as the parse score.Again, we marked as ?gold?
any parse that had thebest parse score for each sentence, and trained a newparse selection model.Table 8 shows the results of parsing with thesemodels.
The results are impressive, significantlyhigher than all our previous unsupervised methods.Interestingly, we note that there is no significantdifference between the initial count and EM trainedmodels for the English data.
To explore why thismight be so, we looked at the tagger accuracy forboth models over the respective training data sets,shown in Table 9.
The results are not conclusive.
Forboth languages, the EM trained model is less accu-rate, though not significantly so for Japanese.
How-ever, this insignificant tagger accuracy decrease forJapanese produced a significant increase in parseraccuracy, while a more pronounced tagger accuracydecrease had no significant effect on parser accuracyin English.700Language Initial counts EM trainedJapanese 84.4 83.3English 71.7 64.6Table 9: Tagger accuracy over the training data, usingboth the initial counts and the EM trained models.There is much potential for further work in thisdirection, experimenting with more training data ormore estimation iterations, or even looking at dif-ferent estimators as suggested in Johnson (2007)and Ravi et al (2010b).
There is also the issue ofwhether tag accuracy is the best measure for indicat-ing potential parse accuracy.
The Japanese parsingresults are already equivalent to those achieved us-ing gold standard tags.
It is possible that parsing ac-curacy is reasonably insensitive to tagger accuracy,but it is also possible that there is a better metric tolook at, such as tag accuracy over frequently con-fused tags.6 DiscussionThe results of Table 8 show that, using no humanannotated data, we can get exact match results thatare almost half way between our random baselineand our gold-standard-trained upperbound.
EDM F-scores of 90% and 83% over in-domain data com-pare well with dependency-based scores from otherparsers, although a direct comparison is very diffi-cult to do (Clark and Curran, 2007a; Miyao et al,2007).
It still remains to see whether this level of ac-curacy is good enough to be useful.
The main aim ofthis work is to bootstrap the treebanking process fornew grammars, but to conclusively show the efficacyof our methods in that situation requires a long-termexperiment that we are now starting, based on theresults we have here.
Another possible use for thesemethods was alluded to in Section 2: producing anew model for a new domain.Results at every stage have been much worse forthe catb data set, compared to the other jhpstgt En-glish data set.
While sentence length plays somepart, the major reason for this discrepancy was do-main mismatch between the training and test data.One method that has been successfully used for do-main adaption in parsing is self-training (McCloskyet al, 2006).
In this process, data from the new do-main is parsed with the parser trained on the old do-Source of ?Gold?
Data Exact Match F-scoreRandom Selection 8.30 0.671Supertags (initial counts) 14.61 0.782Gold Standard 22.29 0.839Self-training 15.92 0.791Table 10: Accuracy results over the out-of-domain catbdata set, using the initial counts unsupervised model toproduce in-domain training data in a self-training set up.The previous results are shown for easy comparison.main, and then the top analyses of the parsed newdomain data are added to the training data, and theparser is re-trained.
This is generally considered asemi-supervised method, since the original parseris trained on gold standard data.
In our case, wewanted to test whether parsing data from the new do-main using our unsupervised parse selection modelwas accurate enough to still get an improvement us-ing self-training for domain adaptation.It is not immediately clear what one might con-sider to be the ?domain?
of the catb test set, since do-main is generally very vaguely defined.
In this case,there was a limited amount of text available fromother essays by the same author.8 While the topicsof these essays vary, they all relate to the social sideof technical communities, and so we used this to rep-resent in-domain data for the catb test set.
It is, how-ever, a fairly small amount of data for self-training,being only around 1000 sentences.
We added the re-sults of parsing this data to the training set we usedto create the initial counts model and again retrainedand parsed.
Table 10 shows the results.
Previous re-sults for the catb data set are given for comparison.The results show that the completely unsuper-vised parse selection method produces a top parsethat is at least accurate enough to be used in self-training, providing a cheap means of domain adapta-tion.
In future work, we hope to explore this avenueof research further.7 Conclusions and Further WorkComparing Tables 8 and 4, we can see that for bothEnglish and Japanese, we are able to achieve parseselection accuracy well above our baseline of a ran-8http://www.catb.org/esr/writings/homesteading/701dom selection-based model using only the informa-tion available in the grammar and raw text.
Thiswas in part because it is possible to extract a rea-sonable tagging model from uncorrected parse data,due to the constrained nature of these grammars.These models will hopefully allow grammar engi-neers to more easily build statistical models for newlanguages, using nothing more than their new gram-mar and raw text.Since fully evaluating the potential for buildingmodels for new languages is a long-term ongoingexperiment, we looked at a more short-term eval-uation of our unsupervised parse selection meth-ods: building models for new domains.
A pre-liminary self-training experiment, using our initialcounts tagger trained model as the starting point,showed promising results for domain adaptation.There are plenty of directions for further workarising from these results.
The issues surroundingwhat makes a good tagger for this purpose, and howcan we best learn one without gold training data,would be one possibly fruitful avenue for furtherexploration.
Another interesting slant would be toinvestigate domain effects of the tagger.
Previouswork has already found that training just a lexicalmodel on a new domain can improve parsing results.Since the optimal tagger ?training?
we saw here (forEnglish) was merely to read off frequency counts forparsed data, it would be easy to retrain the tagger ondifferent domains.
Alternatively, it would be inter-esting so see how much difference it makes to trainthe tagger on one set of data, and use that to tag amodel training set from a different domain.
Othermethods of incorporating the tagger output couldalso be investigated.
Finally, a user study involv-ing a grammar engineer working on a new languagewould be useful to validate the results we found hereand confirm whether they are indeed helpful in boot-strapping a new grammar.AcknowledgementsThis research was supported by Australian ResearchCouncil grant no.
DP0988242 and Microsoft Re-search Asia.ReferencesJason Baldridge.
2008.
Weakly supervised supertaggingwith grammar-informed initialization.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics (Coling 2008), pages 57?64, Manch-ester, UK.Srinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: an approach to almost parsing.
Compu-tational Linguistics, 25(2):237?265.Emily M. Bender, Dan Flickinger, and Stephan Oepen.2002.
The grammar matrix: An open-source starter-kit for the rapid development of cross-linguisticallyconsistent broad-coverage precision grammars.
InProceedings of the Workshop on Grammar Engineer-ing and Evaluation at the 19th International Con-ference on Computational Linguistics, pages 8?14,Taipei, Taiwan.Emily M. Bender.
2008.
Evaluating a crosslinguisticgrammar resource: A case study of Wambaya.
In Pro-ceedings of the 46th Annual Meeting of the ACL, pages977?985, Columbus, USA.Philip Blunsom.
2007.
Structured Classification forMultilingual Natural Language Processing.
Ph.D.thesis, Department of Computer Science and SoftwareEngineering, the University of Melbourne.Eric Brill.
1995.
Unsupervised learning of disambigua-tion rules for part of speech tagging.
In Proceedingsof the Third Workshop on Very Large Corpora, pages1?13, Cambridge, USA.Ted Briscoe and John Carroll.
2006.
Evaluating theaccuracy of an unlexicalised statistical parser on thePARC DepBank.
In Proceedings of the 44th AnnualMeeting of the ACL and the 21st International Confer-ence on Computational Linguistics, pages 41?48, Syd-ney, Australia.David Carter.
1997.
The treebanker: a tool for super-vised training of parsed corpora.
In Proceedings of aWorkshop on Computational Environments for Gram-mar Development and Linguistic Engineering, pages9?15, Madrid, Spain.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theACL, pages 173?180, Ann Arbor, USA.Stephen Clark and James R. Curran.
2006.
Partial train-ing for a lexicalized-grammar parser.
In Proceedingsof the Human Language Technology Conference of theNorth American Chapter of the ACL (NAACL), pages144?151, New York City, USA.Stephen Clark and James R. Curran.
2007a.
Formalism-independent parser evaluation with CCG and Dep-Bank.
In Proceedings of the 45th Annual Meeting ofthe ACL, pages 248?255, Prague, Czech Republic.702Stephen Clark and James R. Curran.
2007b.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.Ann Copestake, Dan Flickinger, Ivan A.
Sag, and CarlPollard.
2005.
Minimal recursion semantics: An in-troduction.
Research on Language and Computation,vol 3(no 4):pp 281?332.Mary Dalrymple.
2006.
How much can part-of-speechtagging help parsing?
Natural Language Engineering,12(4):373?389.Rebecca Dridan.
2009.
Using lexical statistics to im-prove HPSG parsing.
Ph.D. thesis, Saarland Univer-sity.Dan Flickinger.
2002.
On building a more efficientgrammar by exploiting types.
In Stephan Oepen, DanFlickinger, Jun?ichi Tsujii, and Hans Uszkoreit, edi-tors, Collaborative Language Engineering, pages 1?17.
Stanford: CSLI Publications.Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.2007.
Evaluating impact of re-training a lexical dis-ambiguation model on domain adaptation of an HPSGparser.
In Proceedings of the 10th International Con-ference on Parsing Technology (IWPT 2007), pages11?22, Prague, Czech Republic.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Compu-tational Linguistics, 33(3):355?396, September.Mark Johnson.
2007.
Why doesnt EM find good HMMPOS-taggers?
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 296?305,Prague, Czech Republic.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
In Pro-ceedings of the 6th Conference on Natural LanguageLearning, Taipei, Taiwan.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conferenceof the NAACL, pages 152?159, New York City, USA.Bernard Merialdo.
1994.
Tagging english text witha probabilistic model.
Computational Linguistics,20(2):155?171.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature for-est models for probabilistic HPSG parsing.
Computa-tional Linguistics, 34(1):35?80.Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii.
2007.Towards framework-independent evaluation of deeplinguistic parsers.
In Proceedings of the GEAF 2007Workshop, Palo Alto, California.Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2008.
Task-oriented eval-uation of syntactic parsers and their representations.
InProceedings of the 46th Annual Meeting of the ACL,pages 46?54, Columbus, USA.Stephan Oepen and John Carroll.
2000.
Ambiguity pack-ing in constraint-based parsing - practical results.
InProceedings of the 1st Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 162?169, Seattle, USA.Stephan Oepen, Dan Flickinger, Kristina Toutanova, andChristopher D. Manning.
2004.
LinGO redwoods.
arich and dynamic treebank for HPSG.
Journal of Re-search in Language and Computation, 2(4):575?596.Stephan Oepen.
2001.
[incr tsdb()] ?
competence andperformance laboratory.
User manual, ComputationalLinguistics, Saarland University, Saarbru?cken, Ger-many.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press,Chicago, USA.Sujith Ravi, Jason Baldridge, and Kevin Knight.
2010a.Minimized models and grammar-informed initializa-tion for supertagging with highly ambiguous lexicons.In Proceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 495?503,Uppsala, Sweden.Sujith Ravi, Ashish Vaswani, Kevin Knight, and DavidChiang.
2010b.
Fast, greedy model minimization forunsupervised tagging.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(Coling 2010), pages 940?948, Beijing, China.Laura Rimell and Stephen Clark.
2008.
Adaptinga lexicalized-grammar parser to contrasting domains.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2008), pages 475?484, Honolulu, USA.Yves Schabes and Aravind K. Joshi.
1991.
Parsing withlexicalized tree adjoining grammar.
In Masaru Tomita,editor, Current Issues in Parsing Technology, chap-ter 3, pages 25?48.
Kluwer.Melanie Siegel and Emily M. Bender.
2002.
Efficientdeep processing of japanese.
In Proceedings of the 3rdWorkshop on Asian Language Resources and Interna-tional Standardization.
Coling 2002 Post-ConferenceWorkshop., Taipei, Taiwan.Yasuhito Tanaka.
2001.
Compilation of a multilingualparallel corpus.
In Proceedings of PACLING 2001,pages 265?268, Kitakyushu, Japan.Kristina Toutanova, Chistopher D. Manning, Stuart M.Shieber, Dan Flickinger, and Stephan Oepen.
2002.Parse disambiguation for a rich HPSG grammar.
InFirst Workshop on Treebanks and Linguistic Theories(TLT2002), pages 253?263.703Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Proceed-ings of the 18th International Conference on Compu-tational Linguistics (COLING 2000), pages 947?953,Saarbrcken, Germany.Yi Zhang, Stephan Oepen, and John Carroll.
2007.
Ef-ficiency in unification-based n-best parsing.
In Pro-ceedings of the 10th international conference on pars-ing technologies (IWPT 2007), pages 48?59, Prague,Czech Republic.704
