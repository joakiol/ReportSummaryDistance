Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 22?27,Beijing, China, July 28, 2015.c?2015 Association for Computational LinguisticsTransition-based Dependency DAG Parsing Using Dynamic OraclesAlper Tokg?ozIstanbul Technical UniversityDepartment of Computer EngineeringIstanbul, Turkeytokgoza@itu.edu.trG?uls?en Eryi?gitIstanbul Technical UniversityDepartment of Computer EngineeringIstanbul, Turkeygulsen.cebiroglu@itu.edu.trAbstractIn most of the dependency parsing stud-ies, dependency relations within a sen-tence are often presented as a tree struc-ture.
Whilst the tree structure is suf-ficient to represent the surface relations,deep dependencies which may result tomulti-headed relations require more gen-eral dependency structures, namely Di-rected Acyclic Graphs (DAGs).
Thisstudy proposes a new dependency DAGparsing approach which uses a dynamicoracle within a shift-reduce transition-based parsing framework.
Although thereis still room for improvement on per-formance with more feature engineer-ing, we already obtain competitive perfor-mances compared to static oracles as a re-sult of our initial experiments conductedon the ITU-METU-Sabanc?
Turkish Tree-bank (IMST).1 IntroductionSyntactic parsing is the process of determiningthe grammatical structure of a sentence as con-forming to the grammatical rules of the relevantnatural language.
The structure of the sentenceis determined according to the grammar formal-ism that the parser is built upon.
Phrase struc-ture parsers, also known as constituency parsers,parse a sentence by splitting it into its smallerconstituents.
On the other hand, in dependencyparsers, the structure of the sentence is representedas dependency trees consisting of directed depen-dency links between a dependent and a head word.Data-driven dependency parsing frameworkshave gained increasing popularity in recent yearsand been used in a wide range of applications suchas machine translation (Ding and Palmer, 2005),textual entailment (Yuret et al., 2013) and questionanswering (Xu et al., 2014).
Most data-driven de-pendency parsers achieve state-of-the art parsingperformances with a language agnostic approachon the different syntactic structures of differentlanguages (Buchholz and Marsi, 2006).
Mod-ern data-driven dependency parsers can be catego-rized into two groups: graph-based and transition-based parsers.
Graph-based parsers rely on theglobal optimization of models aiming to find span-ning trees over dependency graphs.
On the otherhand, transition-based parsers work basically withgreedy local decisions that are deterministicallyselected by oracles, which are generic machinelearning models trained to make decisions aboutthe next transition action.
In a recent study,Zhang and Nivre (2012) propose a new approachon transition-based parsing that aims to provideglobal learning instead of greedy local decisions.Despite the high performances of both graph-based and transition-based dependency parsers,these are generally bounded by the constraint thateach dependent may not have multiple heads.Therefore, the resulting parsing output is a treewhere words correspond to nodes and dependencyrelations correspond to edges.
Although depen-dency trees yield satisfactory performances, theyare inadequate in capturing dependencies at dif-ferent levels of semantic interpretations or morecomplicated linguistic phenomena (e.g.
relativeclauses, anaphoric references) which could resultin multi-head dependencies together with exist-ing surface dependency relations.
An example isgiven in Figure 1 which is taken from the Turk-ish IMST Treebank (Sulubacak and Eryi?git, 2015).In Figure 1, the dependent token ?Umut?
depends22on more than one head token with SUBJECT re-lations: 1) the verb ?kos?mak?
(to run) and 2) theverb ?d?us?mek?
(to fall).
Adding a second rela-tion (emphasized with a dash-dotted line in the fig-ure) to the token ?Umut?
breaks the condition thateach token may have at most one head, and ren-ders existing dependency tree parsers incompati-ble for this setup.
It is also worth mentioning thatthe deep dependencies in the IMST are not dis-criminated from surface dependencies by the useof different labels.Umutkos?ar+kend?us?t?u?Umut??
[he] runs?(WHILE)?
[he] fell?
?Umut fell as [he was] running.
?SUBJECTSUBJECT DERIV MODIFIER PREDICATEFigure 1: Example for Turkish multi-head depen-dencies.In this paper, for the first time in the litera-ture, we investigate the impact of using dynamicoracles for parsing multi-head dependency struc-tures by extending the approach of Goldberg andNivre (2012).
We provide comparisons with thereplication of the basic shift-reduce DAG pars-ing algorithm of Sagae and Tsujii (2008) and afirst time implementation of their proposed arc-eager parsing algorithm.
The remainder of the pa-per first gives a background information about thetopic, then introduces the DAG parsing frameworkand the proposed algorithms together with experi-ments and results.2 BackgroundAlthough it is possible to discover the syntactic re-lations with a two stage model by first finding theregular surface dependencies and then finding thedeep relations with post-processing as in Nivre etal.
(2010), it is not always straightforward to de-cide which dependencies should be treated as sur-face relations or deep relations as in the case ofTurkish.
Thus, in this study, we focus on singlestage models and aim to discover the entire set ofrelations in a single pass.
McDonald and Pereira(2006) use graph-based algorithms for DAG pars-ing simply using approximate interference in anedge-factored dependency model starting from de-pendency trees.
On the other hand, Sagae andTsujii (2008) propose a transition-based counter-part for DAG parsing which made available forparsing multi-headed relations.
They modified theexisting shift-reduce bottom-up dependency pars-ing algorithm of Nivre and Nilsson (2005) to al-low multiple heads per token by the use of cycleremoval and pseudo-projectivization as a prepro-cessing stage.
They report higher performancescores on the Danish treebank compared to Mc-Donald and Pereira (2006).A standard way of determining transition ac-tions in a shift-reduce dependency parser is us-ing static oracles.
During the training stage, thelearning instances for the oracle are prepared bythe use of manually annotated gold-standard parsetrees and the current parsing configuration.
Dur-ing the parsing stage, the already trained oracledecides on the next transition operation.
One ofthe problems with static oracles lays beneath thespurious ambiguity, which implies there might bemore than one transition sequence for a given sen-tence and the sequence proposed by an oracle maynot be the easiest to learn.
The second problem oc-curs when the parser makes a parsing error whichleads to a parser configuration from which the cor-rect parse tree is not derivable.
The algorithm doesnot provide any solutions for dealing with the er-ror propagation caused by such situations.
Theidea of dynamic oracles introduced by Goldbergand Nivre (2012) rises for handling the aforemen-tioned handicaps of static oracles.
Rather than re-turning a unique transition for a given configura-tion, a dynamic oracle returns a set of valid tran-sitions regarding the current configuration, whichwould allow the algorithm to explore non-optimalconfigurations during the training procedure.3 Transition-Based Solutions forDependency DAG ParsingTransition-based parsing frameworks consider thetransition system to be an abstract machine thatprocesses input sentences and produces corre-sponding parsing graphs.
The tokens of the in-put sequence and the partially created dependencystructures are kept within the following data struc-tures:1. a buffer ?
which includes the remaining un-processed tokens in the input sequence in aqueue,232.
a stack ?
which consists of the tokens beingprocessed,3.
a set A of assigned dependency arcs.The transition actions explained in the follow-ing subsections are basic stack and queue opera-tions that correspond to parsing actions markingdependency relations.
The algorithm starts with abuffer ?
initialized with all tokens of a sentencepreserving their sequence, and an empty stack ?.The parsing process finishes when there are nonodes left in ?
and only the artificial root in ?.3.1 Basic shift-reduce parsing with multipleheadsThe first algorithm that is capable of parsing DAGstructures is the standard algorithm of Sagae andTsujii (2008).
The complete list of the transitionsof this algorithm is as follows:?
Shift: Pops the first item of the buffer andpushes it onto the top of the stack.?
Left-Reduce: Pops the top two items of thestack, creates a left arc between them wherethe top item is assigned as the head of theitem below, and pushes the head token backonto the stack.?
Right-Reduce: Pops the top two items ofthe stack, creates a right arc between them,where the item below is assigned as the headof the top item, and pushes the head tokenback onto the stack.?
Left-Attach: Creates a left arc between thetop two items of the stack, where the top itemis assigned as the head of the one below.
Thestack and the buffer remain unchanged.?
Right-Attach: Creates a right dependency arcbetween the two top items on the stack andassigns the top token as the dependent of thetoken below.
As the second step, it pops thetop of the stack and places it into the buffer?.3.2 Multi-Head Arc-Eager ParsingAlgorithmThe second transition algorithm introduced but notimplemented by Sagae and Tsujii (2008) is a vari-ation of the Arc-Eager algorithm of Nivre et al.
(2007) and has the following transition operations:?
Shift: Pops the first item of the buffer andpushes it onto the top token of the stack.?
Left-Arc: Creates a left dependency arc be-tween the top token of the stack and the firsttoken of the input buffer, where the first tokenin the buffer becomes the head and the one atthe top of the stack becomes the dependent.It is also worth noticing that the stack and theinput buffer remains unchanged.?
Right-Arc: Creates a right dependency arcbetween the top token of the stack and thefirst token on the input buffer, where the to-ken in the stack becomes the head, and thetoken which is in front of the buffer becomesthe dependent.
It is also worth noticing thatthe stack and the input buffer remains un-changed.?
Reduce: Pops the top item of the stack if andonly if it was previously associated with atleast one head.3.3 Multi-Head Arc Eager Parsing with aDynamic OracleIn order to design a dynamic oracle with the ca-pability of parsing multi-head dependencies, weneed an efficient method for computing the costof each transition.
To this end, we extend thedynamic oracle defined by Goldberg and Nivre(2012), considering DAG parsing arc-eager sys-tem of Sagae and Tsujii (2008).
Extended arc-eager transition system will operate in the sameway as previously defined in Section 3.2, within adynamic oracle system whose cost function is de-fined with a transition operation, the current con-figuration c = (?|s, b|?,A)1and the gold parseof the sentence (Ggold).
Differing from Goldbergand Nivre (2012), for ease of computation, we pre-fer marking transitions as zero cost or costly in-stead of computing the exact cost:?
Cost(LeftAttach, c,Ggold) Attaching s to bwith a left arc is costly, if there is a right arcbetween s and b, or it is already attached witha left arc.?
Cost(RightAttach, c,Ggold) Attaching s tob by creating right arc is costly, if there is aleft arc between s and b, or it is already at-tached with a right arc.1In c = (?|s, b|?,A), s denotes the top token of the stack?, b denotes first item of buffer ?, A denotes revealed arcs24?
Cost(Reduce, c,Ggold) Popping s from thestack means it will be no longer possible toassociate it with any head or dependent frombuffer ?, therefore it is costly if it has headsor dependents in the ?.?
Cost(Shift, c,Ggold) Pushing b onto thestack means it will be no longer possible toassociate it with any heads or dependents instack ?, therefore it is costly if it has a heador dependent token in the ?.Since left attach and right attach operations donot change the parser configuration (i.e.
these op-erations cannot lead to a parser configuration fromwhich the gold tree is not reachable), their cost ismeasured according to the validity of the attach-ment.
The only difference of our multi-head vari-ant from the single head arc-eager transition sys-tem is that the left and right arc operations do notchange the parser state.
As such, it is essentially arelaxed version of the single-head system.
There-fore, since the arc-decomposition property holdsfor the single-head system (as proven by Goldbergand Nivre (2013)), it also holds for our variant.We use the same online training procedure (withthe perceptron algorithm) as Goldberg and Nivre(2012) given in Algorithm 1.Algorithm 1 Online training with dynamic oracle1: procedure TRAIN2: w ?
03: for I ?
1, ITERATIONS do4: c?
cs(x)5: for sentence x do6: while c is not terminal do7: tp?
argmaxtw.?
(c, t)8: ZC ?
{t|o(t; c;Ggold) = true}9: to?
argmaxtZCw.?
(c, t)10: if tp6?
ZC then11: w ?
w + ?
(c, to) ??
(c, tp)12: tn?NEXT(I, tp, ZC)13: c?
tn(c)14: procedure NEXT(I, t, ZC)15: if t ?
ZC then16: return t17: else18: RANDOM ELEMENT (ZC)The proposed oracle will return a set of zerocost transition operations (denoted as ZC at line8) where the costs are calculated according to thecost function defined above.
Feature weights willbe updated only if the perceptron model makes atransition prediction that does not belong to thezero cost transitions (lines 10 and 11).
After that,the next transition operation is chosen by the func-tion NEXT, which returns the transition that is pre-dicted by the model if it belongs to zero cost tran-sitions; if not, it returns a random transition whichbelongs to the zero cost transition set.4 ExperimentsIn order to apply the specified DAG parsing al-gorithm to non-projective sentences, a pseudo-projective transformation operation is applied tothe IMST.
For that aim, we apply Head scheme2described by Nivre (2005).
Moreover, before theapplication of this pseudo-projective transforma-tion, the cyclic dependency paths are handled asdescribed by Sagae and Tsujii (2008), by reversingthe shortest arc within the cyclic dependency pathuntil no cyclic path remains.
99.3% precision and99.2% recall are acquired on IMST by applyingthe pseudo-projective transformation and detrans-formation operations.
As a learning component,we follow the work of Sagae and Tsujii (2008)and use a Maximum Entropy model for the clas-sification with the greedy search algorithm.
Forthe dynamic oracle experiment, we use an aver-aged perceptron algorithm iterating 15 times overthe training data.The following features are used in all of the ex-periments:?
The POS tag, lemma and morphological fea-tures of the top 3 tokens on the stack and thefirst 3 tokens in the buffer.?
The POS tag and dependency relations of therightmost and leftmost modifiers of the top 2items on the stack.?
The number of heads and dependents of thetop item of the stack and the first item of thebuffer.?
The dependency relations of the top of thestack.2Although other schemes could be tried for DAGs for bet-ter performance, this is left for future work due to time con-straints.25?
Whether the top 2 tokens on the stack have adependency relation between them or not.?
Whether the top token of the stack and thefirst of the buffer have a dependency relationbetween them or not, and if so the directionand the type of the relation.?
Combination of the surface form of the toptoken of the stack and its number of left andright modifiers.?
Combination of the surface form of the firsttoken of the buffer and its number of left andright modifiers.?
The surface forms and POS tags of heads ofthe top token of the stack and the first tokenof the buffer.?
The previous two parsing actions.For training and evaluation purposes, we use theIMST with ten-fold cross validation.
Experimentresults are given in Table 4.Table 1: Unlabeled scores of experiments withusing IMST.Experiment Precision Recall F1Static-Standard 79.42 77.56 78.50Static-Eager 78.90 76.79 77.83Dynamic-Eager 79.68 81.17 80.42As shown in Table 4, the static arc-eager DAGimplementation for Turkish performs slightlyworse than the arc-standard algorithm.
This is notsurprising in the light of previous studies (Nivre,2008; Eryi?git et al., 2008) reporting that the arcstandard algorithm performs better in tree parsingdue to the smaller number of classes to be learnedby the oracle.
However, the proposed multi-headarc-eager algorithm with dynamic oracle (referredto as Dynamic-Eager) yields the best precision, re-call and F1 scores among the three experiments.3In this study, although there is still room forimprovement on performance with more featureengineering, we obtain better results on TurkishIMST treebank between static and dynamic ora-cles with our newly proposed method for parsing3The difference of this model from the runner-up modelsare found to be statistically significant according to McNe-mar?s test (p < 0.0001)DAGs.
This encourages us to test our system withdifferent languages as future work with the expec-tation that the ameliorations will be much higherthan the reported ones in the single-head scenario.5 Conclusion and Future WorksIn this paper, we experimented with three differ-ent transition-based algorithms for DAG parsingwhich eliminate the single-head constraint of tra-ditional algorithms and allows multi-head depen-dency relations to represent more complicated lin-guistic phenomena along with surface relations.We present the first results for arc-eager DAGparsing with static oracles and propose a new arc-eager DAG parsing algorithm using dynamic ora-cles.
Our initial experiments conducted on Turkishpave the way for future research on the usage ofthe dynamic arc-eager DAG parsing for other lan-guages.
For future work, we will first conduct ex-periments on how well the Dynamic-Eager algo-rithm performs on different treebanks, includingmulti-head dependencies (such as the Danish tree-bank (Kromann, 2003)).
Secondly, we will con-duct experiments on previously described static-oracle parsing algorithms by using different clas-sifiers such as Support Vector Machines.AcknowledgmentsWe hereby acknowledge that this study is part ofa research project named ?Parsing Web 2.0 Sen-tences?
that is supported by T?UB?ITAK (TurkishScientific and Technological Research Council)1001 program (grant number 112E276) and partof the ICT COST Action IC 1207.ReferencesSabine Buchholz and Erwin Marsi.
2006.
Conll-xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning, pages 149?164.Association for Computational Linguistics.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 541?548.
Association for Computa-tional Linguistics.G?uls?en Eryi?git, Joakim Nivre, and Kemal Oflazer.2008.
Dependency parsing of Turkish.
Computa-tional Linguistics, 34(3):357?389.26Yoav Goldberg and Joakim Nivre.
2012.
A dynamicoracle for arc-eager dependency parsing.
In COL-ING, pages 959?976.Yoav Goldberg and Joakim Nivre.
2013.
Trainingdeterministic parsers with non-deterministic oracles.Transactions of the association for ComputationalLinguistics, 1:403?414.Matthias T Kromann.
2003.
The danish dependencytreebank and the underlying linguistic theory.
InProc.
of the SecondWorkshop on Treebanks and Lin-guistic Theories (TLT).Ryan T McDonald and Fernando CN Pereira.
2006.Online learning of approximate dependency parsingalgorithms.
In EACL.
Citeseer.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics, pages 99?106.
Association forComputational Linguistics.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(02):95?135.Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-los Gomez-Rodriguez.
2010.
Evaluation of depen-dency parsers on unbounded dependencies.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics, pages 833?841.
Associ-ation for Computational Linguistics.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34(4):513?553.Kenji Sagae and Jun?ichi Tsujii.
2008.
Shift-reducedependency DAG parsing.
In Proceedings of the22nd International Conference on ComputationalLinguistics-Volume 1, pages 753?760.
Associationfor Computational Linguistics.Umut Sulubacak and G?uls?en Eryi?git.
2015.
A rede-fined Turkish dependency grammar and its imple-mentations: A new Turkish web treebank & the re-vised Turkish treebank.
under review.Kun Xu, Sheng Zhang, Yansong Feng, and DongyanZhao.
2014.
Answering natural language questionsvia phrasal semantic parsing.
In Natural LanguageProcessing and Chinese Computing, pages 333?344.Springer.Deniz Yuret, Laura Rimell, and Ayd?n Han.
2013.Parser evaluation using textual entailments.
Lan-guage resources and evaluation, 47(3):639?659.Yue Zhang and Joakim Nivre.
2012.
Analyzingthe effect of global learning and beam-search ontransition-based dependency parsing.
In COLING(Posters), pages 1391?1400.27
