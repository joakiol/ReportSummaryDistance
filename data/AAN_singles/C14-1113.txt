Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1197?1207, Dublin, Ireland, August 23-29 2014.Query-focused Multi-Document Summarization: Combining a TopicModel with Graph-based Semi-supervised LearningYanran Li and Sujian Li?Key Laboratory of Computational Linguistics,Peking University, MOE, China{liyanran,lisujian}@pku.edu.cnAbstractGraph-based learning algorithms have been shown to be an effective approach for query-focusedmulti-document summarization (MDS).
In this paper, we extend the standard graph ranking algo-rithm by proposing a two-layer (i.e.
sentence layer and topic layer) graph-based semi-supervisedlearning approach based on topic modeling techniques.
Experimental results on TAC datasetsshow that by considering topic information, we can effectively improve the summary perfor-mance.1 IntroductionQuery-focused multi-document summarization (MDS) can facilitate users to grasp the main idea of thedocuments according to the users?
concern.
In query-focused summarization, one query is firstly pro-posed at the beginning of the documents.
Then according to the given query and its influence on sen-tences, a ranking score is assigned to each of the sentences and higher ranked sentences are picked intoa summary.Among existing approaches, graph-based semi-supervised learning algorithms have been shown to bean effective way to impose a query?s influence on sentences (Zhou et al, 2003; Zhou et al, 2004; Wanet al, 2007).
Specifically, a weighted network is constructed where each sentence is modeled as a nodeand relationships between sentences are modeled as directed or undirected edges.
With the assumptionthat a query is the most important node, initially, a positive score is assigned to the query and zero to theremaining nodes.
All nodes then spread their ranking scores to their nearby neighbors via the weightednetwork.
This spreading process is repeated until a global stable state is achieved, and all nodes obtaintheir final ranking scores.The primary disadvantage of existing learning method is that sentences are ranked without consideringtopic level information.
As we know, a collection of related documents usually covers a few differenttopics.
For example, the specific event ?Quebec independence?
may involve the topics such as ?leaderin independence movement?, ?referendum?, ?related efforts in independence movement?
and so on.
Itis important to discover the latent topics when summarizing a document collection, because sentences inan important topic would be more important than those talking about trivial topics (Hardy et al, 2002;Harabagiu and Lacatusu, 2005; Otterbacher et al, 2005; Wan and Yang, 2008).The topic models (Blei et al, 2003) offer a good opportunity for the topic-level information modelingby offering clear and rigorous probabilistic interpretations over other existing clustering techniques.
Sofar, LDA has been widely used in summarization task by discovering topics latent in the documentcollections (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al, 2010; Mason andCharniak, 2011; Delort and Alfonseca, 2012).
However, as far as we know, how to combine topicinformation and semi-supervised learning into a unified framework has seldom been exploited.In this paper, inspired by the graph-based semi-supervised strategy and topic models, we proposea two-layer (i.e.
sentence layer and topic layer) graph-based semi-supervised learning approach for?correspondence authorThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http:// creativecommons.org/licenses/by/4.0/1197query-focused MDS.
By using two revised versions of LDA topic model (See Section 2), our approachnaturally models the relations between topics and sentences, and further use these relations to constructthe two-layer graph.
Experiments on the TAC datasets demonstrate that we can improve summarizationperformance under the framework of two-layer graph-based semi-supervised learning.The rest of this paper is organized as follows: Section 2 describes our LDA based topic models, W-LDA and S-LDA.
Section 3 presents the construction of the two-layer graph and the semi-supervisedlearning and the experimental results are provided in Section 4.
Then, Section 5 describes related workon query-focused multi-document summarization and topic modeling techniques and we conclude thispaper in Section 6.2 Topic Modeling2.1 Model DescriptionAs discussed in Section 1, a collection of documents often involves different topics related to a specificevent.
The basic idea of our summarization approach is to discover the latent topics and cluster sentencesaccording to the topics.
Inspired by (Chemudugunta et al, 2006) and (Li et al, 2011), we find 4 types ofwords in the text: (1) Stop words that occur frequently in the text.
(2) Background words that describethe general information about an event, such as ?Quebec?
and ?independence?.
(3) Aspect words talkingabout topics across the corpus.
(4) Document-specific words that are local to a single document and donot appear across different corpus.
Similar ideas can also be found in many LDA based summarizationtechniques (Haghighi and Vanderwende, 2009; Li et al, 2011; Delort and Alfonseca, 2012).Stop words can easily be filtered out by a standard list of stopwords.
We use a background worddistribution ?Bto model vocabularies commonly used in the document collection.
We assume that thereare K aspect topics shared across corpus and each topic is associated with a topic-word distribution?k, k ?
[1,K].
For each document m, there is a document-specific word distribution ?m, m ?
[K +1,K + M ].
Each word w is modeled as a mixture of background topics, document-specific topics oraspect topics.
We use a latent parameter ywto denote whether it is a background word, a document-specific word or an aspect word.
ywis sampled from a multinomial distribution with parameter pi.2.2 W-LDA and S-LDAWe describe two models: a word level model W-LDA and a sentence level S-LDA.
Their difference onlylies in whether the words within a sentence are generated from the same topic.W-LDA: Figure 1 and Figure 3 show the graphical model and generation process of W-LDA, which isbased on Chemudugunta et al?s work (2007).
Using the Gibbs sampling technique, in each iteration twolatent parameters ywand zware sampled simultaneously as follows:P (yw= 0) ?Nm0,?w+ ?Nm,?w+ 3?EwB+ ?
?w?Ew?B+ V ?
(1)P (yw= 1) ?Nm1,?w+ ?Nm,?w+ 3?Ewm+ ?
?w?Ew?m+ V ?
(2)P (yw= 2, zw= k) ?Nm2,?w+ ?Nm,?w+ 3?
?Ckm+ ?
?k?Ckm+K?Ewk+ ?
?w?Ew?k+ V ?
(3)where Nm0,?w, Nm1,?wand Nm2,?wdenote the number of words assigned to background, document-specific and aspect topic in current document.
Nm,?wdenotes the total number of words in currentdocument.
EwB, EwmandEwkare the number of times that wordw appears in background topic, document-specific topic and aspect topic k. Ckmdenotes the number of words assigned to topic k in current docu-ment.With one Gibbs sampling, we can make the following estimation:?wk=Ewk+ ?
?w?Ew?k+ V ?
(4)1198Then, the probability that a sentence s is generated from topic k is computed based on the probabilitythat each of its aspect words is generated from topic k:P (s|zs= k) =?w?s,yw=2?wk(5)Figure 1: Graphical model for W-LDAFigure 2: Graphical model for S-LDA1.
Draw background distribution ?B?
Dir(?)2.
For each document m:draw doc proportion vector ?m?
Dir(?
)draw doc proportion vector pim?
Dir(?
)draw doc specific distribution ?m?
Dir(?)3.
For each topic k:draw topic distribution ?k?
Dir(?)4.
For each word w in document m:(a) draw yw?
Multi(pim)(b) if yw= 0: draw w ?
?Bif yw= 1: draw w ?
?mif yw= 2:draw zw?
Multi(?m)w ?
Multi(?zw)Figure 3: Generation process for W-LDA1.
Draw background distribution ?B?
Dir(?)2.
For each document m:draw doc proportion vector ?m?
Dir(?
)draw doc proportion vector pim?
Dir(?
)draw doc specific distribution ?m?
Dir(?)3.
For each topic k:draw topic distribution ?k?
Dir(?)4.
For each sentence s in document m:4.1 draw zs?
Multi(?m)4.2 for each word in sentence s:(a) draw yw?
Multi(pim)(b) if yw= 0: draw w ?
?Bif yw= 1: draw w ?
?mif yw= 2: draw w ?
Multi(?zw)Figure 4: Generation process of S-LDAS-LDA: In S-LDA, each sentence is treated as a whole and words within a sentence are generatedfrom the same topic (Gruber et al., 2007).
Its graphical model and generated process are shown in Figure2 and Figure 4.
In S-LDA, we firstly sample the topic zsfor each sentence as follows:P (zs= k|z?s, y, w) ??
(?w?Ew?k+ V ?)?
(?w?Ew?k+NAs+ V ?)??w?s,yw=2?
(Ewk+Nws+ ?)?
(Ewk+ ?
)?Ckm+ ??k?Ck?m+K?
(6)Ckmdenotes the number of sentences in document m assigned to topic k. NAsdenotes the number ofaspect words in current sentence.
Then ywis sampled.In our experiments, we set hyperparameters ?
= 1, ?
= 0.5, ?
= 0.01.
We run 500 burn-in iterationsthrough all documents in the collection to stabilize the distribution of z and y before collecting samples.3 Graph-based Semi-supervised LearningAs stated before, the consideration of higher level information (i.e.
topics) would be helpful for sentenceranking in summarization.
In our two-layer graph, the upper layer is composed of topic nodes and thelower layer is composed of sentences nodes, among which there is one node representing the query.1199Formally, given a document set D, let G =< Vs, Vt, E > be the two-layer graph, where Vs={s1, s2, ..., sN} denotes the set of all the sentence nodes and s1is the query.
Vt= {z1, z2, ..., zK}corresponds to all the topic nodes.
The collection of edges E in the graph consists of the relationswithin layers and between layers.
And the edge weights are measured according to the similaritiesbetween nodes, which are computed based on the topic distribution from our two topic model extensions.Specifically, we introduce four edge weight matrices?WN?K,?WK?N, U and P to describe the sentence-to-topic relations, the topic-to-sentence relations, the sentence-to-sentence relations and the topic-to-topic relations respectively.Firstly, the row-normalized edge weight matrices?WN?Kand?WK?Ndenotes the similarity matrixbetween sentences and topics,?Wi,j=sim(si, zk)?k?sim(si, zk?
)?Wi,j=sim(si, zk)?jsim(sj, zk)(7)where sim(si, zk) = p(si|zsi= zk) is the probability that the sentence is generated from that topiccalculated in Equation (5).The edge weight matrix U describe the sentence-to-sentence relations.
In the same way, the simi-larity between two sentences is the cosine similarity between their topic distributions, sim(si, sj) =1C1?kp(si|zsi= k) ?
p(sj|zsj= zk), where C1=?
?kp2(si|zsi= k)?
?kp2(sj|zsj= k) is thenormalized factor.
Since the row-normalization process will make the sentence-to-sentence relation ma-trix asymmetric, we adopt the following strategy: let Sim(s) denote the similarity matrix between sen-tences, where Sim(s)(i, j) = sim(si, sj) and D denotes the diagonal matrix with (i, i)-element equalto the sum of the ithrow of Sim(s).
Edge weight matrix between sentences U is calculated as follows:U = D?12Sim(s)D?12(8)Then, the edge weight matrix between topics P is the normalized symmetric matrix of the similairtymatrix between two topics.
The cosine similarity between two topics is calculated according to word-topic distribution.sim(zi, zj) =1C2?wp(w|zi)p(w|zj) =1C2?w?wzi?wzj(9)where C2=?
?wp2(w|zi) ??
?wp2(w|zj) is the normalized factor.We further transform the task to an optimizing problem based on the assumption that closely relatednodes (sentences and topics) tend to have similar scores.
So we would give more penalty for the differ-ence between closely related nodes with regard to edge weight matrices?WN?K,?WK?N, U and P .
Thismotivates the following optimization function ?
(f, g) in Equation (10) similar to the graph harmonicfunction(Zhu et al, 2003).
f denotes the sentence score vector and g denotes the topic score vector.Intuitively, ?
(f, g) measures the sum of difference between graph nodes; the more they differ, the larger?
(f, g) would be.?
(f, g) = a?0?i,j?NUi,j(fi?
fj)2+ a?0?i,j?KPi,j(gi?
gj)2+ (1?
a)?0?i?N?0?j?K?Wij(fi?
gj)2+ (1?
a)?0?i?N?0?j?K?Wij(gi?
fj)2(10)The score vectors can be achieved by minimizing the function in Equation (10).
That is,(f, g)=argminf,g?
(f, g).
We can get the following equations (details are shown in Appendix).f = aUf +12(1?
a)(?W +?WT)gg = aPg +12(1?
a)(?WT+?W )f(11)1200Equation (11) conforms to our intuition: (1) A sentence would be important if it is heavily connectedwith many important sentences and a topic would be important if it is closely related to other importanttopics.
(2) A sentence would be important if it is expressing an important topic, and in turn a topic wouldbe important if it is referred by an important sentence.
Based on Equation (11), the ranking algorithm isdesigned in a semi-supervised way, where the score of the labeled query is fixed to the largest score of 1during each iteration, as shown in Figure 5.
Then, our algorithm iteratively calculates the score of topicsand sentences until convergence1.Input: The sentence set {s1, s2, ..., sN}, topic set{z1, z2, ..., zK}, edge weight matrix?W ,?W , Uand P .
s1is the query.Output: Sentence score vector f and topic scorevector g.BEGIN1.
Initialization, k=0:f0= (1, 0, 0, ..., 0)T, g0= (0, 0, ..., 0)T2.
Update sentence score vectorfk+1= aUfk+12(1?
a)(?W +?WT)gk3.
Update topic score vectorgk+1= aPgk+12(1?
a)(?WT+?W )fk4.
fix the score of query in fk+1to 1.5. k=k+1 Go to Step 2 until convergence.ENDFigure 5: Sentence Ranking AlgorithmInput: The sentence set S = {s1, s2, ..., sN},sentence score vector fOutput: Summary Y.BEGIN:1.
Initialization: Y = ?, X = {S ?
s1}.2. while word num is less than 100:(a) sm= argmaxsi?Xf(si)(b) If sim(sm, s) < Thsem, for all s ?
Y :Y = Y + {sm}(c) X = x?
{sm}ENDFigure 6: Sentence Selection Algorithm3.1 Summary GenerationSentence compression can largely improve summarization quality (Zajic et al, 2007; Peng et al, 2011).Since sentence compression is not the main task in this paper, we just use the revised sentence compres-sion techniques in (Li et al, 2011).Here, we remove the redundant modifiers such as adverbials, relativeclause modifiers, abbreviations, participials and infinitive modifiers for each sentence.As for the sentence selection process, sentences with higher ranking score are selected into the sum-mary.
Then Maximum Marginal Relevance (MMR)(Goldstein et al, 1999) is further used for redundancyremoval.
We just apply a simple greedy algorithm for sentence selection as shown in Figure 6.
We use Yto denote the summary set which contains the selected summary sentences.
The algorithm first initializesY to ?
and X as the set {S ?
s1}.
During each iteration, we select the highest ranked sentence sjfromthe sentence set X.
We need to assure that the value of semantic similarity between two sentences is lessthan Thsem.
Thsemdenotes the threshold for the cosine similarity between two sentences and is set to0.5 in our model.4 ExperimentsThe query-focused MDS task defined in TAC (Text Analysis Conference) evaluations requires generatinga concise and well organized summary for a collection of related documents according to a given query.The query usually consists of a narrative/question sentence.
Our experiment data is composed of TAC(2008-2009) data2, which contain 48 and 44 document collections respectively.
We use docset-A datasets in TAC which has 10 documents per collection.
The average numbers of sentences per documentin TAC2008 and TAC2009 are 252 and 243 respectively, and the system-generated summary is limitedto 100 words.
It is noted that the corpus of TAC2008 and TAC2009 are similar.
In our experiment, weapply the optimal topic number trained on TAC2008 dataset to TAC2009 dataset.1In our experiments, if |fki?
fk+11| ?
0.0001(1 ?
i ?
N) and |gki?
gk+11| ?
0.0001(1 ?
i ?
T ), iteration stops.2TAC data sets are for the update summarization tasks, where the summarization for docset-A can be seen the query-focusedsummarization task referred in this paper.12011 23 4Figure 7: ROUGE score via (1)(2) topic number and (3)(4) parameter a on TAC2008.As for evaluation metrics, we use ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin,2004) measures.
ROUGE measures summary quality by counting overlapping units such as the n-gram,word sequences and word pairs between the candidate summary and the reference summary.
We reportROUGE-1, ROUGE-2, and ROUGE-SU43scores and their corresponding 95% confidential intervals,to evaluate the performance of the system-generated summaries.
As a preprocessing step, stopwordsare firstly removed with a list of 598 stop words and the remaining words are then stemmed usingPorterStemmer.4.4.1 Parameter TuningThere are two parameters to tune in our model.
The first parameter is a in Equation (11) that controlsthe tradeoff between influence from topics and from sentences.
The second one is the topic numberK in LDA topic model.
The combination of the two factors makes it hard to find a global optimizedsolution.
So we apply a gradient search strategy.
At first, parameter a is fixed to a given value.
Thenthe performance of using different topic numbers is evaluated.
After that, we fix the topic number to thevalue which has achieved the best performance, and conduct experiments to find an appropriate value fora.
Here, we use TAC2008 as training data and test our model on TAC2009.First, a is set to 0.5, then we change topic number K from 2 to 20 at the interval of 2.
The ROUGEscore reaches their peaks when the topic number is around 12, as shown in Figure 7(1) and Figure 7(2).Then we fix the number of K to 12 and change the value of parameter a from 0 to 1 with the intervalof 0.1.
When the value of a is set to 0, the model degenerates into a one-layer graph ranking algorithmwhere topic clustering information is neglected.
As we can see from Figure 7(3) and Figure 7(4) , theROUGE scores reach their peaks around 0.6 and then drop afterwards.
Thus, the topic number is set to12 and a is set to 0.6 in the test dataset.3Jackknife scoring for ROUGE is used in order to compare with the human summaries.4http://tartarus.org/martin/PorterStemmer/12024.2 Baseline ComparisonWe firstly compare W-LDA and S-LDA with other clustering approaches.
To be fair, we use the identicalsentence compression techniques and preprocessing methods for all baselines.
Summaries are truncatedto the same length of 100 words.Standard-LDA: A simplified version of W-LDA without considering the background or document-specific information.K-means: Using the K-means clustering algorithm for graph construction.
We firstly randomly selectK sentences as initial centroid for clusters and then iteratively assign a sentence to each cluster.
Thecentroid is recomputed until convergence.
The similarity between nodes in the graph (sentence or cluster)is computed using the standard cosine measure based on the tf-idf information.
K is set to 12, the sameas topic number in LDA.Agglomerative: a bottom-up hierarchical clustering algorithm and starts with the sentences as indi-vidual clusters and, at each step, merges the most similar or closest pair of clusters, until the number ofthe clusters reduces to the desired number K = 12.Divisive: a top-down hierarchical clustering algorithm and starts with one, all-inclusive cluster and,at each step, splits the largest cluster until the number of clusters increases to the desired number K,K = 12.Approach Rouge-1 Rouge-2 Rouge-SU4W-LDA 0.3791 (0.3702-0.3880) 0.1092 (0.1047-0.1135) 0.1382 (0.1350-0.1414)S-LDA 0.3802 (0.3721-0.3883) 0.1109 (0.1061-0.1157) 0.1398 (0.1342-0.1454)Standard LDA 0.3702 (0.3614-0.3790) 0.1012 (0.0960-0.1064) 0.1292 (0.1242-0.1344)K-means 0.3658 (0.3582-0.3734) 0.1046 (0.0992-0.1080) 0.1327 (0.1263-0.1391)Agglomerative 0.3681 (0.3612-0.3750) 0.1042 (0.091-0.1093) 0.1319 (0.1266-0.1272)Divisive 0.3676 (0.3610-0.3742) 0.1021 (0.0981-0.1061) 0.1320 (0.1275-0.1365)Table 1: Comparison with other clustering baselines.Table 1 presents the performance of different clustering algorithms for summarization.
Traditionalclustering algorithms such as K-means, Agglomerative and Divisive clustering achieve comparative re-sults.
Compared with traditional clustering algorithms, LDA based models (W-LDA, S-LDA, Standard-LDA) achieve better results.
This can be explained by the clear and rigorous probabilistic interpretationof topic models.
Background information and document-specific information would influence the per-formance of topic modeling (Chemudugunta et al, 2006), that is why S-LDA and W-LDA achieve betterROUGE performance than the standard LDA.
We can also see that S-LDA is slightly better than W-LDAin regard with ROUGE performance.
The reason can be explained as follows: The aim of topic mod-eling in this task is to cluster sentences according to their topics.
So treating sentence as a unit in topicmodeling would be better than treating it as a set of independent words.
In addition, forcing the words inone sentence to share the same aspect topic can ensure semantic cohesion of the mined topics.Next, we compare our model with the following widely used summarization approaches.Manifold: One-layer graph-based semi-supervised approach developed byWan et al.(2008).
Sentencerelations are calculated according to tf ?
idf and topic information is neglected.LexRank: An unsupervised graph-based summarization approach(Erkan and Radev, 2004), which isa revised version of the famous web ranking algorithm PageRank.KL-Divergence: The approach developed by (Lin et al, 2006) by using a KL-divergence based sen-tence selection strategy.KL(Ps||Qd) =?wP (w)logP (w)Q(w)(12)where Psis the unigram distribution of candidate summary and Qddenotes the unigram distribution ofdocument collection.
Since this approach is designed for general summarization, query influence is notconsidered.1203Hiersum: A LDA based approach proposed by (Haghighi and Vanderwende, 2009), where unigramdistribution is calculated from LDA topic model in Equation (12).MEAD: A centroid based summary algorithm by (Radev et al, 2004).
Cluster centroids in MEADconsists of words which are central not only to one article in a cluster, but to all the articles.
Similarity ismeasured by using tf ?
idf .Approach Rouge-1 Rouge-2 Rouge-SU4W-LDA 0.3891 (0.3802-0.3980) 0.1192 (0.1147-0.1235) 0.1482 (0.1450-0.1514)S-LDA 0.3902 (0.3821-0.3983) 0.1209 (0.1161-0.1257) 0.1498 (0.1442-0.1554)Manifold 0.3581 (0.3508-0.3656) 0.1007 (0.0952-0.1062) 0.1267 (0.1214-0.1320)LexRank 0.3442 (0.3381-0.3502) 0.0817 (0.0782-0.0852) 0.1106 (0.1064-0.1148)KL-divergence 0.3468 (0.3410-0.3526) 0.0820 (0.0782-0.0858) 0.1117 (0.1073-0.1161)Hiersum 0.3599 (0.3526-0.3672) 0.1004 (0.0956-0.1052) 0.1280 (0.1221-0.1339)MEAD 0.3451 (0.3390-0.3512) 0.0862 (0.0817-0.0907) 0.1131 (0.1080-0.1182)Table 2: Performance comparison with baselinesPerformance is presented at Table 2.
We can find that ROUGE performance of one-layer graph rank-ing algorithms such as Manifold and LexRank, where topic information is neglected, achieve worseresults than all two-layer models where topic information is considered (See Table 1).
This verifies ourprevious claim (Hardy et at., 2002; Harabagiu and Lacatusu, 2005; Wan and Yang, 2008) that the con-sideration of topic information will improve summarization performance.
S-LDA and W-LDA achievebetter performance than KL-divergence and Hiersum.
This is because the sentence selection strategy forKL-divergence and Hiersum tries to select sentence best representing the document as shown in Equation(12), but do not consider the influence of query.4.3 Manual EvaluationW-LDA and S-LDA get comparative ROUGE scores.
To obtain a more accurate measure to decide whichapproach is better, we perform a simple user study concerning the following aspects on 40 randomlyselected topics in TAC2009: (1) Overall quality.
(2) Focus: Whether the summary contains less irrelevantcontent?
(3) Responsiveness: Whether the summary is responsive to the query.
(4) Non-Redundancy:Whether the summary is non-redundant.
Each respect is rated from 1 (very poor) to 5 (very good).
Fournative speakers who are Ph.D. students in computer science (none are authors) performed the task.The average score and standard deviation for W-LDA and S-LDA are displayed in Table 3.
We can seethat the two models almost tie in foucs and non-redundancy.
This is because two models use the samesentence selection strategy based on MMR for redundancy removal and propagation model to imposethe query?s influence on sentences.
S-LDA outperforms W-LDA in overall ranking and responsivenessranking.
This implies that treating sentence as a unit in topic modeling would be preferable to justtreating it as a series of independent words.S-LDA W-LDAOverall 3.98?
0.52 3.58?
0.55Focus 3.65?
0.54 3.35?
0.61Responsiveness 3.73?
0.43 3.38?
0.46Non-Redundancy 3.48?
0.51 3.45?
0.48Table 3: Manual evaluation for S-LDA and W-LDA.5 Related WorkGraph-based ranking approaches have been hot these days for both generic and query-focused summa-rization (Zhou et al, 2003; Zhou et al, 2004; Erkan and Radev, 2004; Wan et al, 2007; Wei et al, 2008).Commonly used graph-based ranking algorithms are mainly inspired by the link analysis algorithm inweb research such as PageRank (Page et al, 1999).
(Wan et al, 2007) proposed the approach that treated1204the task of query-focused MDS as a semi-supervised learning task, in which the query is treated as alabeled node, and sentences as unlabeled nodes.
Then the scores of sentences are determined from themanifold learning algorithm proposed by (Zhou et al, 2003) or the harmonic approach proposed by (Zhuet al, 2003).It is worthy of noting that researchers have found that by considering topic level information, the sum-marization performance can be effectively improved (Hardy et al, 2002; Wan and Yang, 2008; Harabagiuand Lacatusu, 2005).
For example, (Otterbacher et al, 2005) models documents as a stochastic graph andcalculates sentence ranking scores with a topic-sensitive version of PageRank.
(Wan and Yang, 2008)developed a two-layer graph by clustering sentences by using standard clustering algorithms such asK-means or agglomerate clustering.
However, his algorithm is for general summarization where theinfluence of query is not considered.A significant portion of recent work incorporates LDA topic models (Blei et al, 2008) in summarizationtasks for their clear and rigorous probabilistic topic interpretations (Daume and Marcu, 2006; Titov andMcDonald, 2008; Haghighi and Vanderwende, 2009; Mason and Charniak, 2011; Li et al, 2013a; Liet al, 2013b).
(Haghighi and Vanderwende, 2009) introduced a LDA based model called Hiersum tofind the subtopics or aspects by combining KL-divergence criterion for selecting relevant sentences.AYESSUM (Daume and Marcu, 2006) and the Special Words and Background model (Chemuduguntaet al, 2006) are very similar to Hiersum.
In the same way, (Delort and Alfonseca, 2012) tried to use LDAto model different levels of information for novelty detection in update summarization.
Furthermore,(Paul and Dredze, 2013) extends their f-LDA to jointly model combinations of drug, aspect and route ofadministration as an exploratory tool for extractive summarization.6 Conclusions and Future WorkIn this paper, we propose a two-layer graph-based semi-supervised algorithm for query-focused MDS.Topic modeling techniques are used for sentence clustering and further graph construction.
By consider-ing different kinds of information such as background or document-specific information, our two LDAtopic model extensions achieve better results than traditional clustering algorithms.One primary disadvantage of our models is that it is hard to decide the topic numberK in LDA modelsand how to define topic number is still a open problem in LDA topic models.
From Figure 7, we cansee that summarization performance is sensitive to topic number.
We train the value of topic numberon TAC2008 dataset and test the model on TAC2009.
Such process makes sense because the corpussizes and contents of two datasets are similar.
But it would be hard to extend optimal topic number inTAC2008 to other datasets.
Using non-parametric topic modeling techniques where topic number doesnot have to be predefined is one of our future works.AcknowledgementsThanks Jiwei Li for the insightful reviews and careful polishment.
We also thank the three anonymousreviewers for their helpful comments.
This work was partially supported by National High TechnologyResearch and Development Program of China (No.
2012AA011101), National Key Basic Research Pro-gram of China (No.
2014CB340504), National Natural Science Foundation of China (No.
61273278),and National Key Technology R&D Program (No: 2011BAH10B04-03).ReferencesEdoardo M. Airoldi, Blei D M, Fienberg S E, et al.
Mixed membership stochastic blockmodels[J].
In The Journalof Machine Learning Research, 2008, 9(1981-2014): 3.David Blei, Andrew Ng and Micheal Jordan.
2003.
Latent dirichlet allocation.
In The Journal of Machine LearningResearch.Chaltanya Chemudugunta, Padhraic Smyth and Mark Steyers.
Modeling General and Specific Aspects of Docu-ments with a Probabilistic Topic Model..
In Advances in Neural Information Processing Systems 19: Proceed-ings of the 2006 Conference.1205Hal Daume and Daniel Marcu H. 2006.
Bayesian Query-Focused Summarization.
In Proceedings of the 21st In-ternational Conference on Computational Linguistics and the 44th annual meeting of the Association for Com-putational Linguistics, pages 305-312.Jean-Yves Delort and Enrique Alfonseca.
DualSum: a topic-model based approach for update summarization.
InProceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.Gune Erkan and Dragomir Radev.
2004.
Lexrank: graph-based lexical centrality as salience in text summarization.In Journal of Artificial Intelligence Research.Jade Goldstein, Mark Kantrowitz, Vibhu Mittal and Jaime Carbonell.
1999.
Summarizing Text Documents: Sen-tence Selection and EvaluationMetrics.
In Proceedings of the 22nd annual international ACM SIGIR conferenceon Research and development in information retrieval.Amit Gruber, Yair Weiss and Michal Rosen-Zvi.
Hidden topic Markov models.
In International Conference onArtificial Intelligence and Statistics.
2007Aria Haghighi and Lucy Vanderwende.
2009.
Exploring content models for multi-document summarization.
InProceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapterof the Association for Computational Linguistics, pages 362370.Sanda Harabagiu and Finley Lacatusu.
2005.
Topic themes for multi-document summarization.
In Proceedings ofthe 28th annual international ACM SIGIR conference on Research and development in information retrieval.Hilda Hardy, Nobuyuki Shimizu, Tomek Strzakowski, Liu Ting, Xinyang Zhang and Bowden Wize.
2002.
Cross-document summarization by concept classification.
In Proceedings of the 25th annual international ACM SIGIRconference on Research and development in information retrieval.Feng Jin, Minlie Huang, and Xiaoyan Zhu.
2010.
The summarization systems at tac 2010.
In Proceedings of thethird Text Analysis Conference, TAC-2010.Jiwei Li and Sujian Li.
2013.
Evolutionary Hierarchical Dirichlet Process for Timeline Summarization.
In ACL2013.Jiwei Li and Claire Cardie.
2014.
Timeline Generation: Tracking individuals on Twitter.
In WWW 2014.Peng Li, Yinglin Wang, Wei Gao and Jiang Jing.
2011.
Generating aspect-oriented multi-document summariza-tion with event-aspect model.
In Proceedings of the Conference on Empirical Methods in Natural LanguageProcessing.Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun Nie.
2006.
An information-theoretic approach to auto-matic evaluation of summaries.
In Proceedings of the main conference on Human Language Technology Con-ference of the North American Chapter of the Association of Computational Linguistics.Chin-Yew Lin.
Improving summarization performance by sentence compression: a pilot study.
In Proceedings thesixth international workshop on Information retrieval with Asian languages.Rebecca Mason and Eugene Charniak.
2011.
Extractive multi-document summaries should explicitly not containdocument-specific content.
In proceedings of ACL HLT.Jahna Otterbacher, Gne Erkan, and Dragomir R. Radev.
Using random walks for question-focused sentence re-trieval.
In Proceedings of the conference on Human Language Technology and Empirical Methods in NaturalLanguage Processing.
Association for Computational Linguistics, 2005.Lawrence Page, Sergey Brin, Rajeev Motwani and Terry Winograd.
1999.
The Pagerank Citation Ranking: Bring-ing Order to the Web.
Technical report, Stanford Digital Libraries.Michael J. Paul and Mark Dredze.
Drug extraction from the web: Summarizing drug experiences with multi-dimensional topic models.
In Proceedings of NAACL-HLT.
2013.Wei-Ting Peng, Wei-Ta Chu, Chia-Han Chang, et al.
Editing by viewing: automatic home video summarization byviewing behavior analysis[J].
In Multimedia, IEEE Transactions on, 2011, 13(3): 539-550.Dragomir Radev, Allison T, Blair-Goldensohn S, et al.
MEAD-a platform for multidocument multilingual textsummarization[C].
In Proceedings of the 4th International Conference on Language Resources and Evaluation,2004.1206Ivan Titov and Ryan McDonald.
2008.
Modeling on- line reviews with multi-grain topic models.
In InternationalWorld Wide Web Conference.Xiaojun Wan, Jianwu Yang and Jianguo Xiao.
2007.
Manifold-ranking based topic-focused multi-document sum-marization.
In Proceedings of International Joint Conference on Artificial Intelligence.Xiaojun Wan and Jianwu Yang.
2008.
Multi-document Summarization using cluster-based link analysis.
In Pro-ceedings of the 31st annual international ACM SIGIR conference on Research and development in informationretrieval.Furu Wei, Wenjie Li, Qin Lu, and Yanxiang He.
2008.
Query-sensitive mutual reinforcement chain and its appli-cation in query-oriented multi-document summarization.
In Proceedings of the 31st annual international ACMSIGIR conference on Research and development in information retrieval.David Zajic, et al.
Multi-candidate reduction: Sentence compression as a tool for document summarization tasks.In Information Processing & Management 43.6 (2007): 1549-1570.Dengzhong Zhou, Jason Weston, Arthur Gretton, Olivier Bousquet and Bernhard Schlkopf.
2003.
Ranking on DataManifolds.
In Proceedings of the Conference on Advances in Neural Information Processing Systems.Dengyou Zhou, Olivier Bousquet, Thomas Navin and JasonWeston.
2004.
Learning with Local and Global Con-sistency.
In Proceedings of Advances in neural information processing systems.Xiaojin Zhu, Z. Ghahramani, and J. Lafferty.
Semi-supervised learning using gaussian fields and functions.
InProceedings of the 20th International Joint Conference on Machine Learning, 2003.APPENDIXTo optimize ?
(f, g), shown in Equation (10), we set the partial derivative with respect to fmto 0, form ?
[1, N ].
Let ?mndenote the index function as follows:?mn={1 if m = n0 if m ?= n0 =??
(f, g)ft= 2a?i,jUi,j(fi?
fj)(?it?
?jt) + 2(1?
a)??i,j?Wij(fi?
gj)?it?
2(1?
a)?i,j?Wij(gi?
fj)?jt= 2(1?
a)?j?Wtj(ft?
gj) + 2(1?
a)?i?Wit(gi?
ft)+ 2a?jUtj(ft?
fj) + 2a?iUit(fi?
ft)= ft[4a?jUtj+ 2(1?
a)?j?Wtj+ 2(1?
a)?j?Wjt]?
4a?jUtjfj?
2(1?
a)?j?Wtjgj?
2(1?
a)?j?Wjtgj?jUtj= 1?j?Wtj= 1?j?Wjt= 1ft= a?jUtjfj+12(1?
a)[?j(?Wtj+?Wjt)gj]So we have:f = aUf +12(1?
a)(?W +?WT)gA similar approach is used to obtain the second part of Equation (11).1207
