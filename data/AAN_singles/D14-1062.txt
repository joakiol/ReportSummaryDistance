Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 566?576,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLatent Domain Phrase-based Models for AdaptationHoang Cuong and Khalil Sima?anInstitute for Logic, Language and ComputationUniversity of AmsterdamScience Park 107, 1098 XG Amsterdam, The Netherlands{c.hoang,k.simaan}@uva.nlAbstractPhrase-based models directly trainedon mix-of-domain corpora can besub-optimal.
In this paper we equipphrase-based models with a latent domainvariable and present a novel method foradapting them to an in-domain task rep-resented by a seed corpus.
We derive anEM algorithm which alternates betweeninducing domain-focused phrase pairestimates, and weights for mix-domainsentence pairs reflecting their relevancefor the in-domain task.
By embeddingour latent domain phrase model in asentence-level model and training thetwo in tandem, we are able to adapt allcore translation components together?
phrase, lexical and reordering.
Weshow experiments on weighing sentencepairs for relevance as well as adaptingphrase-based models, showing significantperformance improvement in both tasks.1 Mix vs.
Latent Domain ModelsDomain adaptation is usually perceived as utiliz-ing a small seed in-domain corpus to adapt an ex-isting system trained on an out-of-domain corpus.Here we are interested in adapting an SMT sys-tem trained on a large mix-domain corpus Cmixto an in-domain task represented by a seed paral-lel corpus Cin.
The mix-domain scenario is in-teresting because often a large corpus consists ofsentence pairs representing diverse domains, e.g.,news, politics, finance, sports, etc.At the core of a standard state-of-the-art phrase-based system (Och and Ney, 2004) is a phrasetable {?e?,?f?}
extracted from the word-alignedtraining data together with estimates for Pt(e?
|?f)and Pt(?f | e?).
Because the translations ofwords often vary across domains, it is likelythat in a mix-domain corpus Cmixthe translationambiguity will increase with the domain diver-sity.
Furthermore, the statistics in Cmixwill re-flect translation preferences averaged over the di-verse domains.
In this sense, phrase-based mod-els trained on Cmixcan be considered domain-confused.
This often leads to suboptimal perfor-mance (Gasc?o et al., 2012; Irvine et al., 2013).Recent adaptation techniques can be seen asmixture models, where two or more phrase ta-bles, estimated from in- and mix-domain corpora,are combined together by interpolation, fill-up, ormultiple-decoding paths (Koehn and Schroeder,2007; Bisazza et al., 2011; Sennrich, 2012; Raz-mara et al., 2012; Sennrich et al., 2013).
Herewe are interested in the specific question how toinduce a phrase-based model from Cmixfor in-domain translation?
We view this as in-domainfocused training on Cmix, a complementary adap-tation step which might precede any further com-bination with other models, e.g., in-, mix- orgeneral-domain.The main challenge is how to induce from Cmixa phrase-based model for the in-domain task,given only Cinas evidence?
We present an ap-proach whereby the contrast between in-domainprior distributions and ?out-domain?
distributionsis exploited for softly inviting (or recruiting) Cmixphrase pairs to either camp.
To this end we in-566troduce a latent domain variable D to signify in-(D1) and out-domain (D0) respectively.1With the introduction of the latent variables, weextend the translation tables in phrase-based mod-els from generic Pt(e?
|?f) to domain-focused byconditioning them on D, i.e., Pt(e?
|?f,D) and de-composing them as follows:Pt(e?
|?f,D) =Pt(e?
|?f)P(D | e?,?f)?e?Pt(e?
|?f)P(D | e?,?f).
(1)Where P(D | e?,?f) is viewed as the latent phrase-relevance models, i.e., the probability that aphrase pair is in- (D1) or out-domain (D0).
In theend, our goal is to replace the domain-confusedtables, Pt(e?
|?f) and Pt(?f | e?
), with the in-domainfocused ones, Pt(e?
|?f,D1) and Pt(?f | e?, D1).2Note how Pt(e?
|?f,D1) and Pt(?f | e?, D1) containsPt(e?
|?f) and Pt(?f | e?)
as special case.Eq.
1 shows that the key to training the latentphrase-based translation models is to train the la-tent phrase-relevance models, P (D | e?,?f).
Ourapproach is to embed P (D | e?,?f) in asymmetricsentence-level models P (D | e, f) and train themon Cmix.
We devise an EM algorithm where atevery iteration, in- or out-domain estimates pro-vide full sentence pairs ?e, f?
with expectations{P (D | e, f) | D ?
{0, 1}}.
Once these ex-pectation are in Cmix, we induce re-estimates forthe latent phrase-relevance models, P (D | e?,?f).Metaphorically, during each EM iteration the cur-rent in- or out-domain phrase pairs compete oninviting Cmixsentence pairs to be in- or out-domain, which bring in new (weights for) in- andout-domain phrases.
Using the same algorithm wealso show how to adapt all core translation com-ponents in tandem, including also lexical weightsand lexicalized reordering models.Next we detail our model, the EM-based invita-tion training algorithm and provide technical so-lutions to a range of difficulties.
We report exper-1Crucially, the lack of explicit out-domain data in Cmixisa major technical difficulty.
We follow (Cuong and Sima?an,2014) and in the sequel present a relatively efficient solutionbased on a kind of ?burn-in?
procedure.2It is common to use these domain-focused models asadditional features besides the domain-confused features.However, here we are more interested in replacing thedomain-confused features rather than complementing them.This distinguishes this work from other domain adaptationliterature for MT.iments showing good instance weighting perfor-mance as well as significantly improved phrase-based translation performance.2 Model and training by invitationEq.
1 shows that the key to training the latentphrase-based translation models is to train the la-tent phrase-relevance models, P (D | e?,?f).
Asmentioned, for training P (D | e?,?f) on parallelsentences in Cmixwe embed them in two asym-metric sentence-level models {P (D | e, f) | D ?
{0, 1}}.2.1 Domain relevance sentence modelsIntuitively, sentence models for domain relevanceP (D | e, f) are somewhat related to data selec-tion approaches (Moore and Lewis, 2010; Axel-rod et al., 2011).
The dominant approach to dataselection uses the contrast between perplexitiesof in- and mix-domain language models.3In thetranslation context, however, often a source phrasehas different senses/translations in different do-mains, which cannot be distinguished with mono-lingual language models (Cuong and Sima?an,2014).
Therefore, our proposed latent sentence-relevance model includes two major latent com-ponents - monolingual domain-focused relevancemodels and domain-focused translation modelsderives as follows:P (D | e, f) =P (e, f, D)?D?
{D1,D0}P (e, f, D), (2)where P (e, f, D) can be decomposed as:P (f, e, D) =12(P (D)Plm(e | D)Pt(f | e, D)+ P (D)Plm(f | D)Pt(e | f, D)).(3)Here?
Pt(e|f, D) and similarly Pt(f|e, D): the latentdomain-focused translation models aim at cap-turing the faithfulness of translation with re-spect to different domains.
We simplify this as3Note that earlier work on data selection exploits the con-trast between in- and mix-domain.
In (Cuong and Sima?an,2014), we present the idea of using the language and transla-tion models derived separately from in- and out-domain data,and show how it helps for data selection.567?bag-of-possible-phrases?
translation models:4Pt(e|f, D) :=??e?,?f?
?A(e,f)Pt(e?|?f,D)c(e?,?f),(4)where A(e, f) is the multiset of phrases in?e, f?
and c(?)
denotes their count.
Sub-modelPt(e?|?f,D) is given by Eq.
1.?
Plm(e|D), Plm(f|D): the latent monolingualdomain-focused relevance models aim at cap-turing the relevance of e and f for identifyingdomain D but here we consider them languagemodels (LMs).5As mentioned, the out-domainLMs differ from previous works, e.g., (Axel-rod et al., 2011), which employ mix-domainLMs.
Here, we stress the difficulty in findingdata to train out-domain LMs and present a so-lution based on identifying pseudo out-domaindata.?
P (D): the domain priors aim at modelingthe percentage of relevant data that the learn-ing framework induces.
It can be estimatedvia phrase-level parameters but here we prefersentence-level parameters:6P (D) :=??e,f?
?CmixP (D | e, f)?D??e,f?
?CmixP (D | e, f)(5)2.2 Training by invitationGenerally, our model can be viewed to have latentparameters ?
= {?D0,?D1}.
The training pro-cedure seeks ?
that maximize the log-likelihoodof the observed sentence pairs ?e, f?
?
Cmix:L =??e,f?
?Cmixlog?DP?D(D, e, f).
(6)It is obvious that there does not exist a closed-formsolution for Equation 6 because of the existence of4We design our latent domain translation models with ef-ficiency as our main concern.
Future extensions could in-clude the lexical and reordering sub-models (as suggested byan anonymous reviewer.
)5Relevance for identification or retrieval could be differ-ent from frequency or fluency.
We leave this extension forfuture work.6It should be noted that in most phrase-based SMT sys-tems bilingual phrase probabilities are estimated heuristicallyfrom word alignmened data which often leads to overfitting.Estimating P (D) from sentence-level parameters rather thanfrom phrase-level parameters helps us avoid the overfittingwhich often accompanies phrase extraction.the log-term log?.
The EM algorithm (Dempsteret al., 1977) comes as an alternative solution to fitthe model.
It can be seen to maximizeL via block-coordinate ascent on a lower bound F(q,?)
usingan auxiliary distribution q(D | e, f)F(q,?)
=??e,f?
?Dq(D | e, f) logP?D(D, e, f)q(D | e, f)(7)where the inequality results, i.e., L ?
F(q,?
),derived from log being concave and Jensen?s in-equality.
We rewrite the Free Energy F(q,?
)(Neal and Hinton, 1999) as follows:F =??e,f?
?Dq(D | e, f) logP?D(D | e, f)q(D | e, f)+??e,f?
?Dq(D | e, f) logP?
(e, f)=??e,f?logP?
(e, f) (8)?KL[q(D | e, f) || P?D(D | e, f)],where KL[?
|| ?]
is the KL-divergence.With the introduction of the KL-divergence, thealternating E and M steps for our EM algorithmare easily derived asE-step : qt+1(9)argmaxq(D | e,f)F(q,?t) =argminq(D | e,f)KL[q(D|e, f) || P?tD(D|e, f)]= P?tD(D | e, f)M-step : ?t+1(10)argmax?F(qt+1,?)
=argmax???e,f?
?Dq(D | e, f) logP?D(D, e, f)The iterative procedure is illustrated in Fig-ure 1.7At the E-step, a guess for P (D | e?,?f) canbe used to update Pt(?f | e?, D) and Pt(e?
|?f,D)(i.e., using Eq.
1) and consequently Pt(f | e, D)and Pt(e | f, D) (i.e., using Eq.
4).
These resultingtable estimates, together with the domain-focusedLMs and the domain priors are served as expectedcounts to update P (D | e, f).8At the M-step,7For simplicity, we ignore the LMs and prior models inthe illustration in Fig.
1.8Since we only use the in-domain corpus as priors to ini-tilize the EM parameters, in technical perspective we do notwant P (D | e, f) parameters to go too far off from the initial-ization.
We therefore prefer the averaged style in practice,i.e., at the iteration n we update the P (D |e, f) parameters,P(n)(D|e, f) as1n(P(n)(D | e, f) +?n?1i=1P(i)(D | e, f)).568P (e?|?f,D)P (?f |e?, D)P (e|f, D)P (f|e, D)P (f, e, D)P (D|e?,?f)P (D|e, f)Phrase-level Sentence-levelRe-update phrase-level parametersUpdate sentence-level parametersFigure 1: Our probabilistic invitation framework.the new estimates for P (D | e, f) can be used to(softly) fill in the values of hidden variable D andestimate parameters P (D | e?,?f) and P (D).
TheEM is guaranteed to converge to a local maximumof the likelihood under mild conditions (Neal andHinton, 1999).Before EM training starts we must provide a?reasonable?
initial guess for P (D | e?,?f).
Wemust also train the out-domain LMs, which needsthe construction of pseudo out-domain data.9One simple way to do that is inspired by burn-in in sampling, under the guidance of an in-domain data set, Cinas prior.
At the begin-ning, we train Pt(e?
|?f,D1) and Pt(?f | e?, D1)for all phrases learned from Cin.
We also trainPt(e?
|?f) and Pt(?f | e?)
for all phrases learnedfrom Cmix.
During burn-in we assume that theout-domain phrase-based models are the domain-confused phrase-based models, i.e., Pt(e?
|?f,D0)?
Pt(e?
|?f) and Pt(?f | e?, D0) ?
Pt(?f | e?).
Weisolate all the LMs and the prior models from ourmodel, and apply a single EM iteration to updateP (D | e, f) based on those domain-focused mod-els Pt(e?
|?f,D) and Pt(?f | e?, D).In the end, we use P (D | e, f) to fill in the val-ues of hidden variable D in Cmix, so it providesus with an initialization for P (D | e?,?f).
Subse-quently, we also rank sentence pairs in CmixwithP (D1| e, f) and select a subset of smallest scor-ing pairs as a pseudo out-domain subset to trainPlm(e | D0) and Plm(f | D0).
Once the latentdomain-focused LMs have been trained, the LMprobabilities stay fixed during EM.
Crucially, it9The in-domain LMs Plm(e | D1) and Plm(f | D1) canbe simply trained on the source and target sides of Cinre-spectively.is important to scale the probabilities of the fourLMs to make them comparable: we normalize theprobability that a LM assigns to a sentence by thetotal probability this LM assigns to all sentencesin Cmix.3 Intrinsic evaluationWe evaluate the ability of our model to retrieve?hidden?
in-domain data in a large mix-domaincorpus, i.e., we hide some in-domain data in alarge mix-domain corpus.
We weigh sentencepairs under our model with P (D1| e?,?f) andP (D1| e, f) respectively.
We report pseudo-precision/recall at the sentence-level using arange of cut-off criteria for selecting the topscoring instances in the mix-domain corpus.
Agood relevance model expects to score higher forthe hidden in-domain data.Baselines Two standard perplexity-based se-lection models in the literature have beenimplemented as the baselines: cross-entropydifference (Moore and Lewis, 2010) and bilingualcross-entropy difference (Axelrod et al., 2011),investigating their ability to retrieve the hidingdata as well.
Training them over the data to learnthe sentences with their relevance, we then rankthe sentences to select top of pairs to evaluate thepseudo-precision/recall at the sentence-level.Results We use a mix-domain corpus Cgof 770Ksentence pairs of different genres.10There is alsoa Legal corpus of 183K pairs that serves as thein-domain data.
We create Cmixby selecting anarbitrary 83K pairs of in-domain pairs and addingthem to Cg(the hidden in-domain data); we usethe remaining 100k in-domain pairs as Cin.To train the baselines, we construct interpo-lated 4-gram Kneser-Ney LMs using BerkeleyLM(Pauls and Klein, 2011).
Training our model onthe data takes six EM-iterations to converge.1110Count of sentence pairs: European Parliament (Koehn,2005): 183, 793; Pharmaceuticals: 190, 443, Software:196, 168, Hardware: 196, 501.11After the fifth EM iteration we do not observe any sig-nificant increase in the likelihood of the data.
Note that weuse the same setting as for the baselines to train the latentdomain-focused LMs for use in our model ?
interpolated 4-gram Kneser-Ney LMs using BerkeleyLM.
This training set-ting is used for all experiments in this work.569051015202530354045505560657075808590951001 2 3 4 5 6 7 8 9 10 11 12 13 14 15Pseudo-Precision(Sentence-Level)Top Percentage(a): Pseudo-Precision (Sentence-Level)Iter.
1 Iter.
2 Iter.
3 Iter.
4 Iter.
505101520253035404550556065701 2 3 4 5 6 7 8 9 10 11 12 13 14 15Pseudo-Recall(Sentence-Level)Top Percentage(b): Pseudo -Recall (Sentence -Level)Iter.
1 Iter.
2 Iter.
3 Iter.
4 Iter.
5Figure 2: Intrinsic evaluation.Fig.
2 helps us examine how the pseudo sen-tence invitation are done during each EM iter-ation.
For later iterations we observe a betterpseudo-precision and pseudo-recall at sentence-level (Fig.
2(a), Fig.
2(b)).
Fig.
2 also revealsa good learning capacity of our learning frame-work.
Nevertheless, we observe that the baselinesdo not work well for this task.
This is not new,as pointed out in our previous work (Cuong andSima?an, 2014).Which component type contributes more to theperformance, the latent domain language modelsor the latent domain translation models?
Furtherexperiments have been carried on to neutralizeeach component type in turn and build a selectionsystem with the rest of our model parameters.
Itturns out that the latent domain translation mod-els are crucial for performance for the learningframework, while the latent domain LMs make afar smaller yet substantial contribution.
We referreaders to our previous work (Cuong and Sima?an,2014), which provides detail analysis of the dataselection problem.4 Translation experiments: SettingData We use a mix-domain corpus consisting of4M sentence pairs, collected from multiple re-sources including EuroParl (Koehn, 2005), Com-mon Crawl Corpus, UN Corpus, News Commen-tary.
As in-domain corpus we use ?Consumerand Industrial Electronics?
manually collectedby Translation Automation Society (TAUS.com).The corpus statistics are summarized in Table 1.System We train a standard state-of-the-artEnglish SpanishCmixSents 4MWords 113.7M 127.1MDomain:ElectronicsCinSents 109KWords 1, 485, 558 1, 685, 716DevSents 984Words 13130 14, 955TestSents 982Words 13, 493 15, 392Table 1: The data preparation.phrase-based system, using it as the baseline.12There are three main kinds of features for thetranslation model in the baseline - phrase-basedtranslation features, lexical weights (Koehn et al.,2003) and lexicalized reordering features (Koehnet al., 2005).13Other features include the penal-ties for word, phrase and distance-based reorder-ing.The mix-domain corpus is word-aligned usingGIZA++ (Och and Ney, 2003) and symmetrizedwith grow(-diag)-final-and (Koehn et al., 2003).We limit phrase length to a maximum of sevenwords.
The LMs are interpolated 4-grams withKneser-Ney, trained on 2.2M English sentencesfrom Europarl augmented with 248.8K sentencesfrom News Commentary Corpus (WMT 2013).We tune the system using k-best batch MIRA(Cherry and Foster, 2012).
Finally, we use Moses12We use Stanford Phrasal - a standard state-of-the-artphrase-based translation system developed by Cer et al.
(2010).13The lexical weights and the lexical reordering featureswill be described in more detail in Section 6.57019.9120.48 20.520.6420.51 20.5219.82020.220.420.620.82121.2Baseline Iter.
1 Iter.
2 Iter.
3 Iter.
4 Iter.
5Electrics (Training Data: 1 Million)Figure 3: BLEU averaged over multiple runs.
(Koehn et al., 2007) as decoder.14We report BLEU (Papineni et al., 2002), ME-TEOR 1.4 (Denkowski and Lavie, 2011) and TER(Snover et al., 2006), with statistical significanceat 95% confidence interval under paired bootstrapre-sampling (Press et al., 1992).
For every systemreported, we run the optimizer at least three times,before running MultEval (Clark et al., 2011) forresampling and significance testing.Outlook In Section 5 we examine the effect oftraining only the latent domain-focused phrase ta-ble using our model.
In Section 6 we proceed fur-ther to estimate also latent domain-focused lexicalweights and lexicalized reordering models, exam-ining how they incrementally improve the transla-tion as well.5 Adapting phrase table onlyHere we investigate the effect of adapting thephrase table only; we will delay adapting thelexical weights and lexicalized reordering fea-tures to Section 6.
We build a phrase-based sys-tem with the usual features as the baseline, in-cluding two bi-directional phrase-based models,plus the penalties for word, phrase and distortion.We also build a latent domain-focused phrase-based system with the two bi-directional latentphrase-based models, and the standard penaltiesdescribed above.We explore training data sizes 1M , 2Mand 4M sentence pairs.
Three baselines aretrained yielding 95.77M , 176.29M and 323.88Mphrases respectively.
We run 5 EM iterations to14While we implement the latent domain phrase-basedmodels using Phrasal for some advantages, we prefer to useMoses for decoding.train our learning framework.
We use the pa-rameter estimates for P (D | e?,?f) derived at eachEM iteration to train our latent domain-focusedphrase-based systems.
Fig.
3 presents the results(in BLEU) at each iteration in detail for the case of1M sentence pairs.
Similar improvements are ob-served for METEOR and TER.
Here, we consis-tently observe improvements at p-value = 0.0001for all cases.It should be noted that when doubling the train-ing data to 2M and 4M , we observe the similarresults.Finally, for all cases we report their best resultin Table 2.
Here, note how the improvement couldbe gained when doubling the training data.DataSystem Avg ?
p-value1MBaseline 19.91 ?
?Our System 20.64 +0.73 0.00012MBaseline 20.54 ?
?Our System 21.41 +0.87 0.00014MBaseline 21.44 ?
?Our System 22.62 +1.18 0.0001Table 2: BLEU averaged over multiple runs.It is also interesting to consider the averageentropy of phrase table entries in the domain-confused systems, i.e.,??
?e?,?f?pt(e?|?f) log pt(e?|?f)number of phrases?e?,?f?against that in the domain-focused systems??
?e?,?f?pt(e?|?f,D1) log pt(e?|?f,D1)number of phrases?e?,?f?.Following (Hasler et al., 2014) in Table 3 we alsoshow that the entropy decreases significantly in571the adapted tables in all cases, which indicates thatthe distributions over translations of phrases havebecome sharper.Baseline Iter.
1 Iter.
2 Iter.
3 Iter.
4 Iter.
50.210 0.187 0.186 0.185 0.185 0.184Table 3: Average entropy of distributions.In practice, the third iteration systems usuallyproduce best translations.
This is somewhat ex-pected because as EM invites more pseudo in-domain pairs in later iterations, it sharpens theestimates of P (D1| e?,?f), making pseudo out-domain pairs tend to 0.0.
Table 4 shows the per-centage of entries with P (D1| e?,?f) < 0.01 atevery iteration, e.g., 34.52% at the fifth iteration.This induced schism in Cmixdiminishes the dif-ference between the relevance scores for certainsentence pairs, limiting the ability of the latentphrase-based models to further discriminate in thegray zone.Entries P (D1|?f, e?)
< 0.01Iter.
1 22.82%Iter.
2 27.06%Iter.
3 30.07%Iter.
4 32.47%Iter.
5 34.52%Table 4: Phrase analyses.Finally, to give a sense of the improvementin translation, we (randomly) select cases wherethe systems produce different translations andpresent some of them in Table 5.
These ex-amples are indeed illuminating, e.g., ?can repro-duce signs of audio?/?can play signals audio?,?password teacher?/?password master?, reveal-ing thoroughly the benefit derived from adaptingthe phrase models from being domain-confused tobeing domain-focused.
Table 6 presents phrase ta-ble entries, i.e., pt(e | f) and pt(e | f,D1), for the?can reproduce signs of audio?/?can play signalsaudio?
example.6 Fully adapted translation modelThe preceding experiments reveal that adaptingthe phrase tables significantly improves transla-tion performance.
Now we also adapt the lexicalse?nales reproducirEntries signals signs play reproduceBaseline 0.29 0.36 0.15 0.20Iter.
1 0.36 0.23 0.29 0.16Iter.
2 0.37 0.19 0.32 0.17Iter.
3 0.37 0.17 0.34 0.16Iter.
4 0.37 0.16 0.36 0.16Iter.
5 0.37 0.15 0.37 0.16Table 6: Phrase entry examples.and reordering components.
The result is a fullyadapted, domain-focused, phrase-based system.Briefly, the lexical weights provide smooth es-timates for the phrase pair based on word trans-lation scores P (e | f) between pairs of words?e, f?, i.e., P (e | f) =c(e,f)?ec(e,f)(Koehn etal., 2003).
Our latent domain-focused lexicalweights, on the other hand, are estimated ac-cording to P (e | f, D1), i.e., P (e | f, D1) =P (e | f)P (D1| e, f)?fP (e | f)P (D1| e, f).The lexicalized reordering models with orien-tation variable O, P (O | e?,?f), model how likelya phrase ?e?,?f?
directly follows a previous phrase(monotone), swaps positions with it (swap), oris not adjacent to it (discontinous) (Koehn et al.,2005).
We make these domain-focused:P (O | e?,?f,D1) =P (O | e?,?f)P (D1| O, e?,?f)?OP (O | e?,?f)P (D1| O, e?,?f)(11)Estimating P (D1| O, e?,?f) and P (D1| e, f) issimilar to estimating P (D1| e?,?f) and hinges onthe estimates of P (D1| e, f) during EM.The baseline for the following experiments is astandard state-of-the-art phrase-based system, in-cluding two bi-directional phrase-based transla-tion features, two bi-directional lexical weights,six lexicalized reordering features, as well as thepenalties for word, phrase and distortion.
We de-velop three kinds of domain-adapted systems thatare different at their adaptation level to fit the task.The first (Sys.
1) adapts only the phrase-basedmodels, using the same lexical weights, lexical-ized reordering models and other penalties as thebaseline.
The second (Sys.
2) adapts also the lex-ical weights, fixing all other features as the base-line.
The third (Sys.
3) adapts both the phrase-based models, lexical weights and lexicalized re-572Translation ExamplesInput El reproductor puede reproducir se?nales de audio grabadas en mix-mode cd, cd-g, cd-extra y cd text.Reference The player can play back audio signals recorded in mix-mode cd, cd-g, cd-extra and cd text.Baseline The player can reproduce signs of audio recorded in mix-mode cd, cd-g, cd-extra and cd text.Our System The player can play signals audio recorded in mix-mode cd, cd-g, cd-extra and cd text.Input Se puede crear un archivo autodescodificable cuando el archivo codificado se abre con la contrase?na maestra.Reference A self-decrypting file can be created when the encrypted file is opened with the master password.Baseline To create an file autodescodificable when the file codified commenced with the password teacher.Our System You can create an archive autodescodificable when the file codified opens with the password master.Input Repite todas las pistas (?unicamente cds de v?
?deo sin pbc)Reference Repeat all tracks (non-pbc video cds only)Baseline Repeated all avenues (only cds video without pbc)Our System Repeated all the tracks (only cds video without pbc)Table 5: Translation examples yielded by a domain-confused phrase-based system (the baseline) and adomain-focused phrase-based system (our system).ordering models15, fixing other penalties as thebaseline.MetricSystem Avg ?
p-valueConsumer and Industrial Electronics(In-domain: 109K pairs; Dev: 982 pairs; Test: 984 pairs)BLEUBaseline 22.9 ?
?Sys.
1 23.4 +0.5 0.008Sys.
2 23.9 +1.0 0.0001Sys.
3 24.0 +1.1 0.0001METEORBaseline30.0?
?Sys.
1 30.4 +0.4 0.0001Sys.
2 30.8 +0.8 0.0001Sys.
3 30.9 +0.9 0.0001TERBaseline 59.5 ?
?Sys.
1 58.8 -0.7 0.0001Sys.
2 58.0 -1.5 0.0001Sys.
3 57.9 -1.6 0.0001Table 7: Metric scores for the systems, which areaverages over multiple runs.Table 7 presents results for training data sizeof 4M parallel sentences.
It shows that the fullydomain-focused system (Sys.
3) significantly im-proves over the baseline.
The table also showsthat the latent domain-focused phrase-based mod-els and lexical weights are crucial for the im-proved performance, whereas adapting the re-ordering models makes a far smaller contribution.Finally we also apply our approach to other15We run three EM iterations to train our invitation frame-work, and then use the parameter estimates for P (D1| e?,?f),P (D1| e, f) and P (D1| O, e?,?f) to train these domain-focused features.
We adopt this training setting for all otherdifferent tasks in the sequel.tasks where the relation between their in-domaindata and the mix-domain data varies substantially.Table 8 presents their in-domain, tuning and testdata in detail, as well as the translation resultsover them.
It shows that the fully domain-focusedsystems consistently and significantly improve thetranslation accuracy for all the tasks.7 Combining multiple modelsFinally, we proceed further to test our latentdomain-focused phrase-based translation modelon standard domain adaptation.
We conduct ex-periments on the task ?Professional & BusinessServices?
as an example.16For standard adap-tation we follow (Koehn and Schroeder, 2007)where we pass multiple phrase tables directly tothe Moses decoder and tune them together.
Forbaseline we combine the standard phrase-basedsystem trained on Cmixwith the one trained onthe in-domain data Cin.
We also combine our la-tent domain-focused phrase-based system with theone trained on Cin.
Table 9 presents the resultsshowing that combining our domain-focused sys-tem adapted from Cmixwith the in-domain modeloutperforms the baseline.16We choose this task for additional experiments becauseit has very small in-domain data (23K).
This is supposedto make adaptation difficult because of the robust large-scalesystems trained on Cmix.573MetricSystem Avg ?
p-valueProfessional & Business Services(In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs)BLEUBaseline 22.0 ?
?Our System 23.1 +1.1 0.0001METEORBaseline 30.8 ?
?Our System 31.4 +0.6 0.0001TERBaseline 58.0 ?
?Our System 56.6 -1.4 0.0001Financials(In-domain: 31K pairs; Dev: 1, 000 pairs; Test: 1, 000 pairs)BLEUBaseline 31.1 ?
?Our System 31.8 +0.7 0.0001METEORBaseline 36.3 ?
?Our System 36.6 +0.3 0.0001TERBaseline 48.8 ?
?Our System 48.3 -0.5 0.0001Computer Hardware(In-domain: 52K pairs; Dev: 1, 021 pairs; Test: 1, 054 pairs)BLEUBaseline 24.6 ?
?Our System 25.3 +0.7 0.0001METEORBaseline 32.4 ?
?Our System 33.1 +0.7 0.0001TERBaseline 56.4 ?
?Our System 55.0 -1.4 0.0001Computer Software(In-domain: 65K pairs; Dev: 1, 100 pairs; Test: 1, 000 pairs)BLEUBaseline 27.4 ?
?Our System 28.3 +0.9 0.0001METEORBaseline 34.0 ?
?Our System 34.7 +0.7 0.0001TERBaseline 51.7 ?
?Our System 50.6 -1.1 0.0001Pharmaceuticals & Biotechnology(In-domain: 85K pairs; Dev: 920 pairs; Test: 1, 000 pairs)BLEUBaseline 31.6 ?
?Our System 32.4 +0.8 0.0001METEORBaseline 34.0 ?
?Our System 34.4 +0.4 0.0001TERBaseline 51.4 ?
?Our System 50.6 -0.8 0.0001Table 8: Metric scores for the systems, which areaverages over multiple runs.8 Related workA distantly related, but clearly complementary,line of research focuses on the role of docu-ment topics (Eidelman et al., 2012; Zhang et al.,2014; Hasler et al., 2014).
An off-the-shelf LatentDirichlet Allocation tool is usually used to inferdocument-topic distributions.
On one hand, thissetting may not require in-domain data as prior.On the other hand, it requires meta-information(e.g., document information).Part of this work (the latent sentence-relevancemodels) relates to data selection (Moore andLewis, 2010; Axelrod et al., 2011), wheresentence-relevance weights are used for hard-MetricSystem Avg ?
p-valueProfessional & Business Services(In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs)BLEUIn-domain 46.5 ?
?+ Mix-domain 46.6 ?
?+ Our system 47.9 +1.3 0.0001METEORIn-domain 39.8 ?
?+ Mix-domain 40.1 ?
?+ Our System 41.1 +1.0 0.0001TERIn-domain 38.2 ?
?+ Mix-domain 38.0 ?
?+ Our System 36.9 -1.1 0.0001Table 9: Domain adaptation experiments.
Metricscores for the systems, which are averages overmultiple runs.filtering rather than weighting.
The idea of usingsentence-relevance estimates for phrase-relevanceestimates relates to Matsoukas et al.
(2009) whoestimate the former using meta-information overdocuments as main features.
In contrast, our workovercomes the mutual dependence of sentence andphrase estimates on one another by training bothmodels in tandem.Adaptation using small in-domain data hasa different but complementary goal to anotherline of research aiming at combining a domain-adapted system with the another trained on the in-domain data (Koehn and Schroeder, 2007; Bisazzaet al., 2011; Sennrich, 2012; Razmara et al., 2012;Sennrich et al., 2013).
Our work is somewhat re-lated to, but markedly different from, phrase pairweighting (Foster et al., 2010).
Finally, our latentdomain-focused phrase-based models and invita-tion training paradigm can be seen to shift atten-tion from adaptation to making explicit the role ofdomain-focused models in SMT.9 ConclusionWe present a novel approach for in-domain fo-cused training of a phrase-based system on amix-of-domain corpus by using prior distributionsfrom a small in-domain corpus.
We derive an EMtraining algorithm for learning latent domain rel-evance models for the phrase- and sentence-levelsin tandem.
We also show how to overcome thedifficulty of lack of explicit out-domain data bybootstrapping pseudo out-domain data.In future work, we plan to explore generativeBayesian models as well as discriminative learn-ing approaches with different ways for estimat-574ing the latent domain relevance models.
We hy-pothesize that bilingual, but also monolingual, rel-evance models can be key to improved perfor-mance.AcknowledgementsWe thank Ivan Titov for stimulating discussions,and three anonymous reviewers for their com-ments on earlier versions.
The first author is sup-ported by the EXPERT (EXPloiting Empirical ap-pRoaches to Translation) Initial Training Network(ITN) of the European Union?s Seventh Frame-work Programme.
The second author is sup-ported by VICI grant nr.
277-89-002 from theNetherlands Organization for Scientific Research(NWO).
We thank TAUS for providing us withsuitable data.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domaindata selection.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,USA.
Association for Computational Linguistics.Arianna Bisazza, Nick Ruiz, and Marcello Federico.2011.
Fill-up versus interpolation methods forphrase-based smt adaptation.
In IWSLT, pages 136?143.Daniel Cer, Michel Galley, Daniel Jurafsky, andChristopher D. Manning.
2010.
Phrasal: A toolkitfor statistical machine translation with facilities forextraction and incorporation of arbitrary model fea-tures.
In Proceedings of the NAACL HLT 2010Demonstration Session, HLT-DEMO ?10, pages 9?12, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Colin Cherry and George Foster.
2012.
Batch tun-ing strategies for statistical machine translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL HLT ?12, pages 427?436, Stroudsburg, PA,USA.
Association for Computational Linguistics.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for opti-mizer instability.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies: Short Pa-pers - Volume 2, HLT ?11, pages 176?181, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Hoang Cuong and Khalil Sima?an.
2014.
La-tent domain translation models in mix-of-domainshaystack.
In Proceedings of COLING 2014, the25th International Conference on ComputationalLinguistics: Technical Papers, pages 1928?1939,Dublin, Ireland, August.
Dublin City University andAssociation for Computational Linguistics.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theem algorithm.
JOURNAL OF THE ROYAL STATIS-TICAL SOCIETY, SERIES B, 39(1):1?38.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, WMT ?11, pages 85?91, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic models for dynamic transla-tion model adaptation.
In Proceedings of the 50thAnnual Meeting of the Association for Computa-tional Linguistics: Short Papers - Volume 2, ACL?12, pages 115?119, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In Proceed-ings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?10,pages 451?459, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Guillem Gasc?o, Martha-Alicia Rocha, Germ?anSanchis-Trilles, Jes?us Andr?es-Ferrer, and FranciscoCasacuberta.
2012.
Does more data always yieldbetter translations?
In Proceedings of the 13th Con-ference of the European Chapter of the Associationfor Computational Linguistics, EACL ?12, pages152?161, Stroudsburg, PA, USA.
Association forComputational Linguistics.Eva Hasler, Phil Blunsom, Philipp Koehn, and BarryHaddow.
2014.
Dynamic topic adaptation forphrase-based mt.
In Proceedings of the 14th Con-ference of the European Chapter of the Associa-tion for Computational Linguistics, pages 328?337,Gothenburg, Sweden, April.
Association for Com-putational Linguistics.Ann Irvine, John Morgan, Marine Carpuat, Daume HalIII, and Dragos Munteanu.
2013.
Measuring ma-chine translation errors in new domains.
pages 429?440.575Philipp Koehn and Josh Schroeder.
2007.
Experi-ments in domain adaptation for statistical machinetranslation.
In Proceedings of the Second Work-shop on Statistical Machine Translation, StatMT?07, pages 224?227, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,USA.
Association for Computational Linguistics.Philipp Koehn, Amittai Axelrod, Alexandra Birch,Chris Callison-Burch, Miles Osborne, and DavidTalbot.
2005.
Edinburgh System Description forthe 2005 IWSLT Speech Translation Evaluation.
InInternational Workshop on Spoken Language Trans-lation.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Conference Pro-ceedings: the tenth Machine Translation Summit,pages 79?86, Phuket, Thailand.
AAMT, AAMT.Spyros Matsoukas, Antti-Veikko I. Rosti, and BingZhang.
2009.
Discriminative corpus weight es-timation for machine translation.
In Proceedingsof the 2009 Conference on Empirical Methods inNatural Language Processing: Volume 2 - Volume2, EMNLP ?09, pages 708?717, Stroudsburg, PA,USA.
Association for Computational Linguistics.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of the ACL 2010 Conference Short Pa-pers, ACLShort ?10, pages 220?224, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Radford M. Neal and Geoffrey E. Hinton.
1999.Learning in graphical models.
chapter A Viewof the EM Algorithm That Justifies Incremental,Sparse, and Other Variants, pages 355?368.
MITPress, Cambridge, MA, USA.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29(1):19?51, March.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Comput.
Linguist., 30(4):417?449, Decem-ber.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Adam Pauls and Dan Klein.
2011.
Faster and smallern-gram language models.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies - Volume 1, HLT ?11, pages 258?267, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.William H. Press, Saul A. Teukolsky, William T. Vet-terling, and Brian P. Flannery.
1992.
NumericalRecipes in C (2Nd Ed.
): The Art of Scientific Com-puting.
Cambridge University Press, New York,NY, USA.Majid Razmara, George Foster, Baskaran Sankaran,and Anoop Sarkar.
2012.
Mixing multiple trans-lation models in statistical machine translation.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics: Long Papers- Volume 1, ACL ?12, pages 940?949, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Rico Sennrich, Holger Schwenk, and Walid Aransa.2013.
A multi-domain translation model frame-work for statistical machine translation.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 832?840, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.Rico Sennrich.
2012.
Perplexity minimization fortranslation model domain adaptation in statisticalmachine translation.
In Proceedings of the 13thConference of the European Chapter of the Asso-ciation for Computational Linguistics, EACL ?12,pages 539?549, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micci-ulla, and J. Makhoul.
2006.
A study of translationedit rate with targeted human annotation.
In Pro-ceedings of Association for Machine Translation inthe Americas, pages 223?231.Min Zhang, Xinyan Xiao, Deyi Xiong, and Qun Liu.2014.
Topic-based dissimilarity and sensitivitymodels for translation rule selection.
Journal of Ar-tificial Intelligence Research, 50(1):1?30.576
