Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440?447,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsBiographies, Bollywood, Boom-boxes and Blenders:Domain Adaptation for Sentiment ClassificationJohn Blitzer Mark DredzeDepartment of Computer and Information ScienceUniversity of Pennsylvania{blitzer|mdredze|pereria@cis.upenn.edu}Fernando PereiraAbstractAutomatic sentiment classification has beenextensively studied and applied in recentyears.
However, sentiment is expressed dif-ferently in different domains, and annotatingcorpora for every possible domain of interestis impractical.
We investigate domain adap-tation for sentiment classifiers, focusing ononline reviews for different types of prod-ucts.
First, we extend to sentiment classifi-cation the recently-proposed structural cor-respondence learning (SCL) algorithm, re-ducing the relative error due to adaptationbetween domains by an average of 30% overthe original SCL algorithm and 46% overa supervised baseline.
Second, we identifya measure of domain similarity that corre-lates well with the potential for adaptationof a classifier from one domain to another.This measure could for instance be used toselect a small set of domains to annotatewhose trained classifiers would transfer wellto many other domains.1 IntroductionSentiment detection and classification has receivedconsiderable attention recently (Pang et al, 2002;Turney, 2002; Goldberg and Zhu, 2004).
Whilemovie reviews have been the most studied domain,sentiment analysis has extended to a number ofnew domains, ranging from stock message boardsto congressional floor debates (Das and Chen, 2001;Thomas et al, 2006).
Research results have beendeployed industrially in systems that gauge marketreaction and summarize opinion from Web pages,discussion boards, and blogs.With such widely-varying domains, researchersand engineers who build sentiment classificationsystems need to collect and curate data for each newdomain they encounter.
Even in the case of marketanalysis, if automatic sentiment classification wereto be used across a wide range of domains, the ef-fort to annotate corpora for each domain may be-come prohibitive, especially since product featureschange over time.
We envision a scenario in whichdevelopers annotate corpora for a small number ofdomains, train classifiers on those corpora, and thenapply them to other similar corpora.
However, thisapproach raises two important questions.
First, itis well known that trained classifiers lose accuracywhen the test data distribution is significantly differ-ent from the training data distribution 1.
Second, it isnot clear which notion of domain similarity shouldbe used to select domains to annotate that would begood proxies for many other domains.We propose solutions to these two questions andevaluate them on a corpus of reviews for four differ-ent types of products from Amazon: books, DVDs,electronics, and kitchen appliances2.
First, we showhow to extend the recently proposed structural cor-1For surveys of recent research on domain adaptation, seethe ICML 2006 Workshop on Structural Knowledge Transferfor Machine Learning (http://gameairesearch.uta.edu/) and the NIPS 2006 Workshop on Learning when testand training inputs have different distribution (http://ida.first.fraunhofer.de/projects/different06/)2The dataset will be made available by the authors at publi-cation time.440respondence learning (SCL) domain adaptation al-gorithm (Blitzer et al, 2006) for use in sentimentclassification.
A key step in SCL is the selection ofpivot features that are used to link the source and tar-get domains.
We suggest selecting pivots based notonly on their common frequency but also accordingto their mutual information with the source labels.For data as diverse as product reviews, SCL cansometimes misalign features, resulting in degrada-tion when we adapt between domains.
In our secondextension we show how to correct misalignments us-ing a very small number of labeled instances.Second, we evaluate the A-distance (Ben-Davidet al, 2006) between domains as measure of the lossdue to adaptation from one to the other.
The A-distance can be measured from unlabeled data, and itwas designed to take into account only divergenceswhich affect classification accuracy.
We show that itcorrelates well with adaptation loss, indicating thatwe can use the A-distance to select a subset of do-mains to label as sources.In the next section we briefly review SCL and in-troduce our new pivot selection method.
Section 3describes datasets and experimental method.
Sec-tion 4 gives results for SCL and the mutual informa-tion method for selecting pivot features.
Section 5shows how to correct feature misalignments using asmall amount of labeled target domain data.
Sec-tion 6 motivates the A-distance and shows that itcorrelates well with adaptability.
We discuss relatedwork in Section 7 and conclude in Section 8.2 Structural Correspondence LearningBefore reviewing SCL, we give a brief illustrativeexample.
Suppose that we are adapting from re-views of computers to reviews of cell phones.
Whilemany of the features of a good cell phone review arethe same as a computer review ?
the words ?excel-lent?
and ?awful?
for example ?
many words are to-tally new, like ?reception?.
At the same time, manyfeatures which were useful for computers, such as?dual-core?
are no longer useful for cell phones.Our key intuition is that even when ?good-qualityreception?
and ?fast dual-core?
are completely dis-tinct for each domain, if they both have high correla-tion with ?excellent?
and low correlation with ?aw-ful?
on unlabeled data, then we can tentatively alignthem.
After learning a classifier for computer re-views, when we see a cell-phone feature like ?good-quality reception?, we know it should behave in aroughly similar manner to ?fast dual-core?.2.1 Algorithm OverviewGiven labeled data from a source domain and un-labeled data from both source and target domains,SCL first chooses a set ofm pivot features which oc-cur frequently in both domains.
Then, it models thecorrelations between the pivot features and all otherfeatures by training linear pivot predictors to predictoccurrences of each pivot in the unlabeled data fromboth domains (Ando and Zhang, 2005; Blitzer et al,2006).
The `th pivot predictor is characterized byits weight vector w`; positive entries in that weightvector mean that a non-pivot feature (like ?fast dual-core?)
is highly correlated with the correspondingpivot (like ?excellent?
).The pivot predictor column weight vectors can bearranged into a matrix W = [w`]n`=1.
Let ?
?
Rk?dbe the top k left singular vectors of W (here d indi-cates the total number of features).
These vectors arethe principal predictors for our weight space.
If wechose our pivot features well, then we expect theseprincipal predictors to discriminate among positiveand negative words in both domains.At training and test time, suppose we observe afeature vector x.
We apply the projection ?x to ob-tain k new real-valued features.
Now we learn apredictor for the augmented instance ?x, ?x?.
If ?contains meaningful correspondences, then the pre-dictor which uses ?
will perform well in both sourceand target domains.2.2 Selecting Pivots with Mutual InformationThe efficacy of SCL depends on the choice of pivotfeatures.
For the part of speech tagging problemstudied by Blitzer et al (2006), frequently-occurringwords in both domains were good choices, sincethey often correspond to function words such asprepositions and determiners, which are good indi-cators of parts of speech.
This is not the case forsentiment classification, however.
Therefore, we re-quire that pivot features also be good predictors ofthe source label.
Among those features, we thenchoose the ones with highest mutual information tothe source label.
Table 1 shows the set-symmetric441SCL, not SCL-MI SCL-MI, not SCLbook one <num> so all a must a wonderful loved itvery about they like weak don?t waste awfulgood when highly recommended and easyTable 1: Top pivots selected by SCL, but not SCL-MI (left) and vice-versa (right)differences between the two methods for pivot selec-tion when adapting a classifier from books to kitchenappliances.
We refer throughout the rest of this workto our method for selecting pivots as SCL-MI.3 Dataset and BaselineWe constructed a new dataset for sentiment domainadaptation by selecting Amazon product reviews forfour different product types: books, DVDs, electron-ics and kitchen appliances.
Each review consists ofa rating (0-5 stars), a reviewer name and location,a product name, a review title and date, and the re-view text.
Reviews with rating > 3 were labeledpositive, those with rating < 3 were labeled neg-ative, and the rest discarded because their polaritywas ambiguous.
After this conversion, we had 1000positive and 1000 negative examples for each do-main, the same balanced composition as the polaritydataset (Pang et al, 2002).
In addition to the labeleddata, we included between 3685 (DVDs) and 5945(kitchen) instances of unlabeled data.
The size of theunlabeled data was limited primarily by the numberof reviews we could crawl and download from theAmazon website.
Since we were able to obtain la-bels for all of the reviews, we also ensured that theywere balanced between positive and negative exam-ples, as well.While the polarity dataset is a popular choice inthe literature, we were unable to use it for our task.Our method requires many unlabeled reviews anddespite a large number of IMDB reviews availableonline, the extensive curation requirements madepreparing a large amount of data difficult 3.For classification, we use linear predictors on un-igram and bigram features, trained to minimize theHuber loss with stochastic gradient descent (Zhang,3For a description of the construction of the polaritydataset, see http://www.cs.cornell.edu/people/pabo/movie-review-data/.2004).
On the polarity dataset, this model matchesthe results reported by Pang et al (2002).
When wereport results with SCL and SCL-MI, we require thatpivots occur in more than five documents in each do-main.
We set k, the number of singular vectors of theweight matrix, to 50.4 Experiments with SCL and SCL-MIEach labeled dataset was split into a training set of1600 instances and a test set of 400 instances.
Allthe experiments use a classifier trained on the train-ing set of one domain and tested on the test set ofa possibly different domain.
The baseline is a lin-ear classifier trained without adaptation, while thegold standard is an in-domain classifier trained onthe same domain as it is tested.Figure 1 gives accuracies for all pairs of domainadaptation.
The domains are ordered clockwisefrom the top left: books, DVDs, electronics, andkitchen.
For each set of bars, the first letter is thesource domain and the second letter is the targetdomain.
The thick horizontal bars are the accura-cies of the in-domain classifiers for these domains.Thus the first set of bars shows that the baselineachieves 72.8% accuracy adapting from DVDs tobooks.
SCL-MI achieves 79.7% and the in-domaingold standard is 80.4%.
We say that the adaptationloss for the baseline model is 7.6% and the adapta-tion loss for the SCL-MImodel is 0.7%.
The relativereduction in error due to adaptation of SCL-MI forthis test is 90.8%.We can observe from these results that there is arough grouping of our domains.
Books and DVDsare similar, as are kitchen appliances and electron-ics, but the two groups are different from one an-other.
Adapting classifiers from books to DVDs, forinstance, is easier than adapting them from booksto kitchen appliances.
We note that when transfer-ring from kitchen to electronics, SCL-MI actuallyoutperforms the in-domain classifier.
This is possi-ble since the unlabeled data may contain informationthat the in-domain classifier does not have access to.At the beginning of Section 2 we gave exam-ples of how features can change behavior across do-mains.
The first type of behavior is when predictivefeatures from the source domain are not predictiveor do not appear in the target domain.
The second is442657075808590D->B E->B K->B B->D E->D K->Dbaseline SCL SCL-MIbooks72.8 76.879.770.7 75.4 75.4 70.9 66.1 68.680.4 82.477.2 74.0 75.8 70.6 74.3 76.2 72.7 75.4 76.9dvd657075808590B->E D->E K->E B->K D->K E->Kelectronics kitchen70.8 77.5 75.9 73.0 74.1 74.182.7 83.7 86.884.487.774.5 78.7 78.9 74.079.481.4 84.0 84.4 85.9Figure 1: Accuracy results for domain adaptation between all pairs using SCL and SCL-MI.
Thick blacklines are the accuracies of in-domain classifiers.domain\polarity negative positivebooks plot <num> pages predictable reader grisham engagingreading this page <num> must read fascinatingkitchen the plastic poorly designed excellent product espressoleaking awkward to defective are perfect years now a breezeTable 2: Correspondences discovered by SCL for books and kitchen appliances.
The top row shows featuresthat only appear in books and the bottom features that only appear in kitchen appliances.
The left and rightcolumns show negative and positive features in correspondence, respectively.when predictive features from the target domain donot appear in the source domain.
To show how SCLdeals with those domain mismatches, we look at theadaptation from book reviews to reviews of kitchenappliances.
We selected the top 1000 most infor-mative features in both domains.
In both cases, be-tween 85 and 90% of the informative features fromone domain were not among the most informativeof the other domain4.
SCL addresses both of theseissues simultaneously by aligning features from thetwo domains.4There is a third type, features which are positive in one do-main but negative in another, but they appear very infrequentlyin our datasets.Table 2 illustrates one row of the projection ma-trix ?
for adapting from books to kitchen appliances;the features on each row appear only in the corre-sponding domain.
A supervised classifier trained onbook reviews cannot assign weight to the kitchenfeatures in the second row of table 2.
In con-trast, SCL assigns weight to these features indirectlythrough the projection matrix.
When we observethe feature ?predictable?
with a negative book re-view, we update parameters corresponding to theentire projection, including the kitchen-specific fea-tures ?poorly designed?
and ?awkward to?.While some rows of the projection matrix ?
are443useful for classification, SCL can also misalign fea-tures.
This causes problems when a projection isdiscriminative in the source domain but not in thetarget.
This is the case for adapting from kitchenappliances to books.
Since the book domain isquite broad, many projections in books model topicdistinctions such as between religious and politicalbooks.
These projections, which are uninforma-tive as to the target label, are put into correspon-dence with the fewer discriminating projections inthe much narrower kitchen domain.
When we adaptfrom kitchen to books, we assign weight to these un-informative projections, degrading target classifica-tion accuracy.5 Correcting MisalignmentsWe now show how to use a small amount of targetdomain labeled data to learn to ignore misalignedprojections from SCL-MI.
Using the notation ofAndo and Zhang (2005), we can write the supervisedtraining objective of SCL on the source domain asminw,v?iL(w?xi + v?
?xi, yi)+ ?||w||2 + ?||v||2 ,where y is the label.
The weight vector w ?
Rdweighs the original features, while v ?
Rk weighsthe projected features.
Ando and Zhang (2005) andBlitzer et al (2006) suggest ?
= 10?4, ?
= 0, whichwe have used in our results so far.Suppose now that we have trained source modelweight vectors ws and vs. A small amount of tar-get domain data is probably insufficient to signif-icantly change w, but we can correct v, which ismuch smaller.
We augment each labeled target in-stance xj with the label assigned by the source do-main classifier (Florian et al, 2004; Blitzer et al,2006).
Then we solveminw,v?j L (w?xj + v?
?xj , yj) + ?||w||2+?||v ?
vs||2 .Since we don?t want to deviate significantly from thesource parameters, we set ?
= ?
= 10?1.Figure 2 shows the corrected SCL-MI model us-ing 50 target domain labeled instances.
We chosethis number since we believe it to be a reasonableamount for a single engineer to label with minimaleffort.
For reasons of space, for each target domaindom \ model base base scl scl-mi scl-mi+targ +targbooks 8.9 9.0 7.4 5.8 4.4dvd 8.9 8.9 7.8 6.1 5.3electron 8.3 8.5 6.0 5.5 4.8kitchen 10.2 9.9 7.0 5.6 5.1average 9.1 9.1 7.1 5.8 4.9Table 3: For each domain, we show the loss due to transferfor each method, averaged over all domains.
The bottom rowshows the average loss over all runs.we show adaptation from only the two domains onwhich SCL-MI performed the worst relative to thesupervised baseline.
For example, the book domainshows only results from electronics and kitchen, butnot DVDs.
As a baseline, we used the label of thesource domain classifier as a feature in the target, butdid not use any SCL features.
We note that the base-line is very close to just using the source domainclassifier, because with only 50 target domain in-stances we do not have enough data to relearn all ofthe parameters inw.
As we can see, though, relearn-ing the 50 parameters in v is quite helpful.
The cor-rected model always improves over the baseline forevery possible transfer, including those not shown inthe figure.The idea of using the regularizer of a linear modelto encourage the target parameters to be close to thesource parameters has been used previously in do-main adaptation.
In particular, Chelba and Acero(2004) showed how this technique can be effectivefor capitalization adaptation.
The major differencebetween our approach and theirs is that we only pe-nalize deviation from the source parameters for theweights v of projected features, while they workwith the weights of the original features only.
Forour small amount of labeled target data, attemptingto penalize w using ws performed no better thanour baseline.
Because we only need to learn to ig-nore projections that misalign features, we can makemuch better use of our labeled data by adapting only50 parameters, rather than 200,000.Table 3 summarizes the results of sections 4 and5.
Structural correspondence learning reduces theerror due to transfer by 21%.
Choosing pivots bymutual information allows us to further reduce theerror to 36%.
Finally, by adding 50 instances of tar-get domain data and using this to correct the mis-aligned projections, we achieve an average relative444657075808590E->B K->B B->D K->D B->E D->E B->K E->Kbase+50-targ SCL-MI+50-targbooks kitchen70.9 76.0 70.7 76.878.5 72.780.4 87.776.6 70.8 76.6 73.0 77.9 74.380.7 84.3dvd electronics82.4 84.473.285.9Figure 2: Accuracy results for domain adaptation with 50 labeled target domain instances.reduction in error of 46%.6 Measuring AdaptabilitySections 2-5 focused on how to adapt to a target do-main when you had a labeled source dataset.
Wenow take a step back to look at the problem of se-lecting source domain data to label.
We study a set-ting where an engineer knows roughly her domainsof interest but does not have any labeled data yet.
Inthat case, she can ask the question ?Which sourcesshould I label to obtain the best performance overall my domains??
On our product domains, for ex-ample, if we are interested in classifying reviewsof kitchen appliances, we know from sections 4-5that it would be foolish to label reviews of books orDVDs rather than electronics.
Here we show how toselect source domains using only unlabeled data andthe SCL representation.6.1 The A-distanceWe propose to measure domain adaptability by us-ing the divergence of two domains after the SCLprojection.
We can characterize domains by theirinduced distributions on instance space: the moredifferent the domains, the more divergent the distri-butions.
Here we make use of the A-distance (Ben-David et al, 2006).
The key intuition behind theA-distance is that while two domains can differ inarbitrary ways, we are only interested in the differ-ences that affect classification accuracy.Let A be the family of subsets of Rk correspond-ing to characteristic functions of linear classifiers(sets on which a linear classifier returns positivevalue).
Then theA distance between two probabilitydistributions isdA(D,D?)
= 2 supA?A|PrD [A] ?
PrD?
[A]| .That is, we find the subset in A on which the distri-butions differ the most in the L1 sense.
Ben-Davidet al (2006) show that computing the A-distance fora finite sample is exactly the problem of minimiz-ing the empirical risk of a classifier that discrimi-nates between instances drawn fromD and instancesdrawn from D?.
This is convenient for us, since it al-lows us to use classification machinery to computethe A-distance.6.2 Unlabeled Adaptability MeasurementsWe follow Ben-David et al (2006) and use the Hu-ber loss as a proxy for the A-distance.
Our proce-dure is as follows: Given two domains, we computethe SCL representation.
Then we create a data setwhere each instance ?x is labeled with the identityof the domain from which it came and train a linearclassifier.
For each pair of domains we compute theempirical average per-instance Huber loss, subtractit from 1, and multiply the result by 100.
We referto this quantity as the proxy A-distance.
When it is100, the two domains are completely distinct.
Whenit is 0, the two domains are indistinguishable using alinear classifier.Figure 3 is a correlation plot between the proxyA-distance and the adaptation error.
Suppose wewanted to label two domains out of the four in such a4450246810121460 65 70 75 80 85 90 95 100Proxy A-distanceAdaptation Loss EK BD DEDK BE, BKFigure 3: The proxy A-distance between each do-main pair plotted against the average adaptation lossof as measured by our baseline system.
Each pair ofdomains is labeled by their first letters: EK indicatesthe pair electronics and kitchen.way as to minimize our error on all the domains.
Us-ing the proxy A-distance as a criterion, we observethat we would choose one domain from either booksor DVDs, but not both, since then we would not beable to adequately cover electronics or kitchen appli-ances.
Similarly we would also choose one domainfrom either electronics or kitchen appliances, but notboth.7 Related WorkSentiment classification has advanced considerablysince the work of Pang et al (2002), which we useas our baseline.
Thomas et al (2006) use discoursestructure present in congressional records to performmore accurate sentiment classification.
Pang andLee (2005) treat sentiment analysis as an ordinalranking problem.
In our work we only show im-provement for the basic model, but all of these newtechniques also make use of lexical features.
Thuswe believe that our adaptation methods could be alsoapplied to those more refined models.While work on domain adaptation for senti-ment classifiers is sparse, it is worth noting thatother researchers have investigated unsupervisedand semisupervised methods for domain adaptation.The work most similar in spirit to ours that of Tur-ney (2002).
He used the difference in mutual in-formation with two human-selected features (thewords ?excellent?
and ?poor?)
to score features ina completely unsupervised manner.
Then he clas-sified documents according to various functions ofthese mutual information scores.
We stress that ourmethod improves a supervised baseline.
While wedo not have a direct comparison, we note that Tur-ney (2002) performs worse on movie reviews thanon his other datasets, the same type of data as thepolarity dataset.We also note the work of Aue and Gamon (2005),who performed a number of empirical tests on do-main adaptation of sentiment classifiers.
Most ofthese tests were unsuccessful.
We briefly note theirresults on combining a number of source domains.They observed that source domains closer to the tar-get helped more.
In preliminary experiments weconfirmed these results.
Adding more labeled dataalways helps, but diversifying training data does not.When classifying kitchen appliances, for any fixedamount of labeled data, it is always better to drawfrom electronics as a source than use some combi-nation of all three other domains.Domain adaptation alone is a generally well-studied area, and we cannot possibly hope to coverall of it here.
As we noted in Section 5, we areable to significantly outperform basic structural cor-respondence learning (Blitzer et al, 2006).
We alsonote that while Florian et al (2004) and Blitzer et al(2006) observe that including the label of a sourceclassifier as a feature on small amounts of target datatends to improve over using either the source aloneor the target alne, we did not observe that for ourdata.
We believe the most important reason for thisis that they explore structured prediction problems,where labels of surrounding words from the sourceclassifier may be very informative, even if the cur-rent label is not.
In contrast our simple binary pre-diction problem does not exhibit such behavior.
Thismay also be the reason that the model of Chelba andAcero (2004) did not aid in adaptation.Finally we note that while Blitzer et al (2006) didcombine SCL with labeled target domain data, theyonly compared using the label of SCL or non-SCLsource classifiers as features, following the work ofFlorian et al (2004).
By only adapting the SCL-related part of the weight vector v, we are able tomake better use of our small amount of unlabeleddata than these previous techniques.4468 ConclusionSentiment classification has seen a great deal of at-tention.
Its application to many different domainsof discourse makes it an ideal candidate for domainadaptation.
This work addressed two importantquestions of domain adaptation.
First, we showedthat for a given source and target domain, we cansignificantly improve for sentiment classification thestructural correspondence learning model of Blitzeret al (2006).
We chose pivot features using not onlycommon frequency among domains but also mutualinformation with the source labels.
We also showedhow to correct structural correspondence misalign-ments by using a small amount of labeled target do-main data.Second, we provided a method for selecting thosesource domains most likely to adapt well to giventarget domains.
The unsupervised A-distance mea-sure of divergence between domains correlates wellwith loss due to adaptation.
Thus we can use the A-distance to select source domains to label which willgive low target domain error.In the future, we wish to include some of the morerecent advances in sentiment classification, as wellas addressing the more realistic problem of rank-ing.
We are also actively searching for a larger andmore varied set of domains on which to test our tech-niques.AcknowledgementsWe thank Nikhil Dinesh for helpful advice through-out the course of this work.
This material is basedupon work partially supported by the Defense Ad-vanced Research Projects Agency (DARPA) un-der Contract No.
NBCHD03001.
Any opinions,findings, and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of DARPA orthe Department of Interior-National BusinessCenter(DOI-NBC).ReferencesRie Ando and Tong Zhang.
2005.
A framework forlearning predictive structures from multiple tasks andunlabeled data.
JMLR, 6:1817?1853.Anthony Aue and Michael Gamon.
2005.
Customiz-ing sentiment classifiers to new domains: a case study.http://research.microsoft.com/ anthaue/.Shai Ben-David, John Blitzer, Koby Crammer, and Fer-nando Pereira.
2006.
Analysis of representations fordomain adaptation.
In Neural Information ProcessingSystems (NIPS).John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Empirical Methods in Natural Lan-guage Processing (EMNLP).Ciprian Chelba and Alex Acero.
2004.
Adaptation ofmaximum entropy capitalizer: Little data can help alot.
In EMNLP.Sanjiv Das and Mike Chen.
2001.
Yahoo!
for ama-zon: Extracting market sentiment from stock messageboards.
In Proceedings of Athe Asia Pacific FinanceAssociation Annual Conference.R.
Florian, H. Hassan, A.Ittycheriah, H. Jing, N. Kamb-hatla, X. Luo, N. Nicolov, and S. Roukos.
2004.
Astatistical model for multilingual entity detection andtracking.
In of HLT-NAACL.Andrew Goldberg and Xiaojin Zhu.
2004.
Seeingstars when there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization.
InHLT-NAACL 2006 Workshop on Textgraphs: Graph-based Algorithms for Natural Language Processing.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of Associationfor Computational Linguistics.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification using ma-chine learning techniques.
In Proceedings of Empiri-cal Methods in Natural Language Processing.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition from con-gressional floor-debate transcripts.
In Empirical Meth-ods in Natural Language Processing (EMNLP).Peter Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings of Association forComputational Linguistics.Tong Zhang.
2004.
Solving large scale linear predic-tion problems using stochastic gradient descent al-gorithms.
In International Conference on MachineLearning (ICML).447
