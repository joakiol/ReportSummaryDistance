A Spoken Language Interface to a Virtual Reality System (Video)Stephanie S. Everett, Kenneth WauchopeNavy Ctr.
for Appl ied Research in AINaval Research LaboratoryWashington,  DC 20375, USAeveret t  I wauchope?aic ,  n r l .
navy .
mi lManuel A. P f i rezDept.
Electr ical  & Computer  Engineer ingRecinto Univers i tar io de Mayaguez,  UPRMayaguez,  PR  00681mperez?exodo,  upr .
c lu .
edu1 Descr ip t ion  o f  video tapeFormat: VHS.
Duration: 9 min.
15 sec.Immersive, interactive 3D computer display sys-tems (often called virtual reality systems, or virtualenvironments) are rapidly emerging as practical op-tions for training, command and control (C2), haz-ardous operations, visualization and other applica-tions.
However, the need for improved control andnavigation techniques i well recognized (Herndonet al, 1994).
The Navy Center for Applied Re-search in Artificial Intelligence has developed NAU-TILUS (Navy AUTomated Intelligent Language Un-derstanding System), a general-purpose natural lan-guage processing system, which has previously beenintegrated with the graphical user interface of asimulation-based C2 system to illustrate the advan-tages to be gained by combining natural languageunderstanding (NLU) and direct manipulation i ahuman-computer interface (Wauchope, 1994).
Us-ing the NAUTILUS system in the interface to a vir-tual environment (VE) is a natural extension of thiswork.The purpose of the project documented in thisvideo was to demonstrate and explore some of thecapabilities of a NLU interface to a VE system, andto identify some of the research issues that need tobe addressed in this area.
It is important o rec-ognize that NLU is not simply speech recognition,where each individual utterance maps to a specificcommand.
In a NLU system, a given sentence mayhave different meanings depending on the context, soa logical analysis of the utterance is required to de-termine the appropriate interpretation.
This allowsus to take advantage of certain powerful inguisticproperties as described below.One major difficulty with interfaces to VE systemsis that the user's hands and eyes are occupied inthe virtual world, so standard input devices uch asmice and keyboards that require a physical supportand/or visual attention are impractical.
Joysticks,36gloves, and other manual input devices are usefulfor some types of control (pointing, manipulating ob-jects), but they are not well suited to more abstractinput functions.
Language, however, is ideally suitedto abstract manipulations; it is also the most natu-ral form of communication for humans, and does notrequire the use of one's hands or eyes.
It is especiallyuseful for controlling things that do not have a phys-ical presence in the VE, such as object scale, displaycharacteristics, and time.
It also provides a pow-erful means to access the knowledge that underliesthe VE by allowing the user to ask questions of thesystem.
Using speech output in combination withspeech recognition helps to avoid the use of textualdisplays which can be difficult to read on immer-sire presentation equipment, and which can interferewith the user's view and the "reality" of the virtualworld.The prototype system shown in this film usesoff-the-shelf speech recognition and synthesis tech-nology combined with the NAUTILUS system andVIEWER (Solan and Hill, 1993), a 3D tactical sce-nario playback system developed by NRL's TacticalElectronic Warfare Division for a separate project.Building the prototype involved the creation of anapplication-specific d tionary and lexical semanticsfor NAUTILUS, a few minor extensions to its En-glish grammar, and the development of two sets ofcode: one to translate the logical forms generated byNAUTILUS into messages for the application soft-ware, and one to interpret hese messages and in-struct VIEWER to produce the appropriate actionsor responses.The interface supports two classes of spoken input:commands and questions.
The commands allow theuser to control the playback of the simulation andits speed, as well as various display characteristics,such as viewpoint (Show me lhe lop-down/ou~-the-window view) and overlays (Display the map rings).The user can also tell the system to hide or displayindividual objects (Show the Thunderbird) or sets ofobjects (Hide all the friendly aircraft that don't havemissiles).
Commands are also used to manipulatetime (Run the simulation forward/backward; Set theclock to zero).
In addition, the user can move fromone object to another by name or by description,rather than by flying or pointing (Put me on theDoomsday; Put me on the hostile ship), or by speci-fying a particular location (Move me to 23 N, 40 E;lncrease my altitude to 4000 feet).Questions allow the user to access informationcontained in the knowledge base that underliesthe simulation.
He/she can ask for informationabout the virtual world (How many hostile shipsare there?
), or about a specific object (What is theThunderbird's heading?
; What is my viewing alti-tude?)
or set of objects (Do any of the friendlyships have emitters on board?).
The user can alsoask about the state of various aspects of the simu-lation (Is the simulation running?, What is the timeincrement ?
).NAUTILUS keeps a history of prior references andtheir denotations, which allows the use of anaphoricreference (pronouns like it or them).
It also supportsrelative clauses (All the ships that have missiles onboard), and elliptical follow-ups involving substitu-tion noun phrases (How about the Titanic?
).User feedback is handled in one of two ways:changes in the graphical display in response tocommands, and verbal answers output through thespeech synthesizer in response to questions.
In thefilm, the answers consist primarily of yes/no andshort noun phrases.
The addition of a naturallanguage generation module to generate appropri-ate verbal responses would improve feedback andsmooth the flow of the discourse.The grammar for the speech recognition moduleconsists of a vocabulary of just over 300 words anda set of about 140 rules that support he recognitionof approximately 1.2 million utterances.
In order tomake the interface flexible and easy to use, the ruleshave been designed to allow the user to phrase anutterance in a variety of ways, improving the nat-uralness of the interaction by taking advantage ofthe strong linguistic processing capabilities of NAU-TILUS.The use of natural language understanding in ahuman-computer interface can help provide the rich-ness of control required in complex interactive nvi-ronments.
NLU provides a way to control abstractfeatures uch as time without the need for manualcontrols, and enables the user to access informationin the underlying knowledge base(s) by asking ques-tions.
It also allows simulation objects to be referred37to by description, as sets, and generically rather thanjust individually.
This film shows a demonstrationof a prototype system that illustrates ome of thecapabilities of NLU in an interface to a virtual envi-ronment system.
However, it must be stressed thatthis area of HCI is still in its infancy, and there area number of research issues that will need to be ad-dressed in order to realize the full potential of thistechnology.Re ferencesKenneth P. Herndon, Andries van Dam and MichaelGleicher.
1994.
The challenges of 3D interaction.A CM SIGCHI Bulletin, 26:4, pages 36-43.Brian T. Solan and Tobin A. Hill 1993.
An appli-cation of object-oriented problem solving in elec-tronic warfare simulation.
NRL technical reportNRL/FR/570%93-9551.Kenneth Wauchope.
1994.
Eucalyptus: integrat-ing natural language input with a graphical userinterface.
NRL technical report NRL/FR/5510-94-9711.
