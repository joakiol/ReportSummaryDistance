Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 7?11,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsSemantic Parsing using Distributional Semantics and Probabilistic LogicIslam Beltagy?Katrin Erk?Raymond Mooney?
?Department of Computer Science?Department of LinguisticsThe University of Texas at AustinAustin, Texas 78712?
{beltagy,mooney}@cs.utexas.edu?katrin.erk@mail.utexas.eduAbstractWe propose a new approach to semanticparsing that is not constrained by a fixedformal ontology and purely logical infer-ence.
Instead, we use distributional se-mantics to generate only the relevant partof an on-the-fly ontology.
Sentences andthe on-the-fly ontology are represented inprobabilistic logic.
For inference, weuse probabilistic logic frameworks likeMarkov Logic Networks (MLN) and Prob-abilistic Soft Logic (PSL).
This seman-tic parsing approach is evaluated on twotasks, Textual Entitlement (RTE) and Tex-tual Similarity (STS), both accomplishedusing inference in probabilistic logic.
Ex-periments show the potential of the ap-proach.1 IntroductionSemantic Parsing is probably best defined as thetask of representing the meaning of a natural lan-guage sentence in some formal knowledge repre-sentation language that supports automated infer-ence.
A semantic parser is best defined as havingthree parts, a formal language, an ontology, and aninference mechanism.
Both the formal language(e.g.
first-order logic) and the ontology define theformal knowledge representation.
The formal lan-guage uses predicate symbols from the ontology,and the ontology provides them with meanings bydefining the relations between them.1.
A formalexpression by itself without an ontology is insuf-ficient for semantic interpretation; we call it un-interpreted logical form.
An uninterpreted logicalform is not enough as a knowledge representation1For conciseness, here we use the term ?ontology?
to referto a set of predicates as well as a knowledge base (KB) ofaxioms that defines a complex set of relationships betweenthembecause the predicate symbols do not have mean-ing in themselves, they get this meaning from theontology.
Inference is what takes a problem repre-sented in the formal knowledge representation andthe ontology and performs the target task (e.g.
tex-tual entailment, question answering, etc.
).Prior work in standard semantic parsing uses apre-defined set of predicates in a fixed ontology.However, it is difficult to construct formal ontolo-gies of properties and relations that have broadcoverage, and very difficult to do semantic parsingbased on such an ontology.
Consequently, currentsemantic parsers are mostly restricted to fairly lim-ited domains, such as querying a specific database(Kwiatkowski et al., 2013; Berant et al., 2013).We propose a semantic parser that is not re-stricted to a predefined ontology.
Instead, weuse distributional semantics to generate the neededpart of an on-the-fly ontology.
Distributional se-mantics is a statistical technique that representsthe meaning of words and phrases as distributionsover context words (Turney and Pantel, 2010; Lan-dauer and Dumais, 1997).
Distributional infor-mation can be used to predict semantic relationslike synonymy and hyponymy between words andphrases of interest (Lenci and Benotto, 2012;Kotlerman et al., 2010).
The collection of pre-dicted semantic relations is the ?on-the-fly ontol-ogy?
our semantic parser uses.
A distributionalsemantics is relatively easy to build from a largecorpus of raw text, and provides the wide cover-age that formal ontologies lack.The formal language we would like to use in thesemantic parser is first-order logic.
However, dis-tributional information is graded in nature, so theon-the-fly ontology and its predicted semantic re-lations are also graded.
This means, that standardfirst-order logic is insufficient because it is binaryby nature.
Probabilistic logic solves this problembecause it accepts weighted first order logic for-mulas.
For example, in probabilistic logic, the7synonymy relation between ?man?
and ?guy?
isrepresented by: ?x.
man(x) ?
guy(x) | w1andthe hyponymy relation between ?car?
and ?vehi-cle?
is: ?x.
car(x) ?
vehicle(x) | w2where w1and w1are some certainty measure estimated fromthe distributional semantics.For inference, we use probabilistic logicframeworks like Markov Logic Networks(MLN) (Richardson and Domingos, 2006) andProbabilistic Soft Logic (PSL) (Kimmig et al.,2012).
They are Statistical Relational Learning(SRL) techniques (Getoor and Taskar, 2007) thatcombine logical and statistical knowledge in oneuniform framework, and provide a mechanism forcoherent probabilistic inference.
We implementedthis semantic parser (Beltagy et al., 2013; Beltagyet al., 2014) and used it to perform two tasksthat require deep semantic analysis, RecognizingTextual Entailment (RTE), and Semantic TextualSimilarity (STS).The rest of the paper is organized as follows:section 2 presents background material, section3 explains the three components of the semanticparser, section 4 shows how this semantic parsercan be used for RTE and STS tasks, section 5presents the evaluation and 6 concludes.2 Background2.1 Logical SemanticsLogic-based representations of meaning have along tradition (Montague, 1970; Kamp and Reyle,1993).
They handle many complex semantic phe-nomena such as relational propositions, logicaloperators, and quantifiers; however, they can nothandle ?graded?
aspects of meaning in languagebecause they are binary by nature.
Also, the logi-cal predicates and relations do not have semanticsby themselves without an accompanying ontology,which we want to replace in our semantic parserwith distributional semantics.To map a sentence to logical form, we use Boxer(Bos, 2008), a tool for wide-coverage semanticanalysis that produces uninterpreted logical formsusing Discourse Representation Structures (Kampand Reyle, 1993).
It builds on the C&C CCGparser (Clark and Curran, 2004).2.2 Distributional SemanticsDistributional models use statistics on contextualdata from large corpora to predict semantic sim-ilarity of words and phrases (Turney and Pantel,2010; Mitchell and Lapata, 2010), based on theobservation that semantically similar words occurin similar contexts (Landauer and Dumais, 1997;Lund and Burgess, 1996).
So words can be rep-resented as vectors in high dimensional spacesgenerated from the contexts in which they occur.Distributional models capture the graded natureof meaning, but do not adequately capture log-ical structure (Grefenstette, 2013).
It is possi-ble to compute vector representations for largerphrases compositionally from their parts (Lan-dauer and Dumais, 1997; Mitchell and Lapata,2008; Mitchell and Lapata, 2010; Baroni andZamparelli, 2010; Grefenstette and Sadrzadeh,2011).
Distributional similarity is usually a mix-ture of semantic relations, but particular asymmet-ric similarity measures can, to a certain extent,predict hypernymy and lexical entailment distri-butionally (Lenci and Benotto, 2012; Kotlermanet al., 2010).2.3 Markov Logic NetworkMarkov Logic Network (MLN) (Richardson andDomingos, 2006) is a framework for probabilis-tic logic that employ weighted formulas in first-order logic to compactly encode complex undi-rected probabilistic graphical models (i.e., Markovnetworks).
Weighting the rules is a way of soft-ening them compared to hard logical constraints.MLNs define a probability distribution over possi-ble worlds, where a world?s probability increasesexponentially with the total weight of the logicalclauses that it satisfies.
A variety of inferencemethods for MLNs have been developed, however,their computational complexity is a fundamentalissue.2.4 Probabilistic Soft LogicProbabilistic Soft Logic (PSL) is another recentlyproposed framework for probabilistic logic (Kim-mig et al., 2012).
It uses logical representations tocompactly define large graphical models with con-tinuous variables, and includes methods for per-forming efficient probabilistic inference for the re-sulting models.
A key distinguishing feature ofPSL is that ground atoms have soft, continuoustruth values in the interval [0, 1] rather than bi-nary truth values as used in MLNs and most otherprobabilistic logics.
Given a set of weighted in-ference rules, and with the help of Lukasiewicz?srelaxation of the logical operators, PSL builds agraphical model defining a probability distribution8over the continuous space of values of the randomvariables in the model.
Then, PSL?s MPE infer-ence (Most Probable Explanation) finds the over-all interpretation with the maximum probabilitygiven a set of evidence.
It turns out that this op-timization problem is second-order cone program(SOCP) (Kimmig et al., 2012) and can be solvedefficiently in polynomial time.2.5 Recognizing Textual EntailmentRecognizing Textual Entailment (RTE) is the taskof determining whether one natural language text,the premise, Entails, Contradicts, or not related(Neutral) to another, the hypothesis.2.6 Semantic Textual SimilaritySemantic Textual Similarity (STS) is the task ofjudging the similarity of a pair of sentences ona scale from 1 to 5 (Agirre et al., 2012).
Goldstandard scores are averaged over multiple humanannotations and systems are evaluated using thePearson correlation between a system?s output andgold standard scores.3 ApproachA semantic parser is three components, a formallanguage, an ontology, and an inference mecha-nism.
This section explains the details of thesecomponents in our semantic parser.
It also pointsout the future work related to each part of the sys-tem.3.1 Formal Language: first-order logicNatural sentences are mapped to logical form us-ing Boxer (Bos, 2008), which maps the inputsentences into a lexically-based logical form, inwhich the predicates are words in the sentence.For example, the sentence ?A man is driving a car?in logical form is:?x, y, z. man(x) ?
agent(y, x) ?
drive(y) ?patient(y, z) ?
car(z)We call Boxer?s output alone an uninterpretedlogical form because predicates do not have mean-ing by themselves.
They still need to be connectedwith an ontology.Future work: While Boxer has wide coverage,additional linguistic phenomena like generalizedquantifiers need to be handled.3.2 Ontology: on-the-fly ontologyDistributional information is used to generate theneeded part of an on-the-fly ontology for the giveninput sentences.
It is encoded in the form ofweighted inference rules describing the seman-tic relations connecting words and phrases in theinput sentences.
For example, for sentences ?Aman is driving a car?, and ?A guy is driving avehicle?, we would like to generate rules like?x.man(x)?
guy(x) |w1indicating that ?man?and ?guy?
are synonyms with some certainty w1,and ?x.
car(x)?
vehicle(x) | w2indicating that?car?
is a hyponym of ?vehicle?
with some cer-tainty w2.
Other semantic relations can also beeasily encoded as inference rules like antonyms?x.
tall(x)?
?short(x) |w, contextonymy rela-tion ?x.
hospital(x) ?
?y.
doctor(y) | w. Fornow, we generate inference rules only as syn-onyms (Beltagy et al., 2013), but we are experi-menting with more types of semantic relations.In (Beltagy et al., 2013), we generate infer-ence rules between all pairs of words and phrases.Given two input sentences T and H , for all pairs(a, b), where a and b are words or phrases of Tand H respectively, generate an inference rule:a ?
b | w, where the rule?s weight w =sim(?
?a ,?
?b ), and sim is the cosine of the anglebetween vectors?
?a and?
?b .
Note that this simi-larity measure cannot yet distinguish relations likesynonymy and hypernymy.
Phrases are defined interms of Boxer?s output to be more than one unaryatom sharing the same variable like ?a little kid?which in logic is little(k) ?
kid(k), or two unaryatoms connected by a relation like ?a man is driv-ing?
which in logic is man(m) ?
agent(d,m) ?drive(d).
We used vector addition (Mitchell andLapata, 2010) to calculate vectors for phrases.Future Work: This can be extended in manydirections.
We are currently experimenting withasymmetric similarity functions to distinguish se-mantic relations.
We would also like to use longerphrases and other compositionality techniques asin (Baroni and Zamparelli, 2010; Grefenstette andSadrzadeh, 2011).
Also more inference rules canbe added from paraphrases collections like PPDB(Ganitkevitch et al., 2013).3.3 Inference: probabilistic logical inferenceThe last component is probabilistic logical infer-ence.
Given the logical form of the input sen-tences, and the weighted inference rules, we usethem to build a probabilistic logic program whosesolution is the answer to the target task.
A proba-bilistic logic program consists of the evidence set9E, the set of weighted first order logical expres-sions (rule base RB), and a query Q.
Inference isthe process of calculating Pr(Q|E,RB).Probabilistic logic frameworks define a proba-bility distribution over all possible worlds.
Thenumber of constants in a world depends on thenumber of the discourse entities in the Boxer out-put, plus additional constants introduced to han-dle quantification.
Mostly, all constants are com-bined with all literals, except for rudimentary typechecking.4 TasksThis section explains how we perform the RTEand STS tasks using our semantic parser.4.1 Task 1: RTE using MLNsMLNs are the probabilistic logic framework weuse for the RTE task (we do not use PSL here asit shares the problems of fuzzy logic with proba-bilistic reasoning).
The RTE?s classification prob-lem for the relation between T and H , and giventhe rule base RB generated as in 3.2, can besplit into two inference tasks.
The first is find-ing if T entails H , Pr(H|T,RB).
The secondis finding if the negation of the text ?T entails H ,Pr(H|?T,RB).
In case Pr(H|T,RB) is high,while Pr(H|?T,RB) is low, this indicates En-tails.
In case it is the other way around, this indi-cates Contradicts.
If both values are close to eachother, this means T does not affect probability ofH and that is an indication of Neutral.
We train aclassifier to map the two values to the final classi-fication decision.Future Work: One general problem withMLNs is its computational overhead especiallyfor the type of inference problems we have.
Theother problem is that MLNs, as with most otherprobabilistic logics, make the Domain ClosureAssumption (Richardson and Domingos, 2006)which means that quantifiers sometimes behave inan undesired way.4.2 Task 2: STS using PSLPSL is the probabilistic logic we use for the STStask since it has been shown to be an effectiveapproach to compute similarity between struc-tured objects.
PSL does not work ?out of thebox?
for STS, because Lukasiewicz?s equation forthe conjunction is very restrictive.
We addressedthis problem (Beltagy et al., 2014) by replacingSICK-RTE SICK-STSdist 0.60 0.65logic 0.71 0.68logic+dist 0.73 0.70Table 1: RTE accuracy and STS CorrelationLukasiewicz?s equation for the conjunction withan averaging equation, then change the optimiza-tion problem and the grounding technique accord-ingly.For each STS pair of sentences S1, S2, we runPSL twice, once where E = S1, Q = S2andanother where E = S2, Q = S1, and output thetwo scores.
The final similarity score is producedfrom a regressor trained to map the two PSL scoresto the overall similarity score.Future Work: Use a weighted average wheredifferent weights are learned for different parts ofthe sentence.5 EvaluationThe dataset used for evaluation is SICK:Sentences Involving Compositional Knowledgedataset, a task for SemEval 2014.
The initial datarelease for the competition consists of 5,000 pairsof sentences which are annotated for both RTE andSTS.
For this evaluation, we performed 10-foldcross validation on this initial data.Table 1 shows results comparing our fullapproach (logic+dist) to two baselines, adistributional-only baseline (dist) that uses vectoraddition, and a probabilistic logic-only baseline(logic) which is our semantic parser without distri-butional inference rules.
The integrated approach(logic+dist) out-performs both baselines.6 ConclusionWe presented an approach to semantic parsing thathas a wide-coverage for words and relations, anddoes not require a fixed formal ontology.
Anon-the-fly ontology of semantic relations betweenpredicates is derived from distributional informa-tion and encoded in the form of soft inference rulesin probabilistic logic.
We evaluated this approachon two task, RTE and STS, using two probabilisticlogics, MLNs and PSL respectively.
The semanticparser can be extended in different direction, es-pecially in predicting more complex semantic re-lations, and enhancing the inference mechanisms.10AcknowledgmentsThis research was supported by the DARPA DEFTprogram under AFRL grant FA8750-13-2-0026.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe author and do not necessarily reflect the viewof DARPA, DoD or the US government.
Some ex-periments were run on the Mastodon Cluster sup-ported by NSF Grant EIA-0303609.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In Proceedingsof Semantic Evaluation (SemEval-12).Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of Conference on Empirical Methods inNatural Language Processing (EMNLP-10).Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-rette, Katrin Erk, and Raymond Mooney.
2013.Montague meets Markov: Deep semantics withprobabilistic logical form.
In Proceedings of theSecond Joint Conference on Lexical and Computa-tional Semantics (*SEM-13).Islam Beltagy, Katrin Erk, and Raymond Mooney.2014.
Probabilistic soft logic for semantic textualsimilarity.
In Proceedings of Association for Com-putational Linguistics (ACL-14).Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP-13).Johan Bos.
2008.
Wide-coverage semantic analysiswith Boxer.
In Proceedings of Semantics in TextProcessing (STEP-08).Stephen Clark and James R. Curran.
2004.
Parsingthe WSJ using CCG and log-linear models.
In Pro-ceedings of Association for Computational Linguis-tics (ACL-04).Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL-HLT-13).L.
Getoor and B. Taskar, editors.
2007.
Introductionto Statistical Relational Learning.
MIT Press, Cam-bridge, MA.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-11).Edward Grefenstette.
2013.
Towards a formal distri-butional semantics: Simulating logical calculi withtensors.
In Proceedings of Second Joint Conferenceon Lexical and Computational Semantics (*SEM2013).Hans Kamp and Uwe Reyle.
1993.
From Discourse toLogic.
Kluwer.Angelika Kimmig, Stephen H. Bach, MatthiasBroecheler, Bert Huang, and Lise Getoor.
2012.A short introduction to Probabilistic Soft Logic.In Proceedings of NIPS Workshop on ProbabilisticProgramming: Foundations and Applications (NIPSWorkshop-12).Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings ofConference on Empirical Methods in Natural Lan-guage Processing (EMNLP-13).T.
K. Landauer and S. T. Dumais.
1997.
A solution toPlato?s problem: The Latent Semantic Analysis the-ory of the acquisition, induction, and representationof knowledge.
Psychological Review.Alessandro Lenci and Giulia Benotto.
2012.
Identify-ing hypernyms in distributional semantic spaces.
InProceedings of the first Joint Conference on Lexicaland Computational Semantics (*SEM-12).Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments, and Computers.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedingsof Association for Computational Linguistics (ACL-08).Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Journal ofCognitive Science.Richard Montague.
1970.
Universal grammar.
Theo-ria, 36:373?398.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning,62:107?136.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research(JAIR-10).11
