Automatic Extraction of Systematic Polysemy Using Tree-cutNor iko  TomuroDePaul  UniversitySchool of Computer  Science, Telecommunicat ions and Informat ion Systems243 S. Wabash Ave.Chicago, IL 60604tomuro@cs.depaul.eduAbst rac tThis paper describes an automatic method forextracting systematic polysemy from a hierar-chically organized semantic lexicon (WordNet).Systematic polysemy is a set of word sensesthat are related in systematic and predictableways.
Our method uses a modification of a treegeneralization technique used in (Li and Abe,1998), and generates a tree-cut, which is a listof clusters that partition a tree.
We comparethe systematic relations extracted by our auto-matic method to manually extracted WordNetcousins.1 In t roduct ionIn recent years, several on-line broad-coveragesemantic lexicons became available, includingLDOCE (Procter, 1978), WordNet (Miller,1990) and HECTOR .
(Kilgarriff, 1998a).These lexicons have been used as a domain-independent semantic resource as well as anevaluation criteria in various Natural LanguageProcessing (NLP) tasks, such as InformationRetrieval (IR), Information Extraction (IE) andWord Sense Disambiguation (WSD).However, those lexicons are rather complex.For instance, WordNet (version 1.6) contains atotal of over 120,000 words and 170,000 wordsenses, which are grouped into around 100,000synsets (synonym sets).
In addition to the size,word entries in those lexicon are often polyse-mous.
For instance, 20% of the words in Word-net have more than one sense, and the averagenumber of senses of those polysemous words isaround 3.
Also, the distinction between wordsenses tends to be ambiguous and arbitrary.For example, the following 6 senses are listedin WordNet for the noun "door":1. door  - a swinging or sliding barrier2.
door  - the space in a wall3.
door  - anything providing a means ofaccess (or escape)4. door  - a swinging or sliding barrier thatwill close off access into a car5.
door  - a house that is entered via a door6.
door  - a room that is entered via a doorBecause of the high degree of ambiguity, usingsuch complex semantic lexicons brings some se-rious problems to the performance of NLP sys-tems.
The first, obvious problem is the com-putational intractability: increased processingtime needed to disambiguate multiple possibili-ties will necessarily slow down the system.
An-other problem, which has been receiving atten-tion in the past few years, is the inaccuracy:when there is more than one sense applicable ina given context, different systems (or human in-dividuals) may select different senses as the cor-rect sense.
Indeed, recent studies in WSD showthat, when sense definitions are fine-grained,similar senses become indistinguishable to hu-man annotators and often cause disagreementon the correct tag (Ng et al, 1999; Veronis,1998; Kilgarriff, 1998b).
Also in IR and IEtasks, difference in the correct sense assignmentwill surely degrade recall and precision of thesystems.
Thus, it is apparent that, in order fora lexicon to be useful as an evaluation criteriafor NLP systems, it must represent word sensesat the level of granularity that captures humanintuition.In Lexical Semantics, everal approaches havebeen proposed which organize a lexicon basedon systematic polysemy: 1 a set of word sensesthat are related in systematic and predictableISystematic polysemy (in the sense we use in thispaper) is also referred to as regular polysemy (Apresjan,1973) or logical polyseray (Pustejovsky, 1995).20ways (e.g.
ANIMAL and MEAT meanings of theword "chicken").
2 In particular, (Buitelaar,1997, 1998) identified systematic relations thatexist between abstract semantic concepts inthe WordNet noun hierarchy, and defined aset of underspecified semantic classes that rep-resent the relations.
Then he extracted allpolysemous nouns in WordNet according tothose underspecified classes and built a lexiconcalled CORELEX.
For example, a CORELEXclass AQU (which represents a relation betweenARTIFACT and QUANTITY) contains words suchas "bottle", "bucket" and "spoon".Using the abstract semantic lasses and orga-nizing a lexicon based on systematic polysemyaddresses the two problems mentioned above inthe following ways.
For the first problem, usingthe abstract classes can reduce the size of thelexicon by combining several related senses intoone sense; thus computation becomes more effi-cient.
For the second problem, systematic poly-semy does reflect our general intuitions on wordmeanings.
Although the distinction betweensystematic vs. non-systematic relations (or re-lated vs. unrelated meanings) is sometimes un-clear, systematicity of the related senses amongwords is quite intuitive and has been well stud-ied in Lexical Semantics (for example, (Apres-jan, 1973; Cruse, 1986; Nunberg, 1995; Copes-take and Briscoe, 1995)).However, there is one critical issue still tobe addressed: the level of granularity at whichthe abstract classes are defined.
The prob-lem is that, when the granularity of the ab-stract classes is too coarse, systematic rela-tions defined at that level may not hold uni-formly at more fine-grained levels (Vossen etal., 1999).
For instance, the CORELEX classAQU mentioned above also contains a word"dose" .3 Here, the relation between the sensesof "dose" is different from that of "bottle","bucket" and "spoon", which can be labeled asCONTAINER-CONTAINERFUL relation.
We arguethat human intuitions can distinguish meanings2Note that systematic polysemy should be contrastedwith homonymy which refers to words which have morethan one unrelated sense (e.g.
FINANCIAL_INSTITUTIONand SLOPING_LAND meanings ofthe word "bank").3Senses of "dose" in WordNet are: (1) a measuredportion of medicine taken at any one time, and (2)the quantity of an active agent (substance orradiation)taken in or absorbed at any one time.ARTIFACTAIRCRAFT TOY/ \  / l \airplane helicopter ball kite puzzleFigure 1: An example thesaurus treeat this level, where differences between the sys-tematic relations are rather clear, and thereforelexicons that encode word senses at this level ofgranularity have advantages over fine-grained aswell as coarse-grained lexicons in various NLPtasks.Another issue we like to address is the waysfor extracting systematic polysemy.
Most of-ten, this procedure is done manually.
For ex-ample, the current version of WordNet (1.6)encodes the similarity between word senses (orsynsets) by a relation called cousin.
But thosecousin relations were identified manually by theWordNet lexicographers.
A similar effort wasalso made in the EuroWordnet project (Vossenet al, 1999).
However, manually inspecting alarge, complex lexicon is very time-consumingand often prone to inconsistencies.In this paper, we propose a method which au-tomatically extracts ystematic polysemy froma hierarchically organized semantic lexicon(WordNet).
Our method uses a modification ofa tree generalization technique used in (Li andAbe, 1998), and generates a tree-cut, which is alist of clusters that partition a tree.
Then, wecompare the systematic relations extracted byour automatic method to the WordNet cousins.Preliminary results show that our method dis-covered most of the WordNet cousins as well assome more interesting relations.2 T ree  Genera l i za t ion  us ing Tree-cutand  MDLBefore we present our method, we first give abrief summary of the tree-cut echnique whichwe adopted from (Li and Abe, 1998).
This tech-nique is used to acquire generalized case framepatterns from a corpus using a thesaurus tree.2.1 Tree-cut  ModelsA thesaurus tree is a hierarchically organizedlexicon where leaf nodes encode lexical data21(i.e., words) and internal nodes represent ab-stract semantic lasses.
A tree-cut is a partitionof a thesaurus tree.
It is a list of internal/leafnodes in the tree, and each node represents aset of all leaf nodes in a subtree rooted by thenode.
Such set is also considered as a clus-ter.
4 Clusters in a tree-cut exhaustively coverall leaf nodes of the tree, and they are mutu-ally disjoint.
For example, for a thesaurus treein Figure 1, there are 5 tree-cuts: \[airplane, he-licopter, ball, kite, puzzle\], \[AIRCRAFT, ball,kite, puzzle\], \[airplane, helicopter, TOY\], \[AIR-CRAFT, TOY\] and \[ARTIFACT\].
Thus, a tree-cut corresponds to one of the levels of abstrac-tion in the tree.Using a thesaurus tree and the idea of tree-cut, the problem of acquiring generalized caseframe patters (for a fixed verb) from a corpusis to select the best tree-cut hat accounts forboth observed and unobserved case frame in-stances.
In (Li and Abe, 1998), this generaliza-tion problem is viewed as a problem of select-ing the best model for a tree-cut hat estimatesthe true probability distribution, given a samplecorpus data.Formally, a tree-cut model M is a pair consist-ing of a tree-cut F and a probability parametervector O of the same length,M = (F, O)where F and ?
are:(1)F=\[Cx,..,Ck\],O=\[P(C,),..,P(Ck)\] (2)words, that is, P(C) = ~=1 P(nj).
Here, com-pared to knowing all P(nj) (where 1 < j < m)individually, knowing one P(C) can only facil-itate an estimate of uniform probability distri-bution among members as the best guess, thatis, P(nj) = P(C) for all j.
Therefore, in general, mwhen clusters C1..Cm are merged and general-ized to C according to the thesaurus tree, theestimation of a probability model becomes lessaccurate.2.2 The  MDL Pr inc ip leTo select the best tree-cut model, (Li and Abe,1998) uses the Minimal Description Length(MDL) principle (Rissanen, 1978).
The MDL isa principle of data compression i  InformationTheory which states that, for a given dataset,the best model is the one which requires theminimum length (often measured in bits) to en-code the model (the model description length)and the data (the data description length).
Forthe problem of case frame generalization, theMDL principle fits very well in that it capturesthe trade-off between the simplicity of a model,which is measured by the number of clusters ina tree-cut, and the goodness of fit to the data,which is measured by the estimation accuracyof the probability distribution.The calculation of the description length fora tree-cut model is as follows.
Given a the-saurus tree T and a sample S consisting ofthe case frame instances, the total descriptionlength L(M, S) for a tree-cut model M = (F, 0)iswhere Ci (1 < i < k) is a cluster in the tree-cut, P(Ci) is the probability of a cluster Ci,and ~/k=l P(Ci) = 1.
For example, supposea corpus contained 10 instances of verb-objectrelation for the verb "fly", and the frequencyof object noun n, denoted f(n), are as follows:f ( airpl ane ) -- 5, f ( helicopter ) = 3, f ( bal l ) =O, f(kite) -- 2, f(puzzle) = 0.
Then, the set oftree-cut models for the thesaurus tree shown inFigure 1 includes (\[airplane, helicopter, TOY\],\[0.5, 0.3, 0.2\]) and (\[AIRCRAFT, TOY\], \[0.8,0.2\]).Note that P(C) is the probability of clusterC = {nl, .., nm) as a whole.
It is essentially thesum of all (true) probabilities of the member4A leaf node is also a cluster whose cardinality is 1.L(M,S)=L(F)+L(eT)+L(SJF, e) (3)where L(F) is the model description length,L(OIF) is the parameter description length (ex-plained shortly), and L(SIF , O) is the data de-scription length.
Note that L(F) + L(OIF ) es-sentially corresponds to the usual notion of themodel description length.Each  length in L(M, S) is calculated as fol-lows.
5 The model description length L(F) isL( r )  = log21GI (4)where G is the set of all cuts in T, and IG I de-notes the size of G. This value is a constant for?
SFor justification and detailed explanation of theseformulas, ee (Li and Abe, 1998).22all models, thus it is omitted in the calculationof the total length.The parameter description length L(OIF ) in-dicates the complexity of the model.
It is thelength required to encode the probability dis-tribution of the clusters in the tree-cut F. It iscalculated askL(Olr)  = x Zog21Sl (5)where k is the length of ?, and IS\[ is the size ofS.Finally, the data description length L(SIF, O)is the length required to encode the whole sam-ple data.
It is calculated asL(S IF ,  e )  = - log2P(n) (6)nESwhere, for each n E C and each C E F,andP(n) -  P(C)ICl (7)P(C) -  f (c)  (8)ISlNote here that, in (7), the probability of C is di-vided evenly among all n in C. This way, wordsthat are not observed in the sample receive anon-zero probability, and the data sparsenessproblem is avoided.Then, the best model is the one which re-quires the minimum total description length.Figure 2 shows the MDL lengths for all fivetree-cut models that can be produced for thethesaurus tree in Figure 1.
The best model isthe one with the tree-cut \[AIRCRAFT, ball, kite,puzzle\] indicated by a thick curve in the figure.3 C lus ter ing  Systemat ic  Po lysemy3.1 Generalization TechniqueUsing the generalization technique in (Li andAbe, 1998) described in the previous section,we wish to extract systematic polysemy au-tomatically from WordNet.
Our assumptionis that, if a semantic concept is systemati-cally related to another concept, words thathave one sense under one concept (sub)tree arelikely to have another sense under the otherconcept (sub)tree.
To give an example, Fig-ure 3 shows parts of WordNet noun trees forARTIFACT and MEASURE, where subtrees underCONTAINER and C0NTAINERFUL respectively con-tain "bottle", "bucket" and "spoon".
Note adashed line in the figure indicates an indirectlink for more than one level.Based on this assumption, it seems system-atic polysemy in the two trees can be extractedstraight-forwardly by clustering each tree ac-cording to polysemy as a feature, and by match-ing of clusters taken from each tree.
6 To thisend, the notion of tree-cut and the MDL prin-ciple seem to comprise an excellent ool.However, we must be careful in adopting Liand Abe's technique directly: since the problemwhich their technique was applied to is funda-mentally different from ours, some proceduresused in their problem may not have any inter-pretation in our problem.
Although both prob-lems are essentially a tree generalization prob-lem, their problem estimates the true probabil-ity distribution from a random sample of exam-ples (a corpus), whereas our problem does nothave any additional data to estimate, since alldata (a lexicon) is already known.
This differ-ence raises the following issue.
In the calcu-lation of the data description length in equa-tion (6), each word in a cluster, observed or un-observed, is assigned an estimated probability,which is a uniform fraction of the probabilityof the cluster.
This procedure does not haveinterpretation if it is applied to our problem.Instead, we use the distribution of feature fre-quency proportion of the clusters, and calculatethe data description length by the following for-mula:kL(SIF, e) = - f(Ci) ?
log2P(Ci) (9)i=lwhere F = \[C1,.., Ck\], 0 = \[P(C,),.., P(Ck)\].This corresponds to the length required to en-code all words in a cluster, for all clustersin a tree-cut, assuming Huffman's algorithm(Huffman, 1952) assigned a codeword of length- log2P(Ci) to each cluster C/ (whose propor-6We could also combine two (or possibly more) treesinto one tree and apply clustering over that tree once.In this paper, we describe clustering of two trees for ex-ample purpose.23F\[A\]\[AC,TOY\]\[ap,heli,TOY\]\[AC,ball,kite,puz\].\[ap,hel,ball,kite,puz\]L(eIF) L(SIF.e) L(M,S)1.66 11.60 13.263.32 14.34 17.664.98 14.44 19.426.64 4.96 11.608.31 5.06 13.37ARTIFACT/ \ k.o.o/o.2 o.o/ \ / ~kairplane helicopter ball kite puzzleFigure 2: The MDL lengths and the final tree-cutARTIFACT MEASURECONTAINER MEDICINE dose CONTAINERFULVESSEL spoon dose bottle bucket spoon / \bottle bucketFigure 3: Parts of WordNet trees ARTIFACT and MEASUREtion is P(C i )  = .~_~_d~ Isl J"All other notions and formulas are applicableto our problem without modification.3.2 C lus ter ing  MethodOur clustering method uses the the modifiedgeneralization technique described in the lastsection to generate tree-cuts.
But before we ap-ply the method, we must transform the data inWordnet.
This is because WordNet differs froma theaurus tree in two ways: it is a graph ratherthan a tree, and internal nodes as well as leafnodes carry data, First, we eliminate multipleinheritance by separating shared subtrees.
Sec-ond, we bring down every internal node to aleaf level by creating a new duplicate node andadding it as a child of the old node (thus makingthe old node an internal node).After trees are transformed, our method ex-tracts systematic polysemy by the followingthree steps.
In the first step, all leaf nodes ofthe two trees are marked with either 1 or 0 (1if a node/word appears in both trees, or 0 oth-erwise),In the second step, the generalization tech-nique is applied to each tree, and two tree-cutsare obtained.
To search for the best tree-cut,instead of computing the description length forM1 possible tree-cuts in a tree, a greedy dy-namic programming algorithm is used.
Thisalgorithm , called F ind-MDL in (Li and Abe,1998), finds the best tree-cut for a tree by recur-sively finding the best tree-cuts for all of its sub-trees and merging them from bottom up.
Thisalgorithm is quite efficient, since it is basically adepth-first search with minor overhead for com-puting the description length.Finally in the third step, clusters from the twotree-cuts are matched up, and the pairs whichhave substantial overlap are selected as system-atic polysemy.Figure 4 shows parts of the final tree-cutsfor ARTIFACT and MEASURE obtained by ourmethod.
~In both trees, most of the clusters inthe tree-cuts are from nodes at depth 1 (count-ing the root as depth 0).
That is because thetree-cut echnique used in our method is sensi-tive to the structure of the tree.
More specifi-cally, the MDL principle inherently penalizes acomplex tree-cut by assigning a long parame-ter length.
Therefore, unless the entropy of thefeature distribution is large enough to make thedata length overshadow the parameter length,simpler tree-cuts partitioned at abstract levelsare preferred.
This situation tends to happenoften when the tree is bushy and the total fea-ture frequency is low.
This was precisely thecase with ARTIFACT and MEASURE, where bothTin the figure, bold letters indicate words which arepolysemous in the two t ree .24ARTIFACT0,STRUCTURE INSTRUMEN- ARTICLETALITY / i \base !building J /  ~ MEDICINE~ 10.02foot IMPLEMENT DEVICE CONTAINER / ~ TABLEWARE / \UTENSIL RODJ \mixer porcelainyard/ '"...
I ~  inhalant dose / l ~foot VESSEL spoon spoon dish platek .o t  / \bottle bucket0.36DEFINITEQUANTITYbit blockouncebottle bucket spoonMEASUREQUANTITY .
~k MEASURE / PERIODCONTAINERFUL dose load LINEAR morning flash sixties UNIT l / ~  quartermile knot yard footFigure 4: Parts of the final tree-cuts for ARTIFACT and MEASUREtrees were quite bushy, and only 4% and 14% ofthe words were polysemous in the two categoriesrespectively.4 Eva luat ionTo test our method, we chose 5 combinationsfrom WordNet noun Top categories (which wecall top relation classes), and extracted clus-ter pairs which have more than 3 overlappingwords.
Then we evaluated those pairs in twoaspects: related vs. unrelated relations, andautomatic vs. manual clusters.4.1 Re la ted  vs. Unre la ted  C lustersOf the cluster pairs we extracted automatically,not all are systematically related; some are un-related, homonymous relations.
They are essen-tially false positives for our purposes.
Table 1shows the number of related and unrelated re-lations in the extracted cluster pairs.Although the results vary among categorycombinations, the ratio of the related pairs israther low: less than 60% on average.
There areseveral reasons for this.
First, there are somepairs whose relations are spurious.
For exam-ple, in ARTIFACT-GROUP class, a pair \[LUMBER,SOCIAL_GROUP\] was extracted.
Words whichare common in the two clusters are "picket","board" and "stock".
This relation is obviouslyhomonymous.Second, some clusters obtained by tree-cutare rather abstract, so that pairing two ab-stract clusters results in an unrelated pair.
Forexample, in ARTIFACT-MEASURE class, a pair\[INSTRUMENTALITY, LINEAR_UNIT\] was selected.Words which are common in the two clus-ters include "yard", "foot" and "knot" (seethe previous Figure 4).
Here, the conceptINSTRUMENTALITY is very general (at depth1), and  it also contains many (polysemous)words.
So, matching this cluster with an-other abstract cluster is likely to yield a pairwhich has just enough overlapping words butwhose relation is not systematic.
In the caseof \[INSTRUMENTALITY, LINEAR_UNIT\], the situ-ation is even worse, because the concept ofLINEAR_UNIT in MEASURE represents a collectionof terms that were chosen arbitrarily in the his-25Table 1: Related vs.
Unrelated RelationsTop relation class Related UnrelatedACTION-L0CATIONARTIFACT-GROUPARTIFACT-MEASUREARTIFACT-SUBSTANCECOMMUNICATION-PERSON10 118 97 1919 1212 11Total 66 52Total1127263123118% ofrelated90.966.726.961.352.255.9tory of the English language.4.2 Automat ic  vs. Manua l  C lus tersTo compare the cluster pairs our method ex-tracted automatically to manually extractedclusters, we use WordNet cousins.
A cousinrelation is relatively new in WordNet, and thecoverage is still incomplete.
However, it givesus a good measure to see whether our auto-matic method discovered systematic relationsthat correspond to human intuitions.A cousin relation in WordNet is defined be-tween two synsets (currently in the noun treesonly), and it indicates that senses of a word thatappear in both of the (sub)trees rooted by thosesynsets are related, s The cousins were manuMlyextracted by the WordNet lexicographers.
Ta-ble 2 shows the number of cousins listed for eachtop relation class and the number of cousins ourautomatic method recovered (in the 'Auto' col-umn).
As you see, the total recall ratio is over80% (27/33~ .82).In the right three columns of Table 2, we alsoshow the breakdown of the recovered cousins,whether each recovered one was an exact match,or it was more general or specific than the cor-responding WordNet cousin.
From this, wecan see that more than half of the recoveredcousins were more general than the WordNetcousins.
That is partly because some WordNetcousins have only one or two common words.For example, a WordNet cousin \[PAINTING,COLORING_MATERIAL\] in ARTIFACT-SUBSTANCEhas only one common word "watercolor".
SuchSActually, cousin is one of the three relations whichindicate the grouping of related senses of a word.
Othersare sister and twin.
In this paper, we use cousin to referto all relations listed in "cousin.tps" file (available in aWordNet distribution).26a minor relation tends to be lost in our tree gen-eralization procedure.
However, the main rea-son is the difficulty mentioned earlier in the pa-per: the problem of applying the tree-cut ech-nique to a bushy tree when the data is sparse.In addition to the WordNet cousins, our auto-matic extraction method discovered several in-teresting relations.
Table 3 shows some exam-ples,5 Conc lus ions  and  Future  WorkIn this paper, we proposed an automaticmethod for extracting systematic polysemyfrom WordNet.
As we reported, preliminary re-sults show that our method identified almostall WordNet cousins as well as some new ones.One difficulty is that applying the generaliza-tion technique using the MDL principle to thebushy WordNet trees seems to yield a tree-cutat rather abstract level.For future work, we plan to compare thesystematic relations extracted by our automaticmethod to corpus data.
In particular, we liketo test whether our method extracts the samegroups of senses which human annotatorsdisagreed (Ng et al, 1999).
We also like to testwhether our method agrees with the findingthat multiple senses which occur in a discourseare often systematically polysemous (Krovetz,1998).Re ferencesApresjan, J.
(1973).
Regular Polysemy.
Lin-guistics, (142).Buitelaar, P. (1997).
A Lexicon for Underspec-ified Semantic Tagging.
In Proceedings off theA CL S IGLEX Workshop on Tagging TextTable 2: Automatic Clusters vs. WordNet CousinsTop relation class WN cousinACTION-LOCATIONARTIFACT-GROUPARTIFACT-MEASUREARTIFACT-SUBSTANCECOMMUNICATION-PERSONTotal 33Auto  Exact Gen Spec2 1 0 1 06 6 1 5 01 1 0 1 015 13 3 9 19 6 5 1 027 9 17 1Table 3: Examples of Automatically Extracted Systematic PolysemyTop relation classACTION-LOCATIONRelation\[ACTION, POINT\]Common Words\[VOICE, SINGER\]"drop", "circle", "intersection", dig","crossing", "bull's eye"ARTIFACT-GROUP \[STRUCTURE, P OPLE\] "house", "convent", "market", "center"ARTIFACT-SUBSTANCE \[FABRIC, CHEMICAL_COMPOUND\] "acetate", "nylon", "acrylic", "polyester"COMMUNI CATI 0N-PERSON\[WRITING, RELIGIOUS-PERSON\]"soprano", "alto", "tenor", "baritone""John", "Matthew", "Jonah", "Joshua","Jeremiah"with Lexical Semantics, Washington, D.C.,pp.
25-33.Buitelaar, P. (1998).
CORELEX: SystematicPolysemy and Underspecification.
Ph.D. dis-sertation, Department of Computer Science,Brandeis University.Copestake, A. and Briscoe, T. (1995).
Semi-productive Polysemy and Sense Extension.Journal of Semantics, 12.Cruse, D. (1986).
Lexical Semantics, Cam-bridge University Press.Huffman, D. A.
(1952).
A Model for the Con-struction of Minimum Redundancy Codes.In Proceedings ofthe IRE, 40.Kilgarriff, A.
(1998a)~ SENSEVAL: An Exer-cise in Evaluating Word Sense Disambigua-tion Programs.
In Proceedings of the LRECKilgarriff, A.
(1998b).
Inter-tagger Agree-ment.
In Advanced Papers of the SENSE-VAL Workshop, Sussex, UK.Krovetz, R. (1998).
More than One Sense PerDiscourse.
In Advanced Papers of the SEN-SEVAL Workshop, Sussex, UK.Li, H. and Abe, N. (1998).
Generalizing CaseFrames Using a Thesaurus and the MDLPrinciple, Computational Linguistics, 24(2),pp.
217-244Miller, G.
(eds.)
(1990).
WORDNET: An On-line Lexical Database.
International Journalof Lexicography, 3 (4).Ng, H.T., Lim, C. and Foo, S. (1999).
ACase Study on Inter-Annotator Agreementfor Word Sense Disambiguationl In Proceed-ings of the A CL SIGLEX Workshop on Stan-dardizing Lexical Resources, College Park,MD.Nunberg, G. (1995).
Transfers of Meaning.Journal of Semantics, 12.Procter, P. (1978).
Longman dictionary ofContemporary English, Longman Group.Pustejovsky, J.
(1995).
The Generative Lexi-con, The MIT Press.Rissanen, J.
(1978).
Modeling by ShortestData Description.
Automatic, 14.Veronis, J.
(1998).
A Study of Polysemy Judge-ments and Inter-annotator Agreement.
InAdvanced Papers of the SENSEVAL Work-shop, Sussex, UK.Vossen, P., Peters, W. and Gonzalo, J.
(1999).Towards a Universal Index of Meaning.
InProceedings of the A CL SIGLEX Workshopon Standardizing Lexical Resources, CollegePark, MD.27
