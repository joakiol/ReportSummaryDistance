Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 97?100,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsYou?ve Got Answers: Towards Personalized Models for Predicting Successin Community Question AnsweringYandong Liu and Eugene AgichteinEmory University{yliu49,eugene}@mathcs.emory.eduAbstractQuestion answering communities such as Ya-hoo!
Answers have emerged as a popular al-ternative to general-purpose web search.
Bydirectly interacting with other participants, in-formation seekers can obtain specific answersto their questions.
However, user success inobtaining satisfactory answers varies greatly.We hypothesize that satisfaction with the con-tributed answers is largely determined by theasker?s prior experience, expectations, andpersonal preferences.
Hence, we begin to de-velop personalized models of asker satisfac-tion to predict whether a particular questionauthor will be satisfied with the answers con-tributed by the community participants.
Weformalize this problem, and explore a varietyof content, structure, and interaction featuresfor this task using standard machine learningtechniques.
Our experimental evaluation overthousands of real questions indicates that in-deed it is beneficial to personalize satisfactionpredictions when sufficient prior user historyexists, significantly improving accuracy overa ?one-size-fits-all?
prediction model.1 IntroductionCommunity Question Answering (CQA) has re-cently become a viable method for seeking infor-mation online.
As an alternative to using general-purpose web search engines, information seekersnow have an option to post their questions (oftencomplex, specific, and subjective) on CommunityQA sites such as Yahoo!
Answers, and have theirquestions answered by other users.
Hundreds of mil-lions of answers have already been posted for tens ofmillions of questions in Yahoo!
Answers.
However,the success of obtaining satisfactory answers in theavailable CQA portals varies greatly.
In many cases,the questions posted by askers go un-answered, orare answered poorly, never obtaining a satisfactoryanswer.In our recent work (Liu et al, 2008) we have in-troduced a general model for predicting asker sat-isfaction in community question answering.
Wefound that previous asker history is a significant fac-tor that correlates with satisfaction.
We hypothesizethat asker?s satisfaction with contributed answers islargely determined by the asker expectations, priorknowledge and previous experience with using theCQA site.
Therefore, in this paper we begin to ex-plore how to personalize satisfaction prediction -that is, to attempt to predict whether a specific in-formation seeker will be satisfied with any of thecontributed answers.
Our aim is to provide a ?per-sonalized?
recommendation to the user that they?vegot answers that satisfy their information need.To the best of our knowledge, ours is the first ex-ploration of personalizing prediction of user satis-faction in complex and subjective information seek-ing environments.
While information seeker sat-isfaction has been studied in ad-hoc IR context(see (Kobayashi and Takeda, 2000) for an overview),previous studies have been limited by the lack of re-alistic user feedback.
In contrast, we deal with com-plex information needs and community-providedanswers, trying to predict subjective ratings pro-vided by users themselves.
Furthermore, while au-tomatic complex QA has been an active area of re-search, ranging from simple modification to factoidQA technique (e.g., (Soricut and Brill, 2004)) toknowledge intensive approaches for specialized do-mains, the technology does not yet exist to automat-ically answer open domain, complex, and subjectivequestions.
Hence, this paper contributes to both theunderstanding of complex question answering, andexplores evaluation issues in a new setting.The rest of the paper is organized as follows.
Wedescribe the problem and our approach in Section2, including our initial attempt at personalizing sat-isfaction prediction.
We report results of a large-scale evaluation over thousands of real users and97tens of thousands of questions in Section 3.
Ourresults demonstrate that when sufficient prior askerhistory exists, even simple personalized models re-sult in significant improvement over a general pre-diction model.
We discuss our findings and futurework in Section 4.2 Predicting Asker Satisfaction in CQAWe first briefly review the life of a question in aQA community.
A user (the asker) posts a questionby selecting a topical category (e.g., ?History?
), andthen enters the question and, optionally, additionaldetails.
After a short delay the question appears inthe respective category list of open questions.
Atthis point, other users can answer the question, voteon other users?
answers, or interact in other ways.The asker may be notified of the answers as they aresubmitted, or may check the contributed answers pe-riodically.
If the asker is satisfied with any of theanswers, she can choose it as best, and rate the an-swer by assigning stars.
At that point, the questionis considered as closed by asker.
For more detailedtreatment of user interactions in CQA see (Liu etal., 2008).
If the asker rates the best answer withat least three out of five ?stars?, we believe the askeris satisfied with the response.
But often the askernever closes the answer personally, and instead, af-ter a period of time, the question is closed automat-ically.
In this case, the ?best?
answer may be cho-sen by the votes, or alternatively by automaticallypredicting answer quality (e.g., (Jeon et al, 2006)or (Agichtein et al, 2008)).
While the best answerchosen automatically may be of high quality, it is un-known if the asker?s information need was satisfied.Based on our exploration we believe that the mainreasons for not ?closing?
a question are a) the askerloses interest in the information and b) none of theanswers are satisfactory.
In both cases, the QA com-munity has failed to provide satisfactory answers ina timely manner and ?lost?
the asker?s interest.
Weconsider this outcome to be ?unsatisfied?.
We nowdefine asker satisfaction more precisely:Definition 1 An asker in a QA community is consid-ered satisfied iff: the asker personally has closed thequestion and rated the best answer with at least 3?stars?.
Otherwise, the asker is unsatisfied.This definition captures a key aspect of asker satis-faction, namely that we can reliably identify whenthe asker is satisfied but not the converse.2.1 Asker Satisfaction Prediction FrameworkWe now briefly review our ASP (Asker Satisfac-tion Prediction) framework that learns to classifywhether a question has been satisfactorily answered,originally introduced in (Liu et al, 2008).
ASP em-ploys standard classification techniques to predict,given a question thread, whether an asker would besatisfied.
A sample of features used to represent thisproblem is listed in Table 1.
Our features are or-ganized around the basic entities in a question an-swering community: questions, answers, question-answer pairs, users, and categories.
In total, we de-veloped 51 features for this task.
A sample of thefeatures used are listed in the Figure 1.?
Question Features: Traditional question answer-ing features such as the wh-type of the question(e.g., ?what?
or ?where?
), and whether the ques-tion is similar to other questions in the category.?
Question-Answer Relationship Features: Over-lap between question and answer, answer length,and number of candidate answers.
We also usefeatures such as the number of positive votes(?thumbs up?
in Yahoo!
Answers), negative votes(?thumbs down?
), and derived statistics such asthe maximum of positive or negative votes re-ceived for any answer (e.g., to detect cases of bril-liant answers or, conversely, blatant abuse).?
Asker User History: Past asker activity historysuch as the most recent rating, average past satis-faction, and number of previous questions posted.Note that only the information available about theasker prior to posting the question was used.?
Category Features: We hypothesized that userbehavior (and asker satisfaction) varies by topi-cal question category, as recently shown in refer-ence (Agichtein et al, 2008).
Therefore we modelthe prior of asker satisfaction for the category,such as the average asker rating (satisfaction).?
Text Features: We also include word unigrams andbigrams to represent the text of the question sub-ject, question detail, and the answer content.
Sep-arate feature spaces were used for each attribute tokeep answer text distinct from question text, withfrequency-based filtering.Classification Algorithms: We experimented witha variety of classifiers in the Weka framework (Wit-ten and Frank, 2005).
In particular, we com-pared Support Vector Machines, Decision trees, andBoosting-based classifiers.
SVM performed the best98Feature DescriptionQuestion FeaturesQ: Q punctuation density Ratio of punctuation to words in the questionQ: Q KL div wikipedia KL divergence with Wikipedia corpusQ: Q KL div category KL divergence with ?satisfied?
questions in categoryQ: Q KL div trec KL divergence with TREC questions corpusQuestion-Answer Relationship FeaturesQA: QA sum pos vote Sum of positive votes for all the answersQA: QA sum neg vote Sum of negative votes for all the answersQA: QA KL div wikipedia KL Divergence of all answers with Wikipedia corpusAsker User History FeaturesUH: UH questions resolved Number of questions resolved in the pastUH: UH num answers Number of all answers this user has received in the pastUH: UH more recent rating Rating for the last question before current questionUH: UH avg past rating Average rating given when closing questions in the pastCategory FeaturesCA: CA avg time to close Average interval between opening and closingCA: CA avg num answers Average number of answers for that categoryCA: CA avg asker rating Average rating given by asker for categoryCA: CA avg num votes Average number of ?best answer?
votes in categoryTable 1: Sample features: Question (Q), Question-Answer Relationship (QA), Asker history (UH), and Cat-egory (CA).of the three during development, so we report resultsusing SVM for all the subsequent experiments.2.2 Personalizing Asker Satisfaction PredictionWe now describe our initial attempt at personalizingthe ASP framework described above to each asker:?
ASP Pers+Text: We first consider the naive per-sonalization approach where we train a separateclassifier for each user.
That is, to predict a par-ticular asker?s satisfaction with the provided an-swers, we apply the individual classifier trainedsolely on the questions (and satisfaction labels)provided in the past by that user.?
ASP Group: A more robust approach is to train aclassifier on the questions from the group of userssimilar to each other.
Our current grouping wasdone simply by the number of questions posted,essentially grouping users with similar levels of?activity?.
As we will show below, text featuresonly help for users with at least 20 previous ques-tions.
So, we only include text features for groupsof users with at least 20 questions.Certainly, more sophisticated personalization mod-els and user clustering methods could be devised.However, as we show next, even the simple modelsdescribed above prove surprisingly effective.3 Experimental EvaluationWewant to predict, for a given user and their currentquestion whether the user will be satisfied, accord-ing to our definition in Section 2.
In other words, our?truth?
labels are based on the rating subsequentlygiven to the best answer by the asker herself.
It isusually more valuable to correctly predict whethera user is satisfied (e.g., to notify a user of success).#Questions per Asker # Questions # Answers # Users1 132,279 1,197,089 132,2792 31,692 287,681 15,8463-4 23,296 213,507 7,0485-9 15,811 143,483 2,56810-14 5,554 54,781 48115-19 2,304 21,835 13720-29 2,226 23,729 9330-49 1,866 16,982 4950-100 842 4,528 14Total: 216,170 1,963,615 158,515Table 2: Distribution of questions, answers and askers.Hence, we focus on the Precision, Recall, and F1values for the satisfied class.Datasets: Our data was based on a snapshot of Ya-hoo!
Answers crawled in early 2008, containing216,170 questions posted in 100 topical categoriesby 158,515 askers, with associated 1,963,615 an-swers in total.
More detailed statistics, arranged bythe number of questions posted by each asker arereported in (Table 2).
The askers with only onequestion (i.e., no prior history) dominate the dataset,as many users try the service once and never comeback.
However, for personalized satisfaction, at leastsome prior history is needed.
Therefore, in this earlyversion of our work, we focus on users who haveposted at least 2 questions - i.e., have the minimalhistory of at least one prior question.
In the future,we plan to address the ?cold start?
problem of pre-dicting satisfaction of new users.Methods compared:?
ASP: A ?one-size-fits-all?
satisfaction predictorthat is trained on 10,000 randomly sampled ques-tions with only non-textual features (Section 2.1).?
ASP+Text: The ASP classifier with text features.?
ASP Pers+Text and ASP Group: A personal-ized classifiers described in Section 2.2.3.1 Experimental ResultsFigure 1 reports the satisfaction prediction accu-racy for ASP, ASP Text, ASP Pers+Text, andASP Group for groups of askers with varying num-ber of previous questions posted.
Surprisingly,for ASP Text, textual features only become help-ful for users with more than 20 or 30 previousquestions posted and degrade performance other-wise.
Also note that baseline ASP classifier isnot able to achieve higher accuracy even for userswith large amount of past history.
In contrast,the ASP Pers+Text classifier, trained only on thepast question(s) of each user, achieves surprisinglygood accuracy ?
often significantly outperformingthe ASP and ASP Text classifiers.
The improve-ment is especially dramatic for users with at least99Figure 1: Precision, Recall, and F1 of ASP, ASP Text, ASP Pers+Text, and ASP Group for predicting satisfaction ofaskers with varying number of questions20 previous questions.
Interestingly, the simplestrategy of grouping users by number of previousquestions (ASP Group) is even more effective, re-sulting in accuracy higher than both other meth-ods for users with moderate amount of history.
Fi-nally, for users with only 2 questions total (that is,only 1 previous question posted) the performanceof ASP Pers+Text is surprisingly high.
We foundthat the classifier simply ?memorizes?
the outcomeof the only available previous question, and uses itto predict the rating of the current question.To better understand the improvement of person-alized models, we report the most significant fea-tures, sorted by Information Gain (IG), for threesample ASP Pers+Text models (Table 3).
Interest-ingly, whereas for Pers 1 and Pers 2, textual featuressuch as ?good luck?
in the answer are significant, forPers 3 non-textual features are most significant.We also report the top 10 features with the high-est information gain for the ASP and ASP Groupmodels (Table 4).
Interestingly, while asker?s aver-age previous rating is the top feature for ASP, thelength of membership of the asker is the most impor-tant feature for ASP Group, perhaps allowing theclassifier to distinguish more expert users from theactive newbies.
In summary, we have demonstratedpromising preliminary results on personalizing sat-isfaction prediction even with relatively simple per-sonalization models.Pers 1 (97 questions) Pers 2 (49 questions) Pers 3 (25 questions)UH total answers received Q avg pos votes Q content kl trecUH questions resolved ?would?
in answer Q content kl wikipedia?good luck?
in answer ?answer?
in question UH total answers received?is an?
in answer ?just?
in answer UH questions resolved?want to?
in answer ?me?
in answer Q content kl asker all cate?we?
in answer ?be?
in answer Q prev avg rating?want in?
answer ?in the?
in question CA avg asker rating?adenocarcinoma?
in question CA History ?anybody?
in question?was?
in question ?who is?
in question Q content typo density?live?
in answer ?those?
in answer Q detail lenTable 3: Top 10 features by Information Gain for threesample ASP Pers+Text models.IG ASP IG ASP Group0.104117 Q prev avg rating 0.30981 UH membersince in days0.102117 Q most recent rating 0.25541 Q prev avg rating0.047222 Q avg pos vote 0.22556 Q most recent rating0.041773 Q sum pos vote 0.15237 CA avg num votes0.041076 Q max pos vote 0.14466 CA avg time close0.03535 A ques timediff in minutes 0.13489 CA avg asker rating0.032261 UH membersince in days 0.13175 CA num ans per hour0.031812 CA avg asker rating 0.12437 CA num ques per hour0.03001 CA ratio ans ques 0.09314 Q avg pos vote0.029858 CA num ans per hour 0.08572 CA ratio ans quesTable 4: Top 10 features by information gain for ASP(trained for all askers) and ASP Group (trained for thegroup of askers with 20 to 29 questions)4 ConclusionsWe have presented preliminary results on personal-izing satisfaction prediction, demonstrating signif-icant accuracy improvements over a ?one-size-fits-all?
satisfaction prediction model.
In the future weplan to explore the personalization more deeply fol-lowing the rich work in recommender systems andcollaborative filtering, with the key difference thatthe asker satisfaction, and each question, are unique(instead of shared items such as movies).
In sum-mary, our work opens a promising direction towardsmodeling personalized user intent, expectations, andsatisfaction.ReferencesE.
Agichtein, C. Castillo, D. Donato, A. Gionis, andG.
Mishne.
2008.
Finding high-quality content insocial media with an application to community-basedquestion answering.
In Proceedings of WSDM.J.
Jeon, W.B.
Croft, J.H.
Lee, and S. Park.
2006.
Aframework to predict the quality of answers with non-textual features.
In Proceedings of SIGIR.Mei Kobayashi and Koichi Takeda.
2000.
Informationretrieval on the web.
ACM Computing Surveys, 32(2).Y.
Liu, J. Bian, and E. Agichtein.
2008.
Predicting in-formation seeker satisfaction in community questionanswering.
In Proceedings of SIGIR.R.
Soricut and E. Brill.
2004.
Automatic question an-swering: Beyond the factoid.
In HLT-NAACL.I.
Witten and E. Frank.
2005.
Data Mining: Practicalmachine learning tools and techniques.
Morgan Kauf-man, 2nd edition.100
