Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 351?354,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsKCDC: Word Sense Induction by Using Grammatical Dependencies andSentence Phrase StructureRoman KernKnow-CenterGraz, Austriarkern@know-center.atMarkus MuhrKnow-CenterGraz, Austriammuhr@know-center.atMichael GranitzerGraz University of Technology,Know-CenterGraz, Austriamgrani@know-center.atAbstractWord sense induction and discrimination(WSID) identifies the senses of an am-biguous word and assigns instances of thisword to one of these senses.
We have builda WSID system that exploits syntactic andsemantic features based on the results ofa natural language parser component.
Toachieve high robustness and good general-ization capabilities, we designed our sys-tem to work on a restricted, but grammat-ically rich set of features.
Based on theresults of the evaluations our system pro-vides a promising performance and robust-ness.1 IntroductionThe goal of the SemEval-2 word sense induc-tion and discrimination task, see Manandhar etal.
(2010), is to identify the senses of ambiguousnouns and verbs in an unsupervised manner and tolabel unseen instances of these words with one ofthe induced senses.
The most common approachtowards this task is to apply clustering or graphpartitioning algorithms on a representation of thewords that surround an ambiguous target word, seefor example Niu et al (2007) and Pedersen (2007).We followed this approach by employing a cluster-ing algorithm to detect the individual senses, butfocused on generating feature sets different to themainstream approach.
Our feature sets utilize theoutput of a linguistic processing pipeline that cap-tures the syntax and semantics of sentence partsclosely related with the target word.2 System OverviewThe base of our system is to apply a parser on thesentence in which the target word occurs.
Contex-tual information, for example the sentences sur-rounding the target sentence, are currently notexploited by our system.
To analyze the sen-tences we applied the Stanford Parser (Version1.6.2), which is based on lexicalized probabilis-tic context free grammars, see Klein and Man-ning (2003).
This open-source parser not only ex-tracts the phrase structure of a given sentence, butalso provides a list of so called grammatical rela-tions (typed dependencies), see de Marneffe et al(2006).
These relations reflect the dependenciesbetween the words within the sentence, for exam-ple the relationship between the verb and the sub-ject.
See Chen et al (2009) for an application ofgrammatical dependencies for word sense disam-biguation.2.1 Feature ExtractionThe phrase structure and the grammatical depen-dencies are sources for the feature extraction stage.To illustrate the result of the parser and feature ex-traction stages we use an example sentence, wherethe target word is the verb ?file?
:Afterward , I watched as a butt-ton of good , butmisguided people filed out of the theater , andimmediately lit up a smoke .2.1.1 Grammatical Dependency FeaturesThe Stanford Parser provides 55 different gram-matical dependency types.
Figure 2 depicts the listof the grammatical dependencies identified by theStanford Parser for the example sentence.
Only alimited subset of these dependencies are selectedto build the grammatical feature set.
This subsethas been defined based on preliminary tests on thetrial dataset.
For verbs only dependencies that rep-resent the association of a verb with prepositionalmodifiers and phrasal verb particles are selected(prep, prepc, prt).
If the verb is not associatedwith a preposition or particle, a synthetic ?miss-ing?
feature is added instead (!prep, !prt).
Fornouns the selected dependencies are the preposi-tions (for head nouns that are the object of a prepo-sition) and noun compound modifiers (pobj, nn).351Figure 1: Phrase tree of the example sentence.
The noun phrase ?misguided people?
is connected to thetarget word via the nsubj dependency and the phrase ?the theater?
is associated with the target verb viathe prep and pobj dependencies.Figure 2: List of grammatical dependencies as de-tected by the Stanford Parser.If the noun is associated with a verb the grammati-cal dependencies of this verb are also added to thefeature set.The name of the dependency and the word (i.e.preposition or particle) are used to construct thegrammatical feature.
The different features areweighted.
The weights have been derived fromtheir frequencies within the trial dataset and listedin table 1.
For the example sentence the extractedgrammatical features are:?out?, ?of?, prep, prt2.1.2 Phrase Term FeaturesThe second set of features are generated from thesentence phrase structure.
In figure 1 the parse treefor the example sentence is depicted.Again we tried to keep the feature set as smallas possible.
Starting with the target word onlyphrases that are directly associated with the am-biguous word are selected.
To identify thesephrases the grammatical dependencies are ex-ploited.
For nouns as target words the associatedverb is searched at first.
Given a verb the phrasescontaining the head noun of a subject or object re-lationship are identified.
If the verb is accompa-Feature Weightprepc, prt, nn, pobj 0.9prep 0.45!prep, !prt 0.5?prepositions?, ?particles?
0.97Table 1: Weights of the grammatical features,which were derived from their distribution withinthe trial dataset.nied by a preposition, the phrase carrying the ob-ject of the preposition is also added.
All nouns andadjectives from these these phrases are then col-lected.
The phrase words together with the verb,prepositions and particles are lemmatized usingtools also provided by the Stanford Parser project.The weights of the phrase term features arebased on the frequency of the words within thetraining dataset, where N is the total number ofsentences and Nfis the number of sentences inwhich the lemmatized phrase term occurs in:weightf= log(NNf+ 1) + 1 (1)In our example sentence the extracted phraseterm features are:of, misguided, file, theater, people, out2.2 Phrase Term ExpansionThe feature space of the phrase terms is expectedto be very sparse.
Additionally different phraseterms may have similar semantics.
Therefore thephrase terms are optionally expanded with asso-ciated terms, where semantically similar termsshould be associated with the same terms.To calculate the statistics for term expansion weused the training dataset (although other datasets352would be more suitable for this purpose).
Thedataset is split into sentences.
Stopwords andrarely used words, which occur in less than 3 sen-tences, were removed.
The remaining words werefinally lemmatized.
For a given phrase term thetop 100 associated terms are used to build thefeature set.
The association weight between twoterms is based on the Pointwise Mutual Informa-tion:weightpmi(ti, tj) =log2(P (ti|tj)P (tj))log2(1P (tj))(2)For example the top 10 associated terms fortheater are:theater.n, movie.n, opera.n,vaudeville.n, wnxt-abc.n, imax.n,orpheum.n, pullulate.v, projector.n,psychomania.n2.3 Sense InductionTo detect the individual senses within the trainingdataset we applied unsupervised machine learningtechniques.
For each ambiguous word a matrix- M|Instances|?|Features|- is created and a clus-tering algorithm is applied, namely the Growingk-Means, see Daszykowski et al (2002).
Thisalgorithm needs the number of clusters and cen-troids as initialization parameters, where the initialcentroids are calculated using a directed randomseed finder as described in Arthur and Vassilvitskii(2007).
We used the Jensen-Shannon Divergencefunction for the grammatical dependency featuresand the Cosine Similarity for the phrase term fea-ture sets as relatedness function.For each cluster number we re-run the clus-tering with different random initial centroids (30times) and for each run we calculate a cluster qual-ity criterion.
The overall cluster quality criterion isthe mean of all feature quality criteria, which arecalculated based on the set of clusters the featureoccurs in - Cf- the number of instances of eachcluster - Nc- and the number of instances withina cluster where the feature occurs in - Nc,f:FQCf=weightf|Cf|?
?c?CfNc,fNc(3)QCrun= FQCf(4)The cluster quality criterion is calculated foreach run and the combination of the mean andstandard deviations are then used to calculate astability criterion to detect the number of clusters,which is based on the intuition that the correctcluster count yields the lowest variation of QCvalues:SCk=mean(QC)stdev(QC)(5)Starting with two clusters the number of clustersis incremented until the stability criterion starts todecline.
For the cluster number with the higheststability criterion the run with the highest qual-ity criterion is selected as final clustering solution.The result of the sense induction processing is alist of centroids for the identified clusters.2.4 Sense AssignmentThe final processing step is to assign an instanceof an ambiguous word to one of the pre-calculatedsenses.
The sentence with the target word is pro-cessed exactly like the training sentences to gener-ate a set of features.
Finally the word is assignedto the sense cluster with the maximum relatedness.3 System Configurations & ResultsOur system can be configured to use a combina-tion of feature sets for the word sense inductionand discrimination calculations: a) KCDC-GD:Grammatical dependency features, b) KCDC-PT:Phrase terms features, c) KCDC-PC: Expandedphrase term features, d) KCDC-PCGD: All train-ing sentences are first processed by using the ex-panded phrase term features and then by usingthe grammatical dependency features with an ad-ditional feature that encodes the cluster id foundby the phrase features.In the evaluation we also submitted multipleruns of the same configuration1to assess the in-fluence of the random initialization of the cluster-ing algorithm.
Judging from the results the ran-dom seeding has no pronounced impact and it in-fluence should decrease when the number of clus-tering runs for each cluster number is increased.All configurations found on average about 3senses for target words in the test set (2.8 for verbs,3.3 for nouns), with exception of the KCDC-PTconfiguration which identified only 1.5 senses onaverage.
In the gold standard the number of sensesfor verbs is 3.12 and for nouns 4.46, which showsthat the stability criterion tends to underestimatethe number of senses slightly.To compare the performance of the differ-ent configurations, one can use the average rankwithin the evaluation result lists.
Judging from the1labeled KCDC-GD-2, KCDC-GDC for configuration ?a?and KCDC-PC-2 for the configuration ?c?353rankings, the configurations that utilize the gram-matical dependencies and the expanded phraseterms provide similar performance.
The config-uration that takes the phrase terms directly as fea-tures comes in last, which is expected due to thesparse nature of the feature representation and thelow number of detected senses.Comparing the performance of our system withthe two baselines shows that our system did out-perform the random baseline in all evaluation runsand the most frequent baseline (MFS) in all runswith the exception of the F-Score based unsuper-vised evaluation, where the MFS baseline has notbeen beaten by any system.
Although none of oursubmitted configurations was ranked first in any ofthe evaluations, their ranking was still better thanaverage, with the exception of the KCDC-PT con-figuration.Another observation that can be made is the dif-ference in performance between nouns and verbs.Our system, especially the grammatical depen-dency based configurations, is tailored towardsverbs.
Therefore the better performance of verbsin the evaluation is in line with the expectations.When looking at the results of the individual tar-get words one can notice that for a set of wordsthe quality of the sense detection is above average.For 16 of the 100 words a V-Measure of more than30% in at least one configuration was achieved(average: 7.8%)2.
This can be seen as indicatorthat our selection of features is effective for a spe-cific group of words.
For the remaining words anaccording feature set has to be developed in futurework.4 ConclusionFor the SemEval 2010 word sense induction anddiscrimination task we have tried to build a systemthat uses a minimal amount of information whilestill providing a competitive performance.
Thissystem contains a parser component to analyze thephrase structure of a sentence and the grammat-ical dependencies between words.
The extractedfeatures are then clustered to detect the senses ofambiguous words.
In the evaluation runs our sys-tem did demonstrate a satisfying performance fora number of words.The design of our system offers a wide rangeof possible enhancements.
For example the inte-2The best performing target words are: root.v,presume.v, figure.v, weigh.v, cheat.vgration of preposition disambiguation and noun-phrase co-reference resolution could help to fur-ther improve the word sense discrimination effec-tiveness.AcknowledgmentsThe Know-Center is funded within the Austrian COMETProgram - Competence Centers for Excellent Technologies -under the auspices of the Austrian Federal Ministry of Trans-port, Innovation and Technology, the Austrian Federal Min-istry of Economy, Family and Youth and by the State ofStyria.
COMET is managed by the Austrian Research Pro-motion Agency FFG.
Results are partially funded by the EU-ROSTARS project 4811 MAKIN?IT.ReferencesD.
Arthur and S. Vassilvitskii.
2007. k-means++:The advantages of careful seeding.
In Proceedingsof the eighteenth annual ACM-SIAM symposium onDiscrete algorithms, page 10271035.
Society for In-dustrial and Applied Mathematics Philadelphia, PA,USA.Ping Chen, Wei Ding, Chris Bowes, and David Brown.2009.
A fully unsupervised word sense disambigua-tion method using dependency knowledge.
HumanLanguage Technology Conference.M Daszykowski, B Walczak, and D L Massart.
2002.On the optimal partitioning of data with K-means,growing K-means, neural gas, and growing neuralgas.
Journal of chemical information and computersciences, 42(6):1378?89.M.C.
de Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In LREC 2006.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
Proceedings of the 41stAnnual Meeting on Association for ComputationalLinguistics - ACL ?03, pages 423?430.Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-gach, and Sameer S. Pradhan.
2010.
SemEval-2010 Task 14: Word Sense Induction & Disam-biguation.
In Proceedings of SemEval-2, Uppsala,Sweden, ACL.Zheng-yu Niu, Dong-hong Ji, and Chew-lim Tan.2007.
I2R: Three Systems for Word Sense Discrim-ination, Chinese Word Sense Disambiguation, andEnglish Word Sense Disambiguation.
In Proceed-ings of the 4th International Workshop on SemanticEvaluations.
ACL.T.
Pedersen.
2007.
Umnd2: Senseclusters applied tothe sense induction task of senseval-4.
In Proceed-ings of the 4th International Workshop on SemanticEvaluations.
ACL.354
