Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 49?56,Sydney, July 2006. c?2006 Association for Computational LinguisticsTowards Large-scale Non-taxonomic Relation Extraction: Estimating thePrecision of Rote Extractors?Enrique Alfonseca??
Maria Ruiz-Casado??
?Precision and Intelligence LaboratoryTokyo Institute of Techonologyenrique@lr.pi.titech.ac.jpoku@pi.titech.ac.jpManabu Okumura?
Pablo Castells?
?Computer Science DepartmentUniversidad Autonoma de Madridenrique.alfonseca@uam.esmaria.ruiz@uam.espablo.castells@uam.esAbstractIn this paper, we describe a rote extrac-tor that learns patterns for finding seman-tic relations in unrestricted text, with newprocedures for pattern generalisation andscoring.
An improved method for estimat-ing the precision of the extracted patternsis presented.
We show that our method ap-proximates the precision values as evalu-ated by hand much better than the proce-dure traditionally used in rote extractors.1 IntroductionWith the large growth of the information stored inthe web, it is necessary to have available automaticor semi-automatic tools so as to be able to processall this web content.
Therefore, a large effort hasbeen invested in developing automatic or semi-automatic techniques for locating and annotatingpatterns and implicit information from the web,a task known as Web Mining.
In the particularcase of web content mining, the aim is automati-cally mining data from textual web documents thatcan be represented with machine-readable seman-tic formalisms such as ontologies and semantic-web languages.Recently, there is an increasing interest in au-tomatically extracting structured information fromlarge corpora and, in particular, from the Web(Craven et al, 1999).
Because of the character-istics of the web, it is necessary to develop effi-cient algorithms able to learn from unannotateddata (Riloff and Schmelzenbach, 1998; Soderland,1999; Mann and Yarowsky, 2005).
New types ofweb content such as blogs and wikis, are also a?This work has been sponsored by MEC, project numberTIN-2005-06885.source of textual information that contain an un-derlying structure from which specialist systemscan benefit.Consequently, rote extractors (Brin, 1998;Agichtein and Gravano, 2000; Ravichandran andHovy, 2002) have been identified as an appropri-ate method to look for textual contexts that happento convey a certain relation between two concepts.In this paper, we describe a new procedure for es-timating the precision of the patterns learnt by arote extractor, and how it compares to previous ap-proaches.
The solution proposed opens new pos-sibilities for improving the precision of the gener-ated patterns, as described below.This paper is structured as follows: Section 2describe related work; Section 3 and 4 describe theproposed procedure and its evaluation, and Sec-tion 5 presents the conclusions and future work.2 Related workExtracting information using Machine Learningalgorithms has received much attention sincethe nineties, mainly motivated by the MessageUnderstanding Conferences.
From the mid-nineties, there are systems that learn extractionpatterns from partially annotated and unannotateddata (Huffman, 1995; Riloff, 1996; Riloff andSchmelzenbach, 1998; Soderland, 1999).Generalising textual patterns (both manuallyand automatically) for the identification of rela-tions has been proposed since the early nineties(Hearst, 1992), and it has been applied to extend-ing ontologies with hyperonymy and holonymy re-lations (Morin and Jacquemin, 1999; Kietz et al,2000; Cimiano et al, 2004; Berland and Char-niak, 1999).
Finkelstein-Landau andMorin (1999)learn patterns for company merging relations withexceedingly good accuracies.
Recently, kernel49methods are also becoming widely used for rela-tion extraction (Bunescu and Mooney, 2005; Zhaoand Grishman, 2005).Concerning rote extractors from the web, theyhave the advantage that the training corpora canbe collected easily and automatically, so theyare useful in discovering many different relationsfrom text.
Several similar approaches have beenproposed (Brin, 1998; Agichtein and Gravano,2000; Ravichandran and Hovy, 2002), with vari-ous applications: Question-Answering (Ravichan-dran and Hovy, 2002), multi-document NamedEntity Coreference (Mann and Yarowsky, 2003),and generating biographical information (Mannand Yarowsky, 2005).
Szpektor et al (2004) ap-plies a similar, with no seed lists, to extract auto-matically entailment relationships between verbs,and Etzioni et al (2005) report very good resultsextracting Named Entities and relationships fromthe web.2.1 Rote extractorsRote extractors (Mann and Yarowsky, 2005) es-timate the probability of a relation r(p, q) giventhe surrounding context A1pA2qA3.
This is cal-culated, with a training corpus T , as the numberof times that two related elements r(x, y) from Tappear with that same contextA1xA2yA3, dividedby the total number of times that x appears in thatcontext together with any other word:P (r(p, q)|A1pA2qA3) =Px,yr c(A1xA2yA3)Px,z c(A1xA2zA3)(1)x is called the hook, and y the target.
In order totrain a Rote extractor from the web, this procedureis mostly used (Ravichandran and Hovy, 2002):1.
Select a pair of related elements to be usedas seed.
For instance, (Dickens,1812) for therelation birth year.2.
Submit the query Dickens AND 1812 to asearch engine, and download a number ofdocuments to build the training corpus.3.
Keep all the sentences containing both ele-ments.4.
Extract the set of contexts between them andidentify repeated patterns.
This may just bethe m characters to the left or to the right(Brin, 1998), the longest common substringof several contexts (Agichtein and Gravano,2000), or all substrings obtained with a suf-fix tree constructor (Ravichandran and Hovy,2002).5.
Download a separate corpus, called hook cor-pus, containing just the hook (in the example,Dickens).6.
Apply the previous patterns to the hook cor-pus, calculate the precision of each patternin the following way: the number of times itidentifies a target related to the hook dividedby the total number of times the pattern ap-pears.7.
Repeat the procedure for other examples ofthe same relation.To illustrate this process, let us suppose that wewant to learn patterns to identify birth years.
Wemay start with the pair (Dickens, 1812).
From thedownloaded corpus, we extract sentences such asDickens was born in 1812Dickens (1812 - 1870) was an English writerDickens (1812 - 1870) wrote Oliver TwistThe system identifies that the contexts of the lasttwo sentences are very similar and chooses theirlongest common substring to produce the follow-ing patterns:<hook> was born in <target><hook> ( <target> - 1870 )The rote extractor needs to estimate automati-cally the precision of the extracted patterns, in or-der to keep the best ones.
So as to measure theseprecision values, a hook corpus is now down-loaded using the hook Dickens as the only queryword, and the system looks for appearances of thepatterns in this corpus.
For every occurrence inwhich the hook of the relation is Dickens, if thetarget is 1812 it will be deemed correct, and oth-erwise it will be deemed incorrect (e.g.
in Dickenswas born in Portsmouth).3 Our proposal3.1 MotivationIn a rote extractor as described above, we believethat the procedure for calculating the precision ofthe patterns may be unreliable in some cases.
Forexample, the following patterns are reported byRavichandran and Hovy (2002) for identifying therelations Inventor, Discoverer and Location:Relation Prec.
PatternInventor 1.0 <target> ?s <hook> andInventor 1.0 that <target> ?s <hook>Discoverer 0.91 of <target> ?s <hook>Location 1.0 <target> ?s <hook>In the particular application in which they areused (relation extraction for Question Answering),they are useful because there is initially a ques-tion to be answered that indicates whether we are50looking for an invention, a discovery or a location.However, if we want to apply them to unrestrictedrelation extraction, we have the problem that thesame pattern, the genitive construction, representsall these relations, apart from the most commonuse indicating possession.If patterns like these are so ambiguous, thenwhy do they receive so high a precision estimate?One reason is that the patterns are only evalu-ated for the same hook for which they were ex-tracted.
To illustrate this with an example, letus suppose that we obtain a pattern for the rela-tion located-at using the pairs (New York, ChryslerBuilding).
The genitive construction can be ex-tracted from the context New York?s ChryslerBuilding.
Afterwards, when estimating the pre-cision of this pattern, only sentences containing<target>?s Chrysler Building are taken into ac-count.
Because of this, most of the pairs extractedby this pattern may extract the target New York,apart from a few that extract the name of the ar-chitect that built it, van Allen.
Thus we can expectthat the genitive pattern will receive a high preci-sion estimate as a located-at pattern.For our purposes, however, we want to collectpatterns for several relations such as writer-book,painter-picture, director-film, actor-film, and wewant to make sure that the obtained patterns areonly applicable to the desired relation.
Patternslike <target> ?s <hook> are very likely to be ap-plicable to all of these relations at the same time,so we would like to be able to discard them auto-matically by assigning them a low precision.3.2 Suggested improvementsTherefore, we propose the following three im-provements to this procedure:1.
Collecting not only a hook corpus but also atarget corpus should help in calculating theprecision.
In the example of the Chryslerbuilding, we have seen that in most casesthat we look for the pattern ?s Chrysler build-ing the previous words are New York, andso the pattern is considered accurate.
How-ever, if we look for the pattern New York?s,we shall surely find it followed by many dif-ferent terms representing different relations,and the precision estimate will decrease.2.
Testing the patterns obtained for one relationusing the hook and target corpora collectedfor other relations.
For instance, if the geni-tive construction has been extracted as a pos-sible pattern for the writer-book relation, andwe apply it to a corpus about painters, the roteextractor can detect that it also extracts pairswith painters and paintings, so that particularpattern will not be very precise for that rela-tion.3.
Many of the pairs extracted by the patternsin the hook corpora were not evaluated at allwhen the hook in the extracted pair was notpresent in the seed lists.
To overcome this,we propose to use the web to check whetherthe extracted pair might be correct, as shownbelow.3.3 AlgorithmIn our implementation, the rote extractor startswith a table containing some information about therelations for which we want to learn patterns.
Thisprocedure needs a little more information than justthe seed list, which is provided as a table in theformat displayed in Table 1.
The data provided foreach relation is the following: (a) The name of therelation, used for naming the output files contain-ing the patterns; (b) the name of the file contain-ing the seed list; (c) the cardinality of the relation.For instance, given that many people can be bornon the same year, but for every person there is justone birth year, the cardinality of the relation birthyear is n:1; (d) the restrictions on the hook andthe target.
These can be of the following three cat-egories: unrestricted, if the pattern can extract anysequence of words as hook or target of the relation,Entity, if the pattern can extract as hook or targetonly things of the same entity type as the wordsin the seed list (as annotated by the NERC mod-ule), or PoS, if the pattern can extract as hook ortarget any sequence of words whose sequence ofPoS labels was seen in the training corpus; and (e)a sequence of queries that could be used to check,using the web, whether an extracted pair is corrector not.We assume that the system has used the seed listto extract and generalise a set of patterns for eachof the relations using training corpora (Ravichan-dran and Hovy, 2002; Alfonseca et al, 2006a).Our procedure for calculating the patterns?
preci-sions is as follows:1.
For every relation,(a) For every hook, collect a hook corpusfrom the web.51Relation name Seed-list Cardinality Hook-type Target-type Web queriesbirth year birth-date.txt n:1 entity entity $1 was born in $2death year death-date.txt n:1 entity entity $1 died in $2birth place birth-place.txt n:1 entity entity $1 was born in $2country-capital country-capital.txt 1:1 entity entity $2 is the capital of $1author-book author-book.txt n:n entity unrestricted $1 is the author of $2director-film director-film.txt 1:n entity unrestricted $1 directed $2, $2 directed by $1Table 1: Example rows in the input table for the system.
(b) For every target, collect a target corpusfrom the web.2.
For every relation r,(a) For every pattern P , collected duringtraining, apply it to every hook and tar-get corpora to extract a set of pairs.For every pair p = (ph, pt),?
If it appears in the seed list of r, con-sider it correct.?
If it appears in the seed list of otherrelation, consider it incorrect.?
If the hook ph appears in the seed listof r with a different target, and thecardinality is 1:1 or n:1, consider itincorrect.?
If the target pt appears in r?s seed listwith a different hook, and the cardi-nality is 1:1 or 1:n, incorrect.?
Otherwise, the seed list does notprovide enough information to eval-uate p, so we perform a test on theweb.
For every query provided for r,the system replaces $1 with ph and$2 with pt, and sends the query toGoogle.
The pair is deemed correctif and only if there is at least one an-swer.The precision of P is estimated as thenumber of extracted pairs that are sup-posedly correct divided by the totalnumber of pairs extracted.In this step, every pattern that did not apply atleast twice in the hook and target corpora is alsodiscarded.3.4 ExampleAfter collecting and generalising patterns forthe relation director-film, we apply each pat-tern to the hook and target corpora collectedfor every relation.
Let us suppose that wewant to estimate the precision of the pattern<target> ?s <hook>and we apply it to the hook and the target cor-pora for this relation and for author-book.
Pos-sible pairs extracted are (Woody Allen, Bananas),(Woody Allen, Without Fears), (Charles Dickens,A Christmas Carol).
Only the first one is correct.The rote extractor proceeds as follows:?
The first pair appears in the seed list, so it isconsidered correct.?
Although Woody Allen appears as hook in theseed list andWithout Fears does not appear astarget, the second pair is still not consideredincorrect because the directed-by relation hasn:n cardinality.?
The third pair appears in the seed list forwriter-book, so it is directly marked as incor-rect.?
Finally, because still the system has not madea decision about the second pair, it queriesGoogle with the sequencesWoody Allen directed Without FearsWithout Fears directed by Woody AllenBecause neither of those queries provide anyanswer, it is considered incorrect.In this way, it can be expected that the patternsthat are equally applicable to several relations,such as writer-book, director-film or painter-picture will attain a low precision because theywill extract many incorrect relations from the cor-pora corresponding to the other relations.4 Experiment and results4.1 Rote extractor settingsThe initial steps of the rote extractor follows thegeneral approach: downloading a training cor-pus using the seed list and extracting patterns.The training corpora are processed with a part-of-speech tagger and a module for Named EntityRecognition and Classification (NERC) that anno-tates people, organisations, locations, dates, rela-tive temporal expressions and numbers (Alfonsecaet al, 2006b), so this information can be includedin the patterns.
Furthermore, for each of the termsin a pair in the training corpora, the system also52Birth year:BOS/BOS <hook> (/( <target> -/- number/entity )/) EOS/EOSBOS/BOS <hook> (/( <target> -/- number/entity )/) British/JJ writer/NNBOS/BOS <hook> was/VBD born/VBN on/IN the/DT first/JJ of/IN time expr/entity ,/, <target> ,/, at/IN location/entity ,/, of/INBOS/BOS <hook> (/( <target> -/- )/) a/DT web/NN guide/NNBirth place:BOS/BOS <hook> was/VBD born/VBN in/IN <target> ,/, in/IN central/JJ location/entity ,/,BOS/BOS <hook> was/VBD born/VBN in/IN <target> date/entity and/CC moved/VBD to/TO location/entityBOS/BOS Artist/NN :/, <hook> -/- <target> ,/, location/entity (/( number/entity -/-BOS/BOS <hook> ,/, born/VBN in/IN <target> on/IN date/entity ,/, worked/VBN as/INAuthor-book:BOS/BOS <hook> author/NN of/IN <target> EOS/EOSBOS/BOS Odysseus/NNP :/, Based/VBN on/IN <target> ,/, <hook> ?s/POS epic/NN from/IN Greek/JJ mythology/NNBOS/BOS Background/NN on/IN <target> by/IN <hook> EOS/EOSdid/VBD the/DT circumstances/NNS in/IN which/WDT <hook> wrote/VBD "/??
<target> "/??
in/IN number/entity ,/, and/CCCapital-country:BOS/BOS <hook> is/VBZ the/DT capital/NN of/IN <target> location/entity ,/, location/entity correct/JJ time/NNBOS/BOS The/DT harbor/NN in/IN <hook> ,/, the/DT capital/NN of/IN <target> ,/, is/VBZ number/entity of/IN location/entityBOS/BOS <hook> ,/, <target> EOS/EOSBOS/BOS <hook> ,/, <target> -/- organization/entity EOS/EOSFigure 1: Example patterns extracted from the training corpus for each several kinds of relations.stores in a separate file the way in which they areannotated in the training corpus: the sequences ofpart-of-speech tags of every appearance, and theentity type (if marked as such).
So, for instance,typical PoS sequences for names of authors are?NNP?1 (surname) and ?NNP NNP?
(first nameand surname).
A typical entity kind for an authoris person.In the case that a pair from the seed list is foundin a sentence, a context around the two words inthe pair is extracted, including (a) at most fivewords to the left of the first word; (b) all thewords in between the pair words; (c) at most fivewords to the right of the second word.
The contextnever jumps over sentence boundaries, which aremarked with the symbols BOS (Beginning of sen-tence) and EOS (End of sentence).
The two relatedconcepts are marked as <hook> and <target>.Figure 1 shows several example contexts extractedfor the relations birth year, birth place, writer-book and country-capital city.The approach followed for the generalisationis the one described by (Alfonseca et al, 2006a;Ruiz-Casado et al, in press), which has a fewmodifications with respect to Ravichandran andHovy (2002)?s, such as the use of the wildcard * torepresent any sequence of words, and the additionof part-of-speech and Named Entity labels to thepatterns.The input table has been built with the fol-lowing nineteen relations: birth year, death year,birth place, death place, author?book, actor?film, director?film, painter?painting, Employee?organisation, chief of state, soccer player?team,1All the PoS examples in this paper are done with PennTreebank labels.Relation Seeds Extr.
Gener.
Filt.Birth year 244 2374 4748 30Death year 216 2178 4356 14Birth place 169 764 1528 28Death place 76 295 590 6Author-book 198 8297 16594 283Actor-film 49 739 1478 3Director-film 85 6933 13866 200Painter-painting 92 597 1194 15Employee-organisation 62 1667 3334 6Chief of state 55 1989 3978 8Soccer player-team 194 4259 8518 39Soccer team-city 185 180 360 0Soccer team-manager 43 994 1988 9Country/region-capital city 222 4533 9066 107Country/region-area 226 762 1524 2Country/region-population 288 318 636 3Country-bordering country 157 6828 13656 240Country-inhabitant 228 2711 5422 17Country-continent 197 1606 3212 21Table 2: Number of seed pairs for each relation,and number of unique patterns in each step.soccer team-city, soccer team-manager, countryor region?capital city, country or region?area,country or region?population, country?borderingcountry, country-name of inhabitant (e.g.
Spain-Spaniard), and country-continent.
The time re-quired to build the table and the seed lists was lessthan one person-day, as some of the seed lists weredirectly collected from web pages.For each step, the following settings have beenset:?
The size of the training corpus has been setto 50 documents for each pair in the originalseed lists.
Given that the typical sizes of thelists collected are between 50 and 300 pairs,this means that several thousand documentsare downloaded for each relation.?
Before the generalisation step, the rote ex-tractor discards those patterns in which thehook and the target are too far away to eachother, because they are usually difficult togeneralise.
The maximum allowed distance53No.
Pattern Applied Prec1 Prec2 Real1Biography|Hymns|Infography|Life|Love|POETRY|Poetry|Quotations|Search|Sketch|Woolf|charts|genius|kindness|poets/NN */*OF|Of|about|by|for|from|like|of/IN <hook> (/( <target> -/-6 1.00 1.00 1.002 "/??
<hook> (/( <target> -/- 4 1.00 1.00 1.003[BOS]/[BOS] <hook> was/VBD born/VBN about|around|in/IN <target>B.C.|B.C.E|BC/NNP at|in/IN3 1.00 1.00 1.004[BOS]/[BOS] <hook> was/VBD born/VBN about|around|in/IN <target>B.C.|B.C.E|BC/NNP at|in/IN location/entity3 1.00 1.00 1.005[BOS]/[BOS] <hook> was/VBD born/VBN around/IN <target> B.C.E/NNP at/INlocation/entity ,/, a/DT3 1.00 1.00 1.006[BOS]/[BOS] <hook> was/VBD born/VBN around|in/IN <target> B.C.|B.C.E/NNPat|in/IN location/entity ,/,3 1.00 1.00 1.007[BOS]/[BOS] */* ATTRIBUTION|Artist|Author|Authors|Composer|Details|Email|Extractions|Myth|PAL|Person|Quotes|Title|Topic/NNP :/, <hook> (/(<target> -/-3 1.00 1.00 1.008classical/JJ playwrights/NNS of/IN organisation/entity ,/, <hook> was/VBDborn/VBN near/IN location/entity in/IN <target> BCE/NNP ,/, in/IN the/DTvillage/NN3 1.00 1.00 1.009 [BOS]/[BOS] <hook> (/( <target> -/- )/) 2 1.00 1.00 1.0010 [BOS]/[BOS] <hook> (/( <target> -|--/- )/) 2 1.00 1.00 1.0011 [BOS]/[BOS] <hook> (/( <target> person/entity BC/NNP ;/, Greek/NNP :/, 2 1.00 1.00 1.0012ACCESS|AND|Alice|Author|Authors|BY|Biography|CARL|Dame|Don|ELIZABETH|(...)|web|writer|writerMuriel|years/NNP <hook> (/( <target> -|- -/-8 0.75 1.0013 -/- <hook> (/( <target> -/- 3 0.67 1.00 0.6714 -|--/- <hook> (/( <target> -/- 3 0.67 1.00 0.6715 [BOS]/[BOS] <hook> (/( <target> -/- 60 0.62 1.00 0.8116 [BOS]/[BOS] <hook> (/( <target> -/- */* )/) 60 0.62 1.00 0.8117 [BOS]/[BOS] <hook> (/( <target> -|--/- 60 0.62 1.00 0.8118 ,|:/, <hook> (/( <target> -/- 32 0.41 0.67 0.2819 [BOS]/[BOS] <hook> ,/, */* (/( <target> -|--/- 15 0.40 1.00 0.6720 ,|:|;/, <hook> (/( <target> -|--/- 34 0.38 0.67 0.2921AND|Alice|Authors|Biography|Dame|Don|ELIZABETH|Email|Fiction|Frances|GEORGE|Home|I.|Introduction|Jean|L|Neben|PAL|PAULA|Percy|Playwrights|Poets|Sir|Stanisaw|Stanislaw|W.|WILLIAM|feedback|history|writer/NNP <hook>(/( <target> -/-3 0.33 n/a 0.6722 AND|Frances|Percy|Sir/NNP <hook> (/( <target> -/- 3 0.33 n/a 0.6723Alice|Authors|Biography|Dame|Don|ELIZABETH|Email|Fiction|Frances|GEORGE|Home|I.|Introduction|Jean|L|Neben|PAL|PAULA|Percy|Playwrights|Poets|Sir|Stanisaw|Stanislaw|W.|WILLIAM|feedback|history|writer/NN <hook>(/( <target> -/-3 0.33 n/a 0.6724 [BOS]/[BOS] <hook> ,|:/, */* ,|:/, <target> -/- 7 0.28 0.67 0.4325 [BOS]/[BOS] <hook> ,|:/, <target> -/- 36 0.19 1.00 0.1126 [BOS]/[BOS] <hook> ,/, */* (/( <target> )/) 20 0.15 0.33 0.1027 [BOS]/[BOS] <target> <hook> ,/, 18 0.00 n/a 0.0028 In|On|on/IN <target> ,/, <hook> grew|was/VBD 17 0.00 0.00 0.0029 In|On|on/IN <target> ,/, <hook> grew|was|went/VBD 17 0.00 0.00 0.0030[BOS]/[BOS] <hook> ,/, */* DE|SARAH|VON|dramatist|novelist|playwright|poet/NNP (/( <target> -/-3 0.00 n/a 1.0TOTAL 436 0.46 0.84 0.54Table 3: Patterns for the relation birth year, results extracted by each, precision estimated with thisprocedure and with the traditional hook corpus approach, and precision evaluated by hand).between them has been set to 8 words.?
At each step, the two most similar patternsare generalised, and their generalisation isadded to the set of patterns.
No pattern is dis-carded at this step.
This process stops whenall the patterns resulting from the generalisa-tion of existing ones contain wildcards adja-cent to either the hook or the target.?
For the precision estimation, for each pair inthe seed lists, 50 documents are collected forthe hook and other 50 for the target.
Becauseof time constraints, and given that the totalsize of the hook and the target corpora ex-ceeds 100,000 documents, for each pattern asample of 250 documents is randomly cho-sen and the patterns are applied to it.
Thissample is built randomly but with the fol-lowing constraints: there should be an equalamount of documents selected from the cor-pora from each relationship; and there shouldbe an equal amount of documents from hookcorpora and from target corpora.4.2 Output obtainedTable 2 shows the number of patterns obtained foreach relation.
Note that the generalisation proce-dure applied produces new (generalised) patternsto the set of original patterns, but no original pat-tern is removed, so they all are evaluated; this iswhy the set of patterns increases after the gener-alisation.
The filtering criterion was to keep thepatterns that applied at least twice on the test cor-pus.It is interesting to see that for most relations thereduction of the pruning is very drastic.
This isbecause of two reasons: Firstly, most patterns arefar too specific, as they include up to 5 words ateach side of the hook and the target, and all thewords in between.
Only those patterns that havegeneralised very much, substituting large portionswith wildcards or disjunctions are likely to applyto the sentences in the hook and target corpora.54Secondly, the samples of the hook and target cor-pora used are too small for some of the relationsto apply, so few patterns apply more than twice.Note that, for some relations, the output of thegeneralisation step contains less patterns that theoutput of the initial extraction step: that is due tothe fact that the patterns in which the hook andthe target are not nearby were removed in betweenthese two steps.Concerning the precision estimates, a full eval-uation is provided for the birth-year relation.
Ta-ble 3 shows in detail the thirty patterns obtained.It can also be seen that some of the patterns withgood precision contain the wildcard *.
For in-stance, the first pattern indicates that the presenceof any of the words biography, poetry, etc.
any-where in a sentence before a person name and adate or number between parenthesis is a strong in-dication that the target is a birth year.The last columns in the table indicate the num-ber of times that each rule applied in the hook andtarget corpora, and the precision of the rule in eachof the following cases:?
As estimated by the complete program(Prec1).?
As estimated by the traditional hook cor-pus approach (Prec2).
Here, cardinality isnot taken into account, patterns are evaluatedonly on the hook corpora from the same rela-tion, and those pairs whose hook is not in theseed list are ignored.?
The real precision of the rule (real).
In or-der to obtain this metric, two different an-notators evaluated the pairs applied indepen-dently, and the precision was estimated fromthe pairs in which they agreed (there was a96.29% agreement, Kappa=0.926).As can be seen, in most of the cases our procedureproduces lower precision estimates.If we calculate the total precision of all the rulesaltogether, shown in the last row of the table, wecan see that, without the modifications, the wholeset of rules would be considered to have a totalprecision of 0.84, while that estimate decreasessharply to 0.46 when they are used.
This valueis nearer the precision of 0.54 evaluated by hand.Although it may seem surprising that the precisionestimated by the new procedure is even lower thanthe real precision of the patterns, as measured byhand, that is due to the fact that the web queriesconsider unknown pairs as incorrect unless theyRelation Prec1 Prec2 RealBirth year 0.46 [0.41,0.51] 0.84 [0.81,0.87] 0.54 [0.49,0.59]Death year 0.29 [0.24,0.34] 0.55 [0.41,0.69] 0.38 [0.31,0.44]Birth place 0.65 [0.62,0.69] 0.36 [0.29,0.43] 0.84 [0.79,0.89]Death place 0.82 [0.73,0.91] 1.00 [1.00,1.00] 0.96 [0.93,0.99]Author-book 0.07 [0.07,0.07] 0.26 [0.19,0.33] 0.03 [0.00,0.05]Actor-film 0.07 [0.01,0.13] 1.00 [1.00,1.00] 0.02 [0.00,0.03]Director-film 0.03 [0.03,0.03] 0.26 [0.18,0.34] 0.01 [0.00,0.01]Painter-painting 0.10 [0.07,0.12] 0.35 [0.23,0.47] 0.17 [0.12,0.22]Employee-organisation 0.31 [0.22,0.40] 1.00 [1.00,1.00] 0.33 [0.26,0.40]Chief of state 0.00 [0.00,0.00] - 0.00 [0.00,0.00]Soccer player-team 0.07 [0.06,0.08] 1.00 [1.00,1.00] 0.08 [0.04,0.12]Soccer team-city - - -Soccer team-manager 0.61 [0.53,0.69] 1.00 [1.00,1.00] 0.83 [0.77,0.88]Country/region-capital city 0.12 [0.11,0.13] 0.23 [0.22,0.24] 0.12 [0.07,0.16]Country/region-area 0.09 [0.00,0.19] 1.00 [1.00,1.00] 0.06 [0.02,0.09]Country/region-population 1.00 [1.00,1.00] 1.00 [1.00,1.00] 1.00 [1.00,1.00]Country-bordering country 0.17 [0.17,0.17] 1.00 [1.00,1.00] 0.15 [0.10,0.20]Country-inhabitant 0.01 [0.00,0.01] 0.80 [0.67,0.93] 0.01 [0.00,0.01]Country-continent 0.16 [0.14,0.18] 0.07 [0.04,0.10] 0.00 [0.00,0.01]Table 4: Precision estimates for the whole set ofextracted pairs by all rules and all relations.appear in the web exactly in the format of thequery in the input table.
Specially for not verywell-known people, we cannot expect that all ofthem will appear in the web following the pattern?X was born in date?, so the web estimates tendto be over-conservative.Table 4 shows the precision estimates for everypair extracted with all the rules using both proce-dures, with 0.95 confidence intervals.
The realprecision has been estimating by sampling ran-domly 200 pairs and evaluating them by hand, asexplained above for the birth year relation.
As canbe observed, out of the 19 relations, the precisionestimate of the whole set of rules for 11 of themis not statistically dissimilar to the real precision,while that only holds for two relationships usingthe previous approach.Please note as well that the precisions indicatedin the table refer to all the pairs extracted by all therules, some of which are very precise, but some ofwhich are very imprecise.
If the rules are to beapplied in an annotation system, only those witha high precision estimate would be used, and ex-pectedly much better overall results would be ob-tained.5 Conclusions and future workWe have described here a new procedure for es-timating the precision of the patterns learnt by arote extractor that learns from the web.
Comparedto other similar approaches, it has the followingimprovements:?
For each pair (hook,target) in the seed list, atarget corpora is also collected (apart fromthe hook corpora), and the evaluation is per-formed using corpora from several relations.55This has been observed to improve the esti-mate of the rule?s precision, given that theevaluation pairs not only refer to the elementsin the seed list.?
The cardinality of the relations is taken intoconsideration in the estimation process usingthe seed list.
This is important, for instance,to be able to estimate the precision in n:n re-lations like author-work, given that we can-not assume that the only books written bysomeone are those in the seed list.?
For those pairs that cannot be evaluated usingthe seed list, a simple query to the Googlesearch engine is employed.The precisions estimated with this procedureare significantly lower than the precisions obtainedwith the usual hook corpus approach, specially forambiguous patterns, and much near the precisionestimate when evaluated by hand.Concerning future work, we plan to estimate theprecision of the patterns using the whole hook andtarget corpora, rather than using a random sample.A second objective we have in mind is not to throwaway the ambiguous patterns with low precision(e.g.
the possessive construction), but to train amodel so that we can disambiguate which is therelation they are conveying in each context (Girjuet al, 2003).ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: Ex-tracting relations from large plain-text collections.In Proceedings of ICDL, pages 85?94.E.
Alfonseca, P. Castells, M. Okumura, and M. Ruiz-Casado.
2006a.
A rote extractor with edit distance-based generalisation and multi-corpora precisioncalculation.
In Poster session of ACL-2006.E.
Alfonseca, A. Moreno-Sandoval, J. M. Guirao, andM.
Ruiz-Casado.
2006b.
The wraetlic NLP suite.In Proceedings of LREC-2006.M.
Berland and E. Charniak.
1999.
Finding parts invery large corpora.
In Proceedings of ACL-99.S.
Brin.
1998.
Extracting patterns and relations fromthe World Wide Web.
In Proceedings of the WebDBWorkshop at EDBT?98.R.
Bunescu and R. J. Mooney.
2005.
A shortest pathdependency kernel for relation extraction.
In Pro-ceedings of the HLT Conference and EMNLP.P.
Cimiano, S. Handschuh, and S. Staab.
2004.
To-wards the self-annotating web.
In Proceedings of the13th World Wide Web Conference, pages 462?471.M.
Craven, D. DiPasquo, D. Freitag, A. McCallum,T.
Mitchell, K. Nigam, and S. Slattery.
1999.
Learn-ing to construct knowledge bases from the worldwide web.
Artificial Intelligence, 118(1?2):69?113.O.
Etzioni, M. Cafarella, D. Downey, A.-M. Popescu,T.
Shaked, S. Soderland, D. S. Weld, and A. Yates.2005.
Unsupervised named entity extraction fromthe web: An experimental study.
Artificial Intelli-gence, 165(1):91?134.M.
Finkelstein-Landau and E. Morin.
1999.
Extractingsemantic relationships between terms: supervisedvs.
unsupervised methods.
In Workshop on Ontolo-gial Engineering on the Global Info.
Infrastructure.R.
Girju, A. Badulescu, and D. Moldovan.
2003.Learning semantic constraints for the automatic dis-covery of part-whole relations.
In HLT-NAACL-03.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In COLING-92.S.
Huffman.
1995.
Learning information extractionpatterns from examples.
In IJCAI-95 Workshop onNew Approaches to Learning for NLP.J.
Kietz, A. Maedche, and R. Volz.
2000.
A methodfor semi-automatic ontology acquisition from a cor-porate intranet.
In Workshop ?Ontologies and text?.G.
S. Mann and D. Yarowsky.
2003.
Unsupervisedpersonal name disambiguation.
In CoNLL-2003.G.
S. Mann and D. Yarowsky.
2005.
Multi-field in-formation extraction and cross-document fusion.
InProceedings of ACL 2005.E.
Morin and C. Jacquemin.
1999.
Projecting corpus-based semantic links on a thesaurus.
In ACL-99.D.
Ravichandran and E. Hovy.
2002.
Learning surfacetext patterns for a question answering system.
InProceedings of ACL-2002, pages 41?47.E.
Riloff and M. Schmelzenbach.
1998.
An empiricalapproach to conceptual case frame acquisition.
InProceedings of WVLC, pages 49?56.E.
Riloff.
1996.
Automatically generating extractionpatterns from untagged text.
In AAAI.M.
Ruiz-Casado, E. Alfonseca, and P. Castells.
inpress.
Automatising the learning of lexical patterns:an application to the enrichment of WordNet by ex-tracting semantic relationships from the Wikipedia.Data and Knowledge Engineering, in press.S.
Soderland.
1999.
Learning information extractionrules for semi-structured and free text.
MachineLearning, 34(1?3):233?272.I.
Szpektor, H. Tanev, I. Dagan, and B. Coppola.
2004.Scaling web-based acquisition of entailment rela-tions.
In Proceedings of EMNLP 2004.S.
Zhao and R. Grishman.
2005.
Extracting relationswith integrated information using kernel methods.In Proceedings of ACL-2005.56
