Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1660?1669,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsCached Long Short-Term Memory Neural Networksfor Document-Level Sentiment ClassificationJiacheng Xu?
Danlu Chen?
Xipeng Qiu??
Xuanjing Huang?Software School, Fudan University ?School of Computer Science, Fudan University ?Shanghai Key Laboratory of Intelligent Information Processing, Fudan University?
?825 Zhangheng Road, Shanghai, China??
{jcxu13,dlchen13,xpqiu,xjhuang}@fudan.edu.cnAbstractRecently, neural networks have achieved greatsuccess on sentiment classification due to theirability to alleviate feature engineering.
How-ever, one of the remaining challenges is tomodel long texts in document-level sentimentclassification under a recurrent architecturebecause of the deficiency of the memory unit.To address this problem, we present a CachedLong Short-Term Memory neural networks(CLSTM) to capture the overall semantic in-formation in long texts.
CLSTM introducesa cache mechanism, which divides memoryinto several groups with different forgettingrates and thus enables the network to keepsentiment information better within a recur-rent unit.
The proposed CLSTM outperformsthe state-of-the-art models on three publiclyavailable document-level sentiment analysisdatasets.1 IntroductionSentiment classification is one of the most widelyused natural language processing techniques inmany areas, such as E-commerce websites, onlinesocial networks, political orientation analyses (Wil-son et al, 2009; O?Connor et al, 2010), etc.Recently, deep learning approaches (Socher et al,2013; Kim, 2014; Chen et al, 2015; Liu et al, 2016)have gained encouraging results on sentiment clas-sification, which frees researchers from handcraftedfeature engineering.
Among these methods, Recur-rent Neural Networks (RNNs) are one of the most?
Corresponding author.prevalent architectures because of the ability to han-dle variable-length texts.Sentence- or paragraph-level sentiment analysisexpects the model to extract features from limitedsource of information, while document-level senti-ment analysis demands more on selecting and stor-ing global sentiment message from long texts withnoises and redundant local pattern.
Simple RNNsare not powerful enough to handle the overflow andto pick up key sentiment messages from relativelyfar time-steps .Efforts have been made to solve such a scalabil-ity problem on long texts by extracting semantic in-formation hierarchically (Tang et al, 2015a; Tai etal., 2015), which first obtain sentence representa-tions and then combine them to generate high-leveldocument embeddings.
However, some of these so-lutions either rely on explicit a priori structural as-sumptions or discard the order information withina sentence, which are vulnerable to sudden changeor twists in texts especially a long-range one (Mc-Donald et al, 2007; Mikolov et al, 2013).
Re-current models match people?s intuition of readingword by word and are capable to model the intrinsicrelations between sentences.
By keeping the wordorder, RNNs could extract the sentence representa-tion implicitly and meanwhile analyze the semanticmeaning of a whole document without any explicitboundary.Partially inspired by neural structure of humanbrain and computer system architecture, we presentthe Cached Long Short-Term Memory neural net-works (CLSTM) to capture the long-range senti-ment information.
In the dual store memory model1660proposed by Atkinson and Shiffrin (1968), memo-ries can reside in the short-term ?buffer?
for a lim-ited time while they are simultaneously strengthen-ing their associations in long-term memory.
Accord-ingly, CLSTM equips a standard LSTM with a sim-ilar cache mechanism, whose internal memory is di-vided into several groups with different forgettingrates.
A group with high forgetting rate plays a roleas a cache in our model, bridging and transiting theinformation to groups with relatively lower forget-ting rates.
With different forgetting rates, CLSTMlearns to capture, remember and forget semantics in-formation through a very long distance.Our main contributions are as follows:?
We introduce a cache mechanism to diversifythe internal memory into several distinct groupswith different memory cycles by squashingtheir forgetting rates.
As a result, our model cancapture the local and global emotional informa-tion, thereby better summarizing and analyzingsentiment on long texts in an RNN fashion.?
Benefiting from long-term memory unit with alow forgetting rate, we could keep the gradi-ent stable in the long back-propagation process.Hence, our model could converge faster than astandard LSTM.?
Our model outperforms state-of-the-art meth-ods by a large margin on three document-leveldatasets (Yelp 2013, Yelp 2014 and IMDB).
Itworth noticing that some of the previous meth-ods have utilized extra user and product infor-mation.2 Related WorkIn this section, we briefly introduce related work intwo areas: First, we discuss the existing document-level sentiment classification approaches; Second,we discuss some variants of LSTM which addressthe problem on storing the long-term information.2.1 Document-level Sentiment ClassificationDocument-level sentiment classification is a stickytask in sentiment analysis (Pang and Lee, 2008),which is to infer the sentiment polarity or intensityof a whole document.
The most challenging part isthat not every part of the document is equally in-formative for inferring the sentiment of the wholedocument (Pang and Lee, 2004; Yessenalina et al,2010).
Various methods have been investigated andexplored over years (Wilson et al, 2005; Pang andLee, 2008; Pak and Paroubek, 2010; Yessenalinaet al, 2010; Moraes et al, 2013).
Most of thesemethods depend on traditional machine learning al-gorithms, and are in need of effective handcraftedfeatures.Recently, neural network based methods areprevalent due to their ability of learning discrimina-tive features from data (Socher et al, 2013; Le andMikolov, 2014; Tang et al, 2015a).
Zhu et al (2015)and Tai et al (2015) integrate a tree-structuredmodel into LSTM for better semantic composi-tion; Bhatia et al (2015) enhances document-levelsentiment analysis by using extra discourse par-ing results.
Most of these models work well onsentence-level or paragraph-level sentiment classifi-cation.
When it comes to the document-level sen-timent classification, a bottom-up hierarchical strat-egy is often adopted to alleviate the model complex-ity (Denil et al, 2014; Tang et al, 2015b; Li et al,2015).2.2 Memory Augmented Recurrent ModelsAlthough it is widely accepted that LSTM has morelong-lasting memory units than RNNs, it still suffersfrom ?forgetting?
information which is too far awayfrom the current point (Le et al, 2015; Karpathy etal., 2015).
Such a scalability problem of LSTMs iscrucial to extend some previous sentence-level workto document-level sentiment analysis.Various models have been proposed to increasethe ability of LSTMs to store long-range informa-tion (Le et al, 2015; Salehinejad, 2016) and twokinds of approaches gain attraction.
One is to aug-ment LSTM with an external memory (Sukhbaataret al, 2015; Monz, 2016), but they are of poor per-formance on time because of the huge external mem-ory matrix.
Unlike these methods, we fully exploitthe potential of internal memory of LSTM by adjust-ing its forgetting rates.The other one tries to use multiple time-scalesto distinguish different states (El Hihi and Bengio,1995; Koutnik et al, 2014; Liu et al, 2015).
Theypartition the hidden states into several groups andeach group is activated and updated at different fre-quencies (e.g.
one group updates every 2 time-step,1661the other updates every 4 time-step).
In these meth-ods, different memory groups are not fully inter-connected, and the information is transmitted fromfaster groups to slower ones, or vice versa.However, the memory of slower groups are notupdated at every step, which may lead to senti-ment information loss and semantic inconsistency.In our proposed CLSTM, we assign different forget-ting rates to memory groups.
This novel strategyenable each memory group to be updated at everytime-step, and every bit of the long-term and short-term memories in previous time-step to be taken intoaccount when updating.3 Long Short-Term Memory NetworksLong short-term memory network (LSTM) (Hochre-iter and Schmidhuber, 1997) is a typical recurrentneural network, which alleviates the problem of gra-dient diffusion and explosion.
LSTM can capturethe long dependencies in a sequence by introducinga memory unit and a gate mechanism which aimsto decide how to utilize and update the informationkept in memory cell.Formally, the update of each LSTM componentcan be formalized as:i(t) = ?
(Wix(t) +Uih(t?1)), (1)f (t) = ?
(Wfx(t) +Ufh(t?1)), (2)o(t) = ?
(Wox(t) +Uoh(t?1)), (3)c?
(t) = tanh(Wcx(t) +Uch(t?1)), (4)c(t) = f (t)  c(t?1) + i(t)  c?
(t), (5)h(t) = o(t)  tanh(c(t)), (6)where ?
is the logistic sigmoid function.
Opera-tor  is the element-wise multiplication operation.i(t), f (t), o(t) and c(t) are the input gate, forget gate,output gate, and memory cell activation vector attime-step t respectively, all of which have the samesize as the hidden vector h(t) ?
RH .
Wi, Wf ,Wo ?
RH?d and Ui, Uf , Uo ?
RH?H are train-able parameters.
Here, H and d are the dimension-ality of hidden layer and input respectively.+C?+C~outputforgetinputINOUTC?+C~outputCIFGINOUTFigure 1: (a) A standard LSTM unit and (b) a CIFG-LSTM unit.
There are three gates in (a), the inputgate, forget gate and output gates, while in (b), thereare only two gates, the CIFG gate and output gate.4 Cached Long Short-Term MemoryNeural NetworkLSTM is supposed to capture the long-term andshort-term dependencies simultaneously, but whendealing with considerably long texts, LSTM alsofails on capturing and understanding significant sen-timent message (Le et al, 2015).
Specifically, theerror signal would nevertheless suffer from gradientvanishing in modeling long texts with hundreds ofwords and thus the network is difficult to train.Since the standard LSTM inevitably loses valu-able features, we propose a cached long short-termmemory neural networks (CLSTM) to capture in-formation in a longer steps by introducing a cachemechanism.
Moreover, in order to better control andbalance the historical message and the incoming in-formation, we adopt one particular variant of LSTMproposed by Greff et al (2015), the Coupled Inputand Forget Gate LSTM (CIFG-LSTM).Coupled Input and Forget Gate LSTM Previousstudies show that the merged version gives perfor-mance comparable to a standard LSTM on languagemodeling and classification tasks because using theinput gate and forget gate simultaneously incurs re-dundant information (Chung et al, 2014; Greff etal., 2015).In the CIFG-LSTM, the input gate and forget gateare coupled as one uniform gate, that is, let i(t) =1 ?
f (t).
We use f (t) to denote the coupled gate.Formally, we will replace Eq.
5 as below:c(t) = f (t)  c(t?1) + (1?
f (t)) c?
(t) (7)Figure 1 gives an illustrative comparison of a stan-dard LSTM and the CIFG-LSTM.1662Cached LSTM Cached long short-term mem-ory neural networks (CLSTM) aims at capturingthe long-range information by a cache mechanism,which divides memory into several groups, and dif-ferent forgetting rates, regarded as filters, are as-signed to different groups.Different groups capture different-scale depen-dencies by squashing the scales of forgetting rates.The groups with high forgetting rates are short-termmemories, while the groups with low forgetting ratesare long-term memories.Specially, we divide the memory cells into Kgroups {G1, ?
?
?
, GK}.
Each group includes a in-ternal memory ck, output gate ok and forgettingrate rk.
The forgetting rate of different groups aresquashed in distinct ranges.We modify the update of a LSTM as follows.r(t)k = ?k???
(Wkrx(t) +K?j=1Uj?kf h(t?1)j )??
,(8)o(t)k = ?
(Wkox(t) +K?j=1Uj?ko h(t?1)j ), (9)c?
(t)k = tanh(Wkcx(t) +K?j=1Uj?kc h(t?1)j ), (10)c(t)k = (1?
r(t)k ) c(t?1)k + (r(t)k ) c?
(t)k , (11)h(t)k = o(t)k  tanh(c(t)k ), (12)where r(t)k represents forgetting rate of the k-thmemory group at step t; ?k is a squash function,which constrains the value of forgetting rate rkwithin a range.
To better distinguish the differentrole of each group, its forgetting rate is squashed intoa distinct area.
The squash function ?k(z) could beformalized as:rk = ?k(z) =1K ?
z+k ?
1K , (13)where z ?
(0, 1) is computed by logistic sigmoidfunction.
Therefore, rk can constrain the forgettingrate in the range of (k?1K , kK ).Intuitively, if a forgetting rate rk approaches to 0,the group k tends to be the long-term memory; if a+SOFTMAXx1 x2 x3 x4high medium low inputInputForwardBackwardOutputFigure 2: An overview of the proposed architecture.Different styles of arrows indicate different forget-ting rates.
Groups with stars are fed to a fully con-nected layers for softmax classification.
Here is aninstance of B-CLSTM with text length equal to 4and the number of memory groups is 3.rk approaches to 1, the group k tends to be the short-term memory.
Therefore, group G1 is the slowest,while groupGK is the fastest one.
The faster groupsare supposed to play a role as a cache, transiting in-formation from faster groups to slower groups.Bidirectional CLSTM Graves and Schmidhuber(2005) proposed a Bidirectional LSTM (B-LSTM)model, which utilizes additional backward informa-tion and thus enhances the memory capability.We also employ the bi-directional mechanism onCLSTM and words in a text will receive informa-tion from both sides of the context.
Formally, theoutputs of forward LSTM for the k-th group is[?
?h (1)k ,?
?h (2)k , .
.
.
,?
?h (T )k ].
The outputs of backwardLSTM for the k-th group is [?
?h (1)k ,?
?h (2)k , .
.
.
,?
?h (T )k ].Hence, we encode each word wt in a given textw1:T as h(t)k :h(t)k =?
?h (t)k ??
?h (t)k , (14)where the ?
indicates concatenation operation.Task-specific Output Layer for Document-levelSentiment Classification With the capability ofmodeling long text, we can use our proposed modelto analyze sentiment in a document.
Figure 2 givesan overview of the architecture.Since the first group, the slowest group, is sup-posed to keep the long-term information and can bet-ter represent a whole document, we only utilize the1663Dataset Type Train Size Dev.
Size Test Size Class Words/Doc Sents/DocIMDB Document 67426 8381 9112 10 394.6 16.08Yelp 2013 Document 62522 7773 8671 5 189.3 10.89Yelp 2014 Document 183019 22745 25399 5 196.9 11.41Table 1: Statistics of the three datasets used in this paper.
The rating scale (Class) of Yelp2013 and Yelp2014range from 1 to 5 and that of IMDB ranges from 1 to 10.
Words/Doc is the average length of a sample andSents/Doc is the average number of sentences in a document.final state of this group to represent a document.
Asfor the B-CLSTM, we concatenate the state of thefirst group in the forward LSTM at T -th time-stepand the first group in the backward LSTM at firsttime-step.Then, a fully connected layer followed by a soft-max function is used to predict the probability distri-bution over classes for a given input.
Formally, theprobability distribution p is:p = softmax(Wp ?
z+ bp), (15)where Wp and bp are model?s parameters.
Here zis ?
?h (T )1 in CLSTM, and z is [?
?h (T )1 ??
?h (1)1 ] in B-CLSTM.5 TrainingThe objective of our model is to minimize the cross-entropy error of the predicted and true distributions.Besides, the objective includes an L2 regularizationterm over all parameters.
Formally, suppose we havem train sentence and label pairs (w(i)1:Ti , y(i))mi=1, theobject is to minimize the objective function J(?):J(?)
= ?
1mm?i=1logp(i)y(i) +?2 ||?||2, (16)where ?
denote all the trainable parameters of ourmodel.6 ExperimentIn this section, we study the empirical result of ourmodel on three datasets for document-level senti-ment classification.
Results show that the proposedmodel outperforms competitor models from severalaspects when modelling long texts.6.1 DatasetsMost existing datasets for sentiment classificationsuch as Stanford Sentiment Treebank (Socher et al,2013) are composed of short paragraphs with sev-eral sentences, which cannot evaluate the effective-ness of the model under the circumstance of encod-ing long texts.
We evaluate our model on three pop-ular real-world datasets, Yelp 2013, Yelp 2014 andIMDB.
Table 1 shows the statistical information ofthe three datasets.
All these datasets can be publiclyaccessed1.
We pre-process and split the datasets inthe same way as Tang et al (2015b) did.?
Yelp 2013 and Yelp 2014 are review datasetsderived from Yelp Dataset Challenge2 of year2013 and 2014 respectively.
The sentiment po-larity of each review is 1 star to 5 stars, whichreveals the consumers?
attitude and opinion to-wards the restaurants.?
IMDB is a popular movie review dataset con-sists of 84919 movie reviews ranging from 1 to10 (Diao et al, 2014).
Average length of eachreview is 394.6 words, which is much largerthan the length of two Yelp review datasets.6.2 Evaluation MetricsWe use Accuracy (Acc.)
and MSE as evaluationmetrics for sentiment classification.
Accuracy is astandard metric to measure the overall classificationresult and Mean Squared Error (MSE) is used to fig-ure out the divergences between predicted sentimentlabels and the ground truth ones.6.3 Baseline ModelsWe compare our model, CLSTM and B-CLSTMwith the following baseline methods.?
CBOW sums the word vectors and applies anon-linearity followed by a softmax classifica-tion layer.1http://ir.hit.edu.cn/?dytang/paper/acl2015/dataset.7z2http://www.yelp.com/dataset_challenge1664Model IMDB Yelp 2014 Yelp 2013Acc.
(%) MSE Acc.
(%) MSE Acc.
(%) MSECBOW 34.8 2.867 56.8 0.620 54.5 0.706PV (Tang et al, 2015b) 34.1 3.291 56.4 0.643 55.4 0.692RNTN+Recurrent (Tang et al, 2015b) 40.0 3.112 58.2 0.674 57.4 0.646UPNN (CNN) (Tang et al, 2015b) 40.5 2.654 58.5 0.653 57.7 0.659JMARS* (Diao et al, 2014) - 3.143 - 0.998 - 0.970UPNN (CNN)* (Tang et al, 2015b) 43.5 2.566 60.8 0.584 59.6 0.615RNN 20.5 6.163 41.0 1.203 42.8 1.144LSTM 37.8 2.597 56.3 0.592 53.9 0.656CIFG-LSTM 39.1 2.467 55.2 0.598 57.3 0.558CLSTM 42.1 2.399 59.2 0.539 59.4 0.587BLSTM 43.3 2.231 59.2 0.538 58.4 0.583CIFG-BLSTM 44.5 2.283 60.1 0.527 59.2 0.554B-CLSTM 46.2 2.112 61.9 0.496 59.8 0.549Table 2: Sentiment classification results of our model against competitor models on IMDB, Yelp 2014 andYelp 2013.
Evaluation metrics are classification accuracy (Acc.)
and MSE.
Models with * use user andproduct information as additional features.
Best results in each group are in bold.Dataset IMDB Yelp13 Yelp14Hidden layer units 120 120 120Number of groups 3 4 4Weight Decay 1e?4 1e?4 5e?4Batch size 128 64 64Table 3: Optimal hyper-parameter configuration forthree datasets.?
JMARS is one of the state-of-the-art recom-mendation algorithm (Diao et al, 2014), whichleverages user and aspects of a review with col-laborative filtering and topic modeling.?
CNN UPNN (CNN) (Tang et al, 2015b) can beregarded as a CNN (Kim, 2014).
Multiple fil-ters are sensitive to capture different semanticfeatures during generating a representation in abottom-up fashion.?
RNN is a basic sequential model to model texts(Elman, 1991).?
LSTM is a recurrent neural network with mem-ory cells and gating mechanism (Hochreiterand Schmidhuber, 1997).?
BLSTM is the bidirectional version of LSTM,and can capture more structural informationand longer distance during looking forward andback (Graves et al, 2013).?
CIFG-LSTM & CIFG-BLSTM are CoupledInput Forget Gate LSTM and BLSTM, de-noted as CIFG-LSTM and CIFG-BLSTM re-spectively (Greff et al, 2015).
They combinethe input and forget gate of LSTM and requiresmaller number of parameters in comparisonwith the standard LSTM.6.4 Hyper-parameters and InitializationFor parameter configuration, we choose parameterson validation set mainly according to classificationaccuracy for convenience because MSE always hasstrong correlation with accuracy.
The dimension ofpre-trained word vectors is 50.
We use 120 as thedimension of hidden units, and choose weight de-cay among { 5e?4, 1e?4, 1e?5 }.
We use Adagrad(Duchi et al, 2011) as optimizer and its initial learn-ing rate is 0.01.
Batch size is chosen among { 32,64, 128 } for efficiency.
For CLSTM, the number ofmemory groups is chosen upon each dataset, whichwill be discussed later.
We remain the total numberof the hidden units unchanged.
Given 120 neuronsin all for instance, there are four memory groups andeach of them has 30 neurons.
This makes modelcomparable to (B)LSTM.
Table 3 shows the optimalhyper-parameter configurations for each dataset.For model initialization, we initialize all recur-rent matrices with randomly sampling from uni-form distribution in [-0.1, 0.1].
Besides, we useGloVe(Pennington et al, 2014) as pre-trained wordvectors.
The word embeddings are fine-tuned duringtraining.
Hyper-parameters achieving best results on16650 2 4 6 8 1030405060EpochesAcc(%)LSTMCIFG-LSTMBLSTMCIFG-BLSTMB-CLSTM(a) Accuracy on Yelp 20130 2 4 6 8 101.52.0EpochesMSELSTMCIFG-LSTMBLSTMCIFG-BLSTMB-CLSTM(b) MSE on Yelp 2013Figure 3: Convergence speed experiment on Yelp 2013.
X-axis is the iteration epoches and Y-axis is theclassifcication accuracy(%) achieved.the validation set are chosen for final evaluation ontest set.6.5 ResultsThe classification accuracy and mean square error(MSE) of our models compared with other competi-tive models are shown in Table 2.
When comparingour models to other neural network models, we haveseveral meaningful findings.1.
Among all unidirectional sequential models,RNN fails to capture and store semantic fea-tures while vanilla LSTM preserves sentimen-tal messages much longer than RNN.
It showsthat internal memory plays a key role in textmodeling.
CIFG-LSTM gives performancecomparable to vanilla LSTM.2.
With the help of bidirectional architecture,models could look backward and forward tocapture features in long-range from global per-spective.
In sentiment analysis, if users showtheir opinion at the beginning of their review,single directional models will possibly forgetthese hints.3.
The proposed CLSTM beats the CIFG-LSTMand vanilla LSTM and even surpasses the bidi-rectional models.
In Yelp 2013, CLSTMachieves 59.4% in accuracy, which is only 0.4percent worse than B-CLSTM, which revealsthat the cache mechanism has successfully andeffectively stored valuable information withoutthe support from bidirectional structure.4.
Compared with existing best methods, ourmodel has achieved new state-of-the-art re-sults by a large margin on all document-level datasets in terms of classification accu-racy.
Moreover, B-CLSTM even has surpassedJMARS and CNN (UPNN) methods which uti-lized extra user and product information.5.
In terms of time complexity and numbers of pa-rameters, our model keeps almost the same asits counterpart models while models of hierar-chically composition may require more compu-tational resources and time.6.6 Rate of ConvergenceWe compare the convergence rates of our mod-els, including CIFG-LSTM, CIFG-BLSTM and B-CLSTM, and the baseline models (LSTM andBLSTM).
We configure the hyper-parameter tomake sure every competing model has approxi-mately the same numbers of parameters, and vari-ous models have shown different convergence ratesin Figure 3.
In terms of convergence rate, B-CLSTMbeats other competing models.
The reason why B-CLSTM converges faster is that the splitting mem-ory groups can be seen as a better initialization andconstraints during the training process.16661 2 3 4 5 659.56060.561Group NumberAcc(%)(a) Acc on Yelp 20131 2 3 4 5 6606162Group Number(b) Acc on Yelp 20141 2 3 4 5 6444546Group Number(c) Acc on IMDBFigure 4: Classification accuracy on different number of memory group on three datasets.
X-axis is thenumber of memory group(s).10 20 30 40 50 60 70 80 90 1003035404550Length Ranking (%)Acc(%)CBOW CIFG-LSTM CLSTMCIFG-BLSTM B-CLSTMFigure 5: Study of model sensitivity on documentlength on IMDB.
All test samples are sorted by theirlength and divided into 10 parts.
Left most dotmeans classification accuracy on the shortest 10%samples.
X-axis is length ranking from 0% to 100%.6.7 Effectiveness on Grouping MemoryFor the proposed model, the number of memorygroups is a highlight.
In Figure 4, we plot the bestprediction accuracy (Y-axis) achieved in validationset with different number of memory groups on alldatasets.
From the diagram, we can find that ourmodel outperforms the baseline method.
In Yelp2013, when we split the memory into 4 groups, itachieves the best result among all tested memorygroup numbers.
We can observe the dropping trendswhen we choose more than 5 groups.For fair comparisons, we set the total amount ofneurons in our model to be same with vanilla LSTM.Therefore, the more groups we split, the less the neu-rons belongs to each group, which leads to a worsecapacity than those who have sufficient neurons foreach group.6.8 Sensitivity on Document LengthWe also investigate the performance of our modelon IMDB when it encodes documents of differentlengths.
Test samples are divided into 10 groupswith regard to the length.
From Figure 5, we candraw several thoughtful conclusions.1.
Bidirectional models have much better perfor-mance than the counterpart models.2.
The overall performance of B-CLSTM is bet-ter than CIFG-BLSTM.
This means that ourmodel is adaptive to both short texts and longdocuments.
Besides, our model shows powerin dealing with very long texts in comparisonwith CIFG-BLSTM.3.
CBOW is slightly better than CIFG-LSTM dueto LSTM forgets a large amount of informationduring the unidirectional propagation.7 ConclusionIn this paper, we address the problem of effectivelyanalyzing the sentiment of document-level texts inan RNN architecture.
Similar to the memory struc-ture of human, memory with low forgetting rate cap-tures the global semantic features while memorywith high forgetting rate captures the local seman-tic features.
Empirical results on three real-world1667document-level review datasets show that our modeloutperforms state-of-the-art models by a large mar-gin.For future work, we are going to design a strategyto dynamically adjust the forgetting rates for fine-grained document-level sentiment analysis.AcknowledgmentsWe appreciate the constructive work from XinchiChen.
Besides, we would like to thank the anony-mous reviewers for their valuable comments.
Thiswork was partially funded by National Natural Sci-ence Foundation of China (No.
61532011 and61672162), the National High Technology Re-search and Development Program of China (No.2015AA015408).ReferencesRichard C Atkinson and Richard M Shiffrin.
1968.
Hu-man memory: A proposed system and its control pro-cesses.
The psychology of learning and motivation,2:89?195.Parminder Bhatia, Yangfeng Ji, and Jacob Eisenstein.2015.
Better document-level sentiment analysis fromrst discourse parsing.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing,(EMNLP).Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Shiyu Wu, andXuanjing Huang.
2015.
Sentence modeling withgated recursive neural network.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing.Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,and Yoshua Bengio.
2014.
Empirical evaluation ofgated recurrent neural networks on sequence model-ing.
NIPS Deep Learning Workshop.Misha Denil, Alban Demiraj, Nal Kalchbrenner, PhilBlunsom, and Nando de Freitas.
2014.
Modelling,visualising and summarising documents with a sin-gle convolutional neural network.
arXiv preprintarXiv:1406.3830.Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexan-der J. Smola, Jing Jiang, and Chong Wang.
2014.Jointly modeling aspects, ratings and sentiments formovie recommendation (JMARS).
In The 20thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?14, New York,NY, USA - August 24 - 27, 2014, pages 193?202.John C Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive Subgradient Methods for Online Learningand Stochastic Optimization.
Journal of MachineLearning Research, 12:2121?2159.Salah El Hihi and Yoshua Bengio.
1995.
Hierarchicalrecurrent neural networks for long-term dependencies.In NIPS, pages 493?499.Jeffrey L Elman.
1991.
Distributed representations,simple recurrent networks, and grammatical structure.Machine Learning, 7(2-3):195?225.Alex Graves and Ju?rgen Schmidhuber.
2005.
Frame-wise phoneme classification with bidirectional lstmand other neural network architectures.
Neural Net-works, 18(5):602?610.Alan Graves, Navdeep Jaitly, and Abdel-rahman Mo-hamed.
2013.
Hybrid speech recognition with deepbidirectional lstm.
In Automatic Speech Recognitionand Understanding (ASRU), 2013 IEEE Workshop on,pages 273?278.
IEEE.Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn?
?k,Bas R Steunebrink, and Ju?rgen Schmidhuber.
2015.LSTM: A Search Space Odyssey.
arXiv.org, March.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Andrej Karpathy, Justin Johnson, and Fei-Fei Li.
2015.Visualizing and understanding recurrent networks.
In-ternational Conference on Learning Representations(ICLR), Workshop Track.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 1746-1751.Jan Koutnik, Klaus Greff, Faustino Gomez, and JuergenSchmidhuber.
2014.
A clockwork rnn.
pages 1863?1871.Quoc V Le and Tomas Mikolov.
2014.
Distributed rep-resentations of sentences and documents.
In ICML,volume 14, pages 1188?1196.Quoc V Le, Navdeep Jaitly, and Geoffrey E Hin-ton.
2015.
A simple way to initialize recurrentnetworks of rectified linear units.
arXiv preprintarXiv:1504.00941.Jiwei Li, Thang Luong, Dan Jurafsky, and Eduard H.Hovy.
2015.
When are tree structures necessary fordeep learning of representations?
In Llus Mrquez,Chris Callison-Burch, Jian Su, Daniele Pighin, andYuval Marton, editors, EMNLP, pages 2304?2314.The Association for Computational Linguistics.PengFei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu, andXuanjing Huang.
2015.
Multi-timescale long short-term memory neural network for modelling sentencesand documents.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing.1668Pengfei Liu, Xipeng Qiu, and Xuanjing Huang.
2016.Recurrent neural network for text classification withmulti-task learning.
In Proceedings of InternationalJoint Conference on Artificial Intelligence.Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured mod-els for fine-to-coarse sentiment analysis.
In AnnualMeeting-Association For Computational Linguistics,volume 45, page 432.
Citeseer.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient Estimation of Word Represen-tations in Vector Space.
arXiv.org.Ke Tran Arianna Bisazza Christof Monz.
2016.
Recur-rent memory networks for language modeling.
In Pro-ceedings of NAACL-HLT, pages 321?331.Rodrigo Moraes, Joao Francisco Valiati, and WilsonP Gavia?O Neto.
2013.
Document-level senti-ment classification: An empirical comparison betweensvm and ann.
Expert Systems with Applications,40(2):621?633.Brendan O?Connor, Ramnath Balasubramanyan, Bryan RRoutledge, and Noah A Smith.
2010.
From Tweets toPolls: Linking Text Sentiment to Public Opinion TimeSeries.
ICWSM 2010.Alexander Pak and Patrick Paroubek.
2010.
Twitter as acorpus for sentiment analysis and opinion mining.
InLREc, volume 10, pages 1320?1326.Bo Pang and Lillian Lee.
2004.
A sentimental edu-cation: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42Nd Annual Meeting on Association for Compu-tational Linguistics, ACL ?04, Stroudsburg, PA, USA.Association for Computational Linguistics.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global Vectors for Word Rep-resentation.
EMNLP, pages 1532?1543.Hojjat Salehinejad.
2016.
Learning over long time lags.arXiv preprint arXiv:1602.04335.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts.
2013.
Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the conference on empirical meth-ods in natural language processing (EMNLP), volume1631, page 1642.
Citeseer.Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al2015.
End-to-end memory networks.
In Advances inNeural Information Processing Systems, pages 2431?2439.Kai Sheng Tai, Richard Socher, and Christopher D Man-ning.
2015.
Improved Semantic RepresentationsFrom Tree-Structured Long Short-Term Memory Net-works.
ACL, pages 1556?1566.Duyu Tang, Bing Qin, and Ting Liu.
2015a.
DocumentModeling with Gated Recurrent Neural Network forSentiment Classification.
EMNLP, pages 1422?1432.Duyu Tang, Bing Qin, and Ting Liu.
2015b.
LearningSemantic Representations of Users and Products forDocument Level Sentiment Classification.
ACL, pages1014?1023.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the conferenceon human language technology and empirical methodsin natural language processing, pages 347?354.
Asso-ciation for Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analysis.Computational linguistics, 35(3):399?433.Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010.Multi-level structured models for document-level sen-timent classification.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1046?1056.
Association for Com-putational Linguistics.Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015.Long short-term memory over recursive structures.
InProceedings of the 32nd International Conference onMachine Learning, pages 1604?1612.1669
