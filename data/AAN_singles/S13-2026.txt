Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 144?147, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsMELODI: A Supervised Distributional Approachfor Free Paraphrasing of Noun CompoundsTim Van de CruysIRIT, CNRStim.vandecruys@irit.frStergos AfantenosIRIT, Toulouse Universitystergos.afantenos@irit.frPhilippe MullerIRIT, Toulouse Universityphilippe.muller@irit.frAbstractThis paper describes the system submittedby the MELODI team for the SemEval-2013Task 4: Free Paraphrases of Noun Compounds(Hendrickx et al 2013).
Our approach com-bines the strength of an unsupervised distri-butional word space model with a supervisedmaximum-entropy classification model; thedistributional model yields a feature represen-tation for a particular compound noun, whichis subsequently used by the classifier to inducea number of appropriate paraphrases.1 IntroductionInterpretation of noun compounds is making explicitthe relation between the component nouns, for in-stance that running shoes are shoes used in runningactivities, while leather shoes are made from leather.The relations can have very different meanings, andexisting work either postulates a fixed set of rela-tions (Tratz and Hovy, 2010) or relies on appropri-ate descriptions of the relations, through constrainedverbal paraphrases (Butnariu et al 2010) or uncon-strained paraphrases as in the present campaign.
Thelatter is much simpler for annotation purposes, butraises difficult challenges involving not only com-pound interpretation but also paraphrase evaluationand ranking.In terms of constrained verbal paraphrasesWubben (2010), for example, uses a supervisedmemory-based ranker using features from theGoogle n-gram corpus as well as WordNet.
Nultyand Costello (2010) rank paraphrases of compoundsaccording to the number of times they co-occurredwith other paraphrases for other compounds.
Theyuse these co-occurrences to compute conditionalprobabilities estimating is-a relations between para-phrases.
Li et al(2010) provide a hybrid sys-tem which combines a Bayesian algorithm exploit-ing Google n-grams, a score which captures humanpreferences at the tail distribution of the trainingdata, as well as a metric that captures pairwise para-phrase preferences.Our methodology consists of two steps.
First,an unsupervised distributional word space model isconstructed, which yields a feature representationfor a particular compound.
The feature representa-tion is then used by a maximum entropy classifier toinduce a number of appropriate paraphrases.2 Methodology2.1 Distributional word space modelIn order to induce appropriate feature representa-tions for the various noun compounds, we start byconstructing a standard distributional word spacemodel for nouns.
We construct a co-occurrencematrix of the 5K most frequent nouns1 by the 2Kmost frequent context words2, which occur in a win-dow of 5 words to the left and right of the targetword.
The bare frequencies of the word-context ma-trix are weighted using pointwise mutual informa-tion (Church and Hanks, 1990).Next, we compute a joint, compositional repre-sentation of the noun compound, combining the se-1making sure all nouns that appear in the training and testset are included2excluding the 50 most frequent context words as stop words144mantics of the head noun with the modifier noun.
Todo so, we make use of a simple vector-based multi-plicative model of compositionality, as proposed byMitchell and Lapata (2008).
In order to compute thecompositional representation of a compound noun,this model takes the elementwise multiplication ofthe vectors for the head noun and the modifier noun,i.e.pi = uivifor each feature i.
The resulting features are used asinput to our next classification step.We compare the performance of the abovemen-tioned compositional model with a simpler modelthat only takes into account the semantics of thehead noun.
This model only uses the context fea-tures for the head noun as input to our second clas-sification step.
This means that the model only takesinto account the semantics of the head noun, and ig-nores the semantics of the modifier noun.2.2 Maximum entropy classificationThe second step of our paraphrasing system consistsof a supervised maximum entropy classification ap-proach.
Training vectors for each noun compoundfrom the training set are constructed according tothe approach described in the previous section.
The(non-zero) context features yielded by the first stepare used as input for the maximum entropy classi-fier, together with the appropriate paraphrase labelsand the label counts (used to weight the instances),which are extracted from the training set.We then deploy the model in order to induce aprobability distribution over the various paraphraselabels.
Every paraphrase label above a threshold ?
isconsidered an appropriate paraphrase.
Using a por-tion of held-out training data (20%), we set ?
= 0.01for our official submission.
In this paper, we show anumber of results using different thresholds.2.3 Set of paraphrases labelsFor our classification approach to work, we need toextract an appropriate set of paraphrase labels fromthe training data.
In order to create this set, wesubstitute the nouns that appear in the training set?sparaphrases by dummy variables.
Table 1 gives anexample of three different paraphrases and the re-sulting paraphrase labels after substitution.
Notethat we did not apply any NLP techniques to prop-erly deal with inflected words.We apply a frequency threshold of 2 (counted overall the instances), so we discard paraphrase labelsthat appear only once in the training set.
This givesus a total of 285 possible paraphrase labels.One possible disadvantage of this supervised ap-proach is a loss of recall on unseen paraphrases.
Arough estimation shows that our set of training labelsaccounts for only 25% of the similarly constructedlabels extracted from the test set.
However, the mostfrequently used paraphrase labels are present in bothtraining and test set, so this does not prevent oursystem to come up with a number of suitable para-phrases for the test set.2.4 Implementational detailsAll frequency co-occurrence information has beenextracted from the ukWaC corpus (Baroni et al2009).
The corpus has been part of speech taggedand lemmatized with Stanford Part-Of-Speech Tag-ger (Toutanova and Manning, 2000; Toutanova etal., 2003).
Distributional word space algorithmshave been implemented in Python.
The maximumentropy classifier was implemented using the Maxi-mum Entropy Modeling Toolkit for Python and C++(Le, 2004).3 ResultsTable 2 shows the results of the different systems interms of the isomorphic and non-isomorphic evalu-ation measures defined by the task organizers (Hen-drickx et al 2013).
For comparison, we include anumber of baselines.
The first baseline assigns thetwo most frequent paraphrase labels (Y of X, Y forX) to each test instance; the second baseline assignsthe four most frequent paraphrase labels (Y of X, Yfor X, Y on X, Y in X); and the third baseline assignsall of the possible 285 paraphrase labels as correctanswer for each test instance.For both our primary system (the multiplicativemodel) and our contrastive system (the head nounmodel), we vary the threshold used to select the finalset of paraphrases.
A threshold ?
= 0.01 results ina smaller set of paraphrases, whereas a threshold of?
= 0.001 results in a broad set of paraphrases.
Ourofficial submission uses the former threshold.145compound paraphrase paraphrase labeltextile company company that makes textiles Y that makes Xstextile company company that produces textiles Y that produces Xstextile company company in textile industry Y in X industryTable 1: Example of induced paraphrase labelsmodel ?
isomorphic non-isomorphicbaseline (2) ?
.058 .808baseline (4) ?
.090 .633baseline (all) ?
.332 .200multiplicative .01 .130 .548.001 .270 .259head noun .01 .136 .536.001 .277 .302Table 2: ResultsFirst of all, we note that the different baselinemodels are able to obtain substantial scores for thedifferent evaluation measures.
The first two base-lines, which use a limited number of paraphraselabels, perform very well in terms of the non-isomorphic evaluation measure.
The third baseline,which uses a very large number of candidate para-phrase labels, gets more balanced results in terms ofboth the isomorphic and non-isomorphic measure.Considering our different thresholds, the resultsof our models are in line with the baseline re-sults.
A larger threshold, which results in a smallernumber of paraphrase labels, reaches a higher non-isomorphic score.
A smaller threshold, which re-sults in a larger number of paraphrase labels, givesmore balanced results for the isomorphic and non-isomorphic measure.There does not seem to be a significant differencebetween our primary system (multiplicative) and ourcontrastive system (head noun).
For ?
= 0.01, theresults of both models are very similar; for ?
=0.001, the head noun model reaches slightly betterresults, in particular for the non-isomorphic score.Finally, we note that our models do not seem toimprove significantly on the baseline scores.
For?
= 0.001, the results of our models seem somewhatmore balanced compared to the all baseline, but thedifferences are not very large.
In general, our sys-tems (in line with the other systems participating inthe task) seem to have a hard time beating a num-ber of simple baselines, in terms of the evaluationmeasures defined by the task.4 ConclusionWe have presented a system for producing free para-phrases of noun compounds.
Our methodology con-sists of two steps.
First, an unsupervised distribu-tional word space model is constructed, which isused to compute a feature representation for a par-ticular compound.
The feature representation is thenused by a maximum entropy classifier to induce anumber of appropriate paraphrases.Although our models do seem to yield slightlymore balanced scores than the baseline models, thedifferences are not very large.
Moreover, there isno substantial difference between our primary mul-tiplicative model, which takes into account the se-mantics of both head and modifier noun, and ourcontrastive model, which only uses the semantics ofthe head noun.ReferencesMarco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-muid O?
Se?aghdha, Stan Szpakowicz, and Tony Veale.2010.
Semeval-2 task 9: The interpretation of nouncompounds using paraphrasing verbs and prepositions.In Proceedings of the 5th International Workshop onSemantic Evaluation, pages 39?44, Uppsala, Sweden,July.
Association for Computational Linguistics.Kenneth W. Church and Patrick Hanks.
1990.
Word as-sociation norms, mutual information & lexicography.Computational Linguistics, 16(1):22?29.146Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Diar-muid O?
Se?aghdha, Stan Szpakowicz, and Tony Veale.2013.
SemEval-2013 task 4: Free paraphrases of nouncompounds.
In Proceedings of the International Work-shop on Semantic Evaluation, SemEval ?13, June.Zhang Le.
2004.
Maximum entropy modeling toolkit forpython and c++.
http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html.Guofu Li, Alejandra Lopez-Fernandez, and Tony Veale.2010.
Ucd-goggle: A hybrid system for noun com-pound paraphrasing.
In Proceedings of the 5th In-ternational Workshop on Semantic Evaluation, pages230?233, Uppsala, Sweden, July.
Association forComputational Linguistics.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
proceedings of ACL-08: HLT, pages 236?244.Paul Nulty and Fintan Costello.
2010.
Ucd-pn: Select-ing general paraphrases using conditional probability.In Proceedings of the 5th International Workshop onSemantic Evaluation, pages 234?237, Uppsala, Swe-den, July.
Association for Computational Linguistics.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In Proceedings of theJoint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Corpora(EMNLP/VLC-2000), pages 63?70.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of HLT-NAACL 2003, pages 252?259.Stephen Tratz and Eduard Hovy.
2010.
A taxonomy,dataset, and classifier for automatic noun compoundinterpretation.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 678?687, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Sander Wubben.
2010.
Uvt: Memory-based pairwiseranking of paraphrasing verbs.
In Proceedings of the5th International Workshop on Semantic Evaluation,pages 260?263, Uppsala, Sweden, July.
Associationfor Computational Linguistics.147
