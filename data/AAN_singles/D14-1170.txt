Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1624?1633,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsAutomatic Generation of Related Work Sections in Scientific Papers:An Optimization ApproachYue Hu and Xiaojun WanInstitute of Computer Science and TechnologyThe MOE Key Laboratory of Computational LinguisticsPeking University, Beijing, China{ayue.hu,wanxiaojun}@pku.edu.cnAbstractIn this paper, we investigate a challeng-ing task of automatic related work gener-ation.
Given multiple reference papers asinput, the task aims to generate a relatedwork section for a target paper.
The gen-erated related work section can be usedas a draft for the author to complete hisor her final related work section.
Wepropose our Automatic Related WorkGeneration system called ARWG to ad-dress this task.
It first exploits a PLSAmodel to split the sentence set of the giv-en papers into different topic-biased parts,and then applies regression models tolearn the importance of the sentences.
Atlast it employs an optimization frame-work to generate the related work section.Our evaluation results on a test set of 150target papers along with their referencepapers show that our proposed ARWGsystem can generate related work sec-tions with better quality.
A user study isalso performed to show ARWG canachieve an improvement over genericmulti-document summarization baselines.1 IntroductionThe related work section is an important part of apaper.
An author often needs to help readers tounderstand the context of his or her researchproblem and compare his or her current workwith previous works.
A related work section isoften used for this purpose to show the differ-ences and advantages of his or her work, com-pared with related research works.
In this study,we attempt to automatically generate a relatedwork section for a target academic paper with itsreference papers.
This kind of related work sec-tions can be used as a basis to reduce the author?stime and effort when he or she wants to completehis or her final related work section.Automatic related work section generation is avery challenging task.
It can be considered a top-ic-biased, multiple-document summarizationproblem.
The input is a target academic paper,which has no related work section, along with itsreference papers.
The goal is to create a relatedwork section that describes the related works andaddresses the relationship between the target pa-per and the reference papers.
Here we assumethat the set of reference papers has been given aspart of the input.
Existing works in the NLP andrecommendation systems communities have al-ready focused on the task of finding referencepapers.
For example, citation prediction (Nal-lapati et al., 2008) aims at finding individual pa-per citation patterns.Generally speaking, automatic related worksection generation is a strikingly different prob-lem and it is much more difficult in comparisonwith general multi-document summarizationtasks.
For example, multi-document summariza-tion of news articles aims at synthesizing con-tents of similar news and removing the redundantinformation contained by the different news arti-cles.
However, each scientific paper has muchspecific content to state its own work and contri-bution.
Even for the papers that investigate thesame research topic, their contributions and con-tents can be totally different.
The related worksection generation task needs to find the specificcontributions of individual papers and arrangethem into one or several paragraphs.In this study, we focus on the problem of au-tomatic related work section generation and pro-pose a novel system called ARWG to address the1624problem.
For the target paper, we assume that theabstract and introduction sections have alreadybeen written by the author and they can be usedto help generate the related work section.
For thereference papers, we only consider and extractthe abstract, introduction, related work and con-clusion sections, because other sections like themethod and evaluation sections always describethe extreme details of the specific work and theyare not suitable for this task.
Then we generatethe related work section using both sentence setswhich are extracted from the target paper andreference papers, respectively.Firstly, we use a PLSA model to group bothsentence sets of the target paper and its referencepapers into different topic-biased clusters.
Sec-ondly, the importance of each sentence in thetarget paper and the reference papers is learnedby using two different Support Vector Regres-sion (SVR) models.
At last, a global optimizationframework is proposed to generate the relatedwork section by selecting sentences from boththe target paper and the reference papers.
Mean-while, the framework selects sentences from dif-ferent topic-biased clusters globally.Experimental results on a test set of 150 targetpapers show our method can generate relatedwork sections with better quality than those ofseveral baseline methods.
With the ROUGEtoolkit, the results indicate the related work sec-tions generated by our system can get higherROUGE scores.
Moreover, our related work sec-tions can get higher rating scores based on a userstudy.
Therefore, our related work sections canbe much more suitable for the authors to preparetheir final related work sections.2 Related WorkThere are few studies to directly address auto-matic related work generation.
Hoang and Kan(2010) proposed a related work summarizationsystem given the set of keywords arranged in ahierarchical fashion that describes the paper?stopic.
They used two different rule-based strate-gies to extract sentences for general topics aswell as detailed ones.A few studies focus on multi-document scien-tific article summarization.
Agarwal et al., (2011)introduced an unsupervised approach to the prob-lem of multi-document summarization.
The inputis a list of papers cited together within the samesource article.
The key point of this approach is atopic based clustering of fragments extractedfrom each co-cited article.
They rank all the clus-ters using a query generated from the contextsurrounding the co-cited list of papers.
Yelogluet al., (2011) compared four different approachesfor multi-document scientific articles summariza-tion: MEAD, MEAD with corpus specific vo-cabulary, LexRank and W3SS.Other studies investigate mainly on the single-document scientific article summarization.
Earlyworks including (Luhn 1958; Baxendale 1958;Edumundson 1969) tried to use various featuresspecific to scientific text (e.g., sentence position,or rhetorical clues features).
They have provedthat these features are effective for the scientificarticle summarization.
Citation information hasbeen already shown effective in summarize thescientific articles.
Works including (Mei andZhai 2008; Qazvinian and Radev 2008; Schwartzand Hearst 2006; Mohammad et al., 2009) em-ployed citation information for the single scien-tific article summarization.
Earlier work (Nakovet al., 2004) indicated that citation sentences maycontain important concepts that can give usefuldescriptions of a paper.Various methods have been proposed for newsdocument summarization, including rule-basedmethods (Barzilay and Elhadad 1997; Marcu andDaniel 1997), graph-based methods (Mani andBloedorn 2000; Erkan and Radev 2004; Michal-cea and Tarau 2005), learning-based methods(Conroy et al., 2001; Shen et al., 2007; Ouyanget al., 2007; Galanis et al., 2008), optimization-based methods (McDonald 2007; Gillick et al.,2009; Xie et al., 2009; Berg-Kirkpatrick et al.,2011; Lei Huang et al., 2011; Woodsend et al.,2012; Galanis 2012), etc.The most relevant work is (Hoang and Kan,2010) as mentioned above.
They also assumedthe set of reference papers was given as part ofthe input.
They also adopt the hierarchical topictree that describes the topic structure in the targetpaper as an essential input for their system.However, it is non-trivial to build the hierar-chical topic tree.
Moreover, they do not considerthe content of the target paper to construct therelated work section, which is actually crucial inthe related work section.
To the best of ourknowledge, no previous works have used super-vised learning and optimization framework todeal with the multiple scientific article summari-zation tasks.3 Problem Analysis and Corpus3.1 Problem Analysis1625We firstly analyze the structure of related worksections briefly.
By using examples for illustra-tion, we can gain insight on how to generate re-lated work sections.
A specific related work ex-ample is shown in Figure 1.This related work section introduces previousrelated works for a paper on Automatic Taxono-my Induction.
From Figure 1, we can have aglance at the structure of related work sections.Related work sections usually discuss severaldifferent topics, such as ?pattern-based?
and?cluster-based?
approaches shown in the Figure1.
Besides the knowledge of previous works, theauthor often compares his own work with theprevious works.
The differences and advantagesare generally mentioned.
The example in Figure1 also indicates this phenomenon.Therefore, we design our system to generaterelated work sections according to the relatedwork section structure mentioned above.
Oursystem takes the target paper for which a relatedwork section needs to be drafted besides its ref-erence papers as input.
The goal of our system isto generate a related work section with the abovestructure.
The generated related work sectionshould have several topic-biased parts.
The au-thor's own work is also needed to be describedand its difference with other works is needed tobe emphasized on.3.2 Corpus and PreprocessingWe build a corpus that contains academic papersand their corresponding reference papers.
Theacademic papers are selected from the ACL An-thology 1 .
The ACL Anthology currently hosts1 http://aclweb.org/anthology/over 24,500 papers from major conferences suchas ACL, EMNLP, COLING in the fields of com-putational linguistics and natural language pro-cessing.
We remove the papers that contain relat-ed work sections with very short length, and ran-domly select 1050 target papers to construct ourwhole corpus.The papers are all in PDF format.
We extracttheir texts by using PDFlib 2  and detect theirphysical structures of paragraphs, subsectionsand sections by using ParsCit3 .
For the targetpapers, the related work sections are directly ex-tracted as the gold summaries.
The references arealso extracted.
For the references that can befound in the ACL Anthology, we download themfrom the ACL Anthology.
The other referencepapers are searched and downloaded by usingGoogle Scholar.
References to books and PhDtheses are discarded, for their verbosity maychange the problem drastically (Mihalcea andCeylan, 2007).The input of our system includes the abstractand introduction sections of the target paper, andthe abstract, introduction, related work and con-clusion sections of the reference papers.
As men-tioned above, the method and evaluation sectionsin the reference papers are not used as input be-cause these sections usually describe extremedetails of the methods and evaluation results andthey are not suitable for related work generation.Note that it is reasonable to make use of the ab-stract and introduction sections of the target pa-per to help generate the related work section,because an author usually has already written theabstract and introduction sections before he orshe wants to write the related work section forthe target paper.
Otherwise, we cannot get anyinformation about the author?s own work.
Allother sections in the target paper are not used.4 Our Proposed System4.1 OverviewIn this paper, we propose a system called ARWGto automatically generate a related work sectionfor a given target paper.
The architecture of oursystem is shown in Figure 2.
We take both thetarget paper and its reference papers as input andthey are represented by several sections men-tioned in Section 3.2.
After preprocessing, weextract the feature vectors for sentences in thetarget paper and the reference papers, respective-2 http://www.pdflib.com/3 http://aye.comp.nus.edu.sg/parsCit/Figure 1: A sample related work section (Yang andCallan 2009)There has been a substantial amount of research on automatictaxonomy induction.
As we mentioned earlier, two mainapproaches are pattern-based and clustering-based.Pattern-based approaches are the main trend for automatictaxonomy induction.
?Pattern-based approaches started from and still pay a great dealof attention to the most common is-a relations.
?Clustering-based approaches usually represent word contexts asvectors and cluster words based on similarities of the vectors(Brown et al., 1992; Lin, 1998).
?Many clustering-based approaches face the challenge ofappropriately labeling non-leaf clusters.
?
In this paper, we takean incremental clustering approach,...
The advantage of theincremental approach is that it eliminates the trouble ofinventing cluster labels and concentrates on placing terms in thecorrect positions in a taxonomy hierarchy.The o k by S ow et al.
(2006) is the most similar to ours ?Moreover, our approach employs heterogeneous features from awide rang ; hile heir approach only used syntactic dependency.Two differenttopicsComparisonwith theauthor?s work1626ly.
The importance scores for sentences in thetarget paper and the reference papers are as-signed by using two SVR based sentence scoringmodels.
The two SVR models are trained forsentences in the target paper and the referencepapers, respectively.
Meanwhile, a topic model isapplied to the whole set of sentences in both thetarget paper and reference papers.
The sentencesare grouped into several different topic-biasedclusters.
The sentences with importance scoresand topic cluster information are taken as theinput for the global optimization framework.
Theoptimization framework extracts sentences todescribe both the author?s own work and back-ground knowledge.
More details of each part willbe discussed in the following sections.4.2 Topic Model LearningAs mentioned in the previous section, the relatedwork section usually addresses several differenttopics.
The topics may be different researchthemes or different aspects of a broad researchtheme.
The related work section should describethe specific details for each topic, respectively.Therefore, we aim to discover the hidden top-ics of the input papers, and we use the Probabil-istic latent semantic analysis (PLSA) (Hofmann,1999) to solve this problem.The PLSA approach models each word in adocument as a sample from a mixture model.The mixture components are multinomial ran-dom variables that can be viewed as representa-tions of ?topics?.
Different words in a documentmay be generated from different topics.
Eachdocument is represented a list of mixing propor-tions for these mixture components and can bereduced to a probability distribution on a fixedset of topics.Considering that the sentences in one papermay relate to different topics, we treat each sen-tence as a ?document?
d. We treat the nounphases in the sentences as the ?words?
w. In or-der to extract the noun phrases, chunking imple-mented by the OpenNLP toolkit 4 is applied tothe sentences.
Noun phrases that contain wordssuch as ?paper?
and ?data?
are discarded.Then the sentences with their correspondingnoun phrases are taken as input into the PLSAmodel.
Here both the sentences in the target pa-per and the sentences in the reference papers aretreated the same in the model.
Finally, we canget the sentence set with topic information anduse it in the subsequent steps.
Each sentence hasa topic weight t in each topic.4.3 Sentence Important AssessmentIn our proposed system, sentence importanceassessment aims to assign an importance score toeach sentence in the target paper and referencepapers.
The score of each sentence will be usedin the subsequent optimization framework.
Wepropose to use the support vector regressionmodel to achieve this goal.
In the above topicmodel learning process, we do not distinguish thesentences in the target paper and reference pa-pers.
In contrast, we train two different supportvector regression models separately for the sen-tences in the target paper and the sentences in thereference papers.
In the related work section, thesentences that describe the author?s own workusually address the differences from the relatedworks, while the sentences that describe the re-lated works often focus on the specific details.We think the two kinds of sentences should betreated differently.Scoring MethodTo construct training data based on the paperscollected, we apply a similarity scoring methodto assign the importance scores to the sentencesin the papers.
The main hypothesis is that thesentences in the gold related work sectionsshould summarize the target paper and referencepapers as well.
Thus the sentences in the paperswhich are more similar to the sentences in thegold related work sections should be consideredmore important and suitable to be selected.
Ourscoring method should assign higher scores tothem.4 http://opennlp.apache.org/Figure 2: System ArchitectureTarget paper Reference papersPreprocessingTopic ModelSentence ScoreAssessment(target)Sentence ScoreAssessment(reference)OptimizationFrameworkPostprocessingRelated WorkSection1627We define the importance score of a sentencein the papers as below:?????(?)
=  ?????????(???
(?, ???))
(1)where s is a sentence in the papers,  ??
is the setof the sentences in the corresponding gold relat-ed work section.
The standard cosine measure isemployed as the similarity function.Considering the difference between the sen-tences that describe the author?s work and thesentences that describe the related works, wesplit the set of sentences in the gold related worksection into two parts: one discusses the author?sown work and the other introduces the relatedworks.
We observe that sentences related to theauthor?s own work often feature specific wordsor phrases (such as ?we?, ?our work?, ?in thispaper?
etc.)
in the related work section.
So wecheck the sentences about whether they containclue words or phrases (i.e., ?in this paper?, ?ourwork?
and 18 other phrases).
If the clue phrasecheck fails, the sentence belongs to the relatedwork part.
If not, it belongs the own work part.Thus for the sentences in the target paper,  ?
?is the set of sentences in the own work part of thegold related work section, while for the sentencesin the reference papers,  ??
is the set of sentencesin the related work part of the gold related worksection.
Then we can use the scoring method tocompute the target scores of the sentences in thetraining set.
It is noteworthy that two SVR mod-els can be trained on the two parts of the trainingdata, respectively.FeatureEach sentence is represented by a set of features.The common features used for the sentences ofthe target paper and reference papers are shownin Table 1.
The additional features applied to thesentences of the target paper are introduced inTable 2.Here, s is a sentence that needs to extract fea-tures.
th is paper title, section headings and sub-section headings set of the reference papers ortarget paper for the two SVR models, respective-ly.
Each feature with ?*?
represent a feature setthat contains similar features.All the features are scaled into [0, 1].
Thus wecan learn SVR models based on the features andimportance scores of the sentences, and then usethe models to predict an importance score foreach sentence in the test set.
The SVR modelsare trained and applied for the target paper andreference papers, respectively.Table 1: Common features employed in the SVRmodelsFeature Description???
(?, ??)?
The similarity between s and eachtitle in th; Stop words are removedand stemming is employed.WS(s,th) Number of words shared by s andth.??(?)?
The position of s in its section orsubsection???(?)?
The parse tree information of s,including the number of nounphrase and verb phrases, the depthof the parse tree, etc.??????(?)?
Indicates whether s is the first sen-tence of the section or subsection?????(?)?
Indicates whether s is the last sen-tence of the section or subsectionSWP(s) The percentage of the stop wordsLength(s) The length of sentence sLength_rw(s) The length of s after removing stopwordsSI(s) The section index of s that indi-cates which section s is from.??????????(?)?
Indicates whether a clue phraseappears in s. the clue phrases in-clude ?our work?, ?propose?
andother 20 words.
Each clue phrasecorresponds to one feature.Table 2: Additional features for sentences in thetarget paperFeature DescriptionHasCitation(s) Indicates whether s contains acitation?h??????????(?)?
Indicates whether s containswords or phrases used for com-parison such as ?in contrast?,?instead?
and other 26 words.Each word or phrase corre-sponds to one feature.4.4 A Global Optimization FrameworkIn the above steps, we can get the predicted im-portance score and topic information for eachsentence in the target paper and reference papers.Here, we introduce a global optimization frame-work to generate the related work section.According to the structure of the related worksection mentioned above, the related work sec-tion usually discusses several topics.
In each top-ic, the related works and their details are intro-duced.
Besides, the author often compares hisown work with these previous works.Therefore, we propose to formulate the genera-tion as an optimization problem.
Basically, wewill be searching for a set of sentences to opti-mize the objective function.1628Table 3: Notations used in this sectionSymbol Description???/???
the sentence in the reference/target paper???/???
the length of sentence ??
?/ ??????/???
the importance score of ???/???????/????
indicates whether ???/???
is selected intothe part of topic j in the generated relatedwork sectionnr/nt the number of sentences in the refer-ence/target papersm the topic count???
the topic weight of  ???/???
in topic j fromthe PLSA modelB the set of unique bigrams??
indicates whether bigram ??
is includedin the result???
the count of the occurrences of bigram ?
?in the both target paper and referencepapers????
the maximum word count of the relatedwork section??
the maximum word count of the part oftopic j which depends on the percentageof sentences belong to topic j??
the total set of bigrams in the whole pa-per set??
the set of bigrams that sentence  ???/???contains???/???
the set of sentences that include bigram??
in the reference/target papers?1,  ?2, ?3 parameters for tuningTo design the objective function, three aspectsshould be considered:1) First, the related work section we generateshould introduce the previous works well.
Inour assumption, sentences with higher im-portance scores are better to be selected.
Inaddition, very short sentences should be pe-nalized.
So we introduce the first part of ourobjective function below:?
(??????
?
???????)??=1??
?=1                    (2)We add the sentence length as a multipli-cation factor in order to penalize the veryshort sentences, or the objective functiontends to select more and shorter sentences.At the same time, the objective function doesnot tend to select the very long sentences.The total length of the sentences selected isfixed.
So if the objective function tends toselect the longer sentences, the fewer sen-tences can be selected.
A tradeoff needs to bemade between the number and the averagelength of the sentences selected.The constraints introduced below ensurethat the sentence can only be selected intoone topic and the topic weight is used tomeasure the degree that the sentence is rele-vant to the specific topic.2) Second, similar to the first part, we shouldconsider the own work part of the relatedwork section.
Thus the second part of our ob-jective function is shown as follows:?
(??????
?
???????)??=1??
?=1                    (3)3) At last, redundancy reduction should be con-sidered in the objective function.
The lastpart of the objective function is shown below:?
????
?|?|?=1                               (4)The intuition is that the more unique bi-grams the related work section contains, theless redundancy the related work section has.We add  ???
as the weight of the bigram in or-der to include more important bigrams.By combing all the parts defined above, wehave the following full objective function:max??,???1?
(???????????
?
???????)?
?=1 +???=1?2?
(???(1??)???????
?
???????)??=1??
?=1 +?3??????|?
?||?|?=1                                                       (5)Subject to:?
?????????
?=1 + ?
?????????
?=1 < ?
?, ???
?
= 1,?
,?
(6)?
?
?????????=1??
?=1 < ?????
(7)?
?
?????????=1??
?=1 < (1 ?
?)????
(8)?
?????
?=1 ?
1, ???
?
= 1,?
, ??
(9)?
?????
?=1 ?
1, ???
?
= 1,?
, ??
(10)?
???????
?
|?
?| ?
?????
?=1 , ???
?
= 1,?
, ??
(11)?
???????
?
|?
?| ?
?????
?=1 , ???
?
= 1,?
, ??
(12)?
?
?????
?=1 + ?
?
??????=1??????????????
?
??
,?
= 1,?
|?|                                                    (13)???
?, ???
?, ??
?
{0,1}                  (14)All the three parts in the objective function arenormalized to [0, 1] by using the maximumlength ????
and the total number of bigrams |??|.
?1, ?2 and ?3 are parameters for tuning the threeparts and we set  ?1+?2+?3 = 1.We explain the constraints as follows:Constraint (6): It ensures that the total wordcount of the part of topic j does not exceed  ?
?.Constraints (7), (8): The two constraints try tobalance the lengths of the previous works partand the own work part, respectively.
?
is set to2/3.Constraints (9), (10): These two constraintsguarantee that the sentence can only be includedinto one topic.1629Constraints (11), (12): When these two con-straints hold, all bigrams that ??
has are selectedif ??
is selected.Constraint (13): This constraint makes surethat at least one sentence in ???
or ???
is select-ed if bigram ??
is selected.Therefore, we transform our optimizationproblem into a linear programing problem.
Wesolve this linear programming problem by usingthe IBM CPLEX optimizer5.
It generally takestens of seconds to solve the problem and it isvery efficient.Finally, ARWG post-processes sentences toimprove readability, including replacing agentiveforms with a citation to the specific article (e.g.,?our work?
?
?
(Hoang and Kan, 2010)?)
for thesentences extracted from reference papers.
Thesentences belonging to different topics are placedseparately.5 Evaluation5.1 Evaluation SetupTo set up our experiments, we divide our datasetwhich contains 1050 target papers and their ref-erence papers into two parts: 700 target papersfor training, 150 papers for test and the other 200papers for validation.
The PLSA topic model isapplied to the whole dataset.
We train two SVRregression models based on the own work partand the previous work part of the training dataand apply the models to the test data.
The globaloptimization framework is used to generate therelated work sections.
We set the maximum wordcount of the generated related work section to beequal to that of the gold related work section.The parameter values of  ?1, ?2 and ?3 are set to0.3, 0.1 and 0.6, respectively.
The parameter val-ues are tuned on the validation data.We compare our system with five baseline sys-tems: MEAD-WT, LexRank-WT, ARWG-WT,MEAD and LexRank.
MEAD 6  (Radev et al.,2004) is an open-source extractive multi-document summarizer.
LexRank 7  (Eran andRadev, 2004) is a multi-document summarizationsystem which is based on a random walk on thesimilarity graph of sentences.
We also implementthe MEAD, LexRank baselines and our method5 www-01.ibm.com/software/integration/optimization/cplex-optimizer/6 http://www.summarization.com/mead/7 In our experiments, LexRank performs much better thanthe more complex variant - C-LexRank (Qazvinian andRadev, 2008), and thus we choose LexRank, rather than C-LexRank, to represent graph-based summarization methodsfor comparison in this paper.with only the reference papers (i.e.
the target pa-per?s content is not considered).
Those methodsare signed by ?-WT?.To evaluate the effectiveness of the SVR mod-els we employ, we implement a baseline systemRWGOF that uses the random walk scores as theimportant scores of the sentences and take thescores as inputs for the same global optimizationframework as our system to generate the relatedwork section.
The random walk scores are com-puted for the sentences in the reference papersand the target paper, respectively.We use the ROUGE toolkit to evaluate thecontent quality of the generated related work sec-tions.
ROUGE (Lin, 2004) is a widely used au-tomatic summarization evaluation method basedon n-gram comparison.
Here, we use the F-Measure scores of ROUGE-1, ROUGE-2 andROUGE-SU4.
The model texts are set as thegold related work sections extracted from thetarget papers, and word stemming is utilized.ROUGE-N is an n-gram based measure betweena candidate text and a reference text.
The recalloriented score, the precision oriented score andthe F-measure score for ROUGE-N are comput-ed as follows:?????
?
??????
?= ?
?
??????????(?????)???????{?????????
????}
/?
?
?????(?????)???????{?????????
????}
(15)?????
?
?????????
?= ?
?
??????????(?????)???????{?????????
????}
/?
?
?????(?????)???????{?????????
????}
(16)?????
?
?????????
?= 2 ?
?????
?
???????
?
?????
?
??????????
/?????
?
???????
+ ?????
?
??????????
(17)where n stands for the length of the n-gram?????
, and ??????????(?????)
is the maxi-mum number of n-grams co-occurring in a can-didate text and a reference text.In addition, we conducted a user study to sub-jectively evaluate the related work sections to getmore evidences.
We selected the related worksections generated by different methods for 15random target papers in the test set.
We askedthree human judges to follow an evaluationguideline we design and evaluate these relatedwork sections.
The human judges are graduatestudents in the computer science field and theydid not know the identities of the evaluated relat-ed work sections.
They were asked to give a rat-ing on a scale of 1 (very poor) to 5 (very good)for the correctness, readability and usefulness ofthe related work sections, respectively:16301) Correctness: Is the related work section ac-tually related to the target paper?2) Readability: Is the related work sectioneasy for the readers to read and grasp thekey content?3) Usefulness: Is the related work sectionuseful for the author to prepare their finalrelated work section?Paired T-Tests are applied to both the ROUGEscores and rating scores for comparing ARWGand baselines and comparing the systems withWT and without WT.5.2 Results and DiscussionTable 4: ROUGE F-measure comparison resultsMethod ROUGE-1 ROUGE-2 ROUGE-SU4Mead-WT0.39720 0.08785 0.14694LexRank-WT0.43267 0.09228 0.16312ARWG-WT0.45077?
{1,2} 0.09987?
{1,2} 0.16731?
{1}#{2}Mead 0.41012?
{1} 0.09642?
{1} 0.15441?
{1}LexRank 0.44235?
{2} 0.10090?
{2} 0.17067?
{2}ARWG ?.
??????{???}
?.
??????{???}
?.
??????{???
}(* represents pairwise t-test value p < 0.01; # rep-resents p < 0.05; the numbers in the brackets rep-resent the indices of the methods compared, e.g.1 for MEAD-WT, 2 for LexRank-WT, etc.
)Table 5: Average rating scores of judgesMethod Correctness Readability UsefulnessMead 2.971 2.664 2.716LexRank 2.958 2.847 2.784ARWG 3.433?# 3.420?# 3.382?#(*# represents pairwise t-test value p < 0.01,compared with Mead and LexRank, respectively.
)Table 6: ROUGE F-measure comparison of dif-ferent sentence importance scoresMethod ROUGE-1 ROUGE-2 ROUGE-SU4RWGOF 0.46932 0.11791 0.18426ARWG 0.47940 0.12176 0.18618The evaluation results over ROUGE metrics arepresented in Table 4.
It shows that our proposedsystem can get higher ROUGE scores, i.e., bettercontent quality.
In our system, we split the sen-tence set into different topic-biased parts, and theimportance scores of sentences in the target pa-per and reference papers are learned differently.So the obtained importance scores of the sen-tences are more reliable.The global optimization framework considersthe extraction of both the previous work part andthe own work part.
We can see the importance ofthe own work part by comparing the results ofthe methods with or without considering the ownwork part.
MEAD, LexRank and our method allget a significant improvement after consideringthe own work part by extracting sentences fromthe target paper.
The results also prove our as-sumption about the related work section structure.Figure 3 presents the fluctuation of ROUGEscores when tuning the parameters ?1, ?2 and ?3.We can see our method generally performs betterthan the baselines.
All the three parts in the ob-jective function are useful to generate relatedwork sections with good quality.The average scores rated by human judges foreach method are showed in Table 5.
We can seethat the related work sections generated by oursystem are more related to the target papers.Moreover, because of the good structure of ourgenerated related work sections, our generatedrelated work sections are considered more reada-ble and more useful for the author to prepare thefinal related work sections.T-test results show that the performance im-provements of our method over baselines arestatistically significant on both automatic andmanual evaluations.
Most of p-values for t-testare far smaller than 0.01.Overall, the results indicate that our methodcan generate much better related work sections00.600.20.40.60 0.3 0.6 0.9 ?1ROUGE-1?2ROUGE-10.4-0.60.2-0.400.600.050.10.150 0.3 0.6 0.9?1ROUGE-2?2ROUGE-20.1-0.150.05-0.100.600.050.10.150.20 0.3 0.6 0.9 ?1ROUGE-SU4?2ROUGE-SU40.15-0.20.1-0.15Figure 3: Parameter influences (horizontal, vertical axis are ?1, ?2 , respectively, ?3 = 1 ?
?1 ?
?2 )1631than the baselines on both automatic and humanevaluations.Table 6 shows the comparison results betweenARWG and RWGOF.
We can see ARWG per-forms better than RWGOF.
It proves that theSVR models can better estimate the importancescores of the sentences.
For the SVR models aretrained from the large dataset, the sentencescores predicted by the SVR models can be morereliable to be used in the global optimizationframework.6 Conclusion and Future WorkThis paper proposes a novel system calledARWG to generate related work sections for ac-ademic papers.
It first exploits a PLSA model tosplit the sentence set of the given papers into dif-ferent topic-biased parts, and then applies regres-sion models to learn the importance scores of thesentences.
At last an optimization framework isproposed to generate the related work section.Evaluation results show that our system can gen-erate much better related work sections than thebaseline methods.In future work, we will make use of citationsentences to improve our system.
Citation sen-tences are the sentences that contains an explicitreference to another paper and they usually high-light the most important aspects of the cited pa-pers.
So citation sentences are likely to containimportant and rich information for generatingrelated work sections.AcknowledgmentsThe work was supported by National NaturalScience Foundation of China (61170166,61331011), Beijing Nova Program (2008B03)and National Hi-Tech Research and Develop-ment Program (863 Program) of China(2012AA011101).
We also thank the anonymousreviewers for very helpful comments.
The corre-sponding author of this paper, according to themeaning given to this role by Peking University,is Xiaojun Wan.ReferenceNitin Agarwal, Kiran Gvr, Ravi Shankar Reddy, andCarolyn Penstein Ros?.
2011.
Towards multi-document summarization of scientific articles:making interesting comparisons with SciSumm.In Proceedings of the Workshop on AutomaticSummarization for Different Genres, Media, andLanguages, pp.
8-15.
Association for Computa-tional Linguistics.Phyllis B. Baxendale.
1958.
Machine-made index fortechnical literature: an experiment.
IBM Journal ofResearch and Development 2, no.
4: 354-361.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies-Volume 1, pp.
481-490.Association for Computational Linguistics.Chih-Chung Chang, and Chih-Jen Lin.
2011.LIBSVM: a library for support vector ma-chines.
ACM Transactions on Intelligent Systemsand Technology (TIST) 2, no.
3: 27.John M. Conroy, and Dianne P. O'leary.
2001.
Textsummarization via hidden markov models.In Proceedings of the 24th annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, pp.
406-407.
ACM.Harold P. Edmundson.
1969.
New methods in auto-matic extracting.
Journal of the ACM (JACM) 16,no.
2: 264-285.G?nes Erkan, and Dragomir R. Radev.
2004.
LexPag-eRank: Prestige in Multi-Document Text Summa-rization.
In EMNLP, vol.
4, pp.
365-371.G?nes Erkan, and Dragomir R. Radev.
2004.LexRank: Graph-based lexical centrality as sali-ence in text summarization.
J. Artif.
Intell.Res.
(JAIR) 22, no.
1: 457-479.Dimitrios Galanis, Gerasimos Lampouras, and IonAndroutsopoulos.
2012.
Extractive Multi-Document Summarization with Integer Linear Pro-gramming and Support Vector Regression.In COLING, pp.
911-926.Dimitrios Galanis, and Prodromos Malakasiotis.
2008.Aueb at tac 2008.
InProceedings of the TAC 2008Workshop.Dan Gillick, and Benoit Favre.
2009.
A scalable glob-al model for summarization.
InProceedings of theWorkshop on Integer Linear Programming forNatural Langauge Processing, pp.
10-18.
Associa-tion for Computational Linguistics.Cong Duy Vu Hoang, and Min-Yen Kan. 2010.
To-wards automated related work summarization.In Proceedings of the 23rd International Confer-ence on Computational Linguistics: Posters, pp.427-435.
Association for Computational Linguis-tics.Lei Huang, Yanxiang He, Furu Wei, and Wenjie Li.2010.
Modeling document summarization as multi-objective optimization.
In Intelligent InformationTechnology and Security Informatics (IITSI), 2010Third International Symposium on, pp.
382-386.IEEE.1632Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the 22nd annual inter-national ACM SIGIR conference on Research anddevelopment in information retrieval, pp.
50-57.ACM.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
InText SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pp.
74-81.Hans Peter Luhn.
1958.
The automatic creation ofliterature abstracts.
IBM Journal of research anddevelopment 2, no.
2: 159-165.Inderjeet Mani, and Eric Bloedorn.
1999.
Summariz-ing similarities and differences among related doc-uments.
Information Retrieval 1, no.
1-2: 35-67.Ryan McDonald.
2007.
A study of global inferencealgorithms in multi-document summarization.Springer Berlin Heidelberg.Qiaozhu Mei, and ChengXiang Zhai.
2008.
Generat-ing Impact-Based Summaries for Scientific Litera-ture.
In ACL, vol.
8, pp.
816-824.Rada Mihalcea, and Paul Tarau.
2005.
A languageindependent algorithm for single and multiple doc-ument summarization.Rada Mihalcea, and Hakan Ceylan.
2007.
Explora-tions in Automatic Book Summarization.In EMNLP-CoNLL, pp.
380-389.Saif Mohammad, Bonnie Dorr, Melissa Egan, AhmedHassan, Pradeep Muthukrishan, Vahed Qazvinian,Dragomir Radev, and David Zajic.
2009.
Using ci-tations to generate surveys of scientific paradigms.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pp.
584-592.
Association for Computa-tional Linguistics.Preslav Nakov, Ariel Schwartz, and M. Hearst.
2004.Citation sentences for semantic analysis of biosci-ence text.
In Proceedings of the SIGIR'04 work-shop on Search and Discovery in Bioinformatics.Ramesh M. Nallapati, Amr Ahmed, Eric P. Xing, andWilliam W. Cohen.
2008.
Joint latent topic modelsfor text and citations.
In Proceedings of the 14thACM SIGKDD international conference onKnowledge discovery and data mining, pp.
542-550.
ACM.Vahed Qazvinian, and Dragomir R. Radev.
2008.
Sci-entific paper summarization using citation sum-mary networks.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics-Volume 1, pp.
689-696.
Association for Computa-tional Linguistics.You Ouyang, Sujian Li, and Wenjie Li.
2007.
Devel-oping learning strategies for topic-based summari-zation.
In Proceedings of the sixteenth ACM con-ference on Conference on information andknowledge management, pp.
79-86.
ACM.Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda Celebi, StankoDimitrov, Elliott Drabek et al.
2004.
MEAD-a plat-form for multidocument multilingual text summa-rization.
Proceedings of the 4th International Con-ference on Language Resources and Evaluation(LREC 2004).Ariel S. Schwartz, and Marti Hearst.
2006.
Summariz-ing key concepts using citation sentences.In Proceedings of the Workshop on Linking Natu-ral Language Processing and Biology: TowardsDeeper Biological Literature Analysis, pp.
134-135.Association for Computational Linguistics.Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, andZheng Chen.
2007.
Document Summarization Us-ing Conditional Random Fields.
In IJCAI, vol.
7,pp.
2862-2867.Andreas  Stolcke, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky,Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer.
2000.
Dialogue actmodeling for automatic tagging and recognition ofconversational speech.
Computational linguis-tics 26, no.
3: 339-373.Kristian Woodsend, and Mirella Lapata.
2012.
Multi-ple aspect summarization using integer linear pro-gramming.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural Lan-guage Processing and Computational NaturalLanguage Learning, pp.
233-243.
Association forComputational Linguistics.Shasha Xie, Benoit Favre, Dilek Hakkani-T?r, andYang Liu.
2009.
Leveraging sentence weights in aconcept-based optimization framework for extrac-tive meeting summarization.
In INTERSPEECH,pp.
1503-1506.Hui Yang, and Jamie Callan.
2009.
A metric-basedframework for automatic taxonomy induction.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Pro-cessing of the AFNLP: Volume 1-Volume 1, pp.271-279.
Association for Computational Linguis-tics.Ozge Yeloglu, Evangelos Milios, and Nur Zincir-Heywood.
2011.
Multi-document summarization ofscientific corpora.
In Proceedings of the 2011 ACMSymposium on Applied Computing, pp.
252-258.ACM.1633
