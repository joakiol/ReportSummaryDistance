The CMU Air Travel Information Service:Understanding Spontaneous SpeechWayne WardSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, Pa 15213IntroductionUnderstanding spontaneous peech presents severalproblems not found in processing read speech input.
Spon-taneous peech is often not fluent.
It contains tutters, filledpauses, restarts, repeats, interjections, etc.
Casual users donot know the lexicon and grammar used by the system.
It istherefore very difficult for a speech understanding systemto achieve good coverage of the lexicon and grammar thatsubjects might use.The Air Travel Information Service task is being used todevelop and evaluate speech understanding systems fordatabase query like tasks.
In the ATIS task, novice usersare asked to perform a task that requires getting infor-marion from the Air Travel database.
This database con-tains information about flights and their fares, airports,aircraft, etc.
Users compose the questions themselves, andare allowed to phrase the queries any way they choose.
Noexplicit grammar or lexicon is given to the subject.At CMU, we are developing a system, called Phoenix,to understand spontaneous speech.
We have implementedan initial version of this system for the ATIS task.
Thispaper presents the design of the Phoenix system and itscurrent status.
We also report results for the first ATISevaluation set distributed by NIST.The Phoenix SystemThe problems posed by spontaneous speech can bedivided into four categories?
User noise - breath noise, filled pauses and other usergenerated noise?
Environment oise - door slams, phone rings, etc.?
Out-of-vocabulary words - The subject says wordsthat the system doesn't know.?
Grammatical coverage - Subjects often usegrammatically ill-formed utterances and restart andrepeat phrases.Phoenix addresssound models, anttexible parsing.these problems by using non-verbalout-of-vocabulary word model andNon-verbal sound modelsModels for sounds other than speech ave been shownto significantly increase performance of HMM-basedrecognizers for noisy input.
\[5\] \[7\] In this technique, ad-dirional models are added to the system that represent non-verbal sounds, just as word models represent verbalsounds.
These models are trained exactly as if they wereword models, but using the noisy input.
Thus, sounds thatare not words are allowed to map onto tokens that are alsonot words.Out-of-vocabulary word modelIn order to deal with out-of-vocabulary words, we areusing a technique ssentially the same as the one presentedby BBN.
\[1\] We have an explicit model for out-of-vocabulary words.
This model allows any triphone (contextdependent phone) to follow any other triphone (given ofcourse that the context is the same) with a bigram prob-ability model.
The bigrams are trained from a large diction-ary of English pronunciations.Flexible parsingWe use a frame based parser similar to the DYPARparser used by Carbonell, et al to process ill-formedtext, \[2\] and the MINDS system previously developed atCMU.
\[8\] Semantic information is represented by a set offrames.
Each frame contains a set of slots representingpieces of information.
In order to fill in the frames, we usea partitioned semantic phrase grammar.
The grammar is asemantic grammar, non-terminals are semantic oncepts in-stead of parts of speech.
The grammar is also written sothat phrases can stand alone (be recognized by a net) aswell as being embedded in a sentence.
Strings of phraseswhich do not form a grammatical English sentence are stillparsed by the system.
The grammar is compiled into a setof finite-state networks.
Networks can "call" other net-works, thereby significantly reducing the overall size of thesystem.
These networks are used to perform patternmatches again.~t word strings.
The grammar is partitioned,instead of one big network, there are many small networks.Each slot type is represented by a separate network whichspecifies all ways of saying the meaning represented by theslot.
This general approach as been described in an earlierpaper.
\[6\]The operation of the parser can be viewed as "phrasespotting".
A beam of possible interpretations are pursued127Table 1:Source Number True Number False No Answer Percent CorrectTranscript 45 47 1 48Speech 36 57 0 39Results as scored by NIST.Source Number True Number False No Answer Percent CorrectTranscript 60 32 1 65Speech 39 54 0 42Table 2: Rescored resultssimultaneously.
An interpretation is a frame with some ofits slots filled.
The finite-state networks perform patternmatches against he input string.
When a phrase is recog-nized, it attemps to extend all current interpretations.
Thisamounts to dynamic programming on series of phrases.The score for an interpretation is the number of input wordsthat it accounts for.
At the end of the utterance, the bestscoring interpretation is output.Sys tem St ructureThe overall structure of the system is shown in Figure 1.We use the Sphinx system as our recognizer module.
\[4\].Currently, it is a Top-1 system.
That is, the recognizer andparser are not integrated.
The grammar used by the parseris used to generate a word pair grammar.
The recognizeruses the word pair grammar in decoding the speech input.The recognizer produces a single best hypothesis.
Thishypothesis then passed to the frame-based parser whichassigns word strings to slots in a flame as explained above.The slots in the flame are then mapped to canonicalform.
This puts all dates, times, names, etc.
in a standardform for the routines that build the database query.
At thisstep ellipsis and anaphora re resolved using current ob-jects built as a result of previous utterances.
Objects con-sist of currently active constraints, the set of flights thatmeet he constraints and a list of individual flights in focus.Resolution of ellipsis and anaphora is relatively simple inthis system.
We are aided greatly by the fact that the slotsin frames are semantic, thus we know the type of objectneeded for the resolution.
The canonical flame representsthe information that was extracted from the utterance.
It isthen used to build a database query.
This query is sent tothe SYBASE database management system and thereturned results are displayed to the user.Resu l tsOur current system has a 484 word vocabulary and aword pair grammar with perplexity 85.
We use theVocabulary-independent phone models generated by Hon.\[3\] We have not yet added the non-verbal and out-of-vocabulary models to the system.
The only technique cur-rently used to cope with spontaneous speech is a word pairgrammar and a flexible parser.128DSPSphinxError ConectingParleriDialog-BuedA~b~ResolutionStructure of PhoenixA Spoken I..anguage Understanding Systemspeech: "show me...nh...I want to see all the flights toDenver afu:r two pro"digitize: 16 KHz, 16 bit samplesVQ codes: A vectorr of 3 bytes, eesch 10 mswords: "show me I want w see all flights tofTalnc:canonicalframe:ATLSAppllcadonTravelDatabaseDenver after two ~n"\[liet\]: I want to,\[flights\]: all flight~\[a.iv~Ioe\]: to Denver\[deparUime,_rang@ after two pm\[list\]: list\[flighm\]: flights\[arrive..loc\]: "DEN"\ [ ,~.
r t_ l * \ ] :  "l iT"\[depart_ame range\]: 1400 2400select air l i~code, fllghLntmaberfrom flight_tablewhere (fr~m_eirport ='PIT' and t~airport ='DEN')and (dcpermr~tirae > 14130)Figure 1: Structure of the Phoenix systemThe test data consists of a total of 93 utterances takenfrom five speakers.
The data was gathered by TI and dis-tributed by NIST.
All utterances were "Class-A".
Bothtranscript and speech input were processed.
The databaseoutput in CAS format was sent to NIST where it wasscored against the reference database answers.
Table 1shows the results of the NIST evaluation.As a result of errors in generating the output to bescored, a significant number of utterances that parsed cor-rectly were scored as incorrect.
Most of these were of threetypes that resulted from a misunderstanding on my part asto what was to be generated.?
Dates - Our system generated dates relative to thetime the system was run instead of relative to whenthe corpus was gathered.Sourc SubsWord 25String 97Table 3: Recognition error rates (percentages)Del Ins Error15 zt 4.497* Abbrevations - We printed codes or abbreviationsrather than the full text description as an answer.?
Round-trip - The test for round-trip fares (of flights)was incorrectly apphed.Output in these situations was correct given the (incor-rect) assumptions that I used.
In order to understand thesystem's behavior, it is useful to look at the scores if thethree bugs were fixed.
This more fully reflects the trueabilities of the system.
After sending our output o NIST, Ifixed these three bugs (total time under three hours) andreprocessed the test data.
Table 2 presents the same testdata after these changes.Analysis of trace ouput for the data showed that 75 per-cent of the transcript utterances parsed correctly.
The ad-ditional degradation to65 percent is a result of other errorsin generating database queries.It is also interesting to examine the word and string errorrates for the recognizer output.
These are shown in Table 3.A string error rate of 97 percent means that only threepercent of the utterances contained no errors.
However, 42percent of the utterances gave correct answers.
This il-lustrates the ability of the parser to handle minor misrecog-nifions in the recognized string.
The word error rate of 44percent is poor given the high quality of the basic recog-nizer and relatively low perplexity of the word pair gram-mar.
We feel that this will improve considerably with theaddition of non-verbal and out-of-vocabulary models andwith better lexical and grammatical coverage.References1.
Asadi, A., Schwartz, R., Makhoul, J.
Automatic Detec-tion Of New Words In A Large Vocabulary ContinuousSpeech Recognition System.
Proceedings of the DARPASpeech and Natural Language Workshop, 1989, pp.
263,265.2.
Carbonell, J.G.
and Hayes, P.J.
Recovery Strategies forParsing Extragrammafical L nguage.
Tech.
Rept.
CMU-CS-84-107, Carnegie-Mellon University Computer ScienceTechnical Report, 1984.3.
Hon, H.W., Lee, K.F., Weide, R. Towards SpeechRecognition Without Vocabulary-Specific Training.Proceedings of the DARPA Speech and Natural LanguageWorkshop, 1989, pp.
271,275.4.
Lee, K.-F. Automatic Speech Recognition: TheDevelopment ofthe SPHINX System.
Kluwer AcademicPublishers, Boston, 1989.5.
Ward, W. Modelling Non-verbal Sounds for SpeechRecognition.
Proceedings of the DARPA Speech andNatural Language Workshop, 1989, pp.
47, 50.6.
Ward, W. Understanding Spontaneous Speech.Proceedings of the DARPA Speech and Natural LanguageWorkshop, 1989, pp.
137, 141.7.
Wflpon, J.G., Rabiner, L.R., Lee, C.H., Goldman, E.R.Automatic Recogmtion ofVocabulary Word Sets in Un-constrained Speech Using Hidden Markov Models.
inpress Transactions ASSP, 1990.8.
Young, S. R., Hauptmann, A. G., Ward, W. H., Smith,E.
T. and Wemer, P. "High Level Knowledge Sources inUsable Speech Recognition Systems".
Communications ofthe ACM 32, 2 (1989), 183-194.129
