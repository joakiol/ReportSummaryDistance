Interactive Machine Learning Techniques for Improving SLU ModelsLee BegejaBernard RengerAT&T Labs-Research180 Park AveFlorham Park, NJ 07932{lee,renger}@research.att.comDavid GibbonZhu LiuBehzad ShahrarayAT&T Labs-Research200 Laurel Ave SMiddletown, NJ  07748{dcg, zliu, behzad}@research.att.comAbstractSpoken language understanding is a criticalcomponent of automated customer service ap-plications.
Creating effective SLU models isinherently a data driven process and requiresconsiderable human intervention.
We de-scribe an interactive system for speech datamining.
Using data visualization and interac-tive speech analysis, our system allows a UserExperience (UE) expert to browse and under-stand data variability quickly.
Supervisedmachine learning techniques are used to cap-ture knowledge from the UE expert.
This cap-tured knowledge is used to build an initialSLU model, an annotation guide, and a train-ing and testing system for the labelers.
Ourgoal is to shorten the time to market by in-creasing the efficiency of the process and toimprove the quality of the call types, the callrouting, and the overall application.1 IntroductionThe use of spoken dialogue systems to automate ser-vices in call centers is continually expanding.
In onesuch system, unconstrained speech recognition is usedin a limited domain to direct call traffic in customer callcenters (Gorin et al 1997).
The challenge in this envi-ronment is not only the accuracy of the speech recogni-tion but more importantly, the knowledge andunderstanding of how the customer request is mapped tothe business requirement.The first step of the process is to collect utterancesfrom customers, which are transcribed.
This gives us abaseline for the types of requests (namely, the user in-tents) that customers make when they call a client.
AUE expert working with the business customer useseither a spreadsheet or a text document to classify thesecalls into call types.
For example,?
?I want a refund?
  REFUND?
?May I speak with an operator?
GET_CUSTOMER_REPThe end result of this process is a document, the an-notation guide, that describes the types of calls that maybe received and how to classify them.
This guide isthen given to a group of ?labelers?
who are trained andgiven thousands of utterances to label.
The utterancesand labels are then used to create the SLU model for theapplication.
The call flow which maps the call types torouting destinations (dialog trajectory) is finalized andthe development of the dialogue application begins.After the field tests, the results are given to the UE ex-pert, who then will refine the call types, create a newannotation guide, retrain the labelers, redo the labels andcreate new ones from new data and rebuild the SLUmodel.Previously, this knowledge was only captured in adocument and was not formalized until the SLU modelwas generated.
Our goal in creating our system is notonly to give the UE expert tools to classify the calls, butto capture and formalize the knowledge that is gained inthe process and to pass it on to the labelers.
We canthus automatically generate training instances and test-ing scenarios for the labelers, thereby creating moreconsistent results.
Additionally, we can use the SLUmodel generated by our system to ?pre-label?
the utter-ances.
The labelers can then view these ?pre-labeled?utterances and either agree or disagree with the gener-ated labels.
This should speed up the overall labelingprocess.More importantly, this knowledge capture will en-able the UE expert to generate and test a SLU model aspart of the process of creating the call types for thespeech data.
The feedback from this initial SLU testallows the UE expert to refine the call types and to im-prove them without having to train a group of labelersand to run a live test with customers.
This results in animproved SLU model and makes it easier to find prob-lems before deployment, thus saving time and money.Figure 1.
System DiagramAt the same time, the process is more efficient due tothe increased uniformity in the way different UE expertsclassify calls into call type labels.We will describe Annomate, an interactive systemfor speech data mining.
In this system, we employ sev-eral machine learning techniques such as clustering andrelevance feedback in concert with standard text search-ing methods.
We focus on interactive dynamic tech-niques and visualization of the data in the context of theapplication.The paper is organized as follows.
The overview ofthe system is presented in Section 2.
Section 3 brieflydiscusses the different components of the system.
Someresults are given in Section 4.
Finally, in Sections 5 and6, we give conclusions and point to some future direc-tions.2 System OverviewIn this section, we will give a system overview andshow how automation has sped up and improved theexisting process.
The UE expert no longer needs tokeep track of utterances or call type labels in spread-sheets.
Our system allows the UE expert to more easilyand efficiently label collected utterances in order toautomatically build a SLU model and an electronic an-notation guide (see System Diagram in Figure 1).
Thebox in Figure 1 contains the new components used inthe improved and more automated process of creatingSLU models and annotation guides.After data collection, the Preprocessing steps (thedata reduction and clustering steps are described inmore detail below) reduce the data that the UE expertneeds to work with thus saving time and money.
TheProcessed Data, which initially only contains the tran-scribed utterances but later will also contain call types,is stored in an XML database which is used by the WebInterface.
At this point, various components of the WebInterface are applied to create call types from utterancesand the Processed Data (utterances and call types) con-tinue to get updated as these changes are applied.
Theseinclude the Clustering Tool to fine-tune the optimalclustering performance by adjusting the clusteringthreshold.
Using this tool, the UE expert can easilybrowse the utterances within each cluster and comparethe members of one cluster with those of its neighboringclusters.
The Relevance Feedback component is im-plemented by the Call Type Editor Tool.
This tool pro-vides an efficient way to move utterances between twocall types and to search relevant utterances for a specificcall type.
The Search Engine is used to search text in theutterances in order to facilitate the use of relevancefeedback.
It is also used to get a handle on utterance andLa-belers TranscribedUtterancesAnnotationGuidePreprocessing?Data Reduction?ClusteringProcessedDataWeb-Enabled User InterfaceClusteringRelevanceFeedbackSearch EngineReportGenerationInitialSLUUserExperienceExpertSLUToolsetVISUALIZATIONcall type proximity using utterance and cluster dis-tances.After a reasonable percentage of the utterances arepopulated or labeled into call types, an initial SLUmodel can be built and tested using the SLU Toolset.Although a reduced dataset is used for labeling (seediscussion on clone families and reduced dataset be-low), all the utterances are used when building the SLUmodel in order to take advantage of more data andvariations in the utterances.
The UE expert can itera-tively refine the SLU model.
If certain test utterancesare being incorrectly classified or are not providing suf-ficient differentiability among certain call types (theSLU metric described below is used to improve calltype differentiation), then the UE expert can go backand modify the problem call types (by adding utterancesfrom other call types or by removing utterances usingthe Web Interface).
The updated Processed Data canthen be used to rebuild the SLU model and it can beretested to ensure the desired result.
This initial SLUmodel can also be used as a guide in determining thecall flow for the application.The Reporting component of the Web Interface canautomatically create the annotation guide from theProcessed Data in the XML database at any time usingthe Annotation Guide Generation Tool.
If changes aremade to utterances or call types, then the annotationguide can be regenerated almost instantly.
Thus, thisWeb Interface allows the UE expert to easily and moreefficiently create the annotation guide in an automatedfashion unlike the manual process that was used before.3 ComponentsMany SLU systems require data collection and someform of utterance preprocessing and possibly utteranceclustering.
Our system uses relevance feedback andSLU tools to improve the SLU process.3.1 Data CollectionNatural language data exists in a variety of forms suchas documents, e-mails, and text chat logs.
We will focushere on transcriptions of telephone conversations, and inparticular, on data collected in response to the firstprompt from an open dialogue system.
The utterancescollected are typically short phrases or single sentences,although in some cases, the caller may make severalstatements.
It is assumed that there may be multipleintents for each utterance.
We have also found that themethods presented here work well when used with theone-best transcription from a large vocabulary auto-matic speech recognition system instead of manual tran-scription.3.2 PreprocessingOur tools add structure to the raw collected data througha series of preprocessing steps.
Utterance redundancy(and even repetition) is inherent in the collection proc-ess and this is tedious for UE experts to deal with asthey examine and work with the dataset.
This sectiondescribes taking the original utterance set and reducingthe redundancy (using text normalization, named entityextraction, and feature extraction) and thereby the vol-ume of data to be examined.
The end product of thisprocessing is a subset of the original utterances thatrepresents the diversity of the input data in a conciseway.
Sets of identical or similar utterances are formedand one utterance is selected at random to representeach set (alternative selection methods are also possible,see the Future Work section).
UE experts may choose toexpand these clone families to view individual mem-bers, but the bulk of the interaction needs to only in-volve a single representative utterance from each set.Text NormalizationThere is a near continuous degree of similarity betweenutterances.
At one extreme are exact text duplicates(data samples in which two different callers say the ex-act same thing).
At the next level, utterances may differonly by transcription variants like ?100?
vs. ?one hun-dred?
or ?$50?
vs. ?fifty dollars.?
Text normalization isused to remove this variation.
Moving further, utter-ances may differ only by the inclusion of verbal pausesor of transcription markup such as: ?uh, eh, backgroundnoise.?
Beyond this, for many applications it is insig-nificant if the utterances differ only by contraction: ?I?dvs.
I would?
or ?I wanna?
vs. ?I want to.?
Acronymexpansions can be included here: ?I forgot my personalidentification number?
vs. ?I forgot my P I N.?
Up tothis point it is clear that these variations are not relevantfor the purposes of intent determination (but of coursethey are useful for training a SLU classifier).
We couldgo further and include synonyms or synonymousphrases: ?I want?
vs. ?I need.?
Synonyms however,quickly become too powerful at data reduction, collaps-ing semantically distinct utterances or producing otherundesirable effects (?I am in want of a doctor.?)
Also,synonyms may be application specific.Text normalization is handled by string replacementmappings using regular expressions.
Note that thesemay be represented as context free grammars and com-posed with named entity extraction (see below) to per-form both operations in a single step.
In addition toone-to-one replacements, the normalization includesmany-to-one mappings (you   	-to-null mappings (to remove noise words).Named Entity ExtractionUtterances that differ only by an entity value shouldalso be collapsed.
For example ?give me extension12345?
and ?give me extension 54321?
should be repre-sented by ?give me extension extension_value.?
Namedentity extraction is implemented through rules encodedusing context free grammars in Backus-Naur form.
Alibrary of generic grammars is available for such thingsas phone numbers and the library may be augmentedwith application-specific grammars to deal with accountnumber formats, for example.
The grammars are view-able and editable, through an interactive web interface.Note that any grammars developed or selected at thispoint may also be used later in the deployed applicationbut that the named entity extraction process may also bedata driven in addition to or instead of being rule based.Feature ExtractionTo perform processing such as clustering, relevancefeedback, or building prototype classifiers, the utter-ances are represented by feature vectors.
At the simplestlevel, individual words can be used as features (i.e., aunigram language model).
In this case, a lexis or vo-cabulary for the corpus of utterances is formed and eachword is assigned an integer index.
Each utterance is thenconverted to a vector of indices and the subsequentprocessing operates on these feature vectors.
Othermethods for deriving features include using bi-grams ortri-grams as features, weighting features based upon thenumber of times a word appears in an utterance or howunusual the word is in the corpus (TF, TF-IDF), andperforming word stemming (Porter, 1980).
When thedataset available for training is very small (as is the casefor relevance feedback) it is best to use less restrictivefeatures to effectively amplify the training data.
In thiscase, we have chosen to use features that are invariant toword position, word count and word morphology andwe ignore noise words.
With this, the following twoutterances have identical feature vector representations:?
I need to check medical claim status?
I need check status of a medical claimNote that while these features are very useful for theprocess of initially analyzing the data and defining calltypes, it is appropriate to use a different set of featureswhen training classifiers with large amounts of datawhen building the SLU model to be fielded.
In that case,tri-grams may be used, and stemming is not necessarysince the training data will contain all of the relevantmorphological variations.ClusteringAfter the data reductions steps above, we use clusteringas a good starting point to partition the dataset into clus-ters that roughly map to call types.Clustering is grouping data based on their intrinsicsimilarities.
After the data reduction steps describedabove, clustering is used as a bootstrapping process tocreate a reasonable set of call types.In any clustering algorithm, we need to define thesimilarity (or dissimilarity, which is also called distance)between two samples, and the similarity between twoclusters of samples.
Specifically, the data samples in ourtask are call utterances.
Each utterance is converted intoa feature vector, which is an array of terms and theirweights.
The distance of two utterances is defined as thecosine distance between corresponding feature vectors.Assume x and y are two feature vectors, the distanced(x,y) between them is given byyxyxyx??
?= 1),(dAs indicated in the previous section, there are differ-ent ways to extract a feature vector from an utterance.The options include named entity extraction, stop wordremoval, word stemming, N-gram on terms, and binaryor TF-IDF (Term frequency ?
inverse document fre-quency) based weights.
Depending on the characteris-tics of the applications in hand, certain combinations ofthese options are appropriate.
For all the results pre-sented in this paper, we applied named entity extraction,stop word removal, word stemming, and 1-gram termwith binary weights to extract the feature vectors.The cluster distance is defined as the maximum dis-tance between any pairs of two utterances, one fromeach cluster.
Figure 2 illustrates the definition of thecluster distance.Figure 2.
Illustration of Cluster Distance.The range of utterance distance is from 0 to 1, andthe range of the cluster distance is the same.
When thecluster distance is 1, it means that there exists at leastone pair of utterances, one from each cluster, that aretotally different (sharing no common term).The clustering algorithm we adopted is the Hierar-chical Agglomerative Clustering (HAC) method.
Thedetails of agglomerative hierarchical clustering algo-rithm can be found in (Jan and Dubes, 1988).
The fol-lowing is a brief description of the HAC procedure.Initially, each utterance is a cluster on its own.
Then, foreach iteration, two clusters with a minimum distancevalue are merged.
This procedure continues until theminimum cluster distance exceeds a preset threshold.The principle of HAC is straightforward, yet the compu-tational complexity and memory requirements may behigh for large size datasets.
We developed an efficientimplementation of HAC by on-the-fly cluster/utterancedistance computation and by keeping track of the clusterdistances from neighboring clusters, such that the mem-ory usage is effectively reduced and the speed is signifi-cantly increased.Our goal is to partition the dataset into call typesrecognized by the SLU model and the clustering resultsprovide a good starting point.
It is easier to transform aset of clusters into call types than to create call typesdirectly from a large set of flat data.
Depending on thedistance threshold chosen in the clustering algorithm,the clustering results may either be conservative (withsmall threshold) or aggressive (with large threshold).
Ifthe clustering is conservative, the utterances of one calltype may be scattered into several clusters, and the UEexpert has to merge these clusters to create the call type.On the other hand, if the cluster is aggressive, there maybe multiple call types in one cluster, and the UE expertneeds to manually split the mixture cluster into differentcall types.
In real applications, we tend to set a rela-tively low threshold since it is easier to merge smallhomogeneous clusters than to split one big heterogene-ous cluster.3.3 Relevance FeedbackAlthough clustering provides a good starting point, find-ing all representative utterances belonging to one calltype is not a trivial task.
Effective data mining tools aredesirable to help the UE expert speed up this manualprocedure.
Our solution is to provide a relevance feed-back mechanism based on support vector machine(SVM) techniques for the UE expert to perform thistedious task.Relevance feedback is a form of query-free retrievalwhere documents are retrieved according to a measureof relevance to given documents.
In essence, a UE ex-pert indicates to the retrieval system that it should re-trieve ?more documents like the ones desired, not theones ignored.?
Selecting relevant documents based onUE expert?s inputs is basically a classification (rele-vant/irrelevant) problem.
We adopted support vectormachine as the classifier for to two reasons: First, SVMefficiently handles high dimensional data, especially atext document with a large vocabulary.
Second, SVMprovides reliable performance with small amount oftraining data.
Both advantages perfectly match the taskat hand.
For more details about SVM, please refer to(Vapnik, 1998; Drucker et al 2002).Relevance feedback is an iterative procedure.
TheUE expert starts with a cluster or a query result by cer-tain keywords, and marks each utterance as either apositive or negative utterance for the working call type.The UE expert?s inputs are collected by the relevancefeedback engine, and they are used to build a SVM clas-sifier that attempts to capture the essence of the call type.The SVM classifier is then applied to the rest of theutterances in the dataset, and it assigns a relevance scorefor each utterance.
A new set of the most relevant utter-ances are generated and presented to the UE expert, andthe second loop of relevance feedback begins.
Duringeach loop, the UE expert does not need to mark all thegiven utterances since the SVM is capable of building areasonable classifier based on very few, e.g., 10, train-ing samples.
The superiority of relevance feedback isthat instead of going through all the utterances one byone to create a specific call type, the UE expert onlyneeds to check a small percentage of utterances to createa satisfactory call type.Figure 3.
The Interface for Relevance Feedback.The relevance feedback engine is implemented bythe Call Type Editor Tool.
This tool provides an inte-grated environment for the UE expert to create a varietyof call types and assign relevant utterances to them.The tool provides an efficient way to move utterancesbetween two call types and to search relevant utterancesfor a specific call type.
The basic search function is tosearch a keyword or a set of keywords within the datasetand retrieve all utterances containing these search terms.The UE expert can then assign these utterances into theappropriate call types.
Relevance feedback serves as anadvanced searching option.
Relevance feedback can beapplied to the positive and negative utterances of a clus-Table 1.
Data Reduction Resultster or call type or can be applied to utterances, from asearch query, which are marked as positive or negative.The interface for the relevance feedback is shown inFigure 3.
In the interface, the UE expert can mark theutterances as positive or negative samples.
The UE ex-pert can also control the threshold of the relevance valuesuch that the relevance feedback engine only returnsutterances with high enough relevance values.
In thetool, we are using an internally developed package forlearning large margin classifiers to implement the SVMclassifier (Haffner et al 2003).3.4 SLU ToolsetThe SLU toolset is based on an internally developedNLU Toolset.
The underlying boosting algorithm fortext classification used, BoosTexter, is described else-where (Freund and Schapire, 1999; Schapire and Singer,2000; Rochery et al 2002).
We added interactive inputand display capabilities via a Web interface allowing theUE expert to easily build and test SLU models.Named entity grammars are constructed as describedabove.
About 20% of the labeled data is set aside fortesting.
The remaining data is used to build the initialSLU model which is used to test the utterances set asidefor testing.
The UE expert can interactively test utter-ances typed into a Web page or can evaluate the testresults of the test data.
For each of the tested utterancesin the test data, test logs show the classification confi-dence scores for each call type.
The confidence scoresare replaced by probability thresholds that have beencomputed using a logistic function.
These scores arethen used to calculate a simple metric which is a meas-ure of call type differentiability.
If the test utterancelabeled by the UE expert is correctly classified, then thecall type is the truth call type.
The SLU metric is calcu-lated as follows and it is averaged over the utterances:?
if the call type is the truth, the score is the dif-ference (positive) between the truth probabilityand the next highest probability?
if the call type is not the truth, the score is thedifference (negative) between the truth prob-ability and the highest probabilityThis metric allows the UE expert to easily spot prob-lem call types or those that might give potential prob-lems in the field.
It is critical that call types are easilydifferentiable in order to properly route the call.
TheUE expert can iteratively build and test the initial SLUmodels until the UE expert has a set of self-consistentcall types before creating the final annotation guide.The final annotation guide would then be used by thelabelers to label all the utterance data needed to buildthe final SLU model.
Thus, the SLU Toolset is criticalfor creating the call types defined in the annotationguide which in turn is needed to label the data for creat-ing the final SLU.Alternatively, the labeled utterances can easily beexported in a format compatible with the internally de-veloped NLU Toolset if further SLU model tuning is tobe performed by the NLU expert using just the com-mand line interface.3.5 ReportingOne of the reporting components is the AnnotationGuide Generation Tool.
The UE expert can use this atany time to automatically generate the annotation guidefrom the Processed Data.
Other reporting componentsinclude summary statistics and spreadsheets containingutterance and call type information.4 ResultsThe performance of the preprocessing techniques hasbeen evaluated on several datasets from various industrysectors.
Approximately 10,000 utterances were col-lected for each application and the results of the datareduction at each processing stage are shown in Table 1.The Redundancy R is given byNUR ?= 1where U is the number of unique utterances after featureextraction and N is the number of original utterances.IndustrySectorOriginalUtterancesUniqueUtterancesUniqueUtterances afterTextNormalizationUniqueUtterancesafter EntityExtractionUniqueUtterancesafter FeatureExtractionRedundancyFinancial 11,623 10,021 9,670 9,165 7,929 31.8%Healthcare 12,080 10,255 9,452 9,382 7,946 34.2%Insurance 12,109 8,865 8,103 7,963 6,530 46.1%Retail 10,240 4,956 4,392 4,318 3,566 65.2%Initial UE experts of the tools have been successfulin producing annotation guides more quickly and withvery good initial F-measures.recallprecisionrecallprecisionF+?
?=2They have also reported that the task is much lesstedious and that they have done a better job of coveringall of the significant utterance clusters.
Further studiesare required to generate quantitative measures of theperformance of the toolset.5 Future WorkIn the future, the system could be improved using otherrepresentative utterance selection algorithms (e.g., se-lecting the utterance with the minimum string edit dis-tance to all others).The grammars for entity extraction were not tunedfor these applications and it is expected that further datareduction will be obtained with improved grammars.6 ConclusionsWe presented an interactive speech data analysis systemfor creating and testing spoken language understandingsystems.
Spoken language understanding is a criticalcomponent of automated customer service applications.Creating effective SLU models is inherently a datadriven process and requires considerable human inter-vention.
The fact that this process relies heavily on hu-man expertise prevents a total automation of theprocess.
Our experience indicates that augmenting thehuman expertise with interactive data analysis tech-niques made possible by machine learning techniquescan go a long way towards increasing the efficiency ofthe process and the quality of the final results.
Theautomatic preprocessing of the utterance data prior to itsuse by the UE expert results in a considerable reductionin the number of utterances that needs to be manuallyexamined.
Clustering uncovers certain structures in thedata that can then be refined by the UE expert.
Super-vised machine learning capabilities provided by interac-tive relevance feedback tend to capture the knowledgeof the UE expert to create the guidelines for labeling thedata.
The ability to test the generated call types duringthe design process helps detect and remove problematiccall types prior to their inclusion in the SLU model.This tool has been used to create the labeling guide forseveral applications by different UE experts.
Asidefrom the increased efficiency and improved quality ofthe generated SLU systems, the tool has resulted in in-creased uniformity in the way different UE experts clas-sify calls into call type labels.AcknowledgementsWe would like to thank Harris Drucker, Patrick Haffner,Steve Lewis, Maria Alvarez-Ryan, Barbara Hollister,Harry Blanchard, Liz Alba, Elliot Familant, Greg Pulz,David Neeves, Uyi Stewart, and Lan Zhang for theircontributions to this work.ReferencesHarris Drucker, Behzad Shahraray, and David C. Gib-bon, 2002.
Support Vector Machines: RelevanceFeedback and Information Retrieval, InformationProcessing and Management, 38(3):305-323.Yaov Freund and Robert Schapire, 1999.
A Short Intro-duction to Boosting, Journal of Japanese Society forArtificial Intelligence, 14(5):771-780.Patrick Haffner, Gokhan Tur, and Jerry Wright, 2003.Optimizing SVMs for complex Call Classification,ICASSP 2003.A.
L. Gorin, G. Riccardi, and J. H. Wright.
1997.
HowMay I Help You?
Speech Communication, 23:113-127.A.
K. Jan and R. C. Dubes, 1988.
Algorithms for Clus-tering Data, Prentice Hall.M.
F. Porter, 1980.
An Algorithm For Suffix Stripping,Program, 14(3):130-137.M.
Rochery, R. Schapire, M. Rahim, N. Gupta, G. Ric-cardi, S. Bangalore, H. Alshawi and S. Douglas,2002.
Combining prior knowledge and boosting forcall classification in spoken language dialogue,ICASSP 2002.SAS Institute Press Release, 2002.
New SAS?
Text Min-ing Software Surfaces Intelligence beyond the Num-bers, 1/21/02.Robert Schapire and Yoram Singer, 2000.
BoosTex-ter: A Boosting-based System for Text Categorization,Machine Learning, 39(2/3):135-168.Gokhan Tur, Robert E. Schapire, and Dilek Hakkani-T?r, 2003.
Active Learning for Spoken LanguageUnderstanding, Proceedings of International Confer-ence on Acoustics, Speech and Signal Processing,ICASSP 2003.V.
N. Vapnik, 1998.
Statistical Learning Theory, JohnWiley & Sons, Inc.
