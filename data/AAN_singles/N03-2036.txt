A Phrase-Based Unigram Model for Statistical Machine TranslationChristoph Tillmann and Fei XiaIBM T.J. Watson Research CenterYorktown Heights, NY 10598{ctill,feixia}@us.ibm.comAbstractIn this paper, we describe a phrase-based un-igram model for statistical machine transla-tion that uses a much simpler set of modelparameters than similar phrase-based models.The units of translation are blocks - pairs ofphrases.
During decoding, we use a block un-igram model and a word-based trigram lan-guage model.
During training, the blocks arelearned from source interval projections usingan underlying word alignment.
We show exper-imental results on block selection criteria basedon unigram counts and phrase length.1 Phrase-based Unigram ModelVarious papers use phrase-based translation systems (Ochet al, 1999; Marcu and Wong, 2002; Yamada and Knight,2002) that have shown to improve translation qualityover single-word based translation systems introduced in(Brown et al, 1993).
In this paper, we present a simi-lar system with a much simpler set of model parameters.Specifically, we compute the probability of a block se-quence bn1 .
The block sequence probability Pr(bn1 ) is de-composed into conditional probabilities using the chainrule:Pr(bn1 ) ?n?i=1Pr(bi|bi?1) (1)=n?i=1p?
(bi|bi?1) ?
p(1??)(bi|bi?1)?n?i=1p?
(bi) ?
p(1??
)(bi|bi?1)We try to find the block sequence that maximizes Pr(bn1 ):bn1 = argmaxbn1 Pr(bn1 ).
The model proposed is a joint11TS4S S S2 3 4TTT23Figure 1: A block sequence that jointly generates 4 targetand source phrases.model as in (Marcu and Wong, 2002), since target andsource phrases are generated jointly.
The approach is il-lustrated in Figure 1.
The source phrases are given on thex-axis and the target phrases are given on the y-axis.The two types of parameters in Eq 1 are defined as:?
Block unigram model p(bi): we compute unigramprobabilities for the blocks.
The blocks are simplerthan the alignment templates in (Och et al, 1999) inthat they do not have any internal structure.?
Trigram language model: the probabilityp(bi|bi?1) between adjacent blocks is computed asthe probability of the first target word in the targetclump of bi given the final two words of the targetclump of bi?1.The exponent ?
is set in informal experiments to be 0.5.No other parameters such as distortion probabilities areused.To select blocks b from training data, we compute uni-gram block co-occurrence counts N(b).
N(b) cannot beSourceTargetSourceTargetFigure 2: The left picture shows three blocks that arelearned from projecting three source intervals.
The rightpicture shows three blocks that cannot be obtain fromsource interval projections .computed for all blocks in the training data: we wouldobtain hundreds of millions of blocks.
The blocks arerestricted by an underlying word alignment.
The wordalignment is obtained from an HMM Viterbi training (Vo-gel et al, 1996).
The HMM Viterbi training is carriedout twice with English as target language and Chinese assource language and vice versa.
We take the intersectionof the two alignments as described in (Och et al, 1999).To generate blocks from the intersection, we proceed asfollows: for each source interval [j, j ?
], we compute theminimum target index i and maximum target index i?
ofthe intersection alignment points that fall into the interval[j, j?].
The approach is illustrated in Figure 2.
In the leftpicture, for example, the source interval [1, 3] is projectedinto the target interval [1, 3] .
The pair ([j, j ?
], [i, i?
])together with the words at the corresponding positionsyields a block learned from this training sentence pair.For source intervals without alignment points in them, noblocks are produced.
We also extend a block correspond-ing to the interval pair ([j, j ?
], [i, i?])
by elements on theunion of the two Viterbi HMM alignments.
A similarblock selection scheme has been presented in (Och et al,1999).
Finally, the target and source phrases are restrictedto be equal or less than 8 words long.
This way we obtain23 millions blocks on our training data including blocksthat occur only once.
This baseline set is further filteredusing the unigram count N(b): Nk denotes the set ofblocks b for which N(b) ?
k. Blocks where the targetand the source clump are of length 1 are kept regardlessof their count.1 We compute the unigram probability p(b)as relative frequency over all selected blocks.We also tried a more restrictive projection scheme: sourceintervals are projected into target intervals and the reverseprojection of the target interval has to be included in theoriginal source interval.
The results for this symmet-rical projection are currently worse, since some blockswith longer target intervals are excluded.
An exampleof 4 blocks obtained from the training data is shown in1To apply the restrictions exhaustively, we have imple-mented tree-based data structures to store the 23 million blockswith phrases of up to length 8 in about 1.6 gigabyte of RAM.Figure 3: An example of 4 recursively nested blocksb1, b2, b3, b4.Figure 3.
?$DATE?
is a placeholder for a date expres-sion.
Block b4 contains the blocks b1 to b3.
All 4 blocksare selected in training: the unigram decoder prefersb4 even if b1,b2, and b3 are much more frequent.
Thesolid alignment points are elements from the intersec-tion, the striped alignment points are elements from theunion.
Using the union points, we can learn one-to-manyblock translations; for example, the pair (c1,?Xinhua newsagency?)
is learned from the training data.We use a DP-based beam search procedure similar to theone presented in (Tillmann, 2001).
We maximize overall block segmentations bn1 for which the source phrasesyield a segmentation of the input source sentence, gen-erating the target sentence simultaneously.
In the currentexperiments, decoding without block re-ordering yieldsthe best translation results.
The decoder translates about180 words per second.2 Experimental ResultsThe translation system is tested on a Chinese-to-Englishtranslation task.
The training data come from severalnews sources.
For testing, we use the DARPA/NIST MT2001 dry-run testing data, which consists of 793 sen-tences with 20, 333 words arranged in 80 documents.2The training data is provided by the LDC and labeled byNIST as the Large Data condition for the MT 2002 eval-uation.
The Chinese sentences are segmented into words.The training data contains 23.7 million Chinese and 25.3million English words.Experimental results are presented in Table 1 and Ta-ble 2.
Table 1 shows the effect of the unigram threshold.The second column shows the number of blocks selected.The third column reports the BLEU score (Papineni et al,2002) along with 95% confidence interval.
We use IBM2We did not use the first 25 documents of the 105-documentdry-run test set because they were used as a development test setbefore the dry-run and were subsequently added to our trainingdata.Table 1: Effect of the unigram threshold on the BLEUscore.
The maximum phrase length is 8.Selection # blocks BLEUr4n4Restriction selectedIBM1 baseline 1.23M 0.11 ?
0.01N2 4.23 M 0.18 ?
0.02N3 1.22 M 0.18 ?
0.01N4 0.84 M 0.17 ?
0.01N5 0.65 M 0.17 ?
0.01Table 2: Effect of the maximum phrase length on theBLEU score.
The unigram threshold is N(b) ?
2.maximum # blocks BLEUr4n4phrase length selected8 4.23 M 0.18 ?
0.027 3.76 M 0.17 ?
0.026 3.26 M 0.17 ?
0.015 2.73 M 0.17 ?
0.014 2.16 M 0.17 ?
0.013 1.51 M 0.16 ?
0.012 0.77 M 0.14 ?
0.011 0.16 M 0.12 ?
0.01Model 1 as a baseline model which is similar to our blockmodel: neither model uses distortion or alignment proba-bilities.
The best results are obtained for the N2 and theN3 sets.The N3 set uses only 1.22 million blocks in contrast toN2 which has 4.23 million blocks.
This indicates that thenumber of blocks can be reduced drastically without af-fecting the translation performance significantly.
Table 2shows the effect of the maximum phrase length on theBLEU score for the N2 block set.
Including blocks withlonger phrases actually helps to improve performance, al-though length 4 already obtains good results.We also ran the N2 on the June 2002 DARPA TIDESLarge Data evaluation test set.
Six research sites andfour commercial off-the-shelf systems were evaluated inLarge Data track.
A majority of the systems were phrase-based translation systems.
For comparison with othersites, we quote the NIST score (Doddington, 2002) onthis test set: N2 system scores 7.44 whereas the officialtop two systems scored 7.65 and 7.34 respectively.3 ConclusionIn this paper, we described a phrase-based unigram modelfor statistical machine translation.
The model is muchsimpler than other phrase-based statistical models.
Weexperimented with different restrictions on the phrasesselected from the training data.
Longer phrases whichoccur less frequently do not help much.AcknowledgmentThis work was partially supported by DARPA and mon-itored by SPAWAR under contract No.
N66001-99-2-8916.ReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, 19(2):263?311.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proc.
of the Second International Confer-ence of Human Language Technology Research, pages138?145, March.Daniel Marcu and William Wong.
2002.
A Phrased-Based, Joint Probability Model for Statistical MachineTranslation.
In Proc.
of the Conf.
on Empirical Meth-ods in Natural Language Processing (EMNLP 02),pages 133?139, Philadelphia, PA, July.Franz-Josef Och, Christoph Tillmann, and Hermann Ney.1999.
Improved Alignment Models for Statistical Ma-chine Translation.
In Proc.
of the Joint Conf.
on Em-pirical Methods in Natural Language Processing andVery Large Corpora (EMNLP/VLC 99), pages 20?28,College Park, MD, June.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of machine translation.
In Proc.
of the40th Annual Conf.
of the Association for Computa-tional Linguistics (ACL 02), pages 311?318, Philadel-phia, PA, July.Christoph Tillmann.
2001.
Word Re-Ordering and Dy-namic Programming based Search Algorithm for Sta-tistical Machine Translation.
Ph.D. thesis, Universityof Technology, Aachen, Germany.Stefan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM BasedWord Alignment in Statistical Ma-chine Translation.
In Proc.
of the 16th Int.
Conf.on Computational Linguistics (COLING 1996), pages836?841, Copenhagen, Denmark, August.Kenji Yamada and Kevin Knight.
2002.
A Decoder forSyntax-based Statistical MT.
In Proc.
of the 40th An-nual Conf.
of the Association for Computational Lin-guistics (ACL 02), pages 303?310, Philadelphia, PA,July.
