Finite State TransducersApproximating Hidden Markov ModelsAndrd  KempeRank  Xerox  Research  Cent re  - Grenob le  Laboratory6, chemin  de Mauper tu i s  - 38240 Mey lan  - F ranceandre, kempe?grenoble, rxrc.
xerox, comht tp  ://www.
rxrc.
xerox, com/research /ml t tAbst rac tThis paper describes the conversion of aHidden Markov Model into a sequentialtransducer that closely approximates thebehavior of the stochastic model.
Thistransformation is especially advantageousfor part-of-speech tagging because the re-sulting transducer can be composed withother transducers that encode correctionrules for the most frequent agging errors.The speed of tagging is also improved.
Thedescribed methods have been implementedand successfully tested on six languages.1 In t roduct ionFinite-state automata have been successfully appliedin many areas of computational linguistics.This paper describes two algorithms 1 which ap-proximate a Hidden Markov Model (HMM) used forpart-of-speech tagging by a finite-state transducer(FST).
These algorithms may be useful beyond thecurrent description on any kind of analysis of writtenor spoken language based on both finite-state tech-nology and HMMs, such as corpus analysis, speechrecognition, etc.
Both algorithms have been fullyimplemented.An HMM used for tagging encodes, like a trans-ducer, a relation between two languages.
One lan-guage contains sequences of ambiguity classes ob-tained by looking up in a lexicon all words of a sen-tence.
The other language contains equences of tagsobtained by statistically disambiguating the class se-quences.
From the outside, an HMM tagger behaveslike a sequential transducer that deterministically1There is a different (unpublished) algorithm byJulian M. Kupiec and John T. Maxwell (p.c.
).maps every class sequence to a tag sequence, e.g.
:\[DET, PRO\] \[ADJ,NOUN\] \[ADJ,NOUN\] ...... \[END\] (i)DET ADJ NOUN ...... ENDThe aim of the conversion is not to generate FSTsthat behave in the same way, or in as similar a wayas possible like IIMMs, but rather FSTs that per-form tagging in as accurate a way as possible.
Themotivation to derive these FSTs from HMMs is thatHMMs can be trained and converted with little man-ual effort.The tagging speed when using transducers i  upto five times higher than when using the underly-ing HMMs.
The main advantage of transforming anHMM is that the resulting transducer can be han-dled by finite state calculus.
Among others, it canbe composed with transducers that encode:?
correction rules for the most frequent taggingerrors which are automatically generated (Brill,1992; Roche and Schabes, 1995) or manuallywritten (Chanod and Tapanainen, 1995), in or-der to significantly improve tagging accuracy 2.These rules may include long-distance depen-dencies not handled by HMM taggers, and canconveniently be expressed by the replace oper-ator (Kaplan and Kay, 1994; Karttunen, 1995;Kempe and Karttunen, 1996).?
further steps of text analysis, e.g.
light parsingor extraction of noun phrases or other phrases(Ait-Mokhtar and Chanod, 1997).These compositions enable complex text analysisto be performed by a single transducer.An IIMM transducer builds on the data (probabil-ity matrices) of the underlying HMM.
The accuracy2Automatically derived rules require less work thanmanually written ones but are unlikely to yield betterresults because they would consider relatively limitedcontext and simple relations only.460of this data has an impact on the tagging accuracyof both the HMM itself and the derived transducer.The training of the HMM can be done on either atagged or untagged corpus, and is not a topic of thispaper since it is exhaustively described in the liter-ature (Bahl and Mercer, 1976; Church, 1988).An HMM can be identically represented by aweighted FST in a straightforward way.
We are,however, interested in non-weighted transducers.2 n -Type  Approx imat ionThis section presents a method that approximatesa (lst order) HMM by a transducer, called n-typeapproximation 3.Like in an HMM, we take into account initial prob-abilities ~r, transition probabilities a and class (i.e.observation symbol) probabilities b.
We do, how-ever, not estimate probabilities over paths.
The tagof the first word is selected based on its initial andclass probability.
The next tag is selected on its tran-sition probability given the first tag, and its classprobability, etc.
Unlike in an HMM, once a decisionon a tag has been made, it influences the followingdecisions but is itself irreversible.A transducer encoding this behaviour can be gen-erated as sketched in figure 1.
In this example wehave a set of three classes, Cl with the two tags tnand t12, c2 with the three tags t21, t22 and t23 , andc3 with one tag t31.
Different classes may containthe same tag, e.g.
t12 and t2s may refer to the sametag.For every possible pair of a class and a tag (e.g.Cl :t12 or I'ADJ,NOUN\] :NOUN) a state is created andlabelled with this same pair (fig.
1).
An initial statewhich does not correspond with any pair, is also cre-ated.
All states are final, marked by double circles.For every state, as many outgoing arcs are createdas there are classes (three in fig.
1).
Each such arcfor a particular class points to the most probablepair of this same class.
If the arc comes from theinitial state, the most probable pair of a class and atag (destination state) is estimated by:argrnkaxpl(ci,tih ) ---- 7r(tik) b(ciltik) (2)If the arc comes from a state other than the initialstate, the most probable pair is estimated by:argmaxp2(ci,tik) = a(tlkltp,eoio~,) b(ciltik) (3)In the example (fig.
1) cl :t12 is the most likely pairof class cl, and c2:t23 the most likely pair of class e2aName given by the author.when coming from the initial state, and c2 :t21 themost likely pair of class c2 when coming from thestate of c3 :t31.Every arc is labelled with the same symbol pairas its destination state, with the class symbol in theupper language and the tag symbol in the lower lan-guage.
E.g.
every arc leading to the state of cl :t12is labelled with Cl :t12.Finally, all state labels can be deleted since thebehaviour described above is encoded in the arc la-bels and the network structure.
The network can beminimized and determinized.We call the model an nl-type model, the resultingFST an nl-type transducer and the algorithm lead-ing from the HMM to this transducer, an nl-typeapproximation of a 1st order HMM.Adapted to a 2nd order HMM, this algorithmwould give an n2-type approximation.
Adapted toa zero order HMM, which means only to use classprobabilities b, the algorithm would give an nO-typeapproximation.n-Type transducers have deterministic states only.3 s -Type  Approx imat ionThis section presents a method that approxi-mates an HMM by a transducer, called s-typeapproximation 4.Tagging a sentence based on a 1st order HMMincludes finding the most probable tag sequence Tgiven the class sequence C of the sentence.
The jointprobability of C and T can be estimated by:p(C ,  T )  = p(c l  .... Cn, t l  .... tn )  =Its) 12 I a(t, lt _l) ItOi=2(4)The decision on a tag of a particular word cannotbe made separately from the other tags.
Tags caninfluence ach other over a long distance via transi-tion probabilities.
Often, however, it is unnecessaryto decide on the tags of the whole sentence at once.In the case ofa 1st order HMM, unambiguous classes(containing one tag only), plus the sentence begin-ning and end positions, constitute barriers to thepropagation of HMM probabilities.
Two tags withone or more barriers inbetween do not influence achother's probability.4Name given by the author.461classesr-}tags of classes22 ~Figure 1: Generation of an nl-type transducer3.1 s -Type  Sentence  Mode lTo tag a sentence, one can split its class sequence atthe barriers into subsequences, then tag them sep-arately and concatenate them again.
The result isequivalent o the one obtained by tagging the sen-tence as a whole.We distinguish between initial and middle sub-sequences.
The final subsequence of a sentence isequivalent o a middle one, if we assume that thesentence nd symbol (.
or !
or ?)
always correspondsto an unambiguous class c~.
This allows us to ig-nore the meaning of the sentence nd position as anHMM barrier because this role is taken by the un-ambiguous class cu at the sentence nd.An initial subsequence Ci starts with the sentenceinitial position, has any number (incl.
zero) of am-biguous classes ca and ends with the first unambigu-ous class c~ of the sentence.
It can be described bythe regular expressionS:Ci = ca* (5)The joint probability of an initial class subse-quence Ci of length r, together with an initial tagsubsequence ~,  can be estimated by:rp(C,, ~1~) = r(tl) b(cl\]tl).
H a(tj\]tj_l) b(cj Itj) (6)j=2A middle subsequence Cm starts immediately af-ter an unambiguous class cu, has any number (incl.SRegular expression operators used in this section areexplained in the annex?zero) of ambiguous classes ca and ends with the fol-lowing unambiguous class c~ :Cm = ca* c~ (7)For correct probability estimation we have to in-clude the immediately preceding unambiguous classcu, actually belonging to the preceding subsequenceCi or Cm.
We thereby obtain an extended middlesubsequence 5:= % ca* (8)The joint probability of an extended middle classsubsequence C~ of length s, together with a tag sub-sequence Tr~ , can be estimated by:$p(c?,7?)
= b(clltl).
I-\[ a(tjltj_ ) b(cjlt ) (9)j=23.2 Construct ion of an s -Type  TransducerTo build an s-type transducer, a large number of ini-tial class subsequences Ci and extended middle classsubsequences C~n are generated in one of the follow-ing two ways:(a) Extraction from a corpusBased on a lexicon and a guesser, we annotate anuntagged training corpus with class labels.
From ev-ery sentence, we extract he initial class subsequenceCi that ends with the first unambiguous class c~ (eq.5), and all extended middle subsequences C~n rang-ing from any unambiguous class cu (in the sentence)to the following unambiguous class (eq.
8).462A frequency constraint (threshold) may be im-posed on the subsequence s lection, so that the onlysubsequences retained are those that occur at leasta certain number of times in the training corpus 6.
(b) Generat ion  of  possible subsequencesBased on the set of classes, we generate all possi-ble initial and extended middle class subsequences,Ci and C,e, (eq.
5, 8) up to a defined length.Every class subsequence Ci or C~ is first dis-ambiguated based on a 1st order HMM, using theViterbi algorithm (Viterbi, 1967; Rabiner, 1990) forefficiency, and then linked to its most probable tagsubsequence ~ or T~ by means of the cross productoperationS:Si -- Ci .x .
T /  ---- c 1 : t l  c2 : t2  .
.
.
.
.
.
Cn :tn (10)01) e.  e S~ = C~ .x.
7~ = el.t1 c2:t2 ...... c, :t ,In all extended middle subsequences S~n, e.g.
:S~ - C~ _ (12)\[DET\] \[ADJ,NOUN\] \[ADJ, NOUN\] \[NOUN\]DET ADJ ADJ NOUNthe first class symbol on the upper side and the firsttag symbol on the lower side, will be marked as anextension that does not really belong to the middlesequence but which is necessary to disambiguate itcorrectly.
Example (12) becomes:s ?
= = (13) TOO.\[DET\] \[ADJ,NOUN\] \[ADJ, NOUN\] \[NOUN\]O.DET ADJ ADJ NOUNWe then build the union uS i of all initial subse-quences Si and the union uS~n of all extended middlesubsequences S,e=, and formulate a preliminary sen-tence model:uS ?
= ~S, uS?~* (14)in which all middle subsequences S ?
are still markedand extended in the sense that all occurrences of allunambiguous classes are mentioned twice: Once un-marked as cu at the end of every sequence Ci or COn,0 at the beginning and the second time marked as c uof every following sequence C?
.
The upper side ofthe sentence model uS?
describes the complete (but6The frequency constraint may prevent he encodingof rare subsequences which would encrease the size ofthe transducer without contributing much to the taggingaccuracy.extended) class sequences of possible sentences, andthe lower side of uS?
describes the corresponding (ex-tended) tag sequences.To ensure a correct concatenation of initial andmiddle subsequences, we formulate a concatenationconstraint for the classes:0 = N \[-*\[ % (15)Jstating that every middle subsequence must begin0 with the same marked unambiguous class % (e.g.0.\[DET\]) which occurs unmarked as c~ (e.g.
\[DET\])at the end of the preceding subsequence since bothsymbols refer to the same occurrence of this unam-biguous class.Having ensured correct concatenation, we deleteall marked classes on the upper side of the relationby means ofand all marked tags on the lower side by means ofBy composing the above relations with the prelim-inary sentence model, we obtain the final sentencemodelS:S = Dc .o.
Rc .o.
uS?
.o.
Dt (18)We call the model an s-type model, the corre-sponding FST an s-type transducer, and the wholealgorithm leading from the HMMto the transducer,an s-type approximation of an HMM.The s-type transducer tags any corpus which con-tains only known subsequences, in exactly the sameway, i.e.
with the same errors, as the correspondingHMM tagger does.
However, since an s-type trans-ducer is incomplete, it cannot tag sentences withone or more class subsequences not contained in theunion of the initial or middle subsequences.3.3 Complet ion  of  an s -Type TransducerAn incomplete s-type transducer S can be completedwith subsequences from an auxiliary, complete n-type transducer N as follows:First, we extract he union of initial and the unionof extended middle subsequences, u u e Si and s Sm fromthe primary s-type transducer S, and the unions ~Si463and ~S,~ from the auxiliary n-type transducer N. Toextract the union ?S i of initial subsequences we usethe following filter:Fs ,=\ [ \<c~, t>\ ]*  <c-,0 \ [?
: \ [ \ ] \ ] *  (19)where (c,, t) is the l-level format 7of the symbol paircu :t. The extraction takes place byusi = \[ N.1L .o.
Fs, \].l.2L (20)where the transducer N is first converted into l-level format 7, then composed with the filter Fs, (eq.19).
We extract the lower side of this composition,where every sequence of N.1L remains unchangedfrom the beginning up to the first occurrence of anunambiguous class c,.
Every following symbol ismapped to the empty string by means of \[?
:\[ \]\].(eq.
19).
Finally, the extracted lower side is againconverted into 2-level format 7.The extraction of the union uSe of extended mid-die subsequences is performed in a similar way.We then make the joint unions of initial and ex-tended middle subsequences 5 :U~/ U O O U : I \[ \] \] (21) - -  ~Si .o .
~SiU e U e U e U e U e = \[, Sm.u s .
,  ,s in I \ [  (22) - \] .o.
\]In both cases (eq.
21 and 22) we union all subse-quences from the principal model S, with all thosesubsequences from the auxiliary model N that arenot in S.Finally, we generate the completed s+n-typctransducer from the joint unions of subsequences uSiand uS~n , as decribed above (eq.
14-18).A transducer completed in this way, disam-biguates all subsequences known to the principalincomplete s-type model, exactly as the underlyingHMM does, and all other subsequences as the aux-iliary n-type model does.4 An  Imp lemented  F in i te -S ta teTaggerThe implemented tagger requires three transducerswhich represent a lexicon, a guesser and any abovementioned approximation of an HMM.All three transducers are sequential, i.e.
deter-ministic on the input side.Both the lexicon and guesser unambiguously mapa surface form of any word that they accept to thecorresponding class of tags (fig.
2, col. 1 and 2):~l-Level and 2-level format are explained in the an-f l ex .First, the word is looked for in the lexicon.
If thisfails, it is looked for in the guesser.
If this equallyfails, it gets the label \[UNKNOWN\] which associatesthe word with the tag class of unknown words.
Tagprobabilities in this class are approximated by tagsof words that appear only once in the training cor-pus.As soon as an input token gets labelled with thetag class of sentence nd symbols (fig.
2: \[SENT\]),the tagger stops reading words from the input.
Atthis point, the tagger has read and stored the wordsof a whole sentence (fig.
2, col. 1) and generated thecorresponding sequence of classes (fig.
2, col. 2).The class sequence is now deterministicallymapped to a tag sequence (fig.
2, col. 3) by means ofthe HMM transducer.
The tagger outputs the storedword and tag sequence of the sentence, and contin-ues in the same way with the remaining sentences ofthe corpus.The \[AT\] ATshare \[NN, VB\] NNof \[IN\] INtripled \[VBD, VBN\] VBDwithin \[IN,RB\] INthat \[CS, DT, WPS\] DTspan INN, VB, VBD\] VBDof \[IN\] INt ime INN, VB\] NN\[SENT\] SENTFigure 2: Tagging a sentence5 Exper iments  and  Resu l tsThis section compares different n-type and s-typetransducers with each other and with the underlyingHMM.The FSTs perform tagging faster than the HMMs.Since all transducers are approximations ofHMMs, they give a lower tagging accuracy than thecorresponding HMMs.
However, improvement in ac-curacy can be expected since these transducers canbe composed with transducers encoding correctionrules for frequent errors (sec.
1).Table 1 compares different ransducers on an En-glish test case.The s+nl-type transducer containing all possiblesubsequences up to a length of three classes is themost accurate (table 1, last line, s+nl -FST (~ 3):95.95 %) but Mso the largest one.
A similar rate ofaccuracy at a much lower size can be achieved withthe s+nl-type, either with all subsequences up to a464HMMaccuracyin %96.77tagging speedin words/sec4 590transducer size creationtime # states # arcs1 29771 21 087927 203 8532 675 564 8874 709 976 785476 107 728211 52 624154 41 5982 049 418 536799 167 952432 96 7129 796 1 311 96292 463 13 681 113n0-FST 83.53 20 582 16 secnl -FST 94.19 17 244 17 secs+nl -FST (20K, F1) 94.74 13 575 3 mins+nl -FST (50K, F1) 94.92 12 760 10 mins+nl -FST (100K, F1) 95.05 12 038 23 mins+nl -FST (100K, F2) 94.76 14 178 2 mins+nl -FST (100K, F4) 94.60 14 178 76 secs+nl -FST (100K, F8) 94.49 13 870 62 sees+nl -FST (1M, F2) 95.67 11 393 7 mins+nl -FST (1M, F4) 95.36 11 193 4 mins+nl -FST (1M, FS) 95.09 13 575 3 mins+nl -FST (< 2) 95.06 8 180 39 mins+nl -FST (< 3) 95.95 4 870 47 hLanguage: EnglishCorpora: 19 944 words for HMM training, 19 934 words for testTag set: 74 tags 297 classesTypes of FST (Finite-State Transducers) :nO, nl n0-type (with only lexical probabilities) or nl-type (sec.
2)s+nl (100K, F2) s-type (sec.
3), with subsequences of frequency > 2, from a trainingcorpus of 100 000 words (sec.
3.2 a), completed with nl-type (sec.
3.3)s+nl (< 2) s-type (sec.
3), with all possible subsequences of length _< 2 classes(sec.
3.2 b), completed with nl-type (sec.
3.3)Computer: ultra2, 1 CPU, 512 MBytes physical RAM, 1.4 GBytes virtual RAMTable 1: Accuracy, speed, size and creation time of some HMM transducerslength of two classes (s+nl-FST (5 2): 95.06 %) orwith subsequences occurring at least once in a train-ing corpus of 100 000 words (s+nl-FST (lOOK, F1):95.05 %).Increasing the size of the training corpus and thefrequency limit, i.e.
the number of times that a sub-sequence must at least occur in the training corpusin order to be selected (sec.
3.2 a), improves the re-lation between tagging accuracy and the size of thetransducer.
E.g.
the s+nl-type transducer that en-codes subsequences from a training corpus of 20 000words (table 1, s+nl -FST (20K, F1): 94.74 %, 927states, 203 853 arcs), performs less accurate taggingand is bigger than the transducer that encodes ub-sequences occurring at least eight times in a corpusof 1 000 000 words (table 1, s+nl -FST (1M, F8):95.09 %, 432 states, 96 712 arcs).Most transducers in table 1 are faster then theunderlying HMM; the n0-type transducer about fivetimes .
There is a large variation in speed betweenSSince n0-type and nl-type transducers have deter-ministic states only, a particular fast matching algorithmcan be used for them.the different ransducers due to their structure andsize.Table 2 compares the tagging accuracy of differenttransducers and the underlying HMM for differentlanguages.
In these tests the highest accuracy wasalways obtained by s-type transducers, either withall subsequences up to a length of two classes 9 orwith subsequences occurring at least once in a corpusof 100 000 words.6 Conc lus ion  and  Future  ResearchThe two methods described in this paper allow theapproximation of an HMM used for part-of-speechtagging, by a finite-state transducer.
Both methodshave been fully implemented.The tagging speed of the transducers i up to fivetimes higher than that of the underlying HMM.The main advantage of transforming an HMMis that the resulting FST can be handled by finite9A maximal length of three classes is not consideredhere because of the high increase in size and a low in-crease in accuracy.465.... HMM-'n0-FSTnl-FSTEnglish96.7783.5394.19s+nl-FST (20K, F1) 94.74s+nl-FST (50K, F1) 94.92s+nl-FST (100K, F1) 95.05s+nl-FST (100K, F2) 94.76s?nl-FST (100K, F4)s+nl-FST (100K, F8)94.6094.49:HMM train.crp.
(#wd)'"test corpus (# words)s+nl-FST (< 2) 95.0619 94419 934#tags  74#classes 297accuracy in %I Dutch I French I GermanI 94"76\[ 98"651 97.6281.99 91.1391.58 98.1892.17 98.3592.24 98.3792.36 98.3792.17 98.3492.02 98.3091.84 98.3292.25 98.3726 386 22 62210 468 6 36847 45230 287\[ Types of FST (Finite-State Transducers) :Portug.
Spanish\[ 97.12 97.6082.97 91.03 93.6594.49 96.19 96.4695.23 96.7195.5795.8195.5195.2996.3396.4996.5696.4296.2796.7696.8796.7496.6495.02 96.23 96.5495.92 96.50 96.9091 060 20 956 16 22139 560 15 536 15 44366 67 55389 303 254cf.
table 1 ITable 2: Accuracy of some HMM transducers for different languagesstate calculus 1?
and thus be directly composed withother transducers which encode tag correction rulesand/or perform further steps of text analysis.Future  research will mainly focus on this pos-sibility and will include composition with, amongothers:?
Transducers that encode correction rules (pos-sibly including long-distance dependencies) forthe most frequent agging errors, ill order tosignificantly improve tagging accuracy.
Theserules can be either extracted automatically froma corpus (Brill, 1992) or written manually(Chanod and Tapanainen, 1995).?
Transducers for light parsing, phrase extractionand other analysis (A'/t-Mokhtar and Chanod,1997).An HMM transducer can be composed with one ormore of these transducers in order to perform com-plex text analysis using only a single transducer.We also hope to improve the n-type model by us-ing look-ahead to the following tags 11.AcknowledgementsI wish to thank the anonymous reviewers of my pa-per for their valuable comments and suggestions.I am grateful to Lauri Karttunen and GregoryGrefenstette (both RXRC Grenoble) for extensiveand frequent discussion during the period of mywork, as well as to Julian Kupiec (Xerox PARC)and Mehryar Mohri (AT&:T Research) for sendingme some interesting ideas before I started.Many thanks to all my colleagues at RXRCGrenoble who helped me in whatever respect, partic-ularly to Anne Schiller, Marc Dymetman and Jean-Pierre Chanod for discussing parts of the work, andto Irene Maxwell for correcting various versions ofthe paper.l?A large library of finite-state functions is availableat Xerox.11Ongoing work has shown that, looking ahead to justone tag is worthless because it makes tagging resultshighly ambiguous.466References  ANNEX: Regu lar  Express ion  OperatorsAit-Mokhtar, Salah and Chanod, Jean-Pierre(1997).
Incremental Finite-State Parsing.
Inthe Proceedings of the 5th Conference of AppliedNatural Language Processing.
ACL, pp.
72-79.Washington, DC, USA.Bahl, Lalit R. and Mercer, Robert L. (1976).
Partof Speech Assignment by a Statistical DecisionAlgorithm.
In IEEE international Symposium on $AInformation Theory.
pp.
88-89.
Ronneby.Brill, Eric (1992).
A Simple Rule-Based Part-of- -ASpeech Tagger.
In the Proceedings of the 3rd con-ference on Applied Natural Language Processing, \app.
152-155.
Trento, Italy.Chanod, Jean-Pierre and Tapanainen, Pasi (1995).
A*Tagging French - Comparing a Statistical and aConstraint Based Method.
In the Proceedings of A+the 7th conference of the EACL, pp.
149-156.ACL.
Dublin, Ireland.
a -> bChurch, Kenneth W. (1988).
A Stochastic PartsProgram and Noun Phrase Parser for Unre-stricted Text.
In Proceedings of the 2nd Con- a <- bference on Applied Natural Language Processing.ACL, pp.
136-143.a:bKaplan, Ronald M. and Kay, Martin (1994).
Reg-ular Models of Phonological Rule Systems.
In (a,b)Computational Linguistics.
20:3, pp.
331-378.Karttunen, Lauri (1995).
The Replace Operator.
R.uIn the Proceedings of the 33rd Annual Meeting R. 1of the Association for Computational Linguistics.
h BCambridge, MA, USA.
cmp-lg/9504032A I  BKempe, Andrd and Karttunen, Lauri (1996).
Par- A ~ Ballel Replacement in Finite State Calculus.
In A - Bthe Proceedings of the 16th International Confer-ence on Computational Linguistics, pp.
622-627. h .x.
BCopenhagen, Denmark.
crap-lg/9607007Rabiner, Lawrence R. (1990).
A Tutorial on Hid- R .o.
qden Markov Models and Selected Applications in it.lLSpeech Recognition.
In Readings in Speech Recog-nition (eds.
A. Waibel, K.F.
Lee).
Morgan Kauf-mann Publishers, Inc. San Mateo, CA., USA.A.2L Roche, Emmanuel and Schabes, Yves (1995).
De-terministic Part-of-Speech Tagging with Finite- Oor f \ ]  State Transducers.
In Computational Linguistics.
?Vol.
21, No.
2, pp.
227-253.Viterbi, A.J.
(1967).
Error Bounds for Convolu-tional Codes and an Asymptotical Optimal De-coding Algorithm.
In Proceedings of IEEE, vol.61, pp.
268-278.Below, a and b designate symbols, A andB designate languages, and R and q desig-nate relations between two languages.
Moredetails on the following operators and point-ers to finite-state literature can be found inhttp ://www.
rxrc.
xerox, com/research/mltt/f stContains.
Set of strings containing at leastone occurrence of a string from A as asubstring.Complement (negation).
All strings ex-cept those from A.Term complement.
Any symbol otherthan a.Kleene star.
Zero or more times h con-catenated with itself.Kleene plus.
One or more times A concate-nated with itself.Replace.
Relation where every a on theupper side gets mapped to a b on the lowerside.Inverse replace.
Relation where every b onthe lower side gets mapped to an a on theupper side.Symbol pair with a on the upper and b onthe lower side.1-Level symbol which is the 1-1eve!
form(.
1L) of the symbol pair a: b.Upper language of R.Lower language of R.Concatenation of all strings of A with allstrings of tl.Union of A and B.Intersection of A and B.Relative complement (minus).
All stringsof A that are not in B.Cross Product (Cartesian product) of thelanguages A and B.Composition of the relations R and q.1-Level form.
Makes a language out ofthe relation R. Every symbol pair becomesa simple symbol.
(e.g.
a: b becomes (a, b)and a which means a :a  becomes (a, a))2-Level form.
Inverse operation to .1L(R.1L.2L = R).Empty string (epsilon).Any symbol in the known alphabet and itsextensions467
