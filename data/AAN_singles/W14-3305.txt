Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 71?78,Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational LinguisticsCimS ?
The CIS and IMS joint submission to WMT 2014translating from English into GermanFabienne Cap_, Marion Weller_f, Anita Rammf, Alexander Fraser__ CIS, Ludwig-Maximilian University of Munich ?
(cap|fraser)@cis.uni-muenchen.def IMS, University of Stuttgart ?
(wellermn|ramm)@ims.uni-stuttgart.deAbstractWe present the CimS submissions to the2014 Shared Task for the language pairEN?DE.
We address the major problemsthat arise when translating into German:complex nominal and verbal morphol-ogy, productive compounding and flex-ible word ordering.
Our morphology-aware translation systems handle wordformation issues on different levels ofmorpho-syntactic modeling.1 IntroductionIn our shared task submissions, we focus on theEnglish to German translation direction: we ad-dress different levels of productivity of the Ger-man language, i.e., nominal and verbal inflec-tion and productive word formation, which leadto data sparsity and thus confuse classical SMTsystems.Our basic goal is to make the two languagesas morphosyntactically similar as possible.
Weuse a parser and a morphological analyser to re-move linguistic features from German that arenot present in English and reorder the Englishinput to make it more similar to the German sen-tence structure.
Prior to training, all words arelemmatised and compounds are split into singlewords.
This is not only beneficial for word align-ment, but it also allows us to generalise over in-flectional variants of the same lexemes and oversingle words which could occur in one place as astandalone word and in another place as part ofa compound.
Translation happens in two steps:first, we translate from English into split, lemma-tised German and then, we perform compoundmerging and generation of inflection as a post-processing step.
This way, we are able to cre-ate German compounds and inflectional vari-ants that have not been seen in the parallel train-ing data.In this paper, we investigate the performance ofwell-established source-side reordering, nomi-nal re-inflection and compound processing sys-tems on an up-to-date shared task.
In addition,we present experimental results on a verbal in-flection component and a syntax-based variantincluding source-side reordering.2 Related WorkRe-Inflection The two-step translation ap-proach we use was described by e.g.
Toutanovaet al.
(2008) and Jeong et al.
(2010), who usea number of morphological and syntacticfeatures derived from both source and targetlanguage.
More recently, Fraser et al.
(2012)describe a similar approach for German usingdifferent CRF-based feature prediction models,one for each of the four grammatical featuresto be predicted for German words in nounphrases, namely number, gender, case anddefiniteness.
This approach also handles word-formation issues such as portmanteau splittingand compounding.
Weller et al.
(2013) addedsubcategorization information in combinationwith source-side syntactic features in order toimprove the prediction of case.De Gispert and Mari?o (2008) generate verbalinflection for translation from English into Span-ish.
They use classifiers trained not only on tar-get language but also on source language fea-tures, which is even more crucial for the predic-tion of verbs than it is for nominal inflection.More recently, Williams and Koehn (2011)translate directly into target language surfaceforms.
Agreement within NPs and PPs, and alsobetween subject and verb is considered duringthe decoding process: they use string-to-treetranslation, where the target language (German)morphology is expressed as a set of unificationconstraints automatically learned from a mor-phologically annotated German corpus.71Compound Processing Compound splittingfor SMT has been addressed by numerous dif-ferent groups, for translation from Germanto English, e.g.
using corpus-based frequen-cies (Koehn and Knight, 2003), using POS-constraints (Stymne et al., 2008), a lattice-basedapproach propagating the splitting decision tothe decoder (Dyer, 2009), a rule-based morpho-logical analyser (Fritzinger and Fraser, 2010) orunsupervised, language-independent segmen-tation (Macherey et al., 2011).Compound processing in the other translationdirection, however, has been much less investi-gated.
Popovic?
et al.
(2006) describe a list-basedapproach, in which words are only re-combinedif they have been seen as compounds in a hugecorpus.
However this approach is limited tothe list?s coverage.
The approach of Stymne(2009) overcomes this coverage issue by mak-ing use of a POS-markup which distinguishesformer compound modifiers from former headsand thus allows for their adequate recombina-tion after translation.
An extension of this ap-proach is reported in Stymne and Cancedda(2011) where a CRF-model is used for compoundprediction.
In Cap et al.
(2014) their approachis extended through using source-language fea-tures and lemmatisation, allowing for maximalgeneralisation over compound parts.Source-side Reordering One major problem inEnglish to German translation is the divergentclausal ordering: in particular, German verbstend to occur at the very end of clauses, whereasEnglish sticks to a rigid SVO order in most cases.Collins et al.
(2005), Fraser (2009) and Gojunand Fraser (2012) showed that restructuring thesource language so that it corresponds to the ex-pected structure of the target language is helpfulfor SMT.3 Inflection PredictionGerman has a rich morphology, both for nom-inal and verbal inflection.
It requires differ-ent forms of agreement, e.g., for adjectives andnouns or verbs and their subjects.
Traditionalphrase-based SMT systems often get such agree-ments wrong.
In our systems, we explicitlymodel agreement using a two-step approach:first we translate from English into lemmatisedGerman and then generate fully inflected formsin a second step.
In this section, we describe ournominal inflection component and first experi-mental steps towards verbal re-inflection.3.1 Noun Phrase InflectionPrior to training, the German data is re-duced to a lemmatised representation contain-ing translation-relevant morphological features.For nominal inflection, the lemmas are markedwith number and gender: gender is consideredas part of the lemma, whereas number is indi-rectly determined by the source-side, as we ex-pect nouns to be translated with their appro-priate number value.
We use a linear chainCRF (Lafferty et al., 2001) to predict the mor-phological features (number, gender, case andstrong/weak).
The features that are part of thelemma of nouns (number, gender) are propa-gated over the rest of the linguistic phrase.
Incontrast, case depends on the role of the NP inthe sentence (e.g.
subject or direct/indirect ob-ject) and is thus to be determined entirely fromthe respective context in the sentence.
The valuefor strong/weak depends on the combination ofthe other features.
Based on the lemma and thepredicted features, inflected forms are then gen-erated using the rule-based morphological anal-yser SMOR (Schmid et al., 2004).
This system isdescribed in more detail in Fraser et al.
(2012).3.2 Verbal InflectionGerman verbs agree in number and person withtheir subjects.
We thus have to derive this in-formation from a noun phrase in nominativecase (= the subject) near the verb.
This informa-tion comes from the nominal inflection predic-tion described in section 3.1.
We predict tenseand mode of the verb using a maximum-entropyclassifier which is trained on English and Ger-man contextual information.
After deriving allinformation needed for the generation of theverbs, the inflected forms are generated withSMOR.4 Compound ProcessingIn English to German translation, compoundprocessing is more difficult than in the oppo-site direction.
Not only do compounds have tobe split accurately, but they also have to be puttogether correctly after decoding.
The disflu-ency of MT output and the difficulty of decidingwhich single words should be merged into com-pounds make this task even more challenging.72(split+lem.
)TrainingParallel Training DataLanguageModelTranslationModelEnglish text............Target Training Data........................German textGerman text............tooltradefruit box Werkzeug KisteHandelObstWerkzeug KisteObst HandelParallel Training Data........................ObsthandelWerkzeugkisteTarget Training DataGerman textPre?ProcessingObsthandel............Werkzeugkiste............tool boxfruit tradeEnglish text German textPost?processing........ObstkisteObsthandelGerman(fluent)TestinginputEnglish....fruit tradefruit box(split+lem.)(split+lem.
)lemmatisesplitsplitlemmatise ........RecombineRe?inflectGerman(split+lem.
)....outputObst KisteObst HandelDecoderFigure 1: Pipeline overview of our primary CimS-CoRI system.We combine compound processing with in-flection prediction (see Section 3) and thus ex-tend the two-step approach respectively: com-pounds are split and lemmatised simultane-ously, again using SMOR.
This allows for maxi-mal generalisation over former compound partsand independently occurring simple words.
Weuse this split representation for training.
Af-ter decoding, we re-combine words into com-pounds again, using our extended CRF-basedapproach, which is based on Stymne and Can-cedda (2011), but includes source-language fea-tures and allows for maximal generalisationthrough lemmatisation.
More details can befound in Cap et al.
(2014).
We then use SMORto generate sound German compounds (includ-ing morphological transformations such as in-troduction or deletion of filler letters).
Finally,the whole text including the newly-created com-pounds, is re-inflected using the nominal in-flection prediction models as described in Sec-tion 3.1 above.
This procedure allows us to createcompounds that have not been seen in the par-allel training data, and also inflectional variantsof seen compounds.
See Figure 1 for an overviewof our compound processing pipeline.4.1 PortmanteausPortmanteaus are a special kind of compound.They are a fusion of a preposition and a defi-nite article (thus not productive) and their casemust agree with the case of the noun.
For ex-ample, ?zum?
can be split into ?zu?
+ ?dem?
=to+theDati ve .
They introduce additional spar-sity to the training data: imagine a noun oc-curred with its definite article in the trainingdata, but not with the portemanteau required attesting time.
Splitting portemanteaus allows aphrase-based SMT system to access phrases cov-ering nouns and their corresponding definite ar-ticles.
In a post-processing step, definite articlesare then re-merged with their preceding prepo-sitions to restore the original portmanteau, see(Fraser et al., 2012) for details.
This generalisa-tion effect is even larger as we not only split port-manteaus, but also lemmatise the articles.5 System descriptionsOur shared task submissions include differentcombinations of the inflection and compoundprocessing procedures as described in the pre-vious two sections.
We give an overview of allour systems in Table 1.
Note that we did notre-train the compound processing CRFs on thenew dataset, but used our models trained on the2009 training data instead.
However, this doesnot hurt performance, as the CRF we use is nottrained on surface forms, but only frequenciesand source-side features instead.
See (Fraser etal., 2012) and (Cap et al., 2014) for more detailson how we trained the respective CRFs.
In con-trast, the verbal classifier has been trained onWMT 2014 data.6 Experimental SettingsIn all our systems, we only used data distributedfor the shared task.
All available German datawas morphologically analysed with SMOR.
Forlemmatisation of the German training data, wedisambiguated SMOR using POS tags we ob-tained through parsing the German section ofthe parallel training data with BitPar (Schmid,73No.
apprart nominal compound verbal source-sidesplitting inflection processing inflection reorderingCimS-RI X XCimS-CoRIP X X XCimS-RIVe X X XCimS-CoRIVe X X X XCimS-Syntax-RORI X X XTable 1: Overview of our submission systems.RI = nominal Re-Inflection, Co = Compound process-ing, Ve = Verbal inflection, RO = source-side Re-Ordering.
Syntax = syntax-based SMT P = primarysubmission.2004) and tagging the big monolingual trainingdata using RFTagger (Schmid and Laws, 2008)1.Note that we did not normalise German lan-guage e.g.
with respect to old vs. new writingconvention etc.
as we did in previous submis-sions (e.g.
(Fraser, 2009)).For the compound prediction CRFs using syn-tactic features derived from the source language,we parsed the English section of the paralleldata using EGRET, a re-implementation of theBerkeley-Parser by Hui Zhang2.
Before trainingour models on the English data, we normalisedall occurrences of British vs. American Englishvariants to British English.
We did so for train-ing, tuning and testing input.Language Model We trained 5-gram languagemodels based on all available German monolin-gual training data from the shared task (roughly1.5 billion words) using the SRILM toolkit (Stol-cke, 2002) with Kneser-Ney smoothing.
We thenused KenLM (Heafield, 2011) for faster process-ing.
For each of our experiments, we traineda separate language model on the whole dataset, corresponding to the different underspeci-fied representations of German used in our ex-periments, e.g.
lemmatised for CimS-RI, lemma-tised with split compounds for CimS-CoRI, etc.Phrase-based Translation model We per-formed word alignment using the multithreadedGIZA++ toolkit (Och and Ney, 2003; Gao andVogel, 2008).
For translation model training anddecoding, we used the Moses toolkit (Koehnet al., 2007) to build phrase-based statisticalmachine translation systems, following theinstructions for the baseline system for theshared task, using only default settings.1We could not parse the whole monolingual dataset dueto time-constraints and thus used RFTagger as a substitute.2available from https://sites.google.com/site/zhangh1982/egret.Syntax-based Translation model As a variantto the phrase-based systems, we applied the in-flection prediction system to a string-to-tree sys-tem with GHKM extraction (Galley et al.
(2004),Williams and Koehn (2012)).
We used the samedata-sets as for the phrase-based systems, andapplied BitPar (Schmid, 2004) to obtain target-side trees.
For this system, we used source-sidereordering according to Gojun and Fraser (2012)relying on parses obtained with EGRET3.Tuning For tuning of feature weights, we usedbatch-mira with ??safe-hope?
(Cherry and Foster,2012) until convergence (or maximal 25 runs).We used the 3,000 sentences of newstest2012 fortuning.
Each experiment was tuned separately,optimising Bleu scores (Papineni et al., 2002)against a lemmatised version of the tuning ref-erence.
In the compound processing systems weintegrated the CRF-based prediction and merg-ing procedure into each tuning iteration andscored each output against the same unsplit andlemmatised reference as the other systems.Testing After decoding, the underspecifiedrepresentation has to be retransformed intofluent German text, i.e., compounds need tobe re-combined and all words have to be re-inflected.
The whole procedure can be dividedinto the following steps:1a) translation into lemmatised Germanrepresentation (RI, RIVe)1b) translation into split and lemmatisedGerman (CoRi, CoRIVe)2) compound merging (CoRI, CoRIVe):3) nominal inflection prediction and gen-eration of full forms using SMOR (all)4) verbal re-inflection (RIVe, CoRIVe)5) merging of portmanteaus (all)3Note that we observed some data-related issues on theSyntax-RORI experiments that we hope to resolve in thenear future.74Experiment mert.log Bleu ci Bleu cs Bleu ci Bleu csnews2012 news2013 news2013 news2014 news2014raw 16.52 18.62 17.61 17.80 17.25CimS-RI 18.51 19.23 18.38 18.33 17.75CimS-CoRIP 18.36 19.13 18.25 18.51 17.87CimS-RIVe 19.08 18.89 18.06 17.86 17.31CimS-CoRIVe 18.69 18.60 17.77 17.38 16.78CimS-Syntax-RORI 18.26 19.04 18.17 18.15 17.59Table 2: Bleu scores for all CimS-submissions of the 2014 shared task.
ci = case-insensitive, cs = case-sensitive; P = primary submission.After these post-processing steps, the text wasautomatically recapitalised and detokenised, us-ing the tools provided by the shared task, whichwe trained on the whole German dataset.
We cal-culated Bleu (Papineni et al., 2002) scores usingthe NIST script version 13a.7 ResultsWe evaluated our systems with the 3,000 sen-tences of last year?s newstest2013 and also the2,737 sentences of the 2014 blind test set for theGerman-English language pair.
The Bleu scoresof our systems are given in Table 2, where rawdenotes our baseline system which we ran with-out any pre- or postprocessing whatsoever.
Notethat the big gap in mert.log scores between rawand the CimS-systems comes from the fact thatraw is scored against the original (i.e.
fully in-flected) version of the tuning reference, while theCimS-systems are scored against the stemmedtuning reference.As for the Bleu scores of the test sets, we ob-serve similar improvements for the CimS-RI andCimS-CoRI systems of +0.5/0.6 with respect tothe raw baseline as we did in previous experi-ments (Cap et al., 2014)4.
In contrast, our sys-tems incorporating verbal prediction inflection(CimS-RIVe/CoRIVe) cannot yet catch up withthe performance of the well-investigated nom-inal inflection and compound processing sys-tems (CimS-RI/CoRI).
We attribute this partly tothe positive influence we assume fully inflectedverbs to have in nominal inflection predictionmodels, but as the verb processing systems arestill under development, there might be other is-sues we have not discovered yet.
We plan to re-4We will have a closer look at the data from a compoundprocessing view in Section 7.1 below.visit these systems and improve them.Finally, the syntax-based reordering systemyields scores that are competitive to those ofCimS-RI/CoRI.
While Syntax-RORI so far only in-corporates source-side reordering and nominalre-inflection, we plan to investigate further ex-tensions of this approach in the future.7.1 Additional EvaluationWe manually screened the filtered 2014 test setand identified 3,456 German compound tokens,whereof 862 did not occur in the parallel trainingdata and thereof, 244 did not even occur in themonolingual training data.
For each of our sys-tems, we calculated the number of compoundreference matches they produced.
The resultsare given in Table 3.system ref newraw 827 0CimS-RI .
864 5CimS-CoRIP 1,064 109CimS-RIVe 853 5CimS-CoRIVe 1,070 122CimS-Syntax-RORI 900 20Table 3: Numbers of compounds produced bythe systems that matched the reference (ref ) anddid not occur in the parallel training data (new).The compound processing systems (with Coin the name) generate many more correct com-pounds than comparable systems without com-pound handling.
Compared to the raw base-line, CoRI/CoRIVe did not only produce 237/243more reference matches, but also 109/122 com-pounds that matched the reference but did notoccur in the parallel training data.
A lookup ofthose 109/122 compounds in the monolingualtraining data (consisting of roughly 1.5 billionwords) revealed, that 8/6 of them did not oc-75cur there either5.
These were thus not accessi-ble to a list-based compound merging approacheither.
This result also shows that despite thefact that CoRIVe does not yield a competitivetranslation quality performance yet, the com-pound processing component seems to bene-fit from the verbal inflection and it is definitelyworth more investigation in the future.Moreover, it can be seen from Table 3 thatthe re-inflection systems (*RI*) produce morereference matches than the raw baseline.
In-terestingly, they even produce some referencematches that have not been seen in the par-allel training data due to inflectional variation,and in the case of the syntax-based system dueto a naive list-based compound merging: eventhough it has not been trained on a split repre-sentation of German text, it might occasionallyoccur that two German nouns occur next to eachother in the MT output.
If so, these two words aremerged into a compound, using a list-based ap-proach, similar to Popovic?
et al.
(2006).8 ReorderingFor the system CimS-Syntax-RORI, English dataparsed with EGRET was reordered using scriptswritten for parse trees produced by the con-stituent parser (Charniak and Johnson, 2005),using a model we trained on the standard PennTreebank sections.
Unfortunately, the reorder-ing scripts could not be straightforwardly ap-plied to EGRET parses and require more signifi-cant modifications than we first expected.We thus decided to parse the Europarl data(v7) with (Charniak and Johnson, 2005) insteadand run our reordering scripts on it (CimS-RO).For evaluation purposes, we build a baseline sys-tem raw?
which has been trained only on Eu-roparl.
Tuning and testing setup is the same asfor the systems described in Section 6 with thedifference that the weights have been tuned onnewstest2013.
The evaluation results are shownin Table 4.
Similarly to previous results reportedin (Gojun and Fraser, 2012), the CimS-RO systemshows an improvement of 0.5 Bleu points whencompared to the raw?
baseline .5Namely: Testflugzeugen (test airplanes), Medientri-bunal (media tribunal), RBS-Mitarbeiter (RBS worker),Schulmauersanierung (school wall renovation), Anti-Terror-Organisationen (anti-terror organisations), andTabakimpfstoffe (tobacco-plant-created vaccines) in bothand in CoRI also Hand-gep?ckgeb?hr (hand luggage fee)and Haftungsstreitigkeiten (liability litigation).Experiment mert.log Bleu ci Bleu csnews2013 news2014 news2014raw?
16.87 16.25 15.31CimS-RO 17.76 16.81 15.81Table 4: Evaluation of the reordering systemtrained on Europarl v7.9 SummaryWe presented the CimS systems, a set ofmorphology-aware translation systems cus-tomised for translation from English to German.Each system operates on a different level ofmorphological description, be it nominal inflec-tion, verbal inflection, compound processingor source-side reordering.
Some of the systemsare well-established (RI, CoRI and RO), othersare still under developement (RIVe, CoRIVe andSyntax-RORI).
However, all of them, with the ex-ception of CoRIVe, lead to improved translationquality when evaluated against a contrastivebaseline without linguistic processing.
In anadditional evaluation, we could show that thecompound processing systems are able to createa considerable number of compounds unseenin the parallel training data.In the future, we will investigate further com-binations and extensions of our morphologicalcomponents, including reordering, compoundprocessing and verbal inflection.
There are stillmany many interesting challenges to be solvedin all of these areas, and this is especially true forverbal inflection.AcknowledgmentsThis work was supported by Deutsche For-schungsgemeinschaft grants Models of Mor-phosyntax for Statistical Machine Translation(Phase 2) and Distributional Approaches to Se-mantic Relatedness.
We would like to thankDaniel Quernheim for sharing the workload ofpreprocessing the data with us.Moreover, we thank Edgar Hoch from the IMSsystem administration for generously providingus with disk space and all our colleagues at IMS,especially Fabienne Braune, Junfei Guo, NinaSeemann and Jason Utt for postponing their ex-periments to let us use most of IMS?
computingfacilities for a whole week.
Thank you each beau-coup!76ReferencesFabienne Cap, Alexander Fraser, Marion Weller, andAoife Cahill.
2014.
How to Produce UnseenTeddy Bears: Improved Morphological Processingof Compounds in SMT.
In Proceedings of EACL2014.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics(ACL), Ann Arbor, Michigan.Colin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.In Proceedings of HLT-NAACL 2012.Michael Collins, Philipp Koehn, and Ivona Kuc?erov?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings ACL 2005.Chris Dyer.
2009.
Using a maximum entropy modelto build segmentation lattices for MT.
In Proceed-ings of HLT-NAACL 2009.Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-bienne Cap.
2012.
Modeling Inflection and WordFormation in SMT.
In Proceedings of EACL 2012.Alexander Fraser.
2009.
Experiments in Morphosyn-tactic Processing for Translation to and from Ger-man.
In Proceedings of WMT 2009.Fabienne Fritzinger and Alexander Fraser.
2010.How to Avoid Burning Ducks: Combining Lin-guistic Analysis and Corpus Statistics for Ger-man Compound Processing.
In Proceedings ofWMT@ACL2010.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a Translation Rule?In Proceedings of HLT-NAACL 2004.Qin Gao and Stephan Vogel.
2008.
Parallel imple-mentations of word alignment tool.
In ACL 2008:Proceedings of the Workshop on Software Engineer-ing, Testing, and Quality Assurance for NaturalLanguage Processing.Adri?
De Gispert and Jos?
B. Mari?o.
2008.
On theimpact of morphology in English to Spanish statis-tical MT.
Speech Communication.Anita Gojun and Alexander Fraser.
2012.
Determin-ing the placement of German verbs in English-to-German SMT.
In Proceedings of EACL 2012.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of WMT2011.Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,and Chris Quirk.
2010.
A discriminative lexiconmodel for complex morphology.
In Proceedings ofAMTA 2010.Philipp Koehn and Kevin Knight.
2003.
EmpiricalMethods for Compound Splitting.
In Proceedingsof EACL 2003.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open Source Toolkit for Statistical Ma-chine Translation.
In Proceedings of ACL 2007(Demo Session).John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Prob-abilistic Models for Segmenting and Labeling Se-quence Data.
In ICML?01.Klaus Macherey, Andrew M. Dai, David Talbot,Ashok C. Popat, and Franz Och.
2011.
Language-independent Compound Splitting with Morpho-logical Operations.
In Proceedings of ACL 2011.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51,.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
Bleu: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof ACL 2002.Maja Popovic?, Daniel Stein, and Hermann Ney.
2006.Statistical Machine Translation of German Com-pound Words.
In Proceedings of FinTAL 2006.Helmut Schmid and Florian Laws.
2008.
Estimationof conditional probabilities with decision trees andan application to fine-grained pos tagging.
In Pro-ceedings of COLING 2008.Helmut Schmid, Arne Fitschen, and Ulrich Heid.2004.
SMOR: A German Computational Morphol-ogy Covering Derivation, Composition and Inflec-tion.
In Proceedings of LREC 2004.Helmut Schmid.
2004.
Efficient Parsing of HighlyAmbiguous Context-Free Grammars with Bit Vec-tors.
In Proceedings of Coling 2004.Andreas Stolcke.
2002.
SRILM ?
an Extensible Lan-guage Modelling Toolkit.
In Proceedings of ICSLN2002.Sara Stymne and Nicola Cancedda.
2011.
Pro-ductive Generation of Compound Words in Sta-tistical Machine Translation.
In Proceedings ofWMT@EMNLP?11.Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.2008.
Effects of Morphological Analysis in Transla-tion between German and English.
In Proceedingsof WMT 2008.Sara Stymne.
2009.
A Comparison of Merging Strate-gies for Translation of German Compounds.
InProceedings of EACL 2009 (Student Workshop).77Kristina Toutanova, Hisami Suzuki, and AchimRuopp.
2008.
Applying Morphology GenerationModels to Machine Translation.
In Proceedings ofHLT-ACL 2008.Marion Weller, Alexander Fraser, and SabineSchulte im Walde.
2013.
Using Subcatego-rization Knowledge to Improve Case Prediction forTranslation to German.
In Proceedings of ACL?13.Philip Williams and Philipp Koehn.
2011.
Agreementconstraints for statistical machine translation intoGerman.
In Proceedings of WMT 2011.Philip Williams and Phillipp Koehn.
2012.
GHKM-Rule Extraction and Scope-3 Parsing in Moses.
InProceedings of WMT 2007.78
