Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 551?561,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLong Short-Term Memory-Networks for Machine ReadingJianpeng Cheng, Li Dong and Mirella LapataSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB{jianpeng.cheng,li.dong}@ed.ac.uk, mlap@inf.ed.ac.ukAbstractIn this paper we address the question of howto render sequence-level networks better athandling structured input.
We propose a ma-chine reading simulator which processes textincrementally from left to right and performsshallow reasoning with memory and atten-tion.
The reader extends the Long Short-TermMemory architecture with a memory networkin place of a single memory cell.
This en-ables adaptive memory usage during recur-rence with neural attention, offering a way toweakly induce relations among tokens.
Thesystem is initially designed to process a singlesequence but we also demonstrate how to inte-grate it with an encoder-decoder architecture.Experiments on language modeling, sentimentanalysis, and natural language inference showthat our model matches or outperforms thestate of the art.1 IntroductionHow can a sequence-level network induce relationswhich are presumed latent during text processing?How can a recurrent network attentively memorizelonger sequences in a way that humans do?
In thispaper we design a machine reader that automaticallylearns to understand text.
The term machine read-ing is related to a wide range of tasks from answer-ing reading comprehension questions (Clark et al,2013), to fact and relation extraction (Etzioni et al,2011; Fader et al, 2011), ontology learning (Poonand Domingos, 2010), and textual entailment (Da-gan et al, 2005).
Rather than focusing on a specifictask, we develop a general-purpose reading simula-tor, drawing inspiration from human language pro-cessing and the fact language comprehension is in-cremental with readers continuously extracting themeaning of utterances on a word-by-word basis.In order to understand texts, our machine readershould provide facilities for extracting and repre-senting meaning from natural language text, storingmeanings internally, and working with stored mean-ings to derive further consequences.
Ideally, sucha system should be robust, open-domain, and de-grade gracefully in the presence of semantic rep-resentations which may be incomplete, inaccurate,or incomprehensible.
It would also be desirable tosimulate the behavior of English speakers who pro-cess text sequentially, from left to right, fixatingnearly every word while they read (Rayner, 1998)and creating partial representations for sentence pre-fixes (Konieczny, 2000; Tanenhaus et al, 1995).Language modeling tools such as recurrent neuralnetworks (RNN) bode well with human reading be-havior (Frank and Bod, 2011).
RNNs treat each sen-tence as a sequence of words and recursively com-pose each word with its previous memory, until themeaning of the whole sentence has been derived.
Inpractice, however, sequence-level networks are metwith at least three challenges.
The first one concernsmodel training problems associated with vanishingand exploding gradients (Hochreiter, 1991; Bengioet al, 1994), which can be partially ameliorated withgated activation functions, such as the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhu-ber, 1997), and gradient clipping (Pascanu et al,2013).
The second issue relates to memory com-pression problems.
As the input sequence gets com-pressed and blended into a single dense vector, suf-551The FBI is chasing a criminal on the run .The FBI is chasing a criminal on the run .The FBI is chasing a criminal on the run .The FBI is chasing a criminal on the run .The FBI is chasing a criminal on the run .The FBI is chasing a criminal on the run .The FBI is chasing a criminal on the run .The FBI is chasing a criminal on the run .The FBI is chasing a criminal on the run .The FBI is chasing a criminal on the run .Figure 1: Illustration of our model while reading thesentence The FBI is chasing a criminal on the run.Color red represents the current word being fixated,blue represents memories.
Shading indicates the de-gree of memory activation.ficiently large memory capacity is required to storepast information.
As a result, the network general-izes poorly to long sequences while wasting memoryon shorter ones.
Finally, it should be acknowledgedthat sequence-level networks lack a mechanism forhandling the structure of the input.
This imposesan inductive bias which is at odds with the fact thatlanguage has inherent structure.
In this paper, wedevelop a text processing system which addressesthese limitations while maintaining the incremental,generative property of a recurrent language model.Recent attempts to render neural networks morestructure aware have seen the incorporation of exter-nal memories in the context of recurrent neural net-works (Weston et al, 2015; Sukhbaatar et al, 2015;Grefenstette et al, 2015).
The idea is to use multiplememory slots outside the recurrence to piece-wisestore representations of the input; read and writeoperations for each slot can be modeled as an at-tention mechanism with a recurrent controller.
Wealso leverage memory and attention to empower arecurrent network with stronger memorization capa-bility and more importantly the ability to discoverrelations among tokens.
This is realized by insert-ing a memory network module in the update of a re-current network together with attention for memoryaddressing.
The attention acts as a weak inductivemodule discovering relations between input tokens,and is trained without direct supervision.
As a pointof departure from previous work, the memory net-work we employ is internal to the recurrence, thusstrengthening the interaction of the two and lead-ing to a representation learner which is able to rea-son over shallow structures.
The resulting model,which we term Long Short-Term Memory-Network(LSTMN), is a reading simulator that can be usedfor sequence processing tasks.Figure 1 illustrates the reading behavior of theLSTMN.
The model processes text incrementallywhile learning which past tokens in the memory andto what extent they relate to the current token beingprocessed.
As a result, the model induces undirectedrelations among tokens as an intermediate step oflearning representations.
We validate the perfor-mance of the LSTMN in language modeling, sen-timent analysis, and natural language inference.
Inall cases, we train LSTMN models end-to-end withtask-specific supervision signals, achieving perfor-mance comparable or better to state-of-the-art mod-els and superior to vanilla LSTMs.2 Related WorkOur machine reader is a recurrent neural network ex-hibiting two important properties: it is incremental,simulating human behavior, and performs shallowstructure reasoning over input streams.Recurrent neural network (RNNs) have been suc-cessfully applied to various sequence modeling andsequence-to-sequence transduction tasks.
The latterhave assumed several guises in the literature suchas machine translation (Bahdanau et al, 2014), sen-tence compression (Rush et al, 2015), and readingcomprehension (Hermann et al, 2015).
A key con-tributing factor to their success has been the abil-ity to handle well-known problems with explodingor vanishing gradients (Bengio et al, 1994), leadingto models with gated activation functions (Hochre-iter and Schmidhuber, 1997; Cho et al, 2014), andmore advanced architectures that enhance the in-formation flow within the network (Koutn?
?k et al,2014; Chung et al, 2015; Yao et al, 2015).A remaining practical bottleneck for RNNs ismemory compression (Bahdanau et al, 2014): sincethe inputs are recursively combined into a singlememory representation which is typically too smallin terms of parameters, it becomes difficult to accu-rately memorize sequences (Zaremba and Sutskever,2014).
In the encoder-decoder architecture, thisproblem can be sidestepped with an attention mech-anism which learns soft alignments between the de-coding states and the encoded memories (Bahdanau552et al, 2014).
In our model, memory and attentionare added within a sequence encoder allowing thenetwork to uncover lexical relations between tokens.The idea of introducing a structural bias to neu-ral models is by no means new.
For example, it isreflected in the work of Socher et al (2013a) whoapply recursive neural networks for learning naturallanguage representations.
In the context of recur-rent neural networks, efforts to build modular, struc-tured neural models date back to Das et al (1992)who connect a recurrent neural network with an ex-ternal memory stack for learning context free gram-mars.
Recently, Weston et al (2015) propose Mem-ory Networks to explicitly segregate memory stor-age from the computation of neural networks in gen-eral.
Their model is trained end-to-end with a mem-ory addressing mechanism closely related to soft at-tention (Sukhbaatar et al, 2015) and has been ap-plied to machine translation (Meng et al, 2015).Grefenstette et al (2015) define a set of differen-tiable data structures (stacks, queues, and dequeues)as memories controlled by a recurrent neural net-work.
Tran et al (2016) combine the LSTM with anexternal memory block component which interactswith its hidden state.
Kumar et al (2016) employa structured neural network with episodic memorymodules for natural language and also visual ques-tion answering (Xiong et al, 2016).Similar to the above work, we leverage memoryand attention in a recurrent neural network for induc-ing relations between tokens as a module in a largernetwork responsible for representation learning.
Asa property of soft attention, all intermediate rela-tions we aim to capture are soft and differentiable.This is in contrast to shift-reduce type neural mod-els (Dyer et al, 2015; Bowman et al, 2016) wherethe intermediate decisions are hard and induction ismore difficult.
Finally, note that our model capturesundirected lexical relations and is thus distinct fromwork on dependency grammar induction (Klein andManning, 2004) where the learned head-modifier re-lations are directed.3 The Machine ReaderIn this section we present our machine reader whichis designed to process structured input while retain-ing the incrementality of a recurrent neural network.The core of our model is a Long Short-Term Mem-ory (LSTM) unit with an extended memory tape thatexplicitly simulates the human memory span.
Themodel performs implicit relation analysis betweentokens with an attention-based memory addressingmechanism at every time step.
In the following, wefirst review the standard Long Short-Term Memoryand then describe our model.3.1 Long Short-Term MemoryA Long Short-TermMemory (LSTM) recurrent neu-ral network processes a variable-length sequencex = (x1,x2, ?
?
?
,xn) by incrementally adding newcontent into a single memory slot, with gates con-trolling the extent to which new content should bememorized, old content should be erased, and cur-rent content should be exposed.
At time step t, thememory ct and the hidden state ht are updated withthe following equations:2664itftotc?t3775=2664ssstanh3775W ?
[ht 1, xt ] (1)ct = ft   ct 1 + it   c?t (2)ht = ot   tanh(ct) (3)where i, f , and o are gate activations.
Comparedto the standard RNN, the LSTM uses additive mem-ory updates and it separates the memory c from thehidden state h, which interacts with the environmentwhen making predictions.3.2 Long Short-Term Memory-NetworkThe first question that arises with LSTMs is the ex-tent to which they are able to memorize sequencesunder recursive compression.
LSTMs can producea list of state representations during composition,however, the next state is always computed from thecurrent state.
That is to say, given the current stateht , the next state ht+1 is conditionally independent ofstates h1 ?
?
?ht 1 and tokens x1 ?
?
?xt .
While the recur-sive state update is performed in aMarkov manner, itis assumed that LSTMs maintain unbounded mem-ory (i.e., the current state alone summarizes well thetokens it has seen so far).
This assumption may failin practice, for example when the sequence is long553Figure 2: Long Short-Term Memory-Network.Color indicates degree of memory activation.or when the memory size is not large enough.
An-other undesired property of LSTMs concerns model-ing structured input.
An LSTM aggregates informa-tion on a token-by-token basis in sequential order,but there is no explicit mechanism for reasoning overstructure and modeling relations between tokens.Our model aims to address both limitations.
Oursolution is to modify the standard LSTM structureby replacing the memory cell with a memory net-work (Weston et al, 2015).
The resulting LongShort-Term Memory-Network (LSTMN) stores thecontextual representation of each input token witha unique memory slot and the size of the memorygrows with time until an upper bound of the memoryspan is reached.
This design enables the LSTM toreason about relations between tokens with a neuralattention layer and then perform non-Markov stateupdates.
Although it is feasible to apply both writeand read operations to the memories with attention,we concentrate on the latter.
We conceptualize theread operation as attentively linking the current to-ken to previous memories and selecting useful con-tent when processing it.
Although not the focus ofthis work, the significance of the write operationcan be analogously justified as a way of incremen-tally updating previous memories, e.g., to correctwrong interpretations when processing garden pathsentences (Ferreira and Henderson, 1991).The architecture of the LSTMN is shown in Fig-ure 2 and the formal definition is provided as fol-lows.
The model maintains two sets of vectorsstored in a hidden state tape used to interact with theenvironment (e.g., computing attention), and a mem-ory tape used to represent what is actually stored inmemory.1 Therefore, each token is associated witha hidden vector and a memory vector.
Let xt de-note the current input; Ct 1 = (c1, ?
?
?
,ct 1) denotesthe current memory tape, and Ht 1 = (h1, ?
?
?
,ht 1)the previous hidden tape.
At time step t, the modelcomputes the relation between xt and x1 ?
?
?xt 1through h1 ?
?
?ht 1 with an attention layer:ati = vT tanh(Whhi +Wxxt +Wh?h?t 1) (4)sti = softmax(ati) (5)This yields a probability distribution over the hiddenstate vectors of previous tokens.
We can then com-pute an adaptive summary vector for the previoushidden tape and memory tape denoted by c?t and h?t ,respectively:?h?tc?t =t 1?i=1sti ?
?hici (6)and use them for computing the values of ct and htin the recurrent update as:2664itftotc?t3775=2664ssstanh3775W ?
[h?t , xt ] (7)ct = ft   c?t + it   c?t (8)ht = ot   tanh(ct) (9)where v,Wh,Wx andWh?
are the new weight terms ofthe network.A key idea behind the LSTMN is to use attentionfor inducing relations between tokens.
These rela-tions are soft and differentiable, and components ofa larger representation learning network.
Althoughit is appealing to provide direct supervision for theattention layer, e.g., with evidence collected froma dependency treebank, we treat it as a submod-ule being optimized within the larger network in adownstream task.
It is also possible to have a morestructured relational reasoning module by stackingmultiple memory and hidden layers in an alternat-ing fashion, resembling a stacked LSTM (Graves,1For comparison, LSTMs maintain a hidden vector and amemory vector; memory networks (Weston et al, 2015) have aset of key vectors and a set of value vectors.5542013) or a multi-hop memory network (Sukhbaataret al, 2015).
This can be achieved by feeding theoutput hkt of the lower layer k as input to the upperlayer (k+ 1).
The attention at the (k+ 1)th layer iscomputed as:ati,k+1 = vT tanh(Whhk+1i +Wlhkt +Wh?h?k+1t 1 ) (10)Skip-connections (Graves, 2013) can be applied tofeed xt to upper layers as well.4 Modeling Two Sequences with LSTMNNatural language processing tasks such as machinetranslation and textual entailment are concernedwith modeling two sequences rather than a singleone.
A standard tool for modeling two sequenceswith recurrent networks is the encoder-decoder ar-chitecture where the second sequence (also knownas the target) is being processed conditioned on thefirst one (also known as the source).
In this sectionwe explain how to combine the LSTMN which ap-plies attention for intra-relation reasoning, with theencoder-decoder network whose attention modulelearns the inter-alignment between two sequences.Figures 3a and 3b illustrate two types of combina-tion.
We describe the models more formally below.Shallow Attention Fusion Shallow fusion simplytreats the LSTMN as a separate module that canbe readily used in an encoder-decoder architecture,in lieu of a standard RNN or LSTM.
As shown inFigure 3a, both encoder and decoder are modeledas LSTMNs with intra-attention.
Meanwhile, inter-attention is triggered when the decoder reads a tar-get token, similar to the inter-attention introduced inBahdanau et al (2014).Deep Attention Fusion Deep fusion combinesinter- and intra-attention (initiated by the decoder)when computing state updates.
We use different no-tation to represent the two sets of attention.
Follow-ing Section 3.2, C and H denote the target memorytape and hidden tape, which store representations ofthe target symbols that have been processed so far.The computation of intra-attention follows Equa-tions (4)?(9).
Additionally, we use A = [a1, ?
?
?
,am]and Y = [g1, ?
?
?
,gm] to represent the source mem-ory tape and hidden tape, with m being the length ofthe source sequence conditioned upon.
We computeinter-attention between the input at time step t andtokens in the entire source sequence as follows:btj = uT tanh(Wgg j +Wxxt +Wg?g?t 1) (11)ptj = softmax(btj) (12)After that we compute the adaptive representation ofthe source memory tape a?t and hidden tape g?t as:?
g?ta?t =m?j=1ptj ?
?g ja j (13)We can then transfer the adaptive source represen-tation a?t to the target memory with another gatingoperation rt , analogous to the gates in Equation (7).rt = s(Wr ?
[g?t ,xt ]) (14)The new target memory includes inter-alignmentrt   a?t , intra-relation ft   c?t , and the new input in-formation it   c?t :ct = rt   a?t + ft   c?t + it   c?t (15)ht = ot   tanh(ct) (16)As shown in the equations above and Figure 3b, themajor change of deep fusion lies in the recurrentstorage of the inter-alignment vector in the targetmemory network, as a way to help the target net-work review source information.5 ExperimentsIn this section we present our experiments for eval-uating the performance of the LSTMN machinereader.
We start with language modeling as itis a natural testbed for our model.
We then as-sess the model?s ability to extract meaning repre-sentations for generic sentence classification taskssuch as sentiment analysis.
Finally, we examinewhether the LSTMN can recognize the semanticrelationship between two sentences by applying itto a natural language inference task.
Our codeis available at https://github.com/cheng6076/SNLI-attention.555(a) Decoder with shallow attention fusion.
(b) Decoder with deep attention fusion.Figure 3: LSTMNs for sequence-to-sequence modeling.
The encoder uses intra-attention, while the decoderincorporates both intra- and inter-attention.
The two figures present two ways to combine the intra- andinter-attention in the decoder.Models Layers PerplexityKN5 ?
141RNN 1 129LSTM 1 115LSTMN 1 108sLSTM 3 115gLSTM 3 107dLSTM 3 109LSTMN 3 102Table 1: Language model perplexity on the PennTreebank.
The size of memory is 300 for all models.5.1 Language ModelingOur language modeling experiments were con-ducted on the English Penn Treebank dataset.
Fol-lowing common practice (Mikolov et al, 2010), wetrained on sections 0?20 (1M words), used sec-tions 21?22 for validation (80K words), and sec-tions 23?24 (90K words for testing).
The datasetcontains approximately 1 million tokens and a vo-cabulary size of 10K.
The average sentence lengthis 21.
We use perplexity as our evaluation metric:PPL = exp(NLL/T ), where NLL denotes the nega-tive log likelihood of the entire test set and T thecorresponding number of tokens.
We used stochas-tic gradient descent for optimization with an ini-tial learning rate of 0.65, which decays by a factorof 0.85 per epoch if no significant improvement hasbeen observed on the validation set.
We renormal-ize the gradient if its norm is greater than 5.
Themini-batch size was set to 40.
The dimensions ofthe word embeddings were set to 150 for all models.In this suite of experiments we compared theLSTMN against a variety of baselines.
The firstone is a Kneser-Ney 5-gram language model (KN5)which generally serves as a non-neural baseline forthe language modeling task.
We also present per-plexity results for the standard RNN and LSTMmodels.
We also implemented more sophisti-cated LSTM architectures, such as a stacked LSTM(sLSTM), a gated-feedback LSTM (gLSTM; Chunget al (2015)) and a depth-gated LSTM (dLSTM;Yao et al (2015)).
The gated-feedback LSTM hasfeedback gates connecting the hidden states acrossmultiple time steps as an adaptive control of the in-formation flow.
The depth-gated LSTM uses a depthgate to connect memory cells of vertically adjacentlayers.
In general, both gLSTM and dLSTM areable to capture long-term dependencies to some de-gree, but they do not explicitly keep past memories.We set the number of layers to 3 in this experiment,mainly to agree with the language modeling exper-iments of Chung et al (2015).
Also note that thatthere are no single-layer variants for gLSTM anddLSTM; they have to be implemented as multi-layersystems.
The hidden unit size of the LSTMN and allcomparison models (except KN5) was set to 300.The results of the language modeling task areshown in Table 1.
Perplexity results for KN5 andRNN are taken from Mikolov et al (2015).
As canbe seen, the single-layer LSTMN outperforms these556he sits down at the piano and playsour view is that we may see a profit declineproducts < unk > have to be first to be winnerseveryone in the world is watching us very closelyFigure 4: Examples of intra-attention (languagemodeling).
Bold lines indicate higher attentionscores.
Arrows denote which word is being focusedwhen attention is computed, but not the direction ofthe relation.two baselines and the LSTM by a significant mar-gin.
Amongst all deep architectures, the three-layerLSTMN also performs best.
We can study the mem-ory activation mechanism of the machine reader byvisualizing the attention scores.
Figure 4 showsfour sentences sampled from the Penn Treebank val-idation set.
Although we explicitly encourage thereader to attend to any memory slot, much attentionfocuses on recent memories.
This agrees with thelinguistic intuition that long-term dependencies arerelatively rare.
As illustrated in Figure 4 the modelcaptures some valid lexical relations (e.g., the de-pendency between sits and at, sits and plays, every-one and is, is and watching).
Note that arcs hereare undirected and are different from the directedarcs denoting head-modifier relations in dependencygraphs.5.2 Sentiment AnalysisOur second task concerns the prediction of senti-ment labels of sentences.
We used the Stanford Sen-timent Treebank (Socher et al, 2013a), which con-tains fine-grained sentiment labels (very positive,positive, neutral, negative, very negative) for 11,855sentences.
Following previous work on this dataset,Models Fine-grained BinaryRAE (Socher et al, 2011) 43.2 82.4RNTN (Socher et al, 2013b) 45.7 85.4DRNN (Irsoy and Cardie, 2014) 49.8 86.6DCNN (Blunsom et al, 2014) 48.5 86.8CNN-MC (Kim, 2014) 48.0 88.1T-CNN (Lei et al, 2015) 51.2 88.6PV (Le and Mikolov, 2014) 48.7 87.8CT-LSTM (Tai et al, 2015) 51.0 88.0LSTM (Tai et al, 2015) 46.4 84.92-layer LSTM (Tai et al, 2015) 46.0 86.3LSTMN 47.6 86.32-layer LSTMN 47.9 87.0Table 2: Model accuracy (%) on the Sentiment Tree-bank (test set).
The memory size of LSTMN modelsis set to 168 to be compatible with previously pub-lished LSTM variants (Tai et al, 2015).we used 8,544 sentences for training, 1,101 for val-idation, and 2,210 for testing.
The average sentencelength is 19.1.
In addition, we also performed a bi-nary classification task (positive, negative) after re-moving the neutral label.
This resulted in 6,920 sen-tences for training, 872 for validation and 1,821 fortesting.
Table 2 reports results on both fine-grainedand binary classification tasks.We experimented with 1- and 2-layer LSTMNs.For the latter model, we predict the sentiment la-bel of the sentence based on the averaged hiddenvector passed to a 2-layer neural network classifierwith ReLU as the activation function.
The mem-ory size for both LSTMN models was set to 168 tobe compatible with previous LSTM models (Tai etal., 2015) applied to the same task.
We used pre-trained 300-D Glove 840B vectors (Pennington etal., 2014) to initialize the word embeddings.
Thegradient for words with Glove embeddings, wasscaled by 0.35 in the first epoch after which all wordembeddings were updated normally.We used Adam (Kingma and Ba, 2015) for op-timization with the two momentum parameters setto 0.9 and 0.999 respectively.
The initial learningrate was set to 2E-3.
The regularization constant was1E-4 and the mini-batch size was 5.
A dropout rateof 0.5 was applied to the neural network classifier.We compared our model with a wide range of top-performing systems.
Most of these models (includ-ing ours) are LSTM variants (third block in Table 2),recursive neural networks (first block), or convolu-557tional neural networks (CNNs; second block).
Re-cursive models assume the input sentences are rep-resented as parse trees and can take advantage ofannotations at the phrase level.
LSTM-type modelsand CNNs are trained on sequential input, with theexception of CT-LSTM (Tai et al, 2015) which op-erates over tree-structured network topologies suchas constituent trees.
For comparison, we also reportthe performance of the paragraph vector model (PV;Le and Mikolov (2014); see Table 2, second block)which neither operates on trees nor sequences butlearns distributed document representations param-eterized directly.The results in Table 2 show that both 1- and2-layer LSTMNs outperform the LSTM baselineswhile achieving numbers comparable to state of theart.
The number of layers for our models was set tobe comparable to previously published results.
Onthe fine-grained and binary classification tasks our2-layer LSTMN performs close to the best systemT-CNN (Lei et al, 2015).
Figure 5 shows examplesof intra-attention for sentiment words.
Interestingly,the network learns to associate sentiment importantwords such as though and fantastic or not and good.5.3 Natural Language InferenceThe ability to reason about the semantic relation-ship between two sentences is an integral part oftext understanding.
We therefore evaluate our modelon recognizing textual entailment, i.e., whether twopremise-hypothesis pairs are entailing, contradic-tory, or neutral.
For this task we used the Stan-ford Natural Language Inference (SNLI) dataset(Bowman et al, 2015), which contains premise-hypothesis pairs and target labels indicating theirrelation.
After removing sentences with unknownlabels, we end up with 549,367 pairs for training,9,842 for development and 9,824 for testing.
Thevocabulary size is 36,809 and the average sentencelength is 22.
We performed lower-casing and tok-enization for the entire dataset.Recent approaches use two sequential LSTMs toencode the premise and the hypothesis respectively,and apply neural attention to reason about their logi-cal relationship (Rockta?schel et al, 2016; Wang andJiang, 2016).
Furthermore, Rockta?schel et al (2016)show that a non-standard encoder-decoder architec-ture which processes the hypothesis conditioned onit ?s tough to watch but it ?s a fantastic moviealthough i did n?t hate this one , it ?s not very good eitherFigure 5: Examples of intra-attention (sentimentanalysis).
Bold lines (red) indicate attention be-tween sentiment important words.the premise results significantly boosts performance.We use a similar approach to tackle this task withLSTMNs.
Specifically, we use two LSTMNs to readthe premise and hypothesis, and then match themby comparing their hidden state tapes.
We performaverage pooling for the hidden state tape of eachLSTMN, and concatenate the two averages to formthe input to a 2-layer neural network classifier withReLU as the activation function.We used pre-trained 300-D Glove 840B vectors(Pennington et al, 2014) to initialize the word em-beddings.
Out-of-vocabulary (OOV) words wereinitialized randomly with Gaussian samples (?=0,s=1).
We only updated OOV vectors in the firstepoch, after which all word embeddings were up-dated normally.
The dropout rate was selected from[0.1, 0.2, 0.3, 0.4].
We used Adam (Kingma and Ba,2015) for optimization with the two momentum pa-rameters set to 0.9 and 0.999 respectively, and theinitial learning rate set to 1E-3.
The mini-batch sizewas set to 16 or 32.
For a fair comparison againstprevious work, we report results with different hid-den/memory dimensions (i.e., 100, 300, and 450).We compared variants of our model against dif-ferent types of LSTMs (see the second block in Ta-ble 3).
Specifically, these include a model whichencodes the premise and hypothesis independentlywith two LSTMs (Bowman et al, 2015), a sharedLSTM (Rockta?schel et al, 2016), a word-by-wordattention model (Rockta?schel et al, 2016), and amatching LSTM (mLSTM; Wang and Jiang (2016)).This model sequentially processes the hypothesis,and at each position tries to match the current wordwith an attention-weighted representation of thepremise (rather than basing its predictions on wholesentence embeddings).
We also compared our mod-558Models h |q|M TestBOW concatenation ?
?
59.8LSTM (Bowman et al, 2015) 100 221k 77.6LSTM-att (Rockta?schel et al, 2016) 100 252k 83.5mLSTM (Wang and Jiang, 2016) 300 1.9M 86.1LSTMN 100 260k 81.5LSTMN shallow fusion 100 280k 84.3LSTMN deep fusion 100 330k 84.5LSTMN shallow fusion 300 1.4M 85.2LSTMN deep fusion 300 1.7M 85.7LSTMN shallow fusion 450 2.8M 86.0LSTMN deep fusion 450 3.4M 86.3Table 3: Parameter counts |q|M, size of hiddenunit h, and model accuracy (%) on the natural lan-guage inference task.els with a bag-of-words baseline which averages thepre-trained embeddings for the words in each sen-tence and concatenates them to create features for alogistic regression classifier (first block in Table 3).LSTMNs achieve better performance comparedto LSTMs (with and without attention; 2nd blockin Table 3).
We also observe that fusion is gen-erally beneficial, and that deep fusion slightly im-proves over shallow fusion.
One explanation is thatwith deep fusion the inter-attention vectors are re-currently memorized by the decoder with a gatingoperation, which also improves the information flowof the network.
With standard training, our deep fu-sion yields the state-of-the-art performance in thistask.
Although encouraging, this result should be in-terpreted with caution since our model has substan-tially more parameters compared to related systems.We could compare different models using the samenumber of total parameters.
However, this would in-evitably introduce other biases, e.g., the number ofhyper-parameters would become different.6 ConclusionsIn this paper we proposed a machine reading simula-tor to address the limitations of recurrent neural net-works when processing inherently structured input.Our model is based on a Long Short-Term Mem-ory architecture embedded with a memory network,explicitly storing contextual representations of in-put tokens without recursively compressing them.More importantly, an intra-attention mechanism isemployed for memory addressing, as a way to in-duce undirected relations among tokens.
The at-tention layer is not optimized with a direct super-vision signal but with the entire network in down-stream tasks.
Experimental results across three tasksshow that our model yields performance comparableor superior to state of the art.Although our experiments focused on LSTMs, theidea of building more structure aware neural modelsis general and can be applied to other types of net-works.
When direct supervision is provided, simi-lar architectures can be adapted to tasks such as de-pendency parsing and relation extraction.
In the fu-ture, we hope to develop more linguistically plausi-ble neural architectures able to reason over nestedstructures and neural models that learn to discovercompositionality with weak or indirect supervision.AcknowledgmentsWe thank members of the ILCC at the School ofInformatics and the anonymous reviewers for help-ful comments.
The support of the European Re-search Council under award number 681760 ?Trans-lating Multiple Modalities into Text?
is gratefullyacknowledged.ReferencesJacob Andreas, Marcus Rohrbach, Trevor Darrell, andDan Klein.
2016.
Learning to compose neural net-works for question answering.
In Proceedings of the2016 NAACL: HLT, pages 1545?1554, San Diego,California.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
In Proceedings of the2014 ICLR, Banff, Alberta.Yoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gradientdescent is difficult.
Neural Networks, IEEE Transac-tions on, 5(2):157?166.Phil Blunsom, Edward Grefenstette, and Nal Kalchbren-ner.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of the 52nd ACL,pages 655?665, Baltimore, Maryland.Samuel R Bowman, Gabor Angeli, Christopher Potts, andChristopher D Manning.
2015.
A large annotated cor-pus for learning natural language inference.
In Pro-ceedings of the 2015 EMNLP, pages 22?32, Lisbon,Portugal.559Samuel R Bowman, Jon Gauthier, Abhinav Rastogi,Raghav Gupta, Christopher D Manning, and Christo-pher Potts.
2016.
A fast unified model for parsing andsentence understanding.
In Proceedings of the 54thACL, pages 1466?1477, Berlin, Germany.Kyunghyun Cho, Bart Van Merrie?nboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014.
Learning phraserepresentations using RNN encoder-decoder for statis-tical machine translation.
In Proceedings of the 2014EMNLP, pages 1724?1734, Doha, Qatar.Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, andYoshua Bengio.
2015.
Gated feedback recurrent neu-ral networks.
In Proceedings of the 32nd ICML, pages2067?2075, Lille, France.Peter Clark, Phil Harrison, and Niranjan Balasubrama-nian.
2013.
A study of the knowledge base require-ments for passing an elementary science test.
In Pro-ceedings of the 3rd Workshop on Automated KB Con-struction, San Francisco, California.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The PASCAL recognising textual entailmentchallenge.
In Proceedings of the PASCAL ChallengesWorkshop on Recognising Textual Entailment.Sreerupa Das, C. Lee Giles, and Guo zheng Sun.
1992.Learning context-free grammars: Capabilities and lim-itations of a recurrent neural network with an exter-nal stack memory.
In Proceedings of the 14th AnnualConference of the Cognitive Science Society, pages791?795.
Morgan Kaufmann Publishers.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A Smith.
2015.
Transition-baseddependency parsing with stack long short-term mem-ory.
In Proceedings of the 53rd ACL, pages 334?343,Beijing, China.Oren Etzioni, Anthony Fader, Janara Christensen,Stephen Soderland, and Mausam.
2011.
Open in-formation extraction: The second generation.
In Pro-ceedings of the 22nd IJCAI, pages 3?10, Barcelona,Spain.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the 2011 EMNLP, pages1535?1545, Edinburgh, Scotland, UK.Fernanda Ferreira and JohnM.
Henderson.
1991.
Recov-ery from misanalyses of garden-path sentences.
Jour-nal of Memory and Language, 30:725?745.Stefan L. Frank and Rens Bod.
2011.
Insensitivity ofthe human sentence-processing system to hierarchicalstructure.
Pyschological Science, 22(6):829?834.Alex Graves.
2013.
Generating sequences with recurrentneural networks.
arXiv preprint arXiv:1308.0850.Edward Grefenstette, Karl Moritz Hermann, Mustafa Su-leyman, and Phil Blunsom.
2015.
Learning to trans-duce with unbounded memory.
In Advances in NeuralInformation Processing Systems, pages 1819?1827.Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom.
2015.
Teaching machines to readand comprehend.
In Advances in Neural InformationProcessing Systems, pages 1684?1692.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Sepp Hochreiter.
1991.
Untersuchungen zu dynamis-chen neuronalen netzen.
Diploma, Technische Univer-sita?t Mu?nchen.Ozan Irsoy and Claire Cardie.
2014.
Deep recursiveneural networks for compositionality in language.
InAdvances in Neural Information Processing Systems,pages 2096?2104.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In Proceedings of the 2014EMNLP, pages 1746?1751, Doha, Qatar.Diederik Kingma and Jimmy Ba.
2015.
Adam: Amethod for stochastic optimization.
In Proceedings ofthe 2015 ICLR, San Diego, California.Dan Klein and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of the42nd ACL, pages 478?485, Barcelona, Spain.Lars Konieczny.
2000.
Locality and parsing complexity.Journal of Psycholinguistics, 29(6):627?645.Jan Koutn?
?k, Klaus Greff, Faustino Gomez, and Ju?rgenSchmidhuber.
2014.
A clockwork RNN.
In Pro-ceedings of the 31st ICML, pages 1863?1871, Beijing,China.Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury,Robert English, Brian Pierce, Peter Ondruska, IshaanGulrajani, and Richard Socher.
2016.
Ask me any-thing: Dynamic memory networks for natural lan-guage processing.
In Proceedings of the 33rd ICML,New York, NY.Quoc V Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
In Pro-ceedings of the 31st ICML, pages 1188?1196, Beijing,China.Tao Lei, Regina Barzilay, and Tommi Jaakkola.
2015.Molding cnns for text: non-linear, non-consecutiveconvolutions.
In Proceedings of the 2015 EMNLP,pages 1565?1575, Lisbon, Portugal.Fandong Meng, Zhengdong Lu, Zhaopeng Tu, Hang Li,and Qun Liu.
2015.
A deep memory-based architec-ture for sequence-to-sequence learning.
In Proceed-ings of ICLR-Workshop 2016, San Juan, Puerto Rico.560Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-nocky`, and Sanjeev Khudanpur.
2010.
Recurrent neu-ral network based language model.
In Proceedings of11th Interspeech, pages 1045?1048, Makuhari, Japan.Tomas Mikolov, Armand Joulin, Sumit Chopra, MichaelMathieu, and Marc?Aurelio Ranzato.
2015.
Learninglonger memory in recurrent neural networks.
In Pro-ceedings of ICLR Workshop, San Diego, California.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2013.
On the difficulty of training recurrent neuralnetworks.
In Proceedings of the 30th ICML, pages1310?1318, Atlanta, Georgia.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: Global vectors for wordrepresentation.
In Proceedings of the 2014 EMNLP,pages 1532?1543, Doha, Qatar.Hoifung Poon and Pedro Domingos.
2010.
Unsuper-vised ontology induction from text.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics, pages 296?305, Uppsala.Keith Rayner.
1998.
Eye movements in reading and in-formation processing: 20 years of research.
Psycho-logical Bulletin, 124(3):372?422.Tim Rockta?schel, Edward Grefenstette, Karl Moritz Her-mann, Toma?s?
Koc?isky`, and Phil Blunsom.
2016.
Rea-soning about entailment with neural attention.
In Pro-ceedings of the 2016 ICLR, San Juan, Puerto Rico.Alexander M Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for abstractive sen-tence summarization.
In Proceedings of the 2015EMNLP, pages 379?389, Lisbon, Portugal.Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-pher D Manning, and Andrew Y Ng.
2011.
Dynamicpooling and unfolding recursive autoencoders for para-phrase detection.
In Advances in Neural InformationProcessing Systems, pages 801?809.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Ng, and ChristopherPotts.
2013a.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Pro-ceedings of the 2013 EMNLP, pages 1631?1642, Seat-tle, Washington.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts.
2013b.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In Proceedings of the 2013 EMNLP, pages1631?1642, Seattle, Washingtton.Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al2015.
End-to-end memory networks.
In Advances inNeural Information Processing Systems, pages 2431?2439.Kai Sheng Tai, Richard Socher, and Christopher D Man-ning.
2015.
Improved semantic representations fromtree-structured long short-term memory networks.
InProceedings of the 53rd ACL, pages 1556?1566, Bei-jing, China.Michael K. Tanenhaus, Michael J. Spivey-Knowlton,Kathleen M. Eberhard, and Julue C. Sedivy.
1995.
In-tegration of visual and linguistic information in spokenlanguage comprehension.
Science, 268:1632?1634.Ke Tran, Arianna Bisazza, and Christof Monz.
2016.
Re-current memory network for language modeling.
InProceedings of the 15th NAACL, San Diego, CA.Shuohang Wang and Jing Jiang.
2016.
Learning natu-ral language inference with lstm.
In Proceedings ofthe 2016 NAACL: HLT, pages 1442?1451, San Diego,California.Jason Weston, Sumit Chopra, and Antoine Bordes.
2015.Memory networks.
In Proceedings of the 2015 ICLR,San Diego, USA.Caiming Xiong, Stephen Merity, and Richard Socher.2016.
Dynamic memory networks for visual and tex-tual question answering.
In Proceedings of the 33rdICML, New York, NY.Kaisheng Yao, Trevor Cohn, Katerina Vylomova, KevinDuh, and Chris Dyer.
2015.
Depth-gated recurrentneural networks.
arXiv preprint arXiv:1508.03790.Wojciech Zaremba and Ilya Sutskever.
2014.
Learningto execute.
arXiv preprint arXiv:1410.4615.561
