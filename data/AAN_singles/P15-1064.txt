Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 656?665,Beijing, China, July 26-31, 2015. c?2015 Association for Computational LinguisticsNegation and Speculation Identification in Chinese LanguageBowei Zou        Qiaoming Zhu       Guodong Zhou*Natural Language Processing Lab, School of Computer Science and TechnologySoochow University, Suzhou, 215006, Chinazoubowei@gmail.com, {qmzhu, gdzhou}@suda.edu.cnAbstractIdentifying negative or speculative narra-tive fragments from fact is crucial fornatural language processing (NLP) appli-cations.
Previous studies on negation andspeculation identification in Chinese lan-guage suffers much from two problems:corpus scarcity and the bottleneck in fun-damental Chinese information processing.To resolve these problems, this paperconstructs a Chinese corpus which con-sists of three sub-corpora from differentresources.
In order to detect the negativeand speculative cues, a sequence labelingmodel is proposed.
Moreover, a bilingualcue expansion method is proposed to in-crease the coverage in cue detection.
Inaddition, this paper presents a new syn-tactic structure-based framework to iden-tify the linguistic scope of a cue, insteadof the traditional chunking-based frame-work.
Experimental results justify theusefulness of our Chinese corpus and theappropriateness of our syntactic struc-ture-based framework which obtainedsignificant improvement over the state-of-the-art on negation and speculationidentification in Chinese language.
*1 IntroductionNegation and speculation are ubiquitous phe-nomena in natural language.
While negation is agrammatical category which comprises variouskinds of devices to reverse the truth value of aproposition, speculation is a grammatical catego-ry which expresses the attitude of a speaker to-wards a statement in terms of degree of certainty,* Corresponding authorreliability, subjectivity, sources of information,and perspective (Morante and Sporleder, 2012).Current studies on negation and speculationidentification mainly focus on two tasks: 1) cuedetection, which aims to detect the signal of anegative or speculative expression, and 2) scoperesolution, which aims to determine the linguisticcoverage of a cue in sentence, in distinguishingunreliable or uncertain information from facts.For example, (E1) and (E2) include a negativecue and a speculative cue respectively, both de-noted in boldface with their linguistic scopesdenoted in square brackets (adopted hereinafter).In sentence (E1), the negative cue ??(not)?
trig-gers the scope of ??????????????
(would not investigate the dereliction of ho-tel)?, within which the fragment ?investigate thedereliction of hotel?
is the part that is repudiated;While the speculative cue ???(expected)?
insentence (E2) triggers the scope ????????
(is still expected to rebound in the late)?, with-in which the fragment ?the benchmark ShanghaiComposite Index will rebound in the late?
is thespeculative part.
(E1) ???????[?????????????].
(All of guests said that they [would not in-vestigate the dereliction of hotel].
)(E2) ????????????????????,?[???????].
(Although dragged down by GEM last Fri-day, the benchmark Shanghai Composite In-dex [is still expected to rebound in the late].
)Negation and speculation identification is veryrelevant for almost all NLP applications involv-ing text understanding which need to discrimi-nate between factual and non-factual information.The treatment of negation and speculation incomputational linguistics has been shown to be656useful for biomedical text processing (Morante etal., 2008; Chowdhury and Lavelli, 2013), infor-mation retrieval (Averbuch, 2004), sentimentanalysis (Councill et al, 2010; Zhu et al, 2014),recognizing textual entailment (Snow et al,2006), machine translation (Baker et al, 2010;Wetzel and Bond, 2012), and so forth.The research on negation and speculationidentification in English has received a noticea-ble boost.
However, in contrast to the significantachievements concerning English, the researchprogress in Chinese language is quite limited.The main reason includes the following two as-pects: First, the scarcity of linguistic resourceseriously limits the advance of related research.To the best of our knowledge, there are no pub-licly available standard Chinese corpus of rea-sonable size annotated with negation and specu-lation.
Second, this may be attributed to the limi-tations of Chinese information processing.The contributions of this paper are as follows:?
To address the aforementioned first issue, thispaper seeks to fill this gap by presenting theChinese negation and speculation corpuswhich consists of three kind of sub-corporaannotated for negative and speculative cues,and their linguistic scopes.
The corpus hasbeen made publicly available for researchpurposes and it is freely downloadable fromhttp://nlp.suda.edu.cn/corpus.?
For cue detection, we propose a feature-basedsequence labeling model to identify cues.
It isworth noting that the morpheme feature isemployed to better represent the composition-al semantics inside Chinese words.
Moreover,for improving the low recall rate which suf-fers from the unknown cues, we propose across-lingual cue expansion strategy based onparallel corpora.?
For scope resolution, we present a new syn-tactic structure-based framework on depend-ency tree.
Evaluation justifies the appropri-ateness and validity of this framework onChinese scope resolution, which outperformsthe chunking-based framework that widelyused in mainstream scope resolution systems.The layout of the rest paper is organized asfollows.
Section 2 describes related work.
Sec-tion 3 provides details about annotation guide-lines and also presents statistics about corpuscharacteristics.
Section 4 describes our approachin detail.
Section 5 reports and discusses our ex-perimental results.
Finally, we conclude ourwork and indicate some future work in Section 6.2 Related WorkCurrently, both cue detection task and scope res-olution task are always modeled as a classifica-tion problem with the purpose of predictingwhether a token is inside or outside the cue andits scope.
Among them, feature-based and ker-nel-based approaches are most popular.In the feature-based framework, Agarwal andYu (2010) employed a conditional random fields(CRFs) model to detect speculative cues andtheir scopes on the BioScope corpus.
The CRFs-based model achieved an F1-meature of 88% indetecting speculative cues.
We train this modelon our corpus as the baseline system for cue de-tection.
Our work is different from theirs in thatwe employ a new feature (morpheme feature)which is particularly appropriate for Chinese.Besides, kernel-based approaches exploit thestructure of the tree that connects cue and its cor-responding scope.
Zou et al (2013) developed atree kernel-based system to resolve the scope ofnegation and speculation, which captures thestructured information in syntactic parsing trees.To the best of our knowledge, this system is thebest English scope resolution system.
For thisreason, we train this system on our corpus as thebaseline system for scope resolution.Compared with a fair amount of works onEnglish negation and speculation identification,unfortunately, few works has been published onChinese.
Ji et al (2010) developed a system todetect speculation in Chinese news texts.
How-ever, only the speculative sentences have beenfound out, with no more fine-grained informationsuch as scope.
The insufficient study on Chinesenegation and speculation identification drives usto construct a high-quality corpus and investigatehow to find an approach that is particularly ap-propriate for Chinese language.3 Corpus ConstructionIn this section, we elaborate on the overall char-acteristics of the Chinese Negation and Specula-tion (abbr., CNeSp) corpus we constructed, in-cluding a brief description of the sources thatconstitute our corpus, general guidelines whichillustrated with lots of examples and some spe-cial cases, and statistics on the overall results ofour corpus.3.1 SourcesTo capture the heterogeneity of language use intexts, the corpus consists of three different657sources and types, including scientific literature,product reviews, and financial articles.Vincze et al (2008) described that it is neces-sary to separate negative and speculative infor-mation from factual especially in science articles,because conclusions of science experiment arealways described by using diversity of expres-sions and include hypothetical asserts or view-points.
For this reason, we adopt the 19 articlesfrom Chinese Journal of Computers (Vol.35(11)),an authoritative academic journal in Chinese, toconstruct the Scientific Literature sub-corpus.Another part of the corpus consists of 311 ar-ticles from ??????
(timely rain for stockmarket)?
column from Sina.com in April, 2013.There are 22.3% and 40.2% sentences in the Fi-nancial Article sub-corpus containing negationand speculation respectively.Many researches have investigated the role ofnegation in sentiment analysis task, as an im-portant linguistic qualifier which leads to achange in polarity.
For example, Councill et al(2010) investigated the problem of determiningthe polarity of sentiment in movie reviews whennegation words occur in the sentences.
On theother hand, speculation is a linguistic expressionthat tends to correlate with subjectivity which isalso crucial for sentiment analysis.
Pang and Lee(2004) showed that subjectivity detection in thereview domain helps to improve polarity classifi-cation.
Therefore, the Product Review sub-corpus consists of 821 comments of hotel servicefrom the website Ctrip.com.3.2 Annotation GuidelinesThe guidelines of our CNeSp corpus have partlyreferred to the existing Bioscope corpus guide-lines (BioScope, 2008) in order to fit the needs ofthe Chinese language.
In annotation process,negative or speculative cues and their linguisticscopes in sentence are annotated.
There are sev-eral general principles below:(G1) Cue is contained in its scope.
(G2) The minimal unit that expresses negation orspeculation is annotated as a cue.
(E3) ????????????.
(The stock is very likely to hit limit up.
)To G2, the modifiers such as prepositions, de-terminers, or adverbs are not annotated as partsof the cue.
For example, in Sentence (E3), ??(very)?
is only a modifier of the speculative cue???
(likely)?, but not a constituent of the cue.For the drawbacks of the Bioscope corpusguidelines either on itself or for Chinese lan-guage, we introduced some modifications.
Thesemain changes are summarized below:(G3) A cue is annotated only relying on its actualsemantic in context.
(E4) ?????????????.
(It is not possible that the broader marketopens high but slips later again.
)To G3, ????
(not possible)?
means that theauthor denies the possibility of the situation that?the broader market opens high but slips lateragain?, which contains negative meanings thanspeculative.
Thus, the phrase ????
(not possi-ble)?
should be labeled as a negative cue.
(G4) A scope should contain the subject whichcontributes to the meaning of the contentbeing negated or speculated if possible.
(E5) *Once again, the Disorder module does[not contribute positively to the prediction].The BioScope corpus suggests that the scopeof negative adverbs usually starts with the cueand ends at the end of the phrase, clause or sen-tence (E5).
However, in our view, the scopeshould contain the subject for the integrity ofmeaning.
Following is an exceptional case.
(G5) Scope should be a continuous fragment insentence.
(E6) ??????????,???[????????????].
(The hotel are furnished with upscale facili-ties, but [cannot offer us one more pillow].
)Some rhetoric in Chinese language, such asparallelism or ellipsis, often gives rise to separa-tion of some sentence constituents from others.For example, in Sentence (E6), the subject of thesecond clause should be ???
(the hotel)?,which is omitted.
In this situation, we only needto identify the negative or speculative part in sen-tence than all semantic constituents which can becompleted through other NLP technologies, suchas zero subject anaphora resolution or semanticrole labeling.
(G6) A negative or speculative character or wordmay not be a cue.
(E7) ???????????.
(We are difficult not to give credit to thevariety of morning snack.
)We have come across several cases where thepresence of a negative or speculative character orword does not denote negative or speculativemeaning.
For example, there are lots of doublenegatives in Chinese language only for empha-sizing than negative meanings.
In Sentence (E7),obviously, the author wants to emphasis thepraise of the variety of breakfast buffet by using658the phrase ????
(be difficult not to)?
whichdoes not imply a negative meaning.The CNeSp corpus is annotated by two inde-pendent annotators who are not allowed to com-municate with each other.
A linguist expert re-solves the differences between the two annota-tors and modified the guidelines when they areconfronted with problematic issues, yielding thegold standard labeling of the corpus.3.3 Statistics and Agreement AnalysisTable 1 summarizes the chief characteristics ofthe three sub-corpora, including Scientific Litera-ture (Sci., for short), Financial Article (Fin.
), andProduct Review (Prod.).
As shown in Table 1,out of the total amount of 16,841 sentences morethan 20% contained negation or speculation, con-firming the availability for corpus.Item Sci.
Fin.
Prod.#Documents 19 311 821#Sentences 4,630 7,213 4,998Avg.
Length of Sentences 30.4 30.7 24.1Negation%Sentence 13.2 17.5 52.9Avg.
Length of Scopes 9.1 7.2 5.1Speculation%Sentence 21.6 30.5 22.6Avg.
Length of Scopes 12.3 15.0 6.9(Avg.
Length: The average number of Chinese characters.
)Table 1.
Statistics of corpus.Type Sci.
Fin.
Prod.Negation Cue 0.96 0.96 0.93 Cue & Scope 0.90 0.91 0.88Speculation Cue 0.94 0.90 0.93 Cue & Scope 0.93 0.85 0.89Table 2.
Inter-annotator agreement.We measured the inter-annotator agreement ofannotating cues and their linguistic scope for allof three sub-corpora between the two independ-ent annotators in terms of Kappa (Cohen, 1960).The results are shown in Table 2.
The 2nd and4th rows of the table show the kappa value ofonly cue annotation for negation and speculation,respectively.
The 3rd and 5th rows show theagreement rate for both cue and its full scope.The most obvious conclusions here are that theidentification of speculation is more complicatedthan negation even for humans because of thehigher ambiguity of cues and the longer averagelength of scopes in speculation.4 Chinese Negation and SpeculationIdentificationAs a pipeline task, negation and speculationidentification generally consists of two basicstages, cue detection and scope resolution.
Theformer detects whether a word or phrase impliesnegative or speculative meanings, while the latterdetermines the sequences of terms which aredominated by the corresponding cue in sentence.In this section, we improve our cue detectionsystem by using the morpheme features of Chi-nese characters and expanding the cue clustersbased on bilingual parallel corpora.
Then, wepresent a new syntactic structure-based frame-work for Chinese language, which regards thesub-structures of dependency tree selected by aheuristic rule as scope candidates.4.1 Cue DetectionMost of the existing cue detection approaches areproposed from feature engineering perspective.They formulate cue detection as a classificationissue, which is to classify each token in sentenceas being the element of cue or not.Feature-based sequence labeling modelAt the beginning, we explore the performance ofan English cue detection system, as described inAgarwal and Yu (2010), which employs a condi-tional random fields (abbr., CRFs) model withlexical and syntactic features.
Unfortunately, theperformance is very low on Chinese texts (Sec-tion 5.1).
This may be attributed to the differentcharacteristic of Chinese language, for example,no word boundaries and lack of morphologicvariations.
Such low performance drives us toinvestigate new effective features which are par-ticularly appropriate for Chinese.
We employedthree kinds of features for cue detection:1) N-gram featuresFor each character ci, assuming its 5-windowscharacters are ci-2 ci-1 ci ci+1 ci+2, we adopt follow-ing features: ci-2, ci-1, ci, ci+1, ci+2, ci-1ci, cici+1, ci-2ci-1ci, ci-1cici+1, cici+1ci+2.2) Lexical featuresTo achieve high performance as much as pos-sible, we also use some useful basic featureswhich are widely used in other NLP tasks onChinese.
The basic feature set consists of POStag, the left/right character and its PoS tag.
It isworth noting that the cue candidates in our modelare characters.
Thus, in order to get these fea-tures, we substitute them with corresponding fea-tures of the words which contain the characters.3) Morpheme featuresThe word-formation of Chinese implies thatalmost all of the meanings of a word are made upby the morphemes, a minimal meaningful unit inChinese language contained in words.
This more659fine-grained semantics are the compositional se-mantics inside Chinese words namely.
We as-sume that the morphemes in a given cue are alsolikely to be contained in other cues.
For example,???(guess)?
is a given speculative cue whichconsists of ??
(guess)?
and ??
(speculate)?,while the morpheme ??(guess)?
could be ap-peared in ???(suppose)?.
In consideration ofthe Chinese characteristics, we use every poten-tial character in cues to get the morpheme feature.A Boolean feature is taken to represent themorpheme information.
Specifically, the charac-ters which appear more than once within differ-ent cues in training corpus were selected as thefeatures.
The morpheme feature is set to 1, if thecharacter is a negative or speculative morpheme.For the ability of capturing the local infor-mation around a cue, we choose CRFs, a condi-tional sequence model which represents theprobability of a hidden state sequence givensome observations, as classifier to label eachcharacter with a tag indicating whether it is outof a cue (O), the beginning of the cue (B) or apart of the cue except the beginning one (I).
Inthis way, our CRFs-based cue identifier performssequential labeling by assigning each characterone of the three tags and a character assignedwith tag B is concatenated with following char-acters with tag I to form a cue.Cross-lingual Cue Expansion StrategyThe feature-based cue detection approach men-tioned above shows that a bottleneck lies in lowrecall (see Table 4).
This is probably due to theabsence of about 12% negation cues and 17%speculation cues from the training data.
It is achallenging task to identify unknown cues withthe limited amount of training data.
Hence, wepropose a cross-lingual cue expansion strategy.In the approach, we take use of the top 5 Chi-nese cues in training corpus as our ?anchor set?.For each cue, we search its automatically alignedEnglish words from a Chinese-English parallelcorpus to construct an English word cluster.
Theparallel corpus consisting of 100,000 sentencepairs is built by using Liu's approach (Liu et al,2014), which combines translation model withlanguage model to select high-quality translationpairs from 16 million sentence pairs.
The wordalignment was obtained by running Giza++ (Ochand Ney, 2003).
In each cluster, we record thefrequency of each unique English word.
Consid-ering the word alignment errors in cross-lingualclusters, we filter the clusters by word alignmentprobability which is formulated as below:( | ) (1 ) ( | )?
??
?
?A E C C EP P w w P w w( , ) ( , )(1 )( ) ( )?
??
?
?E C E CC EP w w P w wP w P w( , ) ( , )(1 )( , ) ( , )?
??
?
??
?E C E CEi C Ci Ei ialign w w align w walign w w align w w       (1)where ( | )E CP w w  is the translation probability ofEnglish word wE conditioned on Chinese wordwC, reversely, while ( | )C EP w w  is the translationprobability of Chinese word wC conditioned onEnglish word wE.
( , )m nalign w w  is the number ofalignments of word wm and word wn in parallelcorpus.
?i ( , )mi nalign w w  is the sum of the num-ber of alignments which contain word wn.
Theparameter ??
[0,1] is the coefficient controllingthe relative contributions from the two directionsof translation probability.Then we conduct the same procedure in theother direction to construct Chinese word clus-ters anchored by English cues, until no new wordcomes about.
For example, applying the aboveapproach from the cue ???
(may)?, we obtain59 Chinese speculative cues.
All of words in thefinal expansion cluster are identified as cues.4.2 Scope ResolutionCurrently, mainstream approaches formulatedthe scope resolution as a chunking problem,which classifies every word of a sentence as be-ing inside or outside the scope of a cue.
However,unlike in English, we found that plenty of errorsoccurred in Chinese scope resolution by usingwords as the basic identifying candidate.In this paper we propose a new framework us-ing the sub-structures of dependency tree asscope candidates.
Specifically, given a cue, weadopt the following heuristic rule to get the scopecandidates in the dependency tree.Setting constituent X and its siblings as the rootnodes of candidate structure of scope, X shouldbe the ancestor node of cue or cue itself.For example, in the sentence ?????????????????????
(All of guestssaid that they would not investigate the derelic-tion of hotel)?, the negative cue ??(not)?
hasfour constituent Xs and seven scope candidates,as shown in Figure 1.
According to the aboverule, three ancestor nodes {Xa: ???
(said)?, Xb:???
(investigate)?, and Xc: ??(would)?}
cor-respond to three scope candidates (a, b1, and c),660Figure 1.
Examples of a negative cue and its seven scope candidates in dependency tree.Feature Description InstantiationCue:C1: Itself Tokens of cue ?
(not)C2: PoS PoS of cue d(adverb)Scope candidate:S1: Itself Tokens of headword ??
(investigate)S2: PoS PoS of headword v(verb)S3: Dependency type Dependency type of headword VOBS4: Dependency type of child nodes Dependency type of child nodes of headword ADV+VOBS5: Distance<candidate, left word> Number of dependency arcs between the first word of can-didate and its left word3S6: Distance<candidate, right word> Number of dependency arcs between the last word of can-didate and its right word0Relationship between cue and scope candidate:R1: Path Dependency relation path from cue to headword ADV-ADVR2: Distance<cue, headword> Number of dependency arcs between cue and headword 2R3: Compression path Compression version of path ADVR4: Position Positional relationship of cue with scope candidate L_N(Left-nested)Table 3.
Features and their instantiations for scope resolution.and the cue itself is certainly a scope candidate(d).
In addition, the Xb node has two siblings independency tree {???(guests)?
and ??(allof)?}.
Therefore, the two scope candidates cor-responding to them are b2 and b3, respectively.Similarly, the sibling of the Xc node is labeled ascandidate c2.A binary classifier is applied to determineeach candidate as either part of scope or not.
Inthis paper, we employ some lexical and syntacticfeatures about cue and candidate.
Table 3 lists allof the features for scope resolution classification(with candidate b1 as the focus constituent (i.e.,the scope candidate) and ??(not)?
as the giv-en cue, regarding candidate b1 in Figure 1(2)).For clarity, we categorize the features into threegroups according to their relevance with the giv-en cue (C, in short), scope candidate (S, in short),and the relationship between cue andcandidate(R, in short).
Figure 2 shows four kinds of posi-tional features between cue and scope candidatewe defined (R4).Figure 2.
Positional features.Some features proposed above may not be ef-fective in classification.
Therefore, we adopt agreedy feature se-lection algorithm as describedin (Jiang and Ng, 2006) to pick up positive fea-tures incrementally according to their contribu-661tions on the development data.
Additionally, acue should have one continuous block as itsscope, but the scope identifier may result in dis-continuous scope due to independent candidatein classification.
For this reason, we employ apost-processing algorithm as described in Zhu etal.
(2010) to identify the boundaries.5 ExperimentationIn this section, we evaluate our feature-basedsequence labeling model and cross-lingual cueexpansion strategy on cue detection, and reportthe experimental results to justify the appropri-ateness of our syntactic structure-based frame-work on scope resolution in Chinese language.The performance is measured by Precision (P),Recall (R), and F1-score (F).
In addition, forscope resolution, we also report the accuracy inPCS (Percentage of Correct Scopes), withinwhich a scope is fully correct if the output ofscope resolution system and the correct scopehave been matched exactly.5.1 Cue DetectionResults of the Sequence Labeling ModelEvery sub-corpus is randomly divided into tenequal folds so as to perform ten-fold cross vali-dation.
Lexical features are gained by using anopen-source Chinese language processing plat-form, LTP1(Che et al, 2010) to perform wordsegmentation, POS tagging, and syntactic pars-ing.
CRF++0.582 toolkit is employed as our se-quence labeling model for cue detection.Table 4 lists the performances of cue detectionsystems using a variety of features.
It shows thatthe morpheme features derived from the word-formation of Chinese improve the performancefor both negation and speculation cue detectionsystems on all kinds of sub-corpora.
However,the one exception occurs in negation cue detec-tion on the Product Review sub-corpus, in whichthe performance is decreased about 4.55% inprecision.
By error analysis, we find out the mainreason is due to the pseudo cues.
For example,???(very)?
is identified by the negative mor-pheme ??
(-un)?, which is a pseudo cue.Table 4 also shows a bottleneck of our se-quence labeling model, which lies in low recall.Due to the diversity of Chinese language, manycues only appear a few times in corpus.
For ex-1 http://www.ltp-cloud.com2 https://crfpp.googlecode.com/svn/trunk/doc/index.htmlample, 83% (233/280) of speculative cues appearless than ten times in Financial Article sub-corpus.
This data sparse problem directly leads tothe low recall of cue detection.Negation SpeculationSci.
P R F1 P R F1Agarwal?s 48.75 36.44 41.71 46.16 33.49 38.82N-gram 64.07 49.64 55.94 62.15 42.87 50.74+Lexical 76.68 57.36 65.63 70.47 48.31 57.32+Morpheme 81.37 59.11 68.48 76.91 50.77 61.16Fin.Agarwal?s 41.93 39.15 40.49 50.39 42.80 46.29N-gram 56.05 45.48 50.21 60.37 44.16 51.01+Lexical 71.61 50.12 58.97 68.96 48.72 57.10+Morpheme 78.94 53.37 63.68 75.43 51.29 61.06Prod.Agarwal?s 58.47 47.31 52.30 45.88 34.13 39.14N-gram 71.33 54.69 61.91 49.38 39.31 43.77+Lexical 86.76 65.41 74.59 64.85 44.63 52.87+Morpheme 82.21 66.82 73.72 70.06 45.31 55.03Table 4.
Contribution of features to cue detection.Results of the Cross-lingual Cue ExpansionStrategyBefore cue expansion, we select the parameter ?as defined in formula (1) by optimizing the F1-measure score of on Financial Article sub-corpus.Figure 3 shows the effect on F1-measure of vary-ing the coefficient from 0 to 1.
We can see thatthe best performance can be obtained by select-ing parameter 0.6 for negation and 0.7 for specu-lation.
Then we apply these parameter valuesdirectly for cue expansion.Figure 3.
The effect of varying the value of pa-rameter ?
on Financial Article sub-corpus.Table 5 lists the performances of feature-basedsystem, expansion-based system, and the com-bined system.
A word is identified as a cue bycombined system if it is identified by one of theabove systems (Feat-based or Exp-based) at least.For both negation and speculation, the cross-lingual cue expansion approach provides signifi-cant improvement over the feature-based se-quence labeling model, achieving about 15-20%662better recall with little loss in precision.
Moreimportantly, the combined system obtains thebest performance.Negation SpeculationSci.
P R F1 P R F1Feat-based 81.37 59.11 68.48 76.91 50.77 61.16Exp-based 68.29 76.24 72.05 62.74 68.07 65.30Combined 75.17 78.91 76.99 70.98 75.71 73.27Fin.Feat-based 78.94 53.37 63.68 75.43 51.29 61.06Exp-based 70.31 64.49 67.27 67.46 68.78 68.11Combined 72.77 67.02 69.78 71.60 69.03 70.29Prod.Feat-based 82.21 66.82 73.72 70.06 45.31 55.03Exp-based 78.30 86.47 82.18 62.18 63.47 62.82Combined 81.94 89.23 85.43 67.56 69.61 68.57Table 5.
Performance of cue detection.5.2 Syntactic Structure-based Scope Reso-lutionConsidering the effectiveness of different fea-tures, we divide the Financial Article sub-corpusinto 5 equal parts, within which 2 parts are usedfor feature selection.
Then, the feature selectiondata are divided into 5 equal parts, within which4 parts for training and the rest for developing.On this data set, a greedy feature selection algo-rithm (Jiang and Ng, 2006) is adopted to pick uppositive features proposed in Table 3.
In addition,SVMLight3 with the default parameter is selectedas our classifier.Table 6 lists the performance of selected fea-tures.
7 features {C1, C2, S4, S5, S6, R1, R4}are selected consecutively for negation scoperesolution, while 9 features {C2, S1, S3, S4, S5,R1, R2, R3, R4} are selected for speculationscope resolution.
We will include those selectedfeatures in all the remaining experiments.Type Feature set Sci.
Fin.
Prod.Negation Selected features 62.16 56.07 60.93All features 59.74 54.20 55.42Speculation Selected features 54.16 49.64 52.89All features 52.33 46.27 48.07Table 6.
Feature selection for scope resolution ongolden cues (PCS %).The feature selection experiments suggest thatthe feature C2 (POS of cue) plays a critical rolefor both negation and speculation scope resolu-tion.
It may be due to the fact that cues of differ-ent POS usually undertake different syntacticroles.
Thus, there are different characteristics intriggering linguistic scopes.
For example, an ad-jective cue may treat a modificatory structure as3 http://svmlight.joachims.orgits scope, while a conjunction cue may take thetwo connected components as its scope.As a pipeline task, the negation and specula-tion identification could be regarded as a combi-nation of two sequential tasks: first, cue detection,and then scope resolution.
Hence, we turn to amore realistic scenario in which cues are auto-matically recognized.Type Corpus P R F1 PCSNegationSci.
55.32 53.06 54.17 59.08Fin.
42.14 46.37 44.15 49.24Prod.
50.57 48.55 49.54 52.17SpeculationSci.
45.68 47.15 46.40 48.36Fin.
34.21 31.80 32.96 41.33Prod.
32.64 33.59 33.11 39.78Table 7.
Performance of scope resolution withautomatic cue detection.Table 7 lists the performance of scope resolu-tion by using automatic cues.
It shows that auto-matic cue detection lowers the performance by3.08, 6.83, and 8.76 in PCS for the three sub-corpora, respectively; while it lowers the perfor-mance by 5.80, 8.31 and 13.11 in PCS for specu-lation scope resolution on the three sub-corpora,respectively (refer to Table 6).
The main reasonof performance lost is the error propagation fromthe automatic cue detection.We employ a start-of-the-art chunking-basedscope resolution system (described in Zou et al,(2013)) as a baseline, in which every word insentence has been labelled as being the elementof the scope or not.
Table 8 compares our syntac-tic structure-based framework with the chunking-based framework on scope resolution.
Note thatall the performances are achieved on FinancialArticle sub-corpus by using golden cues.
Theresults in Table 8 shows that our scope resolutionsystem outperforms the chunking ones both onnegation and speculation, improving 8.75 and7.44 in PCS, respectively.Type System PCSNegation Chunking-based 47.32 Ours 56.07Speculation Chunking-based 42.20 Ours 49.64Table 8.
Comparison with the chunking-basedsystem on Financial Article sub-corpus.6 ConclusionIn this paper we construct a Chinese corpus fornegation and speculation identification, whichannotates cues and their linguistic scopes.
Forcue detection, we present a feature-based se-quence labeling model, in which the morpheme663feature is employed to better catch the com-position semantics inside the Chinese words.Complementally, a cross-lingual cue expansionstrategy is pro-posed to increase the coverage incue detection.
For scope resolution, we present anew syntactic structure-based framework to iden-tify the linguistic scope of a cue.
Evaluation jus-tifies the usefulness of our Chinese corpus andthe appropriateness of the syntactic structure-based framework.
It also shows that our ap-proach outperforms the state-of-the-art chunkingones on negation and speculation identificationin Chinese language.In the future we will explore more effectivefeatures to improve the negation and speculationidentification in Chinese language, and focus onjoint learning of the two subtasks.AcknowledgmentsThis research is supported by the National Natu-ral Science Foundation of China, No.61272260,No.61331011, No.61273320, No.61373097, andthe Major Project of College Natural ScienceFoundation of Jiangsu Province,No.11KJA520003.
The authors would like tothank the anonymous reviewers for their insight-ful comments and suggestions.ReferenceShashank Agarwal and Hong Yu.
2010.
Detectinghedge cues and their scope in biomedical text withconditional random fields.
Journal of BiomedicalInformatics, 43, 953-961.Mordechai Averbuch, Tom H. Karson, BenjaminBen-Ami, Oded Maimon, and Lior Rokach.
2004.Context-sensitive medical information retrieval.
InProceedings of the 11th World Congress on Medi-cal Informatics (MEDINFO?04), 1-8.Kathrin Baker, Michael Bloodgood, Bonnie Dorr,Nathaniel W. Filardo, Lori Levin, and Christine Pi-atko.
2010.
A modality lexicon and its use in au-tomatic tagging.
In Proceedings of the SeventhConference on International Language Resourcesand Evaluation (LREC?10), 1402-1407.BioScope.
2008.
Annotation guidelines.http://www.inf.u-szeged.hu/rgai/project/nlp/bioscope/Annotationguidelines2.1.pdfWanxiang Che, Zhenghua Li, Ting Liu.
2010.
LTP: AChinese language technology platform.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics (COLING'10): Demon-strations, 13-16.Md.
Faisal Mahbub Chowdhury and Alberto Lavelli.2013.
Exploiting the scope of negations and heter-ogeneous features for relation extraction: A casestudy for drug-drug interaction extraction.
In Pro-ceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT'13), 765-771.Jacob Cohen.
1960.
A coefficient of agreement fornominal scales.
Educational and PsychologicalMeasurement, 20, 37-46.Isaac Councill, Ryan McDonald, and Leonid Ve-likovich.
2010.
What?s great and what?s not:Learning to classify the scope of negation for im-proved sentiment analysis.
In Proceedings of theWorkshop on Negation and Speculation in NaturalLanguage Processing, 51-59.Zhengping Jiang and Hwee T. Ng.
2006.
Semanticrole labeling of NomBank: A maximum entropyapproach.
In Proceedings of the Human LanguageTechnology Conference and Conference on Empir-ical Methods in Natural Language Processing(EMNLP?06), 138-145.Le Liu, Yu Hong, Hao Liu, Xing Wang, and JianminYao.
2014.
Effective selection of translation modeltraining data.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics (ACL'14), Short Papers, 569-573.Feng Ji, Xipeng Qiu, Xuanjing Huang.
2010.
Explor-ing uncertainty sentences in Chinese.
In Proceed-ings of the 16th China Conference on InformationRetreval, 594-601.Roser Morante, Anthony Liekens, and Walter Daele-mans.
2008.
Learning the scope of negation in bi-omedical texts.
In Proceedings of the Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Pro-cessing (EMNLP?08), 715-724.Roser Morante and Caroline Sporleder.
2012.
Modali-ty and negation: an introduction to the special issue.Comput.
Linguist.
38, 2, 223-260.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignmentmodels.
Comput.
Linguist.
29, 1, 19-51.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: sentiment analysis using subjectivity.
In Pro-ceedings of the 42nd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL'04),271-278.Rion Snow, Lucy Vanderwende, and Arul Menezes.2006.
Effectively using syntax for recognizingfalse entailment.
In Proceedings of the Main Con-ference on Human Language Technology Confer-ence of the North American Chapter of the Associ-664ation of Computational Linguistics (HLT-NAACL?06), 33-40.Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas,Gy?rgy M?ra and J?nos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for un-certainty, negation and their scopes.
BMC Bioin-formatics, 9(Suppl 11):S9.Dominikus Wetzel, and Francis Bond.
2012.
Enrich-ing parallel corpora for statistical machine transla-tion with semantic negation rephrasing.
In Pro-ceedings of the 6th Workshop on Syntax, Semanticsand Structure in Statistical Translation, 20-29.Qiaoming Zhu, Junhui Li, Hongling Wang, andGuodong Zhou.
2010.
A Unified Framework forScope Learning via Simplified Shallow SemanticParsing.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Pro-cessing (EMNLP?10), 714-724.Xiaodan Zhu, Hongyu Guo, Saif Mohammad, andSvetlana Kiritchenko.
2014.
An empirical study onthe effect of negation words on sentiment.
In Pro-ceedings of the 52nd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL?14),304-313.Bowei Zou, Guodong Zhou, and Qiaoming Zhu.
2013.Tree kernel-based negation and speculation scopedetection with structured syntactic parse features.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing(EMNLP?13), 968-976.665
