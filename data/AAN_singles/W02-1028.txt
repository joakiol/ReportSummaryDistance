A Bootstrapping Method for Learning Semantic Lexicons usingExtraction Pattern ContextsMichael Thelen and Ellen RiloSchool of ComputingUniversity of UtahSalt Lake City, UT 84112 USAfthelenm,rilog@cs.utah.eduAbstractThis paper describes a bootstrapping al-gorithm called Basilisk that learns high-quality semantic lexicons for multiple cate-gories.
Basilisk begins with an unannotatedcorpus and seed words for each semanticcategory, which are then bootstrapped tolearn new words for each category.
Basiliskhypothesizes the semantic class of a wordbased on collective information over a largebody of extraction pattern contexts.
Weevaluate Basilisk on six semantic categories.The semantic lexicons produced by Basiliskhave higher precision than those producedby previous techniques, with several cate-gories showing substantial improvement.1 IntroductionIn recent years, several algorithms have been devel-oped to acquire semantic lexicons automatically orsemi-automatically using corpus-based techniques.For our purposes, the term semantic lexicon will referto a dictionary of words labeled with semantic classes(e.g., \bird" is an animal and \truck" is a vehicle).Semantic class information has proven to be usefulfor many natural language processing tasks, includ-ing information extraction (Rilo and Schmelzen-bach, 1998; Soderland et al, 1995), anaphora resolu-tion (Aone and Bennett, 1996), question answering(Moldovan et al, 1999; Hirschman et al, 1999), andprepositional phrase attachment (Brill and Resnik,1994).
Although some semantic dictionaries do exist(e.g., WordNet (Miller, 1990)), these resources oftendo not contain the specialized vocabulary and jargonthat is needed for specic domains.
Even for rela-tively general texts, such as the Wall Street Journal(Marcus et al, 1993) or terrorism articles (MUC-4 Proceedings, 1992), Roark and Charniak (Roarkand Charniak, 1998) reported that 3 of every 5 termsgenerated by their semantic lexicon learner were notpresent in WordNet.
These results suggest that auto-matic semantic lexicon acquisition could be used toenhance existing resources such as WordNet, or toproduce semantic lexicons for specialized domains.We have developed a weakly supervised bootstrap-ping algorithm called Basilisk that automaticallygenerates semantic lexicons.
Basilisk hypothesizesthe semantic class of a word by gathering collectiveevidence about semantic associations from extractionpattern contexts.
Basilisk also learns multiple se-mantic classes simultaneously, which helps constrainthe bootstrapping process.First, we present Basilisk?s bootstrapping algo-rithm and explain how it diers from previous workon semantic lexicon induction.
Second, we presentempirical results showing that Basilisk outperformsa previous algorithm.
Third, we explore the idea oflearning multiple semantic categories simultaneouslyby adding this capability to Basilisk as well as an-other bootstrapping algorithm.
Finally, we presentresults showing that learning multiple semantic cat-egories simultaneously improves performance.2 Bootstrapping using CollectiveEvidence from Extraction PatternsBasilisk (Bootstrapping Approach to SemantIcLexicon Induction using Semantic Knowledge) is aweakly supervised bootstrapping algorithm that au-tomatically generates semantic lexicons.
Figure 1shows the high-level view of Basilisk?s bootstrappingprocess.
The input to Basilisk is an unannotatedtext corpus and a few manually dened seed wordsfor each semantic category.
Before bootstrappingbegins, we run an extraction pattern learner overthe corpus which generates patterns to extract ev-ery noun phrase in the corpus.The bootstrapping process begins by selecting asubset of the extraction patterns that tend to ex-tract the seed words.
We call this the pattern pool.Association for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
214-221.Proceedings of the Conference on Empirical Methods in NaturalThe nouns extracted by these patterns become can-didates for the lexicon and are placed in a candidateword pool.
Basilisk scores each candidate word bygathering all patterns that extract it and measur-ing how strongly those contexts are associated withwords that belong to the semantic category.
Theve best candidate words are added to the lexicon,and the process starts over again.
In this section, wedescribe Basilisk?s bootstrapping algorithm in moredetail and discuss related work.extraction patterns andtheir extractionsadd extractions ofbest patternsbest patternsselectpattern poolsemanticlexiconwordsseedcandidateword poolBASILISKinitializeadd 5 best candidate wordsFigure 1: Basilisk Algorithm2.1 BasiliskThe input to Basilisk is a text corpus and a set of seedwords.
We generated seed words by sorting the wordsin the corpus by frequency and manually identifyingthe 10 most frequent nouns that belong to each cat-egory.
These seed words form the initial semanticlexicon.
In this section we describe the learning pro-cess for a single semantic category.
In Section 3 wewill explain how the process is adapted to handlemultiple categories simultaneously.To identify new lexicon entries, Basilisk relieson extraction patterns to provide contextual evi-dence that a word belongs to a semantic class.
Asour representation for extraction patterns, we usedthe AutoSlog system (Rilo, 1996).
AutoSlog?sextraction patterns represent linguistic expressionsthat extract a noun phrase in one of three syntac-tic roles: subject, direct object, or prepositionalphrase object.
For example, three patterns thatwould extract people are: \<subject> was arrested",\murdered <direct object>", and \collaborated with<pp object>".
Extraction patterns represent linguis-tic contexts that often reveal the meaning of a wordby virtue of syntax and lexical semantics.
Extractionpatterns are typically designed to capture role rela-tionships.
For example, consider the verb \robbed"when it occurs in the active voice.
The subject of\robbed" identies the perpetrator, while the directobject of \robbed" identies the victim or target.Before bootstrapping begins, we run AutoSlog ex-haustively over the corpus to generate an extractionGenerate all extraction patterns in the corpusand record their extractions.lexicon = fseed wordsgi := 0BOOTSTRAPPING1.
Score all extraction patterns2.
pattern pool = top ranked 20+i patterns3.
candidate word pool = extractionsof patterns in pattern pool4.
Score candidate words in candidate word pool5.
Add top 5 candidate words to lexicon6.
i := i + 17.
Go to Step 1.Figure 2: Basilisk?s bootstrapping algorithmpattern for every noun phrase that appears.
Thepatterns are then applied to the corpus and all oftheir extracted noun phrases are recorded.
Figure 2shows the bootstrapping process that follows, whichwe explain in the following sections.2.1.1 The Pattern Pool and Candidate PoolThe rst step in the bootstrapping process is toscore the extraction patterns based on their tendencyto extract known category members.
All words thatare currently dened in the semantic lexicon are con-sidered to be category members.
Basilisk scores eachpattern using the RlogF metric that has been usedfor extraction pattern learning (Rilo, 1996).
Thescore for each pattern is computed as:RlogF (patterni) =FiNi log2(Fi) (1)where Fiis the number of category members ex-tracted by patterniand Niis the total number ofnouns extracted by patterni.
Intuitively, the RlogFmetric is a weighted conditional probability; a pat-tern receives a high score if a high percentage of itsextractions are category members, or if a moderatepercentage of its extractions are category membersand it extracts a lot of them.The top N extraction patterns are put into a pat-tern pool.
Basilisk uses a value of N=20 for the rstiteration, which allows a variety of patterns to beconsidered, yet is small enough that all of the pat-terns are strongly associated with the category.11\Depleted" patterns are not included in this set.
Apattern is depleted if all of its extracted nouns are alreadydened in the lexicon, in which case it has no unclassiedwords to contribute.The purpose of the pattern pool is to narrow downthe eld of candidates for the lexicon.
Basilisk col-lects all noun phrases (NPs) extracted by patterns inthe pattern pool and puts the head noun of each NPinto the candidate word pool.
Only these nouns areconsidered for addition to the lexicon.As the bootstrapping progresses, using the samevalue N=20 causes the candidate pool to becomestagnant.
For example, let?s assume that Basiliskperforms perfectly, adding only valid category wordsto the lexicon.
After some number of iterations, allof the valid category members extracted by the top20 patterns will have been added to the lexicon, leav-ing only non-category words left to consider.
For thisreason, the pattern pool needs to be infused with newpatterns so that more nouns (extractions) becomeavailable for consideration.
To achieve this eect,we increment the value of N by one after each boot-strapping iteration.
This ensures that there is alwaysat least one new pattern contributing words to thecandidate word pool on each successive iteration.2.1.2 Selecting Words for the LexiconThe next step is to score the candidate words.
Foreach word, Basilisk collects every pattern that ex-tracted the word.
All extraction patterns are usedduring this step, not just the patterns in the pat-tern pool.
Initially, we used a scoring function thatcomputes the average number of category membersextracted by the patterns.
The formula is:score(wordi) =PiXj=1FjPi(2)where Piis the number of patterns that extractwordi, and Fjis the number of distinct categorymembers extracted by pattern j.
A word receivesa high score if it is extracted by patterns that alsohave a tendency to extract known category members.As an example, suppose the word \Peru" is in thecandidate word pool as a possible location.
Basilisknds all patterns that extract \Peru" and computesthe average number of known locations extracted bythose patterns.
Let?s assume that the three patternsshown below extract \Peru" and that the underlinedwords are known locations.
\Peru" would receive ascore of (2+3+2)/3 = 2.3.
Intuitively, this meansthat patterns that extract \Peru" also extract, onaverage, 2.3 known location words.\was killed in <np>"Extractions: Peru, clashes, a shootout, El Salvador,Colombia\<np> was divided"Extractions: the country, the Medellin cartel, Colombia,Peru, the army, Nicaragua\ambassador to <np>"Extractions: Nicaragua, Peru, the UN, PanamaUnfortunately, this scoring function has a problem.The average can be heavily skewed by one patternthat extracts a large number of category members.For example, suppose word w is extracted by 10 pat-terns, 9 which do not extract any category membersbut the tenth extracts 50 category members.
Theaverage number of category members extracted bythese patterns will be 5.
This is misleading becausethe only evidence linking word w with the semanticcategory is a single, high-frequency extraction pat-tern (which may extract words that belong to othercategories as well).To alleviate this problem, we modied the scor-ing function to compute the average logarithm of thenumber of category members extracted by each pat-tern.
The logarithm reduces the influence of any sin-gle pattern.
We will refer to this scoring metric asthe AvgLog function, which is dened below.
Sincelog2(1) = 0, we add one to each frequency count sothat patterns which extract a single category mem-ber contribute a positive value.AvgLog(wordi) =PiXj=1log2(Fj+ 1)Pi(3)Using this scoring metric, all words in the candi-date word pool are scored and the top ve words areadded to the semantic lexicon.
The pattern pool andthe candidate word pool are then emptied, and thebootstrapping process starts over again.2.1.3 Related WorkSeveral weakly supervised learning algorithmshave previously been developed to generate seman-tic lexicons from text corpora.
Rilo and Shepherd(Rilo and Shepherd, 1997) developed a bootstrap-ping algorithm that exploits lexical co-occurrencestatistics, and Roark and Charniak (Roark andCharniak, 1998) rened this algorithm to focus moreexplicitly on certain syntactic structures.
Hale, Ge,and Charniak (Ge et al, 1998) devised a techniqueto learn the gender of words.
Caraballo (Caraballo,1999) and Hearst (Hearst, 1992) created techniquesto learn hypernym/hyponym relationships.
None ofthese previous algorithms used extraction patterns orsimilar contexts to infer semantic class associations.Several learning algorithms have also been de-veloped for named entity recognition (e.g., (Collinsand Singer, 1999; Cucerzan and Yarowsky, 1999)).
(Collins and Singer, 1999) used contextual informa-tion of a dierent sort than we do.
Furthermore, ourresearch aims to learn general nouns (e.g., \artist")rather than proper nouns, so many of the featurescommonly used to great advantage for named entityrecognition (e.g., capitalization and title words) arenot applicable to our task.The algorithm most closely related to Basilisk ismeta-bootstrapping (Rilo and Jones, 1999), whichalso uses extraction pattern contexts for semanticlexicon induction.
Meta-bootstrapping identies asingle extraction pattern that is highly correlatedwith a semantic category and then assumes that all ofits extracted noun phrases belong to the same cat-egory.
However, this assumption is often violated,which allows incorrect terms to enter the lexicon.Rilo and Jones acknowledged this issue and useda second level of bootstrapping (the \Meta" boot-strapping level) to alleviate this problem.
Whilemeta-bootstrapping trusts individual extraction pat-terns to make unilateral decisions, Basilisk gath-ers collective evidence from a large set of extrac-tion patterns.
As we will demonstrate in Sec-tion 2.2, Basilisk?s approach produces better re-sults than meta-bootstrapping and is also consid-erably more ecient because it uses only a singlebootstrapping loop (meta-bootstrapping uses nestedbootstrapping).
However, meta-bootstrapping pro-duces category-specic extraction patterns in addi-tion to a semantic lexicon, while Basilisk focuses ex-clusively on semantic lexicon induction.2.2 Single Category ResultsTo evaluate Basilisk?s performance, we ran experi-ments with the MUC-4 corpus (MUC-4 Proceedings,1992), which contains 1700 texts associated with ter-rorism.
We used Basilisk to learn semantic lexiconsfor six semantic categories: building, event, hu-man, location, time, and weapon.
Before we ranthese experiments, one of the authors manually la-beled every head noun in the corpus that was foundby an extraction pattern.
These manual annota-tions were the gold standard.
Table 1 shows thebreakdown of semantic categories for the head nouns.These numbers represent a baseline: an algorithmthat randomly selects words would be expected toget accuracies consistent with these numbers.Three semantic lexicon learners have previouslybeen evaluated on the MUC-4 corpus (Rilo andShepherd, 1997; Roark and Charniak, 1998; Riloand Jones, 1999), and of these meta-bootstrappingachieved the best results.
So we implemented themeta-bootstrapping algorithm ourselves to directlyCategory Total Percentagebuilding 188 2.2%event 501 5.9%human 1856 21.9%location 1018 12.0%time 112 1.3%weapon 147 1.7%other 4638 54.8%Table 1: Breakdown of semantic categoriescompare its performance with that of Basilisk.
Adierence between the original implementation andours is that our version learns individual nouns (asdoes Basilisk) instead of noun phrases.
We believethat learning individual nouns is a more conservativeapproach because noun phrases often overlap (e.g.,\high-power bombs" and \incendiary bombs" wouldcount as two dierent lexicon entries in the origi-nal meta-bootstrapping algorithm).
Consequently,our meta-bootstrapping results dier from those re-ported in (Rilo and Jones, 1999).Figure 3 shows the results for Basilisk (ba-1) andmeta-bootstrapping (mb-1).
We ran both algorithmsfor 200 iterations, so that 1000 words were added tothe lexicon (5 words per iteration).
The X axis showsthe number of words learned, and the Y axis showshow many were correct.
The Y axes have dierentranges because some categories are more prolic thanothers.
Basilisk outperforms meta-bootstrapping forevery category, often substantially.
For the humanand location categories, Basilisk learned hundreds ofwords, with accuracies in the 80-89% range throughmuch of the bootstrapping.
It is worth noting thatBasilisk?s performance held up well on the humanand location categories even at the end, achieving79.5% (795/1000) accuracy for humans and 53.2%(532/1000) accuracy for locations.3 Learning Multiple SemanticCategories SimultaneouslyWe also explored the idea of bootstrapping multiplesemantic classes simultaneously.
Our hypothesis wasthat errors of confusion2 between semantic categoriescan be lessened by using information about multi-ple categories.
This hypothesis makes sense only if aword cannot belong to more than one semantic class.In general, this is not true because words are oftenpolysemous.
But within a limited domain, a wordusually has a dominant word sense.
Therefore wemake a \one sense per domain" assumption (similar2We use the term confusion to refer to errors where aword is labeled as category X when it really belongs tocategory Y .01020304050607080901000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesBuildingba-1mb-10501001502002503000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesEventba-1mb-101002003004005006007008000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesHumanba-1mb-10501001502002503003504004505000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesLocationba-1mb-105101520253035400 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesTimeba-1mb-1010203040506070800 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesWeaponba-1mb-1Figure 3: Basilisk and Meta-Bootstrapping Results,Single Categoryto the \one sense per discourse" observation (Gale etal., 1992)) that a word belongs to a single semanticcategory within a limited domain.
All of our ex-periments involve the MUC-4 terrorism domain andcorpus, for which this assumption seems appropriate.3.1 MotivationFigure 4 shows one way of viewing the task of se-mantic lexicon induction.
The set of all words in thecorpus is visualized as a search space.
Each cate-gory owns a certain territory within the space (de-marcated with a dashed line), representing the wordsthat are true members of that category.
Not all ter-ritories are the same size, since some categories havemore members than others.ABCDEF                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Figure 4: Bootstrapping a Single CategoryFigure 4 illustrates what happens when a semanticlexicon is generated for a single category.
The seedwords for the category (in this case, category C) arerepresented by the solid black area in category C?sterritory.
The hypothesized words in the growinglexicon are represented by a shaded area.
The goalof the bootstrapping algorithm is to expand the areaof hypothesized words so that it exactly matches thecategory?s true territory.
If the shaded area expandsbeyond the category?s true territory, then incorrectwords have been added to the lexicon.
In Figure 4,category C has claimed a signicant number of wordsthat belong to categories B and E. When generatinga lexicon for one category at a time, these confusionerrors are impossible to detect because the learnerhas no knowledge of the other categories.ACD EFB                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Figure 5: Bootstrapping Multiple CategoriesFigure 5 shows the same search space when lexi-cons are generated for six categories simultaneously.If the lexicons cannot overlap, then we constrain theability of a category to overstep its bounds.
Cate-gory C is stopped when it begins to encroach uponthe territories of categories B and E because wordsin those areas have already been claimed.3.2 Simple Conflict ResolutionThe easiest way to take advantage of multiple cate-gories is to add simple conflict resolution that en-forces the \one sense per domain" constraint.
Ifmore than one category tries to claim a word, thenwe use conflict resolution to decide which categoryshould win.
We incorporated a simple conflict reso-lution procedure into Basilisk, as well as the meta-bootstrapping algorithm.
For both algorithms, theconflict resolution procedure works as follows.
(1) Ifa word is hypothesized for category A but has alreadybeen assigned to category B during a previous iter-ation, then the category A hypothesis is discarded.
(2) If a word is hypothesized for both category Aand category B during the same iteration, then it0204060801001200 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesBuildingba-Mba-10501001502002503000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesEventba-Mba-101002003004005006007008000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesHumanba-Mba-101002003004005006000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesLocationba-Mba-105101520253035400 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesTimeba-Mba-101020304050607080900 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesWeaponba-Mba-1Figure 6: Basilisk, MCAT vs. 1CATis assigned to the category for which it receives thehighest score.
In Section 3.4, we will present empiri-cal results showing how this simple conflict resolutionscheme aects performance.3.3 A Smarter Scoring Function forMultiple CategoriesSimple conflict resolution helps the algorithmrecognize when it has encroached on another cate-gory?s territory, but it does not actively steer thebootstrapping in a more promising direction.
Amore intelligent way to handle multiple categoriesis to incorporate knowledge about other categoriesdirectly into the scoring function.
We modiedBasilisk?s scoring function to prefer words that havestrong evidence for one category but little or noevidence for competing categories.
Each word wiinthe candidate word pool receives a score for categorycabased on the following formula:di(wi,ca) = AvgLog(wi,ca) - maxb6=a(AvgLog(wi,cb))where AvgLog is the candidate scoring function usedpreviously by Basilisk (see Equation 3) and the maxfunction returns the maximum AvgLog value overall competing categories.
For example, the score foreach candidate location word will be its AvgLogscore for the location category minus its maxi-mum AvgLog score for all other categories.
A wordis ranked highly only if it has a high score for the01020304050607080901000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesBuildingmb-Mmb-10501001502002500 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesEventmb-Mmb-101002003004005006007000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesHumanmb-Mmb-101002003004005006000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesLocationmb-Mmb-10510152025300 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesTimemb-Mmb-101020304050607080901000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesWeaponmb-Mmb-1Figure 7: Meta-Bootstrapping, MCAT vs. 1CATtargeted category and there is little evidence that itbelongs to a dierent category.
This has the eectof steering the bootstrapping process away from am-biguous parts of the search space.3.4 Multiple Category ResultsWe will use the abbreviation 1CAT to indicate thatonly one semantic category was bootstrapped, andMCAT to indicate that multiple semantic categorieswere simultaneously bootstrapped.
Figure 6 com-pares the performance of Basilisk-MCAT with con-flict resolution (ba-M) against Basilisk-1CAT (ba-1).Most categories show small performance gains, withthe building, location, and weapon categoriesbenetting the most.
However, the improvementusually doesn?t kick in until many bootstrapping it-erations have passed.
This phenomenon is consistentwith the visualization of the search space in Figure 5.Since the seed words for each category are not gener-ally located near each other in the search space, thebootstrapping process is unaected by conflict reso-lution until the categories begin to encroach on eachother?s territories.Figure 7 compares the performance of Meta-Bootstrapping-MCAT with conflict resolution(mb-M) against Meta-Bootstrapping-1CAT (mb-1).
Learning multiple categories improves theperformance of meta-bootstrapping dramaticallyfor most categories.
We were surprised that theimprovement for meta-bootstrapping was much0204060801001200 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesBuildingba-M+ba-Mmb-M0501001502002503000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesEventba-M+ba-Mmb-M01002003004005006007008009000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesHumanba-M+ba-Mmb-M01002003004005006000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesLocationba-M+ba-Mmb-M0510152025303540450 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesTimeba-M+ba-Mmb-M01020304050607080901000 200 400 600 800 1000CorrectLexiconEntriesTotal Lexicon EntriesWeaponba-M+ba-Mmb-MFigure 8: MetaBoot-MCAT vs. Basilisk-MCAT vs.Basilisk-MCAT+more pronounced than for Basilisk.
It seems thatBasilisk was already doing a better job with errorsof confusion, so meta-bootstrapping had more roomfor improvement.Finally, we evaluated Basilisk using the di scoringfunction to handle multiple categories.
Figure 8 com-pares all three MCAT algorithms, with the smarterdi version of Basilisk labeled as ba-M+.
Over-all, this version of Basilisk performs best, showinga small improvement over the version with simpleconflict resolution.
Both multiple category versionsof Basilisk also consistently outperform the multiplecategory version of meta-bootstrapping.Table 2 summarizes the improvement of thebest version of Basilisk over the original meta-bootstrapping algorithm.
The left-hand column rep-resents the number of words learned and each cell in-dicates how many of those words were correct.
Theseresults show that Basilisk produces substantially bet-ter accuracy and coverage than meta-bootstrapping.Figure 9 shows examples of words learned byBasilisk.
Inspection of the lexicons reveals many un-usual words that could be easily overlooked by some-one building a dictionary by hand.
For example, thewords \deserter" and \narcoterrorists" appear in avariety of terrorism articles but they are not com-monly used words in general.We also measured the recall of Basilisk?s lexiconsafter 1000 words had been learned, based on the goldTotal MetaBoot BasiliskWords 1CAT MCAT+building100 21 (21.0%) 39 (39.0%)200 28 (14.0%) 72 (36.0%)500 33 (6.6%) 100 (20.0%)800 39 (4.9%) 109 (13.6%)1000 43 (4.3%) n/aevent100 61 (61.0%) 61 (61.0%)200 89 (44.5%) 114 (57.0%)500 146 (29.2%) 186 (37.2%)800 172 (21.5%) 240 (30.0%)1000 190 (19.0%) 266 (26.6%)human100 36 (36.0%) 84 (84.0%)200 53 (26.5%) 173 (86.5%)500 143 (28.6%) 431 (86.2%)800 224 (28.0%) 681 (85.1%)1000 278 (27.8%) 829 (82.9%)location100 54 (54.0%) 84 (84.0%)200 99 (49.5%) 175 (87.5%)500 237 (47.4%) 371 (74.2%)800 302 (37.8%) 509 (63.6%)1000 310 (31.0%) n/atime100 9 (9.0%) 30 (30.0%)200 13 (6.5%) 33 (16.5%)500 21 (4.2%) 37 (7.4%)800 25 (3.1%) 43 (5.4%)1000 26 (2.6%) 45 (4.5%)weapon100 23 (23.0%) 42 (42.0%)200 24 (12.0%) 62 (31.0%)500 29 (5.8%) 85 (17.0%)800 33 (4.1%) 88 (11.0%)1000 33 (3.3%) n/aTable 2: Lexicon Resultsstandard data shown in Table 1.
The recall resultsrange from 40-60%, which indicates that a good per-centage of the category words are being found, al-though there are clearly more category words lurkingin the corpus.4 ConclusionsBasilisk?s bootstrapping algorithm exploits twoideas: (1) collective evidence from extraction pat-terns can be used to infer semantic category associ-ations, and (2) learning multiple semantic categoriessimultaneously can help constrain the bootstrappingprocess.
The accuracy achieved by Basilisk is sub-stantially higher than that of previous techniques forsemantic lexicon induction on the MUC-4 corpus,and empirical results show that both of Basilisk?sideas contribute to its performance.
We also demon-Building: theatre store cathedral temple palacepenitentiary academy houses school mansionsEvent: ambush assassination uprisings sabotagetakeover incursion kidnappings clash shoot-outHuman: boys snipers detainees commandoesextremists deserter narcoterrorists demonstratorscronies missionariesLocation: suburb Soyapango capital Oslo regionscities neighborhoods Quito corregimientoTime: afternoon evening decade hour Marchweeks Saturday eve anniversary WednesdayWeapon: cannon grenade launchers rebombcar-bomb rifle pistol machineguns rearmsFigure 9: Example Semantic Lexicon Entriesstrated that learning multiple semantic categories si-multaneously improves the meta-bootstrapping algo-rithm, which suggests that this is a general observa-tion which may improve other bootstrapping algo-rithms as well.5 AcknowledgmentsThis research was supported by the National ScienceFoundation under award IRI-9704240.ReferencesChinatsu Aone and Scott William Bennett.
1996.
Ap-plying machine learning to anaphora resolution.
InConnectionist, Statistical, and Symbolic Approaches toLearning for Natural Language Understanding, pages302{314.
Springer-Verlag, Berlin.E.
Brill and P. Resnik.
1994.
A Transformation-based Approach to Prepositional Phrase AttachmentDisambiguation.
In Proceedings of the Fifteenth In-ternational Conference on Computational Linguistics(COLING-94).S.
Caraballo.
1999.
Automatic Acquisition of aHypernym-Labeled Noun Hierarchy from Text.
InProceedings of the 37th Annual Meeting of the Associ-ation for Computational Linguistics, pages 120{126.M.
Collins and Y.
Singer.
1999.
Unsupervised Modelsfor Named Entity Classication.
In Proceedings of theJoint SIGDAT Conference on Empirical Me thods inNatural Language Processing and Very Large Corpora(EMNLP/VLC-99).S.
Cucerzan and D. Yarowsky.
1999.
Language Inde-pendent Named Entity Recognition Combining Morphological and Contextual Evidence.
In Proceedings ofthe Joint SIGDAT Conference on Empirical Me thodsin Natural Language Processing and Very Large Cor-pora (EMNLP/VLC-99).W.
Gale, K. Church, and David Yarowsky.
1992.
Amethod for disambiguating word senses in a large cor-pus.
Computers and the Humanities, 26:415{439.Niyu Ge, John Hale, and Eugene Charniak.
1998.
A sta-tistical approach to anaphora resolution.
In Proceed-ings of the Sixth Workshop on Very Large Corpora.M.
Hearst.
1992.
Automatic Acquisition of Hy-ponyms from Large Text Corpora.
In Proceedings ofthe Fourteenth International Conference on Computa-tional Linguistics (COLING-92).Lynette Hirschman, Marc Light, Eric Breck, and John D.Burger.
1999.
Deep Read: A reading comprehensionsystem.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a Large Annotated Corpus of English:The Penn Treebank.
Computational Linguistics,19(2):313{330.George Miller.
1990.
Wordnet: An on-line lexicaldatabase.
In International Journal of Lexicography.Dan Moldovan, Sanda Harabagiu, Marius Pasca, RadaMihalcea, Richard Goodrum, Roxana G^irju, andVasile Rus.
1999.
Lasso: A tool for surng the an-swer net.
In Proceedings of the Eighth Text REtrievalConference (TREC-8).MUC-4 Proceedings.
1992.
Proceedings of the FourthMessage Understanding Conference (MUC-4).
Mor-gan Kaufmann, San Mateo, CA.E.
Rilo and R. Jones.
1999.
Learning Dictionaries forInformation Extraction by Multi-Level Bootstrapping.In Proceedings of the Sixteenth National Conference onArticial Intelligence.E.
Rilo and M. Schmelzenbach.
1998.
An EmpiricalApproach to Conceptual Case Frame Acquisition.
InProceedings of the Sixth Workshop on Very Large Cor-pora, pages 49{56.E.
Rilo and J. Shepherd.
1997.
A Corpus-Based Ap-proach for Building Semantic Lexicons.
In Proceed-ings of the Second Conference on Empirical Methodsin Natural Language Processing, pages 117{124.E.
Rilo.
1996.
Automatically Generating ExtractionPatterns from Untagged Text.
In Proceedings of theThirteenth National Conference on Articial Intelli-gence, pages 1044{1049.
The AAAI Press/MIT Press.B.
Roark and E. Charniak.
1998.
Noun-phrase Co-occurrence Statistics for Semi-automatic SemanticLexicon Construction.
In Proceedings of the 36th An-nual Meeting of the Association for ComputationalLinguistics, pages 1110{1116.Stephen Soderland, David Fisher, Jonathan Aseltine,and Wendy Lehnert.
1995.
CRYSTAL: Inducing aconceptual dictionary.
In Proceedings of the Four-teenth International Joint Conference on Articial In-telligence, pages 1314{1319.
