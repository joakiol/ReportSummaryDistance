Felix Bildhauer & Roland Sch?fer (eds.
), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 9?15,Gothenburg, Sweden, April 26 2014.c?2014 Association for Computational LinguisticsFocused Web Corpus CrawlingRoland Sch?ferFreie Universit?t Berlinroland.schaefer@fu-berlin.deAdrien BarbaresiENS Lyonadrien.barbaresi@ens.lyon.orgFelix BildhauerFreie Universit?t Berlinfelix.bildhauer@fu-berlin.deAbstractIn web corpus construction, crawling is anecessary step, and it is probably the mostcostly of all, because it requires expen-sive bandwidth usage, and excess crawl-ing increases storage requirements.
Ex-cess crawling results from the fact that theweb contains a lot of redundant content(duplicates and near-duplicates), as wellas other material not suitable or desirablefor inclusion in web corpora or web in-dexes (for example, pages with little textor virtually no text at all).
An optimizedcrawler for web corpus construction wouldideally avoid crawling such content in thefirst place, saving bandwidth, storage, andpost-processing costs.
In this paper, weshow in three experiments that two simplescores are suitable to improve the ratio be-tween corpus size and crawling effort forweb corpus construction.
The first scoreis related to overall text quality of the pagecontaining the link, the other one is relatedto the likelihood that the local block en-closing a link is boilerplate.1 Crawl Optimization and Yield RatiosOptimizing a crawling strategy consists in maxi-mizing its weighted coverage WC(t) at any timet during a crawl (Olston and Najork, 2010, 29),i. e., the summed weight of the documents down-loaded until t, where the weight of each crawleddocument is calculated as a measure of the useful-ness of the document relative to the purpose of thecrawl.
To maximize WC, it is vital to guess theweight of the documents behind harvested linksbefore download, such that documents with poten-tially lesser weight have a lower probability of be-ing downloaded.
So-called focused crawlers (in abroad sense) are designed to maximize WC withrespect to some specific definition of documentweight, for example when documents with a highsearch-engine relevance (measured as its Page-Rank or a similar score), documents about specificsubjects, or documents in a specific language aredesired (Chakrabarti et al., 1999; Menczer et al.,2004; Baykan et al., 2008; Safran et al., 2012).For our purpose, i. e., web corpus crawling, a doc-ument with a high weight can simply be defined asone which is not removed from the corpus by thepost-processing tools due to low linguistic qual-ity and/or a document which contributes a highamount of text to the corpus.
Recently, an inter-esting approach to crawl optimization along suchlines was suggested which relies on statistics aboutthe corpus yield from known hosts (Suchomeland Pomik?lek, 2012).
Under this approach, theweight (rather of a whole web host) is taken to bethe ratio of good documents from the host remain-ing in the corpus after a specific post-processingchain has been applied to the documents.
Har-vested URLs pointing to certain hosts are priori-tized accordingly.
We follow a similar route likeSuchomel and Pomik?lek, but look at document-local features instead of host statistics.Throughout this paper, we refer to the yield ra-tio instead of WC, although they are related no-tions.
We define the yield ratio Ydfor a set Dcofcrawled unprocessed documents and a set Drofretained documents after filtering and processingfor inclusion in a corpus, with Dr?
Dc, as:Yd=|Dr||Dc|(1)For example, a document yield ratio Yd= 0.219means that 21% of the crawled documents sur-vived the cleaning procedure (i. e., were not clas-sified as duplicates or spam, were long enough,written in the target language, etc.)
and ended upin the corpus.
In order to maximize Yd, 79% ofthe documents should not have been downloadedin the first place in this example.
A parallel defini-tion is assumed for Ybfor the respective amountsof bytes.
The document yield ratio is easier to in-terpret because the byte yield ratio depends on theamount of markup which has to be stripped, andwhich might vary independently of the quality ofthe downloaded web pages.Obviously, the yield ratio ?
like the weightedcoverage ?
depends highly on the definition ofwhat a good document is, i. e., what the goal ofthe crawl is.
We assume, similar to Suchomel andPomik?lek?s approach, that our tools reliably filterout documents that are interesting documents forinclusion a corpus, and that calculating a yield ra-tio based on the output of those tools is thereforereasonable.12 Experiment 1: Seed and Crawl QualityIn this experiment, we examine the correlation be-tween the yield ratio of crawler seed URLs andthe yield ratio of short Breadth-First Search (BFS)crawls based on those URLs.
We used the Her-itrix (1.14) web crawler (Mohr et al., 2004) andan older version of the texrex web page clean-ing toolkit (Sch?fer and Bildhauer, 2012).
Thetools perform, among other things, boilerplate de-tection and text quality evaluation in the form ofthe so-called Badness score (Sch?fer et al., 2013).A document receives a low Badness score if themost frequent function words of the target lan-guage have a high enough frequency in the doc-ument.
The Badness score is based on previousideas from language identification and web doc-ument filtering (Grefenstette, 1995; Baroni et al.,2009).Originally, this experiment was carried out inthe context of an evaluation of sources of differ-ent seed URLs for crawls.
In a preliminary step,we began by collecting seed URLs from varioussources:1This claim should be backed up by forms of ex-trinsic/task-based evaluation (Sch?fer and Bildhauer, 2013,p.
104 ff).
Such an evaluation (in the form of a collocation ex-traction task) was recently presented for our corpora in workby Stefan Evert (Biemann et al., 2013).1. the DMOZ directory2.
the Etools meta search engine3.
the FriendFeed social service aggregator4.
the identi.ca social bookmarking service5.
Wikipedia dumpsWe scraped the content behind the URLs andran a state-of-the-art language identifier (Lui andBaldwin, 2012) on it in order to obtain language-classified seed URLs (Barbaresi, 2013).2We thenlooked specifically at the following languages as-sociated as the single dominant language with atleast one top-level domain (TLD):1.
Dutch (.nl)2.
French (.fr)3.
Indonesian (.id)4.
Swedish (.se)We randomly sampled 1, 000 seed URLs foreach of the 20 permutations of seed sourcesand languages/TLDs, downloaded them and usedtexrex to determine the document yield ratiofor the documents behind the 1, 000 seeds.
Thesoftware was configured to perform boilerplate re-moval, removal of documents based on high Bad-ness scores, perfect duplicate removal, and dele-tion of documents shorter than 1, 000 characters(after boilerplate removal).
Then, we crawledthe respective TLDs, starting the crawls with the1, 000 seed URLs, respectively.
In each crawl, wedownloaded 2 GB of raw data, cleaned them, andcalculated the document yield ratio using the sameconfiguration of texrex as we used for cleaningthe seed documents.
Figure 1 plots the data and anappropriate linear model.We see that there is a strong correlation (ad-justed R2= 0.7831) between the yield ratio ofthe documents behind the seed URLs and the yieldratio of the documents found by using the seedsfor BFS crawling.
It follows that giving high pri-ority to links from pages which are themselvesconsidered high-quality documents by the post-processing tools will likely lead to more efficientcrawling.
Since there is no fundamental distinc-tion between initial URL seeds and URLs har-vested at a later time during the crawl, this effectis likely to extend to the whole run time of a crawl.2See also Barbaresi, this volume.100.100.150.200.250.300.350.050.100.150.20Figure 1: Yield ratio Ydof the crawls (y axis) plot-ted against the yield ratio of the documents be-hind the crawls?
1,000 seeds (x axis).
(Higher Ydis better.)
Linear model: Intercept = ?0.0098,Coefficient = 0.6332, R2= 0.7831 (adjusted),p < 0.001 (ANOVA).3 Experiment 2: Crawlingwith Cyclic URL SelectionUsing the same configuration of tools as in Sec-tion 2, we performed a crawl targeting Flem-ish documents in the Belgian .be national TLD,which hosts both Flemish and French documentsin substantial proportions.
Usually, even undermore favorable conditions (i. e., when we crawl aTLD which contains mostly documents in the tar-get language), the yield ratio of a BFS crawl de-creases rapidly in the initial phase, then staying ata low level (Sch?fer and Bildhauer, 2013, p. 31).Figure 2 illustrates this with an analysis of a .deBFS crawl from late 2011, also processed with thesame tools as mentioned in Section 2.
Notice thatthe .de domain hosts German documents almostexclusively.The interesting complication in this experimentis thus the non-target language present in theTLD scope of the crawler and the related questionwhether, simply speaking, predominantly Flemishdocuments link to other predominantly Flemishdocuments rather than French documents.
Sincethe Badness score (calculated as described in Sec-tion 2) includes a form of language identification,the yield ratio takes into account this additionalcomplication.We tested whether the decline of the yield ra-tio could be compensated for by selecting ?highquality?
URLs in the following manner: The crawlprogressed in five phases.
In the first short burn-in phase, we crawled 1, 000, 000 documents, andin each of the second to fifth phase, we crawled10, 000, 000 documents.
After each phase, the020040060080010000.000.050.100.150.200.250.30Figure 2: Yield ratio (y axis) over time for aBFS crawl in .de in November/December 2011started with 231, 484 seed URLs scraped fromBing.
The yield ratio was calculated at 1, 000snapshots of 400 MB of data (= one Heritrix ARCfile).
For snapshots s1..s500: Yd= 0.141, forsnapshots s501..s1000: Yd= 0.071.
The verticalbar marks the point at which the seeds were ex-hausted.
(Sch?fer and Bildhauer, 2013, p. 31)crawl was halted, the crawler frontier was emptied,and the crawl was then re-started with a selectionof the URLs harvested in the previous phase.
Onlythose URLs were used which came from docu-ments with a Badness score of 10 or lower (= doc-uments in which the distribution of the most fre-quent function words fits the expected distributionfor Flemish very well, cf.
Section 2), and from textblocks with a boilerplate score (Sch?fer and Bild-hauer, 2012) in [0.5, 1] (= likely not boilerplate).Additionally, it was made sure that no URLs werere-used between the five phases.
The very promis-ing results are plotted in Figure 3.05001000150020000.000.100.200.30Figure 3: Yield ratio over crawl time with cyclicURL selection in the .be TLD.
The x axis showsthe crawl progression in snapshots of 400 MB ofraw crawled data (= one Heritrix ARC file).
The yaxis shows the yield ratio for each snapshot.
Thefive phases are clearly distinguishable by the sud-den increases in yield ratio.11phase adjusted R2p (ANOVA)1 0.8288 < 0.0012 0.9187 < 0.0013 0.8308 < 0.0014 0.9125 < 0.0015 0.9025 < 0.001Table 1: Fit of linear models for the decrease inthe yield ratios of the first 100 snapshots in eachof the five phases of the .be crawl.
For the firstphase, only 50 snapshots were crawled and fitted.The decline of the yield ratio is almost linearfor the first 100 snapshots in the five phases (cf.Table 1), where each phase has roughly 500 snap-shots in total, and one snapshot corresponds to400 MB of downloaded raw data.
After this de-cline, the yield ratio remains at low levels around0.05.
Cyclic URL selection, however, repeatedlymanages to push the yield ratio to above 0.2 for ashort period.
The subsequent sharp decline showsthat link selection/prioritization should rather beimplemented in the crawler frontier managementin order to achieve a constant effect over longercrawls (cf.
Section 5).4 Experiment 3: Internal Crawl AnalysisFor the last experiment, we used the most recentversion of the texrex toolkit, which writes fulllink structures for the processed documents as aby-product.3An internal analysis of a small por-tion of a crawled data set from the German TLDwas performed, which is part of the raw mate-rial of the DECOW corpus (Sch?fer and Bild-hauer, 2012).
The data set contains 11, 557, 695crawled HTML documents and 81, 255, 876 httplinks extracted from the crawled documents (only<a> tags).
Among the link URLs in the sam-ple, 711, 092 are actually links to documents inthe sample, so we could analyze exactly those711, 092 links.
It should be noticed that we onlylooked at links to different hosts, such that host-internal links (navigation to ?Home?, etc.)
are notincluded in the analysis.In this experiment, we were interested specif-ically in the many documents which we usuallydiscard right away simply because they are eithervery short (below 2 KB of unstripped HTML) orperfect duplicates of other documents.
This is a3The new version (release name hyperhyper) has beenreleased and documented at http://texrex.sf.net/.positives negativestrue 69, 273 342, 430false 237, 959 61, 430Table 2: Confusion matrix for binary downloaddecisions based on the Badness of the documentcontaining the URL for the DECOW crawl sam-ple described in Section 4.
Badness threshold at10.
Precision=0.225, Recall=0.530, F1=0.316.step of document selection which usually precedesthe cleansing used for the experiments describedin Sections 2 and 3.
The analysis shows that of the711, 092 link URLs in the sample, 130, 703 pointto documents which are not perfect duplicates ofother documents and which are over 2 KB long.580, 389 of them point to documents which do notsatisfy these criteria.
We then evaluated the qualityof the link environments in terms of their Badnessand boilerplate scores.
The results are shown inFigures 4 and 5.40.20.40.60.805101520253035404550retained deletedFigure 4: Badness scores of the links in the crawlanalysis described in Section 4.
The x axis showsthe Badness scores of the documents which linkedto the retained (?good?)
and the deleted (?bad?)documents.
The y axis shows the proportion ofretained/deleted documents for which the Badnessscore is ?
x.
(Lower Badness scores are better.
)The observable correlation between the qualityof a link?s context and the quality of the page be-hind the link is stronger for the boilerplate scorethan for the Badness score.
For example, hadwe only followed links from documents with aBadness score of 10 or lower (= better), then4Notice that the older version of texrex used in theexperiments described in Sections 2 and 3 assigns a boiler-plate score of 1 to text blocks which are most likely goodtext, while the new texrex-hyperhyper assigns 1 to textblocks which are most likely boilerplate.
Take this into ac-count when comparing the thresholds mentioned there andthose reported here.120.50.60.70.80.91.0?1?0.8?0.5?0.200.20.40.60.81retained deletedFigure 5: Boilerplate scores of the links in thecrawl analysis described in Section 4.
The x axisshows the boilerplate scores of the blocks whichlinked to the retained (?good?)
and the deleted(?bad?)
documents.
The y axis shows the propor-tion of retained/deleted documents for which theboilerplate score is?
x.
(Lower boilerplate scoresare better.
)positives negativestrue 83, 650 522, 350false 58, 039 47, 053Table 3: Confusion matrix for binary down-load decisions based on the boilerplate score ofthe block containing the URL for the DECOWcrawl sample described in Section 4.
Boilerplatethreshold at 0.5.
Precision=0.590, Recall=0.640,F1=0.614.0.59?580, 389 = 342, 430 bad documents wouldnot have been downloaded, but at the same time0.47?130, 703 = 61, 430 good documents wouldhave been lost.
Tables 2 and 3 show a confusionmatrix for a reasonable Badness threshold (10) anda reasonable boilerplate threshold (0.5).
Obvi-ously, if we use Badness and boilerplate scores ofthe link context to make a binary download deci-sion, the accuracy is much too low, which is whywe suggest to merely prioritize URLs instead ofdiscarding them, cf.
Section 5.5 Conclusion andPlanned Crawler ArchitectureWe have shown that two standard cleaning algo-rithms used in web corpus construction, i. e., textquality evaluation based on frequent short wordsand boilerplate detection (as implemented in thetexrex toolkit) have a high potential for optimiz-ing web corpus crawling through the prioritizationof harvested URLs in a crawler system.We are now in the process of designing a customweb corpus crawler system called HeidiX, whichintegrates the texrex post-processing tools forweight estimation based on the methods describedin this paper.
Cf.
Figure 6, which schematicallyshows the current design draft.5HeidiX is designed with a system of rankedURL back queues for harvested links (cf.UrlQueues).
Each queue holds URLs for whichthe weight estimation is within a specifiable in-terval, such that the most promising URLs are inone queue, etc.
The actual downloading is per-formed by massively parallel fetcher threads inthe FetcherPool, which (in the final software) willtalk to a DNS cacher and a politeness manager,which handles caching of Robots Exclusion In-formation and politeness intervals.
The fetcherthreads pop URLs from one of the ranked queues,which is selected randomly with prior probabili-ties inversely proportional to the rank of the queue.Thus, promising URLs are popped more often andless promising ones less often.For guessing the weight, pluggable modulescan be used and combined in the Focused-Walker container.
Currently, we have the stan-dard UrlSeenFilter, which is based on our ownself-scaling Bloom Filter implementation (Bloom,1970; Almeida et al., 2007), and which pre-vents any URL from being queued more thanonce.
We have plans for a URL-based languageguesser (Baykan et al., 2008) in the form ofthe LanguagePredictor, and a prioritizer basedon the yield from specific hosts as described inSuchomel and Pomik?lek (2012) in the form ofthe HostYieldPrioritizer, which reads statistics di-rectly from the texrex module.
The texrexmodule extracts all hyperlinks from processeddocuments and tags them with the quality scoresdescribed in this paper, such that the QualityPri-oritizer module can adjust the expected weight ofthe document behind each URL.The HeidiX architecture also features an al-ternative queueing strategy in the form of theRandomWalker, which allows users to obtain uni-form random samples from the web based on ex-isting algorithms (Henzinger et al., 2000; Rus-mevichientong et al., 2001).
Since obtaining suchsamples is a goal which is mostly orthogonal to the5Like texrex, it is written entirely in the FreePascaldialect of ObjectPascal (http://freepascal.org/),uses only very few additional C libraries, and will be releasedunder the GPL 3.13texrexUrlQueuesFetcherPoolRandomWalker(CLARAx)HTMLCorpusURLWWWDNSCacherPolitenessManagerSnapshotsStatisticsSnapshotKeeperLanguagePredictorQualityPrioritizerHostYieldPrioritizerFocusedWalkerUrlSeenFilterFigure 6: HeidiX Crawler Architecture.
Grayed modules are done as of March 2014.
The Focused-Walker implements an ?efficiently locate good corpus document?
URL prioritization scheme; the Ran-domWalker implements bias-corrected Random Walk URL selection for obtaining uniform random sam-ples.one assumed in this paper, we do not discuss thisfurther here.
Finally, a SnapshotKeeper moduleallows users to halt and continue crawls by writ-ing/reading the current state of the relevant com-ponents to/from disk.We hope that HeidiX will become a valuabletool in both the efficient construction of very largeweb corpora (FocusedWalker) and the construc-tion of smaller unbiased reference samples as wellas web analysis (RandomWalker).ReferencesPaulo S?rgio Almeida, Carlos Baquero, NunoPregui?a, and David Hutchison.
2007.
Scalablebloom filters.
Information Processing Letters,101:255?261.Adrien Barbaresi.
2013.
Crawling microblogging ser-vices to gather language-classified urls.
workflowand case study.
In 51st Annual Meeting of the As-sociation for Computational Linguistics Proceed-ings of the Student Research Workshop, pages 9?15,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky WideWeb: A collection of very large linguistically pro-cessed web-crawled corpora.
Language Resourcesand Evaluation, 43(3):209?226.Eda Baykan, Monika Henzinger, and Ingmar Weber.2008.
Web page language identification based onURLs.
In Proceedings of the VLDB Endowment,pages 176?187.Chris Biemann, Felix Bildhauer, Stefan Evert, DirkGoldhahn, Uwe Quasthoff, Roland Sch?fer, Jo-hannes Simon, Leonard Swiezinski, and TorstenZesch.
2013.
Scalable construction of high-qualityweb corpora.
Journal for Language Technology andComputational Linguistics, 28(2):23?60.Burton Bloom.
1970.
Space/time trade-offs in hashcoding with allowable errors.
Communications ofACM, 13(7):422?426.Soumen Chakrabarti, Martin van den Berg, and ByronDom.
1999.
Focused crawling: a new approachto topic-specific web resource discovery.
ComputerNetworks, 31:1623?1640.Gregory Grefenstette.
1995.
Comparing two languageidentification schemes.
In Proceedings of the 3rd In-ternation conference on Statistical Analysis of Tex-tual Data (JADT 1995), pages 263?268, Rome.Monika R. Henzinger, Allan Heydon, Michael Mitzen-macher, and Marc Najork.
2000.
On near-uniformURL sampling.
In Proceedings of the 9th Inter-national World Wide Web conference on ComputerNetworks: The International Journal of Computerand Telecommunications Networking, pages 295?308.
North-Holland Publishing Co.Marco Lui and Timothy Baldwin.
2012. langid.py: AnOff-the-shelf Language Identification Tool.
In Pro-ceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2012),Jeju, Republic of Korea.Filippo Menczer, Gautam Pant, and Padmini Srini-vasan.
2004.
Topical web crawlers: Evaluatingadaptive algorithms.
ACM Trans.
Internet Technol.,4(4):378?419.14Gordon Mohr, Michael Stack, Igor Ranitovic, Dan Av-ery, and Michele Kimpton.
2004.
Introductionto Heritrix, an archival quality web crawler.
InProceedings of the 4th International Web ArchivingWorkshop (IWAW?04).Christopher Olston and Marc Najork.
2010.
WebCrawling, volume 4(3) of Foundations and Trendsin Information Retrieval.
now Publishers, Hanover,MA.Paat Rusmevichientong, David M. Pennock, SteveLawrence, and C. Lee Giles.
2001.
Methods forsampling pages uniformly from the World WideWeb.
In In AAAI Fall Symposium on Using Uncer-tainty Within Computation, pages 121?128.M.S.
Safran, A. Althagafi, and Dunren Che.
2012.Improving relevance prediction for focused Webcrawlers.
In IEEE/ACIS 11th International Confer-ence on Computer and Information Science (ICIS),2012, pages 161?166.Roland Sch?fer and Felix Bildhauer.
2012.
Build-ing large corpora from the web using a new ef-ficient tool chain.
In Nicoletta Calzolari, KhalidChoukri, Thierry Declerck, Mehmet U?gur Do?gan,Bente Maegaard, Joseph Mariani, Jan Odijk, andStelios Piperidis, editors, Proceedings of the EightInternational Conference on Language Resourcesand Evaluation (LREC?12), pages 486?493, Istan-bul.
ELRA.Roland Sch?fer and Felix Bildhauer.
2013.
Web Cor-pus Construction.
Synthesis Lectures on HumanLanguage Technologies.
Morgan and Claypool, SanFrancisco.Roland Sch?fer, Adrien Barbaresi, and Felix Bildhauer.2013.
The good, the bad, and the hazy: Design de-cisions in web corpus construction.
In Stefan Evert,Egon Stemle, and Paul Rayson, editors, Proceedingsof the 8th Web as Corpus Workshop (WAC-8), pages7?15, Lancaster.
SIGWAC.V?t Suchomel and Jan Pomik?lek.
2012.
Effcient Webcrawling for large text corpora.
In Adam Kilgarriffand Serge Sharoff, editors, Proceedings of the sev-enth Web as Corpus Workshop, pages 40?44.15
