Proceedings of the 6th Workshop on Statistical Machine Translation, pages 294?302,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsTopic Adaptation for Lecture Translation throughBilingual Latent Semantic ModelsNick Ruiz?Free University of Bozen-BolzanoBolzano, Italynicruiz@fbk.euMarcello FedericoFBK-irstFondazione Bruno KesslerTrento, Italyfederico@fbk.euAbstractThis work presents a simplified approach tobilingual topic modeling for language modeladaptation by combining text in the sourceand target language into very short documentsand performing Probabilistic Latent SemanticAnalysis (PLSA) during model training.
Dur-ing inference, documents containing only thesource language can be used to infer a fulltopic-word distribution on all words in the tar-get language?s vocabulary, from which we per-form Minimum Discrimination Information(MDI) adaptation on a background languagemodel (LM).
We apply our approach on theEnglish-French IWSLT 2010 TED Talk exer-cise, and report a 15% reduction in perplexityand relative BLEU and NIST improvementsof 3% and 2.4%, respectively over a baselineonly using a 5-gram background LM over theentire translation task.
Our topic modeling ap-proach is simpler to construct than its counter-parts.1 IntroductionAdaptation is usually applied to reduce the per-formance drop of Statistical Machine Translation(SMT) systems when translating documents that de-viate from training and tuning conditions.
In thispaper, we focus primarily on language model (LM)adaptation.
In SMT, LMs are used to promote fluenttranslations.
As probabilistic models of sequencesof words, language models guide the selection andordering of phrases in translation.
With respect to?This work was carried out during an internship period atFondazione Bruno Kessler.LM training, LM adaptation for SMT tries to im-prove an existing LM by using smaller amounts oftexts.
When adaptation data represents the trans-lation task domain one generally refers to domainadaptation, while when they just represent the con-tent of the single document to be translated one typ-ically refers to topic adaptation.We propose a cross-language topic adaptationmethod, enabling the adaptation of a LM based onthe topic distribution of the source document dur-ing translation.
We train a latent semantic topicmodel on a collection of bilingual documents, inwhich each document contains both the source andtarget language.
During inference, a latent topic dis-tribution of words across both the source and tar-get languages is inferred from a source documentto be translated.
After inference, we remove allsource language words from the topic-word distribu-tions and construct a unigram language model whichis used to adapt our background LM via MinimumDiscrimination Information (MDI) estimation (Fed-erico, 1999, 2002; Kneser et al, 1997).We organize the paper as follows: In Section 2,we discuss relevant previous work.
In Section 3, wereview topic modeling.
In Section 4, we review MDIadaptation.
In Section 5, we describe our new bilin-gual topic modeling based adaptation technique.
InSection 6, we report adaptation experiments, fol-lowed by conclusions and future work in Section 7.2 Previous workZhao et al (2004) construct a baseline SMT systemusing a large background language model and use itto retrieve relevant documents from large monolin-294gual corpora and subsequently interpolate the result-ing small domain-specific language model with thebackground language model.
In Sethy et al (2006),domain-specific language models are obtained byincluding only the sentences that are similar to theones in the target domain via a relative entropy basedcriterion.Researchers such as Foster and Kuhn (2007) andKoehn and Schroeder (2007) have investigated mix-ture model approaches to adaptation.
Foster andKuhn (2007) use a mixture model approach that in-volves splitting a training corpus into different com-ponents, training separate models on each compo-nent, and applying mixture weights as a function ofthe distances of each component to the source text.Koehn and Schroeder (2007) learn mixture weightsfor language models trained with in-domain and out-of-domain data respectively by minimizing the per-plexity of a tuning (development) set and interpolat-ing the models.
Although the application of mixturemodels yields significant results, the number of mix-ture weights to learn grows linearly with the numberof independent language models applied.Most works focus on monolingual languagemodel adaptation in the context of automatic speechrecognition.
Federico (2002) combines Probabilis-tic Latent Semantic Analysis (PLSA) (Hofmann,1999) for topic modeling with the minimum dis-crimination information (MDI) estimation criterionfor speech recognition and notes an improvementin terms of perplexity and word error rate (WER).Latent Dirichlet Allocation (LDA) techniques havebeen proposed as an alternative to PLSA to constructpurely generative models.
LDA techniques includevariational Bayes (Blei et al, 2003) and HMM-LDA(Hsu and Glass, 2006).Recently, bilingual approaches to topic model-ing have also been proposed.
A Hidden MarkovBilingual Topic AdMixture (HM-BiTAM) model isproposed by Zhao and Xing (2008), which con-structs a generative model in which words from atarget language are sampled from a mixture of topicsdrawn from a Dirichlet distribution.
Foreign wordsare sampled via alignment links from a first-orderMarkov process and a topic specific translation lexi-con.
While HM-BiTAM has been used for bilingualtopic extraction and topic-specific lexicon mappingin the context of SMT, Zhao and Xing (2008) notethat HM-BiTAM can generate unigram languagemodels for both the source and target language andthus can be used for language model adaptationthrough MDI in a similar manner as outlined in Fed-erico (2002).
Another bilingual LSA approach isproposed by Tam et al (2007), which consists oftwo hierarchical LDA models, constructed from par-allel document corpora.
A one-to-one correspon-dence between LDA models is enforced by learn-ing the hyperparameters of the variational Dirichletposteriors in one LDA model and bootstrapping thesecond model by fixing the hyperparameters.
Thetechnique is based on the assumption that the topicdistributions of the source and target documents areidentical.
It is shown by Tam et al (2007) that thebilingual LSA framework is also capable of adapt-ing the translation model.
Their work is extendedin Tam and Schultz (2009) by constructing paral-lel document clusters formed by monolingual doc-uments using M parallel seed documents.Additionally, Gong et al (2010) propose transla-tion model adaptation via a monolingual LDA train-ing.
A monolingual LDA model is trained from ei-ther the source or target side of the training corpusand each phrase pair is assigned a phrase-topic dis-tribution based on:M?
ji =wjk ?Mji?mk=1wjk, (1)where M j is the topic distribution of document jand wk is the number of occurrences of phrase pairXk in document j.Mimno et al (2009) extend the original con-cept of LDA to support polylingual topic models(PLTM), both on parallel (such as EuroParl) andpartly comparable documents (such as Wikipedia ar-ticles).
Documents are grouped into tuples w =(w1, ...,wL) for each language l = 1, ..., L. Eachdocument wl in tuple w is assumed to have thesame topic distribution, drawn from an asymmetricDirichlet prior.
Tuple-specific topic distributions arelearned using LDA with distinct topic-word concen-tration parameters ?l.
Mimno et al (2009) show thatPLTM sufficiently aligns topics in parallel corpora.2953 Topic Modeling3.1 PLSAThe original idea of LSA is to map documents toa latent semantic space, which reduces the dimen-sionality by means of singular value decomposition(Deerwester et al, 1990).
A word-document matrixA is decomposed by the formulaA = U?V t, whereU and V are orthogonal matrices with unit-lengthcolumns and ?
is a diagonal matrix containing thesingular values of A. LSA approximates ?
by cast-ing all but the largest k singular values in ?
to zero.PLSA is a statistical model based on the likeli-hood principle that incorporates mixing proportionsof latent class variables (or topics) for each obser-vation.
In the context of topic modeling, the latentclass variables z ?
Z = {z1, ..., zk} correspond totopics, from which we can derive probabilistic distri-butions of words w ?W = {w1, ..., wm} in a docu-ment d ?
D = {d1, ..., dn} with k << n. Thus, thegoal is to learn P (z | d) and P (w|z) by maximizingthe log-likelihood function:L(W,D) =?d?D?w?Wn(w, d) logP (w | d), (2)where n(w, d) is the term frequency of w in d.Using Bayes?
formula, the conditional probabilityP (w | d) is defined as:P (w | d) =?z?ZP (w | z)P (z | d).
(3)Using the Expectation Maximization (EM) algo-rithm (Dempster et al, 1977), we estimate the pa-rameters P (z|d) and P (w|z) via an iterative pro-cess that alternates two steps: (i) an expectationstep (E) in which posterior probabilities are com-puted for each latent topic z; and (ii) a maximiza-tion (M) step, in which the parameters are updatedfor the posterior probabilities computed in the previ-ous E-step.
Details of how to efficiently implementthe re-estimation formulas can be found in Federico(2002).Iterating the E- and M-steps will lead to a con-vergence that approximates the maximum likelihoodequation in (2).A document-topic distribution ??
can be inferredon a new document d?
by maximizing the followingequation:??
= arg max?
?wn(w, d?)
log?zP (w | z)?z,d?
,(4)where ?z,d?
= P (z | d?).
(4) can be maximized byperforming Expectation Maximization on documentd?
by keeping fixed the word-topic distributions al-ready estimated on the training data.
Consequently,a word-document distribution can be inferred by ap-plying the mixture model (3) (see Federico, 2002 fordetails).4 MDI AdaptationAn n-gram language model approximates the prob-ability of a sequence of words in a text W T1 =w1, ..., wT drawn from a vocabulary V by the fol-lowing equation:P (W T1 ) =T?i=1P (wi|hi), (5)where hi = wi?n+1, ..., wi?1 is the history of n ?1 words preceding wi.
Given a training corpus B,we can compute the probability of a n-gram from asmoothed model via interpolation as:PB(w|h) = f?B(w|h) + ?B(h)PB(w|h?
), (6)where f?B(w|h) is the discounted frequency of se-quence hw, h?
is the lower order history, where|h|?1 = |h?|, and ?B(h) is the zero-frequency prob-ability of h, defined as:?B(h) = 1.0?
?w?Vf?B(w|h).Federico (1999) has shown that MDI Adaptationis useful to adapt a background language modelwith a small adaptation text sample A, by assum-ing to have only sufficient statistics on unigrams.Thus, we can reliably estimate P?A(w) constraintson the marginal distribution of an adapted languagemodel PA(h,w) which minimizes the Kullback-Leibler distance from B, i.e.:PA(?)
= arg minQ(?
)?hw?V nQ(h,w) logQ(h,w)PB(h,w).
(7)296The joint distribution in (7) can be computed us-ing Generalized Iterative Scaling (Darroch and Rat-cliff, 1972).
Under the unigram constraints, the GISalgorithm reduces to the closed form:PA(h,w) = PB(h,w)?
(w), (8)where?
(w) =P?A(w)PB(w).
(9)In order to estimate the conditional distribution ofthe adapted LM, we rewrite (8) and simplify theequation to:PA(w|h) =PB(w|h)?(w)?w?
?V PB(w?|h)?(w?).
(10)The adaptation model can be improved bysmoothing the scaling factor in (9) by an exponentialterm ?
(Kneser et al, 1997):?
(w) =(P?A(w)PB(w))?, (11)where 0 < ?
?
1.
Empirically, ?
values less thanone decrease the effect of the adaptation ratio to re-duce the bias.As outlined in Federico (2002), the adapted lan-guage model can also be written in an interpolationform:f?A(w|h) =f?B(w|h)?
(w)z(h), (12)?A(h) =?B(h)z(h?
)z(h), (13)z(h) = (?w:NB(h,w)>0f?B(w|h)?
(w)) + ?B(h)z(h?
),(14)which permits to efficiently compute the normaliza-tion term for high order n-grams recursively and byjust summing over observed n-grams.
The recursionends with the following initial values for the emptyhistory :z() =?wPB(w)?
(w), (15)PA(w|) = PB(w)?(w)z()?1.
(16)MDI adaptation is one of the adaptation methodsprovided by the IRSTLM toolkit and was applied asexplained in the following section.5 Bilingual Latent Semantic ModelsSimilar to the treatment of documents in HM-BiTAM (Zhao and Xing, 2008), we combine paralleltexts into a document-pair (E,F) containing n par-allel sentence pairs (ei, fi), 1 < i ?
n, correspond-ing to the source and target languages, respectively.Based on the assumption that the topics in a paralleltext share the same semantic meanings across lan-guages, the topics are sampled from the same topic-document distribution.
We make the additional as-sumption that stop-words and punctuation, althoughhaving high word frequencies in documents, willgenerally have a uniform topic distribution acrossdocuments; therefore, it is not necessary to removethem prior to model training, as they will not ad-versely affect the overall topic distribution in eachdocument.
In order to ensure the uniqueness be-tween word tokens between languages, we annotateE with special characters.
We perform PLSA train-ing, as described in Section 3.1 and receive word-topic distributions P (w|z), w ?
VE ?
VFGiven an untranslated text E?, we split E?
intoa sequence of documents D. For each documentdi ?
D, we infer a full word-document distribu-tion by learning ??
via (4).
Via (3), we can generatethe full word-document distribution P (w | d) forw ?
VF .We then convert the word-document probabilitiesinto pseudo-counts via a scaling function:n(w | d) =P (w | d)maxw?
P (w?
| d)?
?, (17)where ?
is a scaling factor to raise the probabil-ity ratios above 1.
Since our goal is to generate aunigram language model on the target language foradaptation, we remove the source words generatedin (17) prior to building the language model.From our newly generated unigram languagemodel, we perform MDI adaptation on the back-ground LM to yield an adapted LM for translatingthe source document used for the PLSA inferencestep.6 ExperimentsOur experiments were done using the TED Talkscollection, used in the IWSLT 2010 evaluation task1.1http://iwslt2010.fbk.eu/297In IWSLT 2010, the challenge was to translate talksfrom the TED website2 from English to French.
Thetalks include a variety of topics, including photog-raphy and pyschology and thus do not adhere toa single genre.
All talks were given in Englishand were manually transcribed and translated intoFrench.
The TED training data consists of 329 par-allel talk transcripts with approximately 84k sen-tences.
The TED test data consists of transcriptionscreated via 1-best ASR outputs from the KIT QuaeroEvaluation System.
It consists of 758 sentences and27,432 and 27,307 English and French words, re-spectively.
The TED talk data is segmented at theclause level, rather than at the level of sentences.Our SMT systems are built upon the Moses open-source SMT toolkit (Koehn et al, 2007)3.
The trans-lation and lexicalized reordering models have beentrained on parallel data.
One 5-gram backgroundLM was constructed from the French side of theTED training data (740k words), smoothed with theimproved Kneser-Ney technique (Chen and Good-man, 1999) and computed with the IRSTLM toolkit(Federico et al, 2008).
The weights of the log-linearinterpolation model were optimized via minimumerror rate training (MERT) (Och, 2003) on the TEDdevelopment set, using 200 best translations at eachtuning iteration.This paper investigates the effects of languagemodel adaptation via bilingual latent semantic mod-eling on the TED background LM against a baselinemodel that uses only the TED LM.6.1 Bilingual Latent Semantic ModelUsing the technique outlined in Section 5, we con-struct bilingual documents by splitting the parallelTED training corpus into 41,847 documents of 5lines each.
While each individual TED lecture couldbe used as a document, our experimental goal isto simulate near-time translation of speeches; thus,we prefer to construct small documents to simulatetopic modeling on a spoken language scenario inwhich the length of a talk is not known a priori.We annotate the English source text for removal af-ter inference.
Figure 1 contains a sample documentconstructed for PLSA training.
(In fact, we distin-2http://www.ted.com/talks/3http://www.statmt.org/moses/robert lang is a pioneer of the newest kind of origami ?
us-ing math and engineering principles to fold mind-blowinglyintricate designs that are beautiful and , sometimes , veryuseful .
my talk is ?
flapping birds and space telescopes .?
and you would think that should have nothing to do withone another , but i hope by the end of these 18 minutes, you ?ll see a little bit of a relation .
robert lang est unpionnier des nouvelles techniques d?
origami - base?es surdes principes mathe?matiques et d?
inge?nierie permettant decre?er des mode`les complexes et e?poustouflants , qui sontbeaux et parfois , tre`s utiles .
ma confe?rence s?
intitule ?oiseaux en papier et te?lescopes spatiaux ?
.
et vous pensezprobablement que les uns et les autres n?
ont rien en com-mun , mais j?
espe`re qu?
a` l?
issue de ces 18 minutes , vouscomprendrez ce qui les relie .Figure 1: A sample bilingual document used for PLSAtraining.guish English words from French words by attach-ing to the former a special suffix.)
By using our in-house implementation, training of the PLSA modelon the bilingual collection converged after 20 EMiterations.Using our PLSA model, we run inference on eachof the 476 test documents from the TED lectures,constructed by splitting the test set into 5-line docu-ments.
Since our goal is to translate and evaluate thetest set, we construct monolingual (English) docu-ments.
Figure 2 provides an example of a documentto be inferred.
We collect the bilingual unigrampseudocounts after 10 iterations of inference and re-move the English words.
The TED lecture data istranscribed by clauses, rather than full sentences, sowe do not add sentence splitting tags before trainingour unigram language models.As a result of PLSA inference, the probabilitiesof target words increase with respect to the back-ground language model.
Table 1 demonstrates thisphenomenon by outlining several of the top rankedwords that have similar semantic meaning to non-stop words on the source side.
In every case, theprobabilityPA(w) increases fairly substantially withrespect to the PB(w).
As a result, we expect that theadapted language model will favor both fluent andsemantically correct translations as the adaptation issuggesting better lexical choices of words.298we didn ?t have money , so we had a cheap , little ad , but wewanted college students for a study of prison life .
75 peo-ple volunteered , took personality tests .
we did interviews .picked two dozen : the most normal , the most healthy .Figure 2: A sample English-only document (#230) usedfor PLSA inference.
A full unigram word distributionwill be inferred for both English and French.Rank Word PA(w) PB(w) PA(w)/PB(w)20 gens 8.41E-03 4.55E-05 184.8422 vie 8.30E-03 1.09E-04 76.1551 prix 2.59E-03 8.70E-05 29.7780 e?cole 1.70E-03 6.13E-05 27.7383 argent 1.60E-03 3.96E-05 40.0486 personnes 1.52E-03 2.75E-04 5.2394 aide 1.27E-03 7.71E-05 16.4798 e?tudiants 1.20E-03 7.12E-05 16.85119 marche?
9.22E-04 9.10E-05 10.13133 e?tude 7.63E-04 4.55E-05 16.77173 e?ducation 5.04E-04 2.97E-05 16.97315 prison 2.65E-04 1.98E-05 13.38323 universite?
2.60E-04 2.97E-05 8.75Table 1: Sample unigram probabilities of the adaptationmodel for document #230, compared to the baseline un-igram probabilities.
The French words selected are se-mantically related to the English words in the adapteddocument.
The PLSA adaptation infers higher unigramprobabilities for words with latent topics related to thesource document.6.2 MDI AdaptationWe perform MDI adaptation with each of the un-igram language models to update the backgroundTED language model.
We configure the adaptationrate parameter ?
to 0.3, as recommended in Fed-erico (2002).
The baseline LM is replaced with eachadapted LM, corresponding to the document to betranslated.
We then calculate the mean perplexity ofthe adapted LMs and the baseline, respectively.
Theperplexity scores are shown in Table 2.
We observe a15.3% relative improvement in perplexity score overthe baseline.6.3 ResultsWe perform MT experiments on the IWSLT 2010evaluation set to compare the baseline and adaptedLMs.
In the evaluation, we notice a 0.85 improve-ment in BLEU (%), yielding a 3% improvement overthe baseline.
The same performance trend in NISTis observed with a 2.4% relative improvement com-pared to the unadapted baseline.
Our PLSA andMDI-based adaptation method not only improvesfluency but also improves adequacy: the topic-based adaptation approach is attempting to suggestmore appropriate words based on increased unigramprobabilities than that of the baseline LM.
Table 3demonstrates a large improvement in unigram se-lection for the adapted TED model in terms of theindividual contribution to the NIST score, with di-minishing effects on larger n-grams.
The majorityof the overall improvements are on individual wordselection.Examples of improved fluency and adequacy areshown in Figure 3.
Line 285 shows an example of atranslation that doesn?t provide much of an n-gramimprovement, but demonstrates more fluent output,due to the deletion of the first comma and the move-ment of the second comma to the end of the clause.While ?installation?
remains an inadequate noun inthis clause, the adapted model reorders the rootwords ?rehab?
and ?installation?
(in comparisonwith the baseline) and improves the grammaticalityof the sentence; however, the number does not matchbetween the determiner and the noun phrase.
Line597 demonstrates a perfect phrase translation withrespect to the reference translation using semanticparaphrasing.
The baseline phrase ?d?origine?
istransformed and attributed to the noun.
Instead oftranslating ?original?
as a phrase for ?home?, theadapted model captures the original meaning of theword in the translation.
Line 752 demonstrates animprovement in adequacy through the replacementof the word ?quelque?
with ?autre.?
Additionally,extra words are removed.These lexical changes result in the improvementin translation quality due to topic-based adaptationvia PLSA.LM Perplexity BLEU (%) NISTAdapt TED 162.44 28.49 6.5956Base TED 191.76 27.64 6.4405Table 2: Perplexity, BLEU, and NIST scores for the base-line and adapted models.
The perplexity scores are aver-aged across each document-specific LM adaptation.299NIST 1-gram 2-gram 3-gramAdapt TED 4.8077 1.3925 0.3229Base TED 4.6980 1.3527 0.3173Difference 0.1097 0.0398 0.0056Table 3: Individual unigram NIST scores for n-grams 1-3of the baseline and adapted models.
The improvement ofthe adapted model over the baseline is listed below.
(Line 285), j?
ai eu la chance de travailler dans les installations , rehabj?
ai eu la chance de travailler dans les rehab installation ,j?
ai la chance de travailler dans un centre de de?sintoxication,(Line 597)d?
origine , les ide?es qui ont de la valeur ?d?
avoir des ide?es originales qui ont de la valeur ?d?
avoir des ide?es originales qui ont de la valeur ?
(Line 752)un nom qui appartient a` quelque chose d?
autre , le soleil .un nom qui appartient a` autre chose , le soleil .le nom d?
une autre chose , le soleil .Figure 3: Three examples of improvement in MT results:the first sentence in each collection corresponds to thebaseline, the second utilizes the adapted TED LMs, andthe third is the reference translation.7 ConclusionsAn alternative approach to bilingual topic modelinghas been presented that integrates the PLSA frame-work with MDI adaptation that can effectively adapta background language model when given a docu-ment in the source language.
Rather than trainingtwo topic models and enforcing a one-to-one cor-respondence for translation, we use the assumptionthat parallel texts refer to the same topics and havea very similar topic distribution.
Preliminary exper-iments show a reduction in perplexity and an overallimprovement in BLEU and NIST scores on speechtranslation.
We also note that, unlike previous worksinvolving topic modeling, we did not remove stopwords and punctuation, but rather assumed that thesefeatures would have a relatively uniform topic distri-bution.One downside to the MDI adaptation approachis that the computation of the normalization termz(h) is expensive and potentially prohibitive duringcontinuous speech translation tasks.
Further investi-gation is needed to determine if there is a suitableapproximation that avoids computing probabilitiesacross all n-grams.AcknowledgmentsThis work was supported by the T4ME network ofexcellence (IST-249119), funded by the DG INFSOof the European Commission through the SeventhFramework Programme.
The first author received agrant under the Erasmus Mundus Language & Com-munication Technologies programme.ReferencesDavid M. Blei, Andrew Ng, and Michael Jordan.Latent Dirichlet Allocation.
JMLR, 3:993?1022,2003.Stanley F. Chen and Joshua Goodman.
An empiricalstudy of smoothing techniques for language mod-eling.
Computer Speech and Language, 4(13):359?393, 1999.J.
N. Darroch and D. Ratcliff.
Generalized itera-tive scaling for log-linear models.
The Annals ofMathematical Statistics, 43(5):1470?1480, 1972.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harsh-man.
Indexing by Latent Semantic Analysis.Journal of the American Society for InformationScience, 41:391?407, 1990.A.
P. Dempster, N. M. Laird, and D. B. Rubin.Maximum-likelihood from incomplete data viathe EM algorithm.
Journal of the Royal Statis-tical Society, B, 39:1?38, 1977.Marcello Federico.
Efficient language model adap-tation through MDI estimation.
In Proceedings ofthe 6th European Conference on Speech Commu-nication and Technology, volume 4, pages 1583?1586, Budapest, Hungary, 1999.Marcello Federico.
Language Model Adaptationthrough Topic Decomposition and MDI Estima-tion.
In Proceedings of the IEEE InternationalConference on Acoustics, Speech and Signal Pro-cessing, volume I, pages 703?706, Orlando, FL,2002.300Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
IRSTLM: an Open Source Toolkit for Han-dling Large Scale Language Models.
In Pro-ceedings of Interspeech, pages 1618?1621, Mel-bourne, Australia, 2008.George Foster and Roland Kuhn.
Mixture-modeladaptation for SMT.
In Proceedings of the Sec-ond Workshop on Statistical Machine Transla-tion, pages 128?135, Prague, Czech Republic,June 2007.
Association for Computational Lin-guistics.
URL http://www.aclweb.org/anthology/W/W07/W07-0217.Zhengxian Gong, Yu Zhang, and Guodong Zhou.Statistical Machine Translation based on LDA.In Universal Communication Symposium (IUCS),2010 4th International, pages 286 ?290, oct.2010.
doi: 10.1109/IUCS.2010.5666182.Thomas Hofmann.
Probabilistic Latent SemanticAnalysis.
In Proceedings of the 15th Conferenceon Uncertainty in AI, pages 289?296, Stockholm,Sweden, 1999.Bo-June (Paul) Hsu and James Glass.
Style & topiclanguage model adaptation using HMM-LDA.
Inin Proc.
ACL Conf.
on Empirical Methods in Nat-ural Language Processing ?
EMNLP, pages 373?381, 2006.Reinhard Kneser, Jochen Peters, and DietrichKlakow.
Language Model Adaptation Using Dy-namic Marginals.
In Proceedings of the 5th Euro-pean Conference on Speech Communication andTechnology, pages 1971?1974, Rhodes, Greece,1997.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Con-stantin, and E. Herbst.
Moses: Open SourceToolkit for Statistical Machine Translation.
InProceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics Com-panion Volume Proceedings of the Demo andPoster Sessions, pages 177?180, Prague, CzechRepublic, 2007.
URL http://aclweb.org/anthology-new/P/P07/P07-2045.pdf.Philipp Koehn and Josh Schroeder.
Experi-ments in Domain Adaptation for Statistical Ma-chine Translation.
In Proceedings of the Sec-ond Workshop on Statistical Machine Transla-tion, pages 224?227, Prague, Czech Republic,June 2007.
Association for Computational Lin-guistics.
URL http://www.aclweb.org/anthology/W/W07/W07-0233.David Mimno, Hanna M. Wallach, Jason Narad-owsky, David A. Smith, and Andrew McCallum.Polylingual Topic Models.
In Proceedings ofthe 2009 Conference on Empirical Methods inNatural Language Processing.
Association forComputational Linguistics, August 2009.
URLhttp://www.cs.umass.edu/?mimno/papers/mimno2009polylingual.pdf.Franz Josef Och.
Minimum Error Rate Train-ing in Statistical Machine Translation.
In Er-hard Hinrichs and Dan Roth, editors, Proceed-ings of the 41st Annual Meeting of the As-sociation for Computational Linguistics, pages160?167, 2003.
URL http://www.aclweb.org/anthology/P03-1021.pdf.Abhinav Sethy, Panayiotis Georgiou, and ShrikanthNarayanan.
Selecting relevant text subsetsfrom web-data for building topic specific lan-guage models.
In Proceedings of the Hu-man Language Technology Conference of theNAACL, Companion Volume: Short Papers,pages 145?148, New York City, USA, June2006.
Association for Computational Linguis-tics.
URL http://www.aclweb.org/anthology/N/N06/N06-2037.Yik-Cheung Tam and Tanja Schultz.
Incorporatingmonolingual corpora into bilingual latent seman-tic analysis for crosslingual lm adaptation.
InAcoustics, Speech and Signal Processing, 2009.ICASSP 2009.
IEEE International Conference on,pages 4821 ?4824, april 2009. doi: 10.1109/ICASSP.2009.4960710.Yik-Cheung Tam, Ian Lane, and Tanja Schultz.Bilingual LSA-based adaptation for statisticalmachine translation.
Machine Translation,21:187?207, December 2007.
ISSN 0922-6567. doi: 10.1007/s10590-008-9045-2.
URLhttp://portal.acm.org/citation.cfm?id=1466799.1466803.Bing Zhao and Eric P. Xing.
HM-BiTAM: Bilin-gual topic exploration, word alignment, and trans-301lation.
In J.C. Platt, D. Koller, Y.
Singer, andS.
Roweis, editors, Advances in Neural Informa-tion Processing Systems 20, pages 1689?1696.MIT Press, Cambridge, MA, 2008.Bing Zhao, Matthias Eck, and Stephan Vogel.
Lan-guage Model Adaptation for Statistical MachineTranslation via Structured Query Models.
In Pro-ceedings of Coling 2004, pages 411?417, Geneva,Switzerland, Aug 23?Aug 27 2004.
COLING.302
