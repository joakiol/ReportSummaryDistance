Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 19?26,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsDetecting Hate Speech on the World Wide WebWilliam Warner and Julia HirschbergColumbia UniversityDepartment of Computer ScienceNew York, NY 10027whw2108@columbia.edu, julia@cs.columbia.eduAbstractWe present an approach to detecting hatespeech in online text, where hate speech isdefined as abusive speech targeting specificgroup characteristics, such as ethnic origin, re-ligion, gender, or sexual orientation.
Whilehate speech against any group may exhibitsome common characteristics, we have ob-served that hatred against each different groupis typically characterized by the use of a smallset of high frequency stereotypical words;however, such words may be used in eithera positive or a negative sense, making ourtask similar to that of words sense disambigua-tion.
In this paper we describe our definitionof hate speech, the collection and annotationof our hate speech corpus, and a mechanismfor detecting some commonly used methodsof evading common ?dirty word?
filters.
Wedescribe pilot classification experiments inwhich we classify anti-semitic speech reach-ing an accuracy 94%, precision of 68% andrecall at 60%, for an F1 measure of .6375.1 IntroductionHate speech is a particular form of offensive lan-guage that makes use of stereotypes to express anideology of hate.
Nockleby (Nockleby, 2000) de-fines hate speech as ?any communication that dis-parages a person or a group on the basis of somecharacteristic such as race, color, ethnicity, gen-der, sexual orientation, nationality, religion, or othercharacteristic.?
In the United States, most hatespeech is protected by the First Amendment ofthe U. S. Constitution, which, except for obscen-ity, ?fighting words?
and incitement, guarantees theright to free speech, and internet commentators exer-cise this right in online forums such as blogs, news-groups, Twitter and Facebook.
However, terms ofservice for such hosted services typically prohibithate speech.
Yahoo!
Terms Of Service 1 prohibitsposting ?Content that is unlawful, harmful, threat-ening, abusive, harassing, tortuous, defamatory, vul-gar, obscene, libelous, invasive of another?s privacy,hateful, or racially, ethnically or otherwise objec-tionable.?
Facebook?s terms 2 are similar, forbid-ding ?content that: is hateful, threatening, or porno-graphic; incites violence.?
While user submissionsare typically filtered for a fixed list of offensivewords, no publicly available automatic classifier cur-rently exists to identify hate speech itself.In this paper we describe the small amount of ex-isting literature relevant to our topic in Section 2.
InSection 3 we motivate our working definition of hatespeech.
In Section 4 we describe the resources andcorpora of hate and non-hate speech we have usedin our experiments.
In Section 5 we describe the an-notation scheme we have developed and interlabelerreliability of the labeling process.
In Section 6 wedescribe our approach to the classification problemand the features we used.
We present preliminaryresults in Section 7, follow with an analysis of clas-sification errors in 8 and conclude in Section 9 withan outline of further work.1Yahoo TOS, paragraph 9ahttp://info.yahoo.com/legal/us/yahoo/utos/utos-173.html2Facebook TOS, paragraph 3.7https://www.facebook.com/legal/terms192 Previous LiteratureThere is little previous literature on identifying hatespeech.In (A Razavi, Diana Inkpen, Sasha Uritsky, StanMatwin, 2010), the authors look for Internet?flames?
in newsgroup messages using a three-stageclassifier.
The language of flames is significantlydifferent from hate speech, but their method couldinform our work.
Their primary contribution is adictionary of 2700 hand-labeled words and phrases.In (Xu and Zhu, 2010), the authors look for offen-sive language in YouTube comments and replacesall but the first letter of each word with asterisks.Again, while the language and the goal is different,the method may have some value for detecting hatespeech.
Their detection method parses the text andarranges it into a hierarchy of clauses, phrases andindividual words.
Both the annotation and the clas-sification strategies found in this paper are based onthe sentiment analysis work found in (Pang and Lee,2008) and (Pang, Lee and Vaithyanathan, 2002).3 Defining Hate SpeechThere are numerous issues involved in defining whatconstitutes hate speech, which need to be resolved inorder to annotate a corpus and develop a consistentlanguage model.
First, merely mentioning, or evenpraising, an organization associated with hate crimesdoes not by itself constitute hate speech.
The name?Ku Klux Klan?
by itself is not hateful, as it may ap-pear in historical articles, legal documents, or otherlegitimate communication.
Even an endorsement ofthe organization does not constitute a verbal attackon another group.
While one may hypothesize thatsuch endorsements are made by authors who wouldalso be comfortable with hateful language, by them-selves, we do not consider these statements to behate speech.For the same reason, an author?s excessive pridein his own race or group doesn?t constitute hatespeech.
While such boasting may seem offensiveand likely to co-occur with hateful language, a dis-paragement of others is required to satisfy the defi-nition.For example, the following sentence does not con-stitute hate speech, even though it uses the word?Aryan?.And then Aryan pride will be true becausehumility will come easily to Aryans whowill all by then have tasted death.On the other hand, we believe that unnecessarylabeling of an individual as belonging to a group of-ten should be categorized as hate speech.
In the fol-lowing example, hate is conveyed when the authorunnecessarily modifies bankers and workers with?jew?
and ?white.
?The next new item is a bumper sticker thatreads: ?Jew Bankers Get Bailouts, WhiteWorkers Get Jewed!?
These are only 10cents each and require a minimum of a$5.00 orderUnnecessarily calling attention to the race or eth-nicity of an individual appears to be a way for anauthor to invoke a well known, disparaging stereo-type.While disparaging terms and racial epithets whenused with the intent to harm always constitute hate-ful language, there are some contexts in which suchterms are acceptable.
For example, such wordsmight be acceptable in a discussion of the wordsthemselves.
For example:Kike is a word often used when trying tooffend a jew.Sometimes such words are used by a speaker whobelongs to the targeted group, and these may be hardto classify without that knowledge.
For example:Shit still happenin and no one is hearinabout it, but niggas livin it everyday.African American authors appear to use the ?N?word with a particular variant spelling, replacing?er?
with ?a?, to indicate group solidarity (Stephens-Davidowitz, 2011).
Such uses must be distin-guished from hate speech mentions.
For our pur-poses, if the identity of the speaker cannot be as-certained, and if no orthographic or other contextualcues are present, such terms are categorized as hate-ful.204 Resources and CorporaWe received data from Yahoo!
and the AmericanJewish Congress (AJC) to conduct our research onhate speech.
Yahoo!
provided data from its newsgroup posts that readers had found offensive.
TheAJC provided pointers to websites identified as of-fensive.Through our partnership with the American Jew-ish Congress, we received a list of 452 URLs previ-ously obtained from Josh Attenberg (Attenberg andProvost, 2010) which were originally collected toclassify websites that advertisers might find unsuit-able.
After downloading and examining the textfrom these sites, we found a significant number thatcontained hate speech according to our working def-inition; in particular, a significant number were anti-semitic.
We noted, however, that sites which whichappeared to be anti-semitic rarely contained explic-itly pejorative terms.
Instead, they presented sci-entifically worded essays presenting extremely anti-semitic ideologies and conclusions.
Some texts con-tained frequent references to a well known hategroup, but did not themselves constitute examples ofhate speech.
There were also examples containingonly defensive statements or declarations of pride,rather than attacks directed toward a specific group.In addition to the data we collected from theseURLs, Yahoo!
provided us with several thou-sand comments from Yahoo!
groups that had beenflagged by readers as offensive, and subsequentlypurged by administrators.
These comments areshort, with an average of length of 31 words, andlacked the contextual setting in which they wereoriginally found.
Often, these purged commentscontained one or more offensive words, but obscuredwith an intentional misspelling, presumably to evadea filter employed by the site.
For common racial ep-ithets, often a single character substitution was used,as in ?nagger?, or a homophone was employed, suchas ?joo.?
Often an expanded spelling was employed,in which each character was separated by a spaceor punctuation mark, so that ?jew?
would become?j@e@w@.
?The two sources of data were quite different, butcomplementary.The Yahoo!
Comment data contained many ex-amples of offensive language that was sometimeshateful and sometimes not, leading to our hypoth-esis that hate speech resembles a word sense dis-ambiguation task, since, a single word may appearquite frequently in hate and non-speech texts.
Anexample is the word ?jew?.
In addition, it provideduseful examples of techniques used to evade simplelexical filters (in case such exist for a particular fo-rum).
Such evasive behavior generally constitutes apositive indicator of offensive speech.Web data captured from Attenberg?s URLs tendedto include longer texts, giving us more context,and contained additional lower frequency offensiveterms.
After examining this corpus, we decidedto attempt our first classification experiments at theparagraph level, to make use of contextual features.The data sets we received were considered offen-sive, but neither was labeled for hate speech per se.So we developed a labeling manual for annotatinghate speech and asked annotators to label a corpusdrawn from the web data set.5 Corpus Collection and AnnotationWe hypothesize that hate speech often employs wellknown stereotypes to disparage an individual orgroup.
With that assumption, we may be further sub-divide such speech by stereotype, and we can distin-guish one form of hate speech from another by iden-tifying the stereotype in the text.
Each stereotypehas a language all its own, with one-word epithets,phrases, concepts, metaphors and juxtapositions thatconvey hateful intent.
Anti-hispanic speech mightmake reference to border crossing or legal identi-fication.
Anti-African American speech often ref-erences unemployment or single parent upbringing.And anti-semitic language often refers to money,banking and media.Given this, we find that creating a language modelfor each stereotype is a necessary prerequisite forbuilding a model for all hate speech.
We decided tobegin by building a classifier for anti-semitic speech,which is rich with references to well known stereo-types.The use of stereotypes also means that some lan-guage may be regarded as hateful even though nosingle word in the passage is hateful by itself.
Of-ten there is a relationship between two or more sen-tences that show the hateful intent of the author.21Using the website data, we captured paragraphsthat matched a general regular expression of wordsrelating to Judaism and Israel 3.
This resulted inabout 9,000 paragraphs.
Of those, we rejected thosethat did not contain a complete sentence, containedmore than two unicode characters in a row, wereonly one word long or longer than 64 words.Next we identified seven categories to whichlabelers would assign each paragraph.
Annota-tors could label a paragraph as anti-semitic, anti-black, anti-asian, anti-woman, anti-muslim, anti-immigrant or other-hate.
These categories were de-signed for annotation along the anti-semitic/not anti-semitic axis, with the identification of other stereo-types capturing mutual information between anti-semitism and other hate speech.
We were interestedin the correlation of anti-semitism with other stereo-types.
The categories we chose reflect the contentwe encountered in the paragraphs that matched theregular expression.We created a simple interface to allow labelersto assign one or more of the seven labels to eachparagraph.
We instructed the labelers to lump to-gether South Asia, Southeast Asia, China and therest of Asia into the category of anti-asian.
Theanti-immigrant category was used to label xenopho-bic speech in Europe and the United States.
Other-hate was most often used for anti-gay and anti-whitespeech, whose frequency did not warrant categoriesof their own.5.1 Interlabeler Agreement and LabelingQualityWe examined interlabeler agreement only for theanti-semitic vs. other distinction.
We had a set of1000 paragraphs labeled by three different annota-tors.
The Fleiss kappa interlabeler agreement foranti-semitic paragraphs vs. other was 0.63.
We cre-ated two corpora from this same set of 1000 para-graphs.
First, the majority corpus was generatedfrom the three labeled sets by selecting the labelwith on which the majority agreed.
Upon examin-ing this corpus with the annotators, we found somecases in which annotators had agreed upon labelsthat seemed inconsistent with their other annotations3jewish|jew|zionist|holocaust|denier|rabbi|israel|semitic|semite?
often they had missed instances of hate speechwhich they subsequently felt were clear cases.
Oneof the authors checked and corrected these apparent?errors?
in annotator labeling to create a gold cor-pus.
Results for both the original majority class an-notations and the ?gold?
annotations are presentedin Section 7.As a way of gauging the performance of humanannotators, we compared two of the annotators?
la-bels to the gold corpus by treating their labeled para-graphs as input to a two fold cross validation ofthe classifier constructed from the gold corpus.
Wecomputed a precision of 59% and recall of 68% forthe two annotators.
This sets an upper bound on theperformance we should expect from a classifier.6 Classification ApproachWe used the template-based strategy presented in(Yarowsky, 1994) to generate features from the cor-pus.
Each template was centered around a singleword as shown in Table 1.
Literal words in an or-dered two word window on either side of a givenword were used exactly as described in (Yarowsky,1994).
In addition, a part-of-speech tagging of eachsentence provided the similar part-of-speech win-dows as features.
Brown clusters as described in(Koo, Carreras and Collins, 2008) were also utilizedin the same window.
We also used the occurrence ofwords in a ten word window.
Finally, we associatedeach word with the other labels that might have beenapplied to the paragraph, so that if a paragraph con-taining the word ?god?
were labeled ?other-hate?, afeature would be generated associating ?god?
withother-hate: ?RES:other-hate W+0:god?.We adapted the hate-speech problem to the prob-lem of word sense disambiguation.
We say thatwords have a stereotype sense, in that they eitheranti-semitic or not, and we can learn the sense ofall words in the corpus from the paragraph labels.We used a process similar to the one Yarowsky de-scribed when he constructed his decisions lists, butwe expand the feature set.
What is termed log-likelihood in (Yarowsky, 1994) we will call log-odds, and it is calculated in the following way.
Alltemplates were generated for every paragraph in thecorpus, and a count of positive and negative occur-rences for each template was maintained.
The ab-22solute value of the ratio of positive to negative oc-currences yielded the log-odds.
Because log-odds isbased on a ratio, templates that do not occur at leastonce as both positive and negative are discarded.
Afeature is comprised of the template, its log-odds,and its sense.
This process produced 4379 features.Next, we fed these features to an SVM classifier.In this model, each feature is dimension in a fea-ture vector.
We treated the sense as a sign, 1 foranti-semitic and -1 otherwise, and the weight of eachfeature was the log-odds times the sense.
The taskof classification is sensitive to weights that are largerelative to other weights in the feature space.
Toaddress this, we eliminated the features whose log-odds fell below a threshold of 1.5.
The resulting val-ues passed to the SVM ranged from -3.99 to -1.5 andfrom +1.5 to +3.2.
To find the threshold, we gener-ated 40 models over an evenly distributed range ofthresholds and selected the value that optimized themodel?s f-measure using leave-1-out validation.
Weconducted this procedure for two sets of indepen-dent data and in both cases ended up with a log-oddsthreshold of 1.5.
After the elimination process, wewere left with 3537 features.The most significant negative feature was the un-igram literal ?black,?, with log-odds 3.99.The most significant positive feature was the part-of-speech trigram ?DT jewish NN?, or a determinerfollowed by jewish followed by a noun.
It was as-signed a log-odds of 3.22.In an attempt to avoid setting a threshold, wealso experimented with binary features, assigning -1to negative feature weights and +1 to positive fea-ture weights, but this had little effect, and are notrecorded in this paper.
Similarly, adjusting the SVMsoft margin parameter C had no effect.We also created two additional feature sets.
Theall unigram set contains only templates that arecomprised of a single word literal.
This set con-tained 272 features, and the most significant re-mained ?black.?
The most significant anti-semiticfeature of this set was ?television,?
with a log-oddsof 2.28.
In the corpus we developed, television fig-ures prominently in conspiracy theories our labelersfound anti-semitic.The positive unigram set contained only unigramtemplates with a positive (indicating anti-semitism)log-odds.
This set contained only 13 features, andthe most significant remained ?television.
?7 Preliminary Results7.1 Baseline AccuracyWe established a baseline by computing the ac-curacy of always assuming the majority (not anti-semitic) classification.
If N is the number of sam-ples and Np is the number of positive (anti-semitic)samples, accuracy is given by (N ?
Np)/N , whichyielded a baseline accuracy of 0.910.7.2 ClassifiersFor each of the majority and gold corpora, wegenerated a model for each type of feature tem-plate strategy, resulting in six classifiers.
We usedSVM light (Joachims, 1999) with a linear kernelfunction.
We performed 10 fold cross validation foreach classifier and recorded the results in Table 2.As expected, our results on the majority corpus werenot as accurate as those on the gold corpus.
Perhapssurprising is that unigram feature sets out performedthe full set, with the smallest feature set, comprisedof only positive unigrams, performing the best.8 Error AnalysisTable 3 contains a summary of errors made by allthe classifiers.
For each classifier, the table reportsthe two kinds of errors a binary classifier can make:false negatives (which drive down recall), and falsepositives (which drive down precision).The following paragraph is clearly anti-semitic,and all three annotators agreed.
Since the classifierfailed to detect the anti-semitism, we use look at thisexample of a false negative for hints to improve re-call.4.
That the zionists and their americansympathizers, in and out of the americanmedia and motion picture industry, whoconstantly use the figure of ?six million?have failed to offer even a shred of evi-dence to prove their charge.23Table 1: Example Feature Templatesunigram ?W+0:america?template literal ?W-1:you W+0:know?template literal ?W-1:go W+0:back W+1:to?template part of speech ?POS-1:DT W+0:age POS+1:IN?template Brown sub-path ?W+0:karma BRO+1:0x3fc00:0x9c00 BRO+2:0x3fc00:0x13000?occurs in ?10 word window ?WIN10:lost W+0:war?other labels ?RES:anti-muslim W+0:jokes?Table 2: Classification PerformanceAccuracy Precision Recall F1Majority All Unigram 0.94 0.00 0.00 0.00Majority Positive Unigram 0.94 0.67 0.07 0.12Majority Full Classifier 0.94 0.45 0.08 0.14Gold All Unigram 0.94 0.71 0.51 0.59Gold Positive Unigram 0.94 0.68 0.60 0.63Gold Full Classifier 0.93 0.67 0.36 0.47Human Annotators 0.96 0.59 0.68 0.63Table 3: Error ReportFalse Negative False PositiveMajority All Unigram 6.0% 0.1%Majority Positive Unigram 5.6% 0.2%Majority Full Classifier 5.5% 0.6%Gold All Unigram 4.4% 1.8%Gold Positive Unigram 3.6% 2.5%Gold Full Classifier 5.7% 1.6%24The linguistic features that clearly flag this para-graph as anti-semitic are the noun phrase containingzionist ... sympathizers, the gratuitous inclusion ofmedia and motion picture industry and the skepti-cism indicated by quoting the phrase ?six million?.It is possible that the first feature could have been de-tected by adding parts of speech and Brown Clusterpaths to the 10 word occurrence window.
A methodfor detecting redundancy might also be employed todetect the second feature.
Recent work on emotionalspeech might be used to detect the third.The following paragraph is more ambiguous.
Theannotator knew that GT stood for gentile, which leftthe impression of an intentional misspelling.
Withthe word spelled out, the sentence might not be anti-semitic.18 ) A jew and a GT mustn?t be buried sideby side.Specialized knowledge of stereotypical languageand the various ways that its authors mask it couldmake a classifier?s performance superior to that ofthe average human reader.The following sentence was labeled negative byannotators but the classifier predicted an anti-semiticlabel.What do knowledgeable jews say?This false positive is nothing more than a case ofover fitting.
Accumulating more data containing theword ?jews?
in the absence of anti-semitism wouldfix this problem.9 Conclusions and Future WorkUsing the feature templates described by Yarowskywe successfully modeled hate speech as a classifica-tion problem.
In terms of f-measure, our best classi-fier equaled the performance of our volunteer anno-tators.
However, bigram and trigram templates de-graded the performance of the classifier.
The learn-ing phase of the classifier is sensitive to features thatought to cancel each other out.
Further research onclassification methods, parameter selection and op-timal kernel functions for our data is necessary.Our definition of the labeling problem could havebeen more clearly stated to our annotators.
The anti-immigrant category in particular may have confusedsome.The recall of the system is low.
This suggeststhere are larger linguistic patterns that our shallowparses cannot detect.
A deeper parse and an analysisof the resulting tree might reveal significant phrasepatterns.
Looking for patterns of emotional speech,as in (Lipscombe, Venditti and Hirschberg, 2003)could also improve our recall.The order of the paragraphs in their original con-text could be used as input into a latent variablelearning model.
McDonald (McDonald et al 2007)has reported some success mixing fine and courselabeling in sentiment analysis.AcknowledgmentsThe authors are grateful for the data and the sup-port of Matthew Holtzman of the American JewishCongress.
We would also like to thank Belle Tseng,Kim Capps-Tanaka, Evgeniy Gabrilovich and Mar-tin Zinkevich of Yahoo!
for providing data as wellas for their financial support.
Without their support,this research would not have been possible.References[Choi et al2005] Yejin Choi, Claire Cardie, Ellen Riloff,Siddharth Patwardhan, Identifying Sources of Opin-ions with Conditional Random Fields and ExtractionPatterns.
In HLT ?05 Association for ComputationalLinguistics Stroudsburg, PA, USA, pp.
355-362, 2005[Yarowsky 1994] David Yarowsky, Decision Lists forLexical Ambiguity Resolution: Application to Ac-cent Restoration in Spanish and French.
In ACL-94,Stroudsburg, PA, pp.
88-95, 1994[Yarowsky 1995] David Yarowsky, Unsupervised WordSense Disambiguation Rivaling Supervised Methods.In ACL-95, Cambridge, MA, pp.
189-196, 1995.
[Nockleby 2000] John T. Nockleby, Hate Speech.
InEncyclopedia of the American Constitution (2nd ed.,edited by Leonard W. Levy, Kenneth L. Karst et al,New York: Macmillan, 2000), pp.
1277-1279 (seehttp://www.jiffynotes.com/a_study_guides/book_notes/eamc_03/eamc_03_01193.html)[Stephens-Davidowitz 2011] Seth Stephens-Davidowitz,The Effects of Racial Animus on Voting: Evidence Us-ing Google Search Data http://www.people.fas.harvard.edu/?sstephen/papers/RacialAnimusAndVotingSethStephensDavidowitz.pdf25[McDonald et al2007] McDonald, R. Hannan, K. Ney-lon, T. Wells, M. Reynar, J.
Structured Modelsfor Fine-to-Coarse Sentiment Analysis.
In ANNUALMEETING- ASSOCIATION FOR COMPUTATIONALLINGUISTICS 2007, CONF 45; VOL 1, pages 432-439[Pang and Lee 2008] Pang, Bo and Lee, Lillian, OpinionMining and Sentiment Analysis.
In Foundations andTrends in Information Retrieval, issue 1-2, vol.
2, NowPublishers Inc., Hanover, MA, USA, 2008 pp.
1?135[Pang, Lee and Vaithyanathan 2002] Pang, Bo and Lee,Lillian and Vaithyanathan, Shivakumar Thumbs up?
:sentiment classification using machine learning tech-niques.
In Proceedings of the ACL-02 conference onEmpirical methods in natural language processing -Volume 10, Association for Computational Linguis-tics, Stroudsburg, PA, USA, 2002 pp.
79-86[Qiu et al2009] Qiu, Guang and Liu, Bing and Bu, Jia-jun and Chen, Chun Expanding domain sentiment lexi-con through double propagation.
In Proceedings of the21st international jont conference on Artificial intelli-gence, Morgan Kaufmann Publishers Inc. San Fran-cisco, CA, USA, 2009 pp.
1199-1204[Joachims 1999] Making large-Scale SVM LearningPractical.
Advances in Kernel Methods - SupportVector Learning, B. Schlkopf and C. Burges and A.Smola (ed.
), MIT-Press, 1999.
[Koo, Carreras and Collins 2008] Simple Semi-supervised Dependency Parsing In Proc.
ACL/HLT2008[Xu and Zhu 2010] Filtering Offensive Language in On-line Communities using Grammatical Relations[A Razavi, Diana Inkpen, Sasha Uritsky, Stan Matwin 2010]Offensive Language Detection Using Multi-level Clas-sification In Advances in Artificial IntelligenceSpringer, 2010, pp.
1627[Attenberg and Provost 2010] Why Label When You CanSearch?
: Alternatives to active learning for applyinghuman resources to build classification models underextreme class imbalance, KDD 2010[Lipscombe, Venditti and Hirschberg 2003] ClassifyingSubject Ratings of Emotional Speech Using AcousticFeatures.
In Proceedings of Eurospeech 2003, Geneva.26
