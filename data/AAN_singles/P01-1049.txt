Building Semantic Perceptron Net for Topic SpottingJimin Liu  and  Tat-Seng ChuaSchool of ComputingNational University of SingaporeSINGAPORE 117543{liujm, chuats}@comp.nus.edu.sgAbstractThis paper presents an approach toautomatically build a semanticperceptron net (SPN) for topic spotting.It uses context at the lower layer toselect the exact meaning of key words,and employs a combination of context,co-occurrence statistics and thesaurus togroup the distributed but semanticallyrelated words within a topic to formbasic semantic nodes.
The semanticnodes are then used to infer the topicwithin an input document.
Experimentson Reuters 21578 data set demonstratethat SPN is able to capture the semanticsof topics, and it performs well on topicspotting task.1.
IntroductionTopic spotting is the problem of identifying thepresence of a predefined topic in a text document.More formally, given a set of n topics together witha collection of documents, the task is to determinefor each document the probability that one or moretopics is present in the document.
Topic spottingmay be used to automatically assign subject codesto newswire stories, filter electronic emails and on-line news, and pre-screen document in informationretrieval and information extraction applications.Topic spotting, and its related problem of textcategorization, has been a hot area of research forover a decade.
A large number of techniques havebeen proposed to tackle the problem, including:regression model, nearest neighbor classification,Bayesian probabilistic model, decision tree,inductive rule learning, neural network, on-linelearning, and, support vector machine (Yang & Liu,1999; Tzeras & Hartmann, 1993).
Most of thesemethods are word-based and consider only therelationships between the features and topics, butnot the relationships among features.It is well known that the performance of theword-based methods is greatly affected by the lackof linguistic understanding, and, in particular, theinability to handle synonymy and polysemy.
Anumber of simple linguistic techniques has beendeveloped to alleviate such problems, ranging fromthe use of stemming, lexical chain and thesaurus(Jing & Tzoukermann, 1999; Green, 1999), toword-sense disambiguation (Chen & Chang, 1998;Leacock et al 1998; Ide & Veronis, 1998) andcontext (Cohen & Singer, 1999; Jing &Tzoukermann, 1999).The connectionist approach has been widelyused to extract knowledge in a wide range ofinformation processing tasks including naturallanguage processing, information retrieval andimage understanding (Anderson, 1983; Lee &Dubin, 1999; Sarkas & Boyer, 1995; Wang &Terman, 1995).
Because the connectionistapproach closely resembling human cognitionprocess in text processing, it seems natural to adoptthis approach, in conjunction with linguisticanalysis, to perform topic spotting.
However, therehave been few attempts in this direction.
This ismainly because of difficulties in automaticallyconstructing the semantic networks for the topics.In this paper, we propose an approach toautomatically build a semantic perceptron net(SPN) for topic spotting.
The SPN is aconnectionist model with hierarchical structure.
Ituses a combination of context, co-occurrencestatistics and thesaurus to group the distributed butsemantically related words to form basic semanticnodes.
The semantic nodes are then used to identifythe topic.
This paper discusses the design,implementation and testing of an SPN for topicspotting.The paper is organized as follows.
Section 2discusses the topic representation, which is theprototype structure for SPN.
Sections 3 & 4respectively discuss our approach to extract thesemantic correlations between words, and buildsemantic groups and topic tree.
Section 5 describesthe building and training of SPN, while Section 6presents the experiment results.
Finally, Section 7concludes the paper.2.
Topic RepresentationThe frame of Minsky (1975) is a well-knownknowledge representation technique.
A framerepresents a high-level concept as a collection ofslots, where each slot describes one aspect of theconcept.
The situation is similar in topic spotting.For example, the topic ?water?
may have manyaspects (or sub-topics).
One sub-topic may beabout ?water supply?, while the other is about?water and environment protection?, and so on.These sub-topics may have some commonattributes, such as the word ?water?, and each sub-topic may be further sub-divided into finer sub-topics, etc.The above points to a hierarchical topicrepresentation, which corresponds to the hierarchyof document classes (Figure 1).
In the model, thecontents of the topics and sub-topics (shown ascircles) are modeled by a set of attributes, which issimply a group of semantically related words(shown as solid elliptical shaped bags orrectangles).
The context (shown as dotted ellipses)is used to identify the exact meaning of a word.topica wordthe context of a wordSub-topicAspect attributecommon attributeFigure 1.
Topic representationHofmann (1998) presented a word occurrencebased cluster abstraction model that learns ahierarchical topic representation.
However, themethod is not suitable when the set of trainingexamples is sparse.
To avoid the problem ofautomatically constructing the hierarchical model,Tong et al(1987) required the users to supply themodel, which is used as queries in the system.Most automated methods, however, avoided thisproblem by modeling the topic as a feature vector,rule set, or instantiated example (Yang & Liu,1999).
These methods typically treat each wordfeature as independent, and seldom considerlinguistic factors such as the context or lexicalchain relations among the features.
As a result,these methods are not good at discriminating alarge number of documents that typically lie nearthe boundary of two or more topics.In order to facilitate the automatic extractionand modeling of the semantic aspects of topics, weadopt a compromise approach.
We model the topicas a tree of concepts as shown in Figure 1.However, we consider only one level of hierarchybuilt from groups of semantically related words.These semantic groups may not correspond strictlyto sub-topics within the domain.
Figure 2 shows anexample of an automatically constructed topic treeon ?water?.ContextsBasic SemanticNodesTopicpriceagreementwatertonwasteenvironmentbankprovidecostumercorporationplantrainrainfalldry  waterwaterwaterrivertouristf ed c b aFigure 2.
An example of a topic treeIn Figure 2, node ?a?
contains the commonfeature set of the topic; while nodes ?b?, ?c?
and?d?
are related to sub-topics on ?water supply?,?rainfall?, and ?water and environment protection?respectively.
Node ?e?
is the context of the word?plant?, and node ?f?
is the context of the word?bank?.
Here we use training to automaticallyresolve the corresponding relationship between anode and an attribute, and the context word to beused to select the exact meaning of a word.
Fromthis representation, we observe that:a) Nodes ?c?
and ?d?
are closely related and maynot be fully separable.
In fact, it is sometimesdifficult even for human experts to decide howto divide them into separate topics.b) The same word, such as ?water?, may appear inboth the context node and the basic semanticnode.c) Some words use context to resolve theirmeanings, while many do not need context.3.
Semantic CorrelationsAlthough there exists many methods to derive thesemantic correlations between words (Lee, 1999;Lin, 1998; Karov & Edelman, 1998; Resnik, 1995;Dagan et al 1995), we adopt a relatively simpleand yet practical and effective approach to derivethree topic -oriented semantic correlations:thesaurus-based, co-occurrence-based and context-based correlation.3.1 Thesaurus based correlationWordNet is an electronic thesaurus popularly usedin many researches on lexical semantic acquisition,and word sense disambiguation (Green, 1999;Leacock et al 1998).
In WordNet, the sense of aword is represented by a list of synonyms (synset),and the lexical information is represented in theform of a semantic network.However, it is well known that the granularityof semantic meanings of words in WordNet is oftentoo fine for practical use.
We thus need to enlargethe semantic granularity of words in practicalapplications.
For example, given a topic on?children education?, it is highly likely that theword ?child?
will be a key term.
However, theconcept ?child?
can be expressed in manysemantically related terms, such as ?boy?, ?girl?,?kid?, ?child?, ?youngster?, etc.
In this case, itmight not be necessary to distinguish the differentmeaning among these words, nor the differentsenses within each word.
It is, however, importantto group all these words into a large synset {child,boy, girl, kid, youngster}, and use the synset tomodel the dominant but more general meaning ofthese words in the context.In general, it is reasonable and often useful togroup lexically related words together to representa more general concept.
Here, two words areconsidered to be lexically related if they are relatedto by the ?is_a?, ?part_of?, ?member_of?, or?antonym?
relations, or if they belong to the samesynset.
Figure 3 lists the lexical relations that weconsidered, and the examples.Since in our experiment, there are manyantonyms co-occur within the topic, we also groupantonyms together to identify a topic.
Moreover, ifa word had two senses of, say, sense-1 and sense-2.And if there are two separate words that arelexically related to this word by sense-1 and sense-2 respectively, we simply group these wordstogether and do not attempt to distinguish the twodifferent senses.
The reason is because if a word isso important to be chosen as the keyword of atopic, then it should only have one dominantmeaning in that topic.
The idea that a keywordshould have only one dominant meaning in a topicis also suggested in Church & Yarowsky (1992).cornmaizemetalzincperimportexportpersosynset         is_a              part_of       member_of      antonymtreeleaffamilysonper  Figure 3: Examples of lexical relationshipBased on the above discussion, we compute thethesaurus-based correlation between the two termst1 and t2, in topic Ti, as:1    (t1 and t2 are in the same synset, or t1=t2)0.8  (t1 and t2 have ?antonym?
relation)0..5  (t1 and t2 have relations of ?is_a?,?part_of?, or ?member_of?
)0   (others)=),( 21)( ttR iL3.2 Co-occurrence based correlationCo-occurrence relationship is like the globalcontext of words.
Using co-occurrence statistics,Veling & van der Weerd (1999) was able to findmany interesting conceptual groups in the Reuters-2178 text corpus.
Examples of the conceptualgroups found include: {water, rainfall, dry},{bomb, injured, explosion, injuries}, and {cola,PEP, Pepsi, Pespi-cola, Pepsico}.
These groupsare meaningful, and are able to capture theimportant concepts within the corpus.Since in general, high co-occurrence words arelikely to be used together to represent (or describe)a certain concept, it is reasonable to group themtogether to form a large semantic node.
Thus fortopic Ti, the co-occurrence-based correlation of twoterms, t1 and t2, is computed as:)(/)(),( 21)(21)(21)( ttdfttdfttR iiico ?
?=  (2)where )( 21)( ttdf i ?
( )( 21)( ttdf i ? )
is the fraction ofdocuments in Ti that contains t1 and (or) t2.3.3 Context based correlationBroadly speaking, there are three kinds of context:domain, topic and local contexts (Ide & Vernois,1998).
Domain context requires extensiveknowledge of domain and is not considered in thispaper.
Topic context can be modeledapproximately using the co-occurrence(1)relationships between the words in the topic.
In thissection, we will define the local context explicitly.The local context of a word t is often defined asthe set of non-trivial words near t. Here a word wdis said to be near t if their word distance is less thana given threshold, which is set to be 5 in ourexperiment.We represent the local context of term tj in topicTi by a context vector cv(i)(tj).
To derive cv(i)(tj), wefirst rank all candidate context words of ti by theirdensity values:)(/)( )()()( jikijijk tnwdm=r  (3)where )()( ji tn is the number of occurrence of tj inTi, and )()( kij wdm is the number of occurrences ofwdk near t j.
We then select from the ranking, the topten words as the context of tj in Ti as:),(),...,,(),,{()( )(10)(10)(2)(2)(1)(1)( ijijijijijijji wdwdwdtcv rrr=  (4)When the training sample is sufficiently large,the context vector will have good statisticmeanings.
Noting again that an important word to atopic should have only one dominant meaningwithin that topic, and this meaning should bereflected by its context.
We can thus draw theconclusion that if two words have a very highcontext similarity within a topic, it will have a highpossibility that they are semantic related.
Thereforeit is reasonable to group them together to form alarger semantic node.
We thus compute thecontext-based correlation between two term t1 andt2 in topic Ti as:2/12)(22/12)(1101)()(2)(1)()(2)(1)(21)(])([*])([**),(),(??
?= =kikkikkikmikikmikicoicwdwdRttRrrrr(5)where  ),(maxarg)( )(2)(1)( isikicoswdwdRkm =For example, in Reuters 21578 corpus,?company?
and ?corp?
are context-related wordswithin the topic ?acq?.
This is because they havevery similar context of ?say, header, acquire,contract?.4.
Semantic Groups & Topic TreeThere are many methods that attempt to constructthe conceptual representation of a topic from theoriginal data set (Veling & van der Weerd, 1999;Baker & McCallum, 1998; Pereira et al 1993).
Inthis Section, we will describe our semantic -basedapproach to finding basic semantic groups andconstructing the topic tree.
Given a set of trainingdocuments, the stages involved in finding thesemantic groups for each topic are given below.A) Extract all distinct terms {t1,  t2, ..tn} from thetraining document set for topic Ti.
For each termtj, compute its df(i)(tj) and cv(i)(t j), where df(i)(tj)is defined as the fraction of documents in T i thatcontain tj.
In other words, df (i)(tj) gives theconditional probability of tj appearing in Ti.B)  Derive the semantic group Gj using tj as themain keyword.
Here we use the semanticcorrelations defined in Section 3 to derive thesemantic relationship between tj and any otherterm tk.
Thus:For each pair (t j,tk), k=1,..n,  set Link(tj,tk)=1if )( iLR (tj,tk)>0,   or,df (i)(tj)>d0  and  )(icoR (tj, tk)>d1  ordf (i)(tj)>d2  and  )(icR  (tj, tk)>d 3.where d0, d1, d2, d3  are predefined thresholds.For all tk with Link(tj,tk)=1, we form a semanticgroup centered around tj denoted by:},...,,{},...,,{ 2121 njjjj ttttttG jk ?=(6)Here tj is the main keyword of node G j and isdenoted by  main(Gj)=t j.C)  Calculate the information value inf (i)(Gj) of eachbasic semantic group.
First we compute theinformation value of each tj:}1,0max{*)()(inf )()( Nptdft ijjiji -=  (7)where?==Nkkijiijtdftdfp1)()()()(and N is the number of topics.
Thus 1/N  denotesthe probability that a term is in any class, and pijdenotes the normalized conditional probabilityof tj in Ti.
Only those terms whose normalizedconditional probability is higher than 1/N willhave a positive information value.The information value of the semantic group Gjis simply the summation of information value ofits constituent terms weighted by theirmaximum semantic correlation with t j as:?==jkkkiijkji twG1)()()( )](inf*[)(inf  (8)where )},(),,(),,(max{ )()()()( kjiLkjickjicoijk ttRttRttRw =D) Select the essential semantic groups using thefollowing algorithm:a) Initialize:},...,,{ 11 nGGGS ?
,  F?Groups ,b) Select the semantic group with highestinformation value:))((infmaxarg )( kiSGkGjk ?
?c) Terminate if inf (i)(Gj) is less than apredefined threshold d4.d) Add Gj into the set Groups:jGSS -= ,  and  }{ jGGroupsGroups ?
?e) Eliminate those groups in S whose key termsappear in the selected group Gj.
That is:For each  SGk ?
, if jk GGmain ?
)( , then}{ kGSS -?f) Eliminate those terms in remaining groups inS that are found in the selected group G j.That is:For each SGk ?
,  jkk GGG -?
,and if F=kG , then  }{ kGSS -?g) If F=S  then stop; else go to step (b).In the above grouping algorithm, the predefinedthresholds d0,d1,d2,d3 are used to control the size ofeach group, and d4 is used to control the number ofgroups.The set of basic semantic groups found thenforms the sub-topics of a 2-layered topic tree asillustrated in Figure 2.5.
Building and Training of SPNThe Combination of local perception and globalarbitrator has been applied to solve perceptionproblems (Wang & Terman, 1995; Liu & Shi,2000).
Here we adopt the same strategy for topicspotting.
For each topic, we construct a localperceptron net (LPN), which is designed for aparticular topic.
We use a global expert (GE) toarbitrate all decisions of LPNs and to model therelationships between topics.
Here we discuss thedesign of both LPN and GE, and their trainingprocesses.5.1 Local Perceptron Net (LPN)We derive the LPN directly from the topic tree asdiscussed in Sectio n 2 (see Figure 2).
Each LPN isa multi- layer feed-forward neural network with atypical structure as shown in Figure 4.In Figure 4, x ij represents the feature value ofkeyword wdi j in the ith semantic group; xijk?s (wherek=1,?10) represent the feature values of the contextwords wdijk?s of keyword wd ij; and aij denotes themeaning of keyword wd i j as determined by itscontext.
Ai corresponds to the ith basic semanticnode.
The weights wi, wi j, and wijk and biases ?i and?
ij are learned from training, and y(i)(x) is the outputof the network.y(i)iAiwijwijkwijaijkxContext  key termSemanticgroupClassijxBasicmeaningq (i)ijqFigure 4: The architecture of LPN for topic iGiven a document:x = {(xi j,cv i j) | i=1,2,?m, j=1,?ij}where m is the number of basic semantic nodes, ijis the number of key terms contained in the i thsemantic node, and cv ij={xi j1,xi j2?
ijijkx } is thecontext of term x ij.
The output y(i) =y(i)(x) iscalculated as follows:?===miiiii Awxyy1)()( )(  (9)where  ])*(exp[11*?
--+=?
ijijk cvxijijkijkijij xwxaq  (10)and)exp(1)exp(111?-+?--===jjijijiijijiiawawA(11)Equation (10) expresses the fact that only if akey term is present in the document (i.e.
xij > 0), itscontext needs to be checked.For each topic Ti, there is a corresponding nety(i) =y(i)(x) and a threshold q(i).
The pair of (y(i)(x),q(i)) is a local binary classifier for Ti such that:If y(i)(x)-q(i) > 0, then Ti is present; otherwiseTi is not present in document x.From the procedures employed to building thetopic tree, we know that each feature is in fact anevidence to support the occurrence of the topic.This gives us the suggestion that the activationfunction for each node in the LPN should be a non-decreasing function of the inputs.
Thus we imposea weight constraint on the LPN as:wi>0,  wi j>0,  wijk>0 (12)5.2 Global expert (GE)Since there are relations among topics, and LPNsdo not have global information, it is inevitable thatLPNs will make wrong decisions.
In order toovercome this problem, we use a global expert(GE) to arbitrate al local decisions.
Figure 5illustrates the use of global expert to combine theoutputs of LPNs.
)()( iiy q-Y(i))()( jjy q-ijW)1()1( q-y)(iQFigure 5: The architecture of global expertGiven a document x, we first use each LPN tomake a local decision.
We then combine theoutputs of LPNs as follows:])([)( )()()()()( )(0)()(ijijiii jjjyijyWyY Q--?+-=>-?qqq(13)where Wij?s are the weights between the globalarbitrator i and the j th LPN; and )(iQ ?s are theglobal bias.
From the result of Equation (13), wehave:If Y(i) > 0; then topic Ti is present; otherwiseTi is not present in document xThe use of Equation (13) implies that:a) If a LPN is not activated, i.e., y(i)  ?
q(i), then itsoutput is not used in the GE.
Thus it will notaffect the output of other LPN.b) The weight Wi j models the relationship orcorrelation between topic i and j.
If Wi j > 0, itmeans that if document x is related to Tj, it mayalso have some contribution ( Wij) to topic T j. Onthe other hand, if Wi j < 0, it means the twotopics are negatively correlated, and a documentx will not be related to both Tj and Ti.The overall structure of SPN is as follows:Input documentLocal PerceptionGlobal Expertxy(i)Y(i)Figure 6: Overall structure of SPN5.3 The Training of SPNIn order to adopt SPN for topic spotting, weemploy the well-known BP algorithm to derive theoptimal weights and biases in SPN.
The trainingphase is divided to two stages.
The first stagelearns a LPN for each topic, while the second stagetrains the GE.
As the BP algorithm is ratherstandard, we will discuss only the error functionsthat we employ to guide the training process.In topic spotting, the goal is to achieve bothhigh recall and precision.
In particular, we want toallow y(x) to be as large (or as small) as possible incases when there is no error, or when +W?x  andq>)(xy  (or -W?x  and q<)( xy ).
Here +W  and -Wdenote the positive and negative training documentsets respectively.
To achieve this, we adopt a newerror function as follows to train the LPN:?W+WW+?W+WW=-+W?-+-+W?++--xxiijijijkxyxywwwE)),((||||||)),((||||||),,,,(qeqeqq(14)where?????
?<-=+)(0)()(21),(2qqqqexxxx ,  and),(),( qeqe --= +- xxEquation (14) defines a piecewise differentiableerror function.
The coefficients||||||+--W+WW  and||||||+-+W+WW  are used to ensure that the contributionsof positive and negative examples are equal.After the training, we choose the node with thebiggest wi value as the common attribute node.Also, we trim the topic representation by removingthose words or context words with very small wij orwijk values.We adopt the following error function to trainGE:?
?
Q+?
Q=Q= W?-W?+-+ni xiiixiiiiijiixYxYWE1])),(()),(([),( ee  (15)where +W i  is the set of positive examples of Ti.6.
Experiment and DiscussionWe employ the ModApte Split version of Reuters-21578 corpus to test our method.
In order to ensurethat the training is meaningful, we select only thoseclasses that have at least one document in each ofthe training and test sets.
This results in 90 classesin both the training and test sets.
After eliminatingdocuments that do not belong to any of these 90classes, we obtain a training set of 7,770documents and a test set of 3,019 documents.From the set of training documents, we derive theset of semantic nodes for each topic using theprocedures outlined in Section 4.
From the trainingset, we found that the average number of semanticnodes for each topic is 132, and the averagenumber of terms in each node is 2.4.
Forillustration, Table 1 lists some examples of thesemantic nodes that we found.
From table 1, wecan draw the following general observations.NodeIDSemantic Node(SN)Method usedto find SNsTopic1 wheat  12 import, export,output1,2,33 farmer, production,mln, ton24 disease, insect, pest 2Wheat5 fall, fell, rise, rose 3 WpiMethod 1 ?
by looking up WordNetMethod 2 ?
by analyzing co-occurrence correlationMethod 3 ?
by analyzing context correlationTable 1: Examples of semantic nodesa) Under the topic ?wheat?, we list four semanticnodes.
Node 1 contains the common attributeset of the topic.
Node 2 is related to the ?buyingand selling of wheat?.
Node 3 is related to?wheat production?
; and node 4 is related to?the effects of insect on wheat production?.
Theresults show that the automatically extractedbasic semantic nodes are meaningful and areable to capture most semantics of a topic.b) Node 1 originally contains two terms ?wheat?and ?corn?
that belong to the same synset foundby looking up WordNet.
However, in thetraining stage, the weight of the word ?corn?was found to be very small in topic ?wheat?,and hence it was removed from the semanticgroup.
This is similar to the discourse basedword sense disambiguation.c) The granularity of information expressed by thesemantic nodes may not be the same as whathuman expert produces.
For example, it ispossible that a human expert may divide node 2into two nodes {import} and {export, output}.d) Node 5 contains four words and is formed byanalyzing context.
Each context vector of thefour words has the same two components:?price?
and ?digital number?.
Meanwhile,?rise?
and ?fall?
can also be grouped togetherby ?antonym?
relation.
?fell?
is actually the pasttense of ?fall?.
This means that by comparingcontext, it is possible to group together thosewords with grammatical variations withoutperforming grammatical analysis.Table 2 summarizes the results of SPN in termsof macro and micro F1 values (see Yang & Liu(1999) for definitions of the macro and micro F1values).
For comparison purpose, the Table alsolists the results of other TC methods as reported inYang & Liu (1999).
From the table, it can be seenthat the SPN method achieves the best macF1value.
This indicates that the method performs wellon classes with a small number of training samples.In terms of the micro F1 measures, SPN out-performs NB, NNet, LSF and KNN, while postinga slightly lower performance than that of SVM.The results are encouraging as they are ratherpreliminary.
We expect the results to improvefurther by tuning the system ranging from theinitial values of various parameters, to the choiceof error functions, context, grouping algorithm, andthe structures of topic tree and SPN.Method MicR MicP micF1 macF1SVM 0.8120 0.9137 0.8599 0.5251KNN 0.8339 0.8807 0.8567 0.5242LSF 0.8507 0.8489 0.8498 0.5008NNet 0.7842 0.8785 0.8287 0.3763NB 0.7688 0.8245 0.7956 0.3886SPN 0.8402 0.8743 0.8569 0.6275Table 2.
The performance comparison7.
ConclusionIn this paper, we proposed an approach toautomatically build semantic perceptron net (SPN)for topic spotting.
The SPN is a connectionistmodel in which context is used to select the exactmeaning of a word.
By analyzing the context andco-occurrence statistics, and by looking upthesaurus, it is able to group the distributed butsemantic related words together to form basicsemantic nodes.
Experiments on Reuters 21578show that, to some extent, SPN is able to capturethe semantics of topics and it performs well ontopic spotting task.It is well known that human expert, whose mostprominent characteristic is the ability to understandtext documents, have a strong natural ability to spottopics in documents.
We are, however, unclearabout the nature of human cognition, and with thepresent state-of-art natural language processingtechnology, it is still difficult to get an in-depthunderstanding of a text passage.
We believe thatour proposed approach provides a promisingcompromise between full understanding and nounderstanding.AcknowledgmentThe authors would like to acknowledge the supportof the National Science and Technology Board, andthe Ministry of Education of Singapore for theprovision of a research grant RP3989903 underwhich this research is carried out.ReferencesJ.R.
Anderson (1983).
A Spreading ActivationTheory of Memory.
J. of Verbal Learning &Verbal Behavior, 22(3):261-295.L.D.
Baker & A.K.
McCallum (1998).Distributional Clustering of Words for TextClassification.
SIGIR?98.J.N.
Chen & J.S.
Chang (1998).
Topic Clusteringof MRD Senses based on Information RetrievalTechnique.
Comp Linguistic, 24(1), 62-95.G.W.K.
Church & D. Yarowsky (1992).
One Senseper Discourse.
Proc.
of 4th DARPA Speech andNatural Language Workshop.
233-237.W.W.
Cohen & Y.
Singer (1999).
Context-Sensitive Learning Method for TextCategorization.
ACM Trans.
on InformationSystems, 17(2), 141-173, Apr.I.
Dagan, S. Marcus & S. Markovitch (1995).Contextual Word Similarity and Estimationfrom Sparse Data.
Computer speech andLanguage, 9:123-152.S.J.
Green (1999).
Building Hypertext Links byComputing Semantic Similarity.
IEEE Trans onKnowledge & Data Engr, 11(5).T.
Hofmann (1998).
Learning and RepresentingTopic, a Hierarchical Mixture Model for WordOccurrences in Document Databases.Workshop on Learning from Text and theWeb, CMU.N.
Ide & J. Veronis (1998).
Introduction to theSpecial Issue on Word Sense Disambiguation:the State of Art.
Comp Linguistics, 24(1), 1-39.H.
Jing & E. Tzoukermann (1999).
InformationRetrieval based on Context Distance andMorphology.
SIGIR?99, 90-96.Y.
Karov & S. Edelman (1998).
Similarity-basedWord Sense Disambiguation, ComputationalLinguistics, 24(1), 41-59.C.
Leacock & M. Chodorow & G. Miller (1998).Using Corpus Statistics and WordNet for SenseIdentification.
Comp.
Linguistic, 24(1), 147-165.L.
Lee (1999).
Measure of DistributionalSimilarity.
Proc of 37 th Annual Meeting ofACL.J.
Lee & D. Dubin (1999).
Context-SensitiveVocabulary Mapping with a SpreadingActivation Network.
SIGIR?99, 198-205.D.
Lin (1998).
Automatic Retrieval and Clust eringof Similar Words.
In COLING-ACL?98, 768-773.J.
Liu & Z. Shi (2000).
Extracting ProminentShape by Local Interactions and GlobalOptimizations.
CVPRIP?2000, USA.M.A.
Minsky (1975).
A Framework forRepresenting Knowledge.
In: Winston P (eds).
?The psychology of computer vision?,McGraw-Hill, New York, 211-277.F.C.N.
Pereira, N.Z.
Tishby & L. Lee (1993).Distributional Clustering of English Words.ACL?93, 183-190.P.
Resnik (1995).
Using Information Content toEvaluate Semantic Similarity in a Taxonomy.Proc of IJCAI-95, 448-453.S.
Sarkas & K.L.
Boyer (1995).
Using PerceptualInference Network to Manage VisionProcesses.
Computer Vision & ImageUnderstanding, 62(1), 27-46.R.
Tong, L. Appelbaum, V. Askman & J.Cunningham (1987).
Conceptual InformationRetrieval using RUBRIC.
SIGIR?87, 247?
253.K.
Tzeras & S. Hartmann (1993).
AutomaticIndexing based on Bayesian InferenceNetworks.
SIGIR?93, 22-34.A.
Veling & P. van der Weerd (1999).
ConceptualGrouping in Word Co-occurrence Networks.IJCAI 99: 694-701.D.
Wang & D. Terman (1995).
Locally ExcitatoryGlobally Inhibitory Oscillator Networks.
IEEETrans.
Neural Network.
6(1).Y.
Yang & X. Liu (1999).
Re-examination of TextCategorization.
SIGIR?99, 43-49.
