The Design of a Computer Language for Linguistic InformationStuart M. ShieberArtificial Intelligence CenterSRI InternationalandCenter for the Study of Language and InformationStanford UniversityAbstractA considerable body of accumulated knowledge aboutthe design of languages for communicating information tocomputers has been derived from the subfields of program-ming language design and semantics.
It has been the goal ofthe PArR group at SRI to utilize a relevant portion of thisknowledge in implementing tools to facilitate communica-tion of linguistic information to computers.
The PATR-IIformalism is our current computer language for encodinglinguistic information.
This paper, a brief overview of thatformalism, attempts to explicate our design decisions interms of a set of properties that effective computer lan-guages hould incorporate.I.
In t roduct ion  IThe goal of natural-language processing research canbe stated quite simply: to endow computers with humanlanguage capability.
The pursuit of this objective, however,has been a di~cult task for at least two reuons: first, thiscapability is far from being a well-understood phenomenon;second, the tools for teaching computers what we do knowabout human language are still very primitive.
The solu-tion of these problems lies within the respective domains oflinguistics and computer science.Similar problems have arisen previously in computerscience.
Whenever a new computer application areaemerges, there follow new modes of communication withcomputers that are geared towards uch area& Computerlanguages are a direct result of this need for effective com-munication with computers.
A considerable body of accu-mulated knowledge about the design of languages for com-municating information to computers has been derived fromthe subfields of programming language design and seman-IThis research has been made possible in part by a gift from the Sys-tems Development Foundation, and was also supported by the DefenseAdvanced Research Projects Agency under Contract N00039-80-C-0575 with the Naval Electronic Systems Command.
The views andconclusions contained in this document are those of the author andshould not be interpreted as representative of the official policies, ei-ther expre.,sed orimplied, of the Defense Advanced Research ProjectsAgency, or the United States government.The author is indebted to Fernando Pereira, Barbara Grosr.
and RayPerrault for their comments on earlier dra/ts.tics.
It has been the goal of the PArR group at SRI 2 toutilize a relevant portion of this knowledge in implementingtools to facilitate communication of linguistic informationto computers.The PATR-II formalism is our current computer lan-guage for encoding linguistic information.
This paper, abrief overview of that formalism, attempts to explicate ourdesign decisions in terms of a set of properties that effec-tive computer languages should incorporate, namely: sim-plicity, power, mathematical weU-foundedness, flexibility,implementability, modularity, and declarativeness.
Moreextensive discussions of various aspects of the PATR-II for-malism and systems can be found in papers by Shieber e ta/., \[83\], Pereira and Shieber \[84\] and Karttunen \[84\].The notion of designing specialized computer lan-guages and systems to encode linguistic information is notnew; PROGRAMMAR \[Winograd, 72\], ATNs \[Woods, 70\],and DIALOGIC \[Grosz, et al, 82\] are but a few of thebetter-known examples.
Furthermore, a trend has arisenrecently in linguistics towards declarativeness in gram-mar formalisms--for instance, lexical-functional grammar(LFG) \[Bresnan, 83\], generalized phrase-structure gram-mar (GPSG) \[Gazdar and Pullum, 82\] and functional uni-fication grammar (UG) \[Kay, 83\].
Finally, in computer .sci-ence there has been a great deal of interest in declarativelanguages (e.g., logic programming and specification lan-guages), and their supporting denotational semantics.
Butto our knowledge, no attempt has yet been made to combinethe three approaches so as to yield a declarative computerlanguage with clear semantics designed specifically for en-coding linguistic information.
Such a language, of whichPATR-II is an example, would reflect a felicitous conver-gence of ideas from linguistics, artificial intelligence, andcomputer science.2.
The Critical Properties of theLanguageIt is not the purpose of this paper to provide a compre~hensive description of the PATR-II project, or even of theformalism itself.
Rather, we will discuss briefly the critical2This rather liquid group ham included at various times: John Bear,Lauri Karttuneu, Fernando Pereira, Jane Robinson, Stan Rosenschein,Susan Stueky, Mabry Tyson, Hans Uszkoreit, and the author.362properties of PATR-II to give a flavor for our approach tothe design of the language.
References to papers with morecomplete descriptions of particular aspects of the projectare provided when appropriate.2.1 .
S impl i c i ty :  An  In t roduct ion  to  thePATR- I I  Formal i smBuilding on a convergence of ideas from the linguisticsand AI communities, PATR-II takes as its primitive opera-tion an extended paltern-matching technique, unification,first used in logic and theorem-proving research and latelyfinding its way into research in linguistics \[Kay, 79; Gazdarand Pullum, 821 and knowledge representation \[Reynolds,70; Ait-Kaci~ 831.
Instead of unifying logic terms, how-ever, PATR unilication operates on directed acyclic graphs(DAG}.
sDAGs can be atomic symbols or sets of label/valuepairs, where the values are themselves DAGs (either atomicor complex).
Two labels can have the same value--thus theuse of the term graph rather than tree.
DAGs are notatedeither by drawing the graph structure itself, with the la-bels marking the arcs, or, as in this paper, by notating thesets of label/value pairs in square brackets, with the labelsseparated from their values by a colon; e.g., a DAG associ-ated with the verb "knight" (as in "Uther wants to knightArthur") would appear (in at least one of our grammars)as\ [ ca t  : vhead: \[aux: falseform: nonf in i tevoice:  ac t ivet rans :  \[pred: knightargl :  <f1134>\[\]arg2: <f1138>\[111syncat: \ [ f i r s t :  \ [cat :  nphead: \[trane: <f1134>\]\]rest :  \ [ f i r s t :  \[cat: nphead: \[trans: <f1188>\]\]rest :  <f1140>lambda\]ta i l :  <fl140>\]\]Reentrant structure is notated by labeling the DAG withan arbitrary label (in angle brackets), then using that labelfor future references to the DAG.Associated with each entry in the lexicon is a set ofDAGs.
4 The root of each DAG will have an arc labeled eataTechnically, these are rooted, directed, acyclic graphs with labeledarcs.
Formal definition of these and other technical notions can befound in Appendix A of Shieber et aL \[83\].
Note that some imple-mentations have been extended to handle cyclic graph structures aswell as graph structures with disjunction and negation \[Karttunen,84\].4In our implementation, this association is not directly encoded--sincethis would yield a grossly inefficient characterization of the lexicon~but is mediated by a morphological nalyzer.
See Section 2.6 forfurther details.whose value will be the calegory of the associated iexicalentry.
Other arcs may encode information about the syn-tactic features, translation, or syntactic subcategorizationof the entry.
But only the label cat has ally special sig-nificance; it provides the link between context-free phrasestructure rules and the DAGs, as explicated below.PATR-II grammars consist of rules with a context-freephrase structure portion and a set of unifications on theDAGs associated with the constituents that participate inthe application of the rule.
The grammar rules describe howconstituents can be built up to form new constituents withassociated DAGs.
The right side of the rule lists the catvalues of the DAGs associated with the filial constituents;the left side, the eat of the parent..
The associated uni-fications specify equivalences that must exist among thevarious DAGs and sub-DAGs of the parent and children.Thus, the formalism uses only one representation---DAGs--for iexical, syntactic, and semantic information, and oneoperation--unification--on this representation.By way of example, we present a trivial grammar for afragment of English with a lexicon associating words withDAGs.S ~ NP  VP< VP a f r> = <NP agr>VP --* V IVPUther:< VP agr> = < V agr>< eat > =np<agr number> = singular<agr person> = thirdArthur:<eat> = np<agr number> = singular<agrperson> = thirdknights:<eat> = v<aqr number> = singular<agr person> = thirdThis grammar (plus lexicon) admits tile two sentences"Uther knights Arthur" and "Arthur knights Uther."
Tilephrase structure associated with the first of these is:\[s INP Utherl \[vp \[v knightsl \[Nr' ArthurlllThe VP rule requires that the agr feature of the DAGassociated with the VP be the same as (unified with) the agrof the V. Thus, the VP's agr feature will have as its valuethe same node as the V's agr, and hence the same valuesfor the person and number features.
Similarly, by virtue ofthe unification associated with the S rule, the NP will havethe same agr value as the VP and, consequently, the V. Wehave thus encoded a form of subject-verb agreement.Note that the process of unification is order-independent.For instance, we would get the same effect regardless ofwhether the unifications at the top of the parse tree wereeffected before or after those at the bottom.
In either case,the DAG associated with, e.g., the VP node would be363\[cat : vpagr: \[person: thirdnumber: s ingu lar \ ] \ ]The.~e trivial examples of grammars and lexicons offerbut a glimp.~e ,~f the techniques used in writing PATR-IIgranmlar~, and do not begin to employ the power of unifi-cati,,n :is rl general information-passing mechanism.
Exam-ples of the use of PATR-\[I for encoding much more complexlinguistic phenr~mena c n be found in Shieber et al \[83\].2 .2.
Power :  Two Var iantsAugmented I)hrase-structure grammars such as PATR-II can in fact be quite powerful.
The ability to encodeunbc,l~nded amcmnts of information in the augmentations(which I'ATR-II obviously allows) gives this formalism thep,~wer c~f a 'rt, ring machine.
As a linguistic theory, thismuch power might be considered isadvantageous; as acompuler language, however, such power is clearly desir-able..-.ince the intent of the language is to enable the mod-eling of m~my kinds of linguistic analyses from a range oftheories.
As s*l,'h, PATR-II is a tool, not a result.N,~v(,rthelc.~s, a good case could be made for maintain-ing at least the decidability of determining whether a stringis admitted by a PATR-II grammar.
This property can beensured by requiring the context-free skeleton to have theproperty ~f off-line parsability \[Pereira, 83\], which was usedoriginally in the definition of LFG to maintain the decid-ability of that f{,rmalism \[Kaplan and Bresnan, 83\].
Off-lineparsability req.ires that the context-free "skeleton" of thegrammar allows no trivial cyclic derivations of the formA ~ A.2.3.
Mathematical Well-Foundedness: ADenotat iona l  Semant icsOne reason for maintaining the simplicity of the barePATR-II formalism is to permit a clean semantics for thelanguage.
We have provided a denotational semantics forPATR-ll \[Pereira and Shieber, 84\] based on the informationsystems domain theory of Dana Scott \[Scott, 82\].
Insofar asmore com\[)lex formalisms, such as GPSG and LFG, can bemodeled a~s appropriate notations for PATR-II grammars,PATR-II's denotational semantics constitutes a frameworkin which the semantics of these formalisms can also be de-fined, discussed, and compared.
As it appears that not allthe power of domain theory is needed for the semantics ofPATR-II, we are currently pursuing the possibility of build-ing a semantics based on a less powerful model, s2.4.
FIexibillty: Mode l ing  L inguist ic  Con-structsClearly, the bare PATR-II formalism, as it was pre-sented in Section 2.1, is sorely inadequate for any majorattempt at building natural-language rammars because ofits verbosity and redundancy.
Efficiency of encoding wass But see Pereira nd Shieber \[84\] for arguments in favor of using domaintheory even if all the available power is not utilized.temporarily sacrificed in an attempt o keep the underlyingformalism simple, general, and semantically well-founded.However, given a simple underlying formalism, we carl buildmore efficient, specialized languages on top of it, nmch asMACLISP might be built on top of pure LISP.
And justas MACLISP need not be implemented (and is not imple-mented) directly in pure LISP, specialized formalisms builtconceptually on top of pure PATR-I1 need not be so imple-mented (although currently we do implement thenl directlythrough pure PATR-II).
The effectiveness of this approachcan be seen in the fact that at lea:st a sizable portion ofEnglish syntax has been encoded in various experimentalPATR-II grammars constructed to date.
The syntactic on-structs encoded include subcategorization f various com-plement ypes (N/as, Ss, etc.
), active, passive, "there" in-sertion, extraposition, raising, and equi-NP constructic)ns,and unbounded ependencies ( uch a~s Wh-movement andrelative clauses).
Other theory-dependent devices that havebeen modeled with PATR-II include head-feature percola-tion \[Gazdar and Puilum, 82\], and LFG-like semantic forms\[Kaplan and Bresnan, 83\].
Note that none of these con-structs and techniques required expansion of the underly-ing formalism; indeed, the constructions all make use of thetechniques described in this section.
See Shieber et al \[83\]for a detailed discussion of the modeling of some ,)f thesephenomena.The devices now available for molding PATR-II to con-form to a particular intended usage or linguistic theory arein their nascent stage, llowever, because of their great im-portance in making the PATR-II system a usaHe one, wewill discuss them briefly.
It is important o keep in mindthat these methods hould not be considered a part of theunderlying formalism, but merely "syntactic sugar" to in-crease the system's utility and allow it to conform to auser's intentions.2.4 .1 .
TemplatesBecause so much of the information in tile PATR-IIgrammars under actual development tends to be encodedin the lexicon, most of our research has been devoted tomethods for removing redundancy in the lexicon by all,w-ing the users themselves to define primitive constructs andoperations on lexical items.
Primitive constructs, such asthe transitive, dyadic, or equi-NP properties of a verb, canbe defined by means of templates, that is, DAGs that en-code some linguistically isolable portion of the DAG of alexical item.
These template DAGs can then be c(~mbinedto build the lexical item out of tile user-defined primitives.As a simple example, we could define (with the follow-ing syntax) the template Verb asLet Verb be<eat> = Vand the template ThirdSing asLet ThirdSing be<agr number> = singular<agr  person> = thirdThe lexical  entry  for "knights" would then be364knights:Verb ThirdSin 9Templates can themselves refer to other templates, en-abling definition of abstract linguistic concepts hierarchi-cally.
For instance, a modal verb template may use an aux-iliary verb template, which in term may be defined usingthe verb template above.
In fact, templates are currentlyemployed for abstracting notions of subcategorization, verbform, semantic type, and a host of other concepts.2.4 .2 .
Lex ica l  Ru lesMore complex relationships among lexical items can beencoded by means of lexical rules These rules, such aspassive and "there" insertion, are user-definable operationson the lexical items, enabling one variant of a word to bebuilt from the specification of another variant.
A lexicalrule is specified as a set of selective unifications relating aninput DAG and an output DAG.
Thus, unification is theprimitive used in this device as well.Lexieal rules are used to encode the relationships amongvarious lexical entries that would typically be thought of astransformations or relation-changing rules (depending onone's ideological outlook}.
Because lexical rules performthese operations, the lexicon need include only a proto-type entry for each verb.
The variant forms can be derivedthrough lexical rules applied in accordance with the mor-phology actually found on the verb.
(The morphologicalanalysis in the implementations of PATR-II is performedby a program based on the system of Koskenniemi \[83\] andwas written by Lauri Karttunen \[83\].
)For instance, given a PATR-II grammar in which theDAGs are used to emulate the f-structures of LFG, wemight write a passive lexical rule as follows (following Bres-nan \[83\]): eDefine Passive as<out cat> = <in cat>< out form > = passprt<out subj> = <in obj><out obj> = <in subj>The rule states in effect that the output DAG (the oneassociated with the passive verb form) marks the lexicalitem as being a passive verb whose object is the inputDAG's subject and whose subject is the input's object.
Suchlexical rules have been used for encoding the active/passivedichotomy, "there" insertion, extraposition, and other  so-cal led relation-changing rules.2.5.
Modu lar i ty  and Dec la ra t lvenessThe PATR-II formalism is a completely declarative for-malism, as evidenced by its denotational semantics and theorder-independence of its definition.
Modularity is achievedthrough the ability to define primitive templates and lex-ical rules that are shared among lexical items, as well asby the declarative nature of the grammar formalism itself,6The example is merely meant to be indicative of the syntax for andoperation of lexical rules.
We do not present this as a valid definitionof Passive for any grammar we have written in PATR-ILremoving problems of interaction of rules.
Rules are guar-anteed to always mean the same thing, regardless of theenvironment of other rules in which they are placed.2.6.
ImplementabilityImplementability is an empirical matter, given credenceby the fact that we now have three implementations ofthe formalism.
One desirable aspect of the simplicity anddeclarative nature of the formalism is that even thoughthe three implementations differ substantially from one an-other, using different parsing algorithms {with both topdown and bottom up properties}, different implementationsof unification, different methods of compiling the rules, allare able to run on exactly the same grammars yielding theidentical results.The three implementations of the PATR-II system cur-rently in operation at SRI are as follows:?
An INTERLISP version for the DEC-2060 using avariant of the Cocke-Kasami-Younger parsing algo-rithm and the KIMMO morphological nalyzer \[Kart-tunen, 83\], and a limited programming environment.?
A ZETALISP version for the Symbolics 3600 using aleft-corner parsing algorithm and the KIMMO mor-phological analyzer, with an extensive programmingenvironment {due primarily to Mabry Tyson} that in-cludes incremental compilation, multiple window de-bugging facilities, tracing, and an integrated editor.?
A Prolog version (DEC-10 Prolog) running on theDEC-2060 by Fernando Pereira, designed primarily asa testbed for experimentation with efficient structure-sharing DAG unification algorithms, and incorporat-ing an Earley-style parsing algorithm.In addition, Lauri Karttunen and his students at theUniversity of Texas have implemented a system based on.PATR-II but with several interesting extensions, includingdisjunction and negation in the graph structures \[b:art-tunen, 84\].
These extensions will undoubtedly be inte-grated into the SRI systems and formal semantics for themare being pursued.3.
Conc lus ionThe PATR-II formalism was designed as a computerlanguage for encoding linguistic information.
The designwas influenced by current heory and practice in computerscience, and especially in the arenas of programming lan-guage design and semantics.
The formalism is simple (con-sisting of just one primitive operation, unification), power-ful (although it can be constrained to be decidable), math-ematieally well-founded (with a complete denotational se-mantics), flexible (as demonstrated by its ability to modelanalyses in GPSG, LFG, DCG and other formalisms), mod-ular (because of its higher-level notational devices uch astemplates and lexical rules), declarative (yielding order-independence of operations), and implementable (as demon-strated by three quite dissimilar implemented systems andone highly developed programming environment).365As we have ,mq)hasized herein, PATR-II seems to rep-l'OSO.l'it.
~'I c(~nvol'~(.llCC of  techniques from several domains--comt)utor science, programming language design, naturallanguage processing and linguistics.
Its positioning at thecenter of these trends arises, however, not from the ad-mixture of many discrete techniques, but rather from theapplication of a single simple yet powerful concept o theencoding of linguistic information.ReferencesAit-Kaci, II., 1~..~83: "A new Model of Computation Based on aCalcuhls of Type Sul)snml)tion," Doctoral Dissertation Pro-posal, I)ept.
of (?
;oml~uter and Information Science, Univer-sity of Pennsylvania (Noveml:er).Bresnan, .loan.
19::t:~: The mental representation of grammaticalrelations (ed.
), (:nmbriHge: MIT Press.Gazdar, C. and C.K.
Pullum, 198'2.
: "GPSG: A Theoretical Syn-opsis," Indiana University I,inguistics Club, Bloomington,Indiana.Grosz, B., N. llaas, (~.
Ilon,.Irix.
J. tlobbs, P. Martin, R. Moore,J.
l~?~l)inson att,I S. Rosenschein, 1982: "DIALOGIC: acore natnral-hmgu;H~e processing system," Proceedings of theNinth International Co,fercnce on Computational Linguis.tics, Prague, Czeehoslavakia (July), pp.
95-100.Kaplan, R. and J. Bresnan, 1983: "LexlcaI-Functionai Gram-mar: A Formal System for Grammatical Representation,"in J.
13resnan (ed.
), The mental representation ofgrammat-ical rclati, rr~ (ed.
), (:ambridge: MIT Press.Karttunen, I.., 1981: "Features and Values, ~ Proceedings ofthe Tenth Inter,atiomd Conference on Computational Lin.guistics, Stanford Universil.y, Stanford California (4-7 July,1984).Karttuneu, L., 1983: "NIMMO: a general morphological proces-sor," Texas Lingui.~tic Forum, Volume 22 (December), pp.161-185.Kay, M., 1979: "Functional C',rammar," in Proceedings of theFifth Annttal Meeting of the Berkeley Linguistics Society,Berkeley, California (17-19 February).Kay, M., 1983: "linifieation Grammar," unpublished memo, Xe-rox Pale Alto Research Center.Koskennicmi, 1<., 198.q: "A Two level Model for Morphologi-cal Analysis and Synthesis," forthcoming Ph.D. dissertation,University of Ilclsinki, llelsinki, Finland.Pereira, F. and D.II.D.
Warren, 1983: "Parsing as Deduction,"in Proceedings of the elst .4nn~tal Meeting of the Associationfor Computath, n~d l.ing,istics 115-17 June), pp.
137-144.Pereira, F. and S. $hi~,ber, 1984: "The Semantics of GrammarFormalisms Seen ~.s Comlmter Languages," Proceedings ofthe Te~,th International Conference on Computational Lin.guistics, Stanford University, Stanford California (4-7 July,1980.Reynolds, J., 1970: "Transformational Systems and the Alge-braic Structure of Atomic Formulas," in D. Miehie (ed.
),Machine Intelligence, Vol.
5, Chapter 7, Edinburgh, Scot-land: Edinburgh University Press, pp.
135-151.Scott, D., 1982: "Domains for Denotationai Semantics," ICALP'82, Aarhus, Denmark (July).Shieber, S., H. Uszkoreit, F. Percira, J. Robinson, and M. Tyson,1983: "The Formalism a.lld Implementation f PATI~.-\[I," inB.
Grosz and M. Stickel, Research on Interactive Acquisi-tion and Use of Knowledge, SRI Final Report 1894, SRIInternational, Menlo Park, California (November).Winograd, T., 1972: Understanding Natural Lattyuage, NewYork, New York: Academic Press.Woods, W., 1970: "Transition Network Grammars for NaturalLanguage Analysis," Communications of the A CM, Vol.
13,No.
10 (October).366
