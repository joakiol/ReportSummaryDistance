NATURAL LANGUAGE INFORMATION RETRIEVAL:TIPSTER-2 FINAL REPORTTomek St rza lkowsk iGE Corporate Research & DevelopmentSchenectady,  NY  12301strza lkowski  @crd .ge .comABSTRACTWe report on the joint GE/NYU naturallanguage information retrieval project as related tothe Tipster Phase 2 research conducted initially atNYU and subsequently at GE R&D Center andNYU.
The evaluation results discussed here wereobtained in connection with the 3rd and 4th TextRetrieval Conferences (TREC-3 and TREC-4).
Themain thrust of this project is to use natural anguageprocessing techniques to enhance the effectiveness offull-text document retrieval.
During the course of thefour TREC conferences, we have built a prototype IRsystem designed around a statistical full-text indexingand search backbone provided by the NIST's Priseengine.
The original Prise has been modified to allowhandling of multi-word phrases, differential termweighting schemes, automatic query expansion, indexpartitioning and rank merging, as well as dealingwith complex documents.
Natural language process-ing is used to preprocess the documents in order toextract content-carrying terms, discover inter-termdependencies and build a conceptual hierarchyspecific to the database domain, and process user'snatural language requests into effective searchqueries.The overall architecture of the system is essentiallythe same for both years, as our efforts were directedat optimizing the performance of all components.
Anotable exception is the new massive query expan-sion module used in routing experiments, whichreplaces a prototype xtension used in the TREC-3system.
On the other hand, it has to be noted that thecharacter and the level of difficulty of TREC querieshas changed quite significantly since the last yearevaluation.
TREC-4 new ad-hoc queries are farshorter, less focused, and they have a flavor of infor-mation requests ("What is the prognosis of ...") ratherthan search directives typical for earlier TRECs("The relevant document will contain ...").
Thismakes building of good search queries a more sensi-tive task than before.
We thus decided to introduceonly minimum number of changes to our indexingand search processes, and even roll back some of theTREC-3 extensions which dealt with longer andsomewhat redundant queries.Overall, our system performed quite well as our posi-tion with respect to the best systems improvedsteadily since the beginning of TREC.
We partici-pated in both main evaluation categories: category Aad-hoc and routing, working with approx.
3.3 GBytesof text.
We submitted 4 official runs in automaticadhoc, manual ad-hoc, and automatic routing (2), andwere ranked 6 or 7 in each category (out of 38 parti-cipating teams).
It should be noted that the mostsignificant gain in performance seems to haveoccurred in precision near the top of the ranking, at5, 10, 15 and 20 documents.
Indeed, our unofficialmanual runs performed after TREC-4 conferenceshow superior esults in these categories, topping bya large margin the best manual scores by any systemin the official evaluation.In general, we can note substantial improvement inperformance when phrasal terms are used, especiallyin ad-hoc runs.
Looking back at TREC-2 andTREC-3 one may observe that these improvementsappear to be tied to the length and specificity of thequery: the longer the query, the more improvementfrom linguistic processes.
This can be seen compar-ing the improvement over baseline for automaticadhoc runs (very short queries), for manual runs(longer queries), and for semi-interactive runs (yetlonger queries).
In addition, our TREC-3 results(with long and detailed queries) showed 20-25%improvement in precision attributed to NLP, as com-pared to 10-16% in TREC-4.OVERVIEWA typical (full-text) information retrieval (IR)task is to select documents from a database inresponse to a user's query, and rank these documentsaccording to relevance.
This has been usually accom-plished using statistical methods (often coupled withmanual encoding) that (a) select terms (words,143phrases, and other units) from documents that aredeemed to best represent their content, and (b) createan inverted index file (or files) that provide an easyaccess to documents containing these terms.
A subse-quent search process will attempt o match prepro-cessed user queries against term-based representa-tions of documents in each case determining a degreeof relevance between the two which depends uponthe number and types of matching terms.
Althoughmany sophisticated search and matching methods areavailable, the crucial problem remains to be that ofan adequate representation of content for both thedocuments and the queries.In term-based representation, a document (aswell as a query) is transformed into a collection ofweighted terms, derived directly from the documenttext or indirectly through thesauri or domain maps.The representation is anchored on these terms, andthus their careful selection is critical.
Since eachunique term can be thought to add a new dimen-sionality to the representation, it is equally critical toweigh them properly against one another so that thedocument is placed at the correct position in the N-dimensional term space.
Our goal here is to have thedocuments on the same topic placed close together,while those on different topics placed sufficientlyapart.
Unfortunately, we often do not know how tocompute terms weights.
The statistical weighting for-mulas, based on terms distribution within the data-base, such as ~.idf, are far from optimal, and theassumptions of term independence which are rou-tinely made are false in most cases.
This situation iseven worse when single-word terms are intermixedwith phrasal terms and the term independencebecomes harder to justify.The simplest word-based representations ofcontent, while relatively better understood, are usu-ally inadequate since single words are rarely specificenough for accurate discrimination, and their group-ing is often accidental.
A better method is to identifygroups of words that create meaningful phrases,especially if these phrases denote important conceptsin the database domain.
For example, joint venture isan important erm in the Wall Street Journal (WSJhenceforth) database, while neither joint nor ventureis important by itself.
In the retrieval experimentswith the training TREC database, we noticed thatboth joint and venture were dropped from the list ofterms by the system because their idf (inverted ocu-ment frequency) weights were too low.
In large data-bases, such as TIPSTER, the use of phrasal terms isnot just desirable, it becomes necessary.The challenge is to obtain "semantic" phrases,or "concepts", which would capture underlyingsemantic uniformity across various surface forms ofexpression.
Syntactic structures are often reasonableindicators of content, certainly better than 'statisticalphrases' - -  where words are grouped solely on thebasis of physical proximity (e.g., "college junior" isnot the same as "junior college") - -  however, thecreation of compound terms makes the term match-ing process more complex since in addition to theusual problems of lexical meaning, one must dealwith structure (e.g., "college junior" is the same as"junior in college").
In order to deal with structure,the parser's output needs to be "normalized" or "reg-ularized" so that complex terms with the same orclosely related meanings would indeed receivematching representations.
One way to regularize syn-tactic structures is to transform them into operator-argument form, or at least head-modifier form, aswill be further explained in this paper.
In effect,therefore, we aim at obtaining a semantic representa-tion.
This result has been achieved to a certain extentin our work thus far.Do we need to parse indeed?
Our recent resultsindicate that some of the critical semantic dependen-cies can in fact be obtained without the intermediatestep of syntactic analysis, and directly from lexical-level representation of text.
We have applied ournoun phrase disambiguation method directly to wordsequences generated using part-of-speech informa-tion, and the results were most promising.
At thistime we have no data how these results compare tothose obtained via parsing.No matter how we eventually arrive at thecompound terms, we hope they would let us to cap-ture more accurately the semantic ontent of a docu-ment.
It is certainly true that the compound termssuch as South Africa, or advanced ocument process-ing, when found in a document, give us a better ideaabout the content of such document han isolatedword matches.
What happens, however, if we do notfind them in a document?
This situation may arisefor several reasons: (1) the term/concept is not there,(2) the concept is there but our system is unable toidentify it, or (3) the concept is not explicitly there,but its presence can be infered using general ordomain-specific knowledge.
This is certainly a seri-ous problem, since we now attach more weight toconcept matching than isolated word matching, andmissing a concept can reflect more dramatically onsystem's recall.
The inverse is also true: finding aconcept where it really isn't makes an irrelevantdocument more likely to be highly ranked than withsingle-word based representation.
Thus, while therewards maybe greater, the risks are increasing aswell.144One way to deal with this problem is to allowthe system to fall back on partial matches and singleword matches when concepts are not available, andto use query expansion techniques to supply missingterms.
Unfortunately, thesaurus-based query expan-sion is usually quite uneffective, unless the subjectdomain is sufficiently narrow and the thesaurussufficiently domain-specific.
For example, the termnatural language may be considered to subsume aterm denoting a specific human language, e.g.,English.
Therefore, a query containing the formermay be expected to retrieve documents containingthe latter.
The same can be said about language andEnglish, unless language is in fact a part of the com-pound term programming language in which case theassociation language - Fortran is appropriate.
This isa problem because (a) it is a standard practice toinclude both simple and compound terms in docu-ment representation, and (b) term associations havethus far been computed primarily at word level(including fixed phrases) and therefore care must betaken when such associations are used in term match-ing.
This may prove particularly troublesome forsystems that attempt term clustering in order tocreate "meta-terms" to be used in document represen-tation.In the remainder of this paper we discuss par-ticulars of the present system and some of the obser-vations made while processing TREC-4 data.
Whilethis description is meant to be self-contained, thereader may want to refer to previous TREC papersby this group for more information about the system.OVERALL DESIGNOur information retrieval system consists of atraditional statistical backbone (NIST's PRISE sys-tem \[2\]) augmented with various natural languageprocessing components that assist the system in data-base processing (stemming, indexing, word andphrase clustering, selectional restrictions), andtranslate a user's information request into aneffective query.
This design is a careful compromisebetween purely statistical non-linguistic approachesand those requiring rather accomplished (and expen-sive) semantic analysis of data, often referred to as'conceptual retrieval'.In our system the database text is first pro-cessed with a fast syntactic parser.
Subsequently cer-tain types of phrases are extracted from the parsetrees and used as compound indexing terms in addi-tion to single-word terms.
The extracted phrases arestatistically analyzed as syntactic ontexts in order todiscover a variety of similarity links between smallersubphrases and words occurring in them.
A furtherfiltering process maps these similarity links ontosemantic relations (generalization, specialization,synonymy, etc.)
after which they are used totransform a user's request into a search query.The user's natural language request is alsoparsed, and all indexing terms occurring in it areidentified.
Certain highly ambiguous, usually single-word terms may be dropped, provided that they alsooccur as elements in some compound terms.
Forexample, "natural" is deleted from a query alreadycontaining "natural language" because "natural"occurs in many unrelated contexts: "natural number","natural logarithm", "natural approach", etc.
At thesame time, other terms may be added, namely thosewhich are linked to some query term through admis-sible similarity relations.
For example, "unlawfulactivity" is added to a query (TREC topic 055) con-taining the compound term "illegal activity" via asynonymy link between "illegal" and "unlawful".After the final query is constructed, the databasesearch follows, and a ranked list of documents isreturned.
In TREC-4, the automatic query expansionhas been limited to to routing runs, where we refinedour version of massive expansion using relevenaceinformation wrt.
the training database.
Query expan-sion via automatically generated omain map wasnot usd in offical ad-hoc runs.
Full details of T IPparser have been described in the TREC-1 report \[8\],as well as in other works \[6,7\], \[9,10,11,12\].As in TREC-3, we used a randomized indexsplitting mechanism which creates not one butseveral balanced sub-indexes.
These sub-indexes canbe searched independently and the results can bemerged meaningfully into a single ranking.LINGUISTIC TERMSSyntactic phrases extracted from TTP parsetrees are head-modifier pairs.
The head in such a pairis a central element of a phrase (main verb, mainnoun, etc.
), while the modifier is one of the adjunctarguments of the head.
In the TREC experimentsreported here we extracted head-modifier word andfixed-phrase pairs only.
The following types of pairsare considered: (1) a head noun and its left adjectiveor noun adjunct, (2) a head noun and the head of itsright adjunct, (3) the main verb of a clause and thehead of its object phrase, and (4) the head of the sub-ject phrase and the main verb.
These types of pairsaccount for most of the syntactic variants \[5\] forrelating two words (or simple phrases) into pairs car-tying compatible semantic ontent.
For example, thepair retrieve+information will be extracted from anyof the following fragments: information retrieval sys-tem; retrieval of information from databases; and145information that can be retrieved by a user-controlled interactive search process.The notorious ambiguity of nominal com-pounds remains a serious difficulty in obtaininghead-modifier pairs of highest accuracy.
In order tocope with this, the pair extractor looks at the distri-bution statistics of the compound terms to decidewhether the association between any two words(nouns and adjectives) in a noun phrase is both syn-tactically valid and semantically significant.
Forexample, we may accept language+natural andprocessing+language from natural language process-ing as correct, however, case+trading would make amediocre term when extracted from insider tradingcase.
On the other hand, it is important o extracttrading+insider to be able to match documents con-taining phrases insider trading sanctions act orinsider trading activity.Proper names, of people, places, events, organ-izations, etc., are often critical in deciding relevanceof a document.
Since names are traditionally capital-ized in English text, spotting them is relatively easy,most of the time.
It is important that all namesrecognized in text, including those made up of multi-ple words, e.g., South Africa or Social Security, arerepresented as tokens, and not broken into singlewords, e.g., South and Africa, which may turn out tobe different names altogether by themselves.
On theother hand, we need to make sure that variants of thesame name are indeed recognized as such, e.g., U.S.President Bill Clinton and President Clinton, with adegree of confidence.
One simple method, which weuse in our system, is to represent a compound namedually, as a compound token and as a set of single-word terms.
This way, if a corresponding full namevariant cannot be found in a document, its com-ponent words matches can still add to the documentscore.TERM WEIGHTING ISSUESFinding a proper term weighting scheme iscritical in term-based retrieval since the rank of adocument is determined by the weights of the termsit shares with the query.
One popular term weightingscheme, known as tf.idf, weights terms propor-tionately to their inverted document frequency scoresand to their in-document frequencies (tO.
The in-document frequency factor is usually normalized bythe document length, that is, it is more significant fora term to occur 5 times in a short 20-word document,than to occur 10 times in a 1000-word article.In our post-TREC-2 experiments we changedthe weighting scheme so that the phrases (but not thenames which we did not distinguish in TREC-2)were more heavily weighted by their idf scores whilethe in-document frequency scores were replaced bylogarithms multiplied by sufficiently large constants.In addition, the top N highest-idf matching terms(simple or compound) were counted more toward thedocument score than the remaining terms.
This 'hot-spot' retrieval option is discussed in the next section.Schematically, these new weights for phrasaland highly specific terms are obtained using the fol-lowing formula, while weights for most of thesingle-word terms remain unchanged:weight (Ti )=( C1 *log (tf )+C 2 " Ix(N ,i ) )*idfIn the above, tx(N,i) is 1 for i <N and is 0 otherwise.The tx(N,i) factor realizes our notion of "hot spot"matching, where only top N matches are used incomputing the document score.
This creates aneffect of "locality", somewhat similar to thatachieved by passage-level retrieval.
In TREC-3,where this weighing scheme was fully deployed forthe first time, it proved very useful for sharpening thefocus of long, frequently convoluted queries.
InTREC-3 where the query length ranged from 20 to100+ valid terms, setting N to 15 or 20 (includingphrasal concepts) typically lead to a precision gain ofabout 20%.
In TREC-4, the average query length isless than 10 terms, which we considered too short forusing locality matching, and this part of the weight-ing scheme was in effect unused in the official runs.This turned out to be a mistake, as we rerun TREC-4experiments after the conference, only to find outthat our results improved visibly when the localitypart of the weighting scheme was restored.Changing the weighting scheme for compoundterms, along with other minor improvements ( uch asexpanding the stopword list for topics) has lead tothe overall increase of precision of 20% to 25% overour baseline results in TREC-3.SUMMARY OF  RESULTSThe bulk of the text data used in TREC-4 hasbeen previously processed for TREC-3 (about 3.3GBytes).
Routing experiments involved some addi-tional new text (about 500 MBytes), which we pro-cessed through our NLP module.
The parameters ofthis process were essentially the same as in TREC-3,and an interested reader is referred to our TREC-3paper.
Two types of retrieval have been done: (1)new topics 201-250 were run in the ad-hoc modeagainst he Disk-2&3 database, l and (2) topics 3-1911 Actually, only 49 topics were used in evaluation, sincerelevance judgements were unavailable for topic 201 due to an er-ror.146(a selection of 50 topics in this range), previouslyused in TREC-1 to TREC-3, were run in the routingmode against the Disk-1 database plus the new dataincluding material from Federal Register, IR Digestand Internet newsgroups.
In each category 2 officialruns were performed, with different set up ofsystem's parameters.
Massive query expansion hasbeen implemented as an automatic feedback modeusing known relevance judgements for these topicswith respect TREC-3 database.Summary statistics for routing runs are shownin Tables 1 and 2.
In general, we can note substan-tial improvement in performance when phrasal termsare used, especially in ad-hoc runs.
Looking back atTREC-2 and TREC-3 one may observe that theseimprovements appear to be tied to the length andspecificity of the query: the longer the query, themore improvement from linguistic processes.
Thiscan be seen comparing the improvement over base-line for automatic adhoc runs (very short queries), formanual runs (longer queries), and for semi-interactiveruns (yet longer queries).
In addition, our TREC-3results (with long and detailed queries) showed 20-25% improvement in precision attributed to NLP, ascompared to 10-16% in TREe-4.
At this time weare unable to explain the much smaller improvementsin routing evaluations: while the massive queryexpansion definitely works, NLP has hard time top-ping these improvements.CONCLUSIONSWe presented in some detail our naturallanguage information retrieval system consisting ofan advanced NLP module and a 'pure' statistical coreengine.
While many problems remain to be resolved,including the question of adequacy of term-basedrepresentation of document content, we attempted todemonstrate that the architecture described here isnonetheless viable.
In particular, we demonstratedthat natural language processing can now be done ona fairly large scale and that its speed and robustnesshas improved to the point where it can be applied toreal IR problems.
We suggest, with some cautionuntil more experiments are run, that natural anguageprocessing can be very effective in creating appropri-ate search queries out of user's initial specificationswhich can be frequently imprecise or vague.
Anencouraging thing to note is the sharp increase ofprecision near the top of the ranking.
This indicates ahigher than average concentration of relevant docu-ments in the first 10-20 documents retrieved, whichcan leverage further gains in performance via anautomatic feedback process.
This should be our focusin TREC-5.Run base xbase nyuge I nyuge2Tot number of docs over all queriesRel 6576 6576 6576 6576RdRet 3641 4967 5078 i 5112i%chg +36.0 +39.0 I +40.0Average precision over all rel docsAvg 0.1697 0 .2715 0.2838 0.2913%chg +60.0 \[+67.0 +72.0Precision at5 docs 0 .3760 0 .5480 0 .5560 0.568010 docs 0.3680 0 .4840 0 .5000 0.522015 docs 0.3427 0 .4680 0 .4880 0.4933Table 1.
Automatic routing with 50 queries from 3-191 range: (1)base  - statistical terms only, no expansion; (2) xbase  - massivequery expansion, o phrases; (3) nyuge l  - phrases, names, withmassive xpansion up to 500 terms; (4) nyuge2 - expansion limitedto 200 terms per query.Run abase aloe mbase mloc ilocTot number of docs over aUqueries6501 6501 i 6501 6501 6501 ReliIRelR 2458 2498 !
3410 3545 3723I%chg +1.6 !
+39.0 +44.0  +51.0Average precision over all rei docsAvg 0.1394 0.1592 0.2082 0.2424 0.2767%chg +14.0 +49.0  +74.0  +98.0Precision at5 docs 0.3755 0.4571 0.5020 0.5592 0.669410 doc 0.3408 0.3939 0.4510 0.4816 0.608215 doc 0.3088 0.3687 0.4082 0.4490 0.5633Table 2.
Ad-hoc runs with queries 202-250: (1) abase  - automaticstatistical terms only; (2) a loc  - automatic phrases and names, lo-cality N=20; (3) mbase  - queries manually expanded, no phrases;(4) mloc  - manual phrases, locality N=20; (5) i l oc  - interactivephrases, locality N=20.At the same time it is important to keep inmind that the NLP techniques that meet our147performance r quirements (or at least are believed tobe approaching these requirements) are still fairlyunsophisticated in their ability to handle naturallanguage text.
In particular, advanced processinginvolving conceptual structuring, logical forms, etc.,is still beyond reach, computationally.
It may beassumed that these advanced techniques will proveeven more effective, since they address the problemof representation-level limits; however the experi-mental evidence is sparse and necessarily limited torather small scale tests.ACKNOWLEDGEMENTSWe would like to thank Donna Harman ofNIST for making her PRISE system available to us.Will Rogers provided valuable assistance in installingupdated versions of PRISE at NYU.
We would alsolike to thank Ralph Weischedel and ConstantinePapageorgiou of BBN for providing and assisting inthe use of the part of speech tagger.
This paper isbased upon work supported by the AdvancedResearch Projects Agency under Tipster Phase-2Contract 94-FI57900-000, and the National ScienceFoundation under Grant IRI-93-02615.REFERENCES\[1\] Frakes, William, B. and Ricardo Baeza-Yates.(eds).
1992.
Information Retrieval Prentice-Hall,Englewood Cliffs, NJ.\[2\] Harman, Donna and Gerald Candela.
1989.
"Retrieving Records from a Gigabyte of text on aMinicomputer Using Statistical Ranking."
Jour-nal of the American Society for Information Sci-ence, 41(8), pp.
581-589.\[3\] Meteer, Marie, Richard Schwartz, and RalphWeischedel.
1991.
"Studies in Part of SpeechLabeling."
Proceedings of the 4th DARPA Speechand Natural Language Workshop, Morgan-Kaufman, SanMateo, CA.
pp.
331-336.\[4\] Sager, Naomi.
1981.
Natural Language Informa-tion Processing.
Addison-Wesley.\[5\] Sparck Jones, K. and J. I. Tait.
1984.
"Automatic search term variant generation.
"Journal of Documentation, 40(1), pp.
50-66.\[6\] Strzalkowski, Tomek and Barbara Vauthey.
1992.
"Information Retrieval Using Robust NaturalLanguage Processing."
Proc.
of the 30th ACLMeeting, Newark, DE, June-July.
pp.
104-111.\[7\] Strzalkowski, Tomek.
1992.
"TIP: A Fast andRobust Parser for Natural Language."
Proceed-ings of the 14th International Conference onComputational Linguistics (COLING), Nantes,France, July 1992. pp.
198-204.\[8\] Strzalkowski, Tomek.
1993.
"Natural LanguageProcessing in Large-Scale Text Retrieval Tasks.
"Proceedings of the First Text REtrieval Confer-ence (TREC-1), NIST Special Publication 500-207, pp.
173-187.\[9\] Strzalkowski, Tomek and Jose Perez-Carballo.1994.
"Recent Developments in NaturalLanguage Text Retrieval."
Proceedings of theSecond Text REtrieval Conference (TREC-2),NIST Special Publication 500-215, pp.
123-136.\[10\] Strzalkowski, Tomek, Jose Perez-Carballo andMihnea Marinescu.
1995.
"Natural LanguageInformation Retirieval: TREC-3 Report.
"Proceedings of the Third Text REtrieval Confer-ence (TREC-3), NIST Special Publication 500-225, pp.
39-53.\[11\] Strzalkowski, Tomek.
1995.
"Natural LanguageInformation Retrieval" Information Processingand Management, Vol.
31, No.
3, pp.
397-417.Pergamon/Elsevier.\[12\] Strzalkowski, Tomek, and Peter Scheyen.
1993.
"An Evaluation of TTP Parser: a preliminaryreport."
Proceedings of International Workshopon Parsing Technologies (IWPT-93), Tilburg,Netherlands and Durbuy, Belgium, August 10-13.148
