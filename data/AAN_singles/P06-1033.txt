Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 257?264,Sydney, July 2006. c?2006 Association for Computational LinguisticsGraph Transformations in Data-Driven Dependency ParsingJens NilssonVa?xjo?
Universityjni@msi.vxu.seJoakim NivreVa?xjo?
University andUppsala Universitynivre@msi.vxu.seJohan HallVa?xjo?
Universityjha@msi.vxu.seAbstractTransforming syntactic representations inorder to improve parsing accuracy hasbeen exploited successfully in statisticalparsing systems using constituency-basedrepresentations.
In this paper, we showthat similar transformations can give sub-stantial improvements also in data-drivendependency parsing.
Experiments on thePrague Dependency Treebank show thatsystematic transformations of coordinatestructures and verb groups result in a10% error reduction for a deterministicdata-driven dependency parser.
Combin-ing these transformations with previouslyproposed techniques for recovering non-projective dependencies leads to state-of-the-art accuracy for the given data set.1 IntroductionIt has become increasingly clear that the choiceof suitable internal representations can be a veryimportant factor in data-driven approaches to syn-tactic parsing, and that accuracy can often be im-proved by internal transformations of a given kindof representation.
This is well illustrated by theCollins parser (Collins, 1997; Collins, 1999), scru-tinized by Bikel (2004), where several transforma-tions are applied in order to improve the analy-sis of noun phrases, coordination and punctuation.Other examples can be found in the work of John-son (1998) and Klein and Manning (2003), whichshow that well-chosen transformations of syntac-tic representations can greatly improve the parsingaccuracy obtained with probabilistic context-freegrammars.In this paper, we apply essentially the sametechniques to data-driven dependency parsing,specifically targeting the analysis of coordinationand verb groups, two very common constructionsthat pose special problems for dependency-basedapproaches.
The basic idea is that we can facili-tate learning by transforming the training data forthe parser and that we can subsequently recoverthe original representations by applying an inversetransformation to the parser?s output.The data used in the experiments come fromthe Prague Dependency Treebank (PDT) (Hajic?,1998; Hajic?
et al, 2001), the largest avail-able dependency treebank, annotated according tothe theory of Functional Generative Description(FGD) (Sgall et al, 1986).
The parser used isMaltParser (Nivre and Hall, 2005; Nivre et al,2006), a freely available system that combines adeterministic parsing strategy with discriminativeclassifiers for predicting the next parser action.The paper is structured as follows.
Section 2provides the necessary background, including adefinition of dependency graphs, a discussion ofdifferent approaches to the analysis of coordina-tion and verb groups in dependency grammar, aswell as brief descriptions of PDT, MaltParser andsome related work.
Section 3 introduces a setof dependency graph transformations, specificallydefined to deal with the dependency annotationfound in PDT, which are experimentally evaluatedin section 4.
While the experiments reported insection 4.1 deal with pure treebank transforma-tions, in order to establish an upper bound on whatcan be achieved in parsing, the experiments pre-sented in section 4.2 examine the effects of differ-ent transformations on parsing accuracy.
Finally,in section 4.3, we combine these transformationswith previously proposed techniques in order tooptimize overall parsing accuracy.
We concludein section 5.2572 Background2.1 Dependency GraphsThe basic idea in dependency parsing is that thesyntactic analysis consists in establishing typed,binary relations, called dependencies, between thewords of a sentence.
This kind of analysis can berepresented by a labeled directed graph, defined asfollows:?
Let R = {r1, .
.
.
, rm} be a set of dependencytypes (arc labels).?
A dependency graph for a string of wordsW = w1 .
.
.
wn is a labeled directed graphG = (W,A), where:?
W is the set of nodes, i.e.
word tokensin the input string, ordered by a linearprecedence relation <.?
A is a set of labeled arcs (wi, r, wj), wi,wj ?
W , r ?
R.?
A dependency graph G = (W,A) is well-formed iff it is acyclic and no node has anin-degree greater than 1.We will use the notation wi r?
wj to symbolizethat (wi, r, wj) ?
A, where wi is referred to asthe head and wj as the dependent.
We say thatan arc is projective iff, for every word wj occur-ring between wi and wk (i.e., wi < wj < wkor wi > wj > wk), there is a path from wi towj .
A graph is projective iff all its arcs are pro-jective.
Figure 1 shows a well-formed (projective)dependency graph for a sentence from the PragueDependency Treebank.2.2 Coordination and Verb GroupsDependency grammar assumes that syntacticstructure consists of lexical nodes linked by binarydependencies.
Dependency theories are thus bestsuited for binary syntactic constructions, whereone element can clearly be distinguished as thesyntactic head.
The analysis of coordination isproblematic in this respect, since it normally in-volves at least one conjunction and two conjuncts.The verb group, potentially consisting of a wholechain of verb forms, is another type of construc-tion where the syntactic relation between elementsis not clear-cut in dependency terms.Several solutions have been proposed to theproblem of coordination.
One alternative isto avoid creating dependency relations betweenthe conjuncts, and instead let the conjunctshave a direct dependency relation to the samehead (Tesnie`re, 1959; Hudson, 1990).
Anotherapproach is to make the conjunction the head andlet the conjuncts depend on the conjunction.
Thisanalysis, which appears well motivated on seman-tic grounds, is adopted in the FGD framework andwill therefore be called Prague style (PS).
It isexemplified in figure 1, where the conjunction a(and) is the head of the conjuncts bojovnost??
andtvrdost??.
A different solution is to adopt a morehierarchical analysis, where the conjunction de-pends on the first conjunct, while the second con-junct depends on the conjunction.
In cases ofmultiple coordination, this can be generalized to achain, where each element except the first dependson the preceding one.
This more syntacticallyoriented approach has been advocated notably byMel?c?uk (1988) and will be called Mel?c?uk style(MS).
It is illustrated in figure 2, which shows atransformed version of the dependency graph infigure 1, where the elements of the coordinationform a chain with the first conjunct (bojovnost??)
asthe topmost head.
Lombardo and Lesmo (1998)conjecture that MS is more suitable than PS forincremental dependency parsing.The difference between the more semanticallyoriented PS and the more syntactically orientedMS is seen also in the analysis of verb groups,where the former treats the main verb as the head,since it is the bearer of valency, while the lattertreats the auxiliary verb as the head, since it is thefinite element of the clause.
Without questioningthe theoretical validity of either approach, we canagain ask which analysis is best suited to achievehigh accuracy in parsing.2.3 PDTPDT (Hajic?, 1998; Hajic?
et al, 2001) consists of1.5M words of newspaper text, annotated in threelayers: morphological, analytical and tectogram-matical.
In this paper, we are only concernedwith the analytical layer, which contains a surface-syntactic dependency analysis, involving a set of28 dependency types, and not restricted to projec-tive dependency graphs.1 The annotation followsFGD, which means that it involves a PS analysis ofboth coordination and verb groups.
Whether betterparsing accuracy can be obtained by transforming1About 2% of all dependencies are non-projective andabout 25% of all sentences have a non-projective dependencygraph (Nivre and Nilsson, 2005).258(?The final of the tournament was distinguished by great fighting spirit and unexpected hardness?)A7Velkougreat?AtrN7bojovnost?
?fighting-spirit?Obj CoJ?aand?CoordA7nec?ekanouunexpected?AtrN7tvrdost?
?hardness?Obj CoP4seitself?AuxTVpvyznac?ovalodistinguishedN2fina?lefinal?SbN2turnajeof-the-tournament?AtrFigure 1: Dependency graph for a Czech sentence from the Prague Dependency Treebank(?The final of the tournament was distinguished by great fighting spirit and unexpected hardness?)A7Velkougreat?AtrN7bojovnost??fighting-spirit?ObjJ?aand?CoordA7nec?ekanouunexpected?AtrN7tvrdost?
?hardness?ObjP4seitself?AuxTVpvyznac?ovalodistinguishedN2fina?lefinal?SbN2turnajeof-the-tournament?AtrFigure 2: Transformed dependency graph for a Czech sentence from the Prague Dependency Treebankthis to MS is one of the hypotheses explored in theexperimental study below.2.4 MaltParserMaltParser (Nivre and Hall, 2005; Nivre et al,2006) is a data-driven parser-generator, which caninduce a dependency parser from a treebank, andwhich supports several parsing algorithms andlearning algorithms.
In the experiments below weuse the algorithm of Nivre (2003), which con-structs a labeled dependency graph in one left-to-right pass over the input.
Classifiers that pre-dict the next parser action are constructed throughmemory-based learning (MBL), using the TIMBLsoftware package (Daelemans and Van den Bosch,2005), and support vector machines (SVM), usingLIBSVM (Chang and Lin, 2005).2.5 Related WorkOther ways of improving parsing accuracy withrespect to coordination include learning patternsof morphological and semantical information forthe conjuncts (Park and Cho, 2000).
More specifi-cally for PDT, Collins et al (1999) relabel coordi-nated phrases after converting dependency struc-tures to phrase structures, and Zeman (2004) usesa kind of pattern matching, based on frequenciesof the parts-of-speech of conjuncts and conjunc-tions.
Zeman also mentions experiments to trans-form the dependency structure for coordinationbut does not present any results.Graph transformations in dependency parsinghave also been used in order to recover non-projective dependencies together with parsers thatare restricted to projective dependency graphs.Thus, Nivre and Nilsson (2005) improve parsingaccuracy for MaltParser by projectivizing trainingdata and applying an inverse transformation to theoutput of the parser, while Hall and Nova?k (2005)apply post-processing to the output of Charniak?sparser (Charniak, 2000).
In the final experi-ments below, we combine these techniques withthe transformations investigated in this paper.3 Dependency Graph TransformationsIn this section, we describe algorithms for trans-forming dependency graphs in PDT from PS toMS and back, starting with coordination and con-tinuing with verb groups.3.1 CoordinationThe PS-to-MS transformation for coordinationwill be designated ?c(?
), where ?
is a data set.The transformation begins with the identificationof a base conjunction, based on its dependencytype (Coord) and/or its part-of-speech (J?).
Forexample, the word a (and) in figure 1 is identifiedas a base conjunction.259Before the actual transformation, the base con-junction and all its dependents need to be classi-fied into three different categories.
First, the baseconjunction is categorized as a separator (S).
Ifthe coordination consists of more than two con-juncts, it normally has one or more commas sep-arating conjuncts, in addition to the base conjunc-tion.
These are identified by looking at their de-pendency type (mostly AuxX) and are also catego-rized as S. The coordination in figure 1 containsno commas, so only the word a will belong to S.The remaining dependents of the base conjunc-tion need to be divided into conjuncts (C) andother dependents (D).
To make this distinction,the algorithm again looks at the dependency type.In principle, the dependency type of a conjuncthas the suffix Co, although special care has to betaken for coordinated prepositional cases and em-bedded clauses (Bo?hmova?
et al, 2003).
The wordsbojovnost??
and tvrdost??
in figure 1, both having thedependency type Obj Co, belong to the categoryC.
Since there are no other dependents of a, thecoordination contains no instances of the categoryD.Given this classification of the words involvedin a coordination, the transformation ?c(?)
isstraightforward and basically connects all the arcsin a chain.
Let C1, .
.
.
, Cn be the elements of C,ordered by linear precedence, and let S1i , .
.
.
, Smibe the separators occurring between Ci and Ci+1.Then every Ci becomes the head of S1i , .
.
.
, Smi ,Smi becomes the head of Ci+1, and C1 becomesthe only dependent of the original head of the baseconjunction.
The dependency types of the con-juncts are truncated by removing the suffix Co.2Also, each word in wd ?
D becomes a dependentof the conjunct closest to its left, and if such a worddoes not exist, wd will depend on the leftmost con-junct.
After the transformation ?c(?
), every coor-dination forms a left-headed chain, as illustratedin figure 2.This new representation creates a problem,however.
It is no longer possible to distinguish thedependents in D from other dependents of the con-juncts.
For example, the word Velkou in figure 2is not distinguishable from a possible dependentin D, which is an obvious drawback when trans-forming back to PS.
One way of distinguishing Delements is to extend the set of dependency types.2Preliminary results indicated that this increases parsingaccuracy.The dependency type r of each wd ?
D can be re-placed by a completely new dependency type r+(e.g., Atr+), theoretically increasing the numberof dependency types to 2 ?
|R|.The inverse transformation, ?
?1c (?
), againstarts by identifying base conjunctions, using thesame conditions as before.
For each identifiedbase conjunction, it calls a procedure that per-forms the inverse transformation by traversingthe chain of conjuncts and separators ?upwards?
(right-to-left), collecting conjuncts (C), separators(S) and potential conjunction dependents (Dpot).When this is done, the former head of the left-most conjunct (C1) becomes the head of the right-most (base) conjunction (Smn?1).
In figure 2,the leftmost conjunct is bojovnost?
?, with the headvyznac?ovalo, and the rightmost (and only) con-junction is a, which will then have vyznac?ovalo asits new head.
All conjuncts in the chain becomedependents of the rightmost conjunction, whichmeans that the structure is converted back to theone depicted in figure 1.As mentioned above, the original structure infigure 1 did not have any coordination dependents,but Velkou ?
Dpot.
The last step of the inversetransformation is therefore to sort out conjunctiondependents from conjunct dependents, where theformer will attach to the base conjunction.
Fourversions have been implemented, two of whichtake into account the fact that the dependencytypes AuxG, AuxX, AuxY, and Pred are the onlydependency types that are more frequent as con-junction dependents (D) than as conjunct depen-dents in the training data set:?
?c: Do not extend arc labels in ?c.
Leave allwords in Dpot in place in ?
?1c .?
?c?
: Do not extend arc labels in ?c.
Attach allwords with label AuxG, AuxX, AuxY or Predto the base conjunction in ?
?1c .?
?c+: Extend arc labels from r to r+ for Delements in ?c.
Attach all words with labelr+ to the base conjunction (and change thelabel to r) in ?
?1c .?
?c+?
: Extend arc labels from r to r+ for Delements in ?c, except for the labels AuxG,AuxX, AuxY and Pred.
Attach all words withlabel r+, AuxG, AuxX, AuxY, or Pred to thebase conjunction (and change the label to r ifnecessary) in ?
?1c .2603.2 Verb GroupsTo transform verb groups from PS to MS, thetransformation algorithm, ?v(?
), starts by identi-fying all auxiliary verbs in a sentence.
These willbelong to the set A and are processed from left toright.
A word waux ?
A iff wmain AuxV??
waux,where wmain is the main verb.
The transformationinto MS reverses the relation between the verbs,i.e., waux AuxV??
wmain, and the former head ofwmain becomes the new head of waux.
The mainverb can be located on either side of the auxiliaryverb and can have other dependents (whereas aux-iliary verbs never have dependents), which meansthat dependency relations to other dependents ofwmain may become non-projective through thetransformation.
To avoid this, all dependents tothe left of the rightmost verb will depend on theleftmost verb, whereas the others will depend onthe rightmost verb.Performing the inverse transformation for verbgroups, ?
?1v (?
), is quite simple and essentiallythe same procedure inverted.
Each sentence is tra-versed from right to left looking for arcs of thetype waux AuxV??
wmain.
For every such arc, thehead of waux will be the new head of wmain, andwmain the new head of waux.
Furthermore, sincewaux does not have dependents in PS, all depen-dents of waux in MS will become dependents ofwmain in PS.4 ExperimentsAll experiments are based on PDT 1.0, which isdivided into three data sets, a training set (?t), adevelopment test set (?d), and an evaluation testset (?e).
Table 1 shows the size of each data set, aswell as the relative frequency of the specific con-structions that are in focus here.
Only 1.3% of allwords in the training data are identified as auxil-iary verbs (A), whereas coordination (S and C)is more common in PDT.
This implies that coor-dination transformations are more likely to havea greater impact on overall accuracy compared tothe verb group transformations.In the parsing experiments reported in sections4.1?4.2, we use ?t for training, ?d for tuning, and?e for the final evaluation.
The part-of-speechtagging used (both in training and testing) is theHMM tagging distributed with the treebank, witha tagging accuracy of 94.1%, and with the tagsetcompressed to 61 tags as in Collins et al (1999).Data #S #W %S %C %A?t 73088 1256k 3.9 7.7 1.3?d 7319 126k 4.0 7.8 1.4?e 7507 126k 3.8 7.3 1.4Table 1: PDT data sets; S = sentence, W = word;S = separator, C = conjunct, A = auxiliary verbT AS?c 97.8?c?
98.6?c+ 99.6?c+?
99.4?v 100.0Table 2: Transformations; T = transformation;AS = attachment score (unlabeled) of ??1(?
(?t))compared to ?tMaltParser is used with the parsing algorithm ofNivre (2003) together with the feature model usedfor parsing Czech by Nivre and Nilsson (2005).In section 4.2 we use MBL, again with the samesettings as Nivre and Nilsson (2005),3 and in sec-tion 4.2 we use SVM with a polynomial kernel ofdegree 2.4 The metrics for evaluation are the at-tachment score (AS) (labeled and unlabeled), i.e.,the proportion of words that are assigned the cor-rect head, and the exact match (EM) score (labeledand unlabeled), i.e., the proportion of sentencesthat are assigned a completely correct analysis.All tokens, including punctuation, are included inthe evaluation scores.
Statistical significance is as-sessed using McNemar?s test.4.1 Experiment 1: TransformationsThe algorithms are fairly simple.
In addition, therewill always be a small proportion of syntactic con-structions that do not follow the expected pattern.Hence, the transformation and inverse transforma-tion will inevitably result in some distortion.
Inorder to estimate the expected reduction in pars-ing accuracy due to this distortion, we first con-sider a pure treebank transformation experiment,where we compare ??1(?
(?t)) to ?t, for all thedifferent transformations ?
defined in the previoussection.
The results are shown in table 2.We see that, even though coordination is morefrequent, verb groups are easier to handle.5 The3TIMBL parameters: -k5 -mM -L3 -w0 -dID.4LIBSVM parameters: -s0 -t1 -d2 -g0.12 -r0 -c1 -e0.1.5The result is rounded to 100.0% but the transformed tree-261coordination version with the least loss of infor-mation (?c+) fails to recover the correct head for0.4% of all words in ?t.The difference between ?c+ and ?c is expected.However, in the next section this will be contrastedwith the increased burden on the parser for ?c+,since it is also responsible for selecting the correctdependency type for each arc among as many as2 ?
|R| types instead of |R|.4.2 Experiment 2: ParsingParsing experiments are carried out in four steps(for a given transformation ?
):1.
Transform the training data set into ?(?t).2.
Train a parser p on ?(?t).3.
Parse a test set ?
using p with output p(?).4.
Transform the parser output into ??1(p(?
)).Table 3 presents the results for a selection of trans-formations using MaltParser with MBL, tested onthe evaluation test set ?e with the untransformeddata as baseline.
Rows 2?5 show that transform-ing coordinate structures to MS improves parsingaccuracy compared to the baseline, regardless ofwhich transformation and inverse transformationare used.
Moreover, the parser benefits from theverb group transformation, as seen in row 6.The final row shows the best combination of acoordination transformation with the verb grouptransformation, which amounts to an improvementof roughly two percentage points, or a ten percentoverall error reduction, for unlabeled accuracy.All improvements over the baseline are statis-tically significant (McNemar?s test) with respectto attachment score (labeled and unlabeled) andunlabeled exact match, with p < 0.01 except forthe unlabeled exact match score of the verb grouptransformation, where 0.01 < p < 0.05.
For thelabeled exact match, no differences are significant.The experimental results indicate that MS ismore suitable than PS as the target representationfor deterministic data-driven dependency parsing.A relevant question is of course why this is thecase.
A partial explanation may be found in the?short-dependency preference?
exhibited by mostparsers (Eisner and Smith, 2005), with MaltParserbeing no exception.
The first row of table 4 showsthe accuracy of the parser for different arc lengthsunder the baseline condition (i.e., with no trans-formations).
We see that it performs very well onbank contains 19 erroneous heads.AS EMT U L U LNone 79.08 72.83 28.99 21.15?c 80.55 74.06 30.08 21.27?c?
80.90 74.41 30.56 21.42?c+ 80.58 74.07 30.42 21.17?c+?
80.87 74.36 30.89 21.38?v 79.28 72.97 29.53 21.38?v??c+?
81.01 74.51 31.02 21.57Table 3: Parsing accuracy (MBL, ?e); T = trans-formation; AS = attachment score, EM = exactmatch; U = unlabeled, L = labeledAS ?e 90.1 83.6 70.5 59.5 45.9Length: 1 2-3 4-6 7-10 11-?t 51.9 29.4 11.2 4.4 3.0?c(?t) 54.1 29.1 10.7 3.8 2.4?v(?t) 52.9 29.2 10.7 4.2 2.9Table 4: Baseline labeled AS per arc length on ?e(row 1); proportion of arcs per arc length in ?t(rows 3?5)short arcs, but that accuracy drops quite rapidlyas the arcs get longer.
This can be related to themean arc length in ?t, which is 2.59 in the un-transformed version, 2.40 in ?c(?t) and 2.54 in?v(?t).
Rows 3-5 in table 4 show the distributionof arcs for different arc lengths in different ver-sions of the data set.
Both ?c and ?v make arcsshorter on average, which may facilitate the taskfor the parser.Another possible explanation is that learning isfacilitated if similar constructions are representedsimilarly.
For instance, it is probable that learningis made more difficult when a unit has differentheads depending on whether it is part of a coordi-nation or not.4.3 Experiment 3: OptimizationIn this section we combine the best results fromthe previous section with the graph transforma-tions proposed by Nivre and Nilsson (2005) to re-cover non-projective dependencies.
We write ?pfor the projectivization of training data and ?
?1p forthe inverse transformation applied to the parser?soutput.6 In addition, we replace MBL with SVM,a learning algorithm that tends to give higher accu-racy in classifier-based parsing although it is more6More precisely, we use the variant called PATH in Nivreand Nilsson (2005).262AS EMT LA U L U LNone MBL 79.08 72.83 28.99 21.15?p MBL 80.79 74.39 31.54 22.53?p??v??c+?
MBL 82.93 76.31 34.17 23.01None SVM 81.09 75.68 32.24 25.02?p SVM 82.93 77.28 35.99 27.05?p??v??c+?
SVM 84.55 78.82 37.63 27.69Table 5: Optimized parsing results (SVM, ?e); T = transformation; LA = learning algorithm; AS =attachment score, EM = exact match; U = unlabeled, L = labeledT P:S R:S P:C R:C P:A R:A P:M R:MNone 52.63 72.35 55.15 67.03 82.17 82.21 69.95 69.07?p??v??c+?
63.73 82.10 63.20 75.14 90.89 92.79 80.02 81.40Table 6: Detailed results for SVM; T = transformation; P = unlabeled precision, R = unlabeled recallcostly to train (Sagae and Lavie, 2005).Table 5 shows the results, for both MBL andSVM, of the baseline, the pure pseudo-projectiveparsing, and the combination of pseudo-projectiveparsing with PS-to-MS transformations.
We seethat pseudo-projective parsing brings a very con-sistent increase in accuracy of at least 1.5 percent-age points, which is more than that reported byNivre and Nilsson (2005), and that the additionof the PS-to-MS transformations increases accu-racy with about the same margin.
We also see thatSVM outperforms MBL by about two percentagepoints across the board, and that the positive effectof the graph transformations is most pronouncedfor the unlabeled exact match score, where theimprovement is more than five percentage pointsoverall for both MBL and SVM.Table 6 gives a more detailed analysis of theparsing results for SVM, comparing the optimalparser to the baseline, and considering specificallythe (unlabeled) precision and recall of the cate-gories involved in coordination (separators S andconjuncts C) and verb groups (auxiliary verbs Aand main verbs M ).
All figures indicate, with-out exception, that the transformations result inhigher precision and recall for all directly involvedwords.
(All differences are significant beyond the0.01 level.)
It is worth noting that the error reduc-tion is actually higher for A and M than for S andC, although the former are less frequent.With respect to unlabeled attachment score, theresults of the optimized parser are slightly belowthe best published results for a single parser.
Halland Nova?k (2005) report a score of 85.1%, apply-ing a corrective model to the output of Charniak?sparser; McDonald and Pereira (2006) achieve ascore of 85.2% using a second-order spanning treealgorithm.
Using ensemble methods and a pool ofdifferent parsers, Zeman and ?Zabokrtsky?
(2005)attain a top score of 87.0%.
For unlabeled exactmatch, our results are better than any previouslyreported results, including those of McDonald andPereira (2006).
(For the labeled scores, we are notaware of any comparable results in the literature.
)5 ConclusionThe results presented in this paper confirm thatchoosing the right representation is importantin parsing.
By systematically transforming therepresentation of coordinate structures and verbgroups in PDT, we achieve a 10% error reduc-tion for a data-driven dependency parser.
Addinggraph transformations for non-projective depen-dency parsing gives a total error reduction ofabout 20% (even more for unlabeled exact match).In this way, we achieve state-of-the-art accuracywith a deterministic, classifier-based dependencyparser.AcknowledgementsThe research presented in this paper was partiallysupported by the Swedish Research Council.
Weare grateful to Jan Hajic?
and Daniel Zeman forhelp with the Czech data and to three anonymousreviewers for helpful comments and suggestions.263ReferencesDaniel M. Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30:479?511.Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and BarboraHladka?.
2003.
The Prague Dependency Treebank:A three-level annotation scenario.
In Anne Abeille?,editor, Treebanks: Building and Using SyntacticallyAnnotated Corpora.
Kluwer Academic Publishers.Chih-Chung Chang and Chih-Jen Lin.
2005.
LIB-SVM: A library for support vector machines.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the First Meet-ing of the North American Chapter of the Associa-tion for Computational Linguistics (NAACL), pages132?139.Michael Collins, Jan Hajic?, Eric Brill, Lance Ramshaw,and Christoph Tillmann.
1999.
A statistical parserfor Czech.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 505?512.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annatual Meeting of the Association for Com-putational Linguistics (ACL), pages 16?23.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Walter Daelemans and Antal Van den Bosch.
2005.Memory-Based Language Processing.
CambridgeUniversity Press.Jason Eisner and Noah A. Smith.
2005.
Parsing withsoft and hard constraints on dependency length.
InProceedings of the 9th International Workshop onParsing Technologies (IWPT).Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,Eva Hajic?ova?, Petr Sgall, and Petr Pajas.
2001.Prague Dependency Treebank 1.0.
LDC, 2001T10.Jan Hajic?.
1998.
Building a Syntactically AnnotatedCorpus: The Prague Dependency Treebank.
In Is-sues of Valency and Meaning, pages 12?19.
PragueKarolinum, Charles University Press.Keith Hall and Vaclav Nova?k.
2005.
Corrective mod-eling for non-projective dependency parsing.
InProceedings of the 9th International Workshop onParsing Technologies (IWPT).Richard Hudson.
1990.
English Word Grammar.
BasilBlackwell.Mark Johnson.
1998.
Pcfg models of linguistictree representations.
Computational Linguistics,24:613?632.Dan Klein and Christopher Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 423?430.Vincenzo Lombardo and Leonardo Lesmo.
1998.Unit coordination and gapping in dependency the-ory.
In Proceedings of the Workshop on Processingof Dependency-Based Grammars, pages 11?20.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of the 11th Conference ofthe European Chapter of the Association for Com-putational Linguistics (EACL).Igor Mel?cuk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press.Joakim Nivre and Johan Hall.
2005.
MaltParser: Alanguage-independent system for data-driven depen-dency parsing.
In Proceedings of the Fourth Work-shop on Treebanks and Linguistic Theories (TLT),pages 137?148.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedings ofthe 43rd Annual Meeting of the Association forComputational Linguistics (ACL), pages 99?106.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.MaltParser: A data-driven parser-generator for de-pendency parsing.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT), pages 149?160.Jong C. Park and Hyung Joon Cho.
2000.
Informedparsing for coordination with combinatory catego-rial grammar.
In Proceedings of the 18th Inter-national Conference on Computational Linguistics(COLING), pages 593?599.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of the 9th International Workshop on ParsingTechnologies (IWPT), pages 125?132.Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.The Meaning of the Sentence in Its Pragmatic As-pects.
Reidel.Lucien Tesnie`re.
1959.
?Ele?ments de syntaxe struc-turale.
Editions Klincksieck.Daniel Zeman and Zdene?k ?Zabokrtsky?.
2005.
Improv-ing parsing accuracy by combining diverse depen-dency parsers.
In Proceedings of the 9th Interna-tional Workshop on Parsing Technologies (IWPT).Daniel Zeman.
2004.
Parsing with a Statistical De-pendency Model.
Ph.D. thesis, Charles University.264
