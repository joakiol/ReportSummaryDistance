A DIRECTED RANDOM PARAGRAPH GENERATORStanley Y.W.
Su & Kenneth E. Harper(The RAND Corporat ion,  Santa Monica, Cal i fornia)I. INTRODUCTIONThe work described in the present paper represents acombination of two widely different approaches to thestudy of language.
The first of these, the automatic gen-eration of sentences by computer, is recent and highlyspecialized: Yngve (1962), Sakai and Nagao (1965),Arsent'eva (1965), Lomkovskaja (1965), Friedman (1967),and Harper (1967) have applied a sentence generator to thestudy of syntactic and semantic problems of the level ofthe (isolated) sentence.
The second, the study of unitsof discourse larger than the sentence, is as old as rhetor-ic, and extremely broad in scope; it includes, in one wayor another, such diverse fields as beyond--the sentenceanalysis (cf.
Hendricks, 1967) and the linguistic study ofliterary texts (Bailey, 1968, 53--76).
The present studyis an application of the technique of sentence generationto an analysis of the paragraph; the latter is seen as aunit of discourse composed of lower-level units (sentences),and characterized by some kind of structure.
To repeat:the object of our investigation is the paragraph; thetechnique is analysis by synthesis, i.e.
via the automaticgeneration of strings of sentences that possess theproperties of paragraphs.--2--Harper's earlier sentence generation program differedfrom other versions in its use of data on lexical co-occurrence and word behavior, both obtained from machineanalysis of written text.
These data are incorporatedwith some modifications in a new program designed to pro-duce strings of sentences that possess the properties ofcoherence and development found in "real" discourse.
(Theactual goal is the production of isolated paragraphs, notan extended discourse.)
In essence the program is designed(i) to generate an initial sentence; (ii) to  "inspect"the result in order to determine strategies for producingthe following sentence; (iii) to build a second sentence,.making use of one of these strategies, and employing, inaddition, such criteria of cohesion as lexical classrecurrence, substitution, anaphora, an4 synonymy; (iv) tocontinue the process for a prescribed number of sentences,observing both the general strategic principles and thelexical context.
Analysis of the output ~ill lead tomodification of the input materials, and the cycle will berepeated.This paper describes the implementations of theseideas, and discusses the theoretical implications of theparagraph generator.
First we give a description of thelanguage materials on which the generator operates.
Thenext section deals with a program which converts thelanguage data into tables with associative links to minimize--3-the storage requirement and access time.
Section 4 describes:(I) the function of the main components of the generationprogram, (2) the generation algorithm.
Section 5 desczibesthe implementation of some linguistic assumptions aboutsemantic and structural connections in a discourse.--5--Table iGOVERNING PROBABILITIESType" of DependentGovernor VT VI N A DV DSS I 0 0 P2 P3 VT 0 0 P1S 0VI 0 0 1 0 0 P4 P5N 0 0 P6 P7 0 0A 0 0 0 0 0 0DV 0 0 0 0 0 0DS 0 0 I 0 0 0The governing probabil it ies for a word are independentof each other.
In paragraph generation the decision toselect a dependent type will be made without regard to theselection of other dependent types.
For example, a nouncan have probabil it ies P6 and P7 of being the governor ofa noun and an adjective respectively.
The selection of anoun as a dependent based on P6 will not affect, and willnot be affected by, the selection of an adjective as adependent.There are two types of co--occurrence data accompanyingevery word in the glossary: a set of governing probabi l -ities and a list of dependents.
The probabil ity valuesassociated with a word are determined on the basis of thesyntactic behavior of the word in the processed text.
Ifa noun occurs in 75 instances as the governor of an--6--adjective in I00 occurrences in a text, the probability ofhavipg an adjective as a dependent is 0.75.
The zeroes andones in Table I are constant for all words in the glossary.These values are not listed in the sets of probabilityvalues for the entrles of the glossary; however, they areknown to the system.
For instance, the set of probabilityvalues for a transitive verb will contain PI' P2' and P3"The probability I of governing a noun as object will notbe listed in the data.The second type of co--occurrence data accompanyingevery word in the glossary is a list of possible dependents.The list is specified in terms of word numbers and semanticclasses (to be described later).
It contains the words thatactually appear in the processed physics text as dependentsof the word with which the list is associated.
Since thelists of dependents are compiled on the basis of word co-occurrence in the text, legitimate word combinations areguaranteed.
In the list of dependents for a verb~ thosewords which can only be the subject are marked "S" andthose which can only be the direct object are marked "0".The co - -occur rence  data  can be regarded  as  e i thersyntact i c  o r  semant ic .
They are  d i s t ingu ished  here  f romboth the dependency rules and part of speech designation,and from the semantic classes that have been established.At present, seventy--four semantic classes have been set up.Some of these are formed distributionally (i.e., on the--7--basis of their tendency to co-occur syntactically with thesame words in text---cf.
Harper, 1965); other classes containwords of the same root, synonyms, hypernyms, and wordsarbitrarily classified as "concrete."
The semantic classi-fications are highly tentative, and are subject to modifi-cation.
Their extent is shown in Table 2.Table 2SEMANTIC DATANumber of Number ofClassification Classes Words in ClassDistributional Classes 22 150Hypernym Classes i0 160Word Families 25 52Synonym--antonym Classes 16 48"Concrete" Words I 54TOTAL 74 464The language materials described above are punchedon approximately 2500 cards.
The data are processed by sconversion program in order to form the data base for theparagraph generation program.--8-3.
DATA CO~VERSION PROGRAMThe paragraph generator is written in PI/I and run onthe IBM 360 Model 65.
It consists of two main programs:a data conversion program and a generation program.
Thesetwo programs run as separate jobs.
The data conversionprogram converts the language materials described aboveinto compact data tables with associative links.
Theconverted data  are  stored on a magnetic tape which is usedas input to the generation program.During the process of paragraph generation it isdesirable that the language data described in the preced-ing section remain in core storage.
However, since thedata base is rather l~rge, its conversion into a morecompact and flexible form is desirable so that storagerequirements and access time can be reduced.In view o?
the characteristics of the language dataand the generation algorithm (to be described latex), westructure the data base in the following way.s.
Words belonging to the same parts of speech orsemantic classes are stored in consecutive locations.
Inthe process of generation, random selection of a wordfrom a part of speech or a semantic class is often re--qu i red .
If words are grouped together in form of a table,a randomly selected number in a proper range can be usedas an index to look up the word from the table.--9--b.
The data storage for each entry of the glossaryis of variable length, since the lists of dependents,governing probabilities, hypernyms and semantic classesassociated with the entries are of variable length.c.
Word numbers in the lists of dependents andsemantic classes are replaced by pointers, which identifythe locations where the word numbers are actually stored.Thus, data tables containing different types of informa-tion are linked to one another, and access to this informa-tion can be carried out by straight table lookup.In keeping with these prlnciples, data tables of theform shown in Fig.
i have been constructed.
An examplewill illustrate the organization of the data base and theprocedure of setting up these data tables.
As a noun,represented by word number 2466, and its associatedlanguage data are read from the input unit, the wordnumber is stored in the block in table (i) reserved fornouns.
The last three digits of the word number are usedas an index to a location in the lookup table (2), wherethe word number 2466 and its address in table (i), i.e.309, are stored.
If the location in table (2) has beenoccupied (when more  than one word has the same last threedigits), the word number and its address are stored at thefirst unused space in table (2) following that location.Table (2) allows us to replace word numbers with theiraddresses after all data have been processed.-10 -.... i3) Words in semantic classes 1~'2 Y3 YL Y47i2x 2x?
i~429~I~30~\[ ' .
3 ~ c1~5I(2) Lookup tableW ord  No.
Address  1?
i(4) Dependent list1 !
i2 i !
r2: 3 -130 Counter = -5726 (Address 340) ~ 4CI (Value -i) ,liC2 (Value-2)  iii"4594 (Address 315)!C16 (Va lue- \ ]b  IiJ 4~(5) Hypernym Listi2~17 Counter = -i613 (Address 290)(6) Semantic class~ 2~19 Counter  = -1_C4 7 (Vs lu e 7z~ 7 2 __74tJ,(8)!Y2 ~Y3Y4Y471(7) Probability valuesI z 2 142I DS i Nz 3 z 4!
vT I vl IFig.
1--Data table organization--U--There are  four  po in ters  assoc ia ted  w i th  each wordstored in table (I).
Pointer D specifies the location intable (4) where the list of dependents associated withthe word is stored.
A countez is used to specify thenumber of words and semantic classes in the list.
Asemantic class in the original data is prefixed by a C(CI identifies senmntic class I).
In table (4) all thecounters and semantic classes (the numerical values) arestored as negative values so that the positive values(i.e.
word numbers) can be conveniently changed to pointersat a later stage.
In our example the pointer D is 130and the words 726 and 4594, and also the semantic classesCI, C2 and C16, are in the dependent list associated withword 2466.
The value which identifies a semantic class intable (4) is actually a pointer to a table which containsthe starting locations of the lists of words in all senmr~-tic classes.
This is illustrated in Fig.
I by the linksfrom table (4) through table (8) to table (3).The set of governing probabilities associated withword 2466 is stored in table (7).
Pointer P specifies thestarting location where the probability values are stored.In the example, P is set to 142.
Notice that no spacesare reserved for adjectives and adverbs bocauae they donot have governing probabilities.The pointer H associated with a word in table (I)specifies the location in table (5) where a counter andi--12--the hypernyms of the word are stored.
Word 613 is ahypernym of the word 2466.
Thus, H is set to 17 which isthe location in table (5) where a counter and the word 613are  stored.
Since the word 2466 is a member of the seman-tic class C47, the pointer S associated with the word 2466is set to the location in table (6) where a counter andC47 is stored.Table (3) contains 74 blocks, which are reserved forthe 74 semantic classes established in the system.
Eachblock contains a counter and the addresses of the wordsin a semantic class.
For example, the address 309 isstored in the 47th block in tBble (3).
Table (3) is thuslinked to table (I).After all data have been entered in the tables, theword numbers (positive values) in tsbles (4) and (5) arereplaced by their addresses in table (i).
This operationis done by using the lookup table (2).The data are  organized in tables with assoc ia t ivelinks.
All word numbers in tables (3), (4), and (5) arereplaced by their addresses in table (i).
From an entryin table (I) (where the generation of a sentence usuallybegins), we can trace its possible dependents; since thesedependents are specified as pointers to their addresses initable (I), it is simple to obtain the lists of dependentsassociated with these dependents.
In turn we can trace thirdlevel dependent lists.
We can easily continue this operation--13--down to any desired leve~.
Table (i) is linked to tables(4), (5), (6), and (7), and tables (4), (5), and (6) arelinked either directly back to table (I) or indirectlythrough table (8) and then table (3) back to table (i).Thus, access to any piece of information in these datatables is gained by simple table lookup.In view of the variability in the number of words ineach part-of--speech and semantic class, and in the numberof governing probabilities, hypernyms, ser~ntic classesand dependents associated with each word, we have packedthese data in large arrays as illustrated in tables (i),(3), (4), (5), (6), and (7).
The advantages are (i)reduction in storage requirements, and (ii) capacity forrapid selection of a word from a part of speech or asen~ntic class.
The disadvantage is that we have placeda restriction on the amount of additional data that maybe added to the existing lists.
To avoid modifying theprogram when new data are added, indices (such as x, y,and z in Fig.
i) to the reserved spaces in tables (I), (3),and (7) are n~de input parametecs to the program.
Atpresent the parameters are set to leave space for expansionof input data.
Further expansion can be handled simply byreadjusting the input parameters.--15--The restriction pattern in Fig.
2 specifies that the sen--tence to be generated should contain a transitive verbwhich belongs to either semantic class C1 or C2.
The verbshould govern (I) a noun as the subject of the sentence,(2) an object which is to be selected from the words insemantic class C15 or the specified words W 1 and W2, and(3) an adverb which does not belong to semantic class C19.The subject of the sentence should not govern an adjective.As i l lustrated in the pattern, each node in a patterncontains a word class and selection restrictions which areposit ively or negatively specified in terms of semanticclass(es), specific word(s) or a word class.
Restrictionpatterns are stored in the following form: Q-PIP2..oPn.Q is a single pattern, or a combination of patterns, andPIP2...P n are single restriction patterns.
Essentially,Q-PIP2...Pn is e rule which specifies that if a sentence(or string of sentences) whose sentence skeleton(s) matchesQ, then it can be followed by a sentence whose sentenceskeleton is one of these Ps.
Thus, one of these Ps israndomly selected to be used as a restriction pattern fora succeeding sentence.
The pattern selection procedure isnot yet coded.
At present, strings of restriction patternsare given directly to the pattern selection routine.
Thegeneration program generates strings of sentences underthe control oz direction of the restrictions specified inthe patterns.
The use of restriction patterns to control--16--the general "development" of paragraphs will be describedin a later section.4.1~3.
Discourse Relator (RELATOR~ Input to thisprocedure are (i) a dependent type, (2) a probabilityvalue, and (3) a restriction pattern.
This proceduredetermines whether the given dependent type conflictswith the restrictions specified in the pattern.
If noconflict is found, this procedure determines whether aword should be selected from the given dependent typebased on the input probability value.
If the selection ofthe dependent type conflicts with the restriction pattern,or if the dependent type fails the probability test, noword will be selected from the dependent type.4.1.4.
~CRITERIA~.
Whenever, during any stage ofsentence generation, the selection of one word from alist of candidates is required, this procedure determineswhich criteria should be applied to control the selection.All criteria (implemented principles of cohesion to bedescribed in a later section) are presented to the genera-tion program in the form of a table that reweights theprobabilities.
The generation program increases or de-creases the probability of selecting words on the basis ofthe values in this table.
It has the following formatI(Fig.
3): each entry is specified by a ser0~ntic class ora specific word followed by a positive or negative value.--17--Identifier WeightC2W I?C12+5--7+3-4Fig.
3 -- Format of a reweighting tableTo i l lustrate the Use of this table, let us supposethat in a certain stage of generat ing a sentence there arefive words, each of which can be the subject of the verbpreviously selected for the sentence.
The selection ofany word from these five will satisfy the restrictionpattern for the sentence.
Instead of randomly selectingone word out of these five candidates, we may want toincrease the probabil ity of selecting a word which willhave semantic connections with the word(s) in the precedingor current sentence.
When there are choices in word selec-tion, all candidates are preassigned equal weights, andcriteria relevant to the current selection are applied toform a reweighting table.
If a word in the list of can--didates matches a word or belongs to a semantic class inthe table, the associated weight is added to its preassignedweight.
The final positive weights of all candidates are--18-added, and a random number in the range from I to thetotal sum is generated to determine which candidate shouldbe selected.4.1.5.
Word Generator ~WOR-GEN).
This procedure findsall possible candidates which satisfy the restrictionsspecified in s restriction pattern, and assigns differentweights to them on the basis of the contents of a proba-bility reweighting table.
It selects a word st randomfrom the candidates according to their weights.4.1.6.
.Random Number Generator ~RA .NDOM).
Input tothis procedure is an integer No This procedure generatesa random integer in the range from i to N.4?2.
The Generation Al~orithmThe general strategy for generating a paragraph is,first, to generate the initial sentence based on a selectedrestriction pattern, and then to generate each noninitialsentence base not only on a selected restriction patternbut also on the semantic properties of the words in allthe previously generated sentences of the paragraph.
Thealgorithm and the sentence generation procedure can best beillustrated by an example.
Let us suppose that the restric-tion pattern shown in Fig.
4(a) is chosen for a sentence.For ease of reference we will letter, the steps involved inthis procedure.a.
If the restriction pattern specifies a restrictionon the selection of the sentence governor (usually a tzars---19-sitive verb (VT) or an intransitive verb (VI)), a VT orVI will be randomly chosen from the specified semanticclass(as) or word(s).
Otherwise a VT or VI will be randomlychosen.
In our example the restriction pattern in Fig.
4(a)specifies that a word should be selected from the wordclass VT which is not a member of the semantic e\[eeBesCI ,  C2, and C3, but is a governor of a word in C16, aword in word class N. and a word in C19.
(Note also thatthe sentence should not contain a sentence adverb.
)There are 16 candidates which satisfy the restrictions.They are shown in Fig.
4(b) by their addresses, at whichword numbers are stored.
A random number is generated inthe range i to 16, for example, 8.
Thus the eighth VT ischosen: word number 3336 whose address is 531.b.
The possible dependent types of a VT are NS(noun subject), NO (noun object), DV (adverb), and DS(sentence adverb).
The probabilities (in percentages) forthe word 3336 to govern words of these dependent typesare, say, 65, i00, 35, and 40 respectively.
The procedureRELATOR is called in order to determine whether the selec-tion of each dependent type agrees with the restrictionpattern.
If the selection of a dependent type conflictswith the pattern, the dependent type is ignored, i.e., noword will be selected from this dependent type as thedependent of the verb 3336.
In our example, NS, NO, andDV have passed this test.--20--~ )DS(--DS)N(+N)Fig.
4(a)---A restriction patternVT p521 527 530(~NS 1201 286 299 4591 ~ N~ 12611 DV \[1722 6 2 ~  ~ ~_711 1 1610 L273J b5c~5 ,174L234 295 z:55 4~9j 1 175\/~  N \[203 293 3~qJ 2466 1214 30B z.
53 IL216 ~\[5 ~.q22631 8 46 951Fig.
A(b)--A dependency tree538 54~541 54 ~ 543lTb ib 1 /79English:Russian:Word No.
:Address:Muxin published a study of linear method in a previous paper.Mu~in opublikoval izu~enie linejnyj metod v predydu~ej rabote,2625 3336 1610 2263 2466 6505317 531 261 42 308 179Fig.
4(c)--A generated~sentence--21--c. The probabilities associated with NS, NO, and DVare used to determine whether words should be selectedfrom these dependent types.
For each dependent type, therandom number generator is called to generate a numberranging from 1 to i00.
If the random number is greaterthan the probability associated with the dependent type,the type is ignored.
Otherwise a word will be selectedfrom this type.
Let us assume that all three types havepassed the probability test.d.l.
A noun is to be selected as the subject of verb3336.
If the sentence to be generated is the first sen-tence of a paragraph, a noun which is in the dependentlist associated with the verb 3336 and also a member ofC16 is chosen.
However, if the sentence is a noninitialone, the procedure CRITERIA is called to form a probabilityreweighting table on the basis of the criteria applicableto the verb 3336 and to this local structure (i.e., aVT dominates an NS).
All candidates (those words whichbelong to C16 and which are in the  dependent list associa-ted with 3336) are first assigned an equal weight.
Thenthe probability reweighting table is used to adjust theweights of the candidates.
Fig.
4(b) shows the candidatesfor the node NS.
An individual word is ~andomly chosenfrom the candidates based on their different weights:word number 2625 whose internal address is 317.--22--d.2.
A noun is to be selected as the object of theverb 3336.
As in d.l, the restriction pattern is consultedend, if the sentence is a noninitial one, the procedureCRITERIA is called.
Fig.
4(b) shows the candidates for thenode NO.
The same probability reweighting scheme isapplied to adjust the weights of the candidates.
A word isselected at random: word number 1610 whose address is 261.d.3~ An adverb is to be selected for the verb 3336.Similar to the previous procedure, the restriction patternrestricts the selection of candidates; CRITERIA is calledfor a noninitial sentence to construct the probabilityreweighting table, and an adverb is randomly selected.
Inthe figure we see the candidates for the node DV, and theadverb 6505, whose address is 179, is chosen.e.
The dependents of the words 2625, 1610, and 6505are now considered with respect to their possible dependentsand associated probabilities.
We are working from the topto the second level of the dependency tree structure.e.l.
The noun 2625 may govern the dependent typesadjective and noun.
Each of these is considered in turnby the same operations described in steps b. and c. Forbrevity, let us assume that none of these dependent typespass the probability test.
Thus, no word is selected fromthese dependent types.e.2.
The noun 1610 may govern the dependent typesadjective and noun with different probability.
Assuming?--23--that the adjective fails the probability test, none ischosen for the word 1610.
Since the restriction patternspecifies that a word should be selected from the wordclass N as a dependent of the word 1610, the same operationdescribed in step d. is performed to select the word 2466,whose address is 308.e.3.
The adverb 6505 selected in d.3.
has no dependentsince adverbs never govern.f.
We now move to the third level of the dependencytree structure.
The noun 2466 may govern the dependenttypes adjective and noun.
Let us assume that the dependenttype A passes the tests described in step c. and thedependent type N fails.
An adjective 2263, whose addressis 42, is selected from the list of candidates shown inthe figure.g.
We now move from the third level to the fourthlevel of the dependency tree structure.
Since the onlyword on the fourth level is an adjective, which does notgovern, we have reached the lowest level.
The generationof a sentence is completed.
Fig.
4(c) shows the generatedsentence.
(In the Russian sentence, m0rphology is ignored.
)The restriction pattern of the sentence just generated,together with those of the previously generated Sentences,are again used as the basis for selecting the restrictionpattern for the  next  sentence .--24--At the present stage of development no criterion isused to determine the end of a paragraph.
The number ofrestriction patterns input to the pattern selection proceduredetermines the number of sentences in a paragraph.
When thesentences of a paragraph have been generated, glossary look-up is performed and the transliterated Russian forms andtheir structural relations are printed.-25-5.
IMPLEMEntATION OF LINGUISTIC ASSUMPTIONSThe structure of paragraphs is poorly understood, andis in any event subject to enormous variety.
Nevertheless,we have adopted a simplified model, which postulates thatthe units (sentences) of a paragraph should be arranged ina recognizable pattern.
Specifically, it is assumed thateach pair of sentences should be characterized by theattributes of development and cohesion.
Development impliesprogression---for example, some kind of spatial, temporal,or logical movement: a paragraph can be assumed to "getsomewhere."
Cohesion, on the other hand, implies contin-uity or relatedness; as such, it is a kind of curb onprogression.
Although it is difficult, perhaps impossible,to distinguish between these two attributes, they will bediscussed separately, in an admittedly artificial way.The chief function of the restriction pattern is to achieveintersentence development, and an overall patter n to thesequence of sentence pairs; to a degree, lexical coherenceis also affected through the restriction pattern (e.g.,through the recurrence of semantic classes).
The mainfunction of the probability rewei~htin~ tables is to .achieve cohesion, through the device of increasing thelikelihood of lexical recurrence; the principle of devel-opment is also implemented here, to the extent that similar,but not identical, words are chosen in noninitial sentences.In general it may be said that the restriction pattern is--26--designed to  e f fec t  an overall pat tern ,  whereas the  reweight-ing tables are more local in effect, dealing with purelylexical materials.5.1.
DevelopmentAn examination of hundreds of sentence pairs, andscores of paragraphs, of Russian scientific texts, suggeststhat the following principles of development are commonlyemployed in intersentence connection: (I) progress fromthe general to the specific (more rarely, the reverse);(2) from whole to part, or from multiplicity to singularity(presumably a variation of the first--cited principle);(3) past action to present; (4) "other" to "present" agent;(5) "other" to "present" place; (6) cause to effect (morerarely, the reverse); (7) action to purpose of the action;(8) action to means of performing the action; (9) simplerephrasing.
Lack of space prevents illustration of theseprinciples; it should be obvious that even this small stockof strategies will suffice for the production of innumerableparagraphs.
It should also be noted that a random orderingof sentences built on the above pair--wise strategies willproduce less than satisfactory results; certain sequencesof sentence pairs are more likely than others to fit intoan acceptable pattern for the paragrgph.It was stated in Sec.
1 that the computer program wouldprovide a means of inspecting the initial sentence of aparagraph before deciding on a strategy for fuzther develop----28--Pattern iNs (--Ns) ~ N ( ~))~'oDv (+c20)Pattern 3NS(+CI6) ~N~(+N~) "~DV(+CI9)~N(+N)Pattern 2NSVT(+VT)~~-~--~V(--DV) (~NS) ~O(-~ethod)ON(+N)Pattern 4Fig.
5 -- Restriction patterns for a paragraph%a.N@ (+N@) N(+N) VT (+C2) DV(+CI9)b. Belov/ proposed/ in paper (i)/ a means/ of determining/NS(+CI6) VT(+C7) DV (+C19) N@(+N~) N(+N)the probability/ of absorption.c.
A theory/ of interaction/ is worked out/ in the present paper.NO (+NO) N(+N) VT (+C7) DV (+C20)d. A method/ of analyzing/ the method/ of analyzing/ theNO (+method) N (+N)magnitude/ of distortion/ is proposed.VT(+VT)The nature/ of scattering/ was investigated/ in an earlier paper.--29-The use o f  pat te rns  to cont ro l  development i s  summar-i zed  in Table 3.
S ince the verb invest igate  in sentence  (a)belongs to semantic c lass  C2, and C2 conta ins  such verbsas "s tudy"  and " invest igate"  which spec i fy  very  genera lac t ions ,  the node VT(+C7) in pat tern  2 cont ro l s  the se lec- -t ion  o f  a verb o f  g reater  spec i f i c i ty ;  the verbs  in C7 areappropr ia te .
The node VT(+C7) in pat tern  3 serves  th i spurpose, i .e .
,  to cont ro l  the development of  act ionsfrom ~enera l i ty  to spec i f i c i ty .
The node NS(+C16) inpat tern  2 spec i f ies  that  an agent fo r  the second sentenceis not the present author implicitly specified in the thirdsentence.
This restriction introduces another type oftext development, i.e.
from other wKiter to present author.The node DV(+CI9) in pattern I and pattern 2, and nodeDV(+C20) in pattern 3 introduce the time progression andlocation chan~e to the paragraph.
Class C19 contains suchadverbs as "in an earlier paper," "in paper I," "in anearlier study," etc., which specify that the time is past.Class C20 contains such adverbs as "in the present work,""in the present paper," etc.
which specify the differentlocations in which some actions were performed.c~U,r_l0,"0W,,-Iq-I -,.4?
~ ,-~t'~ ?
?00 1.1 1-~ ?- .1~-30 -4,;.,-4U 0?
,-4 ,1.4u)(~ "0  "0$.1 .,-I .H0 00 m'~ mO-~ ~ 0~u0 &0..~ (n~ 0?
r4 0~ ,,,4.U~ v .I~..,-.?
.~-~ ~ml.
t  m~4-~ 42 0 4-~Mc,400 -~ mo.~ ~ 0x ~?
,-?
?gU ~ 0'4.-I?
1-1 I J  .r,t~0 " ,~, .CO0~.~ 00O~c0u l:L0'~  0 ,,~ \[.-4,...100.
~ .
0 ~~ 0 ~0 U-,.4~2?
,~ 004.Jc~ ~.,'~ Iw?
?4.1g--31--Table 3 also suggests partial introduction of coherenceinto the sentence-sequence.
For example, the verbs in allsentences are in some degree related, and a general parallel-ism is maintained in the selection of agents and adverbs oftime and location.
Nonetheless it is clear that sentencesa.
through d. do not form n good paragraph.
One deficiencyis the excessively general character of the noun phrase,"the nature of scattering," in a.; a more obvious shortcom-ing is the lack of continuity in the noun object.
Suchdeficiencies suggest the need for greater cohesion.5?2.
CohesionAs a first approximation we have chosen to implementthe following principles of cohesion: (i) selection of a"concrete" word in noun phrases; (2) word repetition;(3) use of hypernyms and synonyms; (4) use of anaphoricwords; (5) increased repetition of members of the samesemantic classes; (6) avoidance of word repetition withina single noun phrase.
The implementation of these tacticsis carried out in the reweighting table described in Sec.
4.In essence each tactic is a criterion for determining whichwords and semantic classes, together with reweighting values,are to be entered into the reweighting table.
The contentof the table depends on the criteria applied for each wordselection; the main generation routine accepts the table asinput for control of the selection, without '~nowing" whichcriteria are used in forming the table.--32--When the principles of cohesion have been applied,sentences a. through d. above might have the following form:a'.
The nature of nuclear scattering was investigated inan earlier paper.b'.
BelDv proposed in paper (i) a means of determining theprobability of such pheonomena.c'.
A theory of proton scattering is worked out in thepresent paper.d'.
A method of analyzing the interaction of these particlesis proposed.The following are some of the improvements in thissentence-sequence over a. through d.:(i) The addition of the "concrete" adjectives~ and ~roton gives the noun phrases in a'.
and c'.a specificity that is lacking in s. and c. This effect isforced upon the generation routine by requiring the selec-tion of dependents in a noun phrase to continue until aword coded as "concrete n has been chosen.
(Since theeffect may also be one of very long noun phrases, a counter--effect is achieved by constant up-weighting of the semanticclass of concrete words in the reweighting table.
)(2) The recurrence of scatterln~ in a'.
and c'.increases continuity in the sentence sequence.
The gener--iation program achieves word repetition in adjacent ornearly adjacent sentences by entering the nour~-subjectsor noun--objects of previously generated sentences (the--33--choice between noun--subject or noun--object is made byreference to the restriction pattern for the sentencebeing generated) into the reweighting table, together witha high positive reweighting value.
Moreover, the possiblegovernors of these nouns are also entered into the tablewith the same reweighting value.
The value controls theprobability of repeating one of the nouns in a previouslygenerated sentence or of selecting a noun which is thegovernor of a word in a previously generated sentence.
Inthe latter case word repetition will occur on the nextlevel of dependency structure, i.e., when the programselects a dependent for the selected governor.
(3) The selection in b'.
and d'.
of phenomenon endparticle, hypernyms of scat terin~ and proton respectively,introduces semantic continuity and, in addition, reducesthe redundancy and monotony of word repetition.
The useof hypernyms and synonyms is implemented by entering anyhypernym and synonym of the words in previous sentencesinto the reweighting table with a positive reweighting value,thus increasing the probability of their selection.
(4) The hypernyms phenomenon and particle in b'.
andd'.
acquire "concreteness" by the addition of anaphoricdependents suc___~h and thes_~e.
The concreteness of the nounphrase such pheqomena in b'.
has presumably been providedby the dependents of scatteri~R in a'.
In the presentsystem the addition of an anaphoric dependent for a hypernym--34--automatically terminates the selection of other dependentsfor the hypernym.
(5) The selection of pro tOo and interaction in e'~and d'o is a result of increasing the repetition of membersof the same semantic class: the semantic classes repre-sented by nuclear in a'o and scatterin~ in c'.
are up--weighted during the generation of e'.
and d'(6) The undesirable repetition in d. is eliminated:words generated in a noun phrase are entered with negativevalue in the reweighting table, so that their repetitionin the same phrase is inhibited.The results of implementing even these few simplecohesion principles are encouraging.
Experimentationwith additional constraints continues.--35--6.
CONCLUSIONThe paragraph generator is currently operational, andproduces output in reasonable times.
Using the strategiesfor achieving development and cohesion so far developed,it is capable of generating ten--sentence strings inapproximately fifteen seconds.
Some of the main difficul-ties connected with the omtput are the following:(i) Deficiencies in the co-occurremce data affectthe quality of individual sentences.
For example, somenouns have very few dependents, a characteristic derivingfrom their behavior in the text on which the data is based;the selection of one of these nouns in a sentence maynullify the effect of applying strategies for developmentor cohesion.
In general a generated paragraph is only asstrong as the weakest link; defective single sentences candisturb the implementation of structural principles.
(2) The grammar permits the generation of simplesentences only.
Complex or compound sentences can, ofcourse, be created by the device of juxtaposing thesesimple sentences with the help of conjunctions or relatives;the conditions under which this can be done remain to bespecified.
(3) The creation of "lexical fields" (containing,e.g~ such words as "to photograph," "camera," "film,")would greatly increase the effect of cohesiono Distribution-al data for the formation of such "fields" is not readilyavailable; if the classes are to be intuitively created,--36--the result will be inconsistent with our present system ofclassification.Study of these problems continues through analysis ofthe output.
The effects of strengthening or relaxingvarious criteria for achieving development and cohesionhave been observed in a series of experiments.
The use ofalternative sets of language input data (e.g., differentdependent probabilities or semantic classes) is also con-templated.
(It should be emphasized that the program isnot oriented on a particular language or set of languagedata.)
The experimental design of the generation programis consistent with this kind of modification.
