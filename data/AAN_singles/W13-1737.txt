Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 288?292,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsEvaluating Unsupervised Language Model Adaptation Methods forSpeaking AssessmentShasha XieMicrosoft1020 Enterprise WaySunnyvale, CA 94089shxie@microsoft.comLei ChenEducational Testing Service600 Rosedale RdPrinceton, NJLChen@ets.orgAbstractIn automated speech assessment, adaptation oflanguage models (LMs) to test questions is im-portant to achieve high recognition accuracyHowever, for large-scale language tests, theordinary supervised training, which uses anexpensive and time-consuming manual tran-scription process, is hard to utilize for LMadaptation.
In this paper, several LM adap-tation methods that require either no manualtranscription process or just a small amount oftranscriptions have been evaluated.
Our ex-periments suggest that these LM adaptationmethods can allow us to obtain considerablerecognition accuracy gain with no or low hu-man transcription cost.Index Terms: language model adaptation, unsuper-vised training, Web as a corpus1 IntroductionAutomated speech assessment, a fast-growing areain the speech research field (Eskenazi, 2009), typ-ically uses an automatic speech recognition (ASR)system to recognize spontaneous speech responsesand use the recognition outputs to generate the fea-tures for scoring.
Since the recognition accuracy di-rectly influences the quality of the speech features,especially for the features related to word entities,such as those measuring grammar accuracy and vo-cabulary richness, it is important to use ASR sys-tems with high recognition accuracy.Adaptation of language models (LMs) to test re-sponses is an effective method to improve recogni-tion accuracy.
However, it is difficult to only usethe ordinary supervised training to adapt LMs to testquestions.
First, for high-stake tests administeredglobally, a very large pool of test questions have tobe used to strengthen the tests?
security and validity.Since a large number of test questions have manypossible answers for each question, a large set of au-dio files needs to be transcribed to cover responsecontent.
Second, due to time and cost constraints,it may not be practical to have a pre-test to collectenough speech responses for adaptation purposes.Therefore, it is important to pursue other methods toobtain LM adaptation data in a faster and lower-costway than the ordinary supervised training.As we will review in Section 2, some promisingtechnologies, such as unsupervised training, activelearning, and LM adaptation based on Web data,have been utilized in broadcast news recognition, di-alog system, and so on.
In this paper on the LMadaptation task used in automated speech scoringsystems, we will report our experiments to obtainLM adaptation data in a faster and more economicalway that requires little human involvement.
To ourknowledge, this is the first such work reported in theautomated speech assessment area.The rest of the paper is organized as follows: Sec-tion 2 reviews the related previous research results;Section 3 describes the English test, the data usedin our experiments, and the ASR system used; Sec-tion 4 reports the experiments of different methodswe tried to obtain LM adaptation data; Section 5 dis-cusses our findings and plans for future research.2882 Previous WorkUnsupervised training is the method of using untran-scribed audio to adapt a language model (LM).
Aninitial ASR model (seed model) is used to recognizethe untranscribed audio, and the obtained ASR out-puts are used in the follow-up LM adaptation.
(Chenet al 2003) utilized unsupervised LM adaptationon broadcast news (BN) recognition.
The unsuper-vised adaptation method reduces the word error rate(WER) by 2% relative to using the baseline LM.
(Bacchiani and Roark, 2003) reported that unsuper-vised LM adaptation provided an absolute error ratereduction of 3.9% over the un-adapted baseline per-formance by using 17 hours of untranscribed adap-tation data.
This was 51% of the 7.7% adaptationerror rate reduction obtained by using an ordinarysupervised adaptation method.Active learning is used to reduce the number oftraining examples to be annotated by automaticallyprocessing the unlabeled examples and then select-ing the most informative ones with respect to a givencost function.
(Riccardi and Hakkani-Tur, 2003;Tur et al 2005) proposed using a combination ofunsupervised and active learning for ASR trainingto minimize the workload of human transcription.Their experiments showed that the amount of la-beled data needed for a given recognition accuracycan be reduced by 75% when combining these twotraining approaches.A recent trend in Natural Language Processing(NLP) and speech recognition research is utilizingWeb data to improve the LMs, especially when in-domain training material is limited.
(Ng et al2005) investigated LM topic adaptation using Webdata.
Experiments in recognizing Mandarin tele-phone conversations showed that use of filtered Webdata leads to a 7% reduction in the character recog-nition error rate.
(Sarikaya et al 2005) used Webdata to adapt LMs used in a spoken dialog system.From a limited in-domain data set, they generateda series of search queries and retrieved Web pagesfrom Google using these queries.
In their recog-nition experiment done on a dialog system, theyachieved a 5.2% word error reduction by using theWeb data, compared to a baseline LM trained on1700 in-domain utterances.3 Test, Data, and ASROur in-domain data was from The Test of Englishfor International Communication, TOEIC R?, whichtests non-native English speakers?
basic speakingability required in international business communi-cations.
In our experiments, we focused on opiniontesting questions.
An example question is: ?Do youagree with the statement that a company should onlyhire experienced employees?
Use specific reasons tosupport your answer?.A state-of-the-art HMM LVCSR system, whichwas provided by a leading ASR vendor, was used inour experiments.
It contains a cross-word tri-phoneacoustic model (AM) and a combination of bi-gram,tri-gram, and up to four-gram LMs.
The AM andLM are trained by supervised training from about800 hours of audio and manual transcriptions ofnon-native English speaking data collected from theTest Of English as a Foreign Language (TOEFL R?
).TOEFL R?
is targeted to assess test-takers?
abilityto use English to study in an institution using En-glish as its primary teaching language.
Speakingcontent from TOEFL R?
data is quite different fromthe content shown in TOEIC R?
data.
When testingthis recognizer on a held-out evaluation set extractedfrom the TOEFL R?
test, a word error rate (WER) of33.0% 1 is observed.
This recognizer was used asthe seed recognizer in our experiments.4 ExperimentsWe collected a set of audio responses from theTOEIC R?
test, focusing on opinion questions.
Thisdata set was randomly selected from different first-language (L1) and English speaking proficiency lev-els.
Then, these audio files were manually tran-scribed.
In our experiments, 1470 responses wereused for LM adaptation and the remaining 184 re-sponses were used to evaluate speech recognition1ASR on non-native speech is more difficult than on nativespeech for various reasons (Livescu and Glass, 2000).
How-ever, a high WER does not rule out the possibility of usingASR outputs for automated scoring, especially when relyingon delivery related features.
For example, (Chen et al 2009)shows that several pronunciation features?
contributions for as-sessment, measured as Pearson correlations between the feat-uers and human scores, only drop about 10% to 20% when us-ing ASR outputs with a WER as high as 50% compared to usinghuman transcriptions.289accuracy.
When using the seed recognizer with-out any adaptation, the WER on the evaluation setis 42.8%, which is much higher than the accuracyachieved on the TOEFL R?
data (33.0%).
Using theordinary supervised training, adapting LMs usingthese 1470 manual transcriptions, the WER is re-duced to 34.7%, close to the performance on thein-domain TOEFL R?
data.
Note that a fixed dictio-nary with a vocabulary size of about 20, 000 words,which in general is much larger than the vocabularymastered by non-native test takers, was used in ourexperiment.4.1 Unsupervised LM adaptationUsing the seed recognizer trained on the TOEFL R?data, we recognized 1470 adaptation responses andselected varying amounts of ASR outputs for LMadaptation.
From ASR outputs of all responses, weselected the responses with high confidence scoresestimated by the seed recognizer so that we coulduse the ASR outputs with higher recognition accu-racy on the LM adaptation task.
We used two meth-ods to measure the confidence score for each re-sponse from word-level confidence scores.
First, wetook the average of all word confidence scores a re-sponse contains, as shown in Equation 1.ConfperWord =1NN?i=1conf(wi) (1)where conf(wi) is the confidence score of word, wi.The other method we used considers each word?s du-ration, as shown in Equation 2.ConfperSec =?Ni=1 d(wi) ?
conf(wi)?Ni=1 d(wi)(2)where d(wi) is the duration of wi.In Figure 1, we showed the WER after runningunsupervised LM adaptation, where the adaptationresponses were selected if they had high word-based(ConfperWord) or duration-based (ConfperSec)confidence scores.
The data sizes used for adapta-tion vary from 0% (without any adaptation) to 100%(using all adaptation data).
We observe continuousreduction of WER when using more and more adap-tation data.
Selecting responses by the word-basedconfidence scores performs a little better than the se-lection method based on the confidence scores nor-malized by corresponding word durations.
However,there is no significant difference between these twoselection criteria.Figure 1: Unsupervised LM adaptation performance us-ing different sizes of development set data.ASR accuracy may vary within each response.Therefore, instead of using entire responses, we alsoexplored using smaller units for LM adaptation.
Allof the ASR outputs were split into word sequenceswith fixed lengths (10-15 words), and the ones withhigher per-word confidence scores (ConfperWord)were extracted for model adaptation.
Our experi-ment shows that using word-sequence pieces ratherthan entire responses leads to a faster WER reduc-tion.
When only using 5% of the adaptation data, weobtained 3.5% absolute WER reduction compared tothe baseline result without adaptation.
Note that weonly obtained 2.5% absolute WER reduction whenusing entire responses in adaptation.4.2 Web data LM adaptationGiven around 40% WER when using our seed ASR,unsupervised learning faces the issue that manyrecognition errors were included in model adapta-tion.
Can we find another source to obtain LMadaptation inputs with fewer errors?
To addressthis question, we explored building a training cor-pus from Web data based on test questions.
Weused BootCat (Baroni and Bernardini, 2004), a cor-pus building tool designed to collect data from theWeb, to collect our LM adaptation data.
Based ontest prompts in the TOEIC R?
test, we manually gen-erated search queries.
After receiving the searchqueries, the BootCat tool searched the Web usingthe Microsoft Bing search engine.
Then, top-ranked290Web pages were downloaded and texts on these Webpages were extracted.
We examined the Web searchresults (including URLs and texts) returned by theBootCat tool.
The returned Web data has variedmatching rates among these prompts and are gen-erally noisy.By using only the default setup provided by theBootCat tool, we collected 5312 sentences in total.After a simple text normalization, we used the ob-tained Web data for LM adaptation, and the WERon the evaluation data was 38.5%.
This WER resultis a little higher than the WER result achieved byunsupervised LM adaptation (38.1%).
Without tran-scribing any response from test-takers, the languagemodel adaptation using Web data already helps toimprove recognition accuracy.
Then, we tried us-ing both the Web data and the ASR hypotheses foradaptation, and we can further decreased the WERto 37.6%.
This is lower than using the two LM adap-tation data sets separately.4.3 Semi-supervised approaches for LMadaptationFor semi-supervised LM adaptation, we replaced thespeech responses of lower confidence scores withtheir corresponding human transcripts.
We hopedthat by using the responses with high confidencescores together with a small amount of human tran-scripts, we could get better performance by intro-ducing less noise during adaptation.
We set differ-ent thresholds for selecting the low confidence re-sponses and replacing them with human transcripts.We find that just manually transcribing a limitedamount of audio data gives us further WER reduc-tion, compared to using unsupervised learning.
Af-ter transcribing just 100 responses, 6.8% of 1470 re-sponses in the adaptation data set, semi-supervisedlearning can achieve 61.73% of the WER reduction(8.1%) obtained by using the ordinary supervisedtraining that requires transcription of all 1470 re-sponses.4.4 DiscussionIn Table 1, we compared the performance of all theadaptation methods mentioned in this paper, includ-ing two unsupervised methods adapted using theASR hypotheses and ?related?
Web data, and onesemi-supervised method 2, replacing the ASR hy-potheses of lower confidence scores with their corre-sponding human transcripts.
For a convenient com-parison, we also include the baseline (without LMadaptation) and the result of using the supervisedadaptation.
All the proposed unsupervised/semi-supervised methods can significantly improve theASR performance compared to the baseline result.For projects with time limits, we can use theseunsupervised/semi-supervised methods to help usget relatively good ASR outputs.Table 1: The WER on the evaluation set using differentLM adaptation methods.baselineunsupervisedsemi super.ASR Web ASR&Web42.8 38.1 38.5 37.6 37.8 34.75 Conclusions and Future WorkIn this paper, we reported our experiments in ap-plying several LM adaptation methods to automatedspeech scoring systems that require few, if any, hu-man transcripts, which are expensive and slow toobtain for large-sized adaptation data sets.
The un-supervised training (using ASR transcriptions froma seed ASR system) clearly shows higher accuracythan a ASR system without any domain adaptation.We also used test questions to collect related textsfrom Web.
Even though such Web data may be noisyand its relatedness to real test responses is not al-ways guaranteed, text data collected from the Webis helpful to adapt LMs to better fit the responses totest questions.
To better cope with recognition er-rors brought on by using the unsupervised trainingmethod, we proposed using human transcriptions ona small amount of poorly recognized responses.
Us-ing such little human involvement further helps toobtain a lower WER.
Therefore, based on the ex-periments described in this paper, we conclude thatthese novel LM adaptation methods provide promis-ing solutions to let us skip the ordinary supervisedtraining for LM adaptation tasks frequently used inautomated speech scoring.2The semi-supervised result was from replacing 100 low-confidence responses with human transcripts.291The reported experiments in this paper were con-ducted on a limited-size data set.
We plan to increasethe testing data to a larger size and hope to covermore types of test questions and spoken tests.
In ad-dition, we plan to investigate how to automaticallygenerate Web search queries based on test questions.ReferencesM.
Bacchiani and B. Roark.
2003.
Unsupervised lan-guage model adaptation.
In 2003 IEEE InternationalConference on Acoustics, Speech, and Signal Process-ing, 2003.
Proceedings.(ICASSP?03).M.
Baroni and S. Bernardini.
2004.
BootCaT: bootstrap-ping corpora and terms from the web.
In Proceedingsof LREC, volume 2004, page 13131316.L.
Chen, J. L Gauvain, L. Lamel, and G. Adda.
2003.Unsupervised language model adaptation for broad-cast news.
In 2003 IEEE International Conference onAcoustics, Speech, and Signal Processing, 2003.
Pro-ceedings.(ICASSP?03).L.
Chen, K. Zechner, and X Xi.
2009.
Improved pro-nunciation features for construct-driven assessment ofnon-native spontaneous speech.
In NAACL-HLT.M.
Eskenazi.
2009.
An overview of spoken languagetechnology for education.
Speech Communication,51(10):832?844.K.
Livescu and J.
Glass.
2000.
Lexical modelingof non-native speech for automatic speech recogni-tion.
In Acoustics, Speech, and Signal Processing,2000.
ICASSP?00.
Proceedings.
2000 IEEE Interna-tional Conference on, volume 3, pages 1683?1686.T.
Ng, M. Ostendorf, M. Y Hwang, M. Siu, I. Bulyko, andX.
Lei.
2005.
Web-data augmented language mod-els for mandarin conversational speech recognition.
InProc.
ICASSP, volume 1.G.
Riccardi and D. Z Hakkani-Tur.
2003.
Active and un-supervised learning for automatic speech recognition.In Proc.
8th European Conference on Speech Commu-nication and Technology.R.
Sarikaya, A. Gravano, and Y. Gao.
2005.
Rapid lan-guage model development using external resources fornew spoken dialog domains.
In Proc.
ICASSP, vol-ume 1, pages 573?576.G.
Tur, D. Hakkani-Tur, and R. E Schapire.
2005.
Com-bining active and semi-supervised learning for spo-ken language understanding.
Speech Communication,45(2):171?186.292
