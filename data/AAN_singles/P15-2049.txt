Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 297?302,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsSieve-Based Entity Linking for the Biomedical DomainJennifer D?Souza and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{jld082000,vince}@hlt.utdallas.eduAbstractWe examine a key task in biomedicaltext processing, normalization of disordermentions.
We present a multi-pass sieveapproach to this task, which has the ad-vantage of simplicity and modularity.
Ourapproach is evaluated on two datasets, onecomprising clinical reports and the othercomprising biomedical abstracts, achiev-ing state-of-the-art results.1 IntroductionEntity linking is the task of mapping an entitymention in a text document to an entity in a knowl-edge base.
This task is challenging because (1) thesame word or phrase can be used to refer to differ-ent entities, and (2) the same entity can be referredto by different words or phrases.
In the biomedicaltext processing community, the task is more com-monly known as normalization, where the goal isto map a word or phrase in a document to a uniqueconcept in an ontology (based on the descriptionof that concept in the ontology) after disambiguat-ing potential ambiguous surface words or phrases.Unlike in the news domain, in the biomedical do-main it is rare for the same word or phrase to re-fer to multiple different concepts.
However, dif-ferent words or phrases often refer to the sameconcept.
Given that mentions in biomedical textare relatively unambiguous, normalizing them in-volves addressing primarily the second challengementioned above.The goal of this paper is to advance the stateof the art in normalizing disorder mentions indocuments from two genres, clinical reports andbiomedical abstracts.
For example, given the dis-order mention swelling of abdomen, a normaliza-tion system should map it to the concept in theontology associated with the term abdominal dis-tention.
Not all disorder mentions can be mappedShARe NCBI(Clinical (BiomedicalReports) Abstracts)Train Test Train TestDocuments 199 99 692 100Disorder mentions 5816 5351 5921 964Mentions w/ ID 4178 3615 5921 964ID-less mentions 1638 1736 0 0Table 1: Corpus statistics.to a given ontology, however.
The reason is thatthe ontology may not include all of the possibleconcepts.
Hence, determining whether a disordermention can be mapped to a concept in the givenontology is part of the normalization task.
Notethat disorders have been the target of many re-search initiatives in the biomedical domain, as oneof its major goals is to alleviate health disorders.Our contributions are three-fold.
First, we pro-pose a simpler and more modular approach tonormalization than existing approaches: a multi-pass sieve approach.
Second, our system achievesstate-of-the-art results on datasets from two gen-res, clinical reports and biomedical abstracts.
Toour knowledge, we are the first to present normal-ization results on two genres.
Finally, to facilitatecomparison with future work on this task, we re-lease the source code of our system.12 CorporaWe evaluate our system on two standard corpora(see Table 1 for their statistics):The ShARe/CLEF eHealth Challenge corpus(Pradhan et al, 2013) contains 298 de-identifiedclinical reports from US intensive care partitionedinto 199 reports for training and 99 reports for test-ing.
In each report, each disorder mention is man-ually annotated with either the unique identifier ofthe concept in the reference ontology to which itrefers, or ?CUI-less?
if it cannot be mapped to any1The code is available from http://www.hlt.utdallas.edu/?jld082000/normalization/.297Figure 1: Example concepts in the ontologies.
The first one is taken from SNOMED-CT and the second one is takenfrom MEDIC ontologies.
In each concept, only its ID and the list of terms associated with it are shown.concept in the reference ontology.
The referenceontology used is the SNOMED-CT resource ofthe UMLS Metathesaurus (Campbell et al, 1998),which contains 128,430 disorder concepts.The NCBI disease corpus (Do?gan et al, 2014)contains 793 biomedical abstracts partitioned into693 abstracts for training and development and100 abstracts for testing.
Similar to the ShARecorpus, a disorder mention in each abstract ismanually annotated with the identifier of the con-cept in the reference ontology to which it refers.The reference ontology used is the MEDIC lex-icon (Davis et al, 2012), which contains 11,915disorder concepts.
Unlike in the ShARe corpus,in NCBI only those disorder mentions that can bemapped to a concept in MEDIC are annotated.
Asa result, all the annotated disorder mentions in theNCBI corpus have a concept identifier.
Unlike inShARe, in NCBI there exist composite disordermentions, each of which is composed of more thanone disorder mention.
A composite disorder men-tion is annotated with the set of the concept iden-tifiers associated with its constituent mentions.We note that each concept in the two ontolo-gies (the UMLS Metathesaurus and MEDIC) isnot only identified by a concept ID, but also asso-ciated with a number of attributes, such as the listof terms commonly used to refer to the concept,the preferred term used to refer to the concept, andits definition.
In our approach, we use only the listof terms associated with each concept ID in thenormalization process.
Figure 1 shows two exam-ple concepts taken from these two ontologies.3 A Multi-Pass Approach toNormalizationDespite the simplicity and modularity of the multi-pass sieve approach and its successful applica-tion to coreference resolution (Raghunathan et al,2010), it has not been extensively applied to otherNLP tasks.
In this section, we investigate its ap-plication to normalization.3.1 Overview of the Sieve ApproachA sieve is composed of one or more heuristic rules.In the context of normalization, each rule normal-izes (i.e., assigns a concept ID to) a disorder men-tion in a document.
Sieves are ordered by theirprecision, with the most precise sieve appearingfirst.
To normalize a set of disorder mentions ina document, the normalizer makes multiple passesover them: in the i-th pass, it uses only the rulesin the i-th sieve to normalize a mention.
If the i-thsieve cannot normalize a mention unambiguously(i.e., the sieve normalizes it to more than one con-cept in the ontology), the sieve will leave it un-normalized.
When a mention is normalized, it isadded to the list of terms associated with the ontol-ogy concept to which it is normalized.
This way,later sieves can exploit the normalization decisionsmade in earlier sieves.
Note that a normalizationdecision made earlier cannot be overridden later.3.2 Normalization SievesIn this subsection, we describe the ten sieves wedesigned for normalization.
For convenience, weuse the word concept to refer to a concept in theontology, and we say that a disorder mention hasan exact match with a concept if it has an exactmatch with one of the terms associated with it.Sieve 1: Exact Match.
This sieve normalizes adisorder mention m to a concept c if m has an ex-act match with c.Sieve 2: Abbreviation Expansion.
This sievefirst expands all abbreviated disorder mentionsusing Schwartz and Hearst?s (2003) algorithmand the Wikipedia list of disorder abbreviations.2Then, it normalizes a disorder mention m to a con-cept c if the unabbreviated version of m has an ex-act match with c.For each unnormalized mention, we pass bothits original form and its new (i.e., unabbreviated)form, if applicable, to the next sieve.
As we willsee, we keep expanding the set of possible formsof an unnormalized mention in each sieve.
When-ever a subsequent sieve processes an unnormalizedmention, we mean that it processes each form ofthe mention created by the preceding sieves.Sieve 3: Subject ?
Object Conversion.
This2http://en.wikipedia.org/wiki/List_of_abbreviations_for_diseases_and_disorders298sieve normalizes a mention to a concept c if anyof its new forms has an exact match with c. Newforms of a mention m are created from its origi-nal and unabbreviated forms by: (1) replacing anypreposition(s) in m with other prepositions (e.g.,?changes on ekg?
converted to ?changes in ekg?
);(2) dropping a preposition from m and swappingthe substrings surrounding it (e.g., ?changes onekg?
converted to ?ekg changes?
); (3) bringing thelast token to the front, inserting a preposition asthe second token, and shifting the remaining to-kens to the right by two (e.g., ?mental status alter-ation?
converted to ?alteration in mental status?
);and (4) moving the first token to the end, insertinga preposition as the second to last token, and shift-ing the remaining tokens to the left by two (e.g.,?leg cellulitis?
converted to ?cellulitis of leg?).
Asin Sieve 2, for each unnormalized mention in thisand all subsequent sieves, both its original and newforms are passed to the next sieve.Sieve 4: Numbers Replacement.
For a disordermention containing numbers between one to ten,new forms are produced by replacing each num-ber in the mention with other forms of the samenumber.
Specifically, we consider the numeral, ro-man numeral, cardinal, and multiplicative formsof a number for replacement.
For example, threenew forms will be created for ?three vessel dis-ease?
: {?3 vessel disease?, ?iii vessel disease?,and ?triple vessel disease?}.
This sieve normal-izes a mention m to a concept c if one of the newforms of m has an exact match with c.Sieve 5: Hyphenation.
A disorder mention un-dergoes either hyphenation (if it is not already hy-phenated) or dehyphenation (if it is currently hy-phenated).
Hyphenation proceeds as follows: theconsecutive tokens of a mention are hyphenatedone pair at a time to generate a list of hyphenatedforms (e.g., ?ventilator associated pneumonia?
be-comes {?ventilator-associated pneumonia?, ?ven-tilator associated-pneumonia?}).
Dehyphenationproceeds as follows: the hyphens in a mention areremoved one at a time to generate a list of dehy-phenated forms (e.g., ?saethre-chotzen syndrome?becomes ?saethre chotzen syndrome?).
This sievenormalizes a mention m to a concept c if one ofthe new forms of m has an exact match with c.Sieve 6: Suffixation.
Disorder mentions satisfy-ing suffixation patterns manually observed in thetraining data are suffixated.
For example, ?infec-tious source?
becomes ?source of infectious?
inSieve 3, which then becomes ?source of infection?in this sieve.
This sieve normalizes a mention mto a concept c if the suffixated form of m has anexact match with c.Sieve 7: Disorder Synonyms Replacement.
Formentions containing a disorder term, new formsare created by replacing the disorder term with itssynonyms.3For example, ?presyncopal events?becomes {?presyncopal disorders?, ?presyncopalepisodes?, etc.}.
In addition, one more form is cre-ated by dropping the disorder modifier term (e.g.,?iron-overload disease?
becomes ?iron overloaddisease?
in Sieve 5, which becomes ?iron over-load?
in this sieve).
For mentions that do notcontain a disorder term, new forms are createdby appending the disorder synonyms to the men-tion.
E.g., ?crohns?
becomes {?crohns disease?,?crohns disorder?, etc.}.
This sieve normalizes amention m to a concept c if any of the new formsof m has an exact match with c.Sieve 8: Stemming.
Each disorder mention isstemmed using the Porter (1980) stemmer, and thestemmed form is checked for normalization by ex-act match with the stemmed concept terms.Sieve 9: Composite Disorder Mentions/Terms.A disorder mention/concept term is composite ifit contains more than one concept term.
Notethat composite concept terms only appear in theUMLS ontology (i.e., the ontology for the ShARedataset), and composite disorder mentions onlyappear in the NCBI corpus.
Hence, different rulesare used to handle the two datasets in this sieve.
Inthe ShARe corpus, we first split each compositeterm associated with each concept in the UMLSontology (e.g., ?common eye and/or eyelid symp-tom?)
into separate phrases (e.g., {?common eyesymptom?, ?common eyelid symptom?
}), so eachconcept may now be associated with additionalterms (i.e., the split terms).
This sieve then nor-malizes a mention to a concept c if it has an exactmatch with c. In the NCBI corpus, we considereach disorder mention containing ?and?, ?or?, or?/?
as composite, and split each such compositemention into its constituent mentions (e.g., ?pinealand retinal tumors?
is split into {?pineal tumors?,?retinal tumors?}).
This sieve then normalizes acomposite mention m to a concept c as follows.First, it normalizes each of its split mentions to aconcept c if the split mention has an exact match3A list of the disorder word synonyms is manually createdby inspection of the training data.299with c. The normalized form of m will be theunion of the concepts to which each of its splitmentions is normalized.4Sieve 10: Partial Match.
Owing to the differ-ences in the ontologies used for the two domains,the partial match rules for the ShARe corpus aredifferent from those for the NCBI corpus.
InShARe, a mention m is normalized to a conceptc if one of the following ordered set of rules is ap-plicable: (1) m has more than three tokens and hasan exact match with c after dropping its first tokenor its second to last token; (2) c has a term with ex-actly three tokens and m has an exact match withthis term after dropping its first or middle token;and (3) all of the tokens in m appear in one of theterms in c and vice versa.
In NCBI, a mention isnormalized to the concept with which it shares themost tokens.
In the case of ties, the concept withthe fewest tokens is preferred.Finally, the disorder mentions not normalized inany of the sieves are classified as ?CUI-less?.4 Related WorkIn this section, we focus on discussing the two sys-tems that have achieved the best results reported todate on our two evaluation corpora.
We also dis-cuss a state-of-the-art open-domain entity-linkingsystem whose underlying approach is similar inspirit to ours.DNorm (Leaman et al, 2013), which adopts apairwise learning-to-rank approach, achieves thebest normalization result on NCBI.
The inputs totheir system are linear vectors of paired querymentions and candidate concept terms, where thelinear vectors are obtained from a tf-idf vectorspace representation of all unique tokens from thetraining disorder mentions and the candidate con-cept terms.
Among all the candidate conceptsthat a given query disorder mention is paired with,the system normalizes the query mention to thehighest ranked candidate.
Similarity scores forranking the candidates are computed by multiply-ing the linear tf-idf vectors of the paired query-candidate mentions and a learned weight matrix.The weight matrix represents all possible pairs ofthe unique tokens used to create the tf-idf vec-tor.
At the beginning of the learning phase, theweight matrix is initialized as an identity matrix.The matrix weights are then iteratively adjusted4Note that a composite mention in NCBI may be associ-ated with multiple concepts in the ontology.by stochastic gradient descent for all the conceptterms, their matched training data mentions, andtheir mismatched training data mentions.
Afterconvergence, the weight matrix is then employedin the scoring function to normalize the test disor-der mentions.Ghiasvand and Kate?s (Ghiasvand and Kate,2014) system has produced the best results to dateon ShARe.
It first generates variations of a givendisorder word/phrase based on a set of learned editdistance patterns for converting one word/phraseto another, and then attempts to normalize thesequery phrase variations by performing exact matchwith a training disorder mention or a concept term.Rao et al?s (2013) open-domain entity-linkingsystem adopts an approach that is similar in spiritto ours.
It links organizations, geo-political en-tities, and persons to the entities in a Wikipedia-derived knowledge base, utilizing heuristics formatching mention strings with candidate conceptphrases.
While they adopt a learning-based ap-proach where the outcomes of the heuristics areencoded as features for training a ranker, theirheuristics, like ours, employ syntactic transforma-tions of the mention strings.5 EvaluationIn this section, we evaluate our multi-pass sieveapproach to normalization of disorder mentions.Results on normalizing gold disorder mentions areshown in Table 2, where performance is reportedin terms of accuracy (i.e., the percentage of golddisorder mentions correctly normalized).Row 1 shows the baseline results, which are thebest results reported to date on the ShARe andNCBI datasets by Leaman et al (2013) and Ghi-asvand and Kate (2014), respectively.
As we cansee, the baselines achieve accuracies of 89.5 and82.2 on ShARe and NCBI, respectively.The subsequent rows show the results of our ap-proach when our ten sieves are added incremen-tally.
In other words, each row shows the re-sults obtained after adding a sieve to the sieves inthe previous rows.
Our best system results, high-lighted in bold in Table 2, are obtained when allour ten sieves are employed.
These results are sig-nificantly better than the baseline results (pairedt-tests, p < 0.05).To better understand the usefulness of eachsieve, we apply paired t-tests on the results in ad-jacent rows.
We find that among the ten sieves,300ShARe NCBIBASELINE 89.5 82.2OUR SYSTEMSieve 1 (Exact Match) 84.04 69.71+ Sieve 2 (Abbrev.)
86.13 74.17+ Sieve 3 (Subj/Obj) 86.40 74.27+ Sieve 4 (Numbers) 86.45 75.00+ Sieve 5 (Hyphen) 86.62 75.21+ Sieve 6 (Affix) 88.11 75.62+ Sieve 7 (Synonyms) 88.45 76.56+ Sieve 8 (Stemming) 90.47 77.70+ Sieve 9 (Composite) 90.53 78.00+ Sieve 10 (Partial) 90.75 84.65Table 2: Normalization accuracies on the test datafrom the ShARe corpus and the NCBI corpus.Sieve 2 improves the results on both datasets at thelowest significance level (p < 0.02), while Sieves6, 7, 8, and 10 improve results on both datasetsat a slightly higher significance level (p < 0.05).Among the remaining four sieves (3, 4, 5, 9),Sieve 3 improves results only on the clinical re-ports (p < 0.04), Sieve 4 improves results onlyon the biomedical abstracts dataset (p < 0.02),and Sieves 5 and 9 do not have any significant im-pact on either dataset (p > 0.05).
The last findingcan be ascribed to the low proportions of hyphen-ated (Sieve 5) and composite (Sieve 9) disordermentions found in the test datasets.
After remov-ing Sieves 5 and 9, accuracies drop insignificantly(p > 0.05) by 0.3% and 1.14% on the clinical re-ports and biomedical abstracts, respectively.6 Error AnalysisIn this section, we discuss the two major types oferror made by our system.Failure to unambiguously resolve a mention.Errors due to ambiguous normalizations where adisorder mention is mapped to more than one con-cept in the Partial Match sieve comprise 11?13%of the errors made by our system.
For example,?aspiration?
can be mapped to ?pulmonary aspi-ration?
and ?aspiration pneumonia?, and ?growthretardation?
can be mapped to ?fetal growth re-tardation?
and ?mental and growth retardationwith amblyopia?.
This ambiguity typically ariseswhen the disorder mention under consideration isanaphoric, referring to a previously mentioned en-tity in the associated text.
In this case, context canbe used to disambiguate the mention.
Specifically,a coreference resolver can first be used to iden-tify the coreference chain to which the ambiguousmention belongs, and then the ambiguous mentioncan be normalized by normalizing its coreferentyet unambiguous counterparts instead.Normalization beyond syntactic transforma-tions.
This type of error accounts for about 64?71% of the errors made by our system.
It oc-curs when a disorder mention?s string is so lexi-cally dissimilar with the concept terms that noneof our heuristics can syntactically transform it intoany of them.
For example, using our heuristics,?bleeding vessel?
cannot be matched with any ofthe terms representing its associated concept, suchas ?vascular hemorrhage?, ?rupture of blood ves-sel?, and ?hemorrhage of blood vessel?.
Similarly,?dominantly inherited neurodegeneration?
cannotbe matched with any of the terms representing itsassociated concept, such as ?hereditary neurode-generative disease?.
In this case, additional infor-mation beyond a disorder mention?s string and theconcept terms is needed to normalize the mention.For example, one can exploit the contexts sur-rounding the mentions in the training set.
Specifi-cally, given a test disorder mention, one may firstidentify a disorder mention in the training set thatis ?sufficiently?
similar to it based on context, andthen normalize it to the concept that the trainingdisorder mention is normalized to.
Another pos-sibility is to exploit additional knowledge basessuch as Wikipedia.
Specifically, one can queryWikipedia for the test mention?s string, then em-ploy the titles of the retrieved pages as alternatemention names.7 ConclusionWe have presented a multi-pass sieve approachto the under-studied task of normalizing disordermentions in the biomedical domain.
When nor-malizing the gold disorder mentions in the ShAReand NCBI corpora, our approach achieved accu-racies of 90.75 and 84.65, respectively, which arethe best results reported to date on these corpora.Above all, to facilitate comparison with futurework, we released the source code of our normal-ization system.AcknowledgmentsWe thank the three anonymous reviewers for theirdetailed and insightful comments on an earlierdraft of this paper.
This work was supported inpart by NSF Grants IIS-1147644 and IIS-1219142.301ReferencesKeith E. Campbell, Diane E. Oliver, and Edward H.Shortliffe.
1998.
The Unified Medical LanguageSystem: Towards a collaborative approach for solv-ing terminologic problems.
Journal of the AmericanMedical Informatics Assocication, 5(1):12?16.Allan Peter Davis, Thomas C. Wiegers, Michael C.Rosenstein, and Carolyn J. Mattingly.
2012.MEDIC: A practical disease vocabulary used at theComparative Toxicogenomics Database.
Database,2012:bar065.Rezarta Islamaj Do?gan, Robert Leaman, and ZhiyongLu.
2014.
NCBI disease corpus: A resource for dis-ease name recognition and concept normalization.Journal of Biomedical Informatics, 47:1?10.Omid Ghiasvand and Rohit Kate.
2014.
UWM: Disor-der mention extraction from clinical text using CRFsand normalization using learned edit distance pat-terns.
In Proceedings of the 8th International Work-shop on Semantic Evaluation, pages 828?832.Robert Leaman, Rezarta Islamaj Do?gan, and Zhiy-ong Lu.
2013.
DNorm: Disease name normaliza-tion with pairwise learning to rank.
Bioinformatics,pages 2909?2917.Martin F. Porter.
1980.
An algorithm for suffix strip-ping.
Program, 14(3):130?137.Sameer Pradhan, Noemie Elhadad, B South, DavidMartinez, Lee Christensen, Amy Vogel, HannaSuominen, W Chapman, and Guergana Savova.2013.
Task 1: ShARe/CLEF eHealth EvaluationLab 2013.
Online Working Notes of CLEF, 230.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, pages 492?501.Delip Rao, Paul McNamee, and Mark Dredze.
2013.Entity linking: Finding extracted entities in a knowl-edge base.
In Multi-source, Multi-lingual Informa-tion Extraction and Summarization, pages 93?115.Ariel Schwartz and Marti Hearst.
2003.
A simple al-gorithm for identifying abbreviation definitions inbiomedical text.
In Proceedings of the 8th PacificSymposium on Biocomputing, pages 451?462.302
