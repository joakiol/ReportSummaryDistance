Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 421?432, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsAutomatically Constructing a Normalisation Dictionary for MicroblogsBo Han,??
Paul Cook,?
and Timothy Baldwin???
NICTA Victoria Research Laboratory?
Department of Computing and Information Systems, The University of Melbournehanb@student.unimelb.edu.au, paulcook@unimelb.edu.au,tb@ldwin.netAbstractMicroblog normalisation methods often utilisecomplex models and struggle to differenti-ate between correctly-spelled unknown wordsand lexical variants of known words.
In thispaper, we propose a method for construct-ing a dictionary of lexical variants of knownwords that facilitates lexical normalisation viasimple string substitution (e.g.
tomorrow fortmrw).
We use context information to generatepossible variant and normalisation pairs andthen rank these by string similarity.
Highly-ranked pairs are selected to populate the dic-tionary.
We show that a dictionary-based ap-proach achieves state-of-the-art performancefor both F-score and word error rate on a stan-dard dataset.
Compared with other methods,this approach offers a fast, lightweight andeasy-to-use solution, and is thus suitable forhigh-volume microblog pre-processing.1 Lexical NormalisationA staggering number of short text ?microblog?
mes-sages are produced every day through social me-dia such as Twitter (Twitter, 2011).
The immensevolume of real-time, user-generated microblogs thatflows through sites has been shown to have utilityin applications such as disaster detection (Sakaki etal., 2010), sentiment analysis (Jiang et al2011;Gonza?lez-Iba?n?ez et al2011), and event discovery(Weng and Lee, 2011; Benson et al2011).
How-ever, due to the spontaneous nature of the posts,microblogs are notoriously noisy, containing manynon-standard forms ?
e.g., tmrw ?tomorrow?
and2day ?today?
?
which degrade the performance ofnatural language processing (NLP) tools (Ritter etal., 2010; Han and Baldwin, 2011).
To reduce thiseffect, attempts have been made to adapt NLP toolsto microblog data (Gimpel et al2011; Foster et al2011; Liu et al2011b; Ritter et al2011).
An al-ternative approach is to pre-normalise non-standardlexical variants to their standard orthography (Liu etal., 2011a; Han and Baldwin, 2011; Xue et al2011;Gouws et al2011).
For example, se u 2morw!!
!would be normalised to see you tomorrow!
The nor-malisation approach is especially attractive as a pre-processing step for applications which rely on key-word match or word frequency statistics.
For ex-ample, earthqu, eathquake, and earthquakeee ?
allattested in a Twitter corpus ?
have the standardform earthquake; by normalising these types to theirstandard form, better coverage can be achieved forkeyword-based methods, and better word frequencyestimates can be obtained.In this paper, we focus on the task of lexical nor-malisation of English Twitter messages, in whichout-of-vocabulary (OOV) tokens are normalised totheir in-vocabulary (IV) standard form, i.e., a stan-dard form that is in a dictionary.
Following other re-cent work on lexical normalisation (Liu et al2011a;Han and Baldwin, 2011; Gouws et al2011; Liu etal., 2012), we specifically focus on one-to-one nor-malisation in which one OOV token is normalised toone IV word.Naturally, not all OOV words in microblogs arelexical variants of IV words: named entities, e.g.,are prevalent in microblogs, but not all named en-tities are included in our dictionary.
One chal-lenge for lexical normalisation is therefore to dis-421tinguish those OOV tokens that require normalisa-tion from those that are well-formed.
Recent un-supervised approaches have not attempted to distin-guish such tokens from other types of OOV tokens(Cook and Stevenson, 2009; Liu et al2011a), lim-iting their applicability to real-world normalisationtasks.
Other approaches (Han and Baldwin, 2011;Gouws et al2011) have followed a cascaded ap-proach in which lexical variants are first identified,and then normalised.
However, such two-step ap-proaches suffer from poor lexical variant identifica-tion performance, which is propagated to the nor-malisation step.
Motivated by the observation thatmost lexical variants have an unambiguous standardform (especially for longer tokens), and that a lexi-cal variant and its standard form typically occur insimilar contexts, in this paper we propose methodsfor automatically constructing a lexical normalisa-tion dictionary ?
a dictionary whose entries consistof (lexical variant, standard form) pairs ?
that en-ables type-based normalisation.Despite the simplicity of this dictionary-basednormalisation method, we show it to outperformpreviously-proposed approaches.
This very fast,lightweight solution is suitable for real-time pro-cessing of the large volume of streaming microblogdata available from Twitter, and offers a simple solu-tion to the lexical variant detection problem that hin-ders other normalisation methods.
Furthermore, thisdictionary-based method can be easily integratedwith other more-complex normalisation approaches(Liu et al2011a; Han and Baldwin, 2011; Gouwset al2011) to produce hybrid systems.After discussing related work in Section 2, wepresent an overview of our dictionary-based ap-proach to normalisation in Section 3.
In Sections 4and 5 we experimentally select the optimised con-text similarity parameters and string similarity re-ranking method.
We present experimental results onthe unseen test data in Section 6, and offer some con-cluding remarks in Section 7.2 Related WorkGiven a token t, lexical normalisation is the taskof finding argmaxP (s|t) ?
argmaxP (t|s)P (s),where s is the standard form, i.e., an IV word.
Stan-dardly in lexical normalisation, t is assumed to be anOOV token, relative to a fixed dictionary.
In prac-tice, not all OOV tokens should be normalised; i.e.,only lexical variants (e.g., tmrw ?tomorrow?)
shouldbe normalised and tokens that are OOV but other-wise not lexical variants (e.g., iPad ?iPad?)
shouldbe unchanged.
Most work in this area focuses onlyon the normalisation task itself, oftentimes assumingthat the task of lexical variant detection has alreadybeen completed.Various approaches have been proposed to esti-mate the error model, P (t|s).
For example, in workon spell-checking, Brill and Moore (2000) improveon a standard edit-distance approach by consider-ing multi-character edit operations; Toutanova andMoore (2002) build on this by incorporating phono-logical information.
Li et al2006) utilise distri-butional similarity (Lin, 1998) to correct misspelledsearch queries.In text message normalisation, Choudhury et al(2007) model the letter transformations and emis-sions using a hidden Markov model (Rabiner, 1989).Cook and Stevenson (2009) and Xue et al2011)propose multiple simple error models, each of whichcaptures a particular way in which lexical variantsare formed, such as phonetic spelling (e.g., epik?epic?)
or clipping (e.g., walkin ?walking?).
Never-theless, optimally weighting the various error mod-els in these approaches is challenging.Without pre-categorising lexical variants into dif-ferent types, Liu et al2011a) collect Googlesearch snippets from carefully-designed queriesfrom which they then extract noisy lexical variant?standard form pairs.
These pairs are used to traina conditional random field (Lafferty et al2001) toestimate P (t|s) at the character level.
One short-coming of querying a search engine to obtain train-ing pairs is it tends to be costly in terms of time andbandwidth.
Here we exploit microblog data directlyto derive (lexical variant, standard form) pairs, in-stead of relying on external resources.
In more-recent work, Liu et al2012) endeavour to improvethe accuracy of top-n normalisation candidates byintegrating human cognitive inference, character-level transformations and spell checking in their nor-malisation model.
The encouraging results shift thefocus to reranking and promoting the correct nor-malisation to the top-1 position.
However, like muchprevious work on lexical normalisation, this work422assumes perfect lexical variant detection.Aw et al2006) and Kaufmann and Kalita (2010)consider normalisation as a machine translation taskfrom lexical variants to standard forms using off-the-shelf tools.
These methods do not assume that lexi-cal variants have been pre-identified; however, thesemethods do rely on large quantities of labelled train-ing data, which is not available for microblogs.Recently, Han and Baldwin (2011) and Gouwset al2011) propose two-step unsupervised ap-proaches to normalisation, in which lexical vari-ants are first identified, and then normalised.
Theyapproach lexical variant detection by using a con-text fitness classifier (Han and Baldwin, 2011) orthrough dictionary lookup (Gouws et al2011).However, the lexical variant detection of both meth-ods is rather unreliable, indicating the challengeof this aspect of normalisation.
Both of theseapproaches incorporate a relatively small normal-isation dictionary to capture frequent lexical vari-ants with high precision.
In particular, Gouws etal.
(2011) produce a small normalisation lexiconbased on distributional similarity and string simi-larity (Lodhi et al2002).
Our method adopts asimilar strategy using distributional/string similarity,but instead of constructing a small lexicon for pre-processing, we build a much wider-coverage nor-malisation dictionary and opt for a fully lexicon-based end-to-end normalisation approach.
In con-trast to the normalisation dictionaries of Han andBaldwin (2011) and Gouws et al2011) which fo-cus on very frequent lexical variants, we focus onmoderate frequency lexical variants of a minimumcharacter length, which tend to have unambiguousstandard forms; our intention is to produce normali-sation lexicons that are complementary to those cur-rently available.
Furthermore, we investigate the im-pact of a variety of contextual and string similaritymeasures on the quality of the resulting lexicons.In summary, our dictionary-based normalisation ap-proach is a lightweight end-to-end method whichperforms both lexical variant detection and normal-isation, and thus is suitable for practical online pre-processing, despite its simplicity.3 A Lexical Normalisation DictionaryBefore discussing our method for creating a normal-isation dictionary, we first discuss the feasibility ofsuch an approach.3.1 FeasibilityDictionary lookup approaches to normalisation havebeen shown to have high precision but low recall(Han and Baldwin, 2011; Gouws et al2011).
Fre-quent (lexical variant, standard form) pairs such as(u, you) are typically included in the dictionariesused by such methods, while less-frequent itemssuch as (g0tta, gotta) are generally omitted.
Be-cause of the degree of lexical creativity and largenumber of non-standard forms observed on Twitter,a wide-coverage normalisation dictionary would beexpensive to construct manually.
Based on the as-sumption that lexical variants occur in similar con-texts to their standard forms, however, it shouldbe possible to automatically construct a normalisa-tion dictionary with wider coverage than is currentlyavailable.Dictionary lookup is a type-based approach tonormalisation, i.e., every token instance of a giventype will always be normalised in the same way.However, lexical variants can be ambiguous, e.g., ycorresponds to ?you?
in yeah, y r right!
LOL but?why?
in AM CONFUSED!!!
y you did that?
Nev-ertheless, the relative occurrence of ambiguous lex-ical variants is small (Liu et al2011a), and it hasbeen observed that while shorter variants such as yare often ambiguous, longer variants tend to be un-ambiguous.
For example bthday and 4eva are un-likely to have standard forms other than ?birthday?and ?forever?, respectively.
Therefore, the normali-sation lexicons we produce will only contain entriesfor OOVs with character length greater than a spec-ified threshold, which are likely to have an unam-biguous standard form.3.2 Overview of approachOur method for constructing a normalisation dictio-nary is as follows:Input: Tokenised English tweets1.
Extract (OOV, IV) pairs based on distributionalsimilarity.4232.
Re-rank the extracted pairs by string similarity.Output: A list of (OOV, IV) pairs ordered by stringsimilarity; select the top-n pairs for inclusion inthe normalisation lexicon.In Step 1, we leverage large volumes of Twitterdata to identify the most distributionally-similar IVtype for each OOV type.
The result of this pro-cess is a set of (OOV, IV) pairs, ranked by dis-tributional similarity.
The extracted pairs will in-clude (lexical variant, standard form) pairs, such as(tmrw, tomorrow), but will also contain false posi-tives such as (Tusday, Sunday) ?
Tusday is a lexicalvariant, but its standard form is not ?Sunday?
?
and(Youtube,web) ?
Youtube is an OOV named en-tity, not a lexical variant.
Nevertheless, lexical vari-ants are typically formed from their standard formsthrough regular processes (Thurlow, 2003) ?
e.g.,the omission of characters ?
and from this per-spective Sunday and web are not plausible standardforms for Tusday and Youtube, respectively.
In Step2, we therefore capture this intuition to re-rank theextracted pairs by string similarity.
The top-n itemsin this re-ranked list then form the normalisation lex-icon, which is based only on development data.Although computationally-expensive to build,this dictionary can be created offline.
Once built,it then offers a very fast approach to normalisation.We can only reliably compute distributional simi-larity for types that are moderately frequent in a cor-pus.
Nevertheless, many lexical variants are suffi-ciently frequent to be able to compute distributionalsimilarity, and can potentially make their way intoour normalisation lexicon.
This approach is not suit-able for normalising low-frequency lexical variants,nor is it suitable for shorter lexical variant typeswhich ?
as discussed in Section 3.1 ?
are morelikely to have an ambiguous standard form.
Never-theless, previously-proposed normalisation methodsthat can handle such phenomena also rely in part ona normalisation lexicon.
The normalisation lexiconswe create can therefore be easily integrated with pre-vious approaches to form hybrid normalisation sys-tems.4 Contextually-similar Pair GenerationOur objective is to extract contextually-similar(OOV, IV) pairs from a large-scale collection of mi-croblog data.
Fundamentally, the surrounding wordsdefine the primary context, but there are differentways of representing context and different similar-ity measures we can use, which may influence thequality of generated normalisation pairs.In representing the context, we experimentally ex-plore the following factors: (1) context window size(from 1 to 3 tokens on both sides); (2) n-gram or-der of the context tokens (unigram, bigram, trigram);(3) whether context words are indexed for relativeposition or not; and (4) whether we use all contexttokens, or only IV words.
Because high-accuracylinguistic processing tools for Twitter are still underexploration (Liu et al2011b; Gimpel et al2011;Ritter et al2011; Foster et al2011), we do notconsider richer representations of context, for exam-ple, incorporating information about part-of-speechtags or syntax.
We also experiment with a numberof simple but widely-used geometric and informa-tion theoretic distance/similarity measures.
In par-ticular, we use Kullback?Leibler (KL) divergence(Kullback and Leibler, 1951), Jensen?Shannon (JS)divergence (Lin, 1991), Euclidean distance and Co-sine distance.We use a corpus of 10 million English tweets to doparameter tuning over, and a larger corpus of tweetsin the final candidate ranking.
All tweets were col-lected from September 2010 to January 2011 viathe Twitter API.1 From the raw data we extractEnglish tweets using a language identification tool(Lui and Baldwin, 2011), and then apply a simpli-fied Twitter tokeniser (adapted from O?Connor et al(2010)).
We use the Aspell dictionary (v6.06)2 todetermine whether a word is IV, and only includein our normalisation dictionary OOV tokens withat least 64 occurrences in the corpus and characterlength ?
4, both of which were determined throughempirical observation.
For each OOV word type inthe corpus, we select the most similar IV type toform (OOV, IV) pairs.
To further narrow the searchspace, we only consider IV words which are mor-phophonemically similar to the OOV type, follow-ing settings in Han and Baldwin (2011).31https://dev.twitter.com/docs/streaming-api/methods2http://aspell.net/3We only consider IV words within an edit distance of 2 or aphonemic edit distance of 1 from the OOV type, and we further424In order to evaluate the generated pairs, we ran-domly selected 1000 OOV words from the 10 mil-lion tweet corpus.
We set up an annotation taskon Amazon Mechanical Turk,4 presenting five in-dependent annotators with each word type (with nocontext) and asking for corrections where appropri-ate.
For instance, given tmrw, the annotators wouldlikely identify it as a non-standard variant of ?to-morrow?.
For correct OOV words like iPad, on theother hand, we would expect them to leave the wordunchanged.
If 3 or more of the 5 annotators makethe same suggestion (in the form of either a canoni-cal spelling or leaving the word unchanged), we in-clude this in our gold standard for evaluation.
Intotal, this resulted in 351 lexical variants and 282correct OOV words, accounting for 63.3% of the1000 OOV words.
These 633 OOV words were usedas (OOV, IV) pairs for parameter tuning.
The re-mainder of the 1000 OOV words were ignored onthe grounds that there was not sufficient consensusamongst the annotators.5Contextually-similar pair generation aims to in-clude as many correct normalisation pairs as pos-sible.
We evaluate the quality of the normalisationpairs using ?Cumulative Gain?
(CG):CG =N ?
?i=1rel?iSuppose there are N ?
correct generated pairs(oovi, ivi), each of which is weighted by rel?i, thefrequency of oovi to indicate its relative importance;for example, (thinkin, thinking) has a higher weightthan (g0tta, gotta) because thinkin is more frequentthan g0tta in our corpus.
In this evaluation we don?tconsider the position of normalisation pairs, and nordo we penalise incorrect pairs.
Instead, we push dis-tinguishing between correct and incorrect pairs intothe downstream re-ranking step in which we incor-porate string similarity information.Given the development data and CG, we run anexhaustive search of parameter combinations overonly consider the top 30% most-frequent of these IV words.4https://www.mturk.com/mturk/welcome5Note that the objective of this annotation task is to identifylexical variants that have agreed-upon standard forms irrespec-tive of context, as a special case of the more general task oflexical normalisation (where context may or may not play a sig-nificant role in the determination of the normalisation).our development corpus.
The five best parametercombinations are shown in Table 1.
We notice theCG is almost identical for the top combinations.
Asa context window size of 3 incurs a heavy process-ing and memory overhead over a size of 2, we usethe 3rd-best parameter combination for subsequentexperiments, namely: context window of?2 tokens,token bigrams, positional index, and KL divergenceas our distance measure.To better understand the sensitivity of the methodto each parameter, we perform a post-hoc parame-ter analysis relative to a default setting (as under-lined in Table 2), altering one parameter at a time.The results in Table 2 show that bigrams outper-form other n-gram orders by a large margin (notethat the evaluation is based on a log scale), andinformation-theoretic measures are superior to thegeometric measures.
Furthermore, it also indicatesusing the positional indexing better captures context.However, there is little to distinguish context mod-elling with just IV words or all tokens.
Similarly,the context window size has relatively little impacton the overall performance, supporting our earlierobservation from Table 1.5 Pair Re-ranking by String SimilarityOnce the contextually-similar (OOV, IV) pairs aregenerated using the selected parameters in Section4, we further re-rank this set of pairs in an at-tempt to boost morphophonemically-similar pairslike (bananaz, bananas), and penalise noisy pairslike (paninis, beans).Instead of using the small 10 million tweet cor-pus, from this step onwards, we use a larger cor-pus of 80 million English tweets (collected over thesame period as the development corpus) to developa larger-scale normalisation dictionary.
This is be-cause once pairs are generated, re-ranking based onstring comparison is much faster.
We only includein the dictionary OOV words with a token frequency> 15 to include more OOV types than in Section 4,and again apply a minimum length cutoff of 4 char-acters.To measure how well our re-ranking method pro-motes correct pairs and demotes incorrect pairs (in-cluding both OOV words that should not be nor-malised, e.g.
(Youtube,web), and incorrect normal-425Rank Window size n-gram Positional index?
Lex.
choice Sim/distance measure log(CG)1 ?3 2 Yes All KL divergence 19.5712 ?3 2 No All KL divergence 19.5623 ?2 2 Yes All KL divergence 19.5624 ?3 2 Yes IVs KL divergence 19.5615 ?2 2 Yes IVs JS divergence 19.554Table 1: The five best parameter combinations in the exhaustive search of parameter combinationsWindow size n-gram Positional index?
Lexical choice Similarity/distance measure?1 19.325 1 19.328 Yes 19.328 IVs 19.335 KL divergence 19.328?2 19.327 2 19.571 No 19.263 All 19.328 Euclidean 19.227?3 19.328 3 19.324 JS divergence 19.311Cosine 19.170Table 2: Parameter sensitivity analysis measured as log(CG) for correctly-generated pairs.
We tune one parameter ata time, using the default (underlined) setting for other parameters; the non-exhaustive best-performing setting in eachcase is indicated in bold.isations for lexical variants, e.g.
(bcuz, cause)), wemodify our evaluation metric from Section 4 toevaluate the ranking at different points, using Dis-counted Cumulative Gain (DCG@N : Jarvelin andKekalainen (2002)):DCG@N = rel1 +N?i=2relilog2 (i)where reli again represents the frequency of theOOV, but it can be gain (a positive number) or loss(a negative number), depending on whether the ithpair is correct or incorrect.
Because we also expectcorrect pairs to be ranked higher than incorrect pairs,DCG@N takes both factors into account.Given the generated pairs and the evaluation met-ric, we first consider three baselines: no re-ranking(i.e., the final ranking is that of the contextual simi-larity scores), and re-rankings of the pairs based onthe frequencies of the OOVs in the Twitter corpus,and the IV unigram frequencies in the Google Web1T corpus (Brants and Franz, 2006) to get less-noisyfrequency estimates.
We also compared a variety ofre-rankings based on a number of string similaritymeasures that have been previously considered innormalisation work (reviewed in Section 2).
We ex-periment with standard edit distance (Levenshtein,1966), edit distance over double metaphone codes(phonetic edit distance: (Philips, 2000)), longestcommon subsequence ratio over the consonant editdistance of the paired words (hereafter, denoted asconsonant edit distance: (Contractor et al2010)),and a string subsequence kernel (Lodhi et al2002).In Figure 1, we present the DCG@N results foreach of our ranking methods at different rank cut-offs.
Ranking by OOV frequency is motivated bythe assumption that lexical variants are frequentlyused by social media users.
This is confirmedby our findings that lexical pairs like (goin, going)and (nite, night) are at the top of the ranking.However, many proper nouns and named entitiesare also used frequently and ranked at the top,mixed with lexical variants like (Facebook, speech)and (Youtube,web).
In ranking by IV word fre-quency, we assume the lexical variants are usuallyderived from frequently-used IV equivalents, e.g.
(abou, about).
However, many less-frequent lexicalvariant types have high-frequency (IV) normalisa-tions.
For instance, the highest-frequency IV wordthe has more than 40 OOV lexical variants, such astthe and thhe.
These less-frequent types occupy thetop positions, reducing the cumulative gain.
Com-pared with these two baselines, ranking by defaultcontextual similarity scores delivers promising re-sults.
It successfully ranks many more intuitive nor-malisation pairs at the top, such as (2day, today)and (wknd,weekend), but also ranks some incorrectpairs highly, such as (needa, gotta).The string similarity-based methods perform bet-ter than our baselines in general.
Through man-ual analysis, we found that standard edit dis-426tance ranking is fairly accurate for lexical vari-ants with low edit distance to their standard forms,but fails to identify heavily-altered variants like(tmrw, tomorrow).
Consonant edit distance is simi-lar to standard edit distance, but places many longerwords at the top of the ranking.
Edit distanceover double metaphone codes (phonetic edit dis-tance) performs particularly well for lexical vari-ants that include character repetitions ?
commonlyused for emphasis on Twitter ?
because such rep-etitions do not typically alter the phonetic codes.Compared with the other methods, the string subse-quence kernel delivers encouraging results.
It mea-sures common character subsequences of length nbetween (OOV, IV) pairs.
Because it is computa-tionally expensive to calculate similarity for largern, we choose n=2, following Gouws et al2011).As N (the lexicon size cut-off) increases, the per-formance drops more slowly than the other meth-ods.
Although this method fails to rank heavily-altered variants such as (4get, forget) highly, it typi-cally works well for longer words.
Given that we fo-cus on longer OOVs (specifically those longer than4 characters), this ultimately isn?t a great handicap.6 EvaluationGiven the re-ranked pairs from Section 5, here weapply them to a token-level normalisation task us-ing the normalisation dataset of Han and Baldwin(2011).6.1 MetricsWe evaluate using the standard evaluation metrics ofprecision (P), recall (R) and F-score (F) as detailedbelow.
We also consider the false alarm rate (FA)and word error rate (WER), also as shown below.FA measures the negative effects of applying nor-malisation; a good approach to normalisation shouldnot (incorrectly) normalise tokens that are alreadyin their standard form and do not require normalisa-tion.6 WER, like F-score, shows the overall benefitsof normalisation, but unlike F-score, measures howmany token-level edits are required for the output tobe the same as the ground truth data.
In general, dic-tionaries with a high F-score/low WER and low FA6FA + P ?
1 because some lexical variants might be incor-rectly normalised.are preferable.P =# correctly normalised tokens# normalised tokensR =# correctly normalised tokens# tokens requiring normalisationF =2PRP +RFA =# incorrectly normalised tokens# normalised tokensWER =# token edits needed after normalisation# all tokens6.2 ResultsWe select the three best re-ranking methods, andbest cut-off N for each method, based on thehighest DCG@N value for a given method overthe development data, as presented in Figure 1.Namely, they are string subsequence kernel (S-dict,N=40,000), double metaphone edit distance (DM-dict, N=10,000) and default contextual similaritywithout re-ranking (C-dict, N=10,000).7We evaluate each of the learned dictionaries in Ta-ble 3.
We also compare each dictionary with theperformance of the manually-constructed Internetslang dictionary (HB-dict) used by Han and Bald-win (2011), the small automatically-derived dictio-nary of Gouws et al2011) (GHM-dict), and com-binations of the different dictionaries.
In addition,the contribution of these dictionaries in hybrid nor-malisation approaches is also presented, in which wefirst normalise OOVs using a given dictionary (com-bined or otherwise), and then apply the normalisa-tion method of Gouws et al2011) based on con-sonant edit distance (GHM-norm), or the approachof Han and Baldwin (2011) based on the summationof many unsupervised approaches (HB-norm), to theremaining OOVs.
Results are shown in Table 3, anddiscussed below.6.2.1 Individual DictionariesOverall, the individual dictionaries derived by there-ranking methods (DM-dict, S-dict) perform bet-7We also experimented with combining ranks using MeanReciprocal Rank.
However, the combined rank didn?t improveperformance on the development data.
We plan to explore otherranking aggregation methods in future work.427Ncut?offsDiscounted Cumulative Gain10K30K50K70K90K110K130K150K170K190K?60K?40K?20K020K40KWithout rerankOOVfrequencyIVfrequencyEditdistanceConsonant edit dist.Phoneticedit dist.Stringsubseq.
kernelFigure 1: Re-ranking based on different string similarity methods.ter than that based on contextual similarity (C-dict)in terms of precision and false alarm rate, indicatingthe importance of re-ranking.
Even though C-dictdelivers higher recall ?
indicating that many lexi-cal variants are correctly normalised ?
this is offsetby its high false alarm rate, which is particularly un-desirable in normalisation.
Because S-dict has betterperformance than DM-dict in terms of both F-scoreand WER, and a much lower false alarm rate thanC-dict, subsequent results are presented using S-dictonly.Both HB-dict and GHM-dict achieve better than90% precision with moderate recall.
Compared tothese methods, S-dict is not competitive in terms ofeither precision or recall.
This result seems ratherdiscouraging.
However, considering that S-dict is anautomatically-constructed dictionary targeting lexi-cal variants of varying frequency, it is not surprisingthat the precision is worse than that of HB-dict ?which is manually-constructed ?
and GHM-dict ?which includes entries only for more-frequent OOVsfor which distributional similarity is more accurate.Additionally, the recall of S-dict is hampered by therestriction on lexical variant token length of 4 char-acters.6.2.2 Combined DictionariesNext we look to combining HB-dict, GHM-dictand S-dict.
In combining the dictionaries, a givenOOV word can be listed with different standardforms in different dictionaries.
In such cases we usethe following preferences for dictionaries ?
moti-vated by our confidence in the normalisation pairsof the dictionaries ?
to resolve conflicts: HB-dict> GHM-dict > S-dict.When we combine dictionaries in the second sec-tion of Table 3, we find that they contain com-plementary information: in each case the recalland F-score are higher for the combined dictio-nary than any of the individual dictionaries.
Thecombination of HB-dict+GHM-dict produces onlya small improvement in terms of F-score over HB-dict (the better-performing dictionary) suggestingthat, as claimed, HB-dict and GHM-dict share manyfrequent normalisation pairs.
HB-dict+S-dict andGHM-dict+S-dict, on the other hand, improve sub-428Method Precision Recall F-Score False Alarm Word Error RateC-dict 0.474 0.218 0.299 0.298 0.103DM-dict 0.727 0.106 0.185 0.145 0.102S-dict 0.700 0.179 0.285 0.162 0.097HB-dict 0.915 0.435 0.590 0.048 0.066GHM-dict 0.982 0.319 0.482 0.000 0.076HB-dict+S-dict 0.840 0.601 0.701 0.090 0.052GHM-dict+S-dict 0.863 0.498 0.632 0.072 0.061HB-dict+GHM-dict 0.920 0.465 0.618 0.045 0.063HB-dict+GHM-dict+S-dict 0.847 0.630 0.723 0.086 0.049GHM-dict+GHM-norm 0.338 0.578 0.427 0.458 0.135HB-dict+GHM-dict+S-dict+GHM-norm 0.406 0.715 0.518 0.468 0.124HB-dict+HB-norm 0.515 0.771 0.618 0.332 0.081HB-dict+GHM-dict+S-dict+HB-norm 0.527 0.789 0.632 0.332 0.079Table 3: Normalisation results using our derived dictionaries (contextual similarity (C-dict); double metaphone ren-dering (DM-dict); string subsequence kernel scores (S-dict)), the dictionary of Gouws et al2011) (GHM-dict), theInternet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries.
In addition,we combine the dictionaries with the normalisation method of Gouws et al2011) (GHM-norm) and the combinedunsupervised approach of Han and Baldwin (2011) (HB-norm).stantially over HB-dict and GHM-dict, respectively,indicating that S-dict contains markedly differententries to both HB-dict and GHM-dict.
The best F-score and WER are obtained using the combinationof all three dictionaries, HB-dict+GHM-dict+S-dict.Furthermore, the difference between the results us-ing HB-dict+GHM-dict+S-dict and HB-dict+GHM-dict is statistically significant (p < 0.01), based onthe computationally-intensive Monte Carlo methodof Yeh (2000), demonstrating the contribution of S-dict.6.2.3 Hybrid ApproachesThe methods of Gouws et al2011) (i.e.GHM-dict+GHM-norm) and Han and Baldwin(2011) (i.e.
HB-dict+HB-norm) have lower preci-sion and higher false alarm rates than the dictionary-based approaches; this is largely caused by lex-ical variant detection errors.8 Using all dic-tionaries in combination with these methods ?HB-dict+GHM-dict+S-dict+GHM-norm and HB-dict+GHM-dict+S-dict+HB-norm ?
gives someimprovements, but the false alarm rates remain high.Despite the limitations of a pure dictionary-basedapproach to normalisation ?
discussed in Section3.1 ?
the current best practical approach to normal-8Here we report results that do not assume perfect detectionof lexical variants, unlike the original published results in eachcase.Error type OOVStandard formDict.
Gold(a) plurals playe players player(b) negation unlike like dislike(c) possessives anyones anyone anyone?s(d) correct OOVs iphone phone iphone(e) test data errors durin during durin(f) ambiguity siging signing singingTable 4: Error types in the combined dictionary (HB-dict+GHM-dict+S-dict)isation is to use a lexicon, combining hand-built andautomatically-learned normalisation dictionaries.6.3 Discussion and Error AnalysisWe first manually analyse the errors in the combineddictionary (HB-dict+GHM-dict+S-dict) and give ex-amples of each error type in Table 4.
The most fre-quent word errors are caused by slight morphologi-cal variations, including plural forms (a), negations(b), possessive cases (c), and OOVs that are correctand do not require normalisation (d).
In addition, wealso notice some missing annotations where lexicalvariants are skipped by human annotations but cap-tured by our method (e).
Ambiguity (f) definitelyexists in longer OOVs, however, these cases do notappear to have a strong negative impact on the nor-malisation performance.
An example of a remain-429Length cut-off (N ) #Variants Precision Recall (?
N ) Recall (all) False Alarm?4 556 0.700 0.381 0.179 0.162?5 382 0.814 0.471 0.152 0.122?6 254 0.804 0.484 0.104 0.131?7 138 0.793 0.471 0.055 0.122Table 5: S-dict normalisation results broken down according to OOV token length.
Recall is presented both over thesubset of instances of length ?
N in the data (?Recall (?
N )?
), and over the entirety of the dataset (?Recall (all)?);?#Variants?
is the number of token instances of the indicated length in the test dataset.ing miscellaneous error is bday ?birthday?, which ismis-normalised as day.To further study the influence of OOV wordlength relative to the normalisation performance, weconduct a fine-grained analysis of the performanceof the derived dictionary (S-dict) in Table 5, bro-ken down across different OOV word lengths.
Theresults generally support our hypothesis that ourmethod works better for longer OOV words.
Thederived dictionary is much more reliable for longertokens (length 5, 6, and 7 characters) in terms of pre-cision and false alarm.
Although the recall is rela-tively modest, in the future we intend to improve re-call by mining more normalisation pairs from largercollections of microblog data.7 Conclusions and Future WorkIn this paper, we describe a method for automat-ically constructing a normalisation dictionary thatsupports normalisation of microblog text through di-rect substitution of lexical variants with their stan-dard forms.
After investigating the impact of dif-ferent distributional and string similarity methodson the quality of the dictionary, we present ex-perimental results on a standard dataset showingthat our proposed methods acquire high quality(lexical variant, standard form) pairs, with reason-able coverage, and achieve state-of-the-art end-to-end lexical normalisation performance on a real-world token-level task.
Furthermore, this dictionary-lookup method combines the detection and normali-sation of lexical variants into a simple, lightweightsolution which is suitable for processing of high-volume microblog feeds.In the future, we intend to improve our dictionaryby leveraging the constantly-growing volume of mi-croblog data, and considering alternative ways tocombine distributional and string similarity.
In addi-tion to direct evaluation, we also want to explore thebenefits of applying normalisation for downstreamsocial media text processing applications, e.g.
eventdetection.AcknowledgementsWe would like to thank the three anonymous re-viewers for their insightful comments, and StephanGouws for kindly sharing his data and discussing hiswork.NICTA is funded by the Australian governmentas represented by Department of Broadband, Com-munication and Digital Economy, and the AustralianResearch Council through the ICT centre of Excel-lence programme.ReferencesAiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for SMS text normal-ization.
In Proceedings of COLING/ACL 2006, pages33?40, Sydney, Australia.Edward Benson, Aria Haghighi, and Regina Barzilay.2011.
Event discovery in social media feeds.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies (ACL-HLT 2011), pages 389?398, Port-land, Oregon, USA.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.
InProceedings of the 38th Annual Meeting of the Associ-ation for Computational Linguistics, pages 286?293,Hong Kong.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and modeling of the structure oftexting language.
International Journal on DocumentAnalysis and Recognition, 10:157?174.430Danish Contractor, Tanveer A. Faruquie, and L. VenkataSubramaniam.
2010.
Unsupervised cleansing of noisytext.
In Proceedings of the 23rd International Confer-ence on Computational Linguistics (COLING 2010),pages 189?196, Beijing, China.Paul Cook and Suzanne Stevenson.
2009.
An unsu-pervised model for text message normalization.
InCALC ?09: Proceedings of the Workshop on Computa-tional Approaches to Linguistic Creativity, pages 71?78, Boulder, USA.Jennifer Foster, O?zlem C?etinoglu, Joachim Wagner,Joseph L. Roux, Stephen Hogan, Joakim Nivre,Deirdre Hogan, and Josef van Genabith.
2011.
#hard-toparse: POS Tagging and Parsing the Twitterverse.In Analyzing Microtext: Papers from the 2011 AAAIWorkshop, volume WS-11-05 of AAAI Workshops,pages 20?25, San Francisco, CA, USA.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging forTwitter: Annotation, features, and experiments.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (ACL-HLT 2011), pages 42?47,Portland, Oregon, USA.Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and NinaWacholder.
2011.
Identifying sarcasm in Twitter:a closer look.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies (ACL-HLT2011), pages 581?586, Portland, Oregon, USA.Stephan Gouws, Dirk Hovy, and Donald Metzler.
2011.Unsupervised mining of lexical variants from noisytext.
In Proceedings of the First workshop on Unsu-pervised Learning in NLP, pages 82?90, Edinburgh,Scotland, UK.Bo Han and Timothy Baldwin.
2011.
Lexical normal-isation of short text messages: Makn sens a #twitter.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies (ACL-HLT 2011), pages 368?378,Portland, Oregon, USA.K.
Jarvelin and J. Kekalainen.
2002.
Cumulated gain-based evaluation of IR techniques.
ACM Transactionson Information Systems, 20(4).Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and TiejunZhao.
2011.
Target-dependent Twitter sentiment clas-sification.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies (ACL-HLT 2011), pages151?160, Portland, Oregon, USA.Joseph Kaufmann and Jugal Kalita.
2010.
Syntactic nor-malization of Twitter messages.
In International Con-ference on Natural Language Processing, Kharagpur,India.S.
Kullback and R. A. Leibler.
1951.
On information andsufficiency.
Annals of Mathematical Statistics, 22:49?86.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of the Eighteenth International Confer-ence on Machine Learning, pages 282?289, San Fran-cisco, CA, USA.Vladimir I. Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
SovietPhysics Doklady, 10:707?710.Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou.
2006.Exploring distributional similarity based models forquery spelling correction.
In Proceedings of COL-ING/ACL 2006, pages 1025?1032, Sydney, Australia.Jianhua Lin.
1991.
Divergence measures based on theshannon entropy.
IEEE Transactions on InformationTheory, 37(1):145?151.Dekang Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In Proceedings of the 36th An-nual Meeting of the ACL and 17th International Con-ference on Computational Linguistics (COLING/ACL-98), pages 768?774, Montreal, Quebec, Canada.Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.2011a.
Insertion, deletion, or substitution?
normal-izing text messages without pre-categorization nor su-pervision.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies (ACL-HLT 2011), pages71?76, Portland, Oregon, USA.Xiaohua Liu, Shaodian Zhang, Furu Wei, and MingZhou.
2011b.
Recognizing named entities in tweets.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies (ACL-HLT 2011), pages 359?367,Portland, Oregon, USA.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012.
A broad-coverage normalization system for social media lan-guage.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics (ACL2012), Jeju, Republic of Korea.Huma Lodhi, Craig Saunders, John Shawe-Taylor, NelloCristianini, and Chris Watkins.
2002.
Text classifica-tion using string kernels.
J. Mach.
Learn.
Res., 2:419?444.Marco Lui and Timothy Baldwin.
2011.
Cross-domainfeature selection for language identification.
In Pro-ceedings of the 5th International Joint Conference onNatural Language Processing (IJCNLP 2011), pages553?561, Chiang Mai, Thailand.431Brendan O?Connor, Michel Krieger, and David Ahn.2010.
TweetMotif: Exploratory search and topic sum-marization for Twitter.
In Proceedings of the 4th In-ternational Conference on Weblogs and Social Media(ICWSM 2010), pages 384?385, Washington, USA.Lawrence Philips.
2000.
The double metaphone searchalgorithm.
C/C++ Users Journal, 18:38?43.Lawrence R. Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?286.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Un-supervised modeling of Twitter conversations.
InProceedings of Human Language Technologies: The11th Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL-HLT 2010), pages 172?180, Los Angeles,USA.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An ex-perimental study.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP 2011), pages 1524?1534, Edinburgh,Scotland, UK.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes Twitter users: real-timeevent detection by social sensors.
In Proceedings ofthe 19th International Conference on the World WideWeb (WWW 2010), pages 851?860, Raleigh, NorthCarolina, USA.Crispin Thurlow.
2003.
Generation txt?
The sociolin-guistics of young people?s text-messaging.
DiscourseAnalysis Online, 1(1).Kristina Toutanova and Robert C. Moore.
2002.
Pro-nunciation modeling for improved spelling correction.In Proceedings of the 40th Annual Meeting of theACL and 3rd Annual Meeting of the NAACL (ACL-02),pages 144?151, Philadelphia, USA.Official Blog Twitter.
2011.
200 million tweets per day.Retrived at August 17th, 2011.Jianshu Weng and Bu-Sung Lee.
2011.
Event detectionin Twitter.
In Proceedings of the 5th InternationalConference on Weblogs and Social Media (ICWSM2011), Barcelona, Spain.Zhenzhen Xue, Dawei Yin, and Brian D. Davison.
2011.Normalizing microtext.
In Proceedings of the AAAI-11 Workshop on Analyzing Microtext, pages 74?79,San Francisco, USA.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics (COLING 2010), pages 947?953,Saarbru?cken, Germany.432
