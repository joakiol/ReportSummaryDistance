Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),pages 28?36, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational LinguisticsFactuality Detection on the Cheap:Inferring Factuality for Increased Precision in Detecting Negated EventsErik VelldalUniversity of OsloDepartment of Informaticserikve@ifi.uio.noJonathon ReadUniversity of OsloDepartment of Informaticsjread@ifi.uio.noAbstractThis paper describes a system for discriminat-ing between factual and non-factual contexts,trained on weakly labeled data by taking ad-vantage of information implicit in annotationsof negated events.
In addition to evaluatingfactuality detection in isolation, we also evalu-ate its impact on a system for event detection.The two components for factuality detectionand event detection form part of a system foridentifying negative factual events, or coun-terfacts, with top-ranked results in the *SEM2012 shared task.1 IntroductionThe First Joint Conference on Lexical and Compu-tational Semantics (*SEM 2012) is hosting a sharedtask1 (Morante and Blanco, 2012) on identifyingvarious elements of negation, and one of the sub-tasks is to identify negated events.
However, onlyevents occurring in factual statements should be la-beled.
This paper describes pilot experiments onhow to train a factuality classifier by taking advan-tage of implicit information on factuality in anno-tations of negation.
In addition to evaluating factu-ality detection in isolation, we also assess its impactwhen embedded in a system for event detection.
Thesystem was ranked first for the *SEM 2012 subtaskof identifying negated events, and also formed partof the top-ranked system in the shared task overall(Read et al, 2012).
The experiments presented inthis paper further improves on these initial results.1The web site of the 2012 *SEM Shared Task:http://www.clips.ua.ac.be/sem2012-st-neg/Note that the system was designed for submission tothe closed track of the shared task, which means de-velopment is constrained to using the data providedby the task organizers.The rest of the paper is structured as follows.
Westart in Section 2 by giving a brief overview of re-lated work and resources.
In Section 3 we thenpresent the problem statement in more detail, alongwith the relevant data sets.
This section also dis-cusses the notion of (non-)factuality assumed in thecurrent paper.
We then go on to present and evaluatethe factuality classifier in Section 4.
In Section 5we move on to describe the event detection task,which is handled by learning a discriminative rank-ing function over candidate tokens within the nega-tion scope, using features from paths in constituenttrees.
Both the event ranking function and the fac-tuality classifier are implemented using the SupportVector Machine (SVM) framework.
After evaluat-ing the impact of factuality detection on event de-tection, we finally provide some concluding remarksand discussion of future directions in Section 6.2 Related WorkNote that the *SEM 2012 shared task singled outthree separate subtasks for the problem of recogniz-ing negation, namely the identification of negationcues, their in-sentence scopes and the negated fac-tual events.
Most of the systems submitted for theshared task correspondingly implemented a pipelineconsisting of three components, one for each sub-task.
One thing that set the system of Read et al(2012) apart from other shared task submissions isthat it included a fourth component; a dedicated28classifier for identifying the factuality of a givencontext.
It is this latter problem which is the mainfocus of the current paper, along with its interactionswith the task of identifying events.The field has witnessed a growing body of workdealing with uncertainty and speculative languageover the recent years, and in particular so within thedomain of biomedical literature.
These efforts havebeen propelled not least by the several shared tasksthat have targeted such phenomena.
The shared taskat the 2010 Conference on Natural Language Learn-ing (CoNLL) focused on speculation detection forthe domain of biomedical research literature (Farkaset al, 2010), with data sets based on the BioScopecorpus (Vincze et al, 2008) which annotates so-called speculation cues along with their scopes.
TheBioNLP shared tasks of 2009 and 2011 mainly con-cerned recognizing bio-molecular events in text, butoptional subtasks involved detecting whether theseevents were affected by speculation or negation.
Thedata set used for this task is the Genia event corpus(Kim et al, 2008) which annotates the uncertaintyof events according to the three labels certain, prob-able and doubtful (but without explicitly annotatingcue words or scope as in BioScope).The best performer in the BioNLP 2011 support-ing task of detecting speculation modification ofevents, the system of Kilicoglu and Bergler (2011),achieved an end-to-end F1 of 27.25 using a manu-ally compiled dictionary of trigger expressions to-gether with a set of rules operating on syntactic de-pendencies for identifying events and event modifi-cation.
Turning to the task of identifying specula-tion cues in the BioScope data, current state-of-the-art systems, implementing simple supervised classi-fication approaches on the token- or sequence-level,achieves F1-scores of well above 80 (Tang et al,2010; Velldal et al, 2012).
For the task of resolv-ing the scopes of these cues, the current best systemsobtain end-to-end F1-scores close to 60 in held-outtesting (Morante et al, 2010; Velldal et al, 2012).Note that the latter reference is from a forthcom-ing issue of Computational Linguistics specificallyon modality and negation (Morante and Sporleder,2012).
In that same issue, Saur??
and Pustejovsky(2012) present a linguistically motivated system forfactuality profiling with manually crafted rules op-erating on dependency graphs.
Conceptually treat-ing factuality as a perspective that a particular source(speaker) holds toward an event, the system aims tomake this attribution explicit.
It is developed on thebasis of the FactBank corpus (Saur??
and Pustejovsky,2009), containing manual annotations of pairs ofevents and sources along the dimensions of polarity(positive, negative, or underspecified) and certainty(certain, probable, possible, or underspecified.Prabhakaran et al (2010) report experiments withbelief tagging, which in many ways is similar tofactuality detection.
Their starting point is a cor-pus of 10.000 words comprising a variety of genres(newswire text, emails, instructions, etc.)
annotatedfor speaker belief of stated propositions (Diab et al,2009): Propositional heads are tagged as committedbelief (CB), non-committed belief (NCB), or not ap-plicable (NA), meaning no belief is expressed by thespeaker.
To some degree, CB and NCB can be seenas similar to our categories of factuality and non-factuality, respectively.
Applying a one-versus-allSVM classifier by 4-fold cross validation, and usingwide range of both lexical and syntactical features,Prabhakaran et al (2010) report F1-scores of 69.6for CB, 34.1 for NCB, and 64.5 for NA.3 Data Sets and the Notion of FactualityThe data we will be using in the current study istaken from a recently released corpus of ConanDoyle (CD) stories annotated for negation (Moranteand Daelemans, 2012).
The data is annotated withnegation cues, the in-sentence scope of those cues,as well as the negated event, if any.
The cue is theword(s) or affix indicating a negation, The scopethen indicates the maximal extent of that negation,while the event indicates the most basic negated el-ement.
In the annotation guidelines, Morante et al(2011, p. 4) use the term event in a rather generalsense; ?
[i]t can be a process, an action, or a state.
?The guidelines occasionally also refer to the no-tion of negated elements as encompassing ?the mainevent or property actually negated by the negationcue?
(Morante et al, 2011, p. 27).
In the remainderof this paper we will simply take event to conflate allthese senses.Some examples of annotated sentences are shownbelow.
Throughout the paper we will use anglebrackets for marking negation cues, curly brackets29for scopes, and underlines for events.
(1) {There was} ?no?
{answer}.
(2) {I do} ?n?t?
{think that I am a coward} , Watson , but thatsound seemed to freeze my very blood .In the terminology of Saur??
and Pustejovsky (2012),the negation cues are negative polarity particles, andall annotated events in the Conan Doyle data willhave a negative polarity and thereby represent coun-terfacts, i.e., events with negative factuality.
Thisshould not be confused with non-factuality; a coun-terfactual statement is not uncertain.Importantly, however, the Conan Doyle negationcorpus does not explicitly contain any annotation offactuality.
The annotation guidelines specify that?we focus only on annotating information relativeto the negative polarity of an event?
(Morante et al,2011, p. 4).
However, the guidelines also specifythat events should only be annotated for negationsthat (i) have a scope and that (ii) occur in factualstatements (Morante et al, 2011, p. 27).
(As we onlyhave annotations for the sentence-level it is possibleto have a cue without a scope in cases where thecue negates a proposition in a preceding sentence.
)The notion of (non-)factuality assumed in the cur-rent work will reflect the way it is defined in theConan Doyle annotation guidelines.
Morante et al(2011) lists the following types of constructions asnot expressing factual statements (we here show ex-amples from CDDEV for each case):- Imperatives:(3) {Do} ?n?t?
{move} , I beg you , Watson .- Non-factual interrogatives:(4) {You do} ?n?t?
{believe it} , do you , Watson ?- Conditional constructions:(5) If {the law can do} ?nothing?
we must take the risk our-selves .- Modal constructions:(6) {The fault from what I hear may} ?not?
{have been en-tirely on one side} .- Wishes or desires:(7) ?
I hope , ?
said Dr. Mortimer , ?
that {you do} ?not?
{look with suspicious eyes upon everyone [.
.
.
]}- Suppositions or presumptions:(8) I think , Watson , {a brandy and soda would do him} ?no?
{harm} .- Future tense:(9) {The shadow} has departed and {will} ?not?
{return} .Our goal then, will be to correctly identify thesecases in order to separate between factual and non-factual contexts before identifying events.
Note that,while an event, if present, must always be embeddedin the scope, the indicators of factuality are typicallyfound well outside of this scope.
The examples alsoshow that non-factuality here encompasses a widerrange of phenomena than what is traditionally cov-ered in work on identifying hedging or speculation.The examples above illustrate how we can takethe data to implicitly annotate factuality and non-factuality, and we here show how to take advantageof this to train a factuality classifier.
For the exper-iments in this paper we will let positive examplescorrespond to negations that are annotated with botha scope and an event, while negative examples cor-respond to scoped negations with no event.
For ourtraining and development data (CDDEV; more de-tails below), this strategy gives 738 positive exam-ples and 317 negatives, spread over 930 sentences.Our weakly labeled data as defined above comeswith several limitations of course.
The implicit la-beling of factuality will be limited to sentences thatare negated.
We will also not have access to an eventin the cases of non-factuality.
Neither, do we haveany explicit annotation of factuality cue words forthese examples.
All we have are instances of nega-tion that we know to be within some non-delimitedfactual or non-factual context.
For our experimentshere will therefore use the negation cue itself as aplace-holder for the abstract notion of context thatwe are really trying to make predictions about.Table 1 presents some basic statistics for the rele-vant data sets.
For training and development we willuse the negation annotated version of The Hound ofthe Baskerville?s (CDH) and Wisteria Lodge (CDW)(Morante and Daelemans, 2012).
We refer to thecombination of these two data sets as CDDEV.
Forheld-out testing we will use the evaluation datasets prepared for the *SEM 2012 shared task; TheCardboard Box (CDC) and The Red Circle (CDR)(Morante and Blanco, 2012).
We will use CDEVALto refer to the combination of CDC and CDR.
Note30Scoped NegationsData set Sentences Negations Factual Non-factualCDH 3644 984 616 271CDW 787 173 122 46CDDEV 4431 1157 738 317CDC 496 133 87 41CDR 593 131 86 35CDEVAL 1089 264 173 76Table 1: Summary of the Conan Doyle negation data.Note that the total number of negations (column 3) canbe smaller than the number of scoped negations (columns4+5).
The reason is that it is possible to have a cue with-out a scope in cases where the cue negates a propositionin a preceding sentence (which would not be reflectedin these sentence-level annotations).
The numbers in thecolumn ?Factual?
correspond to scoped negations that in-clude an annotated event.that the column Factual correspond to negationswith both a scope and event (i.e., positive examples,in terms of factuality classification), while the Non-factual column correspond to negations with scopeonly and no event (negative examples).4 Factuality DetectionHaving described how we abstractly define our train-ing data above, we can now move on to describeour experiments with training a factuality classifier.It is implemented as a linear binary SVM classi-fier, estimated using the SVMlight toolkit (Joachims,1999).
We start by describing the feature types inSection 4.1 and then present results in Section 4.2.4.1 FeaturesThe feature types we use are mostly variations overbag-of-words (BoW) features.
We include left/rightoriented BoW features centered on the negation cue,recording forms, lemmas, and PoS, and using bothunigrams and bigrams.
These features are extractedboth from the sentence as a whole, and from a localwindow of six tokens to each side of the cue.
Theoptimal window size and the order of n-grams wasdetermined empirically.The reason for including both local and sentence-level BoW features is that we would like to be ableto assign different factuality labels to different in-stances of negation within the same sentence, butat the same time experiments showed sentence-levelfeatures to be very important.Note that, ideally our features should be centeredon the negated event, but since this information isonly available for factual contexts, we instead takethe negation cue as our starting point.
In practice,this seems to provide a good approximation, how-ever, given that the negated event is typically foundin close vicinity of the negation cue.In addition to the BoW type features we have fea-tures explicitly recording the first full-stop punctua-tion following the negation cue (i.e., ?.
?, ?!
?, or ???)
aswell as whether there is an if to the left.
Note that,although this information is already implicit in theBoW features, the model appeared to benefit fromhaving them explicitly coupled with the cue itself.We also experimented with several other featuresthat were not included in the final configuration.These included distance to co-occurring verbs, andmodal verbs in particular.
We also recorded the pres-ence of speculative verbs based on various word listsmanually extracted from the training data.
None ofthese features appeared to contribute information notalready present in the simple BoW features.4.2 ResultsTable 2 provides results for our factuality classifierusing gold cues and gold scopes.
In addition, wealso include results for a baseline approach that sim-ply considers all cases to be factual, i.e., the majorityclass.
Note that, in this case the precision (of fac-tuality labeling) is identical to the accuracy, whichis close to 70% on both the development and held-out set.
The recall for the majority-class baseline isof course at 100%, and the corresponding F1 is ap-proximately 82 on both data sets.
In comparison,our classifier achieves an F1 of 89.92 for the 10-fold cross-validation runs on the development dataand 87.10 on the held-out test data.
The accuracyis 83.98 and 80.72, respectively.
Across both datasets it is clear that the classifier offers substantial im-provements over the baseline.
We do however, ob-serve a drop in performance particularly with respectto precision when moving to the held-out set.Wheninspecting the scores for the two individual sectionsof the held-out set, CDC and CDR, we find that31Data set Model Prec Rec F1 AccCDDEV Baseline 69.95 100.00 82.32 69.95Classifier 84.51 96.07 89.92 83.98CDEVAL Baseline 69.48 100.00 81.99 69.48Classifier 80.60 94.74 87.10 80.72Table 2: Results for factuality detection (using gold nega-tion cues and scopes), reporting 10-fold cross-validationon CDDEV and held-out testing on CDEVAL.the classifier seems to have more difficulties withthe former.
Although recall is roughly the sameacross the two sections (94.25 and 95.24, respec-tively, which is again fairly close to the 10-fold re-call of 96.07), precision suffers a much larger dropon CDC than CDR (78.85 versus 82.47).
On theother hand, it is difficult to reliably assess perfor-mance on the individual test sets, given the limitedamount of data: There are only 128 relevant testcases in CDC and 121 in CDR.
However, there alsoseems to be signs of overfitting, in that an unhealthynumber of the training examples end up as supportvectors in the final model (close to 70%).Note that the F1-scores cited above targets fac-tuality as the positive class label.
However, giventhat this is in fact the majority class it might alsobe instructive to look at F1-scores targeting non-factuality.
(In other words, we will use exactly thesame classifier predictions, but compute our scoresby letting true positives correspond to former truenegatives, false positives to former false negatives,and so on, thereby treating non-factuality as the pos-itive class we are trying to predict.)
Of course,while all accuracy scores will remain unchanged, themajority-class baseline yields an F1 of 0 in this case,as there will be no true positives.
Table 3 lists thenon-factuality scores for the classifier.Given that we are not aware of any other studieson (non-)factuality detection on this data we are notyet able to directly compare our results against thoseof other approaches.
Nonetheless, we believe thestate-of-the-art results cited in Section 2 for relatedtasks such as belief tagging and identifying specu-lation cues give reasons for being optimistic aboutthe results obtained with the simple classifier usedin these initial pilot experiments.Data set Prec Rec F1CDDEV 77.21 66.25 71.31CDEVAL 81.25 50.00 61.91Table 3: Results for non-factuality detection (using goldnegation cues and scopes).
The scores are based on thesame classifier predictions as in Table 2, but treats non-factuality as the positive class.4.3 Error Analysis and Sample Size EffectsIn order to gauge the effect that the size of the train-ing set has on performance we also experimentedwith leaving out portions of the training examplesin our 10-fold cross-validation runs.
Figure 1 plots alearning curve showing how classifier performanceon CDDEV changes as we incrementally includemore training examples.
In order to more clearlybring out the contrasts in performance we here plotresults against non-factuality scores.
We also showthe size of the training set on a logarithmic scale tobetter see whether improvements are constant for n-fold increases of data.
As can be seen, the learningcurve appears to be growing linearly with the incre-ments in larger training samples and it seems safe toassume that the classifier would greatly benefit from0102030405060708010  100F1% of training dataNon-factuality predictionsFigure 1: Learning curve showing the effect on F1 fornon-factuality labeling when withdrawing portions of thetraining partitions (shown on a logarithmic scale) acrossthe 10-fold cross-validation cycles.32additional training data.This impression is strengthened by a manual in-spection of the misclassifications for CDDEV.
Quitea number of errors seem related to a combination ofscarcity and noise in the data.
As a fairly typicalexample, consider the following negation which thesystem incorrectly classifies as factual:(10) ?
I presume , sir , ?
said he at last , ?
that {it was} ?not?
{merely for the purpose of examining my skull that youhave done me the honour to call here last night and againtoday} ?
?One could have hoped that the BoW features record-ing the presence of presume would have tipped thisprediction toward non-factual.
However, while thereare ten occurrences of presume in CDDEV, only threeof these are in contexts that we can actually use aspart of our factuality training data.
Apart from theone in Example (10), these are shown in (11) and(12) below, both of which indicate factual contexts(given the labeling of an event).
We would at leastconsider Example (11) to reveal an error in the goldannotation here, however.
(11) ?
{There is} ?no?
{other claimant} , I presume ?
?
(12) ?
{I presume} ?nothing?
.We also get a few errors for incorrectly labelinga context as factual in cases where there are no ob-vious indicators of non-factuality but the annotationdoes not mark an event, as in:(13) ?
?Nothing?
{of much importance} , Mr. Holmes .For some of the other errors we observed it wouldseem that introducing additional features that aresensitive to the syntactic structure could be bene-ficial.
For example, consider sentence (14) belowwhere we incorrectly classify the first negation asnon-factual;(14) [.
.
. ]
{I had brought it} only to defend myself if attackedand ?not?
{to shoot {an} ?un?
{armed man} who was}running {away} .The error is most likely due to overgeneralizing fromthe presence of if.
By letting the lexical features beextracted from a context constrained by the syntaxtree rather than a simple sliding window, such errorsmight be avoided.For some more optimistic examples, note that thepreviously listed examples of non-factuality in (3)SNPEX{ThereVPVBDwasNPDT?no?NNanswer}..Figure 2: Example of parse tree in the negation data set.through (9) were all selected among cases that werecorrectly predicted by our classifier.In the next section we move on to describe a sys-tem for identifying negated events and assess the im-pact of the factuality classifier on this task (recallfrom Section 3 that only negations occurring in fac-tual statements should be assigned an event).5 Event DetectionTo identify events in factual instances of negation2we employ an automatically-learned discriminativeranking function.
As training data we select all nega-tion scopes that have a single-token3 event, and gen-erate candidates from each token in the scope.
Thecandidate that matches the event is labeled as cor-rect; all others are labeled as incorrect.
For the ex-ample sentence in Figure 2 there are three wordsin the scope and thus three candidates for events:There, was and answer.5.1 FeaturesCandidates are primarily described in terms of pathsin constituent trees.4 In particular, we record thefull path from a candidate token to the constituentwhose projection matches the negation scope (i.e.,the most-specific constituent that subsumes all can-2Note that, although one could of course argue that negatedevents should also be identified for non-factual contexts, that isnot how the task is construed in *SEM 2012 shared task or inthe Conan Doyle data sets.3To simplify the system we assume that all events are singletokens.
It should be noted, however, that 9.85% of events inCDDEV are actually composed of multiple tokens.4Constituent trees from Charniak and Johnson?s Max-Entreranking parser (2005) were provided by the task organizers.33didates).
In Figure 2 this is the S root of thetree; the path that describes the correct candidate isanswer/NN/NP/VP/S.
We also record delexical-ized paths (e.g., ./NN/NP/VP/S) and generalizedpaths (e.g., ./NN//S), as well as bigrams formed ofnodes on the path.
Furthermore, we record some sur-face properties of candidates, namely; lemma, part-of-speech, direction and distance from cue, and po-sition in scope.
Finally, we record the lemma andpart-of-speech of the token immediately precedingthe candidate (development testing showed that in-formation about the token following the candidatewas not beneficial).Based on the features above we learn an SVM-based scoring function using the implementation ofordinal ranking in SVMlight (Joachims, 2002).
Weuse a linear kernel and empirically tune the regu-larization parameter C (governing the trade-off be-tween margin size and errors).5.2 ResultsSimilarly to the learning curve shown above forfactuality detection, Figure 3 plots the F1 of eventdetection on CDDEV when providing increasingamounts of training data and using gold standard in-formation on factuality.
(Note that, except for end-to-end results below, all scores reported in this paperassumes gold negation cues and gold scopes, giventhat we want to isolate the performance of the eventranker and/or factuality classifier.)
We see that theperformance is remarkably strong even at 10% ofthe total data, and increases steadily until around60%, at which point it appears to be leveling off.It is unclear as to whether or not the ranker wouldbenefit from additional data.
We also note differ-ences with respect to the factuality learning curvein Figure 1, both in terms of ?entry performance?and overall trend.
To some degree, there are gen-eral reasons as to why one could expect to see dif-ferences in learning curves for a discriminative rank-ing/regression set-up and a classifier set-up (assum-ing that the class distribution for the latter is unbal-anced, as is typically the case).
For a ranker, ev-ery item provides useful training data, in the sensethat each item provides both positive and negativeexamples (in our case selected from the candidatetokens within a negation scope).
For a classifier, thefew items providing examples of the minority class758085909510  100F1% of training dataEvent predictionsFigure 3: Learning curve showing the effect on F1 forevent detection when using gold factuality and withdraw-ing portions of the training partitions (shown on a loga-rithmic scale) across the 10-fold cross-validation cycles.will typically be the most valuable and it will there-fore easily be more sensitive to having the trainingsample restrained.
Even so, it seems clear that thefactuality detection component and event detectioncomponent belong to different ends of the spectrumin terms of sensitivity to sample size.Table 4 details the results of using the final rank-ing model to predict negated events.
For a compar-ative baseline, we implemented a basic ranker thatuses only the candidate lemma as a single feature.This baseline achieves an F1 of 73.90 (P=74.01,R=73.80) on CDDEV when using factuality informa-tion inferred from the gold-standard (and testing by10-fold cross-validation).
For comparison, the fullranking model achieves an F1 of 90.42 (P=90.75,R=90.10) on the same data set, as seen in Table 4.Of course, the results for event detection us-ing gold-standard factuality also provides the up-per bound for what we can achieve using systempredicted factuality, i.e., applying the classifier de-scribed in Section 4.
In order to assess the im-pact of the factuality classifier we also include re-sults for event detection using the majority-classbaseline, which means simply assuming that all in-stances of negations are factual.
Table 4 lists re-sults for event detection using system predicted fac-tuality, compared to results using baseline and gold-standard factuality.
We find that the factuality clas-sifier greatly improves precision of the event de-34Data set Factuality Prec Rec F1CDDEVBaseline 62.24 90.10 73.62Classifier (10-fold) 78.48 82.98 80.67Gold 90.75 90.10 90.42CDEVALBaseline 58.26 84.94 69.11Classifier (Held-out) 68.72 80.24 74.03Gold 84.94 84.94 84.94Table 4: Results for event detection using various meth-ods for factuality detection.tection.
As can be expected, however, this comeswith a cost in terms of recall.
In both 10-foldcross-validation on CDDEV and held-out testing onCDEVAL we find large improvements in F1, corre-sponding to error reductions of 26.73% and 15.93%respectively.
As expected given the results discussedin Section 4, the improvement is slightly less pro-nounced for the held-out test results than the 10-foldcross-validated development results.
Although thefactuality classifier improves substantially over thebaseline, it is also clear that a large gap remainstoward the ?upper bound?
results of using gold-standard factuality.
We take the results of the pilotexperiments described in this paper as a proof-of-concept for using the CD data for training a factual-ity classifier, and at the same time have high expec-tations that future experimentation with additional(syntactically oriented) feature types should be ableto further advance performance considerably.Building on the system presented in Velldal et al(2012), the initial *SEM 2012 shared task submis-sion of Read et al (2012) also included an SVMnegation cue classifier (including support for mor-phological cues) along with an SVM-based rank-ing model over syntactic constituents for scope res-olution.
Coupled with the components for factual-ity and event detection described above, the end-to-end result for this system on CDEVAL for identify-ing negated events is F1=67.02 (P=60.58, R=75.00),making it the top-ranked submission in the sharedtask.6 Conclusions and Future DirectionsThis paper has demonstrated that a classifier fordiscriminating between factuality and non-factualitycan be trained by taking advantage of implicit in-formation on factuality found in the negation an-notations of the Conan Doyle corpus (Morante andDaelemans, 2012).
Even though the pilot experi-ments described in this paper use just simple lex-ical features, the factuality classifier provides sub-stantial improvements over the majority-class base-line.
We also present a system for detecting negatedevents by learning an SVM-based discriminativeranking function over candidate tokens within thenegation scope.
We show that the factuality classi-fier proves very useful for improving the precisionof event detection.
In order to isolate the perfor-mance of the event ranker and factuality classifier wehave focused on results for gold negation cues andscopes in this paper, although end-to-end results forthe full system presented by Read et al (2012) arealso included.
The system obtained the best resultsfor identifying negative factual events in the 2012*SEM shared task.It is worth noting that there is nothing inherentlynegation specific about our factuality detection ap-proach per se, save for how the training data happensto be extracted in the current study.
One reason forusing the implicit factuality information in the Co-nan Doyle negation corpus is the advantage of get-ting in-domain data, and this also allowed us to staywithin the confines of the closed track for the *SEMshared task.
For future experiments, however, wewould also like to test cross-domain portability byboth training and testing the factuality classifier us-ing other annotated data sets such as FactBank, andalso add features that incorporate predictions fromspeculation cue classifiers trained on BioScope.AcknowledgmentsWe want to thank Roser Morante and EduardoBlanco for their effort in organizing the *SEM 2012shared task and providing the annotations.
We alsowant to thank our colleagues at the University ofOslo (UiO), in particular Lilja ?vrelid and StephanOepen who contributed to the shared task submis-sion.
Large-scale experimentation was facilitated bythe TITAN HPC cluster at UiO.
We also thank theanonymous reviewers for their valuable commentsand suggestions.35ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of the Forty-Third Annual Meetingof the Association for Computational Linguistics, AnnArbor, MI.Mona T. Diab, Lori S. Levin, Teruko Mitamura, OwenRambow, Vinodkumar Prabhakaran, and Weiwei Guo.2009.
Committed belief annotation and tagging.
InProceedings of the Third Linguistic Annotation Work-shop (LAW 2009), pages 68?73, Singapore.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to detect hedges and theirscope in natural language text.
In Proceedings ofthe 14th Conference on Natural Language Learning,pages 1?12, Uppsala.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Bernhard Scho?lkopf, Christo-pher J. C. Burges, and Alexander J. Smola, editors,Advances in Kernel Methods: Support Vector Learn-ing, pages 41?56.
MIT Press, Cambridge, MA.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the EighthACM International Conference on Knowledge Discov-ery and Data Mining, Alberta.Halil Kilicoglu and Sabine Bergler.
2011.
Adapting ageneral semantic interpretation approach to biologi-cal event extraction.
In Proceedings of the BioNLPShared Task 2011, pages 173?182, Portland, OR.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008.Corpus annotation for mining biomedical events fromliterature.
BMC Bioinformatics, 9(10).Roser Morante and Eduardo Blanco.
2012.
*SEM 2012shared task: Resolving the scope and focus of nega-tion.
In Proceedings of the First Joint Conference onLexical and Computational Semantics, Montreal.Roser Morante and Walter Daelemans.
2012.ConanDoyle-neg: Annotation of negation in ConanDoyle stories.
In Proceedings of the Eighth Interna-tional Conference on Language Resources and Evalu-ation, Istanbul.Roser Morante and Caroline Sporleder.
2012.
Modal-ity and negation: An introduction to the special issue.Computational Linguistics, 38(2):1?38.Roser Morante, Vincent Van Asch, and Walter Daele-mans.
2010.
Memory-based resolution of in-sentencescope of hedge cues.
In Proceedings of the 14th Con-ference on Natural Language Learning, pages 40?47,Uppsala.Roser Morante, Sarah Schrauwen, and Walter Daele-mans.
2011.
Annotation of negation cues and theirscope: Guidelines v1.0.
Technical report, Univer-sity of Antwerp.
CLIPS: Computational Linguistics& Psycholinguistics technical report series.Vinodkumar Prabhakaran, Owen Rambow, and Mona T.Diab.
2010.
Automatic committed belief tagging.
InProceedings of the 23rd International Conference onComputational Linguistics, pages 1014?1022, Beijing.Jonathon Read, Erik Velldal, Lilja ?vrelid, and StephanOepen.
2012.
UiO1: Constituent-based discrimina-tive ranking for negation resolution.
In Proceedingsof the First Joint Conference on Lexical and Computa-tional Semantics, Montreal.
Submission under review.Roser Saur??
and James Pustejovsky.
2009.
Factbank:a corpus annotated with event factuality.
LanguageResources and Evaluation, 43(3):227?268.Roser Saur??
and James Pustejovsky.
2012.
Are you surethat this happened?
assessing the factuality degree ofevents in text.
Computational Linguistics, 38(2).Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,and Shixi Fan.
2010.
A cascade method for detect-ing hedges and their scope in natural language text.
InProceedings of the 14th Conference on Natural Lan-guage Learning, pages 13?17, Uppsala.Erik Velldal, Lilja ?vrelid, Jonathon Read, and StephanOepen.
2012.
Speculation and negation: Rules,rankers and the role of syntax.
Computational Lin-guistics, 38(2).Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008.
The Bio-Scope corpus: Biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9 (Suppl.
11).36
