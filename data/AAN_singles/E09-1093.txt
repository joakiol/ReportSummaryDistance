Proceedings of the 12th Conference of the European Chapter of the ACL, pages 817?825,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsLearning Efficient ParsingGertjan van NoordUniversity of GroningenG.J.M.van.noord@rug.nlAbstractA corpus-based technique is described toimprove the efficiency of wide-coveragehigh-accuracy parsers.
By keeping trackof the derivation steps which lead to thebest parse for a very large collection ofsentences, the parser learns which parsesteps can be filtered without significantloss in parsing accuracy, but with an im-portant increase in parsing efficiency.
Aninteresting characteristic of our approachis that it is self-learning, in the sense thatit uses unannotated corpora.1 IntroductionWe consider wide-coverage high-accuracy pars-ing systems such as Alpino, a parser for Dutchwhich contains a grammar based on HPSG anda maximum entropy disambiguation componenttrained on a treebank.
Even if such parsing sys-tems now obtain satisfactory accuracy for a vari-ety of text types, a drawback concerns the compu-tational properties of such parsers: they typicallyrequire lots of memory and are often very slow forlonger and very ambiguous sentences.We present a very simple, fairly general,corpus-based method to improve upon the prac-tical efficiency of such parsers.
We use the accu-rate, slow, parser to parse many (unannotated) in-put sentences.
For each sentence, we keep track ofsequences of derivation steps that were required tofind the best parse of that sentence (i.e., the parsethat obtained the best score, highest probability,according to the parser itself).Given a large set of successful derivation stepsequences, we experimented with a variety ofsimple heuristics to filter unpromising derivationsteps.
A heuristic that works remarkably wellsimply states that for a new input sentence, theparser can only consider derivation step sequencesin which any sub-sequence of length N has beenobserved at least once in the training data.
Exper-imental results are provided for various heuristicsand amounts of training data.It is hard to compare fast, accurate, parsers withslow, slightly more accurate parsers.
In section 3we propose both an on-line and an off-line appli-cation scenario, introducing a time-out per sen-tence, which leads to metrics for choosing be-tween parser variants.In the experimental part we show that, in an on-line scenario, the most successful heuristic leadsto a parser that is more accurate than the baselinesystem, except for unrealistic time-outs per sen-tence of more than 15 minutes.
Furthermore, weshow that, in an off-line scenario, the most suc-cessful heuristic leads to a parser that is more thanfour times faster than the base-line variant with thesame accuracy.2 Background: the Alpino parser forDutchThe experiments are performed using the Alpinoparser for Dutch.
The Alpino system is a linguis-tically motivated, wide-coverage grammar andparser for Dutch in the tradition of HPSG.
It con-sists of about 800 grammar rules and a large lexi-con of over 300,000 lexemes and various rules torecognize special constructs such as named enti-ties, temporal expressions, etc.
Heuristics havebeen implemented to deal with unknown wordsand word sequences.
Based on the categories as-signed to words, and the set of grammar rulescompiled from the HPSG grammar, a left-cornerparser finds the set of all parses, and stores this setcompactly in a packed parse forest.
In order to se-lect the best parse from the parse forest, a best-firstsearch algorithm is applied.
The algorithm con-sults a Maximum Entropy disambiguation modelto judge the quality of (partial) parses.Although Alpino is not a dependency grammar817in the traditional sense, dependency structures aregenerated by the lexicon and grammar rules as thevalue of a dedicated attribute.
The dependencystructures are based on CGN (Corpus GesprokenNederlands, Corpus of Spoken Dutch) (Hoekstraet al, 2003), D-Coi and LASSY (van Noord et al,2006).3 Methodology: balancing efficiency andaccuracy3.1 On-line and off-line parsing scenariosWe focus on the speed of parsing, ignoring othercomputational properties such as memory usage.Problems with respect to parsing are twofold: onthe one hand, parsing simply is too slow for manyinput sentences.
On the other hand, the rela-tion between input sentence and expected speedof parsing is typically unknown.
For simple pars-ing systems based on finite-state, context-free ormildly context-sensitive grammars, it is possibleto establish an upper-bound of required CPU-timebased on the length of an input sentence.
For thevery powerful constraint-based formalisms con-sidered here, such upper-bounds are not avail-able.
In practice, shorter sentences typically canbe parsed fairly quickly, whereas longer sentencessometimes can take a very very long time indeed.As a consequence, measures such as number ofwords parsed per minute, or mean parsing time persentence are somewhat meaningless.
We thereforeintroduce two slightly different scenarios whichinclude a time-out per sentence.On-line scenario.
In some applications, a parseris applied on-line: an actual user is waiting for theresponse of the system, and if the parser requiredminutes of CPU-time, the application would notbe successful.
In such a scenario, we assume thatit is possible to determine a maximum amount ofCPU-time (a time-out) per sentence, depending onother factors such as speed of the other systemcomponents, expected patience of users, etc.
Ifthe parser does not finish before the time-out, it isassumed to have not produced anything.
In depen-dency parsing, the parser produces the empty setof dependencies in such cases, and hence such anevent has an important negative effect on the ac-curacy of the system.
By studying the relation be-tween different time-outs and accuracy, it is possi-ble to choose the most effective parser variant fora particular application.Off-line scenario.
For other applications, anoff-line parsing scenario might be more appropri-ate.
For instance, if we build a question answeringsystem for a medical encyclopedia, and we wish toparse all sentences of that encyclopedia once andfor all, then we are not interested in the amount ofCPU-time the parser spends on a single sentence,but we want to know how much time it will cost toparse everything.In such a scenario, it often still is very useful toset a time-out for each sentence, but in this case thetime-out can be expected to be (much) higher thanin the on-line scenario.
In this scenario, we pro-pose to study the relation between mean CPU-timeand accuracy ?
for various settings of the time-out parameter.
This allows us to determine, forinstance, the mean CPU-time requirements for agiven target accuracy level?3.2 Accuracy: comparing sets ofdependenciesLet Dip be the number of dependencies producedby the parser for sentence i, Dig is the number ofdependencies in the treebank parse, and Dio is thenumber of correct dependencies produced by theparser.
If no superscript is used, we aggregate overall sentences of the test set, i.e.,:Dp =?iDip Do =?iDio Dg =?iDigWe define precision (P = Do/Dp), (R =Do/Dg) and f-score: 2P ?
R/(P + R).An alternative similarity score is based on theobservation that for a given sentence of n words,a parser would be expected to return (about) n de-pendencies.
In such cases, we can simply use thepercentage of correct dependencies as a measureof accuracy.
To allow for some discrepancies be-tween the number of expected and returned depen-dencies, we divide by the maximum (per sentence)of both.
This leads to the following definition ofnamed dependency accuracy.Acc =Do?i max(Dig, Dip)If time-outs are introduced, the difference be-tween f-score and accuracy becomes important.Consider the example in table 1.
Here, the parserproduces reasonable results for the first three,short, sentences, but for the final, long, sentenceno result is produced because of a time-out.818i Dio Dip Dig prec rec f-sc Acc1 8 10 11 80 73 76 732 8 11 10 76 76 76 733 8 9 9 80 80 80 774 0 0 30 80 40 53 39Table 1: Hypothetical result of parser on a test setof four sentences.
The columns labeled precision,recall, f-score and accuracy represent aggregatesover sentences 1 .
.
.
i.The precision, recall and f-score after the firstthree sentences is 80%.
After the ?
much longer?
fourth sentence, recall drops considerably, butprecision remains the same.
As a consequence,the f-score is quite a bit higher than 40%: it is over53%.
The accuracy score after three sentences is77%.
Including the fourth sentence leads to a dropin accuracy to 39%.As this example illustrates, the f-score metric isless sensitive to parse failures than the accuracyscore.
Also, it appears that the accuracy score isa much better characterization of the success ofthis parser: after all, the parser only got 24 cor-rect dependencies out of 60 expected dependen-cies.
The f-score measure, on the other hand, caneasily be misunderstood to suggest that the parserdoes a good job for more than 50%.4 Learning Efficient ParsingIn this section a method is defined for filteringderivation step sequences, based on previous expe-rience of the parser.
In a training phase, the parseris fed with thousands of sentences.
For each sen-tence it finds the best parse, and it stores the rel-evant sequences of derivation steps, that were re-quired to find that best parse.
After the trainingphase, the parser filters those sequences of deriva-tion steps that are unlikely to be useful.
By fil-tering out unlikely derivation step sequences, effi-ciency is expected to improve.
Since certain parsesnow become impossible, a drop in accuracy is ex-pected as well.Although the idea of filtering derivation stepsequences based on previous experience is fairlygeneral, we define the method in more detail withrespect to an actual parsing algorithm: the left-corner parser along the lines of Matsumoto et al(1983), Pereira and Shieber (1987, section 6.5)and van Noord (1997).4.1 Left-corner parsingA left-corner parser is a bottom-up parser withtop-down guidance, which is most easily ex-plained as a non-deterministic search procedure.A specification of the left-corner algorithm canbe provided in DCG as in figure 2 (Pereira andShieber, 1987, section 6.5), where the filter/2goals should be ignored for the moment.
Here,we assume that dictionary look-up is performedby the word/3 predicate, with the first argumenta given word, and the second argument its cate-gory; and that rules are accessible via the predi-cate rule/3, where the first argument representsthe mother category, and the second argument isthe possibly empty list of daughter categories.
Thethird argument of both the word/3 and rule/3predicates are identifiers we need later.In order to analyze a given sentence as an in-stance of the top category, we look up the firstword of the string, and show that this lexical cat-egory is a left-corner of the goal category.
Toshow that a given category is a left-corner of agiven goal category, a rule is selected.
The left-most daughter node of that rule is identified withthe left-corner.
The other daughters of the rule areparsed recursively.
If this succeeds, it remains toshow that the mother node of the rule is a left-corner of the goal category.
The recursion stopsif a left-corner category can be identified with thegoal category.This simple algorithm is improved and extendedin a variety of ways, as in Matsumoto et al (1983)and van Noord (1997), to make it efficient andpractical.
The extensions include a memoizationof the parse/1 predicate and the construction of ashared parse forest (a compact representation ofall parses).4.2 Left-corner splinesFor the left-corner parser, the derivation stepsequences that are of interested are left-cornersplines.
Such a spline consists of a goal category,and the rules and lexical entries which were usedin the left-corner, in the order from the top to thebottom.A spline consists of a goal category, followedby a sequence of derivation step names.
A deriva-tion step name is typically a rule identifier, but itcan also be a lexical type, indicating the lexicalcategory of a word that is the left-corner.
A spe-cial derivation step name is the reserved symbol819toptop catmax xp(np)np det ndet(de)denn n relnoun(de,both,sg)wijnrelrel arg(np)rel pron(de,no obl)dievpvp vpxvpx vprojvp arg v(np)np pnpn(sg,PER)Elvisvprojvproj vcvc vverb(past(sg),transitive)dronk(top,[finish,top_cat,max_xp(np),np_det_n,det(de)]).(n,[finish,n_n_rel,noun(de,both,sg)]).(rel,[finish,rel_arg(np),rel_pron(de,no_obl)]).(vp,[finish,vp_vpx,vpx_vproj,vp_arg_v(np),np_pn,pn(sg,PER),]).
(vproj,[finish,vproj_vc,vc_v,verb(past(sg),transitive)]).Figure 1: Annotated derivation tree of the sentenceDe wijn die Elvis dronk (The wine which Elvisdrank).finish which is used to indicate that the cur-rent category is identified with the goal category(and no further rules are applied).
A spline is writ-ten (g, rn .
.
.
r1) for goal category g and deriva-tion step names r1 .
.
.
rn.
(g, ri .
.
.
r1) is a partialspline of (g, rn .
.
.
ri .
.
.
r1).Consider the annotated derivation tree for thesentence De wijn die Elvis dronk (The wine whichElvis drank) in figure 1.
Boxed leaf nodes con-tain the lexical category as well as the corre-sponding word.
Boxed non-leaf nodes contain thegoal category (italic) and the rule-name.
Non-boxed non-leaf nodes only list the rule name.
Thefirst left-corner spline consists of the goal cate-gory top and the identifiers finish, top cat,max xp(np), np det n, and the lexical typedet(de).
All five left-corner splines of the ex-ample are listed at the bottom of figure 1.Left-corner splines of best parses of a large setof sentences constitute the training data for theparse(Phrase) -->leaf(SubPhrase,Id),{ filter(Phrase,[Id]) },lc(SubPhrase,Phrase,[Id]).leaf(Cat,Id) -->[Word], { word(Word,Cat,Id) }.leaf(Cat,Id) --> { rule(Cat,[],Id) }.lc(Phrase,Phrase,Spline) -->{ filter(Phrase,[finish|Spline]) }.lc(SubPhrase,SuperPhrase,Spline) -->rule(Phrase,[SubPhrase|Rest],Id),{ filter(SuperPhrase,[Id|Spline]) },parse_rest(Rest),lc(Phrase,SuperPhrase,[Id|Spline]).Figure 2: DCG Specification of a non-deterministic left-corner parser, including splinefiltering.techniques we develop to learn to parse new sen-tences more efficiently.4.3 Filtering left-corner splinesThe left-corner parser builds left-corner splinesone step at the time.
For a given goal, it first se-lects a potential left-corner, and then continues ap-plying rules from the bottom to the top until theleft-corner is identified with the goal category.
Atevery step where the algorithm attempts to extenda left-corner spline, we now introduce a filter.
Thepurpose of this filter is to consider only those par-tial left-corner splines that look promising - basedon the parser?s previous experience on the train-ing data.
The specification of the left-corner parsergiven in figure 2 includes calls to this filter.The purpose of the filter is, that at any timethe parser considers extending a left-corner spline(g, ri?1 .
.
.
r1) to (g, ri .
.
.
r1), such an extensiononly is allowed in promising cases.
Obviously,there are many ways such a filter could be defined.We identify the following dimensions:Context size.
A filter for (g, ri .
.
.
r1) will typ-ically ignore at least some of the derivation stepnames from the context.
We experiment with fil-ters which take into consideration g, ri, ri?1 (bi-gram filter); g, ri, ri?1, ri?2 (trigram filter); andg, ri, ri?1, ri?2, ri?3 (fourgram filter).
A furtherfilter, labeled prefix filter, takes the full history intoaccount: g, ri .
.
.
r1.
The prefix filter thus ensuresthat the parser only considers left-corner splinesthat are partial splines of splines observed in thetraining data.820Required evidence.
For the various filters, whatkind of evidence from the training data do we re-quire in order for the filter to accept this particularderivation step?
In initial experiments, we usedrelative frequencies.
For instance, the trigram fil-ter would allow any tuple g, ri?2, ri?1, ri for someconstant threshold ?
, provided:C(g, .
.
.
riri?1ri?2 .
.
.
)C(g, .
.
.
ri?1ri?2 .
.
.
)> ?However, we found that filters are more effective(and require much less space ?
see below), whichsimply require that every step has been observedoften enough in the training data:C(g, .
.
.
riri?1ri?2 .
.
.)
> ?In particular, the case where ?
= 0 gave surpris-ingly good results.4.4 Comparison with link tableThe filter we developed is reminiscent of the linkpredicate of (Pereira and Shieber, 1987).
An im-portant difference with the filter developed hereis that the link predicate removes derivation stepswhich cannot lead to a successful parse (by an off-line global analysis of the grammar), whereas wefilter out derivation steps which can lead to a fullparse, but which are not expected to lead to a bestparse.
In our implementation, a variant of the linkpredicate is used as well.4.5 Implementation detailThe definition of the filter predicate depends onour choices with respect to the dimensions identi-fied above.
For instance, if we chose the trigramfilter as our context size, then the training data canbe preprocessed in order to store all goal-trigram-pairs with frequency above the threshold ?
.
Dur-ing parsing, if the filter is given the partial spline(g, riri?1ri?2 .
.
.
), then a simple table look-up forthe tuple (g, ri?2ri?1ri) is sufficient (this suffices,because each of the preceding trigrams will havebeen checked earlier).
In general, the filter pred-icate needs access to a table containing a pair ofgoal category and context, where the context con-sists of sequences of derivation step names.
Thetable contains items for those pairs that occurredwith frequency > ?
in the training data.To access such tables efficiently, an obviouschoice is to use a hash table.
The additional stor-age requirements for such a hash table are consid-erable.
For instance, for the prefix filter four yearsof newspaper text lead to a table with 941,723 en-tries - stored as text the data takes 103Mb.
To savespace, we experimented with a set-up in whichonly the hash keys are stored, but the original in-formation that the hash key was computed from, isremoved.
During parsing, in order to check that agiven tuple is allowable, we compute its hash key,and check if the hash key is in the table.
If so,the computation continues.
The drawback of thismethod is, that in the case a hash collision wouldhave occurred in an ordinary hash table, we nowsimply assume that the input tuple was in the ta-ble.
In other words: the filter is potentially toopermissive in such cases.
In actual practice, we didnot observe a difference with respect to accuracyor CPU-time requirements, but the storage costsdropped considerably.5 Experimental ResultsSome of the experiments have been performedwith the Alpino Treebank.
The Alpino Treebank(van der Beek et al, 2002) consists of manu-ally verified dependency structures for the cdbl(newspaper) part of the Eindhoven corpus (denBoogaart, 1975).
The treebank contains 7137 sen-tences.
Average sentence length is about 20 to-kens.Some further experiments are performed on thebasis of the D-Coi corpus (van Noord et al, 2006).From this corpus, we used the manually veri-fied syntactic annotations of the P-P-H and P-P-L parts.
The P-P-H part consists of over 2200sentences from the Dutch daily newspaper Trouwfrom 2001.
Average sentence length is about 16.5tokens.
The P-P-L part contains 1115 sentencestaken from information brochures of Dutch Min-istries.
Average sentence length is about 18.5 to-kens.For training data, we used newspaper text fromthe TwNC (Twente Newspaper) corpus (Ordelmanet al, 2007).
We used Volkskrant 2001, NRC2000, Algemeen Dagblad 1999.
In addition, weused Volkskrant 1997 newspaper data extractedfrom the Volkskrant 1997 CDROM.5.1 Results on Alpino TreebankFigure 3 presents results obtained on the AlpinoTreebank.
In the graphs, the various filters arecompared with the baseline variant of the parser.Each of the filters outperforms the default modelfor all given time-out values.
In fact, the base-8211 5 10 50 50020406080timeout (sec)accuracy (%CA)bigramtrigramfourgramprefixbaseline5 10 15 20 2520406080mean cputime (sec)accuracy (%CA)bigramtrigramfourgramprefixbaselineFigure 3: Accuracy versus time-out (on-line scenario), and accuracy versus mean CPU-time (off-linescenario) for various time-outs.
The graphs compare the default setting of Alpino with the effect of thevarious filters based on all available training data.
Evaluation on the Alpino treebank.line parser improves upon the prefix filter only forunrealistic time-outs larger than fifteen minutes ofCPU-time.
The difference in accuracy for a giventime-out value can be considerable: as much as12% for time-outs around 30 seconds of CPU-time.If we focus on mean CPU-time (off-line sce-nario), differences are even more pronounced.Without the filter, an accuracy of about 63% is ob-tained for a mean CPU-time of 6 seconds.
The pre-fix filtering method obtains accuracy of more than86% for the same mean CPU-time.
For that levelof accuracy, the base-line model requires a meanCPU-time of about 25 seconds.
In other words, forthe same level of accuracy, the prefix filter leads toa parser that is more than four times faster.5.2 Effect of the amount of training dataIn the first two graphs of figure 4 we observe theeffect of the amount of training data.
As can be ex-pected, increasing the amount of data increases theaccuracy, and decreases efficiency (because morederivation steps have been observed, hence fewerderivations are filtered out).
Generally, modelsthat take into account larger parts of the history re-quire more data to obtain good accuracy, but theyare also faster.
For each of the variants, addingmore training data after about 40 million wordsdoes not lead to much further improvement; thelittle improvement that is observed, is balanced bya slight increase in parse times too.It is interesting to note that the accuracy of someof the filters improves slightly upon the baselineparser (without any filtering).
This can be ex-plained by the fact that the Alpino parser includesa best-first beam search to select the best parsefrom the parse forest.
Apparently, in some casesthe filter throws away candidate parses whichwould otherwise confuse this heuristic best searchprocedure.5.3 Experiment with D-Coi dataIn this section, we confirm the experimental re-sults obtained on the Alpino Treebank by perform-ing similar experiments on the D-Coi data.
Thepurpose of this confirmation is twofold.
On theone hand, the Alpino Treebank might not be areliable test set for the Alpino parser, because ithas been used quite intensively during the devel-opment of various components of the system.
Onthe other hand, we might regard the experiments inthe previous section as development experimentsfrom which we learn the best parameters of theapproach.
The real evaluation of the technique isnow performed using only the best method foundon the development set, which is the prefix filterwith ?
= 0.We performed experiments with two parts of theD-Coi corpus.
The first data set, P-P-H, containsnewspaper data, and is therefore comparable both822with the Alpino Treebank, and more importantly,with the training data that we used to develop thefilters.
In order to check if the success of the fil-tering methods requires that training data and testdata need to be taken from similar texts, we alsoprovide experimental results on a test set consist-ing of different material: the P-P-L part of theD-Coi corpus, which contains text extracted frominformation brochures published by Dutch Min-istries.The third and fourth graphs in figure 4 provideresults obtained on the P-P-H corpus.
The in-creased efficiency of the prefix filter is slightly lesspronounced.
This may be due to the smaller meansentence length of this data set.
Still, the prefix fil-tering method performs much better for a large va-riety of time-outs.
Only for very high, unrealistic,time-outs, the baseline parser obtains better accu-racy.
The same general trend is observed in theP-P-L data-set.
From these results we tentativelyconclude that the proposed technique is applicableacross text types and domains.6 DiscussionOne may wonder how the technique introduced inthis paper relates to techniques in which the dis-ambiguation model is used directly during parsingto eliminate unlikely partial parses.
An examplein the context of wide coverage unification-basedparsing is the beam thresholding technique em-ployed in the Enju HPSG parser for English (Tsu-ruoka et al, 2004; Ninomiya et al, 2005).In a beam-search parser, unlikely partial analy-ses are constructed, and then - based on the proba-bility assigned to these partial analyses - removedfrom further consideration.
One potential advan-tage of the use of our filters may be, that many ofthese partial analyses will not even be constructedin the first place, and therefore no time is spent onthese alternatives at all.We have not performed a detailed comparison,because the statistical model employed in Alpinocontains some features which refer to arbitrarylarge parts of a parse.
Such non-local features arenot allowed in the Enju approach.A parsing system may also combine both typesof techniques.
In that case there is room forfurther experimentation.
For instance, duringthe learning phase, it may be beneficial to allowfor a wider beam, to obtain more reliable filters.During testing, the beam can perhaps be smallerthan usual, since the filters already rule out manyof the competing parses.The idea that corpora can be used to improveparsing efficiency was an important ingredient ofa technique that was called grammar specializa-tion.
An overview of grammar specialization tech-niques is given in (Sima?an, 1999).
For instance,Rayner and Carter (1996) use explanation-basedlearning to specialize a given general grammar to aspecific domain.
They report important efficiencygains (the parser is about three times faster), cou-pled with a mild reduction of coverage (5% loss).In contrast to our approach in which no manualannotation is required, Rayner and Carter (1996)report that for each sentence in the training data,the best parse was selected manually from the setof parses generated by the parser.
For the exper-iments described in the paper, this constituted aneffort of two and a half person-months.
As a con-sequence, they use only 15.000 training examples(taken from ATIS, so presumably relatively shortsentences).
In our experiments, we used up to 4million sentences.A further difference is related to the pruningstrategies.
Our pruning strategies are extremelysimple.
The cutting criteria employed in grammarspecialization either require carefully manuallytuning, or require more complicated statisticaltechniques (Samuelsson, 1994); automaticallyderived cutting criteria, however, perform consid-erably worse.A possible improvement of our approach con-sists of predicting whether for a given input sen-tence the filter should be used, or whether the sen-tence appears to be ?easy?
enough to allow for afull parse.
For instance, one may chose to usethe filter only for sentences of a given minimumlength.
Initial experiments indicate that such asetup may improve somewhat over the results pre-sented here.AcknowledgmentsThis research was carried out in part in thecontext of the STEVIN programme which isfunded by the Dutch and Flemish governments(http://taalunieversum.org/taal/technologie/stevin/).82320 40 60 8085868788Million wordsAccuracy (%CA)bigramtrigramfourgramprefixno filter20 40 60 80051015Million wordsMeancputime (sec)bigramtrigramfourgramprefixno filter1 5 10 50 50020406080timeout (sec)accuracy (%CA)prefix filterdefault2 4 6 8 1020406080mean cputime (sec)accuracy (%CA)prefix filterdefault1 5 10 50 50020406080timeout (sec)accuracy (%CA)prefix filterdefault5 10 1520406080mean cputime (sec)accuracy (%CA)prefix filterdefaultFigure 4: The first two graphs present accuracy (left) and mean CPU-time (right) as a function of theamount of training data used.
Evaluation on 10% of the Alpino Treebank.
The third and fourth graphpresent accuracy versus time-out, and accuracy versus mean CPU-time for various time-outs.
The graphcompares the baseline system with the parser which uses the prefix filter based on all available trainingdata.
Evaluation on the D-Coi P-P-H 1-109 data-set (newspaper text).
The two last graphs are similar,based on the D-Coi P-P-L data-set (brochures).824ReferencesP.
C. Uit den Boogaart.
1975.
Woordfrequentiesin geschreven en gesproken Nederlands.
Oost-hoek, Scheltema & Holkema, Utrecht.
WerkgroepFrequentie-onderzoek van het Nederlands.Heleen Hoekstra, Michael Moortgat, Bram Renmans,Machteld Schouppe, Ineke Schuurman, and Tonvan der Wouden, 2003.
CGN Syntactische Anno-tatie, December.Y.
Matsumoto, H. Tanaka, H. Hirakawa, H. Miyoshi,and H. Yasukawa.
1983.
BUP: a bottom up parserembedded in Prolog.
New Generation Computing,1(2).Takashi Ninomiya, Yoshimasa Tsuruoka, YusukeMiyao, and Jun?ichi Tsujii.
2005.
Efficacy of beamthresholding, unification filtering and hybrid pars-ing.
In Proceedings of the International Workshopon Parsing Technologies (IWPT).Roeland Ordelman, Franciska de Jong, Arjan van Hes-sen, and Hendri Hondorp.
2007.
Twnc: a mul-tifaceted Dutch news corpus.
ELRA Newsletter,12(3/4):4?7.Fernando C. N. Pereira and Stuart M. Shieber.
1987.Prolog and Natural Language Analysis.
Center forthe Study of Language and Information Stanford.Manny Rayner and David Carter.
1996.
Fast pars-ing using pruning and grammar specialization.
In34th Annual Meeting of the Association for Compu-tational Linguistics, Santa Cruz.Christer Samuelsson.
1994.
Grammar specializationthrough entropy thresholds.
In 32th Annual Meet-ing of the Association for Computational Linguis-tics, New Mexico.
ACL.Khalil Sima?an.
1999.
Learning Efficient Disambigua-tion.
Ph.D. thesis, University of Utrecht.Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Tsu-jii.
2004.
Towards efficient probabilistic hpsg pars-ing: integrating semantic and syntactic preferenceto guide the parsing.
In Beyond Shallow Analyses -Formalisms and statistical modeling for deep analy-ses, Hainan China.
IJCNLP.Leonoor van der Beek, Gosse Bouma, Robert Malouf,and Gertjan van Noord.
2002.
The Alpino depen-dency treebank.
In Computational Linguistics in theNetherlands.Gertjan van Noord, Ineke Schuurman, and VincentVandeghinste.
2006.
Syntactic annotation of largecorpora in STEVIN.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (LREC), Genoa, Italy.Gertjan van Noord.
1997.
An efficient implementationof the head corner parser.
Computational Linguis-tics, 23(3):425?456.
cmp-lg/9701004.825
