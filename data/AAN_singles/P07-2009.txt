Proceedings of the ACL 2007 Demo and Poster Sessions, pages 33?36,Prague, June 2007. c?2007 Association for Computational LinguisticsLinguistically Motivated Large-Scale NLP with C&C and BoxerJames R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australiajames@it.usyd.edu.auStephen ClarkComputing LaboratoryOxford UniversityWolfson Building, Parks RoadOxford, OX1 3QD, UKstephen.clark@comlab.ox.ac.ukJohan BosDipartimento di InformaticaUniversita` di Roma ?La Sapienza?via Salaria 11300198 Roma, Italybos@di.uniroma1.it1 IntroductionThe statistical modelling of language, together withadvances in wide-coverage grammar development,have led to high levels of robustness and efficiencyin NLP systems and made linguistically motivatedlarge-scale language processing a possibility (Mat-suzaki et al, 2007; Kaplan et al, 2004).
This pa-per describes an NLP system which is based on syn-tactic and semantic formalisms from theoretical lin-guistics, and which we have used to analyse the en-tire Gigaword corpus (1 billion words) in less than5 days using only 18 processors.
This combinationof detail and speed of analysis represents a break-through in NLP technology.The system is built around a wide-coverage Com-binatory Categorial Grammar (CCG) parser (Clarkand Curran, 2004b).
The parser not only recoversthe local dependencies output by treebank parserssuch as Collins (2003), but also the long-range dep-dendencies inherent in constructions such as extrac-tion and coordination.
CCG is a lexicalized gram-mar formalism, so that each word in a sentence isassigned an elementary syntactic structure, in CCG?scase a lexical category expressing subcategorisationinformation.
Statistical tagging techniques can as-sign lexical categories with high accuracy and lowambiguity (Curran et al, 2006).
The combination offinite-state supertagging and highly engineered C++leads to a parser which can analyse up to 30 sen-tences per second on standard hardware (Clark andCurran, 2004a).The C&C tools also contain a number of Maxi-mum Entropy taggers, including the CCG supertag-ger, a POS tagger (Curran and Clark, 2003a), chun-ker, and named entity recogniser (Curran and Clark,2003b).
The taggers are highly efficient, with pro-cessing speeds of over 100,000 words per second.Finally, the various components, including themorphological analyser morpha (Minnen et al,2001), are combined into a single program.
The out-put from this program ?
a CCG derivation, POS tags,lemmas, and named entity tags ?
is used by themodule Boxer (Bos, 2005) to produce interpretablestructure in the form of Discourse RepresentationStructures (DRSs).2 The CCG ParserThe grammar used by the parser is extracted fromCCGbank, a CCG version of the Penn Treebank(Hockenmaier, 2003).
The grammar consists of 425lexical categories, expressing subcategorisation in-formation, plus a small number of combinatory ruleswhich combine the categories (Steedman, 2000).
AMaximum Entropy supertagger first assigns lexicalcategories to the words in a sentence (Curran et al,2006), which are then combined by the parser usingthe combinatory rules and the CKY algorithm.Clark and Curran (2004b) describes log-linearparsing models for CCG.
The features in the modelsare defined over local parts of CCG derivations andinclude word-word dependencies.
A disadvantageof the log-linear models is that they require clus-ter computing resources for practical training (Clarkand Curran, 2004b).
We have also investigated per-ceptron training for the parser (Clark and Curran,2007b), obtaining comparable accuracy scores andsimilar training times (a few hours) compared withthe log-linear models.
The significant advantage of33the perceptron training is that it only requires a sin-gle processor.
The training is online, updating themodel parameters one sentence at a time, and it con-verges in a few passes over the CCGbank data.A packed chart representation allows efficient de-coding, with the same algorithm ?
the Viterbi al-gorithm ?
finding the highest scoring derivation forthe log-linear and perceptron models.2.1 The SupertaggerThe supertagger uses Maximum Entropy taggingtechniques (Section 3) to assign a set of lexical cate-gories to each word (Curran et al, 2006).
Supertag-ging has been especially successful for CCG: Clarkand Curran (2004a) demonstrates the considerableincreases in speed that can be obtained through useof a supertagger.
The supertagger interacts with theparser in an adaptive fashion: initially it assignsa small number of categories, on average, to eachword in the sentence, and the parser attempts to cre-ate a spanning analysis.
If this is not possible, thesupertagger assigns more categories, and this pro-cess continues until a spanning analysis is found.2.2 Parser OutputThe parser produces various types of output.
Fig-ure 1 shows the dependency output for the exam-ple sentence But Mr. Barnum called that a worst-case scenario.
The CCG dependencies are defined interms of the arguments within lexical categories; forexample, ?
(S [dcl ]\NP1 )/NP2 , 2?
represents the di-rect object of a transitive verb.
The parser alsooutputs grammatical relations (GRs) consistent withBriscoe et al (2006).
The GRs are derived through amanually created mapping from the CCG dependen-cies, together with a python post-processing scriptwhich attempts to remove any differences betweenthe two annotation schemes (for example the way inwhich coordination is analysed).The parser has been evaluated on the predicate-argument dependencies in CCGbank, obtaining la-belled precision and recall scores of 84.8% and84.5% on Section 23.
We have also evaluated theparser on DepBank, using the Grammatical Rela-tions output.
The parser scores 82.4% labelled pre-cision and 81.2% labelled recall overall.
Clark andCurran (2007a) gives precison and recall scores bro-ken down by relation type and also compares theMr._2 N/N_1 1 Barnum_3called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 3 that_5worst-case_7 N/N_1 1 scenario_8a_6 NP[nb]/N_1 1 scenario_8called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 2 scenario_8called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 1 Barnum_3But_1 S[X]/S[X]_1 1 called_4(ncmod _ Barnum_3 Mr._2)(obj2 called_4 that_5)(ncmod _ scenario_8 worst-case_7)(det scenario_8 a_6)(dobj called_4 scenario_8)(ncsubj called_4 Barnum_3 _)(conj _ called_4 But_1)Figure 1: Dependency output in the form of CCGdependencies and grammatical relationsperformance of the CCG parser with the RASP parser(Briscoe et al, 2006).3 Maximum Entropy TaggersThe taggers are based on Maximum Entropy tag-ging methods (Ratnaparkhi, 1996), and can all betrained on new annotated data, using either GIS orBFGS training code.The POS tagger uses the standard set of grammat-ical categories from the Penn Treebank and, as wellas being highly efficient, also has state-of-the-art ac-curacy on unseen newspaper text: over 97% per-word accuracy on Section 23 of the Penn Treebank(Curran and Clark, 2003a).
The chunker recognisesthe standard set of grammatical ?chunks?
: NP, VP,PP, ADJP, ADVP, and so on.
It has been trained onthe CoNLL shared task data.The named entity recogniser recognises the stan-dard set of named entities in text: person, loca-tion, organisation, date, time, monetary amount.
Ithas been trained on the MUC data.
The named en-tity recogniser contains many more features than theother taggers; Curran and Clark (2003b) describesthe feature set.Each tagger can be run as a ?multi-tagger?, poten-tially assigning more than one tag to a word.
Themulti-tagger uses the forward-backward algorithmto calculate a distribution over tags for each word inthe sentence, and a parameter determines how manytags are assigned to each word.4 BoxerBoxer is a separate component which takes a CCGderivation output by the C&C parser and generates asemantic representation.
Boxer implements a first-order fragment of Discourse Representation Theory,34DRT (Kamp and Reyle, 1993), and is capable ofgenerating the box-like structures of DRT known asDiscourse Representation Structures (DRSs).
DRT isa formal semantic theory backed up with a modeltheory, and it demonstrates a large coverage of lin-guistic phenomena.
Boxer follows the formal the-ory closely, introducing discourse referents for nounphrases and events in the domain of a DRS, and theirproperties in the conditions of a DRS.One deviation with the standard theory is theadoption of a Neo-Davidsonian analysis of eventsand roles.
Boxer also implements Van der Sandt?stheory of presupposition projection treating propernames and defininite descriptions as anaphoric ex-pressions, by binding them to appropriate previouslyintroduced discourse referents, or accommodatingon a suitable level of discourse representation.4.1 Discourse Representation StructuresDRSs are recursive data structures ?
each DRS com-prises a domain (a set of discourse referents) and aset of conditions (possibly introducing new DRSs).DRS-conditions are either basic or complex.
The ba-sic DRS-conditions supported by Boxer are: equal-ity, stating that two discourse referents refer to thesame entity; one-place relations, expressing proper-ties of discourse referents; two place relations, ex-pressing binary relations between discourse refer-ents; and names and time expressions.
ComplexDRS-conditions are: negation of a DRS; disjunctionof two DRSs; implication (one DRS implying an-other); and propositional, relating a discourse ref-erent to a DRS.Nouns, verbs, adjectives and adverbs introduceone-place relations, whose meaning is representedby the corresponding lemma.
Verb roles and prepo-sitions introduce two-place relations.4.2 Input and OutputThe input for Boxer is a list of CCG derivations deco-rated with named entities, POS tags, and lemmas fornouns and verbs.
By default, each CCG derivationproduces one DRS.
However, it is possible for oneDRS to span several CCG derivations; this enablesBoxer to deal with cross-sentential phenomena suchas pronouns and presupposition.Boxer provides various output formats.
The de-fault output is a DRS in Prolog format, with dis-______________________| x0 x1 x2 x3 ||______________________|| named(x0,barnum,per) || named(x0,mr,ttl) || thing(x1) || worst-case(x2) || scenario(x2) || call(x3) || but(x3) || event(x3) || agent(x3,x0) || patient(x3,x1) || theme(x3,x2) ||______________________|Figure 2: Easy-to-read output format of Boxercourse referents represented as Prolog variables.Other output options include: a flat structure, inwhich the recursive structure of a DRS is unfolded bylabelling each DRS and DRS-condition; an XML for-mat; and an easy-to-read box-like structure as foundin textbooks and articles on DRT.
Figure 2 shows theeasy-to-read output for the sentence But Mr. Barnumcalled that a worst-case scenario.The semantic representations can also be outputas first-order formulas.
This is achieved using thestandard translation from DRS to first-order logic(Kamp and Reyle, 1993), and allows the outputto be pipelined into off-the-shelf theorem proversor model builders for first-order logic, to performconsistency or informativeness checking (Blackburnand Bos, 2005).5 Usage of the ToolsThe taggers (and therefore the parser) can acceptmany different input formats and produce many dif-ferent output formats.
These are described using a?little language?
similar to C printf format strings.For example, the input format %w|%p \n indicatesthat the program expects word (%w) and POS tag(%p) pairs as input, where the words and POS tagsare separated by pipe characters, and each word-POStag pair is separated by a single space, and wholesentences are separated by newlines (\n).
Anotherfeature of the input/output is that other fields can beread in which are not used in the tagging process,and also form part of the output.The C&C tools use a configuration managementsystem which allows the user to override all of thedefault parameters for training and running the tag-gers and parser.
All of the tools can be used as stand-alone components.
Alternatively, a pipeline of the35tools is provided which supports two modes: localfile reading/writing or SOAP server mode.6 ApplicationsWe have developed an open-domain QA system builtaround the C&C tools and Boxer (Ahn et al, 2005).The parser is well suited to analysing large amountsof text containing a potential answer, because ofits efficiency.
The grammar is also well suited toanalysing questions, because of CCG?s treatment oflong-range dependencies.
However, since the CCGparser is based on the Penn Treebank, which con-tains few examples of questions, the parser trainedon CCGbank is a poor analyser of questions.
Clarket al (2004) describes a porting method we have de-veloped which exploits the lexicalized nature of CCGby relying on rapid manual annotation at the lexi-cal category level.
We have successfully applied thismethod to questions.The robustness and efficiency of the parser; itsability to analyses questions; and the detailed out-put provided by Boxer make it ideal for large-scaleopen-domain QA.7 ConclusionLinguistically motivated NLP can now be usedfor large-scale language processing applications.The C&C tools plus Boxer are freely availablefor research use and can be downloaded fromhttp://svn.ask.it.usyd.edu.au/trac/candc/wiki.AcknowledgementsJames Curran was funded under ARC Discovery grantsDP0453131 and DP0665973.
Johan Bos is supported by a ?Ri-entro dei Cervelli?
grant (Italian Ministry for Research).ReferencesKisuh Ahn, Johan Bos, James R. Curran, Dave Kor, MalvinaNissim, and Bonnie Webber.
2005.
Question answeringwith QED at TREC-2005.
In Proceedings of TREC-2005.Patrick Blackburn and Johan Bos.
2005.
Representation andInference for Natural Language.
A First Course in Compu-tational Semantics.
CSLI.Johan Bos.
2005.
Towards wide-coverage semantic interpreta-tion.
In Proceedings of IWCS-6, pages 42?53, Tilburg, TheNetherlands.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.
Thesecond release of the RASP system.
In Proceedings of theInteractive Demo Session of COLING/ACL-06, Sydney.Stephen Clark and James R. Curran.
2004a.
The importance ofsupertagging for wide-coverage CCG parsing.
In Proceed-ings of COLING-04, pages 282?288, Geneva, Switzerland.Stephen Clark and James R. Curran.
2004b.
Parsing the WSJusing CCG and log-linear models.
In Proceedings of ACL-04, pages 104?111, Barcelona, Spain.Stephen Clark and James R. Curran.
2007a.
Formalism-independent parser evaluation with CCG and DepBank.
InProceedings of the 45th Annual Meeting of the ACL, Prague,Czech Republic.Stephen Clark and James R. Curran.
2007b.
Perceptron train-ing for a wide-coverage lexicalized-grammar parser.
In Pro-ceedings of the ACL Workshop on Deep Linguistic Process-ing, Prague, Czech Republic.Stephen Clark, Mark Steedman, and James R. Curran.
2004.Object-extraction and question-parsing using CCG.
InProceedings of the EMNLP Conference, pages 111?118,Barcelona, Spain.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguistics,29(4):589?637.James R. Curran and Stephen Clark.
2003a.
Investigating GISand smoothing for maximum entropy taggers.
In Proceed-ings of the 10th Meeting of the EACL, pages 91?98, Bu-dapest, Hungary.James R. Curran and Stephen Clark.
2003b.
Language inde-pendent NER using a maximum entropy tagger.
In Proceed-ings of CoNLL-03, pages 164?167, Edmonton, Canada.James R. Curran, Stephen Clark, and David Vadas.
2006.Multi-tagging for lexicalized-grammar parsing.
In Proceed-ings of COLING/ACL-06, pages 697?704, Sydney.Julia Hockenmaier.
2003.
Data and Models for StatisticalParsing with Combinatory Categorial Grammar.
Ph.D. the-sis, University of Edinburgh.H.
Kamp and U. Reyle.
1993.
From Discourse to Logic; AnIntroduction to Modeltheoretic Semantics of Natural Lan-guage, Formal Logic and DRT.
Kluwer, Dordrecht.Ron Kaplan, Stefan Riezler, Tracy H. King, John T. MaxwellIII, Alexander Vasserman, and Richard Crouch.
2004.Speed and accuracy in shallow and deep stochastic pars-ing.
In Proceedings of HLT and the 4th Meeting of NAACL,Boston, MA.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007.Efficient HPSG parsing with supertagging and CFG-filtering.
In Proceedings of IJCAI-07, Hyderabad, India.Guido Minnen, John Carroll, and Darren Pearce.
2001.
Ap-plied morphological processing of English.
Natural Lan-guage Engineering, 7(3):207?223.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the EMNLP Conference,pages 133?142, Philadelphia, PA.Mark Steedman.
2000.
The Syntactic Process.
The MIT Press,Cambridge, MA.36
