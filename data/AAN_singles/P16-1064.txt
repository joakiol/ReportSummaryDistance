Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 676?685,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsDetecting Common Discussion Topics Across CultureFrom News Reader CommentsBei Shi1, Wai Lam1, Lidong Bing2and Yinqing Xu11Department of Systems Engineering and Engineering ManagementThe Chinese University of Hong Kong, Hong Kong2Machine Learning DepartmentCarnegie Mellon University, Pittsburgh, PA 15213{bshi,wlam,yqxu}@se.cuhk.edu.hklbing@cs.cmu.eduAbstractNews reader comments found in manyon-line news websites are typicallymassive in amount.
We investigatethe task of Cultural-common TopicDetection (CTD), which is aimed atdiscovering common discussion topicsfrom news reader comments writtenin different languages.
We propose anew probabilistic graphical model calledMCTA which can cope with the languagegap and capture the common semanticsin different languages.
We also develop apartially collapsed Gibbs sampler whicheffectively incorporates the term trans-lation relationship into the detection ofcultural-common topics for model param-eter learning.
Experimental results showimprovements over the state-of-the-artmodel.1 IntroductionNowadays the rapid development of informationand communication technology enables more andmore people around the world to engage in themovement of globalization.
One effect of global-ization is to facilitate greater connections betweenpeople bringing cultures closer than before.
Thisalso contributes to the convergence of some ele-ments of different cultures (Melluish, 2014).
Forexample, there is a growing tendency of peoplewatching the same movie, listening to the samemusic, and reading the news about the same event.This kind of cultural homogenization brings theemergence of commonality of some aspects of dif-ferent cultures worldwide.
It would be beneficialto identify such common aspects among cultures.For example, it can provide some insights forglobal market and international business (Cavus-gil et al, 2014).Many news websites from different regions inthe world report significant events which are of in-terests to people from different continents.
Thesewebsites also allow readers around the world togive their comments in their own languages.
Thevolume of comments is often enormous espe-cially for popular events.
In a news website,readers from a particular culture background tendto write comments in their own preferred lan-guages.
For some important or global events,we observe that readers from different cultures,via different languages, express common discus-sion topics.
For instance, on March 8 2014,Malaysia Airlines Flight MH370, carrying 227passengers and 12 crew members, disappeared.Upon the happening of this event, many news ar-ticles around the world reported it and many read-ers from different continents commented on thisevent.
Through analyzing the reader commentsmanually, we observe that both English-speakingand Chinese-speaking readers expressed in theircorresponding languages their desire for prayingfor the MH370 flight.
This is an example of acultural-common discussion topic.
Identifyingsuch cultural-common topics automatically can fa-cilitate better understanding and organization ofthe common concerns or interests of readers withdifferent language background.
Such technologycan be deployed for developing various applica-tions.
One application is to build a reader com-ment digest system that can organize comments bycultural-common discussion topics and rank thetopics by popularity.
This provides a functional-ity of analyzing the common focus of readers fromdifferent cultures on a particular event.
An exam-ple of such application is shown in Figure 3.
Un-der each event, reader comments are grouped bycultural-common topics.676In this paper, we investigate the task ofCultural-common Topic Detection (CTD) on mul-tilingual news reader comments.
Reader com-ments about a global event, written in differentlanguages, from different news websites aroundthe world exist in massive amount.
The main goalof this task is to discover cultural-common dis-cussion topics from raw multilingual news readercomments for a news event.
One challenge is thatthe discussion topics are unknown.
Another chal-lenge is related to the language gap issue.
Pre-cisely, the words of reader comments in differentlanguages are composed of different terms in theircorresponding languages.
Such language gap is-sue poses a great deal of challenge for identifyingcultural-common discussion topics in multilingualnews comments settings.One recent work done by Prasojo et al (2015) isto organize news reader comments around entitiesand aspects discussed by readers.
Such organiza-tion of reader comments cannot handle the iden-tification of common discussion topics.
On theother hand, the Muto model proposed by Boyd-Graber and Blei (2009) can extract common top-ics from multilingual documents.
This modelmerely outputs cross-lingual topics of matchingword pairs.
One example of such kind of topiccontains key terms of word pairs such as ?plane:??
ocean:??
.
.
.
?.
The assumption of one-to-one mapping of words has some drawbacks.
Onedrawback is that the correspondence of identi-fied common topics is restricted to the vocabu-lary level.
Another drawback is that the one-to-one mapping of words cannot fit the original wordoccurrences well.
For example, the English term?plane?
appears in the English documents fre-quently while the Chinese translation ????
ap-pears less.
It is not reasonable that ?plane?
and ????
share the same probability mass in commontopics.
Another closely related existing work is thePCLSA model proposed by Zhang et al (2010).PCLSA employs a mixture of English words andChinese words to represent common topics.
Itincorporates bilingual constraints into the Proba-bilistic Latent Semantic Analysis (PLSA) model(Hofmann, 2001) and assumes that word pairs inthe dictionary share similar probability in a com-mon topic.
However, similar to one-to-one map-ping of words, such bilingual constraints cannothandle well the original word co-occurrence ineach language resulting in a degradation of the co-herence and interpretability of common topics.We propose a new probabilistic graphical modelwhich is able to detect cultural-common topicsfrom multilingual news reader comments in an un-supervised manner.
In principle, no labeled data isneeded.
In this paper, we focus on dealing withtwo languages, namely, English and Chinese newsreader comments.
Different from prior works, wedesign a technique based on auxiliary distributionswhich incorporates word distributions from theother language and can capture the common se-mantics on the topic level.
We develop a partiallycollapsed Gibbs sampler which decouples the in-ference of topic distribution and word distribution.We also incorporate the term translation relation-ship, derived from a bilingual dictionary, into thedetection of cultural-common topics for model pa-rameter learning.We have prepared a data set by collecting En-glish and Chinese reader comments from differ-ent regions reflecting different culture.
Our exper-imental results are encouraging showing improve-ments over the state-of-the-art model.2 Related WorkPrasojo et al (2015) and Biyani et al (2015) or-ganized news reader comments via identified en-tities or aspects.
Such kind of organization viaentities or aspects cannot capture common topicsdiscussed by readers.
Digesting merely based onentities fails to work in multilingual settings dueto the fact that the common entities have distinctmentions in different languages.Zhai et al (2004) discovered common topicsfrom comparable texts via a PLSA based mixturemodel.
Paul and Girju (2009) proposed a Mixed-Collection Topic Model for finding common top-ics from different collections.
Despite the fact thatthe above models can find a kind of common topic,they only deal with a single language setting with-out considering the language gap.Some works discover common latent topicsfrom multilingual corpora.
For aligned corpora,they assume that the topic distribution in each doc-ument is the same (Vuli?c et al, 2011; Vuli?c andMoens, 2014; Erosheva et al, 2004; Fukumasuet al, 2012; Mimno et al, 2009; Ni et al, 2009;Zhang et al, 2013; Peng et al, 2014).
However,aligned corpora are often unavailable for mostdomains.
For unaligned corpora, cross-lingualtopic models use some language resources, such677as a bilingual dictionary or a bilingual knowledgebase to bridge the language gap (Boyd-Graber andBlei, 2009; Zhang et al, 2010; Jagarlamudi andDaum?e III, 2010).
As mentioned above, the goalsof Boyd-Graber and Blei (2009) as well as Jagarla-mud and Daum?e (2010) focus on mining the corre-spondence of topics at the vocabulary level, whichare different from that of Zhang et al (2010) andours.
The model in Zhang et al (2010) adds theconstraints of word translation pairs into PLSA.These constraints cannot handle the original wordco-occurrences well.
In contrast, we consider thelanguage gap by incorporating word distributionsfrom the other language, capturing the commonsemantics on the topic level.
Moreover, we use afully Bayesian paradigm with a prior distribution.Some existing topic methods conduct cross-lingual sentiment analysis (Lu et al, 2011; Guoet al, 2010; Lin et al, 2014; Boyd-Graber andResnik, 2010).
These models are not suitable forour CTD task because they mainly detect com-mon elements related to product aspects.
More-over some works focus more on detecting senti-ments.3 Our Proposed Model3.1 Model DescriptionThe problem definition of the CTD task is de-scribed as follows.
For a particular event, bothEnglish and Chinese news reader comments arecollected from different regions reflecting differ-ent culture.
The set of English comments is de-noted by E and the set of Chinese comments is de-noted by C. The goal of the CTD task is to extractcultural-common topics k ?
{1, 2, .
.
.
,K} fromE and C. The set of multilingual news reader com-ments of each event are processed within the sameevent.Our proposed model is called MultilingualCultural-common Topic Analysis (MCTA) whichis based on graphical model paradigm as depictedin Figure 1.
The plate on the right representscultural-common topics.
Each cultural-commontopic k is represented by an English word distri-bution ?ekover English vocabulary ?eand a Chi-nese word distribution ?ckover Chinese vocabu-lary ?c.
We make use of a bilingual dictionary,which is composed of many-to-many word trans-lations among English and Chinese words.
Tocapture common semantics of multilingual newsreader comments, we design two auxiliary distri-?
?e ?e?
?ed?cdzenzcnwenwcn ?c?cN edKN cdN edwN cdwFigure 1: Our proposed graphical modelbutions ?e, with dimension ?e, and ?c, with di-mension ?c, to help the generation of ?ekand ?ck.Precisely, we generate ?eand ?cfrom the Dirichletprior distributionsDir(?
?1|?e|) andDir(?
?1|?c|)respectively, where 1Ddenotes a D-dimensionalvector whose components are 1.
Then we draw?ekfrom the mixture of ?ekand the translation of?ck.
It is formulated as:?ek?
?
(?ck)TMc?e+ (1?
?
)?ekwhere ?e, ?c?
Dir(?
)(1)where ?
?
(0, 1) is a parameter which balancesthe nature of original topics and transferred infor-mation from the other language.
Mc?eis a map-ping |?c| ?
|?e| matrix from ?cto ?e.
Each ele-mentMc?eijis the mapping occurrence probabilityof the English term wejgiven the Chinese term wciin the set of news reader comments.
This proba-bility is calculated as:Mc?eij=C(wej) + 1|T (wci)|+?we?T (wci)C(we)(2)where C(wej) is the count of wejin all news readercomments and T (wci) is the set of English transla-tions of wcifound in the bilingual dictionary.
The?add-one?
smoothing is adopted.
Note that thesum of each row is equal to 1.
Using the sameprinciple, we can derive ?ckwhich can be formu-lated as:?ck?
?
(?ek)TMe?c+ (1?
?
)?ckwhere ?e, ?c?
Dir(?
)(3)As a result, the incorporation of ?ekand ?ckon thetopic level encourages the word distribution ?ekand ?ckto share common semantic components ofreader comments in different languages.The upper left plate in Figure 1 represents En-glish reader comments.
Neddenotes the num-ber of English reader comments and Nedwdenotes678the number of words in the English comment de.Each English reader comment deis characterizedby a K-dimensional topic membership vector ?ed,which is assumed to be generated by the priorDir(?
?
1K).
For each word wenin an Englishcomment de, we generate the topic znefrom ?ed.We generate the word wenfrom the correspondingdistribution ?ek.The bottom left plate in Figure 1 represents Chi-nese reader comments.
Similarly, we generate thetopic distribution ?cdfrom the prior Dir(?
?
1K).The topic zncof each word wcnin a Chinese com-ment dcis generated from ?cd.
We generate wordwcnfrom the corresponding distribution ?ck.The generative process is formally depicted as:?
For each topic k ?
K- choose auxiliary distributions ?ek?
Dir(?
?1|?e|) and ?ck?
Dir(?
?
1|?c|)- choose English word distribution ?ekand?ckusing Eq.
1 and Eq.
3 respectively.?
For each English comment de?
E , choose?ed?
Dir(?
?
1K)- For each position n in de- draw zen?Multi(?ed)- draw wen?Multi(?ek)?
For each Chinese comment dc?
C, choose?cd?
Dir(?
?
1K)- For each position n in dc- draw zcn?Multi(?cd)- draw wcn?Multi(?ck)Note that for simplicity, we present our model onthe bilingual setting of Chinese and English.
It canbe extended to multilingual setting via introduc-ing auxiliary distributions for each language.
Eachtopic word distribution for each language is gener-ated by the convex combination of all the auxiliarydistributions.3.2 Posterior InferenceIn order to decouple the inference of znand ?kfor each language, we develop a partially col-lapsed Gibbs method which just discards ?edand?cd.
Given ?ek, we sample the new assignments ofthe topic zediin English news reader comments dewith the following conditional probability:P (zedi= k|ze,?i,We, ?, ?ek) ?
(Ne,?idk+?k)?
?ek(4)where ze,?idenotes the topic assignments exceptthe assignment of the ith word.
Nedkis the numberAlgorithm 1 Partially Collapsed Gibbs Samplingfor MCTA1: Initialize z, ?ek, ?ck, ?ek, ?ck2: for iter = 1 to Maxiterdo3: for each English comment d in E do4: for each word wenin d do5: draw zenusing Eq.
46: end for7: end for8: for each Chinese comment d in C do9: for each word wcnin d do10: draw zcnusing Eq.
5.11: end for12: end for13: Update ?ek, ?ckby Eq.
8 and Eq.
914: Update ?ek, ?ckaccording to Eq.
1 andEq.
315: end for16: Output ?dkby Eq.
10of words in English document dewhose topics areassigned to k. Similarly, we sample zcdiwith thefollowing equation:P (zcdi= k|zc,?i,Wc, ?, ?ck) ?
(Nc,?idk+?k)?
?ck(5)Given the topic assignments, the probability ofthe entire comment set can be:p(W |z, ?ek, ?ck) =?w??e(?ekw)Nekw??w?
?c(?ckw)Nckw(6)where Nekwis the number of words w in Englishnews reader comments assigned to the topic k andNckwis the number of words w in Chinese newsreader comments assigned to the topic k.Using Eq.
6, we can obtain the posterior likeli-hood related to ?ekand ?ck:LMAP=?wi??eNekwilog(??wj?
?cMc?eji?ckwj+ (1?
?)?ekwi)+?wi??cNckwilog(??wj?
?eMe?cji?ekwj+ (1?
?)?ckwi)+?wi??e(?
?
1) log ?ekwi+?wi??c(?
?
1) log ?ckwi(7)We optimize Eq.
7 under the constraints of?wi?
?e?ekwi= 1 and?wi?
?c?ckwi= 1.
Us-ing the fixed-point method, we obtain the update679equations of ?ekwtand ?ckwtshown in Eq.
8 andEq.
9.?ekwt?
[(1?
?)Nekwt??wj?
?cMc?ejt?ckwj+ (1?
?)?ekwt+?wi??c?NckwiMe?cti??wj?
?eMe?cji?ekwj+ (1?
?
)?ckwi]?ekwt+ ?(8)?ckwt?
[(1?
?)Nckwt??wj?
?eMe?cjt?ekwj+ (1?
?)?ckwt+?wi??e?NekwiMc?eti??wj?
?cMc?eji?ckwj+ (1?
?
)?ekwi]?ckwt+ ?
(9)Moreover, the posterior estimates for the topicdistribution ?dcan be computed as follows.
?dk=Ndk+ ??k?KNdk+K?
(10)The whole detailed algorithm is depicted in Al-gorithm 1.
When ?
= 0, the updated equations of?ekand ?ckcan be simplified as:?ekwt?
Nekwt+ ??ckwt?
Nckwt+ ?
(11)Then we have:?ek?
Dir(Nekw1+ ?,Nekw2+ ?, .
.
.
)?ck?
Dir(Nckw1+ ?,Nckw2+ ?, .
.
.
)(12)Therefore, the algorithm degrades to a Gibbs sam-pler of LDA.4 Experiments4.1 Data Set and PreprocessingWe have prepared a data set by collecting En-glish and Chinese comments from different re-gions reflecting different culture for some signif-icant events as depicted in Table 1.
The Englishreader comments are collected from Yahoo1andthe Chinese reader comments are collected fromSina News2.
We first remove news reader com-ments whose length is less than 5 words.
We re-move the punctuations and the stop words.
For En-glish comments, we also stem each word to its root1http://news.yahoo.com2http://news.sina.com.cn/world/Event Title#Englishcomments#Chinesecomments1 MH370 flight accident 8608 52232 ISIS in Iraq 6341 32633 Ebola occurs 2974 16224 Taiwan Crashed Plane 6780 26485 iphone6 publish 5837 43526 Shooting of Michael Brown 17547 36937 Charlie Hebdo shooting 1845 5518 Shanghai stampede 3824 31759 Lee Kuan Yew death 2418 153410 AIIB foundation 7221 3198Table 1: The statistics for the data setform using Porter Stemmer (Porter, 1980).
Forthe Chinese reader comments, we use the Jiebapackage3to segment and remove Chinese stopwords.
We utilize an English-Chinese dictionaryfrom MDBG4.4.2 Comparative MethodsThe PCLSA model proposed by Zhang etal.
(2010) can be regarded as the state-of-the-artmodel for detecting latent common topics frommultilingual text documents.
We implementedPCLSA as one of the comparative methods in ourexperiments.Another comparative model used in the experi-ment is LDA (Blei et al, 2003), which can gen-erate K English topics and K Chinese topicsfrom English and Chinese reader comments re-spectively.
Then we translate Chinese topics intoEnglish topics and use symmetric KL divergenceto align translated Chinese topics with original En-glish topics.
Each aligned topic pair is regarded asa cultural-common topic.4.3 Experiment SettingsFor each event, we partitioned the comments intoa subset of 90% for the graphical model param-eter estimation.
The remaining 10% is used as aholdout data for the evaluation of the CCP met-ric as discussed in Section 4.4.1.
We repeated theruns five times.
For each run, we randomly splitthe comments to obtain the holdout data.
As a re-sult, we have five runs for our method as well ascomparative methods.
We make use of the hold-out data of one event, namely the event ?MH3703https://github.com/fxsjy/jieba4http://www.mdbg.net/chindict/chindict.php?page=cc-cedict680Flight Accident?, to estimate the number of top-ics K for all models and ?
in Eq.
1 for our model.The setting of K is described in Section 4.4.3.
Weset ?
= 0.5 after tuning.
For hyper-parameters,we set ?
to 0.5 and ?
to 0.01.
When performingour Gibbs algorithm, we set the maximum itera-tion number as 1000, and the burn-in sweeps as100.4.4 Cultural-common Topic EvaluationWe conduct quantitative experiments to evaluatehow well our MCTA model can discover cultural-common topics.4.4.1 Evaluation MetricsWe use two metrics to evaluate the topic quality.The first metric is the ?cross-collection perplex-ity?
measure denoted as CCP which is similar tothe one used in Zhang et al (2010).
The CCPof high quality cultural-common topics should belower than those topics which are not shared by theEnglish and Chinese reader comments.
The calcu-lation of CCP consists of two steps: 1) For eachk ?
K, we translate ?ekinto Chinese word distri-bution T (?ek) and translate ?ckEnglish word dis-tribution T (?ck).
To translate ?ekand ?ck, we lookup the bilingual dictionary and conduct word-to-word translation.
If one word has several trans-lations, we distribute its probability mass equallyto each English translation.
2) We use T (?ek) tofit the holdout Chinese comments C and T (?ck) tofit the holdout English comments E using Eq.
13(Blei et al, 2003).
Eq.
13 depicts the calculationof CCP.
The lower the CCP value is, the better theperformance is.CCP =12exp{?
?d?E?w?d?k?Klog p(k|?d)p(w|T (?ck))?d?ENed}+12exp{?
?d?C?w?d?k?Klog p(k|?d)p(w|T (?ek))?d?CNcd}(13)For each detected common topic, we wish toevaluate the degree of commonality.
We de-sign another metric called ?topic commonality dis-tance?
denoted by TCD.
We first evaluate the KL-divergence between the English topic and trans-lated Chinese topic.
We also evaluate the KL-divergence between the Chinese topic and trans-lated English topic.
Then TCD is computed asthe average sum of the two KL-divergences.
Thelower the TCD measure is, the better the topic is.Event LDA PCLSA MCTA1 1963.57 1842.24 1784.052 1940.03 1831.55 1756.923 1958.09 1905.43 1808.014 1916.49 1847.16 1775.325 1901.44 1797.92 1744.076 1916.70 1853.66 1786.777 1945.22 1897.15 1824.108 1942.29 1862.14 1749.439 1943.53 1856.70 1739.6610 1866.23 1815.44 1749.49avg.
1929.36 1850.94 1771.78Table 2: Topic quality evaluation as measured byCCPThe topic detected by PCLSA is a mixture ofEnglish and Chinese words.
We obtain Englishrepresentation and Chinese representation of thetopic by the conditional probabilities as given inEq.
14.p(we|?ek) =p(we|?k)?w?
?ep(w|?k)p(wc|?ck) =p(wc|?k)?w?
?cp(w|?k)(14)4.4.2 Experimental ResultsThe average CCP values of the three models areshown in Table 2.
Our MCTA model achievesthe best performance compared with PCLSA andLDA.
Both MCTA and PCLSA achieve a betterCCP than LDA because they can bridge the lan-guage gap in the multilingual news reader com-ments to some extent.
Compared with PCLSA,our MCTA model demonstrates a 4.2% improve-ment.
Our MCTA model provides a better char-acterization of the collections.
One reason is thatour MCTA model learns the word distribution ofcultural-common topics using an effective topicmodeling with a prior Dirichlet distribution.
Itis similar to the advantage of LDA over PLSA.Moreover, the bilingual constraints in PCLSA can-not handle the original natural word co-occurrencewell in each language.
In contrast, MCTA rep-resents cultural-common topics as a mixture ofthe original topics and the translated topics, whichcapture the comment semantics more effectively.The average TCD of three models are shown inTable 3.
Our MCTA outperforms the two compar-ative methods.
The cultural-common topics iden-681Event LDA PCLSA MCTA1 0.029 0.0075 0.00422 0.029 0.0072 0.00433 0.033 0.0076 0.00464 0.031 0.0075 0.00465 0.033 0.0086 0.00696 0.029 0.0066 0.00587 0.036 0.0080 0.00448 0.033 0.0079 0.00349 0.034 0.0088 0.003610 0.029 0.0067 0.0036avg.
0.032 0.0076 0.0045Table 3: Topic quality evaluation as measured byTCDtified by MCTA have better topic commonality be-cause our MCTA model can capture the commonsemantics between news reader comments in dif-ferent languages.4.4.3 Determining Number of TopicsAs mentioned in Section 4.3, we use the hold-out data of one event to determine K. For each?
?
{0.2, 0.5, 0.8}, we vary K in the range of[5, 200].
Figure 2 depicts the effect of K on thecross-collection perplexity as measured by CCP.We can see that CCP decreases with the increaseof the number of topics.
Moreover, through man-ual inspection we observed that when K is 30 ormore, even though CCP decreases, the topics willbe repeated.
Similar observations for the numberof topics can be found in Paul and Girju (2009).Therefore, we set K = 30.
We can also see thatour model is not very sensitive to the balance pa-rameter ?.0 50 100 150 200K168017001720174017601780180018201840CCP?
= 0.2?
= 0.5?
= 0.8Figure 2: The effect of KEvent LDA PCLSA MCTA1 0.128 0.117 0.1382 0.144 0.126 0.1583 0.122 0.117 0.1204 0.138 0.138 0.1695 0.128 0.109 0.1526 0.134 0.138 0.1527 0.103 0.108 0.1118 0.110 0.099 0.1249 0.080 0.085 0.09610 0.138 0.133 0.154avg.
0.122 0.117 0.137Table 4: Topic coherence evaluation4.5 Topic Coherence EvaluationWe also evaluate the coherence of topics gener-ated by PCLSA and MCTA, which indicates theinterpretability of topics.
Following Newman etal.
(2010), we use a pointwise mutual information(PMI) score to measure the topic coherence.
Wecompute the average PMI score of top 20 topicword pairs using Eq.
15.
Newman et al (2010)observed that it is important to use an externaldata set to evaluate PMI.
Therefore, we use a 20-word sliding window in Wikipedia (Shaoul, 2010)to identify the co-occurrence of word pairs.PMI(wi, wj) = logP (wi, wj)P (wi)P (wj)(15)The experimental results are shown in Table 4.
Wecan see that our MCTA model generally improvesthe coherence of the learned topics compared withPCLSA.
The word-to-word bilingual constraintsin PCLSA are not as effective.
On the other hand,our MTCA model incorporates the bilingual trans-lations using auxiliary distributions which incor-porate word distributions from the other languageon the topic level and can capture common seman-tics of multilingual reader comments.5 Application and Case StudyWe present an application for news comment di-gest and show some examples of detected cultural-common discussion topics in Figure 3.
Under eachevent, the system can group reader comments intocultural-common discussion topics which can cap-ture common concerns of readers in different lan-guages.
For each common topic, it shows topranked words and corresponding reader comments682Event: MH370 Flight AccidentTopic TermsReader Commentsfamily hope love deadpeople victim passengersad sorry life??(family)??(family)??(family)??(family)??(disappoint)??(hope)??(sad)?(wish)?(pain)??
(sad)I feel sorry for the families of the victims of this flight - this aircraft piece being found probablybrings that terrible day backI feel so sorry for the relatives of the missing passengers who are doomed to spend the rest of theirlives getting their hopes continuously...The family members should now begin to have a closure as the plane?s flaperon has been identified.The Australians have been proven correct as...????????????????????????????????????????????????????????????????????????????????????????????????370??????????????????
?...ocean island search Indiamile area locate Australiadrift west??
(ocean currents)???
(Indian ocean)??(Region)??(mile)??(search)??(search)??(rescure)??(seafloor)??(Australia)??
(sea area)They were looking in the West Australian current.
That would have brought the part to the northof Australia.
If it got into I equatorial current...They need to start their sonar scans about 1000 miles south of the tip of India seeing how thecurrents in that ocean work, and how long it took for that piece to float to the island so far out.
It?spretty simple to estimate seeing how Fukushima fishing boats travelled a set distance over a settime, given a set current...look at current maps.
well off the western coast of Aus is the S. Equitorial Current in the IndianOcean which flows in a circular counter clockwise pattern.
It most certainly could have comefrom a plane that crashed off the AUS coast.???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????MH370????????????????????????????????????????????????????????????????????????
?.......Event:ISIS in IraqTopic TermsReader Commentsmuslim islam religionworld christian god peoplebelieve jew human??(belief)??(religion)??(world)??(believe)???(world)???(Islam)???(muslim)?(people)??(jew)??
(human)1 don?t understand Muslims, Islam or the Holy Qur?an!
The aim of Islam is not to instil Shariaover the entire world, Islam preaches that you believe in God worship Him alone and do rightgood by your belief.....Oh, I get it.
It?s about the badness of Muslims being humbled and humiliated in prison by Ameri-cans.
But IS rapes and mutilates and pilliages...If there was no Muslim religion in Iraq, there would be no ISIS because there would have been nonecessity for a thug like Saddam to control...ISIS???????????????ISIS????????????????????????ISIS??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?......Event:AIIB FoundationTopic TermsReader Commentsbank aiib world imf asiandevelop investmentinstitution infrastructuremember??(bank)??(finance)????
(The WorldBank)??
(The WorldBank)????(Financeinstitution)??????
(Asian DevelopmentBank)??(member)????????(IMF)??(International)??
(world)Looks like all the rats are jumping off the sinking world bank and IMF ship.
America has pushedtheir bulling ways long enough and people are...The Federal Reserve, the World Bank, The IMF, and the BIS are failed, self-serving institutions.One can only hope that China will stimulate world growth and the suppression by the west willfinally come to an end.
The US dollar no longer deserves to be the world?s reserve currency.Bank shopping !!!
No more stranglehold by the IMF and World Bank.
If Ukraine had only waitedanother year.
Too bad.????????????????????????????????????[??]?????????????????????????????????????????????????????????????????????????????????????????????????????
?...Figure 3: Some sample common discussion topics of some events683according to ?edkand ?cdk.
Considering the event?MH370 Flight accident?, it shows two of the de-tected cultural-common topics.
The first one in-dicates that readers pray for the family in the ac-cident and the second one is related to the searchof the crashed plane.
For the common topic aboutpraying for the family, we can see that the top-ics contain both English words and Chinese wordswhich are very relevant and share common se-mantics of ?family?
and ?hope?.
Moreover, thecorresponding English and Chinese reader com-ments, both of which mention the family in the ac-cident, illustrate a high coherent common discus-sion topic.
Similarly for the second common topic,there are common semantics between English andChinese top ranked words about the search of thecrashed plane.
Some of the English comments andChinese comments mention the query of the posi-tion of the crashed plane.
Interesting common top-ics are also generated for other events, such as thecommon topic of religion for the event ?ISIS inIraq?
and the topic of economic organization forthe event ?AIIB foundation?.6 ConclusionsWe investigate the task of cultural-common dis-cussion topic detection from multilingual newsreader comments.
To tackle the task, we developa new model called MCTA which can cope withthe language gap and extract coherent cultural-common topics from multilingual news readercomments.
We also develop a partially collapsedGibbs sampler which incorporates the term trans-lation relationship into the detection of cultural-common topics effectively for model parameterlearning.AcknowledgmentsThe work described in this paper is substantiallysupported by grants from the Research GrantCouncil of the Hong Kong Special AdministrativeRegion, China (Project Code: 14203414) and theDirect Grant of the Faculty of Engineering, CUHK(Project Code: 4055034).
This work is also affil-iated with the CUHK MoE-Microsoft Key Labo-ratory of Human-centric Computing and InterfaceTechnologies.
We also thank the anonymous re-viewers for their insightful comments.ReferencesPrakhar Biyani, Cornelia Caragea, and NarayanBhamidipati.
2015.
Entity-specific sentiment clas-sification of yahoo news comments.
arXiv preprintarXiv:1506.03775.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jordan Boyd-Graber and David M Blei.
2009.
Mul-tilingual topic models for unaligned text.
In Pro-ceedings of the 25th Conference on Uncertainty inArtificial Intelligence, pages 75?82.Jordan Boyd-Graber and Philip Resnik.
2010.
Holisticsentiment analysis across languages: Multilingualsupervised latent dirichlet alocation.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, pages 45?55.S Tamer Cavusgil, Gary Knight, John R Riesenberger,Hussain G Rammal, and Elizabeth L Rose.
2014.International business: strategy, management andthe new realities.
Pearson Australia.Elena Erosheva, Stephen Fienberg, and John Lafferty.2004.
Mixed-membership models of scientific pub-lications.
Proceedings of the National Academy ofSciences, 101(suppl 1):5220?5227.Kosuke Fukumasu, Koji Eguchi, and Eric P Xing.2012.
Symmetric correspondence topic models formultilingual text analysis.
In Advances in NeuralInformation Processing Systems, pages 1286?1294.Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,and Zhong Su.
2010.
Opinionit: a text miningsystem for cross-lingual opinion analysis.
In Pro-ceedings of the 19th ACM International Conferenceon Information and Knowledge Management, pages1199?1208.Thomas Hofmann.
2001.
Unsupervised learningby probabilistic latent semantic analysis.
MachineLearning, 42(1-2):177?196.Jagadeesh Jagarlamudi and Hal Daum?e III.
2010.
Ex-tracting multilingual topics from unaligned compa-rable corpora.
In Proceedings of the 32nd Euro-pean Conference on IR Research, pages 444?456.Springer.Zheng Lin, Xiaolong Jin, Xueke Xu, Weiping Wang,Xueqi Cheng, and Yuanzhuo Wang.
2014.
A cross-lingual joint aspect/sentiment model for sentimentanalysis.
In Proceedings of the 23rd ACM Inter-national Conference on Information and KnowledgeManagement, pages 1089?1098.Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin KTsou.
2011.
Joint bilingual sentiment classificationwith unlabeled parallel corpora.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 320?330.684Steve Melluish.
2014.
Globalization, culture andpsychology.
International Review of Psychiatry,26(5):538?543.David Mimno, Hanna M Wallach, Jason Naradowsky,David A Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 880?889.David Newman, Jey Han Lau, Karl Grieser, and Tim-othy Baldwin.
2010.
Automatic evaluation oftopic coherence.
In Human Language Technologies:the 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 100?108.Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.2009.
Mining multilingual topics from wikipedia.In Proceedings of the 18th International Conferenceon World Wide Web, pages 1155?1156.Michael Paul and Roxana Girju.
2009.
Cross-culturalanalysis of blogs and forums with mixed-collectiontopic models.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 1408?1417.Nanyun Peng, Yiming Wang, and Mark Dredze.2014.
Learning polylingual topic models from code-switched social media documents.
In Proceedingsof the 52nd Annual Meeting of the Association forComputational Linguistics, volume 2, pages 674?679.Martin F Porter.
1980.
An algorithm for suffix strip-ping.
Program, 14(3):130?137.Radityo Eko Prasojo, Mouna Kacimi, and Werner Nutt.2015.
Entity and aspect extraction for organiz-ing news comments.
In Proceedings of the 24thACM International Conference on Information andKnowledge Management, pages 233?242.Cyrus Shaoul.
2010.
The westbury lab wikipedia cor-pus.
Edmonton, AB: University of Alberta.Ivan Vuli?c and Marie-Francine Moens.
2014.
Prob-abilistic models of cross-lingual semantic similarityin context based on latent cross-lingual concepts in-duced from comparable data.
In Proceedings of the2014 Conference on Empirical Methods in NaturalLanguage Processing, pages 349?362.Ivan Vuli?c, Wim De Smet, and Marie-Francine Moens.2011.
Identifying word translations from compara-ble corpora using latent topic models.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 479?484.ChengXiang Zhai, Atulya Velivelli, and Bei Yu.
2004.A cross-collection mixture model for comparativetext mining.
In Proceedings of the 10th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, pages 743?748.Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai.2010.
Cross-lingual latent topic extraction.
In Pro-ceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1128?1137.Tao Zhang, Kang Liu, and Jun Zhao.
2013.
Cross lin-gual entity linking with bilingual topic model.
InProceedings of the 23rd International Joint Confer-ence on Artificial Intelligence, pages 2218?2224.685
