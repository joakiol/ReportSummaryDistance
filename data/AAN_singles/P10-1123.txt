Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1209?1219,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAssessing the Role of Discourse References in Entailment InferenceShachar Mirkin, Ido DaganBar-Ilan UniversityRamat-Gan, Israel{mirkins,dagan}@cs.biu.ac.ilSebastian Pado?University of StuttgartStuttgart, Germanypado@ims.uni-stuttgart.deAbstractDiscourse references, notably coreferenceand bridging, play an important role inmany text understanding applications, buttheir impact on textual entailment is yet tobe systematically understood.
On the ba-sis of an in-depth analysis of entailmentinstances, we argue that discourse refer-ences have the potential of substantiallyimproving textual entailment recognition,and identify a number of research direc-tions towards this goal.1 IntroductionThe detection and resolution of discourse refer-ences such as coreference and bridging anaphoraplay an important role in text understanding appli-cations, like question answering and informationextraction.
There, reference resolution is used forthe purpose of combining knowledge from multi-ple sentences.
Such knowledge is also importantfor Textual Entailment (TE), a generic frameworkfor modeling semantic inference.
TE reduces theinference requirements of many text understand-ing applications to the problem of determiningwhether the meaning of a given textual assertion,termed hypothesis (H), can be inferred from themeaning of certain text (T ) (Dagan et al, 2006).Consider the following example:(1) T: ?Not only had he developed an aversionto the President1 and politics in general,Oswald2 was also a failure with Marina, hiswife.
[...] Their relationship was supposedlyresponsible for why he2 killed Kennedy1.
?H: ?Oswald killed President Kennedy.
?The understanding that the second sentence of thetext entails the hypothesis draws on two corefer-ence relationships, namely that he is Oswald, andthat the Kennedy in question is President Kennedy.However, the utilization of discourse informationfor such inferences has been so far limited mainlyto the substitution of nominal coreferents, whilemany aspects of the interface between discourseand semantic inference needs remain unexplored.The recently held Fifth Recognizing TextualEntailment (RTE-5) challenge (Bentivogli et al,2009a) has introduced a Search task, where thetext sentences are interpreted in the context of theirfull discourse, as in Example 1 above.
Accord-ingly, TE constitutes an interesting framework ?and the Search task an adequate dataset ?
to studythe interrelation between discourse and inference.The goal of this study is to analyze the rolesof discourse references for textual entailment in-ference, to provide relevant findings and insightsto developers of both reference resolvers and en-tailment systems and to highlight promising direc-tions for the better incorporation of discourse phe-nomena into inference.
Our focus is on a manual,in-depth assessment that results in a classificationand quantification of discourse reference phenom-ena and their utilization for inference.
On this ba-sis, we develop an account of formal devices forincorporating discourse references into the infer-ence computation.
An additional point of inter-est is the interrelation between entailment knowl-edge and coreference.
E.g., in Example 1 above,knowing that Kennedy was a president can alle-viate the need for coreference resolution.
Con-versely, coreference resolution can often be usedto overcome gaps in entailment knowledge.Structure of the paper.
In Section 2, we pro-vide background on the use of discourse refer-ences in natural language processing (NLP) ingeneral and specifically in TE.
Section 3 describesthe goals of this study, followed by our analy-sis scheme (Section 4) and the required inference1209mechanisms (Section 5).
Section 6 presents quan-titative findings and further observations.
Conclu-sions are discussed in Section 7.2 Background2.1 Discourse in NLPDiscourse information plays a role in a rangeof NLP tasks.
It is obviously central to dis-course processing tasks such as text segmenta-tion (Hearst, 1997).
Reference information pro-vided by discourse is also useful for text under-standing tasks such as question answering (QA),information extraction (IE) and information re-trieval (IR) (Vicedo and Ferrndez, 2006; Zelenkoet al, 2004; Na and Ng, 2009), as well as for theacquisition of lexical-semantic ?narrative schema?knowledge (Chambers and Jurafsky, 2009).
Dis-course references have been the subject of atten-tion in both the Message Understanding Confer-ence (Grishman and Sundheim, 1996) and the Au-tomatic Content Extraction program (Strassel etal., 2008).The simplest form of information that discourseprovides is coreference, i.e., information that twolinguistic expressions refer to the same entity orevent.
Coreference is particularly important forprocessing pronouns and other anaphoric expres-sions, such as he in Example 1.
Ability to re-solve this reference translates directly into, e.g., aQA system?s ability to answer questions like Whokilled Kennedy?.A second, more complex type of informationstems from bridging references, such as in the fol-lowing discourse (Asher and Lascarides, 1998):(2) ?I?ve just arrived.
The camel is outside.
?While coreference indicates equivalence, bridgingpoints to the existence of a salient semantic rela-tion between two distinct entities or events.
Here,it is (informally) ?means of transport?, whichwould make the discourse (2) relevant for a ques-tion like How did I arrive here?.
Other types ofbridging relations include set-membership, rolesin events and consequence (Clark, 1975).Note, however, that text understanding systemsare generally limited to the resolution of entity (oreven just pronoun) coreference, e.g.
(Li et al,2009; Dali et al, 2009).
An important reason is theunavailability of tools to resolve the more complex(and difficult) forms of discourse reference such asevent coreference and bridging.1 Another reasonis uncertainty about their practical importance.2.2 Discourse in Textual EntailmentTextual Entailment has been introduced in Sec-tion 1 as a common-sense notion of inference.It has spawned interest in the computational lin-guistics community as a common denominator ofmany NLP tasks including IE, summarization andtutoring (Romano et al, 2006; Harabagiu et al,2007; Nielsen et al, 2009).Architectures for Textual Entailment.
Overthe course of recent RTE challenges (Giampic-colo et al, 2007; Giampiccolo et al, 2008), themain benchmark for TE technology, two archi-tectures for modeling TE have emerged as dom-inant: transformations and alignment.
The goalof transformation-based TE models is to deter-mine the entailment relation T ?
H by find-ing a ?proof?, i.e., a sequence of consequents,(T, T1, .
.
.
, Tn), such that Tn=H (Bar-Haim et al,2008; Harmeling, 2009), and that in each trans-formation, Ti?
Ti+1, the consequent Ti+1 is en-tailed by Ti.
These transformations commonly in-clude lexical modifications and the generation ofsyntactic alternatives.
The second major approachconstructs an alignment between the linguistic en-tities of the trees (or graphs) of T and H , whichcan represent syntactic structure, semantic struc-ture, or non-hierarchical phrases (Zanzotto et al,2009; Burchardt et al, 2009; MacCartney et al,2008).
H is assumed to be entailed by T if its en-tities are aligned ?well?
to corresponding entitiesin T .
Alignment quality is generally determinedbased on features that assess the validity of the lo-cal replacement of the T entity by the H entity.While transformation- and alignment-based en-tailment models look different at first glance, theyultimately have the same goal, namely obtaininga maximal coverage of H by T , i.e.
to identifymatches of as many elements of H within T aspossible.2 To do so, both architectures typicallymake use of inference rules such as ?Y was pur-chased by X?
X paid for Y?, either by directly ap-plying them as transformations, or by using them1Some studies, e.g.
(Markert et al, 2003; Poesio et al,2004), address the resolution of a few specific kinds of bridg-ing relations; yet, wide-scope systems for bridging resolutionare unavailable.2Clearly, the details of how the final entailment decisionis made based on the attained coverage differ substantiallyamong models.1210to score alignments.
Rules are generally drawnfrom external knowledge resources, such as Word-Net (Fellbaum, 1998) or DIRT (Lin and Pantel,2001), although knowledge gaps remain a key ob-stacle (Bos, 2005; Balahur et al, 2008; Bar-Haimet al, 2008).Discourse in previous RTE challenges.
Thefirst two rounds of the RTE challenge used ?self-contained?
texts and hypotheses, where discourseconsiderations played virtually no role.
A first steptowards a more comprehensive notion of entail-ment was taken with RTE-3 (Giampiccolo et al,2007), when paragraph-length texts were first in-cluded and constituted 17% of the texts in the testset.
Chambers et al (2007) report that in a sampleof T ?
H pairs drawn from the development set,25% involved discourse references.Using the concepts introduced above, the im-pact of discourse references can be generally de-scribed as a coverage problem, independent of thesystem?s architecture.
In Example 1, the hypoth-esis word Oswald cannot be safely linked to thetext pronoun he without further knowledge abouthe; the same is true for ?Kennedy ?
PresidentKennedy?
which involves a specialization that isonly warranted in the specific discourse.A number of systems have tried to address thequestion of coreference in RTE as a preprocessingstep prior to inference proper, with most systemsusing off-the-shelf coreference resolvers such asJavaRap (Qiu et al, 2004) or OpenNLP3.
Gen-erally, anaphoric expressions were textually re-placed by their antecedents.
Results were in-conclusive, however, with several reports abouterrors introduced by automatic coreference res-olution (Agichtein et al, 2008; Adams et al,2007).
Specific evaluations of the contributionof coreference resolution yielded both small nega-tive (Bar-Haim et al, 2008) and insignificant pos-itive (Chambers et al, 2007) results.3 Motivation and GoalsThe results of recent studies, as reported in Sec-tion 2.2, seem to show that current resolution ofdiscourse references in RTE systems hardly af-fects performance.
However, our intuition is thatthese results can be attributed to four major lim-itations shared by these studies: (1) the datasets,where discourse phenomena were not well repre-3http://opennlp.sourceforge.netsented; (2) the off-the-shelf coreference resolutionsystems which may have been not robust enough;(3) the limitation to nominal coreference; and (4)overly simple integration of reference informationinto the inference engines.The goal of this paper is to assess the impact ofdiscourse references on entailment with an anno-tation study which removes these limitations.
Tocounteract (1), we use the recent RTE-5 Searchdataset (details below).
To avoid (2), we performa manual analysis, assuming discourse referencesas predicted by an oracle.
With regards to (3), ourannotation scheme covers coreference and bridg-ing relations of all syntactic categories and classi-fies them.
As for (4), we suggest several opera-tions necessary to integrate the discourse informa-tion into an entailment engine.In contrast to the numerous existing datasetsannotated for discourse references (Hovy et al,2006; Strassel et al, 2008), we do not annotate ex-haustively.
Rather, we are interested specifically inthose references instances that impact inference.Furthermore, we analyze each instance from anentailment perspective, characterizing the relevantfactors that have an impact on inference.
To ourknowledge, this is the first such in-depth study.4The results of our study are of twofold interest.First, they provide guidance for the developers ofreference resolvers who might prioritize the scopeof their systems to make them more valuable forinference.
Second, they point out potential direc-tions for the developers of inference systems byspecifying what additional inference mechanismsare needed to utilize discourse information.The RTE-5 Search dataset.
We base our anno-tation on the Search task dataset, a new additionto the recent Fifth RTE challenge (Bentivogli etal., 2009a) that is motivated by the needs of NLPapplications and drawn from the TAC summariza-tion track.
In the Search task, TE systems are re-quired to find all individual sentences in a givencorpus which entail the hypothesis ?
a setting thatis sensible not only for summarization, but also forinformation access tasks like QA.
Sentences arejudged individually, but ?are to be interpreted inthe context of the corpus as they rely on explicitand implicit references to entities, events, dates,places, etc., mentioned elsewhere in the corpus?
(Bentivogli et al, 2009b).4The guidelines and the dataset are available athttp://www.cs.biu.ac.il/?
nlp/downloads/1211Text Hypothesisi T?
Once the reform becomes law, Spain will join the Netherlandsand Belgium in allowing homosexual marriages.
Massachusetts allows homosexualT Such unions are also legal in six Canadian provinces and thenortheastern US state of Massachusetts.marriagesT ?
The official name of 2003 UB313 has yet to be determined.ii T Brown said he expected to find a moon orbiting Xena becausemany Kuiper Belt objects are paired with moons.2003 UB313 is in the Kuiper BeltiiiT ?aAll seven aboard the AS-28 submarine appeared to be in satis-factory condition, naval spokesman said.T ?bBritish crews were working with Russian naval authorities to ma-neuver the unmanned robotic vehicle and untangle the AS-28.The AS-28 mini submarine was trappedunderwaterT The Russian military was racing against time early Friday to res-cue a mini submarine trapped on the seabed.iv T?
China seeks solutions to its coal mine safety.
A mining accident in China has killedseveral minersT A recent accident has cost more than a dozen miners their lives.vT ?
?A remote-controlled device was lowered to the stricken vessel tocut the cables in which the AS-28 vehicle is caught.T ?The mini submarine was resting on the seabed at a depth of about200 meters.The AS-28 mini submarine was trappedunderwaterT Specialists said it could have become tangled up with a metalcable or in sunken nets from a fishing trawler.vi T .
.
.
dried up lakes in Siberia, because the permafrost beneaththem has begun to thaw.The ice is melting in the ArcticTable 1: Examples for discourse-dependent entailment in the RTE-5 dataset, where the inference of Hdepends on reference information from the discourse sentences T ?
/ T ??.
Referring terms (in T ) and targetterms (in H) are shown in boldface.4 Analysis SchemeFor annotating the RTE-5 data, we operationalizereference relations that are relevant for entailmentas those that improve coverage.
Recall from Sec-tion 2.2 that the concept of coverage is applicableto both transformation and alignment models, allof which aim at maximizing coverage of H by T .We represent T and H as syntactic trees, ascommon in the RTE literature (Zanzotto et al,2009; Agichtein et al, 2008).
Specifically, weassume MINIPAR-style (Lin, 1993) dependencytrees where nodes represent text expressions andedges represent the syntactic relations betweenthem.
We use ?term?
to refer to text expressions,and ?components?
to refer to nodes, edges, andsubtrees.
Dependency trees are a popular choicein RTE since they offer a fairly semantics-orientedaccount of the sentence structure that can still beconstructed robustly.
In an ideal case of entail-ment, all nodes and dependency edges of H arecovered by T .For each T ?
H pair, we annotate all relevantdiscourse references in terms of three items: thetarget component in H , the focus term in T , andthe reference term which stands in a reference re-lation to the focus term.
By resolving this ref-erence, the target component can usually be in-ferred; sometimes, however, more than one ref-erence term needs to be found.
We now defineand illustrate these concepts on examples fromTable 1.5The target component is a tree component inH that cannot be covered by the ?local?
materialfrom T .
An example for a tree component is Ex-ample (v), where the target component AS-28 minisubmarine in H cannot be inferred from the pro-noun it in T .
Example (vi) demonstrates an edgeas target component.
In this case, the edge in Hconnecting melt with the modifier in the Arctic isnot found in T .
Although each of the hypothesis?nodes can be covered separately via knowledge-based rules (e.g.
?Siberia ?
Arctic?, ?permafrost?
ice?, ?thaw ?
melt?
), the resulting fragmentsin T are unconnected without the (intra-sentential)coreference between them and lakes in Siberia.For each target component, we identify its focusterm as the expression in T that does not cover thetarget component itself but participates in a refer-ence relation that can help covering it.We follow the focus term?s reference chain toa reference term which can, either separately orin combination with the focus term, help coveringthe target component.
In Example (ii), where the5In our annotation, we assume throughout that someknowledge about basic admissible transformations is avail-able, such as passive to active or derivational transformations;for brevity, we ignore articles in the examples and treat namedentities as single nodes.1212target component in H is 2003 UB313, Xena is thefocus term in T and the reference term is a men-tion of 2003 UB313 in a previous sentence, T ?.
Inthis case, the reference term covers the entire tar-get component on its own.An additional attribute that we record for eachinstance is whether resolving the discourse refer-ence is mandatory for determining entailment, oroptional.
In Example (v), it is mandatory: the in-ference cannot be completed without the knowl-edge provided by the discourse.
In contrast, inExample (ii), inferring 2003 UB313 from Xenais optional.
It can be done either by identify-ing their coreference relation, or by using back-ground knowledge in the form of an entailmentrule, ?Xena ?
2003 UB313?, that is applicablein the context of astronomy.
Optional discoursereferences represent instances where discourse in-formation and TE knowledge are interchange-able.
As mentioned, knowledge gaps constitutea major obstacle for TE systems, and we can-not rely on the availability of any ceratin piece ofknowledge to the inference process.
Thus, in ourscheme, mandatory references provide a ?lowerbound?
with regards to the necessity to resolvediscourse references, even in the presence of com-plete knowledge; optional references, on the otherhand, set an ?upper bound?
for the contribution ofdiscourse resolution to inference, when no knowl-edge is available.
At the same time, this schemeallows investigating how much TE knowledge canbe replaced by (perfect) discourse processing.When choosing a reference term, we search thereference chain of the focus term for the nearestexpression that is identical to the target componentor a subcomponent of it.
If we find such an expres-sion, covering the identical part of the target com-ponent requires no entailment knowledge.
If noidentical reference term exists, we choose the se-mantically ?closest?
term from the reference chain,i.e.
the term which requires the least knowledge toinfer the target component.
For instance, we maypick permafrost as the semantically closet term tothe target ice if the latter is not found in the focusterm?s reference chain.Finally, for each reference relation that we an-notate, we record four additional attributes whichwe assumed to be informative in an evaluation.First, the reference type: Is the relation a coref-erence or a bridging reference?
Second, the syn-tactic type of the focus and reference terms.
Third,the focus/reference terms entailment status ?
doessome kind of entailment relation hold between thetwo terms?
Fourth, the operation that should beperformed on the focus and reference terms to ob-tain coverage of the target component (as specifiedin Section 5).5 Integrating Discourse References intoEntailment RecognitionIn initial analysis we found that the standard sub-stitution operation applied by virtually all previousstudies for integrating coreference into entailmentis insufficient.
We identified three distinct casesfor the integration of discourse reference knowl-edge in entailment, which correspond to differentrelations between the target component, the fo-cus term and the reference term.
This section de-scribes the three cases and characterizes them interms of tree transformations.
An initial version ofthese transformations is described in (Abad et al,2010).
We assume a transformation-based entail-ment architecture (cf.
Section 2.2), although webelieve that the key points of our account are alsoapplicable to alignment-based architecture.
Trans-formations create revised trees that cover previ-ously uncovered target components in H .
Theoutput of each transformation, T1, is comprisedof copies of the components used to construct it,and is appended to the discourse forest, which in-cludes the dependency trees of all sentences andtheir generated consequents.We assume that we have access to a dependencytree for H , a dependency forest for T and its dis-course context, as well as the output of a perfectdiscourse processor, i.e., a complete set of bothcoreference and bridging relations, including thetype of bridging relation (e.g.
part-of, cause).We use the following notation.
We use x, yfor tree nodes, and Sx to denote a (sub-)tree withroot x. lab(x) is the label of the incoming edgeof x (i.e., its grammatical function).
We writeC(x, y) for a coreference relation between Sx andSy, the corresponding trees of the focus and refer-ence terms, respectively.
We write Br(x, y) for abridging relation, where r is its type.
(1) Substitution: This is the most intuitive andwidely-used transformation, corresponding to thetreatment of discourse information in existing sys-tems.
It applies to coreference relations, when anexpression found elsewhere in the text (the refer-ence term) can cover all missing information (the1213be legal alsounionsuchpredmodsubjbe legalalsomarriageshomosexualpred modsubjmodTT 1marriageshomosexualmodT?preFigure 1: The Substitution transformation, demon-strated on the relevant subtrees of Example (i).The dashed line denotes a discourse reference.target component) on its own.
In such cases, thereference term can replace the entire focus term.Apparently (cf.
Section 6), substitution appliesalso to some types of bridging relations, such asset-membership, when the member is sufficient forrepresenting the entire set for the necessary infer-ence.
For example, in ?I met two people yesterday.The woman told me a story.?
(Clark, 1975), sub-stituting two people with woman results in a textwhich is entailed from the discourse, and whichallows inferring ?I met a woman yesterday.
?In a parse tree representation, given a corefer-ence relation C(x, y) (or Br(x, y)), the newly gen-erated tree, T1, consists of a copy of T , where theentire tree Sx is replaced by a copy of Sy .
In Fig-ure 1, which shows Example (i) from Table 1, suchunions is substituted by homosexual marriages.Head-substitution.
Occasionally, substitutingonly the head of the focus term is sufficient.
Insuch cases, only the root nodes x and y are sub-stituted.
This is the case, for example, with syn-onymous verbs with identical subcategorizationframes (like melt and thaw).
As verbs typicallyconstitute tree roots in dependency parses, sub-stituting or merging (see below) their entire treesmight be inappropriate or wasteful.
In such cases,the simpler head-substitution may be applied.
(2) Merge: In contrast to substitution, where amatch for the entire target component is foundelsewhere in the text, this transformation is re-quired when parts of the missing information arescattered among multiple locations in the text.We distinguish between two types of merge trans-formations: (a) dependent-merge, and (b) head-merge, depending on the syntactic roles of themerged components.
(a) Dependent-Merge.
This operation is ap-plicable when the head of either the focus or ref-erence terms (of both) matches the head node ofsubmarineminiontrapped modTT 1submarine AS-28nnT?
apcomp-npnmodmodseabedsubmarineminitrappedmodpnmodmodAS-28nnAS-28T?
bonpcomp-n seabedFigure 2: The dependent-merge (T ?a) and head-merge (T ?b) transformations (Example (iii)).the target component, but modifiers from both ofthem are required to cover the target component?sdependents.
The modifiers are therefore mergedas dependents of a single head node, to createa tree that covers the entire target component.Dependent-merge is illustrated in Figure 2, usingExample (iii).
The component we wish to cover inH is the noun phrase AS-28 mini submarine.
Un-fortunately, the focus term in T , ?mini submarinetrapped on the seabed?, covers only the modifiermini, but not AS-28.
This modifier can however beprovided by the coreferent term in T ?a (left uppercorner).
Once merged, the inference engine can,e.g., employ the rule ?on seabed ?
underwater?to cover H completely.Formally, assume without loss of generality thaty, the reference term?s head, matches the root nodeof the target component.
Given C(x, y), we defineT1 as a copy of T , where (i) the subtree Sx is re-placed by Sy, and (ii) for all children c of x, a copyof Sc is placed under the copy of y in T1 with itsoriginal edge label, lab(c).
(b) Head-merge.
An alternative way to recoverthe missing information in Example (iii) is to finda reference term whose head word itself (ratherthan one of its modifiers) matches the target com-ponent?s missing dependent, as with AS-28 in Fig-ure 2 in the bottom left corner (T ?b).
In terms ofparse trees, we need to add one tree as a depen-dent of the other.
Formally, given C(x, y), simi-larly to dependent-merge, T1 is created as a copyof T where the subtree Sx is replaced by either Sxor Sy, depending on whichever of x and y matchesthe target component?s head.
Assume it is x, forexample.
Then, a copy of Sy is added as a newchild to x.
In our sample, head-merge operationscorrespond to internal coreferences within nomi-nal target components (such as between AS-28 andmini submarine in this case).
The appropriate la-bel, lab(y), in these cases is nn (nominal modi-1214inTT 1T?pcomp-n Chinacost havethanmore comp1 pcomp-nobjhavedozenaccidentsubj recentmodcost havethanmore comp1 pcomp-nobjhavedozenaccidentsubj recentmodmodSolutionseekChinatomod pcomp-nsafety coalminennnnitsgenobjsubjFigure 3: The insertion transformation.
Dottededges mark the newly inserted path (Ex.
(iv)).fier).
Further analysis is required to specify whatother dependencies can hold between such core-ferring heads.
(3) Insertion: The last transformation, insertion,is used when a relation that is realized in H ismissing from T and is only implied via a bridg-ing relation.
In Example (iv), the location that isexplicitly mentioned in H can only be covered byT by resolving a bridging reference with Chinain T ?.
To connect the bridging referents, a newtree component representing the bridging relationis inserted into the consequent tree T1.
In this ex-ample, the component connects China and recentaccident via the in preposition.
Formally, givena bridging relation Br(x, y), we introduce a newsubtree Srz into T1, where z is a child of x andlab(z) = labr.
Srz must contain a variable nodethat is instantiated with a copy of S(y).This transformation stands out from the othersin that it introduces new material.
For each bridg-ing relation, it adds a specific subtrees Sr via anedge labeled with labr.
These two items form thedependency representation of the bridging relationBr and must be provided by the interface betweenthe discourse and the inference systems.
Clearly,their exact form depends on the set of bridging re-lations provided by the discourse resolver as wellas the details of the dependency parses.As shown in Figure 3, the bridging relationlocated-in (r) is represented by inserting a subtreeSrz headed by in (z) into T1 and connecting it toaccident (x) as a modifier (labr).
The subtree Srzconsists of a variable node which is connected toin with a pcomp-n dependency (a nominal head ofa prepositional phrase), and which is instantiatedwith the node China (y) when the transformationis applied.
Note that the structure of Srz and theway it is inserted into T1 are predefined by theabovementioned interface; only the node to whichit is attached and the contents of the variable nodeare determined at transformation-time.As another example, consider the followingshort text from (Clark, 1975): John was murderedyesterday.
The knife lay nearby.
Here, the bridg-ing relation between the murder event and the in-strument, the knife (x), can be addressed by in-serting under x a subtree for the clause with whichas Srz , with a variable which is instantiated by theparse-tree (headed by murdered, y) of the entirefirst sentence John was murdered yesterday.Transformation chaining.
Since our transfor-mations are defined to be minimal, some cases re-quire the application of multiple transformationsto achieve coverage.
Consider Example (v), Ta-ble 1.
We wish to cover AS-28 mini submarine inH from the coreferring it in T , mini submarine inT ?
and AS-28 vehicle in T ??.
A substitution of it byeither coreference does not suffice, since none ofthe antecedents contains all necessary modifiers.
Itis therefore necessary to substitute it first by one ofthe coreferences and then merge it with the other.6 ResultsWe analyzed 120 sentence-hypothesis pairs of theRTE-5 development set (21 different hypotheses,111 distinct sentences, 53 different documents).Below, we summarize our findings, focusing onthe relation between our findings and the assump-tions of previous studies as discussed in Section 3.General statistics.
We found that 44% of thepairs contained reference relations whose resolu-tion was mandatory for inference.
In another 28%,references could optionally support the inferenceof the hypothesis.
In the remaining 28%, refer-ences did not contribute towards inference.
Thetotal number of relevant references was 137, and37 pairs (27%) contained multiple relevant refer-ences.
These numbers support our assumption thatdiscourse references play an important role in in-ference.Reference types.
73% of the identified refer-ences are coreferences and 27% are bridging re-lations.
The most common bridging relation wasthe location of events (e.g.
Arctic in ice meltingevents), generally assumed to be known through-out the document.
Other bridging relations we en-countered include cause (e.g.
between injured andattack), event participants and set membership.1215(%) Pronoun NE NP VPFocus term 9 19 49 23Reference term - 43 43 14Table 2: Syntactic types of discourse references(%) Sub.
Merge InsertionCoreference 62 38 -Bridging 30 - 70Total 54 28 18Table 3: Distribution of transformation typesSyntactic types.
Table 2 shows that 77% of allfocus terms and 86% of the reference terms werenominal phrases, which justifies their prominentposition in work on anaphora and coreference res-olution.
However, almost a quarter of the focusterms were verbal phrases.
We found these focusterms to be frequently crucial for entailment sincethey included the main predicate of the hypothe-sis.6 This calls for an increased focus on the reso-lution of event references.Transformations.
Table 3 shows the relativefrequencies of all transformations.
Again, wefound that the ?default?
transformation, substitu-tion, is the most frequent one, and is helpful forboth coreference and bridging relations.
Substitu-tion is particularly useful for handling pronouns(14% of all substitution instances), the replace-ment of named entities by synonymous names(32%), the replacement of other NPs (38%), andthe substitution of verbal head nodes in eventcoreference (16%).
Yet, in nearly half the cases,a different transformation had to be applied.
In-sertion accounts for the majority of bridging cases.Head-merge is necessary to integrate proper nounsas modifiers of other head nouns.
Dependent-merge, responsible for 85% of the merge transfor-mations, can be used to complete nominal focusterms with missing modifiers (e.g., adjectives), aswell as for merging other dependencies betweencoreferring predicates.
This result indicates theimportance of incorporating other transformationsinto inference systems.Distance of reference terms.
The distance be-tween the focus and the reference terms variedconsiderably, ranging from intra-sentential refer-ence relations and up to several dozen sentences.For more than a quarter of the focus terms, we6The lower proportion of VPs among reference termsstems from bridging relations between VPs and nominal de-pendents, such as the abovementioned ?location?
relation.had to go to other documents to find referenceterms that, possibly in conjunction with the focusterm, could cover the target components.
Interest-ingly, all such cases involved coreference (aboutequally divided between the merge transforma-tions and substitutions), while bridging was al-ways ?document-local?.
This result reaffirms theusefulness of cross-document coreference resolu-tion for inference (Huang et al, 2009).Discourse resolution as preprocessing?
In ex-isting RTE systems, discourse references are typ-ically resolved as a preprocessing step.
Whileour annotation was manual and cannot yield di-rect results about processing considerations, weobserved that discourse relations often hold be-tween complex, and deeply embedded, expres-sions, which makes their automatic resolution dif-ficult.
Of course, many RTE systems attempt tonormalize and simplify H and T , e.g., by split-ting conjunctions or removing irrelevant clauses,but these operations are usually considered a partof the inference rather the preprocessing phase (cf.e.g., Bar-Haim et al (2007)).
Since the resolu-tion of discourse references is likely to profit fromthese steps, it seems desirable to ?postpone?
it un-til after simplification.
In transformation-basedsystems, it might be natural to add discourse-basedtransformations to the set of inference operations,while in alignment-based systems, discourse ref-erences can be integrated into the computation ofalignment scores.Discourse references vs. entailment knowledge.We have stated before that even if a discourse ref-erence is not strictly necessary for entailment, itmay be interesting because it represents an alter-native to the use of knowledge rules to cover thehypothesis.
Sometimes, these rules are generallyapplicable (e.g., ?Alaska?
Arctic?).
However, of-ten they are context-specific.
Consider the follow-ing sentence as T for the hypothesis H: ?The iceis melting in the Arctic?
:(3) T : ?The scene at the receding edge of the ExitGlacier was part festive gathering, part naturetour with an apocalyptic edge.
?While it is possible to cover melting using a rule?melting?
receding?, this rule is only valid underquite specific conditions (e.g., for the subject ice).Instead of determining the applicability of the rule,a discourse-aware system can take the next sen-1216tence into account, which contains a coreferringevent to receding that can cover melting in H:(4) T ?
: ?.
.
.
people moved closer to the rope linenear the glacier as it shied away, practicallygroaning and melting before their eyes.
?Discourse relations can in fact encode arbitrar-ily complex world knowledge, as in the followingpair:(5) H: ?The serial killer BTK was accused of atleast 7 killings starting in the 1970?s.
?T: ?Police say BTK may have killed as manyas 10 people between 1974 and 1991.?Here, the H modifier serial, which does not occurin T , can be covered either by world knowledge(a person who killed 10 people is a serial killer),or by resolving the coreference of BTK to the termthe serial killer BTK which occurs in the discoursearound T .
Our conclusion is that not only candiscourse references often replace world knowl-edge in principle, in practice it often seems easierto resolve discourse references than to determinewhether a rule is applicable in a given context orto formalize complex world knowledge as infer-ence rules.
Our annotation provides further em-pirical support to this claim: An entailment rela-tion exists between the focus and reference termsin 60% of the focus-reference term pairs, and inmany of the remainder, entailment holds betweenthe terms?
heads.
Thus, discourse provides rela-tions which are many times equivalent to entail-ment knowledge rules and can therefore be uti-lized in their stead.7 ConclusionsThis work has presented an analysis of the relationbetween discourse references and textual entail-ment.
We have identified a set of limitations com-mon to the handling of discourse relations in vir-tually all entailment systems.
They include the useof off-the-shelf resolvers that concentrate on nom-inal coreference, the integration of reference in-formation through substitution, and the RTE eval-uation schemes, which played down the role ofdiscourse.
Since in practical settings, discourseplays an important role, our goal was to developan agenda for improving the handling of discoursereferences in entailment-based inference.Our manual analysis of the RTE-5 datasetshows that while the majority of discourse refer-ences that affect inference are nominal coreferencerelations, another substantial part is made up byverbal terms and bridging relations.
Furthermore,we have demonstrated that substitution alone is in-sufficient to extract all relevant information fromthe wide range of discourse references that arefrequently relevant for inference.
We identifiedthree general cases, and suggested matching op-erations to obtain the relevant inferences, formu-lated as tree transformations.
Furthermore, our ev-idence suggests that for practical reasons, the res-olution of discourse references should be tightlyintegrated into entailment systems instead of treat-ing it as a preprocessing step.A particularly interesting result concerns theinterplay between discourse references and en-tailment knowledge.
While semantic knowledge(e.g., from WordNet or Wikipedia) has been usedbeneficially for coreference resolution (Soon et al,2001; Ponzetto and Strube, 2006), reference res-olution has, to our knowledge, not yet been em-ployed to validate entailment rules?
applicability.Our analyses suggest that in the context of de-ciding textual entailment, reference resolution andentailment knowledge can be seen as complemen-tary ways of achieving the same goal, namely en-riching T with additional knowledge to allow theinference of H .
Given that both of the technolo-gies are still imperfect, we envisage the way for-ward as a joint strategy, where reference resolutionand entailment rules mutually fill each other?s gaps(cf.
Example 3).In sum, our study shows that textual entailmentcan profit substantially from better discourse han-dling.
The next challenge is to translate the the-oretical gain into practical benefit.
Our analy-sis demonstrates that improvements are necessaryboth on the side of discourse reference resolutionsystems, which need to cover more types of refer-ences, as well as a better integration of discourseinformation in entailment systems, even for thoserelations which are within the scope of availableresolvers.AcknowledgementsThis work was partially supported by thePASCAL-2 Network of Excellence of the Eu-ropean Community FP7-ICT-2007-1-216886 andthe Israel Science Foundation grant 1112/08.1217ReferencesAzad Abad, Luisa Bentivogli, Ido Dagan, Danilo Gi-ampiccolo, Shachar Mirkin, Emanuele Pianta, andAsher Stern.
2010.
A resource for investigating theimpact of anaphora and coreference on inference.
InProceedings of LREC.Rod Adams, Gabriel Nicolae, Cristina Nicolae, andSanda Harabagiu.
2007.
Textual entailment throughextended lexical overlap and lexico-semantic match-ing.
In Proceedings of the ACL-PASCAL Workshopon Textual Entailment and Paraphrasing.E.
Agichtein, W. Askew, and Y. Liu.
2008.
Combininglexical, syntactic, and semantic evidence for textualentailment classification.
In Proceedings of TAC.Nicholas Asher and Alex Lascarides.
1998.
Bridging.Journal of Semantics, 15(1):83?113.Alexandra Balahur, Elena Lloret, O?scar Ferra?ndez,Andre?s Montoyo, Manuel Palomar, and RafaelMun?oz.
2008.
The DLSIUAES team?s participationin the TAC 2008 tracks.
In Proceedings of TAC.Roy Bar-Haim, Ido Dagan, Iddo Greental, and EyalShnarch.
2007.
Semantic inference at the lexical-syntactic level.
In Proceedings of AAAI.Roy Bar-Haim, Jonathan Berant, Ido Dagan, IddoGreental, Shachar Mirkin, and Eyal Shnarch amdIdan Szpektor.
2008.
Efficient semantic deduc-tion and approximate matching over compact parseforests.
In Proceedings of TAC.Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernardo Magnini.
2009a.
Thefifth pascal recognizing textual entailment chal-lenge.
In Proceedings of TAC.Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, Medea Lo Leggio, and BernardoMagnini.
2009b.
Considering discourse referencesin textual entailment annotation.
In Proceedings ofthe 5th International Conference on Generative Ap-proaches to the Lexicon (GL2009).Johan Bos.
2005.
Recognising textual entailment withlogical inference.
In Proceedings of EMNLP.Aljoscha Burchardt, Marco Pennacchiotti, StefanThater, and Manfred Pinkal.
2009.
Assessingthe impact of frame semantics on textual entail-ment.
Journal of Natural Language Engineering,15(4):527?550.Nathanael Chambers and Dan Jurafsky.
2009.
Unsu-pervised learning of narrative schemas and their par-ticipants.
In Proceedings of ACL-IJCNLP.Nathanael Chambers, Daniel Cer, Trond Grenager,David Hall, Chloe Kiddon, Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage, Eric Yeh,and Christopher D. Manning.
2007.
Learning align-ments and leveraging natural logic.
In Proceedingsof the ACL-PASCAL Workshop on Textual Entail-ment and Paraphrasing.Herbert H. Clark.
1975.
Bridging.
In R. C. Schankand B. L. Nash-Webber, editors, Theoretical issuesin natural language processing, pages 169?174.
As-sociation of Computing Machinery.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entailmentchallenge.
In Machine Learning Challenges, vol-ume 3944 of Lecture Notes in Computer Science,pages 177?190.
Springer.Lorand Dali, Delia Rusu, Blaz Fortuna, DunjaMladenic, and Marko Grobelnik.
2009.
Ques-tion answering based on semantic graphs.
In Pro-ceedings of the Workshop on Semantic Search (Sem-Search 2009).Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database (Language, Speech, andCommunication).
The MIT Press.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third pascal recogniz-ing textual entailment challenge.
In Proceedings ofthe ACL-PASCAL Workshop on Textual Entailmentand Paraphrasing.Danilo Giampiccolo, Hoa Trang Dang, BernardoMagnini, Ido Dagan, and Bill Dolan.
2008.
Thefourth pascal recognizing textual entailment chal-lenge.
In Proceedings of TAC.Ralph Grishman and Beth Sundheim.
1996.
Mes-sage Understanding Conference-6: a brief history.In Proceedings of the 16th conference on Computa-tional Linguistics.Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.2007.
Satisfying information needs with multi-document summaries.
Information Processing &Management, 43:1619?1642.Stefan Harmeling.
2009.
Inferring textual entailmentwith a probabilistically sound calculus.
Journal ofNatural Language Engineering, pages 459?477.Marti A. Hearst.
1997.
Segmenting text into multi-paragraph subtopic passages.
Computational Lin-guistics, 23(1):33?64.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:The 90% solution.
In Proceedings of HLT-NAACL.Jian Huang, Sarah M. Taylor, Jonathan L. Smith, Kon-stantinos A. Fotiadis, and C. Lee Giles.
2009.
Pro-file based cross-document coreference using kernel-ized fuzzy relational clustering.
In Proceedings ofACL-IJCNLP.Fangtao Li, Yang Tang, Minlie Huang, and XiaoyanZhu.
2009.
Answering opinion questions withrandom walks on graphs.
In Proceedings of ACL-IJCNLP.1218Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
Natural Lan-guage Engineering, 4(7):343?360.Dekang Lin.
1993.
Principle-based parsing withoutovergeneration.
In Proceedings of ACL.Bill MacCartney, Michel Galley, and Christopher D.Manning.
2008.
A phrase-based alignment modelfor natural language inference.
In Proceedings ofEMNLP.Katja Markert, Malvina Nissim, and Natalia N. Mod-jeska.
2003.
Using the web for nominal anaphoraresolution.
In Proceedings of EACL Workshop onthe Computational Treatment of Anaphora.Seung-Hoon Na and Hwee Tou Ng.
2009.
A 2-poissonmodel for probabilistic coreference of named enti-ties for improved text retrieval.
In Proceedings ofSIGIR.Rodney D. Nielsen, Wayne Ward, and James H. Mar-tin.
2009.
Recognizing entailment in intelligenttutoring systems.
Natural Language Engineering,15(4):479?501.Massimo Poesio, Rahul Mehta, Axel Maroudas, andJanet Hitzeman.
2004.
Learning to resolve bridgingreferences.
In Proceedings of ACL.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Proceed-ings of HLT.Long Qiu, Min-Yen Kan, and Tat-Seng Chua.
2004.
Apublic reference implementation of the rap anaphoraresolution algorithm.
In Proceedings of LREC.Lorenza Romano, Milen Kouylekov, Idan Szpektor,Ido Dagan, and Alberto Lavelli.
2006.
Investigat-ing a generic paraphrase-based approach for relationextraction.
In Proceedings of EACL.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.Stephanie Strassel, Mark Przybocki, Kay Peterson,Zhiyi Song, and Kazuaki Maeda.
2008.
Linguisticresources and evaluation techniques for evaluationof cross-document automatic content extraction.
InProceedings of LREC.Jose L. Vicedo and Antonio Ferrndez.
2006.
Coref-erence in Q&A.
In Tomek Strzalkowski andSanda M. Harabagiu, editors, Advances in Open Do-main Question Answering, pages 71?96.
Springer.Fabio Massimo Zanzotto, Marco Pennacchiotti, andAlessandro Moschitti.
2009.
A machine learningapproach to textual entailment recognition.
Journalof Natural Language Engineering, 15(4):551?582.Dmitry Zelenko, Chinatsu Aone, and Jason Tibbetts.2004.
Coreference resolution for information ex-traction.
In Proceedings of the ACL Workshop onReference Resolution and its Applications.1219
