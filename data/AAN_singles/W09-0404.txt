Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 37?41,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsTextual Entailment Features for Machine Translation EvaluationSebastian Pado?, Michel Galley, Dan Jurafsky, Christopher D. Manning?Stanford University{pado,mgalley,jurafsky,manning}@stanford.eduAbstractWe present two regression models for the predictionof pairwise preference judgments among MT hy-potheses.
Both models are based on feature sets thatare motivated by textual entailment and incorporatelexical similarity as well as local syntactic featuresand specific semantic phenomena.
One model pre-dicts absolute scores; the other one direct pairwisejudgments.
We find that both models are compet-itive with regression models built over the scoresof established MT evaluation metrics.
Further dataanalysis clarifies the complementary behavior of thetwo feature sets.1 IntroductionAutomatic metrics to assess the quality of machine trans-lations have been a major enabler in improving the per-formance of MT systems, leading to many varied ap-proaches to develop such metrics.
Initially, most metricsjudged the quality of MT hypotheses by token sequencematch (cf.
BLEU (Papineni et al, 2002), NIST (Dod-dington, 2002).
These measures rate systems hypothe-ses by measuring the overlap in surface word sequencesshared between hypothesis and reference translation.With improvements in the state-of-the-art in machinetranslation, the effectiveness of purely surface-orientedmeasures has been questioned (see e.g., Callison-Burchet al (2006)).
In response, metrics have been proposedthat attempt to integrate more linguistic informationinto the matching process to distinguish linguistically li-censed from unwanted variation (Gime?nez andMa`rquez,2008).
However, there is little agreement on what typesof knowledge are helpful: Some suggestions concen-trate on lexical information, e.g., by the integration ofword similarity information as in Meteor (Banerjee andLavie, 2005) or MaxSim (Chan and Ng, 2008).
Otherproposals use structural information such as dependencyedges (Owczarzak et al, 2007).In this paper, we investigate an MT evaluation metricthat is inspired by the similarity between this task andthe textual entailment task (Dagan et al, 2005), which?This paper is based on work funded by the Defense Ad-vanced Research Projects Agency through IBM.
The contentdoes not necessarily reflect the views of the U.S. Government,and no official endorsement should be inferred..HYP: Virus was infected.REF: No one was infected by the virus.no entailmentno entailmentHYP: The virus did not infect anybody.REF: No one was infected by the virus.entailmententailmentFigure 1: Entailment status between an MT system hy-pothesis and a reference translation for good translations(above) and bad translations (below).suggests that the quality of an MT hypothesis should bepredictable by a combination of lexical and structuralfeatures that model the matches and mismatches be-tween system output and reference translation.
We usesupervised regression models to combine these featuresand analyze feature weights to obtain further insightsinto the usefulness of different feature types.2 Textual Entailment for MT Evaluation2.1 Textual Entailment vs. MT EvaluationTextual entailment (TE) was introduced by Dagan etal.
(2005) as a concept that corresponds more closelyto ?common sense?
reasoning than classical, categoricalentailment.
Textual entailment is defined as a relationbetween two natural language sentences (a premise Pand a hypothesis H) that holds if a human reading Pwould infer that H is most likely true.Information about the presence or absence of entail-ment between two sentences has been found to be ben-eficial for a range of NLP tasks such as Word SenseDisambiguation or Question Answering (Dagan et al,2006; Harabagiu and Hickl, 2006).
Our intuition is thatthis idea can also be fruitful in MT Evaluation, as illus-trated in Figure 1.
Very good MT output should entailthe reference translation.
In contrast, missing hypothesismaterial breaks forward entailment; additional materialbreaks backward entailment; and for bad translations,entailment fails in both directions.Work on the recognition of textual entailment (RTE)has consistently found that the integration of more syn-tactic and semantic knowledge can yield gains over37surface-based methods, provided that the linguistic anal-ysis was sufficiently robust.
Thus, for RTE, ?deep?matching outperforms surface matching.
The reason isthat linguistic representation makes it considerably eas-ier to distinguish admissible variation (i.e., paraphrase)from true, meaning-changing divergence.
Admissiblevariation may be lexical (synonymy), structural (wordand phrase placement), or both (diathesis alternations).The working hypothesis of this paper is that the ben-efits of deeper analysis carry over to MT evaluation.More specifically, we test whether the features that al-low good performance on the RTE task can also predicthuman judgments for MT output.
Analogously to RTE,these features should help us to differentiate meaningpreserving translation variants from bad translations.Nevertheless, there are also substantial differencesbetween TE and MT evaluation.
Crucially, TE assumesthe premise and hypothesis to be well-formed sentences,which is not true in MT evaluation.
Thus, a possible crit-icism to the use of TE methods is that the features couldbecome unreliable for ill-formed MT output.
However,there is a second difference between the tasks that worksto our advantage.
Due to its strict compositional nature,TE requires an accurate semantic analysis of all sentenceparts, since, for example, one misanalysed negation orcounterfactual embedding can invert the entailment sta-tus (MacCartney and Manning, 2008).
In contrast, hu-man MT judgments behave more additively: failure of atranslation with respect to a single semantic dimension(e.g., polarity or tense) degrades its quality, but usuallynot crucially so.
We therefore expect that even noisyentailment features can be predictive in MT evaluation.2.2 Entailment-based prediction of MT qualityRegression-based prediction.
Experiences from theannotation of MT quality judgments show that humanraters have difficulty in consistently assigning absolutescores to MT system output, due to the number of waysin which MT output can deviate.
Thus, the human an-notation for the WMT 2008 dataset was collected inthe form of binary pairwise preferences that are con-siderably easier to make (Callison-Burch et al, 2008).This section presents two models for the prediction ofpairwise preferences.The first model (ABS) is a regularized linear regres-sion model over entailment-motivated features (see be-low) that predicts an absolute score for each reference-hypothesis pair.
Pairwise preferences are created simplyby comparing the absolute predicted scores.
This modelis more general, since it can also be used where absolutescore predictions are desirable; furthermore, the modelis efficient with a runtime linear in the number of sys-tems and corpus size.
On the downside, this model isnot optimized for the prediction of pairwise judgments.The second model we consider is a regularized logis-tic regression model (PAIR) that is directly optimized topredict a weighted binary preference for each hypothe-sis pair.
This model is less efficient since its runtime isAlignment score(3) Unaligned material (10)Adjuncts (7) Apposition (2)Modality (5) Factives (8)Polarity (5) Quantors (4)Tense (2) Dates (6)Root (2) Semantic Relations (4)Semantic relatedness (7) Structural Match (5)Compatibility of locations and entities (4)Table 1: Entailment feature groups provided by theStanford RTE system, with number of featuresquadratic in the number of systems.
On the other hand,it can be trained on more reliable pairwise preferencejudgments.
In a second step, we combine the individ-ual decisions to compute the highest-likelihood totalordering of hypotheses.
The construction of an optimalordering from weighted pairwise preferences is an NP-hard problem (via reduction of CYCLIC-ORDERING;Barzilay and Elhadad, 2002), but a greedy search yieldsa close approximation (Cohen et al, 1999).Both models can be used to predict system-levelscores from sentence-level scores.
Again, we have twomethod for doing this.
The basic method (BASIC) pre-dicts the quality of each system directly as the percent-age of sentences for which its output was rated bestamong all systems.
However, we noticed that the man-ual rankings for the WMT 2007 dataset show a tie forbest system for almost 30% of sentences.
BASIC issystematically unable to account for these ties.
Wetherefore implemented a ?tie-aware?
prediction method(WITHTIES) that uses the same sentence-level output asBASIC, but computes system-level quality differently,as the percentage of sentences where the system?s hy-pothesis was scored better or at most ?
worse than thebest system, for some global ?tie interval?
?
.Features.
We use the Stanford RTE system (MacCart-ney et al, 2006) to generate a set of entailment features(RTE) for each pair of MT hypothesis and referencetranslation.
Features are generated in both directionsto avoid biases towards short or long translations.
TheStanford RTE system uses a three-stage architecture.It (a) constructs a robust, dependency-based linguisticanalysis of the two sentences; (b) identifies the bestalignment between the two dependency graphs givensimilarity scores from a range of lexical resources, us-ing a Markov Chain Monte Carlo sampling strategy;and (c) computes roughly 75 features over the alignedpair of dependency graphs.
The different feature groupsare shown in Table 1.
A small number features arereal-valued, measuring different quality aspects of thealignment.
The other features are binary, indicatingmatches and mismatches of different types (e.g., align-ment between predicates embedded under compatibleor incompatible modals, respectively).To judge to what extent the entailment-based modeldelivers improvements that cannot be obtained with es-tablished methods, we also experiment with a feature set38formed from a set of established MT evaluation metrics(TRADMT).
We combine different parametrization of(smoothed) BLEU (Papineni et al, 2002), NIST (Dod-dington, 2002), and TER (Snover et al, 2006), to givea total of roughly 100 features.
Finally, we consider acombination of both feature sets (COMB).3 Experimental EvaluationSetup.
To assess and compare the performance of ourmodels, we use corpora that were created by past in-stances of the WMT workshop.
We optimize the featureweights for the ABS models on the WMT 2006 and2007 absolute score annotations, and correspondinglyfor the PAIR models on the WMT 2007 absolute scoreand ranking annotations.
All models are evaluated onWMT 2008 to compare against the published results.Finally, we need to set the tie interval ?
.
Since wedid not want to optimize ?
, we simply assumed that thepercentage of ties observed on WMT 2007 generalizesto test sets such as the 2008 dataset.
We set ?
so thatthere are ties for first place on 30% of the sentences,with good practical success (see below).Results.
Table 2 shows our results.
The first resultscolumn (Cons) shows consistency, i.e., accuracy in pre-dicting human pairwise preference judgments.
Note thatthe performance of a random baseline is not at 50%, butsubstantially lower.
This is due to (a) the presence ofcontradictions and ties in the human judgments, whichcannot be predicted; and (b) WMT?s requirement tocompute a total ordering of all translations for a givensentence (rather than independent binary judgments),which introduces transitivity constraints.
See Callison-Burch et al (2008) for details.
Among our models, PAIRshows a somewhat better consistency than ABS, as canbe expected from a model directly optimized on pair-wise judgments.
Across feature sets, COMB works bestwith a consistency of 0.53, competitive with publishedWMT 2008 results.The two final columns (BASIC and WITHTIES) showSpearman?s ?
for the correlation between human judg-ments and the two types of system-level predictions.For BASIC system-level predictions, we find thatPAIR performs considerably worse than ABS, by a mar-gin of up to ?
= 0.1.
Recall that the system-level analy-sis considers only the top-ranked hypotheses; apparently,a model optimized on pairwise judgments has a hardertime choosing the best among the top-ranked hypothe-ses.
This interpretation is supported by the large benefitthat PAIR derives from explicit tie modeling.
ABS gainsas well, although not as much, so that the correlation ofthe tie-aware predictions is similar for ABS and PAIR.Comparing different feature sets, BASIC show a simi-lar pattern to the consistency figures.
There is no clearwinner between RTE and TRADMT.
The performanceof TRADMT is considerably better than the performanceof BLEU and TER in the WMT 2008 evaluation, where?
?
0.55.
RTE is able to match the performance of anModel Feature set Cons(Acc.)BASIC(?)WITHTIES(?
)ABS TRADMT 0.50 0.74 0.74ABS RTE 0.51 0.72 0.78ABS COMB 0.51 0.74 0.74PAIR TRADMT 0.52 0.63 0.73PAIR RTE 0.51 0.66 0.77PAIR COMB 0.53 0.70 0.77WMT 2008 (worst) 0.44 0.37WMT 2008 (best) 0.56 0.83Table 2: Evaluation on the WMT 2008 dataset for ourregression models, compared to results fromWMT 2008ensemble of state-of-the-art metrics, which validates ourhope that linguistically motivated entailment featuresare sufficiently robust to make a positive contributionin MT evaluation.
Furthermore, the two individual fea-ture sets are outperformed by the combined feature setCOMB.
We interpret this as support for our regression-based combination approach.Moving to WITHTIES, we see the best results fromthe RTE model which improves by ??
= 0.06 for ABSand ??
= 0.11 for PAIR.
There is less improvement forthe other feature sets, in particular COMB.
We submittedthe two overall best models, ABS-RTE and PAIR-RTEwith tie-aware prediction, to the WMT 2009 challenge.Data Analysis.
We analyzed at the models?
predic-tions to gain a better understanding of the differences inthe behavior of TRADMT-based and RTE-based mod-els.
As a first step, we computed consistency numbersfor the set of ?top?
translations (hypotheses that wereranked highest for a given reference) and for the setof ?bottom?
translations (hypotheses that were rankedworst for a given reference).
We found small but con-sistent differences between the models: RTE performsabout 1.5 percent better on the top hypotheses than onthe bottom translations.
We found the inverse effect forthe TRADMT model, which performs 2 points worse onthe top hypotheses than on the bottom hypotheses.
Re-visiting our initial concern that the entailment featuresare too noisy for very bad translations, this finding indi-cates some ungrammaticality-induced degradation forthe entailment features, but not much.
Conversely, thesenumbers also provide support for our initial hypothesisthat surface-based features are good at detecting verydeviant translations, but can have trouble dealing withlegitimate linguistic variation.Next, we analyzed the average size of the score dif-ferences between the best and second-best hypothesesfor correct and incorrect predictions.
We found that theRTE-based model predicted on average almost twice thedifference for correct predictions (?
= 0.30) than forincorrect predictions (?
= 0.16), while the differencewas considerably smaller for the TRADMT-based model(?
= 0.17 for correct vs. ?
= 0.13 for incorrect).
Webelieve it is this better discrimination on the top hypothe-39Segment TRADMT RTE COMB GoldREF: Scottish NHS boards need to improve criminal records checks foremployees outside Europe, a watchdog has said.HYP: The Scottish health ministry should improve the controls on extra-community employees to check whether they have criminal precedents,said the monitoring committee.
[1357, lium-systran]Rank: 3 Rank: 1 Rank: 2 Rank: 1REF: Arguments, bullying and fights between the pupils have extendedto the relations between their parents.HYP: Disputes, chicane and fights between the pupils transposed inrelations between the parents.
[686, rbmt4]Rank: 5 Rank: 2 Rank: 4 Rank: 5Table 3: Examples of reference translations and MT output from the WMT 2008 French-English News dataset.Rank judgments are out of five (smaller is better).ses that explains the increased benefit the RTE-basedmodel obtains from tie-aware predictions: if the besthypothesis is wrong, chances are much better than forthe TRADMT-based model that counting the second-best hypothesis as ?best?
is correct.
Unfortunately, thisproperty is not shared by COMB to the same degree, andit does not improve as much as RTE.Table 3 illustrates the difference between RTE andTRADMT.
In the first example, RTE makes a more ac-curate prediction than TRADMT.
The human rater?sfavorite translation deviates considerably from the ref-erence translation in lexical choice, syntactic structure,and word order, for which it is punished by TRADMT.In contrast, RTE determines correctly that the propo-sitional content of the reference is almost completelypreserved.
The prediction of COMB is between the twoextremes.
The second example shows a sentence whereRTE provides a worse prediction.
This sentence wasrated as bad by the judge, presumably due to the inap-propriate translation of the main verb.
This problem,together with the reformulation of the subject, leadsTRADMT to correctly predict a low score (rank 5/5).RTE?s deeper analysis comes up with a high score (rank2/5), based on the existing semantic overlap.
The com-bined model is closer to the truth, predicting rank 4.Feature Weights.
Finally, we assessed the impor-tance of the different entailment feature groups in theRTE model.1 Since the presence of correlated featuresmakes the weights difficult to interpret, we restrict our-selves to two general observations.First, we find high weights not only for the score ofthe alignment between hypothesis and reference, butalso for a number of syntacto-semantic match and mis-match features.
This means that we do get an additionalbenefit from the presence of these features.
For example,features with a negative effect include dropping adjuncts,unaligned root nodes, incompatible modality betweenthe main clauses, person and location mismatches (asopposed to general mismatches) and wrongly handledpassives.
Conversely, some factors that increase theprediction are good alignment, matching embeddingsunder factive verbs, and matches between appositions.1The feature weights are similar for the COMB model.Second, we find clear differences in the usefulnessof feature groups between MT evaluation and the RTEtask.
Some of them, in particular structural features,can be linked to the generally lower grammaticality ofMT hypotheses.
A case in point is a feature that firesfor mismatches between dependents of predicates andwhich is too unreliable on the SMT data.
Other differ-ences simply reflect that the two tasks have differentprofiles, as sketched in Section 2.1.
RTE exhibits highfeature weights for quantifier and polarity features, bothof which have the potential to influence entailment deci-sions, but are relatively unimportant for MT evaluation,at least at the current state of the art.4 ConclusionIn this paper, we have investigated an approach to MTevaluation that is inspired by the similarity betweenthis task and textual entailment.
Our two models ?
onepredicting absolute scores and one predicting pairwisepreference judgments ?
use entailment features to pre-dict the quality of MT hypotheses, thus replacing sur-face matching with syntacto-semantic matching.
Bothmodels perform similarly, showing sufficient robustnessand coverage to attain comparable performance to acommittee of established MT evaluation metrics.We have described two refinements: (1) combiningthe features into a superior joint model; and (2) adding aconfidence interval around the best hypothesis to modelties for first place.
Both strategies improve correlation;however, unfortunately the benefits do not currentlycombine.
Our feature weight analysis indicates thatsyntacto-semantic features do play an important role inscore prediction in the RTE model.
We plan to assessthe additional benefit of the full entailment feature setagainst the TRADMT feature set extended by a properlexical similarity metric, such as METEOR.The computation of entailment features is moreheavyweight than traditional MT evaluation metrics.We found the speed (about 6 s per hypothesis on a cur-rent PC) to be sufficient for easily judging the quality ofdatasets of the size conventionally used for MT evalua-tion.
However, this may still be too expensive as part ofan MT model that directly optimizes some performancemeasure, e.g., minimum error rate training (Och, 2003).40ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Translationand Summarization, pages 65?72, Ann Arbor, MI.R.
Barzilay and N. Elhadad.
2002.
Inferring strategiesfor sentence ordering in multidocument news summa-rization.
Journal of Artificial Intelligence Research,17:35?55.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of BLEU in ma-chine translation research.
In Proceedings of EACL,pages 249?256, Trento, Italy.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
Furthermeta-evaluation of machine translation.
In Proceed-ings of the ACL Workshop on Statistical MachineTranslation, pages 70?106, Columbus, OH.Yee Seng Chan and Hwee Tou Ng.
2008.
MAXSIM: Amaximum similarity metric for machine translationevaluation.
In Proceedings of ACL-08: HLT, pages55?62, Columbus, Ohio.William W. Cohen, Robert E. Schapire, and YoramSinger.
1999.
Learning to order things.
Journalof Artificial Intelligence Research, 10:243?270.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The PASCAL recognising textual entailmentchallenge.
In Proceedings of the PASCAL Chal-lenges Workshop on Recognising Textual Entailment,Southampton, UK.Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-morshtein, and Carlo Strapparava.
2006.
Direct wordsense matching for lexical substitution.
In Proceed-ings of ACL, Sydney, Australia.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram cooccurrencestatistics.
In Proceedings of HLT, pages 128?132,San Diego, CA.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2008.
A smorgas-bord of features for automatic MT evaluation.
InProceedings of the Third Workshop on Statistical Ma-chine Translation, pages 195?198, Columbus, Ohio.Sanda Harabagiu and Andrew Hickl.
2006.
Methodsfor using textual entailment in open-domain questionanswering.
In Proceedings of ACL, pages 905?912,Sydney, Australia.Bill MacCartney and Christopher D. Manning.
2008.Modeling semantic containment and exclusion in nat-ural language inference.
In Proceedings of Coling,pages 521?528, Manchester, UK.Bill MacCartney, Trond Grenager, Marie-Catherinede Marneffe, Daniel Cer, and Christopher D. Man-ning.
2006.
Learning to recognize features of validtextual entailments.
In Proceedings of NAACL, pages41?48, New York City, NY.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL, pages 160?167, Sapporo, Japan.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007.
Dependency-based automatic evalu-ation for machine translation.
In Proceedings ofthe NAACL-HLT / AMTA Workshop on Syntax andStructure in Statistical Translation, pages 80?87,Rochester, NY.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofACL, pages 311?318, Philadelphia, PA.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of AMTA, pages 223?231, Cambridge,MA.41
