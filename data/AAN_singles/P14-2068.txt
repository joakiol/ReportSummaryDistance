Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 415?420,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsModeling Factuality Judgments in Social Media TextSandeep Soni Tanushree Mitra Eric Gilbert Jacob EisensteinSchool of Interactive ComputingGeorgia Institute of Technologysoni.sandeepb@gmail.com, {tmitra3,gilbert,jeisenst}@cc.gatech.eduAbstractHow do journalists mark quoted contentas certain or uncertain, and how do read-ers interpret these signals?
Predicates suchas thinks, claims, and admits offer a rangeof options for framing quoted content ac-cording to the author?s own perceptions ofits credibility.
We gather a new datasetof direct and indirect quotes from Twit-ter, and obtain annotations of the perceivedcertainty of the quoted statements.
Wethen compare the ability of linguistic andextra-linguistic features to predict readers?assessment of the certainty of quoted con-tent.
We see that readers are indeed influ-enced by such framing devices ?
and wefind no evidence that they consider otherfactors, such as the source, journalist, orthe content itself.
In addition, we examinethe impact of specific framing devices onperceptions of credibility.1 IntroductionContemporary journalism is increasingly con-ducted through social media services like Twit-ter (Lotan et al, 2011; Hermida et al, 2012).
Asevents unfold, journalists and political commen-tators use quotes ?
often indirect ?
to conveypotentially uncertain information and claims fromtheir sources and informants, e.g.,Figure 1: Indirect quotations in TwitterA key pragmatic goal of such messages is toconvey the provenance and uncertainty of thequoted content.
In some cases, the author may alsointroduce their own perspective (Lin et al, 2006)through the use of framing (Greene and Resnik,2009).
For instance, consider the use of the wordclaims in Figure 1, which conveys the author?sdoubt about the indirectly quoted content.Detecting and reasoning about the certainty ofpropositional content has been identified as a keytask for information extraction, and is now sup-ported by the FactBank corpus of annotations fornewstext (Saur??
and Pustejovsky, 2009).
However,less is known about this phenomenon in socialmedia ?
a domain whose endemic uncertaintymakes proper treatment of factuality even morecrucial (Morris et al, 2012).
Successful automa-tion of factuality judgments could help to detectonline rumors (Qazvinian et al, 2011), and mightenable new applications, such as the computationof reliability ratings for ongoing stories.This paper investigates how linguistic resourcesand extra-linguistic factors affect perceptions ofthe certainty of quoted information in Twitter.
Wepresent a new dataset of Twitter messages that useFactBank predicates (e.g., claim, say, insist) toscope the claims of named entity sources.
Thisdataset was annotated by Mechanical Turk work-ers who gave ratings for the factuality of thescoped claims in each Twitter message.
This en-ables us to build a predictive model of the fac-tuality annotations, with the goal of determiningthe full set of relevant factors, including the pred-icate, the source, the journalist, and the contentof the claim itself.
However, we find that theseextra-linguistic factors do not predict readers?
fac-tuality judgments, suggesting that the journalist?sown framing plays a decisive role in the cred-ibility of the information being conveyed.
Weexplore the specific linguistic feature that affectfactuality judgments, and compare our findingswith previously-proposed groupings of factuality-related predicates.415say tell thinkaccordsuggest claim believ admit ask predictreportexplain denihopelearninsisthearwonder feel statediscovforgetassertguessobservmaintaindoubtCue Words050100150CountsFigure 2: Count of cue words in our dataset.
Eachword is patterned according to its group, as shownin Figure 3.Report BeliefKnowledgeDoubtPerceptionCue Groups0100200300400500600CountsFigure 3: Count of cue groups in our dataset2 Text dataWe gathered a dataset of Twitter messages from103 professional journalists and bloggers whowork in the field of American Politics.1Tweetswere gathered using Twitter?s streaming API, ex-tracting the complete permissible timeline up toFebruary 23, 2014.
A total of 959,754 tweets weregathered, and most were written in early 2014.Our interest in this text is specifically in quotedcontent ?
including ?indirect?
quotes, which mayinclude paraphrased quotations, as in the examplesin Figure 1.
While labeled datasets for such quoteshave been created (O?Keefe et al, 2012; Pareti,2012), these are not freely available at present.
Inany case, the relevance of these datasets to Twittertext is currently unproven.
Therefore, rather thantrain a supervised model to detect quotations, weapply a simple dependency-based heuristic.?
We focus on tweets that contain any member ofa list of source-introducing predicates (we bor-row the terminology of Pareti (2012) and callthis the CUE).
Our complete list ?
shown inTable 1 ?
was selected mainly from the exam-ples presented by Saur??
and Pustejovsky (2012),1We used the website http://muckrack.com.Report say, report, tell, told, observe, state,accord, insist, assert, claim, main-tain, explain, denyKnowledge learn, admit, discover, forget, forgotBelief think, thought, predict, suggest,guess, believeDoubt doubt, wonder, ask, hopePerception sense, hear, feelTable 1: Lemmas of source-introducing predicates(cues) and groups (Saur?
?, 2008).but with reference also to Saur??
?s (2008) dis-sertation for cues that are common in Twitter.The Porter Stemmer is applied to match inflec-tions, e.g.
denies/denied; for irregular casesnot handled by the Porter Stemmer (e.g., for-get/forgot), we include both forms.
We use theCMU Twitter Part-of-Speech Tagger (Owoputiet al, 2013) to select only instances in the verbsense.
Figure 2 shows the distribution of thecues and Figure 3 shows the distribution of thecue groups.
For cues that appear in multiplegroups, we chose the most common group.?
We run the Stanford Dependency parser toobtain labeled dependencies (De Marneffe etal., 2006), requiring that the cue has outgoingedges of the type NSUBJ (noun subject) andCCOMP (clausal complement).
The subtreeheaded by the modifier of the CCOMP relationis considered the claim; the subtree headed bythe modifier of the NSUBJ relation is consid-ered the source.
See Figure 4 for an example.?
We use a combination of regular expressionsand dependency rules to capture expressionsof the type ?CLAIM, according to SOURCE.
?Specifically, the PCOMP path from accordingis searched for the pattern according to*.The text that matches the * is the source and theremaining text other than the source is taken asthe claim.?
Finally, we restrict consideration to tweets inwhich the source contains a named entity ortwitter username.
This eliminates expressionsof personal belief such as I doubt Obama willwin, as well as anonymous sources such asTeam sources report that Lebron has demandeda trade to New York.
Investigating the factual-ity judgments formed in response to such tweetsis clearly an important problem for future re-search, but is outside the scope of this paper.This heuristic pipeline may miss many relevanttweets, but since the overall volume is high, we416Source Cue ClaimI guess, since FBI claims it couldn?t match Tsarnaev, we can assume ...nsubjmarkccompnsubjaux+neg dobjFigure 4: Dependency parse of an example message, with claim, source, and cue.Total journalists 443Total U.S. political journalists 103Total tweets 959754Tweets with cues 172706Tweets with source and claims 40615Total tweets annotated 1265Unique sources in annotated dataset 766Unigrams in annotated dataset 1345Table 2: Count Statistics of the entire data col-lected and the annotated datasetFigure 5: Turk annotation interfaceprioritize precision.
The resulting dataset is sum-marized in Table 2.3 AnnotationWe used Amazon Mechanical Turk (AMT) to col-lect ratings of claims.
AMT has been widely usedby the NLP community to collect data (Snow etal., 2008), with ?best practices?
defined to helprequesters best design Turk jobs (Callison-Burchand Dredze, 2010).
We followed these guidelinesto perform pilot experiments to test the instructionset and the quality of responses.
Based on the pi-lot study we designed Human Intelligence Tasks(HITs) to annotate 1265 claims.Each HIT contained a batch of ten tweets andrewarded $0.10 per hit.
To ensure quality con-trol we required the Turkers to have at least 85%hit approval rating and to reside in the UnitedStates, because the Twitter messages in our datasetwere related to American politics.
For each tweet,we obtained five independent ratings from Turk-ers satisfying the above qualifications.
The rat-ings were based on a 5-point Likert scale rang-ing from ?
[-2] Certainly False?
to ?
[2] CertainlyTrue?
and allowing for ?
[0] Uncertain?.
We alsoallowed for ?Not Applicable?
option to captureratings where the Turkers did not have sufficientknowledge about the statement or if the statementwas not really a claim.
Figure 6 shows the set ofinstructions provided to the Turkers, and Figure 5illustrates the annotation interface.2We excluded tweets for which three or moreTurkers gave a rating of ?Not Applicable,?
leavingus with a dataset of 1170 tweets.
Within this set,the average variance per tweet (excluding ?NotApplicable?
ratings) was 0.585.4 Modeling factuality judgmentsHaving obtained a corpus of factuality ratings, wenow model the factors that drive these ratings.4.1 Predictive accuracyFirst, we attempt to determine the impact of vari-ous predictive features on rater judgments of fac-tuality.
We consider the following features:?
Cue word: after stemming?
Cue word group: as given in Table 1?
Source: represented by the named entity orusername in the source field (see Figure 4)?
Journalist: represented by their Twitter ID?
Claim: represented by a bag-of-words vectorfrom the claim field (Figure 4)These features are used as predictors in a seriesof linear ridge regressions, where the dependentvariable is the mean certainty rating.
We throwout tweets that were rated as ?not applicable?
by amajority of raters, but otherwise ignore ?not appli-cable?
ratings of the remaining tweets.
The goalof these regressions is to determine which fea-tures are predictive of raters?
factuality judgments.The ridge regression regularization parameter wastuned via cross-validation in the training set.
Weused the bootstrap to obtain multiple training/test2The data is available at https://www.github.com/jacobeisenstein/twitter-certainty.417Figure 6: User instructions for the annotation taskFeatures ErrorBaseline .442Cue word .404*Cue word group .42Source .447Journalist .444Claim .476Cue word + cue word group .404*All features .420Table 3: Linear regression error rates for each fea-ture group.
* indicates improvement over the base-line at p < .05.splits (70% training), which were used for signifi-cance testing.Table 3 reports mean average error for each fea-ture group, as well as a baseline that simply re-ports the mean rating across the training set.
Eachaccuracy was compared with the baseline using apaired z-test.
Only the cue word features pass thistest at p < .05.
The other features do not help,even in combination with the cue word.While these findings must be interpreted withcaution, they suggest that readers ?
at least, Me-chanical Turk workers ?
use relatively little inde-pendent judgment to assess the validity of quotedtext that they encounter on Twitter.
Of course,richer linguistic models, more advanced machinelearning, or experiments with more carefully-selected readers might offer a different view.
Butthe results at hand are most compatible with theconclusion that readers base their assessments offactuality only on the framing provided by thejournalist who reports the quote.4.2 Cue words and cue groupsGiven the importance of cue words as a sig-nal for factuality, we want to assess the factual-ity judgments induced by each cue.
A secondquestion is whether proposed groupings of cuewords into groups cohere with such perceptions.Saur??
(2008) describes several classes of source-introducing predicates, which indicate how thesource relates to the quoted claim.
These classesare summarized in Table 1, along with frequently-occuring cues from our corpus.
We rely on Fact-Bank to assign the cue words to classes; the onlyword not covered by FactBank was sense, whichwe placed in predicates of perception.We performed another set of linear regressions,again using the mean certainty rating as the de-pendent variable.
In this case, there was no train-ing/test split, so confidence intervals on the result-ing parameters are computed using the analyticclosed form.
We performed two such regressions:first using only the individual cues as predictors,and then using only the cue groups.
Results areshown in Figures 7 and 8; Figure 7 includes onlycues which appear at least ten times, although allcues were included in the regression.The cues that give the highest factuality coef-ficients are learn and admit, which are labeled aspredicates of knowledge.
These cues carry a sub-stantial amount of framing, as they purport to de-scribe the private mental state of the source.
Theword admit often applies to statements that areperceived as damaging to the source, such as BillGates admits Control-Alt-Delete was a mistake;since there can be no self-interest behind suchstatements, they may be perceived as more likelyto be true.Several of the cues with the lowest factuality co-efficients are predicates of belief: suggest, predictand think.
The words suggest, think, and believealso purport to describe the private mental state ofthe source, but their framing function is the op-posite of the predicates of knowledge: they im-ply that it is important to mark the claim as thesource?s belief, and not a widely-accepted fact.For example, Mubarak clearly believes he has themilitary leadership?s support.A third group of interest are the predicates ofreport, which have widely-varying certainty coef-ficients.
The cues according, report, say, and tell418learnadmitaccord report hear say tellexplain ask insistbeliev claimsuggestpredict hopethink deniCue Words0.00.20.40.60.81.01.21.4Coefficients' EstimatesFigure 7: Linear regression coefficients forfrequently-occurring cue words.
Each word is pat-terned according to its group, shown in Figure 8.KnowledgePerception ReportDoubtBeliefCue Groups0.00.20.40.60.81.01.21.4Coefficients' EstimatesFigure 8: Linear regression coefficients for cueword group.are strongly predictive of certainty, but the cuesclaim and deny convey uncertainty.
Both accord-ing and report are often used in conjunction withimpersonal and institutional sources, e.g., Cuc-cinelli trails McAuliffe by 24 points , according toa new poll.
In contrast, insist, claim, and deny im-ply that there is uncertainty about the quoted state-ment, e.g., Christie insists that Fort Lee Mayorwas never on my radar.
In this case, the fact thatthe predicate indicates a report is not enough todetermine the framing: different sorts of reportscarry radically different perceptions of factuality.5 Related workFactuality and Veridicality The creation ofFactBank (Saur??
and Pustejovsky, 2009) has en-abled recent work on the factuality (or ?veridical-ity?)
of event mentions in text.
Saur??
and Puste-jovsky (2012) propose a two-dimensional factual-ity annotation scheme, including polarity and cer-tainty; they then build a classifier to predict an-notations of factuality from statements in Fact-Bank.
Their work on source-introducing predi-cates provides part of the foundation for this re-search, which focuses on quoted statements in so-cial media text.
de Marneffe et al (2012) conductan empirical evaluation of FactBank ratings fromMechanical Turk workers, finding a high degree ofdisagreement between raters.
They also constructa statistical model to predict these ratings.
We areunaware of prior work comparing the contributionof linguistic and extra-linguistic predictors (e.g.,source and journalist features) for factuality rat-ings.
This prior work also does not measure theimpact of individual cues and cue classes on as-sessment of factuality.Credibility in social media Recent work in thearea of computational social science focuses onunderstanding credibility cues on Twitter.
Suchstudies have found that users express concern overthe credibility of tweets belonging to certain topics(politics, news, emergency).
By manipulating sev-eral features of a tweet, Morris et al (2012) foundthat in addition to content, users often use addi-tional markers while assessing the tweet credibil-ity, such as the user name of the source.
The searchfor reliable signals of information credibility in so-cial media has led to the construction of automaticclassifiers to identify credible tweets (Castillo etal., 2011).
However, this prior work has not ex-plored the linguistic basis of factuality judgments,which we show to depend on framing devices suchas cue words.6 ConclusionPerceptions of the factuality of quoted content areinfluenced by the cue words used to introducethem, while extra-linguistic factors, such as thesource and the author, did not appear to be rele-vant in our experiments.
This result is obtainedfrom real tweets written by journalists; a naturalcounterpart study would be to experimentally ma-nipulate this framing to see if the same perceptionsapply.
Another future direction would be to testwhether the deployment of cue words as framingdevices reflects the ideology of the journalist.
Weare also interested to group multiple instances ofthe same quote (Leskovec et al, 2009), and exam-ine how its framing varies across different newsoutlets and over time.Acknowledgments: This research was supportedby DARPA-W911NF-12-1-0043 and by a Compu-tational Journalism research award from Google.We thank the reviewers for their helpful feedback.419ReferencesChris Callison-Burch and Mark Dredze.
2010.
Cre-ating speech and language data with amazon?s me-chanical turk.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, pages 1?12.Association for Computational Linguistics.Carlos Castillo, Marcelo Mendoza, and BarbaraPoblete.
2011.
Information credibility on twitter.In Proceedings of the 20th international conferenceon World wide web, pages 675?684.
ACM.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al 2006.
Generat-ing typed dependency parses from phrase structureparses.
In Proceedings of LREC, volume 6, pages449?454.Marie C. de Marneffe, Christopher D. Manning, andChristopher Potts.
2012.
Did it happen?
the prag-matic complexity of veridicality assessment.
Com-put.
Linguist., 38(2):301?333, June.Stephan Greene and Philip Resnik.
2009.
More thanwords: Syntactic packaging and implicit sentiment.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 503?511, Boulder, Colorado, June.Association for Computational Linguistics.Alfred Hermida, Seth C Lewis, and Rodrigo Zamith.2012.
Sourcing the arab spring: A case study ofandy carvins sources during the tunisian and egyp-tian revolutions.
In international symposium on on-line journalism, Austin, TX, April, pages 20?21.Jure Leskovec, Lars Backstrom, and Jon Kleinberg.2009.
Meme-tracking and the dynamics of the newscycle.
In Proceedings of the 15th ACM SIGKDD in-ternational conference on Knowledge discovery anddata mining, pages 497?506.
ACM.Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, andAlexander Hauptmann.
2006.
Which side are youon?
: Identifying perspectives at the document andsentence levels.
In Proceedings of the Tenth Con-ference on Computational Natural Language Learn-ing, CoNLL-X ?06, pages 109?116, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Gilad Lotan, Erhardt Graeff, Mike Ananny, DevinGaffney, Ian Pearce, et al 2011.
The arab spring?the revolutions were tweeted: Information flows dur-ing the 2011 tunisian and egyptian revolutions.
In-ternational Journal of Communication, 5:31.Meredith Ringel Morris, Scott Counts, Asta Roseway,Aaron Hoff, and Julia Schwarz.
2012.
Tweetingis believing?
: understanding microblog credibilityperceptions.
In Proceedings of the ACM 2012 con-ference on Computer Supported Cooperative Work,pages 441?450.
ACM.Tim O?Keefe, Silvia Pareti, James R Curran, Irena Ko-prinska, and Matthew Honnibal.
2012.
A sequencelabelling approach to quote attribution.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 790?799.
Association for Computational Linguistics.Olutobi Owoputi, Brendan OConnor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL-HLT, pages 380?390.Silvia Pareti.
2012.
A database of attribution relations.In LREC, pages 3213?3217.Vahed Qazvinian, Emily Rosengren, Dragomir RRadev, and Qiaozhu Mei.
2011.
Rumor has it:Identifying misinformation in microblogs.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 1589?1599.Association for Computational Linguistics.Roser Saur??
and James Pustejovsky.
2009.
Factbank:A corpus annotated with event factuality.
LanguageResources and Evaluation, 43(3):227?268.Roser Saur??
and James Pustejovsky.
2012.
Are yousure that this happened?
assessing the factuality de-gree of events in text.
Comput.
Linguist., 38(2):261?299, June.Roser Saur??.
2008.
A Factuality Profiler for Eventual-ities in Text.
Ph.D. thesis, Brandeis University.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
: Evaluating non-expert annotations for nat-ural language tasks.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 254?263, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.420
