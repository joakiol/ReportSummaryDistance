Processing Broadcast Audio for Information AccessJean-Luc Gauvain, Lori Lamel, Gilles Adda, Martine Adda-Decker,Claude Barras, Langzhou Chen, and Yannick de KercadioSpoken Language Processing GroupLIMSI-CNRS, B.P.
133, 91403 Orsay cedex, France(gauvain@limsi.fr http://www.limsi.fr/tlp)AbstractThis paper addresses recent progress inspeaker-independent, large vocabulary,continuous speech recognition, whichhas opened up a wide range of near andmid-term applications.
One rapidly ex-panding application area is the process-ing of broadcast audio for informationaccess.
At LIMSI, broadcast news tran-scription systems have been developedfor English, French, German, Mandarinand Portuguese, and systems for otherlanguages are under development.
Au-dio indexation must take into accountthe specificities of audio data, such asneeding to deal with the continuousdata stream and an imperfect word tran-scription.
Some near-term applicationsareas are audio data mining, selectivedissemination of information and me-dia monitoring.1 IntroductionA major advance in speech processing technologyis the ability of todays systems to deal with non-homogeneous data as is exemplified by broadcastdata.
With the rapid expansion of different me-dia sources, there is a pressing need for automaticprocessing of such audio streams.
Broadcast au-dio is challenging as it contains segments of vari-ous acoustic and linguistic natures, which requireappropriate modeling.
A special section in theCommunications of the ACM devoted to ?Newson Demand?
(Maybury, 2000) includes contribu-tions from many of the sites carrying out activeresearch in this area.Via speech recognition, spoken document re-trieval (SDR) can support random access to rel-evant portions of audio documents, reducing thetime needed to identify recordings in large multi-media databases.
The TREC (Text REtrieval Con-ference) SDR evaluation showed that only smalldifferences in information retrieval performanceare observed for automatic and manual transcrip-tions (Garofolo et al, 2000).Large vocabulary continuous speech recogni-tion (LVCSR) is a key technology that can be usedto enable content-based information access in au-dio and video documents.
Since most of the lin-guistic information is encoded in the audio chan-nel of video data, which once transcribed can beaccessed using text-based tools.
This research hasbeen carried out in a multilingual environment inthe context of several recent and ongoing Euro-pean projects.
We highlight recent progress inLVCSR and describe some of our work in de-veloping a system for processing broadcast au-dio for information access.
The system has twomain components, the speech transcription com-ponent and the information retrieval component.Versions of the LIMSI broadcast news transcrip-tion system have been developed in American En-glish, French, German, Mandarin and Portuguese.2 Progress in LVCSRSubstantial advances in speech recognition tech-nology have been achieved during the last decade.Only a few years ago speech recognition was pri-marily associated with small vocabulary isolatedword recognition and with speaker-dependent (of-ten also domain-specific) dictation systems.
Thesame core technology serves as the basis for arange of applications such as voice-interactivedatabase access or limited-domain dictation, aswell as more demanding tasks such as the tran-scription of broadcast data.
With the exception ofthe inherent variability of telephone channels, formost applications it is reasonable to assume thatthe speech is produced in relatively stable envi-ronmental and in some cases is spoken with thepurpose of being recognized by the machine.The ability of systems to deal with non-homogeneous data as is found in broadcast au-dio (changing speakers, languages, backgrounds,topics) has been enabled by advances in a vari-ety of areas including techniques for robust signalprocessing and normalization; improved trainingtechniques which can take advantage of very largeaudio and textual corpora; algorithms for audiosegmentation; unsupervised acoustic model adap-tation; efficient decoding with long span languagemodels; ability to use much larger vocabulariesthan in the past - 64 k words or more is commonto reduce errors due to out-of-vocabulary words.With the rapid expansion of different mediasources for information dissemination includingvia the internet, there is a pressing need for au-tomatic processing of the audio data stream.
Thevast majority of audio and video documents thatare produced and broadcast do not have associ-ated annotations for indexation and retrieval pur-poses, and since most of today?s annotation meth-ods require substantial manual intervention, andthe cost is too large to treat the ever increasingvolume of documents.
Broadcast audio is chal-lenging to process as it contains segments of vari-ous acoustic and linguistic natures, which requireappropriate modeling.
Transcribing such data re-quires significantly higher processing power thanwhat is needed to transcribe read speech datain a controlled environment, such as for speakeradapted dictation.
Although it is usually as-sumed that processing time is not a major issuesince computer power has been increasing con-tinuously, it is also known that the amount of dataappearing on information channels is increasingat a close rate.
Therefore processing time is animportant factor in making a speech transcriptionsystem viable for audio data mining and other re-lated applications.
Transcription word error ratesof about 20% have been reported for unrestrictedbroadcast news data in several languages.As shown in Figure 1 the LIMSI broadcastnews transcription system for automatic indexa-tion consists of an audio partitioner and a speechrecognizer.3 Audio partitioningThe goal of audio partitioning is to divide theacoustic signal into homogeneous segments, la-beling and structuring the acoustic content of thedata, and identifying and removing non-speechsegments.
The LIMSI BN audio partitioner re-lies on an audio stream mixture model (Gauvainet al, 1998).
While it is possible to transcribe thecontinuous stream of audio data without any priorsegmentation, partitioning offers several advan-tages over this straight-forward solution.
First,in addition to the transcription of what was said,other interesting information can be extractedsuch as the division into speaker turns and thespeaker identities, and background acoustic con-ditions.
This information can be used both di-rectly and indirectly for indexation and retrievalpurposes.
Second, by clustering segments fromthe same speaker, acoustic model adaptation canbe carried out on a per cluster basis, as opposedto on a single segment basis, thus providing moreadaptation data.
Third, prior segmentation canavoid problems caused by linguistic discontinu-ity at speaker changes.
Fourth, by using acousticmodels trained on particular acoustic conditions(such as wide-band or telephone band), overallperformance can be significantly improved.
Fi-nally, eliminating non-speech segments substan-tially reduces the computation time.
The resultof the partitioning process is a set of speech seg-ments usually corresponding to speaker turns withspeaker, gender and telephone/wide-band labels(see Figure 2).4 Transcription of Broadcast NewsFor each speech segment, the word recognizer de-termines the sequence of words in the segment,associating start and end times and an optionalconfidence measure with each word.
The LIMSIsystem, in common with most of today?s state-of-the-art systems, makes use of statistical modelsof speech generation.
From this point of view,message generation is represented by a languagemodel which provides an estimate of the probabil-ity of any given word string, and the encoding ofthe message in the acoustic signal is representedby a probability density function.
The speaker-independent 65k word, continuous speech rec-ognizer makes use of 4-gram statistics for lan-guage modeling and of continuous density hiddenMarkov models (HMMs) with Gaussian mixturesfor acoustic modeling.
Each word is representedby one or more sequences of context-dependentphone models as determined by its pronunciation.The acoustic and language models are trained onlarge, representative corpora for each task andlanguage.Processing time is an important factor in mak-ing a speech transcription system viable for au-tomatic indexation of radio and television broad-casts.
For many applications there are limita-tions on the response time and the available com-putational resources, which in turn can signifi-cantly affect the design of the acoustic and lan-guage models.
Word recognition is carried out inone or more decoding passes with more accurateacoustic and language models used in successivepasses.
A 4-gram single pass dynamic networkdecoder has been developed (Gauvain and Lamel,2000) which can achieve faster than real-time de-coding with a word error under 30%, running inless than 100 Mb of memory on widely availableplatforms such Pentium III or Alpha machines.5 MultilingualityA characteristic of the broadcast news domain isthat, at least for what concerns major news events,similar topics are simultaneously covered in dif-ferent emissions and in different countries andlanguages.
Automatic processing carried out oncontemporaneous data sources in different lan-guages can serve for multi-lingual indexation andretrieval.
Multilinguality is thus of particular in-terest for media watch applications, where newsmay first break in another country or language.At LIMSI broadcast news transcription systemshave been developed for the American English,French, German, Mandarin and Portuguese lan-guages.
The Mandarin language was chosen be-cause it is quite different from the other lan-guages (tone and syllable-based), and Mandarinresources are available via the LDC as well as ref-erence performance results.Our system and other state-of-the-art sys-tems can transcribe unrestricted American En-glish broadcast news data with word error ratesunder 20%.
Our transcription systems for Frenchand German have comparable error rates for newsbroadcasts (Adda-Decker et al, 2000).
Thecharacter error rate for Mandarin is also about20% (Chen et al, 2000).
Based on our expe-rience, it appears that with appropriately trainedmodels, recognizer performance is more depen-dent upon the type and source of data, than on thelanguage.
For example, documentaries are partic-ularly challenging to transcribe, as the audio qual-ity is often not very high, and there is a large pro-portion of voice over.6 Spoken Document RetrievalThe automatically generated partition and wordtranscription can be used for indexation and in-formation retrieval purposes.
Techniques com-monly applied to automatic text indexation canbe applied to the automatic transcriptions of thebroadcast news radio and TV documents.
Thesetechniques are based on document term frequen-cies, where the terms are obtained after standardtext processing, such as text normalization, tok-enization, stopping and stemming.
Most of thesepreprocessing steps are the same as those used toprepare the texts for training the speech recog-nizer language models.
While this offers advan-tages for speech recognition, it can lead to IR er-rors.
For better IR results, some words sequencescorresponding to acronymns, multiword named-entities (e.g.
Los Angeles), and words precededby some particular prefixes (anti, co, bi, counter)are rewritten as a single word.
Stemming is usedto reduce the number of lexical items for a givenword sense.
The stemming lexicon contains about32000 entries and was constructed using Porter?salgorithm (Porter80, 1980) on the most frequentwords in the collection, and then manually cor-rected.The information retrieval system relies on a un-LexiconAcoustic modelsRecognitionWordAudio signalLanguage modelAnalysisAcousticpartitionedspeech acoustic modelsMusic, noise andnon speechFilter outsegmentstelephone/non-tel modelsword transcription(SGML file)dataMale/female modelsIterativesegmentationand labellingFigure 1: Overview of an audio transcription system.
The audio partitioner divides the data stream intohomogeneous acoustic segments, removing non-speech portions.
The word recognizer identifies thewords in each speech segment, associating time-markers with each word. audiofile filename=19980411 1600 1630 CNN HDL language=english  segment type=wideband gender=female spkr=1 stime=50.25 etime=86.83  wtime stime=50.38 etime=50.77  c.n.n. wtime stime=50.77 etime=51.10  headline wtime stime=51.10 etime=51.44  news wtime stime=51.44 etime=51.63  i?m wtime stime=51.63 etime=51.92  robert wtime stime=51.92 etime=52.46  johnsonit is a day of final farewells in alabama the first funerals for victims of this week?s tornadoes are being held today alongwith causing massive property damage the twisters killed thirty three people in alabama five in georgia and one eachin mississippi and north carolina the national weather service says the tornado that hit jefferson county in alabama hadwinds of more than two hundred sixty miles per hour authorities speculated was the most powerful tornado ever to hit thesoutheast twisters destroyed two churches to fire stations and a school parishioners were in one church when the tornadostruck  /segment  segment type=wideband gender=female spkr=2 stime=88.37 etime=104.86 at one point when the table came onto my back i thought yes this is it i?m ready ready protects protect the children becausethe children screaming the children were screaming they were screaming in prayer that were screaming god help us  /segment  segment type=wideband gender=female spkr=1 stime=104.86 etime=132.37 vice president al gore toured the area yesterday he called it the worst tornado devastation he?s ever seen we will have acomplete look at the weather across the u. s. in our extended weather forecast in six minutes  /segment .
.
. segment type=wideband gender=male spkr=19 stime=1635.60 etime=1645.71 so if their computing systems don?t tackle this problem well we have a potential business disruption and either erroneousdeliveries or misdeliveries or whatever savvy businesses are preparing now so the january first two thousand would just beanother day on the town not a day when fast food and everything else slows down rick lockridge c.n.n.  /segment   /audiofile Figure 2: Example system output obtained by automatic processing of the audio stream of a CNN showbroadcasted on April 11, 1998 at 4pm.
The output includes the partitioning and transcription results.
Toimprove readability, word time stamps are given only for the first 6 words.
Non speech segments havebeen removed and the following information is provided for each speech segment: signal bandwidth(telephone or wideband), speaker gender, and speaker identity (within the show).Transcriptions Werr Base BRFClosed-captions - 46.9% 54.3%10xRT 20.5% 45.3% 53.9%1.4xRT 32.6% 40.9% 49.4%Table 1: Impact of the word error rate on themean average precision using using a 1-gram doc-ument model.
The document collection contains557 hours of broadcast news from the period ofFebruary through June 1998.
(21750 stories, 50queries with the associated relevance judgments.
)igram model per story.
The score of a story is ob-tained by summing the query term weights whichare simply the log probabilities of the terms giventhe story model once interpolated with a generalEnglish model.
This term weighting has beenshown to perform as well as the popular TF  IDFweighting scheme (Hiemstra and Wessel, 1998;Miller et al, 1998; Ng, 1999; Spa?rk Jones et al,1998).The text of the query may or may not includethe index terms associated with relevant docu-ments.
One way to cope with this problem is touse query expansion (Blind Relevance Feedback,BRF (Walker and de Vere, 1990)) based on termspresent in retrieved contemporary texts.The system was evaluated in the TREC SDRtrack, with known story boundaries.
The SDRdata collection contains 557 hours of broadcastnews from the period of February through June1998.
This data includes 21750 stories and a setof 50 queries with the associated relevance judg-ments (Garofolo et al, 2000).In order to assess the effect of the recogni-tion time on the information retrieval results wetranscribed the 557 hours of broadcast news datausing two decoder configurations: a single pass1.4xRT system and a three pass 10xRT system.The word error rates are measured on a 10h testsubset (Garofolo et al, 2000).
The informationretrieval results are given in terms of mean av-erage precision (MAP), as is done for the TRECbenchmarks in Table 1 with and without query ex-pansion.
For comparison, results are also givenfor manually produced closed captions.
Withquery expansion comparable IR results are ob-tained using the closed captions and the 10xRT051015202530354045500 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15Percentageof sectionsNumber of speaker turnsFigure 3: Histogram of the number of speakerturns per section in 100 hours of audio data fromradio and TV sources (NPR, ABC, CNN, CSPAN)from May-June 1996.transcriptions, and a moderate degradation (4%absolute) is observed using the 1.4xRT transcrip-tions.7 Locating Story BoundariesThe broadcast news transcription system also pro-vides non-lexical information along with the wordtranscription.
This information is available inthe partition of the audio track, which identifiesspeaker turns.
It is interesting to see whether ornot such information can be used to help locatestory boundaries, since in the general case theseare not known.
Statistics were made on 100 hoursof radio and television broadcast news with man-ual transcriptions including the speaker identities.Of the 2096 sections manually marked as reports(considered stories), 40% start without a manu-ally annotated speaker change.
This means thatusing only speaker change information for detect-ing document boundaries would miss 40% of theboundaries.
With automatically detected speakerchanges, the number of missed boundaries wouldcertainly increase.
At the same time, 11,160 ofthe 12,439 speaker turns occur in the middle of adocument, resulting in a false alarm rate of almost90%.
A more detailed analysis shows that about50% of the sections involve a single speaker, butthat the distribution of the number of speakerturns per section falls off very gradually (see Fig-ure 3).
False alarms are not as harmful as misseddetections, since it may be possible to merge ad-jacent turns into a single document in subsequentprocessing.
These results show that even perfect00.0020.0040.0060.0080.010.0120.0140.0160.0180 30 60 90 120 150 180 210 240 270 300DensityDuration (seconds)1997 Hub-400.0050.010.0150.020.0250 30 60 90 120 150 180 210 240 270 300DensityDuration (seconds)TREC-9 SDR CorpusFigure 4: Distribution of document durations for100 hours of data from May-June 1996 (top) andfor 557 hours from February-June 1998 (bottom).speaker turn boundaries cannot be used as the pri-mary cue for locating document boundaries.
Theycan, however, be used to refine the placementof a document boundary located near a speakerchange.We also investigated using simple statistics onthe durations of the documents.
A histogram ofthe 2096 sections is shown in Figure 4.
Onethird of the sections are shorter than 30 seconds.The histogram has a bimodal distribution with asharp peak around 20 seconds, and a smaller, flatpeak around 2 minutes.
Very short documentsare typical of headlines which are uttered by sin-gle speaker, whereas longer documents are morelikely to contain data from multiple talkers.
Thisdistribution led us to consider using a multi-scalesegmentation of the audio stream into documents.Similar statistics were measured on the larger cor-pus (Figure 4 bottom).As proposed in (Abberley et al, 1999; John-son et al, 1999), we segment the audio streaminto overlapping documents of a fixed duration.As a result of optimization, we chose a 30 sec-ond window duration with a 15 second overlap.Since there are many stories significantly shorterthan 30s in broadcast shows (see Figure 4) weconjunctured that it may be of interest to use adouble windowing system in order to better tar-get short stories (Gauvain et al, 2000).
The win-dow size of the smaller window was selected tobe 10 seconds.
So for each query, we indepen-dently retrieved two sets of documents, one setfor each window size.
Then for each documentset, document recombination is done by mergingoverlapping documents until no further mergesare possible.
The score of a combined documentis set to maximum score of any one of the com-ponents.
For each document derived from the30s windows, we produce a time stamp locatedat the center point of the document.
However,if any smaller documents are embedded in thisdocument, we take the center of the best scor-ing document.
This way we try to take advantageof both window sizes.
The MAP using a single30s window and the double windowing strategyare shown in Table 2.
For comparison, the IR re-sults using the manual story segmentation and thespeaker turns located by the audio partitioner arealso given.
All conditions use the same word hy-potheses obtained with a speech recognizer whichhad no knowledge about the story boundaries.manual segmentation (NIST) 59.6%audio partitioner 33.3%single window (30s) 50.0%double window 52.3%Table 2: Mean average precision with manual andautomatically determined story boundaries.
Thedocument collection contains 557 hours of broad-cast news from the period of February throughJune 1998.
(21750 stories, 50 queries with theassociated relevance judgments.
)From these results we can clearly see the inter-est of using a search engine specifically designedto retrieve stories in the audio stream.
Using ana priori acoustic segmentation, the mean aver-age precision is significantly reduced comparedto a ?perfect?
manual segmentation, whereas thewindow-based search engine results are muchcloser.
Note that in the manual segmentation allnon-story segments such as advertising have beenremoved.
This reduces the risk of having out-of-topic hits and explains part of the difference be-tween this condition and the other conditions.The problem of locating story boundaries is be-ing further pursued in the context of the ALERTproject, where one of the goals is to identify ?doc-uments?
given topic profiles.
This project is in-vestigating the combined use of audio and videosegmentation to more accurately locate documentboundaries in the continuous data stream.8 Recent Research ProjectsThe work presented in this paper has benefitedfrom a variety of research projects both at the Eu-ropean and National levels.
These collaborativeefforts have enabled access to real-world data al-lowing us to develop algorithms and models well-suited for near-term applications.The European project LE-4 OLIVE: AMultilingual Indexing Tool for BroadcastMaterial Based on Speech Recognition(http://twentyone.tpd.tno.nl/ olive/) addressedmethods to automate the disclosure of the infor-mation content of broadcast data thus allowingcontent-based indexation.
Speech recognitionwas used to produce a time-linked transcript ofthe audio channel of a broadcast, which was thenused to produce a concept index for retrieval.Broadcast news transcription systems for Frenchand German were developed.
The French datacome from a variety of television news shows andradio stations.
The German data consist of TVnews and documentaries from ARTE.
OLIVE alsodeveloped tools for users to query the database,as well as cross-lingual access based on off-linemachine translation of the archived documents,and online query translation.The European project IST ALERT: Alert sys-tem for selective dissemination (http://www.fb9-ti.uni-duisburg.de/alert) aims to associate state-of-the-art speech recognition with audio andvideo segmentation and automatic topic index-ing to develop an automatic media monitoringdemonstrator and evaluate it in the context of realworld applications.
The targeted languages areFrench, German and Portuguese.
Major media-monitoring companies in Europe are participatingin this project.Two other related FP5 IST projects are: CORE-TEX: Improving Core Speech Recognition Tech-nology and ECHO: European CHronicles On-line.
CORETEX (http://coretex.itc.it/), aims atimproving core speech recognition technologies,which are central to most applications involv-ing voice technology.
In particular the projectaddresses the development of generic speechrecognition technology and methods to rapidlyport technology to new domains and languageswith limited supervision, and to produce en-riched symbolic speech transcriptions.
The ECHOproject (http://pc-erato2.iei.pi.cnr.it/echo) aims todevelop an infrastructure for access to histori-cal films belonging to large national audiovisualarchives.
The project will integrate state-of-the-art language technologies for indexing, searchingand retrieval, cross-language retrieval capabilitiesand automatic film summary creation.9 ConclusionsThis paper has described some of the ongoing re-search activites at LIMSI in automatic transcrip-tion and indexation of broadcast data.
Much ofthis research, which is at the forefront of todaystechnology, is carried out with partners with realneeds for advanced audio processing technolo-gies.Automatic speech recognition is a key tech-nology for audio and video indexing.
Most ofthe linguistic information is encoded in the au-dio channel of video data, which once transcribedcan be accessed using text-based tools.
This is incontrast to the image data for which no commondescription language is widely adpoted.
A va-riety of near-term applications are possible suchas audio data mining, selective dissemination ofinformation (News-on-Demand), media monitor-ing, content-based audio and video retrieval.It appears that with word error rates on theorder of 20%, comparable IR results to thoseobtained on text data can be achieved.
Evenwith higher word error rates obtained by run-ning a faster transcription system or by transcrib-ing compressed audio data (Barras et al, 2000;J.M.
Van Thong et al, 2000) (such as that can beloaded over the Internet), the IR performance re-mains quite good.AcknowledgmentsThis work has been partially financed by the Eu-ropean Commission and the French Ministry ofDefense.
The authors thank Jean-Jacques Gan-golf, Sylvia Hermier and Patrick Paroubek fortheir participation in the development of differ-ent aspects of the automatic indexation system de-scribed here.ReferencesDave Abberley, Steve Renals, Dan Ellis and TonyRobinson, ?The THISL SDR System at TREC-8?,Proc.
of the 8th Text Retrieval Conference TREC-8,Nov 1999.Martine Adda-Decker, Gilles Adda, Lori Lamel, ?In-vestigating text normalization and pronunciationvariants for German broadcast transcription,?
Proc.ICSLP?2000, Beijing, China, October 2000.Claude Barras, Lori Lamel, Jean-Luc Gauvain, ?Auto-matic Transcription of Compressed Broadcast Au-dio Proc.
ICASSP?2001, Salt Lake City, May 2001.Langzhou Chen, Lori Lamel, Gilles Adda and Jean-Luc Gauvain, ?Broadcast News Transcription inMandarin,?
Proc.
ICSLP?2000, Beijing, China, Oc-tober 2000.John S. Garofolo, Cedric G.P.
Auzanne, and EllenM.
Voorhees, ?The TREC Spoken Document Re-trieval Track: A Success Story,?
Proc.
of the 6thRIAO Conference, Paris, April 2000.
Also JohnS.
Garofolo et al, ?1999 Trec-8 Spoken Docu-ment Retrieval Track Overview and Results,?
Proc.8th Text Retrieval Conference TREC-8, Nov 1999.
(http://trec.nist.gov).Jean-Luc Gauvain, Lori Lamel, ?Fast Decoding forIndexation of Broadcast Data,?
Proc.
ICSLP?2000,3:794-798, Oct 2000.Jean-Luc Gauvain, Lori Lamel, Gilles Adda, ?Parti-tioning and Transcription of Broadcast News Data,?ICSLP?98, 5, pp.
1335-1338, Dec. 1998.Jean-Luc Gauvain, Lori Lamel, Claude Barras, GillesAdda, Yannick de Kercadio ?The LIMSI SDR sys-tem for TREC-9,?
Proc.
of the 9th Text RetrievalConference TREC-9, Nov 2000.Alexander G. Hauptmann and Michael J. Witbrock,?Informedia: News-on-Demand Multimedia Infor-mation Acquisition and Retrieval,?
Proc Intelli-gent Multimedia Information Retrieval, M. May-bury, ed., AAAI Press, pp.
213-239, 1997.Djoerd Hiemstra, Wessel Kraaij, ?Twenty-One atTREC-7: Ad-hoc and Cross-language track,?
Proc.of the 8th Text Retrieval Conference TREC-7, Nov1998.Sue E. Johnson, Pierre Jourlin, Karen Spa?rck Jones,Phil C. Woodland, ?Spoken Document Retrieval forTREC-8 at Cambridge University?, Proc.
of the 8thText Retrieval Conference TREC-8, Nov 1999.Mark Maybury, ed., Special Section on ?News on De-mand?, Communications of the ACM, 43(2), Feb2000.David Miller, Tim Leek, Richard Schwartz, ?UsingHidden Markov Models for Information Retrieval?,Proc.
of the 8th Text Retrieval Conference TREC-7,Nov 1998.Kenney Ng, ?A Maximum Likelihood Ratio Informa-tion Retrieval Model,?
Proc.
of the 8th Text Re-trieval Conference TREC-8, 413-435, Nov 1999.M.
F. Porter, ?An algorithm for suffix stripping?, Pro-gram, 14, pp.
130?137, 1980.Karen Spa?rk Jones, S. Walker, Stephen E. Robert-son, ?A probabilistic model of information retrieval:development and status,?
Technical Report of theComputer Laboratory, University of Cambridge,U.K., 1998.J.M.
Van Thong, David Goddeau, Anna Litvi-nova, Beth Logan, Pedro Moreno, Michael Swain,?SpeechBot: a Speech Recognition based Audio In-dexing System for the Web?, Proc.
of the 6th RIAOConference, Paris, April 2000.S.
Walker, R. de Vere, ?Improving subject retrieval inonline catalogues: 2.
Relevance feedback and queryexpansion?, British Library Research Paper 72,British Library, London, U.K., 1990.
