Proceedings of NAACL-HLT 2013, pages 342?347,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsApplying Pairwise Ranked Optimisation to Improve the Interpolation ofTranslation ModelsBarry HaddowUniversity of EdinburghScotlandbhaddow@inf.ed.ac.ukAbstractIn Statistical Machine Translation we oftenhave to combine different sources of paralleltraining data to build a good system.
One wayof doing this is to build separate translationmodels from each data set and linearly inter-polate them, and to date the main method foroptimising the interpolation weights is to min-imise the model perplexity on a heldout set.
Inthis work, rather than optimising for this indi-rect measure, we directly optimise for BLEUon the tuning set and show improvements inaverage performance over two data sets and 8language pairs.1 IntroductionStatistical Machine Translation (SMT) requireslarge quantities of parallel training data in order toproduce high quality translation systems.
This train-ing data, however, is often scarce and must be drawnfrom whatever sources are available.
If these datasources differ systematically from each other, and/orfrom the test data, then the problem of combiningthese disparate data sets to create the best possibletranslation system is known as domain adaptation.One approach to domain adaptation is to buildseparate models for each training domain, thenweight them to create a system tuned to the test do-main.
In SMT, a successful approach to building do-main specific language models is to build one fromeach corpus, then linearly interpolate them, choos-ing weights that minimise the perplexity on a suit-able heldout set of in-domain data.
This methodhas been applied by many authors (e.g.
(Koehn andSchroeder, 2007)), and is implemented in popularlanguage modelling tools like IRSTLM (Federico etal., 2008) and SRILM (Stolcke, 2002).Similar interpolation techniques have been devel-oped for translation model interpolation (Foster etal., 2010; Sennrich, 2012) for phrase-based systemsbut have not been as widely adopted, perhaps be-cause the efficacy of the methods is not as clear-cut.
In this previous work, the authors used stan-dard phrase extraction heuristics to extract phrasesfrom a heldout set of parallel sentences, then tunedthe translation model (i.e.
the phrase table) inter-polation weights to minimise the perplexity of theinterpolated model on this set of extracted phrases.In this paper, we try to improve on this perplexityoptimisation of phrase table interpolation weights byaddressing two of its shortcomings.
The first prob-lem is that the perplexity is not well defined becauseof the differing coverage of the phrase tables, andtheir partial coverage of the phrases extracted fromthe heldout set.
Secondly, perplexity may not corre-late with the performance of the final SMT system.So, instead of optimising the interpolationweights for the indirect goal of translation modelperplexity, we optimise them directly for transla-tion performance.
We do this by incorporating theseweights into SMT tuning using a modified version ofPairwise Ranked Optimisation (PRO) (Hopkins andMay, 2011).In experiments on two different domain adapta-tion problems and 8 language pairs, we show thatour method achieves comparable or improved per-formance, when compared to the perplexity minimi-sation method.
This is an encouraging result as it342shows that PRO can be adapted to optimise transla-tion parameters other than those in the standard lin-ear model.2 Optimising Phrase Table InterpolationWeights2.1 Previous ApproachesIn the work of Foster and Kuhn (2007), linear inter-polation weights were derived from different mea-sures of distance between the training corpora, butthis was not found to be successful.
Optimising theweights to minimise perplexity, as described in theintroduction, was found by later authors to be moreuseful (Foster et al 2010; Sennrich, 2012), gener-ally showing small improvements over the defaultapproach of concatenating all training data.An alternative approach is to use log-linear inter-polation, so that the interpolation weights can beeasily optimised in tuning (Koehn and Schroeder,2007; Bertoldi and Federico, 2009; Banerjee et al2011).
However, this effectively multiplies the prob-abilities across phrase tables, which does not seemappropriate, especially for phrases absent from 1 ta-ble.2.2 Tuning SMT SystemsThe standard SMT model scores translation hy-potheses as a linear combination of features.
Themodel score of a hypothesis e is then defined tobe w ?
h(e, f, a) where w is a weight vector, andh(e, f, a) a vector of feature functions defined oversource sentences (f ), hypotheses, and their align-ments (a).
The weights are normally optimised(tuned) to maximise BLEU on a heldout set (the tun-ing set).The most popular algorithm for this weight op-timisation is the line-search based MERT (Och,2003), but recently other algorithms that supportmore features, such as PRO (Hopkins and May,2011) or MIRA-based algorithms (Watanabe et al2007; Chiang et al 2008; Cherry and Foster, 2012),have been introduced.
All these algorithms assumethat the model score is a linear function of the pa-rameters w. However since the phrase table prob-abilities enter the score function in log form, ifthese probabilities are a linear interpolation, then themodel score is not a linear function of the interpola-tion weights.
We will show that PRO can be usedto simultaneously optimise such non-linear parame-ters.2.3 Pairwise Ranked OptimisationPRO is a batch tuning algorithm in the sense thatthere is an outer loop which repeatedly decodes asmall (1000-2000 sentence) tuning set and passesthe n-best lists from this tuning set to the core al-gorithm (also known as the inner loop).
The corealgorithm samples pairs of hypotheses from the n-best lists (according to a specific procedure), anduses these samples to optimise the weight vector w.The core algorithm in PRO will now be explainedin more detail.
Suppose that the N sampled hypoth-esis pairs (x?i , x?i ) are indexed by i and have corre-sponding feature vectors pairs (h?i ,h?i ).
If the gainof a given hypothesis (we use smoothed sentenceBLEU) is given by the function g(x), then we defineyi byyi ?
sgn(g(x?i )?
g(x?i )) (1)For weights w, and hypothesis pair (x?i , x?i ), the(model) score difference ?swi is given by:?swi ?
sw(x?i )?
sw(x?i ) ?
w ?
(h?i ?
h?i)(2)Then the core PRO algorithm updates the weightvector to w?
by solving the following optimisationproblem:w?
= arg maxwN?i=1log (?
(yi?swi )) (3)where ?
(x) is the standard sigmoid function.
Thederivative of the function can be computed easily,and the optimisation problem can be solved withstandard numerical optimisation algorithms such asL-BFGS (Byrd et al 1995).
PRO is normally im-plemented by converting each sample to a trainingexample for a 2 class maximum entropy classifier,with the feature values set to ?hi and the responsesset to the yi, whereupon the log-likelihood is the ob-jective given in Equation (3).
As in maximum en-tropy modeling, it is usual to add a Gaussian prior tothe objective (3) in PRO training.2.4 Extending PRO for Mixture ModelsWe now show how to apply the PRO tuning algo-rithm of the previous subsection to simultaneously343optimise the weights of the translation system, andthe interpolation weights.In the standard phrase-based model, some of thefeatures are derived from logs of phrase translationprobabilities.
If the phrase table is actually a linearinterpolation of two (or more) phrase tables, thenwe can consider these features as also being func-tions of the interpolation weights.
The interpola-tion weights then enter the score differences {?swi }via the phrase features, and we can jointly optimisethe objective in Equation (3) for translation modelweights and interpolation weights.To make this more concrete, suppose that the fea-ture vector consists of m phrase table features andn?m other features1h ?
(log(p1), .
.
.
, log(pm), hm+1, .
.
.
hn) (4)where each pj is an interpolation of two probabilitydistributions pjA and pjB .
So, pj ?
?jpjA+(1?
?j)pjBwith 0 ?
?j ?
1.
Defining ?
?
(?1 .
.
.
?m), theoptimisation problem is then:(w?,??)
= arg max(w,?
)?Ni=1 log(?(yi?s(w,?
)i))(5)where the sum is over the sampled hypothesis pairsand the ?
indicates the difference between themodel scores of the two hypotheses in the pair, asbefore.
The model score s(w,?
)i is given bym?j=1(wj ?
log(?jpjAi + (1?
?j)pjBi)))+n?j=m+1wjhji (6)where w ?
(wi .
.
.
wn).
A Gaussian regularisa-tion term is added to the objective, as it was forPRO.
By replacing the core algorithm of PRO withthe optimisation above, the interpolation weightscan be trained simultaneously with the other modelweights.Actually, the above explanation contains a simpli-fication, in that it shows the phrase features interpo-lated at sentence level.
In reality the phrase features1Since the phrase penalty feature is a constant across phrasepairs it is not interpolated, and so is classed with the the ?other?features.
The lexical scores, although not actually probabilities,are interpolated.are interpolated at the phrase level, then combined togive the sentence level feature value.
This makes thedefinition of the objective more complex than thatshown above, but still optimisable using bounded L-BFGS.3 Experiments3.1 Corpus and BaselinesWe ran experiments with data from the WMT sharedtasks (Callison-Burch et al 2007; Callison-Burch etal., 2012), as well as OpenSubtitles data2 released bythe OPUS project (Tiedemann, 2009).The experiments targeted both the news-commentary (nc) and OpenSubtitles (st) domains,with nc-devtest2007 and nc-test2007for tuning and testing in the nc domain, respec-tively, and corresponding 2000 sentence tuningand test sets selected from the st data.
The news-commentary v7 corpus and a 200k sentence corpusselected from the remaining st data were used asin-domain training data for the respective domains,with europarl v7 (ep) used as out-of-domain train-ing data in both cases.
The language pairs we testedwere the WMT language pairs for nc (English (en)to and from Spanish (es), German (de), French (fr)and Czech (cs)), with Dutch (nl) substituted for dein the st experiments.To build phrase-based translation systems, weused the standard Moses (Koehn et al 2007) train-ing pipeline, in particular employing the usual 5phrase features ?
forward and backward phraseprobabilities, forward and backward lexical scoresand a phrase penalty.
The 5-gram Kneser-Neysmoothed language models were trained by SRILM(Stolcke, 2002), with KenLM (Heafield, 2011) usedat runtime.
The language model is always a linearinterpolation of models estimated on the in- and out-of-domain corpora, with weights tuned by SRILM?sperplexity minimisation3.
All experiments were runthree times with BLEU scores averaged, as recom-mended by Clark et al(2011).
Performance wasevaluated using case-insensitive BLEU (Papineni etal., 2002), as implemented in Moses.The baseline systems were tuned using the Mosesversion of PRO, a reimplementation of the original2www.opensubtitles.org3Our method could also be applied to language model inter-polation but we chose to focus on phrase tables in this paper.344algorithm using the sampling scheme recommendedby Hopkins and May.
We ran 15 iterations of PRO,choosing the weights that maximised BLEU on thetuning set.
For the PRO training of the interpo-lated models, we used the same sampling scheme,with optimisation of the model weights and interpo-lation weights implemented in Python using scipy4.The implementation is available in Moses, in thecontrib/promix directory.The phrase table interpolation and perplexity-based minimisation of interpolation weights usedthe code accompanying Sennrich (2012), also avail-able in Moses.3.2 ResultsFor each of the two test sets (nc and st), we com-pare four different translation systems (three base-line systems, and our new interpolation method):in Phrase and reordering tables were built from justthe in-domain data.joint Phrase and reordering tables were built fromthe in- and out-of-domain data, concatenated.perp Separate phrase tables built on in- and out-of-domain data, interpolated using perplexity min-imisation.
The reordering table is as for joint.pro-mix As perp, but interpolation weights opti-mised using our modified PRO algorithm.So the two interpolated models (perp and pro-mix)are the same as joint except that their 4 non-constantphrase features are interpolated across the two sep-arate phrase tables.
Note that the language modelsare the same across all four systems.The results of this comparison over the 8 languagepairs are shown in Figure 1, and summarised in Ta-ble 1, which shows the mean BLEU change relativeto the in system.
It can be seen that the pro-mixmethod presented here is out-performing the per-plexity optimisation on the nc data set, and perform-ing similarly on the st data set.joint perp pro-mixnc +0.18 +0.44 +0.91st -0.04 +0.55 +0.48Table 1: Mean BLEU relative to in system for eachdata set.
System names as in Figure 1.4www.scipy.org4 Discussion and ConclusionsThe results show that the pro-mix method is a vi-able way of tuning systems built with interpolatedphrase tables, and performs better than the currentperplexity minimisation method on one of two datasets used in experiments.
On the other data set (st),the out-of-domain data makes much less differenceto the system performance in general, most proba-bly because the difference between the in and out-of-domain data sets in much larger (Haddow andKoehn, 2012).
Whilst the differences between pro-mix and perplexity minimisation are not large on thenc test set (about +0.5 BLEU) the results have beendemonstrated to apply across many language pairs.The advantage of the pro-mix method over otherapproaches is that it directly optimises the mea-sure that we are interested in, rather than optimisingan intermediate measure and hoping that translationperformance improves.
In this work we optimise forBLEU, but the same method could easily be used tooptimise for any sentence-level translation metric.AcknowledgmentsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7/2007-2013) under grantagreement 288769 (ACCEPT).ReferencesPratyush Banerjee, Sudip K. Naskar, Johann Roturier,Andy Way, and Josef van Genabith.
2011.
DomainAdaptation in Statistical Machine Translation of User-Forum Data using Component Level Mixture Mod-elling.
In Proceedings of MT Summit.Nicola Bertoldi and Marcello Federico.
2009.
DomainAdaptation for Statistical Machine Translation fromMonolingual Resources.
In Proceedings of WMT.R.
H. Byrd, P. Lu, and J. Nocedal.
1995.
A limitedmemory algorithm for bound constrained optimiza-tion.
SIAM Journal on Scientific and Statistical Com-puting, 16(5):1190?1208.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(meta-)evaluation of machine translation.
In Proceedings ofthe Second Workshop on Statistical Machine Transla-tion, pages 136?158, Prague, Czech Republic, June.Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.345Findings of the 2012 Workshop on Statistical MachineTranslation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, pages 10?51, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Colin Cherry and George Foster.
2012.
Batch TuningStrategies for Statistical Machine Translation.
In Pro-ceedings of NAACL.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online Large-Margin Training of Syntactic and Struc-tural Translation Features.
In Proceedings of EMNLP.Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.2011.
Better hypothesis testing for statistical machinetranslation: Controlling for optimizer instability.
InProceedings of ACL.Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.2008.
IRSTLM: an Open Source Toolkit for HandlingLarge Scale Language Models.
In Proceedings of In-terspeech, Brisbane, Australie.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for SMT.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages128?135, Prague, Czech Republic, June.
Associationfor Computational Linguistics.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative Instance Weighting for Domain Adap-tation in Statistical Machine Translation.
In Proceed-ings of the 2010 Conference on Empirical Methods inNatural Language Processing, pages 451?459, Cam-bridge, MA, October.
Association for ComputationalLinguistics.Barry Haddow and Philipp Koehn.
2012.
Analysingthe Effect of Out-of-Domain Data on SMT Systems.In Proceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 422?432, Montre?al,Canada, June.
Association for Computational Linguis-tics.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages187?197, Edinburgh, Scotland, July.
Association forComputational Linguistics.Mark Hopkins and Jonathan May.
2011.
Tuning asRanking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,pages 1352?1362, Edinburgh, Scotland, UK., July.
As-sociation for Computational Linguistics.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin Domain Adaptation for Statistical Machine Transla-tion.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation, pages 224?227, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the ACL Demo Sessions, pages 177?180,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Franz J. Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedings ofACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Rico Sennrich.
2012.
Perplexity Minimization for Trans-lation Model Domain Adaptation in Statistical Ma-chine Translation.
In Proceedings of EACL.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
Intl.
Conf.
on Spo-ken Language Processing, vol.
2, pages 901?904.Jo?rg Tiedemann.
2009.
News from OPUS - A Collectionof Multilingual Parallel Corpora with Tools and Inter-faces.
In N. Nicolov, K. Bontcheva, G. Angelova, andR.
Mitkov, editors, Recent Advances in Natural Lan-guage Processing (vol V), pages 237?248.
John Ben-jamins, Amsterdam/Philadelphia.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online Large-Margin Training for Sta-tistical Machine Translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech Republic, June.
Association forComputational Linguistics.346cs?en en?cs de?en en?de fr?en en?fr es?en en?esinjoint perppro?mix News CommentaryBleu0102030cs?en en?cs nl?en en?nl fr?en en?fr es?en en?esinjoint perppro?mix Open SubtitlesBleu0510152025Figure 1: Comparison of the performance (BLEU) on in-domain data, of our pro-mix interpolation weighttuning method with three baselines: in using just in-domain parallel training data training; joint also usingeuroparl data; and perp using perplexity minimisation to interpolate in-domain and europarl data.347
