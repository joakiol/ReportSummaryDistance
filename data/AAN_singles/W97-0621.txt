Speech-Graphics Dialogue SystemsAlan W.  Biermann, Michael S. Fulkerson, Greg A. Ke imDuke University{awb, msf, ke J.m)@cs.
duke.
edu1 A Theory of DialogueThe central mechanism of a dialogue system mustbe a planner (Allen et al, 1994; Smith et al, 1995;Young et al, 1989) that seeks the dialogue goal andorganizes all behaviors for that purpose.
Our projectuses a hybrid Prolog-like planner (Smith and Hipp,1994) which first attempts to prove the top-mostgoal and then initiates interactions with the userwhen the proof cannot easily be achieved.
Specif-ically, it attempts to discover key missing axioms inthe proof that prevent its completion and that maybe attainable with the help of the user.
The pur-poses of the interaction are to gather the missinginformation and to eventually achieve the top-mostgoal.Once the structure of the system is settled, a va-riety of desirable behaviors for realistic dialogue canbe programmed.
These include subdialogue behav-iors, variable initiative, the ability to account for auser model, the use of expectation for error correc-tion purposes, and the ability to handle multimediainput and output.
Each of these is described in thefollowing paragraphs.1.1 Subdialogue behaviorsTraditional analyses of human-human dialogue de-compose sequences into segments which are locallycoherent and which individually address their ownsubgoals in the overall dialogue structure.
(Hobbs,1979; Reichman, 1985; Grosz and Sidner, 1986;Lochbaurn, 1991).
Such a segment is opened for aspecific purpose, may involve a series of interactionsbetween participants, and may be closed having suc-cessfuUy achieved the target subgoal.
Such a seg-ment may be interrupted for the purpose of achiev-ing a new, locally discovered, subgoal or for ap-proaching a different goal.
It may also fail to achievesuccess and be abandoned.
Typical dialogues involverepeatedly opening such segments, pursuing one sub-goal, jumping to another, returning to a previoussubgoal and so forth until the highest level goal isachieved or abandoned.The Prolog-like proof tree enables this kind of be-havior because the dialogue segments can be builtaround the explicit subgoals of the proof tree.
Con-trol for the search can be governed by the domaindependent characteristics of the subproofs.
The or-dinary Prolog depth first search is not used and,instead, control can pass from subgoal to subgoalto match the segmental behavior that is normal forsuch dialogues.1.2 Variable initiativeThe primary facility needed for variable initiative isthe ability either to control the movements betweensubgoals (dialogue segments) or to release controland to follow the user's movements (Guinn, 1995;Kitano and Ess-Dykema, 1991; Novick, 1988; Walkerand Whittaker, 1990).
Controlling the movementrequires that the system have domain informationavailable to guide decisions concerning which direc-tions may be good to take.
Having made these deci-sions, the system then jumps to the associated subdi-alogs and follows its plan to completion.
Releasingcontrol to the other participant involves matcbingincoming utterances to expected interactions for thevarious available subgoals and following the user tosubgoals where matches are found.
This is calledplan recognition in the literature and has been theobject of much study (Allen and Perrault, 1980; Lit-man and Allen, 1987; Pollack, 1986; Carberry, 1988;Carberry, 1990).
Mechanisms for both managingmovement between subgoals and deciding when torelease control to the other participant, includingextensive analyses of their effectiveness, are given in(Smith and Hipp, 1994; Guinn, 1995; Guinn, 1996).1.3 Accounting for the user modelEfficient dialogue requires that the knowledge andabilities of the other participant be accounted for(Kobsa and Wahlster, 1989).
When the system pro-121rides information to the user, it is important that'it present he new information at the appropriatelevel.
If the system describes details that are al-ready known to the user, he or she will become de-moralized.
If the system fails to give needed infor-mation, the user will cease to function effectively.The Prolog theorem proving system provides a nat-ural means for encoding and using the user modelwithout major additional mechanisms.
The missingaxiom discovery mechanism simply selects ubdia-logues for interaction at the levels in the proof treewhere the user has knowledge and these levels arewhere the interaction occurs (Smith et al, 1995).1.4 Expectat ion  for the purposes of errorcorrect ionBecause all interactions in a given subdiaiogue areoccurring in the context of the associated subgoal,the actual vocabulary and syntax that are locally ap-propriate may be anticipated (Young et al, 1989).Thus, for example, if the system has asked the userto measure a certain voltage, the Prolog theoremproving tree will include locally the possibilities thatthe user has responded successfully (as "I read sixvolts"), that the user has asked for clarification ("atwhich terminal?
"), that the user needs instruction("how do I measure that?
"), or that the user hasfailed to satisfy the request ("no").
The error cor-rection mechanism looks for an expected input thathas a low Hamming distance (weighted) from the ac-tual recognized input and chooses the best match tothe input that it will respond to.
If the match doesnot exceed a specified threshold, the system couldlook for matches on other recent subdialogues tode-termine whether the user is attempting to move toanother subject.
If no match is found, the systemcould also ask for a repeat of the spoken input.
Intests of the Circuit Fixit Shoppe, the Hamming dis-tance algorithm alone corrected an utterance levelerror rate from 50 percent down to 18.5 percent in aseries of 141 dialogues (Hipp, 1992; Smith and Gor-don, 1996).1.5 Mu l t imed ia  input and outputcapabil it iesThe original version of this architecture envisionedonly speech in and speech out as the communicationmedia.
Speech input (such as "The voltage is sixvolts") was translated to predicated form (such asanswer(measure(1;17,t202,6))) and turned overto the theorem proving mechanism.
Similarly, out-puts from Prolog were converted to strings of textthat were enunciated by a speech synthesizer.
Inrecent years, however, our project (Biermann andLong, 1996) has experimented with multimediagrammars that convert full multimedia communica-tion to and from the internal predicate form.
Thefollowing sections describe our method.2 Mu l t imed ia  GrammarsWe designed a multimedia grammar made up of aseries of operators that relate media syntax and se-mantics.
Each operator accounts pecifically for asyntactic item and simultaneously executes code inthe semantic world which is appropriate for thatsyntax.
For example, in a programming domainwhere one might refer to the lines of code on thescreen, a useful operator is llne which finds the setof all lines on the screen within the current regionof focus.
Other operators find other sets (associatedwith nouns), find subsets of sets (as with the adjec-tive "capitalized"), select out individuals (as withan ordinal), specify relationships (as with contain-ment), and call for changes on the screen (as with"delete").
An important characteristic of such op-erators is that their syntactic and semantic portionsare specified by a general purpose language (C++)so that they can manipulate any media or semanticobjects that the designer may address.
'While ourprototype system has used only spoken and text En-glish and graphical pointing (highlighting orarrows),the approach could conceivably involve full graphi-cal capabilities, mechanical devices, or other input-output media.
Our approach is in contrast with themethods of (Feiner and McKeown, 1993; Wahlster etal., 1993) where communications are split into sev-eral media and then those media are coordinated forpresentation tothe user.
Other work on multimediacommunication is surveyed in (Maybury, 1993).The multimedia grammar is demonstrated in thegeneration of the phrase "the fifth character in thisline" with highlighting of a specified line as givenin Figure 2.
The domain is Pascal tutoring and thephrase specifies a particular character that the sys-tem wishes to comment on.
An example of this typeof reference from the actual system is shown in Fig-ure 1.Such a grammar can be used either for genera-tion or input.
In the generation mode, the targetmeaning is known (it is a particular character "1"in the example) and a sequence of operators i to befound that can achieve the target meaning.
The as-sociated syntax becomes the output o be presentedto the user ("the fifth character in this line" (withpointer) in the example).
In the parsing mode, thetarget syntax is known and a sequence of operatorsis desired that can account for the syntax.
Theseoperators will then compute th e meaning for the ut-122OperatorllneSyntaxlineSemantics\[begin\]\[writln( 'Hello' ) ;\]\[end.
\]Complexitythis this line \[writln (' Hello' ) ; \] Ct~i,_poi,ter * log(n)(with pointer) (with pointer)in in this line writln( 'Hel lo ' ) ; Cm(with pointer)character character in this line \[w\] Jr\] \[i\] \[1;\] [1\] In\] .
.
.
Cchar(with pointer)ordinal fifth character in this line \[i\] Cora * 5(with pointer)the the fifth character in this line 1 Cart(with pointer)Figure 2: The operator grammar generating syntax to select an item on the screen.Figure I: A screen from the Duke Programming Tu-tor: "There is an error at the fifth character in thisline."terance.
Our project uses this grammar for outputonly because we have separately invested major ef-forts in the error correction system that has not beenmerged with the multimedia grammar.This grammar, of course, has the ability to gen-erate a variety of outputs: "this character" (withpointer), "the tenth character", the fifth characterin the second line", "this character in the secondline" (with pointer), etc.
A mechanism needs to bedevised that will select among these choices and thatwill also prune the search to avoid unnecessary com-putation.
Our system uses complexity numbers asshown in Figure 2 and seeks a minimum complexityutterance.
The methodology is experimental and as-signs a complexity constant to each operator.
Thepointer complexity isalso multiplied by log(n) wheren is the number of items that are being distinguishedfrom.
The intuition here is that a geometrical mech-anism centers on the highlighted item.
The ordinalis multiplied by v, the value of the ordinal, on theintuition that the user may actually count out thenumber specified by the ordinal.
The actual valuesof the constants are obtained by training continu-ously as the user operates the system.
This is ex-plained in the next section.3 Learning User PreferencesThe complexity constants (Cline, OG,, etc.)
for gen-eration are learned and continuously updated ur-ing normal operation.
The system gathers feedbackfrom the user via any means the designer may chooseand seeks a set of generation constants hat optimizeuser satisfaction.
In a test of the system (Biermannand Long, 1996), the feedback mechanism was sim-ply the time required for the user to respond.
Aquick response was thus recorded as encouragementto continue the current ype of generation and a longresponse acted to encourage the system to experi-ment with other values for the constants and seeka new optimum.
The specifics of the learning algo-rithm employed are explained in (Long, 1996).The graph in Figure 3 illustrates this process bytracking two of these constants through a samplerun of the system.
The system begins with a biastowards highlighting, evidenced by its lower relativevalue as compared to that of using ordinals.
How-ever,in the middle of the run, the user begins takinga long time to respond to the use of highlighting.Eventually, this drives the system to try ordinals, towhich the user responds more quickly.
This has theeffect of lowering the constant for ordinals, thereby123OrdinalsHighlighting00:45EI ="  oo :3oO 00:'15D .
(/)CI~ 00:00Learning: Adapting to the User's Preferencest n AAA|1 2 3 , S 8 7 12 ,5Utterance Numbert2?J0Figure 3: Adapting to the user's preferences.making it the prefered output mode.
Note that thealgorithm also has an exploration parameter, whichis the probability that it will choose a mode otherthan what is currently prefered.
This allows thealgorithm to periodically test modes that it mightotherwise avoid, and explains why the system usedordinals for the seventh response, despite the higherconstant.4 Building Dialogue SystemsOur most recent voice dialogue system incorporatesmany of the ideas outlined above.
The Duke Pro-gramming Tutor allows students in the introductoryComputer Science course to write and debug sim-ple programs, communicating with the system usingvoice, text and selection with the mouse.
The sys-tem can respond with debugging or tutorial informa-tion, presented as a combination of speech, text andgraphics.
In the fall of 1996, 15 Duke undergradu-ates used the Duke Programming Tutor in place oftheir regular weekly lab.
These sessions lasted abouthalf an hour, and students received less than 3 min-utes of instruction about how to use the system.
Formost students, this was only the second or third timethey had debugged a program.While constructing this system, we often wantedto add modules to explore new ideas: an animatedface, the machine learning of the output mode pref-erences, a novel dialogue control algorithm, etc.
Aswe struggled through integrating each of these newmodules and discovering their dependencies onotherparts of the existing system, we found ourselveswishing for a standardized framework--a communi-cation and architectural infrastructure for voice dia-logue systems.
And while the CSLU Toolkit (Suttonet al, 1996) already promises rapid development ofvoice system applications, it and other commercialsystems rely primarily on finite state models of dia-logne, which may be insufficient for modeling com-plex domains or posing some research questions.
Acomplete set of dialogue application programmer in-teffaces (APIs) would reduce system developmenttime, lead to increased resource sharing and allowmore accurate system and component evaluation(Fulkerson and Kehn, 1997).The high level architecture we envision uses mes-sages to communicate content, and events to de-scribe meta and control information.
For example,SPEECHIN, a speech recognition module, might gen-erate events uch as SpeechStart or SpeechStop, andalso produce a message containing a recognized ut-terance.
This research effort will focus on under-standing and formalizing these communication lan-guages, so that they are not only powerful enoughto capture the dialogue information we can obtaintoday, but also extensible nough to convey novelpieces of the human/machine interaction allowed byfuture developments.
In order to test these ideas,124we are currently converting modules in our existingsystem to allow experimentation with various com-munication languages and architectures.Olalogu4l $yltamSs'ttctl I~\ A,,~la ~~Tl~t I.
~ N ~ \  DIALOGUEFigure 4: A multimodal dialogue system using mes-sages and events.~~~Tl tXT IN  N N\ DIALOGUEFigure 5: Adding learning module to an existingsystem.Consider a hypothetical multimodal dialogue sys-tem that was constructed according to the aboveguidelines, as illustrated in Figure 4.
The messageoutput from dialogue processing contains a predi-cate form of some content o be communicated, anda set of modes in which this can be presented.
Theoutput generation module takes this message as in-put, and generates a response based on this infor-mation, choosing the mode randomly from what isallowed in the message.
In many systems, adding amechanism for learning the user's preferences mightinvolve adding code to a number of modules.
In thesystem we've just described however, the process ismuch easier.
In Figure 5, we see that the learningalgorithm can be inserted between the dialogue pro-cessing and output generation modules.
It receivesevents generated by other modules, and uses tim-ings between output and input events to calculatethe user's response time.
It then modifies the mes-125sage from dialogue to allow only user's current prebered modes, and passes it on to output generation.Note that the API would not only make develop-ment faster and easier, it would also allow multiplelearning algorithms to be tested in a particular do-main.
This type of component evaluation within thecontext of a system is currently much harder to ac-complish.5 SummaryWe have discussed a mechanism for building dia-logue systems, and how one might achieve usefulbehaviors, such as handling subdialogues, allowingvariable initiative, accounting for user differences,correcting for errors, and' communicating in a vari-ety of modes.
We discussed the Duke ProgrammingTutor, a system that demonstrates the integrationof many of these ideas, which has been used by anumber of students.
Finally, we presented our on-going project to make designing, constructing andevaluating new dialogue systems faster and easier.6 AcknowledgementsThis research is supported by the Office of Naval Re-search grant N00014-94-1-0938, the National ScienceFoundation grant IRI-92-21842 and a grant from theResearch Triangle Institute, which is funded in partby the Army Research Office.
Other individuals whohave contributed to the Duke Programming Tutorinclude Curry Guinn, Zheng Liang, Phil Long, Dou-glas Melamed and Krislman Rajagopalan.Re ferencesJ.
F. Allen and C. R. Perrault.
1980.
Analyz-ing intention in dialogues.
Artificial Intelligence,15(3):143-178.James F. Allen, Lenhart K. Schubert, George Fergu-son, Peter Heeman, Chung Hee Hwang, TsuneakiKato, Marc Light, Nathaniel G. Martin, Brad-ford W. Miller, Massimo Poesio, and David R.Traum.
1994.
The TRAINS project: A case studyin building a conversational planning agent.
Tech-nical Report TRAINS Technical Note 94-3, TheUniversity of Rochester, September.A.
W. Biermann and P. M. Long.
1996.
The com-position of messages in speech-graphics interac-tive systems.
In Proceedings of the 1996 Interna-tional Symposium on Spoken Dialogue, pages 97-100, October.Sandra Carberry.
1988.
Modeling the user's plansand goals.
Computational Linguistics, 14(3):23-37.Sandra Carberry.
1990.
Plan recognition i  naturallanguage dialogue.
ACL-MIT Press series in nat-ural language processing.
MIT Press, Cambridge,Massachusetts.S.K.
Feiner and K.R.
McKeown.
1993.
Au-tomating the generation of coordinated multime-dia explanations.
In M.T.
Maybury, editor, In-telligent Multimedia Interfaces, pages 113-134.AAAI/MIT Press.Michael F. Fulkerson and Greg A. Keim.
1997.
De-velopment of a component level API for voice di-alogue systems.
In submission.Barbara J. Grosz and Candace L. Sidner.
1986.
At-tention, intentions, and the structure of discourse.Computational Linguistics, 12(3):175-204, Sep.Curry I. Guinn.
1995.
Meta-Dialogue Behav-iors: Improving the Efficiency of Human-MachineDialogue-A Computational Model of Variable Ini-titive and Negotiation in Collaborative Problem-Solving.
Ph.D. thesis, Duke University.C.
I. Guinn.
1996.
Mechanisms for mixed-initiativehuman-computer collaborative discource.
In Pro-ceedings of the 34th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 278-285.D.
R. Hipp.
1992.
A New Technique for Parsingill-formed Spoken Natural-language Dilaog.
Ph.D.thesis, Duke University.J.
R. Hobbs.
1979.
Coherence and coreference.
Cog-nitive Science, 3:67-90.H.
Kitano and C. Van Ess-Dykema.
1991.
Towarda plan-based understanding model for mixed-initiative dialogues.
In Proceedings of the 29thAnnual Meeting of the Association for Computa-tional Linguistics, pages 25-32.Alfred Kobsa and Wolfgang Wahlster, editors.
1989.User Models in Dialog Systems.
Springer-Verlag,Berlin.D.
J. Litman and J. F. Allen.
1987.
A plan recogni-tion model for subdialogues in conversations.
Cog-nitive Science, 11(2):163-200.K.E.
Lochbaum.
1991.
An algorithm for plan recog-nition in collaborative discource.
In Proceedingsof the 29th Annual Meeting of the Association forComputational Linguistics, pages 33-38.P.
M. Long.
1996.
Improved bounds about on-linelearning of smooth functions of a single variable.In Proceedings of the 1996 Workshop on Algorith-mic Learning Theory.M.T.
Maybury, editor.
1993.
Intelligent MultimediaInterfaces.
AAAI/MIT Press.D.
G. Novick.
1988.
Control of Mixed-Initiative Dis-course Through Meta-Locutionary Acts: A Com-putational Model Ph.D. thesis, University of Ore-gon.M.
E. Pollack.
1986.
A model of plan inference thatdistinguishes between the beliefs of factors and ob-servers.
In Proceedings of the 2~th Annual Meet-ing of the Association for Computational Linguis-tics, pages 207-214.R.
Reichman.
1985.
Getting computers to talk likeyou and me.
The MIT Press, Cambridge, Mass.Ronnie W. Smith and Steven A. Gordon.
1996.Pragmatic issues in handling miscommunication:Observations ofa spoken natural anguage dialogsystem.
In AAAI Workshop on Detecting, Repair-ing, and Preventing Human-Machine Miscommu-nication in Portland, Oregan.Ronnie W. Smith and D. Richard Hipp.
1994.
Spo-ken Natural Language Dialog Systems: A Practi-cal Approach.
Oxford University Press.Ronnie W. Smith, D. Richard Hipp, and Alan W.Biermann.
1995.
An arctitecture for voice dia-log systems based on prolog-style theorem prov-ing.
Computational Linguistics, 21(3):281-320,September.Stephen Sutton, David G. Novick, Ronald Cole,Pieter Vermeulen, Jacques de Villiers, JohanSchalkwyk, and Mark Fanty.
1996.
Building10,000 spoken dialogue systems.
In Proceedingsof the Fourth International Conference on SpokenLanguage Processing, pages 709-712, October.Wolfgang Wahlster, Elisabeeth Andrfi, WolfgangFinkler, Hans-Jiirgen Profitlich, and Thomas Rist.1993.
Plan-based integration of natural languageand graphics generation.
Artificial Intelligence,63:387---427.Marilyn Walker and Steve Whittaker.
1990.
Mixedinitiative in dialogue: An investigation into dis-course segmentation.
In Proceedings, 28th An-nual Meeting of the Association for Computa-tional Linguistics, pages 70.--78.S.
R. Young, A. G. Hauptmann, W. H. Ward, E. T.Smith, and P. Werner.
1989.
High level knowl-edge sources in usable speech recognition systems.Communications ofthe ACM, pages 183-194, Au-gust.126
