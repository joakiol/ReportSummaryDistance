Proceedings of NAACL HLT 2007, pages 9?16,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsTo Memorize or to Predict: Prominence Labeling in Conversational SpeechA.
Nenkova, J. Brenier, A. Kothari, S.
Calhoun?, L. Whitton, D. Beaver, D. JurafskyStanford University{anenkova,jbrenier,anubha,lwhitton,dib,jurafsky}@stanford.edu?University of EdinburghSasha.Calhoun@ed.ac.ukAbstractThe immense prosodic variation of natural con-versational speech makes it challenging to pre-dict which words are prosodically prominent inthis genre.
In this paper, we examine a new fea-ture, accent ratio, which captures how likely it isthat a word will be realized as prominent or not.We compare this feature with traditional accent-prediction features (based on part of speech andN -grams) as well as with several linguistically mo-tivated and manually labeled information structurefeatures, such as whether a word is given, new, orcontrastive.
Our results show that the linguistic fea-tures do not lead to significant improvements, whileaccent ratio alone can yield prediction performancealmost as good as the combination of any other sub-set of features.
Moreover, this feature is useful evenacross genres; an accent-ratio classifier trained onlyon conversational speech predicts prominence withhigh accuracy in broadcast news.
Our results sug-gest that carefully chosen lexicalized features canoutperform less fine-grained features.1 IntroductionBeing able to predict the prominence or pitch accentstatus of a word in conversational speech is impor-tant for implementing text-to-speech in dialog sys-tems, as well as in detection of prosody in conversa-tional speech recognition.Previous investigations of prominence predictionfrom text have primarily relied on robust surface fea-tures with some deeper information structure fea-tures.
Surface features like a word?s part-of-speech(POS) (Hirschberg, 1993) and its unigram and bi-gram probability (Pan and McKeown, 1999; Pan and0Thanks to the Edinburgh-Stanford Link and ONR (MURIaward N000140510388) for generous support.Hirschberg, 2000) are quite useful; content wordsare much more likely to be accented than functionwords, and words with higher probability are lesslikely to be prominent.
More sophisticated linguis-tic features have also been used, generally based oninformation-structural notions of contrast, focus, orgiven-new.
(Hirschberg, 1993).For example, in the Switchboard utterance be-low, there is an intrinsic contrast between the words?women?
and ?men?, making both terms moresalient (words in all capital letters represent promi-nent tokens):you SEE WOMENc GOING off to WARS as WELL asMENc.Similarly the givenness of a word may help deter-mine its prominence.
The speaker needs to focus thehearer?s attention on new entities in the discourse, sothese are likely to be realized as prominent.
Old en-tities, on the other had, need not be prominent; thesetendencies can be seen in the following example.theyold have all the WATERnew theyold WANT.
theyoldcan ACTUALLY PUMP waterold.While previous models have attempted to cap-ture global properties of words (via POS or unigramprobability), they have not in general used wordidentity as a predictive feature, assuming either thatcurrent supervised training sets would be too smallor that word identity would not be robust across gen-res (Pan et al, 2002).
In this paper, we show a wayto capture word identity in a feature, accent ratio,that works well with current small supervised train-ing sets, and is robust to genre differences.We also use a corpus which has been hand-labeled for information structure features (includinggiven/new and contrast information) to investigatethe relative usefulness of both linguistic and shallowfeatures, as well as how well different features com-bine with each other.92 Data and featuresFor our experiments we use 12 Switchboard (God-frey et al, 1992) conversations, 14,555 tokens in to-tal.
Each word was manually labeled for presenceor absence of pitch accent1 , as well as additionalfeatures including information status (or givenness),contrast and animacy distinctions, (Nissim et al,2004; Calhoun et al, 2005; Zaenen et al, 2004), fea-tures that linguistic literature suggests are predictiveof prominence (Bolinger, 1961; Chafe, 1976).All of the features described in detail below havebeen shown to have statistically significant correla-tion with prominence (Brenier et al, 2006).Information status The information status (IS),or givenness, of discourse entities is important forchoosing appropriate reference form (Prince, 1992;Gundel et al, 1993) and possibly plays a role inprominence decisions as well (Brown, 1983).
Noprevious studies have examined the usefulness ofinformation status in naturally occurring conversa-tional speech.
The annotation in our corpus is basedon the givenness hierarchy of Prince: first mentionsof entities were marked as new and subsequent men-tions as old.
Entities that are not previously men-tioned, but that are generally known or semanticallyrelated to other entities in the preceding context aremarked as mediated.
Obviously, the givenness an-notation applies only to referring expressions, i.e.noun phrases the semantic interpretation of which isa discourse entity.
This restriction inherently limitsthe power of the feature for prominence prediction,which has to be performed for all classes of words.Complete details of the IS annotation can be foundin (Nissim et al, 2004).Kontrast One reason speakers make entities inan utterance prominence is because of informationstructure considerations (Rooth, 1992; Vallduv??
andVilkuna, 1998).
That is, parts of an utterance whichdistinguish the information the speaker actually saysfrom the information they could have said, are madesalient, e.g.
because that information answers aquestion, or contrasts with a similar entity in thecontext.
Several possible triggers of this sort ofsalience were marked in the corpus, with words thatwere not kontrastive (in this sense) being marked asbackground:1Of all tokens, 8,429 (or 58%) were not accented.?
contrastive if the word is directly differentiatedfrom a previous topical or semantically-relatedword;?
subset if it refers to a member of a more generalset mentioned in the surrounding context;?
adverbial if a focus-sensitive adverb such as?only?
or ?even?
is associated with the wordbeing annotated;?
correction if the speaker intended to correct orclarify a previous word or phrase;?
answer if the word completes a question by theother speaker;?
nonapplic for filler phrases such as ?in fact?, ?Imean?, etc.Note that only content words in full sentenceswere marked for kontrast, and filler phrases suchas ?in fact?
and ?I mean?
were excluded.
A com-plete description of the annotation guidelines can befound in (Calhoun et al, 2005).Animacy Each noun and pronoun is labeled for theanimacy of its referent (Zaenen et al, 2004).
Thecategories include concrete, non-concrete, human,organizations, place, and time.Dialog act Specifies the function of the utterancesuch as statement, opinion, agree, reject, abandon;or type of question (yes/no, who, rhetoric)In addition to the above theoretically motivatedfeatures, we used several automatically derivableword measures.Part-of-speech Two such features were used, thefull Penn Treebank tagset (called POS) , and a col-lapsed tagset (called BroadPOS) with six broad cat-egories (nouns, verbs, function words, pronouns, ad-jectives and adverbs).Unigram and bigram probability These featuresare defined as log(pw) and log(pwi |pwi?1) respec-tively and their values were calculated from theFisher corpus (Cieri et al, 2004).
High probabilitywords are less likely to be prominent.TF.IDF This measure captures how central a word isfor a particular conversation.
It is a function of thefrequency of occurrence of the word in the conver-sation (nw), the number of conversations that con-tain the word in a background corpus (k) and thenumber of all conversations in the background cor-pus (N ).
Formally, TF.IDF1 = nw ?
log(Nk ).
We10also used a variant, TF.IDF2, computed by normal-izing TF.IDF1 by the number of occurrences of themost frequent word in the conversation.
TF.IDF2 =TF.IDF1/max(nw?conv).
Words with high TF.IDFvalues are important in the conversation and aremore likely to be prominent.Stopword This is a binary feature indicating if theword appears in a high-frequency stopword list fromthe Bow toolkit (McCallum, 1996).
The list spansboth function and content word classes, though nu-merals and some nouns and verbs were removed.Utterance length The number of words.Length The number of characters in the words.
Thisfeature is correlated with phonetic features that havebeen shown to be useful for the task, such as thenumber of vowels or phones in the word.Position from end/beginning The position of theword in the utterance divided by the number ofwords that precede the current word.Accent ratio This final (new) feature takes the?memorization?
of previous productions of a givenword to the extreme, measuring how likely it is thata word belongs to a prominence class or not.
Ourfeature extends an earlier feature proposed by (Yuanet al, 2005), which was a direct estimate of howlikely it is for the word to be accented as observedin some corpus.
(Yuan et al, 2005) showed that theoriginal accent ratio feature was not included in thebest set of features for accent prediction.
We believethe reason for this is the fact that the original ac-cent ratio feature was computed for all words, evenwords in which the value was indistinguishable fromchance (.50).
Our new feature incorporates the sig-nificance of the prominence probability, assuming adefault value of 0.5 for those words for which thereis insufficient evidence in the training data.
Morespecifically,AccentRatio(w) ={kn if B(k, n, 0.5) ?
0.050.5 otherwisewhere k is the number of times word w appearedaccented in the corpus, n is the total number oftimes the word w appeared, and B(k, n, 0.5) isthe probability (under a binomial distribution) thatthere are k successes in n trials if the probabil-ity of success and failure is equal.
Simply put,the accent ratio of a word is equal to the esti-mated probability of the word being accented if thisprobability is significantly different from 0.5, andequal to 0.5 otherwise.
For example, AccentRa-tio(you)=0.3407, AccentRatio(education)=0.8666,and AccentRatio(probably)=0.5.Many of our features for accent prediction arebased only on the 12 training conversations.
Otherfeatures, such as the unigram, bigram, and TF*IDFfeatures, are computed from larger data sources.
Ac-cent ratio is also computed over a larger corpus,since the binomial test requires a minimum of sixoccurrences of a word in the corpus in order to getsignificance and assign an accent ratio value differ-ent from 0.5.
We thus used 60 Switchboard conver-sations (Ostendorf et al, 2001), annotated for pitchaccent, to compute k and n for each word.3 ResultsFor our experiments we used the J48 decision treesin WEKA (Witten and Frank, 2005).
All the resultsthat we report are from 10-fold cross-validation onthe 12 Switchboard conversations.Some previous studies have reported results onprominence prediction in conversational speech withthe Switchboard corpus.
Unfortunately these studiesused different parts of the corpus or different label-ings (Gregory and Altun, 2004; Yuan et al, 2005),so our results are not directly comparable.
Bear-ing this difference in mind, the best reported resultsto our knowledge are those in (Gregory and Altun,2004), where conditional random fields were usedwith both textual, acoustic, and oracle boundary fea-tures to yield 76.36% accuracy.Table 1 shows the performance of decision treeclassifiers using a single feature.
The majority classbaseline (not accented) has accuracy of 58%.
Accentratio is the most predictive feature: the accent ratioclassifier has accuracy of 75.59%, which is two per-cent net improvement above the previously knownbest feature (unigram).
The accent ratio classifierassigns a ?no accent?
class to all words with accentratio lower than 0.38 and ?accent?
to all other words.In Section 4 we discuss in detail the accent ratio dic-tionary, but it is worth noting that it does correctlyclassify even some high-frequency function wordslike ?she?, ?he?, ?do?
or ?up?
as accented.113.1 Combining featuresWe would expect that a combination of featureswould lead to better prediction when compared toa classifier based on a single feature.
Several paststudies have examined classes of features.
In orderto quantify the utility of different specific features,we ran exhaustive experiments producing classifierswith all possible combinations of two, three, fourand five features.As we can see from figure 1 and table 2, the clas-sifiers using accent ratio as a feature perform best,for all sizes of feature sets.
Moreover, the increaseof performance compared to a single-feature classi-fier is very slight when accent ratio is used as fea-ture.
Kontrast seems to combine well with accentratio and all of the best classifiers with more thanone feature use kontrast in addition to accent ratio.This indicates that automatic detection of kontrastcan potentially help in prominence prediction.
Butthe gains are small, the best classifiers without kon-trast but still including accent ratio perform within0.2 percent of the classifiers that use both.On the other hand, classifiers that do not use ac-cent ratio perform poorly compared to those that do,and even a classifier using five features (unigram,broad POS, token length, position from beginningand bigram) performs about as well as a classifierusing solely accent ratio as a feature.
Also, whenaccent ratio is not used, the overall improvement ofthe classifier grows faster with the addition of newfeatures.
This suggest that accent ratio provides richinformation about words beyond that of POS classand general informativeness.2Table 2 gives the specific features in (n + 1)-feature classifiers that lead to better results than thebest n-classifier.
The figures are for the classifiersperforming best overall.
Interestingly, none of thesebest classifiers for all feature set sizes uses POS orunigram as a feature.
We assume that accent ratiocaptures all the relevant information that is presentin the unigram and POS features.
The best classifierwith five features uses, in addition to accent ratio,kontrast, tf.idf, information status and distance fromthe beginning of the utterance.
All of these featuresconvey somewhat orthogonal information: seman-2To verify this we will examine the accent ratio dictionaryin closer detail in the next section.Accent Ratio (AR) 75.59%AR + Kontrast 76.15%AR + END/BEG 75.91%AR + tf.idf2 75.82%AR + Info Status 75.82%AR + Length 75.77%AR + tf.idf1 75.74%AR + unigram 75.71%AR + stopword 75.70%AR + kontrast + length 76.45%AR + kontrast + BEG 76.24%AR + kontrast + unigram 76.24%AR + kontrast + tf.idf1 76.24%AR + kontrast + length + tfidf1 76.56%AR + kontrast + length + stopword 76.54%AR + kontrast + length +tf.idf2 76.52%AR + kontrast + Status + BEG 76.47%AR + kontrast + tf.idf1 + Status + BEG 76.65%AR + kontrast + tf.idf2 + Status + BEG 76.58%Table 2: Performance increase augmenting the ac-cent ratio classifier.tic, topicality, discourse and phrasing informationrespectively.
Still, all of them in combination im-prove the performance over accent ratio as a singlefeature only by one percent.Figure 1 shows the overall improvement of clas-sifiers with the addition of new features in three sce-narios: overall best, best when kontrast is not usedas a feature and best with neither kontrast nor ac-cent ratio.
The best classifier with five features thatdo not include kontrast has accent ratio, broad POS,word length, stopword and bigram as features andhas accuracy of 76.28%, or just 0.27% worse thanthe overall best classifier that uses kontrast and in-formation status.
This indicates that while there issome benefit to using the two features, they do notlead to any substantial boost in performance.
Strik-ingly, the best classifier that uses neither accent ra-tio nor kontrast performs very similarly to a classi-fier using accent ratio as the only feature: 75.82%for the classifier using unigram, POS, tf.idf1, wordlength and position from end of the utterance.3.2 The power of linguistic featuresOne of the objectives of our study was to assess howuseful gold-standard annotations for complex lin-guistic features are for the task of prominence pre-diction.
The results in this section indicate that an-imacy distinctions (concrete/non-concrete, person,time, etc) and dialog act did not have much power12AccentRatio unigram stopword POS tf.idf2 tf.idf1 BroadPos Length Kontrast bigram Info Stat75.59 73.77 70.77 70.28 70.14 69.50 68.64 67.64 67.57 65.87 64.13Table 1: Single feature classifier performance.
Features not in the table (position from end, animacy, utter-ance length and dialog act) all achieve lower accuracy of around 60%1 2 3 4 5737475767778Classifier performanceNumber of featuresPredictionaccuracyOverall bestWithout kontrastWithout accent+ ratio or kontrastFigure 1: Performance increase with the addition ofnew features.as individual features (table 1) and were never in-cluded in a model that was best for a given featureset size (table 2).Information status is somewhat useful and ap-pears in the overall best classifier with five features(table 2).
But when compared with other classifierswith the same number of features, the benefits fromadding information status to the model are small.For example, the accent ratio + information statusclassifier performs 0.23% better than accent ratioalone, but so does the classifier using accent ratioand tf.idf.
There are two reasons that can explainwhy the givenness of the referent is not as helpfulas we might have hoped.
First of all, the informa-tion status distinction applies only to referring ex-pressions and has undefined values for words suchas verbs, adjectives or function words.
Second, in-formation status of an entity influences the form ofreferring expression that is used, with old items be-ing more likely to be pronominalized.
In the numer-ous cases where pronominalization of old informa-tion does occur, features such as POS, unigram oraccent ratio will be sensitive to the change of infor-mation status simply based on the lexical item.Kontrast is by far the most useful linguistic fea-ture.
It is used in all of the best classifiers for anyfeature set size (table 2).
It applies to more wordsthan givenness does, since salience distinctions canbe made for any part-of-speech class.
Still, not allwords were annotated for kontrast either, and more-over kontrast only captures one kind of semanticsalience.
This is particularly true of discourse mark-ers like ?especially?
or ?definitely?
: these would ei-ther be in sentence fragments that weren?t markedfor kontrast, or would probably be marked as ?back-ground?
since they are not salience triggers in a se-mantic sense.
As we can see from figure 1, clas-sifiers that use kontrast perform only slightly betterthan others that use only ?cheaper?
features.4 The accent ratio dictionaryContrary to our initial expectations, both classes inthe accent ratio dictionary (for both low and highprobability of being prominent) cover the full set ofpossible POS categories.
Tables 3 and 4 list words inboth classes (with words sorted by increasing accentratio in each column).
The ?no accent?
class is dom-inated by function words, but also includes nounsand verbs.
One of the drawbacks of POS as a fea-ture for prominence prediction is that normally aux-iliary verbs will be tagged as ?VB?, the same classas other more contentful verbs.
The informativeness(unigram probability) of a word would distinguishbetween these types of verbs, but so does the accentratio measure as well.Furthermore, some relatively frequent words suchas ?too?, ?now?, ?both?, ?no?, ?yes?, ?else?, ?wow?have high accent ratio, that is, a high probability foraccenting.
Such distinctions within the class of func-tion words would not be possible on the basis of in-13.00?.08 .09?.16 .17?.24 .25?.32 .33?.42a could you?d being meuh in because take i?veum minutes oh said we?reuh-huh and since wanna wentthe by says been overan who us those youof grew where into thingto cause they?ve little whatwere gonna am until someas about sort they?re outthan their you?re I hadwith but didn?t that makeat on her don?t wayfor be going this didfrom through i?ll should anythingor which will type i?myou?ve are our we kindwas we?ll just so gowould during though have stuffit huh like got thenwhen is your new shethem bit needs mean heit?s there?s my much doif any many i?d upcan has they knowhim stayed get doesn?tthese supposed there evenTable 3: Accent ratio entries with low prominenceprobability.formativeness, POS, or even information structurefeatures.
Another class like that is words like ?yes?,?okay?, ?sure?
that are mostly accented by virtue ofbeing the only word in the phrase.Some rather common words, ?not?
for example,are not included in the accent ratio dictionary be-cause they do not exhibit a statistically strong pref-erence for a prominence class.
The accent ratio clas-sifier would thus assign class ?accented?
to the word?not?, which is indeed the class this word occurs inmore often.Another fact that becomes apparent with the in-spection of the accent ratio dictionary is that whilecertain words have a statistically significant prefer-ence for deaccenting, there is also a lot of variationin their observed realization.
For example, personalpronouns such as ?I?
and ?you?
have accent ratiosnear 0.33.
This means that every third such pronounwas actually realized as prominent by the speaker.In a conversational setting there is an implicit con-trast between the two speakers, which could partlyexplain the phenomenon, but the situations whichprompt the speaker to realize the distinction in their.58?.74 .75?.79 .80?.86 .87?1.0lot both sometimes halftime no change topicnow seems child elsekids life young obviouslyold tell Texas themselvestoo ready town wowreally easy room goshthree heard pay anywaywork isn?t interesting Dallasnice again true outsideyeah first mother mostlytwo right problems yesperson children agree greatday married war exactlyworking may needed especiallyjob happen told definitelytalking business finally latelyusually still neat thirtyrather daughter sure higherplaces gone house fortygovernment guess okay heyten news seven Iowaparents major best poorpaper fact also gladactually five older basicTable 4: Accent ratio values for words with highprobability for being accented.speech will be the focus of a future linguistic inves-tigation.Kontrast is helpful in predicting ?accented?
classfor some generally low ratio words.
However, evenwith its help, production variation in the conversa-tions cannot be fully explained.
The following ex-amples from our corpus show low accent ratio words(that, did, and, have, had) that were produced asprominent.so i did THAT.
and then i, you know, i DID that for SIXyears.
AND then i stayed HOME with my SON.i HAVE NOT, to be honest, HAD much EXPERIENCEwith CHILDREN in that SITUATION.they?re going to HAVE to WORK it OUT to WORKINGpart TIME.The examples attest to the presence of variationin production: in the first utterance, for example, wesee the words ?did?, ?and?
and ?that?
produced bothas prominent and not prominent.
Intonational phras-ing most probably accounts for some of this varia-tion since it is likely that even words that are typ-ically not prominent will be accented if they occurjust before or after a longer pause.
We come back tothis point in the closing section.145 Robustness of accent ratioWhile accent ratio works well for our data (Table2), a feature based so strongly on memorizing thestatus of each word in the training data might leadto problems.
One potential problem, suggested byPan et al (2002) for lexicalized features in general,is whether a lexical feature like accent ratio mightbe less robust across genres.
Another question iswhether our definition of accent ratio is better thanone that does not use the binomial test: we need toinvestigate whether these statistical tests indeed im-prove performance.
We focus on these two issues inthe next two subsections.Binomial test cut-offAs discussed above, the original accent ratio feature(Yuan et al, 2005) was based directly on the frac-tion of accented occurrences in the training set.
Wemight expect such a use of raw frequencies to beproblematic.
Given what we know about word dis-tributions in text (Baayen, 2001), we would expectabout half of the words in a big corpus to appear onlyonce.
In an accent ratio dictionary without binomialtest cut-off, all such words will have accent ratio ofeither exactly 1 or 0, but one or even few occurrencesof a word would not be enough to determine statis-tical significance.
By contrast, our modified accentratio feature uses binomial test cut-off to make theaccent ratio more robust to small training sets.To test if the binomial test cut-off really improvedthe accent ratio feature, we compared the perfor-mance on Switchboard of classifiers using accentratio with and without cut-off.
The binominal testimproved the performance of the accent ratio fea-ture from 73.49% (Yuan et al original version) to75.59% (our version).Moreover, Yuan et al report that their version ofthe feature did not combine well with other features,while in our experiments best performance was al-ways achieved by the classifiers that made use of theaccent ratio feature in addition to others.A cross-genre experiment: broadcast newsIn a systematic analysis of the usefulness of differ-ent informativeness, syntactic and semantic featuresfor prominence prediction, Pan et al (2002) showedthat word identity is a powerful feature.
But they hy-pothesized that this would not be a useful feature ina domain independent pitch accent prediction task.Their hypothesis that word identity cannot be a ro-bust across genres would obviously carry over to ac-cent ratio.
In order to test the hypothesis, we usedthe accent ratio dictionary derived from the Switch-board corpus to predict prominence in the BostonUniversity Radio corpus of broadcast news.
Usingan accent ratio dictionary from Switchboard and as-signing class ?not accented?
to words with accent ra-tio less than 0.38 and ?accented?
otherwise leads to82% accuracy of prediction for this broadcast newscorpus.
If the accent ratio dictionary is built fromthe BU corpus itself, the performance is 83.67%.3These results indicate that accent ratio is a robustenough feature and is applicable across genres.6 Conclusions and future workIn this paper we introduced a new feature for promi-nence prediction, accent ratio.
The accent ratio ofa word is the (maximum likelihood estimate) prob-ability that a word is accented if there is a signifi-cant preference for a class, and 0.5 otherwise.
Ourexperiments demonstrate that the feature is power-ful both by itself and in combination with other fea-tures.
Moreover, the feature is robust to genre, andaccent ratio dictionaries can be used for predictionof prominence in read news with very good results.Of the linguistic features we examined, kontrastis the only one that is helpful beyond what can begained using shallow features such as n-gram prob-ability, POS or tf.idf.
While the improvements fromkontrast are relatively small, the consistency of thesesmall improvements suggest that developing auto-matic methods for approximating the gold-standardannotation we used here, similar to what has beendone for information status in (Nissim, 2006), maybe worthwhile.
An automatic predictor for kontrastmay also be helpful in other applications such asquestion answering or textual entailment.All of the features in our study were text-based.There is a wide variety of research investigatingphonological or acoustic features as well.
For exam-ple Gregory and Altun (2004) used acoustic features3This result is comparable with the result of (Yuan et al,2005) who in their experiment with the same corpus report thebest result as 83.9% using three features: unigram, bigram andbackwards bigram probability.15such as duration and energy, and phonological fea-tures such as oracle (hand-labeled) intonation phraseboundaries, and the number of phones and sylla-bles in a word.
Although acoustic features are notavailable in a text-to-speech scenario, we hypothe-size that in a task where such features are available(such as in speech recognition applications), acous-tic or phonological features could improve the per-formance of our text-only features.
To test this hy-pothesis, we augmented our best 5-feature classifierwhich did not include kontrast with hand-labeled in-tonation phrase boundary information.
The resultingclassifier reached an accuracy of 77.45%, more thanone percent net improvement over 76.28% accuracyof the model based solely on text features and not in-cluding kontrast.
Thus in future work we plan to in-corporate more acoustic and phonological features.Finally, prominence prediction classifiers need tobe incorporated in a speech synthesis system andtheir performance should be gauged via listeningexperiments that test whether the incorporation ofprominence leads to improvement in synthesis.ReferencesR.
H. Baayen.
2001.
Word Frequency Distributions.Kluwer Academic Publishers.D.L.
Bolinger.
1961.
Contrastive Accent and ContrastiveStress.
Language, 37(1):83?96.J.
Brenier, A. Nenkova, A. Kothari, L. Whitton,D.
Beaver, and D. Jurafsky.
2006.
The (non)utility oflinguistic features for predicting prominence in spon-taneous speech.
In IEEE/ACL 2006 Workshop on Spo-ken Language Technology.G.
Brown.
1983.
Prosodic structure and the given/newdistinction.
Prosody: Models and Measurements,pages 67?77.S.
Calhoun, M. Nissim, M. Steedman, and J.M.
Brenier.2005.
A framework for annotating information struc-ture in discourse.
Pie in the Sky: Proceedings of theworkshop, ACL, pages 45?52.W.
Chafe.
1976.
Givenness, contrastiveness, definite-ness, subjects, topics, and point of view.
Subject andTopic, pages 25?55.C.
Cieri, D. Graff, O. Kimball, D. Miller, and KevinWalker.
2004.
Fisher English training speech part 1transcripts.
LDC.J.
Godfrey, E. Holliman, and J. McDaniel.
1992.SWITCHBOARD: Telephone speech corpus for re-search and development.
In IEEE ICASSP-92.M.
Gregory and Y. Altun.
2004.
Using conditional ran-dom fields to predict pitch accents in conversationalspeech.
Proceedings of ACL, 2004.J.
Gundel, N. Hedberg, and R. Zacharski.
1993.
Cog-nitive status and the form of referring expressions indiscourse.
Language, 69:274?307.J.
Hirschberg.
1993.
Pitch Accent in Context: PredictingIntonational Prominence from Text.
Artificial Intelli-gence, 63(1-2):305?340.A.
McCallum.
1996.
Bow: A toolkit for statistical lan-guage modeling, text retrieval, classification and clus-tering.
http://www.cs.cmu.edu/ mccallum/bow.M.
Nissim, S. Dingare, J. Carletta, and M. Steedman.2004.
An annotation scheme for information status indialogue.
In LREC 2004.M.
Nissim.
2006.
Learning information status of dis-course entities.
In Proceedings of EMNLP 2006.M.
Ostendorf, I. Shafran, S. Shattuck-Hufnagel,L.
Carmichael, and W. Byrne.
2001.
A prosodicallylabeled database of spontaneous speech.
Proc.
of theISCA Workshop on Prosody in Speech Recognition andUnderstanding, pages 119?121.S.
Pan and J. Hirschberg.
2000.
Modeling local contextfor pitch accent prediction.
In Proceedings of ACL-00.S.
Pan and K. McKeown.
1999.
Word informativenessand automatic pitch accent modeling.
In Proceedingsof EMNLP/VLC-99.S.
Pan, K. McKeown, and J. Hirschberg.
2002.
Ex-ploring features from natural language generation inprosody modeling.
Computer speech and language,16:457?490.E.
Prince.
1992.
The ZPG letter: subject, definiteness,and information status.
In S. Thompson and W. Mann,editors, Discourse description: diverse analyses of afund raising text, pages 295?325.
John Benjamins.Mats Rooth.
1992.
A theory of focus interpretation.
Nat-ural Language Semantics, 1(1):75?116.E.
Vallduv??
and M. Vilkuna.
1998.
On rheme and kon-trast.
Syntax and Semantics, 29:79?108.I.
H. Witten and E. Frank.
2005.
Data Mining: Practicalmachine learning tools and techniques.
2nd Edition,Morgan Kaufmann, San Francisco.J.
Yuan, J. Brenier, and D. Jurafsky.
2005.
Pitch AccentPrediction: Effects of Genre and Speaker.
Proceed-ings of Interspeech.A.
Zaenen, J. Carletta, G. Garretson, J. Bresnan,A.
Koontz-Garboden, T. Nikitina, M.C.
O?Connor, andT.
Wasow.
2004.
Animacy Encoding in English: whyand how.
ACL Workshop on Discourse Annotation.16
