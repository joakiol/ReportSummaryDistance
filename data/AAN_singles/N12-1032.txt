2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 315?326,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsGetting More from Morphology in Multilingual Dependency ParsingMatt Hohensee and Emily M. BenderUniversity of WashingtonDepartment of LinguisticsBox 354340Seattle WA 98195-4340, USA{hohensee, ebender}@uw.eduAbstractWe propose a linguistically motivated set offeatures to capture morphological agreementand add them to the MSTParser dependencyparser.
Compared to the built-in morphologi-cal feature set, ours is both much smaller andmore accurate across a sample of 20 morpho-logically annotated treebanks.
We find in-creases in accuracy of up to 5.3% absolute.While some of this results from the feature setcapturing information unrelated to morphol-ogy, there is still significant improvement, upto 4.6% absolute, due to the agreement model.1 IntroductionMost data-driven dependency parsers are meant tobe language-independent.
They do not use anyinformation that is specific to the language beingparsed, and they often rely heavily on n-grams, orsequences of words and POS tags, to make parsingdecisions.
However, designing a parser without in-corporating any specific linguistic details does notguarantee its language-independence; even linguis-tically na?
?ve systems can involve design decisionswhich in fact bias the system towards languages withcertain properties (Bender, 2011).It is often taken for granted that using linguisticinformation necessarily makes a system language-dependent.
But it is possible to design a linguisti-cally intelligent parser without tuning it to a specificlanguage, by modeling at a high level phenomenawhich appear cross-linguistically.
Such a system isstill language-independent; it does not require anyknowledge or modeling of specific languages, butit does use linguistic knowledge to make the mostof the available data.
We present modifications toan existing system, MSTParser (McDonald et al,2006), to incorporate a very simple model of mor-phological agreement.
These modifications improveparsing performance across a variety of languagesby making better use of morphological annotations.2 Background and related work2.1 Morphological marking of agreementMost languages show some morphological agree-ment via inflected noun, adjective, verb, and deter-miner forms, although the degree to which this hap-pens varies.
At one end of the spectrum are analytic,or ?morphologically impoverished?, languages.
Anextreme example is Chinese, which shows no inflec-tion at all; words do not take different forms de-pending on features such as person or gender.
En-glish has some inflection, but is relatively morpho-logically poor.At the other end are synthetic or ?morphologi-cally rich?
languages such as Czech, which has, interalia, four genders and seven cases.
In synthetic lan-guages, words which are syntactically related in cer-tain ways must agree: e.g., subject-verb agreementfor gender or determiner-noun agreement for case(Corbett, 2006).
Words participating in agreementmay be marked explicitly for the property in ques-tion (via affixing or other morphological changes),or may possess it inherently (with no specific affixencoding the property).
Treebanks are often anno-tated to reflect some or all of these properties; thelevel of detail depends on the annotation guidelines.315zahranic?n??
investice rostouforeign investment grow.F.PL.NOM .F.3RD.PL.NOM .3RD.PL.PRESforeign investments growforeign investment grow.3RD.PL .PLTable 1: Sentence in Czech (Hajic?, 1998) and EnglishA sample sentence in English and Czech (Table 1)demonstrates this contrast.
In Czech, the adjectiveand noun agree for gender, number, and case, andthe noun and verb agree for person and number.
Inthe English version, only the noun and verb agree.Agreement can be very useful for data-driven de-pendency parsing.
A statistical parser can learn fromtraining data that, for example, a third-person singu-lar noun is a likely dependent of a verb marked asthird-person singular.
Similarly, it can learn that adeterminer showing genitive case and a noun show-ing dative case are often not syntactically related.It is often assumed that morphological complex-ity correlates with degree of variation in word order.This is because synthetic languages use inflection tomark the roles of constituents, while analytic lan-guages generally assign these roles to specific phrasestructural locations.
Siewierska (1998) investigatedthis empirically and found that it holds to a certainextent: the absence of agreement and/or case mark-ing predicts rigid word order, though their presenceis not particularly predictive of flexible word order.Many parsers rely on word order to establish de-pendencies, so they often perform best on languageswith more rigid word order.
Making use of mor-phological agreement could compensate for greatervariation in word order and help to bring parsing per-formance on flexible-word-order languages up to parwith that on rigid-word-order languages.2.2 MSTParserThe CoNLL-X (Buchholz and Marsi, 2006) andCoNLL 2007 (Nivre et al, 2007) shared tasks fo-cused on multilingual dependency parsing.
Eachsystem was trained on treebanks in a variety of lan-guages and predicted dependency arcs and labels forPOS-tagged data.
The best performers in 2006 wereMSTParser (McDonald et al, 2006), which we usehere, and MaltParser (Nivre et al, 2006a).MSTParser is a data-driven, graph-based parserwhich creates a model from training data by learn-ing weights for arc-level features.
The feature set in-cludes combinations of the word and POS tag of theparent and child of each dependency arc; POS tagsof words between the parent and child; and POS tagsof the parent and child along with those of the pre-ceding and following words.
A similar feature set isconjoined with arc labels in order to perform label-ing, and an optional set of ?second-order?
featuresincludes analogous information about siblings.Morphological features for an arc are generatedby iterating over each pair in the cross product ofthe parent and child tokens?
lists of attributes.
Forevery such pair, thirteen groups of four features eachare generated.
The thirteen groups represent combi-nations of the head and child word forms/lemmasand attributes.
Each group contains subgroups dis-tinguished by whether they use word forms or lem-mas and by whether or not they encode the direc-tion and distance of the dependency.
These featuresare summarized in Table 2.
At run time, MSTParserfinds the highest-scoring parse for each sentence ac-cording to the learned feature weights.Decoding can be performed in projective or non-projective mode, depending on the type of trees de-sired.
Projective trees are those in which every con-stituent (head plus all dependents) forms a completesubtree; non-projective parsing lacks this limitation.2.3 Related workThe organizers of the CoNLL 2007 shared tasknoted that languages with free word order and highmorphological complexity are the most difficult fordependency parsing (Nivre et al, 2007).
Most of theparticipants took language-independent approachestoward leveraging this complexity into better perfor-mance: generating machine learning features basedon each item in a token?s list of morphological at-tributes (Nivre et al, 2006b; Carreras et al, 2006);using the entire list as an atomic feature (Chang etal., 2006; Titov and Henderson, 2007); or generat-ing features based on each pair of attributes in thecross-product of the lists of a potential head and de-pendent (McDonald et al, 2006; Nakagawa, 2007).Language-specific uses of morphological infor-mation have included using it to disambiguate func-tion words (Bick, 2006) or to pick out finite verbs316<hdIdx>*<dpIdx>=<{hdForm|hdLemma}>(<dir+dist>)<hdIdx>*<dpIdx>=<{dpForm|dpLemma}>(<dir+dist>)<hdIdx>*<dpIdx>=<hdAtt>(<dir+dist>)<hdIdx>*<dpIdx>=<dpAtt>(<dir+dist>)<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><{dpForm|dpLemma}>(<dir+dist>)<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><hdAtt>(<dir+dist>)<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><dpAtt>(<dir+dist>)<hdIdx>*<dpIdx>=<{dpForm|dpLemma}><dpAtt>(<dir+dist>)<hdIdx>*<dpIdx>=<{dpForm|dpLemma}><hdAtt>(<dir+dist>)<hdIdx>*<dpIdx>=<hdAtt><dpAtt>(<dir+dist>)<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><hdAtt><dpAtt>(<dir+dist>)<hdIdx>*<dpIdx>=<{dpForm|dpLemma}><hdAtt><dpAtt>(<dir+dist>)<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><{dpForm|dpLemma}><hdAtt><dpAtt>(<dir+dist>)Table 2: Original MSTParser feature templates.
hdForm and dpForm are the head and dependent word forms;hdLemma and dpLemma are the lemmas.
hdAtt and dpAtt are the morphological attributes; hdIdx and dpIdxare their indices.
dir+dist is a string encoding the direction and length of the arc.
Each line represents one feature.Unlabeled<attr>_agrees,head=<headPOS>,dep=<depPOS><attr>_disagrees,head=<headPOS>,dep=<depPOS>head_<attr=value>,head=<headPOS>,dep=<depPOS>dep_<attr=value>,head=<headPOS>,dep=<depPOS>Labeled<attr>_agrees&label=<label>,head=<headPOS>,dep=<depPOS><attr>_disagrees&label=<label>,head=<headPOS>,dep=<depPOS>head_<attr=value>&label=<label>,head=<headPOS>,dep=<depPOS>dep_<attr=value>&label=<label>,head=<headPOS>,dep=<depPOS>Table 3: Agreement feature templates.
headPOS and depPOS are the head and dependent coarse POS tags.
(Carreras et al, 2006).
Schiehlen and Spranger(2007) used language-specific rules to add detail toother features, such as fine-grained POS tags or lem-mas.
Attardi et al (2007) modeled agreement ex-plicitly, generating a morphological agreement fea-ture whenever two tokens possess the same valuefor the same linguistic attribute.
The authors noteaccuracy improvements of up to 0.5% for Italianand 0.8% for Catalan using a transition-based parser.A similar approach was used by Goldberg and El-hadad (2010), who improved the accuracy of theirtransition-based Hebrew parser by adding featuresfor gender and number agreement in noun phrases.The potential of morphological information to im-prove parsing performance has been documented innumerous experiments using MaltParser and withvarious morphological attributes as machine learn-ing features, on several morphologically rich lan-guages, including: Russian (Nivre et al, 2008);Swedish (?vrelid and Nivre, 2007); Bangla, Tel-ugu, and Hindi (Nivre, 2009); Turkish (Eryig?it etal., 2008); and Basque (Bengoetxea and Gojenola,2010).
These experiments, however, did not includeany higher-level features such as agreement.Goldberg and Elhadad (2009) found that usingmorphological features increased the accuracy ofMSTParser on Hebrew only when the morpholog-ical annotations were gold-standard; automatic an-notations decreased accuracy, although MaltParsershowed improvement with both gold and automaticannotations.
The accuracy of MaltParser on Arabicwas improved by different types of morphologicalfeatures depending on whether gold or automatic an-notations were used (Marton et al, 2010).As far as we can tell, no language-independentapproaches to utilizing morphological data thus farhave taken advantage of agreement specifically.
Wetake a linguistically informed approach, maintain-ing language-independence, by explicitly modelingagreement between head and dependent morphol-ogy.3 Methodology3.1 Modifications to parserOur approach builds on the observation that thereare two kinds of information marked in morphol-ogy: symmetric, recorded on both head and depen-317ID TOKEN CPOS MORPH HEAD REL Gloss1 Vznikaj??
VERB num=PL|per=3 0 ROOT arise.3RD.PL2 zbytec?ne?
ADJ num=PL|gen=I|case=NOM 3 ATR unnecessary.PL.INAN.NOM3 konflikty NOUN num=PL|gen=I|case=NOM 1 SBJ conflicts.PL.INAN.NOMnum_agrees,head=NOUN,dep=ADJ num_agrees,head=VERB,dep=NOUNnum_agrees&label=ATR,head=NOUN,dep=ADJ num_agrees&label=SBJ,head=VERB,dep=NOUNgen_agrees,head=NOUN,dep=ADJ head_per=3,head=VERB,dep=NOUNgen_agrees&label=ATR,head=NOUN,dep=ADJ head_per=3&label=SBJ,head=VERB,dep=NOUNcase_agrees,head=NOUN,dep=ADJ dep_gen=I,head=VERB,dep=NOUNcase_agrees&label=ATR,head=NOUN,dep=ADJ dep_gen=I&label=SBJ,head=VERB,dep=NOUNdep_case=NOM,head=VERB,dep=NOUNdep_case=NOM&label=SBJ,head=VERB,dep=NOUNTable 4: Sample sentence (Hajic?, 1998) and agreement features generateddent, and asymmetric, marked on only one or theother.
Symmetric information provides a natural,effectively non-lossy type of back-off that parserscan take advantage of; all that matters is whetherthe information on the head and dependent match.1Furthermore, we don?t need to know ahead of timewhich types of morphological information are sym-metric.
This is extracted from the annotations.In order to take advantage of this property of nat-ural language, we devised a set of features whichmodel agreement.
These allow the learner to op-erate at a higher level, using agreement itself as afeature rather than having to discover agreement andforming generalizations about whether tokens whichagree (or disagree) in various ways are related.
Sinceagreement appears cross-linguistically, such featuresare applicable to a diverse set of languages.Since MSTParser breaks down every parse into aset of arcs, our features are defined at the arc level.Each arc is a head and dependent pair, and each ofthose tokens has a list of morphological features inthe normalized form attribute=value.
We com-pare these lists and add, for every attribute whichis present in both, either an agreement or a disagree-ment feature, depending on whether the head and de-pendent have the same value for that attribute.
Thisfeature encapsulates the attribute, but not the value,as well as the coarse POS tags of the head and thedependent.
If an attribute is present in only one of1If an attribute is marked on both head and dependent andthe value matches, the specific value should not affect the prob-ability or possibility of the dependency relationship.
If the sameattribute is marked on both elements but is independent (not amatter of agreement) we risk losing information, but we hypoth-esize that such information is unlikely to be very predictive.the lists, we add a feature encapsulating whether thetoken is the head or the dependent, the single mor-phological feature (attribute and value), and the twocoarse POS tags.
We also generate both types of fea-tures conjoined with the arc label.
Like the originalfeature set, we include only first-order morphologi-cal features.
See Table 3 for a summary.
A samplesentence in a simplified CoNLL format and the fea-tures it would trigger are shown in Table 4.2We hypothesize that these agreement features willfunction as a type of back-off, allowing the parserto extract more information from the morphologicalmarking.
For instance, they can capture case agree-ment between a determiner and noun.
We expectthat this would lead to higher parsing accuracy, espe-cially when training on smaller datasets, where mor-phological data might be sparse.We made a slight modification to the parser so thatunderscores used in the treebanks to indicate the ab-sence of morphological annotation for a token werenot themselves treated as morphological informa-tion.
This was necessary to ensure that all featureconfigurations performed identically on treebankswith no morphological information.
Depending onthe treebank, this increased or decreased the perfor-mance of the system slightly (by less than 0.5%).3.2 Data collection and preparationWe gathered a range of dependency treebanks, rep-resenting as many language families as possible (Ta-ble 5).
Many of these used the CoNLL shared tasktreebank format, so we adopted it as well, and con-2A more complete description of the system, as well assource code, can be found in (Hohensee, 2012).318Language ISO Treebank Num.sents.Ref.sizeAvg.atts.ReferenceHindi-Urdu hin HUTB 3,855 2,800 3.6 (Bhatt et al, 2009)Hungarian hun Szeged DTB 92,176 9,000 3.3 (Vincze et al, 2010)Czech ces PDT 1.0 73,068 9,000 2.8 (Hajic?, 1998)Tamil tam TamilTB v0.1 600 600 2.8 (Ramasamy and Z?abokrtsky?, 2011)Slovene slv SDT 1,998 1,500 2.6 (Dz?eroski et al, 2006)Danish dan DDT 5,512 5,500 2.4 (Kromann, 2003)Basque eus 3LB* 3,175 2,800 2.4 (Aduriz et al, 2003)Dutch nld Alpino 13,735 9,000 2.4 (Van der Beek et al, 2002)Latin lat LDT 3,423 2,800 2.4 (Bamman and Crane, 2006)Bulgarian bul BulTreeBank 13,221 9,000 2.1 (Simov et al, 2004)Greek (ancient) grc AGDT 21,104 9,000 2.1 (Bamman et al, 2009)Finnish fin Turku 4,307 2,800 2.0 (Haverinen et al, 2010)German deu NEGRA 3,427 2,800 2.0 (Brants et al, 1999)Turkish tur METU-Sabanci 5,620 5,500 1.6 (Oflazer et al, 2003)Catalan cat CESS-ECE* 3,512 2,800 1.5 (Mart?
et al, 2007)Arabic ara PADT 1.0 2,367 2,300 1.2 (Hajic et al, 2004)Italian ita TUT 2,858 2,800 1.1 (Bosco et al, 2000)Portuguese por Floresta 9,359 9,000 1.0 (Afonso et al, 2002)Hebrew (modern) heb DepTB 6,214 5,500 0.9 (Goldberg, 2011)English eng Penn* 49,208 9,000 0.4 (Marcus et al, 1993)Chinese cmn Penn Chinese 28,035 9,000 0.0 (Xue et al, 2005)*Acquired as part of NLTK (Bird et al, 2009)Table 5: Language, ISO 639-2 code, treebank name, total number of sentences, reference size, average number ofmorphological attributes per token, and reference for each treebank used, ordered by average number of attributes.verted the other treebanks to the same.
It includesfor each token: position in the sentence; the tokenitself; a lemma (not present in all datasets); a coarsePOS tag; a fine POS tag; a list of morphological fea-tures; the token?s head; and the label for the depen-dency relation to that head.3 We retained all punctu-ation and other tokens in the treebanks.The POS tagsets used in the treebanks variedwidely.
We normalized the coarse tags to the univer-sal twelve-tag set suggested by Petrov et al (2011),in order to ensure that every treebank had coarse tagsfor use in the agreement features, and to make thefeatures easier to interpret.
It is unlikely that infor-mation was lost in this process: for treebanks withone set of tags, information was added, and for thosewith two, the universal tags aligned closely with thecoarse tags already in the data.Two of the treebanks we used included no mor-phological information.
We included the Penn Chi-nese Treebank as a representative of analytic lan-guages.4 We also included part of the (English) Penn3The original format also included two more fields, projec-tive head and label; neither is used by MSTParser.4Dependency trees were generated from the Penn ChineseTreebank, converted to dependency trees.
For thisdata we generated morphological annotations basedon fine POS tags, consisting of person and numberinformation for nouns and verbs, and person, num-ber, and case information for pronouns.
The GermanNEGRA corpus includes detailed morphological an-notations for about 3,400 sentences (of 20,600), andwe used only that portion.Note that the amount of morphological informa-tion present in any given treebank is a function of themorphological properties of the language as well asthe annotation guidelines: annotations do not nec-essarily encode all of the morphological informa-tion which is actually marked in a language.
Fur-thermore, the presence of a morphological featuredoes not imply that it participates in an agreementrelationship; it merely encodes some piece of mor-phological information about the token.
Finally, an-notation guidelines vary as to whether they providefor the explicit marking of morphological proper-ties which are inherent to a lemma (e.g., gender onnouns) and not marked by separate affixes.Treebank using the Penn2Malt converter: http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html.319We normalized all morphological annotations tothe form attribute=value (e.g., case=NOM).
Fortreebanks that provided values only, this involvedadding attribute names, obtained from the annota-tion guidelines.
The attributes person, number, gen-der, and case appeared often; also included in somedata were verb tense, adjective degree, and pronountype (e.g., personal, possessive, or reflexive).
Wenormalized all features in the data, regardless ofwhether they participate in any agreement relations.Many of the treebanks include data from multipledomains; to minimize the effects of this, we random-ized the order of sentences in each treebank.3.3 Experimental setupAll experiments were performed using 5-fold cross-validation.
Reported accuracies, run times, and fea-ture counts are averages over all five folds.
Weran experiments on multiple cross-validation datasetsizes in order to assess the performance of our modelwhen trained on different amounts of data.
For eachtreebank, we report results on a ?reference size?
:9,000 sentences or the largest size available (for tree-banks of less than 9,000 sentences).For evaluation, we used the module built intoMSTParser.
We focused on the unlabeled accu-racy score (percentage of tokens with correctly as-signed heads, ignoring labels).
We also looked atlabeled accuracies, but found they displayed trendsvery similar, if not identical, to the unlabeled scores.4 ResultsWe ran the system on each treebank at all datasetsizes in projective and non-projective modes, usingno morphological features.
For each language, sub-sequent tests used the algorithm which performedbetter (or non-projective in the case of a tie).4.1 Overall resultsWe ran the parser on each treebank with each offour feature configurations: one with no morpho-logical features (no-morph); one with the originalmorphological features (orig; Table 2); one usingthe agreement features (agr; Table 3); and one us-ing both feature sets (agr+orig).Table 6 displays the unlabeled accuracy, run time,and feature counts when parsing each treebank usingeach feature configuration at the reference size, withthe highest accuracy highlighted.
Excluding Chi-nese, agr generated the best performance in all buttwo cases, outperforming orig by margins rangingfrom 0.8% (Arabic) to 5.3% (Latin) absolute.
In theother cases, agr+orig outperformed agr slightly.In all cases, the total number of machine learningfeatures was approximately the same for no-morphand agr, and for orig and agr+orig, becausethe number of morphological features generated byorig is very large compared to the number gener-ated by agr.
Performance was noticeably faster forthe two smaller feature configurations.Figure 1 shows the error reduction of orig, agr,and agr+orig relative to no-morph, at the refer-ence size.
Despite its relative lack of morphologicalinflection, English shows a fairly high error reduc-tion, because parsing performance on English wasalready high.
Similarly, error reduction on some ofthe morphologically rich languages is lower becausebaseline performance was low.
Calculating the cor-relation coefficient (Pearson?s r) between averagemorphological attributes per token and error reduc-tion gives r = 0.608 for orig, r = 0.560 for agr,and r = 0.428 for agr+orig, with p < 0.01 forthe first two and p < 0.10 for the last, indicatingmoderate correlations for all feature sets.The strength of these correlations depends on sev-eral factors.
Languages differ in what information ismarked morphologically, and in number of agree-ment relationships.
Annotation schemes vary inwhat morphological information they encode, and inhow relevant that information is to agreement.
Somemorphologically complex languages have rigid wordorder, leading to better performance with no mor-phological features at all, and limiting the amountof improvement that is possible.
Finally, it is pos-sible that a stronger correlation is obscured by othereffects due to feature set design, as we will find later.4.2 Performance vs. dataset sizeFigures 2 presents unlabeled accuracy when parsingCzech with the orig and agr configurations.
Im-provement with agr is roughly uniform across alldataset sizes; this was the general trend for all tree-banks.
This is somewhat unexpected; we had pre-dicted that the agreement features would be morehelpful at smaller dataset sizes.320no-morph orig agr agr+origLang.
UAC time feats UAC ?time ?feats UAC ?time ?feats UAC ?time ?featshin 90.0 1.4k 1.6m 92.0 116% 893% 93.8 50% 1% 93.0 144% 893%hun 87.9 4.6k 5.3m 88.7 201% 687% 90.3 10% 0% 89.9 159% 687%ces 80.9 3.3k 4.8m 81.6 71% 454% 85.5 27% 0% 84.5 114% 454%tam 79.0 0.1k 0.5m 79.7 237% 329% 82.1 64% 1% 81.1 279% 330%slv 80.8 0.8k 1.0m 80.4 103% 352% 81.9 21% 1% 80.8 129% 353%dan 87.7 2.0k 1.6m 88.4 71% 256% 89.3 24% 0% 89.3 86% 256%lat 61.7 1.8k 1.6m 65.0 54% 306% 70.3 91% 0% 68.6 119% 306%nld 88.2 2.0k 3.6m 89.0 83% 270% 90.5 16% 0% 90.3 98% 270%eus 78.7 0.7k 1.7m 80.2 80% 229% 82.3 10% 0% 82.3 78% 230%bul 89.9 1.7k 2.6m 90.1 60% 221% 93.0 14% 0% 92.5 54% 222%grc 74.9 8.6k 3.8m 76.9 36% 314% 80.7 45% 0% 79.5 70% 314%deu 90.0 0.9k 1.3m 90.8 33% 189% 92.0 1% 0% 91.7 50% 186%fin 73.3 0.7k 2.4m 76.3 74% 244% 79.1 23% 1% 78.7 84% 245%tur 80.2 1.2k 2.1m 81.5 13% 178% 81.6 ?2% 0% 81.7 29% 178%cat 81.8 3.0k 2.5m 81.9 2% 142% 84.9 -9% 0% 84.0 ?2% 143%ara 78.0 3.2k 2.0m 78.1 65% 94% 78.9 23% 0% 78.7 20% 94%ita 88.3 4.2k 1.8m 88.9 ?3% 59% 90.2 9% 0% 90.3 6% 59%por 88.1 6.4k 5.0m 88.1 18% 46% 89.0 ?3% 0% 88.9 27% 46%heb 87.4 4.3k 3.1m 87.4 ?18% 31% 89.2 ?16% 0% 89.1 ?5% 31%eng 88.1 5.2k 3.1m 88.0 5% 7% 90.6 3% 0% 90.6 ?9% 8%cmn 82.4 7.5k 6.0m 82.4 37% 0% 82.4 16% 0% 82.4 23% 0%Table 6: Unlabeled accuracy, run time in seconds, and number of features for all treebanks and feature configurations.Run time and number of features for orig, agr, and agr+orig are given as percent change relative to no-morph4.3 Gold vs. automatic tagsThe Hebrew treebank includes both automaticallygenerated and gold standard POS and morphologicalannotations.
In order to test how sensitive the agree-ment features are to automatically predicted mor-phological information, tests were run on both ver-sions at the reference size.
These results are not di-rectly comparable to those of Goldberg and Elhadad(2009), because of the parser modifications, POS tagnormalization, and cross-validation described ear-lier.
Comparing results qualitatively, we find lesssensitivity to the automatic tags overall, and that theorig features improve accuracy even when usingautomatic tags.Results appear in Table 7.
Using the automaticdata affects all feature sets negatively by 2.1% to2.9%.
Since the no-morph parser was affected themost, it appears that this decrease is due largelyto errors in the POS tags, rather than the morpho-logical annotations.
The orig features compensatefor this slightly (0.2%), and the agr features more(0.8%); this indicates that including even automaticmorphological information can compensate for in-correct POS tags, and that the agr feature configu-ration is the most robust when given predicted tags.FeatureconfigurationAcc.
ongold dataAcc.
onauto dataDifferenceno-morph 87.4 84.5 ?2.9orig 87.4 84.7 ?2.7agr 89.3 87.2 ?2.1agr+orig 89.1 86.9 ?2.2Table 7: Unlabeled accuracy on Hebrew dataset, withgold and automatic POS and morphological annotations4.4 PPL featureExamining the feature weights from the first cross-validation fold when running the agr feature config-uration on the Czech dataset indicated that 323 of the1,000 highest-weighted features are agreement fea-tures.
Of these, 79 are symmetric (?agrees?
or ?dis-agrees?)
agr features, and 244 asymmetric.
Thiswas unexpected, as the symmetric features wouldseem to be more useful, and it suggested that the la-beled asymmetric agr features might be importantfor reasons other than their modeling of morpholog-ical information.
Careful analysis of the MSTParserfeature set revealed that it does not include a fea-ture which incorporates head POS, dependent POS,and dependency label.
We hypothesized that the la-beled asymmetric agr features were highly ranked321Figure 1: Error reduction relative to no-morph vs. languageFigure 2: Unlabeled accuracy vs. num.
sentences, Czechbecause they capture these three arc features, not be-cause they include with morphological information.To test this, we added a single feature templateto MSTParser which encapsulates head POS, de-pendent POS, and dependency label (the POS-POS-label, or PPL, feature).
Running a subsequent ex-periment on the Czech data and looking at featureweights from the same cross-validation fold, 278 ofthe 1,000 highest-weighted features were PPL fea-tures, and 187 were asymmetric agr features.
Thisindicated that the improvement seen with agr fea-tures was indeed due partly to their inclusion of fea-tures combining label and head and dependent POS.All feature configurations were run on all tree-banks with the PPL feature included; results appearin Table 8.
Performance increases from orig toagr are generally smaller, with a maximum of 4.6%absolute.
This is seen especially on languages withless morphological information, such as English andHebrew; this indicates that for those languages, mostof the previous improvement was due not to agree-ment modeling, but to the PPL effect.Calculating Pearson?s r between morphologicalfeatures per token and the new error reduction datagives a stronger correlation coefficient of 0.748 foragr, with p < 0.01, demonstrating that improve-ment due solely to agreement modeling correlatesstrongly with quantity of morphological informa-tion.
The earlier error reduction data were likelypolluted by improvement due to capturing the PPLinformation.
Correlation for the other feature con-figurations is still moderate (0.506 with p < 0.02for orig and 0.621 with p < 0.01 for agr+orig).5 Future workIn future work, we plan to experiment with morecareful normalization of treebanks.
For instance,if an adjective can agree with either a masculineor a feminine noun, annotating it with both gen=M322no-morph orig agr agr+origLang.
UAC time feats UAC ?time ?feats UAC ?time ?feats UAC ?time ?featshin 90.0 1.4k 1.6 92.0 116% 893% 93.8 50% 1% 93.0 144% 893%hun 87.9 4.6k 5.3 88.7 201% 687% 90.3 10% 0% 89.9 159% 687%ces 80.9 3.3k 4.8 81.6 71% 454% 85.5 27% 0% 84.5 114% 454%tam 79.0 0.1k 0.5 79.7 237% 329% 82.1 64% 1% 81.1 279% 330%slv 80.8 0.8k 1.0 80.4 102% 352% 81.8 21% 0% 80.8 129% 353%dan 87.8 2.0k 1.6 88.4 71% 256% 89.3 24% 0% 89.3 86% 256%lat 61.7 1.8k 1.6 65.0 54% 306% 70.3 91% 0% 68.6 119% 306%nld 88.2 2.0k 3.6 89.0 83% 270% 90.5 16% 0% 90.3 98% 270%eus 78.7 0.7k 1.7 80.2 80% 229% 82.3 10% 0% 82.3 78% 230%bul 89.9 1.7k 2.6 90.2 60% 221% 93.0 14% 0% 92.5 54% 222%grc 74.9 8.6k 3.8 77.0 36% 314% 80.7 45% 0% 79.5 70% 314%deu 90.0 0.9k 1.3 90.8 33% 189% 92.0 1% 0% 91.7 50% 186%fin 73.3 0.7k 2.4 76.3 74% 244% 79.1 23% 0% 78.7 84% 245%tur 80.2 1.2k 2.1 81.5 13% 178% 81.6 -2% 0% 81.7 29% 178%cat 81.8 3.0k 2.5 81.9 1% 142% 84.9 -9% 0% 84.0 -2% 143%ara 77.6 5.4k 1.8 77.7 20% 100% 78.2 -8% 0% 78.0 4% 100%ita 88.4 4.2k 1.8 88.9 -2% 59% 90.2 9% 0% 90.3 6% 59%por 88.1 6.4k 5.0 88.2 18% 46% 89.0 -3% 0% 88.9 27% 46%heb 87.4 4.3k 3.1 87.4 -18% 31% 89.2 -16% 0% 89.1 -5% 31%eng 88.1 5.2k 3.1 88.0 5% 7% 90.6 3% 0% 90.6 -9% 7%cmn 82.4 7.5k 6.0 82.4 37% 0% 82.4 16% 0% 82.4 23% 0%Table 8: Unlabeled accuracy, run time in seconds, and number of features with PPL feature included.
Run time andnumber of features for orig, agr, and agr+orig are given as percent change relative to no-morph.and gen=F (rather than gen=X) would ensure thatagreement with a noun of either gender would becaptured by our features.
Furthermore, we may ex-periment with filtering morphological informationbased on part-of-speech, on attribute, or on whetherthe attribute participates in any agreement relation-ships.
We also intend to perform feature selection onthe original feature set, and investigate the impor-tance of labeled morphological features, which areincluded in agr but not in orig.
Finally, we plan todevelop metrics to measure the degree of word or-der flexibility in a treebank, in order to explore theextent to which it correlates with the degree of im-provement achieved by our system.6 ConclusionsWe developed a simple, language-independentmodel of agreement to better leverage morphologi-cal data in dependency parsing.
Testing on treebankscontaining varying amounts of morphological infor-mation resulted in substantial improvements in pars-ing accuracy while reducing feature counts and runtimes significantly.
Although originally intended tocompensate for lower accuracy on morphologicallyrich languages, the model improved performance onall treebanks with any morphological information.We acknowledge that because our model wastested on treebanks which differ widely in annota-tion guidelines, variables such as the amount of mor-phological information included and the treatmentof non-projective parses and coordination could af-fect parsing performance.
We did not delve intothese factors.
However, we believe this is part of thestrength of the approach: we were able to achieveperformance gains without any detailed knowledgeof the languages and treebanks used.We hope these results will encourage similarlylinguistically motivated design in future systems.This case study provides strong evidence that in-corporating linguistic knowledge into NLP systemsdoes not preclude language independence, and in-deed may enhance it, by leveling performance acrosstypologically differing languages.AcknowledgmentsWe would like to thank everyone who assisted us ingathering treebanks, particularly Maite Oronoz andher colleagues at the University of the Basque Coun-try and Yoav Goldberg, as well as three anonymousreviewers for their comments.323ReferencesI.
Aduriz, M.J. Aranzabe, J.M.
Arriola, A. Atutxa, A.D.de Ilarraza, A. Garmendia, and M. Oronoz.
2003.Construction of a Basque dependency treebank.
InProc.
of the Second Workshop on Treebanks and Lin-guistic Theories (TLT 2003), pages 201?204.S.
Afonso, E. Bick, R. Haber, and D. Santos.
2002.Floresta Sinta?
(c)tica: A treebank for Portuguese.
InProc.
of the Third International Conference on Lan-guage Resources and Evaluation (LREC 2002), page1698.G.
Attardi, F. DellOrletta, M. Simi, A. Chanev, andM.
Ciaramita.
2007.
Multilingual dependency pars-ing and domain adaptation using DeSR.
In Proc.
of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL 2007), pages1112?1118.D.
Bamman and G. Crane.
2006.
The design and useof a Latin dependency treebank.
In Proc.
of the FifthInternational Workshop on Treebanks and LinguisticTheories (TLT 2006), pages 67?78.D.
Bamman, F. Mambrini, and G. Crane.
2009.
An own-ership model of annotation: The Ancient Greek De-pendency Treebank.
In Proc.
of the Eighth Interna-tional Workshop on Treebanks and Linguistic Theories(TLT8), pages 5?15.E.M.
Bender.
2011.
On achieving and evaluatinglanguage-independence in NLP.
Linguistic Issues inLanguage Technology: Special Issue on Interaction ofLinguistics and Computational Linguistics, 6(3):1?26.K.
Bengoetxea and K. Gojenola.
2010.
Application ofdifferent techniques to dependency parsing of Basque.In Proc.
of the First Workshop on Statistical Parsingof Morphologically Rich Languages (SPMRL 2010),pages 31?39.
Association for Computational Linguis-tics.R.
Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D.M.Sharma, and F. Xia.
2009.
A multi-representationaland multi-layered treebank for Hindi/Urdu.
In Proc.
ofthe Third Linguistic Annotation Workshop (LAW III),pages 186?189.
Association for Computational Lin-guistics.E.
Bick.
2006.
LingPars, a linguistically inspired,language-independent machine learner for depen-dency treebanks.
In Proc.
of the Tenth Conference onComputational Natural Language Learning (CoNLL-X), pages 171?175.
Association for ComputationalLinguistics.S.
Bird, E. Klein, and E. Loper.
2009.
Natural LanguageProcessing with Python.
O?Reilly Media.C.
Bosco, V. Lombardo, D. Vassallo, and L. Lesmo.2000.
Building a treebank for Italian: a data-drivenannotation schema.
In Proc.
of the Second Interna-tional Conference on Language Resources and Evalu-ation (LREC 2000), pages 99?106.T.
Brants, W. Skut, and H. Uszkoreit.
1999.
Syntacticannotation of a German newspaper corpus.
Treebanks:Building and using parsed corpora, 20:73.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.
ofthe Tenth Conference on Computational Natural Lan-guage Learning (CoNLL-X), pages 149?164.
Associa-tion for Computational Linguistics.X.
Carreras, M. Surdeanu, and L. Marquez.
2006.
Pro-jective dependency parsing with perceptron.
In Proc.of the Tenth Conference on Computational NaturalLanguage Learning (CoNLL-X), pages 181?185.
As-sociation for Computational Linguistics.M.W.
Chang, Q.
Do, and D. Roth.
2006.
A pipelinemodel for bottom-up dependency parsing.
In Proc.
ofthe Tenth Conference on Computational Natural Lan-guage Learning (CoNLL-X), pages 186?190.
Associa-tion for Computational Linguistics.G.G.
Corbett.
2006.
Agreement.
Cambridge UniversityPress.S.
Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas,Z.
Z?abokrtsky, and A.
Z?ele.
2006.
Towards a Slovenedependency treebank.
In Proc.
of the Fifth Interna-tional Conference on Language Resources and Evalu-ation (LREC 2006).G.
Eryig?it, J. Nivre, and K. Oflazer.
2008.
Depen-dency parsing of Turkish.
Computational Linguistics,34(3):357?389.Y.
Goldberg and M. Elhadad.
2009.
Hebrew de-pendency parsing: Initial results.
In Proc.
of the11th International Conference on Parsing Technolo-gies (IWPT?09), pages 129?133.
Association for Com-putational Linguistics.Y.
Goldberg and M. Elhadad.
2010.
Easy-first depen-dency parsing of Modern Hebrew.
In Proc.
of the FirstWorkshop on Statistical Parsing of MorphologicallyRich Languages (SPMRL 2010), pages 103?107.
As-sociation for Computational Linguistics.Yoav Goldberg.
2011.
Automatic Syntactic Processing ofModern Hebrew.
Ph.D. thesis, Ben Gurion University.J.
Hajic, O. Smrz, P. Zema?nek, J.
S?naidauf, and E. Bes?ka.2004.
Prague Arabic dependency treebank: Develop-ment in data and tools.
In Proc.
of the NEMLAR Inter-national Conference on Arabic Language Resourcesand Tools, pages 110?117.Jan Hajic?.
1998.
Building a syntactically annotatedcorpus: The Prague Dependency Treebank.
In EvaHajic?ova?, editor, Issues of Valency and Meaning:Studies in Honor of Jarmila Panevova?, pages 12?19.Prague Karolinum, Charles University Press.324Katri Haverinen, Timo Viljanen, Veronika Laippala,Samuel Kohonen, Filip Ginter, and Tapio Salakoski.2010.
Treebanking Finnish.
In Proc.
of the NinthInternational Workshop on Treebanks and LinguisticTheories (TLT9, volume 9, pages 79?90.M.
Hohensee.
2012.
It?s only morpho-logical: Model-ing agreement in cross-linguistic dependency parsing.Master?s thesis, University of Washington.M.T.
Kromann.
2003.
The Danish Dependency Tree-bank and the DTAG treebank tool.
In Proc.
of the Sec-ond Workshop on Treebanks and Linguistic Theories(TLT 2003), pages 217?220.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.M.A.
Mart?, M.
Taule?, L. Ma?rquez, and M. Bertran.2007.
CESS-ECE: A multilingual and multilevel an-notated corpus.Y.
Marton, N. Habash, and O. Rambow.
2010.
Improv-ing Arabic dependency parsing with lexical and inflec-tional morphological features.
In Proc.
of the FirstWorkshop on Statistical Parsing of MorphologicallyRich Languages (SPMRL 2010), pages 13?21.
Asso-ciation for Computational Linguistics.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Multi-lingual dependency analysis with a two-stage discrim-inative parser.
In Proc.
of the Tenth Conference onComputational Natural Language Learning (CoNLL-X), pages 216?220.
Association for ComputationalLinguistics.T.
Nakagawa.
2007.
Multilingual dependency pars-ing using global features.
In Proc.
of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL 2007), pages 952?956.J.
Nivre, J.
Hall, and J. Nilsson.
2006a.
Maltparser: Adata-driven parser-generator for dependency parsing.In Proc.
of the Fifth International Conference on Lan-guage Resources and Evaluation (LREC 2006), vol-ume 6, pages 2216?2219.J.
Nivre, J.
Hall, J. Nilsson, G. Eryig?it, and S. Mari-nov. 2006b.
Labeled pseudo-projective dependencyparsing with support vector machines.
In Proc.
ofthe Tenth Conference on Computational Natural Lan-guage Learning (CoNLL-X), pages 221?225.
Associa-tion for Computational Linguistics.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
CoNLL 2007 sharedtask on dependency parsing.
In Proc.
of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL 2007).
Associationfor Computational Linguistics.J.
Nivre, I.M.
Boguslavsky, and L.L.
Iomdin.
2008.
Pars-ing the SynTagRus treebank of Russian.
In Proc.
of the22nd International Conference on Computational Lin-guistics (COLING 2008), volume 1, pages 641?648.Association for Computational Linguistics.J.
Nivre.
2009.
Parsing Indian languages with Malt-Parser.
In Proc.
of the Seventh International Confer-ence on Natural Language Processing (ICON 2009)NLP Tools Contest, pages 12?18.K.
Oflazer, B.
Say, D.Z.
Hakkani-Tu?r, and G. Tu?r.
2003.Building a Turkish treebank.
Text, Speech, and Lan-guage Technology, pages 261?277.L.
?vrelid and J. Nivre.
2007.
When word order andpart-of-speech tags are not enough?Swedish depen-dency parsing with rich linguistic features.
In Proc.
ofthe International Conference on Recent Advances inNatural Language Processing (RANLP), pages 447?451.S.
Petrov, D. Das, and R. McDonald.
2011.
Auniversal part-of-speech tagset.
Arxiv preprintArXiv:1104.2086.Loganathan Ramasamy and Zdene?k Z?abokrtsky?.
2011.Tamil dependency parsing: Results using rule basedand corpus based approaches.
In Proc.
of the 12thInternational Conference on Intelligent Text Process-ing and Computational Linguistics (CICLING 2011),volume 1, pages 82?95, Berlin, Heidelberg.
Springer-Verlag.M.
Schiehlen and K. Spranger.
2007.
Global learn-ing of labelled dependency trees.
In Proc.
of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL 2007), pages1156?1160.Anna Siewierska.
1998.
Variation in major constituentorder: A global and a European perspective.
InAnna Siewierska, editor, Constituent Order in theLanguages of Europe, pages 475?551.
Mouton DeGruyter.K.
Simov, P. Osenova, A. Simov, and M. Kouylekov.2004.
Design and implementation of the Bulgar-ian HPSG-based treebank.
Research on Language &Computation, 2(4):495?522.I.
Titov and J. Henderson.
2007.
Fast and robust mul-tilingual dependency parsing with a generative latentvariable model.
In Proc.
of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL 2007), pages 947?951.L.
Van der Beek, G. Bouma, R. Malouf, and G. Van No-ord.
2002.
The Alpino dependency treebank.
Lan-guage and Computers, 45(1):8?22.325V.
Vincze, D. Szauter, A.
Alma?si, G. Mo?ra, Z. Alexin,and J. Csirik.
2010.
Hungarian dependency treebank.In Proc.
of the Seventh Conference on Language Re-sources and Evaluation (LREC 2010).N.
Xue, F. Xia, F.D.
Chiou, and M. Palmer.
2005.
ThePenn Chinese Treebank: Phrase structure annotationof a large corpus.
Natural Language Engineering,11(2):207?238.326
