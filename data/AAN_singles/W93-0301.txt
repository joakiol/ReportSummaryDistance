Robust Bilingual Word Alignmentfor Machine Aided TranslationIdo Dagan Kenneth W. ChurchAT&T Bell Laboratories600 Mountain AvenueMurray Hill, NJ 07974Wil l iam A. GaleAbstractWe have developed a new program calledword_align for aligning parallel text, text suchas the Canadian Hansards that are available intwo or more languages.
The program takes theoutput of char_align (Church, 1993), a robustalternative to sentence-based alignment pro-grams, and applies word-level constraints us-ing a version of Brown el al.
's Model 2 (Brownet al, 1993), modified and extended to dealwith robustness i sues.
Word_align was testedon a subset of Canadian Hansards upplied bySimard (Simard et al, 1992).
The combina-tion of word_align plus char_align reduces thevariance (average square error) by a factor of5 over char_align alone.
More importantly, be-cause word_align and char_align were designedto work robustly on texts that are smaller andmore noisy than the Hansards, it has been pos-sible to successfully deploy the programs atAT&T Language Line Services, a commercialtranslation service, to help them with difficultterminology.1 IntroductionAligning parallel texts has recently received consid-erable attention (Warwick et al, 1990; Brown et al,1991a; Gale and Church, 1991b; Gale and Church,1991a; Kay and Rosenschein, 1993; Simard et al,1992; Church, 1993; Kupiec, 1993; Matsumoto etal., 1993).
These methods have been used in ma-chine translation (Brown et al, 1990; Sadler, 1989),terminology research and translation aids (Isabelle,1992; Ogden and Gonzales, 1993), bilingual lexi-cography (Klavans and Tzoukermann, 1990), col-location studies (Smadja, 1992), word-sense disam-biguation (Brown et al, 1991b; Gale et al, 1992)and information retrieval in a multilingual environ-ment (Landauer and Littman, 1990).The information retrieval application may beof particular elevance to this audience.
It wouldbe highly desirable for users to be able to expressqueries in whatever language they chose and re-trieve documents that may or may not have beenwritten in the same language as the query.
Lan-dauer and Littman used SVD analysis (or LatentSemantic Indexing) on the Canadian Hansards,parliamentary debates that are published in bothEnglish and French, in order to estimate a kind ofsoft thesaurus.
They then showed that these esti-mates could be used to retrieve documents appro-priately in the bilingual condition where the queryand the document were written in different lan-guages.We have been most interested in the terminol-ogy application.
How does Microsoft, or some othersoftware vendor, want "dialog box," "text box,"and "menu box" to be translated in their man-uals?
Considerable time is spent on terminologyquestions, many of which have already been solvedby other translators working on similar texts.
Itought to be possible for a translator to point atan instance of "dialog box" in the English versionof the Microsoft Windows manual and see how itwas translated in the French version of the samemanual.
Alternatively, the translator can ask for abilingual concordance as shown in Figure 1.
A PC-based terminology reuse tool is being developed todo just exactly this.
The tool depends cruciallyon the results of an alignment program to deter-mine which parts of the source text correspond withwhich parts of the target text.In working with the translators at AT&T Lan-guage Line Services, a commercial translation ser-vice, we discovered that we needed to completelyredesign our alignment programs in order to dealmore effectively with texts supplied by LanguageLine's customers.
All too often the texts are notavailable in electronic form, and may need to bescanned in and processed by an OCR (optical char-acter recognition) device.
Even if the texts areavailable in electronic form, it may not be worththe effort to clean them up by hand.
Real texts arenot like the bIansards; real texts are much smallerand not nearly as clean as the ideal texts that havedisplayed .
In the Save Asaff icha Dana Enregistrer  Enregistrerainsi que son extension .
Dana la boite dex When you choose a command button , theLorsque commande boutonsissez un bouton de commande , la boite de, o .button .
Dr doubl - l ick the Control -r bouton cl iquer lois Systemeouvez  auss i  cl iquer deux  lo i s  sur  la case  duo , .o o .ee ' aa , ' When you move to an emptyLorsque p lacezde Lorsque vous vous placez darts une zone dedialog box , this area is cal led Savedialogue boite cette zone est Enregistrdialogue Enregistrer sous ,  cette zone eat appeledialog box closes and the command isdialogue boite ferme commande executedialogue se ferme et le programme execute la tommenu box .
Or press ESC .
If a dialog box dmenu case Si dialogue boite pmenu Systeme .
II eat egalement poss ib le d ' atext box , an  i i isert ion point ( f lastung vetexte zone insert ion (texte vide , un  point d ' insert ion ( barre verticFigure 1: A small sample of a bilingual concordance, based on the output of word_align.
Four concordancesfor the word "box" are shown, identifying three different ranslations for the word: boite, case, zone.
Theconcordances are selected from English and French versions of the Microsoft Windows manual (with someerrors introduced by OCR).
There are three lines of text for each instance of "box": (1) English, (2) glosses,and (3) French.
The glosses are selected from the French text (the third line), and are written underneaththe corresponding English words, as identified by word_align.been used in previous studies.To deal with these robustness issues, Church(1993) developed a character-based alignmentmethod called char_align.
The method was in-tended as a replacement for sentence-based meth-ods (e.g., (Brown et al, 1991a; Gale and Church,1991b; Kay and Rosenschein, 1993)), which arevery sensitive to noise.
This paper describes anew program, called word_align, that starts withan initial "rough" alignment (e.g., the output ofchar_align or a sentence-based alignment method),and produces improved alignments by exploitingconstraints at the word-level.
The alignment algo-rithm consists of two steps: (1) estimate transla-tion probabilities, and (2) use these probabilitiesto search for most probable alignment path.
Thetwo steps are described in the following section.2 The  a l ignment  A lgor i thm2.1 Es t imat ion  o f  t rans la t ionprobab i l i t i esThe translation probabilities are estimated using amethod based on Brown et al's Model 2 (1993),which is summarized in the following subsection,2.1.1.
Then, in subsection 2.1.2, we describemodifications that achieve three goals: (1) en-able word_align to accept input which may not bealigned by sentence (e.g.
char_align's output), (2)reduce the number of parameters that need to beestimated, and (3) prepare the ground for the sec-ond step, the search for the best alignment (de-scribed in section 2.2).2.1.1 Brown et al's ModelIn the context of their statistical machine trans-lation project (Brown et al, 1990), Brown et alestimate Pr(f\[e), the probability that f, a sentencein one language (say French), is the translation ofe, a sentence in the other language (say English).Pr(fle ) is computed using the concept of alignment,denoted by a, which is a set of connections betweeneach French word in f and the corresponding En-glish word in e. A connection, which we will writef,e specifies that position j in f is connected as  coB j ,  i ,to position i in e. If a French word in f does notcorrespond to any English word in e, then it isconnected to the special word n~ll (position 0 ine).
Notice that this model is directional, as eachFrench position is connected to exactly one posi-tion in the English sentence (which might be thenull word), and accordingly the number of connec-tions in an alignment is equal to the length of theFrench sentence.
However, an English word may beconnected to several words in the French sentence,or not connected at all.Using alignments, the translation probabilityfor a pair of sentences is expressed asPr(fJe)-- Z Pr(f, ale) (1)aE.Awhere .A is the set of all combinatorially possiblealignments for the sentences f and e (calligraphicfont will be used to denote sets).In their paper, Brown et al present a series of5 models of Pr(f\[e).
The first two of these 5 modelsare summarized here.2Mode l  1Model 1 assumes that Pr(f, ale) depends pri-marily on t(f\[e), the probability that an occurrenceof the English word e is translated as the Frenchword f.  That is,mPr(fle) = E Pr(f'ale) = E Cf.e I'I t(fjie*,)ae.4 ae.4 j=l(2)where Cf,e, an irrelevant constant, accounts forcertain dependencies on sentence lengths, whichare not important for our purposes here.
Exceptfor Cf.e, most of the notation is borrowed fromBrown ctal..
The variable, j, is used to refer to aposition in a French sentence, and the variable, i,is used to refer to a position in an English sentence.The expression, f j ,  is used to refer to the Frenchword in position j of a French sentence, and ei isused to refer to the English word in position i ofan English sentence.
An alignment, a, is a set ofpairs (j, i), each of which connects a position in aFrench sentence with a corresponding position inan English sentence.
The expression, aj, is usedto refer to the English position that is connectedto the French position j, and the expression, eoj,is used to refer to the English word in position aj.The variable, m, is used to denote the length ofthe French sentence and the variable, 1, is used todenote the length of the English sentence.There are quite a number of constraints thatcould be used to estimate Pr(f, ale ).
Model 1 de-pends primarily on the translation probabilities,t(f\[e), and does not make use of constraints in-volving the positions within an alignment.
Theseconstraints will be exploited in Model 2.Brown e~ al.
estimate t(f\[e) on the basis of atraining set, a set of English and French sentencesthat have been aligned at the sentence l vel.
Thosevalues of t(f\[e) that maximize the probability ofthe training set are called the maximum likelihoodestimates.
Brown et al show that the max imumlikelihood estimates satisfyPr(con1,~ e)(.fle) =)-~',o"f'eecoW'.., Pr(conf:e) (3)where CO.A/'t,e and CO./V'.e denote sets of con-nections: the set CO.A/'l,e contains all connectionsin the training data between f and e, and theset CO.N'.
e contains all connections between someFrench word and e. The probability of a connec-tion, con~,~ e, is the sum of the probabilities of allalignments that contain it.
Notice that equation3 satisfies the constraint: ~'~.!
t(fle ) = 1, for eachEnglish word e.It follows from the definition of Model 1 thatthe probability of a connection satisfies:Pr(conf~e) = t (4)?
Ck=o t ( f i l ek )Recall that fj refers to the French word in positionj of the French sentence f of length rn, and thatei refers to the English word in position i of theEnglish sentence e of length I.
Also, rememberthat position 0 is reserved for the null word.Equations 3 and 4.are used iteratively to esti-mate t(f\[e).
That is, we start with an initial guessfor t(fle).
We then evaluation the right hand sideof equation 4, and compute the probability of theconnections in the training set.
Then we evaluateequation 3, obtain new estimates for the transla-tion probabilities, and repeat the process, until itconverges.
This iterative process is known as theEM algorithm and has been shown to converge toa stationary point (Baum, 1972; Dempster et al,1977).
Moreover, Brown et aL show that ModelI has a unique maximum, and therefore, in thisspecial case, the EM algorithm is guaranteed toconverge to the max imum likelihood solution, anddoes not depend on the initial guess.Model  2Model 2 improves upon model 1 by making useof the positions within an alignment.
For instance,it is much more likely that the first word of an En-glish sentence will be connected to a word near thebeginning of the corresponding French sentence,than to some word near the end of the French sen-tence.
Model 2 enhances Model 1 with the assuml>-fe  tion that the probability of a connection, conj,'~ ,depends also on j and i (the positions in f ande), as well as on m and I (the lengths of the twosentences).
This dependence is expressed throughthe term a(ilj, m,l), which denotes the probabil-ity of connecting position j in a French sentence oflength m with position i in an English sentenceof length I.
Since each French position is con-nected to exactly one English position, the con-straint ~"~ti= 0 a(i\[j, m, I) = 1 should hold for all j,m and I.
In place of equation 2, we now have:Pr(f\[e) = EPr ( f ,  ale) (5)aEA: EaE.4 i=lwhere Of.
e is an irrelevant constant.As in Model 1, equation 3 holds for the max-imum likelihood estimates of the translation prob-abilities.
The corresponding equation for the max-3imum likelihood estimates of a(iIj, m, l) is:Eco,  f,eecoA,,-,, Pr(con f,'i e)a(ilj, m, l)  = '" '"2??"
f ;eec?Xr  :' Pr(conf,~) (6)where CO.N'~S denotes the set of connections in thetraining data between positions j and i in Frenchand English sentences of lengths m and 1, respec-tively.
Similarly, CO.N'~.
'l denotes the set of con-nections between position j and some English po-sition, in sentences of these lengths.Instead of equation 4, we obtain the followingequation for the probability of a connection:f .e ,  t( fj \[el)" a( ilj, rn, l)~"~k=0 t(fj \[ek)-a(klj, rn, l)Notice that Model 1 is a special case of Model 2,where a(ilj , m, l) is held fixed at1+1 "As before, the EM algorithm is used to com-pute maximum likelihood estimates for t(f le) anda(ilj, m, i) (using first equation 7, and then equa-tions 3 and 6).
However, in this case, Model 2does not have a unique maximum, and thereforethe results depend on the initial guesses.
Brownet al therefore use Model 1 to obtain estimates fort(f le ) which do not depend on the initial guesses.These values are then used as the initial guesses oft( f le ) in Model 2.2.1.2 Our  mode lAs mentioned in the introduction, we are interestedin aligning corpora that are smaller and noisierthan the Hansards.
This implies severe practicalconstraints on the word alignment algorithm.
Asmentioned earlier, we chose to start with the out-put of char_align because it is more robust than al-ternative sentence-based methods.
This choice, ofcourse, requires certain modifications to the modelof Brown et al to accommodate asinput an initialrough alignment (such as produced by char_align)instead of pairs of aligned sentences.
It is alsouseful to reduce the number of parameters that weare trying to estimate, because we have much lessdata and much more noise.
The paragraphs belowdescribe our modifications which are intended tomeet these somewhat different requirements.
Thetwo major modifications are: (a) replacing thesentence-by-sentence alignment with a single globalalignment for the entire corpus, and (b) replacingthe set of probabilities a(ilj, m, l) with a small setof offset probabilities.Word_align starts with an initial rough align-ment, I, which maps French positions to Englishpositions (if the mapping is partial, we use linearextrapolation to make it complete).
Our goal is tofind a global alignment, A, which is more accuratethan I.
To achieve this goal, we first use I to deter-mine which connections will be considered for A.Let conj,i denote a connection between position jin the French corpus and position i in the Englishcorpus (the super-scripts in eon~,~ are omitted, asthere is no notion of sentences).
We assume thateonj,i is a possible connection only if i falls within alimited window which is centered around I(j), suchthat:I ( j ) -  w < i < I( j)  + w (8)where w is a predetermined parameter specifyingthe size of the window (we typically set w to 20words).
Connections that fall outside this windoware assumed to have a zero probability.
This as-sumption replaces the assumption of Brown et althat connections which cross boundaries of alignedsentences have a zero probability.
In this newframework, equation 3 becomes:~-:~con,.,~co az ,., Pr( conj.i )t(f le) = ~o, , .
,eco J?
.
,  Pr(conj,i) (9)where CO.h/'j,e and COA/'.,e are taken from the setof possible connections, as defined by (8).Turning to Model 2, the parameters ofthe forma(ilj , rn, l) are somewhat more problematic.
First,since there are no sentence boundaries, there are nodirect equivalents for i, j, m and 1.
Secondly, thereare too many parameters to be estimated, given thelimited size of our corpora Cone parameter for eachcombination of i , j ,m and l).
Fortunately, theseparameters are highly redundant.
For example, itis likely that a(i\[j, m, l) will be very close to a(i +l l j+  1,re, l) and a(itj, rn+ 1,1+ 1).In order to deal with these concerns, we re-place probabilities of the form a(ilj, m, 1) with asmall set of offset probabilities.
We use k to denotethe offset between i, an English position which cor-responds to the French position j,  and the Englishposition which the input alignment I connects toj:  k = i -  I( j).
An offset probability, o(k), is theprobability of having an offset k for some arbitraryconnection.
According to (8), k ranges between-w and w. Thus, instead of equation 6, we haveo(k) = Y:~,,.,~coJ?~ Pr(conj,i) (10)~---,?~,.,~.CO.W Pr( e?ni,i )where COAl is the set of all connections and CO.hfkis the set of all connections with offset k. Insteadof equation 7, we havePr(conj.i) = t(f l  \[el)" o(i - I ( j ))X"I(#) +~ ~rr.\[en).
o(h I ( j ))  z..,h=i(j)_w -~a~(11)The last three equations are used in the EMalgorithm in an iterative fashion as before to es-timate the translation probabilities and the offsetprobabilities.
Table 1 and Figure 2 show some val-ues that were estimated in this way.
The inputconsisted of a pair of Microsoft Windows manu-als in English (125,000 words) and its equivalent inFrench (143,000 words).
Table 1 shows four Frenchwords and the four most likely translations, ortedby t(e\]f) 1.
Note that the correct ranslation(s) areusually near the front of the list, though there is atendency for the program to be confused by collo-cates such as "information about".
Figure 2 showsthe probability estimates for offsets from the ini-tial alignment I.
Note that smaller offsets are morelikely than larger ones, as we would expect.
More-over, the distribution is reasonably close to normal,as indicated by the dotted line, which was gener-ated by a Gaussian with a mean of 0 and standarddeviation of 10 2 .We have found it useful to make use of three fil-ters to deal with robustness i sues.
Empirically, wefound that both high frequency and low frequencywords caused difficulties and therefore connectionsinvolving these words are filtered out.
The thresh-olds are set to exclude the most frequent functionwords and punctuations, as well as words with lessthan 3 occurrences.
In addition, following a similarfilter by Brown et al, small values of t(f\[e) are setto 0 after each iteration of the EM algorithm be-cause these small values often correspond to inap-propriate translations.
Finally, connections to nullare ignored.
Such connections model French wordsthat are often omitted in the English translation.However, because of OCR errors and other sourcesof noise, it was decided that this phenomenon wastoo difficult to model.Some words will not be aligned because of theseheuristics.
It may not be necessary, however, toalign all words in order to meet the goal of help-ing translators (and lexicographers) with difficultterminology.2.2 F ind ing  the  most  probab lea l ignmentThe EM algorithm produces two sets of maxi-mum likelihood probability estimates: translationprobabilities, t(fle), and offset probabilities, o(k).Brown et al select heir preferred alignment simplyby choosing the most probable alignment accordingto the maximum likelihood probabilities, relative tothe given sentence alignment.
In the terms of ourl ln this example, French is used as the source lan-guage a~ad English as the taxget.2The center of the estimated distribution seemsmore fiat than in a normal distribution.
This mightbe explained by a higher tendency for local changesof word order within phrases than for order changesamong phrases.
This is merely a hypothesis, though,which requires further testing.model, it is necessary to select the alignment Athat maximizes:I\] t(file')'?
(i-X(J)) (12)con:.,eAUnfortunately, this method does not model the de-pendence between connections for French wordsthat are near one another.
For example, the factthat the French position j was connected to theEnglish position i will not increase the probabilitythat j + 1 will be connected to an English positionnear i.
The absence of such dependence can easilyconfuse the program, mainly in aligning adjacentoccurrences of the same word, which are commonin technical texts.
Brown et al introduce such de-pendence in their Model 4.
We have selected asimpler alternative defined in terms of offset prob-abilities.2.2.1 Determin ing  the set of  re levantconnect ionsThe first step in finding the most probable align-ment is to determine the relevant connections foreach French position.
Relevant connections are re-quired to be reasonably likely, that is, their trans-lation probability (t(f\[e)) should exceed some min-imal threshold.
Moreover, they are required to fallwithin a window between I(j) - w and I( j) + w inthe English corpus, as in the previous tep (param-eter estimation).
We call a French position relevantif it has at least one relevant connection.
Eachalignment A then consists of exactly one connec-tion for each relevant French position (the irrele-vant positions are ignored).2.2.2 Determining the most probableal ignmentTo model the dependency between connections inan alignment, we assume that the offset of a con-nection is determined relative to the preceding con-nection in A, instead of relative to the initial align-ment, I.
For this purpose, we define A' (j) as a lin-ear extrapolation from the preceding connection inA:NE (13) A'( j )  = A(jpre~) + (j - jp,e~) IV Fwhere Jv,?~ is the last French position before jwhich is aligned by A and NE and NF are thelengths of the English and French corpora.
A ' ( j )thus predicts the connection of j ,  knowing the con-nection of jp,?~ and assuming that the two lan-guages have the same word order, instead of (12),the most probable alignment maximizesH t(f j lei),  o ( i -  A'( j )) .
(14)eona,.~A5ozonefermerin formationsinsertionEnglish translations (with probabilities)box (0.58) area (0.28) want (0.04) In (0.02)close (0.44) when (0.08) Close (0.07) selected (0.06)information (0.66) about (0.15) For (0.12) see (0.04)insertion (0.61) point (0.23) Edit (0.06) To (0.05)Table 1: Estimated translation probabilitieso0oa.00 c~000-20 -10 0 10 20French wordOffsetFigure 2: Estimated offset probabilities (solid line) along with a Gaussian (dashed line) for comparison.We approximate the offset probabilities, 0(k), rela-tive to A', using the max imum likelihood estimateswhich were computed relative to I (as described inSection 2.1.2).We use a dynamic programming algorithm tofind the most probable alignment.
This enablesus to know the value A(jp,e~) when dealing withposition j.
To avoid connections with very lowprobability (due to a large offset) we require thatt(f j  \[el).
o(i-- A'(j)) exceeds a pre-specified thresh-old T s. If the threshold is not exceeded, theconnection is dropped from the alignment, andt(f j Jei) ,  o(i - A'(j)) for that connection is set toT when computing (14).
T can therefore be inter-preted as a global setting of the probability thata random position will be connected to the null3In fact, the threshold on t(f, le,), which is used todetermine the relevant connections (described in theprevious ubsection), is used just as an efficient earlyapplication of the threshold T. This early applicationis possible when t(f~le~)" o(k,,~==) < T, where k,~== isthe value of k with maximal o(k).English word 4.
A similar dynamic programmingapproach was used by Gale and Church for wordalignment (Gale and Church, 1991a), to handle de-pendency between connections.3 EvaluationWord_align was first evaluated on a representativesample of Canadian Hansards (160,000 words inEnglish and French).
The sample was kindly pro-vided by Simard et al, along with alignments ofsentence boundaries as determined by their panelof 8 judges (Simard et al, 1992).Ten iterations of the EM algorithm were com-puted to estimate the parameters of the model.The window size was set to 20 words in each di-rection, and the minimal threshold for t(fJe) wasset to 0.005.
We considered connections whosesource and target words had frequencies between 3and 1700 (1700 is the highest frequency of a con-tent word in the corpus.
We thus excluded as many4As mentioned earlier, we do not estimate directlytranslation probabilities for the null English word.function words as possible, but no content words).In this experiment, we used French as the sourcelanguage and English as the target language.Figure 3 presents the alignment error rate ofword_align.
It is compared with the error rate ofword_align's input, i.e.
the initial rough alignmentwhich is produced by char_align.
The errors aresampled at sentence boundaries, and are measuredas the relative distance between the output of thealignment program and the "true" alignment, asdefined by the human judges 5.
The histogramspresent errors in the range of-20-20, which cov-ers about 95% of the data s. It can be seen thatword_align decreases the error rate significantly(notice the different scales of the vertical axes).
In55% of the cases, there is no error in word_align'soutput (distance of 0), in 73% the distance fromthe correct alignment is at most i, and in 84% thedistance is at most 3.A second evaluation of word_align was per-formed on noisy technical documents, of the typetypically available for AT&T Language Line Ser-vices.
We used the English and French versions ofa manual of monitoring equipment (about 65,000words), both scanned by an OCR device.
We sam-pled the English vocabulary with frequency be-tween three and 450 occurrences, the same vocabu-lary that was used for alignment.
We sampled 100types from the top fifth by frequency of the vocabu-lary (quintile), 80 types from the second quintile, 60from the third, 40 from the fourth, and 20 from thebottom quintile.
We used this stratified samplingbecause we wanted to make more accurate state-ments about our error rate by tokens than we wouldhave obtained from random sampling, or even fromequal weighting of the quintiles.
After choosing the300 types from the vocabulary list, one token foreach type was chosen at random from the corpus.By hand, the best corresponding position in theFrench version was chosen, to be compared withword_align ' s output.Table 2 summarizes the results of the secondexperiment.
The figures indicate the expected rela-tive frequency of each offset from the correct align-ment.
This relative frequency was computed ac-cording to the word frequencies in the stratifiedsample.
As shown in the table, for 60.5% of the to-kens the alignment is accurate, and in 84% the off-set from the correct alingment is at most 3.
Thesefigures demonstrate he usefulness of word_align forconstructing bilingual exicons, and its impact on5As explained eaxlier, word_align produces a partialMignment.
For the purpose of the evaluation, we usedlinear interpolation to get Mignments for all the posi-tions in the sample.6Recall that the window size we used is 20 wordsin each direction, which means that word_align cannotrecover from larger errors in char_align.-20 -10 0 10char_align errors (in wor(~s)20o-20.
.
.
.
.
nR Hno .
.
.
.- I0 0 10 20~t~cl_align errors (in wor~s)Figure 3: Word_align reduces the variance (averagesquare error) by a factor of 5 over char_align alone(notice the vertical scales).the quality of bilingual concordances (as in Fig-ure 1).
Indeed, using bilingual concordances whichare based on word_align's output, the translators atAT&T Language Line Services are now producingbilingual terminology lexicons at a rate of 60-100terms per hour!
This is compared with the previousrate of about 30 terms per hour using char_align'soutput, and an extremely lower rate before align-ment tools were available.4 ConclusionsCompared with other word alignment algorithms(Brown et al, 1993; Gale and Church, 1991a),word_align does not require sentence alignment asinput, and was shown to produce useful align-ments for small and noisy corpora.
Its robust-ness was achieved by modifying Brown et al'sModel 2 to handle an initial "rough" alignment,reducing the number of parameters and introduc-ing a dependency between alignments of adjacentwords.
Taking the output of char_align as in-put, word_align produces ignificantly better, word-7Offset fromcorrect alignment01234Percentage60.5%10.8%7.5%5.2%1.6%Accumulativepercentage60.5%71.3%78.8%84%85.6%Table 2: Word_align's precision on noisy input,scanned by an OCR device.level, alignments on the kind of corpora that aretypically available to translators.
This improve-ment increased the rate of constructing bilingualterminology lexicons at AT&T Language Line Ser-vices by a factor of 2-3.
In addition, the align-ments may also be helpful to developers of lexiconsfor machine translation systems.
Word_align thusprovides an example how a model such as Brownet al's Model 2, that was originally designed forresearch in statistical machine translation, can bemodified to achieve practical, though less ambi-tious, goals in the near term.REFERENCESL.
E. Bantu.
1972.
An inequality and an associ-ated maximization technique in statistical es-timation of probabilistic functions of a markovprocess.
Inequalities, 3:1-8.P.
Brown, J. Cooke, S. Della Pietra,V.
Della Pietra, F. Jelinek, R.L.
Mercer, andRoossin P.S.
1990.
A statistical approach tolanguage translation.
Computational Linguis-tics, 16(2):79-85.P.
Brown, J. Lai, and R. Mercer.
1991a.
Aligningsentences in parallel corpora.
In Proc.
of theAnnual Meeting of the ACL.P.
Brown, S. Della Pietra, V. Della Pietra, andR.
Mercer.
1991b.
Word sense disambiguationusing statistical methods.
In Proc.
of the An-nual Meeting of the A CL.Peter Brown, Stephen Della Pietra, Vincent DellaPietra, and Robert Mercer.
1993.
The mathe-matics of machine translation: parameter sti-mation.
Computational Linguistics.
to appear.Kenneth W. Church.
1993.
Char_align: A programfor aligning parallel texts at character level.
InProc.
of the Annual Meeting of the ACL.A.
P. Dempster, N. M. Laird, and D. B. Rubin.1977.
Maximum liklihood from incompletedata via the EM algorithm.
Journal of theRoyal Statistical Society, 39(B):1-38.William Gale and Kenneth Church.
1991a.
Identi-fying word correspondence in parallel text.
InProc.
of the DARPA Workshop on Speech andNatural Language.William Gale and Kenneth Church.
1991b.
A pro-gram for aligning sentences in bilingual cor-pora.
In Proc.
of the Annual Meeting of theACL.William Gale, Kenneth Church, and DavidYarowsky.
1992.
Using bilingual materialsto develop word sense disambiguation meth-ods.
In Proc.
of the International Conferenceon Theoretical nd Methodolgical Issues in Ma-chine Translation.P.
Isabelle.
1992.
Bi-textual aids for translators.In Proc.
of the Annual Conference of the UWCenter for the New OED and Text Research.M.
Kay and M. Rosenschein.
1993.
Text-translation alignment.
Computational Linguis-tics.
to appear.J.
Klavans and E. Tzoukermann.
1990.
The bicordsystem.
In Proc.
of COLING.Julian Kupiec.
1993.
An algorithm for findingnoun phrase correspondences in bilingual cor-pora.
In Proc.
of the Annual Meeting of theACL.Thomas K. Landauer and Michael L. Littman.1990.
Fully automatic ross-language docu-ment retrieval using latent semantic indexing.In Proc.
of the Annual Conference of the UWCenter for the New OED and Text Research.Yuji Matsumoto, Hiroyuki Ishimoto, Takehito Ut-suro, and Makoto Nagao.
1993.
Structuralmatching of parallel texts.
In Prac.
of the An-nual Meeting of the ACL.William Ogden and Margarita Gonzaies.
1993.Norm - a system for translators.
Demonstra-tion at ARPA Workshop on Human LanguageTechnology.V.
Sadler.
1989.
Working with analogical seman-tics: Disambiguation techniques in DLT.
ForisPublications.M.
Simard, G. Foster, and P. Isabelle.
1992.
Us-ing cognates to align sentences in bilingual cor-pora.
In Proc.
of the International Conferenceon Theoretical nd Methodolgical lssues in Ma-chine Translation.Frank Smadja.
1992.
How to compile a bilingualcollocational lexicon automatically.
In AAAIWorkshop on Statistically-based Natural Lan-guage Processing Techniques, July.S.
Warwick, J. Hajic, and G. Russell.
1990.
Search-ing on tagged corpora: linguistically motivatedconcordance analysis.
In Proc.
of the AnnualConference of the UW Center for the NewOED and Text Research.8
