WSD and Closed Semantic ConstraintJiangsheng Yu?Institute of Computational LinguisticsPeking University, Beijing, China, 100871Abstract The application-driven constructionof lexicon has been emphasized as a methodologyof Computational Lexicology recently.
We focus onthe closed semantic constraint of the argument(s)of any verb concept by the noun concepts in aWordNet-like lexicon, which theoretically is relatedto Word Sense Disambiguation (WSD) at differ-ent levels.
From the viewpoint of Dynamic Lexi-con, WSD provides a way of automatic construc-tion for the closed semantic constraints and alsobenefits from the semantic descriptions.Keywords dynamic lexicon, evolution, WSD,WordNet-like lexicon, closed semantic constraint1 IntroductionAs the underlying resource of semantic analysis,the most important descriptions in a semantic lexi-con are the relationships between verbs and nouns,which usually comes down to the closed semanticconstraint of each argument of any verb.
Once thestructure of the semantic lexicon is determined,the closed semantic constraint becomes a well-defined problem.Example 1.1 The verb da?
has many mean-ings in Chinese, which differ in da?
ha?izi (pun-ish the child), da?
ma?oyi?
(weaver the sweater),da?
jia`ngyo?u (buy the soy), etc.
Actually, the se-mantics of da?
is distinguished by the semantics ofits arguments.Different from the traditional lexicon, we advo-cate the conception of dynamic lexicon ([15]) andits evolution oriented to some particular applica-tion, which will be mentioned in Section 2.
TheWordNet-like lexicon is treated as a dynamic one,which means that the structures representing se-mantic knowledge could be changed according tosome empirical standards.
In the next section,we?ll define the WSD based on the WordNet-likelexicon, and then discuss the training of conceptTagSet and the statistical model of WSD.
By thestatistical WSD, in Section 4, we introduce an ap-proach to the automatic construction of the closed?This paper is supported by National Foundation of Nat-ural Science (Research on Chinese Information Extraction),No.
69483003 and Project 985 in Peking University.semantic constraints in a WordNet-like lexicon.The last section is the conclusion.2 Dynamic Lexicon and ItsStructural EvolutionDefinition 2.1 A dynamic lexicon is a tripleLex = ?S,R, T ?
in which1.
S is a well-structured set with a type1 t,2.
R is the set of deductive rules on S, and3.
T is the set of all structural transformationsof S, keeping the type t.Definition 2.2 Lexicon ?S?, R, T ?
is called theevolution result of the lexicon ?S,R, T ?
if ?t ?
T ?such that St?
S?
(or briefly S  S?).
The pro-cess of a dynamic lexicon to its evolution result iscalled an evolution.
Obviously, T is a group withthe operation of composition.Definition 2.3 ?S,R, T ?
is called simple struc-tured if T is a commutative group, otherwise com-plex structured.The more complex is the structure of S, the moredifficult are the applications of R and T .
Sincesome part of semantic knowledge is representedby the structure, the complexity balance betweenthe structure and R (or T ) is one of the seriousproblems in Computational Lexicology.Definition 2.4 Let ?
(S) denote the least numberof operations constructing S, and ?
(S  S?)
theleast number of operations from S to S?.
It?s easyto verify thatTheorem 2.1 ?(?)
is a distance, i.e., it satisfiesthat ?S, S?, S??,1.
?
(S  S?)
?
02.
?
(S  S?)
= 0 ?
S = S?3.
?
(S  S?)
= ?(S?
S)4.
?
(S  S??)
?
?
(S  S?)
+ ?(S?
S??
)1For instance, labeled tree or complete lattice.Corollary 2.1 ?(S?)
?
?
(S) + ?
(S  S?
)Definition 2.5 The degree of structural destruc-tion from S to S?, is defined by?
(S  S?)
= 1??(S?)?
(S) + ?
(S  S?
)(1)Property 2.1 0 ?
?
(S  S?)
?
1Definition 2.6 Let S  S1  ?
?
?
Sn  ?
?
?be a sequence of evolution, the sequence is calledconvergent if there exists a constant A s.t.
0 ?A ?
1 and limn???
(S  Sn) = A.It?s easy to see that a local evolution of the lex-icon may not be an optimization even for a spe-cific application.
The index ?
indicates the con-vergence of lexical structure, guaranteeing a stablemachine learning of the dynamic lexicon.
Actually,the structure of the so-called common knowledgeis nothing but a statistical distribution, which iseffected by the cultures and personal experiences.Oriented to a particular application, such as IE,IR, MT, etc, the appropriate semantic descriptionsin a WordNet-like lexicon seem necessary.Example 2.1 C = {earthquake, quake, tem-blor, seism} is not only a kind of C ?
={geological phenomenon}, but also a kind of C ??
={natural disaster}.3 WSD based on WordNet-like LexiconWhat does it mean that a machine could under-stand a given sentence S or a text T?
As weknow, Turing Test of NLU includes at least themeaning of any word w in S or T .
Thus, the pre-requisite WSD is to tag the semantic informationof w automatically.
WordNet2 in Princeton Uni-versity, in despite of its disputed quality, providesan approach to the formalization of concepts innatural language, in which a concept is defined bya synonym set (SynSet).
A more important workin WordNet is the construction of a well-structuredconcept network based on the hypernymy relation(the main framework) and other accessorial rela-tions, such as, the opposite relation, the holonymyrelation, entailment, cause, etc.Definition 3.1 A WordNet-like lexicon is a dy-namic lexicon with the type of WordNet:1. restricted to each category, S is a labeled treefrom the viewpoint of the hypernymy relationfor both noun concepts and verb concepts,2The specification of WordNet could be found in [3], [4],[5], [9], [10], [11], etc.2.
some accessorial relations between the noun(or verb) concepts, and3.
closed semantic constraint of the argument(s)of each verb concept from the noun concepts.The WordNet-like lexicon is complex structured, itmay not have the same ontology of WordNet, nei-ther the semantic knowledge representations.
Butthe description method seems a general format forall languages from the fact of EuroWordNet (see[12]), Chinese Concept Dictionary (CCD, see [7],[13], [14] and [15]), Korean WordNet, Tamil Word-Net, etc.Definition 3.2 Let ?
be the set of all words,then ?, the set of all concepts (or SynSets) in aWordNet-like lexicon, is a subset of 2?.
The setof all SynSets containing w is denoted by ?
(w), inwhich each element is called a sense of w.Definition 3.3 Given a well-defined sentenceS = w1w2 ?
?
?wn, WSD is the computable pro-cessing which tags wi a unique sense si ={wi, wi1 , ?
?
?
, wik} such that each derived combi-natorial path is a well-defined sentence with thesemantics of S. The Principle of Substitution pro-vides a corpus-based empirical approach to testa SynSet well-defined or not.
The SynSet is thesmallest unit in a WordNet-like lexicon, which isthe underlying of the structural descriptions be-tween the concepts.The training of concept TagSet and the statisticalmodel of WSD are interactional, which is the mainidea of our approach to WSD based on a WordNet-like lexicon.3.1 The Training of TagSetThe traditional semantic tags are from some on-tology, the apriority of which is often criticizedby computational linguists.
For us, the empiri-cal method must impenetrate each step of WSDbecause of the complexity of language knowledge.The statistical approach to WSD needs a wellconcept-tagged corpus as the training set for theconcept TagSet and the statistical data in the Hid-den Markov Model (HMM).
To avoid the sparsedata problem, only a few real subsets of ?
couldact as the TagSet in the statistical model (see [15]and [16]).
The first step leads to a set of structuredTagSets {T1,T2, ?
?
?
,Tm}, then the second step isto choose the most efficient one which makes thebest accuracy of the statistical concept tagging.Different from those unframed tags, the deductiverule along the hypernymy trees works out the senseof w by the following property:Property 3.1 Suppose that the TagSet is T ={C1, C2, ?
?
?
, Ck}, and the word w in a given sen-tence is tagged by Ci, then the sense of w hereis the SynSet C which satisfies that Ci  C andw ?
C, where  is the partial ordering of the nodesin the hypernymy tree.3.2 Statistical Model of WSDIn some sense, WSD is the kernel problem of bothNLU and NLP ([1], [6], [8]).
POS and concept tagare two random variables in the HMM of WSD.Sometimes POS of w determines its sense, some-times not.
But in most cases, a sense of w impliesa unique POS.
The distribution of w?s senses withthe POS, P , is important in the (POS, concept)-tagging.
A Hidden Markov Model with two pa-rameters will be adopted as the main statisticalmodel for WSD, and the Statistical Decision The-ory and Bayesian Analysis, which are good at an-alyzing the small samples, conducted as a com-parison.
The training corpus, T , is done by hand,where the cursor sensitive display of the senses pro-vides the help information.Definition 3.4 Consider the well-defined sen-tence S = w1w2 ?
?
?wn.
By the lexicon, letS = w1/P (i)1w2/P (i)2?
?
?wn/P (i)n be a possible POStagged result, where i ?
I. Definef(i) = argmaxj?JP(C(i,j)1 ?
?
?C(i,j)n |P(i)1 ?
?
?P(i)n )= argmaxj?JP(C(i,j)1 ?
?
?C(i,j)n , P(i)1 ?
?
?P(i)n )= argmaxj?JP(C(i,j)1 ?
?
?C(i,j)n )(2)The HMM of concept can simulate the HMM withtwo parameters of (POS, concept).
f(i) in (2) ispredigested tof(i) = argmaxj?JP(C(i,j)1 )n?k=2P(C(i,j)k |C(i,j)k?1 ) (3)Property 3.2 There exists a unique map g fromthe set of {P (i)1 P(i)2 ?
?
?P(i)n |i ?
I} to the set of{C(i,j)1 C(i,j)2 ?
?
?C(i,j)n |(i, j) ?
I?J}, which satisfiesthatg(P (i)1 ?
?
?P(i)n ) = C(i,f(i))1 ?
?
?C(i,f(i))n (4)where ?i, k,?C ?
?
(wk) s.t.
C(i,f(i))k  C. Ifthere is C ?
6= C satisfying C ?
?
?
(wk) andC(i,f(i))k  C?, then the one with more distribu-tion is the selected sense of wk.Property 3.3 Let s = w1w2 ?
?
?wn be any pos-sible segmented sequence of S, correspondinga set of probabilities of POS sequences As ={P(P (i)1 P(i)2 ?
?
?P(i)n )|i ?
I}.
Each P(i)1 P(i)2 ?
?
?P(i)ncorresponds a set of probabilities of concept se-quences B(i)s = {P(C(i,j)1 C(i,j)2 ?
?
?C(i,j)n )|j ?
J},where C(i,j)k has the POS of P(i)k , thenargmaxs(a ?maxs(As) + b ?maxi,s(B(i)s )) (5)is the choice of segmentation, where a > 0, b > 0and a+ b = 1.
More precisely, (5) is rewritten byargmaxs{maxi{a ?
P(P (i)s ) + b ?
P(g(P(i)s ))}} (6)where P (i)s = P(i)1 P(i)2 ?
?
?P(i)n .4 WSD driven Closed Seman-tic ConstraintFrom the corpus and the statistical WSD, we canmake an induction of the arguments along the hy-pernymy tree, which leads to the closed semanticconstraints automatically.
At the same time, theclosed semantic constraints also provide a possibleapproach to the empirical optimization of ?N and?V .
While the total optimization of a WordNet-like lexicon is still an open problem.4.1 Similarity between ConceptsDefinition 4.1 A labeled tree is a 5-tuple T =?N,Q,D, P, L?
satisfying that:1.
N is a finite set of nodes2.
Q is a finite set of labels3.
D is a partial ordering on N , called domi-nance relation4.
P is a strict partial ordering on N , calledprecedence relation5.
(?x ?
N)(?y ?
N)[(x, y) ?
D]6.
(?x, y ?
N)[[(x, y) ?
P ?
(y, x) ?
P ] ?
[(x, y) /?
D ?
(y, x) /?
D]]7.
(?x, y, z, w ?
N)[[(w, x) ?
P ?
(w, y) ?
D ?
(x, z) ?
D] ?
(y, z) ?
P ]8.
L : N ?
Q is a label mapDefinition 4.2 A hypernymy tree is a labeledtree, in which the label map is one-to-one.
Al-ways, we presume that the hypernymy tree is notdegenerative.In a hypernymy tree of a WordNet-like lexi-con, a node is a code and a label is a SynSet.Since the label map is injective, without gen-erality, a SynSet is usually denoted by a node.We assume that the precedence relation betweenthe brother nodes always implies an ordering oftime, usage, frequency, mood, etc.
For instance,{spring, springtime} ?
{summer, summertime} ?
{fall, autumn} ?
{winter,wintertime} as the hy-ponyms of {season, time of year}.Definition 4.3 Let f, b and B denote father, thenearest younger-brother and the nearest elder-brother respectively, satisfying that f = fb, f =fB and Bb = bB = 1.Definition 4.4 ?x, y ?
N , let z ?
N be theirnearest ancestor satisfying z = fm(x) and z =fn(y), D(x, y)def= m + n. k ?
N is called theoffset of x from its eldest brother if ?Bk(x) and@Bk+1(x).
Let the offset of y is l, the similaritybetween x and y is:?
If mn = 1, S(x, y)def= ?0, |k ?
l|??
If mn 6= 1, S(x, y)def= ?m+ n, 0?Definition 4.5 Suppose that S(x1, y1) = ?a1, b1?and S(x2, y2) = ?a2, b2?, the comparison of simi-larities is defined as follows:1. a1 = a2?
S(x1, y1)  S(x2, y2) ?
b1 ?
b22.
a1 6= a2?
If a1 < a2, then S(x1, y1) ?
S(x2, y2)?
If a1 > a2, then S(x2, y2) ?
S(x1, y1)Theorem 4.1 ?
{S(x, y)|x, y ?
N},?
is a totallyordered set.The elementary structural transformations in aWordNet-like lexicon include:1. insert a non-root brother-node;2. collapse a non-root node to its father-node;3. root is adding a new root;4. add a link between two labeled trees;5. delete a link between two labeled trees.4.2 Induction of Constraints?N (or ?V ) denotes the set of noun (or verb) con-cepts.
Let C ?
?V be a verb concept with oneargument.
Suppose that we have gotten the ini-tial closed semantic constraint of its argument,C ?
?
?N , from a concept-tagged sentence.
Alink from C ?
to C is added between ?N and ?V .If C ??
from another sentence is also a close se-mantic constraint of C?s argument, then the in-fimum of C ?
and C ?
?, inf(C ?, C ??
), is the new C ?.
?x ?
?, C ?
 x, if the substitution from C to x stillinduces well-formed sentences, then the inductionsucceeds.
Otherwise, the disjointed union C ?
?C ?
?is the closed semantic constraint.Definition 4.6 The induction of the closedsemantic constraints of C,D ?
?
is defined byCuD =??
?inf(C,D) if ?x[inf(C,D)  x]succeeds in the substitutionC ?D otherwiseDefinition 4.7 By Theorem 4.1, the inductionbetween C ?D and E ?
?
is defined by(C ?D) u E ={(C u E) ?D if S(C,E)  S(D,E)C ?
(D u E) otherwiseTheoretically, if C1?C2??
?
?
?Cn is the closed se-mantic constraint of the argument of C ?
?V , then?i, ?x[Ci  x] succeeds in the substitution.
Thus,in the WordNet-like lexicon, there are n links from?N to ?V for C, where n is called the length of theconstraint.
The approach to the closed semanticconstraints of the verb concepts with two argu-ments is similar.4.3 Clustering of ConstraintsDefinition 4.8 Suppose that there are N argu-ments for all verb concepts and the length of thei-th constraint is li, then l?
=N?i=1li/N is called theaverage length of the constraints.l?
indicates the rationality of the concept classifi-cation in a WordNet-like lexicon, which also actsas an index of the evolution.
Our presuppositionis that the optimization of the lexicon must havethe least average length of the constraints.
Theclustering of noun concepts constrained by the ar-guments of the verb concepts should be a standardof the classification of ?N .Definition 4.9 S  S1  ?
?
?
Sn  ?
?
?
isCauchy sequence of evolution iff ? > 0,?N ?N,?i, j > N, ?
(Si  Sj) < .Theorem 4.2 The Cauchy sequence of evolutionis convergent.
And ? > 0,?i, j ?
N s.t.
|l?
(Si) ?l?
(Sj)| < .
?N is structured by not only the hypernymy rela-tion but also the closed semantic constraints.
Ofcourse, the hypernymy relation in ?N is princi-pal, but not necessarily unique.
As described inExample 2.1, the distinct angles of view provideenough space for the evolution.
By the hypernymyrelation in ?V , we haveProperty 4.1 ?C,C ?
?
?V , C  C ?, if the closedsemantic constraint of C ?
is C1 ?
C2 ?
?
?
?
?
Cn,then ?Cn+1, ?
?
?
, Cm ?
?N such that (((C1 ?C2 ??
?
??Cn)uCn+1)u?
?
?uCm) is the closed semanticconstraint of C.This property provides an approach to the empiri-cal testing of the concept classification of ?V if ?Nis fixed.
Separately, ?N (or ?V ) can be evaluatedby some indexes and evolves to a satisfiable result.A little more complicated, the closed semantic con-straints destroy the independent evolution of ?Nand ?V .
If ?V is fixed, then the optimization of?N may be implemented (but not completely re-liable) and vice versa.
While it is still an openproblem to define a numerical measure that couldformalize the optimization of the total structuresin a WordNet-like lexicon, especially ?N and ?V .5 ConclusionA scheme of the closed semantic constraint in aWordNet-like lexicon based on WSD has been de-scribed as an application driven construction ofa dynamic lexicon.
At the same time, the fur-ther topic leads to how the rule-based concepttagging benefits from the descriptions of seman-tic constraint.
The empirical method is much em-phasized in the WSD and the development of thedynamic lexicon, such as the TagSet training, theSynSet testing and the evolution of a WordNet-likelexicon (see [15], [16] and [17]).
The author be-lieves that the computable part in ComputationalLexicology is nothing but the evolution of the dy-namic lexicon oriented to a particular application,which is actually the optimization of the languageknowledge base.AcknowledgementI appreciate all my colleagues participating in theCCD project, the blithesome collaboration withthem is always memorable for me.
Many thanks tomy friends in the Second and the Third Workshopon Chinese Lexical Semantics for their kindly dis-cussion with the author.
Lastly, the most thankfulwords are given to my wife for her longtime toler-ance to my weaselling from the housework underthe false pretense of research.References[1] ALPAC 1966 Language and Machine: Com-puters in Translation and Linguistics, Na-tional Research Council Automatic LanguageProcessing Advisory Committee, Washing-ton, D.C.[2] Aristotle 1941 Categoriae, in The BasicWorks of Aristotle, R. McKeon (ed).
RandomHouse, New York.
[3] Beckwith R. 1998 Design and Implementationof the WordNet Lexical Database and Search-ing Software, in [5], pp105-127.
[4] Fellbaum C. 1998 A Semantic Net of EnglishVerbs, in [5], pp69-104.
[5] Fellbaum C. (ed) 1999 WordNet: An Elec-tronic Lexical Database, The MIT Press.
[6] Ide N. and Ve?ronis J.
1998 Introduction toSpecial Issue on Word Sense Disambiguation:The State of Art, Computational Linguistics,Vol.
24, No.
1, pp1-40.
[7] Liu Y, Yu S.W.
and Yu J.S.
Building a Bilin-gual WordNet: New Approaches and algo-rithms, accepted by COLING2002.
[8] Manning C.D.
and Schu?tze H. 1999, Founda-tions of Statistical Natural Language Process-ing, The MIT Press.
[9] Miller G.A.
et al1993 Introduction to Word-Net: An On-line Lexical Database, in the at-tached specification of WordNet 1.6.
[10] Miller G.A.
1998 Nouns in WordNet, in [5],pp23-46.
[11] Priss U.
1999 The Formalization of WordNetby Methods of Relational Concept Analysis, in[5], pp179-196.
[12] Vossen P.
(ed.)
1998 EuroWordNet: A Multi-linugual Database with Lexical Semantic Net-works.
Dordrecht: Kluwer.
[13] Yu J.S.
and Yu S.W.
et al2001 Introduc-tion to Chinese Concept Dictionary, in Inter-national Conference on Chinese Computing(ICCC2001), pp361-367.
[14] Yu J.S.
2001 The Structure of Chinese Con-cept Dictionary, accepted by Journal of Chi-nese Information Processing, 2001.
[15] Yu J.S.
2001 Evolution of WordNet-like Lexi-con, in The First International Conference ofGlobal WordNet, Mysore, India, 2002.
[16] Yu J.S.
and Yu S.W.
2002 Word Sense Dis-ambiguation based on Integrated LanguageKnowledge Base, in The 2nd InternationalConference on East-Asian Language Pro-cessing and Internet Information Technology(EALPIIT?2002).
[17] Yu J.S.
2002 Statistical Methods in WordSense Disambiguation, draft (can be down-loaded from http://icl.pku.edu.cn/yujs/) ofseminar at the Institute of ComputationalLinguistics, Peking Univ..
