In: Proceedings of CoNLL-2000 and LLL-2000, pages 87-90, Lisbon, Portugal, 2000.Generating Synthetic Speech Prosody with Lazy Learningin Tree StructuresLaurent  Bl inIRISA-ENSSATF-22305 Lannion, Franceblin@enssat, frLaurent  M ic le tIRISA-ENSSATF-22305 Lannion, Francemiclet@enssat, frAbst rac tWe present ongoing work on prosody predic-tion for speech synthesis.
This approach con-siders sentences as tree structures and infersthe prosody from a corpus of such structuresusing machine learning techniques.
The predic-tion is achieved from the prosody of the closestsentence of the corpus through tree similaritymeasurements, using either the nearest neigh-bour algorithm or an analogy-based approach.We introduce two different ree structure rep-resentations, the tree similarity metrics consid-ered, and then we discuss the different predic-tion methods.
Experiments are currently underprocess to qualify this approach.1 I n t roduct ionNatural prosody production remains a problemin speech synthesis ystems.
Several automaticprediction methods have already been tried forthis, including decision trees (Ross, 1995), neu-ral networks (Traber, 1992), and HMMs (Jensenet al, 1994).
The original aspect of our pre-diction approach is a tree structure representa-tion of sentences, and the use of tree similar-ity measurements to achieve the prosody pre-diction.
We think that reasoning on a wholestructure rather than on local features of a sen-tence should better reflect the many relationsinfluencing the prosody.
This approach is anattempt o achieve such a goal.The data used in this work is a part of theBoston University Radio (WBUR) News Cor-pus (Ostendorfet al, 1995).
The prosodic infor-mation consists of ToBI labeling of accents andbreaks (Silverman et al, 1992).
The syntacticand part-of-speech informations were obtainedfrom the part of the corpus processed in thePenn Treebank project (Marcus et al, 1993).We firstly describe the tree structures definedfor this work, then present he tree metrics thatwe are using, and finally discuss how they aremanipulated to achieve the prosody prediction.2 T ree  S t ructuresSo far we have considered two types of struc-tures in this work: a simple syntactic structureand a performance structure (Gee and Grosjean,1983).
Their comparison in use should providesome interesting knowledge about the usefulnessor the limitations of the elements of informationincluded in each one.2.1 Syntactic StructureThe syntactic structure considered is built ex-clusively from the syntactic parsing of the givensentences.
This parsing, with the relative syn-tactic tags, constitute the backbone of the struc-ture.
Below this structure lie the words of thesentence, with their part-of-speech tags.
Addi-tional levels of nodes can be added deeper inthe structure to represent the syllables of eachword, and the phonemes of each syllable.The syntactic structure corresponding to thesentence "Hennessy will be a hard act to follow"is presented in Figure 1 as an example (the syl-lable level has been omitted for clarity).2.2 Per fo rmance  StructureThe performance structure used in our approachis a combination of syntactic and phonologicalinformations.
Its upper part is a binary treewhere each node represents a break between thetwo parts of the sentence contained into the sub-trees of the node.
This binary structure definesa hierarchy: the closer to the root the node is,the more salient (or stronger) the break is.87SHennessy \[NNP\] a \ [D~j j \ ]  act ~I vPto \ [TO~ow \[VB\]Figure 1: Syntactic structure for the sentence"Hennessy will be a hard act to follow".
(Syn-tactic labels: S: simple declarative clause, NP:noun phrase, VP: verb phrase.
Part-of-speechlabels: NNP: proper noun, MD: modal, VB:base form verb, DT: determiner, J J: adjective,NN: singular noun, TO: special abel for "to").The lower part represents the phonologicalphrases into which the whole sentence is dividedby the binary structure, and uses the same rep-resentation levels as in the syntactic structure.The only difference comes from a simplificationperformed by joining the words into phonolog-ical words (composed of one content word -noun, adjective, verb or adverb - and of thesurrounding function words).
Each phonologi-cal phrase is labeled with a syntactic ategory(the main one), and no break is supposed tooccur inside.A possible performance structure for the sameexample: "Hennessy will be a hard act to fol-low" is shown in Figure 2.I Hennessy \[NNP\] P ~ N PI will be \[VB\] a hard\[J J\] act \[NNIFigure 2: Performance structure for the sen-tence "Hennessy will be a hard act to follow".The syntactic and part-of-speech labels have thesame meaning as in Figure 1.
B1, B2 and B3are the break-related nodes.Unlike the syntactic structure, a first step ofprediction is done in the performance structurewith the break values.
This prosody informa-tion is known for the sentences in the corpus,but has to be predicted for new ones (to putour system in a full synthesis context whereno prosodic value is available).
The currentlyused method (Bachenko and Fitzpatrick, 1990)provides rules to infer a default phrasing fora sentence.
Not only the effects of this esti-mation will have to be quantified, but we planto develop a more accurate solution to predictthis structure accordingly to any corpus speakercharacteristics.3 T ree  Met r i csNow that the tree structures are defined, weneed the tools to predict the prosody.
We haveconsidered two similarity metrics to calculatethe "distance" between two tree structures, in-spired from the Wagner and Fisher's editing dis-tance (Wagner and Fisher, 1974).3.1 Pr inc ip lesIntroducing a small set of elementary transfor-mation operators upon trees (insertion or dele-tion of a node, substitution of a node by an-other one) it is possible to determine a set ofspecific operation sequences that transform anygiven tree into another one.
Specifying costsfor each elementary operation (possibly a func-tion of the node values) allows the evaluationof a whole transformation cost by adding theoperation costs in the sequence.
Therefore thetree distance can be defined as the cost of thesequence minimizing this sum.3.2 Cons idered  Met r i csMany metrics can be defined from this princi-ple.
The differences come from the applicationconditions which can be set on the operators.
Inour experiments, two metrics are tested.
Theyboth preserve the order of the nodes in the trees,an essential condition in our application.The first one (Selkow, 1977) allows only sub-stitutions between odes at the same depth levelin the trees.
Moreover, the insertion or deletionof a node involves respectively the insertion ordeletion of the whole subtree depending of thenode.
These strict conditions hould be able tolocate very close structures.The other one (Zhang, 1995) allows the sub-stitutions of nodes whatever theirs locations areinside the structures.
It also allows the insertionor deletion of lonely nodes inside the structures.Compared to the previous metric, these less rig-orous stipulations hould not only retrieve the88very close structures, but also other ones whichwouldn't have been found.Moreover, these two algorithms also providea mapping between the nodes of the trees.
Thismapping illustrates the operations which led tothe final distance value: the parts of the treeswhich were inserted or deleted, and the oneswhich were substituted or unchanged.3.3 Operation CostsAs exposed in section 3.1, a tree is "close" toanother one because of the definition of the op-erators costs.
In this work, they have been setto allow the only comparison of nodes of samestructural nature (break-related nodes together,syllable-related nodes together...), and to repre-sent the linguistic "similarity" between compa-rable elements (to set that an adjective may be"closer" to a noun than to a determiner...).These operation costs are currently manuallyset.
To decide on the scale of values to affectis not an easy task, and it needs some humanexpertise.
One possibility would be to furtherautomate the process for setting these values.4 P rosody  Pred ic t ionThe tree representations and the metrics cannow be used to predict the prosody of a sen-tence.4.1 Nearest Neighbour Predict ionThe simple method that we have firstly used isthe nearest neighbour algorithm: given a newsentence, the closest match among the corpusof sentences of known prosody is retrieved andused to infer the prosody of the new sentence.The mapping from the tree distance computa-tions can be used to give a simple way to knowwhere to apply the prosody of one sentence ontothe other one, from the words linked inside.Unfortunately, this process may not be aseasy.
The ideal mapping would be that eachword of the new sentence had a correspondingword in the other sentence.
Hopeless, the twosentences may not be as closed as desired, andsome words may have been inserted or deleted.To decide on the prosody for these unlinkedparts is a problem.4.2 Analogy-Based Predict ionA potential way to improve the prediction isbased on analogy.
The previous mapping be-tween the two structures defines a tree transfor-mation.
The idea of this approach is based onthe knowledge brought by other pairs of struc-tures from the corpus sharing the same trans-formation.This approach can be connected to the ana-logical framework defined by Pirrelli and Yvon,where inference processes are presented for sym-bolic and string values by the mean of two no-tions: the analogical proportion, and the ana-logical transfer (Pirrelli and Yvon, 1999).Concerning our problem, and given threeknown tree structures T1, T2, T3 and a new oneT I, an analogical proportion would be expressedas: T1 is to T2 as T3 is to T ~ if and only if the setof operations transforming T1 into T2 is equiva-lent to the one transforming T3 into T I, accord-ingly to a specific tree metric.
There are variouslevels for defining this transformation equiva-lence.
A strict identity would be for instancethe insertion of the same structure at the sameplace, representing the same word (and havingthe same syntactic function in the sentence).
Aless strict equivalence could be the insertion ofa different word having the same number of syl-lables.
Weaker and weaker conditions can beset.
As a consequence, these different possibili-ties have to be tested accordingly to the amountof diversity in the corpus to prove the efficiencyof this equivalence.Next, the analogical transfer would be to ap-ply on the phrase described by T3 the prosodytransformation defined between T1 and T2 as toget the prosody of the phrase of T ~.
The for-malization of this prosody transfer is still underdevelopment.From these two notions, the analogical infer-ence would be therefore defined as:?
firstly, to retrieve all analogical proportionsinvolving T ~ and three known structures inthe corpus;?
secondly, to compute the analogical trans-fer for each 3-tuple of known structures,and to store its result in a set of possibleoutputs if the transfer succeeds.This analogical inference as described abovemay be a long task in the retrieval of every 3-tuple of known structures ince a tree trans-formation can be defined between any pair ofthem.
For very dissimilar structures, the set of89operations would be very complex and uneasyto employ.
A first way to improve this searchis to keep the structure resulting of the near-est neighbour computation as T3.
The trans-formation between T t and T3 should be one ofthe simplest (accordingly to the operations cost;see section 3.3), and then the search would belimited to the retrieval of a pair (T1,T2) sharingan equivalent ransformation.
However, this isstill time-consuming, and we are trying to de-fine a general way to limit the search in such atree structure space, for example based on treeindexing for efficiency (Daelemans et al, 1997).5 F i r s t  Resu l tsBecause of the uncompleted evelopment ofthis approach, most experiments are still underprogress.
So far they were run to find the clos-est match of held-out corpus sentences using thesyntactic structure and the performance struc-ture, for each of the distance metrics.
We areusing both the "actual" and estimated perfor-mance structures to quantify the effects of thisestimation.
Cross-validation tests have beenchosen to validate our method.These experiments are not all complete, butan initial analysis of the results doesn't seem toshow many differences between the tree metricsconsidered.
We believe that this is due to thesmall size of the corpus we are using.
With onlyaround 300 sentences, most structures are verydifferent, so the majority of pairwise compar-isons should be very distant.
We are currentlyrunning experiments where the tree structuresare generated at the phrase level.
This strat-egy implies to adapt the tree metrics to takeinto consideration the location of the phrases inthe sentences (two similar structures hould beprivileged if they have the same location in theirrespective sentences).6 Conc lus ionWe have presented a new prosody predictionmethod.
Its original aspect is to consider sen-tences as tree structures.
Tree similarity metricsand analogy-based learning in a corpus of suchstructures are used to predict the prosody of anew sentence.
Further experiments are neededto validate this approach.An additional development of our methodwould be the introduction of focus labels.
Ina dialogue context, some extra information canrefine the intonation.
With the tree structuresthat we are using, it is easy to introduce spe-cial markers upon the nodes of the structure.According to their nature and location, theycan indicate some focus either on a word, on aphrase or on a whole sentence.
With the adap-tation of the tree metrics, the prediction processis kept unchanged.Re ferencesJ.
Bachenko and E. Fitzpatrick.
1990.
A compu-tational grammar of discourse-neutral prosodicphrasing in English.
Comp.
Ling., 16(3):155-170.W.
Daelemans, A. van den Bosch, and T. Weijters.1997.
IGTree: Using trees for compression andclassification i lazy learning algorithms.
In Arti\].Intel.
Review, volume 11, pages 407-423.
KluwerAcademic Publishers.J.
P. Gee and F. Grosjean.
1983.
Performance struc-tures: a psycholinguistic and linguistic appraisal.Cognitive Psychology, 15:411-458.U.
Jensen, R. K. Moore, P. Dalsgaard, and B. Lind-berg.
1994.
Modelling intonation contours atthe phrase level using continuous density HMMs.Comp.
Speech and Lang., 8:247-260.M.
P. Marcus, B. Santorini, and M. A.Marcinkiewicz.
1993.
Building a large anno-tated corpus of English: the Penn Treebank.Comp.
Ling., 19.M.
Ostendorf, P. J.
Price, and S. Shattuck-Hufnagel.1995.
The Boston University Radio News Corpus.Technical Report ECS-95-001, Boston U.V.
Pirrelli and F. Yvon.
1999.
The hidden dimen-sion: a paradigmatic view of data-driven NLP.
J.of Exp.
and Theo.
Artif.
Intel., 11(3):391-408.K.
Ross.
1995.
Modeling of intonation for speechsynthesis.
Ph.D. thesis, Col. of Eng., Boston U.S. M. Selkow.
1977.
The tree-to-tree editing prob-lem.
Inf.
Processing Letters, 6(6):184-186.K.
Silverman, M. E. Beckman, J. Pitrelli, M. Osten-doff, C. W. Wightman, P. J.
Price, J.
B. Pier-rhumbert, and J. Hirschberg.
1992.
TOBI: Astandard for labelling English prosody.
In Int.Conf.
on Spoken Lang.
Processing, pages 867-870.C.
Traber, 1992.
Talking machines: theories, mod-els and designs, chapter F0 generation with adatabase of natural F0 patterns and with a neuralnetwork, pages 287-304.R.
A. Wagner and M. J. Fisher.
1974.
The string-to-string correction problem.
J. of the Asso.
forComputing Machinery, 21(1):168-173.K.
Zhang.
1995.
Algorithms for the constrainedediting distance between ordered labeled trees andrelated problems.
Pattern Reco., 28(3):463-474.90
