Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 47?55,Gothenburg, Sweden, April 26 2014.c?2014 Association for Computational LinguisticsA Hybrid Disambiguation Measure for Inaccurate Cultural Heritage DataJulia Efremova1, Bijan Ranjbar-Sahraei2, Toon Calders1,31Eindhoven University of Technology, The Netherlands2Maastricht University, The Netherlands3Universit?e Libre de Bruxelles, Belgiumi.efremova@tue.nl, b.ranjbarsahraei@maastrichtuniversity.nl,toon.calders@ulb.ac.beAbstractCultural heritage data is always associ-ated with inaccurate information and dif-ferent types of ambiguities.
For instance,names of persons, occupations or placesmentioned in historical documents are notstandardized and contain numerous varia-tions.
This article examines in detail var-ious existing similarity functions and pro-poses a hybrid technique for the followingtask: among the list of possible names, oc-cupations and places extracted from his-torical documents, identify those that arevariations of the same person name, oc-cupation and place respectively.
The per-formance of our method is evaluated onthree manually constructed datasets andone public dataset in terms of precision,recall and F-measure.
The results demon-strate that the hybrid technique outper-forms current methods and allows to sig-nificantly improve the quality of culturalheritage data.1 IntroductionInaccurate information and lack of common iden-tifiers are problems encountered when combininginformation from heterogeneous sources.
Thereare a number of reasons that can cause inaccu-rate information such as spelling variations, ab-breviations, translation from one language into an-other and modifying long names into shorter ones.Inaccurate information often occurs in many do-mains, for example, during information extractionfrom the Web or when attributing a publication toits proper author.
Inaccurate information is verytypical in cultural heritage data as well.
In his-torical documents a real person could be men-tioned many times, for instance in civil certificatessuch as birth, marriage and death certificates orin property transfer records and tax declarations.The name of the same person, his occupation andthe place in such documents varies a lot.
Whenworking with such information, researchers haveto identify which person references mentioned indifferent historical documents belong to the sameperson entity.
This problem has been referred to inliterature in many different ways but is best knownas entity resolution (ER), record linkage or dupli-cate detection (Lisbach and Meyer, 2013; Chris-ten, 2012; Bhattacharya and Getoor, 2007).
Theprocess of ER in historical documents is alwaysaccompanied by inaccurate information as well.As an example, there are more than 100 variants ofthe first name Jan, such as Johan, Johannes, Janis,Jean or the profession musician in historical doc-uments can be spelled as musikant, muzikant oreven muzikant bij de tiende afd.
The latter meansthe musician in the 10th department.The past few decades have seen a large re-search interest in the problem of inaccurate infor-mation.
As a result, a large number of methodsfor comparing string has been developed.
Thesestandard methods are called string similarity func-tions.
Some of those well known techniques arecharacter-based, token-based or based on phoneticfunctions, for instance Levenshtein Edit distance,Jaro Winkler distance, Monge Elkan distance,Smith Waterman distance, Soundex and DoubleMetaphone.
(Elmagarmid et al., 2007; Navarro,2001; Winkler, 1995).
Each of the mentioned sim-ilarity functions perform optimally for a particulardataset domain.
For example, the phonetic func-tion Soundex works great for encoding names bysound as it pronounced in English, but neverthe-less sometimes it is also used to encode namesin other European languages.
However, only lit-tle work has been done in studying combinationsof similarity functions, and in their simultaneoususe for achieving more reliable results.
Bilenko(2003) in his work computes names similarity with47affine gaps to train the Support Vector Machinesclassifier.
Ristard and Yianilos (1998) designeda learnable Levenshtein distance for solving thestring similarity problem.
Tejada et al.
(2002)learned weights of different types of string trans-formations.In this paper we explore various traditionalstring similarity functions for solving data ambi-guities and also design a supervised hybrid tech-nique.
We carry out our experiments on three man-ually constructed datasets: Dutch names, occupa-tions and places, and also on one publicly avail-able dataset of restaurants.
The clarified function,that will allow us to recognize difficult ambiguitiesin textual fields, later will be incorporated into theoverall ER process for a large historical database.The main contributions of this paper is a practicalstudy of existing techniques and the design and theextensive analysis of a hybrid technique that allowus to achieve a significant improvement in results.The remainder of this paper is structured as fol-lows.
In Section 2 we begin by presenting typicalambiguities in real-life cultural heritage data.
inSection 3 we give an overview of standard stringsimilarity functions.
We describe the general hy-brid approach in Section 4.
In Section 5 we de-scribe the prediction models that we use in thehybrid approach.
In Section 6 we provide detailsabout carrying out the experiments.
In Section 7we present an evaluation of the results.
Section8 offers a discussion about applying the designedapproach to real-world data.
Concluding remarksare given in Section 9.2 A Real-Life Cultural Heritage DataIn this paper we use historical documents such asbirth, marriage and death certificated provided byBrabants Historisch Informatie Centrum (BHIC)1to extract most common person names, occupa-tions and places.
Civil certificates are belongingto North Brabant, a province of the Netherlands, inthe period 1700 - 1920.
To study the name ambi-guity we used a subset of data consisting of 10000randomly selected different documents.Then foreach name we obtain its standardized code in thedatabase of Meertens Instituut2which has a largecollection of Dutch names and last names andtheir typical variations.
In the same way, in thedatabase of The Historical Sample of the Nether-1http://www.bhic.nl2http://www.meertens.knaw.nl/nvb/lands (HSN)3, for each occupation and place ex-tracted from civil certificates where possible, weobtain its standardized code (van Leeuwen et al.,2002; Mandemakers et al., 2013).
Historians havespent a number of years for creating a database ofnames, occupations and places variations.
Usingsuch data gives us a unique opportunity to exploretypical variations in different domains and to de-sign a robust technique which is able to deal withthem automatically.The resulting Name variations dataset contains2170 distinct names that correspond to 1326 stan-dardized forms.
Table 1 shows a typical exampleof the constructed dataset of name variations.ref id Name name id1 Eustachius 12 Statius 13 Stefan 24 Stephan 25 Stephanus 2Table 1: An example of a name variation datasetThe second dataset of Occupations contains1401 occupation records which belong to 1098standardized occupations.The third dataset of Places contains 1196 lo-cations records belonging to 617 standardizedplaces.3 Traditional Similarity FunctionsThere are three main different types of string sim-ilarity functions that can be used for variationtasks, namely character-based, phonetic-based andtoken-based.
Each of them we investigate in detailbelow.3.1 Character-Based SimilarityCharacter-based similarities operate on charactersequences and their composition which makesthem suitable for identifying imprecise names andspelling errors.
They compute the similarity be-tween two strings as the difference between theircommon characters.
In this paper, we will con-sider the Levenshtein edit distance (LE), Jaro (J),Jaro Winkler (JW), Smith Waterman (SW), SmithWaterman with Gotohs backtracing (GH), Needle-man Wunch (NW) and Monge Elkan (ME) stringsimilarities (Elmagarmid et al., 2007; Christen,2012; Naumann and Herschel, 2010).
All ofthem return a number between 0 and 1 inclusively,3http://www.iisg.nl/hsn/data/occupations.html48where the highest value when two names are ex-actly the same.
Table 3 shows an example of com-puted character-based similarities for three namepair-variants.3.2 Phonetic-Based SimilarityPhonetic similarity functions analyze the soundsof the names being compared instead of theirspelling differences.
For example, the two namesStefan and Stephan barely differ phonetically, butnevertheless they have different spellings.
Pho-netic functions encode every name with phonetickeys based on a set of rules.
For instance, somealgorithms ignore all vowels and compare onlythe groups of consonants, other algorithms ana-lyze consonant combinations and thier sound thatdescribe a large number of sounds.
In this pa-per, we analyze 4 phonetic functions: Soundex(SN), Double Metaphone (DM), IBMAlphaCode(IA) and New York State Identification and Intelli-gence System (NY) (Christen, 2006).
The Table 2shows an example of applied phonetic keys to en-code imprecise names.Name SN DM IA NYSIISStefan S315 STFN 00182 STAFANStephan S315 STFN 00182 STAFPANStephanus S3152 STFNS 00182 STAFPANTable 2: An example of phonetic keys3.3 Token-Based SimilarityToken-based functions divide two strings into setsof tokens s1and s2, then they compute the in-tersection between two sets based on the num-ber of equal tokens.
Some token-based functions,for instance Dice similarity (DS), Jaccard coeffi-cient(JS) and Cosine similarity (CS) (Cohen et al.,2003), consider as a token the whole word in astring.
In our case most of the person names, lo-cations and places are quite different and there areonly few intersections between token-words avail-able.
Another approach, a q-gram (QG) tokeniza-tion (McNamee and Mayfield, 2004), divides astring into smaller tokens of size q. QG calculatesthe similarity between two strings by counting thenumber of q-grams in common and dividing by thenumber of q-grams in the longer string.
In this pa-per we consider bigrams (q = 2).
For example, thename ?stefan?
contains the bigrams ?st?, ?te?, ?ef?,?fa?,?an?.
An example of applied QG and JS simi-larities is shown in Table 3.two names LE J JW SW GH NW ME QG JS(Stefan, Stephan) 0.71 0.85 0.89 0.58 0.57 0.79 0.57 0.5 0(Stefan, Stephanus) 0.56 0.80 0.86 0.58 0.57 0.61 0.57 0.38 0(Stephan, Stephanus) 0.78 0.93 0.97 1 1 0.78 1 0.75 0Table 3: An example of character and token-basedsimilarities3.4 Exploration of Standard MethodsThe goal of this paper is to investigate in how farthe terms variation task can be addressed by usingstandard methods and improve the results by ap-plying a hybrid technique.
Fig.
1 shows for eachstring similarity function the distribution betweentwo non-matching pairs of records on the one handand two matching pairs of records on the other fordifferent measures.
The more discriminative themeasure is, the larger is the separation between thedistributions.
However, in this figure, each of sim-ilarity functions is considered independently andcan be expected to only perform well in certainsituations.
Therefore, the goal of this paper is todesign an appropriate hybrid technique, which al-lows to achieve better performance results by us-ing a combination of traditional measures.4 General Hybrid ApproachIn this article we propose a new hybrid approachwhich takes advantage of a number of existingstring similarities.
Our method takes into accountthe most relevant string similarity by obtaining aranking of each in terms of its importance for aclassification task.
The outline of the algorithmof the hybrid approach is shown below.
The al-gorithm uses training data B which is provided inthe form of matching and non-matching pairs ofterms.
First, in steps 1 to 5 the algorithm calcu-lates pairwise similarities between two terms byevery string function (sim1, sim2, ..., simK).
Insteps 6 to 8 the algorithm computes for every simian importance rate using the Random Forest tech-nique (Genuer et al., 2010; Breiman, 2001).
Insubsection 4.2 we describe in more detail the pro-cess of selecting the most important string similar-ities.
Then, in steps 10 to 22 the algorithm itera-tively constructs the set of the similarity functionsT?which is a subset of Sim.
It starts from anempty set T?and at each iteration it adds to T?the measure that has the highest importance rateand after that it learns the classifier C. After everyiteration the algorithm evaluates the performance49(a) Levenshtein (b) Smith Waterman (c) Monge Elkan (d) Jaro Winkler(e) Jaro (f) Gotoh (g) Needleman Wunch (h) Qgrams Distance(i) Dice Similarity (j) Jaccard Similarity (k) Double Metaphone (l) Soundex(m) IBMAlphaCode (n) NYSIISCodeFigure 1: The distribution between two matching and two non-matching pairs of records for each string similarity functionin term of maximum F measure Fmeas on thevalidation set R. The algorithm stops if Fmeasdoesn?t increase anymore or if the size T?reachesthe parameter ?
which can be set as a fraction ofthe total number of string similarity functions.4.1 Pairwise Similarity CalculationIn order to solve the name ambiguity problem it isnecessary to compute the similarity score betweentwo records.
Most of the standard string similarityfunctions and standard classifiers require a pair-wise records comparison.
We convert each datasetdescribed in Section 2 into a dataset of variantpairs using random combinations of records.
Twodifferently spelled terms are equal when their stan-dardized codes are the same and different other-wise.
The example of term pair-variants dataseton is shown in Table 4.Name1 Name2 classStatius Eustachius 1Statius Stefan 0Stefan Stephanus 1Table 4: An example of term pair-variants4.2 Measure SelectionUsing only the most important measures for solv-ing the terms variation task can significantly re-duce the computational cost.
Therefore, we be-fore learning the classifier we apply a selectiontechnique.
Generally, there are two common tech-niques that allow to reduce the number of dimen-sions: filters and wrappers (Das, 2001).
Typi-cally filter-based approaches require only one sin-gle scan, whereas wrapper-based ones iterativelylook for the set of features which are the mostsuitable which leads to larger computational over-50Algorithm 1 Hybrid Disambiguation MeasureInput: Training set B = {b1, ..., b?
}Validation setR = {r1, ..., r?
}Set of similarity measures Sim = (sim1, ..., simK)Maximum allowed number of similarity measures ?L{C,B, T?}
classifier C with learning algorithm Lwhich is trained on the training set BOutput: A hybrid measure Simhbbased on classifier C1: for each b in B do2: for each sim in Sim do3: compute sim(b)4: end for5: end for6: for each sim in Sim do7: compute RFsim{B}8: end for9: T??
?10: Fmeas1{R} ?
011: i?
212: while |T?| ?
?
do13: select simithat maximizes RF importance rate14: Sim?
Sim?
{simi}15: T??
T??
{simi}16: L{C,B, T?
}17: Calculate model performance Fmeasi{R}18: if max(Fmeasi{R}) > max(Fmeasi?1{R})then19: break20: end if21: i?
i+ 122: end while23: Simhb?
L{C,B, T?
}24: return Simhbcorresponding to T?and Cheads.
For designing a hybrid approach we de-cided to use Random Forest (RF) wrappers toevaluate the weight of every similarity measures.RF, according to many different sources is consid-ered as one of the most reliable methods which isable to deal with high-dimensional and noisy data(Saeys et al., 2007).
RF generates a forest of clas-sification trees and then assign an importance rankto each similarity function based on its usefulnessfor the classification purpose.
We use RF resultsto perform a stepwise procedure and to constructthe set of measures T?.4.3 Hybrid Score Computation and pairwiseClassificationWe consider the problem of terms variations asa prediction problem.
There are many availableclassification techniques that are suitable for a pre-diction task.
Many of them require a prior trainingphase on a representative subset of data to makea more efficient prediction on new data.
Afterthat, pairs of references are classified into classesMatched or non-Matched based on a thresholdvalue of the score function.
The score func-tion computes the final similarity score betweentwo terms based on results of single comparisonmeasures.
For learning the score function weuse a training dataset B.
We explore 2 robustclassifiers that could be applied to cultural her-itage dataset domains.
They are the Logistic Re-gression (LG) and the Support Vector Machine(SVM) (Hastie et al., 2003; Cristianini and Shawe-Taylor, 2000).
They are two of the most widely-used classifiers that are suitable for the predictiontask (James et al., 2013).
It is important to addthat we also carried out our experiments and ap-plied three more classifiers, namely Linear Dis-criminant Analysis, Quadratic Discriminant Anal-ysis and k-nearest neighbors (Hastie et al., 2003;Verma, 2012; Zezula et al., 2006).
However re-sults were not improved significantly on all of ourdatasets, so we do not include those classifiers inthe designed hybrid approach.5 The Prediction ModelsIn this Section we will briefly describe models thatwe incorporated into our hybrid approach to ad-dress the problem of inaccurate cultural heritagedata.5.1 Logistic regressionWe apply a logistic regression as a predictivemodel and calculate the score function as follows:Simhb(ai, aj) =11 + e?z, (1)where z = ?0+ ?1?
sim1(ai, aj) + ?2?sim2(ai, aj) + ?
?
?+ ?n?
simK(ai, aj) is an uti-lization of a linear regression model with parame-ters represented by ?1to ?k.
The parameters ?0to ?nare learned in a training phase.
The func-tions sim1(ai, aj) to simK(ai, aj) represent sin-gle similarity measures between two terms aiandaj.5.2 Support Vector MachinesWe apply and explore SVM as a predictive model.The basic idea of SVM is that the training data ismapped into a new high dimensional space whereit is possible to apply linear models to separate theclasses.
A kernel function performs the mappingof the training data into the new space.
After that,a separation between classes is done by maximiz-ing a separation margin between cases belongingto different classes.
In our hybrid approach we usethe SVM classifier with a radial basis kernel func-tion and train it on the training set B.516 ExperimentsOur experiments are conducted on four datasets.Three datasets, namely names, occupations andplaces variations are manually constructed fromCultural Heritage Data.
They are discussed in de-tail in Section 2.The fourth dataset is a public dataset calledRestaurant.
It is a standard benchmark datasetwhich is widely used in data matching studies(Christen, 2012; Bilenko et al., 2003).
It containsinformation about 864 restaurant names and ad-dresses where 112 records are duplicated.
It wasobtained by integrating records from two sources:Fodors and Zagats guidebooks.
The Restaurantdataset was taken from the SecondString toolkit4.We carried out our experiments in accordanceto the algorithm described in Section 4.
At first,we convert each dataset into a dataset of variantpairs using random combinations of records.
Thenfor each pair of records we compute string simi-larity functions.
We randomly divided all avail-able data into two subsets, namely training andtest sets.
To construct the set of string similari-ties we use 70% of the training set to learn RFimportance rate and the other 30% of the train-ing set to validate results under stepwise selec-tion procedure as it was described in the algorithmin Section 4.
The resulting set of selected stringsimilarities for each dataset is shown in Table 5.After constructing the set of string similarities welearn the classifier on the complete training set andthen evaluate it on the test set.
In order to assessthe performance of our results, we apply a 10-foldcross-validation method.
We randomly partitionthe available dataset into 10 equal size subsets.Then one subset was chosen as the validation datafor testing the classifier, and the remaining subsetsare used for training the classifier.
Then the cross-validation process is repeated 10 times, with eachof the 10 subsets used exactly once as the valida-tion dataset.DatasetNames IA SN DM LE SWOccupations JW J LE NW QGPlaces QG JW LE SW JRestaurants QG JW CS NWTable 5: Selected string similarities during thestepwise procedure4http://secondstring.sourceforge.net/7 Evaluation ResultsIn order to evaluate the performance of standardstring similarity functions and the applied hybridapproach, we compute the sets of True Positives(TP), False Positives (FP) and False Negatives(FN) as the correctly identified, incorrectly identi-fied and incorrectly rejected matches, respectively.Fig.
2 demonstrates the performance of standardand hybrid approaches on four examined datasets.The logistic regression as well as SVM clas-sifiers which are used in the hybrid approach oneach of the dataset outperform standard string sim-ilarities.
The improvement in results is significant,especially it is clearly seen on the dataset of occu-pations.
For a more detailed analysis, Fig.
3 showsthe evaluation of results in terms of F-measureand the threshold value for all continuous meth-ods.
Moreover, Table 6 shows the maximum val-ues of the F-measure for the five best performingmethods for each of the datasets.
Two upper rowsof the table belong only to the hybrid approach.SVM and logistic regression in the combinationwith the RF selection technique both demonstraterobustness on the multiple datasets domains.Names Occupations Places RestaurantsMethod Max.F Method Max.F Method Max.F Method Max.FSVM 0.94 SVM 0.93 SVM 0.95 LG 0.95LG 0.92 LG 0.86 LG 0.93 SVM 0.91LE 0.89 J 0.82 QG 0.88 JW 0.87J 0.87 LE 0.77 JW 0.87 QG 0.81SW 0.86 JW 0.71 J 0.85 GH 0.76Table 6: The maximum F-Measure values for the five best-performing methodsIn addition to analyzing the hybrid approach,in this section we investigate in more detail func-tions which demonstrate not the typical behavioron the precision and recall plots.
For instance, onFig.
2 for SW, GH and ME similarities the simul-taneous growth of the precision is accomplishedby the growth in the recall on the interval (0, 0.3).The same situation occurs for SW and GH simi-larities on datasets of occupations and places.
InTable 7 for SW similarity on the dataset on namesfor three levels of the threshold we show its perfor-mance indicators, namely TP, FP and FN valuesand calculated the precision and recall.
With themaximum similarity score SW incorrectly identifyas positive 99 pairs of names.
With the slightlydecrease in the threshold value, the larger numberof pairs are identified correctly as the variation of52(a) The dataset of names (b) The dataset of occupations(c) The dataset of places (d) Public dataset of restaurantsFigure 2: Evaluation of single string similarities and the hybrid approach in terms of precision and recallThreshold TP FP FN Precision Recall0.96 809 99 3853 0.89 0.170.98 355 99 4307 0.78 0.081 200 99 4462 0.67 0.04Table 7: Evaluation measures for SW similarityfor 3 levels of the thresholdthe same name.
Therefore, to make it absolutelyclear, in Table 8 we gave an example of such pairsof names that are included into 99 FP and causethe simultaneous grows of precision and recall.8 DiscussionThe proposed hybrid approach shows very goodresults in performing the pairwise terms com-parison for completely different dataset domains.Nevertheless, the bottleneck of the algorithm isthat it is expensive to apply it to real-world dataand compare all possible combinations of records.Name1 Name2 SW GH MEPeternella Peter 1 1 1Pauline Paul 1 1 1Henriette Henri 1 1 1Table 8: Example of FP pairs of names accordingto the maximum value of SW, GH and ME func-tionsThere are various available techniques for re-ducing the amount of candidate pairs to be com-pared.
Common techniques are partitioning datainto smaller subsets and comparing only recordswith the same partition.
Two widely used partitionapproaches are blocking and windowing methods(Naumann and Herschel, 2010; Bilenko et al.,2003).
The blocking technique assigns to eachrecord a special blocking key, for instance year,place of the documents or the first 3 letters ofthe last name.
The windowing technique such as53(a) The dataset of names (b) The dataset of occupations(c) The dataset of places (d) Public dataset of restaurantsFigure 3: Evaluation of single string similarities and the hybrid approach in terms of F-Measure and ThresholdSorted Neighborhood method sorts data accordingto some key, for instance year of the documents,and then slides a window of fixed size across thesorted data.
Reducing the number of candidatepairs may result that two references that refer tothe same entity appear in different partitions andthen they will never be compared.
Therefore, ournext work will focus on searching the best parti-tion method (or best hybrid methods) that allowsto reduce the number of potential candidate pairsand keep all references referring to the same entitywithin the same partition.9 ConclusionIn this paper we studied a number of tradi-tional string similarities and proposed the hybridapproach applied on different cultural heritagedataset domains.
It is obvious that dealing withhistorical documents, where attributes informationis often imprecise, is not possible by using onlyone string similarity.
Therefore, we investigatedhow to improve the performance by using a num-ber of string similarities and applied supervisedlearning technique.As future step, the authors are working on in-corporating the hybrid approach into overall entityresolution process in a large genealogical database(Efremova et al., 2014), which aims to discoverwhich of the person references mentioned in dif-ferent historical documents refer to the same per-son entity.
The genealogical database contains acollection of historical documents where names,occupations and places are the essential attributes.Therefore it is very important to find a robust andreliable approach which is able to compare mainpersonal information in the noisy data.10 AcknowledgmentsThe authors are grateful to the BHIC Center, inparticular to Rien Wols and Anton Schuttelaars forthe support in data gathering, data analysis and di-rection.54ReferencesIndrajit Bhattacharya and Lise Getoor.
2007.
Collec-tive entity resolution in relational data.
ACM Trans.Knowl.
Discov.
Data, 1(1).Mikhail Bilenko and Raymond J. Mooney.
2003.Adaptive duplicate detection using learnable stringsimilarity measures.
In Proceedings of the NinthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?03, pages39?48.
ACM.Mikhail Bilenko, Raymond Mooney, William Cohen,Pradeep Ravikumar, and Stephen Fienberg.
2003.Adaptive name matching in information integration.Intelligent Systems, 18(5):16?23.Leo Breiman.
2001.
Random forests.
Machine Learn-ing, 45(1):5?32.Peter Christen.
2006.
A comparison of personal namematching: techniques and practical issues.
In Pro-ceedings of the Workshop on Mining Complex Data(MCD06), held at IEEE ICDM06, pages 290?294.Peter Christen.
2012.
Data matching.
Springer Pub-lishing Company, Incorporated.William W. Cohen, Pradeep Ravikumar, andStephen E. Fienberg.
2003.
A comparison ofstring distance metrics for name-matching tasks.
InProceedings of IJCAI-03 Workshop on InformationIntegration, pages 73?78.Nello Cristianini and John Shawe-Taylor.
2000.
Anintroduction to Support Vector Machines: and otherkernel-based learning methods.
Cambridge Univer-sity Press.Sanmay Das.
2001.
Filters, wrappers and a boosting-based hybrid for feature selection.
In Proceedingsof the Eighteenth International Conference on Ma-chine Learning, ICML ?01, pages 74?81.
MorganKaufmann Publishers Inc.Julia Efremova, Bijan Ranjbar-Sahraei, Frans A.Oliehoek, Toon Calders, and Karl Tuyls.
2014.A baseline method for genealogical entity resolu-tion.
In Proceedings of the Workshop on PopulationReconstruction, organized in the framework of theLINKS project.Ahmed K. Elmagarmid, Panagiotis G. Ipeirotis, andVassilios S. Verykios.
2007.
Duplicate record de-tection: A survey.
Knowledge and Data Engineer-ing, IEEE Transactions on, 19(1):1?16.Robin Genuer, Jean-Michel Poggi, and ChristineTuleau-Malot.
2010.
Variable selection us-ing random forests.
Pattern Recognition Letters,31(14):2225?2236.Trevor Hastie, Robert Tibshirani, and Jerome Fried-man.
2003.
The elements of statistical learning.Springer, corrected edition.Gareth James, Daniela Witten, Trevor Hastie, andRobert Tibshirani.
2013.
An introduction to sta-tistical learning: with applications in R. SpringerPublishing Company, Incorporated.Bertrand Lisbach and Victoria Meyer.
2013.
Linguisticidentity matching.
Springer.Kees Mandemakers, Sanne Muurling, Ineke Maas,Bart Van de Putte, Richard L. Zijdeman, Paul Lam-bert, Marco H.D.
van Leeuwen, Frans van Pop-pel, and Andrew Miles.
2013.
HSN standard-ized, HISCO-coded and classified occupational ti-tles.
IISG Amsterdam.Paul McNamee and James Mayfield.
2004.
Charac-ter n-gram tokenization for european language textretrieval.
Information Retrieval, 7(1-2):73?97.Felix Naumann and Melanie Herschel.
2010.
An Intro-duction to Duplicate Detection.
Morgan and Clay-pool Publishers.Gonzalo Navarro.
2001.
A guided tour to approximatestring matching.
ACM Comput.
Surv., 33(1):31?88.Eric Sven Ristad, Peter N. Yianilos, and Senior Mem-ber.
1998.
Learning string edit distance.
IEEETransactions on Pattern Analysis and Machine In-telligence, 20:522?532.Yvan Saeys, I?naki Inza, and Pedro Larra?naga.
2007.A review of feature selection techniques in bioin-formatics.
Bioinformatics, 23(19):2507?2517,September.Sheila Tejada, Craig A. Knoblock, and Steven Minton.2002.
Learning domain-independent string transfor-mation weights for high accuracy object identifica-tion.
In Proceedings of the eighth ACM SIGKDD in-ternational conference on Knowledge discovery anddata mining, KDD ?02, pages 350?359.
ACM.Marco H. D. van Leeuwen, Ineke Maas, and AndrewMiles.
2002.
HISCO.
Historical international stan-dard classification of occupations.
Leuven Univer-sity Press.J P Verma.
2012.
Data Analysis in Management withSPSS Software.
Springer.William E. Winkler.
1995.
Matching and record link-age.
In Business Survey Methods, pages 355?384.Wiley.Pavel Zezula, Giuseppe Amato, Vlastislav Dohnal, andMichal Batko.
2006.
Similarity search: the metricspace approach.
Springer.55
