Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 880?885,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsMorphological Segmentation for Keyword SpottingKarthik Narasimhan1, Damianos Karakos2, Richard Schwartz2, Stavros Tsakalidis2,Regina Barzilay11Computer Science and Artificial Intelligence Laboratory,Massachusetts Institute of Technology2Raytheon BBN Technologies{karthikn, regina}@csail.mit.edu{dkarakos, schwartz, stavros}@bbn.comAbstractWe explore the impact of morpholog-ical segmentation on keyword spotting(KWS).
Despite potential benefits, state-of-the-art KWS systems do not use mor-phological information.
In this paper,we augment a state-of-the-art KWS sys-tem with sub-word units derived from su-pervised and unsupervised morphologicalsegmentations, and compare with phoneticand syllabic segmentations.
Our exper-iments demonstrate that morphemes im-prove overall performance of KWS sys-tems.
Syllabic units, however, rival theperformance of morphological units whenused in KWS.
By combining morphologi-cal, phonetic and syllabic segmentations,we demonstrate substantial performancegains.1 IntroductionMorphological analysis plays an increasingly im-portant role in many language processing appli-cations.
Recent research has demonstrated thatadding information about word structure increasesthe quality of translation systems and alleviatessparsity in language modeling (Chahuneau et al.,2013b; Habash, 2008; Kirchhoff et al., 2006; Stal-lard et al., 2012).In this paper, we study the impact of morpho-logical analysis on the keyword spotting (KWS)task.
The aim of KWS is to find instances of agiven keyword in a corpus of speech data.
Thetask is particularly challenging for morphologi-cally rich languages as many target keywords areunseen in the training data.
For instance, in theTurkish dataset (Babel, 2013) we use, from the2013 IARPA Babel evaluations, 36.06% of the testwords are unseen in the training data.
However,81.44% of these unseen words have a morpholog-ical variant in the training data.
Similar patternsare observed in other languages used in the Babelevaluations.
This observation strongly supportsthe use of morphological analysis to handle out-of-vocabulary (OOV) words in KWS systems.Despite this potential promise, state-of-the-artKWS systems do not commonly use morphologi-cal information.
This surprising fact can be due tomultiple reasons, ranging from the accuracy of ex-isting morphological analyzers to the challenge ofintegrating morphological information into exist-ing KWS architectures.
While using morphemesis likely to increase coverage, it makes recogni-tion harder due to the inherent ambiguity in therecognition of smaller units.
Moreover, it is notclear a priori that morphemes, which are based onthe semantics of written language, are appropriatesegmentation units for a speech-based application.We investigate the above hypotheses in thecontext of a state-of-the-art KWS architec-ture (Karakos et al., 2013).
We augment wordlattices with smaller units obtained via segmenta-tion of words, and use these modified lattices forkeyword spotting.
We consider multiple segmen-tation algorithms, ranging from near-perfect su-pervised segmentations to random segmentations,along with unsupervised segmentations and purelyphonetic and syllabic segmentations.
Our exper-iments show how sub-word units can be used ef-fectively to improve the performance of KWS sys-tems.
Further, we study the extent of impact of thesubwords, and the manner in which they can beused in KWS systems.2 Related WorkPrior research on applications of morphologicalanalyzers has focused on machine translation, lan-guage modeling and speech recognition (Habash,2008; Chahuneau et al., 2013a; Kirchhoff et al.,2006).
Morphological analysis enables us to linktogether multiple inflections of the same root,thereby alleviating word sparsity common in mor-880phologically rich languages.
This results in im-proved language model perplexity, better wordalignments and higher BLEU scores.Recent work has demonstrated that even mor-phological analyzers that use little or no supervi-sion can help improve performance in languagemodeling and machine translation (Chahuneau etal., 2013b; Stallard et al., 2012).
It has also beenshown that segmentation lattices improve the qual-ity of machine translation systems (Dyer, 2009).In this work, we leverage morphological seg-mentation to reduce OOV rates in KWS.
We in-vestigate segmentations produced by a range ofmodels, including acoustic sub-word units.
We in-corporate these subword units into a lattice frame-work within the KWS system.
We also demon-strate the value of using alternative segmentationsinstead of or in combination with morphemes.
Inaddition to improving the performance of KWSsystems, this finding may also benefit other appli-cations that currently use morphological segmen-tation for OOV reduction.3 Segmentation MethodsSupervised Morphological Segmentation Dueto the unavailability of gold morphological seg-mentations for our corpus (Babel, 2013), we usea resource-rich supervised system as a proxy.
Astraining data for this system, we use the Mor-phoChallenge 2010 corpus1which consists of1760 gold segmentations for Turkish.We consider two supervised frameworks, bothmade up of two stages.
In the first stage, com-mon to both systems, we use a FST-based mor-phological parser (C?
?oltekin, 2010) that generates aset of candidate segmentations, leveraging a largedatabase of Turkish roots and affixes.
This stagetends to overgenerate, segmenting each word ineight different ways on average.
In the next stage,we filter the resulting segmentations using one oftwo supervised filters (described below) trained onthe MorphoChallenge corpus.In the first approach, we use a binary log-linearclassifier to accept/reject each segmentation hy-pothesis.
For each word, this classifier may ac-cept multiple segmentations, or rule out all the al-ternatives.
In the second approach, to control thenumber of segmentations per word, we train a log-linear ranker that orders the segmentations for aword in decreasing order of likelihood.
In our1http://research.ics.aalto.fi/events/morphochallenge2010/Feature Examplemorpheme unigrams tak, acakmorpheme bigram ?tak, acak?phonetic seq.
unigrams t.a.k., 1v.dZ.a.k.phonetic seq.
bigram ?t.a.k., 1v.dZ.a.k.
?number of morphemes 2morpheme lengths 3, 4Table 1: Example of features used in the super-vised filters for the segmentation tak-acak.
Eachphone is followed by a dot for clarity.training corpus, each word has on average 2.5 goldsegmentations.
Hence, we choose the top two seg-mentations per word from the output of the rankerto use in our KWS system.
In both filters, weuse several features like morpheme unigrams, bi-grams, lengths, number of morphemes, and phonesequences corresponding to the morphemes.In our supervised systems, we can encode fea-tures that go beyond individual boundaries, likethe total number of morphemes in the segmenta-tion.
This global view distinguishes our classi-fier/ranker from traditional approaches that modelsegmentation as a sequence tagging task (Ruoko-lainen et al., 2013; Kudo et al., 2004; Kru-engkrai et al., 2006).
Another departure of ourapproach is the use of phonetic information, inthe form of phonetic sequences corresponding tothe morpheme unigrams and bigrams.
The hy-pothesis is that syllabic boundaries are correlatedwith morpheme boundaries to some extent.
Thephonetic sequences for words are obtained usinga publicly available Text-to-Phone (T2P) system(Lenzo, 1998).Unsupervised Morphological SegmentationWe employ a widely-used unsupervised sys-tem Morfessor (Creutz and Lagus, 2005) whichachieves state-of-the-art unsupervised perfor-mance in the MorphoChallenge evaluation.
Mor-fessor uses probabilistic generative models withsparse priors which are motivated by the MinimumDescription Length (MDL) principle.
The systemderives segmentations from raw data, without re-liance on extra linguistic sources.
It outputs a sin-gle segmentation per word.Random Segmentation As a baseline, we in-clude sub-word units from random segmentations,where we mark a segmentation boundary at eachcharacter position in a word with a fixed probabil-ity p. For comparison purposes, we consider two881Sub-word units ExampleMorphemes tak - acakRandom t - aka - c - a - kPhones t - a - k - 1v - dZ - a - kSyllables ta - k1v - dZakTable 2: Segmentations of the word takacak intodifferent types of sub-word units.types of random segmentations that match the su-pervised morphological segmentations in terms ofthe number of uniques morphemes and the averagemorpheme length, respectively.
These segmenta-tions are obtained by adjusting the segmentationprobability p appropriately.Phones and Syllables In addition to letter-based segmentation, we also consider other sub-word units that stem from word acoustics.
In par-ticular, we consider segmentation using phonesand syllables, which are available for the Babeldata we work with.Table 2 shows examples of different segmenta-tions for the Turkish word takacak.4 Keyword SpottingThe keyword spotting system used in this workfollows, to a large extent, the pipeline of (Bulykoet al., 2012).
Using standard speech recognitionmachinery, the system produces a detailed latticeof word hypotheses.
The resulting lattice is used toextract keyword hits with nominal posterior prob-ability scores.We modify this basic architecture in two ways.First, we use subwords instead of whole-words inthe decoding lexicon.
Second, we represent key-words using all possible paths in a lattice of sub-words.
For each sequence of matching arcs in thelattice, the posteriors of these arcs are multipliedtogether to form the score of detection (hit).
Apost-processing step adds up (or takes the max of)the scores of all hits of each keyword which havesignificant overlap in time.
Finally, the hit lists areprocessed by the score normalization and combi-nation method described in (Karakos et al., 2013).We use whole-word extraction for words in vo-cabulary, but rely on subword models for OOVwords.
Since we combine the hits separately forIV and OOV keywords, using subwords can onlyimprove the performance of the overall system.Language Dev Set Eval SetTurkish 403 226Assamese 158 563Bengali 176 629Haitian 107 319Lao 110 194Tamil 238 700Zulu 323 1251Table 3: Number of OOV keywords in the differ-ent Dev and Eval sets.5 Experimental SetupData The segmentation algorithms described inSection 3 are tested using the setup of the KWSsystem described in Section 4.
Our experimentsare conducted using the IARPA Babel Programlanguage collections for Turkish, Assamese, Ben-gali, Haitian, Lao, Tamil and Zulu (Babel, 2013)2.The dataset contains audio corpora and a set ofkeywords.
The training corpus for KWS consistsof 10 hours of speech, while the development andtest sets have durations of 10 and 5 hours, respec-tively.
We evaluate KWS performance over theOOV keywords in the data, which are unseen inthe training set, but appear in the development/testset.
Table 3 contains statistics on the number ofOOV keywords in the data for each language.In our experiments, we consider the pre-indexedcondition, where the keywords are known only af-ter the decoding of the speech has taken place.Evaluation Measures We consider two differ-ent evaluation metrics.
To evaluate the accuracyof the different segmentations, we compare themagainst gold segmentations from the MorphoChal-lenge data for Turkish.
This set consists of 1760words, which are manually segmented.
We usea measure of word accuracy (WordAcc), whichcaptures the accuracy of all segmentation deci-sions within the word.
If one of the segmenta-tion boundaries is wrong in a proposed segmen-tation, then that segmentation does not contributetowards the WordAcc score.
We use 10-fold cross-validation for the supervised segmentations, whilewe use the entire set for unsupervised and acousticcases.We evaluate the performance of our KWS sys-tem using a widely used metric in KWS, the Ac-2We perform the experiments with supervised segmenta-tion only on Turkish, due to the lack of gold morphologicaldata for the other languages.882tual Term Weighted Value (ATWV) measure, asdescribed in (Fiscus et al., 2007).
This measureuses a combination of penalties for misses andfalse positives to score the system.
The maximumscore achievable is 1.0, if there are no misses andfalse positives, while the score can be lower than0.0 if there are a lot of misses or false positives.6 ResultsTable 4 summarizes the performance of all con-sidered segmentation systems in the KWS task onTurkish.
The quality of the segmentations com-pared to the gold standard is also shown.
Table 5shows the OOV ATWV performance on the sixother languages, used in the second year of theIARPA Babel project.
We summarize below ourconclusions based on these results.Using sub-word units improves overall KWSperformance If we use a word-based KWS sys-tem, the ATWV score will be 0.0 since the OOVkeywords are not present in the lexicon.
En-riching our KWS system with sub-word segmentsyields performance gains for all the segmentationmethods, including random segmentations.
How-ever, the observed gain exhibits significant vari-ance across the segmentation methods.
For in-stance, the gap between the performance of theKWS system using the best supervised classifier-based segmenter (CP) and that using the unsuper-vised segmenter (U) is 0.059, which correspondsto a 43.7% in relative gain.
Table 4 also shows thatwhile methods with shorter sub-units (U, P) yieldlower OOV rate, they do not necessarily fare betterin the KWS evaluation.Syllabic units rival the performance of mor-phological units A surprising discovery from ourexperiments is the good performance of the syl-labic segmentation-based KWS system (S).
It out-performs all the alternative segmentations on thetest set, and ranks second on the development setbehind the CP system.
These units are particularlyattractive as they can easily be computed fromacoustic input and do not require any prior linguis-tic knowledge.
We hypothesize that the granular-ity of this segmentation is crucial to its success.For instance, a finer-grained phone-based segmen-tation (P) performs substantially worse than othersegmentation algorithms as the derived sub-unitsare shorter and hence, harder to recognize.Improving morphological accuracy beyond acertain level does not translate into improvedKWS performance We observe that the segmen-tation accuracy and KWS performance are notpositively correlated.
Clearly, bad segmentationstranslate into poor ATWV scores, as in the caseof random and unsupervised segmentations.
How-ever, gains on segmentation accuracy do not al-ways result in better KWS performance.
For in-stance, the ranker systems (RP, RNP) have betteraccuracies on MC2010, while the classifier sys-tems (CP, CNP) perform better on the KWS task.This discrepancy in performance suggests that fur-ther gains can be obtained by optimizing segmen-tations directly with respect to KWS metrics.Adding phonetic information improves mor-phological segmentation For all the morpholog-ical systems, adding phonetic information resultsin consistent performance gains.
For instance,it increases segmentation accuracy by 4% whenadded to the classifier (CNP and CP in table 4).The phonetic information used in our experimentsis computed automatically using a T2P system(Lenzo, 1998), and can be easily obtained for arange of languages.
This finding sheds new lighton the relation between phonetic and morphologi-cal systems, and can be beneficial for morpholog-ical analyzers developed for other applications.Combining morphological, phonetic and syl-labic segmentations gives better results than ei-ther in isolation As table 4 shows, the best KWSresults are achieved when syllabic and morphemicsystems are combined.
The best combination sys-tem (CP+P+S) outperforms the best individualsystem (S) by 5.5%.
This result suggests that mor-phemic, phonemic and syllabic segmentations en-code complementary information which benefitsKWS systems in handling OOV keywords.Morphological segmentation helps KWSacross different languages Table 5 demonstratesthat we can obtain gains in KWS performanceacross different languages using unsupervised seg-mentation.
The improvement is significant in 3 ofthe 6 languages - as high as 3.2% for Assameseand Bengali, and 2.7% for Tamil (absolute per-centages).
As such, the results of Table 2 can-not be directly compared to those of Table 1 sincethe system architecture is slightly different3.
How-3The keyword spotting pipeline is based on the one usedby the Babelon team in the 2014 NIST evaluation (Tsakalidis,2014).
The pipeline was much more involved than the one de-scribed for Turkish; multiple search methods (with/withoutfuzzy search) and data structures (lattices, confusion net-works and generalized versions of these) were all used incombination (Karakos and Schwartz, 2014).
The recognition883MethodUniqueunitsAvg.
unitlengthReductionin OOV (abs)WordAccDevATWVTestATWVPhone-based (P) 51 1 36.06% 0.06% 0.099 0.164Syllable-based (S) 2.1k 3.62 23.91% 10.29% 0.127 0.201Classifier w/ phone info (CP) 18.5k 6.39 18.20% 80.41% 0.146 0.194Classifier w/o phone info (CNP) 19k 6.42 21.50% 75.66% 0.133 0.181Ranker w/ phone info (RP) 10k 5.62 16.86% 86.03% 0.104 0.153Ranker w/o phone info (RNP) 10k 5.71 16.44% 84.19% 0.109 0.159Unsupervised (U) 2.4k 5.44 22.45% 39.57% 0.080 0.135RANDLen-Classifier 11.7k 6.39 0.73% 5.11% 0.061 0.086RANDNum-Classifier 18.2k 3.03 8.56% 3.69% 0.111 0.154RANDLen-Ranker 11.6k 5.62 1.94% 5.79% 0.072 0.136RANDNum-Ranker 11.7k 6.13 1.15% 5.34% 0.081 0.116CP + P - - - - 0.190 0.246RP + P - - - - 0.150 0.210CP + P + S - - - - 0.208 0.257RP + P + S - - - - 0.186 0.249Word-based for IV words - - - - 0.385 0.400Table 4: Segmentation Statistics and ATWV scores on Babel Turkish data along with WordAcc onMorphoChallenge 2010 data.
All rows except the last are for OOV words.
Absolute reduction is from aninitial OOV of 36.06%.
Higher ATWV scores are better.
Best system scores are shown in bold.Assamese Bengali Haitian Lao Tamil ZuluDev Test Dev Test Dev Test Dev Test Dev Test Dev TestP + S 0.213 0.230 0.277 0.296 0.371 0.342 0.228 0.139 0.349 0.267 0.279 0.215P + S + U 0.214 0.263 0.294 0.328 0.393 0.342 0.237 0.146 0.395 0.284 0.275 0.218Table 5: ATWV scores for languages used in the second year of the IARPA Babel project, using twoKWS systems: Phone + Syllable (P+S) and Phone + Syllable + Unsupervised Morphemes (P+S+U).Bold numbers show significant performance gains obtained by adding morphemes to the system.ever, they are indicative of the large gains (1.5%,on average, over the six languages) that can be ob-tained through unsupervised morphology, on topof a very good combined phonetic/syllabic system.7 ConclusionWe explore the extent of impact of morphologicalsegmentation on keyword spotting (KWS).
To in-vestigate this issue, we augmented a KWS systemwith sub-word units derived by multiple segmen-tation algorithms.
Our experiments demonstratethat morphemes improve the overall performanceof KWS systems.
Syllabic units, however, rival theperformance of morphemes in the KWS task.
Fur-thermore, we demonstrate that substantial perfor-mance gains in KWS performance are obtained bycombining morphological, phonetic and syllabicwas done with audio features supplied by BUT (Karafi?at etal., 2014), which were improved versions of those used forTurkish.segmentations.
Finally, we also show that addingphonetic information improves the quality of mor-phological segmentation.AcknowledgementsThis work was supported by the IntelligenceAdvanced Research Projects Activity (IARPA)via Department of Defense US Army ResearchLaboratory contract number W911NF-12-C-0013.The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposesnotwithstanding any copyright annotation thereon.Disclaimer: The views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the officialpolicies or endorsements, either expressed or im-plied, of IARPA, DoD/ARL, or the U.S. Govern-ment.
We thank the MIT NLP group and theEMNLP reviewers for their comments and sugges-tions.884ReferencesIARPA Babel.
2013.
Language collection re-leases; Turkish: IARPA-babel105b-v0.4, As-samese: IARPA-babel102b-v0.5a, Bengali: IARPA-babel103b-0.4b, Haitian Creole: IARPA-babel201b-v0.2b, Lao: IARPA-babel203b-v3.1a, Tamil:IARPA-babel204b-v1.1b, Zulu: IARPA-babel206b-v0.1e.Ivan Bulyko, Owen Kimball, Man-Hung Siu, Jos?e Her-rero, and Dan Blum.
2012.
Detection of un-seen words in conversational Mandarin.
In Proc.
ofICASSP, Kyoto, Japan, Mar.Victor Chahuneau, Eva Schlinger, Noah A. Smith, andChris Dyer.
2013a.
Translating into morpholog-ically rich languages with synthetic phrases.
InEMNLP, pages 1677?1687.
ACL.Victor Chahuneau, Noah A. Smith, and Chris Dyer.2013b.
Knowledge-rich morphological priors forbayesian language models.
In HLT-NAACL, pages1206?1215.
The Association for Computational Lin-guistics.C?a?gr?
C??oltekin.
2010.
A freely available morpho-logical analyzer for Turkish.
In Proceedings ofthe 7th International conference on Language Re-sources and Evaluation (LREC2010), pages 820?827.Mathias Creutz and Krista Lagus.
2005.
Inducing themorphological lexicon of a natural language fromunannotated text.
In Proceedings of the Interna-tional and Interdisciplinary Conference on AdaptiveKnowledge Representation and Reasoning (AKRR),pages 106?113.Chris Dyer.
2009.
Using a maximum entropy modelto build segmentation lattices for MT.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguis-tics, NAACL ?09, pages 406?414, Stroudsburg, PA,USA.
Association for Computational Linguistics.Jonathan G. Fiscus, Jerome Ajot, John S. Garofolo, andGeorge Doddington.
2007.
Results of the 2006spoken term detection evaluation.
In Workshop onSearching Spontaneous Conversational Speech.Nizar Habash.
2008.
Four techniques for online han-dling of out-of-vocabulary words in arabic-englishstatistical machine translation.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics on Human Language Technolo-gies: Short Papers, HLT-Short ?08, pages 57?60,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Martin Karafi?at, Franti?sek Gr?ezl, Mirko Hanne-mann, Karel Vesel?y, Igor Szoke, and Jan ?Honza??Cernock?y.
2014.
BUT 2014 Babel system: Anal-ysis of adaptation in NN based systems.
In Pro-ceedings of Interspeech 2014, Singapore, Septem-ber.
IEEE.Damianos Karakos and Richard Schwartz.
2014.
Sub-word modeling.
In IARPA Babel PI Meeting, July.Damianos Karakos, Richard Schwartz, Stavros Tsaka-lidis, Le Zhang, Shivesh Ranjan, Tim Ng, RogerHsiao, Guruprasad Saikumar, Ivan Bulyko, LongNguyen, John Makhoul, Frantisek Grezl, MirkoHannemann, Martin Karafiat, Igor Szoke, KarelVesely, Lori Lamel, and Viet-Bac Le.
2013.
Scorenormalization and system combination for improvedkeyword spotting.
In Proc.
ASRU 2013, Olomouc,Czech Republic.Katrin Kirchhoff, Dimitra Vergyri, Jeff Bilmes, KevinDuh, and Andreas Stolcke.
2006.
Morphology-based language modeling for conversational arabicspeech recognition.
Computer Speech and Lan-guage, 20(4):589?608.Canasai Kruengkrai, Virach Sornlertlamvanich, andHitoshi Isahara.
2006.
A conditional random fieldframework for Thai morphological analysis.
InLREC.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields toJapanese morphological analysis.
In In Proc.
ofEMNLP, pages 230?237.Kevin Lenzo.
1998.
Text-to-phoneme converterbuilder.
http://www.cs.cmu.edu/afs/cs.cmu.edu/user/lenzo/html/areas/t2p/.Accessed: 2014-03-11.Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,and Mikko Kurimo.
2013.
Supervised morpholog-ical segmentation in a low-resource learning settingusing conditional random fields.
In Proceedings ofthe Seventeenth Conference on Computational Nat-ural Language Learning, pages 29?37, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.David Stallard, Jacob Devlin, Michael Kayser,Yoong Keok Lee, and Regina Barzilay.
2012.
Unsu-pervised morphology rivals supervised morphologyfor Arabic MT.
In ACL (2), pages 322?327.
TheAssociation for Computer Linguistics.Stavros Tsakalidis.
2014.
The Babelon OpenKWS14systems.
In IARPA Babel PI Meeting, July.885
