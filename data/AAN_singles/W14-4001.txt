Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1?10,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsVector Space Models for Phrase-based Machine TranslationTamer Alkhouli1, Andreas Guta1, and Hermann Ney1,21Human Language Technology and Pattern Recognition GroupRWTH Aachen University, Aachen, Germany2Spoken Language Processing GroupUniv.
Paris-Sud, France and LIMSI/CNRS, Orsay, France{surname}@cs.rwth-aachen.deAbstractThis paper investigates the applicationof vector space models (VSMs) to thestandard phrase-based machine translationpipeline.
VSMs are models based oncontinuous word representations embed-ded in a vector space.
We exploit wordvectors to augment the phrase table withnew inferred phrase pairs.
This helpsreduce out-of-vocabulary (OOV) words.In addition, we present a simple way tolearn bilingually-constrained phrase vec-tors.
The phrase vectors are then used toprovide additional scoring of phrase pairs,which fits into the standard log-linearframework of phrase-based statistical ma-chine translation.
Both methods resultin significant improvements over a com-petitive in-domain baseline applied to theArabic-to-English task of IWSLT 2013.1 IntroductionCategorical word representation has been widelyused in many natural language processing (NLP)applications including statistical machine transla-tion (SMT), where words are treated as discreterandom variables.
Continuous word representa-tions, on the other hand, have been applied suc-cessfully in many NLP areas (Manning et al.,2008; Collobert and Weston, 2008).
However,their application to machine translation is still anopen research question.
Several works tried to ad-dress the question recently (Mikolov et al., 2013b;Zhang et al., 2014; Zou et al., 2013), and this workis but another step in that direction.While categorical representations do not encodeany information about word identities, continuousrepresentations embed words in a vector space, re-sulting in geometric arrangements that reflect in-formation about the represented words.
Such em-beddings open the potential for applying informa-tion retrieval approaches where it becomes possi-ble to define and compute similarity between dif-ferent words.
We focus on continuous represen-tations whose training is influenced by the sur-rounding context of the token being represented.One motivation for such representations is to cap-ture word semantics (Turney et al., 2010).
Thisis based on the distributional hypothesis (Harris,1954) which says that words that occur in similarcontexts tend to have similar meanings.We make use of continuous vectors learnedusing simple neural networks.
Neural networkshave been gaining increasing attention recently,where they have been able to enhance strong SMTbaselines (Devlin et al., 2014; Sundermeyer etal., 2014).
While neural language and transla-tion modeling make intermediate use of continu-ous representations, there have been also attemptsat explicit learning of continuous representationsto improve translation (Zhang et al., 2014; Gao etal., 2013).This work explores the potential of word se-mantics based on continuous vector representa-tions to enhance the performance of phrase-basedmachine translation.
We present a greedy algo-rithm that employs the phrase table to identifyphrases in a training corpus.
The phrase tableserves to bilingually restrict the phrases spottedin the monolingual corpus.
The algorithm is ap-plied separately to the source and target sides ofthe training data, resulting in source and target cor-pora of phrases (instead of words).
The phrasecorpus is used to learn phrase vectors using thesame methods that produce word vectors.
Thevectors are then used to provide semantic scor-ing of phrase pairs.
We also learn word vectorsand employ them to augment the phrase table withparaphrased entries.
This leads to a reduction in1the OOV rate which translates to improved BLEUand and TER scores.
We apply the two methods onthe IWSLT 2013 Arabic-to-English task and showsignificant improvements over a strong in-domainbaseline.The rest of the paper is structured as follows.Section 2 presents a background on word andphrase vectors.
The construction of the phrasecorpus is discussed in Section 3, while Section 4demonstrates how to use word and phrase vectorsin the standard phrase-based SMT pipeline.
Ex-periments are presented in Section 5, followed byan overview of the related word in Section 6, andfinally Section 7 concludes the work.2 Vector Space ModelsOne way to obtain context-based word vectors isthrough a neural network (Bengio et al., 2003;Schwenk, 2007).
With a vocabulary size V , one-hot encoding of V -dimensional vectors is used torepresent input words, effectively associating eachword with a D-dimensional vector in the V ?Dinput weight matrix, where D is the size of thehidden layer.
Similarly, one-hot encoding on theoutput layer associates words with vectors in theoutput weight matrix.Alternatively, a count-based V-dimensionalword co-occurrence vector can serve as a wordrepresentation (Lund and Burgess, 1996; Lan-dauer and Dumais, 1997).
Such representationsare sparse and high-dimensional, which might re-quire an additional dimensionality reduction step(e.g.
using SVD).
In contrast, learning word rep-resentations via neural models results directly inrelatively low-dimensional, dense vectors.
In thiswork, we follow the neural network approach toextract the feature vectors.
Whether word vectorsare extracted by means of a neural network or co-occurrence counts, the context surrounding a wordinfluences its final representation by design.
Suchcontext-based representations can be used to de-termine semantic similarities.The construction of phrase representations, onthe other hand, can be done in different ways.The compositional approach constructs the vectorrepresentation of a phrase by resorting to its con-stituent words (or sub-phrases) (Gao et al., 2013;Chen et al., 2010).
Kalchbrenner and Blunsom(2013) obtain continuous sentence representationsby applying a sequence of convolutions, startingwith word representations.Another approach for phrase representationconsiders phrases as atomic units that can not bedivided further.
The representations are learneddirectly in this case (Mikolov et al., 2013b; Hu etal., 2014).In this work, we follow the second approach toobtain phrase vectors.
To this end, we apply thesame methods that yield word vectors, with thedifference that phrases are used instead of words.In the case of neural word representations, a neuralnetwork that is presented with words at the inputlayer is presented with phrases instead.
The result-ing vocabulary size in this case would be the num-ber of distinct phrases observed during training.Although learning phrase embeddings directly isamenable to data sparsity issues, it provides uswith a simple means to build phrase vectors mak-ing use of tools already developed for word vec-tors, focussing the effort on preprocessing the dataas will be discussed in the next section.3 Phrase CorpusWhen training word vectors using neural net-works, the network is presented with a corpus.To build phrase vectors, we first identify phrasesin the corpus and generate a phrase corpus.
Thephrase corpus is similar to the original corpus ex-cept that its words are joined to make up phrases.The new corpus is then used to train the neural net-work.
The columns of the resulting input weightmatrix of the network are the phrase vectors corre-sponding to the phrases encountered during train-ing.Mikolov et al.
(2013b) identify phrases using amonolingual point-wise mutual information crite-rion with discounting.
Since our end goal is togenerate phrase vectors that are helpful for trans-lation, we follow a different approach: we con-strain the phrases by the conventional phrase tableof phrase-based machine translation.
This is doneby limiting the phrases identified in the corpus tohigh quality phrases occurring in the phrase table.The quality is determined using bilingual scoresof phrase pairs.
While the phrase vectors of a lan-guage are eventually obtained by training the neu-ral network on the monolingual phrase corpus ofthat language, the reliance on bilingual scores to2Algorithm 1 Phrase Corpus Construction1: p?
12: for p?
numPasses do3: i?
24: for i?
corpus.size?1 do5: w??
join(ti, ti+1) .
create a phrase using the current and next tokens6: v??
join(ti?1, ti) .
create a phrase using the previous and current tokens7: joinForward?
score(w?
)8: joinBackward?
score(v?
)9: if joinForward ?
joinBackward and joinForward ?
?
then10: ti?
w?11: remove ti+112: i?
i+2 .
newly created phrase not available for further merge during current pass13: else14: if joinBackward > joinForward and joinBackward ?
?
then15: ti?1?
v?16: remove ti17: i?
i+2 .
newly created phrase not available for further merge during current pass18: else19: i?
i+120: end if21: end if22: end for23: p?
p+124: end forconstruct the monolingual phrase corpus encodesbilingual information in the corpus, namely, thecorpus will include phrases that having a match-ing phrase in the other language, which is in linewith the purpose for which the phrases are con-structed, that is, their use in the phrase-based ma-chine translation pipeline which is explained in thenext section.
In addition, the aforementioned scor-ing serves to exclude noisy phrase-pair entries dur-ing the construction of the phrase corpus.
Next, weexplain the details of the construction algorithm.3.1 Phrase SpottingWe propose Algorithm 1 as a greedy approach forphrase corpus construction.
It is a multi-pass algo-rithm where each pass can extend tokens obtainedduring the previous pass by a single token at most.Before the first pass, all tokens are words.
Duringthe passes the tokens might remain as words or canbe extended to become phrases.
Given a token tiat position i, a scoring function is used to scorethe phrase (ti, ti+1) and the phrase (ti?1, ti).
Thephrase having a higher score is adopted as long asits score exceeds a predefined threshold ?
.
Thescoring function used in lines 7 and 8 is based onthe phrase table.
If the phrase does not belong tothe phrase table it is given a score ?
?< ?
.
If thephrase exists, a bilingual score is computed usingthe phrase table fields as follows:score(?f ) = maxe?
{L?i=1wigi(?f , e?
)}(1)where gi(?f , e?)
is the ith feature of the bilingualphrase pair (?f , e?).
The maximization is carried outover all phrases e?
of the other language.
The scoreis the weighted sum of the phrase pair features.Throughout our experiments, we use 2 phrasal and2 lexical features for scoring, with manual tuningof the weights wi.The resulting corpus is then used to train phrasevectors following the same procedure of trainingword vectors.4 End-to-end TranslationIn this section we will show how to employ phrasevectors in the phrase-based statistical machinetranslation pipeline.34.1 Phrase-based Machine TranslationThe phrase-based decoder consists of a search us-ing a log-linear framework (Och and Ney, 2002)as follows:e?
?I1= argmaxI,eI1{maxK,sK1M?m=1?mhm(eI1,sK1, fJ1)}(2)where eI1= e1...eIis the target sentence, fJ1=f1... fJis the source sentence, sK1= s1...sKisthe hidden alignment or derivation.
The mod-els hm(eI1,sK1, fJ1) are weighted by the weights ?mwhich are tuned using minimum error rate train-ing (MERT) (Och, 2003).
The rest of the sectionpresents two ways to integrate vector representa-tions into the system described above.4.2 Semantic Phrase FeatureWords that occur in similar contexts tend to havesimilar meanings.
This idea is known as the dis-tributional hypothesis (Harris, 1954), and it moti-vates the use of word context to learn word repre-sentations that capture word semantics (Turney etal., 2010).
Extending this notion to phrases, phrasevectors that are learned based on the surroundingcontext encode phrase semantics.
Since we willuse phrase vectors to compute a feature of a phrasepair in the following, we refer to the feature as asemantic phrase feature.Given a phrase pair (?f , e?
), we can use the phrasevectors of the source and target phrases to computea semantic phrase feature as follows:hM+1(?f , e?)
= sim(Wx?f,ze?)
(3)where sim is a similarity function, x?fand ze?are theS-dimensional source and T -dimensional targetvectors respectively corresponding to the sourcephrase?f and target phrase e?.
W is an S?T linearprojection matrix that maps the source space to thetarget space (Mikolov et al., 2013a).
The matrixis estimated by optimizing the following criterionwith stochastic gradient descent:minWN?i=1||Wxi?
zi||2(4)where the training data consists of the pairs{(x1,z1), ...,(xN,zN)} corresponding to the sourceand target vectors.Since the source and target phrase vectors arelearned separately, we do not have an immedi-ate mapping between them.
As such mapping isneeded for the training of the projection matrix,we resort to the phrase table to obtain it.
A sourceand a target phrase vectors are paired if there is acorresponding phrase pair entry in the phrase tablewhose score exceeds a certain threshold.
Scoringis computed using Eq.
1.
Similarly, word vectorsare paired using IBM 1 p(e| f ) and p( f |e) lexica.Noisy entries are assumed to have a probabilityless than a certain threshold and are not used topair word vectors.4.3 ParaphrasingWhile the standard phrase table is extracted usingparallel training data, we propose to extend it andinfer new entries relying on continuous representa-tions.
With a similarity measure (e.g.
cosine sim-ilarity) that computes the similarity between twophrases, a new phrase pair can be generated by re-placing either or both of its constituent phrases bysimilar phrases.
The new phrase is referred to as aparaphrase of the phrase it replaces.
This enablesa richer use of the bilingual data, as a source para-phrase can be borrowed from a sentence that is notaligned to a sentence containing the target side ofthe phrase pair.
It also enables the use of monolin-gual data, as the source and target paraphrases donot have to occur in the parallel data.
The cross-interaction between sentences in the parallel dataand the inclusion of the monolingual data to ex-tend the phrase table are potentially capable of re-ducing the out-of-vocabulary (OOV) rate.In order to generate a new phrase rule, we en-sure that noisy rules do not contribute to the gener-ation process, depending on the score of the phrasepair (cf.
Eq.
1).
High scoring entries are para-phrased as follows.
To paraphrase the source side,we perform a k-nearest neighbor search over thesource phrase vectors.
The top-k similar entriesare considered paraphrases of the given phrase.The same can be done for the target side.
We as-sign the newly generated phrase pair the same fea-ture values of the pair used to induce it.
However,two extra phrase features are added: one measur-ing the similarity between the source phrase andits paraphrase, and another for the target phraseand its paraphrase.
The new feature values forthe original non-paraphrased entries are set to the4highest similarity value.We focus on a certain setting that avoids in-terference with original phrase rules, by extend-ing the phrase table to cover OOVs only.
Thatis, source-side paraphrasing is performed only ifthe source paraphrase does not already occur inthe phrase table.
This ensures that original entriesare not interfered with and only OOVs are affectedduring translation.
Reducing OOVs by extendingthe phrase table has the advantage of exploitingthe full decoding capabilities (e.g.
LM scoring),as opposed to post-decoding translation of OOVs,which would not exhibit any decoding benefits.The k-nearest neighbor (k-NN) approach iscomputationally prohibitive for large phrase tablesand large number of vectors.
This can be allevi-ated by resorting to approximate k-NN search (e.g.locality sensitive hashing).
Note that this searchis performed during training time to generate ad-ditional phrase table entries, and does not affectdecoding time, except through the increase of thephrase table size.
In our experiments, the train-ing time using exact k-NN search was acceptable,therefore no search approximations were made.5 ExperimentsIn the following we first provide an analysis of theword vectors that are later used for translation ex-periments.
We use word vectors (as opposed tophrase vectors) for phrase table paraphrasing toreduce the OOV rate.
Next, we present end-to-end translation results using the proposed seman-tic feature and our OOV reduction method.The experiments are based on vectors trainedusing the word2vec1toolkit, setting vector dimen-sionality to 800 for Arabic and 200 for Englishvectors.
We used the skip-gram model with a max-imum skip length of 10.
The phrase corpus wasconstructed using 5 passes, with scores computedaccording to Eq.
1 using 2 phrasal and 2 lexicalfeatures.
The phrasal and lexical weights were setto 1 and 0.5 respectively, with all features beingnegative log-probabilities, and the scoring thresh-old ?
was set to 10.
All translation experimentsare performed with the Jane toolkit (Vilar et al.,2010; Wuebker et al., 2012).1https://code.google.com/p/word2vec/5.1 Baseline SystemOur phrase-based baseline system consists of twophrasal and two lexical translation models, trainedusing a word-aligned bilingual training corpus.Word alignment is automatically generated byGIZA++(Och and Ney, 2003) given a sentence-aligned bilingual corpus.
We also include bi-nary count features and bidirectional hierarchicalreordering models (Galley and Manning, 2008),with three orientation classes per direction result-ing in six reordering models.
The baseline also in-cludes word penalty, phrase penalty and a simpledistance-based distortion model.The language model (LM) is a 4-gram mix-ture LM trained on several data sets using mod-ified Kneser-Ney discounting with interpolation,and combined with weights tuned to achieve thelowest perplexity on a development set using theSRILM toolkit (Stolcke, 2002).
Data selectionis performed using cross-entropy filtering (Mooreand Lewis, 2010).5.2 Word VectorsHere we analyze the quality of word vectors usedin the OOV reduction experiments.
The vectorsare trained using an unaltered word corpus.
Webuild a lexicon using source and target word vec-tors together with the projection matrix using thesimilarity score sim(Wxf,ze)), where the projec-tion matrix W is used to project the source wordvector xf, corresponding to the source word f , tothe target vector space.
The similarity between theprojection result Wxfand the target word vectorzeis computed.
In the following we will refer tothese scores computed using vector representationas VSM-based scores.The resulting lexicon is compared to the IBM1 lexicon2.
Given a source word, we select thethe best target word according to the VSM-basedscore.
This is compared to the best translationbased on the IBM 1 probability.
If both transla-tions coincide, we refer to this as a 1-best match.We also check whether the best translation accord-ing to IBM 1 matches any of the top-5 translationsbased on the VSM model.
A match in this case isreferred to as a 5-best match.2We assume for the purpose of this experiment that theIBM 1 lexicon provides perfect translations, which is not nec-essarily the case in practice.5corpus Lang.
# tokens # segmentsWIT Ar 3,185,357 147,256UN Ar 228,302,244 7,884,752arGiga3 Ar 782,638,101 27,190,387WIT En 2,951,851 147,256UN En 226,280,918 7,884,752news En 1,129,871,814 45,240,651Table 1: Arabic and English corpora statistics.The vectors are trained on a mixture of in-domain data (WIT) which correspond to TEDtalks, and out-of-domain data (UN).
These sets areprovided as part of the IWSLT 2013 evaluationcampaign.
We include the LDC2007T40 ArabicGigaword v3 (arGiga3) and English news crawl ar-ticles (2007 through 2012) to experiment with theeffect of increasing the size of the training corpuson the quality of the word vectors.
Table 1 showsthe corpora statistics obtained after preprocessing.The fractions of the 1- and 5-best matches areshown in table 2.
The table is split into two halves.The upper part investigates the effect of increasingthe amount of Arabic data while keeping the En-glish data fixed (2nd row), the effect of increasingthe amount of the English data while keeping theArabic data fixed (3rd row), and the effect of usingmore data on both sides (4th row).
The projectionis done on the representation of the Arabic word f ,and the similarity is computed between the projec-tion and the representation of the English word e.In the lower half of the table, the same effects areexplored, except that the projection is performedon the English side instead.
The results indicatethat the accuracy increases when increasing theamount of data only on the side being projected.More data on the corresponding side (i.e.
the sidebeing projected to) decreases the accuracy.
Thesame behavior is observed whether the projectedside is Arabic (upper half) or English (lower half).All in all, the accuracy values are low.
The accu-racy increases about three times when looking atthe 5-best instead of the 1-best accuracy.
While theaccuracies 32.2% and 33.1% are low, they reflectthat the word representations are encoding someinformation about the words, although this infor-mation might not be good enough to build a word-to-word lexicon.
However, using this informationfor OOV reduction might still yield improvementsas we will see in the translation results.Arabic Englishword corpus size 231M 229Mphrase corpus size 126M 115Mword corpus vocab.
size 467K 421Kphrase corpus vocab.
size 5.8M 5.3M# phrase vectors 934K 913KTable 3: Phrase vectors statistics.5.3 Phrase VectorsTranslation experiments pertaining to the pro-posed semantic feature are presented here.
Thefeature is based on phrase vectors which are builtwith the word2vec toolkit in a similar way wordvectors are trained, except that the training cor-pus is the phrase corpus containing phrases con-structed as described in section 3.
Once trained, anew feature is added to the phrase table.
The fea-ture is computed for each phrase pair using phrasevectors as described in Eq.
3.Table 3 shows statistics about the phrase corpusand the original word corpus it is based on.
Al-gorithm 1 is used to build the phrase corpus using5 passes.
The number of phrase vectors trainedusing the phrase corpus are also shown.
Note thatthe tool used does not produce vectors for all 5.8MArabic and 5.3M English phrases in the vocab-ulary.
Rather, noisy phrases are excluded fromtraining, eventually leading to 934K Arabic and913K English phrase embeddings.We perform two experiments on the IWSLT2013 Arabic-to-English evaluation data set.
In thefirst experiment, we examine how the semanticfeature affects a small phrase table (2.3M phrasepairs) trained on the in-domain data (WIT).
Thesecond experiment deals with a larger phrase table(34M phrase pairs), constructed by a linear inter-polation between in- and out-of-domain phrase ta-bles including UN data, resulting in a competitivebaseline.
The two baselines have hierarchical re-ordering models (HRMs) and a tuned mixture LM,in addition to the standard models, as described insection 5.1.
The results are shown in table 4.In the small experiment, the semantic phrasefeature improves TER by 0.7%, and BLEU by0.4% on the test set eval13.
The translation seemsto benefit from the contextual information en-coded in the phrase vectors during training.
Thisis in contrast to the training of the standard phrase6Arabic Data English Data 1-bestMatch %5-bestMatches %WIT+UN WIT+UN 8.0 26.1WIT+UN+arGiga3 WIT+UN 10.9 32.2WIT+UN WIT+UN+news 4.9 17.9WIT+UN+arGiga3 WIT+UN+news 7.5 25.7WIT+UN WIT+UN 8.4 27.2WIT+UN WIT+UN+news 10.9 33.1WIT+UN+arGiga3 WIT+UN 5.7 18.9WIT+UN+arGiga3 WIT+UN+news 8.3 25.2Table 2: The effect of increasing the amount of data on the quality of word vectors.
VSM-based scores arecompared to IBM model 1 p(e| f ) (upper half) and p( f |e) (lower half), effectively regarding the IBM 1models as the true probability distributions.
In the upper part, the projection is done on the representationof the Arabic word f , and the similarity is computed between the projection and the representation of theEnglish word e. In the lower half of the table, the role of f and e is interchanged, where the English sidein this case will be projected.system dev2010 eval2013BLEU TER BLEU TERWIT 29.1 50.5 28.9 52.5+ feature 29.1 ?50.1 ?29.3 ?51.8+ paraph.
29.2 ?50.2 ?29.5 ?51.8+ both 29.2 50.2 ?29.4 ?51.8WIT+UN 29.7 49.3 30.5 50.5+ feature 29.8 49.2 30.2 50.7Table 4: Semantic feature and paraphrasing re-sults.
The symbol ?
indicates statistical signifi-cance with p < 0.01.features, which disregards context.
As for the hi-erarchical reordering models which are part of thebaseline, they do not capture lexical informationabout the context.
They are only limited to the or-dering information.
The skip-gram-based phrasevectors used for the semantic feature, on the otherhand, discard ordering information, but uses con-textual lexical information for phrase representa-tion.
In this sense, HRMs and the semantic featurecan be said to complement each other.
Using thesemantic feature for the large phrase table did notyield improvements.
The difference compared tothe baseline in this case is not statistically signifi-cant.All reported results are averages of 3 MERT op-timizer runs.
Statistical significance is computedusing the Approximate Randomization (AR) test.We used the multeval toolkit (Clark et al., 2011)for evaluation.5.4 Paraphrasing and OOV ReductionThe next set of experiments investigates the re-duction of the OOV rate through paraphrasing,and its impact on translation.
Paraphrasing is per-formed employing the cosine similarity, and the k-NN search is done on the source side, with k = 3.The nearest neighbors are required to satisfy a ra-dius threshold r > 0.3, i.e., neighbors with a simi-larity value less or equal to r are rejected.
Trainingthe projection matrices is performed using a smallamount of training data amounting to less than 30ktranslation pairs.To examine the effect of OOV reduction, weperform paraphrasing on a resource-limited sys-tem, where a small amount of parallel data ex-ists, but a larger amount of monolingual data isavailable.
Such a system is simulated by train-ing word vectors on the WIT+UN data monolin-gually , while extracting the phrase table using themuch smaller in-domain WIT data set only.
Table5 shows the change in the number of OOV wordsafter introducing the paraphrased rules to the WIT-based phrase table.
19% and 30% of the originalOOVs are eliminated in the dev and eval13 sets,respectively.
This reduction translates to an im-provement of 0.6% BLEU and 0.7% TER as indi-cated in table 4.Since BLEU or TER are based on word iden-tities and do not detect semantic similarities, wemake a comparison between the reference transla-tions and translations of the system that employed7# OOVphrase table dev eval13WIT 185 254WIT+paraph.
150 183Vocab.
size 3,714 4,734Table 5: OOV change due to paraphrasing.
Vocab-ulary refers to the number of unique tokens in theArabic dev and test sets.OOV VSM-basedTranslationReferenceI??
?K found unfolded?
?Qk interested keen?
?m.?jail imprisoned?CK.claim report??.J??
confusing confoundingIJk encourage rallied forAK?Q?
villagers redneckTable 6: Examples of OOV words that were trans-lated due to paraphrasing.
The examples areextracted from the translation hypotheses of thesmall experiment.OOV reduction.
Examples are shown in Table 6.Although the reference words are not matched ex-actly, the VSM translations are semantically closeto them, suggesting that OOV reduction in thesecases was somewhat successful, although not re-warded by either of the scoring measures used.6 Related WorkBilingually-constrained phrase embeddings weredeveloped in (Zhang et al., 2014).
Initial embed-dings were trained in an unsupervised manner, fol-lowed by fine-tuning using bilingual knowledge tominimize the semantic distance between transla-tion equivalents, and maximizing the distance be-tween non-translation pairs.
The embeddings arelearned using recursive neural networks by de-composing phrases to their constituents.
Whileour work includes bilingual constraints to learnphrase vectors, the constraints are implicit in thephrase corpus.
Our approach is simple, focusingon the preprocessing step of preparing the phrasecorpus, and therefore it can be used with differentexisting frameworks that were developed for wordvectors.Zou et al.
(2013) learn bilingual word embed-dings by designing an objective function that com-bines unsupervised training with bilingual con-straints based on word alignments.
Similar toour work, they compute an additional feature forphrase pairs using cosine similarity.
Word vec-tors are averaged to obtain phrase representations.In contrast, our approach learns phrase representa-tions directly.Recurrent neural networks were used with min-imum translation units (Hu et al., 2014), which arephrase pairs undergoing certain constraints.
At theinput layer, each of the source and target phrasesare modeled as a bag of words, while the outputphrase is predicted word-by-word assuming con-ditional independence.
The approach seeks to al-leviate data sparsity problems that would arise ifphrases were to be uniquely distinguished.
Ourapproach does not break phrases down to words,but learns phrase embeddings directly.Chen et al.
(2010) represent a rule in the hierar-chical phrase table using a bag-of-words approach.Instead, we learn phrase vectors directly withoutresorting to their constituent words.
Moreover,they apply a count-based approach and employIBM model 1 probabilities to project the targetspace to the source space.
In contrast, our map-ping is similar to that of Mikolov et al.
(2013a)and is learned directly from a small set of bilin-gual data.Mikolov et al.
(2013a) proposed an efficientmethod to learn word vectors through feed-forward neural networks by eliminating the hid-den layer.
They do not report end-to-end sentencetranslation results as we do in this work.Mikolov et al.
(2013b) learn direct representa-tions of phrases after joining a training corpus us-ing a simple monolingual point-wise mutual in-formation criterion with discounting.
Our workexploits the rich bilingual knowledge provided bythe phrase table to join the corpus instead.Gao et al.
(2013) learn shared space mappingsusing a feed-forward neural network and representa phrase vector as a bag-of-words vector.
The vec-tors are learned aiming to optimize an expectedBLEU criterion.
Our work is different in that welearn two separate source and target mappings.8We also do not follow their bag-of-words phrasemodel approach.Marton et al.
(2009) proposed to eliminateOOVs by looking for similar words using distri-butional vectors, but they prune the search spacelimiting it to candidates observed in the same con-text as that of the OOV.
We do not employ such aheuristic.
Instead, we perform a k-nearest neigh-bor search spanning the full phrase table to para-phrase its rules and generate new entries.Estimating phrase table scores using monolin-gual data was investigated in (Klementiev et al.,2012), by building co-occurrence context vectorsand using a small dictionary to induce new scoresfor existing phrase rules.
Our work explores theuse of distributional vectors extracted from neu-ral networks, moreover, we induce new phraserules to extend the phrase table.
New phrase ruleswere also generated in (Irvine and Callison-Burch,2014), where new phrases were produced as acomposition of unigram translations.7 ConclusionIn this work we adapted vector space models toprovide the state-of-the-art phrase-based statisti-cal machine translation system with semantic in-formation.
We leveraged the bilingual knowledgeof the phrase table to construct source and targetphrase corpora to learn phrase vectors, which wereused to provide semantic scoring of phrase pairs.Word vectors allowed to extend the phrase tableand eliminate OOVs.
Both methods proved bene-ficial for low-resource tasks.Future work would investigate decoder inte-gration of semantic scoring that extends beyondphrase boundaries to provide semantically coher-ent translations.AcknowledgmentsThis material is partially based upon work sup-ported by the DARPA BOLT project under Con-tract No.
HR0011- 12-C-0015.
Any opinions,findings and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of DARPA.The research leading to these results has also re-ceived funding from the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement no287658.ReferencesYoshua Bengio, Rejean Ducharme, and Pascal Vincent.2003.
A neural probabilistic language model.
Jour-nal of Machine Learning Research, 3:1137?1155.Boxing Chen, George Foster, and Roland Kuhn.
2010.Bilingual sense similarity for statistical machinetranslation.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 834?843.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis test-ing for statistical machine translation: Controllingfor optimizer instability.
In 49th Annual Meet-ing of the Association for Computational Linguis-tics:shortpapers, pages 176?181, Portland, Oregon,June.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning, pages 160?167.
ACM.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and Robust Neural Network Joint Models forStatistical Machine Translation.
In 52nd AnnualMeeting of the Association for Computational Lin-guistics, Baltimore, MD, USA, June.Michel Galley and Christopher D. Manning.
2008.A simple and effective hierarchical phrase reorder-ing model.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?08, pages 848?856, Stroudsburg, PA,USA.
Association for Computational Linguistics.Jianfeng Gao, Xiaodong He, Wen-tau Yih, andLi Deng.
2013.
Learning semantic representationsfor the phrase translation model.
arXiv preprintarXiv:1312.0482.Zellig S Harris.
1954.
Distributional structure.
Word.Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.2014.
Minimum translation modeling with recur-rent neural networks.
In Proceedings of the 14thConference of the European Chapter of the Associ-ation for Computational Linguistics, pages 20?29,Gothenburg, Sweden, April.
Association for Com-putational Linguistics.Ann Irvine and Chris Callison-Burch.
2014.
Hal-lucinating phrase translations for low resource mt.In Proceedings of the Conference on ComputationalNatural Language Learning (CoNLL).Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1700?1709, Seattle,Washington, USA, October.
Association for Com-putational Linguistics.9Alexandre Klementiev, Ann Irvine, Chris Callison-Burch, and David Yarowsky.
2012.
Toward statisti-cal machine translation without parallel corpora.
InProceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 130?140.
Association for Computa-tional Linguistics.Thomas K Landauer and Susan T Dumais.
1997.
Asolution to plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological review,104(2):211.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments, & Computers, 28(2):203?208.Christopher D Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to informa-tion retrieval, volume 1.
Cambridge university pressCambridge.Yuval Marton, Chris Callison-Burch, and PhilipResnik.
2009.
Improved statistical machine trans-lation using monolingually-derived paraphrases.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume1-Volume 1, pages 381?390.
Association for Com-putational Linguistics.Tomas Mikolov, Quoc V Le, and Ilya Sutskever.2013a.
Exploiting similarities among lan-guages for machine translation.
arXiv preprintarXiv:1309.4168.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.R.C.
Moore and W. Lewis.
2010.
Intelligent Selectionof Language Model Training Data.
In ACL (ShortPapers), pages 220?224, Uppsala, Sweden, July.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native Training and Maximum Entropy Models forStatistical Machine Translation.
In Proc.
of the 40thAnnual Meeting of the Association for Computa-tional Linguistics (ACL), pages 295?302, Philadel-phia, PA, July.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proc.
of the41th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 160?167, Sap-poro, Japan, July.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech & Language, 21(3):492?518.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.on Speech and Language Processing (ICSLP), vol-ume 2, pages 901?904, Denver, CO, September.Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,and Hermann Ney.
2014.
Translation Modelingwith Bidirectional Recurrent Neural Networks.
InProceedings of the Conference on Empirical Meth-ods on Natural Language Processing, October.Peter D Turney, Patrick Pantel, et al.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of artificial intelligence research,37(1):141?188.David Vilar, Daniel Stein, Matthias Huck, and Her-mann Ney.
2010.
Jane: Open source hierarchi-cal translation, extended with reordering and lexi-con models.
In ACL 2010 Joint Fifth Workshop onStatistical Machine Translation and Metrics MATR,pages 262?270, Uppsala, Sweden, July.Joern Wuebker, Matthias Huck, Stephan Peitz, MalteNuhn, Markus Freitag, Jan-Thorsten Peter, SaabMansour, and Hermann Ney.
2012.
Jane 2: Opensource phrase-based and hierarchical statistical ma-chine translation.
In International Conference onComputational Linguistics, pages 483?491, Mum-bai, India, December.Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, andChengqing Zong.
2014.
Bilingually-constrainedphrase embeddings for machine translation.
In Pro-ceedings of the 52th Annual Meeting on Associa-tion for Computational Linguistics.
Association forComputational Linguistics.Will Y Zou, Richard Socher, Daniel M Cer, andChristopher D Manning.
2013.
Bilingual word em-beddings for phrase-based machine translation.
InEMNLP, pages 1393?1398.10
