AN EVALUATION OF LEXICALIZATION IN PARSINGAravind K. Joshi and Yves SchabesDepartment of Computer and Information ScienceUniversity of Pennsylvania, Philadelphia, PA 19104-6389joshi/schabes@linc.cis.upenn.eduAbst ractIn this paper, we evaluate a two-pass parsing strategy proposed for the so-called 'lexicalized' grammar.In 'lexicalized' grammars (Schabes, Abeill$ and Joshi, 1988), each elementary structure is systematicallyassociated with a lexical item called anchor.
These structures pecify extended omains of locality (ascompared to CFGs) over which constraints can be stated.
The 'grammar' consists of a lexicon where eachlexical item is associated with a finite number of structures for which that item is the anchor.
There areno separate grammar ules.
There are, of course, ~rules' which tell us how these structures are combined.A general two-pass parsing strategy for 'lexicalized' grammars follows naturally.
In the first stage,the parser selects a set of elementary structures associated with the lexical items in the input sentence,and in the second stage the sentence is parsed with respect o this set.
We evaluate this strategy withrespect o two characteristics.
First, the amount of filtering on the entire grammar is evaluated: oncethe first pass is performed, the parser uses only a subset of the grammar.
Second, we evaluate the use ofnon-local information: the structures elected uring the first pass encode the morphological value (andtherefore the position in the string) of their anchor; this enables the parser to use non-local informationto guide its search.We take Lexicalized Tree Adjoining Grammars as an instance of lexicallzed grammar.
We il lustratethe organization of the grammar.
Then we show how a general Earley-type TAG parser (Schabes andJoshi, 1988) can take advantage of lexicalization.
Empirical data show that the filtering of the grammarand the non-local information provided by the two-pass trategy improve the performance of the parser.1 LEX ICAL IZED GRAMMARSMost current l inguistic theories give lexical accounts of several phenomena that  used to be considered purelysyntact ic .
The information put  in the lexicon is thereby increased in both amount  and complexity:  see, forexample,  lexical rules in LFG (Kap lan  and Bresnan, 1983), GPSG (Gazdar ,  Klein, Pu l lum and Sag, 1985),HPSG (Pol lard and Sag, 1987), Combinatory  Categor ia l  Grammars  (Steedman 1985, 1988), Kar t tunen 'sversion of Categor ia l  Grammar  (Kar t tunen 1986, 1988), some versions of GB theory (Chomsky 1981), andLex icon-Grammars  (Gross 1984).We say that  a grammar  is ' lexical ized' if it consists of.. 1?
a finite set of s t ructures each associated with a lexical item; each lexical i tem will be called the anchorof the corresponding structure;  the structures define the domain of local i ty over which constraints arespecified; constraints are local with respect o their anchor;?
an operat ion or operat ions for composing the structures.Notice that  Categor ia l  Grammars  (as used for example by Ades and Steedman,  1982 and Steedman,  1985and 1988) are ' lexical ized' according to our definition since each basic category has a lexical i tem associatedwith it.A general  two-step pars ing s t rategy for ' lexical ized'  g rammars  follows natural ly.
In the first stage, theparser selects a set of e lementary  structures associated with the lexical i tems in the input sentence, and inthe second stage the sentence is parsed with respect to this set.
The st rategy is independent  of the natureof the e lementary  structures in the under ly ing grammar.
In principle, any pars ing a lgor i thm can be used inthe second stage.1 By qexicalization' we mean that in each structure there is a lexical item that is realized.
We do not mean simply addingfeature structures (such as head) and unification equations to the rules of the formalism.402The first step selects a relevant subset of the entire grammar, since only the structures associated withthe words in the input string are selected for the parser.
In the worst case, this filtering would select theentire grammar.
The number of structures filtered during this pass depends on the nature of the input stringand on characteristics of the grammar such as the number of structures, the number of lexical entries, thedegree of lexical ambiguity, and the languages it defines.Since the structures elected during the first step encode the morphological value of their anchor (andtherefore its position in the input string), the first step also enables the parser to use non-local information toguide its search.
The encoding of the value of the anchor of each structure constrains the way the structurescan be combined.
It seems that this information is particularly useful for parsing algorithms that have sometop-down behavior.This parsing strategy is general and any standard parsing technique can be used in the second step.Perhaps the advantages of the first step could be captured by some other technique.
However this strategyis extremely simple and is consistent with the linguistic motivations for lexicalization.2 LEX ICAL IZED TAGSNot every grammar is in a 'lexicalized' form.
2 In the process of lexicalizing a grammar, we require thatthe 'lexicalized' grammar produce not only the same language as the original grammar, but also the samestructures (or tree set).For example, a CFG, in general, will not be in a 'lexicalized' form.
The domain of locality of CFGscan be easily extended by using a tree rewriting grammar (Schabes, Abeill~ and Joshi, 1988) that uses onlysubstitution as a combining operation.
This tree rewriting grammar consists of a set of trees that are notrestricted to be of depth one (as in CFGs).
Substitution can take place only on non-terminal nodes of thefrontier of each tree.
Substitution replaces a node marked for substitution by a tree rooted by the same labelas the node (see Figure 1; the substitution ode is marked by a down arrow ~.
).However, in the general case, CFGs cannot be 'lexicalized', if only substitution is used.
Furthermore, ingeneral, there is not enough freedom to choose the anchor of each structure.
This is important because wewant the choice of the anchor for a given structure to be determined on purely linguistic grounds.If adjunction is used as an additional operation to combine these structures, CFGs can be lexicalized.Adjunction builds a new tree from an auxiliary tree fl and a tree ot .
It inserts an auxiliary tree in anothertree (see Figure 1).
Adjunction is more powerful than substitution.
It can weakly simulate substitution, butit also generates languages that could not be generated with substitution.
3SubstitutionLL/\ Ca) (!3)AdjunclionFigure 1: Combining operationsSubstitution and adjunction enable us to lexicalize CFGs.
The 'anchors' can be freely chosen (Schabes,Abeill~ and Joshi, 1988).
The resulting system now falls in the class of mildly context-sensitive languages~Notice the similarity of the definition of ' lexicalized' g rammar  with the ofltlne parsibi l l ty constraint  (Kap lan and  Bresnan1983).
As consequences of our definition, each structure has at least one lexical i tem (its anchor) at tached to it and all sentencesare finitely ambiguous.3It is also possible to encode a context-free grammar  with auxi l iary trees us ing adjunct ion only.
However, a l though thelanguages correspond, the set of trees do not correspond.403(Joshi, 1985).
Elementary structures of extended omain of locality combined with substitution and adjunc-tion yield Lexicalized TAGs.TAGs were first introduced by Joshi, Levy and Takahashi (1975) and Joshi (1985).
For more detailson the original definition of TAGs, we refer the reader to Joshi (1985), Kroch and Joshi (1985), or Vijay-Shanker (1987).
It is known that Tree Adjoining Languages (TALs) are mildly context sensitive.
TALsproperly contain context-free languages.TAGs with substitution and adjunction are naturally lexicalized.
4 A Lexicalized Tree Adjoining Grammaris a tree-based system that consists of two finite sets of trees: a set of initial trees, I and a set of auxiliarytrees A (see Figure 2).
The trees in I t3 A are called e lementary  trees.
Each elementary tree is constrainedto have at least one terminal symbol which acts as its anchor.tlee:st One 'head' nodelerminal orsubstitution nodesAuxil~ Item:?
/ x \\] \]Figure 2: Schematic initial and auxiliary treesThe t ree  set of a TAG G, 7"(G) is defined to be the set of all derived trees starting from S-type initialtrees in I.
The st r ing language generated by a TAG, ?
(G), is defined to be the set of all terminal stringsof the trees in 7-(G).By lexicalizing TAGs, we have associated lexical information to the 'production' system encoded by theTAG trees.
We have therefore kept the computational dvantages of 'production-like' formalisms (such asCFGs, TAGs) while allowing the possibility of linking them to lexical information.
Formal properties ofTAGs hold for Lexicalized TAGs.As first shown by Kroch and Joshi (1985), the properties of TAGs permit us to encapsulate diverse syn-tactic phenomena in a very natural way.
TAG's extended omain of locality and its factoring recursion fromlocal dependencies lead, among other things, to localizing the so-called unbounded ependencies.
Abeill6(1988a) uses the distinction between substitution and adjunction to capture the different extraction prop-erties between sentential subjects and complements.
Abeill6 (1988c) makes use of the extended omain oflocality and lexicalization to account for NP island constraint violations in light verb constructions; in suchcases, extraction out of NP is to be expected, without the use of reanalysis.
The relevance of LexicalizedTAGs to idioms has been suggested by Abeill6 and Schabes (1989).We will now give some examples of structures that appear in a Lexicalized TAG lexicon.Some examples of initial trees are (for simplicity, we have omitted unification equations associated withthe trees): 54In some earlier work of Joshi (1969, 1973), the use of the two operations 'adjoining' and 'replacement' (a restricted case ofsubstitution) was investigated both mathematically and linguistically.
However, these investigations dealt with string rewritingsystems and not tree rewriting systems.5The trees are simplified and the feature structures on the trees are not displayed.
I is the mark for substitution odes, *is the mark for the foot node of an auxiliary tree and NA stands for null adjunction constraint.
This is the only adjunctionconstraint not indirectly stated by feature structures.
We put indices on some non-terminals to express yntactic roles (0 forsubject, 1 for first object, etc.).
The index shown on the empty string (c) and the corresponding filler in the same tree is forthe purpose of indicating the filler-gap dependency.404NPAD$ NsS S ~ SA A NP4(+wh) S / ~NPo$ VP NPo$ VP A NPo$ VPNPo$ VP\] (-2) A (-3) A ("4) V NPi$ PP2 ("1) V V NPi~ V NP1NAboy left saw saw e put P25 NP2$(-5)Examples of auxiliary trees (they correspond to predicates taking sentential complements or modifiers):S S SNPo$ VP NPo$ VP NPo$ VP VP NA /1",,,.
A A AV Sl*NA (,81) V NPi,I, S2*NA (,82) V Sl*NA (,83) V VP*NA (,84) A N*NAI I i I Ithink promise saw has pretty(,85)In this approach, the argument structure is not just a list of arguments.
It is the syntactic structureconstructed with the lexical value of the predicate and with all the nodes of its arguments that  eliminatesthe redundancy often noted between phrase structure rules and subcategorization frames.
62.1 ORGANIZAT ION OF  THE GRAMMARA Lexicalized TAG is organized into two major parts: a lex icon  and t ree  fami l ies ,  which are sets of trees.
7TAG's  factoring recursion from dependencies, the extended domain of locality of TAGs, and lexicalizationof elementary trees make Lexicalized TAG an interesting framework for grammar writing.
Abeill~ (1988b)discusses the writing of a Lexicalized TAG for French.
Abeill~, Bishop, Cote and Schabes (1989) similarlydiscuss the writing of a Lexicalized TAG grammar for English.2.1.1 TREE FAMIL IESA t ree  fami ly  is essentially a set of sentential trees sharing the same argument structure abstracted fromthe lexical instantiation of the anchor (verb, predicative noun or adjective).
Because of the extended omainof locality of Lexicalized TAG, the argument structure is not stated by a special mechanism but is implicitlystated in the topology of the trees in a tree family.
Each tree in a family can be thought of as all possiblesyntactic ' transformations'  of a given argument structure.
Information (in the form of feature structures)that  is valid independent of the value of the anchor is stated on the tree of the tree family.
For example, theagreement between the subject and the main verb or auxiliary verb is stated on each tree of the tree family.Currently, the trees in a family are explicitly enumerated.6 Optional arguments are stated in the structure.7There are actually two types of lexicons: a morphological lexicon which lists the possible morphological variations for aword and a syntactic lexicon which associates the variations of a given word to syntactic elementary trees.
In this paper wewill ignore the morphological lexicon and we will use the term lexicon for refering to the syntactic lexicon.405The following trees, among others, compose the tree family of verbs taking one object (the family isnamed npOVnpl): sSNP NPNPi*NA SNA SNA NPi,I-(+wh) SS ~ NPi$(+wh) S/ ~  NPi,I,(+wh) S / /k~ NPi$(+wh) S NP0,1, VP/ \NPo$ VP NPoNA VP A NPoNA VP V?
NP1NAVO NPiNAI l k  , IV0 NPI$ ei V0 NPI$ et E i V?
NPI$ Ei(o npOVnpl) ( ROnpOVnpl) (ZalnpOVnpl) WOnpOVnpl) ( o~ WlnpO Vnp l)ompOVnpl is an initial tree corresponding to the declarative sentence, flROnpOVnpl is an auxiliary treecorresponding to a relative clause where the subject has been relativized, flRlnpOVnpl corresponds to therelative clause where the object has been relativized, o~ WOnpOVnpl is an initial tree corresponding to awh-question on the subject, ot WlnpOVnpl corresponds to a wh-question on the object.2.1.2 THE LEX ICONThe lex icon is the heart of the grammar.
It associates a word with tree families or trees.
Words are notassociated with basic categories as in a CFG-based grammar, but with tree-structures corresponding tominimal inguistic structures.
Multi-level dependencies can thus be stated in the lexicon.It also states some word-specific feature structure quations (such as the agreement value of a given verb)that have to be added to the ones already stated on the trees (such as the equality of the value of the subjectand verb agreements).An example of a lexical entry follows:loves, V {V.b:<mode>=ind,V.b:<agr pers>= S,V.b:<agr hum>= singular,V.b : <tense>=present} :npOVnpl.It should be emphasized that in our approach the category of a word is not a non-terminal symbol but amulti-level structure corresponding to minimal linguistic structures: sentences (for predicative verbs, nounsand adjectives) or phrases (NP for nouns, AP for adjectives, PP for prepositions yielding adverbial phrases).2.2 PARSING LEX ICAL IZED TAGsAn Earley-type parser for TAGs has been developed by Schabes and Joshi (1988).
It is a general TAGparser.
It handles adjunction and substitution.
It can take advantage of lexicalization.
It uses the structuresselected after the first pass to parse the sentence.
The parser is able to use the non-local information givenby the first step to filter out prediction and completion states.2.2.1 TAK ING ADVANTAGE OF LEX ICAL IZAT IONIf an offline behavior is adopted, the Earley-type parser for TAGs can be used with no modification forparsing Lexicalized TAGs.
First the trees corresponding to the input, string are selected and then the parserparses the input string with respect o this set of trees.However, Lexicalized TAGs simplify some cases of the algorithm.
For example, since by definition eachtree has at least one lexical item attached to it (its anchor), it will not be the case that a tree can be predicted8The trees are simplified, o is the mark for the node under wlfich the lexical insertion of the anchor is performed.406for substitution and completed in the same states et.
Similarly, it will not be the case that an auxiliary treecan be left predicted for adjunction and right completed in the same states set.But most importantly the algorithm can be extended to take advantage of Lexicalized TAGs.
Once thefirst pass has been performed, a subset of the grammar is selected.
Each structure ncodes the morphologicalvalue (and therefore the positions in the string) of its anchor.
Identical structures with different anchor valuesare merged together (by identical structures we mean identical trees and identical information, such as featurestructures, stated on those trees).
9 This enables us to use the anchor position information while processingefficiently the structures.
For example, given the sentenceThe 1 men 2 who 3 saw 4 the 5 woman 6 who 7 saw 8 .John 9 are  10 happy nthe following trees (among others) are selected after the first pass: l?NPA sNP SComps* s NPo$ VP NP /~ NP NPA NP?
VP A I AD D$ N Comp \[ A D$ N N V A,~ A~i v NPi$ I I I I I I Ithe(l,5) men(2) who(3,7) saw(4,8) woman(6) John(9 ) are(lo) happy(l DThe trees for men and for woman are distinguished since they carry different agreement feature structures(not shown in the figure).Notice that there is only one tree for the relative clauses introduced by saw but that its anchor positioncan be 4 or 8.
Similarly for who and the.The anchor positions of each structure impose constraints on the way that the structures can be combined(the anchor positions must appear in increasing order in the combined structure).
This helps the parser tofilter out predictions or completions for adjunction or substitution.
For example, the tree corresponding tomen will not be predicted for substitution in any of the trees corresponding to saw since the anchor positionswould not be in the right order.We have been evaluating the influence of the filtering of the grammar and the anchor position informationon the behavior of the Earley-type parser.
We have conducted experiments on a feature structure-basedLexicalized English TAG whose lexicon defines 200 entries associated with 130 different elementary trees(the trees are differentiated by their topology and their feature structures but not by their anchor value).Twenty five sentences of length ranging from 3 to 14 words were used to evaluate the parsing strategy.
Foreach experiment, he number of trees given to the parser and the number of states were recorded.In the first experiment (referred to as one pass, OP), no first pass was performed.
The entire grammar(i.e., the 130 trees) was used to parse each sentence.
In the second experiment (referred to as two passesno anchor, NA), the two-pass trategy was used but the anchor positions were not used in the parser.
Andin the third experiment (referred to as two passes with anchor, A), the two-pass trategy was used and theinformation given by the anchor positions was used by the parser.The average behavior of the parser for each experiment is given in Figure 3.
The first pass filtered onaverage 85% (always at least 75%) of the trees.
The filtering of the grammar by itself decreased by 86% thenumber of states ( (NA - OP) /OP) .
The additional use of the information given by the anchor positionsfurther decreased by 50% ((A - NA) /NA)  the number of states.
The decrease given by the filtering of thegrammar and by the information of the anchor positions is even bigger on the number of attempts to add astate (not reported in the table), nThis set of experiments shows that the two-pass trategy increases the performance of the Earley-typeparser for TAGs.
The filtering of the grammar affects the parser the most.
The information given by anchor9Unlike our previous suggestions (Schabes, Abeill6 and Josh_i, 1988), we do not distinguish each structure by its anchorposit ion since it increases unnecessarily the number of states of the Earley parser.
By factoring recursion, the Earley parserenables us to process only once parts  of a tree that are associated with several exlcal items selecting the same tree.
However,if terminat ion is required for a pure top-down parser, it is necessary to distinguish each structure by its anchor position.1?The example is simplified to i l lustrate our point.l lA  state is effectively added to a states set if it does not exist in the set aleady.407position in the first pass allows further improvement of the parser's performance (- 50% of the numberof states on the set of experiments).
The bottom-up non-local information given by the anchor positionsimproves the top-down component of the Earley-type parser.
(NA-OP) /OP (A-OP) /OP (A - NA) /NA(%) (%) (%)trees -85 -85 0states -86 -93 -50Figure 3: Empirical evalualion of lhe two-pass trategyWe performed our evaluation on a relatively small grammar and we did not evaluate the variations acrossgrammars.
The lexical degree of ambiguity of each word, the number of structures in the grammar, thenumber of lexical entries, and the length (and nature) of the input sentences are parameters to be considered.Although it might appear easy to conjecture the influence of these parameters, the actual experiments aredifficult to perform since statistical data on these parameters are hard to obtain.
We hope to perform somelimited experiments along those lines.3 CONCLUSIONIn 'lexicalized' grammars, each elementary structure is systematically associated with a lexical anchor.
Thesestructures pecify extended omains of locality (as compared to the domain of locality in CFGs) over whichconstraints can be stated.
The 'grammar'  consists of a lexicon in which each lexical item is associated witha finite number of structures for which that item is the anchor.Lexicalized grammars uggest a natural two-step parsing strategy.
The first step selects the set ofstructures corresponding to each word in the sentence.
The second step tries to combine the selectedstructures.We take Lexicalized TAGs as an instance of lexicalized grammar.
We illustrate the organization of thegrammar Then we show how the Earley-type parser can take advantage of the two-step parsing strategy.Experimental data show that its performance is thereby drastically improved.
The first pass not only filtersthe grammar used by the parser to produce a relevant subset but also enables the parser to use non-localbottom-up information to guide its search.
In Schabes and Joshi (1989) it is also shown that Lexicalizationguarantees termination of the parsing algorithm of feature structures for Lexicalized TAGs without a specialmechanism such as the use of restrictors.The organization of lexicalized grammars, the simplicity and effectiveness of the two-pass trategy (someother technique would perhaps achieve similar results) seem attractive from a linguistic point of view andfor processing.408ReferencesAbeill~, Anne, August 1988 (a).
Parsing French with Tree Adjoining Grammar: some Linguistic Accounts.
InProceedings of the 12 th International Conference on Computational Linguistics (COLING'88).
Budapest.Abeill~, Anne, 1988 (b).
A Lexicalized Tree Adjoining Grammar for French: the General Framework.
TechnicalReport MS-CIS-88-64, Department of Computer and Information Science, University of Pennsylvania.Abeill~, Anne, 1988 (c).
Light Verb Constructions and Extraction out of NP in Tree Adjoining Grammar.
In Papersfrom the P4th Regional Meeting of the Chicago Linguistic Society.
Chicago.Abeill~, Anne and Schabes, Yves, 1989.
Parsing Idioms in Tree Adjoining Grammars.
In Fourth Conference of theEuropean Chapter of the Association for Computational Linguistics (EACL'89).
Manchester.Abeill6, Anne; M., Bishop Kathleen; Cote, Sharon; and Schabes, Yves, 1989.
A Lexicalized Tree Adjoining Grammarfor English.
Technical Report, Department of Computer and Information Science, University of Pennsylvania.Ades, A. E. and Steedman, M. J., 1982.
On the Order of Words.
Linguistics and Philosophy 3:517-558.Chomsky, N., 1981.
Lectures on Government and Binding.
Foris, Dordrecht.Gazdar, G.; Klein, E.; Pullum, G. K.; and Sag, I.
A., 1985.
Generalized Phrase Structure Grammars.
BlackwellPublishing, Oxford.
Also published by Harvard University Press, Cambridge, MA.Gross, Maurice, 2-6 July 1984.
Lexicon-Grammar nd the Syntactic Analysis of French.
In Proceedings of the 10 thInternational Conference on Computational Linguistics (COLING'84).
Stanford.Joshi, Aravind K., August 1969.
Properties of Formal Grammars with Mixed Type of Rules and their LinguisticRelevance.
In Proceedings of the International Conference on Computational Linguistics.
Sanga Saby.Joshi, Aravind K., 1973.
A Class of Transformational Grammars.
In M. Gross, M. Halle and Schutzenberger, M.P.
(editors), The Formal Analysis of Natural Languages.
Mouton, La Hague.Joshi, Aravind K., 1985.
How Much Context-Sensitivity is Necessary for Characterizing Structural Descriptions--Tree Adjoining Grammars.
In Dowty, D.; Karttunen, L.; and Zwicky, A.
(editors), Natural Language Processing--Theoretical, Computational and Psychological Perspectives.
Cambridge University Press, New York.
Originallypresented in a Workshop on Natural Language Parsing at Ohio State University, Columbus, Ohio, May 1983.Joshi, A. K.; Levy, L. S.; and Takahashi, M., 1975.
Tree Adjunct Grammars.
J. Comput.
Syst.
Sci.
10(1).Kaplan, R. and Bresnan, J., 1983.
Lexical-functional Grammar: A Formal System for Grammatical Representation.In Bresnan, J.
(editor), The Mental Representation of Grammatical Relations.
MIT Press, Cambridge MA.Karttunen, Lauri, 1986.
Radicals Lexicalism.
Technical Report CSLI-86-68, CSLI, Stanford University.
To alsoappear in New Approaches to Phrase Structures, University of Chicago Press, Baltin, M. and Kroch A., Chicago,1988.Kroch, A. and Joshi, A. K., 1985.
Linguistic Relevance of Tree Adjoining Grammars.
Technical Report MS-CIS-85-18,Department of Computer and Information Science, University of Pennsylvania.Pollard, Carl and Sag, Ivan A., 1987.
Information-Based Syntax and Semantics.
Vol 1: Fundamentals.
CSLI.Schabes, Yves and Joshi, Aravind K., June 1988.
An Earley-Type Parsing Algorithm for Tree Adjoining Grammars.In 26 th Meeting of the Association for Computational Linguistics (ACL '88).
Buffalo.Schabes, Yves and Joshi, Aravind K., August 1989.
The Relevance of Lexicalization to Parsing.
In Proceedings ofthe International Workshop on Parsing Technologies.
Pittsburgh.Schabes, Yves; Abeill6, Anne; and Joshi, Aravind K., August 1988.
Parsing Strategies with 'Lexicalized' Grammars:Application to Tree Adjoining Grammars.
In Proceedings of the 12 th International Conference on ComputationalLinguistics (COLING'88).
Budapest.Steedman, M. J., 1985.
Dependency and Coordination in the Grammar of Dutch and English.
Language 61:523-568.Steedman, M., 1987.
Combinatory Grammars and Parasitic Gaps.
Natural Language and Linguistic Theory 5:403-439.Vijay-Shanker, K., 1987.
A Study of Tree Adjoining Grammars.
PhD thesis, Department of Computer and Informa-tion Science, University of Pennsylvania.409
