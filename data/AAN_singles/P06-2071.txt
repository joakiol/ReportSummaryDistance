Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 547?554,Sydney, July 2006. c?2006 Association for Computational LinguisticsDiscriminating image senses by clustering with multimodal featuresNicolas LoeffDept.
of Computer ScienceUniversity of Illinois, UCloeff@uiuc.eduCecilia Ovesdotter AlmDept.
of LinguisticsUniversity of Illinois, UCebbaalm@uiuc.eduDavid A. ForsythDept.
of Computer ScienceUniversity of Illinois, UCdaf@uiuc.eduAbstractWe discuss Image Sense Discrimination(ISD), and apply a method based on spec-tral clustering, using multimodal featuresfrom the image and text of the embeddingweb page.
We evaluate our method on anew data set of annotated web images, re-trieved with ambiguous query terms.
Ex-periments investigate different levels ofsense granularity, as well as the impact oftext and image features, and global versuslocal text features.1 Introduction and problem clarificationSemantics extends beyond words.
We focus on im-age sense discrimination (ISD)1 for web imagesretrieved from ambiguous keywords, given a mul-timodal feature set, including text from the doc-ument which the image was embedded in.
Forinstance, a search for CRANE retrieves images ofcrane machines, crane birds, associated other ma-chinery or animals etc., people, as well as imagesof irrelevant meanings.
Current displays for im-age queries (e.g.
Google or Yahoo!)
simply listretrieved images in any order.
An application isa user display where images are presented in se-mantically sensible clusters for improved imagebrowsing.
Another usage of the presented modelis automatic creation of sense discriminated imagedata sets, and determining available image sensesautomatically.ISD differs from word sense discrimination anddisambiguation (WSD) by increased complexityin several respects.
As an initial complication,both word and iconographic sense distinctions1Cf.
(Schu?tze, 1998) for a definition of sense discrimina-tion in NLP.matter.
Whereas a search term like CRANE canrefer to, e.g.
a MACHINE or a BIRD; iconographicdistinctions could additionally include birds stand-ing, vs. in a marsh land, or flying, i.e.
sense-distinctions encoded by further descriptive modi-fication in text.
Therefore, as the number of textsenses grow with corpus size, the iconographicsenses grow even faster, and enumerating icono-graphic senses is extremely challenging; espe-cially since dictionary senses do not capture icono-graphic distinctions.
Thus, we focus on image-driven word senses for ISD, but we acknowledgethe importance of iconography for visual meaning.Also, an image often depicts a related mean-ing.
E.g.
a picture retrieved for SQUASH maydepict a squash bug (i.e.
an insect on a leaf ofa squash plant) instead of a squash vegetable,whereas this does not really apply in WSD, whereeach instance concerns the ambiguous term itself.Therefore, it makes sense to consider the divi-sion between core sense, related sense, and un-related sense in ISD, and, as an additional com-plication, their boundaries are often blurred.
Mostimportantly, whereas the one-sense-per-discourseassumption (Yarowsky, 1995) also applies to dis-criminating images, there is no guarantee ofa local collocational or co-occurrence contextaround the target image.
Design or aesthetics mayinstead determine image placement.
Thus, con-sidering local text around the image may not be ashelpful as local context is for standard WSD.
Infact, the query term may even not occur in thetext body.
On the other hand, one can assume thatan image spotlights the web page topic and that ithighlights important document information.
Also,images mostly depict concrete senses.
Lastly, ISDfrom web data is complicated by web pages beingmore domain-independent than news wire, the fa-547(a) squash flower (b) tennis?
(c) hook (d) food (e) bow (f) speakerFigure 1: Example RELATED images for (a) vegetable and (b) sports senses for SQUASH, and for (c-d) fish and (e-f) musicalinstrument for BASS.
Related senses are associated with the semantic field of a core sense, but the core sense is visually absentor undeterminable.Figure 2: Which fish or instruments are BASS?
Image sense annotation is more vague and subjective than in text.vored corpus for WSD.
As noted by (Yanai andBarnard, 2005), whereas current image retrievalengines include many irrelevant images, a data setof web images gives a more real-world point ofdeparture for image recognition.Outline Section 2 discusses the corpus data andimage annotation.
Section 3 presents the featureset and the clustering model.
Subsequently, sec-tion 4 introduces the evaluation used, and dis-cusses experimental work and results.
In section5, this work is positioned with respect to previouswork.
We conclude with an outline of plans forfuture work in section 6.2 Data and annotationYahoo!
?s image query API was used to obtain acorpus of pairs of semantically ambiguous images,in thumbnail and true size, and their correspond-ing web sites for three ambiguous keywords in-spired by (Yarowsky, 1995): BASS, CRANE, andSQUASH.
We apply query augmentation (cf.
Ta-ble 1), and exact duplicates were filtered out byidentical image URLs, but cases occurred whereboth thumbnail and true-size image were included.Also, some images shared the same webpage orcame from the same site.
Generally, the lat-ter gives important information about shared dis-course topic, however the images do not necessar-ily depict the same sense (e.g.
a CRANE bird vs.a meadow), and image features can separate theminto different clusters.Annotation overview The images were anno-tated with one of several labels by one of the au-thors out of context (without considering the website and its text), after applying text-based filter-ing (cf.
section 3.1).
For annotation purposes, im-ages were numbered and displayed on a web pagein thumbnail size.
In case the thumbnail was notsufficient for disambiguation, the image linked atits true size to the thumbnail was inspected.2 Thetrue-size view depended on the size of the orig-inal picture and showed the image and its name.However, the annotator tried to resist name influ-ence, and make judgements based just on the im-age.
For each query, 2 to 4 core word senses (e.g.squash vegetable and squash sport for SQUASH)were distinguished from inspecting the data.
How-ever, because ?context?
was restricted to the imagecontent, and there was no guarantee that the imageactually depicts the query term, additional anno-tator senses were introduced.
Thus, for most coresenses, a RELATED label was included, accountingfor meanings that seemed related to core meaningbut lacked a core sense object in the image.
Someexamples for RELATED senses are in Fig.
1.
In ad-dition, for each query term, a PEOPLE label wasincluded because such images are common due tothe nature of how people take pictures (e.g.
por-traits of persons or group pictures of crowds, whencore or related senses did not apply), as was an2We noticed a few cases where Yahoo!
retrieved a thumb-nail image different from the true size image.548Word (#Annot.
images) QueryTerms Senses Coverage Examples of visual annotation cuesBASS(2881)5: bass, bass guitar,bass instrument,bass fishing, seabass1.
fish 35% any fish, people holding catch2.
musical instrument 28% any bass-looking instrument, playing3.
related: fish 10% fishing (gear, boats, farms), rel.
food, rel.
charts/maps4.
related: musical instrument 8% speakers, accessories, works, chords, rel.
music5.
unrelated 12% miscellaneous (above senses not applicable)6. people 7% faces, crowd (above senses not applicable)CRANE(2650)5: crane,construction cranes,whooping crane,sandhill crane,origami cranes1.
machine 21% machine crane, incl.
panoramas2.
bird 26% crane bird or chick3.
origami 4% origami bird4.
related: machine 11% other machinery, construction, motor, steering, seat5.
related: bird 11% egg, other birds, wildlife, insects, hunting, rel.
maps/charts6.
related: origami 1% origami shapes (stars, pigs), paper folding7.
people 7% faces, crowd (above senses not applicable)8. unrelated 18% miscellaneous (above senses not applicable)9. karate 1% martial artsSQUASH(1948)10: squash+: rules,butternut, vegetable,grow, game of,spaghetti, winter,types of, summer1.
vegetable 24% squash vegetable2.
sport 13% people playing, court, equipment3.
related:vegetable 31% agriculture, food, plant, flower, insect, vegetables4.
related:sport 6% other sports, sports complex5.
people 10% faces, crowd (above senses not applicable)6. unrelated 16% miscellaneous (above senses not applicable)Table 1: Web images for three ambiguous query terms were annotated manually out of context (without considering theweb page document).
For each term, the number of annotated images, the query retrieval terms, the senses, their distribution,and rough sample annotation guidelines are provided, with core senses marked in bold face.
Because image retrieval enginesrestrict hits to 1000 images, query expansion was conducted by adding narrowing query terms from askjeeves.com toincrease corpus size.
We selected terms relevant to core senses, i.e.
the main discrimination phenomenon.UNRELATED label for irrelevant images which didnot fit other labels or were undeterminable.For a human annotator, even when using morenatural word senses, assigning sense labels to im-ages based on image alone is more challengingand subjective than labeling word senses in tex-tual context.
First of all, the annotation is heav-ily dependent on domain-knowledge and it is notfeasible for a layperson to recognize fine-grainedsemantics.
For example, it is straightforward forthe layperson to distinguish between a robin and acrane, but determining whether a given fish shouldhave the common name bass applied to it, orwhether an instrument is indeed a bass instrumentor not, is extremely difficult (see Fig.
2; e.g.
de-ciding if a picture of a fish fillet is a picture of afish is tricky).
Furthermore, most images displayobjects only partially; for example just the neckof a classical double bass instead of the whole in-strument.
In addition, scaling, proportions, andcomponents are key cues for object discrimina-tion in real-life, e.g.
for singling out an electricbass from an electric guitar, but an image maynot provide these detail.
Thus, senses are evenfuzzier for ISD than WSD labeling.
Given thatlaypeople are in the majority, it is fair to assumetheir perspective and naiveness.
This latter factalso led to annotations?
level of specificity differ-ing according to search term.
Annotation criteriadepended on the keyword term and its senses andtheir coverage, as shown in Table 1.
Neverthe-less, several border-line cases for label assignmentoccurred.
Considering that the annotation task isKeywordquery FilteringImage featureextraction Text feature extraction1.
Compute pair-wise document affinities2.
Compute eigenvalues3.
Embed and clusterEvaluation of purityFigure 3: Overview of algorithmquite subjective, this is to be expected.
In fact,one person?s labeling often appears as justifiableas a contradicting label provided by another per-son.
We explore the vagueness and subjective na-ture of image annotation further in a companionpaper (Alm, Loeff, Forsyth, 2006).3 ModelOur goal is to provide a mapping between im-ages and a set of iconographically coherent clus-ters for a given query word, in an unsupervisedframework.
Our approach involves extractingand weighting unordered bags-of-words (BOWs;henceforth) features from the webpage text, sim-ple local and global features from the image, andrunning spectral clustering on top.
Fig.
3 shows anoverview of the implementation.5493.1 Feature extractionDocument and text filtering A pruning processwas used to filter out image-document pairs basedon e.g.
language specification, exclusion of ?In-dex of?
pages, pages lacking an extractable targetimage, or a cutoff threshold of number of tokensin the body.
For remaining documents, text waspreprocessed (e.g.
lower-casing, removing punc-tuation, tokens being very short, having numbersor no vowels, etc.).
We used a stop word list, butavoided stemming to make the algorithm languageindependent in other respects.
When using imagefeatures, grayscale images (no color histograms)and images without salient regions (no keypointsdetected) were also removed.Text features We used the following BOWs:(a) tokens in the page body; (b) tokens in a ?10window around the target image (if multiple, thefirst was considered); (c) tokens in a ?10 windowaround any instances of the query keyword (e.g.squash); (d) tokens of the target image?s alt at-tribute; (e) tokens of the title tag; (f) some metatokens.3 Tf-idf was applied to a weighted aver-age of the BOWs.
Webpage design is flexible, andsome inconsistencies and a certain degree of noiseremained in the text features.Image features Given the large variability inthe retrieved image set for a given query, it is dif-ficult to model images in an unsupervised fash-ion.
Simple features have been shown to provideperformance rivaling that of more elaborate mod-els in object recognition (Csurka et al 2004) and(Chapelle, Haffner, and Vapnik, 1999), and thefollowing image bags of features were considered:Bags of keypoints: In order to obtain a compactrepresentation of the textures of an image, patchesare extracted automatically around interesting re-gions or keypoints in each image.
The keypointdetection algorithm (Kadir and Brady, 2001) usesa saliency measure based on entropy to select re-gions.
After extraction, keypoints were repre-sented by a histogram of gradient magnitude ofthe pixel values in the region (SIFT) (Lowe, 2004).These descriptors were clustered using a GaussianMixture with ?
300 components, and the result-ing global patch codebook (i.e.
histogram of code-book entries) was used as lookup table to assigneach keypoint to a codebook entry.3Adding to META content, keywords was an attribute, butis irregular.
Embedded BODY pairs are rare; thus not used.Color histograms: Due to its similarity tohow humans perceive color, HSV (hue, saturation,brightness) color space was used to bin pixel colorvalues for each image.
Eight bins were used perchannel, obtaining an 83 dimensional vector.3.2 Measuring similarity between imagesFor the BOWs text representation, we use the com-mon measure of cosine similarity (cs) of two tf-idf vectors (Jurafsky and Martin, 2000).
The co-sine similarity measure is also appropriate for key-point representation as it is also an unordered bag.There are several measures for histogram compar-ison (i.e.
L1, ?2).
As in (Fowlkes et al 2004) weuse the ?2 distance measure between histogramshi and hj .
?2i,j =12512?k=1(hi(k)?
hj(k))2hi(k) + hj(k)(1)3.3 Spectral ClusteringSpectral clustering is a powerful way to sepa-rate non-convex groups of data.
Spectral meth-ods for clustering are a family of algorithms thatwork by first constructing a pairwise-affinity ma-trix from the data, computing an eigendecomposi-tion of the data, embedding the data into this low-dimensional manifold, and finally applying tradi-tional clustering techniques (i.e.
k-means) to it.Consider a graph with a set of n vertices eachone representing an image document, and theedges of the graph represent the pairwise affinitiesbetween the vertices.
Let W be an n?n symmet-ric matrix of pairwise affinities.
We define theseas the Gaussian-weighted distanceWij = exp(??t(1?
csti,j)?
?k(1?
cski,j)?
?c?2i,j),(2)where {?t, ?k, ?c} are scaling parameters for text,keypoints, and color features.It has been shown that the use of multiple eigen-vectors of W is a valid space onto which the datacan be embedded (Ng, Jordan, Weiss, 2002).
Inthis space noise is reduced while the most signif-icant affinities are preserved.
After this, any tra-ditional clustering algorithm can be applied in thisnew space to get the final clusters.
Note that thisis a nonlinear mapping of the original space.
Inparticular, we employ a variant of k-means, whichincludes a selective step that is quasi-optimal ina Vector Quantization sense (Ueda and Nakano,1994).
It has the added advantage of being more550robust to initialization than traditional k-means.The algorithm follows,1.
For given documents, compute the affinitymatrix W as defined in equation 2.2.
Let D be a diagonal matrix whose (i, i)-thelement is the sum of W ?s i-th row, and de-fine L = D?1/2WD?1/2.3.
Find the k largest eigenvectors V of L.4.
Define E as V , with normalized rows.5.
Perform clustering on the columns of E,which represent the embedding of each im-age into the new space, using a selective stepas in (Ueda and Nakano, 1994).Why Spectral Clustering?
Why apply a vari-ant of k-means in the embedded space as opposedto the original feature space?
The k-means algo-rithm cannot separate non-convex clusters.
Fur-thermore, it is unable to cope with noisy dimen-sions (this is especially true in the case of the textdata) and highly non-ellipsoid clusters.
(Ng, Jor-dan, Weiss, 2002) stated that spectral clusteringoutperforms k-means not only on these high di-mensional problems, but also in low-dimensional,multi-class data sets.
Moreover, there are prob-lems where Euclidean measures of distance re-quired by k-means are not appropriate (for in-stance histograms), or others where there is noteven a natural vector space representation.
Also,spectral clustering provides a simple way of com-bining dissimilar vector spaces, like in this casetext, keypoint and color features.4 Experiments and resultsIn the first set of experiments, we used all featuresfor clustering.
We considered three levels of sensegranularity: (1) all senses (All), (2) merging re-lated senses with their corresponding core sense(Meta), (3) just the core senses (Core).
For ex-periments (1) and (2), we used 40 clusters and alllabeled images.
For (3), we considered only im-ages labeled with core senses, and thus reduced thenumber of clusters to 20 for a more fair compari-son.
Results were evaluated according to globalcluster purity, cf.
Equation 3.4Global purity =?clusters# of most common sense in clustertotal # images(3)4Purity did not include the small set of outlier images, de-fined as images whose ratio of distances to the second closestand closest clusters was below a threshold.Word All senses Meta senses Core sensesBASS 6 senses 4 senses 2 sensesMedian 0.60 0.73 0.94Range 0.03 0.02 0.02Baseline 0.35 0.45 0.55CRANE 9 senses 6 senses 4 sensesMedian 0.49 0.65 0.86Range 0.05 0.07 0.07Baseline 0.27 0.37 0.50SQUASH 6 senses 4 senses 2 sensesMedian 0.52 0.71 0.94Range 0.03 0.04 0.03Baseline 0.32 0.56 0.64Table 2: Median and range of global clustering purityfor 5 runs with different initializations.
For each keyword, thetable lists the number of senses, median, and range of globalcluster purity, followed by the baseline.
All senses used thefull set of sense labels and 40 clusters.
Meta senses mergedcore senses with their respective related senses, consideringall images and using 40 clusters.
Core senses were clusteredinto 20 clusters, using only images labeled with core sense la-bels.
Purity was stable across runs, and peaked for Core.
Thebaseline reflected the frequency of the most common sense.Word Img TxtWin BodyTxt BaselineBASSMedian 0.71 0.83 0.93 0.55Range 0.05 0.03 0.05CRANEMedian 0.61 0.84 0.85 0.50Range 0.07 0.04 0.05SQUASHMedian 0.71 0.91 0.96 0.64Range 0.05 0.04 0.03Table 3: Global and local features?
performance.
Coresense images were grouped into 20 clusters, on the basis ofindividual feature types, and global cluster purity was mea-sured.
The table lists the median and range from 5 runs withdifferent initializations.
Img included just image features;TxtWin local tokens in a ?10 window around the target im-age anchor; BodyTxt global tokens in the page BODY; andBaseline uses the most common sense.
Text performed bet-ter than image features, and global text appeared better thanlocal.
All features performed above the baseline.Median and range results are reported for fiveruns, given each condition, comparing against thebaseline (i.e.
choosing the most common sense).Table 2 shows that purity was surprisingly good,stable across query terms, and that it was high-est when only core sense data was considered.
Inaddition, purity tended to be slightly higher forBASS, which may be related to the annotator beingless confident about its fine-grained sense distinc-tions, and thus less strict for assigning core senselabels for this query term.5 In addition, we lookedat the relative performance of individual globaland local features using 20 clusters and only core5A slightly modified HTML extractor yielded similar re-sults (?0-2% median, ?0-5% range cf.
to Tables 2 - 4).551Figure 4: First 30 images from a CRANE BIRD cluster consisting of 81 images in the median run.
Individual cluster purityfor all senses was 0.67, and for meta senses 0.83.
Not all clusters were as pure as this one; global purity for all 40 cluster was0.49.
This cluster appeared to show some iconography; mostly standing cranes.
Interestingly, another cluster contained severalimages of flying cranes.
Most weighted tokens: cranes whooping birds wildlife species.
Table 1 has sense labels.Figure 5: Global purity does not tell the whole story SQUASH VEGETABLE cluster of 22 images in the median run.Individual cluster purity for all senses was 0.5, and for meta senses 1.0.
Global purity for all 40 cluster was 0.52.
This clusterboth shows visually coherent images, and a sensible meta semantic field.
Most weighted tokens: chayote calabaza add bittercup.
Presumably, some tokens reflect the vegetable?s use within the cooking domain.sense data based on a particular feature.
Table 3shows that global text features were most infor-mative (although not homogenously), but also thateach feature type performed better than the base-line in isolation.
This indicates that an optimal fea-ture combination may improve over current per-formance, using manually selected parameters.
Inaddition, purity is not the whole story.
Figs.
4and 5 show examples of two selected interestingclusters obtained for CRANE and SQUASH, respec-tively, using combined image and text features andall individual senses.6 Inspection of image clus-ters indicated that image features, both in isolationand when used in combination, appeared to con-6The UIUC-ISD data set and results are currently athttp://www.visionpc.cs.uiuc.edu/isd/.tribute to more visually balanced clusters, espe-cially in terms of colors and shading.
This showsthat further exploring image features may be vi-tal for attaining more subtle iconographic senses.Moreover, as discussed in the introduction, imagesare not necessarily anchored in the immediate textwhich they refer to.
This could explain why lo-cal text features do not perform as well as globalones.
Lastly, in addition, Fig.
6 shows an exampleof a partial cluster where the algorithm inferred aspecific related sense.We also experimented with different number ofclusters for BASS.
The results are in Table 4, lack-ing a clear trend, with comparable variation to dif-ferent initializations.
This is surprising, since wewould expect purity to increase with number of552Figure 6: RELATED: SQUASH VEGETABLE cluster, consisting of 27 images.
The algorithm discovered a specific SQUASHBUG-PLANT sense, which appears iconographic.
Individual cluster purity for all senses was 0.85, and individual meta purity:1.0.
Global purity for all 40 clusters: 0.52.
Most weighted tokens: bugs bug beetle leaf-footed kentucky.# Clusters 6 10 20 40 80AllMedian 0.61 0.55 0.58 0.60 0.61Range 0.03 0.05 0.03 0.03 0.04MetaMedian 0.75 0.70 0.70 0.73 0.72Range 0.04 0.07 0.04 0.02 0.04Table 4: Impact of cluster size?
We ran BASS for differentnumber of clusters (5 runs each with distinct initializations),and recorded median and range of global purity for all sixsenses of the query term, and for the four meta senses, with-out a clear trend.clusters (Schu?tze, 1998), but may be due to thespectral clustering.
Inspection showed that 6 clus-ters were dominated by core senses, whereas with40 clusters a few were also dominated by RE-LATED senses or PEOPLE.
No cluster was domi-nated by an UNRELATED label, which makes sensesince semantic linkage should be absent betweenunrelated items.5 Comparison to previous workSpace does not allow a complete review of theWSD literature.
(Yarowsky, 1995) demonstratedthat semi-supervised WSD could be successful.
(Schu?tze, 1998) and (Lin and Pantel, 2002a, b)show that clustering methods are helpful in thisarea.While ISD has received less attention, imagecategorization has been approached previouslyby adding text features.
For example, (Frankel,Swain, and Athitsos, 1996)?s WebSeer systemattempted to mutually distinguish photos, hand-drawn, and computer-drawn images, using a com-bination of HTML markup, web page text, and im-age information.
(Yanai and Barnard, 2005) foundthat adding text features could benefit identifyingrelevant web images.
Using text-annotated images(i.e.
images annotated with relevant keywords),(Barnard and Forsyth, 2001) clustered them ex-ploring a semantic hierarchy; similarly (Barnard,Duygulu, and Forsyth, 2002) conducted art clus-tering, and (Barnard and Johnson, 2005) used text-annotated images to improve WSD.
The latter pa-per obtained best results when combining text andimage features, but contrary to our findings, im-age features performed better in isolation than justtext.
They did use a larger set of image featuresand segmentation, however, we suspect that dif-ferences can rather be attributed to corpus type.
Infact, (Yanai, Shirahatti, and Barnard, 2005) notedthat human evaluators rated images obtained viaa keyword retrieval method higher compared toimage-based retrieval methods, which they relateto the importance of semantics for what humansregard as matching, and because pictorial seman-tics is hard to detect.
(Cai et al 2004) use similar methods to rankvisual search results.
While their work does notfocus explicitly on sense and does not provide in-depth discussion of visual sense phenomena, thesedo appear in, for example, figs.
7 and 9 of their pa-per.
An interesting aspect of their work is the useof page layout segmentation to associate text withimages in web documents.
Unfortunately, the au-553thors only provide an illustrative query example,and no numerical evaluation, making any com-parison difficult.
(Wang et al 2004) use similarfeatures with the goal to improve image retrievalthrough similarity propagation, querying specificweb sites.
(Fuji and Ishikawa, 2005) deal withimage ambiguity for establishing an online mul-timedia encyclopedia, but their method does notintegrate image features, and appears to dependon previous encyclopedic background knowledge,limited to a domain set.6 ConclusionIt is remarkable how high purity is, consideringthat we are using relatively simple image and textrepresentation.
In most corpora used to date for re-search on illustrated text, word sense is an entirelysecondary phenomenon, whereas our data set wascollected as to emphasize possible ambiguities as-sociated with word sense.
Our results suggest thata surprisingly degree of the meaning of an illus-trated object is exposed on the surface.This work is an initial attempt at addressingthe ISD problem.
Future work will involve learn-ing the algorithm?s parameters without supervi-sion, and develop a semantically meaningful im-age taxonomy.
In particular, we intend to explorethe notion of iconographic senses; surprisinglygood results on image classification by (Chapelle,Haffner, and Vapnik, 1999) using image featuressuggest that iconography plays an important rolein the semantics of images.
An important aspectis to enhance our understanding of the interplaybetween text and image features for this purpose.Also, it remains an unsolved problem how to enu-merate iconographic senses, and use them in man-ual annotation and classification.
Experimentalwork with humans performing similar tasks mayprovide increased insight into this issue, and canalso be used to validate clustering performance.7 AcknowledgementsWe are grateful to Roxana Girju and RichardSproat for helpful feedback, and to AlexanderSorokin.ReferencesC.
O. Alm, N. Loeff, and D. Forsyth.
2006.
Challenges forannotating images for sense disambiguation.
ACL work-shop on Frontiers in Linguistically Annotated Corpora.K.
Barnard and D. Forsyth.
2001.
Learning the semantics ofwords and pictures.
ICCV, 408?415.K.
Barnard, P. Duygulu, and D. Forsyth.
2002.
Modeling thestatistics of image features and associated text.
SPIE.K.
Barnard and M. Johnson.
2005.
Word sense disambigua-tion with pictures.
Artificial Intelligence, 167, 13?30.D.
Cai et al 2004.
Hierarchical clustering of WWW imagesearch results using visual, textual and link information.ACM Multimedia, 952-959.O.
Chapelle and P. Haffner and V. Vapnik.
1999.
Supportvector machines for histogram-based image classification.IEEE Neural Networks, 10(5), 1055?1064.G.
Csurka et al 2004.
Visual categorization with bagsof keypoints.
ECCV Int.
Workshop on Stat.
Learning inComputer Vision.C.
Frankel, M. Swain, and V. Athitsos.
1996.
WebSeer: animage search engine for the World Wide Web.
Univ.
ofChicago, Computer Science, Technical report #96-14.C.
Fowlkes, S. Belongie, F. Chung, and J. Malik.
2004.Spectral grouping using the Nystro?m method.
IEEEPAMI, 26(2),214-225.A.
Fuji and T. Ishikawa.
2005.
Toward the automatic com-pilation of multimedia encyclopedias: associating imageswith term descriptions on the web.
IEEE WI, 536-542.D.
Jurafsky and J. Martin 2000.
Speech and Language Pro-cessing, Prentice Hall.T.
Kadir and M. Brady.
2001.
Scale, saliency and imagedescription.
Int.
Journal of Computer Vision, 45 (2):83?105.D.
Lin and P. Pantel.
2002a.
Concept discovery from text.COLING, 577?583.D.
Lowe.
2004.
Distinctive image features from scale-invariant keypoints.
Int.
Journal of Computer Vision,60(2), 91?110.A.
Ng, M. Jordan, and Y. Weiss.
2002.
On spectral cluster-ing: analysis and an algorithm.
NIPS 14.P.
Pantel and D. Lin.
2002b.
Discovering word senses fromtext.
KDD, 613?619.H.
Schuetze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?123.J.
Shi and J. Malik.
2000.
Normalized cuts and image seg-mentation.
IEEE PAMI, 22(8):888?905.N.
Ueda.
and R. Nakano.
1994.
A new competitive learn-ing approach based on an equidistortion principle fordesigning optimal vector quantizers.
Neural Networks,7(8):1211?1227.X.-J.
Wang et al 2004.
Multi-model similarity propagationand its application for image retrieval.
MM,944?951.K.
Yanai and K. Barnard.
2005.
Probabilistic web imagegathering.
SIGMM, 57?64.K.
Yanai, N. V. Shirahatti, and K. Barnard.
2005.
Evaluationstrategies for image understanding and retrieval.
SIGMM,217-226.D.
Yarowsky.
1995.
Unsupervised word sense disambigua-tion rivaling supervised methods.
ACL, 189?196.554
