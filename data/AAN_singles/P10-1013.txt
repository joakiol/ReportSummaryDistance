Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118?127,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsOpen Information Extraction using WikipediaFei WuUniversity of WashingtonSeattle, WA, USAwufei@cs.washington.eduDaniel S. WeldUniversity of WashingtonSeattle, WA, USAweld@cs.washington.eduAbstractInformation-extraction (IE) systems seekto distill semantic relations from natural-language text, but most systems use super-vised learning of relation-specific examplesand are thus limited by the availability oftraining data.
Open IE systems such asTextRunner, on the other hand, aim to handlethe unbounded number of relations foundon the Web.
But how well can these opensystems perform?This paper presents WOE, an open IE systemwhich improves dramatically on TextRunner?sprecision and recall.
The key to WOE?s per-formance is a novel form of self-supervisedlearning for open extractors ?
using heuris-tic matches between Wikipedia infobox at-tribute values and corresponding sentences toconstruct training data.
Like TextRunner,WOE?s extractor eschews lexicalized featuresand handles an unbounded set of semanticrelations.
WOE can operate in two modes:when restricted to POS tag features, it runsas quickly as TextRunner, but when set to usedependency-parse features its precision andrecall rise even higher.1 IntroductionThe problem of information-extraction (IE), gen-erating relational data from natural-language text,has received increasing attention in recent years.A large, high-quality repository of extracted tu-ples can potentially benefit a wide range of NLPtasks such as question answering, ontology learn-ing, and summarization.
The vast majority ofIE work uses supervised learning of relation-specific examples.
For example, the WebKBproject (Craven et al, 1998) used labeled exam-ples of the courses-taught-by relation to in-duce rules for identifying additional instances ofthe relation.
While these methods can achievehigh precision and recall, they are limited by theavailability of training data and are unlikely toscale to the thousands of relations found in texton the Web.An alternative paradigm, Open IE, pioneeredby the TextRunner system (Banko et al, 2007)and the ?preemptive IE?
in (Shinyama and Sekine,2006), aims to handle an unbounded number ofrelations and run quickly enough to process Web-scale corpora.
Domain independence is achievedby extracting the relation name as well as itstwo arguments.
Most open IE systems use self-supervised learning, in which automatic heuristicsgenerate labeled data for training the extractor.
Forexample, TextRunner uses a small set of hand-written rules to heuristically label training exam-ples from sentences in the Penn Treebank.This paper presents WOE (Wikipedia-basedOpen Extractor), the first system that au-tonomously transfers knowledge from random ed-itors?
effort of collaboratively editing Wikipedia totrain an open information extractor.
Specifically,WOE generates relation-specific training examplesby matching Infobox1 attribute values to corre-sponding sentences (as done in Kylin (Wu andWeld, 2007) and Luchs (Hoffmann et al, 2010)),but WOE abstracts these examples to relation-independent training data to learn an unlexical-ized extractor, akin to that of TextRunner.
WOEcan operate in two modes: when restricted toshallow features like part-of-speech (POS) tags, itruns as quickly as Textrunner, but when set to usedependency-parse features its precision and recallrise even higher.
We present a thorough experi-mental evaluation, making the following contribu-tions:?
We present WOE, a new approach to open IEthat uses Wikipedia for self-supervised learn-1An infobox is a set of tuples summarizing the key at-tributes of the subject in a Wikipedia article.
For example,the infobox in the article on ?Sweden?
contains attributes likeCapital, Population and GDP.118ing of unlexicalized extractors.
Comparedwith TextRunner (the state of the art) on threecorpora, WOE yields between 72% and 91%improved F-measure ?
generalizing well be-yond Wikipedia.?
Using the same learning algorithm and fea-tures as TextRunner, we compare four dif-ferent ways to generate positive and negativetraining data with TextRunner?s method, con-cluding that our Wikipedia heuristic is respon-sible for the bulk of WOE?s improved accuracy.?
The biggest win arises from using parser fea-tures.
Previous work (Jiang and Zhai, 2007)concluded that parser-based features are un-necessary for information extraction, but thatwork assumed the presence of lexical features.We show that abstract dependency paths area highly informative feature when performingunlexicalized extraction.2 Problem DefinitionAn open information extractor is a functionfrom a document, d, to a set of triples,{?arg1,rel,arg2?
}, where the args are nounphrases and rel is a textual fragment indicat-ing an implicit, semantic relation between the twonoun phrases.
The extractor should produce onetriple for every relation stated explicitly in the text,but is not required to infer implicit facts.
In thispaper, we assume that all relational instances arestated within a single sentence.
Note the dif-ference between open IE and the traditional ap-proaches (e.g., as in WebKB), where the task isto decide whether some pre-defined relation holdsbetween (two) arguments in the sentence.We wish to learn an open extractor without di-rect supervision, i.e.
without annotated trainingexamples or hand-crafted patterns.
Our input isWikipedia, a collaboratively-constructed encyclo-pedia2.
As output, WOE produces an unlexicalizedand relation-independent open extractor.
Our ob-jective is an extractor which generalizes beyondWikipedia, handling other corpora such as the gen-eral Web.3 Wikipedia-based Open IEThe key idea underlying WOE is the automaticconstruction of training examples by heuristicallymatching Wikipedia infobox values and corre-sponding text; these examples are used to generate2We also use DBpedia (Auer and Lehmann, 2007) as acollection of conveniently parsed Wikipedia infoboxes											 	Figure 1: Architecture of WOE.an unlexicalized, relation-independent (open) ex-tractor.
As shown in Figure 1, WOE has three maincomponents: preprocessor, matcher, and learner.3.1 PreprocessorThe preprocessor converts the raw Wikipedia textinto a sequence of sentences, attaches NLP anno-tations, and builds synonym sets for key entities.The resulting data is fed to the matcher, describedin Section 3.2, which generates the training set.Sentence Splitting: The preprocessor first renderseach Wikipedia article into HTML, then splits thearticle into sentences using OpenNLP.NLP Annotation: As we discuss fully in Sec-tion 4 (Experiments), we consider several varia-tions of our system; one version, WOEparse, usesparser-based features, while another, WOEpos, usesshallow features like POS tags, which may bemore quickly computed.
Depending on whichversion is being trained, the preprocessor usesOpenNLP to supply POS tags and NP-chunk an-notations ?
or uses the Stanford Parser to create adependency parse.
When parsing, we force the hy-perlinked anchor texts to be a single token by con-necting the words with an underscore; this trans-formation improves parsing performance in manycases.Compiling Synonyms: As a final step, the pre-processor builds sets of synonyms to help thematcher find sentences that correspond to infoboxrelations.
This is useful because Wikipedia edi-tors frequently use multiple names for an entity;for example, in the article titled ?University ofWashington?
the token ?UW?
is widely used torefer the university.
Additionally, attribute valuesare often described differently within the infoboxthan they are in surrounding text.
Without knowl-edge of these synonyms, it is impossible to con-struct good matches.
Following (Wu and Weld,2007; Nakayama and Nishio, 2008), the prepro-cessor uses Wikipedia redirection pages and back-119ward links to automatically construct synonymsets.
Redirection pages are a natural choice, be-cause they explicitly encode synonyms; for ex-ample, ?USA?
is redirected to the article on the?United States.?
Backward links for a Wiki-pedia entity such as the ?Massachusetts Institute ofTechnology?
are hyperlinks pointing to this entityfrom other articles; the anchor text of such links(e.g., ?MIT?)
forms another source of synonyms.3.2 MatcherThe matcher constructs training data for thelearner component by heuristically matchingattribute-value pairs from Wikipedia articles con-taining infoboxes with corresponding sentences inthe article.
Given the article on ?Stanford Univer-sity,?
for example, the matcher should associate?established,1891?
with the sentence ?Theuniversity was founded in 1891 by .
.
.
?
Given aWikipedia page with an infobox, the matcher iter-ates through all its attributes looking for a uniquesentence that contains references to both the sub-ject of the article and the attribute value; thesenoun phrases will be annotated arg1 and arg2in the training set.
The matcher considers a sen-tence to contain the attribute value if the value orits synonym is present.
Matching the article sub-ject, however, is more involved.Matching Primary Entities: In order to matchshorthand terms like ?MIT?
with more completenames, the matcher uses an ordered set of heuris-tics like those of (Wu and Weld, 2007; Nguyen etal., 2007):?
Full match: strings matching the full name ofthe entity are selected.?
Synonym set match: strings appearing in theentity?s synonym set are selected.?
Partial match: strings matching a prefix or suf-fix of the entity?s name are selected.
If thefull name contains punctuation, only a prefixis allowed.
For example, ?Amherst?
matches?Amherst, Mass,?
but ?Mass?
does not.?
Patterns of ?the <type>?
: The matcher firstidentifies the type of the entity (e.g., ?city?
for?Ithaca?
), then instantiates the pattern to createthe string ?the city.?
Since the first sentence ofmost Wikipedia articles is stylized (e.g.
?Thecity of Ithaca sits .
.
.
?
), a few patterns sufficeto extract most entity types.?
The most frequent pronoun: The matcher as-sumes that the article?s most frequent pronoundenotes the primary entity, e.g., ?he?
for thepage on ?Albert Einstein.?
This heuristic isdropped when ?it?
is most common, becausethe word is used in too many other ways.When there are multiple matches to the primaryentity in a sentence, the matcher picks the onewhich is closest to the matched infobox attributevalue in the parser dependency graph.Matching Sentences: The matcher seeks a uniquesentence to match the attribute value.
To producethe best training set, the matcher performs threefilterings.
First, it skips the attribute completelywhen multiple sentences mention the value or itssynonym.
Second, it rejects the sentence if thesubject and/or attribute value are not heads of thenoun phrases containing them.
Third, it discardsthe sentence if the subject and the attribute valuedo not appear in the same clause (or in parent/childclauses) in the parse tree.Since Wikipedia?s Wikimarkup language is se-mantically ambiguous, parsing infoboxes is sur-prisingly complex.
Fortunately, DBpedia (Auerand Lehmann, 2007) provides a cleaned set of in-foboxes from 1,027,744 articles.
The matcher usesthis data for attribute values, generating a trainingdataset with a total of 301,962 labeled sentences.3.3 Learning ExtractorsWe learn two kinds of extractors, one (WOEparse)using features from dependency-parse trees andthe other (WOEpos) limited to shallow features likePOS tags.
WOEparse uses a pattern learner toclassify whether the shortest dependency path be-tween two noun phrases indicates a semantic rela-tion.
In contrast, WOEpos (like TextRunner) trainsa conditional random field (CRF) to output certaintext between noun phrases when the text denotessuch a relation.
Neither extractor uses individualwords or lexical information for features.3.3.1 Extraction with Parser FeaturesDespite some evidence that parser-based featureshave limited utility in IE (Jiang and Zhai, 2007),we hoped dependency paths would improve preci-sion on long sentences.Shortest Dependency Path as Relation: Unlessotherwise noted, WOE uses the Stanford Parserto create dependencies in the ?collapsedDepen-dency?
format.
Dependencies involving preposi-tions, conjuncts as well as information about thereferent of relative clauses are collapsed to getdirect dependencies between content words.
As120noted in (de Marneffe and Manning, 2008), thiscollapsed format often yields simplified patternswhich are useful for relation extraction.
Considerthe sentence:Dan was not born in Berkeley.The Stanford Parser dependencies are:nsubjpass(born-4, Dan-1)auxpass(born-4, was-2)neg(born-4, not-3)prep in(born-4, Berkeley-6)where each atomic formula represents a binary de-pendence from dependent token to the governortoken.These dependencies form a directed graph,?V,E?, where each token is a vertex in V , and Eis the set of dependencies.
For any pair of tokens,such as ?Dan?
and ?Berkeley?, we use the shortestconnecting path to represent the possible relationbetween them:Dan?????????
?nsubjpass born??????
?prep in BerkeleyWe call such a path a corePath.
While we willsee that corePaths are useful for indicating whena relation exists between tokens, they don?t neces-sarily capture the semantics of that relation.
Forexample, the path shown above doesn?t indicatethe existence of negation!
In order to capture themeaning of the relation, the learner augments thecorePath into a tree by adding all adverbial andadjectival modifiers as well as dependencies like?neg?
and ?auxpass?.
We call the result an ex-pandPath as shown below:WOE traverses the expandPath with respect to thetoken orders in the original sentence when out-putting the final expression of rel.Building a Database of Patterns: For each of the301,962 sentences selected and annotated by thematcher, the learner generates a corePath betweenthe tokens denoting the subject and the infobox at-tribute value.
Since we are interested in eventu-ally extracting ?subject, relation, object?
triples,the learner rejects corePaths that don?t start withsubject-like dependencies, such as nsubj, nsubj-pass, partmod and rcmod.
This leads to a collec-tion of 259,046 corePaths.To combat data sparsity and improve learn-ing performance, the learner further generalizesthe corePaths in this set to create a smaller setof generalized-corePaths.
The idea is to elimi-nate distinctions which are irrelevant for recog-nizing (domain-independent) relations.
Lexicalwords in corePaths are replaced with their POStags.
Further, all Noun POS tags and ?PRP?are abstracted to ?N?, all Verb POS tags to ?V?,all Adverb POS tags to ?RB?
and all AdjectivePOS tags to ?J?.
The preposition dependenciessuch as ?prep in?
are generalized to ?prep?.
Takethe corePath ?Dan?????????
?nsubjpass born??????
?prep inBerkeley?
for example, its generalized-corePathis ?N?????????
?nsubjpass V ????
?prep N?.
We call sucha generalized-corePath an extraction pattern.
Intotal, WOE builds a database (named DBp) of15,333 distinct patterns and each pattern p is asso-ciated with a frequency ?
the number of matchingsentences containing p. Specifically, 185 patternshave fp ?
100 and 1929 patterns have fp ?
5.Learning a Pattern Classifier: Given the largenumber of patterns in DBp, we assume few validopen extraction patterns are left behind.
Thelearner builds a simple pattern classifier, namedWOEparse, which checks whether the generalized-corePath from a test triple is present in DBp, andcomputes the normalized logarithmic frequency asthe probability3:w(p) =max(log(fp)?
log(fmin), 0)log(fmax)?
log(fmin)where fmax (50,259 in this paper) is the maximalfrequency of pattern in DBp, and fmin (set 1 inthis work) is the controlling threshold that deter-mines the minimal frequency of a valid pattern.Take the previous sentence ?Dan was not bornin Berkeley?
for example.
WOEparse first identi-fies Dan as arg1 and Berkeley as arg2 basedon NP-chunking.
It then computes the corePath?Dan?????????
?nsubjpass born??????
?prep in Berkeley?and abstracts to p=?N?????????
?nsubjpass V ?????prepN?.
It then queries DBp to retrieve the fre-quency fp = 29112 and assigns a probabil-ity of 0.95.
Finally, WOEparse traverses thetriple?s expandPath to output the final expression?Dan,wasNotBornIn,Berkeley?.
As shownin the experiments on three corpora, WOEparseachieves an F-measure which is between 72% to91% greater than TextRunner?s.3.3.2 Extraction with Shallow FeaturesWOEparse has a dramatic performance improve-ment over TextRunner.
However, the improve-ment comes at the cost of speed ?
TextRunner3How to learn a more sophisticated weighting function isleft as a future topic.1210.0 0.1 0.2 0.3 0.4 0.5 0.60.00.20.40.60.81.0recallprecisionP/R Curve on WSJWOEparseWOEposTextRunner0.0 0.1 0.2 0.3 0.4 0.5 0.60.00.20.40.60.81.0recallprecisionP/R Curve on WebWOEparseWOEposTextRunner0.0 0.1 0.2 0.3 0.4 0.5 0.60.00.20.40.60.81.0recallprecisionP/R Curve on WikipediaWOEparseWOEposTextRunnerFigure 2: WOEposperforms better than TextRunner, especially on precision.
WOEparsedramatically im-proves performance, especially on recall.runs about 30X faster by only using shallow fea-tures.
Since high speed can be crucial when pro-cessing Web-scale corpora, we additionally learn aCRF extractor WOEpos based on shallow featureslike POS-tags.
In both cases, however, we gen-erate training data from Wikipedia by matchingsentences with infoboxes, while TextRunner useda small set of hand-written rules to label trainingexamples from the Penn Treebank.We use the same matching sentence set behindDBp to generate positive examples for WOEpos.Specifically, for each matching sentence, we labelthe subject and infobox attribute value as arg1and arg2 to serve as the ends of a linear CRFchain.
Tokens involved in the expandPath are la-beled as rel.
Negative examples are generatedfrom random noun-phrase pairs in other sentenceswhen their generalized-CorePaths are not in DBp.WOEpos uses the same learning algorithm andselection of features as TextRunner: a two-orderCRF chain model is trained with the Mallet pack-age (McCallum, 2002).
WOEpos?s features includePOS-tags, regular expressions (e.g., for detectingcapitalization, punctuation, etc..), and conjunc-tions of features occurring in adjacent positionswithin six words to the left and to the right of thecurrent word.As shown in the experiments, WOEpos achievesan improved F-measure over TextRunner between18% to 34% on three corpora, and this is mainlydue to the increase on precision.4 ExperimentsWe used three corpora for experiments: WSJ fromPenn Treebank, Wikipedia, and the general Web.For each dataset, we randomly selected 300 sen-tences.
Each sentence was examined by two peo-ple to label all reasonable triples.
These candidatetriples are mixed with pseudo-negative ones andsubmitted to Amazon Mechanical Turk for veri-fication.
Each triple was examined by 5 Turk-ers.
We mark a triple?s final label as positive whenmore than 3 Turkers marked them as positive.4.1 Overall Performance AnalysisIn this section, we compare the overall perfor-mance of WOEparse, WOEpos and TextRunner(shared by the Turing Center at the University ofWashington).
In particular, we are going to answerthe following questions: 1) How do these systemsperform against each other?
2) How does perfor-mance vary w.r.t.
sentence length?
3) How doesextraction speed vary w.r.t.
sentence length?Overall Performance ComparisonThe detailed P/R curves are shown in Figure 2.To have a close look, for each corpus, we ran-domly divided the 300 sentences into 5 groups andcompared the best F-measures of three systems inFigure 3.
We can see that:?
WOEpos is better than TextRunner, especiallyon precision.
This is due to better trainingdata from Wikipedia via self-supervision.
Sec-tion 4.2 discusses this in more detail.?
WOEparse achieves the best performance, es-pecially on recall.
This is because the parserfeatures help to handle complicated and long-distance relations in difficult sentences.
In par-ticular, WOEparse outputs 1.42 triples per sen-tence on average, while WOEpos outputs 1.05and TextRunner outputs 0.75.Note that we measure TextRunner?s precision& recall differently than (Banko et al, 2007)did.
Specifically, we compute the precision & re-call based on all extractions, while Banko et alcounted only concrete triples where arg1 is aproper noun, arg2 is a proper noun or date, and122Figure 3: WOEposachieves an F-measure, which isbetween 18% and 34% better than TextRunner?s.WOEparseachieves an improvement between 72%and 91% over TextRunner.
The error bar indicatesone standard deviation.the frequency of rel is over a threshold.
Our ex-periments show that focussing on concrete triplesgenerally improves precision at the expense of re-call.4 Of course, one can apply a concreteness fil-ter to any open extractor in order to trade recall forprecision.The extraction errors by WOEparse can be cat-egorized into four classes.
We illustrate themwith the WSJ corpus.
In total, WOEparse got85 wrong extractions on WSJ, and they arecaused by: 1) Incorrect arg1 and/or arg2from NP-Chunking (18.6%); 2) A erroneous de-pendency parse from Stanford Parser (11.9%);3) Inaccurate meaning (27.1%) ?
for exam-ple, ?she, isNominatedBy, PresidentBush?
iswrongly extracted from the sentence ?If she isnominated by President Bush ...?5; 4) A patterninapplicable for the test sentence (42.4%).Note WOEparse is worse than WOEpos in the lowrecall region.
This is mainly due to parsing er-rors (especially on long-distance dependencies),which misleads WOEparse to extract false high-confidence triples.
WOEpos won?t suffer from suchparsing errors.
Therefore it has better precision onhigh-confidence extractions.We noticed that TextRunner has a dip pointin the low recall region.
There are two typicalerrors responsible for this.
A sample error ofthe first type is ?Sources, sold, theCompany?extracted from the sentence ?Sources said4For example, consider the Wikipedia corpus.
Fromour 300 test sentences, TextRunner extracted 257 triples (at72.0% precision) but only extracted 16 concrete triples (with87.5% precision).5These kind of errors might be excluded by monitor-ing whether sentences contain words such as ?if,?
?suspect,??doubt,?
etc.. We leave this as a topic for the future.Figure 4: WOEparse?s F-measure decreases moreslowly with sentence length than WOEpos and Tex-tRunner, due to its better handling of difficult sen-tences using parser features.he sold the company?, where ?Sources?
iswrongly treated as the subject of the objectclause.
A sample error of the second type is?thisY ear, willStarIn, theMovie?
extractedfrom the sentence ?Coming up this year, Longwill star in the new movie.
?, where ?this year?
iswrongly treated as part of a compound subject.Taking the WSJ corpus for example, at the dippoint with recall=0.002 and precision=0.059,these two types of errors account for 70% of allerrors.Extraction Performance vs.
Sentence LengthWe tested how extractors?
performance varieswith sentence length; the results are shown in Fig-ure 4.
TextRunner and WOEpos have good perfor-mance on short sentences, but their performancedeteriorates quickly as sentences get longer.
Thisis because long sentences tend to have compli-cated and long-distance relations which are diffi-cult for shallow features to capture.
In contrast,WOEparse?s performance decreases more slowlyw.r.t.
sentence length.
This is mainly becauseparser features are more useful for handling diffi-cult sentences and they help WOEparse to maintaina good recall with only moderate loss of precision.Extraction Speed vs.
Sentence LengthWe also tested the extraction speed of differentextractors.
We used Java for implementing theextractors, and tested on a Linux platform witha 2.4GHz CPU and 4G memory.
On average, ittakes WOEparse 0.679 seconds to process a sen-tence.
For TextRunner and WOEpos, it only takes0.022 seconds ?
30X times faster.
The detailedextraction speed vs. sentence length is in Figure 5,showing that TextRunner and WOEpos?s extractiontime grows approximately linearly with sentencelength, while WOEparse?s extraction time grows123Figure 5: Textrnner and WOEpos?s running timeseems to grow linearly with sentence length, whileWOEparse?s time grows quadratically.quadratically (R2 = 0.935) due to its reliance onparsing.4.2 Self-supervision with Wikipedia Resultsin Better Training DataIn this section, we consider how the process ofmatching Wikipedia infobox values to correspond-ing sentences results in better training data thanthe hand-written rules used by TextRunner.To compare with TextRunner, we tested fourdifferent ways to generate training examples fromWikipedia for learning a CRF extractor.
Specif-ically, positive and/or negative examples are se-lected by TextRunner?s hand-written rules (tr forshort), by WOE?s heuristic of matching sentenceswith infoboxes (w for short), or randomly (r forshort).
We use CRF+h1?h2 to denote a particu-lar approach, where ?+?
means positive samples,?-?
means negative samples, and hi ?
{tr, w, r}.In particular, ?+w?
results in 221,205 positive ex-amples based on the matching sentence set6.
Allextractors are trained using about the same num-ber of positive and negative examples.
In contrast,TextRunner was trained with 91,687 positive ex-amples and 96,795 negative examples generatedfrom the WSJ dataset in Penn Treebank.The CRF extractors are trained using the samelearning algorithm and feature selection as Tex-tRunner.
The detailed P/R curves are in Fig-ure 6, showing that using WOE heuristics to la-bel positive examples gives the biggest perfor-mance boost.
CRF+tr?tr (trained using TextRun-ner?s heuristics) is slightly worse than TextRunner.Most likely, this is because TextRunner?s heuris-tics rely on parse trees to label training examples,6This number is smaller than the total number ofcorePaths (259,046) because we require arg1 to appear be-fore arg2 in a sentence ?
as specified by TextRunner.and the Stanford parse on Wikipedia is less accu-rate than the gold parse on WSJ.4.3 Design Desiderata of WOEparseThere are two interesting design choices inWOEparse: 1) whether to require arg1 to appearbefore arg2 (denoted as 1?2) in the sentence;2) whether to allow corePaths to contain prepo-sitional phrase (PP) attachments (denoted as PPa).We tested how they affect the extraction perfor-mance; the results are shown in Figure 7.We can see that filtering PP attachments (PPa)gives a large precision boost with a noticeable lossin recall; enforcing a lexical ordering of relationarguments (1?2) yields a smaller improvement inprecision with small loss in recall.
Take the WSJcorpus for example: setting 1?2 and PPa achievesa precision of 0.792 (with recall of 0.558).
Bychanging 1?2 to 1?2, the precision decreases to0.773 (with recall of 0.595).
By changing PPa toPPa and keeping 1?2, the precision decreases to0.642 (with recall of 0.687) ?
in particular, if weuse gold parse, the precision decreases to 0.672(with recall of 0.685).
We set 1?2 and PPa as de-fault in WOEparse as a logical consequence of ourpreference for high precision over high recall.4.3.1 Different parsing optionsWe also tested how different parsing might ef-fect WOEparse?s performance.
We used three pars-ing options on the WSJ dataset: Stanford parsing,CJ50 parsing (Charniak and Johnson, 2005), andthe gold parses from the Penn Treebank.
The Stan-ford Parser is used to derive dependencies fromCJ50 and gold parse trees.
Figure 8 shows thedetailed P/R curves.
We can see that althoughtoday?s statistical parsers make errors, they havenegligible effect on the accuracy of WOE.5 Related WorkOpen or Traditional Information Extraction:Most existing work on IE is relation-specific.Occurrence-statistical models (Agichtein and Gra-vano, 2000; M. Ciaramita, 2005), graphical mod-els (Peng and McCallum, 2004; Poon and Domin-gos, 2008), and kernel-based methods (Bunescuand R.Mooney, 2005) have been studied.
Snowet al (Snow et al, 2005) utilize WordNet tolearn dependency path patterns for extracting thehypernym relation from text.
Some seed-basedframeworks are proposed for open-domain extrac-tion (Pasca, 2008; Davidov et al, 2007; Davi-dov and Rappoport, 2008).
These works focus1240.0 0.1 0.2 0.3 0.40.00.20.40.60.81.0recallprecisionP/R Curve on WSJCRF+w?w=WOEposCRF+w?trCRF+w?rCRF+tr?trTextRunner 0.0 0.1 0.2 0.3 0.40.00.20.40.60.81.0recallprecisionP/R Curve on WebCRF+w?w=WOEposCRF+w?trCRF+w?rCRF+tr?trTextRunner 0.0 0.1 0.2 0.3 0.40.00.20.40.60.81.0recallprecisionP/R Curve on WikipediaCRF+w?w=WOEposCRF+w?trCRF+w?rCRF+tr?trTextRunnerFigure 6: Matching sentences with Wikipedia infoboxes results in better training data than the hand-written rules used by TextRunner.Figure 7: Filtering prepositional phrase attachments (PPa) shows a strong boost to precision, and we seea smaller boost from enforcing a lexical ordering of relation arguments (1?2).0.0 0.1 0.2 0.3 0.4 0.5 0.60.40.60.81.0recallprecisionP/R Curve on WSJWOEstanfordparse =WOEparseWOECJ50parseWOEgoldparseFigure 8: Although today?s statistical parsersmake errors, they have negligible effect on theaccuracy of WOE compared to operation on goldstandard, human-annotated data.on identifying general relations such as class at-tributes, while open IE aims to extract relationinstances from given sentences.
Another seed-based system StatSnowball (Zhu et al, 2009)can perform both relation-specific and open IEby iteratively generating weighted extraction pat-terns.
Different from WOE, StatSnowball only em-ploys shallow features and uses L1-normalizationto weight patterns.
Shinyama and Sekine pro-posed the ?preemptive IE?
framework to avoidrelation-specificity (Shinyama and Sekine, 2006).They first group documents based on pairwisevector-space clustering, then apply an additionalclustering to group entities based on documentsclusters.
The two clustering steps make it dif-ficult to meet the scalability requirement neces-sary to process the Web.
Mintz et al (Mintz etal., 2009) uses Freebase to provide distant su-pervision for relation extraction.
They applieda similar heuristic by matching Freebase tupleswith unstructured sentences (Wikipedia articles intheir experiments) to create features for learningrelation extractors.
Matching Freebase with ar-bitrary sentences instead of matching Wikipediainfobox with corresponding Wikipedia articleswill potentially increase the size of matched sen-tences at a cost of accuracy.
Also, their learnedextractors are relation-specific.
Alan Akbik etal.
(Akbik and Bro?, 2009) annotated 10,000 sen-tences parsed with LinkGrammar and selected 46general linkpaths as patterns for relation extrac-tion.
In contrast, WOE learns 15,333 general pat-terns based on an automatically annotated set of125301,962 Wikipedia sentences.
The KNext sys-tem (Durme and Schubert, 2008) performs openknowledge extraction via significant heuristics.
Itsoutput is knowledge represented as logical state-ments instead of information represented as seg-mented text fragments.Information Extraction with Wikipedia: TheYAGO system (Suchanek et al, 2007) extendsWordNet using facts extracted from Wikipediacategories.
It only targets a limited number of pre-defined relations.
Nakayama et al (Nakayama andNishio, 2008) parse selected Wikipedia sentencesand perform extraction over the phrase structuretrees based on several handcrafted patterns.
Wuand Weld proposed the KYLIN system (Wu andWeld, 2007; Wu et al, 2008) which has the samespirit of matching Wikipedia sentences with in-foboxes to learn CRF extractors.
However, itonly works for relations defined in Wikipedia in-foboxes.Shallow or Deep Parsing: Shallow features, likePOS tags, enable fast extraction over large-scalecorpora (Davidov et al, 2007; Banko et al, 2007).Deep features are derived from parse trees withthe hope of training better extractors (Zhang etal., 2006; Zhao and Grishman, 2005; Bunescuand Mooney, 2005; Wang, 2008).
Jiang andZhai (Jiang and Zhai, 2007) did a systematic ex-ploration of the feature space for relation extrac-tion on the ACE corpus.
Their results showed lim-ited advantage of parser features over shallow fea-tures for IE.
However, our results imply that ab-stracted dependency path features are highly in-formative for open IE.
There might be several rea-sons for the different observations.
First, Jiang andZhai?s results are tested for traditional IE where lo-cal lexicalized tokens might contain sufficient in-formation to trigger a correct classification.
Thesituation is different when features are completelyunlexicalized in open IE.
Second, as they noted,many relations defined in the ACE corpus areshort-range relations which are easier for shallowfeatures to capture.
In practical corpora like thegeneral Web, many sentences contain complicatedlong-distance relations.
As we have shown ex-perimentally, parser features are more powerful inhandling such cases.6 ConclusionThis paper introduces WOE, a new approach toopen IE that uses self-supervised learning over un-lexicalized features, based on a heuristic matchbetween Wikipedia infoboxes and correspondingtext.
WOE can run in two modes: a CRF extrac-tor (WOEpos) trained with shallow features likePOS tags; a pattern classfier (WOEparse) learnedfrom dependency path patterns.
Comparing withTextRunner, WOEpos runs at the same speed, butachieves an F-measure which is between 18% and34% greater on three corpora; WOEparse achievesan F-measure which is between 72% and 91%higher than that of TextRunner, but runs about30X times slower due to the time required forparsing.Our experiments uncovered two sources ofWOE?s strong performance: 1) the Wikipediaheuristic is responsible for the bulk of WOE?s im-proved accuracy, but 2) dependency-parse featuresare highly informative when performing unlexi-calized extraction.
We note that this second con-clusion disagrees with the findings in (Jiang andZhai, 2007).In the future, we plan to run WOE over the bil-lion document CMU ClueWeb09 corpus to com-pile a giant knowledge base for distribution to theNLP community.
There are several ways to furtherimprove WOE?s performance.
Other data sources,such as Freebase, could be used to create an ad-ditional training dataset via self-supervision.
Forexample, Mintz et al consider all sentences con-taining both the subject and object of a Freebaserecord as matching sentences (Mintz et al, 2009);while they use this data to learn relation-specificextractors, one could also learn an open extrac-tor.
We are also interested in merging lexical-ized and open extraction methods; the use of somedomain-specific lexical features might help to im-prove WOE?s practical performance, but the bestway to do this is unclear.
Finally, we wish to com-bine WOEparse with WOEpos (e.g., with voting) toproduce a system which maximizes precision atlow recall.AcknowledgementsWe thank Oren Etzioni and Michele Banko fromTuring Center at the University of Washington forproviding the code of their software and useful dis-cussions.
We also thank Alan Ritter, Mausam,Peng Dai, Raphael Hoffmann, Xiao Ling, Ste-fan Schoenmackers, Andrey Kolobov and DanielSuskin for valuable comments.
This material isbased upon work supported by the WRF / TJ CableProfessorship, a gift from Google and by the AirForce Research Laboratory (AFRL) under primecontract no.
FA8750-09-C-0181.
Any opinions,126findings, and conclusion or recommendations ex-pressed in this material are those of the author(s)and do not necessarily reflect the view of the AirForce Research Laboratory (AFRL).ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: Ex-tracting relations from large plain-text collections.In ICDL.Alan Akbik and Ju?gen Bro?.
2009.
Wanderlust: Ex-tracting semantic relations from natural languagetext using dependency grammar patterns.
In WWWWorkshop.So?ren Auer and Jens Lehmann.
2007.
What have inns-bruck and leipzig in common?
extracting semanticsfrom wiki content.
In ESWC.M.
Banko, M. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfrom the Web.
In Procs.
of IJCAI.Razvan C. Bunescu and Raymond J. Mooney.
2005.Subsequence kernels for relation extraction.
InNIPS.R.
Bunescu and R.Mooney.
2005.
A shortestpath dependency kernel for relation extraction.
InHLT/EMNLP.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In ACL.M.
Craven, D. DiPasquo, D. Freitag, A. McCallum,T.
Mitchell, K. Nigam, and S. Slattery.
1998.
Learn-ing to extract symbolic knowledge from the worldwide web.
In AAAI.Dmitry Davidov and Ari Rappoport.
2008.
Unsuper-vised discovery of generic relationships using pat-tern clusters and its evaluation by automatically gen-erated sat analogy questions.
In ACL.Dmitry Davidov, Ari Rappoport, and Moshe Koppel.2007.
Fully unsupervised discovery of concept-specific relationships by web mining.
In ACL.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
Stanford typed dependencies manual.http://nlp.stanford.edu/downloads/lex-parser.shtml.Benjamin Van Durme and Lenhart K. Schubert.
2008.Open knowledge extraction using compositionallanguage processing.
In STEP.R.
Hoffmann, C. Zhang, and D. Weld.
2010.
Learning5000 relational extractors.
In ACL.Jing Jiang and ChengXiang Zhai.
2007.
A systematicexploration of the feature space for relation extrac-tion.
In HLT/NAACL.A.
Gangemi M. Ciaramita.
2005.
Unsupervised learn-ing of semantic relations between concepts of amolecular biology ontology.
In IJCAI.Andrew Kachites McCallum.
2002.
Mallet:A machine learning for language toolkit.
Inhttp://mallet.cs.umass.edu.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In ACL-IJCNLP.T.
H. Kotaro Nakayama and S. Nishio.
2008.
Wiki-pedia link structure and text mining for semantic re-lation extraction.
In CEUR Workshop.Dat P.T Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka.2007.
Exploiting syntactic and semantic informa-tion for relation extraction from wikipedia.
InIJCAI07-TextLinkWS.Marius Pasca.
2008.
Turning web text and searchqueries into factual knowledge: Hierarchical classattribute extraction.
In AAAI.Fuchun Peng and Andrew McCallum.
2004.
AccurateInformation Extraction from Research Papers usingConditional Random Fields.
In HLT-NAACL.Hoifung Poon and Pedro Domingos.
2008.
Joint Infer-ence in Information Extraction.
In AAAI.Y.
Shinyama and S. Sekine.
2006.
Preemptive infor-mation extraction using unristricted relation discov-ery.
In HLT-NAACL.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In NIPS.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A core of semantic knowl-edge - unifying WordNet and Wikipedia.
In WWW.Mengqiu Wang.
2008.
A re-examination of depen-dency path kernels for relation extraction.
In IJC-NLP.Fei Wu and Daniel Weld.
2007.
Autonomouslly Se-mantifying Wikipedia.
In CIKM.Fei Wu, Raphael Hoffmann, and Danel S. Weld.
2008.Information extraction from Wikipedia: Movingdown the long tail.
In KDD.Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.2006.
A composite kernel to extract relations be-tween entities with both flat and structured features.In ACL.Shubin Zhao and Ralph Grishman.
2005.
Extractingrelations with integrated information using kernelmethods.
In ACL.Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, andJi-Rong Wen.
2009.
Statsnowball: a statistical ap-proach to extracting entity relationships.
In WWW.127
