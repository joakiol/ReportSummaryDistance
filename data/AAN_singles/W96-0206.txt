Better Language Models with Model MergingThors ten  BrantsUn ivers i tg t  des Saar landes,  Computat iona l  L inguist icsP .O .Box  151150, D-66041 Saarbr i icken,  Germanythorst en@coli, uni- sb.
deAbst ractThis paper investigates model merging, a tech-nique for deriving Markov models from text orspeech corpora.
Models are derived by startingwith a large and specific model and by successi-vely combining states to build smaller and moregeneral models.
We present methods to reducethe time complexity of the algorithm and reporton experiments on deriving language models fora speech recognition task.
The experiments showthe advantage of model merging over the standardbigram approach.
The merged model assigns alower perplexity to the test set and uses consi-derably fewer states.In t roduct ionHidden Markov Models are commonly used forstatistical language models, e.g.
in part-of-speechtagging and speech recognition (Rabiner, 1989).The models need a large set of parameters whichare induced from a (text-) corpus.
The parametersshould be optimal in the sense that the resultingmodels assign high probabilities to seen trainingdata as well as new data that arises in an applica-tion.There are several methods to estimate modelparameters.
The first one is to use each word(type) as a state and estimate the transition pro-babilities between two or three words by using therelative frequencies of a corpus.
This method iscommonly used in speech recognition and knownas word-bigram or word-trigram model.
The re-lative frequencies have to be smoothed to handlethe sparse data problem and to avoid zero proba-bilities.The second method is a variation of thefirst method.
Words are automatically grouped,e.g.
by similarity of distribution in the corpus(Pereira et al, 1993).
The relative frequencies ofpairs or triples of groups (categories, clusters) areused as model parameters, each group is represen-ted by a state in the model.
The second methodhas the advantage of drastically reducing the num-ber of model parameters and thereby reducing thesparse data problem; there is more data per groupthan per word, thus estimates are more precise.The third method uses manually defined ca-tegories.
They are linguistically motivated andusually called parts-of-speech.
An important dif-ference to the second method with automaticallyderived categories i that with the manual defini-tion a word can belong to more than one category.A corpus is (manually) tagged with the catego-ries and transition probabilities between two orthree categories are estimated from their relativefrequencies.
This method is commonly used forpart-of-speech tagging (Church, 1988).The fourth method is a variation of the thirdmethod and is also used for part-of-speech tagging.This method oes not need a pre-annotated corpusfor parameter estimation.
Instead it uses a lexiconstating the possible parts-of-speech for each word,a raw text corpus, and an initial bias for the tran-sition and output probabilities.
The parametersare estimated by using the Baum-Welch algorithm(Baum et al, 1970).
The accuracy of the derivedmodel depends heavily on the initial bias, but witha good choice results are comparable to those ofmethod three (Cutting et al, 1992).This paper investigates a fifth method for esti-mating natural language models, combining theadvantages of the methods mentioned above.
Itis suitable for both speech recognition and part-of-speech tagging, has the advantage of automati-cally deriving word categories from a corpus andis capable of recognizing the fact that a word be-longs to more than one category.
Unlike othertechniques it not only induces transition and out-put probabilities, but also the model topology, i.e.,the number of states, and for each state the out-puts that have a non-zero probability.
The me-thod is called model merging and was introducedby (Omohundro, 1992).The rest of the paper is structured as follows.We first give a short introduction to Markov mo-60dels and present the model merging technique.Then, techniques for reducing the time comple-xity are presented and we report two experimentsusing these techniques.Markov  Mode lsA discrete output, first order Markov Model con-sists of?
a finite set of states QU{qs, qe}, q~, qe ~ Q, withq~ the start state, and q~ the end state;?
a finite output alphabet ~;?
a (IQ\] + 1) ?
(IQ\] + 1) matrix, specifying theprobabilities of state transitions p(q'iq) betweenstates q and q~ (there are no transitions into q~,and no transitions originating in qe); for eachstate q E Q U {qs}, the sum of the outgoingtransition probabilities is 1, ~ p(q'\]q) =qlEQU{qe}1;?
a \]Q\] ?
\[~l matrix, specifying the output proba-bilities p(a\]q) of state q emitting output o'; foreach state q E Q, the sum of the output proba-bilities is 1, ~ p(cr\]q) = 1.aE~A Markov model starts running in the startstate q~, makes a transition at each time step, andstops when reaching the end state qe.
The transi-tion from one state to another is done accordingto the probabilities pecified with the transitions.Each time a state is entered (except he start andend state) one of the outputs is chosen (again ac-cording to their probabilities) and emitted.Ass ign ing  Probab i l i t ies  to  DataFor the rest of the paper, we are interested in theprobabilities which are assigned to sequences ofoutputs by the Markov models.
These can be cal-culated in the following way.Given a model M, a sequence of outputs o =o1 .
.
.
o'k and a sequence of states Q = ql.
?
?
qk (ofsame length), the probability that the model run-ning through the sequence of states and emittingthe given outputs is(/I PM(Q, o') = PM(qilqi-1)PM(o'ilqi PM(qelqi)\i=1(with q0 = qs).
A sequence of outputs can be emit-ted by more than one sequence of states, thus wehave to sum over all sequences of states with thegiven length to get the probability that a modelemits a given sequence of outputs:PM(O') = ~ PM(Q, o').QThe probabilities are calculated very efficientlywith the Viterbi algorithm (Viterbi, 1967).
Itstime complexity is linear to the sequence lengthdespite the exponential growth of the search space.Perp lex i tyMarkov models assign rapidly decreasing probabi-lities to output sequences of increasing length.
Tocompensate for different lengths and to make theirprobabilities comparable, one uses the perplexityPP of an output sequence instead of its probabi-lity.
The perplexity is defined as1PPM(O')- ~v/fi ~The probability is normalized by taking the k throot (k is the length of the sequence).
Similarly,the log perplexity LP is defined:- log PM (o')LPM((r) = log PPM(a) -- kHere, the log probability is normalized by dividingby the length of the sequence.PP and LP are defined such that higher per-plexities (log perplexities, resp.)
correspond tolower probabilities, and vice versa.
These mea-sures are used to determine the quality of Markovmodels.
The lower the perplexity (and log perple-xity) of a test sequence, the higher its probability,and thus the better it is predicted by the model.Mode l  Merg ingModel merging is a technique for inducing mo-del parameters for Markov models from a textcorpus.
It was introduced in (Omohundro, 1992)and (Stolcke and Omohundro, 1994) to inducemodels for regular languages from a few samp-les, and adapted to natural language models in(Brants, 1995).
Unlike other techniques it notonly induces transition and output probabilitiesfrom the corpus, but also the model topology, i.e.,the number of states and for each state the outputsthat have non-zero probability.
In n-gram approa-ches the topology is fixed.
E.g., in a pos-n-grammodel, the states are mostly syntactically moti-vated, each state represents a syntactic categoryand only words belonging to the same categoryhave a non-zero output probability in a particu-lar state.
However the n-gram-models make theimplicit assumption that all words belonging tothe same category have a similar distribution in acorpus.
This is not true in most of the cases.By estimating the topology, model merginggroups words into categories, since all words thatcan be emitted by the same state form a category.The advantage of model merging in this respect61a) a bo~ @ '@c@ .@.
@p(SlM~) = ~ ~ 3.7.10 -2D .
@b)  b5 .
@p(SIMb) = ~ --~ 3.7.10 -2?
)@p(SIM~) = ~ -~ 3.7.10 -2C, ~ 0.67 , ;@0.5d)@ @ 05 .
@~ D0.5p(SIMd ) = ~ ~_ 1.6-10 -2 "'--@ .e) p(SiM~ ) = 2~ ~ 6.6.10 -3 4096 - -b Co ~,0yFigure 1: Model merging for a corpus S = {ab, ac, abac}, start ing with the tr ivial  model  in a) and endingwith the generalization (a(blc)) + in e).
Several steps of merging between model b) and c) are not shown.Unmarked transitions and outputs have probability 1.62is that it can recognize that a word (the type)belongs to more than one category, while each oc-currence (the token) is assigned a unique category.This naturally reflects manual syntactic categori-zations, where a word can belong to several syn-tactic classes but each occurrence of a word is un-ambiguous.The AlgorithmModel merging induces Markov models in the fol-lowing way.
Merging starts with an initial, verygeneral model.
For this purpose, the maximumlikelihood Markov model is chosen, i.e., a modelthat exactly matches the corpus.
There is onepath for each utterance in the corpus and eachpath is used by one utterance only.
Each pathgets the same probability l /u ,  with u the numberof utterances in the corpus.
This model is alsoreferred to as the trivial model.
Figure 1.a showsthe trivial model for a corpus with words a, b, c andutterances ab, ac, abac.
It has one path for each ofthe three utterances ab, ac, and abac, and eachpath gets the same probability 1/3.
The trivialmodel assigns a probability of p(SIM~ ) = 1/27to the corpus.
Since the model makes an im-plicit independence assumption between the ut-terances, the corpus probability is calculated bymultiplying the utterance's probabilities, yielding1 /3 .1 /3 .1 /3  = 1/27.Now states are merged successively, except forthe start and end state.
Two states are selectedand removed and a new merged state is added.The transitions from and to the old states are redi-rected to the new state, the transition probabilitiesare adjusted to maximize the likelihood of the cor-pus; the outputs are joined and their probabilitiesare also adjusted to maximize the likelihood.
Onestep of merging can be seen in figure 1.b.
States 1and 3 are removed, a combined state 1,3 is added,and the probabilities are adjusted.The criterion for selecting states to merge isthe probability of the Markov model generatingthe corpus.
We want this probability to stay ashigh as possible.
Of all possible merges (gene-rally, there are k(k - 1)/2 possible merges, with kthe number of states exclusive start and end statewhich are not allowed to merge) we take the mergethat results in the minimal change of the probabi-lity.
For the trivial model and u pairwise differentutterances the probability is p(SIMtri~) = 1/u ~.The probability either stays constant, as in Figure1.b and c, or decreases, as in 1.d and e. The proba-bility never increases because the trivial model isthe maximum likelihood model, i.e., it maximizesthe probability of the corpus given the model.Model merging stops when a predefinedthreshold for the corpus probability is reached.Some statistically motivated criteria for ter-mination using model priors are discussed in(Stotcke and Omohundro, 1994).Using Model MergingThe model merging algorithm needs several op-timizations to be applicable to large natural lan-guage corpora, otherwise the amount of time nee-ded for deriving the models is too large.
Gene-rally, there are O(l 2) hypothetical merges to betested for each merging step (l is the length of thetraining corpus).
The probability of the trainingcorpus has to be calculated for each hypotheticalmerge, which is O(l) with dynamic programming.Thus, each step of merging is O(13).
If we wantto reduce the model from size l 4- 2 (the trivialmodeli which consists of one state for each tokenplus initial and final states) to some fixed size, weneed O(l) steps of merging.
Therefore, deriving aMarkov model by model merging is O(l 4) in time.
(Stolcke and Omohundro, 1994) discuss se-veral computational shortcuts and approximati-ons:1.
Immediate merging of identical initial and finalstates of different utterances.
These merges donot change the corpus probability and thus arethe first merges anyway.2.
Usage of the Viterbi path (best path) only in-stead of summing up all paths to determine thecorpus probability.3.
The assumption that all input samples retaintheir Viterbi path after merging.
Making thisapproximation, it is no longer necessary to re-parse the whole corpus for each hypotheticalmerge.We use two additional strategies to reduce thetime complexity of the algorithm: a series of cas-caded constraints on the merges and the variationof the starting point.ConstraintsWhen applying model merging one can observethat first mainly states with the same output aremerged.
After several steps of merging, it is nolonger the same output but still mainly states thatoutput words of the same syntactic category aremerged.
This behavior can be exploited by intro-ducing constraints on the merging process.
Theconstraints allow only some of the otherwise pos-sible merges.
Only the allowed merges are testedfor each step of merging.We consider constraints that divide the statesof the current model into equivalence classes.
Onlystates belonging to the same class are allowed tomerge.
E.g., we can divide the states into classes63generating the same outputs.
If the current modelhas N states and we divide them into k > 1 non-empty equivalence classes C1 ... C~, then, insteadof N(N - 1)/2, we have to testk .\[C'l(IC{l- \]) < N(N - 1)2 2i=1merges only.The best case for a model of size N is thedivision into N/2 classes of size 2.
Then, only N/2merges must be tested to find the best merge.The best division into k > 1 classes for somemodel of size N is the creation of classes that allhave the same size N/k (or an approximation ifN/k ~ IN).
Then,N N N(~- - 1) v(v -1 )  .
k -2 2must be tested for each step of merging.Thus, the introduction of these constraintsdoes not reduce the order of the time complexity,but it can reduce the constant factor significantly(see section about experiments).The following equivalence classes can be usedfor constraints when using untagged corpora:1.
States that generate the same outputs (unigramconstraint)2. unigram constraint, and additionally all prede-cessor states must generate the same outputs(bigram constraint)3. trigrams or higher, if the corpora are largeenough4.
a variation of one: states that output words be-longing to one ambiguity class, i.e.
can be of acertain number of syntactic lasses.Merging starts with one of the constraints.
Af-ter a number of merges have been performed, theconstraint is discarded and a weaker one is usedinstead.The standard n-gram approaches are specialcases of using model merging and constraints.E.g., if we use the unigram constraint, and mergestates until no further merge is possible under thisconstraint, the resulting model is a standard bi-gram model, regardless of the order in which themerges were performed.In practice, a constraint will be discarded be-fore no further merge is possible (otherwise themodel could have been derived directly, e.g., bythe standard n-gram technique).
Yet, the que-stion when to discard a constraint to achieve bestresults is unsolved.The Starting PointThe initial model of the original model mergingprocedure is the maximum likelihood or trivialmodel.
This model has the advantage of directlyrepresenting the corpus.
But its disadvantage isits huge number of states.
A lot of computationtime can be saved by choosing an initial modelwith fewer states.The initial model must have two properties:1. it must be larger than the intended model, and2.
it must be easy to construct.The trivial model has both properties.
A class ofmodels that can serve as the initial model as wellare n-gram models.
These models are smaller byone or more orders of magnitude than the trivialmodel and therefore could speed up the derivationof a model significantly.This choice of a starting point excludes a lotof solutions which are allowed when starting withthe maximum likelihood model.
Therefore, star-ting with an n-gram model yields a model that isat most equivalent to one that is generated whenstarting with the trivial model, and that can bemuch worse.
But it should be still better thanany n-gram model that is of lower of equal orderthan the initial model.Exper imentsModel  Merging vs. B igramsThe first experiment compares model mergingwith a standard bigram model.
Both are trai-ned on the same data.
We use Ntra~n -- 14,421words of the Verbmobil corpus.
The corpusconsists of transliterated ialogues on businessappointments 1.
The models are tested on Ntest =2,436 words of the same corpus.
Training and testparts are disjunct.The bigram model yields a Markov model wit h1,440 states.
It assigns a log perplexity of 1.20 tothe training part and 2.40 to the test part.Model merging starts with the maximum like-lihood model for the training part.
It has 14,423states, which correspond to the 14,421 words (plusan initial and a final state).
The initial log per-plexity of the training part is 0.12.
This low valueshows that the initial model is very specialized inthe training part.1Many thanks to the Verbmobil project for pro-viding these data.
We use dialogues that wererecorded in 1993 and 94, and which are nowavailable from the Bavarian Archive for SpeechSignals BAS (http://www'ph?netik'uni-muenchen'de/Bas/BasHomeeng.html).64- log\]o P/Ntrain2.5-2.01.51.00.50 1I14lpdlpconstraint , tc h a n g ~I ' I ~ I ' i ' I ; I ' I ' I ' I ' I ' I ' I i I2 3 4 5 6 7 8 9 10 11 12 13 14 ?10 3 mergesI I I I I I i I I I I I I I13 12 11 10 9 8 7 6 5 4 3 2 1 0 ?10 3 statesFigure 2: Log Perplexity of the training part during merging.
Constraints: same output until 12,500 / noneafter 12,500.
The thin lines show the further development if we retain the the same-output constraint untilno further merge is possible.
The length of the training part is gtrain ---- 14,421.- log10 p/Ntest2.8-2.77-2.6-2.5-2.42.3-2.2-0i14I ' I I ' I ' I ' I ' \[ ' I '1 2 3 4 5 6 7 8I I I I I I Iconstraintchange~\%~L~-~lP /Pbigrara (1440 states)/Pmin (113 states)l ' I ' I ' I ' I ' I9 10 11 12 13 14 xl03 mergesI I i ~ I I4 3 2 1 0 ?
10 3 states 13 12 11 10 9 8 7 6 5Figure 3: Log Perplexity of Test Part During Merging.
Constraints: Same Output until 12,500 / none after12,500.
The thin line shows the further development if we retain the same-output constraint, finally yieldinga bigram model.
The length of the test part is Ntest = 2,436.65We start merging with the same-output (uni-gram) constraint to reduce computation time.
Af-ter 12,500 merges the constraint is discarded andfrom then on all remaining states are allowed tomerge.
The constraints and the point of changingthe constraint are chosen for pragmatic reasons.We want the constraints to be as week as possi-ble to allow the maximal number of solutions butat the same time the number of merges must bemanageable by the system used for computation(a SparcServerl000 with 250MB main memory).As the following experiment will show, the exactpoints of introducing/discarding constraints i notimportant for the resulting model.There are Ntrain (Nt,ai,~- 1)/2 ~ 10 s hypothe-tical first merges in the unconstraint case.
Thisnumber is reduced to --~ 7.
105 when using theunigram constraint, thus by a factor of .v 150.By using the constraint we need about a week ofcomputation time on a SparcServer 1000 for thewhole merging process.
Computation would nothave been feasible without this reduction.Figure 2 shows the increase in perplexity du-ring merging.
There is no change during the first1,454 merges.
Here, only identical sequences ofinitial and final states are merged (compare figure1.a to c).
These merges do not influence the pro-bability assigned to the training part and thus donot change the perplexity.Then, perplexity slowly increases.
It can neverdecrease: the maximum likelihood model assignsthe highest probability to the training part andthus the lowest perplexity.Figure 2 also shows the perplexity's slope.
Itis low until about 12,000 merges, then drasticallyincreases.
At about this point, after 12,500 mer-ges, we discard the constraint.
For this reason, thecurve is discontinuous at 12,500 merges.
The effectof further retaining the constraint is shown by thethin lines.
These stop after t2,983 merges, whenall states with the same outputs are merged (i.e.,when a bigram model is reached).
Merging with-out a constraint continues until only three statesremain: the initial and the final state plus oneproper state.Note that the perplexity changes very slowlyfor the largest part, and then changes drasticallyduring the last merges.
There is a constant phasebetween 0 and 1,454 merges.
Between 1,454 and~11,000 merges the log perplexity roughly linearlyincreases with the number of merges, and it explo-des afterwards.What happens to the test part?
Model mer-ging starts with a very special model which then isgeneralized.
Therefore, the perplexity of some ran-dom sample of dialogue data (what the test part issupposed to be) should decrease during merging.Table 1: Number of states and Log Perplexity forthe derived models and an additional, previouslytest part, consisting of 9,784 words.
(a) stan-dard bigram model, (b) constrained model mer-ging (first experiment), (c) model merging startingwith a bigram model(second experiment)(a) (b) (c)model MM starttype bigrams merging with bigramsstates 1,440 113 113Log PP 2.78 2.41 2.39This is exactly what we find in the experiment.Figure 3 shows the log perplexity of the testpart during merging.
Again, we find the disconti-nuity at the point where the constraint is changed.And again, we find very little change in perple-xity during about 12,000 initial merges, and largechanges during the last merges.Model merging finds a model with 113 states,which assigns a log perplexity of 2.26 to the testpart.
Thus, in addition to finding a model withlower log perplexity than the bigram model (2.26vs.
2.40), we find a model that at the same timehas less than 1/10 of the states (113 vs. 1,440).To test if we found a model that predicts newdata better than the bigram model and to be surethat we did not find a model that is simply veryspecialized to the test part, we use a new, previ-ously unseen part of the Verbmobil corpus.
Thispart consists of 9,784 words.
The bigram modelassigns a log perplexity of 2.78, the merged modelwith 113 states assigns a log perplexity of 2.41 (seetable 1).
Thus, the model found by model mergingcan be regarded generally better than the bigrammodel.Im provementsThe derivation of the optimal model took abouta week although the size of the training part wasrelatively small.
Standard speech applications donot use 14,000 words for training as we do in thisexperiment, but 100,000, 200,000 or more.
It isnot possible to start with a model of 100,000 statesand to successively merge them, at least it is notpossible on today's machines.
Each step wouldrequire the test of ,~ 10 9 merges.In the previous experiment, we abandonedthe same-output constraint after 12,500 merges tokeep the influence on the final result as small aspossible.
It can not be skipped from the begin-ning because somehow the time complexity has tobe reduced.
But it can be further retained, untilno further merge under this constraint is possible.66- log10 P/Ntrain2.5-2 .01.51.00.5 s / J r10 11 12I I I4 3 2Ip/- log10 p/Ntest2.82.7 - --,2 .6 -  ~ ....... ,~2.5-  '" \2.4 - z -~2.3-2 .2 -' I ' I10 11 12I J I13 14 x 103 merges i ii i 4 3 21 0 ?
10 3 stateslpbigramlpmin13 14 ?103 mergesI I1 0 ?
103 statesFigure 4: Log Perplexity of training and test parts when starting with a bigram model.
The starting pointis indicated with o, the curves of the previous experiment are shown in thin lines.This yields a bigram model.
The second experi-ment uses the bigram model with 1,440 states asits starting point and imposes no constraints onthe merges.
The results are shown in figure 4.We see that the perplexity curves approachvery fast their counterparts from the previous ex-periment.
The states differ from those of the pre-viously found model, but there is no difference inthe number of states and corpus perplexity in theoptimal point.
So, one could in fact, at least in theshown case, start with the bigram model withoutloosing anything.
Finally, we calculate the perple-xity for the additional test part.
It is 2.39, thusagain lower than the perplexity of the bigram mo-del (see table 1).
It is even slightly lower than inthe previous experiment, but most probably dueto random variation.The derived models are not in any case equiva-lent (with respect to perplexity), regardless whe-ther we start with the trivial model or the bigrammodel.
We ascribe the equivalence in the experi-ment to the particular size of the training corpus.For a larger training corpus, the optimal modelshould be closer in size to the bigram model, oreven larger than a bigram model.
In such a casestarting with bigrams does not lead to an optimalmodel, and a trigram model must be used.Conc lus ionWe investigated model merging, a technique to in-duce Markov models from corpora..
The originalprocedure is improved by introducing constraintsand a different initial model.
The procedures areshown to be applicable to a transliterated speechcorpus.
The derived models assign lower perplexi-ties to test data than the standard bigram modelderived from the same training corpus.
Additio-nally, the merged model was much smaller thanthe bigram model.The experiments revealed a feature of modelmerging that allows for improvement of the me-thod's time complexity.
There is a large initialpart of merges that do not change the model'sperplexity w.r.t, the test part, and that do not in-fluence the final optimal model.
The time neededto derive a model is drastically reduced by abbre-viating these initial merges.
Instead of startingwith the trivial model, one can start with a smal-ler, easy-to-produce model, but one has to ensurethat its size is still larger than the optimal model.AcknowledgementsI would like to thank Christer Samuelsson for veryuseful comments on this paper.
This work wassupported by the Graduiertenkolleg Kognitions-wissenschaft, Saarbriicken.References\[Bahl et al, 1983\] Lalit R. Bahl, Frederick Jeli-nek, and Robert L. Mercer.
1983.
A maximumlikelihood approach to continuous peech reco-gnition.
IEEE Transactions on Pattern Analy-sis and Machine Inlelligence, 5(2):179-190.\[Baum et al, 1970\] Leonard E. Baum, Ted Petrie,George Soules, and Norman Weiss.
1970.
Amaximization technique occuring in the statisti-cal analysis of probabilistic functions in markov67chains.
The Annals of Methematical Statistics,41:164-171.\[Brants, 1995\] Thorsten Brants.
1995.
Estima-ting HMM topologies.
In Tbilisi Symposiumon Language, Logic, and Computation, HumanCommunication Research Centre, Edinburgh,HCRC/RP-72.\[Church, 1988\] Kenneth Ward Church.
1988.
Astochastic parts program and noun phrase par-ser for unrestricted text.
In Proc.
Second Confe-rence on Applied Natural Language Processing,pages 136-143, Austin, Texas, USA.\[Cutting et al, 1992\] Doug Cutting, Julian Ku-piec, Jan Pedersen, and Penelope Sibun.
1992.A practical part-of-speech tagger.
In Procee-dings of the 3rd Conference on Applied NaturalLanguage Processing (ACL), pages 133-140.\[Jelinek, 1990\] F. Jelinek.
1990.
Self-organizedlanguage modeling for speech recognition.
InA.
Waibel and K.-F. Lee, editors, Readings inSpeech Recognition, pages 450-506.
Kaufmann,San Mateo, CA.\[Omohundro, 1992\] S. M. Omohundro.
1992.Best-first model merging for dynamic learningand recognition.
In J. E. Moody, S. J. Han-son, and R. P. Lippmann, editors, Advances inNeural Information Processing Systems 4, pages958-965.
Kaufmann, San Mateo, CA.\[Pereira et al, 1993\] Fernando Pereira, NaftaliTishby, and Lillian Lee.
1993.
Distributionalclustering of english words.
In Proceedings ofthe 31st ACL, Columbus, Ohio.\[Rabiner, 1989\] L. R. Rabiner.
1989.
A tutorialon hidden markov models and selected applica-tions in speech recognition.
In Proceedings ofthe IEEE, volume 77(2), pages 257-285.\[Stolcke and Omohundro, 1994\] Andreas Stolckeand Stephen M. Omohundro.
1994.
Best-firstmodel merging for hidden markov model induc-tion.
Technical Report TR-94-003, Internatio-nal Computer Science Institute, Berkeley, Cali-fornia, USA.\[Viterbi, 1967\] A. Viterbi.
1967.
Error bounds forconvolutional codes and an asymptotically op-timum decoding algorithm.
In IEEE Transac-tions on Information Theory, pages 260-269.68
