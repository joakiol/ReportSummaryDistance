An Investigation of Various Information Sources for Classifying BiologicalNamesManabu Torii, Sachin Kamboj and K. Vijay-ShankerDepartment of Computer and Information SciencesUniversity of DelawareNewark, DE 19716ftorii,kamboj,vijayg@mail.eecis.udel.eduAbstractThe classification task is an integral part ofnamed entity extraction.
This task has notreceived much attention in the biomedi-cal setting, partly due to the fact that pro-tein name recognition has been the focusof the majority of the work in this field.We study this problem and focus on dif-ferent sources of information that can beutilized for the classification task and in-vestigate the extent of their contributionsfor classification in this domain.
However,while developing a specific algorithm forthe classification of the names is not ourmain focus, we make use of some simpletechniques to investigate different sourcesof information and verify our intuitionsabout their usefulness.1 IntroductionIn this paper, we investigate the extent to whichdifferent sources of information contribute towardsthe task of classifying the type of biological en-tity a phrase might refer to.
The classification taskis an integral part of named entity extraction.
Forthis reason, name classification has been studied insolving the named entity extraction task in the NLPand information extraction communities (see, forexample, (Collins and Singer, 1999; Cucerzan andYarowsky, 1999) and various approaches reported inthe MUC conferences (MUC-6, 1995)).
However,many of these approaches do not distinguish the de-tection of the names (i.e., identifying a sequence ofcharacters and words in text as a name) from that ofits classification as separate phases.
Yet, we believethat we will gain from examining the two as sepa-rate tasks as the classification task, the focus of thiswork, is sufficiently distinct from the name identifi-cation task.
More importantly, from the perspectiveof the current work, we hope to show that the sourcesof information that help in solving the two tasks arequite distinct.Similar to the approaches of name classifica-tion of (Collins and Singer, 1999; Cucerzan andYarowsky, 1999), we investigate both name internaland external clues.
However, we believe that the sit-uation in the specialized domain of biomedicine issufficiently distinct, that the clues for this domainneed further investigation and that the classificationtask has not received the similar attention deserved.A large number of name extraction methods pro-posed in this specialized domain have focused onextracting protein names only (Fukuda et al, 1998;Franzen et al, 2002; Tanabe et al, 2002).
Since onlyone class is recognized, the only task these methodsdirectly address is that of identifying a string of char-acters and/or words that constitute a protein name.These methods do not, at least in an explicit manner,have to consider the classification task.There are some important reasons to consider thedetection of names of other types of entities of bio-logical relevance.
Information extraction need notbe limited to protein-protein interactions, and ex-tracting names of other types of entities will be re-quired for other types of interactions.
Secondly,classification of names can help improve the preci-sion of the methods.
For example, KEX (Fukudaet al, 1998) is a protein name recognizer and hencelabels each name it detects as a protein.
However,names of different types of entities share similar sur-face characteristics (including use of digits, specialcharacters, and capitalizations).
Due to this reason,KEX and other protein name recognizers can picknames of entities other than proteins (and label themas proteins).
(Narayanaswamy et al, 2003) reportsthat by recognizing that some of these names as notthose of proteins allows their method to improve theprecision of protein name detection.
Thirdly detect-ing names of different classes will help in corefer-ence resolution, the importance of which is well rec-ognized in the IE domain.
In such specialized do-mains, the sortal/class information will play an im-portant role for this task.
In fact, the coreference res-olution method described in (Castan?o et al, 2002)seeks to use such information by using the UMLSsystem1 and by applying type coercion.
Finally,many information extraction methods are based onidentifying or inducing patterns by which informa-tion (of the kind being extracted) is expressed in nat-ural language text.
If we can tag the text with occur-rences of various types of names (or phrases that re-fer to biological entities) then better generalizationsof patterns can be induced.There are at least two efforts (Narayanaswamy etal., 2003; Kazama et al, 2002) that consider therecognition of names of different classes of biomed-ical relevance.
Work reported in (Pustejovsky et al,2002; Castan?o et al, 2002) also seeks to classify orfind the sortal information of phrases that refer tobiological entities.
However, classification was notthe primary focus of these papers and hence the de-tails and accuracy of the classification methods arenot described in much detail.
Other related worksinclude those of (Hatzivassiloglou et al, 2001; Liuet al, 2001) that use external or contextual clues todisambiguate ambiguous expressions.
While theseworks maybe viewed as similar to word sense dis-ambiguation (WSD), the one reported in (Hatzivas-siloglou et al, 2001) in particular is close to classifi-cation as well.
In this work, using context of individ-ual occurrences, names are disambiguated betweengene, protein and RNA senses.1The Unified Medical Language System (UMLS) was de-veloped at National Library of Medicine, a National Institutesof Health at Bethesda, USA.While our interest is in classification of phrasesthat refer to entities of biomedical significance, inthis work we limit ourselves to name classification.In our investigations, we wish to use an annotatedcorpus for both inducing and evaluating features.We are unaware of any large corpus where phrasesare annotated with their classes.
However, large cor-pora for named entity extraction in this domain arebeing developed, and fortunately, corpora such asGENIA being developed at University of Tokyo arefreely available.
We make use of this corpus andhence investigate the classification of names only.However, we believe that the conclusions we drawin this regard will apply equally to classification ofphrases other than names as well.2 Sources of Information for NameClassificationTo classify a name we consider both the wordswithin the name (i.e., name internal) as well as thenearby words, the context of occurrences.2.1 Using Name Internal InformationMethods for learning to identify names try to in-duce patterns of words and special characters thatmight constitute names.
Hence the entire sequenceof words in a name is important and necessary forname identification purposes.
In contrast, for classi-fication purposes, some parts of the names are moreimportant than the others and some may play no roleat all.
For example, in the name cyclic AMP re-sponse element-binding protein, the last word, pro-tein, is sufficient for its classification.
Similarly,Adherence-isolated monocytes, can be classified onthe basis of its last word, monocytes.The fact that the last word of a name often bearsthe most information about the class of the name isnot surprising.
In English, often the type of objectreferred by a noun phrase is given by the head noun.Viewing a name as a noun phrase, the head noun islikely to determine its class.
And in English nounphrases, the head noun is often the rightmost wordbecause of the right-branching structure of Englishnoun phrases.
Quite often the nouns correspond toconcepts (or classes) in an ontology.
In such cases,we call these nouns functional terms or f-terms, fol-lowing the terminology used in some name recog-nizers proposed for the biomedical domain.2.1.1 F-termsThe notion of f-terms, was first introduced in thedesign of KEX (Fukuda et al, 1998).
In this work,a set of words such as proteins and receptors, weremanually selected as f-terms.
In this protein namerecognition system, as well as in Yapex (Franzen etal., 2002), f-terms are only used for locating namesin text.
On the other hand, the system reportedin (Narayanaswamy et al, 2003), which identifiesthe names of other classes as well, generalizes themto also classify names as well.
Thus, f-terms areidentified with types/classes.The existing methods that use f-terms rely on amanually selected list of f-terms.
However, manualselection methods are usually susceptible to errors ofomission.
In Section 4.1, we consider a method thattries to automatically select a list of f-terms and theresultant word classes based on the GENIA corpus.We then use this generated list to test our intuitionsabout f-terms.We also consider f-terms extended to consist oftwo consecutive words.
We refer to these as bigramf-terms to differentiate them from single word only(unigram) f-terms.
The use of bigrams will help usto classify names when the last word is not an f-term,but the last two words together can uniquely clas-sify the name.
For example, Allergen -specific T cellclones cannot be classified using the last word alone.However, a name ending with cell clones as the lastbigram is likely to be a ?Source?.2.1.2 SuffixesOften the information about the class designatedby a noun can be found in its suffix, particularlyin a scientific domain.
If f-terms can be viewedas words that designate a class of entities then notethat suffixes also play the same role.
For example,words ending with the suffix -amine are nitrogencompounds and those ending with -cytes are cells.Thus using suffixes results in a generalization at theword level.
A method of selecting a list of suffixesand associating classes with them is described inSection 4.1.2.1.3 Example-based ClassificationOf course, not all names can be classified on thebasis of f-terms and suffixes only.
Sometimes namesare chosen on a more ad hoc manner and do not re-flect any underlying meaning.
In such cases, match-ing with names found in a dictionary would be theonly name-internal method possible.We cannot simply use an ?exact matching?
algo-rithm since such a method would only work if thename was already present in our dictionary.
As it isnot reasonable at this time to have a dictionary thatcontains all possible names, we can attempt to useapproximate matches to find similar names in thedictionary and use them for classification purposes.Such a method then can be thought of finding a wayto generalize from the names in a dictionary, insteadof relying on simple memorization.However, assuming a large dictionary is not feasi-ble at this time especially for all the classes.
So ouralternate is to look at examples from GENIA corpus.The candidate examples that we will use for classi-fication would be the ones that most closely matcha given name that needs to be classified.
Hence, themethod we are following here essentially becomesan example-based classification method such as k-nearest neighbor method.
One approach to this taskis described in Section 4.3.2.2 Using ContextWe now turn our attention to looking at clues thatare outside the name being classified.
Using contexthas been widely used for WSD and has also been ap-plied to name classification (for example, in (Collinsand Singer, 1999; Cucerzan and Yarowsky, 1999)).This approach has also been adopted for the biomed-ical domain as illustrated in the work of (Hatzivas-siloglou et al, 2001; Narayanaswamy et al, 2003;Castan?o et al, 2002)2.In the WSD work involving the use of context, wecan find two approaches: one that uses few strongcontextual evidences for disambiguation purposes,as exemplified by (Yarowsky, 1995); and the otherthat uses weaker evidences but considers a combi-nation of a number of them, as exemplified by (Galeet al, 1992).
We explore both the methods.
In Sec-tion 4.4, we discuss our formulation and present asimple way of extracting contextual clues.2(Castan?o et al, 2002) can be seen as using context in itstype coercion rules.3 Experimental Setup3.1 Division of the corpusWe divided the name-annotated GENIA corpus(consisting of 2000 abstracts) into two parts?1500abstracts were used to derive all the clues: f-terms,suffixes, examples (for matching) and finally contex-tual features.
These derived sources of informationwere then used to classify the names found in the re-maining 500 abstracts.
The keys from the annotatedcorpus were then used to compute the precision andrecall figures.
We will call these two parts the train-ing and test sections.Since we pick the names from the test sectionand classify them, we are entirely avoiding the nameidentification task.
Of course, this means that we donot account for errors in classification that might re-sult from errors in identifying names.
However, webelieve that this is appropriate for two reasons.
Ourinvestigation focuses on how useful the above men-tioned features are for classification and we felt thatthis might be slanted based on the name identifierwe use and its characteristics.
Secondly, most of theerrors are due to not finding the correct extent of thename, either because additional neighboring wordsare included or because some words/characters arenot included.
In our experience, most of these errorshappen at the beginning part of the name and, hence,should not unduly affect the classification.3.2 Classes of NamesIn our method, we classify names into one of thefive classes that we call Protein, Protein Part, Chem-ical, Source and Others.
We don?t have any partic-ularly strong reasons for this set of classes althoughwe wish to point out that the first four in this choicecorresponds to the classes used by the name recog-nizer of (Narayanaswamy et al, 2003).
It must benoted that the class proteins not only include pro-teins but also protein families, and genes; all ofwhich are recognized by many protein name recog-nizers.
The GENIA class names were then mappedonto our class names.3.3 TokenizationAfter the assignment of classes, all the extractednames were tokenized.
Noting that changing a digitby another, a Greek character by another, a Ro-man numeral by another rarely ever results in ob-taining another name of a different class, our nametokenization marks these occurrences accordingly.To remove variability in naming, hyphens and ex-tra spaces were removed.
Also, as acronyms are notuseful for detecting types, their presence is identi-fied (in our case we use a simplistic heuristic thatacronyms are words with 2 or more consecutive up-per case characters).3.4 Evaluation MethodologyWe used an n-fold cross-validation to verify that theresults and conclusions we draw are not slanted by aparticular division of the 2000 abstracts.
The corpuswas divided into sets of 500 abstracts - the composi-tion of each set being random - thus obtaining 4 dif-ferent partitions.
In the first partition, the first threesets were combined to form the Training Set and thelast was used as the Test Set.
In the second partition,the second, third and fourth sets formed the TrainingSet and the first was used as the Test Set and so on.The overall results that we report in Section 5were the average of results on the four partitions.However, the first partition was used for more de-tailed investigation.4 Classification MethodGiven an unclassified name, we first tried to clas-sify it on the basis of the f-terms and the suffixes.
Ifthat failed, we applied our string matcher to try tofind a match and assign a category to the unknownname.
Finally, we used context to assign classes tothe names that were still left unclassified.4.1 F-Term and Suffix ExtractionSince we consider f-terms to be nouns that appearat the end of a name and denote a type of entity,their presence in the name suffices for its classifi-cation.
Hence, we use the last words of namesfound in the training set to see if they can uniquelyidentify the class.
To generate a list of f-terms andtheir respective classes, we count each word or pairof words that is found at the end of any name.
Aunigram or bigram, w, was selected as an f-term if itappeared at least 5 times and if the conditional prob-ability P(classj w) for any class exceeds a thresholdwhich we set at 0.95.In the counting to estimate this conditional prob-ability we ignore the presence of digits, Greek char-acters and Roman numerals as discussed in the Sec-tion 3.3.
For example, in latent membrane protein1 the ?1?
at the end is ignored and ?protein?
will beselected as the unigram for the count.The number of f-terms selected for chemicals wasthe lowest.
This is not surprising considering chem-ical names have few words defining subtypes ofchemicals.
acetate was an example chosen for thisclass.
Some other examples of extracted f-terms andtheir associated classes are: cell, tissue, virus (forSource); kinase, plasmid and protein (for Proteins);subunit, site and chain (for Protein Parts) and bind-ings and defects (for Others).
A couple of surprisingwords were selected.
Due to the limitations of ourmethod, we do not check if a last name indeed de-notes a class of entities but merely note that the nameis strongly associated with a class.
Hence, proteinnames like Ras and Tax were also selected.For suffix extraction, we considered suffixes oflength three, four and five.
Since we argued ear-lier that the suffixes that we are considering playthe same role as f-terms, we only consider the suf-fixes of the last word.
This prevents the classifica-tion of cortisol- dependent BA patients (a ?Source?
)as a ?Chemical?
on the basis of the suffix -isol.
Also,like in the case of f-terms, digits, Greek charactersetc at the end of a name were ignored.
However,unlike f-terms, if the last word is an acronym thewhole name is dropped, as taking the suffix of anacronym wouldn?t result in any generalization.
Theprobability of a class given a suffix is then calculatedand only those suffixes which had a probability ofgreater than the probability threshold were selected.When generating the list of suffixes, we have twopossibilities.
We could choose to consider nameswhich ended with an f-term that was selected or notconsider these names under the assumption that f-terms would be sufficient to classify such names.
Wefound that considering the suffixes of the f-terms re-sults in a significant increase in the recall with lit-tle or no change in precision.
This rather surprisingresult can be understood if we consider the kindsof names that show up in the class Others.
A suf-fix such as ation was selected because a number ofnames ending with selected f-terms like transplan-tation, transformation, and association.
This suffixallows us to classify AP-1 translocation on the basisof the suffix despite the fact that translocation wasnot chosen as an f-term.4.2 Classification based on f-terms and suffixesGiven a set of f-terms and suffixes, along with theirassociated classes, selected from the training part,names in the test portion were classified by lookingat the words that end the names.
If a name endedwith a selected f-term, then the name was tagged asbelonging to the corresponding class.
If a match wasnot found, the suffix of the last word of the name wasextracted and a match was attempted with the knownlist of suffixes.
If no match was found, the name wasleft unclassified.4.3 Classifying Names using Similar ExamplesWe had discussed earlier the use of similar examplesto classify a new occurrence of a name.
To find sim-ilar examples, standard string matching algorithmsare often used which produce a similarity score thatvaries inversely with the number of edit operationsneeded to match two strings identically.
However,we abandoned the use of standard string matchingprograms as their performance for classification pur-poses was rather poor.
Primarily this was due tothe fact that these algorithms do not distinguish be-tween matches at the beginning and at the end of thename strings.
As discussed before, for classificationpurposes the position of words is important and wenoticed that matches at the beginning of the stringswere hardly ever relevant unlike the case with thoseat the end.
For this reason, we developed our ownmatching algorithm.Given a name in the test corpus, we try to findhow similar it is to candidate examples taken fromthe training portion.
For each pair of names, we firsttry to pair together the individual words that makeup the names allowing for some partial matching.These partial matches allow for certain kinds of sub-stitutions that we do not believe will affect the classi-fication.
These include dropping a plural ?s?, substi-tuting one Greek character by another, changing anuppercase character by the same character in lowercase, changing an Arabic/Roman single digit by an-other, changing a Roman numeral by an Arabic one,and dropping digits.
Each substitution draws a smallpenalty (although dropping digits incurs a slightlygreater penalty) and only a perfect match receives ascore of 1 for matching of individual words.
Com-plete mismatches receive a score of 0.We then try to assign a score to the whole pair ofnames.
We begin by assigning position numbers toeach pair of words (including matches, mismatchesand drops) starting from the rightmost match whichis assigned a position of zero.
Mismatches to theright of the first match, if any, are assigned negativepositions.
We then use a weight table that gives moreweightage to lower position numbers (i.e., towardsthe end of the strings rather than the beginning) toassign a weight to each pair of words depending onthe position.
Then the score of the entire match isgiven by a weighted sum of the match scores, nor-malized for length of the string.
Assigning a scoreof 0 for a mismatch is tantamount to saying that amismatch does not contribute towards the similarityscore.
A negative score for a mismatch would resultin assigning a penalty.We only consider those strings as candidate exam-ples if their similarity score is greater than a thresh-old .
To assign a class to a name instance, we lookat the k nearest neighbors, as determined by theirsimilarity scores to the name being classified.
Toassign a class to the name, we weight the voting ofeach of the k (or fewer) candidates by their similarityscore.
A class is assigned only if the the ratio of thescores of the top two candidates exceeds a thresh-old, .
The precision should tend to increase withthis ratio.
34.4 Classifying Based on ContextTo identify the best sources of contextual informa-tion for classifying names, we considered two pos-sibilities ?
the use of a single strong piece of ev-idence and the use of a combination of weak evi-dences.
For the former we made use Decision Listssimilar to Yarowsky?s method for Word Sense Dis-ambiguation (WSD) (Yarowsky, 1995).
However,we found that this method had a poor recall.43As always, the reason for using a threshold is that it allowsus to find the appropriate level of compromise between preci-sion and recall.
Given that there are different sources of infor-mation, there is no need to insist that particular method assign aclass tag if we are not comfortable with the level of confidencethat we have in such an assignment.4Due to space limitations, we don?t discuss why we mighthave obtained the poor recall that we got for the decision listHence, we decided to use a combination of weakevidences and employ the Naive-Bayes assumptionof independence between evidences, similar to themethod described in (Gale et al, 1992).
To dothis, the words that occurred within a window andthat matched some template pattern were selectedas features if their scores 5 exceeded some thresh-old (which we name a).
Also, unlike Decision Lists,all the features presented in the context of a nameinstance were involved in its classification and theprobability that a name instance has a certain classwas calculated by multiplying probabilities associ-ated with all the features.
As some of the evi-dences might be fairly weak, we wanted to classifyonly those cases where the combination of featuresstrongly indicated a particular class.
This is doneby comparing the two probabilities associated withthe best two classes for an instance.
A class wasassigned to a particular name instance only whenthe ratio of the two probabilities was beyond a cer-tain threshold (which will call b).
Together withthe threshold, a for the feature selection, choice ofthis threshold could allow trade-off between preci-sion and recall for classification accuracies.5 Results and Evaluation5.1 F-Terms and SuffixesTable 1 gives the precision and recall values for thefirst partition for both f-terms and suffixes.6 Ascan be seen, the recall for ?Chemical?
is very lowas compared to the other classes.
This is due totwo reasons?firstly most chemical names consist ofonly one word and secondly we found that chemicalnames do not end with an indicative word.The number of f-terms and suffixes extracted byour program was considerably less for Chemicalsand Protein Parts as compared to Proteins and Oth-ers.
While this is consistent with the the explana-tion of poor recall for chemicals, it can be noted thatthe low number of f-terms and suffixes extracted forprotein parts did not affect its recall in the same man-ner.
As expected the precision remains high for allclasses.method.5The scores were simply the conditional probability of aclass given a word6The suffix list includes f-terms.F-Term and suffix String Matching ContextF-Term Suffix Alone After Suffix a = 5, b = 2 a = 2, b = 5Class Prec.
Rec.
Prec.
Rec.
Prec.
Rec.
Prec.
Rec.
Prec.
Rec.
Prec.
Rec.Chemical 0.97 0.05 0.98 0.19 0.89 0.54 0.90 0.59 0.85 0.06 0.55 0.10Protein 0.97 0.35 0.98 0.55 0.92 0.81 0.93 0.81 0.70 0.31 0.53 0.76Protein Part 0.98 0.40 0.98 0.33 0.86 0.75 0.85 0.76 0.75 0.05 0.37 0.12Source 0.98 0.61 0.97 0.62 0.95 0.87 0.94 0.89 0.83 0.10 0.78 0.10Others 0.99 0.69 0.97 0.71 0.96 0.87 0.96 0.91 0.80 0.05 0.74 0.03Total 0.98 0.49 0.98 0.57 0.93 0.81 0.93 0.84 0.72 0.17 0.53 0.36Table 1: Results for the various stages of our method.Figure 1: Precision-Recall Tradeoff5.2 Using ExamplesFor the string matching, we tried three different setof values for the parameters ,  and k,that is (0.3,2, 3), (0.7, 2, 1) and (0.7, 2, 5).
We found that theresults were marginally better for the set (0.3,2,3)and, hence, show the results for this set only.
Table1 shows the results of applying the string matchingto the first partition ?
all by itself and on names notclassified after the suffix stage.
As can be seen, therecall is higher than the previous stages but with aslight reduction in precision.5.3 Results for ContextWe ran the context classifier for different values ofthe parameters f, a and b but finally chose a value of5 for f because choosing a higher frequency thresh-old does not improve the precision but hurts the re-call.
Figure 1 shows the precision plotted against therecall for different choice of a and b.The values of the precision and recall on the firstClass Precision RecallChemical 0.87 0.62Protein 0.84 0.90Protein Part 0.86 0.79Source 0.94 0.87Others 0.96 0.90Total 0.90 0.87Table 2: Overall Resultspartition for each individual class and the two setsof thresholds are shown in Table 1.
The first set,that considers stronger evidences (since a is higher),achieves higher precision but recall is not satisfac-tory.
Most of the word evidences chosen tended toindicate a classification of proteins and hence thehigher recall for this class.
Allowing weaker evi-dences (because a = 2) means more contextual ev-idences were selected and hence a higher recall wasobtained (particularly for protein).
But precision islowered except for Source and Others (which inci-dentally don?t show an increase in recall).5.4 Overall ResultsTable 2 shows the precision and recall for all the dif-ferent classes, averaging it out for the 4 different par-titions.
We observed very little variance between theresults for the different partitions.6 Conclusions and Future WorkWe have considered a few name internal and exter-nal sources of information that help in the classifica-tion of names.
Despite using fairly simple methodsto classify the names, we have obtained encourag-ing results which we take to suggest that that ourintuitions about them are on the right track.
Wefeel that the effectiveness of f-terms and suffixes thatgeneralize the idea of f-terms, the matching algo-rithm that places more emphasis on partial matchesof words to the right vindicates our stance that theclassification of names is a task sufficiently distinctfrom the name identification process and warrantsan independent investigation.
Even the use of con-text is different for the two tasks as in the latter taskonly the immediately neighboring words are usedand that too for purpose of demarking the extrem-ities of the name string.While the high precision of f-terms and suffixbased classification was expected, the recall of thesemethods was higher than expected.
It is also clearthat these methods do not help much with the chem-icals class.
We believe that in addition to suffix,the knowledge of other chemical root forms (such as?meth?
), e.g., used in (Narayanaswamy et al, 2003),would be useful.We would like to focus more on the matching partof the work.
In particular, rather than hand-codingour intuitions in terms of weights for the differentparameters, we would like to automatically, e.g., us-ing a held-out validation set, have these set and seeto what extent the automated choice of parametersshow the bias for the rightmost words in the match-ing.
We would also like to generalize our work fur-ther by not limiting the classes to the ones chosenhere but allow a wider range of classes.
To do this,we would like to consider the GENIA classes andcollapse classes at various levels of their ontologyand try to see at what level of fine-grained distinc-tions can classification still be done satisfactorily.
Inregards to the use of the contextual method, whilewe have some preliminary ideas, we need to inves-tigate further why the use of a single strong clue,as exemplified by the decision list method, does notwork as well as it seems to for the WSD task.ReferencesJ.
Castan?o, M. Zhang, and J. Pustejovsky.
2002.Anaphora Resolution in Biomedical Literature.
InProc.
of International Symposium on Reference Res-olution.M.
Collins and Y.
Singer.
1999.
Unsupervised Modelsfor Named Entity Classification.
In Proc.
of EMNLP1999.S.
Cucerzan and D. Yarowsky 1999.
Language Inde-pendent Named Entity Recognition Combining Mor-phological and Contextual Evidence.
In Proc.
of JointSIGDAT Conference on Empirical Methods in NLPand Very Large Corpora, 90?99.W.
Gale, K. W. Church, and D. Yarowsky.
1992.
Amethod for disambiguating word senses in a large cor-pus.
Computers and the Humanities, 26:415?439.K.
Fukuda, T. Tsunoda, A. Tamura, and T. Takagi.
1998.Toward information extraction: identifying proteinnames from biological papers.
Proc.
of ISMB 1998,707?18.K.
Franze?n, G. Eriksson, F. Olsson, L. Asker, P. Lide?n,and J. Co?ster.
2002.
Protein names and how tofind them.
International Journal of Medical Informat-ics special issue on Natural Language Processing inBiomedical Applications, 67:49?61.V.
Hatzivassiloglou, P. A. Duboue, and A. Rzhetsky.2001.
Disambiguating proteins, genes, and RNA intext: a machine learning approach.
Bioinformatics, 17Suppl 1: S97?S106.J.
Kazama, T. Makino, Y. Ota, and J. Tsujii.
2002.
Tun-ing Support Vector Machines for Biomedical NamedEntity Recognition.
In Proc.
of the ACL-02 Workshopon Natural Language Processing in the BiomedicalDomain, 1?8.H.
Liu, Y. Lussier, and C. Friedman.
2001.
Disambigut-ing Biomedical Terms in Biomedical Narrative Text:an Unsupervised.
Journal of Biomedical Informatics,34 (4): 249-61.Proc.
of the Sixth Message Understanding Conference(MUC-6).
1995.
Morgan Kaufmann.M.
Narayanaswamy, K. E. Ravikumar, and K. Vijay-Shanker.
2003.
A Biological Named Entity Recog-nizer.
In Proc.
of PSB 2003.
8.J.
Pustejovsky, J. Castan?o, J. Zhang, M. Kotecki, andB.
Cochran.
2002.
Robust Relational Parsing OverBiomedical Literature: Extracting Inhibit Relations.In Proc.
of PSB 2002, 7:362?373.L.
Tanabe and W. J. Wilbur 2002.
Tagging gene andprotein names in full text articles.
In Proc.
of the ACL-02 Workshop on Natural Language Processing in theBiomedical Domain, 9?13.D.
Yarowsky.
1995.
Unsupervised Word Sense Disam-biguation Rivaling Supervised Methods.
In Proc.
ofACL 1995, 189?196.
