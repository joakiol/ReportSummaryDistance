Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 66?76,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Virtual Manipulative for Learning Log-Linear ModelsFrancis Ferraro and Jason EisnerDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD, USA{ferraro, jason}@cs.jhu.eduAbstractWe present an open-source virtual ma-nipulative for conditional log-linear mod-els.
This web-based interactive visual-ization lets the user tune the probabili-ties of various shapes?which grow andshrink accordingly?by dragging slidersthat correspond to feature weights.
Thevisualization displays a regularized train-ing objective; it supports gradient as-cent by optionally displaying gradientson the sliders and providing ?Step?
and?Solve?
buttons.
The user can sam-ple parameters and datasets of differ-ent sizes and compare their own pa-rameters to the truth.
Our web-site, http://cs.jhu.edu/?jason/tutorials/loglin/, guides the userthrough a series of interactive lessons andprovides auxiliary readings, explanations,practice problems and resources.1 IntroductionWe argue that if one is going to teach only a sin-gle machine learning technique in a computationallinguistics course, it should be conditional log-linear modeling.
Such models are pervasive in nat-ural language processing.
They have the formp~?
(y | x) ?
exp(~?
?
~f (x, y)), (1)where ~f extracts a feature vector from context xand outcome y ?
Y(x).
The set of possible out-comes Y(x) might depend on the context x.11The model is equivalent to logistic regression when y isa binary variable, that is, when Y(x) = {0, 1}.We then present an interactive web visualiza-tion that guides students through playing with log-linear models and their estimation.
This open-source tool, available at http://cs.jhu.edu/?jason/tutorials/loglin/, is in-tended to develop intuitions, so that basic log-linear models can be then taken for granted in fu-ture lectures.
It can be used near the start of acourse, perhaps after introducing probability no-tation and n-gram models.We used the tool in our Natural Language Pro-cessing (NLP) class and received very positivefeedback.
Students were excited by it, with somesaying the tool helped develop their ?physical in-tuition?
for log-linear models.
Other test userswith no technical background also enjoyed work-ing through the introductory lessons and foundthat they began to understand the model.The app includes 18 ready-to-use lessons for in-dividual or small-group study or classroom use.Each lesson, e.g.
Figure 1, guides the student tofit a probability model p~?
(y | x) over some collec-tion Y of shapes, words, or other images such asparse trees.
Each lesson is peppered with ques-tions; students can be asked to answer some ofthese questions in writing.2 Ambitious instruc-tors can add new lessons or edit existing ones bywriting configuration files (see section 5.3).
Thisis useful for emphasizing specific concepts or ap-plications.
Section 8 provides some history andapplications of log-linear modeling, as well as as-signment ideas.2There are approximately 6 questions per lesson.
Wefound that answering all the questions took our students about2300 words, or just under 23 words per question, which wasprobably both unreasonable and unnecessary.66Figure 1: The first lesson; the lower half is larger on the actual application.2 Why Teach With Log-Linear Models?Log-linear models are very handy in NLP.
Theycan be used throughout a course, when one needs?
a global classifier for an applied task, such asdetecting sentiment, topic, spam, or gender;?
a local classifier for structure annotation,such as tags or segment boundaries;?
a local classifier to be applied repeatedly insequential decision-making;?
a local conditional probability within somegenerative process, such as an n-gram model,HMM, PCFG, probabilistic FSA or FST,noisy-channel MT model, or Bayes net;?
a global structured prediction method.
Herey is a complete structured object such as atagging, segmentation, parse, alignment, ortranslation.
Then p(y | x) is a Markov ran-dom field or a conditional random field, de-pending on whether x is empty or not.Log-linear models over discrete variables arealso sufficiently expressive for an NLP course.Students may experiment freely with adding theirown creative model features that refer to salient at-tributes or properties of the data, since the proba-bility (1) may consider any number of informativefeatures of the (x, y) pair.How about training?
Estimation of the pa-rameter weights ~?
from a set of fully observed(x, y) pairs is simply a convex optimization prob-lem.
Maximizing the regularized conditional log-likelihoodF (~?)
=(N?i=1log p~?
(yi | xi))?
C ?R(~?)
(2)is a simple, uniform training principle that can beused throughout the course.
The scaled regular-izer C ?
R(~?)
prevents overfitting on sparse fea-tures.
This is arguably more straightforward thanthe traditional NLP smoothing methods for esti-mating probabilities from sparse data (Chen andGoodman, 1996), which require applying variousad hoc formulas to counts, and which do not gen-eralize well to settings where there is not a naturalsequence of backoff models.
There exist fast andusable tools that students can use to train their log-67linear models, including, among others, MegaM(Daume?
III, 2004), and NLTK (Bird et al 2009).3Formally, log-linear models are a good gate-way to a more general understanding of undirectedgraphical models and the exponential family, in-cluding globally normalized joint or conditionaldistributions over trees and sequences.One reason that log-linear models are both ver-satile and pedagogically useful is that they do notjust make predictions, but explicitly model proba-bilities.
These can be?
combined with other probabilities using theusual rules of probability;?
marginalized at test time to obtain the prob-ability that the outcome y has a particularproperty (e.g., one can sum over alignments);?
marginalized at training time in the case ofincomplete data y (e.g., the training data maynot include alignments);?
used to choose among possible decisions bycomputing their expected loss (risk).The training procedure also takes a probabilis-tic view.
Equation (2) helps illustrate impor-tant statistical principles such as maximum likeli-hood,4 regularization (the bias-variance tradeoff),and cross-validation, as well as optimization prin-ciples such as gradient ascent.Log-linear models also provide natural exten-sions of commonly taught NLP methods.
For ex-ample, under a probabilistic context-free gram-mar (PCFG),5 p(parse tree | sentence) is propor-tional to a product of rule probabilities.
Simplyreplacing each rule probability with an arbitrarynon-negative potential?an exponentiated weight,or sum of weights of features of that rule?givesan instance of (1).
The same parsing algorithmsstill apply without modification, as does the sameinside-outside approach to computing the poste-rior expectation of rule counts and feature counts.Immediate variants include CRF CFGs (Finkel3A caveat is that generic log-linear training tools will iter-ate over the setY(x) in order to maximize (1) and to computethe constant of proportionality in (1) and the gradient of (2).This is impractical when Y(x) is large, as in language mod-eling or structured prediction.
See Section 8.4Historically, this objective has been regarded as the opti-mization dual of a maximum entropy problem (Berger et al1996), motivating the log-linear form of (2).
We have consid-ered adding a maximum entropy view to our manipulative.5Likewise for Markov or hidden Markov models.et al 2008), in which the rule features becomeposition-dependent and sentence-dependent, andlog-linear PCFGs (Berg-Kirkpatrick et al 2010),in which the feature-rich rule potentials are locallyrenormalized into rule probabilities via (1).For all these reasons, we recommend log-linearmodels as one?s ?go-to?
machine learning tech-nique when teaching.
Other linear classifiers,such as perceptrons and SVMs, similarly choosey given x based on a linear score ~f ?
~?
(x, y)?butthese scores have no probabilistic interpretation,and the procedures for training ~?
are harder to un-derstand or to justify.
Thus, they can be taughtas variants later on or in another course.
Furtherreading includes (Smith, 2011).3 The Teaching ChallengeUnfortunately, there is a difficulty with introduc-ing log-linear models early in a course.
Oncegrasped, they seem very simple.
But they are notso easy to grasp for a student who has not hadany experience with high-dimensional parametricfunctions, feature design, or statistical estimation.The interaction among the parameters can be be-wildering.
Log-likelihood, gradient ascent, andoverfitting may also be new ideas.Students who lack intuitions about these mod-els will fail to follow subsequent lectures.
Theywill also have trouble with homework projects?interpreting the weights learned by their model,and diagnosing problems with their features ortheir implementation.
A student cannot even de-sign appropriate feature sets without understand-ing how the weights of these features interact todefine a distribution.
We will discuss some of thenecessary intuitions in sections 6 and 7.We would like equations (1), (2), and the gradi-ent formula to be more than just recipes.
The stu-dent should regard them as familiar objects withpredictable behavior.
Like computer science, ped-agogy proceeds by layering new ideas on top ofalready-familiar abstractions.
A solid understand-ing of basic log-linear models is prerequisite to?
using them in NLP applications that havetheir own complexities,?
using them as component distributions withinlarger probability models or decision rules,?
generalizing the algorithms for working with(1) and (2) to settings where one cannot eas-ily enumerate Y .684 (Virtual) ManipulativesFamiliar concrete concepts have often been in-voked to help develop intuitions about abstractmathematical concepts.
Specifically within earlymath education, manipulatives?tactile objects?have been shown to be effective hands-on teachingtools.
Examples include Cuisenaire rods for ex-ploring arithmetic concepts like sums, ratios, andplace value, or geoboards for exploring geometricconcepts like area and perimeter.6 The key idea isto ground and link the mathematical language to awell-known physical object that can be inspectedand manipulated.
For more, see the classic and re-cent analyses from Sowell (1989) and Carbonneauet al(2013).Research has shown concrete manipulatives tobe effective, but practical widespread use of thempresents certain problems, including procurementof necessary materials, replicability, and applica-bility to certain groups of students and to con-cepts that have no simple physical realization.These issues have spurred interest over the pasttwo decades in virtual manipulatives implementedin software, including the creation of the NationalLibrary of Virtual Manipulatives.7 Both Clementsand McMillen (1996) and Moyer et al(2002) pro-vide accessible overviews of virtual manipulativesin early math education.
Virtual manipulativesgive students the ability to effect changes on acomplex system and so learn its underlying prop-erties (Moyer et al 2002).
This last point is par-ticularly relevant to log-linear models.Members of the NLP and speech communitieshave previously explored manipulatives and theidea of ?learning by doing.?
Eisner (2002) im-plemented HMM posterior inference and forward-backward training on a spreadsheet, so that editingthe data or initial parameters changed the numeri-cal computations and the resulting graphs.
VIS-PER, an applied educational tool that wrappedvarious speech technologies, was targeted towardunderstanding the acoustics and overall recog-nition pipeline (Nouza et al 1997).
Light etal.
(2005) developed web interfaces for a num-ber of core NLP technologies and systems, suchas parsers, part-of-speech taggers, and finite-state6Cuisenaire rods are color-coded blocks with lengths from1 to 10.
A geoboard is a board representing the plane, withpegs at the integral points.
A rubber band can be stretchedaround selected pegs to define a polygon.7nlvm.usu.edu/en/nav/vlibrary.html andenlvm.usu.edu/ma/nav/doc/intro.jsptransducers.
Matt Post created a Model 1 stackdecoder visualization for a recent machine trans-lation class (Lopez et al 2013).8 Most manipula-tives/interfaces targeted at NLP have been virtual,but a notable exception is van Halteren (2002),who created a (physical) board game for parsing.In machine learning, there is a plethora of vir-tual manipulatives demonstrating central conceptssuch as decision boundaries and kernel methods.9There are also several systems for teaching artifi-cial intelligence: these tend to to involve control-ling virtual robots10 or physical ones (Tokic andBou Ammar, 2012).
Overall, manipulatives forNLP and ML seem to be a successful pedagogi-cal direction that we hope will continue.Next, we present our main contribution, a vir-tual manipulative that teaches log-linear models.We ground the models in simple objects such ascircles and regular polygons, in order to appeal tothe students?
physical intuitions.
Later lessons canmove on from shapes, instead using words or im-ages from a particular application of interest.5 Our Log-Linear Virtual ManipulativeFigure 1 shows a screenshot of the tool,available at http://cs.jhu.edu/?jason/tutorials/loglin/.
We encourage you toplay with it as you read.5.1 Student InterfaceSuccessive lessons introduce various challenges orsubleties.
In each lesson, the user experimentswith modeling some given dataset D using somegiven set of K features.
Dataset: For each contextx, the outcomes y ?
Y(x) are displayed as shapes,images or words.
Features: For each feature fi,there is a slider to manipulate ?i.Each shape y is sized proportionately to itsmodel probability p~?
(y | x) (equation (1)), so itgrows or shrinks as the user changes ~?.
In con-trast, the empirical probabilityp?
(y | x) =c(x, y)c(x)(= ratio of counts) (3)is constant and is shown by a gray outline.8github.com/mjpost/stack-decoder9E.g., http://cs.cmu.edu/?ggordon/SVMs/svm-applet.html.10E.g., http://www-inst.eecs.berkeley.edu/?cs188/pacman/pacman.html and http://www.cs.rochester.edu/trac/quagents.69The size and color of y indicate how p~?
(y | x)compares to this empirical probability (Figure 2).Reinforcing this, the observed count c(x, y) isshown at the upper left of y, while the expectedcount c(x) ?
p~?
(y | x) is shown at the upper right,following the same color scheme (Figure 1).We begin with globally normalized models(only one context x).
For example, the data inFigure 1?30 solid circles, 15 striped circles, 10solid triangles, and 5 striped triangles?are to bemodeled with the two indicator features fcircle andfsolid.
With ~?
= 0 we have the uniform dis-tribution, so the solid circle is contained in itsgray outline (p?
(solid circle) > p~?
(solid circle)),the striped triangle contains its gray outline(p?
(striped triangle) < p~?
(striped triangle)), andthe striped circle and gray outline are coincident(p?
(striped circle) = p~?
(striped circle)).A student can try various activities:In the outcome matching activity, the goal is tomatch the model p~?
to p?.
The game is to make allof the outcomes match their corresponding grayoutlines in size (and color).
The student ?wins?once the maximum number of objects turn gray.In the feature matching activity, the goal is tomatch the expected feature vector Ep~?
[~f ] to theobserved feature vector Ep?
[~f ].
In Figure 1, thestudent would seek a model that correctly predictsthe total number of circles and the total numberof solid objects?even if the specific number ofsolid circles is predicted wrong.
(The predictedand observed counts for a feature can easily befound by adding up the displayed counts of indi-vidual outcomes having that feature.
For conve-nience, they are also displayed in a tooltip on thefeature?s slider.)
This game can always be won,even if the given features are not adequately ex-pressive to succeed at outcome matching on thegiven dataset.In the log-likelihood activity, the goal is to max-imize the log-likelihood.
The log-likelihood bar(Figure 1) adapts to changes in ~?, just like theshapes.
The game is to make the bar as long aspossible.11 In later lessons, the student insteadtries to maximize a regularized version of the log-likelihood bar, which is visibly shortened by apenalty for large weights (to prevent overfitting).Winning any of these games with more complexmodels becomes difficult or at least tedious, so au-11Once the gradient is introduced in a later lesson, knowingwhen you have ?won?
becomes clearer.Quantityof Interest> 0 = 0 < 0red gray bluep??
p~?Ep?
[?]?Ep~?
[?
]15 30 60?~?FFigure 2: Color and area indicate differences be-twen the empirical distribution (gray outline) andmodel distribution.
Red (or blue) indicates amodel probability or parameter that should be in-creased (or decreased) to fit the data.Figure 3: Gradient components use the same colorcoding as given in Figure 2.
The length of eachcomponent indicates its potential effect on the ob-jective.
Note that the sliders use a nonlinear scalefrom ??
to +?.tomatic methods come as a relief.
The student mayview hints on the sliders, showing which way eachslider should be nudged (Figure 3).
These hintscorrespond to components of the log-likelihoodgradient.
Further automation is offered by the?Step?
button, which automatically nudges all pa-rameters by taking a step of gradient ascent,12 andeven more by the ?Solve?
button, which steps allthe way to the maximum.13Our lessons guide the student to appreciate therelationship among the three activities.
First, fea-ture matching is a weaker, attainable version ofoutcome matching (when outcome matching is12When `1 regularization is used, the optimal ~?
often con-tains many 0 weights, and a step is not permitted to jumpover a (possibly optimal) weight of 0.
It stops at 0, though ifwarranted, it can continue past 0 on the next step.13The ?Solve?
button adapts the stepsize at each step, usinga backtracking line search with the Armijo condition.
Thisensures convergence.70possible it certainly achieves feature matching aswell).
Second, feature matching is equivalentto maximizing the (unregularized) log-likelihood.Thus the mismatch is 0 iff the gradient of log-likelihood is 0.
In fact, the mismatch equals thegradient even when they are not 0!
Thus, drag-ging the sliders in the direction of the gradienthints can be viewed as a correct strategy for eitherthe feature matching game or the log-likelihoodgame.
This connection shows that the current gra-dient of log-likelihood can easily be computed bysumming up the observed and currently predictedcounts of each feature.
After understanding thisand playing with the ?Step?
and ?Solve?
buttons,the student should be able to imagine writing codeto train log-linear models.5.2 Guided ExplorationWe expect students to ?learn by playing.?
The usercan experiment at any time with the sliders, withgradient ascent and its stepsize, with the type andstrength of regularization, and with the size of thedataset.
The user can also sample new data or newparameters, and can peek at the true parameters.These options are described further in Section 7.We encourage experimentation by providingtooltips that appear whenever a student hovers themouse pointer over a element of the GUI.
Tooltipsprovide guidance about whatever the student islooking at right then.
Some are static explanations(e.g., what does this gray bar represent?).
Othersdynamically update with changes to the parame-ters (e.g., the tooltips on the feature sliders showthe observed and expected counts of that feature).Students see the tooltips repeatedly, which canhelp them absorb and reinforce concepts over anextended period of time.
Students who like tolearn by browsing and experimenting can point tovarious tooltips and get a sense of how the differ-ent concepts fit together.
Some tooltips explicitlyrefer to one another, linking GUI elements such asthe training objective, the regularization choices,and the gradient.Though the user is welcome to play, we alsoprovide some guidance.
Each lesson displays in-structions that explain the current dataset, jus-tify modeling choices, introduce new functional-ity, lead the user through a few activities, and asklesson-specific questions.
The first lesson alsolinks to a handout with a more formal textbook-style treatment.
The last lesson links to furtherFigure 4: Inventory of available shapes(circle/triangle/square/pentagon) and fills(solid/striped/hollow).
Text and arbitrary im-ages may be used instead of shapes.
Color andsize are reserved to indicate how the currentmodel?s predictions of outcome counts or featurecounts compare to the empirical values?seeFigure 2.reading and exercises.5.3 Instructor Interface: Creating andTailoring LessonsAn instructor may optionally wish to tailor lessonsto his or her students?
needs, interests, and abil-ities.
Shapes provide a nice introduction to log-linear models, but eventually NLP students willwant to think about NLP problems, whereas vi-sion students will want to think about vision prob-lems.
Thus, we have designed the manipulative tohandle text and arbitrary images, as well as the 12shape-fill combinations shown in Figure 4.Tailoring lessons to the students?
needs is assimple as editing a couple of text files.
These mustspecify (1) a set of features, (2) a set of contexts,14and (3) for each context, a set of featurized events,including counts and visual positions.
This simpleformat allows one to describe some rather involvedmodels.
Some of the features may be ?hidden?from the student, thereby allowing the student toexperience model mismatch.
Note that the visualpositioning information is pedagogically impor-tant: aligning objects by orthogonal descriptionscan make feature contrasts stand out more, e.g.,circles vs. triangles or solid vs. striped.The configuration files can turn off certain fea-tures on a per-lesson basis (without program-14The set of contexts may be omitted when there is onlyone context (i.e., an unconditioned model).71ming).
This is useful for, e.g., hiding the ?Solve?button in early lessons, adding new tooltips, orspecializing the existing tooltips on a per-lessonbasis.However, being a manipulative rather than atutoring system, our software does not monitorthe user?s progress through a lesson and provideguidance via lesson-specific hints, warnings, ques-tions, or feedback.
(The software is open-source,so others are free to extend it in this way.
)5.4 Back-End ImplementationAnyone can use our virtual manipulative sim-ply by visiting its website.
There is no start-upcost.
Aside from reading the data, model and in-structions from the web server, it is fully client-side.
The Javascript back-end uses common andwell-supported open-source libraries that providea consistent experience across browsers.15 Themanipulative relies on certain capabilities from theHTML5 standard.
Not all browsers in currentuse support these capabilities, notably Internet Ex-plorer 9 and under.
The tool works with recentversions of Firefox, Chrome and Safari.6 Pedagogical Aims6.1 Modeling and EstimationWhen faced with a dataset D of (x, y) pairs, oneoften hopes to choose an appropriate model.When are log-linear models appropriate?
Whydoes their hypothesis space include the uniformdistribution?
For what feature sets does it includeevery distribution?One should also understand statistical estima-tion.
How do the features interact?
When esti-mating their weights, can raising one weight alteror reverse the desired changes to other weights?How can parameter estimation go wrong statis-tically (overfitting, perhaps driving parameters to??)?
What might happen if we have a verylarge feature set?
Can we design regularized es-timators that prevent overfitting (the bias-variancetradeoff)?
What is the effect of the regularizationconstant on small and large datasets?
On rare andfrequent contexts?
On rare and frequent features?On useful features (including features that alwaysor never fire) and useless ones?15Specifically and in order, d3 (d3js.org/), jQuery(jquery.com/), jQuery UI (jqueryui.com),jQuery Tools (jquerytools.org/), and qTip(craigsworks.com/projects/qtip/).Finally, one is responsible for feature design.Which features usefully distinguish among theevents?
How do non-binary features work andwhen are they appropriate?
When can a featuresafely be omitted because it provides no additionalmodeling power?
How does the choice of featuresaffect generalization, particularly if the objectiveis regularized?
In particular, how do shared fea-tures and backoff features allow a model to gen-eralize to novel contexts and outcomes (or rareones)?
How do the resulting patterns of general-ization relate qualitatively to traditional smoothingtechniques in NLP (Chen and Goodman, 1996)?6.2 Training AlgorithmWe also aim to convey intuitions about a specifictraining algorithm.
We use the regularized condi-tional log-likelihood (2) to define the goodness ofa parameter vector ~?.
The best choice is then the ~?that solves equation (4):0 = ?~?F = Ep?
[~f(X,Y )]?
Ep~?
[~f(X,Y )]?
C?~?R(~?
)(4)where because our model is conditional, p~?
(x, y)denotes the hybrid distribution p?
(x) ?
p~?
(y | x).Many important concepts are visible in (2) and(4).
As discussed earlier, (4) includes the dif-ference between observed and expected featurecounts,Ep?
[~f(X,Y )]?
Ep~?
[~f(X,Y )].
(5)Students must internalize this concept and themeaning of the two counts above.
This preparesthem to understand the extension to structured pre-diction, where these counts can be more diffi-cult to compute (see Section 8).
It also preparesthem to generalize to training latent-variable mod-els (Petrov and Klein, 2008).
In that setting, theobserved count can no longer be observed but isreplaced by another expectation under the model,conditioned on the partial training data.
(4) also includes a weight decay term for regu-larization.
We allow both `1 and `2 regularization:R(~?)
= ?~?
?1 versus R(~?)
= ?~??22.
One can seeexperimentally that strong `1 regularization triesto use a few larger weights and leave the rest at0, while strong `2 regularization tries to share thework among many smaller weights.
One can ob-serve how for a given C, the regularization term is72more important for small datasets, since for largerdatasets it is dominated by the log-likelihood.Once one can compute the gradient, one can?follow?
it along the surface, in a way that is guar-anteed to increase the convex objective function upto its global maximum.
The ?Solve?
button doesthis and indeed one can watch the log-likelihoodbar continually increase.
Yet one should observewhat might go wrong here as well.
Gradient ascentcan oscillate if a fixed stepsize is used (by click-ing ?Step?
repeatedly).
One may also notice that?Solve?
is somewhat slow to converge on someproblems, which motivates considering alternativeoptimization algorithms (Malouf, 2002).We should note that we are not concernedwith efficiency issues, e.g., tractably computingthe normalizers Z(x).
Efficient normalization isa crucial practical ingredient in using log-linearmodels, but our primary concern is to impart anear-physical intuitive understanding of the mod-els themselves.
See Section 8 or Smith (2011) forstrategies on computing the normalizer.7 Provided LessonsIn this section we provide an overview of the 18currently available lessons.
(Of course, you canwork through the lessons yourself for further de-tails.)
?Core?
lessons that build intuition precedethe ?applied?
lessons focused on NLP tasks orproblems.
Instructors should feel especially freeto replace or reorder the ?applied?
lessons.Core lessons 1?5 provide a basic introductionto log-linear modeling, using unconditioned distri-butions over only four shapes as shown in Figure1.
We begin by matching outcomes using just ?cir-cle?
and ?solid?
features.
We discover in lesson 2that it is redundant to add ?triangle?
and ?striped?features.
In lesson 3 we encounter a dataset whichthese features cannot fit, because the shape andfill attributes are not statistically independent.
Weremedy this in lesson 4 with a conjunctive ?stripedtriangle?
feature.Because outcome matching fails in lesson 3,lessons 3?4 introduce feature matching and log-likelihood as suitable alternatives.
Lesson 5 brieflyillustrates a non-binary feature function, ?numberof sides?
(taking values 3, 4, and 5 on triangles,squares, and pentagons).
This clarifies the match-ing of feature counts: here we are trying to predictthe total number of sides in the dataset.Lessons 6?8 focus on optimization.
They moveup to the harder setting of 9 shapes with 6 fea-tures, so we tell students how to turn on the gra-dient ?hints?
on the sliders.
We explain how thesehints relate to feature matching and log-likelihood.We invite the students to try using the hints onearlier lessons?and on new random datasets thatthey can generate by clicking.
In Lesson 7, weintroduce the ?Step?
and ?Solve?
buttons to helpeven more with a difficult dataset.
Students use allthese GUI elements to climb the convex objectiveand increase the log-likelihood bar.At this point we introduce regularization.
Les-son 6 invited students to generate small randomdatasets and observe their high variance and thetendency to overfit them.
Lesson 8 gives a moredramatic illustration of overfitting: with no ob-served pentagons, the solver sends ?pentagon ???
to make p~?
(pentagon) ?
0.
We prevent thisby adding a regularization penalty, which reservessome probability for pentagons.
Striped pentagonsturn out to be the least likely pentagons, becausestripes were observed to be uncommon on othershapes (so ?striped < 0).
Thus we see that ourchoice of features allows this ?smoothing method?to make useful generalizations about novel out-comes.Lessons 9?10 consider the effect of `1 versus`2 regularization, and the competition between theregularizer (scaled by the constant C) and the log-likelihood (scaled by the dataset size N ).16Lessons 11?13 introduce conditional models,showing how features are shared among three con-texts.
The third context is unobserved, yet ourtrained model makes plausible predictions aboutit.
The conditional probabilities of unobservedshapes are positive even without regularization, incontrast to the joint probabilities in lesson 9.We see that a frequent context x generally hasmore influence on the parameters.
But this neednot be true if the parameters do not help to distin-guish among the particular outcomes Y(x).Lessons 14?15 explore feature design in condi-tional models.
We model conditional probabilitiesof the form p(fill | shape).
?Unigram?
featurescan favor certain fills y regardless of the shape.?Bigram?
features that look at y and x togethercan favor different fills for each shape type.
Wesee that features that depend only on the shape xcannot distinguish among fills y, and so have no16Clever students may think to try setting C < 0, whichbreaks convexity of the objective function.73effect on the conditional probabilities p(y | x).Lesson 15 illustrates how regularization pro-motes generalization and feature selection.
Oncewe have a full set of bigram features, the uni-gram features are redundant.
We never have toput a high weight on ?solid?
: we can accomplishthe same thing by putting high weights on ?solidtriangle?
and ?solid circle?
separately.
Yet thismisses a generalization because it does not pre-dict that ?solid?
is also likely for pentagons.
For-tunately, regularization encourages us to avoid toomany high weights.
So we prefer to put a singlehigh weight on ?solid,?
and use the ?solid triangle?and ?solid circle?
features only to model smallershape-specific deviations from that generalization.As a result, we will indeed extrapolate that pen-tagons tend to be solid as well.Lesson 16 begins the application-drivenlessons:One lesson builds on the ?unigram?
and ?bi-gram?
concepts to create a ?bigram languagemodel?
?a model of shape sequences over a vo-cabulary of 9 shapes.
A shape?s probability de-pends not only on its attributes but also on theattributes that it shares with the previous shape.What is the probability of a striped square giventhat the previous shape was also striped, or asquare, or a striped square?We also apply log-linear modeling to the taskof text categorization (spam detection).
We chal-lenge the students to puzzle out how this modelis set up and how to generalize it to three-waycategorization.
Our contexts in this case aredocuments?actually very short phrases.
Mostcontexts are seen only once, with an outcome of ei-ther ?mail?
or ?spam.?
Our feature set implementslogistic regression (footnote 1): each feature con-joins y = spam with some property of the textx, such as ?contains ?parents?,?
?has boldface,?
or?mentions money.
?Additional linguistic application lessons may beadded in the near future?e.g., modeling the rela-tive probability of grammar rules or parse trees.The final lesson summarizes what has beenlearned, mentions connections to other ideas inmachine learning, and points the student to furtherresources.8 Graduating to Real ApplicationsAt the time of writing, 3266 papers in the ACLAnthology mention log-linear models, with 137using ?log-linear,?
?maximum entropy?
or ?max-ent?
in the paper title.
These cover a wide range ofapplications that can be considered in lectures orhomework projects.Early papers may cover the most fundamen-tal applications and the clearest motivation.
Con-ditional log-linear models were first popularizedin computational linguistics by a group of re-searchers associated with the IBM speech and lan-guage group, who called them ?maximum entropymodels,?
after a principle that can be used to mo-tivate their form (Jaynes, 1957).
They applied themethod to various binary or multiclass classifica-tion problems in NLP, such as prepositional phraseattachment (Ratnaparkhi et al 1994), text catego-rization (Nigam et al 1999), and boundary pre-diction (Beeferman et al 1999).Log-linear models can be also used for struc-tured prediction problems in NLP such as tagging,parsing, chunking, segmentation, and languagemodeling.
A simple strategy is to reduce struc-tured prediction to a sequence of multiclass pre-dictions, which can be individually made with aconditional log-linear model (Ratnaparkhi, 1998).A more fully probabilistic approach?used in theoriginal ?maximum entropy?
papers?is to use (1)to define the conditional probabilities of the stepsin a generative process that gradually produces thestructure (Rosenfeld, 1994; Berger et al 1996).17This idea remains popular today and can be usedto embed rich distributions into a variety of gener-ative models (Berg-Kirkpatrick et al 2010).
Forexample, a PCFG that uses richly annotated non-terminals involves a large number of context-freerules.
Rather than estimating their probabilitiesseparately, or with traditional backoff smoothing,a better approach is to use (1) to model the proba-bility of all rules given their left-hand sides, basedon features that consider attributes of the nonter-minals.18The most direct approach to structured predic-tion is to simply predict the structured output allat once, so that y is a large structured object withmany features.
This is conceptually natural butmeans that the normalizer Z(x) involves sum-ming over a large space Y(x) (footnote 3).
One17Even predicting the single next word in a sentence can bebroken down into a sequence of binary decisions in this way.This avoids normalizing over the large vocabulary (Mnih andHinton, 2008).18E.g., case, number, gender, tense, aspect, mood, lexicalhead.
In the case of a terminal rule, the spelling or morphol-ogy of the terminal symbol can be considered.74can restrict Y(x) before training (Johnson et al1999).
More common is to sum efficiently bydynamic programming or sampling, as is typicalin linear-chain conditional random fields (Laffertyet al 2001), whole-sentence language modeling(Rosenfeld et al 2001), and CRF CFGs (Finkelet al 2008).
This topic is properly deferred untilsuch algorithmic techniques are introduced later inan NLP class, for example in a unit on parsing (seediscussion in section 2).
We prepare students forit by mentioning this point in our final lesson.19Our final lesson also leads to a web page wherewe link to log-linear software and to variouspencil-and-paper problems, homework projects,and readings that an instructor may consider as-signing.
We welcome suggested additions to thispage.9 ConclusionWe have introduced an open-source, web-basedvirtual manipulative for log-linear models.
In-cluded with the code are 18 lessons peppered withquestions, a handout that gives a formal treatmentof the necessary derivations, and auxiliary infor-mation including further reading, practice prob-lems, and recommended software.
A version isavailable at http://cs.jhu.edu/?jason/tutorials/loglin/.Acknowledgements We would like to thank theanonymous reviewers for their helpful feedbackand suggestions and the entire Fall 2012 NaturalLanguage Processing course at Johns Hopkins.ReferencesDoug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical models for text segmentation.
Ma-chine Learning, 34(1?3):177?210.Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,DeNero, John DeNero, and Dan Klein.
2010.
Pain-less unsupervised learning with features.
In Pro-ceedings of NAACL, June.Adam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum-entropyapproach to natural language processing.
Compu-tational Linguistics, 22(1):39?71.19This material can also be connected to other topics inmachine learning.
Dynamic programming and sampling arealso used for exact or approximate computation of normal-izers in undirected graphical models (Markov random fieldsor conditional random fields), which are really just log-linearmodels for structured prediction of tuples.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.Kira J. Carbonneau, Scott C. Marley, and James P.Selig.
2013.
A meta-analysis of the efficacy ofteaching mathematics with concrete manipulatives.Journal of Educational Psychology, 105(2):380 ?400.Stanley Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques.
In Proceedingsof ACL.Douglas H. Clements and Sue McMillen.
1996.
Re-thinking ?concrete?
manipulatives.
Teaching Chil-dren Mathematics, 2(5):pp.
270?279.Hal Daume?
III.
2004.
Notes on CG andLM-BFGS optimization of logistic regression.Paper available at http://pub.hal3.name#daume04cg-bfgs, implementation available athttp://hal3.name/megam/, August.Jason Eisner.
2002.
An interactive spreadsheetfor teaching the forward-backward algorithm.
InDragomir Radev and Chris Brew, editors, Proceed-ings of the ACL Workshop on Effective Tools andMethodologies for Teaching NLP and CL, pages 10?18, Philadelphia, July.Jenny Rose Finkel, Alex Kleeman, and Christopher DManning.
2008.
Efficient, feature-based, condi-tional random field parsing.
Proceedings of ACL-08: HLT, pages 959?967.E.
T. Jaynes.
1957.
Information theory and statisticalmechanics.
Physics Reviews, 106:620?630.Mark Johnson, Stuart Geman, Stephen Canon, ZhiyiChi, and Stefan Riezler.
1999.
Estimators forstochastic ?unification-based?
grammars.
In Pro-ceedings of ACL.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of ICML.Marc Light, Robert Arens, and Xin Lu.
2005.
Web-based interfaces for natural language processingtools.
In Proceedings of the Second ACL Workshopon Effective Tools and Methodologies for TeachingNLP and CL, pages 28?31, Ann Arbor, Michigan,June.
Association for Computational Linguistics.Adam Lopez, Matt Post, Chris Callison-Burch,Jonathan Weese, Juri Ganitkevitch, Narges Ah-midi, Olivia Buzek, Leah Hanson, Beenish Jamil,Matthias Lee, et al2013.
Learning to translate withproducts of novices: Teaching MT with competi-tive challenge problems.
Transactions of the ACL(TACL), 1.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
In Pro-ceedings of CoNLL, pages 49?55.75Andriy Mnih and Geoffrey Hinton.
2008.
A scalablehierarchical distributed language model.
In Pro-ceedings of NIPS.Patricia S. Moyer, Johnna J. Bolyard, and Mark A.Spikell.
2002.
What are virtual manipulatives?Teaching Children Mathematics, 8(6):372?377.Kamal Nigam, John Lafferty, and Andrew McCallum.1999.
Using maximum entropy for text classifica-tion.
In IJCAI-99 Workshop on Machine Learningfor Information Filtering, pages 61?67.Jan Nouza, Miroslav Holada, and Daniel Hajek.
1997.An educational and experimental workbench for vi-sual processing of speech data.
In Fifth EuropeanConference on Speech Communication and Technol-ogy.Slav Petrov and Dan Klein.
2008.
Sparse multi-scalegrammars for discriminative latent variable parsing.In Proceedings of EMNLP, pages 867?876, October.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.1994.
A maximum entropy model for prepositionalphrase attachment.
In Proceedings of the ARPA Hu-man Language Technology Workshop, pages 250?255.Adwait Ratnaparkhi.
1998.
Maximum Entropy Modelsfor Natural Language Ambiguity Resolution.
Ph.D.thesis, University of Pennsylvania, July.Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.2001.
Whole-sentence exponential language mod-els: A vehicle for linguistic-statistical integration.Computer Speech and Language, 15(1).Ronald Rosenfeld.
1994.
Adaptive Statistical Lan-guage Modeling: A Maximum Entropy Approach.Ph.D.
thesis, Carnegie Mellon University.Noah A. Smith.
2011.
Linguistic Structure Prediction.Synthesis Lectures on Human Language Technolo-gies.
Morgan and Claypool, May.Evelyn J Sowell.
1989.
Effects of manipulative materi-als in mathematics instruction.
Journal for Researchin Mathematics Education, pages 498?505.Michel Tokic and Haitham Bou Ammar.
2012.
Teach-ing reinforcement learning using a physical robot.In Proceedings of the ICML Workshop on TeachingMachine Learning.Hans van Halteren.
2002.
Teaching nlp/cl throughgames: The case of parsing.
In Proceedings ofthe ACL Workshop on Effective Tools and Method-ologies for Teaching NLP and CL, pages 1?9,Philadelphia, Pennsylvania, USA, July.
Associationfor Computational Linguistics.76
