Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 248?257,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsInteractive Topic ModelingYuening HuDepartment of Computer ScienceUniversity of Marylandynhu@cs.umd.eduJordan Boyd-GraberiSchoolUniversity of Marylandjbg@umiacs.umd.eduBrianna SatinoffDepartment of Computer ScienceUniversity of Marylandbsonrisa@cs.umd.eduAbstractTopic models have been used extensively as atool for corpus exploration, and a cottage in-dustry has developed to tweak topic modelsto better encode human intuitions or to bettermodel data.
However, creating such extensionsrequires expertise in machine learning unavail-able to potential end-users of topic modelingsoftware.
In this work, we develop a frame-work for allowing users to iteratively refinethe topics discovered by models such as la-tent Dirichlet alocation (LDA) by adding con-straints that enforce that sets of words must ap-pear together in the same topic.
We incorporatethese constraints interactively by selectivelyremoving elements in the state of a MarkovChain used for inference; we investigate a va-riety of methods for incorporating this infor-mation and demonstrate that these interactivelyadded constraints improve topic usefulness forsimulated and actual user sessions.1 IntroductionProbabilistic topic models, as exemplified by prob-abilistic latent semantic indexing (Hofmann, 1999)and latent Dirichlet alocation (LDA) (Blei et al,2003) are unsupervised statistical techniques to dis-cover the thematic topics that permeate a large cor-pus of text documents.
Topic models have had con-siderable application beyond natural language pro-cessing in computer vision (Rob et al, 2005), bi-ology (Shringarpure and Xing, 2008), and psychol-ogy (Landauer et al, 2006) in addition to their canon-ical application to text.For text, one of the few real-world applicationsof topic models is corpus exploration.
Unannotated,noisy, and ever-growing corpora are the norm ratherthan the exception, and topic models offer a way toquickly get the gist a large corpus.11For examples, see Rexa http://rexa.info/, JSTORContrary to the impression given by the tablesshown in topic modeling papers, topics discoveredby topic modeling don?t always make sense to os-tensible end users.
Part of the problem is that theobjective function of topic models doesn?t always cor-relate with human judgements (Chang et al, 2009).Another issue is that topic models ?
with their bag-of-words vision of the world ?
simply lack the nec-essary information to create the topics as end-usersexpect.There has been a thriving cottage industry addingmore and more information to topic models to cor-rect these shortcomings; either by modeling perspec-tive (Paul and Girju, 2010; Lin et al, 2006), syn-tax (Wallach, 2006; Gruber et al, 2007), or author-ship (Rosen-Zvi et al, 2004; Dietz et al, 2007).
Sim-ilarly, there has been an effort to inject human knowl-edge into topic models (Boyd-Graber et al, 2007;Andrzejewski et al, 2009; Petterson et al, 2010).However, these are a priori fixes.
They don?t helpa frustrated consumer of topic models staring at acollection of topics that don?t make sense.
In thispaper, we propose interactive topic modeling (ITM),an in situ method for incorporating human knowl-edge into topic models.
In Section 2, we review priorwork on creating probabilistic models that incorpo-rate human knowledge, which we extend in Section 3to apply to ITM sessions.
Section 4 discusses theimplementation of this process during the inferenceprocess.
Via a motivating example in Section 5, simu-lated ITM sessions in Section 6, and a real interactivetest in Section 7, we demonstrate that our approach isable to focus a user?s desires in a topic model, bettercapture the key properties of a corpus, and capturediverse interests from users on the web.http://showcase.jstor.org/blei/, and the NIHhttps://app.nihmaps.org/nih/.2482 Putting Knowledge in Topic ModelsAt a high level, topic models such as LDA take asinput a number of topics K and a corpus.
As output,a topic model discovers K distributions over words?
the namesake topics ?
and associations betweendocuments and topics.
In LDA both of these out-puts are multinomial distributions; typically they arepresented to users in summary form by listing theelements with highest probability.
For an exampleof topics discovered from a 20-topic model of NewYork Times editorials, see Table 1.When presented with poor topics learned fromdata, users can offer a number of complaints:2these documents should have similar topics butdon?t (Daume?
III, 2009); this topic should have syn-tactic coherence (Gruber et al, 2007; Boyd-Graberand Blei, 2008); this topic doesn?t make any senseat all (Newman et al, 2010); this topic shouldn?t beassociated with this document but is (Ramage et al,2009); these words shouldn?t be the in same topicbut are (Andrzejewski et al, 2009); or these wordsshould be in the same topic but aren?t (Andrzejewskiet al, 2009).Many of these complaints can be addressed byusing ?must-link?
constraints on topics, retaining An-drzejewski et als (2009) terminology borrowed fromthe database literature.
A ?must-link?
constraint is agroup of words whose probability must be correlatedin the topic.
For example, Figure 1 shows an exampleconstraint: {plant, factory}.
After this constraint isadded, the probabilities of ?plant?
and ?factory?
ineach topic are likely to both be high or both be low.It?s unlikely for ?plant?
to have high probability in atopic and ?factory?
to have a low probability.
In thenext section, we demonstrate how such constraintscan be built into a model and how they can even beadded while inference is underway.In this paper, we view constraints as transitive; if?plant?
is in a constraint with ?factory?
and ?factory?is in a constraint with ?production,?
then ?plant?
isin a constraint with ?production.?
Making this as-sumption can simplify inference slightly, which wetake advantage of in Section 3.1, but the real reasonfor this assumption is because not doing so would2Citations in this litany of complaints are offline solutions foraddressing the problem; the papers also give motivation whysuch complaints might arise.Constraints Prior Structure{}dogbark tree plant factory leash??????
{plant, factory}dogbark treeplantfactoryleash??
??2???
{plant, factory}{dog, bark, leash}dogbarktreeplant factoryleash????2??
?3?Figure 1: How adding constraints (left) creates new topicpriors (right).
The trees represent correlated distributions(assuming ?
>> ?).
After the {plant, factory} constraintis added, it is now highly unlikely for a topic drawn fromthe distribution to have a high probability for ?plant?
anda low probability for ?factory?
or vice versa.
The bottompanel adds an additional constraint, so now dog-relatedwords are also correlated.
Notice that the two constraintsthemselves are uncorrelated.
It?s possible for both, either,or none of ?bark?
and ?plant?
(for instance) to have highprobability in a topic.introduce ambiguity over the path associated with anobserved token in the generative process.
As long asa word is either in a single constraint or in the generalvocabulary, there is only a single path.
The details ofthis issue are further discussed in Section 4.3 Constraints Shape TopicsAs discussed above, LDA views topics as distribu-tions over words, and each document expresses anadmixture of these topics.
For ?vanilla?
LDA (no con-straints), these are symmetric Dirichlet distributions.A document is composed of a number of observedwords, which we call tokens to distinguish specificobservations from the more abstract word (type) as-sociated with each token.
Because LDA assumesa document?s tokens are interchangeable, it treatsthe document as a bag-of-words, ignoring potentialrelations between words.This problem with vanilla LDA can be solved byencoding constraints, which will ?guide?
differentwords into the same topic.
Constraints can be addedto vanilla LDA by replacing the multinomial distri-bution over words for each topic with a collection of249tree-structured multinomial distributions drawn froma prior as depicted in Figure 1.
By encoding worddistributions as a tree, we can preserve conjugacyand relatively simple inference while encouragingcorrelations between related concepts (Boyd-Graberet al, 2007; Andrzejewski et al, 2009; Boyd-Graberand Resnik, 2010).
Each topic has a top-level dis-tribution over words and constraints, and each con-straint in each topic has second-level distributionover the words in the constraint.
Critically, the per-constraint distribution over words is engineered to benon-sparse and close to uniform.
The top level distri-bution encodes which constraints (and unconstrainedwords) to include; the lower-level distribution forcesthe probabilities to be correlated for each of the con-straints.In LDA, a document?s token is produced in thegenerative process by choosing a topic z and sam-pling a word from the multinomial distribution ?z oftopic z.
For a constrained topic, the process now cantake two steps.
First, a first-level node in the tree isselected from ?z .
If that is an unconstrained word,the word is emitted and the generative process forthat token is done.
Otherwise, if the first level nodeis constraint l, then choose a word to emit from theconstraint?s distribution over words piz,l.More concretely, suppose for a corpus with Mdocuments we have a set of constraints ?.
The priorstructure has B branches (one branch for each wordnot in a constraint and one for each constraint).
Thenthe generative process for constrained LDA is:1.
For each topic i ?
{1, .
.
.K}:(a) draw a distribution over the B branches (words andconstraints) ?i ?
Dir(~?
), and(b) for each constraint ?j ?
?, draw a distribution overthe words in the constraint pii,j ?
Dir(?
), wherepii,j is a distribution over the words in ?j2.
Then for each document d ?
{1, .
.
.M}:(a) first draw a distribution over topics ?d ?
Dir(?
),(b) then for each token n ?
{1, .
.
.
Nd}:i. choose a topic assignment zd,n ?
Mult(?d),and thenii.
choose either a constraint or word fromMult(?zd,n):A. if we chose a word, emit that word wd,nB.
otherwise if we chose a constraint index ld,n,emit a word wd,n from the constraint?s dis-tribution over words in topic zd,n: wd,n ?Mult(pizd,n,ld,n).In this model, ?, ?, and ?
are Dirichlet hyperpa-rameters set by the user; their role is explained below.3.1 Gibbs Sampling for Topic ModelsIn topic modeling, collapsed Gibbs sampling (Grif-fiths and Steyvers, 2004) is a standard procedure forobtaining a Markov chain over the latent variablesin the model.
Given certain technical conditions,the stationary distribution of the Markov chain isthe posterior (Neal, 1993).
Given M documents thestate of a Gibbs sampler for LDA consists of topicassignments for each token in the corpus and is rep-resented as Z = {z1,1 .
.
.
z1,N1 , z2,1, .
.
.
zM,NM }.
Ineach iteration, every token?s topic assignment zd,nis resampled based on topic assignments for all thetokens except for zd,n.
(This subset of the state isdenoted Z?(d,n)).
The sampling equation for zd,n isp(zd,n = k|Z?
(d,n), ?, ?)
?Td,k + ?Td,?
+K?Pk,wd,n + ?Pk,?
+ V ?
(1)where Td,k is the number of times topic k is used indocument d, Pk,wd,n is the number of times the typewd,n is assigned to topic k, and ?, ?
are the hyperpa-rameters of the two Dirichlet distributions, and B isthe number of top-level branches (this is the vocab-ulary size for vanilla LDA).
When a dot replaces asubscript of a count, it represents the marginal sumover all possible topics or words, e.g.
Td,?
=?k Td,k.The count statistics P and T provide summaries ofthe state.
Typically, these only change based on as-signments of latent variables in the sampler; in Sec-tion 4 we describe how changes in the model?s struc-ture (in addition to the latent state) can be reflectedin these count statistics.Contrasting with the above inference is the infer-ence for a constrained model.
(For a derivation, seeBoyd-Graber, Blei, and Zhu (2007) for the generalcase or Andrzejewski, Zhu, and Craven (2009) forthe specific case of constraints.)
In this case thesampling equation for zd,n is changed to p(zd,n =k|Z?
(d,n), ?, ?, ?)?????
?Td,k+?Td,?+K?Pk,wd,n+?Pk,?+V ?if ?l, wd,n 6?
?lTd,k+?Td,?+K?Pk,l+Cl?Pk,?+V ?Wk,l,wd,n+?Wk,l,?+Cl?wd,n ?
?l, (2)where Pk,wd,n is the number of times the uncon-strained word wd,n appears in topic k; Pk,l is the250number of times any word of constraint ?l appears intopic k; Wk,l,wd,n is the number of times word wd,nappears in constraint ?l in topic k; V is the vocabu-lary size; Cl is the number of words in constraint ?l.Note the differences between these two samplers forconstrained words; however, for unconstrained LDAand for unconstrained words in constrained LDA, theconditional probability is the same.In order to make the constraints effective, we setthe constraint word-distribution hyperparameter ?to be much larger than the hyperparameter for thedistribution over constraints and vocabulary ?.
Thisgives the constraints higher weight.
Normally, esti-mating hyperparameters is important for topic mod-eling (Wallach et al, 2009).
However, in ITM, sam-pling hyperparameters often (but not always) undoesthe constraints (by making ?
comparable to ?
), so wekeep the hyperparameters fixed.4 Interactively adding constraintsFor a static model, inference in ITM is the same asin previous models (Andrzejewski et al, 2009).
Inthis section, we detail how interactively changingconstraints can be accommodated in ITM, smoothlytransitioning from unconstrained LDA (n.b.
Equa-tion 1) to constrained LDA (n.b.
Equation 2) with oneconstraint, to constrained LDA with two constraints,etc.A central tool that we will use is the strategic unas-signment of states, which we call ablation (distinctfrom feature ablation in supervised learning).
Asdescribed in the previous section, a sampler storesthe topic assignment of each token.
In the implemen-tation of a Gibbs sampler, unassignment is done bysetting a token?s topic assignment to an invalid topic(e.g.
-1, as we use here) and decrementing any countsassociated with that word.The constraints created by users implicitly signalthat words in constraints don?t belong in a giventopic.
In other models, this input is sometimes usedto ?fix,?
i.e.
deterministically hold constant topic as-signments (Ramage et al, 2009).
Instead, we changethe underlying model, using the current topic assign-ments as a starting position for a new Markov chainwith some states strategically unassigned.
How muchof the existing topic assignments we use leads to fourdifferent options, which are illustrated in Figure 2.Previous New[bark:2, dog:3, leash:3 dog:2][bark:2, bark:2, plant:2, tree:3][tree:2,play:2,forest:1,leash:2][bark:2, dog:3, leash:3 dog:2][bark:2, bark:2, plant:2, tree:3][tree:2,play:2,forest:1,leash:2][bark:2, dog:3, leash:3 dog:2][bark:2, bark:2, plant:2, tree:3][tree:2,play:2,forest:1,leash:2][bark:-1, dog:-1, leash:-1 dog:-1][bark:-1, bark:-1, plant:-1, tree:-1][tree:2,play:2,forest:1,leash:2][bark:2, dog:3, leash:3 dog:3][bark:2, bark:2, plant:2, tree:3][tree:2,play:2,forest:1,leash:2][bark:-1, dog:-1, leash:3 dog:-1][bark:-1, bark:-1, plant:2, tree:3][tree:2,play:2,forest:1,leash:2][bark:2, dog:3, leash:3 dog:2][bark:2, bark:2, plant:2, tree:3][tree:2,play:2,forest:1,leash:2][bark:-1, dog:-1, leash:-1 dog:-1][bark:-1, bark:-1, plant:-1, tree:-1][tree:-1,play:-1,forest:-1,leash:-1]NoneTermDocAllFigure 2: Four different strategies for state ablation afterthe words ?dog?
and ?bark?
are added to the constraint{?leash,?
?puppy?}
to make the constraint {?dog,?
?bark,??leash,?
?puppy?}.
The state is represented by showing thecurrent topic assignment after each word (e.g.
?leash?
inthe first document has topic 3, while ?forest?
in the thirddocument has topic 1).
On the left are the assignmentsbefore words were added to constraints, and on the rightare the ablated assignments.
Unassigned words are giventhe new topic assignment -1 and are highlighted in red.All We could revoke all state assignments, essen-tially starting the sampler from scratch.
This doesnot allow interactive refinement, as there is nothingto enforce that the new topics will be in any wayconsistent with the existing topics.
Once the topicassignments of all states are revoked, the counts forT , P and W (as described in Section 3.1) will bezero, retaining no information about the state the userobserved.Doc Because topic models treat the document con-text as exchangeable, a document is a natural contextfor partial state ablation.
Thus if a user adds a set ofwords S to constraints, then we have reason to sus-pect that all documents containing any one of S mayhave incorrect topic assignments.
This is reflectedin the state of the sampler by performing the UNAS-SIGN (Algorithm 1) operation for each word in anydocument containing a word added to a constraint.Algorithm 1 UNASSIGN(d, n, wd,n, zd,n = k)1: T : Td,k ?
Td,k ?
12: If wd,n /?
?old,P : Pk,wd,n ?
Pk,wd,n ?
13: Else: suppose wd,n ?
?oldm ,P : Pk,m ?
Pk,m ?
1W : Wk,m,wd,n ?Wk,m,wd,n ?
1251This is equivalent to the Gibbs2 sampler of Yaoet al (2009) for incorporating new documents ina streaming context.
Viewed in this light, a useris using words to select documents that should betreated as ?new?
for this refined model.Term Another option is to perform ablation onlyon the topic assignments of tokens whose words haveadded to a constraint.
This applies the unassignmentoperation (Algorithm 1) only to tokens whose corre-sponding word appears in added constraints (i.e.
asubset of the Doc strategy).
This makes it less likelythat other tokens in similar contexts will follow thewords explicitly included in the constraints to newtopic assignments.None The final option is to move words into con-straints but keep the topic assignments fixed.
Thus,P and W change, but not T , as described in Algo-rithm 2.3 This is arguably the simplest option, andin principle is sufficient, as the Markov chain shouldfind a stationary distribution regardless of the startingposition.
In practice, however, this strategy is lessinteractive, as users don?t feel that their constraintsare actually incorporated in the model, and inertiacan keep the chain from reflecting the constraints.Algorithm 2 MOVE(d, n, wd,n, zd,n = k,?l)1: If wd,n /?
?old,P : Pk,wd,n ?
Pk,wd,n ?
1, Pk,l ?
Pk,l + 1W : Wk,l,wd,n ?Wk,l,wd,n + 12: Else, suppose wd,n ?
?oldm ,P : Pk,m ?
Pk,m ?
1, Pk,l ?
Pk,l + 1W : Wk,m,wd,n ?Wk,m,wd,n ?
1Wk,l,wd,n ?Wk,l,wd,n + 1Regardless of what ablation scheme is used, afterthe state of the Markov chain is altered, the nextstep is to actually run inference forward, samplingassignments for the unassigned tokens for the ?first?time and changing the topic assignment of previouslyassigned tokens.
How many additional iterations are3This assumes that there is only one possible path in the con-straint tree that can generate a word; in other words, this as-sumes that constraints are transitive, as discussed at the end ofSection 2.
In the more general case, when words lack a uniquepath in the constraint tree, an additional latent variable specifieswhich possible paths in the constraint tree produced the word;this would have to be sampled.
All other updating strategiesare immune to this complication, as the assignments are leftunassigned.required after adding constraints is a delicate tradeoffbetween interactivity and effectiveness, which weinvestigate further in the next sections.5 Motivating ExampleTo examine the viability of ITM, we begin with aqualitative demonstration that shows the potentialusefulness of ITM.
For this task, we used a corpusof about 2000 New York Times editorials from theyears 1987 to 1996.
We started by finding 20 initialtopics with no constraints, as shown in Table 1 (left).Notice that topics 1 and 20 both deal with Russia.Topic 20 seems to be about the Soviet Union, withtopic 1 about the post-Soviet years.
We wanted tocombine the two into a single topic, so we created aconstraint with all of the clearly Russian or Sovietwords (boris, communist, gorbachev, mikhail, russia,russian, soviet, union, yeltsin ).
Running inferenceforward 100 iterations with the Doc ablation strat-egy yields the topics in Table 1 (right).
The twoRussia topics were combined into Topic 20.
Thiscombination also pulled in other relevant words thatnot near the top of either topic before: ?moscow?and ?relations.?
Topic 1 is now more about electionsin countries other than Russia.
The other 18 topicschanged little.While we combined the Russian topics, other re-searchers analyzing large corpora might preserve theSoviet vs. post-Soviet distinction but combine topicsabout American government.
ITM allows tuning forspecific tasks.6 Simulation ExperimentNext, we consider a process for evaluating our ITMusing automatically derived constraints.
These con-straints are meant to simulate a user with a predefinedlist of categories (e.g.
reviewers for journal submis-sions, e-mail folders, etc.).
The categories grow moreand more specific during the session as the simulatedusers add more constraint words.To test the ability of ITM to discover relevantsubdivisions in a corpus, we use a dataset with pre-defined, intrinsic labels and assess how well the dis-covered latent topic structure can reproduce the cor-pus?s inherent structure.
Specifically, for a corpuswith M classes, we use the per-document topic dis-tribution as a feature vector in a supervised classi-252Topic Words1election, yeltsin, russian, political, party, democratic, russia, presi-dent, democracy, boris, country, south, years, month, government, vote,since, leader, presidential, military2new, york, city, state, mayor, budget, giuliani, council, cuomo, gov,plan, year, rudolph, dinkins, lead, need, governor, legislature, pataki,david3nuclear, arms, weapon, defense, treaty, missile, world, unite, yet, soviet,lead, secretary, would, control, korea, intelligence, test, nation, country,testing4president, bush, administration, clinton, american, force, reagan, war,unite, lead, economic, iraq, congress, america, iraqi, policy, aid, inter-national, military, see...20soviet, lead, gorbachev, union, west, mikhail, reform, change, europe,leaders, poland, communist, know, old, right, human, washington,western, bring, partyTopic Words1election, democratic, south, country, president, party, africa, lead, even,democracy, leader, presidential, week, politics, minister, percent, voter,last, month, years2new, york, city, state, mayor, budget, council, giuliani, gov, cuomo,year, rudolph, dinkins, legislature, plan, david, governor, pataki, need,cut3 nuclear, arms, weapon, treaty, defense, war, missile, may, come, test,american, world, would, need, lead, get, join, yet, clinton, nation4president, administration, bush, clinton, war, unite, force, reagan, amer-ican, america, make, nation, military, iraq, iraqi, troops, international,country, yesterday, plan...20soviet, union, economic, reform, yeltsin, russian, lead, russia, gor-bachev, leaders, west, president, boris, moscow, europe, poland,mikhail, communist, power, relationsTable 1: Five topics from a 20 topic topic model on the editorials from the New York times before adding a constraint(left) and after (right).
After the constraint was added, which encouraged Russian and Soviet terms to be in the sametopic, non-Russian terms gained increased prominence in Topic 1, and ?Moscow?
(which was not part of the constraint)appeared in Topic 20.fier (Hall et al, 2009).
The lower the classificationerror rate, the better the model has captured the struc-ture of the corpus.46.1 Generating automatic constraintsWe used the 20 Newsgroups corpus, which contains18846 documents divided into 20 constituent news-groups.
We use these newsgroups as ground-truthlabels.5We simulate a user?s constraints by ranking wordsin the training split by their information gain (IG).6After ranking the top 200 words for each classby IG, we delete words associated with multiplelabels to prevent constraints for different labelsfrom merging.
The smallest class had 21 wordsremaining after removing duplicates (due to high4Our goal is to understand the phenomena of ITM, not classifica-tion, so these classification results are well below state of theart.
However, adding interactively selected topics to the stateof the art features (tf-idf unigrams) gives a relative error reduc-tion of 5.1%, while just adding topics from vanilla LDA givesa relative error reduction of 1.1%.
Both measurements wereobtained without tuning or weighting features, so presumablybetter results are possible.5http://people.csail.mit.edu/jrennie/20Newsgroups/In preprocessing, we deleted short documents, leaving 15160documents, including 9131 training documents and 6029 testdocuments (default split).
Tokenization, lemmatization, andstopword removal was performed using the Natural LanguageToolkit (Loper and Bird, 2002).
Topic modeling was performedusing the most frequent 5000 lemmas as the vocabulary.6IG is computed by the Rainbow toolboxhttp://www.cs.umass.edu/ mccallum/bow/rainbow/overlaps of 125 words between ?talk.religion.misc?and ?soc.religion.christian,?
and 110 words between?talk.religion.misc?
and ?alt.atheism?
), so the top 21words for each class were the ingredients for oursimulated constraints.
For example, for the class?soc.religion.christian,?
the 21 constraint words in-clude ?catholic, scripture, resurrection, pope, sab-bath, spiritual, pray, divine, doctrine, orthodox.?
Wesimulate a user?s ITM session by adding a word toeach of the 20 constraints until each of the constraintshas 21 words.6.2 Simulation schemeStarting with 100 base iterations, we perform suc-cessive rounds of refinement.
In each round a newconstraint is added corresponding to the newsgrouplabels.
Next, we perform one of the strategies forstate ablation, add additional iterations of Gibbs sam-pling, use the newly obtained topic distribution ofeach document as the feature vector, and performclassification on the test / train split.
We do this for21 rounds until each label has 21 constraint words.The number of LDA topics is set to 20 to match thenumber of newsgroups.
The hyperparameters for allexperiments are ?
= 0.1, ?
= 0.01, and ?
= 100.At 100 iterations, the chain is clearly not con-verged.
However, we chose this number of iterationsbecause it more closely matches the likely use case asusers do not wait for convergence.
Moreover, whileinvestigations showed that the patterns shown in Fig-253ure 4 were broadly consistent with larger numbersof iterations, such configurations sometimes had toomuch inertia to escape from local extrema.
More iter-ations make it harder for the constraints to influencethe topic assignment.6.3 Investigating Ablation StrategiesFirst, we investigate which ablation strategy best al-lows constraints to be incorporated.
Figure 3 showsthe classification error of six different ablation strate-gies based on the number of words in each constraint,ranging from 0 to 21.
Each is averaged over five dif-ferent chains using 10 additional iterations of Gibbssampling per round (other numbers of iterations arediscussed in Section 6.4).
The model runs forward 10iterations after the first round, another 10 iterationsafter the second round, etc.
In general, as the numberof words per constraint increases, the error decreasesas models gain more information about the classes.Strategy Null is the non-interactive baseline thatcontains no constraints (vanilla LDA), but runs infer-ence for a comparable number of rounds.
All Initialand All Full are non-interactive baselines with allconstraints known a priori.
All Initial runs the modelfor the only the initial number of iterations (100 it-erations in this experiment), while All Full runs themodel for the total number of iterations added for theinteractive version.
(That is, if there were 21 roundsand each round of interactive modeling added 10 iter-ations, All Full would have 210 iterations more thanAll Initial).While Null sees no constraints, it serves as anupper baseline for the error rate (lower error beingbetter) but shows the effect of additional inference.All Full is a lower baseline for the error rate sinceit both sees the constraints at the beginning and alsoruns for the maximum number of total iterations.
AllInitial sees the constraints before the other ablationtechniques but it has fewer total iterations.The Null strategy does not perform as well asthe interactive versions, especially with larger con-straints.
Both All Initial and All Full, however, showa larger variance (as denoted by error bands aroundthe average trends) than the interactive schemes.
Thiscan be viewed as akin to simulated annealing, as theinteractive search has more freedom to explore inearly rounds.
As more constraint words are addedeach round, the model is less free to explore.Words per constraintError0.380.400.420.440.460.480.500 5 10 15 20StrategyAll FullAll InitialDocNoneNullTermFigure 3: Error rate (y-axis, lower is better) using differentablation strategies as additional constraints are added (x-axis).
Null represents standard LDA, as the unconstrainedbaseline.
All Initial and All Full are non-interactive, con-strained baselines.
The results of None, Term, Doc aremore stable (as denoted by the error bars), and the errorrate is reduced gradually as more constraint words areadded.The error rate of each interactive ablation strategyis (as expected) between the lower and upper base-lines.
Generally, the constraints will influence notonly the topics of the constraint words, but also thetopics of the constraint words?
context in the samedocument.
Doc ablation gives more freedom for theconstraints to overcome the inertia of the old topicdistribution and move towards a new one influencedby the constraints.6.4 How many iterations do users have to wait?Figure 4 shows the effect of using different numbersof Gibbs sampling iterations after changing a con-straint.
For each of the ablation strategies, we run{10, 20, 30, 50, 100} additional Gibbs sampling iter-ations.
As expected, more iterations reduce error,although improvements diminish beyond 100 itera-tions.
With more constraints, the impact of additionaliterations is lessened, as the model has more a prioriknowledge to draw upon.For all numbers of additional iterations, while theNull serves as the upper baseline on the error ratein all cases, the Doc ablation clearly outperformsthe other ablation schemes, consistently yielding alower error rate.
Thus, there is a benefit when themodel has a chance to relearn the document contextwhen constraints are added.
The difference is evenlarger with more iterations, suggesting Doc needsmore iterations to ?recover?
from unassignment.The luxury of having hundreds or thousands ofadditional iterations for each constraint would be im-254Words per constraintError0.400.420.440.460.480.50100 5 10 15 20200 5 10 15 20300 5 10 15 20500 5 10 15 201000 5 10 15 20StrategyDocNoneNullTermFigure 4: Classification accuracy by strategy and number of additional iterations.
The Doc ablation strategy performsbest, suggesting that the document context is important for ablation constraints.
While more iterations are better, thereis a tradeoff with interactivity.practical.
For even moderately sized datasets, evenone iteration per second can tax the patience of in-dividuals who want to use the system interactively.Based on these results and an ad hoc qualitative ex-amination of the resulting topics, we found that 30additional iterations of inference was acceptable; thisis used in later experiments.7 Getting Humans in the LoopTo move beyond using simulated users adding thesame words regardless of what topics were discov-ered by the model, we needed to expose the modelto human users.
We solicited approximately 200judgments from Mechanical Turk, a popular crowd-sourcing platform that has been used to gather lin-guistic annotations (Snow et al, 2008), measure topicquality (Chang et al, 2009), and supplement tradi-tional inference techniques for topic models (Chang,2010).
After presenting our interface for collectingjudgments, we examine the results from these ITMsessions both quantitatively and qualitatively.7.1 Interface for soliciting refinementsFigure 5 shows the interface used in the MechanicalTurk tests.
The left side of the screen shows thecurrent topics in a scrollable list, with the top 30words displayed for each topic.Users create constraints by clicking on words fromthe topic word lists.
The word lists use a color-codingscheme to help the users keep track of which wordsthey are currently grouping into constraints.
The rightside of the screen displays the existing constraints.Users can click on icons to edit or delete each one.The constraint currently being built is also shown.Figure 5: Interface for Mechanical Turk experiments.Users see the topics discovered by the model and selectwords (by clicking on them) to build constraints to beadded to the model.Clicking on a word will remove that word from thecurrent constraint.As in Section 6, we can compute the classificationerror for these users as they add words to constraints.The best users, who seemed to understand the taskwell, were able to decrease classification error.
(Fig-ure 6).
The median user, however, had an error re-duction indistinguishable from zero.
Despite this, wecan examine the users?
behavior to better understandtheir goals and how they interact with the system.7.2 Untrained users and ITMMost of the large (10+ word) user-created constraintscorresponded to the themes of the individual news-groups, which users were able to infer from thediscovered topics.
Common constraint themes that255RoundRelativeError0.940.960.981.000 1 2 3 4Best Session10 Topics20 Topics50 Topics75 TopicsFigure 6: The relative error rate (using round 0 as a base-line) of the best Mechanical Turk user session for each ofthe four numbers of topics.
While the 10-topic model doesnot provide enough flexibility to create good constraints,the best users could clearly improve classification withmore topics.matched specific newsgroups included religion, spaceexploration, graphics, and encryption.
Other com-mon themes were broader than individual news-groups (e.g.
sports, government and computers).
Oth-ers matched sub-topics of a single newsgroup, suchas homosexuality, Israel or computer programming.Some users created inscrutable constraints, like(?better, people, right, take, things?)
and (?fbi, let,says?).
They may have just clicked random words tofinish the task quickly.
While subsequent users coulddelete poor constraints, most chose not to.
Becausewe wanted to understand broader behavior we madeno effort to squelch such responses.The two-word constraints illustrate an interestingcontrast.
Some pairs are linked together in the corpus,like (?jesus, christ?)
and (?solar, sun?).
With others,like (?even, number?)
and (?book, list?
), the usersseem to be encouraging collocations to be in thesame topic.
However, the collocations may not be inany document in this corpus.
Another user created aconstraint consisting of male first names.
A topic didemerge with these words, but the rest of the wordsin that topic seemed random, as male first names arenot likely to co-occur in the same document.Not all sensible constraints led to successful topicchanges.
Many users grouped ?mac?
and ?windows?together, but they were almost never placed in thesame topic.
The corpus includes separate newsgroupsfor Macintosh and Windows hardware, and divergentcontexts of ?mac?
and ?windows?
overpowered theprior distribution.The constraint size ranged from one word to over40.
In general, the more words in the constraint,the more likely it was to noticeably affect the topicdistribution.
This observation makes sense givenour ablation method.
A constraint with more wordswill cause the topic assignments to be reset for moredocuments.8 DiscussionIn this work, we introduced a means for end-usersto refine and improve the topics discovered by topicmodels.
ITM offers a paradigm for non-specialistconsumers of machine learning algorithms to refinemodels to better reflect their interests and needs.
Wedemonstrated that even novice users are able to under-stand and build constraints using a simple interfaceand that their constraints can improve the model?sability to capture the latent structure of a corpus.As presented here, the technique for incorporatingconstraints is closely tied to inference with Gibbssampling.
However, most inference techniques areessentially optimization problems.
As long as it ispossible to define a transition on the state space thatmoves from one less-constrained model to anothermore-constrained model, other inference procedurescan also be used.We hope to engage these algorithms with moresophisticated users than those on Mechanical Turkto measure how these models can help them betterexplore and understand large, uncurated data sets.
Aswe learn their needs, we can add more avenues forinteracting with topic models.AcknowledgementsWe would like to thank the anonymous reviewers, Ed-mund Talley, Jonathan Chang, and Philip Resnik fortheir helpful comments on drafts of this paper.
Thiswork was supported by NSF grant #0705832.
JordanBoyd-Graber is also supported by the Army ResearchLaboratory through ARL Cooperative AgreementW911NF-09-2-0072 and by NSF grant #1018625.Any opinions, findings, conclusions, or recommenda-tions expressed are the authors?
and do not necessar-ily reflect those of the sponsors.256ReferencesDavid Andrzejewski, Xiaojin Zhu, and Mark Craven.2009.
Incorporating domain knowledge into topic mod-eling via Dirichlet forest priors.
In Proceedings ofInternational Conference of Machine Learning.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
Journal of Machine Learn-ing Research, 3:993?1022.Jordan Boyd-Graber and David M. Blei.
2008.
Syntactictopic models.
In Proceedings of Advances in NeuralInformation Processing Systems.Jordan Boyd-Graber and Philip Resnik.
2010.
Holisticsentiment analysis across languages: Multilingual su-pervised latent Dirichlet alocation.
In Proceedings ofEmperical Methods in Natural Language Processing.Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambiguation.In Proceedings of Emperical Methods in Natural Lan-guage Processing.Jonathan Chang, Jordan Boyd-Graber, Chong Wang, SeanGerrish, and David M. Blei.
2009.
Reading tea leaves:How humans interpret topic models.
In Neural Infor-mation Processing Systems.Jonathan Chang.
2010.
Not-so-latent Dirichlet alocation:Collapsed Gibbs sampling using human judgments.
InNAACL Workshop: Creating Speech and LanguageData With Amazon?ss Mechanical Turk.Hal Daume?
III.
2009.
Markov random topic fields.
InProceedings of Artificial Intelligence and Statistics.Laura Dietz, Steffen Bickel, and Tobias Scheffer.
2007.Unsupervised prediction of citation influences.
In Pro-ceedings of International Conference of Machine Learn-ing.Thomas L. Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
Proceedings of the National Academyof Sciences, 101(Suppl 1):5228?5235.Amit Gruber, Michael Rosen-Zvi, and Yair Weiss.
2007.Hidden topic Markov models.
In Artificial Intelligenceand Statistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explorations, 11(1):10?18.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In Proceedings of Uncertainty in ArtificialIntelligence.Thomas K. Landauer, Danielle S. McNamara, Dennis S.Marynick, and Walter Kintsch, editors.
2006.
Proba-bilistic Topic Models.
Laurence Erlbaum.Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexan-der Hauptmann.
2006.
Which side are you on?
identi-fying perspectives at the document and sentence levels.In Proceedings of the Conference on Natural LanguageLearning (CoNLL).Edward Loper and Steven Bird.
2002.
NLTK: the natu-ral language toolkit.
In Tools and methodologies forteaching.Radford M. Neal.
1993.
Probabilistic inference usingMarkov chain Monte Carlo methods.
Technical ReportCRG-TR-93-1, University of Toronto.David Newman, Jey Han Lau, Karl Grieser, and TimothyBaldwin.
2010.
Automatic evaluation of topic coher-ence.
In Conference of the North American Chapter ofthe Association for Computational Linguistics.Michael Paul and Roxana Girju.
2010.
A two-dimensional topic-aspect model for discovering multi-faceted topics.
In Association for the Advancement ofArtificial Intelligence.James Petterson, Smola Alex, Tiberio Caetano, Wray Bun-tine, and Narayanamurthy Shravan.
2010.
Word fea-tures for latent Dirichlet alocation.
In Neural Informa-tion Processing Systems.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled LDA: Asupervised topic model for credit attribution in multi-labeled corpora.
In Proceedings of Emperical Methodsin Natural Language Processing.Fergus Rob, Li Fei-Fei, Perona Pietro, and Zisserman An-drew.
2005.
Learning object categories from Google?simage search.
In International Conference on Com-puter Vision.Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,and Padhraic Smyth.
2004.
The author-topic model forauthors and documents.
In Proceedings of Uncertaintyin Artificial Intelligence.Suyash Shringarpure and Eric P. Xing.
2008. mStruct:a new admixture model for inference of populationstructure in light of both genetic admixing and allelemutations.
In Proceedings of International Conferenceof Machine Learning.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and fast?but is it good?
Evalu-ating non-expert annotations for natural language tasks.In Proceedings of Emperical Methods in Natural Lan-guage Processing.Hanna Wallach, David Mimno, and Andrew McCallum.2009.
Rethinking LDA: Why priors matter.
In Pro-ceedings of Advances in Neural Information ProcessingSystems.Hanna M. Wallach.
2006.
Topic modeling: Beyond bag-of-words.
In Proceedings of International Conferenceof Machine Learning.Limin Yao, David Mimno, and Andrew McCallum.
2009.Efficient methods for topic model inference on stream-ing document collections.
In Knowledge Discovery andData Mining.257
