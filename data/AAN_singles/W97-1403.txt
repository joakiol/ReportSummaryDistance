Towards Generation of Fluent Referring Act ionin Mult imodal  SituationsTsuneak i  KatoNTT  Information andCommunicat ion  Systems Labs.Yokosuka, Kanagawa 239, JAPANkat o?nttnly, isl.
ntt.
co. jpYuk iko  I. NakanoNTT  Informat ion andCommunicat ion  Systems Labs.Yokosuka, Kanagawa 239, JAPANyukiko?nttnly, isl.
ntt.
co. jpAbst ractReferring actions in multimodal situationscan be thought of as linguistic expressionswell coordinated with several physical ac-tions.
In this paper, what patterns of lin-guistic expressions are commonly used andhow physical actions are temporally coordi-nated to them are reported based on corpusexaminations.
In particular, by categoriz-ing objects according to two features, visi-bility and membership, the schematic pat-terns of referring expressions are derived.The difference between the occurrence fre-quencies of those patterns in a multimodalsituation and a spoken-mode situation ex-plains the findings of our previous research.Implementation based on these results is ongoing.1 In t roduct ionA lot of active studies have been conducted on thetemporal coordination of natural language and vi-sual information.
The visual information consideredincludes pointing gestures (Andrd & Rist, 1996), fa-cial expressions and iconic gestures (Cassell et al,1994), and graphical effects uch as highlighting andblinking (Dalal et al, 1996; Feiner et ah, 1993).Among those we have been focusing on generat-ing effective xplanations by using natural anguagetemporally coordinated with pictures and gestures.The experimental system we implemented is for ex-plaining the installation and operation of a telephonewith an answering machine feature, and simulatesinstruction dialogues performed by an expert in aface-to-face situation with a telephone in front of her(Kato et al, 1996).
The system explains by usingsynthesized speech coordinated with pointing ges-tures from a caricatured agent and simulated opera-tions implemented by the switching of figures.
Oneof the important issues for enhancing this type ofsystem is to shed light on what makes referring ac-tions fluent in multimodal situations and to build amechanism to generate such fluent actions.We also empirically investigated how communica-tive modes influence the content and style of re-ferring actions made in dialogues (Kato & Nakano,1995).
Experiments were conducted to obtain a cor-pus consisting of human-to-human i struction di-alogues on telephone installation in two settings.One is a spoken-mode dialogue situation (SMD here-after), in which explanations are given using justvoice.
The other is a multimodal dialogue situation(MMD hereafter), in which both voice and visual in-formation, mainly the current state and outlook ofthe expert's telephone and her pointing gestures toit, can be communicated.
Detailed analysis of the re-ferring actions observed in that corpus revealed thefollowing two properties.P I :  The availability of pointing, communicationthrough the visual channel reduces the amountof information conveyed through the speech orlinguistic channel.
In initial identification, theusage of linguistic expressions on shape/size,characters/marks, and related objects decreasesin MMD, while the usage of position informa-tion does not decrease.P2: In SMD, referring actions tend to be realized toan explicit goal and divided into a series of fine-grained steps.
The participants try to achievethem step by step with many confirmations.Although our findings were very suggestive for an-alyzing the properties of referring actions in multi-modal situations, they were still descriptive and notsufficient o allow their use in designing referringaction generation mechanisms.
Then, as the nextstep, we have been examining that corpus closer andtrying to derive some schemata of referring actions,which would be useful for implementation of mul-timodal dialogue systems.
This paper reports theresults of these activities.Two short comments must be made to make ourresearch standpoint clearer.
First, our purpose is togenerate referring actions that model human refer-ring actions in mundane situations.
Theoreticallyspeaking, as Appelt pointed out, it is enough forreferring to provide sufficient description to distin-Towards Generation of Fluent Referring Action in Multimodal Situations 21guish one object from the other candidates (Appelt,1985).
For example, a pointing action to the objectmust be enough, or description of the object's posi-tion, such as "the upper left button of the dial but-tons" also must be considered sufficient.
However,we often observe referring actions that consist of alinguistic expression, "a small button with the markof a handset above and to the left of the dial but-tons", accompanied with a pointing gesture.
Sucha referring action is familiar to us even though it isredundant from a theoretical viewpoint.
Such famil-iar actions that the recipient does not perceive asawkward is called fluent in this paper.
Our objec-tive is to generate such fluent referring actions, andis rather different from those of (Appelt, 1985) and(Dale & Haddock, 1991).Second, in our research, a referring action is con-sidered as the entire sequence of actions needed forallowing the addressee to identify the intended ob-ject and incorporating its achievement into part ofthe participants' hared knowledge.
In order to referto an object in a box, an imperative sentence such as"Open the box, and look inside" may be used.
Sucha request shifts the addressee's attention, and to seeit as a part of the referring action may be problem-atic.
It is, however, reasonable to think that boththe request for looking into the box and the assertionof the fact that an object is in the box come fromdifferent plans for achieving the same goal, identify-ing the object.
As Cohen claimed that it is usefulto understand referring expressions from the view-point of speech act planning (Cohen, 1984), it is notso ridiculous to go one step further and to considerthe entire sequence of actions, including attentionshifts, as an instance of a plan for object referring.Moreover, this approach better suits implementing areferring action generation mechanism as a planner.The next section describes what kinds of linguisticexpression are used for referring actions in MMD andcompares them with those in SMD.
In particular, bycategorizing objects according to two features: visi-bility and membership, schemata for object referringexpressions of each category are derived.
In the thirdsection, how several kinds of actions such as point-ing gestures are accompanied by such expressions ireported.
In the fourth section, implementation ofreferring action generation is discussed based on ourfindings described thus far.
Finally, in the last sec-tion, our findings are summarized and future workis discussed.2 Linguistic expression in referringactionsReferring actions in multimodal situations can bethought of as linguistic expressions well coordinatedwith several physical actions.
The linguistic expres-sions for referring to objects, referring expressions,are focused on in this section, and in the next sec-NIII i utt?n3III Butio:nl    IButton2Figure 1: The telephone used in the corpustion, how those expressions should be coordinatedwith actions is discussed.2.1 Ob ject  categor i zat ionThe top and left side views of the telephone usedare shown in Fig.
1.
Although the objects such asbuttons can be identified by using several featuressuch as position, color, shape, and size, the two fea-tures described below proved to be dominant in thereferring expressions used.Visibi l i ty: Some objects are located on the sideor back of the telephone, and can not be seenunless the body is turned over or looked into.Some objects lie underneath the cover, andopening that cover is needed in order to seethem.
Such objects are categorized into invis-ible ones and distinguished from visible ones,which are located on the top 1.Membersh ip :  Aligned buttons of the same shapeand color are usually recognized as a group.Members of such a group are distinguished fromisolated ones 2.In Fig.
1, socket 1 on the side is invisible and iso-lated, button 1 on the left of the top surface is visibleand isolated, button 2 on the lower right of the topsurface is a visible group member, and button 3 onthe upper right is an invisible group member as it isunderneath a cassette cover usually closed.According to this categorization, we determinedwhich patterns of referring expressions were fre-quently observed for each type of object.
Thepatterns thus extracted can be expected to yieldtAs you have already realized, this feature is not in-trinsic to the object, but depends on the state of thetelephone when the object is referred to.
Buttons un-derneath the cover are visible when the cover is open.2The recognition of a group may differ among people.In daily life, however, we believe an effective level ofconsensus can be attained.22 T. Kato and Y.L Nakanoschemata for referring expression generation.
Threeexplanations of five experts in two situations, MMDand SMD, i.e.
fifteen explanations in each situation,were analyzed.
The apprentices differed with eachexplanation.
Every referring expression analyzed in-volved initial identification, which is used to makethe first effective reference to an object, and to in-troduce it into the explanation.
All objects werereferred to in the context in which the expert madethe apprentice identify it and then requested thatsome action be performed on it.
All explanationswere done in Japanese 3.2.2 Schemata  for re fer r ing  to v is ib lei so lated ob jec tsReferring actions to visible isolated objects arerather simple and were basic for all cases.
Two ma-jor patterns were observed and can be treated asthe schemata.
Table 1 shows these two schemata 4,called RS1 and RS2 hereafter.
RS1 contains twosentences.
The first asserts the existence of the ob-ject at a described position.
The second is an im-perative sentence for requesting that an action beperformed on the object identified.
In the first sen-tence, a postpositional phrase describing the objectposition precedes the object description.
The objectdescription is a noun phrase that has modifiers de-scribing features of the object such as its color orsize followed by a head common noun describing theobject category.
That  is, its structure is\[object description np\]\[feature description pp/comp\]  *\[object class name n\]In RS2,  the imperative sentence requesting anaction contains a description referring to the ob-ject.
This description has the same structure as RS1shown above.
In most cases, the first feature descrip-tion is a description of the object position.
In bothschemata, object position is conveyed first, other fea-tures second, and the requested action follows.
Thisorder of information flow seems natural for identify-ing the object and then acting on it.
Examples ofreferring expressions 5 that fit these schemata re3 Japanese is a head-final language.
Complements andpostpositional phrases on the left modify nouns or nounphrases on the right, and construct noun phrases.
Thatis, a simphfied version of Japanese grammar containspp --~ np p, np ~ pp rip, np --~ comp rip, and np ~ n.Sentences are constructed by a rule, s ~ pp* v. The or-der of pps is almost free syntactically, being determinedby pragmatic onstraints.
Older information precedesnewer (Kuno, 1978).4Schemata re represented as sequences of terminalsymbols, non terminal symbols each of those has a formof \[semantic content syntactic category\], and schema ID.A slash appearing in a syntactic ategory means optionsrather than a slash feature.tAll examples are basically extracted from the corpusexamined.
Those, however, were slightly modified by(1) daiarubotan no hidariue ni juwaki nodial-buttons upper-left LOC handsetmaaku ga tsuita chiisai botanmark SUBJ being-placed-on small buttonga arimasu, sore wo oshi tekudasai.SUBJ exist, it OBJ push REQUEST'On the upper left of the dial buttons, there is asmall button with the mark of a handset.
Pleasepush it.
'(2) daiarubotan o hidariue nojuwaki no maakudial-buttons upper-left handset markga tsuita chiisai botan wo oshiSUBJ being-placed-on small button OBJ pushtekudasai.REQUEST'Please push the small button with the mark ofa handset on the upper left of the dial buttons.
'In RS1,  the achievement of identification is con-firmed by making the first sentence a tag questionor putting a phrase for confirmation after that sen-tence.
Sometimes it is implicitly confirmed by as-serting the existence of the object as the speaker'sbelief.
In RS2, confirmation can be made by puttinga phrase for confirmation after the noun phrase de-scribing the object or by putting there a pause anda demonstrative pronoun appositively.Another pattern was observed in which RS1 waspreceded by an utterance referring to a landmarkused in the position description.
This is also shownin Table 1 as RS11.
In RS11,  reference to the land-mark is realized by an imperative sentence that di-rects attention to the landmark or a tag questionthat confirms its existence.
Examples are(3)(4)hontai no hidariue wo mi tekudasai, sokobody upper-left oaa look REQUEST thereni chiisai botan ga arimasu.LOC small button SUBJ exist'Please look at the upper left of the body.
Thereis a small button there.
'daiarubotan no 1 arimasu yone.
sonodial-button 1 exist CONFIRM itshidariue ni chiisai botan ga arimasu.upper-left LOC small button suBJ exist'There is dial button 1, isn't there?
On its up-per left, there is a small button.
'Table 1 shows the numbers of occurrences of eachpattern in MMD and SMD.
The total occurrencenumber was 30, as two objects fell under this cate-gory.
RS11 and RS1 frequently occur in SMD.removing non-fluencies and the diversities caused by thefactors mentioned in section 2.4 below.Towards Generation of Fluent Referring Action in Multimodal Situations 23Table 1: The schemata for referring to visible isolated objects and their occurrence frequencyID Pat tern /Descr i tp ion  MMD SMD\[position p\] ni(LoC)RS1 \[object description up\] ga(svBJ) arimasu(exist).
12 19\[object np\] wo(OBJ) \[action v\] tekudasai(REQUEST).RS2 \[object description up\] wo(OBJ) 13 5\[action v\] tekudasai(REQVESW)RS11 \[referring to a landmark s\], RS1 0 4Others 5 22.3 Schemata  for re fer r ing to invis ibleob jects  and  group  membersFive objects fell into the category of invisible isolatedobjects.
Two schemata described in the previoussubsection, RS1 and RS2, were used for referring tothese objects by conveying the fact of which surfacethe object was located on as the position description.For example,(5) hontai no hidarigawa ni sashikomiguchi gabody left-side LOC socket SUBJarimasu, soko ni sore wo ire tekudasai.exist there LOC it OBJ put REQUEST'There is a socket on the left side of the body.Please put it there.
'(6) sore wo hontai hidarigawa no sashikomiguchiit OBJ body left-side socketni ire tekudasaiLOC put REQUEST'Please put it into the socket on the left side ofthe body.
'In addition, RS11 and its RS2 corespondent,RS12, were used frequently.
In these patterns, thesurface on which the object is located is referred toin advance.
It is achieved by an imperative sentencethat directs attention to the surface or asks that thebody be turned, or by an description of the side fol-lowed by a confirmation.
Examples are(7) hontai hidarigawa no sokumen wo mibody left side OBJ looktekudasai, soko ni .
.
.REQUEST there LOC ...'Please look at the left side of the body.
Onthat side, .
.
.
'(8) hontai no hidari sokumen desu ne.body left side COPULA CONFIRMsoko no mannakani .. .there center LOC..
.
'The left side of the body, you see?
On the cen-ter of that side, .
.
.
'Table 2 shows the schemata based on these pat-terns and their numbers of occurrence; the total is75.
RS2 is frequently used in MMD, while RS11 isfrequently used in SMD.For referring to a visible group member, patternsare observed in which the group the object belongsto is referred to as a whole, in advance, and then theobject is referred to as a member of that group.
Thefirst sentence of RS1 is mainly used for referring tothe group as a whole.
For example,(9) daiarubotan no shita ni onaji iro nodial-buttons below LOC SAME colorbotan ga itsutsu narande imasu.buttons suBJ five aligned be'Below the dial buttons, there are five buttonsof the same color.
'After this, RS1 or RS2 follows.
These patterns,hereafter called RS21 and RS22, respectively, areshown in Table 3.
In each pattern, the relative posi-tion of the object in the group is used as the positioninformation conveyed later.
In RS21, the followingsentence, for example, follows the above.
(10) sono migihashi ni supiika no maaku gathose right-most LOC speaker mark SUBJtsuita botan ga arimasubeing-placed-on button SUBJ exist'On the right most of those, there is a buttonwith the mark of a speaker.
'RS1 and RS2, in which a referring expression to agroup does not constitute an utterance by itself arealso observed, such as(11) ichiban-shita no botan no retsu no migihashibottom buttons line right-mostni supiika no maaku ga tsuitaLOC speaker mark SUBJ being-placed-onbotan ga arimasu.button SUBJ exist'On the r ight  most of the line of buttons onthe bottom, there is a button with a mark of aspeaker.
'In the above, although the expression referring tothe group is part of the expression referring to the24 T. Kato and Y.L NakanoTable 2: The schemata for referring to invisible objects and their occurrence frequencyIDRS1RS2R$11RSi2OthersPat tern /Descr i tp ion  I MMD\[referring to the side s/np\], RS1\[referring to the side s/np\], RS2SMD16 1123 710 335 521 19member, information that the object is a member ofa specific group is conveyed and the position relativeto the group is used for describing the object's posi-tion.
There are other patterns which do not containsuch descriptions of groups at all.
For example,(12) hontai migishita no supiika botan wo oshibody right-lower speaker button oBz pushtekudasai.REQUEST"Push the speaker button on the lower right ofthe body.
'According to this characteristic, RS1 and RS2 aredivided into two patterns.
RS1 and RS2 with de-scriptions of a group are called RSI '  and RS2' re-spectively, and RS1 and RS2 without descriptionsof a group are called RSI" and RS2".
Table 3 showsthe numbers of occurrence.
The total number is 60,as four objects fell into this category 6.
RSI" andRS2" are frequently observed in MMD, while RS21and RS22 are frequently observed in SMD.Just one object was an invisible group member inour corpus.
It was the button underneath the cas-sette cover.
All referring expressions in both MMDand SMD contain an imperative sentence request-ing that the cassette cover be opened.
It is con-sidered that this imperative sentence corresponds tothe imperative sentences in RS11 and RS12 that di-rect attention to the side of the body or ask that thebody be turned.
Subsequent referring expressionsfollow the same patterns as for visible group mem-bers.
The distribution of the patterns is also similar.That is, the schemata for referring to invisible groupmembers are obtained as combinations of those forinvisible objects and group members.2.4 Factors  that  compl i ca te  re fer r ingexpress ionsThe previous two subsections derived the schematafor referring expressions in line with the objects' cat-egorization based on two features.
The schemata re6One object belonged to a group that contained anobject already referred to.
This implies that the grouphad already been identified.
The usage of RS21 andRS22 was relatively scarce for that object.
This sug-gests that referring expressions should be affected by thehistory of the group as well as of the object itself.just skeletons, and referring expressions with morediverse forms appear in the collected corpus.
Themost important origin of this diversity is that ex-planation dialogue is a cooperative process (Clark &Wilkes-Gibbs, 1990).
First, several stages of a refer-ring action can trigger confirmation.
Those confir-mations are realized by using various linguistic de-vices such as interrogative sentences, tag questions,and specific intonations.
Second, related to incre-mental elaboration, appositive and supplemental ex-pressions are observed.
For example,(13) rusu botan arimasu ne, gamen noOUT button exist CONFIRM displayshita, "rusu" to kakareta shiroi botan.under "OUT" with being-labeled white button'There is an OUT button, under the display, awhite button labeled "OUT.
"'These inherent dialogue features complicate re-ferring expressions.
Moreover, it is difficult to de-rive patterns from exchanges in which the appren-tice plays a more active role such as talking aboutor checking her idea on the procedure in advance.The second origin of diversity relates to the factthat experts ometimes try to achieve multiple goalsat the same time.
Labeling an object with a propername is sometimes achieved simultaneously withidentifying it.
This phenomena, however, could beschematized to some extent.
Two patterns are ob-served.
The one is to put the labeling sentence suchas "This is called the speaker button" after the firstsentence in RS1 or the noun phrase describing theobject in RS2.
The other is to use a proper name asthe head of the noun phrase describing the object.An example is "the speaker button with the mark ofa speaker".The third origin is the effect of the dialogue con-text which is determined external to the referringexpressions.
For example, almost half of the refer-ring expressions categorized into Others in the abovetables fit one of the following two patterns, calledRS3 hereafter.\[object function pp/comp\] \[object rip\] ga(SUBJ)\[position rip\] ni(LOC) arimasu(ezist).\[description f the features of the object s\] *Towards Generation of Fluent Referring Action in Multimodal Situations 25Table 3: The schemata for referring to group members and their occurrence frequencyIDRS$1RS22RSI'RS2'RS1"RS2"OthersPat tern /Descr i tp ion\[referring to the group s\], RSi\[referring to the group s\], RS2RS1 (with group descriptions)RS2 (with group descriptions)RS1 (w/o group descriptions)RS2 (w/o group descriptions)MMD SMD4 127 150 47 912 723 87 5\[object function pp/comp\] [object np\] ga(SUBJ)\[position pp/comp\] [object description up\]desu(COeVLa).Both patterns, which assert the features of theobject including its position, handle the availabilityof the object as old information.
Examples of RS3are(14) onryou wo chousetsusuru botan gavolume oBJ control button SUBJdaiarubotan o hidariue ni arimasu.dial-buttons upper-left LOC exist'The button for controlling the volume is lo-cated to the upper left of the dial buttons.
'(15) sonotame no botan ga daiarubotan nofor-it button suBJ dial-buttonshidariue ni aru chiisai botan desu.upper-left LOC exist small button COPULA'The button for it is the small button to theupper left of the dial buttons.
'These patterns are used when the existence of aspecific function or an object used for such a func-tion was previously asserted.
In those cases, as suchan information is old, RS3 is appropriate, while allother schemata described above are not.
Although itmust be possible to classify pattern RS3 into smallerclasses and to discuss the occurrence frequency andthe situations in which they occur, the small num-bers involved prevented further investigation.2.5 Relat ion  to  prev ious  researchThe occurrence frequency of each schemata listedabove supports the findings of our previous researchsummarized as P1 and P2 in the introduction.
InRS1 and RS2, which are basis of all schemata, theobject position is conveyed almost always under theguidance of the schemata themselves.
In particu-lar, it is mandatory in RS1.
So, the amount ofinformation conveyed for identifying objects, howmuch is needed depends as a matter of course onthe modes available, is controlled by feature descrip-tions other than position information.
This causesP1, the property that the usage of position informa-tion does not decrease in MMD, while other kinds ofinformation do decrease.
In addition, this propertyis seen more strongly in MMD; RSI" and RS2" areused frequently wherein a group member directly isreferred to directly to the object; the group is notmentioned.In SMD, RSI?
and RS2?
are used more fre-quently than in MMD.
This means that references tothe surface where the object is located and the groupit belongs to tend to be made in an utterance differ-ent from the utterance referring to the object itself.In addition, R$'1 also appears more frequently inSMD than in MMD.
This means an identificationrequest and an action request are made separately.These are indications of P2, the property that ac-tions tend to be realized as an explicit goal and di-vided into a series of fine-grained steps in SMD.3 Actions coordinated withreference xpressionsIn MMD, several kinds of physical actions accom-pany referring expressions.
Proper coordination be-tween such physical actions and linguistic expres-sions makes the referring actions fluent.
In addition,referring expressions in MMD frequently use demon-stratives uch as "kore(this)" and "koko(here)" in re-lation to these actions.
Investigating the constraintsor patterns of this coordination and applying themto the schemata of referring expressions makes itpossible to generate fluent action statements.Physical actions in referring actions in MMD aredivided into the following three categories.Exh ib i t  act ions:  Actions for making object visiblesuch as turning a body or opening a cassettecover 7.7Exhibit actions contain both general actions liketurning the body and machine specific actions like open-ing the cassette cover.
There may be some differences be-tween these two types of actions.
For example, in refer-ring expressions, the latter is usually requested directlyby an imperative sentence, while the former is requestedindirectly by directing attention to a specific side or im-plicitly by mentioning that side.26 T. Kato and Y.L Nakano0.0 2.5 5.0sI honntai hidarigawa no ire tekudasaibody left-side \[ put REQUESTsoshitara mou ittan wo koko no sashikomiguchi ni\]\[ then the other end OBJ here socket LOCThen, put the other end into this socket on the left side of the bodyFigure 2: An example of temporal coordination of exhibit actionsPointing gestures: Deictic actions pointing to/atobjectsSimulated operations: Actions that are parts ofmachine operations uch as pushing a buttonand picking up a handset.
In instruction di-alogues, experts ometimes just simulate theseactions without actual execution.This section reports the patterns of temporal coor-dination of these actions with linguistic expressions,based on the observation of the corpus.
Videotapesof just 48 referring actions (4 experts referred to 12objects once each) were examined.
As the amountof data is so small, we provide only a qualitativediscussion.3.1 Exhibit actionsOnly invisible objects need exhibit actions whenthey are referred to.
Being those objects referredto, whichever scheme listed above is used, the infor-mation of the position or surface where the objectis located is conveyed ahead of other features of theobject.
That is, letting the linguistic expression justbefore the referring expression be Lbfr, the positiondescription be Lpos, and the object description beLob1, the temporal relation of those can be summa-rized as follows using Allen's temporal logic (Allen,1084).Lblr before Lpos before LobjAccompanying these expressions, exhibit actionAt, and pointing gesture Ap, have the following re-lations.Lobj starts ApLbyr before At before LobjLpo, overlaps \]overlaps -1 \] during \]during -1 AeThe pointing gesture to the object begins at thesame time of the beginning of the object description.The exhibit action is done between the end of the ut-terance just before the referring action and the be-ginning of the object description.
The exhibit actionand position description relates loosely.
There maybe a temporal overlap between them or one may bedone during the other.
More precise relation thanthis could not concluded.
In order to keep theserelations, pauses of a proper length are put beforeand/or after the position description if needed.Fig.
2 shows a schematic depiction of the aboverelations and an example taken from the corpus.3.2 Point ing gestures and s imulatedoperationsPointing gestures are faithfully synchronized to lin-guistic expressions.
During action statements, al-most all descriptions of objects or positions are ac-companied by pointing gestures.
Landmarks andobject groups are also pointed to.
When a point-ing gesture is not made to the currently describedobject, no pointing gesture is made.
Pointing ges-tures to objects other than the currently describedone never happen.
One exception to this constraintis scheme RS3.
When the subject part of RS3, whichis an object description, is uttered, no pointing ges-ture is provided.
A pointing gesture begins as theposition description begins.The linguistic description of an object, Lobj, and apointing gesture to it, Ap, basically satisfy the tem-poral relation, Lobj starts Ap.
That is, Lobj and Apbegin at the same time, but Ap lasts longer.
How-ever, the constraint mentioned above, that pointinggesture to objects other than currently described onenever happen overrides this relation.
As a result, ingeneral, the pointing gesture to an object begins af-ter finishing reference to other objects.
As other?
Towards Generation of Fluent Referring Action in Multimodal Situations0.0 2.5 5.0 7.5 s" k i l l?
"T, -: " rT' "v  "l'r, TM27kochira nothese Iidaiarubotan no tonari ni \[I dail-buttons next LOCPointing gesture to the dial buttons Pointing gestures to the buttonNext of these dial buttons, there is a button with the mark of a handset.
Please push this.I L Ijuwaki no maaku no tsuita botan arimasu yonehandset mark being-placed-on button exist CONFRIMkore wo oshi temi tekudasaithis OBJ push REQUESTFigure 3: An example of temporal coordination of pointing gesturesobjects are usually mentioned as landmarks for de-scribing the object position, a pointing esture to theobject begins midway through position description.Ap usually lasts after Lobj.
In particular, a pointinggesture to the main object of a referring expressionlasts till the utterance nds and the addressee ac-knowledges it.
So, in the case of RS1, a pointinggesture lasts till the end of the sentence that assertsobject existence.When more than one noun phrase or postposi-tional phrase describing the same object are utteredsuccessively as in cases of appositive xpressions, thepointing gestures are once suspended at the end ofa phrase and resumed at the beginning of the nextphrase.
This is prominent when the next phrase be-gins with a demonstrative such as "this".Simulated operations are synchronized with theverb describing the corresponding operation.
Theirsynchronization is more precise than the case of ex-hibit actions.
As a simulated operation such as but-ton pushing is similar to a pointing gesture, a sus-pension and resumption similar to one mentionedabove is done probably to distinguish them.Fig.
3 shows an example taken from the corpus.
Inthis example, it is not clear whether the last actionis a pointing gesture or a simulated operation.4 D iscuss ion  on  imp lementat ionWe have began to implement a referring action gen-eration mechanism using the schemata derived andcoordination patterns described so far.
The experi-mental system we are now developing shows a GIFpicture of the telephone, and draws a caricaturedagent over it.
The pointing gestures are realizedby redrawing the agent.
As every picture is anno-tated by the object positions it contains, generatinga pointing gesture and interpreting user's one arepossible and easy.
Other actions uch as turning thebody and opening the cassette cover are realized byplaying a corresponding digital movie at exactly thesame place as the GIF picture displayed s. The firstframe of the digital move is the same as the GIFpicture shown at that point of the time, and whilethe movie is being played, the picture in the back-ground is switched to the one equaling the last frameof the movie.
Fig.
4 depicts this mechanism.
Thoseactions need considerable time as do human experts.This is in contrast o our previous ystem which im-plemented such actions by switching pictures o thetime taken was negligible.The framework adopted for coordination betweenutterances and actions is synchronization by refer-ence points (Blakowski, 1992).
The beginning andend points of intonational phrases must be eligibleas reference points.
It's still under consideration ifjust waiting, for which pauses are put after the ac-tion finished earlier, is enough or more sophisticatedoperations such as acceleration and deceleration i.e.changing utterance speed, are needed.
The need fordynamic planning such as used in PPP (Andre &Rist, 1996) should be examined.5 Summary  and  fu ture  workWhat patterns of linguistic expressions are com-monly used and how physical actions are temporallycoordinated to them were reported based on cor-8Tcl/tk is used for showing the GIF pictures anddrawing the agent as well as other basic input/outputfunctions; xanim is used for playing the digital movies.28 T. Kato and Y.L NakanoUser ViewI I : Snapshot A Turning from A to B Snapshot B t2n ?fiDra~/.~nactbn fern AtDI ShowingGIFpietureAp, ~G~ ShowingGIFpictureBFigure 4: The implementation f turn actionspus examinations.
In particular, by categorizing ob-jects according to two features, visibility and mem-bership, the schemata of referring expressions couldbe derived.
This categorization is still not sufficientfor uniquely determining each reference xpression,and some other features must impact he expressionsused.
This is, however, a good first step, as the twomost dominant features were obtained.
Moreover,the difference between the occurrence frequencies ofthose schemata in MMD and SMD explains the find-ings of our previous research.
Implementation basedon these results is on going.There is a lot of future work beyond the imple-mentation issues.
First, the reported coordinationpatterns between linguistic expressions and actionsmust be verified in a quantitative manner.
An ob-jective criterion for annotating visual information ishard to establish.
Overcoming this problem is im-portant and unavoidable.
Next, our research mustbe generalized in two perspectives: the results mustbe confirmed in many materials other than our tele-phone; the degree of dependency on the languageused must be examined.One of major problems temming from our ap-proach is that the importance ofcriteria is not clear.Although the criteria can be derived by observingand modeling the explanations made by experts,there may be fluent explanation techniques not yetobserved.
Deviations from the criteria do not causea big problem, and the recipients do not perceivethem to be awkward.
These problems can be ex-amined when the system currently implemented ismade to generate several types of referring actionsexperimentally.Appelt, D.E., "Planning English Referring Expres-sions", Artificial Intelligence 26, 1985, pp.
1 - 33Blakowski, G., Hiiel, J., Langrehr, U. and Miilh~er,J., "Tool Support for the Synchronization andPresentation of Distributed Multimedia", Com-puter Communication, Vol.
15, No.
10, 1992, pp.611 - 618Cassell, J., Pelachaud, C., Badler, N. and et al,"Animated Conversation: Rule-based Generationof Facial Expression, Gesture & Spoken Intona-tion for Multiple Conversational Agents", SIG-GRAPH 94, pp.
413 - 420, 1994Cohen, P.R., "The Pragmatics of Referring andthe Modality of Communication", ComputationalLinguistics, Vol.
10, No.
2, 1984, pp.
97 - 146Clark, H.H.
and Wilkes-Gibbs, D. "Referring as aCollaborative Process", "Intentions in Communi-cation" eds.
Cohen, P.R., Morgan, J. and Pollack,M.E., The MIT Press, 1990, pp.
463 - 493Dalai, M., Feiner, S., McKeown, K. and et al,"Nagotiation for Automated Generation of Tem-poral Multimedia Presentations", Proc.
of ACMMultimedia 96, pp.
55 - 64, 1996Dale, R. and Haddock, N., "Content Determinationin the Generation of Referring Expressions", Com-putational Intelligence, Vol.
7, No.
4, 1991. pp.
252- 265Feiner, S.K., Litman, D.J., McKeown, K.R, and Pas-sonneau, R.J., "Towards Coordinated TemporalMultimedia Presentations", "Intelligent Multime-dia Interfaces" eds.
Maybury, M.T., The AAAIPress/The MIT Press, 1993, pp.
117 - 138Kato, T. and Nakano, Y.I., "Referent Identifica-tion Requests in Multi-Modal Dialogs", Procs.
ofInt.
Conf.
on Cooperative Multimodal Communi-cation, Vol.
2, pp.
175 - 191, 1995Kato, T., Nakano, Y.I., Nakajima, H. and Hasegawa,T., "Interactive Multimodal Explanations andtheir Temporal Coordination", Procs.
of ECAI-96, pp.
261 - 265, 1996Kuno, S., "Danwa no Bunpou", Taishuukan Shoten,1978, In JapaneseReferencesAllen, J.F., "Towards a General Theory of Actionand Time", Artificial Intelligence, Vol.
23, No.
2,1984, pp.
123- 154Andr4, E. and Rist, T., "Coping with Temporal Con-straints in Multimedia Presentation Planning",Procs.
of AAAI-96, Vol.
1, pp.
142 - 147, 1996
