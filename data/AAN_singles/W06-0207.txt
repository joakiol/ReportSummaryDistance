Proceedings of the Workshop on Information Extraction Beyond The Document, pages 56?65,Sydney, July 2006. c?2006 Association for Computational LinguisticsLoLo: A System based on Terminology for Multilingual ExtractionYousif AlmasDepartment of ComputingUniversity of SurreyGuildford, Surrey, GU2 7XH, UKy.almas@surrey.ac.ukKhurshid AhmadDepartment of Computer ScienceTrinity College,Dublin-2.
IRELANDkahmad@cs.tcd.ieAbstractAn unsupervised learning method, basedon corpus linguistics and special lan-guage terminology, is described that canextract time-varying information fromtext streams.
The method is shown to be?language-independent?
in that its useleads to sets of regular-expressions thatcan be used to extract the information intypologically distinct languages like Eng-lish and Arabic.
The method uses the in-formation related to the distribution of N-grams, for automatically extracting?meaning bearing?
patterns of usage in atraining corpus.
The analysis of an Eng-lish news wire corpus (1,720,142 tokens)and Arabic news wire corpus (1,720,154tokens) show encouraging results.1 IntroductionOne of the recent trends in (adaptive) IE hasbeen motivated by the empirical argument thatannotated corpora, either annotated automaticallyor annotated manually, can provide sufficientinformation for creating the knowledge base ofan IE system (McLernon and Kushmerick,2006).
Another equally important trend is to usemanually selected seed patterns to initiate learn-ing: In turn, active-training methods use seedpatterns to learn new related patterns from un-annotated corpora.
Many of the adaptive IE sys-tems rely on the existing part-of-speech (POS)taggers (Debnath and Giles, 2005) and/or syntac-tic parsers (Stevenson and Greenwood, 2005) foranalysing and annotating text corpora.
The use ofcorpora in IE, especially adaptive IE, should, inprinciple, alleviate the need for manually creat-ing the rules for information extraction.The successful use of POS/syntactic taggers isdependent on the availability of the knowledgeof (natural) language used by the authors ofdocuments in a given corpus.
There is a wealthof POS taggers and parsers available for Englishlanguage, as it has been the most widely usedlanguage in computational linguistics.
However,this is not the case for strategically importantlanguages like Arabic and Chinese; to start with,in Chinese one does not have the luxury of sepa-rating word-tokens by a white space and in Ara-bic complex rules are required to identify mor-phemes compared to English.
The developmentof segmentation programs in these languages hascertainly helped (Gao et al, 2005; Habash andRambow, 2005).
More work is needed in under-standing these languages such that the knowl-edge thus derived can be used to power taggersand parsers.Typically, IE systems are used to analysenews wire corpora, telephone conversations, andmore recently in bio-informatics.
The first twosystems deal with language of everyday commu-nications ?the general language- whereas bio-informatics deals with a specialist domain andhas its own ?special language?.
English speciallanguages, for example languages of law, com-merce, finance, science & technology, each havea limited vocabulary and idiosyncratic syntacticstructures when compared with English used inan everyday context.
The same is true of Ger-man, French, Russian, Chinese, Arabic or Hindi.It appears that few works, if any, take advantageof the properties of special language to build IEsystems.Our objective is to use methods and tech-niques of IE in the automatic analysis of special-ist news that streams in such a way that informa-tion extracted at an earlier period of time may becontradicted or reinforced by information ex-tracted at a later time.
The impact of news onfinancial and commodity markets is of consider-56able import and is often called sentiment analy-sis.
The prefix ?sentiment?
is used to distinguishthis kind of analysis from the more quantitativeanalysis of assets (called fundamental analysis)and that of price movements (called technicalanalysis).
There is a great deal of discussion infinancial economics, econometrics, and in thenewly emergent discipline of investor psychol-ogy about the impact of ?good?
and ?bad?
newson the behaviour of both investors and brokers.Three Nobel Prizes have been awarded on theimpact of market (trader and investor) sentimenton the value of shares, currencies, derivativesand other financial instruments (Shiller, 2000).Financial news, in addition to e-mails and blogs,has contributed to the catastrophic failures ofmajor trading institutions (Mackenzie, 2000;Hardie & Mackenzie, 2005).One of the key proponents of news impactanalysis is the Economics Nobel Laureate RobertEngle who has written about asymmetry of in-formation in a market ?
the brokers have moreknowledge than any given individual, rumourshave different impact on different actors in themarket.
Engle?s statistical analysis suggests thatthe ?bad?
news has longer lasting effect than?good?
news (Engle, 1993).
Usually, sentimentanalysis is carried out using news proxies whichinclude dates/times and the names of agenciesreleasing key items of financial data (Andersonet al, 2002) or data like the age of a firm, itsnumber of initial public offerings, return on in-vestment, etc.
These proxies are then regressedwith share, currency or commodity prices.
Newsimpact analysis is moving into its next phasewhere the text of news is analysed albeit to a lim-ited extent (Cutler et al, 1989; Chan, 2003).
Theanalysis sometimes looks at the frequency distri-bution of pre-specified keywords ?directionalmetaphors like rose/fall, up/down, health meta-phors like anaemic/healthy and animal meta-phors like bullish/bearish.
A system is trainedto correlate and to learn the changes in distribu-tion of the prescribed metaphorical keywords,together with names of organisations, to thechanges in the value of financial instruments(Seo et al, 2002; Omrane et al, 2005; Koppeland Shtrimberg 2004).We are attempting to create a language-informed framework for news impact analysisusing techniques of corpus linguistics and speciallanguage analysis.
The purpose is to automati-cally extract patterns from a corpus of domainspecific texts without prescribing the metaphori-cal keywords and organisation names.
This, webelieve, can be achieved by looking at the lexicalsignature of a specialist domain and extractingcollocational patterns of the individual items ofthe lexical signature.
The lexical signature in-cludes key vocabulary items of the domain andnames of people, places and things in the do-main.
There are instances in the part-of-speechtagging literature (Brill, 1993) and in IE where acorpus is used and words within a grammaticalcategory help to extract rules and patterns com-prising essential information about a domain ortopic (Wilks, 1998; Yangarber, 2003).
Brill,Wilks and Yangarber induce grammars of a uni-versal kind:  we focus on inducing a local gram-mar that deals with the patterning of the items inthe signature.
Note that in all these cases ofgrammar induction the intuition of the grammarbuilder plays a critical part whether it be in thechoice of syntactic transformation rules (Brill1993), or in choosing sense taggers and implic-itly semantic rules (Wilks, 1998; Ciravegna andWilks, 2003), or in choosing user supplied seedpatterns (Yangarber, 2003).
Most of the work ingrammar induction is focussed on English or ty-pologically similar languages.
We have deliber-ately chosen typologically different languages(English and Arabic) to evaluate the extent towhich our method of ?grammar induction?
is lan-guage independent.We describe a method for building domainspecific IE systems:  the patterns used to extractdomain specific information are the N-gram col-location patterns of domain specific terms.
Thepatterns are extracted from un-annotated domain-specific text corpora.
We show how one can ana-lyse the N-gram patterns and render them asregular expressions.The thesaurus used to identify domain specificwords is itself constructed automatically from a(training) special-language corpus.
The fre-quency distribution of domain specific terms in aspecial language corpus shows characteristic dif-ferences from the distribution of the same termsin a general language corpus.
There is little orno difference in the distribution of the so-calledgrammatical or closed class terms in a specialand a general language corpus.Furthermore, amongst the domain specificterms, a few tend to dominate the frequency dis-tribution ?
the so-called lexical signature of adomain.
These signature terms are used as nu-cleates for compound terms in a domain.
Theoccurrence of the signature terms, either on theirown or in a compound or a phrase, is equallyidiosyncratic in that these dominant single or57compound terms co-occur more frequently withone set of words than with others.
The behaviourof signature terms appears to be governed by agrammar that is local to the specialism and is notelsewhere in the general language (Harris, 1991);local grammar is used in general language fortelling times and dates in metaphorical expres-sions (Gross, 1997), and in the lexicography fordescribing the language of definitions of lemmasin a lexicon (Barnbrook and Sinclair, 1996;Barnbrook, 2002).
The local grammar approach,rooted in the lexical signature of a given domaincan be used to extract ?sentiment?
bearing sen-tences in financial markets (Ahmad el al., 2006)or in the description of work in a scientific labo-ratory (Ahmad & Al-Sayed, 2005).We introduce a system that can help in build-ing domain specific IE systems in English andlanguages that are typologically distinct fromEnglish, specifically Arabic.
The developmentof LoLo was inspired by Engle?s pioneeringwork in econometrics where news impact analy-sis is regarded as critical to the analysis of mar-ket movement: however much of the work infinancial economics relates to the correlation ofthe timings of news announcements rather thanthe content of the news stream (Ahmad et al,2006).LoLo can manage a corpus and extract keyterms.
Given the keyword list, the system thenidentifies collocates and selects significant collo-cates on well defined statistical criterion(Smadja, 1994).
Finally, local grammar rules areidentified and an IE system is created.LoLo has been used to build a local grammarto extract ?sentiment?
or key (changing) marketevents in English and in Arabic from unseentexts.
The system can help visualise the distribu-tion of extracted patterns synchronised with themovement of financial markets.IE systems need to be adaptive, as the special-isms in particular and the world in general ischanging rapidly and this change is usually re-flected in language use.
There is an equally im-portant need to build cross language IE systemsas information may be in different languages.The lexically-motivated approach we describe inthis paper responds to the need for an adaptive,cross domain and cross language IE systems.2 MethodFor the extraction of local grammar from acorpus of special language texts it is important tofocus on the keywords.
The patterns in whichthe keywords are embedded are assumed to com-prise the principal elements of a subject specificlocal grammar.The manner in which we derive the localgrammar is shown in the algorithm below (Fig-ure 1).ALGORITHM: DISCOVER LOCAL GRAMMAR1.
SELECT a special language corpus (SL, comprisingNspecial words and vocabulary VSpecial).i.
USE a frequency list of single words from a corpus oftexts used in day-to-day communications (SG comprisingNgeneral words and vocabulary Vgeneral) ?
for example, theBritish National Corpus for the English language:Fgeneral:={f(w1),f(w2),f(w3)??.fVgeneral}ii.
CREATE a frequency ordered list of words in SL texts iscomputedFspecial:={f(w1), f(w2), f(w3)???}iii.
COMPUTE the differences in the distribution of thesame words in the two different corpora is computed us-ing the in SG and SL:Weirdness (wi)= f(wi)special/f(wi)general*Ngeneral/Nspecialiv.
CALCULATE z-score for the Fspeicalzf(wi)=(f(wi)-fav_special)/?special2.
CREATE KEY a set of Nkey keywords ordered accord-ing to the magnitude of the two z-scoresKEY:={key1, key2, key3?
?keyNkey)such that z(fkeyi) & z(weridnesskeyi)> 1i.
EXTRACT collocates of each Key in SL over a windowof M word neighbourhood.ii.
COMPUTE the strength of collocation using three meas-ures due to Smadja (1994):U-score, k, and z-scoreiii.
EXTRACT sentences in the corpus that comprise highlycollocating key-words ((U,ko,k1)>=(10,1,1)) iv.
FORM Corpus SL?a.
For each Sentencei in SL?:b.
COMPUTE the frequency of every word in Sentencei.c.
REPLACE words with frequency less than a thresholdvalue (fthreshold) by a place marker #;d. FOR more than one contiguous place marker, use*3.
GENERATE trigrams in SL?
;  note frequency of eachtrigram together with its position in the sentences:i.
FIND all the longest possible contiguous trigramsacross all sentences in SL?
and note their frequencyii.
ORDER the (contiguous) trigrams according to fre-quency of occurrenceiii.
(CONTIGUOUS) TRIGRAMS with frequency above athreshold form THE LOCAL GRAMMARFigure 1.
Algorithm for the acquisition of local-grammar patterns.Briefly, given a specialist corpus (SL), key-words are identified, and collocates of the key-words are extracted.
Sentences containing keycollocates are then used to construct a sub-corpus(SL?).
The sub-corpus SL?
is then analyzed andtrigrams above a frequency threshold in the sub-corpus are extracted; the position of the trigramsin each of the sentences is also noted.
The sub-corpus is searched again for contiguous trigramsacross the sentences:   The sentences are ana-lyzed for the existence of the trigrams in the cor-rect position ?
if a trigram that, for example, isnoted for its frequency as a sentence initial posi-tion, is found to co-occur with another frequenttrigram that exists at the next position, then thetwo trigrams will be deemed to form a pattern.58This process is continued until all the trigrams inthe sentence are matched with the significanttrigrams.The local grammar then comprises significantcontiguous trigrams that are found.
These do-main specific patterns, extracted from the spe-cialist corpus SL?
(and its constituent sub-corpus)are then used to extract similar patterns and in-formation from a test corpus to validate the pat-terns thus found in the training corpus.
Followingis a demonstration of how the algorithm worksusing English and Arabic texts.2.1 Extracting Patterns in EnglishWe present an analysis of a corpus of financialnews wire texts: 1204 news report produced byReuters UK Financial News comprising 431,850tokens.
One of the frequent words in the corpusis percent?
3622 occurrences, a relative fre-quency of 0.0084%.
When the frequency of thiskeyword is looked up in the British NationalCorpus (100 million words), it was found thatpercent is 287 times more frequent in the finan-cial corpus than in the British National Corpus ?this ratio is sometimes termed weirdness (of spe-cial language); the weirdness of grammaticalwords the and to is unity as these tokens are dis-tributed with the same (relative) frequency inReuters Financial and the BNC.
The z-scorecomputed using the frequency of the token in theReuters Financial is 12.64: the distribution ofpercent is 12 standard deviations above the meanof all words in the financial corpus.
(The z-scorecomputed for weirdness is positive as well).
Theheuristic here is this: a token is a candidate key-word if both its z-scores are greater than a smallpositive number.
So percent -most frequent to-ken with frequency and weirdness z-score overzero- was accepted as a keyword.The collocates of the keyword percent werethen extracted by using mutual information sta-tistics presented by Smadja (1994).
A collocatein this terminology can be anywhere in the vicin-ity of +/- N-words.
The frequency at eachneighbourhood is calculated and then used tocompute the ?peaks?
in the histogram formed bythe neighbourhood frequencies and the strengthof the collocation calculated on a similar basis.The keyword generally collocates with certainwords that have frequencies higher than itself ?the upward collocates- and collocates with cer-tain words that have lesser frequency ?
the down-wards collocates (These terms were coined byJohn Sinclair).
Upwards collocates are usuallygrammatical words and downwards collocatesare lexical words ?
nouns, adjectives- and hencethe downwards collocates are treated as candi-date compound words.
There were 46 collocatesof percent in our corpus ?
34 downwards collo-cates and 12 upwards collocates.
A selection of5 downwards and upwards are shown in Table 1and 2 respectively.Collocate Frequency U-score k-scoreshares 1150 1047 3.01rose 514 2961 2.43year 2046 396 2.40profit 1106 263 1.65down 486 996 1.40Table 1.
Downward collocates of percent in acorpus of 431,850 words.Collocate Frequency U-score k-scorethe 23157 6744 14.40to 12190 7230 10.29in 9768 4941 8.49a 10657 3024 8.44of 10123 3957 8.24Table 2.
Upward collocates of percent in a cor-pus of 431,850 words.The financial texts comprise a large number ofnumerals (integers and decimals) and these wewill denote as <no>.
The numerals collocatestrongly with percent for obvious reasons.
Thecollocates are then used to extract trigrams com-prising the collocates that occur at particular po-sitions in the various sentences of our corpus:Token A Token B Token C Freq Position<no> percent and 16 1rose <no> percent 18 1<no> percent after 23 2<no> percent of 47 2<no> percent rise 11 2Table 3.
Trigrams of percent.There are many other frequent patterns wherethe frequency of individual tokens is quite lowbut at least one member of the trigram has higherfrequency: such low frequency tokens are omit-ted and marked by the (#) symbol.
All the tri-grams containing such tokens with at least twoothers are used to extract other significant tri-grams.
Sometimes more than one low frequencytokens precede or succeed high frequency tokensand they are denoted by the symbol (*) as shownin Table 4.
The search for contiguous trigramsleads to larger and more complex patterns, Table5 provides some examples.59Token A Token B Token C Freq Positionrose <no> percent 18 1# <no> percent 29 2# shares were 10 2* <no> percent 57 2<no> percent # 24 2Table 4.
Trigrams of percent with omitted lowfrequency words (denoted as * for multiple to-kens and # for a single token).Local Grammar Patterns Freq<s> the * <no> percent 28<s> * rose <no> percent 26<s> # shares # <no> percent 22<s> * fell <no> percent 20<s> * <no>  percent 18<s> # shares were  up <no> percent at 17Table 5.
Some of top patterns of percent   (<s>identifies a sentence boundary).2.2 Extracting Patterns in ArabicArabic is written from right to left and its writ-ing system does not employ capitalization.
Thelanguage is highly inflected compared to Eng-lish; words are generated using a root-and-pattern morphology.
Prefixes and suffixes can beattached to the morphological patterns for gram-matical purposes.
For example, the grammaticalconjunction ?and?
in Arabic is attached to thebeginning of the following word.
Words are alsosensitive to the gender and number they refer toand their lexical structure change accordingly.As a result, more word types can be found inArabic corpora compared to English of same sizeand type.
Short vowels which are represented asmarks in Arabic are also omitted from usualArabic texts resulting in some words havingsame lexical structures but different semantics.These grammatical and lexical features ofArabic cause more complexity and ambiguity,especially for NLP systems designed for thor-ough processing of Arabic texts compared toEnglish.
A shallow and statistical approach forIE using texts of specialism can be useful to ab-stract many complexities of Arabic texts.Given a 431,563 word corpus comprising2559 texts of Reuters Arabic Financial News andthe same thresholds we used with the Englishcorpus, percent (al-meaa, QRST?)
is again the mostfrequent term with frequency and weirdness z-score greater than zero.
It has 3125 occurrences(0.0072%), a frequency z-score of 19.03 and aweirdness of 76 compared against our ModernStandard Arabic Corpus (MSAC).There were 31 collocates of percent; 7 up-wards and 23 downwards.
The downwards collo-cates of percent appear to collocate with namesof instruments i.e.
shares and indices (Table 6).The upwards collocate are with the so-calledclosed class words as in English like in, on andthat (Table 7).Collocate Freq U-score k-scoreby-a-ratio(be-nesba, QVWXY) 1257 39191 7.87point(noqta, QZ[\) 1167 9946 6.44the-year(al-aam, ?^_T3.34 344 1753 (?index(moasher,`abc) 1130 409 2.55million(milyoon, ?efgc) 2281 600 2.32share(saham, hij) 705 206 1.84Table 6.
Downward collocates of percent (al-meaa, QRST?
).Collocate Freq U-score k-scorein(fee,kl) 21236 434756 40.99to(ela, mT9.81 25145 3339 (?from(min, nc) 10344 4682 9.58on(ala,mgo ) 5275 117 3.10that(ann, 2.65 260 5130 (?
?Table 7.
Upward collocates of percent (almeaa,QRST?
).Using the same thresholds the trigrams (Table8) appear to be different from the English tri-grams in that the words of movement are not in-cluded here ?
this is because Arabic has a richermorphological system compared to English andFinancial Arabic is not as standardised as Finan-cial English: however, it will not be difficult totrain the system to recognise the variants of roseand fell   in Financial Arabic.
Table 9 lists someof the patterns.60Token A Token B Token C Freq Position<no> in  (fee,kl)percent(al-meaa,QRST1 197 (?in(fee,kl)percent(al-meaa,QRST1 39 * (?in(fee,kl)percent(al-meaa,QRST?
)to(ela, mT2 22 (?percent(al-meaa,QRST?
)to(ela, mT?)
<no> 21 3# in  (fee,kl)percent(al-meaa,QRST4 66 (?Table 8.
Trigrams of percent (almeaa, QRST?
).Local Grammar Patterns Freq<no>       *       <s> kl      QRST?
*percent     in 34>s<     *      QVWXY      > no<     kl       QRST?
mT?
> no<to     percent  in                 by-a-ratio 23>s<      #        hij        #      >no<       kl       ?QRSTpercent     in                              share 21>s # < `abc # ^p^Z\ qj?s?
QVWXY>no <kl QRST?
mT?
>no < QZ[\>/s<point         to percent in      by-ratio  wider         index 18`abc *>no<  QZ[\        ??
>no< kl      QRST?
mT?>no <  QZ[\>/s<point          to  percent  in        namely   point           index 16kl     *      ?ev     #       QVWXY     >no <      kl     QRST?
qc      *with   percent   in                  by-ratio         day           in10Table 9.
Some patterns of percent(almeaa, QRST?
).3 Experimental ResultsWe have argued that a method that is focused onfrequency at the lexical level(s) of linguistic de-scription ?
single words, compounds, and N-grams- will perhaps lead to patterns that are idio-syncratic of a specialist domain without recourseto a thesaurus.
There are a number of linguisticmethods ?
that focus on syntactic and semanticlevel of description which might be of equal orbetter use.In order to show the effectiveness of ourmethod we apply it to sentiment analysis ?
ananalysis that attempts to extract qualitative opin-ion expressed about a range of human and natu-ral artefacts ?
films, cars, financial instrumentsfor instance.
Broadly speaking, sentiments infinancial markets relate to the ?rise?
and ?fall?
offinancial instruments (shares, currencies, com-modities and energy prices): inextricably thesesentiments relate to change in the prices of theinstruments.
In both English and Arabic, wehave found that percent or equivalent is a key-word and trigrams and longer N-grams embed-ded with this keyword relate to metaphoricalmovement words?
up, down, rise, fall.
However,in English this association is further contextual-ised with other keywords ?
shares, stocks- and inArabic the contextualisation is with shares andthe principal commodity of many Arab stateseconomies ?
oil.
Our system ?discovered?
bothby following a lexical level of linguistic descrip-tion.For each of the two languages of interest to us,we have created 1.72 million token corpora.Each corpus was then divided into two (roughly)equal sized sub corpora: training corpus and test-ing corpus; the testing corpus is sub-divided intotwo testing corpora Test1 and Test2 (Table 10).First, we extract patterns from the Training Cor-pus using the discover local grammar algorithm(Figure 1) and also from Test1.
Next, the Train-ing1 and Test1 corpora are merged and patternsextracted from the merged corpus.
The intuitionwe have is that as the size of the corpus is in-creased the patterns extracted from a smallersized corpus will be elaborated: some of the pat-terns that are idiosyncratic of the smaller sizedcorpus will become statistically insignificant andhence will be ignored.
The conventional way oftesting would have been to see how many pat-terns discovered in the training corpus are foundin the testing corpora; we are quantifying theseresults currently.
In the following we describe aninitial test of our method after introducing LoLo.English Arabic Corpus Texts Tokens Texts TokensTraining1 2408 861,492 5118 860,020Test1 1204 431,850 2559 431,563Training2 (Training1+Test1) 3612 1,293,342 7677 1,293,342Test2 1204 426,800 2559 428,571Total 4816 1,720,142 10,236 1,720,154Table 10.
Training and testing corpora used inour experiments.3.1 LoLoLoLo (stands for Local-Grammar for LearningTerminology and means ?pearl?
in Arabic) is de-veloped using the .NET platform.
It contains fourcomponents summarised in Table 11.Component FunctionalityCORPUS ANALYSER Discover domain specific extraction patternsRULES EDITOR Group, label and evaluate patterns and slotsINFORMATION  EXTRACTOR Extract informationINFORMATION VISUALISER Visualise patterns over timeTable 11.
Summary of LoLo?s components.61The various components of LoLo ?the Ana-lyser, Editor, Extractor and the Visualiser, canbe used to extract and present patterns; the sys-tem has utilities to change script and the direc-tion of writing (Arabic is right-to-left and Eng-lish left-to-right).
Table 12 is an exemplar outputfrom LoLo: ?rise in profit?
event patterns ex-pressed similarly in English and Arabic financialnews headlines found by the Corpus Analyser.English *  profit  up  <no>  percent?^yz?
??^Y??
<no> *  kl     QRST?
Arabic percent  in            profit  rise (up)Table 12.
?Rise in profit?
patterns in Arabic andEnglish where the * usually comprises names oforganisations or enterprises.The pattern acquisition algorithm presentedearlier is implemented in the Corpus Analysercomponent, which is the focus of this paper.
Itcan be used for discovering frequent patterns incorpora.
The user has the option to filter smallerpatterns contained in larger ones and to mine forinterrupted or non-interrupted patterns.
It canalso distinguish between single word and multiword slots.Before mining for patterns, a corpus pre-processor routine performs a few operations toimprove the pattern discovery.
It identifies anypunctuation marks attached to the words andseparates them.
it also identifies the sentencesboundaries and converts all the numerical tokensto one tag ?<no>?
as numbers can be part ofsome patterns, especially in the domain of finan-cial news.The Rules Editor is at its initial stages of de-velopment, currently it can export the extractionpatterns discovered by the Corpus Analyser asregular expressions.A time-stamped corpus can be visualised us-ing the Information Visualiser.
The Visualisercan display a time-series that shows how the ex-tracted events emerge, repeat and fade over timein relation to other events or imported time seriesi.e.
of financial instruments.
This can be usefulfor analysing any relations between differentevents or detecting trends in one or more corporaor with other time-series.LoLo facilitates other corpus and computa-tional linguistics tasks as well, including generat-ing concordances and finding collocations fromtexts encoded in UTF-8.
This is particularly use-ful for Arabic and languages using the Arabicwriting system like Persian and Urdu which lacksuch resources.3.2 Training and Testing3.2.1 EnglishWe consider the English Training1 corpus first.We extracted the significant collocates of all thehigh frequency/high weirdness words, where?high?
defined using the associated z-scores, inthe training corpus.
Trigrams were then ex-tracted and high frequency trigrams were chosenand all sentences comprising the trigrams wereused to form a (training) sub corpus.
The sub-corpus was then analysed for extracting the localgrammar.The 10 high frequency N-grams extractedautomatically from the Training1 Corpus(861,492) are listed in Table 13.
The Test1 cor-pus has most of the trigrams in the Training1corpus, particularly some of the larger N-grams(Table 14).Rank Top 10 patterns comprising ?percent?
Freq1 <s> the * <no> percent 452 <s> the * was up <no> percent at <no>,  <no> </s>  333 <s> * <no> percent #, <no> </s> 244 <s> * up <no> percent 215 <s> the * was down <no> percent at <no> , <no> </s> 196 <s> * <no> percent after 186 <s> * <no> percent to <no> , <no> yen 187 <s>, # shares were up <no> percent at <no> 178 <s> shares in * <no> percent 159 <s> * rose <no> percent to <no> 1410 <s> # shares rose <no> percent to <no> 1310 <s> fell <no> percent to <no> 13Table 13.
Patterns of percent extracted fromTraining1 corpus.Patterns Freq<s> # shares # <no> percent 22<s> shares in * <no> percent 13<s> # shares were up <no> percent at 17Table 14.
Patterns of percent extracted fromTest1 corpus found as sub-patterns in Training1.We then merged the Training1 and Test1 cor-pora together and created Training2 corpus com-prising of 3612 texts and 1,293,342 tokens.
TheAlgorithm was executed on the merged corpusand a new set of patterns were extracted, in par-ticular the most frequent pattern in the Training1Corpus (<s> the * <no> percent), was elabo-62rated by the Algorithm as well as those patternsshown in Table 15.Training1 Corpus Freq Training2 Corpus Freq<s> the * was down <no>percent at <no> , <no> </s> 19<s> the * index was down<no> percent at <no> ,  <no></s>23<s> the * was up <no>percent at <no>,  <no> </s> 33 <s> the * index was up <no> percent at <no> ,  <no> </s> 34Table 15.
Comparison between two patterns inTraining1 and Training2 corpora.The patterns related to the collocations ofshares and percent from Training1 were pre-served in Training2.
The test on Test2 corpusshowed similar results: the smaller N-grams re-lated to the movement of instruments were simi-lar to the Test1 Corpus.
The analysis of Arabictexts is shown below with similar results.3.2.2 ArabicSome of frequent N-grams extracted automati-cally from the Training1 Arabic corpus (860,020)are shown in Table 16.
Similar to the Englishcorpora the Test1 Arabic corpus has most of thetrigrams in the Training1 Corpus and some largerN-grams(Table 17).Rank Top 10 patterns comprising ?percent?
Freq<no>    *    <s>     kl      QRST1       *       ?
percent    in 35*       kl        *       QVWXY       > no<       kl       QRST2  ?
percent  in                      by-ratio               in  31*>no<  QZ[\  QVWXY > no < kl  QRST?
mT?
>no < QZ[\  >/s< 3          point           to percent in       by-ratio  point 28<s>         *       kl        QRST?
kl      *  4                                    in      percent      in 24>s <   *     QVWXY      > no<    kl      QRST?
mT?
>no< 4                     to    percent    in                by-ratio 24*        kl        *       mT?
>no <       kl      QRST5 ?
percent    in                       to                  in 21>s #< `abc* ^p^Z\   QVWXY >no<kl  QRST?
mT?
>no<QZ[\>/s< 5          point      to percent in   by-ratio zone  index 21Table 16.
Patterns of percent (almeaa, QRST?)
ex-tracted from Training1 Arabic corpus.Patterns Freq*      QVWXY      <no>        kl       QRST?
#percent      in                   by-ratio 10*       QVWXY     <no>        kl       QRST?
klin     percent    in                   by-ratio  10>s <    *   QVWXY   > no <     kl      QRST?
*percent   in               by-ratio 11Table 17.
Patterns of percent (almeaa, QRST?)
ex-tracted from Test1 Arabic corpus found as sub-patterns in Training1.After merging the Training1 and Test1 Arabiccorpora together into a corpus of 7677 texts and1,293,342 tokens, new set of patterns were ex-tracted as well.
Some of the frequent patterns inthe training corpus were elaborated more as welllike the pattern shown in Table 18 where the to-ken and-rise (wa-ertifaa, ??qyz? )
was added to thepattern.Training1 Corpus Freq Training2 Corpus Freq# <s>   `abc #^p^Z\ qj?s?QVWXY <no> mT?
QRST?
kl <no>QZ[\      </s>13<s> ;<=???
`abc  # ^p^Z\ qj?s?QVWXY <no> mT?
QRST?
kl <no> QZ[\</s>17Table 18.
Comparison between two patterns inTraining1 and Training2 Arabic corpora.4 EvaluationWe have used the Rules Editor and the Informa-tion Extractor to evaluate the patterns on a cor-pus comprising 2408 texts and 858,650 tokenscreated by merging Test1 and Test2 corpora.
TheArabic evaluation corpus comprised 5118 textsand 860,134 tokens.
The N-gram pattern extrac-tor (where N > 4) showed considerable promisein that who or what went up/or down was unam-biguously extracted from the English test corpususing patterns generated through the trainingcorpus.
Initial results show high precision withthe longer N-grams in English (Table 19) andArabic (Table 20).Table 19.
Patterns with high precision (English).Table 20.
Patterns with high precision (Arabic).Pattern Precision<ORG> shares were down <no> percent at <no> 100% (13/13)<Movement> <no> percent to <no> , <no> yen 100% (17/17)the <Index> was up <no> percent at <no> , <no>  92% (11/12)<ORG> shares # up <no> percent at 88% (30/34)Pattern Preci-sion`abc  <no>  <Index>  QZ[\  ??
<no>   kl   QRST?
mT?
<no> QZ[\point         to  percent  in             viz  point                          index100%(42/42)  <Index>   	   #   >no<     #   > no <      ?percent  in                     point                for-shares               index100%(27/27)<Movement> `abc <Index> QVWXY ^p^Z\ qj?s?>no <mT?
QRST?
kl>no<QZ[\point        to percent in       by-ratio zone wider                  index97%(33/34)<Movement>  <Index>   > no < ?   ? >no <point         to percent in            zone                  index77%(27/35)63However, some patterns return many extractedinformation that require trimming.
For examplemany organizations names are extracted in Ara-bic using the pattern shown in table 21 but theyusually have the word by-a-ratio (be-nesba, QVWXY)attached at the end resulting in low precision.Pattern Precision<ORG> rose <no> percent to <no> 36% (5/14)<Movement> hij     Q?`a   <no>  <ORG> kl  QRST?percent in                       company  share30%(25/83)Table 21.
Patterns with low precision in Englishand ArabicBecause we have used the same trainingthresholds for English and Arabic, the patterns inArabic appeared without the motion words.However the system can extract these wordsalong with the org/instrument/index names be-cause they appear frequently as slots in the pat-terns.The N-gram patterns (when N ?
4) show poorresults in that either such patterns found in thetraining corpus are not found in the test corpus,or the patterns retrieved from test corpora are atsemantic variance with the same pattern in thetraining corpus.
This suggests that there is anoptimal length of individual patterns in our localgrammar.5 AfterwordThe patterns extracted from the English (andArabic) corpora confirm to an extent the view ofthe proponents of local grammar, of a speciallanguage, that there are certain words (in ourcase percent, shares, index) that appear to have aspecific grammatical category in the sense thatthe neighbourhood of these words is occupied bya small number of other words (up, down, fall,rise, <no> for instance).
If we were to apply thegrammars typically used in part-of-speech tag-gers and syntactic parsing in general, the idio-syncratic behaviour of the pivotal keywords inspecialist language does not become apparent:the pivotal keywords are regarded as nounphrases and the association of these phrases iswith other general categories of verb phrase, ad-jectival phrase and adverbial phrase.The patterns we have extracted could havebeen extracted with the help of a thesaurus.
And,this is the question which is critical to us: how tocreate and maintain a thesaurus within a domain.This is illustrated in a small way by our experi-ment on the Training1 corpus where the term in-dex was not statistically significant for it to ap-pear in the trigrams that populate the localgrammar.
However, in Training2, the larger cor-pus did contain significant frequency of the termindex for it to make into a pattern of its own.Furthermore, many of the patterns in Training1persisted in Training2.
Smaller N-grams persistas well in the various Training and Test corpora?
these patterns in themselves act like unitsaround which other trigrams nucleate.The evaluation of our Algorithm is still con-tinuing and we are in the process of setting upexperiments with human volunteers, especiallythose with some knowledge of financial mattersto evaluate the output of LoLo.
We intend to useinformation retrieval metrics of recall and thevarious F?
measures.The local grammar movement has made er-ratic progress since its inception in the 1960?s.Now, with the advent of accessible computerswith substantive memories, with the advent ofthe Internet and the concomitant treasure ofmulti-lingual text deposits and text streams, onecan explore the use of such grammars in address-ing the major challenges in information extrac-tion.ReferenceAhmad, Khurshid.
and Al-Sayed, Rafif.
(2005)Community of Practice and the Special Language?Ground?.
In (Eds.)
Clarke, S and Coakes, E.  En-cyclopaedia of Knowledge Management andCommunity of Practice.
Hershey (PA): The IdeaGroup Reference.Ahmad, Khurshid., Cheng, David.
and Almas, Yousif.
(2006) ?Multi-lingual Sentiment Analysis of Fi-nancial News Streams.?
In Proc.
of the 1st Interna-tional Conference on Grid in Finance, Palermo.Andersen, Torben., Bollerslev, Tim., Diebold, Fran-cis, and Vega, Clara.
(2002).
?Micro effects ofmacro announcements: Real Time Price Discoveryin Foreign Exchange?.
National Bureau of Eco-nomic Research Working Paper 8959.
(Available athttp://www.nber.org/papers/w8959).Barnbrook, Geoffrey.
and Sinclair, John McH.
(1996)'Parsing Cobuild Entries'.
In (Eds.)
John McH.Sinclair, Martin Hoelter & Carol Peters.
The Lan-guages of Definition: the Formalization of Dic-tionary Definitions for Natural Language Process-ing: Luxembourg: Office for Official Publicationsof the European Communities, pp 13-58.64Barnbrook, Geoffrey.
(2002)  Defining Language: Alocal grammar of definition sentences.
Amsterdam:John Benjamins Publishers.Omrane, Walid., Bauwens, Luc., and Giot, Pierre.
(2005) ?News Announcements, Market Activityand Volatility in the Euro/Dollar Foreign ExchangeMarket?.
Journal of International Money and Fi-nance, 24 (7), pp.
1108-1125.Brill, Eric.
(1993) ?Automatic Grammar Inductionand Parsing Free Text: A transformation-based ap-proach?.
In Proc.
of 31th Annual Meeting of the As-sociation for Computational Linguistics, Ohio.Chan, Wesley.
(2003) ?Stock Price Reaction to Newsand No-News.
Drift and Reversal after Headlines?.Journal of Financial Economics, 70(2), pp.
223-260.Ciravegna, Fabio.
and Wilks, Yorick.
(2003) ?Design-ing Adaptive Information Extraction for the Se-mantic Web in Amilcare?.
In (Eds.)
SiegfriedHandschuh and Steffen Staab, Annotation for theSemantic Web, Frontiers in Artificial Intelligenceand Applications.
US: IOS Press.Cutler, David., Poterba, James., and Summers, Law-rence.
(1989) ?What Moves Stock Prices??.
Jour-nal of Portfolio Management, 15(3), pp.
4-12.Debnath, Sandip.
and Giles, C. Lee.
(2005) ?A Learn-ing Based Model for Headline Extraction of NewsArticles to Find Explanatory Sentences for Events?.In Proc.
of the 3rd international conference onKnowledge capture, Alberta, Canada.Engle, Robert.
and K. Ng, Victor.
(1993) ?Measuringand Testing the Impact of News on Volatility?,Journal of Finance, 48(5), pp.
1749-1777.Gao, Jianfeng., Li, Mu., Wu, Andi.
and Huang,Chang-Ning (2005) ?Chinese Word Segmentationand Named Entity Recognition: A Pragmatic Ap-proach?, Journal of Computational Linguistics,31(4), Cambridge, Mass.
: MIT Press, pp.
531-574Gross, Maurice.
(1997) ?The Construction of LocalGrammars?.
In (Eds.)
Roche, E. and Schab?s, Y.,Finite-State Language Processing, Language,Speech, and Communication, Cambridge, Mass.
:MIT Press, pp.
329-354.Habash, Nizar.
and Rambow, Owen.
(2005) ?ArabicTokenization, Morphological Analysis, and Part-of-Speech Tagging in One Fell Swoop?.
In Proc.
ofthe Conference of American Association for Com-putational Linguistics (ACL?05), Ann Arbour, MI.Halliday, Michael, A. K. (1993) 'On the language ofPhysical Sciences'.
In (Eds.)
Halliday, Michael.and Martin, J. R., Writing Science pp.
54-68.
Lon-don: The Falmer Press.Hardie, Iain.
and MacKenzie, Donaled.
(2005)  ?AnEconomy of Calculation: Agencement and Distrib-uted Cognition in a Hedge Fund?.
(Available athttp://www.sps.ed.ac.uk/staff/An%20Economy%20of%20Calculation.pdf).Harris, Zellig.
(1991) A Theory of Language and In-formation: A Mathematical Approach.
Oxford:Clarendon Press.Kittredge, Richard.
and Lehrberger, John.
(1982) Sub-language: Studies of language in restricted seman-tic domains.
Berlin: Walter de Gruyter.Koppel, Moshe and Shtrimberg, Itai.
(2004) ?GoodNews or Bad News?
Let the Market Decide?.
InAAAI Spring Symposium on Exploring Attitude andAffect in Text, Palo Alto: AAAI Press, pp.
86-88.Mackenzie, Donald.
(2000).
?Fear in the Markets?.London Review of Books, 22(8), pp 31-32.McLernon, Brian.
and Kushmerick, Nicholas.
(2006)?Transductive Pattern Learning for Information Ex-traction?.
In Proc.
of EACL 2006 Workshop onAdaptive Text Extraction and Mining, Trento.Seo, Young-Woo., Giampapa, Joseph.
and Sycara,Katia.
(2002) ?Text Classification for IntelligentAgent Portfolio Management?.
In Proc.
of the 1stInternational Joint Conference on AutonomousAgents and Multi-Agent Systems, pp.
802-803, Bo-logna.Shiller, Robert.
(2000) Irrational Exuberance.
Prince-ton: Princeton University Press.Sinclair, John McH.
(1996) Collins COBUILDGrammar Patterns 1: Verbs.
HarperCollins, Glas-gow.Smadja, Frank.
(1994) ?Retrieving Collocations fromText: Xtract?.
In (Eds.)
Armstrong, S., Using LargeCorpora.
London: MIT Press.Stevenson, Mark.
and Greenwood, Mark A.
(2005) ?ASemantic Approach to IE Pattern Induction?.
InProc.
of the 43rd Meeting of the Association forComputational Linguistics (ACL?05), pp.
379-386,Ann Arbour, MI.Wilks, Yorick (1998) ?Inducing Adequate Grammarsfrom Electronic Texts?, EPSRC ROPA GrantGR/K/66215 Final Report.
(Available athttp://nlp.shef.ac.uk/research/reports/k66215.html).Yangarber, Roman.
(2003) ?Counter-Training in Dis-covery of Semantic Patterns?.
In Proc.
of the 41stAnnual Meeting of the Association for Computa-tional Linguistics (ACL 2003), pp.
343-350, Sap-poro, Japan.65
