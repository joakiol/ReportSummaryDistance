Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 375?383,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsEnhanced word decomposition by calibrating the decision threshold ofprobabilistic models and using a model ensembleSebastian SpieglerIntelligent Systems Laboratory,University of Bristol, U.K.spiegler@cs.bris.ac.ukPeter A. FlachIntelligent Systems Laboratory,University of Bristol, U.K.peter.flach@bristol.ac.ukAbstractThis paper demonstrates that the use ofensemble methods and carefully calibrat-ing the decision threshold can signifi-cantly improve the performance of ma-chine learning methods for morphologi-cal word decomposition.
We employ twoalgorithms which come from a family ofgenerative probabilistic models.
The mod-els consider segment boundaries as hiddenvariables and include probabilities for let-ter transitions within segments.
The ad-vantage of this model family is that it canlearn from small datasets and easily gen-eralises to larger datasets.
The first algo-rithm PROMODES, which participated inthe Morpho Challenge 2009 (an interna-tional competition for unsupervised mor-phological analysis) employs a lower or-der model whereas the second algorithmPROMODES-H is a novel development ofthe first using a higher order model.
Wepresent the mathematical description forboth algorithms, conduct experiments onthe morphologically rich language Zuluand compare characteristics of both algo-rithms based on the experimental results.1 IntroductionWords are often considered as the smallest unitof a language when examining the grammaticalstructure or the meaning of sentences, referred toas syntax and semantics, however, words them-selves possess an internal structure denominatedby the term word morphology.
It is worthwhilestudying this internal structure since a languagedescription using its morphological formation ismore compact and complete than listing all pos-sible words.
This study is called morpholog-ical analysis.
According to Goldsmith (2009)four tasks are assigned to morphological analy-sis: word decomposition into morphemes, build-ing morpheme dictionaries, defining morphosyn-tactical rules which state how morphemes canbe combined to valid words and defining mor-phophonological rules that specify phonologicalchanges morphemes undergo when they are com-bined to words.
Results of morphological analy-sis are applied in speech synthesis (Sproat, 1996)and recognition (Hirsimaki et al, 2006), machinetranslation (Amtrup, 2003) and information re-trieval (Kettunen, 2009).1.1 BackgroundIn the past years, there has been a lot of inter-est and activity in the development of algorithmsfor morphological analysis.
All these approacheshave in common that they build a morphologi-cal model which is then applied to analyse words.Models are constructed using rule-based meth-ods (Mooney and Califf, 1996; Muggleton andBain, 1999), connectionist methods (Rumelhartand McClelland, 1986; Gasser, 1994) or statisti-cal or probabilistic methods (Harris, 1955; Haferand Weiss, 1974).
Another way of classifying ap-proaches is based on the learning aspect duringthe construction of the morphological model.
Ifthe data for training the model has the same struc-ture as the desired output of the morphologicalanalysis, in other words, if a morphological modelis learnt from labelled data, the algorithm is clas-sified under supervised learning.
An example fora supervised algorithm is given by Oflazer et al(2001).
If the input data has no information to-wards the desired output of the analysis, the algo-rithm uses unsupervised learning.
Unsupervisedalgorithms for morphological analysis are Lin-guistica (Goldsmith, 2001), Morfessor (Creutz,2006) and Paramor (Monson, 2008).
Minimally orsemi-supervised algorithms are provided with par-tial information during the learning process.
This375has been done, for instance, by Shalonova et al(2009) who provided stems in addition to a wordlist in order to find multiple pre- and suffixes.
Acomparison of different levels of supervision formorphology learning on Zulu has been carried outby Spiegler et al (2008).Our two algorithms, PROMODES andPROMODES-H, perform word decomposi-tion and are based on probabilistic methodsby incorporating a probabilistic generativemodel.1 Their parameters can be estimatedfrom either labelled data, using maximum like-lihood estimates, or from unlabelled data byexpectation maximization2 which makes themeither supervised or unsupervised algorithms.The purpose of this paper is an analysis of theunderlying probabilistic models and the types oferrors committed by each one.
Furthermore, it isinvestigated how the decision threshold can be cal-ibrated and a model ensemble is tested.The remainder is structured as follows.
In Sec-tion 2 we introduce the probabilistic generativeprocess and show in Sections 2.1 and 2.2 howwe incorporate this process in PROMODES andPROMODES-H. We start our experiments with ex-amining the learning behaviour of the algorithmsin 3.1.
Subsequently, we perform a position-wisecomparison of predictions in 3.2, show how wefind a better decision threshold for placing mor-pheme boundaries in 3.3 and combine both algo-rithms using a model ensemble to leverage indi-vidual strengths in 3.4.
In 3.5 we examine howthe single algorithms contribute to the result of theensemble.
In Section 4 we will compare our ap-proaches to related work and in Section 5 we willdraw our conclusions.2 Probabilistic generative modelIntuitively, we could say that our models describethe process of word generation from the left to theright by alternately using two dice, the first for de-ciding whether to place a morpheme boundary inthe current word position and the second to get acorresponding letter transition.
We are trying toreverse this process in order to find the underlyingsequence of tosses which determine the morphemeboundaries.
We are applying the notion of a prob-1PROMODES stands for PRObabilistic MOdel for differentDEgrees of Supervision.
The H of PROMODES-H refers toHigher order.2In (Spiegler et al, 2009; Spiegler et al, 2010a) we havepresented an unsupervised version of PROMODES.abilistic generative process consisting of words asobserved variables X and their hidden segmenta-tion as latent variables Y .
If a generative model isfully parameterised it can be reversed to find theunderlying word decomposition by forming theconditional probability distribution Pr(Y |X).Let us first define the model-independent com-ponents.
A given word w j ?W with 1?
j ?
|W |consists of n letters and has m = n?1 positionsfor inserting boundaries.
A word?s segmentation isdepicted as a boundary vector b j = (b j1, .
.
.
,b jm)consisting of boundary values b ji ?
{0,1} with1?
i?
m which disclose whether or not a bound-ary is placed in position i.
A letter l j,i-1 precedesthe position i in w j and a letter l ji follows it.
Bothletters l j,i-1 and l ji are part of an alphabet.
Fur-thermore, we introduce a letter transition t ji whichgoes from l j,i-1 to l ji.2.1 PROMODESPROMODES is based on a zero-order model forboundaries b ji and on a first-order model for lettertransitions t ji.
It describes a word?s segmentationby its morpheme boundaries and resulting lettertransitions within morphemes.
A boundary vectorb j is found by evaluating each position i withargmaxb jiPr(b ji|t ji) = (1)argmaxb jiPr(b ji)Pr(t ji|b ji) .The first component of the equation above isthe probability distribution over non-/boundariesPr(b ji).
We assume that a boundary in i is in-serted independently from other boundaries (zero-order) and the graphemic representation of theword, however, is conditioned on the length ofthe word m j which means that the probabilitydistribution is in fact Pr(b ji|m j).
We guarantee?1r=0 Pr(b ji=r|m j) = 1.
To simplify the notationin later explanations, we will refer to Pr(b ji|m j)as Pr(b ji).The second component is the letter transitionprobability distribution Pr(t ji|b ji).
We suppose afirst-order Markov chain consisting of transitionst ji from letter l j,i-1 ?
AB to letter l ji ?
A where Ais a regular letter alphabet and AB=A?
{B} in-cludes B as an abstract morpheme start symbolwhich can occur in l j,i-1.
For instance, the suf-fix ?s?
of the verb form gets, marking 3rd personsingular, would be modelled as B?
s whereas amorpheme internal transition could be g?
e. We376guarantee ?l ji?A Pr(t ji|b ji)=1 with t ji being a tran-sition from a certain l j,i?1 ?
AB to l ji.
The ad-vantage of the model is that instead of evaluatingan exponential number of possible segmentations(2m), the best segmentation b?j=(b?j1, .
.
.
,b?jm) isfound with 2m position-wise evaluations usingb?ji = argmaxb jiPr(b ji|t ji) (2)=??????????
?1, if Pr(b ji=1)Pr(t ji|b ji=1)> Pr(b ji=0)Pr(t ji|b ji=0)0, otherwise .The simplifying assumptions made, however,reduce the expressive power of the model by notallowing any dependencies on preceding bound-aries or letters.
This can lead to over-segmentationand therefore influences the performance of PRO-MODES.
For this reason, we have extended themodel which led to PROMODES-H, a higher-orderprobabilistic model.2.2 PROMODES-HIn contrast to the original PROMODES model, wealso consider the boundary value b j,i-1 and mod-ify our transition assumptions for PROMODES-H in such a way that the new algorithm appliesa first-order boundary model and a second-ordertransition model.
A transition t ji is now definedas a transition from an abstract symbol in l j,i-1 ?
{N ,B} to a letter in l ji ?
A.
The abstract sym-bol is N or B depending on whether b ji is 0 or 1.This holds equivalently for letter transitions t j,i-1.The suffix of our previous example gets would bemodelled N ?
t?B?
s.Our boundary vector b j is then constructed fromargmaxb jiPr(b ji|t ji, t j,i-1,b j,i-1) = (3)argmaxb jiPr(b ji|b j,i-1)Pr(t ji|b ji, t j,i-1,b j,i-1) .The first component, the probability distributionover non-/boundaries Pr(b ji|b j,i-1), satisfies?1r=0 Pr(b ji=r|b j,i-1)=1 with b j,i-1,b ji ?
{0,1}.As for PROMODES, Pr(b ji|b j,i-1) is short-hand for Pr(b ji|b j,i-1,m j).
The secondcomponent, the letter transition proba-bility distribution Pr(t ji|b ji,b j,i-1), fulfils?l ji?A Pr(t ji|b ji, t j,i-1,b j,i-1)=1 with t ji beinga transition from a certain l j,i?1 ?
AB to l ji.
Onceagain, we find the word?s best segmentation b?j in2m evaluations withb?ji = argmaxb jiPr(b ji|t ji, t j,i-1,b j,i-1) = (4)????
?1, if Pr(b ji=1|b j,i-1)Pr(t ji|b ji=1, t j,i-1,b j,i-1)> Pr(b ji=0|b j,i-1)Pr(t ji|b ji=0, t j,i-1,b j,i-1)0, otherwise .We will show in the experimental results that in-creasing the memory of the algorithm by lookingat b j,i?1 leads to a better performance.3 Experiments and ResultsIn the Morpho Challenge 2009, PROMODESachieved competitive results on Finnish, Turkish,English and German ?
and scored highest on non-vowelized and vowelized Arabic compared to 9other algorithms (Kurimo et al, 2009).
For theexperiments described below, we chose the SouthAfrican language Zulu since our research workmainly aims at creating morphological resourcesfor under-resourced indigenous languages.
Zuluis an agglutinative language with a complex mor-phology where multiple prefixes and suffixes con-tribute to a word?s meaning.
Nevertheless, itseems that segment boundaries are more likely incertain word positions.
The PROMODES familyharnesses this characteristic in combination withdescribing morphemes by letter transitions.
Fromthe Ukwabelana corpus (Spiegler et al, 2010b) wesampled 2500 Zulu words with a single segmenta-tion each.3.1 Learning with increasing experienceIn our first experiment we applied 10-fold cross-validation on datasets ranging from 500 to 2500words with the goal of measuring how the learningimproves with increasing experience in terms oftraining set size.
We want to remind the reader thatour two algorithms are aimed at small datasets.We randomly split each dataset into 10 subsetswhere each subset was a test set and the corre-sponding 9 remaining sets were merged to a train-ing set.
We kept the labels of the training setto determine model parameters through maximumlikelihood estimates and applied each model tothe test set from which we had removed the an-swer keys.
We compared results on the test setagainst the ground truth by counting true positive(TP), false positive (FP), true negative (TN) and377false negative (FN) morpheme boundary predic-tions.
Counts were summarised using precision3,recall4 and f-measure5, as shown in Table 1.Data Precision Recall F-measure500 0.7127?0.0418 0.3500?0.0272 0.4687?0.02841000 0.7435?0.0556 0.3350?0.0197 0.4614?0.02501500 0.7460?0.0529 0.3160?0.0150 0.4435?0.02062000 0.7504?0.0235 0.3068?0.0141 0.4354?0.01682500 0.7557?0.0356 0.3045?0.0138 0.4337?0.0163(a) PROMODESData Precision Recall F-measure500 0.6983?0.0511 0.4938?0.0404 0.5776?0.03951000 0.6865?0.0298 0.5177?0.0177 0.5901?0.02051500 0.6952?0.0308 0.5376?0.0197 0.6058?0.01732000 0.7008?0.0140 0.5316?0.0146 0.6044?0.01102500 0.6941?0.0184 0.5396?0.0218 0.6068?0.0151(b) PROMODES-HTable 1: 10-fold cross-validation on Zulu.For PROMODES we can see in Table 1a thatthe precision increases slightly from 0.7127 to0.7557 whereas the recall decreases from 0.3500to 0.3045 going from dataset size 500 to 2500.This suggests that to some extent fewer morphemeboundaries are discovered but the ones which arefound are more likely to be correct.
We believethat this effect is caused by the limited memoryof the model which uses order zero for the occur-rence of a boundary and order one for letter tran-sitions.
It seems that the model gets quickly sat-urated in terms of incorporating new informationand therefore precision and recall do not drasti-cally change for increasing dataset sizes.
In Ta-ble 1b we show results for PROMODES-H. Acrossthe datasets precision stays comparatively con-stant around a mean of 0.6949 whereas the recallincreases from 0.4938 to 0.5396.
Compared toPROMODES we observe an increase in recall be-tween 0.1438 and 0.2351 at a cost of a decrease inprecision between 0.0144 and 0.0616.Since both algorithms show different behaviourwith increasing experience and PROMODES-Hyields a higher f-measure across all datasets, wewill investigate in the next experiments how thesedifferences manifest themselves at the boundarylevel.3 precision = T PT P+FP .4recall = T PT P+FN .5 f -measure = 2?precision?recallprecision+recall .TNPH?=?0.8726?TNP???=?0.9472??TPPH=?0.5394?TPP???=?0.3045??FPPH=?0.1274?FPP???=?0.0528???FNPH?=?0.4606??FNP????=?0.6955??0.3109?
0.7889?0.2111?0.6891?+?0.0819?(net)?+?0.0486?(net)?0.5698?0.8828?0.4302?0.1172?
?Figure 1: Contingency table for PROMODES [greywith subscript P] and PROMODES-H [black withsubscript PH] results including gross and netchanges of PROMODES-H.3.2 Position-wise comparison of algorithmicpredictionsIn the second experiment, we investigated whichaspects of PROMODES-H in comparison to PRO-MODES led to the above described differences inperformance.
For this reason we broke downthe summary measures of precision and recallinto their original components: true/false positive(TP/FP) and negative (TN/FN) counts presented inthe 2?
2 contingency table of Figure 1.
For gen-eral evidence, we averaged across all experimentsusing relative frequencies.
Note that the relativefrequencies of positives (TP + FN) and negatives(TN + FP) each sum to one.The goal was to find out how predictionsin each word position changed when applyingPROMODES-H instead of PROMODES.
Thiswould show where the algorithms agree andwhere they disagree.
PROMODES classifies non-boundaries in 0.9472 of the times correctly as TNand in 0.0528 of the times falsely as boundaries(FP).
The algorithm correctly labels 0.3045 of thepositions as boundaries (TP) and 0.6955 falsely asnon-boundaries (FN).
We can see that PROMODESfollows a rather conservative approach.When applying PROMODES-H, the majority ofthe FP?s are turned into non-boundaries, how-ever, a slightly higher number of previously cor-rectly labelled non-boundaries are turned intofalse boundaries.
The net change is a 0.0486 in-crease in FP?s which is the reason for the decreasein precision.
On the other side, more false non-378boundaries (FN) are turned into boundaries thanin the opposite direction with a net increase of0.0819 of correct boundaries which led to the in-creased recall.
Since the deduction of precisionis less than the increase of recall, a better over-allperformance of PROMODES-H is achieved.In summary, PROMODES predicts more accu-rately non-boundaries whereas PROMODES-H isbetter at finding morpheme boundaries.
So far wehave based our decision for placing a boundary ina certain word position on Equation 2 and 4 as-suming that P(b ji=1| .
.
.
)> P(b ji=0| .
.
.
)6 gives thebest result.
However, if the underlying distribu-tion for boundaries given the evidence is skewed,it might be possible to improve results by introduc-ing a certain decision threshold for inserting mor-pheme boundaries.
We will put this idea to the testin the following section.3.3 Calibration of the decision thresholdFor the third experiment we slightly changed ourexperimental setup.
Instead of dividing datasetsduring 10-fold cross-validation into training andtest subsets with the ratio of 9:1 we randomly splitthe data into training, validation and test sets withthe ratio of 8:1:1.
We then run our experimentsand measured contingency table counts.Rather than placing a boundary ifP(b ji=1| .
.
.)
> P(b ji=0| .
.
.)
which correspondsto P(b ji=1| .
.
.)
> 0.50 we introduced a decisionthreshold P(b ji=1| .
.
.)
> h with 0?
h?
1.
Thisis based on the assumption that the underlyingdistribution P(b ji| .
.
.)
might be skewed and anoptimal decision can be achieved at a differentthreshold.
The optimal threshold was sought onthe validation set and evaluated on the test set.An overview over the validation and test resultsis given in Table 2.
We want to point out that thethreshold which yields the best f-measure resulton the validation set returns almost the sameresult on the separate test set for both algorithmswhich suggests the existence of a general optimalthreshold.Since this experiment provided us with a set ofdata points where the recall varied monotonicallywith the threshold and the precision changed ac-cordingly, we reverted to precision-recall curves(PR curves) from machine learning.
FollowingDavis and Goadrich (2006) the algorithmic perfor-6Based on Equation 2 and 4 we use the notation P(b ji| .
.
.
)if we do not want to specify the algorithm.mance can be analysed more informatively usingthese kinds of curves.
The PR curve is plotted withrecall on the x-axis and precision on the y-axis forincreasing thresholds h. The PR curves for PRO-MODES and PROMODES-H are shown in Figure2 on the validation set from which we learnt ouroptimal thresholds h?.
Points were connected forreadability only ?
points on the PR curve cannotbe interpolated linearly.In addition to the PR curves, we plotted isomet-rics for corresponding f-measure values which aredefined as precision= f -measure?recall2recall?
f -measure and are hy-perboles.
For increasing f-measure values the iso-metrics are moving further to the top-right cornerof the plot.
For a threshold of h = 0.50 (markedby ?3?)
PROMODES-H has a better performancethan PROMODES.
Nevertheless, across the entirePR curve none of the algorithms dominates.
Onecurve would dominate another if all data pointsof the dominated curve were beneath or equalto the dominating one.
PROMODES has its opti-mal threshold at h?
= 0.36 and PROMODES-H ath?
= 0.37 where PROMODES has a slightly higherf-measure than PROMODES-H.
The points of op-timal f-measure performance are marked with ?4?on the PR curve.Prec.
Recall F-meas.PROMODES validation (h=0.50) 0.7522 0.3087 0.4378PROMODES test (h=0.50) 0.7540 0.3084 0.4378PROMODES validation (h?=0.36) 0.5857 0.7824 0.6699PROMODES test (h?=0.36) 0.5869 0.7803 0.6699PROMODES-H validation (h=0.50) 0.6983 0.5333 0.6047PROMODES-H test (h=0.50) 0.6960 0.5319 0.6030PROMODES-H validation (h?=0.37) 0.5848 0.7491 0.6568PROMODES-H test (h?=0.37) 0.5857 0.7491 0.6574Table 2: PROMODES and PROMODES-H on vali-dation and test set.Summarizing, we have shown that both algo-rithms commit different errors at the word posi-tion level whereas PROMODES is better in pre-dicting non-boundaries and PROMODES-H givesbetter results for morpheme boundaries at the de-fault threshold of h = 0.50.
In this section, wedemonstrated that across different decision thresh-olds h for P(b ji=1| .
.
.)
> h none of algorithmsdominates the other one, and at the optimal thresh-old PROMODES achieves a slightly higher perfor-mance than PROMODES-H.
The question whicharises is whether we can combine PROMODES andPROMODES-H in an ensemble that leverages indi-vidual strengths of both.3790.4 0.5 0.6 0.7 0.8 0.9 10.40.50.60.70.80.91RecallPrecisionPromodesPromodes?HPromodes?EF?measure isometricsDefault resultOptimal result (h*)Figure 2: Precision-recall curves for algorithms on validation set.3.4 A model ensemble to leverage individualstrengthsA model ensemble is a set of individually trainedclassifiers whose predictions are combined whenclassifying new instances (Opitz and Maclin,1999).
The idea is that by combining PROMODESand PROMODES-H, we would be able to avoid cer-tain errors each model commits by consulting theother model as well.
We introduce PROMODES-Eas the ensemble of PROMODES and PROMODES-H. PROMODES-E accesses the individual proba-bilities Pr(b ji=1| .
.
.)
and simply averages them:Pr(b ji=1|t ji)+Pr(b ji=1|t ji,b j,i-1, t j,i-1)2> h .As before, we used the default thresholdh = 0.50 and found the calibrated thresholdh?
= 0.38, marked with ?3?
and ?4?
in Figure 2and shown in Table 3.
The calibrated thresholdimproves the f-measure over both PROMODES andPROMODES-H.Prec.
Recall F-meas.PROMODES-E validation (h=0.50) 0.8445 0.4328 0.5723PROMODES-E test (h=0.50) 0.8438 0.4352 0.5742PROMODES-E validation (h?=0.38) 0.6354 0.7625 0.6931PROMODES-E test (h?=0.38) 0.6350 0.7620 0.6927Table 3: PROMODES-E on validation and test set.The optimal solution applying h?
= 0.38 ismore balanced between precision and recall andboosted the original result by 0.1185 on the testset.
Compared to its components PROMODES andPROMODES-H the f-measure increased by 0.0228and 0.0353 on the test set.In short, we have shown that by combiningPROMODES and PROMODES-H and finding theoptimal threshold, the ensemble PROMODES-Egives better results than the individual modelsthemselves and therefore manages to leverage theindividual strengths of both to a certain extend.However, can we pinpoint the exact contributionof each individual algorithm to the improved re-sult?
We try to find an answer to this question inthe analysis of the subsequent section.3.5 Analysis of calibrated algorithms andtheir model ensembleFor the entire dataset of 2500 words, we haveexamined boundary predictions dependent on therelative word position.
In Figure 3 and 4 we haveplotted the absolute counts of correct boundaries(TP) and non-boundaries (TN) which PROMODESpredicted but not PROMODES-H, and vice versa,as continuous lines.
We furthermore provided thenumber of individual predictions which were ulti-mately adopted by PROMODES-E in the ensembleas dashed lines.In Figure 3a we can see for the default thresh-old that PROMODES performs better in predictingnon-boundaries in the middle and the end of theword in comparison to PROMODES-H.
Figure 3b380shows the statistics for correctly predicted bound-aries.
Here, PROMODES-H outperforms PRO-MODES in predicting correct boundaries across theentire word length.
After the calibration, shownin Figure 4a, PROMODES-H improves the correctprediction of non-boundaries at the beginning ofthe word whereas PROMODES performs better atthe end.
For the boundary prediction in Figure 4bthe signal disappears after calibration.Concluding, it appears that our test languageZulu has certain features which are modelled bestwith either a lower or higher-order model.
There-fore, the ensemble leveraged strengths of both al-gorithms which led to a better overall performancewith a calibrated threshold.4 Related workWe have presented two probabilistic genera-tive models for word decomposition, PROMODESand PROMODES-H. Another generative modelfor morphological analysis has been describedby Snover and Brent (2001) and Snover et al(2002), however, they were interested in findingparadigms as sets of mutual exclusive operationson a word form whereas we are describing a gener-ative process using morpheme boundaries and re-sulting letter transitions.Moreover, our probabilistic models seem to re-semble Hidden Markov Models (HMMs) by hav-ing certain states and transitions.
The main differ-ence is that we have dependencies between statesas well as between emissions whereas in HMMsemissions only depend on the underlying state.Combining different morphological analysershas been performed, for example, by Atwell andRoberts (2006) and Spiegler et al (2009).
Theirapproaches, though, used majority vote to decidewhether a morpheme boundary is inserted in a cer-tain word position or not.
The algorithms them-selves were treated as black-boxes.Monson et al (2009) described an indirectapproach to probabilistically combine ParaMor(Monson, 2008) and Morfessor (Creutz, 2006).They used a natural language tagger which wastrained on the output of ParaMor and Morfes-sor.
The goal was to mimic each algorithm sinceParaMor is rule-based and there is no access toMorfessor?s internally used probabilities.
The tag-ger would then return a probability for starting anew morpheme in a certain position based on theoriginal algorithm.
These probabilities in com-bination with a threshold, learnt on a differentdataset, were used to merge word analyses.
Incontrast, our ensemble algorithm PROMODES-Edirectly accesses the probabilistic framework ofeach algorithm and combines them based on anoptimal threshold learnt on a validation set.5 ConclusionsWe have presented a method to learn a cali-brated decision threshold from a validation set anddemonstrated that ensemble methods in connec-tion with calibrated decision thresholds can givebetter results than the individual models them-selves.
We introduced two algorithms for word de-composition which are based on generative prob-abilistic models.
The models consider segmentboundaries as hidden variables and include prob-abilities for letter transitions within segments.PROMODES contains a lower order model whereasPROMODES-H is a novel development of PRO-MODES with a higher order model.
For bothalgorithms, we defined the mathematical modeland performed experiments on language data ofthe morphologically complex language Zulu.
Wecompared the performance on increasing train-ing set sizes and analysed for each word positionwhether their boundary prediction agreed or dis-agreed.
We found out that PROMODES was bet-ter in predicting non-boundaries and PROMODES-H gave better results for morpheme boundaries ata default decision threshold.
At an optimal de-cision threshold, however, both yielded a simi-lar f-measure result.
We then performed a fur-ther analysis based on relative word positions andfound out that the calibrated PROMODES-H pre-dicted non-boundaries better for initial word posi-tions whereas the calibrated PROMODES for mid-and final word positions.
For boundaries, the cali-brated algorithms had a similar behaviour.
Subse-quently, we showed that a model ensemble of bothalgorithms in conjunction with finding an optimalthreshold exceeded the performance of the singlealgorithms at their individually optimal threshold.AcknowledgementsWe would like to thank Narayanan Edakunni andBruno Gole?nia for discussions concerning this pa-per as well as the anonymous reviewers for theircomments.
The research described was sponsoredby EPSRC grant EP/E010857/1 Learning the mor-phology of complex synthetic languages.3810.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10100200300400500600700800Relative word positionAbsolute true negatives(TN)Performance on non?boundaries, default thresholdPromodes (unique TN)Promodes?H (unique TN)Promodes and Promodes?E (unique TN)Promodes?H and Promodes?E (unique TN)(a) True negatives, default0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10100200300400500600700800Relative word positionAbsolute true positives (TP)Performance on boundaries, default thresholdPromodes (unique TP)Promodes?H (unique TP)Promodes and Promodes?E (unique TP)Promodes?H and Promodes?E (unique TP)(b) True positives, defaultFigure 3: Analysis of results using default threshold.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10100200300400500600700800Relative word positionAbsolute true negatives(TN)Performance on non?boundaries, calibrated thresholdPromodes (unique TN)Promodes?H (unique TN)Promodes and Promodes?E (unique TN)Promodes?H and Promodes?E (unique TN)(a) True negatives, calibrated0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10100200300400500600700800Relative word positionAbsolute true positives (TP)Performance on boundaries, calibrated thresholdPromodes (unique TP)Promodes?H (unique TP)Promodes and Promodes?E (unique TP)Promodes?H and Promodes?E (unique TP)(b) True positives, calibratedFigure 4: Analysis of results using calibrated threshold.382ReferencesJ.
W. Amtrup.
2003.
Morphology in machine trans-lation systems: Efficient integration of finite statetransducers and feature structure descriptions.
Ma-chine Translation, 18(3):217?238.E.
Atwell and A. Roberts.
2006.
Combinatory hy-brid elementary analysis of text (CHEAT).
Proceed-ings of the PASCAL Challenges Workshop on Un-supervised Segmentation of Words into Morphemes,Venice, Italy.M.
Creutz.
2006.
Induction of the Morphology of Nat-ural Language: Unsupervised Morpheme Segmen-tation with Application to Automatic Speech Recog-nition.
Ph.D. thesis, Helsinki University of Technol-ogy, Espoo, Finland.J.
Davis and M. Goadrich.
2006.
The relationshipbetween precision-recall and ROC curves.
Interna-tional Conference on Machine Learning, Pittsburgh,PA, 233?240.M.
Gasser.
1994.
Modularity in a connectionistmodel of morphology acquisition.
Proceedings ofthe 15th conference on Computational linguistics,1:214?220.J.
Goldsmith.
2001.
Unsupervised learning of the mor-phology of a natural language.
Computational Lin-guistics, 27:153?198.J.
Goldsmith.
2009.
The Handbook of ComputationalLinguistics, chapter Segmentation and morphology.Blackwell.M.
A. Hafer and S. F. Weiss.
1974.
Word segmenta-tion by letter successor varieties.
Information Stor-age and Retrieval, 10:371?385.Z.
S. Harris.
1955.
From phoneme to morpheme.
Lan-guage, 31(2):190?222.T.
Hirsimaki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-pioja, and J. Pylkkonen.
2006.
Unlimited vocabu-lary speech recognition with morph language mod-els applied to Finnish.
Computer Speech And Lan-guage, 20(4):515?541.K.
Kettunen.
2009.
Reductive and generative ap-proaches to management of morphological variationof keywords in monolingual information retrieval:An overview.
Journal of Documentation, 65:267 ?290.M.
Kurimo, S. Virpioja, and V. T. Turunen.
2009.Overview and results of Morpho Challenge 2009.Working notes for the CLEF 2009 Workshop, Corfu,Greece.C.
Monson, K. Hollingshead, and B. Roark.
2009.Probabilistic ParaMor.
Working notes for the CLEF2009 Workshop, Corfu, Greece.C.
Monson.
2008.
ParaMor: From ParadigmStructure To Natural Language Morphology Induc-tion.
Ph.D. thesis, Language Technologies Institute,School of Computer Science, Carnegie Mellon Uni-versity, Pittsburgh, PA, USA.R.
J. Mooney and M. E. Califf.
1996.
Learning thepast tense of English verbs using inductive logic pro-gramming.
Symbolic, Connectionist, and StatisticalApproaches to Learning for Natural Language Pro-cessing, 370?384.S.
Muggleton and M. Bain.
1999.
Analogical predic-tion.
Inductive Logic Programming: 9th Interna-tional Workshop, ILP-99, Bled, Slovenia, 234.K.
Oflazer, S. Nirenburg, and M. McShane.
2001.Bootstrapping morphological analyzers by combin-ing human elicitation and machine learning.
Com-putational.
Linguistics, 27(1):59?85.D.
Opitz and R. Maclin.
1999.
Popular ensemblemethods: An empirical study.
Journal of ArtificialIntelligence Research, 11:169?198.D.
E. Rumelhart and J. L. McClelland.
1986.
Onlearning the past tenses of English verbs.
MITPress, Cambridge, MA, USA.K.
Shalonova, B. Gole?nia, and P. A. Flach.
2009.
To-wards learning morphology for under-resourced fu-sional and agglutinating languages.
IEEE Transac-tions on Audio, Speech, and Language Processing,17(5):956965.M.
G. Snover and M. R. Brent.
2001.
A Bayesianmodel for morpheme and paradigm identification.Proceedings of the 39th Annual Meeting on Asso-ciation for Computational Linguistics, 490 ?
498.M.
G. Snover, G. E. Jarosz, and M. R. Brent.
2002.Unsupervised learning of morphology using a noveldirected search algorithm: Taking the first step.
Pro-ceedings of the ACL-02 workshop on Morphologicaland phonological learning, 6:11?20.S.
Spiegler, B. Gole?nia, K. Shalonova, P. A. Flach, andR.
Tucker.
2008.
Learning the morphology of Zuluwith different degrees of supervision.
IEEE Work-shop on Spoken Language Technology.S.
Spiegler, B. Gole?nia, and P. A. Flach.
2009.
Pro-modes: A probabilistic generative model for worddecomposition.
Working Notes for the CLEF 2009Workshop, Corfu, Greece.S.
Spiegler, B. Gole?nia, and P. A. Flach.
2010a.
Un-supervised word decomposition with the Promodesalgorithm.
In Multilingual Information Access Eval-uation Vol.
I, CLEF 2009, Corfu, Greece, LectureNotes in Computer Science, Springer.S.
Spiegler, A. v. d. Spuy, and P. A. Flach.
2010b.
Uk-wabelana - An open-source morphological Zulu cor-pus.
in review.R.
Sproat.
1996.
Multilingual text analysis for text-to-speech synthesis.
Nat.
Lang.
Eng., 2(4):369?380.383
