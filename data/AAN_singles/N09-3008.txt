Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 43?48,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMultiple Word Alignment with Profile Hidden Markov ModelsAditya Bhargava and Grzegorz KondrakDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta, Canada, T6G 2E8{abhargava,kondrak}@cs.ualberta.caAbstractProfile hidden Markov models (ProfileHMMs) are specific types of hidden Markovmodels used in biological sequence analysis.We propose the use of Profile HMMs forword-related tasks.
We test their applicabilityto the tasks of multiple cognate alignment andcognate set matching, and find that they workwell in general for both tasks.
On the lattertask, the Profile HMM method outperformsaverage and minimum edit distance.
Giventhe success for these two tasks, we furtherdiscuss the potential applications of ProfileHMMs to any task where consideration of aset of words is necessary.1 IntroductionIn linguistics, it is often necessary to align words orphonetic sequences.
Covington (1996) uses align-ments of cognate pairs for the historical linguis-tics task of comparative reconstruction and Ner-bonne and Heeringa (1997) use alignments to com-pute relative distances between words from variousDutch dialects.
Algorithms for aligning pairs ofwords have been proposed by Covington (1996) andKondrak (2000).
However, it is often necessary toalign multiple words.
Covington (1998) proposeda method to align multiple words based on a hand-crafted scale of similarity between various classesof phonemes, again for the purpose of comparativereconstruction of languages.Profile hidden Markov models (Profile HMMs)are specific types of hidden Markov models usedin biological sequence analysis, where they haveyielded success for the matching of given sequencesto sequence families as well as to multiple sequencealignment (Durbin et al, 1998).
In this paper, weshow that Profile HMMs can be adapted to the taskof aligning multiple words.
We apply them to setsof multilingual cognates and show that they pro-duce good alignments.
We also use them for the re-lated task of matching words to established cognatesets, useful for a situation where it is not immedi-ately obvious to which cognate set a word should bematched.
The accuracy on the latter task exceeds theaccuracy of a method based on edit distance.Profile HMMs could also potentially be used forthe computation of word similarity when a wordmust be compared not to another word but to an-other set of words, taking into account propertiesof all constituent words.
The use of Profile HMMsfor multiple sequence alignment also presents ap-plications to the acquisition of mapping dictionaries(Barzilay and Lee, 2002) and sentence-level para-phrasing (Barzilay and Lee, 2003).This paper is organized as follows: we first de-scribe the uses of Profile HMMs in computationalbiology, their structure, and then discuss their appli-cations to word-related tasks.
We then discuss ourdata set and describe the tasks that we test and theirexperimental setups and results.
We conclude witha summary of the results and a brief discussion ofpotential future work.2 Profile hidden Markov modelsIn computational biology, it is often necessary todeal with multiple sequences, including DNA andprotein sequences.
For such biological sequenceanalysis, Profile HMMs are applied to the commontasks of simultaneously aligning multiple related se-quences to each other, aligning a new sequence to43Begin EndDLILMLM1I1D1I0Figure 1: A prototypical Profile HMM of length L. Mi isthe ith match state, Ii is the ith insert state, and Di is theith delete state.
Delete states are silent and are used toindicate gaps in a sequence.an already-aligned family of sequences, and evalu-ating a new sequence for membership in a family ofsequences.Profile HMMs consist of several types of states:match states, insert states, delete states, as well asa begin and end state.
For each position in a Pro-file HMM, there is one match state, one insert state,and one delete state.
A Profile HMM can thus be vi-sualized as a series of columns, where each columnrepresents a position in the sequence (see Figure 1).Any arbitrary sequence can then be represented as atraversal of states from column to column.Match states form the core of the model; eachmatch state is represented by a set of emission prob-abilities for each symbol in the output alphabet.These probabilities indicate the distribution of val-ues for a given position in a sequence.
Each matchstate can probabilistically transition to the next (i.e.next-column) match and delete states as well as thecurrent (i.e.
current-column) insert state.Insert states represent possible values that can beinserted at a given position in a sequence (before amatch emission or deletion).
They are representedin the same manner as match states, with each out-put symbol having an associated probability.
Insertstates are used to account for symbols that have beeninserted to a given position that might not other-wise have occurred ?naturally?
via a match state.
In-sert states can probabilistically transition to the nextmatch and delete states as well as the current insertstate (i.e.
itself).
Allowing insert states to transitionto themselves enables the consideration of multiple-symbol inserts.MMIIIMAG...CA-AG.CAG.AA---AAACAG...CFigure 2: A small DNA multiple alignment from (Durbinet al, 1998, p. 123).Similarly, delete states represent symbols thathave been removed from a given position.
For a se-quence to use a delete state for a given position indi-cates that a given character position in the model hasno corresponding characters in the given sequence.Hence, delete states are by nature silent and thushave no emission probabilities for the output sym-bols.
This is an important distinction from matchstates and insert states.
Each delete state can prob-abilistically transition to the next match and deletestates as well as the current insert state.Figure 2 shows a small example of a set of DNAsequences.
The match columns and insert columnsare marked with the letters M and I respectively inthe first line.
Where a word has a character in amatch column, it is a match state emission; whenthere is instead a gap, it is a delete state occur-rence.
Any characters in insert columns are insertstate emissions, and gaps in insert columns repre-sent simply that the particular insert state was notused for the sequence in question.Durbin et al (1998) describe the uses of Pro-file HMMs for tasks in biological sequence analy-sis.
Firstly, a Profile HMM must be constructed.
Ifa Profile HMM is to be constructed from a set ofaligned sequences, it is necessary to designate cer-tain columns as match columns and others as insertcolumn.
The simple heuristic that we adopt is tolabel those columns match states for which half ormore of the sequences have a symbol present (ratherthan a gap).
Other columns are labelled insert states.Then the probability akl of state k transitioning tostate l can be estimated by counting the number oftimesAkl that the transition is used in the alignment:akl = Akl?l?
Akl?Similarly, the probability ek(a) of state k emittingsymbol a is estimated by counting the number of44times Ek(a) that the emission is used in the align-ment:ek(a) = Ek(a)?a?
Ek(a?
)There is the danger that some probabilities may beset to zero, so it is essential to add pseudocounts.The pseudocount methods that we explore are de-scribed in section 3.If a Profile HMM is to be constructed from a setof unaligned sequences, an initial model is gener-ated after which it can be trained to the sequencesusing the Baum-Welch algorithm.
The length of themodel must be chosen, and is usually set to the av-erage length of the unaligned sequences.
To gener-ate the initial model, which amounts to setting thetransition and emission probabilities to some initialvalues, the probabilities are sampled from Dirichletdistributions.Once a Profile HMM has been constructed, it canbe used to evaluate a given sequence for member-ship in the family.
This is done via a straightforwardapplication of the forward algorithm (to get the fullprobability of the given sequence) or the Viterbi al-gorithm (to get the alignment of the sequence to thefamily).
For the alignment of multiple unaligned se-quences, a Profile HMM is constructed and trainedas described above and then each sequence can bealigned using the Viterbi algorithm.It should also be noted that Profile HMMs aregeneralizations of Pair HMMs, which have beenused for cognate identification and word similar-ity (Mackay and Kondrak, 2005) between pairs ofwords.
Unlike Pair HMMs, Profile HMMs areposition-specific; this is what allows their applica-tion to multiple sequences but also means that eachProfile HMM must be trained to a given set of se-quences, whereas Pair HMMs can be trained over avery large data set of pairs of words.3 Adapting Profile HMMs to wordsUsing Profile HMMs for biological sequences in-volves defining an alphabet and working with relatedsequences consisting of symbols from that alphabet.One could perform tasks with cognates sets in a sim-ilar manner; cognates are, after all, related words,and words are nothing more than sequences of sym-bols from an alphabet.
Thus Profile HMMs presentpotential applications to similar tasks for cognatesets.
We apply Profile HMMs to the multiple align-ment of cognate sets, which is done in the samemanner as multiple sequence alignment for biolog-ical sequences described above.
We also test Pro-file HMMs for determining the correct cognate setto which a word belongs when given a variety ofcognate sets for the same meaning; this is done in asimilar manner to the sequence membership evalua-tion task described above.Although there are a number of Profile HMMpackages available (e.g.
HMMER), we decided todevelop an implementation from scratch in order toachieve greater control over various adjustable pa-rameters.1 We investigated the following parame-ters:Favouring match states When constructing a Pro-file HMM from unaligned sequences, thechoice of initial model probabilities can have asignificant effect on results.
It may be sensibleto favour match states compared to other stateswhen constructing the initial model; since thetransition probabilities are sampled from aDirichlet distribution, the option of favouringmatch states assigns the largest returned proba-bility to the transition to a match state.Pseudocount method We implemented three pseu-docount methods from (Durbin et al, 1998).
Inthe following equations, ej(a) is the probabilityof state j emitting character a. cja representsthe observed counts of state j emitting symbola.
A is the weight given to the pseudocounts.Constant value A constant value AC is addedto each count.
This is a generalization ofLaplace?s rule, where C = 1A .ej(a) = cja +AC?a?
cja?
+ABackground frequency Pseudocounts areadded in proportion to the backgroundfrequency qa, which is the frequency ofoccurrence of character a.ej(a) = cja +Aqa?a?
cja?
+A1Our implementation is available online at http://www.cs.ualberta.ca/?ab31/profilehmm.45Substitution matrix (Durbin et al, 1998)Given a matrix s(a, b) that gives the log-odds similarity of characters a and b, wecan determine the conditional probabilityof a character b given character a:P (b|a) = qbes(a,b)Then we define fja to be the probabilityderived from the counts:fja = cja?a?
cja?Then the pseudocount values are set to:?ja = A?bfjbP (a|b)Finally, the pseudocount values are addedto the real counts as above:ej(a) = cja + ?ja?a?
cja?
+ ?ja?Pseudocount weight The weight that the pseudo-counts are given (A in the above equations).Smoothing during Baum-Welch The problem hasmany local optima and it is therefore easy forthe Baum-Welch algorithm to get stuck aroundone of these.
In order to avoid local optima,we tested the option of adding pseudocountsduring Baum-Welch (i.e.
between iterations)rather than after it.
This serves as a formof noise injection, effectively bumping Baum-Welch away from local optima.4 Data for experimentsOur data come from the Comparative IndoeuropeanData Corpus (Dyen et al, 1992).
The data consistof words in 95 languages in the Indoeuropean fam-ily organized into word lists corresponding to oneof 200 meanings.
Each word is represented in theEnglish alphabet.
Figure 3 shows a sample fromthe original corpus data.
We manually converted thedata into disjoint sets of cognate words, where eachcognate set contains only one word from each lan-guage.
We also removed words that were not cog-nate with any other words.On average, there were 4.37 words per cognateset.
The smallest cognate set had two words (sincea 026 DAY...b 003026 53 Bulgarian DEN026 47 Czech E DENY026 45 Czech DEN026 43 Lusatian L ZEN026 44 Lusatian U DZEN026 50 Polish DZIEN026 51 Russian DEN026 54 Serbocroatian DAN026 42 Slovenian DAN026 41 Latvian DIENA026 05 Breton List DEIZ, DE(Z)026 04 Welsh C DYDD026 20 Spanish DIA026 17 Sardinian N DIE026 11 Ladin DI026 08 Rumanian List ZI026 09 Vlach ZUE026 15 French Creole C ZU026 13 French JOUR026 14 Walloon DJOU026 10 Italian GIORNO...Figure 3: An excerpt from the original corpus data.
Thefirst two numbers denote the meaning and the language,respectively.we excluded those words that were not cognate withany other words), and the largest had 84 words.There were on average 10.92 cognate sets in a mean-ing.
The lowest number of cognate sets in a meaningwas 1, and the largest number was 22.5 Multiple cognate alignmentSimilar to their use for multiple sequence alignmentof sequences in a family, we test Profile HMMs forthe task of aligning cognates.
As described above,an initial model is generated.
We use the aforemen-tioned heuristic of setting the initial model length tothe average length of the sequences.
The transitionprobabilities are sampled from a uniform-parameterDirichlet distribution, with each parameter havinga value of 5.0.
The insert-state emission probabil-ities are set to the background frequencies and thematch-state emission probabilities are sampled froma Dirichlet distribution with parameters set in pro-portion to the background frequency.
The model is46MIIMIIMI MIIMIIMID--E--N- D--E--NYZ--E--N- DZ-E--N-DZIE--N- D--A--N-DI-E--NA D--E--IZD--I--A- D--Y--DDD--I--E- Z-----U-Z--U--E- Z-----I-J--O--UR D-----I-DJ-O--U- G--IORNOFigure 4: The alignment generated via the Profile HMMmethod for some cognates.
These were aligned together,but we show them in two columns to preserve space.trained to the cognate set via the Baum-Welch algo-rithm, and then each word in the set is aligned tothe model using the Viterbi algorithm.
The wordsare added to the training via a summation; therefore,the order in which the words are considered has noeffect, in contrast to iterative pairwise methods.The setting of the parameter values is discussed insection 6.5.1 ResultsTo evaluate Profile HMMs for multiple cognatealignment, we analyzed the alignments generated fora number of cognate sets.
We found that increasingthe pseudocount weight to 100 improved the qualityof the alignments by effectively biasing the modeltowards similar characters according to the substitu-tion matrix.Figure 4 shows the Profile HMM alignment for acognate set of words with the meaning ?day.?
Aswith Figure 2, the alignment?s first line is a guidelabel used to indicate which columns are matchcolumns and which are insert columns; note thatconsecutive insert columns represent the same insertstate and so are not aligned by the Profile HMM.While there were some duplicate words (i.e.
wordsthat had identical English orthographic representa-tions but came from different languages), we do notshow them here for brevity.In this example, we see that the Profile HMMmanages to identify those columns that are morehighly conserved as match states.
The ability toidentify characters that are similar and align themcorrectly can be attributed to the provided substitu-tion matrix.Note that the characters in the insert columnsshould not be treated as aligned even though theyrepresent emissions from the same insert state (thishighlights the difference between match and insertstates).
For example, Y, A, Z, D, R, and O are allplaced in a single insert column even though theycannot be traced to a single phoneme in a protoformof the cognate set.
Particularly infrequent charac-ters are more likely to be put together than separatedeven if they are phonetically dissimilar.There is some difficulty, also evident from otheralignments we generated, in isolating phonemes rep-resented by pairs of characters (digraphs) as singularentities.
In the given example, this means that the dzin dzien was modelled as a match state and then aninsert state.
This is, however, an inherent difficultyin using data represented only with the English al-phabet, which could potentially be addressed if thedata were instead represented in a standard phoneticnotation such as IPA.6 Cognate set matchingEvaluating alignments in a principled way is diffi-cult because of the lack of a gold standard.
To adjustfor this, we also evaluate Profile HMMs for the taskof matching a word to the correct cognate set froma list of cognate sets with the same meaning as thegiven word, similar to the evaluation of a biologi-cal sequence for membership in a family.
This isrealized by removing one word at a time from eachword list and then using the resulting cognate setswithin the meaning as possible targets.
A model isgenerated from each possible target and a log-oddsscore is computed for the word using the forwardalgorithm.
The scores are then sorted and the high-est score is taken to be the cognate set to which thegiven word belongs.
The accuracy is then the frac-tion of times the correct cognate set is identified.To determine the best parameter values, we useda development set of 10 meanings (roughly 5%of the data).
For the substitution matrix pseudo-count method, we used a log-odds similarity ma-trix derived from Pair HMM training (Mackay andKondrak, 2005).
The best results were achievedwith favouring of match states enabled, substitution-matrix-based pseudocount, pseudocount weight of0.5, and pseudocounts added during Baum-Welch.476.1 ResultsWe employed two baselines to generate scores be-tween a given word and cognate set.
The first base-line uses the average edit distance of the test wordand the words in the given cognate set as the scoreof the word against the set.
The second baseline issimilar but uses the minimum edit distance betweenthe test word and any word in the given cognate setas the score of the word against the entire set.
For ex-ample, in the example set given in Figure 4, the aver-age edit distance between zen and all other words inthe set is 2.58 (including the hidden duplicate words)and the minimum edit distance is 1.
All other can-didate sets are similarly scored and the one with thelowest score is considered to be the correct clusterwith ties broken randomly.With the parameter settings described in the pre-vious section, the Profile HMM method correctlyidentifies the corresponding cognate set with an ac-curacy of 93.2%, a substantial improvement over theaverage edit distance baseline, which obtains an ac-curacy of 77.0%.Although the minimum edit distance baseline alsoyields an impressive accuracy of 91.0%, its score isbased on a single word in the candidate set, and sowould not be appropriate for cases where consider-ation of the entire set is necessary.
Furthermore, thebaseline benefits from the frequent presence of du-plicate words in the cognate sets.
Profile HMMs aremore robust, thanks to the presence of identical orsimilar characters in corresponding positions.7 ConclusionsProfile HMMs present an approach for working withsets of words.
We tested their use for two cognate-related tasks.
The method produced good-qualitymultiple cognate alignments, and we believe thatthey could be further improved with phoneticallytranscribed data.
For the task of matching words tocorrect cognate sets, we achieved an improvementover the average edit distance and minimum edit dis-tance baselines.Since Profile HMM training is highly sensitive tothe choice of initial model, we would like to ex-plore more informed methods of constructing theinitial model.
Similarly, for building models fromunaligned sequences, the addition of domain knowl-edge would likely prove beneficial.
We also plan toinvestigate better pseudocount methods, as well asthe possibility of using n-grams as output symbols.By simultaneously considering an entire set of re-lated words, Profile HMMs provide a distinct ad-vantage over iterative pairwise methods.
The suc-cess on our tasks of multiple alignment and cognateset matching suggests applicability to similar tasksinvolving words, such as named entity recognitionacross potentially multi-lingual corpora.AcknowledgementsWe thank Qing Dou for organizing the cognate setsfrom the original data.
We are also grateful to theanonymous reviewers for their valuable comments.This research was funded in part by the Natural Sci-ences and Engineering Research Council of Canada.ReferencesRegina Barzilay and Lillian Lee.
2002.
Bootstrappinglexical choice via multiple-sequence alignment.
InProc.
of EMNLP, pages 164?171.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proc.
of NAACL-HLT, pages16?23.Michael A. Covington.
1996.
An algorithm to alignwords for historical comparison.
Computational Lin-guistics, 22(4):481?496.Michael A. Covington.
1998.
Alignment of multi-ple languages for historical comparison.
In Proc.
ofCOLING-ACL, pages 275?279.Richard Durbin, Sean R. Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological sequence analy-sis: probabilistic models of proteins and nucleic acids.Cambridge University Press.Isidore Dyen, Joseph B. Kruskal, and Paul Black.
1992.An Indoeuropean classification: A lexicostatistical ex-periment.
Transactions of the American PhilosophicalSociety, 82(5).Grzegorz Kondrak.
2000.
A new algorithm for the align-ment of phonetic sequences.
In Proc.
of NAACL, pages288?295.Wesley Mackay and Grzegorz Kondrak.
2005.
Comput-ing word similarity and identifying cognates with pairhidden Markov models.
In Proc.
of CoNLL, pages 40?47.John Nerbonne and Wilbert Heeringa.
1997.
Measur-ing dialect distance phonetically.
In Proc.
of the ThirdMeeting of ACL SIGPHON.48
