Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552?559,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsTowards an Iterative Reinforcement Approach for SimultaneousDocument Summarization and Keyword ExtractionXiaojun Wan                     Jianwu Yang                     Jianguo XiaoInstitute of Computer Science and TechnologyPeking University, Beijing 100871, China{wanxiaojun,yangjianwu,xiaojianguo}@icst.pku.edu.cnAbstractThough both document summarization andkeyword extraction aim to extract conciserepresentations from documents, these twotasks have usually been investigated inde-pendently.
This paper proposes a novel it-erative reinforcement approach to simulta-neously extracting summary and keywordsfrom single document under the assump-tion that the summary and keywords of adocument can be mutually boosted.
Theapproach can naturally make full use of thereinforcement between sentences and key-words by fusing three kinds of relation-ships between sentences and words, eitherhomogeneous or heterogeneous.
Experi-mental results show the effectiveness of theproposed approach for both tasks.
The cor-pus-based approach is validated to workalmost as well as the knowledge-based ap-proach for computing word semantics.1 IntroductionText summarization is the process of creating acompressed version of a given document that de-livers the main topic of the document.
Keywordextraction is the process of extracting a few salientwords (or phrases) from a given text and using thewords to represent the text.
The two tasks are simi-lar in essence because they both aim to extractconcise representations for documents.
Automatictext summarization and keyword extraction havedrawn much attention for a long time because theyboth are very important for many text applications,including document retrieval, document clustering,etc.
For example, keywords of a document can beused for document indexing and thus benefit toimprove the performance of document retrieval,and document summary can help to facilitate usersto browse the search results and improve users?search experience.Text summaries and keywords can be eitherquery-relevant or generic.
Generic summary andkeyword should reflect the main topics of thedocument without any additional clues and priorknowledge.
In this paper, we focus on genericdocument summarization and keyword extractionfor single documents.Document summarization and keyword extrac-tion have been widely explored in the natural lan-guage processing and information retrieval com-munities.
A series of workshops and conferenceson automatic text summarization (e.g.
SUMMAC,DUC and NTCIR) have advanced the technologyand produced a couple of experimental online sys-tems.
In recent years, graph-based ranking algo-rithms have been successfully used for documentsummarization (Mihalcea and Tarau, 2004, 2005;ErKan and Radev, 2004) and keyword extraction(Mihalcea and Tarau, 2004).
Such algorithms makeuse of ?voting?
or ?recommendations?
betweensentences (or words) to extract sentences (or key-words).
Though the two tasks essentially sharemuch in common, most algorithms have been de-veloped particularly for either document summari-zation or keyword extraction.Zha (2002) proposes a method for simultaneouskeyphrase extraction and text summarization byusing only the heterogeneous sentence-to-wordrelationships.
Inspired by this, we aim to take intoaccount all the three kinds of relationships amongsentences and words (i.e.
the homogeneous rela-tionships between words, the homogeneous rela-tionships between sentences, and the heterogene-ous relationships between words and sentences) in552a unified framework for both document summari-zation and keyword extraction.
The importance ofa sentence (word) is determined by both the impor-tance of related sentences (words) and the impor-tance of related words (sentences).
The proposedapproach can be considered as a generalized formof previous graph-based ranking algorithms andZha?s work (Zha, 2002).In this study, we propose an iterative reinforce-ment approach to realize the above idea.
The pro-posed approach is evaluated on the DUC2002dataset and the results demonstrate its effectivenessfor both document summarization and keywordextraction.
Both knowledge-based approach andcorpus-based approach have been investigated tocompute word semantics and they both performvery well.The rest of this paper is organized as follows:Section 2 introduces related works.
The details ofthe proposed approach are described in Section 3.Section 4 presents and discusses the evaluationresults.
Lastly we conclude our paper in Section 5.2 Related Works2.1 Document SummarizationGenerally speaking, single document summariza-tion methods can be either extraction-based or ab-straction-based and we focus on extraction-basedmethods in this study.Extraction-based methods usually assign a sali-ency score to each sentence and then rank the sen-tences in the document.
The scores are usuallycomputed based on a combination of statistical andlinguistic features, including term frequency, sen-tence position, cue words, stigma words, topic sig-nature (Hovy and Lin, 1997; Lin and Hovy, 2000),etc.
Machine learning methods have also been em-ployed to extract sentences, including unsupervisedmethods (Nomoto and Matsumoto, 2001) and su-pervised methods (Kupiec et al, 1995; Conroy andO?Leary, 2001; Amini and Gallinari, 2002; Shen etal., 2007).
Other methods include maximal mar-ginal relevance (MMR) (Carbonell and Goldstein,1998), latent semantic analysis (LSA) (Gong andLiu, 2001).
In Zha (2002), the mutual reinforce-ment principle is employed to iteratively extractkey phrases and sentences from a document.Most recently, graph-based ranking methods, in-cluding TextRank ((Mihalcea and Tarau, 2004,2005) and LexPageRank (ErKan and Radev, 2004)have been proposed for document summarization.Similar to Kleinberg?s HITS algorithm (Kleinberg,1999) or Google?s PageRank (Brin and Page,1998), these methods first build a graph based onthe similarity between sentences in a document andthen the importance of a sentence is determined bytaking into account global information on thegraph recursively, rather than relying only on localsentence-specific information.2.2 Keyword ExtractionKeyword (or keyphrase) extraction usually in-volves assigning a saliency score to each candidatekeyword by considering various features.
Krulwichand Burkey (1996) use heuristics to extract key-phrases from a document.
The heuristics are basedon syntactic clues, such as the use of italics, thepresence of phrases in section headers, and the useof acronyms.
Mu?oz (1996) uses an unsupervisedlearning algorithm to discover two-word key-phrases.
The algorithm is based on Adaptive Reso-nance Theory (ART) neural networks.
Steier andBelew (1993) use the mutual information statisticsto discover two-word keyphrases.Supervised machine learning algorithms havebeen proposed to classify a candidate phrase intoeither keyphrase or not.
GenEx (Turney, 2000) andKea (Frank et al, 1999; Witten et al, 1999) aretwo typical systems, and the most important fea-tures for classifying a candidate phrase are the fre-quency and location of the phrase in the document.More linguistic knowledge (such as syntactic fea-tures) has been explored by Hulth (2003).
Morerecently, Mihalcea and Tarau (2004) propose theTextRank model to rank keywords based on theco-occurrence links between words.3 Iterative Reinforcement Approach3.1 OverviewThe proposed approach is intuitively based on thefollowing assumptions:Assumption 1: A sentence should be salient if itis heavily linked with other salient sentences, and aword should be salient if it is heavily linked withother salient words.Assumption 2: A sentence should be salient if itcontains many salient words, and a word should besalient if it appears in many salient sentences.The first assumption is similar to PageRankwhich makes use of mutual ?recommendations?553between homogeneous objects to rank objects.
Thesecond assumption is similar to HITS if words andsentences are considered as authorities and hubsrespectively.
In other words, the proposed ap-proach aims to fuse the ideas of PageRank andHITS in a unified framework.In more detail, given the heterogeneous datapoints of sentences and words, the following threekinds of relationships are fused in the proposedapproach:SS-Relationship: It reflects the homogeneousrelationships between sentences, usually computedby their content similarity.WW-Relationship: It reflects the homogeneousrelationships between words, usually computed byknowledge-based approach or corpus-based ap-proach.SW-Relationship: It reflects the heterogeneousrelationships between sentences and words, usuallycomputed as the relative importance of a word in asentence.Figure 1 gives an illustration of the relationships.Figure 1.
Illustration of the RelationshipsThe proposed approach first builds three graphsto reflect the above relationships respectively, andthen iteratively computes the saliency scores of thesentences and words based on the graphs.
Finally,the algorithm converges and each sentence or wordgets its saliency score.
The sentences with highsaliency scores are chosen into the summary, andthe words with high saliency scores are combinedto produce the keywords.3.2 Graph Building3.2.1  Sentence-to-Sentence Graph ( SS-Graph)Given the sentence collection S={si | 1IiIm} of adocument,  if each sentence is considered as a node,the sentence collection can be modeled as an undi-rected graph by generating an edge between twosentences if their content similarity exceeds 0, i.e.an undirected link between si and sj (iKj) is con-structed and the associated weight is their contentsimilarity.
Thus, we construct an undirected graphGSS to reflect the homogeneous relationship be-tween sentences.
The content similarity betweentwo sentences is computed with the cosine measure.We use an adjacency matrix U to describe GSS witheach entry corresponding to the weight of a link inthe graph.
U= [Uij]m?m is defined as follows:?
?otherwise,j, if issssU jijiij0rrrr(1)where is and jsr are the corresponding term vec-tors of sentences si and sj respectively.
The weightassociated with term t is calculated with tft.isft,where tft is the frequency of term t in the sentenceand isft is the inverse sentence frequency of term t,i.e.
1+log(N/nt), where N is the total number ofsentences and nt is the number of sentences con-taining term t in a background corpus.
Note thatother measures (e.g.
Jaccard, Dice, Overlap, etc.
)can also be explored to compute the content simi-larity between sentences, and we simply choose thecosine measure in this study.Then U is normalized to U~ as follows to makethe sum of each row equal to 1: ?erwise , othU, if UUUmjijmjijijij00~11 (2)3.2.2  Word-to-Word Graph ( WW-Graph)Given the word collection T={tj|1IjIn } of a docu-ment1 , the semantic similarity between any twowords ti and tj can be computed using approachesthat are either knowledge-based or corpus-based(Mihalcea et al, 2006).Knowledge-based measures of word semanticsimilarity try to quantify the degree to which twowords are semantically related using informationdrawn from semantic networks.
WordNet (Fell-baum, 1998) is a lexical database where each1 The stopwords defined in the Smart system have been re-moved from the collection.sentencewordSSWWSW554unique meaning of a word is represented by asynonym set or synset.
Each synset has a gloss thatdefines the concept that it represents.
Synsets areconnected to each other through explicit semanticrelations that are defined in WordNet.
Many ap-proaches have been proposed to measure semanticrelatedness based on WordNet.
The measures varyfrom simple edge-counting to attempt to factor inpeculiarities of the network structure by consider-ing link direction, relative path, and density, suchas  vector, lesk, hso, lch, wup, path, res, lin and jcn(Pedersen et al, 2004).
For example, ?cat?
and?dog?
has higher semantic similarity than ?cat?and ?computer?.
In this study, we implement thevector measure to efficiently evaluate the similari-ties of a large number of word pairs.
The vectormeasure (Patwardhan, 2003) creates a co?occurrence matrix from a corpus made up of theWordNet glosses.
Each content word used in aWordNet gloss has an associated context vector.Each gloss is represented by a gloss vector that isthe average of all the context vectors of the wordsfound in the gloss.
Relatedness between conceptsis measured by finding the cosine between a pair ofgloss vectors.Corpus-based measures of word semantic simi-larity try to identify the degree of similarity be-tween words using information exclusively derivedfrom large corpora.
Such measures as mutual in-formation (Turney 2001), latent semantic analysis(Landauer et al, 1998), log-likelihood ratio (Dun-ning, 1993) have been proposed to evaluate wordsemantic similarity based on the co-occurrenceinformation on a large corpus.
In this study, wesimply choose the mutual information to computethe semantic similarity between word ti and tj asfollows:)()()(log)(jijiji tptp,ttpN,ttsim(3)which indicates the degree of statistical depend-ence between ti and tj.
Here, N is the total numberof words in the corpus and p(ti) and p(tj) are re-spectively the probabilities of the occurrences of tiand tj, i.e.
count(ti)/N and count(tj)/N, wherecount(ti) and count(tj) are the frequencies of ti and tj.p(ti, tj) is the probability of the co-occurrence of tiand tj within a window with a predefined size k, i.e.count(ti, tj)/N, where count(ti, tj) is the number ofthe times ti and tj co-occur within the window.Similar to the SS-Graph, we can build an undi-rected graph GWW to reflect the homogeneous rela-tionship between words, in which each node corre-sponds to a word and the weight associated withthe edge between any different word ti and tj iscomputed by either the WordNet-based vectormeasure or the corpus-based mutual informationmeasure.
We use an adjacency matrix V to de-scribe GWW with each entry corresponding to theweight of a link in the graph.
V= [Vij]n?n, where Vij=sim(ti, tj) if iKj and Vij=0 if i=j.Then V is similarly normalized to V~ to makethe sum of each row equal to 1.3.2.3  Sentence-to-Word Graph ( SW-Graph)Given the sentence collection S={si | 1IiIm} andthe word collection T={tj|1IjIn } of a document,we can build a weighted bipartite graph GSW from Sand T in the following way: if word tj appears insentence si, we then create an edge between si andtj.
A nonnegative weight aff(si,tj) is specified on theedge, which is proportional to the importance ofword tj in sentence si, computed as follows:ijjstttttji isftfisftf,tsaff )(  (4)where t represents a unique term in si and tft, isftare respectively the term frequency in the sentenceand the inverse sentence frequency.We use an adjacency (affinity) matrixW=[Wij]m?n to describe GSW  with each entry Wijcorresponding to aff(si,tj).
Similarly, W is normal-ized to W~ to make the sum of each row equal to 1.In addition, we normalize the transpose of W, i.e.WT, to W?
to make the sum of each row in WTequal to 1.3.3 Reinforcement AlgorithmWe use two column vectors u=[u(si)]m?1 and v=[v(tj)]n?1 to denote the saliency scores of the sen-tences and words in the specified document.
Theassumptions introduced in Section 3.1 can be ren-dered as follows: j jjii suUsu )(~)( (5) i iijj tvVtv )(~)( (6) j jjii tvWsu )(?
)( (7)555 i iijj suWtv )(~)( (8)After fusing the above equations, we can obtainthe following iterative forms: njjjimjjjii tvW)suU*su11)(?
)(~)( (9) miiijniiijj suW)tvV*tv11)(~)(~)( (10)And the matrix form is:vWuUu TT )* ?~ (11)uWvVv TT )* ~~ (12)where * and ) specify the relative contributions tothe final saliency scores from the homogeneousnodes and the heterogeneous nodes and we have*+)=1.
In order to guarantee the convergence ofthe iterative form, u and v are normalized aftereach iteration.For numerical computation of the saliencyscores, the initial scores of all sentences and wordsare set to 1 and the following two steps are alter-nated until convergence,1.
Compute and normalize the scores of sen-tences:)(n-T)(n-T(n) )* 11 ?~ vWuUu ,1(n)(n)(n) / uuu2.
Compute and normalize the scores of words:)(n-T)(n-T(n) )* 11 ~~ uWvVv ,1(n)(n)(n) / vvvwhere u(n) and v(n) denote the vectors computed atthe n-th iteration.Usually the convergence of the iteration algo-rithm is achieved when the difference between thescores computed at two successive iterations forany sentences and words falls below a giventhreshold (0.0001 in this study).4 Empirical Evaluation4.1 Summarization Evaluation4.1.1 Evaluation SetupWe used task 1 of DUC2002 (DUC, 2002) forevaluation.
The task aimed to evaluate genericsummaries with a length of approximately 100words or less.
DUC2002 provided 567 Englishnews articles collected from TREC-9 for single-document summarization task.
The sentences ineach article have been separated and the sentenceinformation was stored into files.In the experiments, the background corpus forusing the mutual information measure to computeword semantics simply consisted of all the docu-ments from DUC2001 to DUC2005, which couldbe easily expanded by adding more documents.The stopwords were removed and the remainingwords were converted to the basic forms based onWordNet.
Then the semantic similarity values be-tween the words were computed.We used the ROUGE (Lin and Hovy, 2003)toolkit (i.e.ROUGEeval-1.4.2 in this study) forevaluation, which has been widely adopted byDUC for automatic summarization evaluation.
Itmeasured summary quality by counting overlap-ping units such as the n-gram, word sequences andword pairs between the candidate summary and thereference summary.
ROUGE toolkit reported sepa-rate scores for 1, 2, 3 and 4-gram, and also forlongest common subsequence co-occurrences.Among these different scores, unigram-basedROUGE score (ROUGE-1) has been shown toagree with human judgment most (Lin and Hovy,2003).
We showed three of the ROUGE metrics inthe experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), and ROUGE-W (based on weighted longest common subse-quence, weight=1.2).In order to truncate summaries longer than thelength limit, we used the ?-l?
option 2 in theROUGE toolkit.4.1.2 Evaluation ResultsFor simplicity, the parameters in the proposed ap-proach are simply set to *=)=0.5, which meansthat the contributions from sentences and wordsare equally important.
We adopt the WordNet-based vector measure (WN) and the corpus-basedmutual information measure (MI) for computingthe semantic similarity between words.
When us-ing the mutual information measure, we heuristi-cally set the window size k to 2, 5 and 10, respec-tively.The proposed approaches with different wordsimilarity measures (WN and MI) are compared2 The ?-l?
option is very important for fair comparison.
Someprevious works not adopting this option are likely to overes-timate the ROUGE scores.556with two solid baselines: SentenceRank and Mutu-alRank.
SentenceRank is proposed in Mihalcea andTarau (2004) to make use of only the sentence-to-sentence relationships to rank sentences, whichoutperforms most popular summarization methods.MutualRank is proposed in Zha (2002) to make useof only the sentence-to-word relationships to ranksentences and words.
For all the summarizationmethods, after the sentences are ranked by theirsaliency scores, we can apply a variant form of theMMR algorithm to remove redundancy and chooseboth the salient and novel sentences to the sum-mary.
Table 1 gives the comparison results of themethods before removing redundancy and Table 2gives the comparison results of the methods afterremoving redundancy.System ROUGE-1 ROUGE-2 ROUGE-WOur Approach(WN) 0.47100*# 0.20424*# 0.16336#Our Approach(MI:k=2) 0.46711# 0.20195# 0.16257#Our Approach(MI:k=5) 0.46803# 0.20259# 0.16310#Our Approach(MI:k=10) 0.46823# 0.20301# 0.16294#SentenceRank 0.45591 0.19201 0.15789MutualRank 0.43743 0.17986 0.15333Table 1.
Summarization Performance before Re-moving Redundancy (w/o MMR)System ROUGE-1 ROUGE-2 ROUGE-WOur Approach(WN) 0.47329*# 0.20249# 0.16352#Our Approach(MI:k=2) 0.47281# 0.20281# 0.16373#Our Approach(MI:k=5) 0.47282# 0.20249# 0.16343#Our Approach(MI:k=10) 0.47223# 0.20225# 0.16308#SentenceRank 0.46261 0.19457 0.16018MutualRank 0.43805 0.17253 0.15221Table 2.
Summarization Performance after Remov-ing Redundancy (w/ MMR)(* indicates that the improvement over SentenceRank is sig-nificant and # indicates that the improvement over Mutual-Rank is significant, both by comparing the 95% confidenceintervals provided by the ROUGE package.
)Seen from Tables 1 and 2, the proposed ap-proaches always outperform the two baselines overall three metrics with different word semanticmeasures.
Moreover, no matter whether the MMRalgorithm is applied or not, almost all performanceimprovements over MutualRank are significantand the ROUGE-1 performance improvementsover SentenceRank are significant when usingWordNet-based measure (WN).
Word semanticscan be naturally incorporated into the computationprocess, which addresses the problem that Sen-tenceRank cannot take into account word seman-tics, and thus improves the summarization per-formance.
We also observe that the corpus-basedmeasure (MI) works almost as well as the knowl-edge-based measure (WN) for computing wordsemantic similarity.In order to better understand the relative contri-butions from the sentence nodes and the wordnodes, the parameter * is varied from 0 to 1.
Thelarger * is, the more contribution is given from thesentences through the SS-Graph, while the lesscontribution is given from the words through theSW-Graph.
Figures 2-4 show the curves over threeROUGE scores with respect to *.
Without loss ofgenerality, we use the case of k=5 for the MImeasure as an illustration.
The curves are similarto Figures 2-4 when k=2 and k=10.0.4350.440.4450.450.4550.460.4650.470.4750 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1*ROUGE-1MI(w/o MMR) MI(w/ MMR)WN(w/o MMR) WN(w/ MMR)Figure 2.
ROUGE-1 vs. *0.170.1750.180.1850.190.1950.20.2050.210 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1*ROUGE-2MI(w/o MMR) MI(w/ MMR)WN(w/o MMR) WN(w/ MMR)Figure 3.
ROUGE-2 vs. *5570.1510.1530.1550.1570.1590.1610.1630.1650 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1*ROUGE-WMI(w/o MMR) MI(w/ MMR)WN(w/o MMR) WN(w/ MMR)Figure 4.
ROUGE-W vs. *Seen from Figures 2-4, no matter whether theMMR algorithm is applied or not (i.e.
w/o MMRor w/ MMR), the ROUGE scores based on eitherword semantic measure (MI or WN) achieves thepeak when * is set between 0.4 and 0.6.
The per-formance values decrease sharply when * is verylarge (near to 1) or very small (near to 0).
Thecurves demonstrate that both the contribution fromthe sentences and the contribution from the wordsare important for ranking sentences; moreover, thecontributions are almost equally important.
Loss ofeither contribution will much deteriorate the finalperformance.Similar results and observations have been ob-tained on task 1 of DUC2001 in our study and thedetails are omitted due to page limit.4.2 Keyword Evaluation4.1.1   Evaluation SetupIn this study we performed a preliminary evalua-tion of keyword extraction.
The evaluation wasconducted on the single word level instead of themulti-word phrase (n-gram) level, in other words,we compared the automatically extracted unigrams(words) and the manually labeled unigrams(words).
The reasons were that: 1) there existedpartial matching between phrases and it was nottrivial to define an accurate measure to evaluatephrase quality; 2) each phrase was in fact com-posed of a few words, so the keyphrases could beobtained by combining the consecutive keywords.We used 34 documents in the first five docu-ment clusters in DUC2002 dataset (i.e.
d061-d065).At most 10 salient words were manually labeledfor each document to represent the document andthe average number of manually assigned key-words was 6.8.
Each approach returned 10 wordswith highest saliency scores as the keywords.
Theextracted 10 words were compared with the manu-ally labeled keywords.
The words were convertedto their corresponding basic forms based onWordNet before comparison.
The precision p, re-call r, F-measure (F=2pr/(p+r)) were obtained foreach document and then the values were averagedover all documents for evaluation purpose.4.1.2 Evaluation ResultsTable 3 gives the comparison results.
The proposedapproaches are compared with two baselines:WordRank and MutualRank.
WordRank is pro-posed in Mihalcea and Tarau (2004) to make useof only the co-occurrence relationships betweenwords to rank words, which outperforms tradi-tional keyword extraction methods.
The windowsize k for WordRank is also set to 2, 5 and 10, re-spectively.System Precision Recall F-measureOur Approach(WN) 0.413 0.504 0.454Our Approach(MI:k=2) 0.428 0.485 0.455Our Approach(MI:k=5) 0.425 0.491 0.456Our Approach(MI:k=10) 0.393 0.455 0.422WordRank(k=2) 0.373 0.412 0.392WordRank(k=5) 0.368 0.422 0.393WordRank(k=10) 0.379 0.407 0.393MutualRank 0.355 0.397 0.375Table 3.
The Performance of Keyword ExtractionSeen from the table, the proposed approachessignificantly outperform the baseline approaches.Both the corpus-based measure (MI) and theknowledge-based measure (WN) perform well onthe task of keyword extraction.A running example is given below to demon-strate the results:Document ID: D062/AP891018-0301Labeled keywords:insurance earthquake insurer damage california FranciscopayExtracted keywords:WN: insurance earthquake insurer quake californiaspokesman cost million wednesday damageMI(k=5): insurance insurer earthquake percent benefitcalifornia property damage estimate rate5585 Conclusion and Future WorkIn this paper we propose a novel approach to si-multaneously document summarization and key-word extraction for single documents by fusing thesentence-to-sentence, word-to-word, sentence-to-word relationships in a unified framework.
Thesemantics between words computed by either cor-pus-based approach or knowledge-based approachcan be incorporated into the framework in a naturalway.
Evaluation results demonstrate the perform-ance improvement of the proposed approach overthe baselines for both tasks.In this study, only the mutual information meas-ure and the vector measure are employed to com-pute word semantics, and in future work manyother measures mentioned earlier will be investi-gated in the framework in order to show the ro-bustness of the framework.
The evaluation of key-word extraction is preliminary in this study, andwe will conduct more thorough experiments tomake the results more convincing.
Furthermore,the proposed approach will be applied to multi-document summarization and keyword extraction,which are considered more difficult than singledocument summarization and keyword extraction.AcknowledgementsThis work was supported by the National ScienceFoundation of China (60642001).ReferencesM.
R. Amini and P. Gallinari.
2002.
The use of unlabeled data toimprove supervised learning for text summarization.
In Pro-ceedings of SIGIR2002, 105-112.S.
Brin and L. Page.
1998.
The anatomy of a large-scale hypertex-tual Web search engine.
Computer Networks and ISDN Sys-tems, 30(1?7).J.
Carbonell and J. Goldstein.
1998.
The use of MMR, diversity-based reranking for reordering documents and producingsummaries.
In Proceedings of SIGIR-1998, 335-336.J.
M. Conroy and D. P. O?Leary.
2001.
Text summarization viaHidden Markov Models.
In Proceedings of SIGIR2001, 406-407.DUC.
2002.
The Document Understanding Workshop 2002.http://www-nlpir.nist.gov/projects/duc/guidelines/2002.htmlT.
Dunning.
1993.
Accurate methods for the statistics of surpriseand coincidence.
Computational Linguistics 19, 61?74.G.
ErKan and D. R. Radev.
2004.
LexPageRank: Prestige inmulti-document text summarization.
In Proceedings ofEMNLP2004.C.
Fellbaum.
1998.
WordNet: An Electronic Lexical Database.The MIT Press.E.
Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G.Nevill-Manning.
1999.
Domain-specific keyphrase extraction.Proceedings of IJCAI-99, pp.
668-673.Y.
H. Gong and X. Liu.
2001.
Generic text summarization usingRelevance Measure and Latent Semantic Analysis.
In Proceed-ings of SIGIR2001, 19-25.E.
Hovy and C. Y. Lin.
1997.
Automated text summarization inSUMMARIST.
In Proceeding of ACL?1997/EACL?1997 Wor-shop on Intelligent Scalable Text Summarization.A.
Hulth.
2003.
Improved automatic keyword extraction givenmore linguistic knowledge.
In Proceedings of EMNLP2003,Japan, August.J.
M. Kleinberg.
1999.
Authoritative sources in a hyperlinkedenvironment.
Journal of the ACM, 46(5):604?632.B.
Krulwich and C. Burkey.
1996.
Learning user informationinterests through the extraction of semantically significantphrases.
In AAAI 1996 Spring Symposium on Machine Learn-ing in Information Access.J.
Kupiec, J. Pedersen, and F. Chen.
1995.
A.trainable documentsummarizer.
In Proceedings of SIGIR1995, 68-73.T.
K. Landauer, P. Foltz, and D. Laham.
1998.
Introduction tolatent semantic analysis.
Discourse Processes 25.C.
Y. Lin and  E. Hovy.
2000.
The automated acquisition of topicsignatures for text Summarization.
In Proceedings of ACL-2000, 495-501.C.Y.
Lin and E.H. Hovy.
2003.
Automatic evaluation of summa-ries using n-gram co-occurrence statistics.
In Proceedings ofHLT-NAACL2003, Edmonton, Canada, May.R.
Mihalcea, C. Corley, and C. Strapparava.
2006.
Corpus-basedand knowledge-based measures of text semantic similarity.
InProceedings of AAAI-06.R.
Mihalcea and P. Tarau.
2004.
TextRank: Bringing order intotexts.
In Proceedings of EMNLP2004.R.
Mihalcea and P.Tarau.
2005.
A language independent algo-rithm for single and multiple document summarization.
InProceedings of IJCNLP2005.A.
Mu?oz.
1996.
Compound key word generation from documentdatabases using a hierarchical clustering ART model.
Intelli-gent Data Analysis, 1(1).T.
Nomoto and Y. Matsumoto.
2001.
A new approach to unsuper-vised text summarization.
In Proceedings of SIGIR2001, 26-34.S.
Patwardhan.
2003.
Incorporating dictionary and corpus infor-mation into a context vector measure of semantic relatedness.Master?s thesis, Univ.
of Minnesota, Duluth.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.
Word-Net::Similarity ?
Measuring the relatedness of concepts.
InProceedings of AAAI-04.D.
Shen, J.-T. Sun, H. Li, Q. Yang, and Z. Chen.
2007.
DocumentSummarization using Conditional Random Fields.
In Proceed-ings of IJCAI 07.A.
M. Steier and R. K. Belew.
1993.
Exporting phrases: A statisti-cal analysis of topical language.
In Proceedings of SecondSymposium on Document Analysis and Information Retrieval,pp.
179-190.P.
D. Turney.
2000.
Learning algorithms for keyphrase extraction.Information Retrieval, 2:303-336.P.
Turney.
2001.
Mining the web for synonyms: PMI-IR versusLSA on TOEFL.
In Proceedings of ECML-2001.I.
H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G.Nevill-Manning.
1999.
KEA: Practical automatic keyphraseextraction.
Proceedings of Digital Libraries 99 (DL'99), pp.254-256.H.
Y. Zha.
2002.
Generic summarization and keyphrase extractionusing mutual reinforcement principle and sentence clustering.In Proceedings of SIGIR2002, pp.
113-120.559
