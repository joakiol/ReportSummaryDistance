Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 141?144,Prague, June 2007. c?2007 Association for Computational LinguisticsFBK-IRST: Kernel Methods for Semantic Relation ExtractionClaudio Giuliano and Alberto Lavelli and Daniele Pighin and Lorenza RomanoFBK-IRST, Istituto per la Ricerca Scientifica e TecnologicaI-38050, Povo (TN), ITALY{giuliano,lavelli,pighin,romano}@itc.itAbstractWe present an approach for semantic rela-tion extraction between nominals that com-bines shallow and deep syntactic processingand semantic information using kernel meth-ods.
Two information sources are consid-ered: (i) the whole sentence where the re-lation appears, and (ii) WordNet synsets andhypernymy relations of the candidate nom-inals.
Each source of information is rep-resented by kernel functions.
In particu-lar, five basic kernel functions are linearlycombined and weighted under different con-ditions.
The experiments were carried outusing support vector machines as classifier.The system achieves an overall F1 of 71.8%on the Classification of Semantic Relationsbetween Nominals task at SemEval-2007.1 IntroductionThe starting point of our research is an approachfor identifying relations between named entities ex-ploiting only shallow linguistic information, such astokenization, sentence splitting, part-of-speech tag-ging and lemmatization (Giuliano et al, 2006).
Acombination of kernel functions is used to representtwo distinct information sources: (i) the global con-text where entities appear and (ii) their local con-texts.
The whole sentence where the entities appear(global context) is used to discover the presence ofa relation between two entities.
Windows of limitedsize around the entities (local contexts) provide use-ful clues to identify the roles played by the entitieswithin a relation (e.g., agent and target of a gene in-teraction).
In the task of detecting protein-proteininteractions, we obtained state-of-the-art results ontwo biomedical data sets.
In addition, promising re-sults have been recently obtained for relations suchas work for and org based in in the news domain1.In this paper, we investigate the use of the aboveapproach to discover semantic relations betweennominals.
In addition to the original feature rep-resentation, we have integrated deep syntactic pro-cessing of the global context and semantic informa-tion for each candidate nominals using WordNet asexternal knowledge source.
Each source of informa-tion is represented by kernel functions.
A tree kernel(Moschitti, 2004) is used to exploit the deep syn-tactic processing obtained using the Charniak parser(Charniak, 2000).
On the other hand, bag of syn-onyms and hypernyms is used to enhance the repre-sentation of the candidate nominals.
The final sys-tem is based on five basic kernel functions (bag-of-words kernel, global context kernel, tree kernel, su-persense kernel, bag of synonyms and hypernymskernel) linearly combined and weighted under dif-ferent conditions.
The experiments were carried outusing support vector machines (Vapnik, 1998) asclassifier.We present results on the Classification of Seman-tic Relations between Nominals task at SemEval-2007, in which sentences containing ordered pairsof marked nominals, possibly semantically related,have to be classified.
On this task, we achieve anoverall F1 of 71.8% (B category evaluation), largelyoutperforming all the baselines.1These results appear in a paper currently under revision.1412 Kernel Methods for Relation ExtractionIn order to implement the approach based on syntac-tic and semantic information, we employed a linearweighted combination of kernels, using support vec-tor machines as classifier.
We designed two familiesof basic kernels: syntactic kernels and semantic ker-nels.
These basic kernels are combined by exploit-ing the closure properties of kernels.
We define ourcomposite kernel KC(x1, x2) as followsn?i=1wiKi(x1, x2)?Ki(x1, x1)Ki(x2, x2), (1)where each basic kernel Ki is normalized and wi ?
{0, 1} is the kernel weight.
The normalization factorplays an important role in allowing us to integrate in-formation from heterogeneous knowledge sources.All basic kernels, but the tree kernel (see Section2.1.3), are explicitly calculated as followsKi(x1, x2) = ??
(x1), ?
(x2)?, (2)where ?(?)
is the embedding vector.
Even thoughthe resulting feature space has high dimensionality,an efficient computation of Equation 2 can be carriedout explicitly since the input representations definedbelow are extremely sparse.2.1 Syntactic KernelsSyntactic kernels are defined over the whole sen-tence where the candidate nominals appear.2.1.1 Global Context KernelBunescu and Mooney (2005) and Giuliano et al(2006) successfully exploited the fact that relationsbetween named entities are generally expressed us-ing only words that appear simultaneously in one ofthe following three contexts.Fore-Between Tokens before and between the twoentities, e.g.
?the head of [ORG], Dr. [PER]?.Between Only tokens between the two entities, e.g.?
[ORG] spokesman [PER]?.Between-After Tokens between and after the twoentities, e.g.
?
[PER], a [ORG] professor?.Here, we investigate whether this assumption isalso correct for semantic relations between nomi-nals.
Our global context kernel operates on the con-texts defined above, where each context is repre-sented using a bag-of-words.
More formally, givena) S1SNPPRPIVPVBDfoundNPDTsomeNNcandyPPINinNPPRP$myNNunderwear..b) SVPVBDfoundNPNNSagentPPINinNPNNtargetFigure 1: A content-container relation test sentenceparse tree (a) and the corresponding RT structure (b).a relation example R, we represent a context C as arow vector?C(R) = (tf(t1, C), tf(t2, C), .
.
.
, tf(tl, C)) ?
Rl, (3)where the function tf(ti, C) records how manytimes a particular token ti is used in C .
Note thatthis approach differs from the standard bag-of-wordsas punctuation and stop words are included in ?C ,while the nominals are not.
To improve the classi-fication performance, we have further extended ?Cto embed n-grams of (contiguous) tokens (up to n =3).
By substituting ?C into Equation 2, we obtainthe n-gram kernel Kn, which counts uni-grams, bi-grams, .
.
.
, n-grams that two patterns have in com-mon2.
The Global Context kernel KGC(R1, R2) isthen defined asKF B(R1, R2) +KB(R1, R2) +KBA(R1, R2), (4)where KFB , KB and KBA are n-gram kernelsthat operate on the Fore-Between, Between andBetween-After patterns respectively.2.1.2 Bag-of-Words KernelThe bag-of-words kernel is defined as the previ-ous kernel but it operates on the whole sentence.2.1.3 Tree KernelTree kernels can trigger automatic feature selec-tion and represent a viable alternative to the man-2In the literature, it is also called n-spectrum kernel.142ual design of attribute-value syntactic features (Mos-chitti, 2004).
A tree kernel KT (t1, t2) evaluatesthe similarity between two trees t1 and t2 in termsof the number of fragments they have in common.Let Nt be the set of nodes of a tree t and F ={f1, f2, .
.
.
, f|F|} be the fragment space of t1 andt2.
ThenKT (t1, t2) =Pni?Nt1Pnj?Nt2?
(ni, nj) , (5)where ?
(ni, nj) =?|F|k=1 Ik(ni) ?
IK(nj) andIk(n) = 1 if k is rooted in n, 0 otherwise.For this task, we defined an ad-hoc class of struc-tured features (Moschitti et al, 2006), the ReducedTree (RT), which can be derived from a sentenceparse tree t by the following steps: (1) remove all theterminal nodes but those labeled as relation entitiesand those POS tagged as verbs, auxiliaries, prepo-sitions, modals or adverbs; (2) remove all the in-ternal nodes not covering any remaining terminal;(3) replace the entity words with placeholders thatindicate the direction in which the relation shouldhold.
Figure 1 shows a parse tree and the resultingRT structure.2.2 Semantic KernelsIn (Giuliano et al, 2006), we used the local contextkernel to infer semantic information on the candi-date entities (i.e., roles played by the entities).
Asthe task organizers provide the WordNet sense androle for each nominal, we directly use this informa-tion to enrich the feature space and do not includethe local context kernel in the combination.2.2.1 Bag of Synonyms and Hypernyms KernelBy using the WordNet sense key provided, eachnominal is represented by the bag of its synonymsand hypernyms (direct and inherited hypernyms).Formally, given a relation example R, each nominalN is represented as a row vector?N(R) = (f(t1, N), f(t2, N), .
.
.
, f(tl, N)) ?
Rl, (6)where the binary function f(ti, N) records if a par-ticular lemma ti is contained into the bag of syn-onyms and hypernyms of N. The bag of synonymsand hypernyms kernel KS&H(R1, R2) is defined asKtarget(R1, R2) +Kagent(R1, R2), (7)where Ktarget and Kagent are defined by substitut-ing the embedding of the target and agent nominalsinto Equation 2 respectively.2.2.2 Supersense KernelWordNet synsets are organized into 45 lexicogra-pher files, based on syntactic category and logicalgroupings.
E.g., noun.artifact is for nouns denotingman-made objects, noun.attribute for nouns denot-ing attributes for people and objects etc.
The super-sense kernel KSS(R1, R2) is a variant of the previ-ous kernel that uses the names of the lexicographerfiles (i.e., the supersense) to index the feature space.3 Experimental Setup and ResultsSentences have been tokenized, lemmatized, andPOS tagged with TextPro3.
We considered each re-lation as a different binary classification task, andeach sentence in the data set is a positive or negativeexample for the relation.
The direction of the rela-tion is considered labelling the first argument of therelation as agent and the second as target.All the experiments were performed using theSVM package SVMLight-TK4, customized to em-bed our own kernels.
We optimized the linear com-bination weights wi and regularization parameter cusing 10-fold cross-validation on the training set.We set the cost-factor j to be the ratio between thenumber of negative and positive examples.Table 1 shows the performance on the test set.
Weachieve an overall F1 of 71.8% (B category evalua-tion), largely outperforming all the baselines, rang-ing from 48.5% to 57.0%.
The average training plustest running time for a relation is about 10 secondson a Intel Pentium M755 2.0 GHz.
Figure 2 showsthe learning curves on the test set.
For all relationsbut theme-tool, accurate classifiers can be learnedusing a small fraction of training.4 Discussion and ConclusionExperimental results show that our kernel-based ap-proach is appropriate also to detect semantic rela-tions between nominals.
However, differently fromrelation extraction between named entities, there isnot a common kernel setup for all relations.
E.g.,3http://tcc.itc.it/projects/textpro/4http://ai-nlp.info.uniroma2.it/moschitti/143010203040506070809030  40  50  60  70  80  90  100F 1Percentage of TrainingLearning CurveCause-EffectInstrument-AgencyProduct-ProducerOrigin-EntityTheme-ToolPart-WholeContent-ContainerFigure 2: Learning curves on the test set.Relation P R F1 AccCause-Effect 67.3 90.2 77.1 72.5Instrument-Agency 76.9 78.9 77.9 78.2Product-Producer 76.2 77.4 76.8 68.8Origin-Entity 62.2 63.9 63.0 66.7Theme-Tool 69.2 62.1 65.5 73.2Part-Whole 65.5 73.1 69.1 76.4Content-Container 78.8 68.4 73.2 74.3Avg 70.9 73.4 71.8 72.9Table 1: Results on the test set.for content-container we obtain the best perfor-mance combining the tree kernel and the bag of syn-onyms and hypernyms kernel; on the other hand, forinstrument-agency the best performance is obtainedby combining the global kernel and the supersensekernel.
Surprisingly, the supersense kernel aloneworks quite well and obtains results comparable tothe bag of synonyms and hypernyms kernel.
Thisresult is particularly interesting as a supersense tag-ger can easily provide a satisfactory accuracy (Cia-ramita and Altun, 2006).
On the other hand, ob-taining an acceptable accuracy in word sense disam-biguation (required for a realistic application of thebag of synonyms and hypernyms kernel) is imprac-tical as a sufficient amount of training for at least allnouns is currently not available.
Hence, the super-sense could play a crucial role to improve the perfor-mance when approaching this task without the nomi-nals disambiguated.
To model the global context us-ing the Fore-Between, Between and Between-Aftercontexts did not produce a significant improvementwith respect to the bag-of-words model.
This ismainly due to the fact that examples have been col-lected from the Web using heuristic patterns/queries,most of which implying Between patterns/contexts(e.g., for the cause-effect relation ?
* comes from *?,?
* out of *?
etc.
).5 AcknowledgementsClaudio Giuliano, Alberto Lavelli and Lorenza Ro-mano are supported by the X-Media project (http://www.x-media-project.org), sponsoredby the European Commission as part of the Infor-mation Society Technologies (IST) programme un-der EC grant number IST-FP6-026978.ReferencesRazvan Bunescu and Raymond J. Mooney.
2005.
Subse-quence kernels for relation extraction.
In Proceedingsof the 19th Conference on Neural Information Pro-cessing Systems, Vancouver, British Columbia.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the First Meeting of theNorth American Chapter of the Association for Com-putational Linguistics, pages 132?139, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informationextraction with a supersense sequence tagger.
In Pro-ceedings of the 2006 Conference on Empirical Meth-ods in Natural Language Processing, pages 594?602,Sydney, Australia, July.Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.2006.
Exploiting shallow linguistic information for re-lation extraction from biomedical literature.
In Pro-ceedings of the Eleventh Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-2006), Trento, Italy, 5-7 April.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2006.
Semantic role labeling via tree kerneljoint inference.
In Proceedings of the Tenth Confer-ence on Computational Natural Language Learning,CoNLL-X.Alessandro Moschitti.
2004.
A study on convolutionkernels for shallow statistic parsing.
In Proceedingsof the 42nd Meeting of the Association for Computa-tional Linguistics (ACL?04), Main Volume, pages 335?342, Barcelona, Spain, July.Vladimir Vapnik.
1998.
Statistical Learning Theory.John Wiley and Sons, New York, NY.144
