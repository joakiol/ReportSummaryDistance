Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2230?2235,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsWho did What: A Large-Scale Person-Centered Cloze DatasetTakeshi Onishi Hai Wang Mohit Bansal Kevin Gimpel David McAllesterToyota Technological Institute at Chicago, Chicago, IL, 60637, USA{tonishi,haiwang,mbansal,kgimpel,mcallester}@ttic.eduAbstractWe have constructed a new ?Who-did-What?dataset of over 200,000 fill-in-the-gap (cloze)multiple choice reading comprehension prob-lems constructed from the LDC English Gi-gaword newswire corpus.
The WDW datasethas a variety of novel features.
First, in con-trast with the CNN and Daily Mail datasets(Hermann et al, 2015) we avoid using articlesummaries for question formation.
Instead,each problem is formed from two indepen-dent articles ?
an article given as the pas-sage to be read and a separate article on thesame events used to form the question.
Sec-ond, we avoid anonymization ?
each choiceis a person named entity.
Third, the problemshave been filtered to remove a fraction that areeasily solved by simple baselines, while re-maining 84% solvable by humans.
We reportperformance benchmarks of standard systemsand propose the WDW dataset as a challengetask for the community.11 IntroductionResearchers distinguish the problem of generalknowledge question answering from that of read-ing comprehension (Hermann et al, 2015; Hill etal., 2016).
Reading comprehension is more diffi-cult than knowledge-based or IR-based question an-swering in two ways.
First, reading comprehen-sion systems must infer answers from a given un-structured passage rather than structured knowledgesources such as Freebase (Bollacker et al, 2008)1Available at tticnlp.github.io/who did whator the Google Knowledge Graph (Singhal, 2012).Second, machine comprehension systems cannot ex-ploit the large level of redundancy present on theweb to find statements that provide a strong syntac-tic match to the question (Yang et al, 2015).
In con-trast, a machine comprehension system must use thesingle phrasing in the given passage, which may bea poor syntactic match to the question.In this paper, we describe the construction of anew reading comprehension dataset that we referto as ?Who-did-What?.
Two typical examples areshown in Table 1.2 The process of forming a prob-lem starts with the selection of a question articlefrom the English Gigaword corpus.
The question isformed by deleting a person named entity from thefirst sentence of the question article.
An informationretrieval system is then used to select a passage withhigh overlap with the first sentence of the questionarticle, and an answer choice list is generated fromthe person named entities in the passage.Our dataset differs from the CNN and Daily Mailcomprehension tasks (Hermann et al, 2015) in thatit forms questions from two distinct articles ratherthan summary points.
This allows problems to bederived from document collections that do not con-tain manually-written summaries.
This also reducesthe syntactic similarity between the question andthe relevant sentences in the passage, increasing theneed for deeper semantic analysis.To make the dataset more challenging we selec-tively remove problems so as to suppress four simple2The passages here only show certain salient portions of thepassage.
In the actual dataset, the entire article is given.
Thecorrect answers are (3) and (2).2230Passage: Britain?s decision on Thursday to drop extradition proceedings against Gen. Augusto Pinochet and allow himto return to Chile is understandably frustrating ... Jack Straw, the home secretary, said the 84-year-old former dictator?sability to understand the charges against him and to direct his defense had been seriously impaired by a series of strokes.... Chile?s president-elect, Ricardo Lagos, has wisely pledged to let justice run its course.
But the outgoing government ofPresident Eduardo Frei is pushing a constitutional reform that would allow Pinochet to step down from the Senate and retainparliamentary immunity from prosecution.
...Question: Sources close to the presidential palace said that Fujimori declined at the last moment to leave the country andinstead he will send a high level delegation to the ceremony, at which Chilean President Eduardo Frei will pass the mandateto XXX.Choices: (1) Augusto Pinochet (2) Jack Straw (3) Ricardo LagosPassage: Tottenham won 2-0 at Hapoel Tel Aviv in UEFA Cup action on Thursday night in a defensive display whichimpressed Spurs skipper Robbie Keane.
... Keane scored the first goal at the Bloomfield Stadium with Dimitar Berbatov,who insisted earlier on Thursday he was happy at the London club, heading a second.
The 26-year-old Berbatov admitted thereports linking him with a move had affected his performances ... Spurs manager Juande Ramos has won the UEFA Cup inthe last two seasons ...Question: Tottenham manager Juande Ramos has hinted he will allow XXX to leave if the Bulgaria striker makes it clear heis unhappy.Choices: (1) Robbie Keane (2) Dimitar BerbatovTable 1: Sample reading comprehension problems from our dataset.baselines ?
selecting the most mentioned person,the first mentioned person, and two language modelbaselines.
This is also intended to produce problemsrequiring deeper semantic analysis.The resulting dataset yields a larger gap betweenhuman and machine performance than existing ones.Humans can answer questions in our dataset withan 84% success rate compared to the estimates of75% for CNN (Chen et al, 2016) and 82% for theCBT named entities task (Hill et al, 2016).
In spiteof this higher level of human performance, variousexisting readers perform significantly worse on ourdataset than they do on the CNN dataset.
For ex-ample, the Attentive Reader (Hermann et al, 2015)achieves 63% on CNN but only 55% on Who-did-What and the Attention Sum Reader (Kadlec et al,2016) achieves 70% on CNN but only 59% on Who-did-What.In summary, we believe that our Who-did-Whatdataset is more challenging, and requires deeper se-mantic analysis, than existing datasets.2 Related WorkOur Who-did-What dataset is related to several re-cently developed datasets for machine comprehen-sion.
The MCTest dataset (Richardson et al, 2013)consists of 660 fictional stories with 4 multiplechoice questions each.
This dataset is too smallto train systems for the general problem of readingcomprehension.The bAbI synthetic question answering dataset(Weston et al, 2016) contains passages describing aseries of actions in a simulation followed by a ques-tion.
For this synthetic data a logical algorithm canbe written to solve the problems exactly (and, in fact,is used to generate ground truth answers).The Children?s Book Test (CBT) dataset, createdby Hill et al (2016), contains 113,719 cloze-stylenamed entity problems.
Each problem consists of 20consecutive sentences from a children?s story, a 21stsentence in which a word has been deleted, and a listof ten choices for the deleted word.
The CBT datasettests story completion rather than reading compre-hension.
The next event in a story is often not de-termined ?
surprises arise.
This may explain whyhuman performance is lower for CBT than for ourdataset ?
82% for CBT vs. 84% for Who-did-What.The 16% error rate for humans on Who-did-Whatseems to be largely due to noise in problem forma-tion introduced by errors in named entity recogni-tion and parsing.
Reducing this noise in future ver-sions of the dataset should significantly improve hu-man performance.
Another difference compared toCBT is that Who-did-What has shorter choice listson average.
Random guessing achieves only 10%on CBT but 32% on Who-did-What.
The reduction2231in the number of choices seems likely to be responsi-ble for the higher performance of an LSTM systemon Who-did-What ?
contextual LSTMs (the atten-tive reader of Hermann et al, 2015) improve from44% on CBT (as reported by Hill et al, 2016) to55% on Who-did-What.Above we referenced the comprehension datasetscreated from CNN and Daily Mail articles by Her-mann et al (2015).
The CNN and Daily Maildatasets together consist of 1.4 million questionsconstructed from approximately 300,000 articles.Of existing datasets, these are the most similar toWho-did-What in that they consist of cloze-stylequestion answering problems derived from news ar-ticles.
As discussed in Section 1, our Who-did-Whatdataset differs from these datasets in not being de-rived from article summaries, in using baseline sup-pression, and in yielding a larger gap between ma-chine and human performance.
The Who-did-Whatdataset alo differs in that the person named entitiesare not anonymized, permitting the use of externalresources to improve performance while remainingdifficult for language models due to suppression.3 Dataset ConstructionWe now describe the construction of our Who-did-What dataset in more detail.
To generate a problemwe first generate the question by selecting a randomarticle ?
the ?question article?
?
from the Giga-word corpus and taking the first sentence of that ar-ticle ?
the ?question sentence?
?
as the source ofthe cloze question.
The hope is that the first sentenceof an article contains prominent people and eventswhich are likely to be discussed in other independentarticles.
To convert the question sentence to a clozequestion, we first extract named entities using theStanford NER system (Finkel et al, 2005) and parsethe sentence using the Stanford PCFG parser (Kleinand Manning, 2003).The person named entities are candidates for dele-tion to create a cloze problem.
For each per-son named entity we then identify a noun phrasein the automatic parse that is headed by that per-son.
For example, if the question sentence is ?Pres-ident Obama met yesterday with Apple FounderSteve Jobs?
we identify the two person nounphrases ?President Obama?
and ?Apple FounderSteve Jobs?.
When a person named entity is selectedfor deletion, the entire noun phrase is deleted.
Forexample, when deleting the second named entity,we get ?President Obama met yesterday with XXX?rather than ?President Obama met yesterday withApple founder XXX?.
This increases the difficultyof the problems because systems cannot rely on de-scriptors and other local contextual cues.
About700,000 question sentences are generated from Gi-gaword articles (8% of the total number of articles).Once a cloze question has been formed we se-lect an appropriate article as a passage.
The ar-ticle should be independent of the question arti-cle but should discuss the people and events men-tioned in the question sentence.
To find a passagewe search the Gigaword dataset using the ApacheLucene information retrieval system (McCandless etal., 2010), using the question sentence as the query.The named entity to be deleted is included in thequery and required to be included in the returnedarticle.
We also restrict the search to articles pub-lished within two weeks of the date of the questionarticle.
Articles containing sentences too similar tothe question in word overlap and phrase matchingnear the blanked phrase are removed.
We select thebest matching article satisfying our constraints.
Ifno such article can be found, we abort the processand move on to a new question.Given a question and a passage we next form thelist of choices.
We collect all person named enti-ties in the passage except unblanked person namedentities in the question.
Choices that are subsetsof longer choices are eliminated.
For example thechoice ?Obama?
would be eliminated if the list alsocontains ?Barack Obama?.
We also discard ambigu-ous cases where a part of a blanked NE appears inmultiple candidate answers, e.g., if a passage has?Bill Clinton?
and ?Hillary Clinton?
and the blankedphrase is ?Clinton?.
We found this simple corefer-ence rule to work well in practice since news arti-cles usually employ full names for initial mentionsof persons.
If the resulting choice list contains fewerthan two or more than five choices, the process isaborted and we move on to a new question.3After forming an initial set of problems we then3The maximum of five helps to avoid sports articles contain-ing structured lists of results.2232remove ?duplicated?
problems.
Duplication arisesbecause Gigaword contains many copies of the samearticle or articles where one is clearly an edited ver-sion of another.
Our duplication-removal processensures that no two problems have very similar ques-tions.
Here, similarity is defined as the ratio of thesize of the bag of words intersection to the size ofthe smaller bag.In order to focus our dataset on the most interest-ing problems, we remove some problems to suppressthe performance of the following simple baselines:?
First person in passage: Select the person that ap-pears first in the passage.?
Most frequent person: Select the most frequentperson in the passage.?
n-gram: Select the most likely answer to fill theblank under a 5-gram language model trained onGigaword minus articles which are too similar toone of the questions in word overlap and phrasematching.?
Unigram: Select the most frequent last name us-ing the unigram counts from the 5-gram model.To minimize the number of questions removed wesolve an optimization problem defined by limitingthe performance of each baseline to a specified targetvalue while removing as few problems as possible,i.e.,max?(C)?C?{0,1}|b|?
(C)|T (C)| (1)subject to?i?C:Ci=1?
(C)|T (C)|N ?
kN =?C?{0,1}|b|?
(C)|T (C)| (2)where T (C) is the subset of the questions solved bythe subset C of the suppressed baselines, ?
(C) is akeeping rate for question set T (C), Ci = 1 indicatesthe i-th baseline is in the subset, |b| is the number ofbaselines, N is a total number of questions, and kis the upper bound for the baselines after suppres-sion.
We choose k to yield random performance forthe baselines.
The performance of the baselines be-fore and after suppression is shown in Table 2.
Thesuppression removed 49.9% of the questions.AccuracyBaseline Before AfterFirst person in passage 0.60 0.32Most frequent person 0.61 0.33n-gram 0.53 0.33Unigram 0.43 0.32Random?
0.32 0.32Table 2: Performance of suppressed baselines.
?Random per-formance is computed as a deterministic function of the numberof times each choice set size appears.
Many questions have onlytwo choices and there are about three choices on average.relaxed train valid testtrain# questions 185,978 127,786 10,000 10,000avg.
# choices 3.5 3.5 3.4 3.4avg.
# tokens 378 365 325 326vocab.
size 347,406 308,602Table 3: Dataset statistics.Table 3 shows statistics of our dataset after sup-pression.
We split the final dataset into train, vali-dation, and test by taking the validation and test tobe a random split of the most recent 20,000 prob-lems as measured by question article date.
In thisway there is very little overlap in semantic subjectmatter between the training set and either validationor test.
We also provide a larger ?relaxed?
trainingset formed by applying less baseline suppression (alarger value of k in the optimization).
The relaxedtraining set then has a slightly different distributionfrom the train, validation, and test sets which are allfully suppressed.4 Performance BenchmarksWe report the performance of several systems tocharacterize our dataset:?
Word overlap: Select the choice c inserted tothe question q which is the most similar to anysentence s in the passage, i.e., CosSim(bag(c +q),bag(s)).?
Sliding window and Distance baselines (and theircombination) from Richardson et al (2013).?
Semantic features: NLP feature based systemfrom Wang et al (2015).2233?
Attentive Reader: LSTM with attention mecha-nism (Hermann et al, 2015).?
Stanford Reader: An attentive reader modifiedwith a bilinear term (Chen et al, 2016).?
Attention Sum (AS) Reader: GRU with a point-attention mechanism (Kadlec et al, 2016).?
Gated-Attention (GA) Reader: Attention SumReader with gated layers (Dhingra et al, 2016).Table 4 shows the performance of each system onthe test data.
For the Attention and Stanford Read-ers, we anonymized the Who-did-What data by re-placing named entities with entity IDs as in the CNNand Daily Mail datasets.We see consistent reductions in accuracy whenmoving from CNN to our dataset.
The Attentiveand Stanford Reader drop by up to 10% and the ASand GA readers drop by up to 17%.
The ranking ofthe systems also changes.
In contrast to the Atten-tive/Stanford readers, the AS/GA readers explicitlyleverage the frequency of the answer in the passage,a heuristic which appears beneficial for the CNNand Daily Mail tasks.
Our suppression of the most-frequent-person baseline appears to more stronglyaffect the performance of these latter systems.5 ConclusionWe presented a large-scale person-centered clozedataset whose scalability and flexibility is suitablefor neural methods.
This dataset is different in a va-riety of ways from existing large-scale cloze datasetsand provides a significant extension to the trainingand test data for machine comprehension.AcknowledgmentsWe thank NVIDIA Corporation for donating GPUsused in this research.References[Bollacker et al2008] Kurt Bollacker, Colin Evans,Praveen Paritosh, Tim Sturge, and Jamie Taylor.2008.
Freebase: a collaboratively created graphdatabase for structuring human knowledge.
In Pro-ceedings of the 2008 ACM SIGMOD internationalconference on Management of data, pages 1247?1250.
[Chen et al2016] Danqi Chen, Jason Bolton, and Christo-pher D. Manning.
2016.
A thorough examination ofSystem WDW CNNWord overlap 0.47 ?Sliding window 0.48 ?Distance 0.46 ?Sliding window + Distance 0.51 ?Semantic features 0.52 ?Attentive Reader 0.53 0.63IAttentive Reader (relaxed train) 0.55Stanford Reader 0.64 0.73IIStanford Reader (relaxed train) 0.65AS Reader 0.57 0.70IIIAS Reader (relaxed train) 0.59GA Reader 0.57 0.74IVGA Reader (relaxed train) 0.60Human Performance 84/100 0.75+IITable 4: System performance on test set.
Human performancewas computed by two annotators on a sample of 100 questions.Result marked I is from (Hermann et al, 2015), results markedII are from (Chen et al, 2016), result marked III is from(Kadlec et al, 2016), and result marked IV is from (Dhingraet al, 2016).the CNN/Daily Mail reading comprehension task.
InProceedings of the 54th Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: LongPapers), pages 2358?2367.
[Dhingra et al2016] Bhuwan Dhingra, Hanxiao Liu,William W. Cohen, and Ruslan Salakhutdinov.2016.
Gated-attention readers for text comprehension.CoRR, abs/1606.01549.
[Finkel et al2005] Jenny Rose Finkel, Trond Grenager,and Christopher Manning.
2005.
Incorporating non-local information into information extraction systemsby Gibbs sampling.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 363?370.
[Hermann et al2015] Karl Moritz Hermann, Toma?s Ko-cisky?, Edward Grefenstette, Lasse Espeholt, Will Kay,Mustafa Suleyman, and Phil Blunsom.
2015.
Teach-ing machines to read and comprehend.
In Advances inNeural Information Processing Systems, pages 1684?1692.
[Hill et al2016] Felix Hill, Antoine Bordes, SumitChopra, and Jason Weston.
2016.
The Goldilocksprinciple: Reading children?s books with explicitmemory representations.
In Proceedings of Interna-tional Conference on Learning Representations.
[Kadlec et al2016] Rudolf Kadlec, Martin Schmid,Ondr?ej Bajgar, and Jan Kleindienst.
2016.
Text2234understanding with the attention sum reader network.In Proceedings of the 54th Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 908?918.
[Klein and Manning2003] Dan Klein and Christopher D.Manning.
2003.
Accurate unlexicalized parsing.
InProceedings of the 41st Annual Meeting on Associa-tion for Computational Linguistics - Volume 1, pages423?430.
[McCandless et al2010] Michael McCandless, ErikHatcher, and Otis Gospodnetic.
2010.
Lucene inAction, Second Edition.
Manning Publications Co.[Richardson et al2013] Matthew Richardson, Christo-pher J.C. Burges, and Erin Renshaw.
2013.
MCTest:A challenge dataset for the open-domain machinecomprehension of text.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 193?203.
[Singhal2012] Amit Singhal.
2012.
Introducing theknowledge graph: things, not strings.
Official Googleblog.
[Wang et al2015] Hai Wang, Mohit Bansal, Kevin Gim-pel, and David McAllester.
2015.
Machine compre-hension with syntax, frames, and semantics.
In Pro-ceedings of the 53rd Annual Meeting of the Associa-tion for Computational Linguistics and the 7th Inter-national Joint Conference on Natural Language Pro-cessing (Volume 2: Short Papers), pages 700?706.
[Weston et al2016] Jason Weston, Antoine Bordes, SumitChopra, and Tomas Mikolov.
2016.
Towards AI-complete question answering: A set of prerequisite toytasks.
In Proceedings of International Conference onLearning Representations.
[Yang et al2015] Yi Yang, Wen-tau Yih, and ChristopherMeek.
2015.
WIKIQA: A challenge dataset for open-domain question answering.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 2013?2018.2235
