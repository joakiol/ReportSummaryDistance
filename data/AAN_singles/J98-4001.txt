A Collaborative Planning Model ofIntentional StructureKaren  E. Lochbaum*u S WEST Advanced TechnologiesAn agent's ability to understand an utterance depends upon its ability to relate that utterance tothe preceding discourse.
The agent must determine whether the utterance begins a new segmentof the discourse, completes the current segment, or contributes to it.
The intentional structureof the discourse, comprised of discourse segment purposes and their interrelationships, plays acentral role in this process (Grosz and Sidner 1986).
In this paper, we provide a computationalmodel for recognizing intentional structure and utilizing it in discourse processing.
The modelis based on the collaborative planning framework of SharedPlans (Grosz and Kraus 1996).1.
IntroductionPeople engage in dialogues for a reason.
Their intentions guide their behavior andtheir conversational partners' recognition of those intentions aids in the latter's un-derstanding of their utterances (Grice 1969; Sidner 1985; Grosz and Sidner 1986).
Inthis paper, we present acomputational model for recognizing the intentional structureof a discourse and utilizing it in discourse processing.The embedded subdialogues in Figures 1 through 3 illustrate a variety of inten-tions that a person or computer system must recognize to respond effectively to itsconversational partner.
These dialogues are drawn from the computational linguis-tics literature and will be used throughout the paper.
We have chosen to use thesedialogues, rather than constructing or collecting new ones, in order to elucidate thedifferences between our theory and previous ones.The dialogue in Figure 1 contains two subtask subdialogues; the dialogue inFigure 2 a correction subdialogue (Litman 1985; Litman and Alien 1987); and thedialogue in Figure 3 two knowledge precondition subdialogues.
The names of thesubdialogue types are suggestive of a conversational participant's reason for engag-ing in them.
Although these reasons are diverse, the dialogues all exhibit a commonstructural regularity; the recognition of this structure is crucial for discourse process-ing.Intuitive analyses of the sample dialogues erve to illustrate this point.
Beforepresenting these analyses, however, we first introduce some terminology that will beused throughout the paper.
A discourse is composed of discourse segments much asa sentence is composed of constituent phrases (Grosz and Sidner 1986).
The segmentalstructure of the sample dialogues is indicated by the bold rule grouping utterancesinto segments.
Whereas the term discourse segment applies to all types of discourse,the term subdialogue is reserved for segments that occur within dialogues.
All of theexamples in this paper are subdialogues.
For expository purposes, we will take theinitiator of a discourseto be female and the other participant to be male, thus affordingthe use of the pronouns he and he in analyzing the sample dialogues.
We will also* 400l Discovery Dr., Boulder, CO 80303.
E-mail: klochba@uswest.com(~) 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 4E: Replace the pump and belt please.A: OK, I found a belt in the back.Is that where it should be?...
\[A removes belt\]A: It's done.E: Now remove the pump.?
?
.E: First you have to remove the flywheel.E: Now take the pump off the base plate.A: Already did.iFigure 1Sample subtask subdialogues (Grosz 1974).
(1) User: Show me the genetic oncept called "employee".
(2) System: OK. <system displays network>ES User: I can't fit ic below it.
a newCan you move it up?ystem: Yes.
<system displays network>(6) User: OK, now make an individual employee conceptwhose first name is ...Figure 2A sample correction subdialogue (Sidner 1983; Litman 1985).use the terms agent and it in more abstract discussions and will apply them to bothpeople and computer systems, both individually as well as in groups?A subtask subdialogue, then, occurs within a task-oriented ialogue and is con-cerned with a subtask of the overall act underlying the dialogue.
1 An agent initiatesa subtask subdialogue to support successful execution of the subtask: communicatingabout the subtask enables the agent to perform it, as well as to coordinate its actionswith its partner's.
In the dialogue of Figure 1, the Apprentice (participant "A") initiatesthe first subdialogue for two reasons: (i) because he believes that removing the beltof the air compressor plays a role in replacing its pump and belt and (ii) because hewants to enlist the Expert's help in removing the belt.
Reason (ii) underlies the sub-dialogue itself, while reason (i) is reflected in the relationship of the subdialogue tothe preceding discourse?
The Expert must recognize both of these reasons to respondeffectively to the Apprentice.
For example, suppose that the Expert believes that theApprentice's belief in (i) is incorrect; that is, she believes that the proposed subtaskdoes not play a role in performing the overall task.
The Expert should then communi-cate that information to the Apprentice (Pollack 1986b).
If we were to design a system1 In some sense, all dialogues are task oriented.
The task simply varies from the physical (e.g., removinga pump) to the more intellectual (e.g., satisfying a knowledge precondition).526Lochbaum A Collaborative Planning Model(1) NM: It looks like we need to do some maintenance onnode39.
(2) NP: Right.How shall we proceed?
(4) NM: Well, first we need to divert he traffic to another node.
(5) NP: Okay.
(6) Then we can replace node39 with a higher capacity switch.
(7) NM: Right.
(8) NP: Okay good.FNM:  we the traffic to?
Which nodes could diver(10!
NP: \[puts up diagram\]I (11_) ode41 looks like it could temporarily handle the extra load.
( I~NM:  I agree.
(13) Why don't you go ahead and divert he traffic to node41and then we can do the replacement.
(14) NP: Okay.
(15y \[NP changes network traffic patterns\](16) That's done.Figure 3Sample knowledge precondition subdialogues.
(Adapted from Lochbaurn, Grosz, and Sidner\[1990\].
)to play the role of the Expert in this scenario and that system were not designedto recognize the relationship of an embedded subdialogue to the previous discourse,then it would not attribute reason (i) to the Apprentice.
It thus would not recognizethat the Apprentice mistakenly believes that the proposed subtask contributes to theoverall task.
As a result, the system would fail to recognize that it should inform theApprentice of his mistaken belief.Correction subdialogues provide an even more striking example of the importanceof recognizing discourse structure.
An agent initiates a correction subdialogue whenit requires help addressing some problem.
For example, the dialogue in Figure 2 isconcerned with modifying a KL-ONE network (Brachman and Schmolze 1985).
TheUser produces utterance (3) of the dialogue because she is unable to perform the nextact that she intends, namely adding a new concept o the displayed portion of thenetwork.
As with the subtask example, the System must recognize this intention torespond appropriately.
In particular, it must recognize that the User is not continuingto perform the subtasks involved in modifying the KL-ONE network, but rather isaddressing a problem that prevents her from continuing with them.
For the Systemto recognize the User's intention, it must recognize that the User has initiated a newsegment of the discourse and also recognize the relationship of that new segment othe preceding discourse.If the System does not recognize that the User has initiated a new discourse seg-ment with utterance (3), then it will not interpret he User's subsequent utterancesin the proper context.
For example, it will take the User's utterance in (4) to be arequest hat the System perform an act in support of further modifying the network,rather than in support of correcting the problem.
If the System does not believe thatthe act of raising up a displayed subnetwork is part of modifying a network, then itwill conclude that the User has mistaken beliefs about how to proceed with the mod-527Computational Linguistics Volume 24, Number 4ification.
In its response, the System may then choose to correct he User's perceivedmisconception, rather than to perform the act requested of it.Even if the System does recognize the initiation of a new discourse segment withutterance (3), i.e., it recognizes that the User is talking about something new, if it doesnot recognize the relationship of the new segment to the preceding discourse, then itmay also respond inappropriately.
For example, if the System does not recognize thatthe act the User cannot perform, i.e., "fitting a new ic below it," is part of modifyingthe network, then the System may respond without taking that larger context intoaccount.
In particular, the System might clear the screen to give the User more roomto create the new concept.
Such a response would be counterproductive to the User,however, who needs to see the employee concept o create a new instantiation of it.The last sample dialogue contains a third type of subdialogue, a knowledge pre-condition subdialogue.
Whereas an agent initiates a correction subdialogue when itis physically unable to perform an action, it initiates a knowledge precondition sub-dialogue when it is "mentally" unable to perform it, i.e., when it lacks the properknowledge.
The dialogue in Figure 3 contains two types of knowledge preconditionsubdialogues.
The first type is concerned with determining a means of performing anact, while the second type is concerned with identifying a parameter of an act.
As withthe other types of subdialogues discussed above, an agent may respond inappropri-ately if it does not recognize the relationship of these subdialogues to the precedingdiscourse.
For example, prior to the second subdialogue in Figure 3, agents NM (theNetwork Manager) and NP (the Network Presenter) have agreed to maintain node39of a local computer network, in part by diverting traffic from node39 to some othernode.
To perform the divert traffic act, however, the agents must identify that othernode.
Agent NM initiates the second subdialogue for this purpose.
If agent NP doesnot recognize the context in which the node is to be identified, i.e., for the purpose ofdiverting network traffic, then it may respond with a description that will not servethat purpose.
For example, it may respond with a description like "the node with thelightest raffic," rather than with a name like "node41.
"Thus, although the sample dialogues include a variety of subdialogue types, thetype of processing required to participate in the dialogues is the same.
In each case,an agent must recognize both the purpose of an embedded subdialogue and the re-lationship of that purpose to the purposes associated with the preceding discourse.These purposes and their interrelationships form the intentional structure of the dis-course (Grosz and Sidner 1986).
In this paper, we present a computational model forrecognizing intentional structure and utilizing it in discourse processing.
Our modelis based on the collaborative planning framework of SharedPlans (Grosz and Sidner1990; Lochbaum, Grosz, and Sidner 1990; Grosz and Kraus 1993, 1996).In the first part of this paper, we describe the SharedPlan model in general.
InSection 2, we summarize its concepts and definitions and then in Section 3 discussthe model of discourse processing that it imposes, Contrasting it with more traditionalapproaches to planning and plan recognition.
In the second part of the paper, we turnto Grosz and Sidner's theory of discourse structure.
We first describe their theory andthen show how SharedPlans may be used to model its intentional component.
Next,we demonstrate how the process for reasoning with SharedPlans presented in Sec-tion 3 may be mapped to the problem of recognizing and reasoning with intentionalstructure.
In this paper, we focus on the, theory of using SharedPlans to model inten-tional structure; however, the theory has also been implemented in a working system(Lochbaum 1994).
In the third part of the paper we evaluate our approach againstprevious ones.
We first return to Grosz and Sidner's theory and evaluate our modelagainst he constraints that it imposes.
We then compare our approach to discourse528Lochbaum A Collaborative Planning Modelprocessing with previous plan-based approaches (Litman 1985; Litman and Allen 1987;Lambert and Carberry 1991; Ramshaw 1991) and show that our approach is aimed atrecognizing and reasoning with a different ype of intention.
Whereas our approach isconcerned with discourse-level intentions, previous approaches have been concernedwith utterance-level intentions.2.
The  SharedP lan  Def in i t ionsGrosz and Sidner (1990) originally proposed SharedPlans as a more appropriate modelof plans for discourse than the single-agent plans based on AI planning formalismssuch as STRIPS (Fikes and Nilsson 1971).
SharedPlans differ from these other typesof plans in providing a model of collaborative, multiagent plans.
Collaborative plansbetter characterize the nature of discourse.
As Grosz and Sidner put it (1990, 418):Discourses are fundamentally examples of collaborative behavior.
Theparticipants in a discourse work together to satisfy various of theirindividual and joint needs.
Thus, to be sufficient o underlie discoursetheory, a theory of actions, plans, and plan recognition must deal ad-equately with collaboration.Models of single-agent plans are not sufficient for this purpose.
As Grosz and Sidnerand others (Searle 1990; Bratman 1992; Grosz and Kraus 1996) have shown, collabora-tion cannot be modeled by simply combining the plans of individual agents.SharedPlans are also distinguished from other planning formalisms in takingplans to be complex mental attitudes rather than abstract data structures.
As Pollacknoted (1990, 77):There are plans and there are plans.
There are the plans that an agent"knows": essentially recipes for performing particular actions or forachieving particular goal states.
And there are the plans that an agentadopts and that subsequently guide his action.Whereas data-structure approaches to planning and plan recognition are focused onthe first type of plan, mental phenomenon approaches are focused on the second.
2 Todistinguish these two types of "plans," we adopt Pollack's terminology and use theterm recipe for the first type.
Recipes are structures of actions; they represent whatagents know when they know a way of doing something.
We also follow Pollack inreserving the term plan for the collection of mental attitudes that an agent, or set ofagents, must hold to act successfully.
Thus, while recipes are comprised primarily ofactions, plans are comprised of beliefs and intentions that are directed at those actions.We elaborate on this point in Section 3.For an agent G to have an individual plan for an act o~, it must satisfy the require-ments given below (Grosz and Kraus 1996).
3We will refer to the act oL as the object iveof the agent's plan.1.
G has a recipe for o~2 The terms data-structure vi w of plans and mental phenomenon view of plans were coined byPollack (1986b).3 These requirements, and those to follow for collaborative plans, omit the case present in Grosz andKraus's (1996) work of one agent contracting an act to another.529Computational Linguistics Volume 24, Number 42.
For each constituent act fli of the recipe,G intends to perform fliG believes that it can perform fliG has an individual plan for fliSharedPlans are more complex than individual plans in several ways.
First, thegroup of agents involved in a SharedPlan must have mutual belief of a recipe foraction.
Second, they must designate a single agent or subgroup of agents to performeach subact in their recipe.
If a single agent is selected, that agent must form anindividual plan for the subact; if a subgroup is selected, they must form a SharedPlan.Third, the agents involved in a SharedPlan must have commitments oward their ownactions, as well as those of their partners.
The requirements for a group of agents GRto have a SharedPlan for o~ are as follows (Grosz and Kraus 1996):O. GR is committed to performing o~1.
GR has a recipe for o~2.
For each single-agent constituent act fli of the recipe, there is an agentG,ol c GR, such that(a) G~i intends to perform fliG~, believes that it can perform fliG~ has an individual plan for fli(b) The group GR mutually believe (2a)(c) The group GR is committed to G~/s success3.
For each multiagent constituent act fli of the recipe, there is a subgroupof agents GR~ C GR such that(a) GR,o~ mutually believe that they can perform fliGR~i has a SharedPlan for fli(b) The group GR mutually believe (3a)(c) The group GR is committed to GR~/s successTable 1 summarizes the operators used by Grosz and Kraus (1993, 1996) to for-malize the requirements of individual and shared plans.
Two of these operators, FIPand PIP, are used to model the plans of individual agents.
An agent has a FIP or fullindividual plan when it has established all of the requirements outlined above.
Whenthe agent has satisfied only a subset of them, it is said to have a partial individualplan or PIE 4 For multiagent plans, Grosz and Kraus provide two SharedPlan opera-tors: FSP and PSE A set of agents have a full SharedPlan (FSP) when all of the mentalattitudes outlined above have been established.
Until then, the agents' plan will onlybe partial (the PSP case).
In what follows, we will use the term SharedPlan when thedegree of completion of a collaborative plan is not at issue.
The definitions of FIP andFSP are given in Figures 4 and 5 respectively.
54 This description of a PIP is only a rough, though useful, approximation to Grosz and Kraus's (1993,1996) formal definition.5 These definitions are high-level schematics of Grosz and Kraus's (1993, 1996) formal definitions.
Theyserve to highlight hose aspects of individual and SharedPlans that are relevant to our work, but omitvarious formal details.530Lochbaum A Collaborative Planning ModelTable 1Operators used in Grosz and Kraus's (1993, 1996) definitions.Operator InterpretationSingle-AgentFIP An agent has a full individual plan for an actPIP An agent has a partial individual plan for an actInt.To An agent intends to perform an actInt.Th An agent intends that a proposition holdCBA An agent can bring about an actBCBA An agent believes that it can bring about an actBEL An agent believes a propositionMulti-agentFSPPSPCBAGMBCBAGMBA set of agents have a full SharedPlan for an actA set of agents have a partial SharedPlan for an actA set of agents can bring about an actA set of agents mutually believe that theycan bring about an actA set of agents mutually believe a propositionFIP(G,c~,Tp,T~,P~)An agent G has a full individual plan at time Tp to perform act c~ at time Tc~ using recipeR~1.
G has a recipe for c~R~ = {fll,pj} A BEL(G, tG E Recipes(a),Tp)2.
For each constituent act fli of the recipe,(a) G intends to perform fliInt.To( G, fli, Tp, T& )There is a recipe R& for fli such thati.
G believes that it can perform j3i according to the recipe(3R&)\[BCBA(G, f~i, R&, T&, Tp, {pj })Aii.
G has a full individual plan for fli using the recipeFIP(G, fl~, Tp, Tfl,, RZ,)\]Figure 4Full individual plan (FIP) definition.As indicated in Clause (1) of the definitions in Figures 4 and 5, recipes are modeledin Grosz and Kraus's definitions as sets of constituent acts and constraints.
To performan act a, an agent must  perform each constituent act (the fli in Clause (1)) in a 's  recipeaccording to the constraints of that recipe (the pj).
Actions themselves may be furtherdecomposed into act-types and parameters.
We will represent an action c~ as a termof the form 6(pl .
.
.
.
.
Pn) where 6 represents the act-type of the action and the Pi itsparameters.
Figure 6 provides a graphical representation of a recipe.The operators Int.To and Int.Th in Grosz and Kraus's definitions are used to rep-resent different types of intentions.
Int.To represents an agent's intention to performan action, while Int.Th represents an agent's intention that a proposit ion hold.
Int.To'soccur in both types of plans (Clause (2a) in Figures 4 and 5), while Int.Th's occuronly in SharedPlans (Clauses (0), (2c) and (3c) in Figure 5).
Int.Th's are used to repre-sent commitment  to the joint activity and also engender the type of helpful behaviorrequired of collaborating agents (Bratman 1992; Grosz and Kraus 1993, 1996).The operators CBA, BCBA, CBAG, and MBCBAG in Grosz and Kraus's definitionsare ability operators; they encode requirements on an agent's ability to perform an531Computational Linguistics Volume 24, Number 4FSP(GR, c~, Tp, Ta, Ra )A group of agents GR have a full shared plan at time Tp to perform act a at time Tausing recipe RaO.
GR is committed to performing oLM B(GR, (VGj ?
GR)Int.Th(Gj, Do(GR, o6 To), Tp, Ta), Tp)1.
GR has a recipe for aRa = {fli,pj} A MB(GR, Ra ?
Reciipes(c~),Tp)2.
For each single-agent constituent act/3/ of the recipe, there is an agent G& E GR, suchthat(a) G& intends to perform/3iInt.To( G& , fli, Tp, T& )There is a recipe R& for/3i such thati.
G& believes that it can perform/31 according to the recipe(3R&)\[BCBA(G&,/3,, R&, T&, Tp, {pj }) Aii.
G& has a full individual plan for fli using the recipeFIP(G&, fli, Tp, T&, R& )\](b) The group GR mutually believe (2a)M B( GR, Int.To( G& , fli, Tp, T& )A(3R~,)\[CBA(G&,/3i, R&, T&, {pj}) AFI  P( G&, /3i, Tp, T&, R& )\], Tp)(c) The group GR is committed to G&'s successMB(GR, (VGj ?
GR)Int.Th( Gj, (3R&)CBA(G&, Z,, R& , T& , {pj }), Tp, T& ), Tp)3.
For each multi-agent constituent act/3i of the recipe, there is a subgroup of agentsGR& C GR such that(a) There is a recipe R& for/31 such thati.
GR& mutually believe that they can perform/31 according to therecipe(3R&)\[MBCBAG(GR&,/3i, R&, T&, Tp, {pj }) Aii.
GR& has a full ShaxedPlan for/3i using the recipeFSP( GR&, /3i, Tp, T&, R& )\](b) The group GR mutually believe (3a)MB(GR, (3R~,)\[CBAG(GR&,/3i, R&, T&, {pj }) AFSP(GR&,/3i, Tp, T&, R& )\], Tp)(c) The group GR is committed to GR~'s successMB(GR, (VGj ?
GR)Int.Th( Gj, (3R& )C BAG( G R&, /3,, R~, T&, {pj }), Tp, T& ), Tp)Figure 5Full SharedPlan (FSP) definition.A recipe for c~ is comprised of a set of constituent acts ({ill,..
?, fl,~}) and constraints((p~ .
.
.
.
,p,~}).fXFigure 6A graphical representation of a recipe.532Lochbaum A Collaborative Planning Modelhas.recipe(G, c~, R, T) 4-~(1) \[basic.level(a) ABEn(G, basic.level(a), T) A R = REmpt~\] V(2) \[-~basic.level(a) A(2a) R = {~,p j}  ^(2al) {\[IGI = 1 ^  BEn(G, R e Recipes(a),T)\] V(2a2) \[IGI > 1 A MB(G, R E Recipes(a), T)\]}\]Figure 7Definition of has.recipe.action.
CBA (read "can bring about") and BCBA ("believes can bring about") are single-agent operators, while CBAG ("can bring about group") and MBCBAG("mutuallybelieve can bring about group") are the corresponding group operators.An agent's ability to perform an action depends upon its ability to satisfy boththe physical and knowledge preconditions of that action (McCarthy and Hayes 1969;Moore 1985; Morgenstern 1987).
For example, for an agent to pick up a particulartower of blocks, it must (i) know how to pick up towers in general, (ii) be able toidentify the tower in question, and (iii) have satisfied the physical preconditions orconstraints associated with picking up towers (e.g., it must have a free hand).
In Groszand Kraus's (1996) definitions, conditions of the form in (iii) depend upon the con-straints under which an act fli is to be performed.
These constraints derive from therecipe in which fli is a constituent and are represented by {pj} in the plan definitionsin Figures 4 and 5.
Conditions of the form in (i) and (ii) above are knowledge precon-ditions.
Knowledge preconditions were not represented in Grosz and Kraus's (1993)original definitions, but were subsequently formalized by the author.
We now presentdefinitions of the relations necessary to model these conditions.2.1 Knowledge Preconditions2.1.1 Determining Recipes.
For an agent G to be able to perform an act a, it mustknow how to perform on; i.e., it must have a recipe for the act.
The relation has.recipe(G,o~, R, T) is used to represent that agent G has a recipe R for an act o~ at time T. Itsformalization is as shown in Figure 7.Clause (1) of the definition indicates that an agent does not need a recipe toperform a basic-level action, i.e., one executable at will (Pollack 1986a).
6 For non-basic-level actions (Clause (2)), the agent of o~ (either a single agent (2al) or a groupof agents (2a2)) must believe that some set of acts, fli, and constraints, pj, constitute arecipe for a.The has.recipe relation can be used to represent one of the knowledge preconditionrequirements of the ability operators, as well as the recipe requirement in Clause (1)of the plan definitions in Figures 4 and 5.2.1.2 Identifying Parameters.
An agent must also be able to identify the parametersof an act o~ to be able to perform it.
For example, if an agent is told, "Now removethe pump \[off the air compressor\]," as in the dialogue of Figure 1, the agent must be ableto identify the pump in question.
The ability to identify an object is highly contextdependent.
For example, as Appelt points out (1985, 200), "the description that one6 Basic-level actions are by their nature single-agent actions.533Computational Linguistics Volume 24, Number 4must know to carry out a plan requiring the identification of 'John's residence' may bequite different depending on whether one is going to visit him, or mail him a letter.
"The relation id.params(G, o~, T) is used to represent that agent G can identify theparameters of act o~ at time T. If o~ is of the form 6(pl .
.
.
.
.
pn), then id.params(G, o~, T)is true if G can identify each of the Pi.
To do so, G must have a description of each pithat is suitable for 6.
The relation id.params is thus defined as follows:id.params(G, 6(pl .
.
.
.
.
p,), T) 4-~ (Vi, 1 < i < n) has.sat.descr(G, pi ~(6, pi), T)The function Y in the above definition is a kind of "oracle" intended to model thecontext-dependent na ure of parameter identification.
This function returns a suitableidentification constraint (Appelt and Kronfeld 1987) for a parameter pi in the contextof an act-type 6.
For example, in the case of sending a letter to John's residence, theconstraint produced by the oracle function would be that John's residence be describedby a postal address.The relation has.sat.descr( G,P, C, T) holds of an agent G, a parameter descriptionP, an identification constraint C, and a time T, if G has a suitable description, asdetermined by C, of the object described as P at time T. To formalize this relation, werely on Kronfeld's (1986, 1990) notion of an individuating set.
An agent's individuatingset for an object is a maximal set of terms such that each term is believed by the agentto denote that object.
For example, an agent's individuating set for John's residencemight include its postal address as well as an identifying physical description such as"the only yellow house on Cherry Street."
To model individuating sets we introducea function IS(G,P, T); the function returns an agent G's individuating set at time Tfor the object hat G believes can be described as P. This function is based on similarelements of the formal anguage that Appelt and Kronfeld (1987) introduce as part oftheir theory of referring.
The function returns a set that contains P as well as the otherdescriptions that G has for the object hat it believes P denotes.The relation has.sat.descr is used to represent that an agent can identify a param-eter for some purpose.
For that to be the case, the agent must have a description, P~,of the parameter such that P' is of the appropriate sort.
For example, for an agent ovisit John's residence, it is not sufficient for the agent o believe that the description"John's residence" refers to the place where John lives.
Rather, the agent needs an-other description of John's residence, one such as "the only yellow house on CherryStreet," that is appropriate for the purpose of visiting him.
To model an agent's abilityto identify a parameter for some purpose, we thus require that the agent have an indi-viduating set for the parameter that contains a description p/such that P/satisfies theidentification constraint that derives from the purpose.
The definition of has.sat.descr isthus as shown in Figure 8.
7 The predicate suff.for.id(C, P~) is true if the identificationconstraint C applies to the parameter description PL The oracle function .~'(6,pi) inid.params is used to produce the appropriate identification constraint on Pi given 6.Identification constraints can derive from syntactic, semantic, discourse, and world~mowledge (Appelt and Kronfeld 1987).In Figures 7 and 8, we have separated the requirements of recipe identificationfrom those of parameter identification.
That is, we have defined has.recipe and id.paramsas independent relations, and do not require an agent o know the parameters of anact to be said to know a recipe for that act.
The separation of these two requirements7 A more precise account of what it means to be able to identify an object is beyond the scope of thispaper; for further details, see the discussions by Hobbs (1985), Appelt (1985), Kronfeld (1986, 1990),and Morgenstern (1988).534Lochbaum A Collaborative Planning Modelhas.sat.descr( G,P, C, T) ?-~{\[\[G\] = 1 A (3P')BEL(G, \[P' ?
IS(G,P,T) Asuff.for.id(C, P')\], T)\] V\[IG\] > 1 A (3P')MB(G,(VGj ?
G)\[P' ?
IS(Gj,P,T) Asuff.for.id(C, P')\], T)\]}Figure 8Definition of has.sat.descr.derives from the distinction between recipes and plans.
Whereas an agent may knowmany recipes for performing an act, he will have a plan for that act only if he isconunitted to its performance using a particular ecipe.
For example, an agent mayknow that one way to hijack a plane involves muggling a gun on that plane, withoutactually intending to hijack a plane at all.
Similarly, an agent can know a way ofhijacking a plane without actually having a particular plane, or gun, in mind.
For thisreason, we do not make id.params a requirement of has.recipe.
The separation of thesetwo requirements has particular consequences for our model of discourse processing,as will be discussed in Section 7.With the addition of the knowledge precondition relations defined above, thedefinitions of the SharedPlan ability operators (CBA, BCBA, CBAG, and MBCBAG)include three components.
The definitions of these operators now state that for anagent o be able to perform an act c~, it must (i) have a recipe for o~ (has.recipe), (ii)be able to identify the parameters of o~ (id.params), and (iii) be able to satisfy theconstraints of its recipe for o~ (the {pj} in the plan definitions).3.
Reasoning with SharedPlansIn more traditional plan-based approaches to natural anguage processing (e.g., thework of Cohen and Perrault \[1979\], Allen and Perrault \[1980\], Sidner \[1985\], Carberry\[1987\], Litman and Allen \[1987\], Lambert and Carberry \[1991\]), reasoning about plansis focused on reasoning about actions.
In these models, actions are represented usingoperators derived from STRIPS (Fikes and Nilsson 1971) and NOAH (Sacerdoti 1977).Such operators include: a header, specifying the action and its parameters; a precondi-tion list, specifying the conditions that must be true for the action to be performed; 8 abody, specifying how the action is to be performed; and an effects list, specifying theconditions that will hold after the action is performed.
Under these models, reasoningabout plans involves reasoning according to rules that derive from the componentsof the plan operators.
For example, Allen (1983) introduces a precondition-action rulestating that if agent G wants to achieve proposition P and P is a precondition of an actACT, then G may want to perform ACT.
This rule is used for plan recognition.
Thecorresponding rule for plan construction states that if agent G wants to execute ACT,then G may want to ensure that precondition P is satisfied.
Heuristics, derived fromboth planning and natural anguage principles, are used to guide the application ofthe rules to recognize (or construct) the best possible plan accounting for an agent'sobservations (or desired effects).8 The preconditions may also be supplemented by a list of applicability conditions, specifying theconditions under which it is reasonable to pursue the action, and a list of constraints specifyingrestrictions on instantiations of the operator's parameters (Litman and Allen 1987; Carberry 1987).535Computational Linguistics Volume 24, Number 4Problem-solving P lan-P l :  {_agent1 and _agent2 build a plan for _agent1 to do _action} 'Action: Build-Plan(_agentl, -agent2, _action)AppCond: want (_agent 1,_action)Constr: plan- for (_plan,_action)action-in-plan-for (Aaction,-action)Prec: selected (_agent 1,_action,_plan)know(_agent2,want (_agent 1,_action))knowref( _agent 1, _prop,prec-of( _prop, _plan))knowref(_agent2,_prop,prec-of(_prop,_plan))knowref( _agent 1, ._l action ,need- do ( _agent 1, _lact ion, _action) )knowref( _agent 2, _l action, need- do ( _agent 1, _laction, _act ion ) )1. for all actions _laction in _planInstantiate-Vars (_agent 1,-agent2,Aaction)2. for all actions _laction in _planBuild-Plan (_agent 1,-agent2,_.laction)have-plan (_agent 1, _plan, _action)have-plan(_agentl,_plan,_action)Body:Effects:Goal:Figure 9Lambert and Carberry's (1991, 49) Build-Plan operator.In these more traditional approaches, the inference rules are typically expressedin terms of the beliefs and goals of the speaker and hearer.
Allen's (1983, 126) precon-dition-action rule, for example, is represented as:SBAW(P) Di SBAW(ACT) - -  if P is a precondition of action ACTwhere SBAW(P) represents that the inferring agent S believes that agent A wantsP.
As Pollack (1986b, 1990) has noted, however, these mental attitudes are typicallytransparent to the reasoning process.
The system reasons about the action operatorsthemselves--in this case whether P is a precondition of ACT--and essentially ignorestile mental attitudes in the rules; they are simply carried forward from antecedent toconsequent.
Pollack has dubbed these approaches data-structure approaches becauseof their focus on the action operators themselves, rather than on the mental attitudesti'lat are required for planning and plan recognition.In more recent work, the action operators have come to incorporate many morerequirements on the agents' mental states.
For example, Lambert and Carberry's op-erator for the action of building a plan is given in Figure 9.
The Build-Plan operatoris used to represent the process by which two agents build a plan for one of themto perform an action.
The preconditions of the operator specify requirements on theagents' mental states, e.g., that the agents know the referents of the subactions thatone of them needs to perform to accomplish the overall action.
The main differencebetween this type of approach and the SharedPlan approach discussed in this paperis in the focus of the representation.'
The representation i  Figure 9 specifies require-ments on performing an action, some of which are requirements on mental states.The SharedPlan definition in Figure 5 specifies requirements on mental states, someof which refer to actions and their decompositions.
One might thus think of the rep-resentation i  Figure 9 as being "inside-out" from that in Figure 5.
Because the focusof the representation in Figure 9 remains on the action and its decomposition, we con-tinue to refer to these types of approaches as data-structure approaches.
We reservetlhe term mental phenomenon approach for those approaches, uch as SharedPlans536Lochbaum A Collaborative Planning Modeland Pollack's individual plans (Pollack 1986b, 1990; Grosz and Kraus 1996), that takemental states to be primary.
We will return to the implications of representations suchas those in Figure 9 in Section 8.2.The process of reasoning with SharedPlans differs significantly from the processof reasoning with plan operators.
Under the SharedPlan approach, agents engaged indiscourse are taken to be collaborating on performing some action or on achievingsome state of affairs.
Each agent brings to their collaboration different beliefs aboutways in which to achieve their goal and the actions necessary for doing so.
Eachagent may have incomplete or incorrect beliefs.
In addition, their beliefs about eachother's beliefs and capabilities to act may be incorrect.
The participants use the dis-course to communicate heir individual beliefs and to establish mutual ones.
Underthe SharedPlan approach, the utterances of a discourse are thus understood as con-tributing information toward establishing the beliefs and intentions that are requiredfor successful collaboration.
These beliefs and intentions are summarized by the fullSharedPlan definition in Figure 5 and form the basis for the discourse participants'utterances.Until the agents have established all of the requirements of a full SharedPlan, theywill have a partial SharedPlan.
The agents' partial SharedPlan evolves over the courseof the agents' discourse as they communicate about the actions they will perform, theeffects of those actions as they perform them, and the need to revise their plans whenthings do not proceed as expected.
The agents' partial SharedPlan is thus always ina state of flux.
At any given point in the agents' discourse, however, it represents hecurrent state of the agents' collaboration.
It thus indicates those beliefs and intentionsthat have been established at that point in the discourse, as well as those that re-main to be established over the course of the remaining discourse.
The agents' partialSharedPlan thus serves to delineate the information that the agents must consider ininterpreting each other's utterances and in determining what they themselves shoulddo or say next.
For the agents' utterances tobe coherent, hey must advance the agents'partial SharedPlan towards completion by helping to establish the "missing" beliefsand intentions.The concept of plan augmentation thus provides the basis for our model of dis-course processing.
Under this approach, discourse participants' utterances are under-stood as augmenting the partial SharedPlan that represents he state of their collab-oration.
Figure 10 provides a high-level specification of this process?
It is based onthe assumption that agents G1 and G2 are collaborating on an act o~ and models Gl'sreasoning in that regard.
It thus stipulates how Gl's beliefs about the agents' partialSharedPlan are augmented over the course of the agents' discourse.
~?It is important o emphasize here that SharedPlans are complex structures thatare distributed in nature.
The full SharedPlan for a group activity does not, typically,reside in any single agent's mind, nor is there any notion of a group mind in whichthe SharedPlan resides.
Rather, the beliefs and intentions that form a SharedPlan aredistributed among the individual minds of the participating agents.
Each agent hasindividual beliefs about its capabilities to act, as well as individual intentions to doso.
In addition, agents have commitments owards other agents' abilities to act (rep-resented by intentions that, Int.Th) and mutual beliefs about others' capabilities and9 The details of this process differ significantly from that described in a previous paper (Lochbaum,Grosz, and Sidner 1990).10 For expository purposes, we will take G1 to be male and G2 to be female.
We have omitted the timeand recipe arguments from the PSP specification for simplicity of exposition and will continue to do sosubsequently when they are not at issue.537Computational Linguistics Volume 24, Number 4Assume:PSP({G1, G2}, o~),G1 is the agent being modeled.Let Prop be the proposition communicated by G2's utterance/4.1.
As a result of the communication, G1 assumesMB({G1, G2}, BEL(G2, Prop)).2.
G1 must then determine the relationship of Prop to the current SharedPlan context:(a) If G1 believes that/d or Prop indicates the initiation of a subsidiary SharedPlanfor an act fl, then G1 willi.
ascribe Int.Th(G2, FSP({G1, G2}, fl)),ii.
determine if he is also willing to adopt such an intention.
(b) If G1 believes that U or Prop indicates the completion of the currentSharedPlan, then G1 willi.
ascribe BEL(G2, FSP({G1, G2},a)),ii.
determine if he also believes the agents' current SharedPlan to becomplete.
(c) Otherwise, G1 willi.
ascribe to G2 a belief that Prop is relevant o the agents' currentSharedPlan,ii.
determine if he also believes that to be the case.3.
(a)If Step (2) is successful, then G1 will signal his agreement (possibly implicitly)an/:l assume mutual belief of the inferred relationship in (2a), (2b), or (2c) asappropriate, updating his view of the agents' PSPs in theprocess.
(b)Otherwise, G1 will query G2 or communicate his dissent.Figure 10The SharedPlan augmentation process.commitments.
The combination of mutual  belief and intention is sufficient to modelcollaboration.
No notion of irreducible joint intention (as in Searle's \[1990\] work), orany other attitude that would refer to a group mind is necessary (Grosz and Kraus1996).The processing outlined in Figure 10 assumes that agent G2 has just communicatedan utterance/d with proposit ional content Prop.
u To make sense of this utterance, G1must  determine how Prop contributes to the agents' PSP for o~.
In some cases, thelinguistic signal/,/  may  aid in this process.
As indicated in Figure 10, Prop may beinterpreted in one of three basic ways.
It may indicate the initiation of a subsidiarySharedPlan (Case (a) of Step (2)), signal the completion of the current SharedPlan(Case (b)), or contribute to it (Case (c)).
In each of these cases, G1 first ascribes aparticular mental attitude to G2 on the basis of her utterance (Step (i) in each case) andthen reasons about the relevance of that mental attitude to the agents' PSP (Step (ii)).If G1 is able to make sense of the utterance in this way, he then updates his beliefsabout the agents' PSP to reflect their mutual  belief of the inferred contribution of Prop(Step (3a)).
Otherwise, if G1 does not understand the relevance of G2"s utterance, or11 The recognition of propositional content from surface form has been studied by other researchers (e.g.,Allen and Perrault \[1980\], Litman and Allen \[1987\], Lambert and Carberry \[1991\]) and is not discussedin this paper.538Lochbaum A Collaborative Planning Modeldisagrees with it, he may simply communicate his dissent o G2 or query her further(Step (3b)).In Case (a) of Step (2), Prop indicates G2"s intention that the agents collaborate onan act ft. G1 first ascribes this intention to G2 and then tries to explain it in the contextof the agents' PSP for a.
If G1 believes that the performance of fl will contribute tothe agents' performance of a, and is willing to collaborate with G2 in this regard, thenG1 will adopt an intention similar to that of G2"s and agree to the collaboration.
Thisprocess is modeled by Step (2aii).
On the basis of his reasoning, G1 will also updatehis view of the agents' PSP to reflect hat fl is an act in the agents' recipe for a forwhich the agents will form a SharedPlan.
This behavior is modeled by Step (3a) of theaugmentation process.
In this step, agent G1 updates his view of the agents' partialplan to reflect heir mutual belief of the communicated information.In Case (b) of Step (2), Prop indicates G2"s belief that the SharedPlan on which theagents are currently focused is complete.
This SharedPlan may represent the agents'primary collaboration or a subsidiary one.
In either case, G1 must determine if he alsobelieves the agents to have established all of the beliefs and intentions required forthem to have a full SharedPlan for a.
If he does, then he will agree with G2 and updatehis view of the agents' PSP for a to reflect hat it is complete.Case (c) of Step (2) is the default case.
If G1 does not believe that Prop indicatesthe initiation or completion of a SharedPlan, then he will take it to contribute to theagents' current SharedPlan i  some way.
G1 will first ascribe this belief to G2 and thenreason about he specific way in which Prop contributes to the agents' PSP for a.
If heis successful in this regard, he will indicate his agreement with G2 and then updatehis view of the agents' PSP to reflect his more specific relationship.Figure 10 provides ahigh-level specification of the use of SharedPlans in interpre-tation.
In Section 6 we will provide algorithms for further modeling two of the steps inthis process, while in Section 10, we will discuss the use of SharedPlans in generation.The main focus of this paper, however, is on modeling the intentional structure of dis-course.
In the next section, we thus provide a model of that structure.
We then showhow the model of utterance interpretation presented in Figure 10 can be mapped to theproblem of recognizing intentional structure and utilizing it in discourse processing.4.
Grosz and Sidner's Theory of Discourse StructureAccording to Grosz and Sidner's (1986) theory, discourse structure is comprised ofthree interrelated components: a linguistic structure, an intentional structure, and anattentional state.
The linguistic structure isa structure that is imposed on the utterancesthemselves; it consists of discourse segments and embedding relationships amongthem.
The linguistic structure of the sample dialogues in Section 1 is indicated by thebold rule grouping utterances into segments.The intentional structure of discourse consists of the purposes of the discoursesegments and their interrelationships.
Discourse segment purposes or DSPs are inten-tions that lead to the initiation of a discourse segment.
DSPs are distinguished fromother intentions by the fact that they, like certain utterance-level intentions describedby Grice (1969), are intended to be recognized.
There are two types of relationshipsthat can hold between DSPs, dominance and satisfaction-precedence.
One DSP dom-inates another if the second provides part of the satisfaction of the first.
That is, theestablishment of the state of affairs represented by the second DSP contributes tothe establishment of the state of affairs represented by the first.
This relationship isreflected by a corresponding embedding relationship in the linguistic structure.
OneDSP satisfaction-precedes another if the first must be satisfied before the second.
This539Computational Linguistics Volume 24, Number 4(x)(2) VPSP(tG1, G21, ct) L FSP(\[G1, G21, ~t )(3) 7 PSP(\[GI,G2\], ~2)u~OSP 2 DSP,(':;t i~i:C;2~TS;0~2i~'~ ~ is dominated by(~n~.
"~a}~C'Pi~F'SP)}~2i~ "~)*omnmwawlmlmmmwmllmimmllmmmwwglw** * , ,mmn.
.
.wmwasmlmmlmmnmwwwwlmlm*FSP(\[GI,G2}, ~ ) is subsidiary to FSP(\[G1,G2\], ix)DSP, DSP,is dominated by Int.Th(ICP l ,FSP({G1,G2}, o0)t1111  i i i l l  i l l i l n  i o l  i i  i i i l l l l  i i o l *  11  I l l l  I I m i l l e  i l l  I o l i l l e  i i I l i l l l ~FSP(\[G1,G2}, ~) is subsidiary to FSP({G1,G2}, ix)Figure 11Modeling intentional structure.relationship is reflected by a corresponding sibling relationship in the linguistic struc-ture.The attentional state component of discourse structure serves as a record of thoseentities that are salient at any point in the discourse; it is modeled by a stack of focusspaces.
With each new discourse segment, a new focus space is pushed onto the stack(possibly after other focus spaces are first popped off), and the objects, properties, and:relations that become salient during the segment are entered into it, as is the segment'sDSP.
One of the primary roles of the focus space stack is to constrain the range of DSPsto which a new DSP can be related; a new DSP can only be dominated by a DSP insome space on the stack.
Once a segment's DSP is satisfied, the segment's focus space:is popped from the stack.5.
A SharedP lan Mode l  of In tent iona l  StructureFigure 11 illustrates the role of SharedPlans in modeling intentional structure.
Asindicated in the figure, we take each segment of a discourse to have an associatedSharedPlan.
The purpose of the segment is taken to be an intention that (Int.Th) thediscourse participants form that plan.
This intention is held by the agent who initiatesthe segment.
Following Grosz and Sidner (1986), we will refer to that agent as theICP for initiating conversational participant; he other participant is the OCP.
DSPsare thus represented as intentions of the form Int.Th(ICP, FSP({ICP, OCP}, fl)) in ourmodel.Relationships between DSPs derive from relationships between the correspondingSharedPlans.
For example, a satisfaction-precedence relationship between DSPs corre-sponds to a temporal dependency between SharedPlans) 2 When one DSP satisfaction-12 Thanks to Christine Nakatani for initially suggesting this correspondence.540Lochbaum A Collaborative Planning Modelprecedes another, the SharedPlan used to model the first must be completed before theSharedPlan used to model the second.
Dominance relationships between DSPs dependupon subsidiary relationships between the corresponding SharedPlans.
In Section 3,we used the term subsidiary SharedPlan to indicate asubordinate r lationship betweenSharedPlans.
More generally, one plan is subsidiary to another if the completion of thefirst plan establishes one of the beliefs or intentions required for the agents to havethe second plan.
One plan is thus subsidiary to another if the completion of the firstplan contributes to the completion of the second.The utterances of a discourse are understood in terms of their contribution tothe SharedPlans associated with the segments of the discourse.
Those segments thathave been completed at the time of processing an utterance have a full SharedPlanassociated with them (e.g., segment (2) in Figure 11), while those that have not havea partial SharedPlan (e.g., segments (1) and (3) in Figure 11).5.1 Dialogue AnalysesWe now return to the dialogues in Section 1 to illustrate the use of SharedPlans inmodeling intentional structure.
In this section of the paper, we simply describe theintentional structure representations for these examples.
In Section 6, we describe theprocess by which these structures may be recognized and reasoned with.5.1.1 Example 1: Subtask Subdialogues.
The overall purpose of the dialogue in Fig-ure 1 may be represented as :13DSP1 = Int.Th(e, FSP( {e, a}, replace(pump(acl )&belt(acl ), {a})))"E intends that the agents collaborate to replace the pump and belt of the air compressor,acl.
"The circumstances surrounding this dialogue are such that only the Apprentice isphysically capable of performing actions; the Expert is in another oom and can onlyinstruct the Apprentice as to which actions to perform.
Both of the agents participate inthe act of replacing the pump and belt of the air compressor, though each agent bringsdifferent skills to the task.
The Expert provides the expertise, while the Apprenticeprovides the manual dexterity.
Thus, the agent specification ofthe FSP in DSP1 includesboth the Expert and the Apprentice, while only the Apprentice is the agent of the replaceact itself.The purpose of the first subdialogue in Figure 1 may be represented as:DSP2 = Int.
Th (a, FSP( {e, a }, remove(belt(acl ), {a })))"A intends that the agents collaborate to remove the belt of the air compressor.
"while the purpose of the second subdialogue may be represented asDSP3 = Int.Th(e, FSP( {e, a}, remove(pump(ael ), {a})))"E intends that the agents collaborate to remove the pump of the air compressor.
"The SharedPlans used to model DSP2 and DSP3 are subsidiary to that used tomodel DSP1 by virtue of the subsidiary plan requirement of the SharedPlan definition.As shown in Clauses (2aii) and (3aii) of the definition in Figure 5, an FSP for an act ozincludes as components full plans for each subact in oz's recipe.
A plan for one of thesubacts fli thus contributes to the FSP for o~, and is therefore subsidiary to it.
Becausethe tasks of removing the air compressor's belt and pump are subtasks of the act ofreplacing the belt and pump, the SharedPlans toperform those subtasks are subsidiaryto the SharedPlan of the main task.
DSP1 thus dominates both DSP2 and DSP3.13 We follow the Prolog convention f specifying variables using initial uppercase letters and constantsusing initial lowercase l tters.541Computational Linguistics Volume 24, Number 4modify_network(NetPiece,Loc,G,T){ type(Net Piece,kl-o ne_net work),type(Lot,screen_location),mpty(Loc) ,frees pace~for(Data,Loc)l<T2}display(NetPiece,G 1 T 1) put(Data,Loc,G2,T2)Figure 12A recipe for modifying a network.5.1.2 Example 2: Correction Subdialogues.
The overall purpose of the dialogue inFigure 2 may be represented as:DSP4 = Int.Th(u, FSP( {u, s}, modify_network(NetPiece, Lo , {u, s})))"U intends that he agents collaborate omodify the piece of network displayed atsome screen location.
"Figure 12 contains one possible recipe for the act modify_network(NetPiece, Loc, G, T).
14The recipe requires that an agent display a piece of a network and then put somenew data at some screen location.
The constraints of the recipe require that the screenlocation be empty and that there be enough free space for the data at that location.The purpose of the subdialogue in Figure 2 may be represented as: 15DSP5 =In t. Th ( u, F S P ( ( u, s }, Achieve (freespace.for( Data, below(gel ) ), { u, s })))"U intends that he agents collaborate ofree up some space below the employee concept.
"The SharedPlan used to model DSP5 is subsidiary to that used to model DSP4 byvirtue of the ability operator BCBA.
As discussed in Section 2, an agent G's ability toperform an act fl depends in part on its ability to satisfy the constraints of its recipefor ft. A plan to satisfy one of the constraints thus contributes to the plan for fl and istherefore subsidiary to it.
Because the condition freespace_for(Data, Loc) is a constraintin the recipe for modify_network(NetPiece, Loc, G, T), the SharedPlan in DSP5 to free upspace on the screen is subsidiary to the SharedPlan in DSP4 to modify the network.DSP4 thus dominates DSPs.5.1.3 Example 3: Knowledge Precondition Subdialogues.
The overall purpose of thedialogue in Figure 3 may be represented as:DSP6 = Int.Th(nm, FSP( {nm, np}, maintain(node39, {nm, np})))"NM intends that he agents collaborate omaintain ode39 of the local computernetwork.
"The purpose of the first subdialogue in Figure 3 may be represented as:DSP7 = Int.Th(np,FSP({nm, np},Achieve( has.recipe( { nm, np} , maintain(node39, {nm, np}), R), { nm, np})))"NP intends that he agents collaborate oobtain a recipe for maintaining node39.
"The SharedPlan used to model DSP7 is subsidiary to the SharedPlan used to modelDSP6 by virtue of the recipe requirement of the SharedPlan definition.
As shown inClause (1) of the definition in Figure 5, for a group of agents to have an FSP for an acto~, they must have mutual belief of a recipe for o~.
The SharedPlan in DSP7 to obtain14 This recipe derives from the operators used in Sidner's (1985) and Litman's (1985) representations fthe acts and constraints underlying the exchange inFigure 2.15 The function Achieve takes propositions to actions (Pollack 1986a).542Lochbaum A Collaborative Planning Modela recipe for maintaining node39 thus contributes to the SharedPlan in DSP6 to do themaintenance and is therefore subsidiary to it.
As a result, DSP6 dominates DSP7.The second subdialogue in Figure 3 is concerned with identifying a parameter  ofan act.
The purpose of this subdialogue may be represented as:DSP8 = Int.Th(nm,FSP({nm, np},Achieve( has.sat.descr( { nm, np } , ToNode, S ( divert_traffic, ToNode ) ),{nm, np})))"NM intends that the agents collaborate o obtain a suitable description of the ToNodeparameter ofthe divert_traffic act.
"The SharedPlan used to model DSPs is subsidiary to that used to model  DSP6 byvirtue of the ability operator BCBA.
As discussed in Section 2, an agent G's abil-ity to perform an act fl depends in part on its ability to identify the parametersof ft. At this point in the agents' discourse, NM and NP have agreed that the actsdivert_traFfic(node39, ToNode, G1) and replace_switch(node39, Switch Type, G2) will be partof their recipe for maintaining node39.
Because the agents' recipe for maintainingnode39 includes the act of diverting network traffic, the SharedPlan in DSP8 to iden-tify the ToNode parameter of the divert_traffic act contributes to the SharedPlan in DSP6to maintain node39.
DSP6 thus dominates DSP8.The SharedPlan in DSP8 is not subsidiary to the SharedPlan in DSP7, because,id.params is not a requirement of has.recipe.
As we argued in Section 2.1, knowing arecipe for an act should not require identifying the parameters of the act or the actsin its recipe.
However,  because an agent must  have a recipe in mind before it can beconcerned with identifying the parameters of the acts in that recipe, the SharedPlanin DSP7 must be completed before the SharedPlan in DSPs.
16 DSP7 thus satisfaction-precedes DSP8.6.
Reasoning with Intentional StructureIntentional structure plays a central role in discourse processing.
For each utterance ofa discourse, an agent must  determine whether the utterance begins a new segment ofthe discourse, completes the current segment, or contributes to it (Grosz and Sidner1986).
If the utterance begins a new segment of the discourse, the agent must  recognizethe DSP of that segment, as well as its relationship to the other DSPs underlying thediscourse and currently in focus.
If the utterance completes the current segment, theagent must  come to believe that the DSP of that segment has been satisfied.
If theutterance contributes to the current segment, the agent must determine the effect ofthe utterance on the segment's DSP.We now show how the SharedPlan reasoning presented in Section 3 may bemapped to the problem of recognizing and reasoning with intentional structure.
Step (2)of the augmentat ion process in Figure 10 is divided into three cases based upon theway in which an utterance affects the SharedPlans underlying a discourse.
An utter-ance may indicate the initiation of a subsidiary SharedPlan (Case (2a)), the completion16 There are several means by which an agent can determine a recipe for an act c~.
If an agent chooses arecipe for c~ from some type of manual (e.g., a cookbook), then the agent will have a complete recipefor c~ before identifying the parameters ofc~'s constituent acts.
On the other hand, when being told arecipe for c~ by another agent, the ignorant agent may interrupt and ask about a parameter ofaconstituent act before knowing all of the constituent acts.
In this case, the agent may have only apartial recipe for c~ before identifying the parameters ofthe acts in that partial recipe.
Thus, if fli is anact in c~'s recipe, a discourse segment concerned with identifying aparameter of fli could belinguistically embedded within a segment concerned with obtaining a recipe for c~.
This case posesinteresting questions for future research regarding the relationship between the two segments' DSPs.543Computational Linguistics Volume 24, Number 4Assume:PSP({G1,G2 },c~),The purpose of the current discourse segment, DSc, is thusDSP?
=Int.Th (ICP, FSP  ({ G1, G2 }, a))G1 is the agent being modeled,S is a stack of SharedPlans used to represent G~'s beliefs as to which portion of theintentional structure is currently in focus.Let Prop be the proposition communicated by G2's utterance/d.2.
G1 must then determine the relationship of Prop to S:(a)Does ld or Prop indicate the initiation of a new discourse segment?If G1 believes that/d or Prop indicates the initiation of a subsidiary SharedPlanfor an act fl, theni.
G1 believes that the DSP of the new segment isInt.Th(G2, FSP({Gi ,  G~ }, fl)).ii.
G1 explains the new segment by determining the relationship of theSharedPlan in (i) to the SharedPlans maintained in S.(b)Does bl or Prop indicate the completion of the current discourse segment?If G1 believes that/d or Prop indicates the satisfaction of DSP~, theni.
G~ believes that G2 believes DSc is complete.ii.
If G1 believes that the agent s' PSP for a is complet% then G1 will alsobelieve that DSPc has been satisfied and thus DS?
is complete.
DSP?
isthus popped from S.(c) Does Prop contribute to the current discourse segment?Otherwise, G1 willi.
ascribe to G~ a belief that Prop contributes to the agents' PSP forii.
determine if he also believes that to be the case.Figure 13Step (2) of the augmentation process.of the current SharedPlan (Case (2b)), or its continuation (Case (2c)).
These three cases:may be mapped to the problem of determining whether an utterance begins a new seg-ment of the discourse, completes the current segment, or contributes to it.
In Figure 13,we have recast Step (2) of the augmentat ion process to reflect this use.The augmentat ion process in Figure 13 specifies the process by which agent G1makes sense of agent G2's utterances given the current discourse context.
We use astack of SharedPlans S to model this context.
The stack corresponds to that port ion ofthe intentional structure that is currently in focus.
It thus mirrors the attentional statecomponent  of discourse structure and contains PSPs corresponding to discourse seg-ments that have not yet been completed.
Because the augmentat ion process dependsmost heavily upon the SharedPlans that are used to represent DSPs, it s imply makesuse of the SharedPlans themselves, rather than the full intentions.
The full intentionsare easily recoverable from the stack representation.Case (2a) in Figure 13 models the recognition of new discourse segments and theirpurposes.
If G1 believes that G2's utterance indicates the initiation of a new SharedPlan,then G1 will take G2 to be initiating a new discourse segment with her utteranceJ 7Gt first ascribes this intention to G2 (Step (2ai)) and then tries to explain it given the17 As discussed in Section 7.3, the DSP of the new segment may be only abstractly specified at this point.544Lochbaum A Collaborative Planning Modelcurrent discourse context (Step (2aii)).
Whereas at the utterance l vel, a hearer mustexplain why a speaker said what he did (Sidner and Israel 1981), at the discourse level,an OCP must explain why an ICP engages in a new discourse segment at a particularjuncture in the discourse.
The latter explanation depends upon the relationship of thenew segment's DSP to the other DSPs underlying the discourse.
In Step (2aii) of theaugmentation process, G1 must thus determine whether the new SharedPlan wouldcontribute to the agents' SharedPlan for o~ or to some other plan on the stack S. Ifthe new SharedPlan does not contribute to any of the plans on the stack, then it istaken as an interruption.
If it does not contribute to the agents' SharedPlan for o~, butto another plan on the stack, one for 7 say, then G1 must also determine whether theplans that are above 7 on the stack have been completed.Case (2b) in Figure 13 models the recognition of a segment's completion.
If G1believes that G2's utterance signals the completion of the current segment, hen G1must reason whether he too believes the segment to be complete.
For that to be thecase, G1 must believe that all of the beliefs and intentions required of an FSP havebeen established over the course of the segment.
The completion of a segment may besignaled in either the linguistic structure or the intentional structure.
For example, inthe linguistic structure, cue phrases uch as "but anyway" may indicate the satisfactionof a DSP (as well as a pop of the focus space stack).
In the intentional structure, thecompletion of a segment may be signaled by the initiation of a new SharedPlan, asdescribed above.Case (2c) models the recognition of an utterance's contribution to the currentdiscourse segment.
When a speaker produces an utterance within a segment, a hearermust determine why the speaker said what he did.
Step (2c) models the hearer'sreasoning by trying to ascribe appropriate beliefs to the speaker.
These beliefs areascribed based on the hearer's beliefs about the state of the agents' SharedPlans andthe steps necessary to complete them.6.1 Modeling the Plan Augmentation ProcessFigure 13 contains a high-level specification of the process of reasoning with inten-tional structure.
It provides a framework in which to develop further mechanismsfor modeling the various steps of this process.
In this section, we present wo suchmechanisms.
The first mechanism presents a method for recognizing the initiation ofa new discourse segment (Step (2a) in Figure 13); the second describes an algorithmfor reasoning about he contribution of an utterance to the current segment (Step (2c)).These two mechanisms are central to the augmentation process, but are not complete;they each model just one aspect of their respective steps of the process.
The completespecification of these steps, as well as that of the augmentation process in general,requires further esearch, as is discussed in Section 10.6.1.1 Case (2a): Initiating a New Discourse Segment.
Step (2ai) of the augmentationprocess involves recognizing agent G2's intention that G1 and G2 form a full SharedPlanfor an act ft.
This intention may be recognized using a conversational default rule,CDRA, shown in Figure 14.
TM The antecedent of this rule consists of two parts: (la) G1must believe that G2 communicated her desire for the performance of act fl to G1,and (lb) G1 must believe that G2 believes they can together perform ft.
The secondcondition precludes the case where G2 is stating her desire to perform the act herself18 This rule extends Grosz and Sidner's (1990) original conversational default rule, CDR1.545Computational Linguistics Volume 24, Number 4(la) BEL(Gi, \[communicates(G2, Gi, Desires(G2, occurs(fl)), T)A(lb) BEL(G~, (BR~)CBAG({Gi, G2}, f~, R~), T)\], T)(2) BEL(Gi, Int.Th(G~, ESP({G1, G:}, fl)), T)Figure 14Conversational default rule CDRA.de f~ltor for G1 to perform the act.
If conditions (la) and (lb) are satisfied, then in theabsence of evidence to the contrary, G1 will believe that G2 intends that they form afull SharedPlan for ft.As given in Figure 14, CDRA is used to recognize an agent's intention based uponits desire for the performance of a particular act ft.
The rule may also be used whenan agent expresses its desire for a particular state of affairs P. In this case, the ex-pressions OCCUrS(fl) 19 and fl are replaced in Figure 14 by P and Achieve(P, {G1, G2}, T)respectively.6.1.2 Case (2c): Contributing to the Current Discourse Segment.
Case (2c) of theaugmentation process involves recognizing an utterance's contribution to the currentSharedPlan.
The SharedPlan definitions place requirements on recipes, abilities, plans,and commitments.
A SharedPlan may thus be affected by utterances containing avariety of information.
We will focus here, however, on utterances that communicateinformation about a single action fl that can be taken to play a role in the recipeof the agents' plan for o~.
We thus do not deal with utterances concerning warnings(e.g., "Do not clog or close the stem vent under any circumstances" \[Ansari 1995\]) orutterances involving multiple actions that are related in particular ways (e.g., "To resetthe printer, flip the switch."
\[Balkanski 1993\]).As with the other cases of Step (2) of the augmentation process, Step (i) of Case (c)involves ascribing a particular belief to agent G2 regarding the relationship of herutterance to the agents' plans.
For the types of utterances we are considering here,this belief is concerned with the relationship of the act fl to the objective of the agents'current plan, i.e., o~.
In particular, G2's reference to fl is understood as indicating beliefof a Contributes relation between fl and oL.
Contributes holds of two actions if theperformance of the first action plays a role in the performance of the second action(Lochbaum, Grosz, and Sidner 1990; Lochbaum 1994).
It is defined as the transitiveclosure of the D(irectly)-Contributes r lation.
One act D-Contributes to another if thefirst act is an element of the second act's recipe (Lochbaum, Grosz, and Sidner 1990;Lochbaum 1994).
2oAn agent ascribes belief in a Contributes relation irrespective of his own beliefsabout his relationship.
Once he has ascribed this belief, he then reasons about whetherihe also believes fl to contribute to o~ and in what way.
Step (2cii) of the augmentationprocess corresponds to this reasoning.
To model this step, we introduce an algorithmbased on the construction of a dynamic recipe representation called a recipe graph19 The predicate occurs(fl) is true if fl was, is, or will be performed at the time associated with fl as one ofits parameters (Balkanski 1993).20 The term "contributes" is overloaded in this paper.
The use of Contributes here refers to a relationbetween actions.
Grosz and Sidner (1986) also describe a contributes relation between DSPs that is theinverse of the dominates relation.
In addition, we have been using contributes informally to refer to theinverse of a subsidiary relationship between plans.546Lochbaum A Collaborative Planning ModelA recipe for c~ is comprised of a set of immediate constituent acts ({ill,... ,fin})and constraints ({pl,..., pro}).12An rgraph for c~ is comprised of a set of constituent acts and a set of constraints.~ ~  "'" ~q}il lJ lpTijl 7ijlFigure 15Graphical recipe and rgraph representations.12I~ l ~i ~,/~'~',N..{E1 "" e r }8 n 1 8nkor rgraph.
21 We first describe the rgraph representation a d then indicate its role inmodeling Gl'S reasoning concerning G2's utterances.Rgraphs result from composing recipes.
Whereas a recipe includes only one levelof action decomposition, an rgraph may include multiple levels.
On analogy withparsing constructs, one can think of a recipe as being like a grammar ule, while anrgraph is like a (partial) parse tree.
2a Whereas a recipe represents information about heabstract performance of an action, an rgraph represents more specialized informationby including instantiations of parameters, agents, and times, as well as multiple levelsof decomposition.
The graphical representations i  Figure 15 contrast he structure ofthese two constructs.The construction of an rgraph corresponds to the reasoning that an agent performsin determining whether or not the performance of a particular act fl makes sense giventhe agent's beliefs about recipes and the state of its individual and shared plans.
Theprocess of rgraph construction can thus be used to model the process by which agentG1 explains G2's presumed belief in a Contributes relation.
In explaining this belief,however, G1 must reason about more than just the agents' immediate SharedPlan.
In2l This terminology was chosen to parallel Kautz's.
He uses the term explanation graph or egraph for hisrepresentation relating event occurrences (Kautz 1990).
A comparison of our representation a dalgorithms with Kautz's can be found elsewhere (Lochbaum 1991, 1994).
In short, Kautz's work isbased on assumptions that are inappropriate for collaborative discourse.
In particular, Kautz assumes amodel of keyhole recognition (Cohen, Perrault, and Allen 1982) in which one agent is observinganother agent without that second agent's knowledge.
In such a situation, only actual eventoccurrences performed by a single agent are reasoned about; Kautz's representation a d algorithmsinclude no means for reasoning about hypothetical, partially specified, or multiagent actions.
Inaddition, in keyhole recognition, o assumptions can be made about the interdependence of observedactions.
Because the agent is not aware that it is being observed, it does not structure its actions tofacilitate the recognition of its motives.
A separate graph must thus be created for each observation.22 Barrett and Weld (1994) and Vilain (1990) provide further discussion of the use of parsing in planningand plan recognition.547Computational Linguistics Volume 24, Number 4Assume:PSP({G1, G2}, a),G1 is the agent being modeled,R~ is the set of recipes that G1 knows for a,H is an rgraph explaining the acts underlying the discourse up to this point,/3 is the act referred to by G2.0.
Initialize Hypothesis: If fl is the first act to be explained in the context ofPSP({G1, G2}, a), expand H by choosing a recipe from R~ and adding it to the rgraph.1.
Isolate Recipe: Let r be the subtree rooted at a in H.2.
Select Act: Choose an act fll in r such that fli can be identified with fl and has notpreviously been used to explain another act.
If no such act exists, then fail.
Otherwise,let r I be the result of identifying fl with fll in r.3.
Update Hypothesis: Let e = constraints(r') U constraints(H).
If e is satisfiable, replacethe subtree r in H by r', otherwise, fail.Figure 16The rgraph construction algorithm.particular, he must also take into account any other collaborations of the agents, aswell as any individual plans of his own.
In so doing, G1 verifies that fl is compatiblewith the rest of the acts the agents have agreed upon, as well as those G1 intends toperform himself.
23The rgraph construction algorithm is given in Figure 16.
It is based on the assump-tion that agents G1 and G2 are collaborating on an act a and models Gl's reasoningconcerning G2"s reference to an act ft.
While PSP({G1, G2}, oL) provides the immedi-ate context for interpreting G2"s utterance, an rgraph H models the remaining contextestablished by the agents' dialogue.
H represents Gl'S hypothesis as to how all ofthe acts underlying the agents' discourse are related.
To make sense of G2's utteranceconcerning t ,  G1 must determine whether fl directly contributes to a while being con-sistent with H. Steps (1) and (2) of the algorithm model the immediate xplanation oft ,  while Step (3) ensures that this explanation is consistent with the rest of the rgraph.The algorithm in Figure 16 is nondeterministic.
Step (0) involves choosing a recipefrom G(s  recipe library, while Step (2) involves choosing an act from that recipe.
Thefailures in Steps (2) and (3) do not imply failure of the entire algorithm, but ratherfailure of a single nondeterministic execution.In Step (0) of the algorithm, Gl'S hypothesis rgraph is initialized to some recipethat he knows for a.
As will be discussed in Section 6.2.3, this recipe may involvephysical actions, such as those involved in lifting a piano, as well as information-gathering actions, such as those involved in satisfying a knowledge precondition.
Atthe start of the agents' collaboration, G1 may or may not have any beliefs as to howthe agents will perform a.
If he believes that the agents will use a particular ecipe, theihypothesis rgraph is initialized to that recipe.
Otherwise, a recipe is selected arbitrarilyfrom Gl"s recipe library.
The initial hypothesis will be refined, and possibly replaced,on the basis of G2's utterances.In Step (1) of the algorithm, the recipe for a is first isolated from the remainderof the rgraph.
This recipe, r, represents Gl'S current beliefs as to how the agents aregoing to perform a.
Step (2) of the algorithm involves identifying fl with a particular23 On the basis of this reasoning, G 1 thus attributes belief in more than just a Contributes relation to G 2.
Inparticular, G1 assumes that G2 also believes that fl is compatible with the other acts the agents haveagreed upon.548Lochbaum A Collaborative Planning Modelact fli in r resulting in a new recipe r'.
If an appropriate fli can be found, it providesan explanation for G2's reference to the act ft.
If an appropriate fli cannot be found,then r cannot be the recipe that G2 has in mind for performing oz.
The algorithm thusfails in this case and backtracks to select a different recipe for 0~.
The new recipe mustaccount for fl as well as all of the other acts previously accounted for by r.Step (3) of the algorithm ensures that the recipe and act chosen to account for aand fl are compatible with the other acts the agents have already discussed in supportof a or the objectives of their other plans.
This is done by adding the constraints ofthe recipe r t to the constraints of the rgraph H and checking that the resulting setis satisfiable.
For G1 to agree to the performance of t ,  the recipe r t must be bothinternally and externally consistent.
That is, the constraints of the recipe must beconsistent themselves, as well as being consistent with the constraints of the recipesthat G1 believes the agents will use to accomplish their other objectives.
24The rgraph construction algorithm fails to produce an explanation for an act fl inthe context of a PSP for a if the algorithm fails for all of the nondeterministic possibil-ities.
This failure corresponds to a discrepancy between agent Gl's beliefs and thoseG1 has attributed to agent G2.
The failure thus indicates that further communicationand replanning are necessary.6.2 Dialogue AnalysesTo further elucidate the augmentation process, we now return to the dialogues givenin Section 1 and show that the processes presented in this section capture the proper-ties highlighted by the informal analyses given in the Introduction.
We present eachanalysis from the perspective of one of the two discourse participants.
Each analysisthus indicates the type of reasoning that is required for a system to assume the roleof that participant in the dialogue.6.2.1 Example 1: Subtask Subdialogues.
The dialogue in Figure 17 (repeated fromFigure 1) contains two subtask subdialogues.
In Section 1 we noted that an OCP mustrecognize the purpose underlying each subdialogue, as well as the relationship of eachpurpose to the preceding discourse, in order to respond appropriately to the ICP.
TheOCP's recognition of DSPs and their interrelationships is modeled by Case (2a) of theaugmentation process in Figure 13.
We illustrate its use by modeling the Apprentice'sreasoning concerning the Expert's first utterance in the second segment in Figure 17,i.e.,(2a) E: Now remove the pump.At this point in the agents' discourse, the stack S consists only of a PSP to replacethe air compressor's pump and belt.
This PSP corresponds to the overall discoursein Figure 17.
The SharedPlan corresponding to the first embedded segment has beencompleted at this point in the discourse and is thus no longer in focus.24 Another distinction between our work and Kautz's (1990) relates to Step (3) of the algorithm inFigure 16 and the use of constraints.
Whereas rgraphs include an explicit representation of constraints,Kautz's egraphs do not.
Constraints are used to guide egraph construction, but are not part of therepresentation tself.
As a result, Kautz's algorithms can only check for constraint satisfaction locally.
Inour algorithm, that would correspond tochecking the satisfiability ofa recipe's constraints beforeadding it to an rgraph, but not afterwards.
By checking the satisfiability ofthe constraint set thatresults from combining the recipe's constraints with the rgraph's constraints, the rgraph constructionalgorithm isable to detect unsatisfiability earlier than an algorithm that checks constraints only locally.549Computational Linguistics Volume 24, Number 4E: Replace the pump and belt please.~,AA~ OK, I found belt in the back.
aIs that where itshould be??
.. \[A removes belt\]It's done.E: Now remove the pump.E: First you have to remove the flywheel.?
.
.E: Now take the pump off the base plate.A: Already did.Figure 17Sample subtask subdialogues (Grosz 1974).In utterance (2a), the Expert expresses her desire that the action remove(pump(acl),{a}) be performed, where acl represents he air compressor the agents are working on.The Apprentice's reasoning concerning this utterance may be modeled using CDRA.Condition (la) of CDRA is satisfied by the communication of this utterance to theApprentice.
Condition (lb) is satisfied by the context surrounding the agents' collabo-ration.
Because the Expert is in another oom and can only instruct he Apprentice asto which actions to perform, the Expert's utterance cannot be expressing her intentionto perform the desired action herself?
In addition, because the Apprentice and Expertare both aware that the Apprentice does not have the necessary expertise to performthe action himself, the Apprentice can assume that the Expert must believe the agentscan perform the act together, thus satisfying Condition (lb) and sanctioning the de-fault conclusion, Bel(a, Int.Th(e, FSP({a, e}, remove(pump(acl), {a})))).
Thus, on the basisof the Expert's utterance and her presumed beliefs concerning the agents' capabilitiesto act, the Apprentice may reason that the Expert is initiating a new discourse segmentwith this utterance.
The purpose of this segment is recognized as:DSP3 =Int.
Th (e, FSP({a, e}, remove(pump (acl), {a } ) ) ).Once the Apprentice recognizes the DSP of this new discourse segment, he mustdetermine its relationship to the other DSPs underlying the discourse?
Subsidiary re-lationships between plans provide the basis for modeling the Apprentice's reasoning?In particular, if the Apprentice believes that a plan for removing the pump wouldfurther some other plan of the agents', then he will believe that DSP3 is dominated bythe DSP involving that other plan.As discussed in Section 5?1.1, the subsidiary relation in question in this examplederives from the constituent plan requirement of the SharedPlan definition.
The Ap-prentice will succeed in recognizing the relationship of the second subdialogue to theremainder of the discourse, if he believes that removing the pump of the air compressorcould be an act in the agents' recipe for replacing its pump and belt.
If the Apprenticedoes not have any beliefs about he relationship between these two acts, he may chooseto assume the necessary D-Contributes relation on the basis of the Expert's utteranceand the current discourse context, or he may choose to query the Expert further.The rgraph construction algorithm may be used to model the Apprentice's reason-ing.
In particular, Steps (1) and (2) of the algorithm in Figure 16 model the reasoning550Lochbaum A Collaborative Planning ModelP1(a)(b) FSP({a,e},removPSP({a,e},replace(pump(acl) & belt(acl),{a})){remove(pump(acl).
{a}),remove(belt(acl),{a})} in \[1\]Recipe(replace(pump(acl) & belt(acl ),{a}))FSP({a,e},remove(belt(acl),{a})) ~ \[3aii\].
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.A explains P3 in terms of the role itplays in completing P1, namelybringing about the conditionmarked (b))(pump(acl),{a}))Figure 18Analysis of the dialogue in Figure 17.\[3ai(E explains P2 in terms of the role it plays incompleting P1, namely bringing about thecondition marked (a)FSP({a,e},remove(belt(acl),{a})) P2The utterances of the firstsubdialogue are understood.~_q.q.d.pz_~_uced in this contextPSP({a,e},remove(purnp(ac 1),{a}))The utterances of the secondsubdialogue are understoodP3necessary for determining that a D-Contributes relation holds between two actions.
Ifthe OCP is able to infer such a D-Contributes relation, he will thus succeed in deter-mining the subsidiary relationship necessary for explaining a subtask subdialogue.
Ifthe OCP is unable to infer such a relationship, then the algorithm will fail.
This failureindicates that the OCP may need to query the ICP further about he appropriateness ofher utterance.
For example, as we noted in Section 1, if the OCP has reason to believethat the proposed subtask will not in fact play a role in the agents' overall task, thenthe OCP should communicate hat information to the ICP.
In addition, if the OCP hasreason to believe that the performance ofthe subtask will conflict with the agents' otherplans and intentions, then the OCP should communicate hat information as well.
Thelatter reasoning is modeled by Step (3) of the rgraph construction algorithm.
Step (3)ensures that the subtask is consistent with the objectives of the agents' other plans.Figure 18 contains a graphical representation f the SharedPlans underlying thediscourse in Figure 17.
It is a snapshot representing the Apprentice's view of theagents' plans just after he explains the initiation of segment (3).
Each box in the figurecorresponds to a discourse segment and contains the SharedPlan used to model thesegment's purpose.
The plan used to model DSP3 is marked P3 in this figure, whilethe plans used to model DSP1 and DSP2 are labeled P1 and P2, respectively.
We willcontinue to follow the convention of co-indexing DSPs with the SharedPlans used tomodel them in the remainder of this paper.The information represented within each SharedPlan i  Figure 18 is separated intotwo parts.
Those beliefs and intentions that have been established at the time of thesnapshot are shown above the dotted line, while those that remain to be established,but are used in determining subsidiary relationships, are shown below the line.
Be-cause the last utterance of segment (2) signals the end of the agents' SharedPlan forremoving the belt, the FSP for that act occurs above the dotted line.
The agents' planfor removing the belt is complete and thus no longer in focus at the start of segment (3).We have included it in the figure for illustrative purposes.
The index in square brackets551Computational Linguistics Volume 24, Number 4(1) User: Show me the generic oncept called "employee".
(2) System: OK. <system displays network>F User: I can't fit a new ic below it.Can you move it up?LSystem: Yes.
<system displays network>(6) User: OK, now make an individual employee conceptwhose first name is ...Figure 19A sample correction subdialogue (Sidner 1983; Litman 1985).to the right of each constituent indicates the clause of the FSP definition from whichthe constituent arose.Subsidiary relationships between plans are represented by arrows in the figure andare explained by the text that adjoins them.
Plans P2 and P3 are thus subsidiary to planP1 because of the constituent plan requirement (Clause (3aii)) of the FSP definition.These subsidiary relationships indicate that DSP2 and DSP3 are both dominated byDSP1.6.2.2 Example 2: Correction Subdialogues.
The dialogue in Figure 19 (repeated fromFigure 2) contains an embedded correction subdialogue.
As discussed in Section 5.1.2,we take the purpose underlying the entire dialogue to be modeled using a SharedPlanto modify a KL-ONE network,(P4) PSP( {u, s}, modify_network(NetPiece, DataLoc, {u, s})).We will assume the role of the System in analyzing this example.The System may have many recipes for modifying a network.
One may involvedeleting a concept from the network, one may involve changing the data in partof the network, and one may involve adding new data to the network.
These threepossibilities are depicted in Figure 20.
At the beginning of the dialogue, the Systemmay have no prior beliefs as to which of these recipes, if any, he and the User willfollow to modify the network.
The rgraph construction algorithm is used to modelthe System's reasoning and, as indicated in Step (0), will select one of these recipesnondeterministically.
If the chosen recipe fails to account for the User's utterances,then it cannot be the recipe that the User has in mind for modifying the network.
Thealgorithm will then backtrack at that point to select a different recipe for modifying thenetwork.
For illustrative purposes, we will assume that the System initially believesthat he and the user are following the first recipe in Figure 20; this recipe involvesdeleting data from the network.
The rgraph that results after the System has explainedutterance (1) is shown in Figure 21.The User's utterance in (3) indicates that she has encountered a problem withthe normal execution of the subtasks involved in modifying a network.
The System'sreasoning regarding this utterance may be modeled using CDRA.
On the basis of theUser's utterance and her presumed beliefs concerning the agents' capabilities regard-ing freeing up space on the screen, the System may reason that the User is initiating anew discourse segment with this utterance.
The purpose of this segment is recognizedas :DSPs=Int.Th (u, FSP ({ u, s}, Achieve(freespace~for (Data, below(gel)), {u, s })))552Lochbaum A Collaborative Planning Modelmodify_network(NetPiece,Loc,G,T){ type(NetPieee,kl-one_.network),type(Loc,~creen_location),l<T2}display(NetPiece,G 1,T1) delete(Data, Loc,G2,T2)modify network(NetPiece,Loc,G,T){ type(NetPiece,kl-one_network),type(Loe,screen_iocation),l<T2}display(NetPiece,G 1,T1 ) change(Data, Loc, G2,T2)modify_network(NetPiece,Loc,G,T){ ttype(NetPiece,kl-one network),type(Loc,screen location),mpty(Loc), freespacefor(Data,Loc)l<T2}display(NetPiece,G 1,T1 ) put(Data, Loc,G2,T2)Figure 20Recipes for modifying a network.modffy_network(gel,Loc,{ u,s }){ type(gel ,M-onenetwork),e(Loc,screen_location) }display(gel, {s}) delete(Data, Loc, { u })Figure 21Initial rgraph explaining utterances (1)-(2) of the dialogue in Figure 19.where gel represents "the generic concept called 'employee'."
To explain the User'sinitiation of the subdialogue, the System must determine how the SharedPlan in DSP5will further the agents' plan in (P4).The System's current beliefs as to how the agents will modify the network, asrepresented by the rgraph in Figure 21, do not provide an explanation for the User'sutterance in (3).
The System's recipe does not include any type of "fit" act.
The rgraphconstruction algorithm thus fails at this point and backtracks to nondeterministicallyselect a different recipe for modifying the network.
Suppose that this time the thirdrecipe in Figure 20 is selected; this is the recipe that includes adding data to thenetwork.
The rgraph that results from using this recipe to explain the User's firstutterance is shown in Figure 22.
This new rgraph also provide an explanation for theUser's utterance in (3); the "fit" act referred to by the User corresponds to the "put" actin the rgraph.
In addition, the constraints of the recipe, along with the requirementsof the ability operators, provide the explanation for the new discourse segment.As discussed in Section 5.1.2, an agent G's ability to perform an act fl depends inpart on its ability to satisfy the constraints of the recipe in which fl is a constituent.Thus, to perform the act put(Data, below(gel), {u}), the User must be able to satisfythe constraints empty(below(gel)) and freespace_for( Data, below(gel)).
The need to satisfythe latter constraint provides the System with an explanation for DSPs.
In particular,553Computational Linguistics Volume 24, Number 4modify_network(gel ,L e,{ u,s })~ { type(gel,kl-one_network),ttype(Loc,screen_location),empty(Loc),freespace_for(Dat,a, Loc)}display(ge 1, { s } ) put(Data,Loc, { u } )Figure 22Second rgraph explaining utterances (1)-(2) of the dialogue in Figure 19.P4(a)PSP({u,s},rnodify_network(gel,below(gel),{u,s})){{display(gel ,{s}), put(Data,below(gel),{u})}, \[111{empty(below(gel)), freespace_for(Data,below(gel))}} in |/.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
d/BCBA(u,put(Data,below(gel),{u}),R, \[2ai\] I{empty(belo ~(gel)),freespace for(Data,below(gel))}) JU engages S in P5 becauseshe needs to satisfy (a)S explains P5 in terms of the role it plays in completing P4,namely bnnging about the condition marked (a)...C.~ PSP({u,s},Achieve(freespace_for(Data, below(gel)),{u,s}))| move(gel,up,{s}) in \[1\]Recipe(Achieve(f reespace_for(Data,below(gel )),{u,s}))Utterances (4)-(5) are understood and produced in this contextFigure 23Analysis of the dialogue in Figure 19.P5the System can reason that the User initiated the new discourse segment in order tosatisfy one of the ability requirements of the agents' SharedPlan to modify the network.The SharedPlan in DSP5 is thus subsidiary to that in (P4) by virtue of the BCBArequirements of the latter plan.
Figure 23 summarizes our analysis of the dialogue.Whereas ubtask subdialogues are explained in terms of constituent plan requirementsof SharedPlans (Clause (3aii)), correction subdialogues are explained in terms of abilityrequirements (Clause (2ai)).Once the System recognizes, and explains, the initiation of the new segment, it willinterpret the User's subsequent u terances in the context of its DSP, rather than the pre-vious one.
It will thus understand utterance (4) to contribute to freeing up space on thescreen, rather than to modifying the network.
This reasoning is modeled by Case (3a)of the augmentation process as follows: First, on the basis of its explanation of DSP5,the System will take the agents to have a PSP for the act Achieve(freespace.for(Data, be-low(gel)), {u, s}).
This plan is marked (P5) in Figure 23 and is pushed onto the stack Sabove the plan in (P4).
As a result, the System will now take the agents to be focusedon the plan in (P5), rather than that in (P4), and thus will interpret he User's subse-quent utterances in terms of the information they contribute towards completing theplan in (P5), rather than that in (P4).The User's utterance in (4) makes reference to an act move(gel, up, {s}).
Using thergraph construction algorithm, this act is understood to directly contribute to the objec-tive of the plan in (P5), i.e., Achieve~freespace~or(Data, below(gel)), {u, s}).
The resultingrgraph is shown in Figure 24.
This rgraph provides an explanation for utterance (4) inthe context of all of the acts involved in the agents' plans.554Lochbaum A Collaborative Planning Modelmodify_network(gel,below(gel), { u,s }~ { type(gel,kl-one twork),type(below(ge 1),screenJocation),empty(below(gel)),freespace foffData,below(ge 1 )) }display(ge 1,{ s }) put(Data,below(gel), { u })IAchieve(freespace_for(Data, below(gel )),{ u,s 1)Imove(gel,up,{s})Figure 24Rgraph explaining utterances (1)-(4) of the dialogue in Figure 19.As noted in Section 1, the System's response to the User's request in (4) shouldtake the context of the agents' entire discourse into account and not simply the con-text of freeing up space on the screen.
In particular, the System should not clearthe currently displayed network from the screen to help the User perform the taskof putting up some new data, but rather should leave the displayed network visi-ble.
The discourse context modeled by the SharedPlans in (P4) and (P5), as well asthe rgraph in Figure 24, enables the System to respond correctly.
In particular, byexamining the plans currently in focus and determining what needs to be done tocomplete them, the System can reason that it should perform an act in support ofAchieve(freespaceqCor(Data, below(gel)), {u, s}).
The System will most likely select he re-quested act of moving gel up, but if it decides to modify that act in some way or toselect a different act, the new act must be compatible with the other acts the agentshave agreed upon.
By inserting the new act into the rgraph and determining thatthe resulting rgraph constraints will not be violated by this addition, the System canensure that its response is in accord with the larger discourse context.6.2.3 Example 3: Knowledge Precondition Subdialogues.
The dialogue in Figure 25(repeated from Figure 3) contains two embedded knowledge precondition subdia-logues.
We will assume the role of the Network Presenter, NP, in analyzing this ex-ample.The overall purpose of the dialogue may be represented as:DSP6 = Int.Th(nm, FSP( {nm, np}, maintain(node39, {nm, np})))and can be recognized on the basis of NM's utterance in (1) and CDRA.
The purposeof the first subdialogue in Figure 25 can be represented as:DSP7 = Int.Th(np,FSP({nm, np},Achieve( has.recipe( {nm, np } , maintain(node39, { nm, np } ) , R ) , { nm, np}))).This first subdialogue is initiated by agent NP, the agent whose reasoning we aremodeling.
We must thus account for NP's generation of an utterance in this example,rather than his interpretation of another agent's utterance.
As will be discussed inSection 10, the use of SharedPlans in generation is an area for future research; however,the basic principles used in interpretation apply here as well.
The current state of theagents' plans provides the basis for an agent's communication.DSP7 represents NP's intention that the agents determine a means of divertingnetwork traffic.
As discussed in Section 5.1.3, for a group of agents G to have a col-555Computational Linguistics Volume 24, Number 4(1) NM: It looks like we need to do some maintenance onnode39.
(2) NP: Right.
(3) How shall we proceed?
(4) NM: Well, first we need to divert he traffic to another node.
(5) NP: Okay.
(6) Then we can replace node39 with a higher capacity switch.
(7) NM: Right.
(8) NP: Okay good.FNM:  nodes could we divert he traffic to?
WhichI (10) NP: \[puts up diagram\]  (11) ode41 looks like it could temporarily handle the extra load.
( I~NM:  I agree.
(13) Why don't you go ahead and divert he traffic to node41and then we can do the replacement.
(14) NP: Okay.
(15) \[NP changes network traffic patterns\](16) That's done., ?
,Figure 25Sample knowledge precondition subdialogues.
(Adapted from Lochbaum, Grosz, and Sidner\[1990\].
)laborative plan for an act a, the group must have mutual belief of a recipe for o~.
Itis this requirement that leads NP to initiate the first subdialogue; deciding upon ameans of performing the objective of the agents' collaboration is a necessary first stepto furthering that collaboration.
The plan in DSP7 to agree on a recipe for maintainingnode39 thus contributes to the plan in DSP6 to do the maintenance, and is thereforesubsidiary to it.
Figure 26 provides a graphical representation f this relationship.Once NM agrees to the subsidiary collaboration, either explicitly or implicitly asin utterance (4), NP will assume that the agents have a partial SharedPlan to obtainthe recipe:(P7) PSP( {nm, np},Achieve( has.recipe( {nm, np} , rnaintain( ode39 , { nrn, np}), R), { nm, np}))NP will thus produce his next utterances in the context of the SharedPlan in (P7),rather than that in DSP6 and will assume that NM will do the same.To make sense of NM's utterance in (4), NP must provide an explanation for itin the context of the agents' SharedPlan in (P7).
The rgraph construction algorithm isused in modeling NP's reasoning.
Whereas in the case of a subtask subdialogue, thealgorithm makes uses of recipes for performing a subtask, in the case of a knowledgeprecondition subdialogue, it makes use of recipes for satisfying a knowledge precon-dition.
Figure 27 contains two recipes an agent might know to obtain a recipe for anact o~.
The first is a single-agent recipe that involves looking up a procedure for a ina manual.
The second recipe is a multiagent recipe that involves the agents commu-nicating to come to agreement about the acts and constraints that will comprise theirrecipe for o~.We use these recipes to model NM's reasoning concerning utterance (4) as follows:In Step (0) of the rgraph construction algorithm, a recipe for the act Achieve(has.recipe({nm, np}, maintain(node39, {nm, np}), R)) is first selected from NP's recipe library.
Forillustrative purposes, we will assume that the second recipe in Figure 27 is selected.556Lochbaum A Collaborative Planning ModelNP engages NM in P7 because thecondition marked (a) needs to besatisfiedPSP({n m,np},A?hleve(has.reclpe({nrn,np},malntaln(node39,{nm,n p}),R),{rim,rip}))Utterances (4)-(8) are understcod and produced in this contextFigure 26Analysis of the first subdialogue in Figure 25.Achieve(has.recipe(G, t,R T),G,T)I {BeI(G, R ~ Recipes(a),T)}look_up(G,R,Manual,T)P7Achieve(has.recipe({ G1, G2},0t,R,T),{G1 , 2} ,T)I {MB({G1,G2}, Re Recipes(~),T),MB({G1 ,G2} Exists R\[ {13i , Oj} ~ Re Recipes(o0\],T)lcommunicate(Gk, m, {13i, pj}, T)Figure 27Recipes for obtaining recipes.Next, we try to identify NM's communicative act in utterance (4) with some act in thatrecipe, and succeed by appropriately instantiating the communicate act.
NP is thus ableto make sense of NM's utterance based on his beliefs about ways of obtaining recipes.Now, however, he must decide whether the act that NM is proposing to include aspart of their recipe for maintaining node39 is compatible with his beliefs about waysof performing that act.
This reasoning is modeled by Step (3) of the augmentationprocess in which the constraints of the rgraph are checked for satisfiability.
The recipefor obtaining recipes that was selected in Step (0) of the algorithm indicates that tohave a recipe for maintaining node39, the agents must have mutual belief that someset of acts and constraints constitute a recipe for that act.
If NP does not believe thatthe act divert_traffic(Nodel, Node2, G) should play a role in maintaining node39, thenthe constraint will not hold and the algorithm will fail.
NP will then communicate hisdissent o NM and possibly propose an alternative act.
In this instance, however, NPis in agreement with NM, as evidence by his "Okay" in utterance,(5).
The rgraph thatresults from his reasoning is shown in Figure 28.To produce utterance (6), NP must reason about the state of the agents' Shared-Plans and determine what needs to be done to complete them.
At this point in thediscourse, the agents are focused on obtaining a recipe for maintaining node39 andhave agreed that the act of diverting network traffic will be included in that recipe.NP might thus propose the performance of another act as part of their recipe.
He doesthis in utterance (6).
In utterance (7), NM agrees to the inclusion of that act.557Computational Linguistics Volume 24, Number 4Achieve(has.recipe( { nm,np },maintain ( ode39, { nm,np }),R), { nm,np }){ MB({ nm,np }, Re Recipes(maintain(node39,{ nm,np })),MB({nm,np}Exists R \[ { { divert_traffic(node39,ToNode,G 1 ) },{ ty pe(ToNode,node) } }Re Recipes(maintain(node39, { nm,np }))\]) }communicate(nm,np, { { divert traffic(node39,ToNode,G 1 ) }, {type(ToNode,node) } } )Figure 28Rgraph explaining utterances (1)-(5) of the dialogue in Figure 25.maintain(node39, { nm, np }){ type(node39,node),type(ToNode, node),ype,switeh_type), T1 < T2}diverttraffic(node39,ToNode,G 1 ,T1)  replace_switch(node39,SwitchType,G2,T2)Figure 29Rgraph explaining utterances (1)-(8) of the dialogue in Figure 25.To produce utterance (8), NP must once again reason about the state of the agents'plans.
If he believes that diverting network traffic from node39 and then replacing thatnode with a higher capacity switch will result in maintaining node39, then he willbelieve that the agents have completed their SharedPlan in (P7) to obtain a recipe formaintaining the node.
His utterance in (8) indicates that this is the case.
Unless agentNM indicates her disagreement, NP will thus assume that the agents have completedtheir SharedPlan in (P7) and will update his beliefs accordingly.
First, he will removethe SharedPlan in (P7) from further consideration; the agents have completed thatplan and have thus satisfied the corresponding discourse purpose.
The plan in (P7)is thus popped from NP's representation of the intentional structure.
Second, NP willupdate his beliefs about the dominating plan in DSP6 based on the knowledge gainedduring the subdialogue.
In particular, the recipe that was decided upon to maintainnode39 will be added to the plan and the rgraph will be updated accordingly.
Figure 29contains the rgraph representing the new discourse context after utterance (8).Utterance (9) indicates the initiation of a new discourse segment, the purpose ofwhich can be recognized as:DSP8 = Int.Th(nm,FSP({nm, np},Achieve( has.sat.descr( { nm, np } , ToNode, .T ( divert_traffic, ToNode ) ) ,(nm, np}))).using CDRA.
As with the other types of subdialogues discussed above, once agent NPrecognizes this DSP, he must determine its relationship to the other DSPs underlyingthe discourse.
In this instance, the only other DSP is that underlying the entire dis-course.
To model agent NP's reasoning, we must thus determine the relationship ofthe SharedPlan in DSP8 to that in DSP6.
The knowledge precondition requirements ofthe latter plan provide that explanation.558Lochbaum A Collaborative Planning ModelP6 PSP({nm,npl,maint ain(node39,{nm,np})) I{{diverLtraffic(node39,ToNede,G1,T1), \[1\] Jreplace_switch( node39,Switch Type,G2, T 2) }, I(type(node39,node),type(ToNode, node), I.... h_ ZI_: }_ iy_?.
......
I(a) BCBA(Gl,dived traffie(node39,ToNode,G1,T1),R) \[2ai\]l1'INM engages NP in P8 because NP explains P8 in terms of the role it playsthe condition marked (a) needs in completing P6, namely bringing about theto be satisfied condition marked (a)I PSP({nm,np},Achieve(hae.eat.descr(G1,ToNode,F(divert_traffic,ToNode)),{nm,np}))Utterances (10)-(12) are understood and produced in this contextFigure 30Analysis of the second subdialogue in Figure 25.Achieve(has.s at.descr(O,pi, F(~,pi),T),{G,G2},T)I {has.sat.descr(G,D,F( ~  p):I3}communicate(G2, , D, T)Figure 31A recipe for obtaining a parameter description.P8As discussed in Section 5.1.3, an agent G's ability to perform an act fl depends inpart on its ability to identify the parameters of ft.
Thus to perform the act divert_traffic(node39, ToNode, G!)
as part of the agents' Shared.Plan to maintain node39, the agentsmust be able to identify the ToNode parameter of the act.
The need to identify thisparameter thus provides NP with an explanation for DSP8.
In particular, NP can reasonthat NM initiated the new discourse segment in order to satisfy one of the abilityrequirements of the agents' SharedPlan to maintain node39.
The SharedPlan in DSPsis thus subsidiary to that in DSP6 by virtue of the BCBA requirements of the latterplan.
Figure 30 summarizes our analysis of the subdialogue.Once NP recognizes, and explains, the initiation of the new segment, he will pro-duce his subsequent utterances in the context of its DSP, rather than the previousone, and will expect NM to do the same.
The rgraph construction algorithm is usedin modeling NP's reasoning.
Whereas in the previous example, the algorithm makesuse of recipes for obtaining recipes, in this case it makes use of recipes for obtainingparameter descriptions.
Figure 31 contains an example of such a recipe.
The recipe isderived from the definition of has.sat.descr in Figure 8 and represents that an agent Gcan bring about has.sat.descr of a parameter Pi by getting another agent G2 to give it adescription D of pi.
The recipe's constraints, however, require that D be of the appro-priate sort, according to the constraint .T(6, Pi), for the identification of the parameterto be successful (Appelt 1985; Kronfeld 1986, 1990; Hintikka 1978).Given the discourse context represented by Figure 30 then, NP should respondto NM's utterance in (9) on the basis of his beliefs about ways in which to identifyparameters.
For example, if NP knows the recipe in Figure 31, then he might respondto NM by communicating some node description to her.
As we noted in Section 1,however, the description that NP uses must be one that is appropriate for the currentcircumstances.
In particular, NP should respond to NM with a description that will559Computational Linguistics Volume 24, Number 4enable both of the agents to identify the node for the purposes of diverting networktraffic.
The rgraph in Figure 29 and the constraints of the recipe in Figure 31 providethe necessary context for modeling NP's behavior.
Because NP knows that the agentsare trying to divert network traffic as part of maintaining node39, as represented bythe rgraph in Figure 29, he should first choose a node that is appropriate for thatcircumstance.
For example, he might choose a node that is spatially close to node39,rather than one that, while lightly loaded, is more distant.
After selecting the node, NPshould then choose a means of identifying it for NM.
For example, he might presenther with a diagram of the network and then tell her how to identify the particular nodeon the diagram; NP's response in utterances (10) and (11) takes this form.
It would notbe appropriate, however, for NP to respond to NM with some internal node name, orwith a description like "the node with the lightest raffic," unless he believed that NMcould identify the node on the basis of that description.
The constraints of the recipe inFigure 31 model this requirement.
They represent that the description communicatedby an agent should be one that will allow the other agent to identify the object inquestion for the purpose of the act to be performed.7.
Comparison with Grosz and Sidner's TheoryGrosz and Sidner (1990) have argued that a theory of DSP recognition depends uponan underlying theory of collaborative plans.
Although SharedPlans provide that lattertheory, the connection between SharedPlans and DSPs was never specified.
In thispaper, we have presented a SharedPlan model for recognizing DSPs and their in-terrelationships.
We now show that this model satisfies the requirements set out byGrosz and Sidner's (1986) theory of discourse structure.
We first discuss the processby which intentional structure is recognized.
Next, we discuss the way in which inten-tional structure interacts with the attentional state component of discourse structure.And finally, we discuss the contextual use of intentional structure in interpretation.7.1 Recognizing Intentional Structure7.1.1 Recognizing Discourse Segments and their Purposes.
In their paper on dis-course structure, Grosz and Sidner give several examples of the types of intentionsthat could serve as DSPs (Grosz and Sidner 1986, 179):1.
Intend that some2.
Intend that some3.
Intend that some4.
Intend that some5.
Intend that someagent intend to perform some physical task.agent believe some fact.agent believe that one fact supports another.agent intend to identify an object.agent know some property of an object.Intentions uch as these, as well as segment beginnings and endings, might be rec-ognized on the basis of linguistic markers, utterance-level intentions, or knowledgeabout actions and objects in the domain of discourse (Grosz and Sidner 1986).In our model, DSPs take the form Int.Th(ICP, FSP({ICP, OCP},fl)).
This type ofDSP addresses several problems with the above examples--problems that motivatedGrosz and Sidner's (1990) subsequent work on SharedPlans--namely the case of oneagent intending another to do something and the so-called master/slave assumption.We recognize DSPs using the conversational default rule, CDRA.
This rule provides ameans of recognizing the initiation of new segments and their purposes based on the560Lochbaum A Collaborative Planning Modelpropositional content of utterances.
Although this use of CDRA is admittedly imited--it requires an ICP to communicate he act that it desires to collaborate on at the outsetof a segment--other sources of information, such as those cited above, could alsobe incorporated into the model to aid in the recognition of new segments and theircorresponding SharedPlans.SharedPlans can also be used in recognizing the completion of discourse segments.Case (2b) of the augmentation process in Figure 13 outlines the required reasoning.A discourse segment is complete when all of the beliefs and intentions required tocomplete its corresponding SharedPlan have been established.
This use of SharedPlansalso appears at first glance to be of limited use---the mental attitudes required of afull SharedPlan may not all be explicitly established over the course of a dialogue orsubdialogue.
However, the OCP may be able to infer the completion of a SharedPlan,and thus the corresponding segment, in combination with information from othersources.
For example, suppose an OCP has some reason to expect he end of a segmentbased on a linguistic signal such as an intonational feature (e.g., as described by Groszand Hirschberg \[1992\]).
If additionally the OCP is able to ascribe the various mentalattitudes "missing" from the SharedPlan that corresponds to that segment, hen theOCP has further evidence for the segment boundary.
These mental attitudes may beascribed on the basis of those of the OCP's beliefs that are in accord with the mentalattitudes comprising the SharedPlan (Pollack 1986a; Grosz and Sidner 1990).7.1.2 Recognizing Relationships between Discourse Segments.
Once an OCP recog-nizes the initiation of a new discourse segment, it must determine the relationshipof that segment's DSP to the other DSPs underlying the discourse (Grosz and Sidner1986).
In our model, relationships between SharedPlans provide the basis for deter-mining the corresponding relationships between DSPs.
An OCP must determine howthe SharedPlan used to model a segment's DSP is related to the other SharedPlansunderlying the discourse.
The information that an OCP considers in determining thisrelationship is delineated by the beliefs and intentions that are required to completeeach of the other plans.
In this way, our model provides a more detailed account ofthe relationships that can hold between DSPs than did Grosz and Sidner's originalformulation.One DSP dominates another if the second provides part of the satisfaction of thefirst.
In our model, subsidiary relationships between SharedPlans provide a meansof determining dominance relationships between DSPs.
If one plan is subsidiary toanother, then the DSP that is modeled using the first plan is dominated by that modeledusing the second.
One DSP satisfaction-precedes another if the first must be satisfiedbefore the second.
This relationship corresponds to a temporal dependency betweenSharedPlans.
When one SharedPlan must be completed before another, the DSP thatis modeled using the first satisfaction-precedes that modeled using the second.7.2 Relationship to Attentional StateThe attentional state component ofdiscourse structure isan abstraction ofthe discourseparticipants' focus of attention; it is modeled using a stack of focus spaces, one foreach segment.
Each focus space contains its segment's DSP, as well as those objects,properties, and relations that become salient over the course of the segment.
One ofthe primary roles of the focus space stack is to constrain the range of DSPs to which anew DSP can be related; a new DSP can only be dominated by a DSP in some spaceon the stack.In our model, a segment's focus space contains a DSP of the form Int.Th(ICP, FSP({ICP, OCP}, fl)).
The operations on the focus space stack depend upon subsidiary rela-561Computational Linguistics Volume 24, Number 4tionships between SharedPlans in the same way that Grosz and Sidner (1986) describethe operations as depending upon DSP relationships.
As each SharedPlan correspond-ing to a discourse segment is completed, the segment's focus space is popped fromthe stack.
Only those SharedPlans in some space on the stack are candidates for sub-sidiary relationships.
The use of the SharedPlan stack S in the augmentation processof Figure 13 reflects the operations of the focus space stack.7.3 The Contextual Role of Intentional StructureAn utterance of a discourse can either begin a new segment of the discourse, com-plete the current segment, or contribute to it (Grosz and Sidner 1986).
Each of thesepossibilities is modeled by a separate case within the augmentation process given inFigure 13.
The initiation and completion of discourse segments was discussed in Sec-tion 7.1.
Our discussion here is thus restricted to the case of an utterance's contributingto a discourse segment.Under Grosz and Sidner's theory, each utterance of a discourse segment con-tributes some information towards achieving the purpose of that segment.
In ourmodel, each utterance is understood in terms of the information it contributes towardscompleting the corresponding SharedPlan.
The FSP definition in Figure 5 constrainsthe range of information that an utterance of a segment can contribute towards thesegment's SharedPlan.
Hence, if an utterance cannot be understood as contributinginformation to the current SharedPlan, then it cannot be part of the current discoursesegment.
That is, the utterance must begin a new segment of the discourse or completethe current segment, but it cannot contribute to it.
In this way, our model provides amore detailed account of the role that intentional structure plays as context in inter-preting utterances than did Grosz and Sidner's original formulation.Because ach utterance of a discourse segment contributes some information to-wards the purpose of that segment, he segment's DSP may not be completely deter-mined until the last utterance of the segment.
However, as Grosz and Sidner (1986)have argued, the OCP must be able to recognize initially at least a generalization ofthe DSP so that the proper moves of attentional state can be made.
Although CDRAprovides a limited method of recognizing new segments and their purposes, it doesconform to this aspect of Grosz and Sidner's theory.
In particular, the initial purpose ofa segment, as recognized by CDRA, is quite generally specified; it consists only of theintention that the agents form a SharedPlan.
However, as the utterances of a discoursesegment provide information about the details of that plan, the segment's purposebecomes more completely determined.
In particular, the purpose comes to include themental attitudes required of a full SharedPlan and established by the dialogue.
Ad-ditionally, although the objective of the agents' plan may only be abstractly specifiedwhen it is initially recognized, it too may be further efined by the utterances of thesegment.8.
Comparison with Previous Plan-Based ApproachesEarly work on plan recognition i  discourse (Allen and Perrault 1980; Cohen, Perrault,and Allen 1982) focused on the problem of reasoning about single utterances.
25 Sub-sequent work (Sidner and Israel 1981; Sidner 1983, 1985; Carberry 1987) extended theearlier approaches to recognize speaker's intentions across multiple utterances.
All of25 More recent work in the area of single utterance r asoning includes that of Cohen and Levesque (1990)and Perrault (1990).
Their work provides a detailed mental state model of speech act processing and isthus focused at a different level of granularity han the work discussed inthis paper.562Lochbaum A Collaborative Planning Model(1) User: Show me the generic concept called "employee".
(2) System: OK. <system displays network>(3) User: I can't fit a new ic below it.
(4) Can you move it up?
(~System: Yes.
<system displays network>(6) User: OK, now make an individual employee conceptwhose first name is ...Figure 32A sample correction subdialogue (Sidner 1983; Litman 1985).these approaches were based on a data-structure view of plans and were designedto recognize utterance-level intentions.
More recent work (Litman 1985; Litman andAllen 1987; Lambert and Carberry 1991; Ramshaw 1991) has been concerned withthe problems introduced by discourses containing subdialogues.
However, the morerecent work has followed in the tradition of the previous work and as a result contin-ues to produce an utterance-to-utterance-based analysis of discourse, rather than onebased on discourse structure.
We now review these approaches and show that theyare aimed at recognizing a different type of intention than that discussed in this paper.8.1 The Approach of Litman and AllenTo model clarification and correction subdialogues, Litman and Allen propose the useof two types of plans: discourse plans and domain plans (Litman 1985; Litman andAllen 1987).
Domain plans represent knowledge about a task, while discourse plansrepresent conversational relationships between utterances and plans.
For example, anagent may use an utterance to introduce, continue, or clarify a plan.In Litman and Allen's model, the process of understanding an utterance ntailsrecognizing a discourse plan from the utterance and then relating that discourse planto some domain plan; the link between plans is captured by the constraints of thediscourse plan.
For example, under Litman and Allen's analysis, utterance (3) of thedialogue in Figure 32 (repeated from Figures 2 and 19) is recognized as an instanceof the CORRECT-PLAN discourse plan; with the utterance, the User is correcting adomain plan to add data to a network.Litman and Allen use a stack of plans to model attentional spects of discourse.The plan stack after processing utterance (3) is shown in Figure 33.
The CORRECT-PLAN discourse plan on top of the stack indicates that the user and system are correct-ing a problem with the step labeled D1 in PLAN2 (the DISPLAY act of the ADD-DATAdomain plan) by inserting a new step into PLAN2 (?newstep) before the step labeledF1 (the FIT step).The plan stack after processing the User's subsequent utterance in (4) is shownin Figure 34.
The IDENTIFY-PARAMETER discourse plan indicates that utterance (4)is being used to identify the ?newstep arameter of the CORRECT-PLAN discourseplan.The boxes in Figures 33 and 34 do not correspond to discourse segments, butrather to individual utterances.
PLAN5 in Figure 34 was introduced by utterance (4)in the dialogue, PLAN4 by utterance (3).
The two utterances are linked together bythe parameter M1, corresponding to the MOVE act in PLAN2.
Although this analy-sis serves as a method of relating the two utterances, it provides only an utterance-to-utterance-based model of discourse processing.
Intuitively, utterances (3)-(5) as a563Computational Linguistics Volume 24, Number 4PLAN4CORRECT-PLAN(user,system, DI,?newstep,Fi,PLAN2)I REQUEST(use~,system, Fl)SURFACE-INFORM(user, system,~CANDO(user,Fi))where STEP(Di,PLAN2)STEP(Fi,PLAN2)AFTER(Di,FI)AGENT(?newstep,system)-CANDO(user, Fi,PLAN2)MODIFIES(?newstep,DI)ENABLES(?newstep,Fi)PLAN2ADD-DATA(user,El,?ic,belowEl)CONSIDER-ASPECT(USer,E1) ?newstep Fl:FIT(system,?ic,belowEl)DI:DISPLAY(system,user,E )Figure 33Plan stack after processing utterance (3) of the dialogue in Figure 32 (Litman 1985).PLAN5IDENTIFY-PARAMETER(user, system,MI,CI,PLAN4)I INFORMREF(user,syst~m,MI,WANT(user,Ml))REQUEST(USer, system,M1)ISURFACE-REQUEST(user,system,INFORMIF(system,user,CANDO(system, Ml)))where PARAMETER(Mi,Ci)STEP(CI,PLAN4)PARAMETER(MI,WANT(user,MI))WANT(system, PLAN4)PLAN4PLAN2CI:CORRECT-PLAN(user,system, DI,Mi,FI,PLAN2) IREQUEST(use~,system, Fl)SURFACE-INFORM(user, system,-CANDO(user, FI))ADD-DATA(user,EI,?ic,belowEI)CONSIDER-.~.~PE~m,?ic,belowE1)I MI:MOVE(system, Ei,up)DI:DISPLAY(system, user,EI)Figure 34Plan stack after processing utterance (4) of the dialogue in Figure 32 (Litman 1985).564Lochbaum A Collaborative Planning Modelwhole are concerned with correcting a problem; utterance (3) identifies the problem,while utterance (4) suggests a method of correcting it.
Under Litman and Allen's anal-ysis, however, utterance (3) is used to correct a problem and utterance (4) is usedto identify a parameter in a discourse plan.
This type of analysis cannot capture thecontribution of a subdialogue to the overall discourse in which it is embedded.
Eachutterance is simply linked to one that precedes it, irrespective of how the utterancesaggregate into segments.In contrast to Litman and Allen's approach, our approach accurately reflects thecompositional structure of discourse; utterances are understood in the context of dis-course segments, and segments in the context of the discourse as a whole.
26 Our anal-ysis of the dialogue in Figure 32 was discussed in Section 6.2.2 and is summarizedby Figure 23.
Under our analysis, utterance (3) introduces a new discourse segment,the purpose of which is to satisfy a constraint that there be enough free space on thescreen to add a new concept.
This new segment is recognized and explained based onthe ability requirements of SharedPlans.
Utterance (4) of the dialogue is understood inthe context of this new discourse segment.
In particular, the act of moving the genericconcept up is understood as a means of satisfying the constraint.In more recent work, Litman.and Allen have augmented their model with a notionof "discourse intentions."
"Discourse intentions are purposes of the speaker, expressedin terms of both the task plans of the speaker (the domain plans) and the plansrecursively generated by these plans (the discourse plans)" (Litman and Allen, 1990,376).
For example, the discourse intention underlying utterance (4) can be glossed as:User intends that System intends thatSystem identify the ?newstep arameterof the CORRECT-PLAN discourse plan.Because Litman and Allen's discourse and domain plans are recognized on the basisof a single utterance, their discourse intentions are actually utterance-level intentions,and not the type of discourse-level intentions discussed in this paper.8.2 Other ApproachesLambert and Carberry (1991) have revised Litman and Allen's dichotomy of plans intoa trichotomy of discourse, problem-solving, and domain plans.
Their discourse plansrepresent means of achieving communicative goals, while their problem-solving plansrepresent means of constructing domain plans.
The Build-Plan operator in Figure 9 isan example of a problem-solving plan; it is used to represent the process by whichtwo agents build a plan for one of them to do an action.
The body of the operatorrequires that the agents (i) Build-Plans for the subacts of that action and (ii) Instantiate-Var(iable)s of those subacts.In Lambert and Carberry's model, the process of understanding anutterance n-tails recognizing a tripartite structure of plans.
Beginning from the surface-level formof an utterance, their system recognizes plans on the discourse level until a plan at thatlevel can be linked to one on the problem-solving level; plans on the problem-solvinglevel are then recognized until one can be linked to a plan on the domain level; furtherplans may then be recognized on that level.26 Although there may be several possible segmentations of a discourse, just as there may be severalpossible parses of a sentence, there is general agreement that utterances do cluster into segments.
Thepoint here is that our analysis reflects this segmentation, whereas Litman and Allen's isutterance-to-utterance bas d and thus does not.565Computational Linguistics Volume 24, Number 4As a model of subdialogue understanding, Lambert and Carberry's approach suf-fers from problems imilar to that of Litman and Allen's.
In particular, Lambert andCarberry's analysis is still utterance-to-utterance based; subdialogues are not recog-nized as separate units, nor is a subdialogue's contribution to the discourse in whichit is embedded recognized.
This is also true of Lambert and Carberry's (1992) morerecent work on modeling negotiation subdialogues.
Although Lambert and Carberryemphasize the importance of recognizing the initiation of negotiation subdialogues,and work through an example involving an embedded negotiation subdialogue, theydo not indicate how these subdialogues are actually recognized as such.
The onlypossibility hinted at in the text (i.e., that the discourse act Address-Believability ac-counts for them) results in a discourse segmentation that does not accurately reflectthe purposes underlying their sample dialogue.Figures 35 and 36 contain the sample dialogue used by Lambert and Carberry(1992).
In Figure 35, the dialogue is segmented as suggested by Lambert and Carberry'sanalysis, while in Figure 36 it is segmented to more accurately reflect the purposesunderlying the discourse.
The subdialogues marked (b) and (d) in Figure 36 are bothinitiated by $1 and are each concerned with a different aspect of the accuracy of S2'sutterance in (6).
Segments (b) and (d) are thus siblings both dominated by segment (a)in Figure 36.
Under Lambert and Carberry's analysis, however, these two subdialoguesare not recognized as separate units.
That they should be can be seen by the coherentdiscourses that remain if either is removed from the dialogue.In addition, although the process of plan construction provides an important con-text for interpreting utterances, trying to formalize this mental activity under a data-structure approach results in a model that conflates recipes and plans (Pollack 1990).For example, each of Lambert and Carberry's domain act operators requires as a pre-condition that the agent have a plan to use that operator to perform the act.
Thatrequirement, however, results in the paradoxical situation whereby a recipe for an acto~ requires having a plan for o~ that uses that recipe.
As another example, the Build-Plan operator in Figure 9 requires as a precondition that each agent know the referentsof the subactions that one of the agents needs to perform to accomplish o~.
However,considering that determining how to perform an act is part of constructing a plan toperform that act, it is odd that a recipe for building a plan for o~ requires knowing thesubactions of o~ as a precondition of its use.
The fact that these inconsistencies do notseem to pose a problem for Lambert and Carberry's model is testament to its data-structure nature; the plan chaining behavior of their reasoner on the various types ofoperators is such that no circularities arise.Ramshaw (1991) has augmented Litman and Allen's two types of plans with adifferent hird type, exploration plans.
This type of plan is added to distinguish thosedomain plans an agent has adopted from those it is simply considering adopting.
Inthis model, understanding an utterance ntails recognizing a discourse plan from theutterance and then relating that plan to a plan on either the exploration level or thedomain level, as determined by the form of the utterance and the plan structures builtfrom previous utterances.
Like the previous approaches, however, Ramshaw's modelis still utterance-to-utterance based.
The three-level structure he manipulates on thebasis of each user query does not account in any way for the structure of discourse.9.
Conc lus ionIn this paper, we have developed a computational model for recognizing the inten-tional structure of a discourse and using that structure in discourse processing.
Shared-Plans are used both to represent the components of intentional structure, i.e., discourse566Lochbaum A Collaborative Planning Model(5) S 1: What is Dr. Smith teaching?
(6) $2: Dr. Smith is teaching Architecture.
(7) S 1: Isn't Dr. Brown teaching Architecture?
(8) $2: No.
(9) Dr. Brown is on sabbatical.EsS I :  see on campus yesterday?
But didn't I him$2: Yes.He was giving a University colloquium.1: OK.(14) But isn't Dr. Smith a theory person?Figure 35Lambert and Carberry's analysis.Figure 36Our analysis.
(a)(5)(6)(b)(7)(8)(9)(c) Fi(11)122(d) I (14)SI: What is Dr. Smith teaching?$2: Dr. Smith is teaching Architecture.SI: Isn't Dr. Brown teaching Architecture?$2: No.Dr.
Brown is on sabbatical.SI: But didn't I see him on campus yesterday?$2: Yes.He was giving a University colloquium.SI: OK.But isn't Dr. Smith a theory person?segment purposes and their interrelationships, and to reason about the use of inten-tional structure in utterance interpretation.We have also shown that our work differs from previous plan-based approachesto discourse processing by providing a model for recognizing and reasoning withdiscourse-level intentions, rather than utterance-level intentions.
The previous ap-proaches address the problem of recognizing the propositional content of an utterancefrom its surface form, but provide only an utterance-to-utterance-based nalysis ofdiscourse.
In contrast, we begin from propositional content and present a model ofdiscourse processing that derives from discourse structure.10.
Future DirectionsThere are three main areas in which the research presented in this paper could beextended.
The first involves the augmentation process, the second its use in modelingintentional structure, and the third its use in building collaborative agents.567Computational Linguistics Volume 24, Number 410.1 The Augmentation ProcessThe augmentation process given in Figures 10 and 13 provides a novel frameworkh}r utterance interpretation based on discourse structure.
This framework outlines therequired steps of the interpretation process and provides constraints on the types ofalgorithms that may be used to model them.
The rules and algorithms presented inSection 6.1 provide a means of modeling the central steps involved in the interpretationprocess, but are only a beginning.
Further esearch is required to develop algorithmsto model the remaining steps.
For example, Case (2c) of the augmentation processmodels the process by which an agent recognizes the contribution of an utterance tothe SharedPlan currently in focus.
In elaborating this case, we concentrated on justone type of utterance.
In particular, we focused on utterances that communicate infor-mation about a single action and reasoned only about that action, and not the otherinformation communicated by the utterance.
The augmentation process could thus beextended to include reasoning about other types of utterances, as well as to includereasoning about the information contained in those utterances.
For example, Balka-nski (1993) has shown that multi-action utterances convey a wealth of informationabout a speaker's beliefs and intentions.
That information should also be taken intoaccount in interpreting the agent's utterances.Step (3b) of the augmentation process deals with the situation in which an agentdoes not understand, or disagrees with, its collaborative partner's utterances.
Therecognition of this case is modeled by the failure of the rgraph construction algorithm.This failure indicates that the algorithm was unable to produce an explanation for anact and thus that further communication a d replanning are necessary.
Our implemen-tation of the algorithm (Lochbaum 1994) models one possible behavior of the agent insuch circumstances.
In particular, the implementation utputs the recipe it was tryingto use to explain the act, along with an explanation for its failure.
This informationcan be viewed as a starting point from which the agents may engage in a negotiationprocess.
The details of that negotiation process are the subject of future research.As we alluded to earlier in the paper, the process of constructing a SharedPlanmay also be used to aid in the generation ofutterances.
Figure 37 provides ahigh-levelspecification of this process.
It is based on the assumption that agents G1 and G2 arecollaborating on an act a and indicates how the requirements of collaboration constrainthe range of information that G1 must consider in formulating his utterances.
Theserequirements are maintained in Gl'S agenda.
27As indicated in Step (4) of Figure 37,Gl'S agenda indicates those beliefs and intentions that are required for the agents tohave a full SharedPlan for a, but that are absent from their current partial SharedPlan.On the basis of his agenda, G1 chooses an item to which to direct his attention,decides what he wants to say about hat item, and then does so (Step (5b) of the processin Figure 37).
The question of how the information on Gl'S agenda is organized is thesubject of future research.
We have not specified the process by which an agent chooseswhat to communicate from among the possible options, or how it then does so.Once Gi has communicated some particular information to G2, he waits for herresponse.
If G2 indicates her agreement with G1, either explicitly or implicitly, G1 thenupdates his beliefs about the agents' PSP to reflect he information he communicated(Step (6)).The augmentation process given in Figure 37 provides a specification of the gen-eration process at a much higher level of detail than previous work in generation.
It27 We follow Grosz and Kraus's (1993) terminology in our use of this term.568Lochbaum A Collaborative Planning ModelAssume:PSP({Gi, G2}, c~),G, is the agent being modeled.G1 is the speaker and must decide what to communicate.4,5.G1 inspects his beliefs about he state of the agents' PSP to determine what beliefs andintentions the agents must establish to complete it.
Call this set Gl'S Agenda.
(a) If the Agenda is empty, then G1 believes that the agents' PSP is complete andso communicates that belief to G2.
(b) Otherwise, G1i.
chooses an item from the Agenda to establish,ii.
decides upon a means of establishing it,iii.
communicates hi  intent o G2.6.
Unless G2 disagrees, G1 assumes mutual belief of what he communicated and updateshis beliefs about he state of the agents' PSP accordingly.Figure 37The SharedPlan augmentation process--generation.is concerned with participating in an extended iscourse, while most work in gen-eration has been concerned with generating short, isolated monologues.
Moore andParis (1993), however, have begun to look at the problem of participating in longerdiscourses and in particular at the problem of responding to follow-up questions.They have argued that while previous work in generation (e.g., the work of McKe-own \[1985\], McCoy \[1989\], Paris \[1988\], and Hovy \[1991\]) has been concerned withwhat information to communicate and how, responding to follow-up questions alsorequires maintaining a record of why the information is being communicated.
Without isuch a representation, the system cannot respond effectively when a hearer does notunderstand or accept its utterances.
It remains to be determined how the process inFigure 37 meshes with previous work in generation.
Our suspicion, however, is thatas with interpretation, the augmentation process provides a model of discourse-levelintentions, while work such as Moore and Paris's (1993) is really providing a modelof utterance-level intentions.
Both types of information are necessary for generatingextended iscourses, but serve different purposes.10.2 Modeling Intentional StructureIn our model, SharedPlans and relationships among them provide the basis for com-puting intentional structure.
We take DSPs to be of the form Int.Th(ICP, FSP({ICP,OCP},fl)) and relationships between DSPs to depend upon subsidiary and tempo-ral relationships between the corresponding SharedPlans.
DSPs that do not involveSharedPlans would thus seem to present a problem for our model; however, manysuch DSPs may still be explained in terms of SharedPlans.
For example, considerDSPs of the form "Intend that some agent intend to perform some physical task," asproposed by Grosz and Sidner (1986, 179).
It is possible to explain this type of DSPin terms of the Int.To requirement of the FSP definition (Clause (2a) in Figure 5).
Ac-cording to that requirement, each of the single-agent acts in the agents' recipe mustbe intended by one of the two collaborating agents.
This requirement might lead oneof the agents, say G1, to engage in a subdialogue to convince the other agent, G2,to adopt such an intention.
The DSP of this subdialogue would be represented asInt.Th(G1, Int.To(G2, fli)) and corresponds to the English gloss given above.
Althoughthis DSP does not involve a SharedPlan, it is still motivated and explained by the569Computational Linguistics Volume 24, Number 4requirements of the FSP definition.
Further research is required to completely developthis extension.10.3 Building Collaborative AgentsAlthough issues in discourse processing provided the original motivation for Shared-Plans, Grosz and Kraus's more recent work (1993, 1996) has also shown the importanceof the formalism to building collaborative agents.
The work presented in this paperalso contributes to that aspect of SharedPlans.
The SharedPlan definitions delineate theinformation about which collaborating agents must communicate, whether they com-municate in a natural anguage or an artificial one.
The model of discourse processingdeveloped in this paper provides a means of processing the agents' communicationsregardless of the form in which they occur.
Rich and Sidner's (1998) work on COLLA-GEN demonstrates the use of the model with an artificial discourse language (Sidner1994).AcknowledgmentsThis work was done as part of mydissertation research at Harvard University,and was supported by a Bellcore graduatefellowship and by U S WEST AdvancedTechnologies.
I would like to thank all ofthe people who contributed to my thesiseffort, particularly Barbara Grosz, StuartShieber, and Candy Sidner.ReferencesAllen, James E 1983.
Recognizing intentionsfrom natural language utterances.
InM.
Brady and R. C. Berwick, editors,Computational Models of Discourse.
MITPress, Cambridge, MA, pages 107-166.Alien, James.
E and C. Raymond Perrault.1980.
Analyzing intention in utterances.Artificial Intelligence, 15:143-178.Ansari, Daniel.
1995.
Deriving Proceduraland Warning Instructions from Deviceand Environment Models.
Master's thesis,University of Toronto.Appelt, Douglas.
1985.
Some pragmaticissues in the planning of definite andindefinite noun phrases.
In Proceedings ofthe 23rd Annual Meeting, Association forComputational Linguistics, pages 198-203,Chicago, IL.Appelt, Douglas and Amichai Kronfeld.1987.
A computational model of referring.In Proceedings ofIJCAI-87, pages 640-647,Milan, Italy.Ba\]kanski, Cecile T. 1993.
Actions, Beliefs andIntentions in Multi-Action Utterances.
Ph.D.thesis, Harvard University.Barrett, Anthony and Daniel S. Weld.
1994.Task decomposition via plan parsing.
InProceedings ofAAAI-94, pages 1117-1122,Seattle, WA.Brachman, Ronald J. and James G.Schmolze.
1985.
An overview of theKL-ONE knowledge representationsystem.
Cognitive Science, 9:171-216.Bratman, Michael E. 1992.
Sharedcooperative activity.
The PhilosophicalReview, 101:327-341.Carberry, Sandra.
1987.
Pragmatic modeling:Toward a robust natural languageinterface.
Computational Intelligence,3:117-136.Cohen, Philip R. and Hector J. Levesque.1990.
Rational interaction as the basis forcommunication.
I  P. R. Cohen, J. L.Morgan, and M. E. Pollack, editors,Intentions in Communication.
MIT Press,Cambridge, MA, pages 221-255.Cohen, Philip R. and C. Raymond Perrault.1979.
Elements of a plan-based theory ofspeech acts.
Cognitive Science, 3:177-212.Cohen, Philip R., C. Raymond Perrault, andJames F. Allen.
1982.
Beyondquestion-answering.
In W. Lehnert andM.
Ringle, editors, Strategies for NaturalLanguage Processing.
Lawrence ErlbaumAssociates, Hillsdale, NJ, pages 245-274.Fikes, Richard E. and Nils J. Nilsson.
1971.STRIPS: A new approach to theapplication of theorem proving toproblem solving.
Artificial Intelligence,2:189-208.Grice, H. P. 1969.
Utterer's meaning andintentions.
Philosophical Review,68(2):147-177.Grosz \[Deutsch\], Barbara J.
1974.
Thestructure of task-oriented dialogs.
In IEEESymposium on Speech Recognition:Contributed Papers, pages 250-253,Pittsburgh, PA.Grosz, Barbara J. and Julia Hirschberg.
1992.Some intonational characteristics ofdiscourse structure.
In Proceedings oftheInternational Conference on Spoken LanguageProcessing, pages 429-432, Banff, Alberta,Canada.570Lochbaum A Collaborative Planning ModelGrosz, Barbara J. and Sarit Kraus.
1993.Collaborative plans for group activities.In Proceedings ofIJCAI-93, pages 367-373,Chambery, Savoie, France.Grosz, Barbara J. and Sarit Kraus.
1996.Collaborative plans for complex groupaction.
Artificial Intelligence, 86(2):269-357.Grosz, Barbara J. and Candace L. Sidner.1986.
Attention, intentions, and thestructure of discourse.
ComputationalLinguistics, 12(3):175-204.Grosz, Barbara J. and Candace L. Sidner.1990.
Plans for discourse.
In P. R. Cohen,J.
L. Morgan, and M. E. Pollack, editors,Intentions in Communication.
MIT Press,Cambridge, MA, pages 417--444.Hintikka, Jaakko 1978.
Answers toquestions.
In H. Hiz, editor, Questions.
D.Reidel, Dordrecht, Holland, pages279-300.Hobbs, Jerry R. 1985.
Ontologicalpromiscuity.
In Proceedings ofthe 23rdAnnual Meeting, Association forComputational Linguistics, pages 61-69,Chicago, IL.Hovy, Eduard.
1991.
Approaches to theplanning of coherent text.
In C. L. Paris,W.
R. Swartout, and W. C. Mann, editors,Natural Language Generation i ArtificialIntelligence and Computational Linguistics.Kluwer Academic Publishers, Boston,MA, pages 83-102.Kautz, Henry A.
1990.
A circumscriptivetheory of plan recognition.
In P. R. Cohen,J.
L. Morgan, and M. E. Pollack, editors,Intentions in Communication.
MIT Press,Cambridge, MA, pages 105-134.Kronfeld, Amichai.
1986.
Donnellan'sdistinction and a computational model ofreference.
In Proceedings ofthe 24th AnnualMeeting, Association for ComputationalLinguistics, pages 186-191, New York, NY.Kronfeld, Amichai.
1990.
Reference andComputation.
Cambridge University Press,Cambridge, England.Lambert, Lynn and Sandra Carberry.
1991.A tripartite plan-based model of dialogue.In Proceedings ofthe 29th Annual Meeting,Association for Computational Linguistics,pages 47-54, Berkeley, CA.Lambert, Lynn and Sandra Carberry.
1992.Modeling negotiation subdialogues.
InProceedings ofthe 30th Annual Meeting,Association for Computational Linguistics,pages 193-200, Newark, DE.Litman, Diane J.
1985.
Plan Recognition andDiscourse Analysis: An Integrated Approachfor Understanding Dialogues.
Ph.D. thesis,University of Rochester.Litman, Diane J. and James F. Allen.
1987.
Aplan recognition model for subdialoguesin conversations.
Cognitive Science,11:163-200.Litman, Diane J. and James F. Allen.
1990.Discourse processing and commonsenseplans.
In P. R. Cohen, J. L. Morgan, andM.
E. Pollack, editors, Intentions inCommunication.
MIT Press, Cambridge,MA, pages 365-388.Lochbaum, Karen E. 1991.
An algorithm forplan recognition in collaborativediscourse.
In Proceedings ofthe 29th AnnualMeeting, Association for ComputationalLinguistics, pages 33-38, Berkeley, CA.Lochbaum, Karen E. 1994.
UsingCollaborative Plans to Model the IntentionalStructure of Discourse.
Ph.D. thesis,Harvard University.
Available asTechnical Report TR-25-94, Center forResearch in Computing Technology,Division of Applied Sciences.Lochbaum, Karen E., Barbara J. Grosz, andCandace L. Sidner.
1990.
Models of plansto support communication: An initialreport.
In Proceedings ofAAAI-90, pages485-490, Boston, MA.McCarthy, John and Patrick J. Hayes.
1969.Some philosophical problems from thestandpoint of artificial intelligence.
InB.
Meltzer and D. Michie, editors, MachineIntelligence 4.Edinburgh University Press,Edinburgh, pages 463-502.McCoy, Kathleen E 1989.
Generatingcontext sensitive responses toobject-related misconceptions.
ArtificialIntelligence, 41(2):157-195.McKeown, Kathleen R. 1985.
Discoursestrategies for generating natural anguagetext.
Artificial Intelligence, 27:1-42.Moore, Johanna D. and CEcile L. Paris.
1993.Planning text for advisory dialogues:Capturing intentional and rhetoricalinformation.
Computational Linguistics,19(4):651-694.Moore, Robert C. 1985.
A formal theory ofknowledge and action.
In J. R. Hobbs andR.
C. Moore, editors, Formal Theories of theCommonsense World.
Ablex PublishingCorp., Norwood, NJ, pages 319-358.Morgenstern, Leora.
1987.
Knowledgepreconditions for actions and plans.
InProceedings ofthe lOth International JointConference on Artificial Intelligence(IJCAI-87), pages 867-874, Milan, Italy.Morgenstern, Leora.
1988.
Foundations of aLogic of Knowledge, Action, andCommunication.
Ph.D. thesis, New YorkUniversity.Paris, C4cile L. 1988.
Tailoring objectdescriptions to the user's level ofexpertise.
Computational Linguistics,14(3):64--78.571Computational Linguistics Volume 24, Number 4Perrault, C. Raymond.
1990.
An applicationof default logic to speech act theory.
InP.
R. Cohen, J. L. Morgan, and M. E.Pollack, editors, Intentions inCommunication.
MIT Press, Cambridge,MA, pages 161-186.Pollack, Martha E. 1986a.
Inferring DomainPlans in Question-Answering.
Ph.D. thesis,University of Pennsylvania.Pollack, Martha E. 1986b.
A model ofplan inference that distinguishesbetween the beliefs of actors andobservers.
In Proceedings ofthe 24th AnnualMeeting, Association for ComputationalLinguistics, pages 207-214, NewYork NY.Pollack, Martha E. 1990.
Plans as complexmental attitudes.
In P. R. Cohen, J. L.Morgan, and M. E. Pollack, editors,Intentions in Communication.
MIT Press,Cambridge, MA, pages 77-103.Ramshaw, Lance A.
1991.
A three-levelmodel for plan exploration.
In Proceedingsof the 29th Annual Meeting, Association forComputational Linguistics, pages 39--46,Berkeley, CA.Rich, Charles and Candace L. Sidner.
1998.COLLAGEN: A collaboration manager forsoftware interface agents.
User Modelingand User-Adapted Interaction.Sacerdoti, Earl D. 1977.
A Structure for Plansand Behavior.
North-Holland, Amsterdam,Netherlands.Searle, John R. 1990.
Collective intentionsand actions.
In P. R. Cohen, J. L. Morgan,and M. E. Pollack, editors, Intentions inCommunication.
MIT Press, Cambridge,MA, pages 401--416.Sidner, Candace L. 1983.
What the speakermeans: The recognition of speakers' plansin discourse.
Computers and Mathematicswith Applications, 9:71-82.Sidner, Candace L. 1985.
Plan parsing forintended response recognition idiscourse.
Computational Intelligence,1(1):1-10.Sidner, Candace L. 1994.
An artificialdiscourse language for collaborativenegotiation.
In Proceedings ofAAAI-94,pages 814-819, Seattle, WA.Sidner, Candace L. and David J. Israel.
1981.Recognizing intended meaning andspeakers' plans.
In Proceedings ofthe 7thInternational Joint Conference on ArtificialIntelligence (IJCAI-81), pages 203-208,Vancouver, British Columbia, Canada.Vilain, Marc.
1990.
Getting serious aboutparsing plans: A grammatical nalysis ofplan recognition.
In Proceedings ofAAAI-90, pages 190-197, Boston, MA.572
