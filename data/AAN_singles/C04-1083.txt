Browsing Help for Faster Document RetrievalEric CrestanSinequa, SinequaLabs51-54, rue Ledru-Rollin92400 Ivry-sur-Seine, Francecrestan@sinequa.comLaboratoire Informatique d?AvignonB.P.
1228 Agroparc339 Chemin des Meinajaries84911 Avignon Cedex 9, FranceClaude de LOUPYSinequa, SinequaLabs51-54, rue Ledru-Rollin92400 Ivry-sur-Seine, Franceloupy@sinequa.comMoDyCo, Universit?
de Paris 10Laboratoire MoDyCo - UMR 7114Universit?
Paris 10, B?timent L200, avenue de la R?publique92001 Nanterre Cedex, FranceAbstractIn this paper, the search engine Intuition isdescribed.
It allows the user to navigatethrough the documents retrieved with a givenquery.
Several ?browse help?
functions areprovided by the engine and described here:conceptualisation, named entities, similardocuments and entity visualization.
Theyintend to ?save the user?s time?.
In order toevaluate the amount of time these features cansave, an evaluation was made.
It involves 6users, 18 queries and the corpus is made of 16years of the newspaper Le Monde.
The resultsshow that, with the different features, a userget faster to the needed information.
fewernon-relevant documents are read (filtering)and more relevant documents are retrieved inless time.1 IntroductionDuring the last 10 years, TREC (Harman, 1993)allowed many researchers to evaluate their searchengines and helped the field to progress.
In 2000,Donna Harman studied the evolution of 2 searchengines from 1993 (Harman, 2000).
She showedthat, after an improvement period, theperformances have been almost the same forseveral years.
This observation seems now classic:improving the heuristics or adding linguisticknowledge to a "good" engine does notdramatically improve its results.
The problem isthat even the best engines do not come up to theexpectations of most users.
So, if the performancesdo not really rise anymore, how can we rise users?satisfaction?In fact, there are other ways to evaluate searchengines than recall and precision.
Time spent tofind answers seems to be the most important onefor the users and several papers present such anevaluation (Borlund and Ingwersen, 1998)(J?rvelin and Kek?l?inen, 2002).
Considering thetime factor, it is quite easy to improve theperformances using procedures in order to help theuser in his/her search.In this paper, we present the different 'browsehelp' features proposed to the users by Intuition,the search engine of Sinequa.
First of all, wepresent the search engine itself (section 2).
Thenfour types of help features are presented in section3: conceptualisation, named entities filtering,similar documents and entity visualization.
Section4 describes the experiments done in order toevaluate the different browsing features andsection 5 presents the results.
These results showthat using browsing help can decrease the timespent on searching.2 Intuition search engineIntuition, the search engine of Sinequa, is basedboth on deep statistics and linguistic knowledgeand treatments (Loupy et al, 2003).
Duringindexing, the documents are analysed with a partof speech tagging, and a lemmatization procedure.But the most original linguistic feature of Intuitionis the use of a semantic lexicon based on the "seealso" relation (Manigot and Pelletier, 1997).
Infact, it is based on bags of words containing unitslinked by a common seme.
For instance, the bag ofwords "Wind" contains wind, hurricane, to blow,tornado, etc.
800 bags of words describe the"Universe".
It seems very poor but it is enough formost applications.
A Salton like vector space(Salton, 1983) of 800 dimensions is created withthese bags of words.
120,000 lemmas arerepresented in this space for French (a word canbelong to several dimensions).
During the analysisof a document, the vector of each term is added tothe others in order to have a documentrepresentation in this space.This analysis allows a thematic characterizationof a document.
Secondly, it increases bothprecision and recall.
When a query is submitted toIntuition, two searches are made in parallel.
Thefirst one is the standard search of documentscontaining the words (lemmas) of the query orsynonyms.
The second one searches for documentswith similar subjects that are having a close vector.Each document of the corpus has two scores andthey are merged according to a user definedheuristic.
The advantage of such an approach isthat the first documents retrieved not only containthe words of the query but are also closely relatedto the subject of the query.
Lastly, this vectorrepresentation of words and documents allows thedisambiguation of words semantically ambiguous.3 Navigation Features3.1 Conceptualization3.1.1 DescriptionThe ?concepts part?
of the interface showsseveral links represented by short noun phrases.When the user clicks on one of these links, a newquery is submitted to the engine.
The documentsretrieved by the first query are then filtered andonly the ones that contain the selected noun phraseare kept.
This is a very convenient way to selectrelevant topics.
The user can select the appropriateconcept corresponding to his/her expectations inorder to reduce the search space.
For instance, theconcepts retrieved with the ?ouragan ?Am?riqueCentral?
1998?
(hurricane ?Central America?
)query are the following (numbers in brackets givethe number of documents in which the conceptsoccur):Conceptsouragan Mitch  (12)Am?rique centrale  (29)Mitch  (10)Honduras  (17)Nicaragua  (18)cyclone Mitch  (85)Guatemala  (12)pays d'Am?rique centrale  (17)Managua  (97)Salvador  (34)Banque interam?ricaine  (34)programme alimentaire  (05)Colombie  (79)glissements de terrain  (14)aide internationale  (86)Costa-Rica  (65)Figure 1: Concepts for query ?ouragan ?Am?riqueCentrale?
1998?Because concepts are extracted from the top listof relevant documents (according to the relevancescore), they can be seen as a summary minedacross them.
The list contains different types ofconcepts, from noun groups to proper nouns.
In thetop of the list comes the answer to the currentquestion (Q1056): ouragan Mitch, Mitch andcyclone Mitch (Mitch hurricane, Mitch and Mitchcyclone).
A click on one of those links will directlylead to the document containing the text string, andthus, to the relevant documents.This way of browsing is even more useful whenthe engine is not able to get rid of an ambiguity.
Ina perfect world, a query divides the documentspace in two parts, the relevant and non-relevantdocuments.
However, what might be relevantregarding to a query, might not be relevantaccording to the user.
Everybody knows that asearch engine often returns non-relevantdocuments.
This is due to both the complexity oflanguages and the difficulty to express aninformation in some words.
Because an enginemay not fit correctly the needs of the user, theproposed way to browse within the retrieveddocuments is very handy.
The user can then selectthe relevant concepts.
Of course, it is also possibleto select several concepts, to eliminate severalothers and then resubmit a query.3.1.2 Concept detectionAs the search engine indexes the documents,several linguistic analysis are applied on each ofthem in order to detect all possible concepts.Morpho-syntactic analysis is needed by conceptdetection because most of the patterns are based onPart-of-Speech sequences.
The concept detectionitself is based on Finite State Automata.
Theautomata were built by linguists in order to catchsyntactic relation such as the ones cited above.
Foreach document, the potential concepts are stored inthe engine database.3.1.3 Concept selectionFor the purpose of concept selection, only thefirst 1000 documents retrieved by the engine (orfewer if relevancy score is too low) are used.
Then,frequencies of concept occurrences in the sub-corpus are compared with the frequencies in theentire corpus.
The selected concepts should be thebest compromise between minimum ambiguity andthe maximum of occurrence.
A specificity score iscomputed for each concept.
This score is used tosort all the occurring noun phrases.
Only the topones are displayed and should represent the mostimportant concepts of the documents.3.2 Named entitiesThe last area of the interface shows severalnamed entities: locations, people and organizations(see section 4.1 for a description of the namedentity recognition procedure).
Like it is done withmeta-data, entities can be used in order to restrictsearch space.
We can filter the documents retrievedby the original query and get only those, whichcontain Managua.Pays (Countries)Etats-Unis  (22)Nicaragua  (21)Honduras  (18)France  (12)Guatemala  (12)Villes (Cities)Managua  (10)Londres  (6)New York  (6)Paris  (5)Washington  (4)Personnes (Persons)Jacques Chirac  (3)Arnoldo Aleman  (2)Bernard Kouchner  (2)Bill Clinton  (2)Daniel Ortega  (2)Soci?t?s (Organizations)Banque mondiale  (6)Banque interam?ricaine de d?veloppement  (4)Fonds mon?taire international  (4)Chrysler  (1)Figure 2: Named entities distribution for queryConcepts for query ?ouragan ?Am?rique Centrale?1998?Named entities become very useful when doingstatistics on a corpus.
For a given query, thedistribution for each entity type can be computedand sorted according to a scoring function.Document frequency (DF) is usually a good way tosort the result.
But the information provided by thesearch engine is very useful against the query.
Thescoring function used by Intuition is based ondocument score ?
and document rank j (1<j<N) fora given category v:The parameter ?
modifies the importance givento the document score, and the parameter ?modifies the importance given to the documentranking.
Figure 2 presents the entities for locations,persons and organizations for the query ?ouragan?Am?rique Centrale?
1998?.
Numbers inparenthesis represent the entity score.3.3 Named Entities visualizationSometimes, additional information is insufficientor not at all present in the documents.
In order toincrease the browsing possibilities, specificinformation can be automatically extracted fromtexts.
For this purpose, we use a document analysisprocess based on transducers in order to detectnamed entities.
This system has been previouslydeveloped in order to participate toquestion/answering task in TREC evaluationcampaign (Voorhees, 2001).
The commonlyestablished notion of names entities has beenextended in order to include more types.
More than50 different types of entities are recognized inFrench and English.The document analysis system can bedecomposed in two main tasks.
First, a morpho-syntactic analysis is done on the documents.
Everyword is reduced to its basic form, and a Part-of-Speech tag is proposed.
In addition to the classicalPOS tags, the lexicon includes semanticinformation.
For example, first names have aspecific tag (?PRENOM?).
These semantic tags areused in the next phase for entity recognition.Transducers are applied in cascade.
Every entityrecognized by one transducer can be used by thenext one.
The analysis results in a list of entitytype, value, position and length in the originaldocument.Figure 3: Visualization of named entitiesEntity recognition and extraction opens up newperspectives for browsing within documents.
Themost trivial use is to display certain entities incolor according to their type.
Users can thenquickly filter documents talking about the rightpersons or places.
He can also immediately findinteresting passages.
Figure 3 shows a documentwith highlighted entities.100)( ?=??
vvscore   where )(1 1 vjNjMivj ?????
?=?
?= =and 0)( 1=?
?= vvvij ?It is clear that this allows an easier quick readingbecause the most representative parts of thedocuments are highlighted.Moreover, it is very easy to find the entities inthe current document.
In Fig.
4, one canimmediately see which locations are mentioned(e.g.
Am?rique Centrale, Salvador, Honduras,Nicaragua, Managua, etc).4 Task descriptionThe evaluation includes six interfaces withdifferent features for the most of them.
They weredesigned in order to evaluate whether thenavigation facilities proposed to users improvetheir ability to find relevant documents.
The sixinterfaces query the same document base:775 000-article collection extracted from theFrench newspaper Le Monde (years 1989 to 2002).The features used for each interface are listed inTable 1.Interface name FeaturesInterface1 Classical searchInterface2 Concept navigationInterface3 Named entity navigationInterface4 Named entity visualizationInterface5 Similar documentsInterface6 All featuresTable 1: Interface profilesInterface1: No additional navigation facilitiesare provided to users.
A simple query box issupplied in order to query Intuition search engine(see Section 2).
A summary of 10 documents perpage is presented to the user.
It gives the articletitle, the relevance score and an abstract consistingin the first 250 bytes from the document.Interface2: Equivalent to Interface1, itfeatures in addition a list of concepts in summarypresentation.
Concepts are extracted according tothe user query (see Section 3.1).Interface3: Equivalent to Interface1, itdisplays also four lists of named entities related tothe documents returned by the engine.
In the leftside column are listed the persons, cities, countiesand companies the most representative (see Section3.2).Interface4: Alike Interface1, the onlydifference resides in the named entitieshighlighting (persons, dates, cities, counties andcompanies) when users open the articles (seeSection 3.3).Interface5: Same as Interface1, it enables,when opening a document, to navigate through oneof the 3 similar documents proposed into anadditional frame.Interface6: It figures a compilation ofadditional features used in all the other interfaces.All the user actions are stored into the searchengine log file, so that we can evaluate how manyusers employ additional features.
On each visitedarticle, users were asked, through buttons, toprecise whether the document was relevant(VALIDATION button) or not (ANNULATIONbutton).
Information such as time and user id wasstored in the log file as well.5 ExperimentIn order to evaluate the six interfaces, a set ofqueries had to be built according to the number ofsubjects available for the experiment.
Furthermore,a specific framework has been set for each user.5.1 MaterialTwo sets of queries were used for thisevaluation.
The first is composed of 12 taskdescription queries, which originate from TREC-6ad-hoc campaign (Voorhees and Harman, 1997).Twelve descriptions were selected among the fiftyproposed for the task according to theirapplicability to a French newspaper corpus.
Wedeliberately selected the description part in order tohave a more precise idea of what document shouldbe considered has relevant.
Moreover, supplying ashort description (2-3 words) would have lead toequivalent queries at the first stage.
Users wouldhave probably copied the proposed keywords inorder to compose their queries.
Then, they weretranslated into French by an external person (notinvolved in the evaluation process).
The second setis composed of 6 factual questions inspired fromthe previous TREC Question/Answeringevaluation campaigns (Voorhees, 2003) andtranslated.
The subjects were asked to retrievedocuments containing the answer.ID Queries301 Identify organizations that participate in internationalcriminal activity, the activity, and, if possible,collaborating organizations and the countriesinvolved.304 Compile a list of mammals that are considered to beendangered, identify their habitat and, if possible,specify what threatens them.305 Which are the most crashworthy, and leastcrashworthy, passenger vehicles?310 Evidence that radio waves from radio towers or carphones affect brain cancer occurrence.311 Document will discuss the theft of trade secrets alongwith the sources of information:  trade journals,business meetings, data from Patent Offices, tradeshows, or analysis of a competitor's products.322 Isolate instances of fraud or embezzlement in theinternational art trade.326 Any report of a ferry sinking where 100 or morepeople lost their lives.327 Identify a country or a city where there is evidence ofhuman slavery being practiced in the eighties ornineties.331 What criticisms have been made of World Bankpolicies, activities or personnel?338 What adverse effects have people experienced whiletaking aspirin repeatedly?339 What drugs are being used in the treatment ofAlzheimer's Disease and how successful are they?342 The end of the Cold War seems to have intensifiedeconomic competition and has started to generateserious friction between nations as attempts are madeby diplomatic personnel to acquire sensitive trade andtechnology information or to obtain information onhighly classified industrial projects.
Identify instanceswhere attempts have been made by personnel withdiplomatic status to obtain information of this nature.Q215 Who is the prime minister of India?Q250 Where did the Maya people live?Q924 What is the average speed of a cheetah?Q942 How many liters in a gallon?Q1056 What hurricane stroked Central America in 198?Q1501 How much of French power is from nuclear energy?Table 2: Sets of queries5.2 Evaluation frameworkThe definition of the framework was constraintby the number of subjects available for thisevaluation.
Because it was an internal experiment,only six persons tested the interfaces.
The groupwas composed of 3 linguists and 3 computerscientists (2 females and 4 males) with differentaptitude levels with search engines.
Each subjectwas given 3 queries (2 descriptive queries and 1question) per interface starting with Interface1 andfinishing with Interface6.
A cross-evaluation wasused so that two subjects would not employ thesame interface with the same question.
At the end,the 18 queries were evaluated with each interface.Because of the corpus nature (newspaper),subjects need a certain amount of time to read thearticle in order to judge it relevant or not.
The timeavailable for each query was limited to 10 minutesduring which the subject was asked to retrieve amaximum of relevant documents.
It is twice thetime devoted to a similar task presented in (Bruzaet al, 2000)1.
We consider that the time needed tofind relevant documents on a newspaper collectionis greater than on the Internet for many reasons:First, the redundancy is much higher on theInternet; Second, we mostly find long narrativearticles on a newspaper collection though webdocuments seems more structured (section title,colors, bold and italic phrase, table, figures, etc.
).This last enables a quicker reading of thedocument.1 Bruza et al have compared three different kinds ofinteractive Internet search: The first was based onGoogle search engine; the second was a directory-basedsearch via Yahoo; and the last was a phrase based queryreformulation assisted search via the HyperindexBrowser.6 ResultsDuring the evaluation, participants could take abreak between each research because of the 3hours required for the full experiment.
Severalcriteria have been used for performancejudgement:?
Time to find the first relevant document,?
Number of relevant documentsretrieved,?
Average recall.They are described in the following sections.6.1 Relevance judgmentFor each visited article, the subjects were askedto click on one of the two following buttons:?
VALIDATION: document is judgedrelevant,?
ANNULATION:  document is judgednon-relevant.An average of 4.9 documents was assessedrelevant per query and user.
Table 3 shows theaverage of relevant and non-relevant documentsfound by every user:User Average Relevant Doc.Average non-Relevant Doc.User1 2.78 2.28User2 3.06 2.78User3 5.28 6.56User4 5.67 6.22User5 5.89 4.94User6 9.39 8.83Average 5.3 5.3Table 3: Average number of relevant and non-relevant document found by participant6.2 Time to first relevant documentTime is a good criterion for navigationeffectiveness judgment.
How long does it take forusers to find the first relevant document?
Thisquestion is probably one of the most important inorder to judge navigability gain over the sixinterfaces.
When no-relevant documents werefound for a query, the time was set to themaximum research time: 600s.The results, presented in Table 4, show the meantime over users/queries to the first relevantdocument.
Responding to our expectations,Interface6 obtains the best result (smallest meantime).Interface Mean time to fiInterface1 2Interface2 1Interface3 1Interface4 2Interface5 2Interface6 1Table 4: Mean time to finddocumentIt shows that an interfacebetter than having only oneAccording to the different resthat a search interface featurinnamed entities as navigationthe search time toward the firsThe other interfaces seem tosome way, that was predictaand Interface5 do notalternative at the summary pagIn this table, no standardbecause the considered data a(different users with diffedifferent queries).
For instancspent by User 1 (na?ve user) owhile the expert user 6 spent31 s in order to find the first re6.3 Number of relevant doThe time to first relevant dbe the only criterion in onavigation effectiveness.uire longer getting to the first00,050,10,150,20,250,30,350,40,450,50 20 40 60 80Averagerecallrst rel.
doc.
(in s) interfaces can req48.089.374.342.840.821.8the first relevantwith all features isor none of them.ults, it also appearsg the concepts or thealternative decreasest relevant document.be of little help.
Inble since Interface4present navigatione level.deviation is givenre not homogeneousrent interfaces fore, the average timen Interface 4 is 452 san average time oflevant document.cuments retrievedocument should notrder to judge theTherefore, somerelevant document, but after that it can fullybenefit from additional features.Interface Average RelevantAverageNon-RelevantInterface1 3.83 7.17Interface2 4.78 5.17Interface3 5.50 3.50Interface4 6.17 7.11Interface5 5.22 4.39Interface6 6.56 4.28Table 5: Average number of relevant and non-relevant documents / interfaceAs expected, Interface6 (all features available tousers) gives maximum relevant documents inaverage.
It scores almost twice as Interface1.Concerning the non-relevant documents, we seethat interfaces 2,3,5 and 6 allow the filtering ofnon-relevant documents or the navigation from arelevant document to another one.
The consistencybetween Interface1 and Interface4 is logicalbecause the user has to look in both cases at thedocument to know it is not relevant.6.4 Average recallIn order to combine the two previous criteria, wecomputed the average recall over all users and allqueries, for a given interface.
In order to computethe recall for a query q, the total number ofrelevant documents was approximated to the total100120140160180200220240260280300320340360380400420440460480500520540560580600Time (in s)I1I2I3I4I5I6Figure 4: Average Recall according to timenumber of documents marked as relevant oversubjects for q.
The recall at time t for a query q, auser u is then computed with the followingformula:( ) ( )( )qNtuqN ,,tu,q,Recall ?where N(q,u,t) is the number of relevantdocuments assessed by user u at time t for query qand N(q) is the total number of unique relevantdocuments found by all the users for query q.The average recall at time t is computed byaveraging the recall over the users and the queries.Figure 4 presents the curves of average recallaccording to time at a sampling rate of 10 seconds.First of all, this figure shows that using any ofthe browsing features improves the documentretrieval performances.
The two better curves areobtained with entity filtering or using all thefeatures.
It is however a little bit strange thatInterface3 rises over Interface6 on the first 120seconds.
Extensive tests should be carried on tocorroborate these results.7 ConclusionIn this paper, several ways to help the user inhis/her search are presented.
We think that it isnow necessary to have such kind of high-levelinteraction with the user.
The evaluations showedthat the navigation features provided here candecrease the time spent on a query.
Firstly, that istrue because the first answer is got more quickly.Secondly, even if the total number of relevantdocuments is not increased, they are retrieved inless time.
Thirdly, the concepts and entities filtersdecrease the number of non-relevant documentsthe user will read.There are some biases in this evaluation.
Almostall the users, even if they are not experts indocument retrieval, knew the search engine and thefeatures used.
Having said, (Bruza et al, 2000)trained their user before the real evaluation.
Itdepends on the targeted users.
Furthermore, 6 usersand 18 queries do not seem to be enough toevaluate 6 different interfaces.
We plan toreproduce this evaluation with more users.One of the important points of the featurespresented in this paper is that most of them arebased on linguistic analysis.
If the use of linguisticin classical document retrieval is controversial, wethink linguistic knowledge and treatments give theeasiest way to interact with users.8 AcknowledgesWe wish to warmly thank the six participants(Vanessa C., Elise L., Frederik C., Eric B., ArnaudD.
et Luc M.) to volunteer and their patience.
Wealso thank the newspaper Le Monde to make theircorpus available for us.ReferencesP.
Borlund and P. Ingwersen.
1998.
Measures ofrelative relevance and ranked half-life:performance indicators for interactive IR.In:Croft, B.W, Moffat, A., van Rijsbergen, C.J,Wilkinson, R., and Zobel, J., eds.
Proceedings ofthe 21st ACM Sigir Conference on Research andDevelopment of Information Retrieval.Melbourne, Australia: ACM Press/York Press,pp.
324-331.P.
Bruza, R. McArthur and S. Dennis.
2000.Interactive Internet search: keyword, directoryand query reformulation mechanisms compared.Proceedings of the 23rd annual internationalACM SIGIR conference on Research anddevelopment in information retrieval.
Athens,Greece, pp.
280-287.D.
Harman.
1993.
Overview of the First TextREtrieval Conference.
National Institute ofStandards and Technology Special Publication500-207.
(1993).D.
Harman.2000.
What we have learned and notlearned from TREC.
Proceeding of the 22ndAnnual Colloquium on IR Research.
SidneySussex College, Cambridge, England.K.
J?rvelin, and J. Kek?l?inen.
2002.
Cumulatedgain-based evaluation of IR techniques.
ACMTransactions on Information Systems (ACMTOIS) 20(4), pp.
422-446.C.
de Loupy, V. Combet and E. Crestan.
2003.Linguistic resources for Information Retrieval.
inENABLER/ELSNET International Roadmap forLanguage Resources.L.
Manigot , B. Pelletier.
1997.
Intuition, uneapproche math?matique et s?mantique dutraitement d'informations textuelles.
Proceedingsof Fractal'1997.
pp.
287-291.G.
Salton.
1983.
Introduction to ModernInformation Retrieval, McGraw-Hill.E.
M. Voorhees.
2003.
Overview of the TREC 2002Question Answering Track, The Eleventh TextRetrieval Conference, NIST Special Publication:SP 500-251.E.
Voorhees, D. Harman.
1997.
Overview of thesixth Text Retrieval Conference; Proceeding ofthe 6th Text REtrieval Conference, NIST SpecialPublication 500-240; pp.
1-24; Gaithersburg,MD, USA.
