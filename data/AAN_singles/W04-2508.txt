Experiments with Interactive Question Answering in Complex ScenariosAndrew Hickl, John Lehmann, John Williams and Sanda HarabagiuLanguage Computer Corporation1701 N. Collins Suite 2000Richardson TX 75080andy@languagecomputer.comAbstractThis paper addresses the pragmatic challengesthat state-of-the-art question/answering sys-tems face in trying to decompose complexinformation-seeking scenarios.
We proposethat question decomposition can be approachedin one of two ways: either by approximatingthe domain-specific knowledge for a particularset of domains, or by identifying the decompo-sition strategies employed by human users.
Wealso present preliminary results from experi-ments that confirm the viability of each of theseapproaches within an interactive Q/A context.1 IntroductionOver the past five years, much research has focused onthe different challenges question-answering systems facewhen answering questions in isolation as opposed toquestions presented as part of a contextualized interac-tion with a user.The domain of interactive question answering is typ-ically concerned with two tasks: the decomposition ofcomplex questions into questions that can be processedby current Question/Answering (Q/A) systems and thedynamic representation of dialogue between a user anda Q/A system.
In this paper, we argue that the effectivedecomposition of questions is much more valuable to theperformance and the development of interactive Q/A sys-tems than any potential advances in dialogue processingalone.
We believe that this is due to the fact that dialogsystems do not operate under the requirement of findinginformation from vast text collections through a sequenceof questions and answers, which is the operating principleof interactive Q/A systems.We also believe that the quality of interactive Q/A sys-tems largely depends on the precision with which theycan find answers to questions.
Unless a Q/A systemcan reliably process questions in isolation with a highdegree of accuracy, it is very unlikely that that systemwill be able to answer questions in an interactive contextwith any degree of precision.
Although dialogue process-ing may improve the quality of answers generated by aquestion-answering system (by resolving ambiguities ina set of possible answers, for example), recent advancesin dialogue processing have not contributed to overall im-provements in answer retrieval or extraction in any mean-ingful way.However, we are not suggesting that dialogue process-ing should play no role in the development of Q/A tech-nology.
A system that decomposes questions well mustcontinue to be responsive to users in order to gather infor-mation about the user?s level of expertise and to anticipatethe user?s information needs throughout the course of theinteraction.In this paper, we present the framework for a new wayof decomposing complex questions that benefits from thelatest technology in both the answering of simple ques-tions and the processing of interactions with users of sev-eral levels of expertise.We present an analysis of two years?
worth of exper-iments that we have conducted in interactive Q/A thattakes advantage of a state-of-the-art question answeringsystem.
Our experiments show that tackling complexquestions requires several ways of modeling the domainof a complex topic as well as meaningful ways of findingtopic-relevant information in text collections.We have also found that the interactions required bythe resolution of complex questions or scenarios engen-der many more forms of questions than the types of ques-tions (i.e.
factoid, definition, list) evaluated in the pastTRECs.
We believe this is due to the fact that complexscenarios may consist of many different possible relationsbetween complex questions that need to be topologicallymodeled as complex answer structures.
Furthermore, ourexperiments have identified three means of decompos-ing complex scenarios into simple questions that can beprocessed reasonably well by current question-answeringsystems.In addition, we have found that ontological require-ments and real-time constraints are major factors in de-veloping interactive question-answering systems that re-alistically satisfy the needs of expert users.
The NLPtechniques required to process questions interactivelyconsist of (1) several syntactic and semantic processesthat lead to the identification of the structure of the ex-pected answer; (2) context modeling that includes co-reference resolution; and (3) the coherent decompositionof a complex scenario into a series of simple questionsthat are likely to be asked by users.The remainder of the paper is structured as follows.Section 2 presents the state-of-the-art in question answer-ing technology.
In Section 3, we detail our method ofprocessing domain-dependent complex questions and thetwo classes of users on which we tested our methods.
InSection 4, we describe our approach to question decom-positions whereas in section 5, we present an analysisof the automatic decompositions generated by our sys-tem and results of our scenario-processing experimentsinvolving expert information analysts.
Section 6 presentsour conclusions and directions for future work.2 Domain-Dependent Complex QuestionsThe decomposition of complex information-seeking sce-narios represents a trio of pragmatic challenges for Q/Asystems.First, effective question decomposition depends on theacknowledgment of the intentions that underlie a user?sinteraction with a Q/A system.
Individuals participatein information-seeking dialogues (whether with other hu-mans or with interactive Q/A systems) in order to learnnew things ?
that is, to gather information that they donot currently possess.
A user?s behavior in a dialogue fo-cuses on that set of speech acts which allow them to max-imize the new information they obtain from the conversa-tion while (at the same time) minimizing the amount ofredundant or previously-established information that theyencounter.
We expect these same principles to govern thedecomposition of complex scenarios as well: the decom-positions generated by a user will focus on returning thedomain-specific information that the user currently doesnot possess.
Expert users (who are assumed to be fa-miliar with a domain) will use interactive Q/A systemsto (1) evaluate their existing knowledge with regards tochanges in the context or (2) seek new information aboutknown entities or events within the domain.
In contrast,non-expert users (who remain unfamiliar with much ofthe ontological structure of a complex domain) will havevery broad and potentially poorly-defined informationalgoals; in these cases, interactive Q/A systems will need toreturn information which will facilitate the novice users?exploration of the domain.Second, we suggest that question decomposition willdepend on the development of semantic ontologies thatare articulated enough to address the domain-specificquestions characteristic of most complex information-seeking scenarios.
Current Q/A technologies are unableto process (or decompose) complex questions without ac-cess to a large amount of domain-specific knowledge.Modeling domain-specific knowledge for complex do-mains, however, is an arduous task: complex domainsnecessarily consist of sets of structured concepts linkedby classes of semantic relations.
Although this kind ofdomain modeling is traditionally considered to be tan-gential to research in NLP, we believe that interactiveQ/A systems must have access to not only the ontolog-ical structure of answers and complex semantic informa-tion, but also modes of probabilistic reasoning that can beused to induce categories of meanings between domainconcepts.Finally, since complex questions represent such di-verse informational goals, it should not be assumed thateven the decompositions produced by expert users willbe sufficiently simple enough to be processed by currentQ/A systems.
We propose that careful study needs to beconducted to identify the new types of context-dependentquestions that are generated as part of interactive Q/A.The rest of this section is ordered as follows.
Section2.1 describes three new types of questions found in thesets of decompositions generated by human users.
Sec-tion 2.2 details a simple solution that expands the cover-age of interactive Q/A systems for specific topic domains.Finally, Section 2.3 distinguishes between two idealizedtypes of users of interactive Q/A systems: experts andnovices.2.1 Scenarios and QuestionsSince complex questions represent such diverse infor-mational goals, it should not be assumed that even thedecompositions produced by expert users will be suffi-ciently simple enough to be processed by current Q/Asystems.COMPLEX QUESTION:What is the current status of India?s Prithvi ballistic missile project?DECOMPOSITION(1) (a)  How should ?India?
be identified?
(1) (b)  Pre?independence or post?independence, post?colonial, or post?1947 India?
(2) (a) What is ?Prithvi??
(2) (b) What does Prithvi mean?
(2) (c) What class of missiles does Prithvi belong to?
(2) (d) What is its range/payload, and other technical details?
(3) (a)  What is the meaning of ?status??
(3) (b)  Does status mean research and development, flight?tests, user?trials,serial production, integration into the armed forces?Figure 1: Scenario decomposition for the topic focusedon Prithvi missiles.The decompositions presented in Figure 1 introduce anumber of novel challenges for Q/A systems.
Three arediscussed below:Clarification Questions.
Questions like What is themeaning of ?status??
represent a new challenge for cur-rent Q/A systems.
Unlike TREC-style definition ques-tions, this class of questions (which we refer to as clar-ification questions) seek to identify the most domain-specific characterization available for the concept, entity,or term in focus.
Although informationally ?simple?,answers to these questions depend on implicit domain-specific knowledge that can only be supplied by an inter-active Q/A system.
In order to answer a question likeWhat is the meaning of ?status?
?, a system must beable (1) to identify the differences between the domain-specific and the domain-general characterization of thefocal item, (2) to recognize which domain-specific sensethe user is seeking, and finally (3) to return informa-tion that will help the user understand all of the domain-dependent semantic entailments of the term.Alternative Set Questions.
Questions produced aspart of a scenario decomposition often ask a system todistinguish between several different possible alterna-tives for the characterization of an entity.
Faced with aquestion like How should ?India?
be identified?
Pre-independence or post-independence?
Post-colonial orpost-1947 India?, the Q/A system must not only identifythe named entity India but must also return enough con-textual information to be able to determine which of thenamed set of entities should be considered most relevantto the current contextual scenario.Although the set of alternatives can be overtly stipu-lated by the user, an interactive Q/A system should ide-ally possess the domain-specific knowledge and the in-ferential capacity to be able to generate these kinds ofalternative sets automatically.
Although a set of alterna-tives may be extractable from a highly-specified semanticontology for a question like What is the meaning of ?sta-tus?
?, it is unlikely that such an ontology can be used toderive the different instantiations of India listed in Howshould ?India?
be identified?.
In this latter case, the sys-tem would have to (1) decide whether some sort of dif-ferentiation was necessary between the available instanti-ations, (2) identify which of the set of instantiations werethe most relevant alternatives, and finally, (3) determinewhich instantiation should be used to retrieve the answer.Contextual-Dependent Ellipsis.
Questions that in-volve syntactic ellipsis must be answered in context.With a question like What does ?Prithvi?
mean?, thesystem must recognize that semantic meaning is eval-uated with regards to a language (here, any of thosespoken on the Indian sub-continent).
The system mustalso be able to (1) identify examples of implicit syn-tactic ellipsis, (2) determine the semantic type of thesyntactically-elided information, and finally, (3) supplythe contextually-relevant members of that semantic classneeded to return the answer.Based on the initial observations above, we concludethat a careful analysis of the questions generated by sce-nario decompositions needs to be conducted to identifynew types of questions that cannot be processed by cur-rent Q/A systems.
By expanding the coverage of Q/Asystems for these kinds of ?informationally simple?
ques-tions, we expect future Q/A systems to be better posi-tioned to process questions with more complex informa-tional goals.
A careful examination of the question de-compositions generated by expert users can help us bet-ter understand what kinds of domain-specific knowledgeshould be made available to an interactive Q/A system.2.2 A Practical SolutionThe goal of question decomposition is to translate com-plex questions into simpler questions that have identi-fiable answer types.
Effective question decompositiondoes not guarantee answers, however: current Q/A sys-tems are only able to provide answers for approximately55% of simple (i.e.
factoid, definition, and list) questions.For most state-of-the-art Q/A systems, correct answersare returned iff the system identifies the correct answertype from the syntax and semantics of the question itself.Although current answer-type hierarchies can be fairlybroad in their coverage of concepts, they do not providean exhaustive treatment of all of the types of informationthat users can request for any particular domain.
In LCC?scurrent Q/A system (Harabagiu, Moldovan, et al, 2003),no answer type could be detected for questions like Whatbusiness was the source of John D. Rockefeller?s fortune?
(TREC-1909) or What 1857 U.S. Supreme Court decisiondenied that blacks were citizens?
(TREC-2259).
Thefailure of our system to return answer types for thesequestions was attributed to identifiable gaps in our se-mantic ontology of answer types.
By revising our an-swer type hierarchy to include classes of businesses orSupreme Court decisions, we could presumably enableour system to identify a viable answer type for each ofthese questions, and thereby improve our chances of re-turning a correct answer to the user.However, the challenge of expanding an answer typehierarchy becomes considerably more difficult when westart considering the very specific semantic ontologiesthat would need to be added to a hierarchy to account forspecific domains such as the development of Prithvi mis-siles in India, opium production in Afghanistan, or AIDSin Africa.
Without expert input into ontology creation foreach of these domains, NLP researchers can have only alimited idea of the conceptual knowledge that necessarilyneeds to be added to the answer type hierarchy in orderto improve Q/A for each of these domains.Given these considerations, we were able to improveour coverage of domain-specific factoid questions by in-corporating a database of 342 question-answer pairs (re-lated to a series of specific domains) into our Q/A sys-tem.
We used this database, known as the QUestion-Answer Base or QUAB, to measure the conceptual sim-ilarity of new questions to question-answer pairs alreadylisted in QUAB.
In the absence of a highly-articulatedanswer-type hierarchy, we assumed that questions thatexhibited a high degree of similarity necessarily encodedWhat types of weapons are available?What kinds of attacks similar to the 9?11 attack are possible?What defenses are there against conventional attacks?What types were used in the past, and where did they attack?What evidence of WMD capability by terrorists exists?
What weapons have been used by terrorists in the past?What threats have been made?
What sorts of threats were they, and where were they targetted at?What unconventional weapons do those groups own?What conventional weapons do those groups own?What sorts of people will they target?What are their specific goals?What organizations are interested in attacking targets on US soil?How difficult is it to engage in a 9?11 style attack?How difficult is it to obtain conventional weapons?What kinds of conventional weapons are available?What kinds of attacks have been thwarted recently?
What are the major targets on US soil?Assuming another terrorist attack occurs on US soil, is it more likely to be a conventional attack or an attack using a weapon of mass destruction?COMPLEX QUESTION:TOPIC DECOMPOSITION + USER DECOMPOSITIONWhat are the risks associated with WMD use?How credible are these threats?What kind of delivery systems do they have?What are the capabilities of those organizations?What kinds of WMD are available?WMD?What defenses are there against WMD attacks?How difficult is it to engage in attacks on the US?Figure 2: Example of questions entered in QUAB.similar informational goals and could be answered withsimilar types of information.
When a new question wasjudged to be conceptually similar to a question in QUAB,the QUAB question?s answer was returned to users asa potential answer.
Questions that were not similar toany existing question in QUAB were submitted automat-ically to our Q/A system without providing any additionalfeedback to users.
This type of methodology allowed usto develop a series of Just-in-Time-Information-Search-Agents (JITISA) which exploited different measures ofconceptual and lexical similarity in order to identify an-swers to domain-specific questions.2.3 Users of Complex Q/AThe performance of interactive Q/A systems can be im-proved by identifying what strategies different users em-ploy to reach their informational goals.
We define aninformational goal as the propositional knowledge thata user is trying to obtain by participating in a dialoguewith a Q/A system.
We suggest that the representationof an informational goal depends crucially on the spe-cific knowledge that individual users bring to the interac-tion with the Q/A system.
Users that possess little or noknowledge of a particular domain will necessarily seeka different level of information than users who are in-timately familiar with the domain.
Based on these as-sumptions, we propose that interactive Q/A systems besensitive to two kinds of users: (1) expert users, whomay be expected to interact with the system based on aworking knowledge of the semantic ontology underlyinga domain, and (2) novice users, who are expected to haveno foreknowledge of the ontological categories specific tothe domain.
By examining the differences in information-seeking techniques employed by expert users (such as in-telligence analysts) and novice users (such as NLP re-searchers), we can better identify user intentions andwork towards anticipating the information needs of anyuser.3 Question DecompositionsWe suggest that question decomposition can be ap-proached in two ways.
The first approach generates a setof questions related to the complex question by maximiz-ing the extraction of information related to the domain.In this way, the Q/A user is provided with full coverageof the information associated with the concepts expressedin the complex question.
This methodology seeks to ap-proximate domain-specific knowledge.
The idea is thatby caching information associated with the domain, thedomain coverage is maximized and the likelihood thatthe retrieved answers meet the users?
information needs isenhanced.
The questions that extract relevant domain in-formation are clustered in related sub-topics and generatea bottom-up decomposition of the complex question.The second approach generates a top-down decompo-sition by monitoring user strategies towards decompo-sition.
The purpose is to derive general relations be-tween topic-specific questions and the subquestions thatthey entail.
Such relations are discovered by combiningdomain specific knowledge with general coherence rela-tions.
The domain knowledge selects decompositions vi-able in the context of a domain scenario, whereas the co-herence relations connect questions of different levels ofcomplexity.In recognition of these diverse goals, we hypothesizethat research in question decomposition should followtwo parallel tracks: topic-centric and user-centric.
Thesetwo proposed strategies have different strengths.
Theuser-centric strategy mimics the user?s intentions whenresolving an information-seeking task but may miss rele-vant information since not all the right questions may becovered.
In contrast, the topic-centric strategy generatesgood recall, but it relies on similarity functions that arehard to encode.
Section 3.1 presents topic-centric work.The rest of the section is organized in the followingway.
Section 3.2 presents user-centric work.
Section 3.3speculates about the contribution of each form of decom-position to interactions with the Q/A system.
We arguethat there are optimal ways for combining advances ineach approach to provide a unified treatment of questiondecomposition.3.1 Topic-Centric MethodAnswers to a complex question are retrieved from a set oftopic-relevant documents.
In our experiments, we haveused two sets of such documents.
In the first pilot eval-uations, we have created our own corpus of documentsrelevant to the topics proposed.
The corpus combineddocuments from Lexus-Nexus with documents we havegathered from the Internet.
The relevance of the docu-ments was provided by the presence of certain conceptswe deemed characteristic for each domain.
In the sec-ond pilot, we used the documents provided by the Centerfor Non-Proliferation Studies (CNS) that were consideredrelevant based on the concepts that could be derived fromthe complex questions and their decompositions.COMPLEX QUESTION:Despite having complete access, to this day UN inspections have been unable tofind any biological weapons, or remnants thereof, in Iraq.
Why has it proven difficultto discover hard information about Iraq?s biological weapons program and what arethe implications of these difficulties fot the intrnational biological arms control regime?RELEVANT CONCEPTSUN inspections comlete access biological weaponsbiological weapons program difficulty biological arms control regimeWhat is the nature of the UN inspector team ?What kinds of technology do UN inspectors have?Do UN inspectors have access to all public, private and government facilities?How might the nature of Iraq aid the government in hiding a bioweapons program?Does the natural terrain provide natural hiding places for bioweapons?How likely is it that Iraq could destroy the biowepons program with no trace?IraqinformationFigure 3: Topic-centric decomposition.As illustrated in Figure 3, topic-relevant conceptsguide the generation of questions than are easier to pro-cess.
Because the simpler questions contain conceptsused also in the complex question, they are related to it.Figure 3 shows several questions created by question pat-terns for which (1) there was at least one text snippet inthe collection that matched a trigger word; and (2) con-tained at least one of the relevant concepts.
When therelevant concept was ?UN inspectors?
the trigger wordswere: ?team?, ?technology?
and ?facilities?, which aretypical of inspections.Questions generated by topic-centric decompositioncould be related to multiple relevant concepts.
Figure 3illustrates also a set of questions that related both to the?Iraq?
concept and to the ?bioweapons program?
con-cept.
The latter concept is a synonym of the concept ?bi-ological weapons program?.3.2 User-Centric MethodDifferent users might decompose a complex question dif-ferently.
By producing an analysis of the complex ques-tion and of the questions produced by each user we canexplore several different paths of searching for the rele-vant information of a complex question.
Additionally, thedifferent paths indicate the kind of topic knowledge eachuser has available.
It also indicates the level of expertiseof each user.In analyzing the complex question, we focus on (1)the focus of the question; (2) the context of the ques-tion and (3) the implied results.
Since complex questionsmay consist of multiple sentences and interrogatives, weproduce such tree-dimensional structures for each sen-tence/interrogative of the complex question, as illustratedin Figure 4.
Figure 4 also lists a set of questions thatmay be derived from the structure associated with eachquestion constituent.
It may be noticed that these ques-tions have multiple natures.
Some can be cast as defini-tion questions, e.g.
What is a biological weapon ?.
Otherquestions are based on knowledge of each sub-topic.
Forexample, Did Iraq violate any international law?, impliesthat international laws govern the international biologicalarms control regime.COMPLEX QUESTION:Despite having complete access, to this day UN inspections have been unable tofind any biological weapons, or remnants thereof, in Iraq.BACKGROUNDTOPIC: biological weaponsCONTEXT: Iraq, UN inspectionsRESULTS: unable to find any bioweapons or remnantsContradiction: UN inspectors have complete accessWhat is a biological weapon?How was complete access granted to UN inspectors?How lilkely is it that any inspector could detect any bioweapon?What limits did the UN inspectors have ?intrnational biological arms control regime?weapons program and what are the implications of these difficulties for theWhy has it proven so difficult to discover hard information about Iraq?s biologicalCOMPLEX QUESTION: CONTINUATIONTOPIC: inspections for biological weaponsCONTEXT: Iraq, UN inspectionsRESULTS: implications for the international biological arms control regimeWhat is the international biological arms control regime?Did Iraq violate any international law?Figure 4: User-centric decomposition.User-centric decompositions are based on the idea thateach user generates a sequence of questions that repre-sents a path from the complex question to a series of ques-tions that are connected through coherence relations ofthe type ELABORATION or CAUSE-EFFECT.
Since def-inition and list questions are also used, the set of coher-ence relations needs to be adapted for the task of interac-tive Q/A.3.3 Experiments with Expert UsersThis section presents a brief case study comparing de-compositions produced by three users of different skilllevels.We collected three decompositions of the followingcomplex scenario: Despite having complete access, tothis day UN inspections have been unable to find anybiological weapons, or remnants thereof, in Iraq.
Whyhas it proven difficult to discover hard information aboutIraq?s biological weapons program and what are the im-plications of these difficulties for the international bio-logical arms control regime?.
This scenario asks users toelaborate about a state of affairs: namely, the failure ofUN weapons inspectors to find evidence of a biologicalweapons program in Iraq.
In addition, users are asked toreturn information about (1) the potential causes of thisstate as well as (2) the expected effects of the contin-ued duration of the state on the ?biological arms controlregime?.
(2) (a) What is a biological weapon?
(2) (b) Is it, for example, a quantity of pathogens or toxins, or is there more to it?COMPLEX QUESTION:Despite having complete access, to this day UN inspections have been unable tofind any biological weapons, or remnants thereof, in Iraq.
Why has it proven difficultto discover hard information about Iraq?s biological weapons program and what arethe implications of these difficulties fot the intrnational biological arms control regime?DECOMPOSITION:accessing sites and facilities?
(1) (a) Is there such a concept as "complete access" or are there inevitably limits torelated systems, they would be found by inspectors?an acceptable level of assurance that were there biological weapons and/or(1) (b) If there are such limits, can inspections in fact be carried out effectively; i.e., with(3) (a) What are the likely signatures of a national biological weapons program andhow likely is it that inspectors from the outside would be able to detect them?
(4) (a) What are the constituent parts of the "international arms control regime" in thecontext of biological weapons?
(4) (b) Does it, for example, solely consist ofthe 1972 Biological and Toxin WeaponsConvention (BWC), ir is there more to it?
(5) (a) Since Iraq was only a signatory (not retifier) of the BWC during the time it wasdeveloping and producing biological weapons (1985?1991), were its actionsin this regard contrary to international law?
(5) (b) If not, did the international community have a different recourse to designatethe Iraqi government as having violated international law or norms by havingacquired biological weapons?Figure 5: Scenario decomposition provided by NIST.We predict that the domain-specific knowledge thatusers possess will directly influence how they performquestion decomposition.
If a user has overt knowledgethat causality can exist between the state described inthe scenario and another set of states or events, then weshould expect decomposition to proceed in an evidentiarymode.
Since the user has evidence that two states maybe causally linked in another domain, subquestions areasked in order to gather information that describes howthis causal relationship is instantiated in the current do-main of interest.
However, if a user only has a beliefor an expectation (and no overt knowledge) that a causallink can be established between two states, we expectdecomposition to be more general and epistemic in na-ture.
Since the user has only a belief that causality existsbetween two states, they must first confirm that this ex-pectation is viable before they can turn to gathering in-formation which supports their claim.
Since they have(by definition) a better conception of the semantic ontol-ogy for the domain, expert users will ask a higher per-centage of factoid and evidence-seeking questions thannovice users.
Likewise, we expect that that the decom-positions of novice users will be characterized by moregeneral questions that seek to evaluate which ontologicalrelationships are available in a particular domain.Although these predictions may prove difficult to eval-uate in many real texts, they do appear to borne out in thefollowing decompositions.NIST Decomposition.
Figure 5 presents the scenariodecomposition generated by NIST as part of the ARDAAQUAINT project.
This decomposition focuses on fourmajor topics: (1) the nature of ?complete access?
interms of the UN inspections in Iraq, (2) the definition ofthe term ?biological weapon?, (3) the potential sourcesof evidence which would point to the existence of a bi-ological weapons program, and finally, (4) the clarifica-tion of international laws concerning biological weapons.Although these four topics are clearly central to the do-main, it is notable that this decomposition does not in-clude any questions that address the issue of finding bio-logical weapons in Iraq.What was the scope of Iraq?s biological weapons program?
In the past?Immediately prior to US invasion?What quantities of biological weapons has Iraq used in past wars?
In other periods?Within Iraq?
Against Iran?Does Iraq have the infrastructure necessary for destroying biological weaponssafely?
For creating biological weapons?Does Iraq have the capacity to store and/or transport biological weapons?
By land?By air?
By sea?
How has that capacity chanced since 1991?Are there personnel within the Iraqi government responsible for destroying biologicalweapons?
Are these people civilians or military personnel?Are there Iraqi personnel (scientist, clerks, military) that we can identify who havebeen traditionally associated with the Iraqi bioweapons program?
What are theirnames?
In what capacity did they participate in the bioweapons program?warfare sickness or contamination?Is there evidence from Iraqi military medical records for possible signs of biologicalIs there evidence from Iraqi civilian hospital records of doctors who have treatedpossible biological weapon sicknesses?
Are there individuals who have witnessedcases of biological weapon sicknesses?Has Iraqi military trained personnel in the use of biological weapons?
At any timein the past 12 years?Does Iraq have any military units tasked with using biological weapons?
Are thoseunits still active?
When were they disbanded?Which countries have been formally allied with Iraq?
Since 1991?Is there evidence that countries may have stored bioweapons for Iraq?
Is therein the past?evidence that other countries have engaged in similar kinds of deals with IraqFigure 6: Scenario decomposition created by an Intelli-gence Analyst.Analyst Decomposition.
Figure 6 presents a sce-nario decomposition generated by an intelligence ana-lyst as part of a pilot study conducted by LCC.
(Formore details on this pilot study, see Section 4.2) In con-trast with the NIST decomposition, this decompositionfocuses on establishing factual evidence for several dif-ferent hypotheses concerning the failure of inspectors tofind bioweapons in Iraq.LCC Decomposition.
Finally, Figure 7 presents a sce-nario decomposition created by a novice LCC researcherwho had no specific training in analysis techniques and nospecific background in the domain.
Although the LCC re-searcher produced considerably more questions than thetwo experts, these questions focus mostly on discover-ing the classes of hypotheses and conceptual relationsthat are found in the domain.
Questions like What doWhat is the nature of a bioweapons program?
What is its goal?
What kinds oftraces does an active bioweapons program leave?weapons?
What sort of equipment?
What sort of chemicals?
What sort of technology?What exactly is a bioweapon?
What kind of infrastructure is requried to make bio?Does Iraq have the infrastructure necessary to produce bioweapons?
How long doesweapons facilities produce?
What is the desired output?
What is the waste output?it take to put together a bioweapons faci;ity?
What kinds of products do most bio?What would consitute ?hard evidence?
of the existence of a bioweapons program?hard evidence??
What is the likelihood that outside inspectors could find these traces?What signatures would a bioweapons program leave?
How much would be needed to beis it that Iraq could hide the program with no traces?
How likely is it that Iraq couldHow could a government or an organization hide a bioweapons program?
How likelydestroy the program with no traces?Have the inspection teams found evidence of bioweapons programs in other countries?Which countries?How might the nature of Iraq aid the government in hiding bioweapons program?What about the natural geography?
How large is Iraq?
Does the natural terrainprovide natural hiding places for bioweapons?Would Iraqi citizens aid the government in hiding a bioweapons program?
Are Iraqicitizens still loyal to the Iraqi government?What is the nature of the UN inspector team?
How many inspectors are there?
Howexperienced are they?
What kinds of technology do the UN inspectors use to find bio?weapons?
What kinds of intelligence do they have?
Do they have informants in Iraq?Do inspectors have access to all public, private and government facilities?
Currently?In the past?
Before the 1991 war?What does complete access mean?
Is there an official definition?
Criteria?What do we know about Iraq?s bioweapons program in the past?
Since 1991?
Whatevidence do we have about its existence?
What products did they produce?When was the last time Iraq produced bioweapons?
How much bioweapon did theyproduce?
Was it exported to anyone?
Was it tested?Where do we get information about Iraqi bioweapons programs?
How reliable is it?Figure 7: Scenario decomposition created by LCC re-searchers.we know about Iraq?s bioweapons program in the past?suggest the researcher?s informational goals were definedat a much more general level than either of the two ex-perts.
In addition, the LCC researcher?s questions pro-vided a broader coverage of the topics within the domain;although this demonstrates that the researcher did havesome familiarity with issues central to the domain, it alsosignifies that he most likely did not have access to knowl-edge that would have allowed him to evaluate which con-cepts were most central to the informational focus definedin the scenario.Comparison.
Although all three of the decomposi-tions above cover many of the same topics (e.g.
the na-ture of bioweapons and bioweapons programs, the Iraqiinfrastructure for supporting bioweapons programs, etc.
),they differ in the the level of specificity of their questions.While both expert and novice decompositions do includequestions that establish domain-specific definitions forparticular keywords or phrases (e.g.
What constitutes?complete access?
for inspectors?
), questions in the ex-pert decompositions appear to focus more on gatheringevidence for particular hypotheses, while questions innovice decompositions focus more on establishing whichkinds of hypotheses are viable in the given domain.
Thisobservation is supported by comparing analogous exam-ples from the decompositions above.
While the expertanalyst was able to ask a rather specific question aboutIraq?s past use of biological weapons that demonstratedan in-depth knowledge of the geopolitical entities in thedomain (i.e.
What quantities of biological weapons hasIraq used in past wars?
In other periods?
Within Iraq?Against Iran?
), the novice LCC user was only able toquestion whether or not the event had occurred at all (i.e.Has Iraq ever used biological weapons?
).The results from this case study appears to confirm thatinteractive Q/A systems need to be sensitive to the inten-tions users bring to their interaction with the system.
TheGricean maxim of quantity is supported in these cases:users participate in dialogues in order to obtain informa-tion that they do not already possess.
Given this assump-tion, we predict that users?
decompositions of complexscenarios focus on questions that allow them to maxi-mize the new information obtained from the system whileminimizing the amount of old (or previously-known) in-formation that the system returns.4 Lessons LearnedThis section presents preliminary results from two ex-periments examining scenario decomposition in an in-teractive Q/A context.
In Section 4.1 we discuss resultswhich confirm that a database of question/answer pairscan be used to approximate the types of specific seman-tic knowledge necessary to process (and answer) domain-dependent complex questions.
In Section 4.2, we outlinefive strategies for question decomposition employed byexperts that could be use to improve the automatic pro-cessing of complex information-seeking scenarios.4.1 Results of the Interactions based on QUABRecent research has shown that the precision of Q/A sys-tems is dependent on the semantic coverage of their an-swer type hierarchies.
For most current interactive Q/Asystems, correct answers can only be returned iff a sys-tem is able to identify the answer type that most closelyapproximates a question?s informational goal.
In mostcases, if the Q/A system cannot identify an appropriateanswer type ?
or if the answer type does not exist in thesemantic ontology ?
no answer can be returned.However, as we pointed out in Section 3.2, ontol-ogy creation may not be possible (or effective) for ev-ery semantic domain that users ask about.
In orderto answer domain-dependent questions, interactive Q/Asystems need to incorporate ways of approximating thedomain-specific information that their answer type hier-archies may lack.
In this section, we present results thatshow that a database of question/answer pairs (known inour system as QUAB) can be used to improve interactiveQ/A for domains that may not have completely specifiedanswer-type hierarchies.The utility of QUAB was evaluated in a series of two?Wizard-of-Oz?-style dialogue pilot experiments con-ducted as part of the ARDA AQUAINT project.
Ineach pilot, professional intelligence analysts interactedwith LCC developers (and the LCC interactive Q/A sys-tem) through an Internet chat-style interface.
LCC usedDomain Answers User Qs QUAB Qs %Q from QUAB %A from QUABIndia - Prithvi 8 7 4 36.4% 50.0%Iraq - Bioweapons 18 9 9 50.0% 50.0%Russia - Nuclear Thefts 18 14 6 30.0% 33.3%China - Arms Control 20 7 2 33.3% 10.0%Iraq - Nuclear Program 25 5 2 28.6% 8.0%Russia - North Korea 27 8 4 33.3% 14.8%Total 116 50 27 35.1% 23.3%Table 2: Results from the second pilotDomain Full Partial Not at all TotalOpium in Afghanistan 3 2 1 6AIDS in Africa 3 2 3 8Black Sea Pollution 5 2 7FARC/Colombia 2 3 5Indonesian Economy 8 3 2 13Cell Phones in 3 6 2 11Ivory CoastJapanese Joint Ventures 2 4 6Microsoft and Viruses 3 1 4Elizardo Sanchez 5 1 6Robotic Surgery 5 4 9Total 39 16 20 75Table 1: Pilot 1 System Effectivenessthe preparation time prior to the first experiment toseed QUAB with 140 domain-specific question/answerpairs.
182 additional question/answer pairs (based on 6of the 12 Spring 2003 AQUAINT domains) were addedto QUAB prior to the second pilot experiment as well.QUAB was primarily used to return answers for domain-specific questions that our interactive Q/A system couldnot process.
Each user question was evaluated in termsof keyword and conceptual similarity with all of thequestion-answer pairs contained in QUAB; if no answercould be provided to the user?s question, the most sim-ilar QUAB answers were returned.
In results compiledfrom both pilots, QUAB provided exactly the correct an-swer 52% (39/75) of the time, and either exactly or par-tially the correct answer 73% (55/75) of the time.
Table 1presents these results organized by question domain.QUAB was also used to provide an interactive com-ponent to our Q/A system as well.
Each question sub-mitted by a user was compared to the database of ques-tion/answer pairs already contained in QUAB.
If theuser?s question was deemed to be conceptually similar toan entry in QUAB, the user was informed that the systemcould return information ?related?
to the user?s question.If a user requested this related information, the QUABentry was presented to the user in the form of a ques-tion/answer pair.
For example, when a user asked thequestion What facilities has Iraq used to produce biolog-ical weapons?, QUAB offered the answer to How doesthe US know about the existence of biological weaponsplants in Iraq?
as related information that could poten-tially facilitate the user?s research.In the second dialogue pilot, question/answer pairsfrom QUAB were presented to the users a total of 27times in 6 different dialogues.
On average, contributionsfrom QUAB made up approximately 35% of the ques-tions and about 23% of the answers considered by usersthroughout the course of the dialogue.
Table 2 presentsresults from the 6 domains considered in the second dia-logue pilot.
(It is important to note that in this pilot, userscould ask the Q/A system to return more answers for anyquestion; this explains why there are often more answersthan questions in each of the dialogues.
)The success of a relatively small QUAB suggest thatthis type of database construction may be an efficient wayto augment interactive Q/A and answer-type detection forvery domain-specific questions.4.2 Results Produced by ExpertsIn this section, we present work from a pilot study that ex-amined how intelligence analysts performed question de-compositions for domains within their areas of expertise.After presenting a case study comparing their individualdecompositions of a domain, we identify five differentdecomposition strategies employed by the analysts.In order to obtain more high-quality data for analysis,we invited three intelligence analysts from the Naval Re-serve to LCC for three days of study.
We were interestedin (1) determining whether users of a specific level of ex-pertise performed decompositions of complex questionsin a similar fashion and (2) identifying possible patternsin their research styles that could be used in the develop-ment of automatic question decomposition strategies.Analysts participated in three tasks.
For the first task,analysts were asked to create short outlines (dubbed?skeleton reports?)
of answers to complex questions us-ing only publicly-available web-based resources.
For thesecond and third tasks, analysts were asked to provide de-compositions of complex questions.
In the second task,analysts were asked to list the questions that they antici-pated they would answer prior to starting their research;in the third task, analysts decomposed questions withoutany other special instructions.
For purposes of compari-son, a LCC developer participated also participated in thedecomposition tasks.Despite their similar levels of training and exper-tise, the differences in the analysts?
individual styleswere striking.
When asked to decompose the ?Iraqibioweapons?
scenario presented in Figures 5 , 6, and 7,analysts produced questions that demonstrated broad dif-ferences in their interpretation of the scenario itself.Analyst 1.
Analyst 1?s decomposition focused on fourspecific aspects of Iraq?s bioweapons program: (1) thehistory of the bioweapons program, (2) the alleged prod-ucts of the program, (3) the personnel involved with cre-ation of the program, and (4) the potential locations forprogram.
Although this analyst?s 10 questions were well-balanced, his decomposition centered on the nature of theprogram itself, and provided no potential for an explana-tion of why the weapons were difficult to find.Analyst 2.
Analyst 2?s decomposition questionedmany of the implicit assumptions set forth in the topicquestion itself.
Instead of providing subquestions thatcould have led to potential answers for this topic ques-tion, his decomposition suggested that he had rejectedthe propositions that the topic question was based on.
Inhis 18 subquestions, he questioned the presuppositions ofthe scenario itself, generating subquestions such as Is itthe case that the UN inspectors are really being denied?complete access??
and Can we be sure that Iraq hadbioweapons at any point in the past?.Analyst 3.
Analyst 3?s decomposition focused on thereasons he believed were responsible for the difficulty infinding Iraq?s bioweapons.
In his 17 questions, he dis-cussed three real hypotheses: (1) the weapons do notexist, (2) the weapons are well hidden in Iraq, (3) theweapons have been moved outside of Iraq.
After identi-fying these three hypotheses, Analyst 3 asked a variety ofsubquestions that gathered evidence for (or against) eachof these three possibilities.Although the analysts produced roughly similar num-bers of subquestions, there was little overlap in the con-tent that they covered.
This suggests that even expert an-alysts differ markedly in their expectations of what con-stitutes the informational goal of a complex information-seeking scenario.In examining the decompositions produced by the ana-lysts, we discovered that the analysts employed a numberof distinct decomposition strategies.
Four of them arediscussed below:Syntactic Decomposition.
Analysts split questionsthat featured syntactic coordination into subquestions thatcontained each of the individual conjuncts.
For exam-ple, a question like How do we know that the UN has notfound any biological or chemical weapons?
was decom-posed as How do we know that the UN has not found anybiological weapons?
and How do we know that the UNhas not found any chemical weapons?
In the data we col-lected, we only found examples of adjective phrase andnoun phrase conjunction; we expect analysts to decom-pose examples of sentence or verb phrase coordination ina similar fashion.Entity Motivations.
Analysts asked questions aboutan entity?s political or economic motives if the topic ques-tion involved a predicate that implied that the entity hadvolitional control over its actions.
For example, a topicquestion like Why does China dispute Taiwan?s indepen-dence?
was decomposed into questions like What areChina?s economic motives for disputing Taiwan?s inde-pendence?
or What are China?s political motives for dis-puting Taiwan?s independence?.State Discovery.
When faced with a question aboutthe existence of a property or past state, analysts gener-ated decompositions that contrasted the previous statusand its current status.
For example, questions like Whattype of nuclear assistance did China give to the MiddleEast between 1980 and 1990?
were routinely decom-posed into questions of the form How does the nuclearassistance given by China to the Middle East from 1980to 1990 compare to nuclear assistance it provides to theMiddle East today?.These subquestions took three forms.
Analysts wantedto know: (1) how the situation in the past differs from thepresent situation, (2) what caused the change from thepast to the present, and (3) what impact the past eventshave on the present.
We hypothesize that the above sub-questions are part of a larger class of subquestions knownas state discovery questions.
Unlike events, which rep-resent a particular moment in time (or set of moments),states are inherently durative and therefore are subject toa wider variety of changes in scope, level, or status overtime.
We believe that questions that make reference to aproperty or a state of being necessarily make an implicitcomparison between periods in time: i.e.
an identifiedpoint (such as the years between 1980 - 1990) and someother reference point (either the current moment or someother salient period).Meronymy.
We found that analysts were sensitive tothe internal structure of many of the named entities ref-erenced in topic questions.
In general, analysts generatedquestions about the subparts of an entity if and only if in-formation about those subparts proved informative in an-swering the topic question as a whole.
Given an examplelike Where are Prithvi missiles manufactured?, analystsgenerated decompositions like Where are the guidancesystems for Prithvi missiles manufactured?
or Where arethe warheads for Prithvi missiles manufactured?.
Furtherresearch is needed to determine when an entity?s compo-nent parts should be considered as part of the informa-tional goal of a question.When faced with topic questions that present a contro-versial or empirically-unverified proposition, we foundthat analysts generated decompositions that questionedthe relative truth of the proposition before generatingother decompositions.
Faced with a complex questionlike How much nuclear material was stolen from the So-viet military after the fall of the Soviet Union?
(whichnecessarily entails that nuclear material was, in fact,stolen from the Soviet military), analysts generated de-compositions like How much nuclear material is knownto have been stolen from the Soviet military?
or Howmuch nuclear material is suspected to have been stolenfrom the Soviet military?
Analysts did not generate thesetypes of questions, however, when the question under dis-cussion reflected a publicly accepted proposition or anempirically-verifiable state.5 ConclusionsIn this paper we presented a new framework for the de-composition of complex information-seeking scenarios.We proposed that question decomposition for interactiveQ/A could be approached in one of two ways: either (1)by approximating the domain-specific knowledge for aparticular set of domains or (2) by identifying the decom-position strategies employed by human users.
In addi-tion, we presented preliminary experimental results thatconfirmed the viability of both of these approaches.
Wediscussed two years?
worth of experiments that investi-gated how users of varying levels of expertise decom-posed complex scenarios, as well as work that describedhow a database of question/answer pairs could be used toimprove the coverage of Q/A systems for domain-specificquestions.ReferencesC.
Fellbaum.
1998.
WordNet: An Electronic Lexical Databaseand Some of its Applications.
MIT Press.S.
Harabagiu, D. Moldovan, C. Clark, M. Bowden, J. Williams,J.
Besley.
2003.
Answer Mining by Combining ExtractionTechniques with Abductive Reasoning.
In Proceedings ofthe Twelfth Text Retrieval Conference (TREC 2003)D. Moldovan, C. Clark, S. Harabagiu, S. Maiorano.
2003.
CO-GEX: A Logic Prover for Question Answering.
In Proceed-ings of the Human Language Technology and North Ameri-can Chapter of the Association for Computational Linguis-tics Conference (HLT-NAACL 2003)E. Voorhees.
2003.
Overview of the TREC 2003 Question An-swering Track.
In Proceedings of the Text REtrieval Confer-ence (TREC-2003), pages 14-27, 2003.
