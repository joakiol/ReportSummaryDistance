Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 33?39,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsTowards a Better Semantic Role Labeling of Complex PredicatesGlorianna JagfeldInstitute for Natural Language ProcessingUniversity of StuttgartPfaffenwaldring 5b, 70569 Stuttgart, Germanyjagfelga@ims.uni-stuttgart.deLonneke van der PlasInstitute of LinguisticsUniversity of MaltaTal-Qroqq, Msida, Maltalonneke.vanderplas@um.edu.mtAbstractWe propose a way to automatically improvethe annotation of verbal complex predicates inPropBank which until now has been treatinglanguage mostly in a compositional manner.In order to minimize the manual re-annotationeffort, we build on the recently introducedconcept of aliasing complex predicates to ex-isting PropBank rolesets which encompass thesame meaning and argument structure.
Wesuggest to find aliases automatically by ap-plying a multilingual distributional model thatuses the translations of simple and complexpredicates as features.
Furthermore, we setup an annotation effort to obtain a frequencybalanced, realistic test set for this task.
Ourmethod reaches an accuracy of 44% on thistest set and 72% for the more frequent testitems in a lenient evaluation, which is not farfrom the upper bounds from human annota-tion.1 IntroductionSemantic Role Labeling (SRL) aims at determining?who?
did ?what?
to ?whom?
in sentences by iden-tifying and associating predicates with their seman-tic arguments.
This information is useful for manydownstream applications, for example for questionanswering (Shen, 2007).
The PropBank corpus(PB) (Palmer et al, 2005) is one of the most widelyused resources for training SRL systems.
It providessenses of (mostly verbal) predicates with their typ-ical semantic arguments annotated in a corpus andaccompanied by a lexical resource.
The sense of apredicate is referred to as a ?roleset?
because it listsall required and possible semantic roles for the pred-icate used in a specific sense.The 12K rolesets in PB describe mostly singleword predicates, to a great part leaving aside multi-word expressions (MWEs).
Complex predicates(CPs), ?predicates which are multi-headed: theyare composed of more than one grammatical ele-ment?
(Ramisch, 2012), are most relevant in the con-text of SRL.
Light verb constructions (LVCs), e.g.take care, and verb particle constructions (VPCs),e.g.
watch out, are the most frequently occurringtypes of CPs.
As Bonial et al (2014) stated ?PB haspreviously treated language as if it were purely com-positional, and has therefore lumped the majority ofMWEs in with lexical verb usages?.
For examplethe predicates in the CPs take a hard line, take timeand many others are all annotated with a sense oftake, meaning acquire, come to have, chose, bringwith you from somewhere.
This results in a loss ofsemantic information in the PB annotations.This is especially critical because CPs are a fre-quent phenomenon.
The Wiki50 corpus (Vincze etal., 2011), which provides a full coverage MWE an-notation, counts 814 occurrences of LVCs and VPCsin 4350 sentences.
This makes for one CP in everyfifth sentence.Recently, Bonial et al (2014) have introduced anapproach to improve the handling of MWEs in PBwhile keeping annotation costs low.
The processis called aliasing.
Instead of creating new framesfor CPs, human annotators map them to existing PBrolesets which encompass the same semantic and ar-gument structure.
For example, the CP give (a) talkcould be mapped to the alias lecture.01.
While this33method significantly reduces the effort to create newrolesets, the time consuming manual mapping is stillrequired.
To address this problem, our work ex-tends this approach by proposing a method to findthe aliases automatically.One way to find the most suitable alias roleset fora given CP is to group predicates by their rolesets as-signed by an automatic SRL system and compute themost similar roleset group by searching for (near-)synonymous predicates of the CP.
The roleset of themost similar roleset group is selected as alias for theCP.Finding synonyms, both single-word and multi-word, from corpora has been done successfully withthe multilingual variant of the distributional hypoth-esis (Van der Plas and Tiedemann, 2006; Van derPlas et al, 2011).
The idea behind this approachis that words or MWEs that share many translationsare probably synonymous.
We use the word align-ments in a parallel corpus to find the translations ofCPs and single predicates.
The predicates are auto-matically annotated with rolesets by an SRL system.This allows us to compute the most suitable rolesetfor a given CP fully automatically.Our contributions are as follows: To the best ofour knowledge, this work is the first to address thehandling of CPs for SRL in an automatic way.
Weare thus able to scale up previous work that re-lies on manual intervention.
In addition, we set upan annotation effort to gather a frequency-balanced,data-driven evaluation set that is larger and more di-verse than the annotated set provided by Bonial etal.
(2014).2 Representing CPs for SRLPrevious work on representing CPs for SRL hasmostly focused on PB.
The currently available ver-sion of the PB corpus represents most CPs as if theywere lexical usages of the verb involved in the pred-icate.
Figure 1 shows an example for the annotationof the LVC take care in PB.1The CP is split up intoits two components that are each assigned their ownroleset.
This annotation ignores the semantic unityof the CP and is unable to capture its single meaningof being concerned with or caring for something.1We show an excerpt of the original sentence found in thecurrently available version of PB (Proposition Bank I).Frank takes care of businesstake.01 care.01WHO?WHAT?WHO?OF WHAT?Figure 1: Current PB representation of the CP take careFrank takes care of business(take+care).01WHO?OF WHAT?Figure 2: Improved representation of the CP take careadopted from (Hwang et al, 2010; Duran et al, 2011)In contrast to this, Hwang et al (2010) suggesta new annotation scheme for LVCs that assigns theargument structure of the LVC independently fromthe argument structure of its components.
First, thearguments of the light verb and true predicate areannotated with roles regarding their relationship tothe combination of the light verb and true predicate.Then, the light verb and predicate lemmas are joinedinto a single predicate.
The result of this process isshown in Figure 2.Duran et al (2011) discuss the analysis of Brazil-ian Portuguese CPs.
Similarly to Hwang et al(2010) they argue that CPs should be treated as sin-gle predicates, not only for LVCs but for all CPs.They automatically extract CP candidates from acorpus and represent, if possible, the meaning of theCPs with one or more single-verb paraphrases.Atkins et al (2003) describe a way in which LVCscan be annotated in FrameNet (Baker et al, 1998),a framework that describes the semantic argumentstructure of predicates with semantic roles specificto the meaning of the predicate.
In contrast to theproposals for PB by Hwang et al (2010) and Duranet al (2011), they suggest to annotate the light verband its counterpart separately.The aliasing process introduced by Bonial et al(2014) tries to extend the coverage of PB for CPswhile keeping the number of rolesets that should benewly created to a minimum.
Bonial et al (2014)conducted a pilot study re-annotating 138 CPs in-volving the verb take.
As a first step, the annotators34determined the meaning(s) of the CP by looking attheir usage in corpora.
If they found that the CP isalready adequately represented by the existing role-sets for take, no further action was needed (18/138).Otherwise, they were instructed to propose as aliasan existing PB entry that encompasses the same se-mantics and argument structure as the CP (100/138).If unable to find an alias, they could suggest to cre-ate a new roleset for this CP (20/138).
Expressionsfor which the annotators were unable to determinethe meaning were marked as idiomatic expressionsthat need further treatment (4/138).2According to this process, take care could bealiased to the existing PB roleset care.01 whoseentry is shown in Figure 3.
This alias replaces(take+care).01 shown in Figure 2 and thus avoidsthe creation of a new roleset.Roleset id: care.01, to be concernedArg0: carer, agentArg1: thing cared for/aboutFigure 3: alias PB roleset for the predicate take careEncouraged by the high proportion of CPs thatcould successfully be aliased in the pilot studyby Bonial et al (2014), we created a method to au-tomatically find aliases for CPs in order to decreasethe amount of human intervention, thereby scalingup the coverage of CPs in PB.3 MethodThe task of finding aliases for CPs automatically isrelated to finding (near-) synonymous predicates andtheir accompanying roleset for the CPs.
To find thenear-synonyms, we apply the distributional hypoth-esis which states that we can assess the similarityof expressions by looking at their contexts (Firth,1957).
As previous work (Van der Plas and Tiede-mann, 2006) has shown that multilingual contextswork better for synonym acquisition than monolin-gual syntactic contexts, we use the translations of theCPs and other predicates to all 20 languages avail-able via the word alignments in a multilingual paral-lel corpus as context.Figure 4 shows an overview of the architecture of2Note that the numbers do not add up to 138 because fourMWEs obtained two different strategies.Extract CPs andPB roleset groupsparallel corpus+ lemma+ POS+ synt.
dependencies+ SRL+ word alignmentsPopulate matrixwith translationcounts (alignmentsof CPs and PBroleset groups)For each CPvector calculatesimilarity witheach PB rolesetgroup vectoralias:roleset with thehighest similar-ity scoreFigure 4: Overview of the alias finderour system.
First, we extract the CPs and all pred-icates that share a PB roleset (PB roleset groups)from the parallel corpus.
For example, all verbsthat were assigned to the roleset care.01 by the SRLsystem belong to the PB roleset group of care.01.The CPs stem from the gold standard MWE annota-tion in the Wiki50 corpus (Vincze et al, 2011).
Weparsed this corpus to obtain lemmas, POS and syn-tactic dependencies and extracted this informationfor all VPCs and LVCs annotated in the corpus.3Figure 5 shows the two patterns we identified thatthe majority of the CPs followed.4We used thesetwo patterns to search for occurrences of the CPs inEuroparl.lemmaPOSdependencytake care give upVERB NOUN VERBObjectParticleFigure 5: Patterns used for finding occurrences of CPsNext, we build a co-occurrence matrix contain-ing as head terms the CP and all PB roleset groupsfound in the parallel corpus.
Figure 6 shows a toyexample of such a matrix for the CP take care.
The3We concentrate on VPCs and LVCs because they are themost frequent types of CP in English.4Here we use the example CPs take care and give up, but thelemmas were of course introduced as variables.35head words are listed in the rows, the translations(i.e.
features) in the columns.
Note that in contrastto previous work on distributional semantics we in-clude PB roleset groups as head words.
These con-tain several distinct verbal predicates but they sharethe same sense.
Consequently, polysemous verbs arefound in several distinct PB roleset groups.ter cui-dado (es)achten(de)prendresoin (fr)pensera (fr)take care 3 3 5 0care.01 4 3 7 1think.01 0 2 1 6Figure 6: Toy example co-occurrence matrixFinally, we measure the similarity between CPsand roleset groups using the cosine similarity be-cause it worked best in previous experiments forfinding synonyms (Van der Plas, 2008).
This resultsin a similarity ranking of PB roleset groups for eachCP, from which we select the roleset with the highestcosine value as alias.4 Experiments4.1 Tools and DataWe processed the English section of the Europarlcorpus (Koehn, 2005) (about 2 million sentences)with the MATE tools (Bj?orkelund et al, 2010) toobtain lemmas, part-of-speech (POS) tags, depen-dency structures and semantic role labels.
These an-notations are used to find occurrences of the CPs andwords assigned with PB rolesets in the English part.The word alignments produced with the grow-diag-final-and-heuristics (Koehn et al, 2003) provided bythe OPUS project (Tiedemann, 2012) are then usedto find their alignments to all other 20 languages inthe corpus and exploited as features in the distribu-tional model.4.2 Evaluation FrameworkHuman Annotation.
In order to evaluate our sys-tem, we set up an annotation effort loosely follow-ing the guidelines provided by Bonial et al (2014).We selected 50 LVCs and 50 VPCs from the Wiki50corpus (Vincze et al, 2011) divided equally overtwo frequency groups: Half of the expressions oc-cur only once in the Wiki50 corpus (low-frequencysubgroup) and the other half occur at least twice(high-frequency subgroup).
All occurrences of these100 CP types in the corpus were selected to accountfor the polysemy of CPs.
Different instances of thesame CP could get assigned to different aliases.
Thisresulted in a total of 197 annotated instances.Four annotators were presented with the CP intheir original sentence context and were asked topropose one or several PB aliases which encompassthe same meaning and argument structure.
One an-notator (A, one of the authors of this article) labeledthe whole set of 100 expressions.
The three otherannotators (B,C,D) each labeled one third of the ex-pressions assigned randomly, so that every expres-sion was annotated by two annotators.First, they were asked to decide if there is al-ready an appropriate PB roleset for the CP andthen provide it.
The annotators were requested todivide these cases into semantically compositionalCPs (e.g.
obtain permission with the roleset ob-tain.01) and uncompositional CPs for which PB al-ready provides a multi-word predicate (e.g.
open.03for open up).
For the remaining CPs, they wereasked to suggest PB rolesets (aliases) that share thesame semantics and argument structure as the CP.The simple inter-annotator agreement5was 67%for annotator A%B, 51% for A&C and 44% forA&D.
These agreement figures are higher than thefigures in Bonial et al (2014), and actual agreementis probably even higher, because synonymous role-sets are regarded as disagreements.
Annotator A dis-cussed the annotations with the other annotators andthey were able to reach a consensus that resulted ina final agreed-upon test set.Table 1 shows the final decisions with respect tothe complete set of 197 expressions.
In line with theresults from Bonial et al (2014) who aliased 100 outof 138 uncompositional take MWEs, we were alsoable to alias most of the CPs in our annotation set.The final Wiki50 set consists of 1547instances of5Kappa scores (Cohen, 1960) are not suited to the presentmulti-label and multi-class setting: Annotators could choosefrom roughly 6K classes and were encouraged to provide mul-tiple synonymous rolesets.6Discarded CPs contained spelling or annotation errors inthe Wiki50 corpus.7We removed two CPs from the ?aliased?
group because ourextraction patterns do not cover LVCs formed with an adjective.36Decision Count MWE examplealiased 96 take partmulti-word PB pred.
60 open upcompositional 18 obtain permissionno alias found 16 go into politicsdiscarded67 take conrolTable 1: Final decisions on the 197 annotated expressionsCPs from the categories ?aliased?
and ?multi-wordPB predicate?
(low-frequency: 34, high-frequency:120).
The latter were included because the predictedroleset of the SRL only coincides with the gold stan-dard for 23 out of 60 instances.
This means that forthe majority of the CPs, even if an adequate PB role-set exists, this roleset was not selected by the SRLsystem.
We hope to also improve these cases withour method.
All CPs were labeled with one to fourappropriate PB alias rolesets.In addition, we evaluated our system on thedataset from Bonial et al (2014), restricted to thetype of CP our system handles (LVCs and VPCs)and verb aliases (as opposed to aliases being a nounor adjective roleset).
We used 70 of the 100 MWEsfrom their annotations.Evaluation Measures and Baseline.
We reportthe accuracy of our system?s predictions as com-pared to the gold standard.
For the STRICT AC-CURACY, an alias is counted as correct if it corre-sponds exactly to one of the gold aliases.
This eval-uation is very rigid and regards synonymous role-sets as incorrect.
Thus, we also compute a more LE-NIENT ACCURACY, which counts an alias as correctif it belongs to the same VerbNet (Kipper-Schuler,2006) verb class as the gold alias.
VerbNet (VN) isa hierarchically organized lexicon of English verbs.It consists of syntactically and semantically coher-ent verb classes, which are extensions of the classesproposed by Levin (1993).
For the PB-VN map-pings, we rely on the resource provided by the Sem-Link project8(Loper et al, 2007) and use the most-specific (deepest) layer of the verb classes.
Since themapping provided in SemLink is not complete (only58% of the rolesets found in PB have a mapping toa corresponding VN class), we discard rolesets thatare not found in SemLink, unless they are correct8http://verbs.colorado.edu/semlink/according to the gold standard in the first place.We compared our system with a baseline systemthat distinguishes between VPCs and LVCs.
ForVPCs, it checks whether there exists a PB multi-word predicate for the expression and selects the firstroleset of that predicate (e.g.
there exists a pred-icate called open up (open.03) for the VPC ?openup?).
For LVCs, it checks whether the noun has acorresponding verb predicate in PB and selects thefirst roleset of this predicate (e.g.
walk.01 for takea walk).
Note that this is an informed baseline thatis very hard to beat and only fails in case of lack incoverage.5 Results and DiscussionWe evaluated our approach on the 160 CPs anno-tated in the course of this work (Wiki50 set), as wellas on the 70 take CPs from Bonial et al (2014) (takeset) and compare our results to the baseline.
Table 2shows percentage coverage, accuracy and the har-monic mean of coverage and accuracy for our sys-tem and the baseline.
We report results on the twoevaluation sets in the strict and lenient evaluation.The first five rows of Table 2 show the results forthe Wiki50 set and its subsets.
We see that our sys-tem scores 44.1 accuracy on the whole test set in thestrict evaluation and 69.0 in the lenient evaluation.These numbers seem quite low, but they are not thatfar apart from the micro averaged IAA from our an-notation effort (53%).
Our system outperforms thebaseline with very high coverage numbers.
It beatsthe baseline in terms of the harmonic mean for allsubsets except the multiword PB predicate subset.This is not surprising as the test items in this subsethave a corresponding multiword PB predicate andall the baseline has to do is select the right sense.The high performance of the baseline on the multi-word PB predicates leads to the high accuracy num-bers for the baseline in all (sub-)sets except fromthe alias subset, which contains the expressions forwhich a true alias was provided.
Our system beatsthe baseline in terms of strict accuracy for the aliassubset.
This is good news because the actual taskis to find new aliases for CPs that are not covered inPB.
The performance on the low-frequency subset islower than on the high-frequency subset, as expectedfor a distributional method.37Set Strict Cov Strict Acc Strict Hm Lenient Cov Lenient Acc Lenient HmWiki50 all 98.7 (65.6) 44.1 (54.5) 60.9 (59.5) 98.0 (59.5) 69.0 (85.9) 81.0 (70.3)alias 98.9 (50.0) 36.6 (34.0) 53.4 (40.5) 98.4 (40.5) 60.0 (68.8) 74.5 (51.0)mw.
PB pred.
98.3 (86.7) 55.9 (71.2) 71.3 (78.1) 97.6 (84.6) 82.5 (97.7) 89.4 (90.7)high-freq.
100.0 (68.3) 45.0 (52.4) 62.1 (59.3) 100.0 (62.7) 72.0 (84.4) 83.7 (72.0)low-freq.
94.1 (50.0) 40.6 (58.5) 56.8 (54.1) 92.6 (41.4) 60.0 (91.7) 72.8 (57.0)take 67.1 (71.4) 25.5 (32.0) 37.0 (44.2) 56.6 (64.9) 60.0 (45.0) 58.3 (53.8)Table 2: Percentage coverage (Cov), accuracy (Acc) and the harmonic mean (Hm) of coverage and accuracy of thepredicted aliases in the Wiki50 set (+ four of its subsets) and the take set; The results of the baseline are in bracketsThe results on the take set are shown in the lastrow of Table 2.
Compared to the Wiki50 set, theyare substantially lower.
We would like to stress thatthe take set is far from what we expect to find in anactual corpus.
This set comprises only CPs that con-tain the word take.
Many test items have been ex-tracted from WordNet and possibly have a very lowfrequency in a general corpus.
This is reflected inthe coverage number, which shows the proportion ofCPs for which our system was able to suggest at leastone alias: It is above 94% for all Wiki50 (sub)sets,but only 67% for the take set.
We constructed theWiki50 set to allow us to get a better estimate ofhow our method would fare in a natural setting.5.1 Error analysisWe examined all expressions from the full Wiki50set for which the top ranked predicted alias was in-correct.
Due to space limitations we only mentionthe main reasons for errors we identified.
First ofall, the limited language domain of the Europarl cor-pus caused a low frequency of some rolesets selectedas gold alias, like fuse.01 (?melt into lump?)
for theVPC melt down.
This problem could be solved byadding more parallel data from different domains.Another source of errors is the fact that our ap-proach requires the output of an SRL system which,in turn, we want to improve.
For 45 out of 160 CPsour system suggested the roleset as alias that was as-signed to the verb by the SRL system, e.g.
leave.02for leave for.
But the automatically attributed role-set is only correct in 21 cases, which means that wereproduced the errors of the SRL in 24 cases.Some LVCs keep their light verb structure in otherlanguages, i.e.
they receive multi-word translations.This diminishes the overlap of translations betweenthe LVC and the PB roleset groups.
PB rolesets areassigned to simplex verbs and therefore predomi-nantly receive simplex translations.
As more fre-quent rolesets have more diverse translations thatcontain more MWEs, these are promoted as aliases.Applying frequency weights to the roleset matrixcould remedy this problem.Lastly, our system adheres to the most frequentsense baseline due to lack of word sense disam-biguation of the CPs and assigns the alias that fitsthe most dominant sense of the CP in the corpus.6 ConclusionsWe have presented an approach to handle CPs inSRL that extends on work from Bonial et al (2014).We automatically link VPCs and LVCs to the PBroleset that best describes their meaning, by rely-ing on word alignments in parallel corpora and dis-tributional methods.
We set up an annotation ef-fort to gather a frequency-balanced, contextualizedevaluation set that is more natural, varied and largerthan the pilot annotations provided by Bonial et al(2014).
Our method can be used to alleviate themanual annotation effort by providing a correct aliasin 44% of the cases (up to 72% for the more frequenttest items when taking synonymous rolesets into ac-count).
These results are not too far from the upperbounds we calculate from human annotations.In future work, we would like to improve ourmethod by incorporating the methods discussed inthe error analysis section.
Additionally, we plan toevaluate the impact of the new CP representation ondownstream applications by retraining an SRL sys-tem on the new annotations.AcknowledgmentsWe thank Anna Konobelkina and two anonymous annota-tors for their efforts as well as the anonymous reviewers.38ReferencesSue Atkins, Charles J. Fillmore, and Christopher R. John-son.
2003.
Lexicographic relevance: selecting infor-mation from corpus evidence.
International Journalof Lexicography, 16.3.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 36th Annual Meeting of the Associationfor Computational Linguistics and 17th InternationalConference on Computational Linguistics - Volume 1,ACL ?98, Stroudsburg, PA, USA.Anders Bj?orkelund, Bernd Bohnet, Love Hafdell, andPierre Nugues.
2010.
A high-performance syntac-tic and semantic dependency parser.
In Coling 2010:Demonstrations, Beijing, China.Claire Bonial, Meredith Green, Jenette Preciado, andMartha Palmer.
2014.
An approach to take multi-wordexpressions.
In Proceedings of the 10th Workshop onMultiword Expressions (MWE), Gothenburg, Sweden.Jacob Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Educational and Psychological Mea-surement, 20(1).Magali Sanches Duran, Carlos Ramisch, Sandra MariaAlu?
?sio, and Aline Villavicencio.
2011.
Identify-ing and analyzing brazilian portuguese complex pred-icates.
In Proceedings of the Workshop on MultiwordExpressions: From Parsing and Generation to the RealWorld, MWE ?11, Stroudsburg, PA, USA.John Rupert Firth.
1957.
A synopsis of linguistic theory1930-55.
1952-59.Jena D. Hwang, Archna Bhatia, Clare Bonial, Aous Man-souri, Ashwini Vaidya, Nianwen Xue, and MarthaPalmer.
2010.
Propbank annotation of multilin-gual light verb constructions.
In Proceedings of theFourth Linguistic Annotation Workshop, LAW IV ?10,Stroudsburg, PA, USA.Karin Kipper-Schuler.
2006.
VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon.
Ph.D. thesis,University of Pennsylvania.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, Stroudsburg, PA, USA.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the Tenth Machine Translation Summit,Phuket, Thailand.Beth Levin.
1993.
English verb classes and alternations:a preliminary investigation.
University of ChicagoPress, Chicago and London.Edward Loper, Szu-Ting Yi, and Martha Palmer.
2007.Combining lexical resources: Mapping between prop-bank and verbnet.
In Proceedings of the 7th In-ternational Workshop on Computational Linguistics,Tilburg, the Netherlands.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpusof semantic roles.
Computational Linguistics Journal,31(1).Carlos Ramisch.
2012.
A generic and open frameworkfor multiword expressions treatment: from acquisitionto applications.
Ph.D. thesis, University of Grenoble(France) and Federal University of Rio Grande do Sul(Brazil).Dan Shen.
2007.
Using semantic role to improve ques-tion answering.
In Proceedings of EMNLP 2007.J?org Tiedemann.
2012.
Parallel data, tools and inter-faces in opus.
In Proceedings of the Eight Interna-tional Conference on Language Resources and Evalu-ation (LREC?12), Istanbul, Turkey.Lonneke van der Plas and J?org Tiedemann.
2006.
Find-ing synonyms using automatic word alignment andmeasures of distributional similarity.
In Proceedingsof ACL-COLING 2006, Sydney, Australia.Lonneke van der Plas, J?org Tiedemann, and IsmailFahmi.
2011.
Automatic extraction of medical termvariants from multilingual parallel translations.
InInteractive Multi-modal Question Answering, Theoryand Applications of Natural Language Processing.Springer-Verlag, Berlin.Lonneke van der Plas.
2008.
Automatic lexico-semanticacquisition for question answering.
Ph.D. thesis, Uni-versity of Groningen.Veronika Vincze, Istv?an Nagy T., and G?abor Berend.2011.
Multiword expressions and named entities inthe wiki50 corpus.
In Proceedings of the Interna-tional Conference Recent Advances in Natural Lan-guage Processing 2011, Hissar, Bulgaria.39
