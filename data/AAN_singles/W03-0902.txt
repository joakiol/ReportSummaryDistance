Extracting and Evaluating General World Knowledge from the BrownCorpusLenhart SchubertUniversity of Rochesterschubert@cs.rochester.eduMatthew TongUniversity of Rochestermt004i@mail.rochester.eduAbstractWe have been developing techniques for ex-tracting general world knowledge from miscel-laneous texts by a process of approximate inter-pretation and abstraction, focusing initially onthe Brown corpus.
We apply interpretive rulesto clausal patterns and patterns of modifica-tion, and concurrently abstract general ?possi-bilistic?
propositions from the resulting formu-las.
Two examples are ?A person may believea proposition?, and ?Children may live withrelatives?.
Our methods currently yield over117,000 such propositions (of variable quality)for the Brown corpus (more than 2 per sen-tence).
We report here on our efforts to eval-uate these results with a judging scheme aimedat determining how many of these propositionspass muster as ?reasonable general claims?about the world in the opinion of human judges.We find that nearly 60% of the extracted propo-sitions are favorably judged according to ourscheme by any given judge.
The percentageunanimously judged to be reasonable claims bymultiple judges is lower, but still sufficientlyhigh to suggest that our techniques may be ofsome use in tackling the long-standing ?knowl-edge acquisition bottleneck?
in AI.1 Introduction: deriving generalknowledge from textsWe have been exploring a new method of gaining gen-eral world knowledge from texts, including fiction.
Themethod does not depend on full or exact interpretation,but rather tries to glean general facts from particulars bycombined processes of compositional interpretation andabstraction.
For example, consider a sentence such as thefollowing from the Brown corpus (Kucera and Francis,1967):Rilly or Glendora had entered her room while sheslept, bringing back her washed clothes.From the clauses and patterns of modification of this sen-tence, we can glean that an individual may enter a room, afemale individual may sleep, and clothes may be washed.In fact, given the following Treebank bracketing, our pro-grams produce the output shown:((S(NP(NP (NNP Rilly) )(CC or)(NP (NNP Glendora) ))(AUX (VBD had) )(VP (VBN entered)(NP (PRP\$ her) (NN room) ))(SBAR (IN while)(S(NP (PRP she) )(VP (VBD slept) )))(\, \,)(S(NP (\-NONE\- \*) )(VP (VBG bringing)(PRT (RB back) )(NP (PRP\$ her) (JJ washed) (NNS clothes) ))))(\.
\.)
)A NAMED-ENTITY MAY ENTER A ROOM.A FEMALE-INDIVIDUAL MAY HAVE A ROOM.A FEMALE-INDIVIDUAL MAY SLEEP.A FEMALE-INDIVIDUAL MAY HAVE CLOTHES.CLOTHES CAN BE WASHED.
((:I (:Q DET NAMED-ENTITY) ENTER[V] (:Q THE ROOM[N]))(:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET ROOM[N]))(:I (:Q DET FEMALE-INDIVIDUAL) SLEEP[V])(:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V](:Q DET (:F PLUR CLOTHE[N])))(:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A]))The results are produced as logical forms (the last fivelines above ?
see Schubert, 2002, for some details), fromwhich the English glosses are generated automatically.Our work so far has focused on data in the Penn Tree-bank (Marcus et al, 1993), particularly the Brown corpusand some examples from the Wall Street Journal corpus.The advantage is that Treebank annotations allow us topostpone the challenges of reasonably accurate parsing,though we will soon be experimenting with ?industrialstrength?
parsers on unannotated texts.We reported some specifics of our approach and somepreliminary results in (Schubert, 2002).
Since then wehave refined our extraction methods to the point where wecan reliably apply them the Treebank corpora, on averageextracting more than 2 generalized propositions per sen-tence.
Applying these methods to the Brown corpus, wehave extracted 137,510 propositions, of which 117,326are distinct.
Some additional miscellaneous examplesare ?A PERSON MAY BELIEVE A PROPOSITION?, ?BILLSMAY BE APPROVED BY COMMITTEES?, ?A US-STATE MAYHAVE HIGH SCHOOLS?, ?CHILDREN MAY LIVE WITH RELA-TIVES?, ?A COMEDY MAY BE DELIGHTFUL?, ?A BOOK MAYBE WRITE-ED (i.e., written) BY AN AGENT?, ?A FEMALE-INDIVIDUAL MAY HAVE A SPOUSE?, ?AN ARTERY CAN BETHICKENED?, ?A HOUSE MAY HAVE WINDOWS?, etc.The programs that produce these results consist of (1) aTreebank preprocessor that makes various modificationsto Treebank trees so as to facilitate the extraction of se-mantic information (for instance, differentiating differentkinds of ?SBAR?, such as S-THAT and S-ALTHOUGH,and identifying certain noun phrases and prepositionalphrases, such as ?next Friday?, as temporal); (2) a pat-tern matcher that uses a type of regular-expression lan-guage to identify particular kinds of phrase structure pat-terns (e.g., verb + complement patterns, with possible in-serted adverbials or other material); (3) a semantic pat-tern extraction routine that associates particular semanticpatterns with particular phrase structure patterns and re-cursively instantiates and collects such patterns for thepreprocessed tree, in bottom-up fashion; (4) abstractionroutines that abstract away modifiers and other ?type-preserving operators?, before semantic patterns are con-structed at the next-higher level in the tree (for instance,stripping the interpreted modifier ?washed?
from the in-terpreted noun phrase ?her washed clothes?
); (5) routinesfor deriving propositional patterns from the resulting mis-cellaneous semantic patterns, and rendering them in asimple, approximate English form; and (6) heuristic rou-tines for filtering out many ill-formed or vacuous propo-sitions.
In addition, semantic interpretation of individualwords involves some simple morphological analysis, forinstance to allow the interpretation of (VBD SLEPT) interms of a predicate SLEEP[V].In (Schubert, 2002) we made some comparisons be-tween our project and earlier work in knowledge extrac-tion (e.g., (muc, 1993; muc, 1995; muc, 1998; Berlandand Charniak, 1999; Clark and Weir, 1999; Hearst, 1998;Riloff and Jones, 1999)) and in discovery of selectionalpreferences (e.g., (Agirre and Martinez, 2001; Grishmanand Sterling, 1992; Resnik, 1992; Resnik, 1993; Zernik,1992; Zernik and Jacobs, 1990)).
Reiterating briefly, wenote that knowledge extraction work has generally em-ployed carefully tuned extraction patterns to locate andextract some predetermined, specific kinds of facts; ourgoal, instead, is to process every phrase and sentence thatis encountered, abstracting from it miscellaneous generalknowledge whenever possible.
Methods for discoveringselectional preferences do seek out conventional patternsof verb-argument combination, but tend to ?lose the con-nection?
between argument types (e.g., that a road maycarry traffic, a newspaper may carry a story, but a road isunlikely to carry a story); in any event, they have not ledso far to amassment of data interpretable as general worldknowledge.Our concern in this paper is with the evaluation of theresults we currently obtain for the Brown corpus.
Theoverall goal of this evaluation is to gain some idea ofwhat proportion of the extracted propositions are likelyto be credible as world knowledge.
The ultimate test ofthis will of course be systems (e.g., QA systems) that usesuch extracted propositions as part of their knowledgebase, but such a test is not immediately feasible.
In themeantime it certainly seems worthwhile to evaluate theoutputs subjectively with multiple judges, to determineif this approach holds any promise at all as a knowledgeacquisition technique.In the following sections we describe the judgingmethod we have developed, and two experiments basedon this method, one aimed at determining whether ?lit-erary style makes a difference?
to the quality of outputsobtained, and one aimed at assessing the overall successrate of the extraction method, in the estimation of severaljudges.2 Judging the output propositionsWe have created judging software that can be used by theresearchers and other judges to assess the quality and cor-rectness of the extracted information.
The current schemeevolved from a series of trial versions, starting initiallywith a 3-tiered judging scheme, but this turned out tobe difficult to use, and yielded poor inter-judge agree-ment.
We ultimately converged on a simplified scheme,for which ease of use and inter-judge agreement are sig-nificantly better.The following are the instructions to a judge using thejudger program in its current form:Welcome to the sentence evaluator for the KNEXT knowledge ex-traction program.
Thank you for your participation.
You will beasked to evaluate a series of sentences based on such criteria ascomprehensibility and truth.
Do your best to give accurate re-sponses.
The judgement categories are selected to try to ensurethat each sentence fits best in one and only one category.
Help isavailable for each menu item, along with example sentences, byselecting ?h?
; PLEASE consult this if this is your first time usingthis program even if you feel confident of your choice.
There isalso a tutorial available, which should also be done if this is yourfirst time.
If you find it hard to make a choice for a particular sen-tence even after carefully considering the alternatives, you shouldprobably choose 6 (HARD TO JUDGE)!
But if you strongly feelnone of the choices fit a sentence, even after consulting the helpfile, please notify Matthew Tong (mtong@cs.rochester.edu) to al-low necessary modifications to the menus or available help infor-mation to occur.
You may quit at any time by typing ?q?
; if you quitpartway through the judgement of a sentence, that partial judge-ment will be discarded, so the best time to quit is right after beingpresented with a new sentence.  here the first sentence to be judged is presented 1.
SEEMS LIKE A REASONABLE GENERAL CLAIM (Of course.
Yes.
)A grand-jury may say a proposition.
A report can be favorable.2.
SEEMS REASONABLE BUT EXTREMELY SPECIFIC OR OBSCURE(I suppose so)A surgeon may carry a cage.
Gladiator pecs can be Reeves-type.3.
SEEMS VACUOUS (That?s not saying anything)A thing can be a hen.
A skiff can be nearest.4.
SEEMS FALSE (No.
I don?t think so.
Hardly)A square can be round.
Individual -s may have a world.5.
SOMETHING IS OBVIOUSLY MISSING (Give me a complete sentence)A person may ask.
A male-individual may attach an importance.6.
HARD TO JUDGE (Huh??
How do you mean that?
I don?t know.
)A female-individual can be psychic.
Supervision can be with a company.Based on this judging scheme, we performed two typesof experiments: an experiment to determine whether lit-erary style significantly impacts the percentage of propo-sitions judged favorably; and experiments to assess over-all success rate, in the judgement of multiple judges.
Weobtained clear evidence that literary style matters, andachieved a moderately high success rate ?
but certainlysufficiently high to assure us that large numbers of po-tentially useful propositions are extracted by our meth-ods.
The judging consistency remains rather low, butthis does not invalidate our approach.
In the worst case,hand-screening of output propositions by multiple judgescould be used to reject propositions of doubtful validity orvalue.
But of course we are very interested in developingless labor-intensive alternatives.
The following subsec-tions provide some details.2.1 Dependence of extracted propositions onliterary styleThe question this experiment addressed was whether dif-ferent literary styles correlated with different degrees ofsuccess in extracting intuitively reasonable propositions.The experiment was carried out twice, first by one of theauthors (who was unaware of the contents of the files be-ing sampled) and the second time by an outside recruit.While further experimentation is desirable, we believethat the evidence from two judges that literary style corre-lates with substantial differences in the perceived qualityof extracted propositions demonstrates that future workon larger corpora should control the materials used forliterary style.Judgements were based on 4 Brown files (ck01, ck13,cd02, cd01).
The 4 files were chosen by one of us onpurely subjective grounds.
Each contains about 2,200words of text.
(Our extraction methods yield about 1proposition for every 8 words of text.
So each file yieldsabout 250-300 propositions.)
The first two, ck01 andck13, are straightforward, realistic narratives in plain, un-adorned English, while cd01 and cd02 are philosophicaland theological essays employing much abstract and fig-urative language.
The expectation was that the first twotexts would yield significantly more propositions judgedto be reasonable general claims about the world than thelatter two.
To give some sense of the contents, the firstfew sentences from each of the texts are extracted here:Initial segments of each of the four textsck01: Scotty did not go back to school.
His parents talked seri-ously and lengthily to their own doctor and to a specialistat the University Hospital?
Mr. McKinley was entitledto a discount for members of his family?
and it was de-cided it would be best for him to take the remainder of theterm off, spend a lot of time in bed and, for the rest, dopretty much as he chose?
provided, of course, he choseto do nothing too exciting or too debilitating.
His teacherand his school principal were conferred with and everyoneagreed that, if he kept up with a certain amount of work athome, there was little danger of his losing a term.ck13: In the dim underwater light they dressed and straight-ened up the room, and then they went across the hall tothe kitchen.
She was intimidated by the stove.
He foundthe pilot light and turned on one of the burners for her.
Thegas flamed up two inches high.
They found the teakettle.And put water on to boil and then searched through theicebox.cd01: As a result, although we still make use of this distinction,there is much confusion as to the meaning of the basicterms employed.
Just what is meant by ?spirit?
and by?matter???
The terms are generally taken for granted asthough they referred to direct and axiomatic elements inthe common experience of all.
Yet in the contemporarycontext this is precisely what one must not do.
For in themodern world neither ?spirit?
nor ?matter?
refer to anygenerally agreed-upon elements of experience...cd02: If the content of faith is to be presented today in a formthat can be ?understanded of the people??
and this, it mustnot be forgotten, is one of the goals of the perennial the-ological task?
there is no other choice but to abandoncompletely a mythological manner of representation.
Thisdoes not mean that mythological language as such can nolonger be used in theology and preaching.
The absurdnotion that demythologization entails the expurgation ofall mythological concepts completely misrepresents Bult-mann?s intention.Extracted propositions were uniformly sampled fromthe 4 files, for a total count of 400, and the number ofjudgements in each judgement category were then sep-arated out for the four files.
In a preliminary version ofthis experiment, the judgement categories were still the 3-level hierarchical ones we eventually dropped in favor ofa 6-alternatives scheme.
Still, the results clearly indicatedthat the ?plain?
texts yielded significantly more propo-sitions judged to be reasonable claims than the moreabstract texts.
Two repetitions of the experiment (withnewly sampled propositions from the 4 files) using the 6-category judging scheme, and the heuristic postprocess-ing and filtering routines, yielded the following unequiv-ocal results.
(The exact sizes of the samples from filesck01, ck13, cd01, and cd02 in both repetitions were 120,98, 85, and 97 respectively, where the relatively highcount for ck01 reflects the relatively high count of ex-tracted propositions for that text.
)  For ck01 and ck13 around 73% of the propositions(159/218 for judge 1 and 162/218 for judge 2) werejudged to be in the ?reasonable general claim?
cat-egory; for cd01 and cd02, the figures were muchlower, at 41% (35/85 for judge 1 and 40/85 for judge2) and less than 55% (53/97 for judge 1 and 47/97for judge 2) respectively.  For ck01 and ck13 the counts in the ?hard to judge?category were 12.5-15% (15-18/120) and 7.1-8.2%(6-7/85) respectively, while for cd01 and cd02 thefigures were substantially higher, viz., 25.9-28.2%(22-24/85) and 19.6-23% (19-34/97) respectively.Thus, as one would expect, simple narrative textsyield more propositions recognized as reasonable claimsabout the world (nearly 3 out of 4) than abstruse an-alytical materials (around 1 out of 2).
The questionthen is then how to control for style when we turnour methods to larger corpora.
One obvious answeris to hand-select texts in relevant categories, such asliterature for young readers, or from authors whosewritings are realistic and stylistically simple (e.g., Hem-ingway).
However, this could be quite laborious sincelarge literary collections available online (such as theworks in Project Gutenberg, http://promo.net/pg/,http://www.thalasson.com/gtn/, with expiredcopyrights) are not sorted by style.
Thus we expect to useautomated style analysis methods, taking account of suchfactors as vocabulary (checking for esoteric vocabularyand vocabulary indicative of fairy tales and other fancifulfiction), tense (analytical material is often in presenttense), etc.
We may also turn our knowledge extractionmethods themselves to the task: if, for instance, we findpropositions about animals talking, it may be best to skipthe text source altogether.2.2 Overall quality of extracted propositionsTo assess the quality of extracted propositions over a widevariety of Brown corpus texts, with judgements made bymultiple judges, the authors and three other individualsmade judgements on the same set of 250 extracted propo-sitions.
The propositions were extracted from the thirdof the Brown corpus (186 files) that had been annotatedwith WordNet senses in the SEMCOR project (Landes etal., 1998) (chiefly because those were the files at handwhen we started the experiment ?
but they do represent abroad cross-section of the Brown Corpus materials).
Weexcluded the cj-files, which contain highly technical ma-terial.Table 1 shows the judgements of the 5 judges (as per-centages of counts out of 250) in each of the six judge-ment categories.
The category descriptions have beenmnemonically abbreviated at the top of the table.
Judge1 appears twice, and this represents a repetition, as a testof self-consistency, of judgements on the same data pre-sented in different randomized orderings.reasonable obscure vacuous false incomplete hard9.6 0.4 7.6 12.89.6 0.4 7.2 11.654.8 14.8 5.6 8.8 5.26.4 3.2 7.68.4 4.860.0 9.661.649.010.49.22.88.412.410.022.5judge 1judge 1judge 2judge 4 64.0judge 5judge 358.4 4.4 0.8 2.8 10.0 23.2Judgements (in %) for 250 randomly sampled propositionsTable 1.As can be seen from the first column, the judges placedabout 49-64% of the propositions in the ?reasonable gen-eral claim?
category.
This result is consistent with the re-sults of the style-dependency study described above, i.e.,the average lies between the ones for ?straightforward?narratives (which was nearly 3 out of 4) and the ones forabstruse texts (which was around 1 out of 2).
This is anencouraging result, suggesting that mining general worldknowledge from texts can indeed be productive.One point to note is that the second and third judge-ment categories need not be taken as an indictment of thepropositions falling under them ?
while we wanted to dis-tinguish overly specific, obscure, or vacuous propositionsfrom ones that seem potentially useful, such propositionswould not corrupt a knowledge base in the way the othercategories would (false, incomplete, or incoherent propo-sitions).
Therefore, we have also collapsed our data intothree more inclusive categories, namely ?true?
(collaps-ing the first 3 categories), ?false?
(same as the original?false?
category), and ?undecidable?
(collapsing the lasttwo categories).
The corresponding variant of Table 1would thus be obtained by summing the first 3 and last2 columns.
We won?t do so explicitly, but it is easy toverify that the proportion of ?true?
judgements compriseabout three out of four judgements, when averaged overthe 5 judges.We now turn to the extent of agreement among thejudgements of the five judges (and judge 1 with himselfon the same data).
The overall pairwise agreement resultsfor classification into six judgement catagories are shownin Table 2.judge 190.156.9 10.461.7 62.457.358.5judge 1judge 2judge 3judge 4judge 554.556.0 49.3judge 2 judge 3 judge 4Table 2.
Overall % agreement among judgesfor 250 propositions60.1A commonly used metric for evaluating interrater relia-bility in categorization of data is the kappa statistic (Car-letta, 1996).
As a concession to the popularity of thatstatistic, we compute it in a few different ways here,though ?
as we will explain ?
we do not consider it par-ticularly appropriate.
For 6 judgement categories, kappacomputed in the conventional way for pairs of judgesranges from .195 to .367, averaging .306.
For 3 (more in-clusive) judgement categories, the pairwise kappa scoresrange from .303 to .462, with an average of .375.These scores, though certainly indicating a positivecorrelation between the assessments of multiple judges,are well below the lower threshold of .67 often employedin deciding whether judgements are sufficiently consis-tent across judges to be useful.
However, to see that thereis a problem with applying the conventional statistic here,imagine that we could improve our extraction methods tothe point where 99% of extracted propositions are judgedby miscellaneous judges to be reasonable general claims.This would be success beyond our wildest dreams ?
yetthe kappa statistic might well be 0 (the worst possiblescore), if the judges generally reject a different one out ofevery one hundred propositions!One somewhat open-ended aspect of the kappa statisticis the way ?expected?
agreement is calculated.
In the con-ventional calculation (employed above), this is based onthe observed average frequency in each judgement cate-gory.
This leads to low scores when one category is over-whelmingly favored by all judges, but the exceptions tothe favored judgement vary randomly among judges (asin the hypothetical situation just described).
A possibleway to remedy this problem is to use a uniform distri-bution over judgement categories to compute expectedagreement.
Under such an assumption, our kappa scoresare significantly better: for 6 categories, they range from.366 to .549, averaging .482; for 3 categories, they rangefrom .556 to .730, averaging .645.
This approaches, andfor several pairs of judges exceeds, the minimum thresh-old for significance of the judgements.1Since the ideal result, as implied above, would beagreement by multiple judges on the ?reasonableness?
ortruth of a large proportion of extracted propositions, itseems worthwhile to measure the extent of such agree-ment as well.
Therefore we have also computed the?survival rates?
of extracted propositions, when we re-ject those not judged to be reasonable general claims by  judges (or, in the case of 3 categories, not judged to betrue by   judges).
Figure 1 shows the results, where thesurvival rate for   judges is averaged over all subsets ofsize   of the 5 available judges.1 2 3 4 5.6.8.4.201Fraction of survivorsnumber of  concurring  judges"true" (3 categories)(6 categories)"reasonable general claim"by multiple judges.75.65.31.28.35.43.57 .59.52.55Fraction of propositions placed in best categoryFigure 1.Thus we find that the survival rate for ?reasonable gen-eral claims?
starts off at 57%, drops to 43% and then 35%for 2 and 3 judges, and drops further to 31% and 28% for4 and 5 judges.
It appears as if an asymptotic level above20% might be reached.
But this may be an unrealisticextrapolation, since virtually any proposition, no matterhow impeccable from a knowledge engineering perspec-tive, might eventually be relegated to one of the other 5categories by some uninvolved judge.
The survival ratesbased on 2 or 3 judges seem to us more indicative of thelikely proportion of (eventually) useful propositions thanan extrapolation to infinitely many judges.
For the 3-wayjudgements, we see that 75% of extracted propositionsare judged ?true?
by individual judges (as noted earlier),and this drops to 65% and then 59% for 2 and 3 judges.Though again sufficiently many judges may eventuallybring this down to 40% or less, the survival rate is cer-tainly high enough to support the claim that our methodof deriving propositions from texts can potentially deliververy large amounts of world knowledge.1The fact that for some pairs of judges the kappa-agreement(with this version of kappa) exceeds 0.7 indicates that with morecareful training of judges significant levels of agreement couldbe reached consistently.3 Conclusions and further workWe now know that large numbers of intuitively reason-able general propositions can be extracted from a cor-pus that has been bracketed in the manner of the PennTreebank.
The number of ?surviving?
propositions forthe Brown corpus, based on the judgements of multiplejudges, is certainly in the tens of thousands, and the dupli-cation rate is a rather small fraction of the overall number(about 15%).Of course, there is the problem of screening out, asfar as possible, the not-so-reasonable propositions.
Onestep strongly indicated by our experiment on the effect ofstyle is to restrict extraction to the kinds of texts that yieldhigher success rates ?
namely those written in straightfor-ward, unadorned language.
As we indicated, both styleanalysis techniques and our own proposition extractionmethods could be used to select stylistically suitable ma-terials from large online corpora.Even so, a significant residual error rate will remain.There are two remedies ?
a short-term, brute-force rem-edy, and a longer-term computational remedy.
The brute-force remedy would be to hand-select acceptable propo-sitions.
This would be tedious work, but it would stillbe far less arduous than ?dreaming up?
such proposi-tions; besides, most of the propositions are of a sort onewould not readily come up with spontaneously (?A per-son may paint a porch?, ?A person may plan an attack?,?A house may have a slate roof?, ?Superstition may blendwith fact?, ?Evidence of shenanigans may be gathered bya tape recorder?, etc.
)The longer-term computational remedy is to use awell-founded parser and grammar, providing syntacticanalyses better suited to semantic interpretation thanTreebank trees.
Our original motivation for using thePenn Treebank, apart from the fact that it instantly pro-vides a large number of parsed sentences from miscella-neous genres, was to determine how readily such parsesmight permit semantic interpretation.
The Penn Tree-bank pays little heed to many of the structural principlesand features that have preoccupied linguists for decades.Would these turn out to be largely irrelevant to seman-tics?
We were actually rather pessimistic about this,since the Treebank data tacitly posit tens of thousands ofphrase structure rules with inflated, heterogeneous right-hand sides, and phrase classifications are very coarse (no-tably, with no distinctions between adjuncts and com-plements, and with many clause-like constructs, whetherinfinitives, subordinate clauses, clausal adverbials, nom-inalized questions, etc., lumped together as ?SBAR?
?and these are surely semantically crucial distinctions).
Sowe are actually surprised at our degree of success in ex-tracting sensible general propositions on the basis of suchrough-and-ready syntactic annotations.Nonetheless, our extracted propositions in the ?some-thing missing?
and ?hard to judge?
categories do quiteoften reflect the limitations of the Treebank analyses.
Forexample, the incompleteness of the proposition ?A male-individual may attach an importance?
seen above as an il-lustration of judgement category 5 can be attributed to thelack of any indication that the PP[to] constituent of theverb phrase in the source sentence is a verb complementrather than an adjunct.
Though our heuristics try to sortout complements from adjuncts, they cannot fully makeup for the shortcomings of the Treebank annotations.
Ittherefore seems clear that we will ultimately need to baseknowledge extraction on more adequate syntactic analy-ses than those provided by the Brown annotations.Another general conclusion concerns the ease or dif-ficulty of broad-coverage semantic interpretation.
Eventhough our interpretive goals up to this point have beenrather modest, our success in providing rough semanticrules for much of the Brown corpus suggests to us thatfull, broad-coverage semantic interpretation is not veryfar out of reach.
The reason for optimism lies in the ?sys-tematicity?
of interpretation.
There is no need to hand-construct semantic rules for each and every phrase struc-ture rule.
We were able provide reasonably comprehen-sive semantic coverage of the many thousands of distinctphrase types in Brown with just 80 regular-expressionpatterns (each aimed at a class of related phrase types)and corresponding semantic rules.
Although our seman-tic rules do omit some constituents (such as prenominalparticiples, non-initial conjuncts in coordination, adver-bials injected into the complement structure of a verb,etc.)
and gloss over subtleties involving gaps (traces),comparatives, ellipsis, presupposition, etc., they are notradical simplifications of what would be required for fullinterpretation.
The simplicity of our outputs is due not somuch to oversimplification of the semantic rules, as to thedeliberate abstraction and culling of information that weperform in extracting general propositions from a specificsentence.
Of course, what we mean here by semantic in-terpretation is just a mapping to logical form.
Our projectsheds no light on the larger issues in text understandingsuch as referent determination, temporal analysis, infer-ence of causes, intentions and rhetorical relations, andso on.
It was the relative independence of the kind ofknowledge we are extracting of these issues that madeour project attractive and feasible in the first place.Among the miscellaneous improvements under consid-eration are the use of lexical distinctions and WordNetabstraction to arrive at more reliable interpretations; theuse of modules to determine the types of neuter pronounsand of traces (e.g., in ?She looked in the cookie jar, but itwas empty?, we should be able to abstract the propositionthat a cookie jar may be empty, using the referent of ?it?
);and extracting properties of events by making use of in-formation in adverbials (e.g.,, from ?He slept soundly?we should be able to abstract the proposition that sleepmay be sound; also many causal propositions can be in-ferred from adverbial constructions).
We also hope todemonstrate extraction results through knowledge elici-taton questions (e.g.,?What do you know about books??,etc.
)4 AcknowledgementsThe authors are grateful to David Ahn for contributingideas and for extensive help in preparing and processingBrown corpus files, conducting some of the reported ex-periments, and performing some differential analyses ofresults.
We also benefited from the discussions and ideascontributed by Greg Carlson and Henry Kyburg in thecontext of our ?Knowledge Mining?
group, and we ap-preciate the participation of group members and outsiderecruits in the judging experiments.
As well, we thankPeter Clark and Phil Harrison (at Boeing Company) fortheir interest and suggestions.
This work was supportedby the National Science Foundation under Grant No.
IIS-0082928.ReferencesEneko Agirre and David Martinez.
2001.
Learningclass-to-class selectional preferences.
In Proc.
of the5th Workshop on Computational Language Learning(CoNLL-2001), Toulouse, France, July 6-7,.Matthew Berland and Eugene Charniak.
1999.
Find-ing parts in very large corpora.
In Proc.
of the 37thAnn.
Meet.
of the Assoc.
for Computational Linguistics(ACL-99), Univ.
of Maryland, June 22 - 27,.Jean Carletta.
1996.
Assessing agreement on classifica-tion tasks: the kappa statistic.
Computational Linguis-tics, 22(2):249?254.Stephen Clark and David Weir.
1999.
An iterativeapproach to estimating frequencies over a semantichierarchy.
In Proc.
of the Joint SIGDAT Confer-ence on Empirical Methods in Natural Language Pro-cessing and Very Large Corpora.
Also available athttp://www.cogs.susx.ac.uk/users/davidw/research/papers.html.Ralph Grishman and John Sterling.
1992.
Acquisitionof selectional patterns.
In Proc.
of COLING-92, pages658?664, Nantes, France.Marti A. Hearst.
1998.
Automated discovery of Word-Net relations.
In Christiane Fellbaum, editor, Word-Net: An Electronic Lexical Database, pages 131?153?MIT Press.H.
Kucera and W.N.
Francis.
1967.
Computational Anal-ysis of Present-Day American English.
Brown Univer-sity Press, Providence, RI.Shari Landes, Claudia Leacock, and Randee I. Tengi.1998.
Building semantic concordances.
In Chris-tiane Fellbaum, editor, WordNet: An Electronic Lexi-cal Database, pages chapter 8, 199?216.
MIT Press,Cambridge, MA.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330, June.1993.
Proc.
of the 5th Message Understanding Confer-ence (MUC-5).
Morgan Kaufmann, Los Altos, CA.1995.
Proc.
of the 6th Message Understanding Confer-ence (MUC-6).
Morgan Kaufmann, Los Altos, CA.1998.
Proc.
of the 7th Message Understanding Confer-ence (MUC-7).
Morgan Kaufmann, Los Altos, CA,April 29 ?
May 1, Virginia.P.
Resnik.
1992.
A class-based approach to lexical dis-covery.
In Proc.
of the 30th Ann.
Meet.
of the Assoc.for Computational Linguistics (ACL-92), pages 327?329, Newark, DE.P.
Resnik.
1993.
Semantic classes and syntactic ambigu-ity.
In Proc.
of ARPA Workshop on Human LanguageTechnology, Plainsboro, NJ.Ellen Riloff and Rosie Jones.
1999.
Learning dictio-naries for information extraction by multi-level boot-strapping.
In Proc.
of the 16th Nat.
Conf.
on ArtificialIntelligence (AAAI-99).Lenhart K. Schubert.
2002.
Can we derive general worldknowledge from texts?
In Proc.
of 2nd Int.
Conf.
onHuman Language Technology Research (HLT 2002),pages 94?97, San Diego, CA, March 24-27.Uri Zernik and Paul Jacobs.
1990.
Tagging for learning:Collecting thematic relations from corpus.
In Proc.of the 13th Int.
Conf.
on Computational Linguistics(COLING-90), pages 34?39, Helsinki.Uri Zernik.
1992.
Closed yesterday and closed minds:Asking the right questions of the corpus to distin-guish thematic from sentential relations.
In Proc.
ofCOLING-92, pages 1304?1311, Nantes, France, Aug.23-28,.
