Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 40?49,Beijing, August 2010A Look inside the Distributionally Similar TermsKow Kurodakuroda@nict.go.jpJun?ichi Kazamakazama@nict.go.jpNational Institute of Information and Communications Technology (NICT)Kentaro Torisawatorisawa@nict.go.jpAbstractWe analyzed the details of aWeb-deriveddistributional data of Japanese nominalterms with two aims.
One aim is toexamine if distributionally similar termscan be in fact equated with ?semanti-cally similar?
terms, and if so to whatextent.
The other is to investigate intowhat kind of semantic relations con-stitute (strongly) distributionally similarterms.
Our results show that over 85%of the pairs of the terms derived fromthe highly similar terms turned out tobe semantically similar in some way.The ratio of ?classmate,?
synonymous,hypernym-hyponym, and meronymic re-lations are about 62%, 17%, 8% and 1%of the classified data, respectively.1 IntroductionThe explosion of online text allows us to enjoya broad variety of large-scale lexical resourcesconstructed from the texts in the Web in an un-supervised fashion.
This line of approach waspioneered by researchers such as Hindle (1990),Grefenstette (1993), Lee (1997) and Lin (1998).At the heart of the approach is a crucial workingassumption called ?distributional hypothesis,?
aswith Harris (1954).
We now see an impressivenumber of applications in natural language pro-cessing (NLP) that benefit from lexical resourcesdirectly or indirectly derived from this assump-tion.
It seems that most researchers are reason-ably satisfied with the results obtained thus far.Does this mean, however, that the distribu-tional hypothesis was proved to be valid?
Notnecessarily: while we have a great deal of con-firmative results reported in a variety of researchareas, but we would rather say that the hypothe-sis has never been fully ?validated?
for two rea-sons.
First, it has yet to be tested under the pre-cise definition of ?semantic similarity.?
Second,it has yet to be tested against results obtained ata truly large scale.One of serious problems is that we have seenno agreement on what ?similar terms?
mean andshould mean.
This paper intends to cast lighton this unsolved problem through an investiga-tion into the precise nature of lexical resourcesconstructed under the distributional hypothesis.The crucial question to be asked is, Can distri-butionally similar terms really be equated withsemantically similar terms or not?
In our investi-gation, we sought to recognize what types of se-mantic relations can be found for pairs of termswith high distributional similarity, and see wherethe equation of distributional similarity with se-mantic similarity fails.
With this concern, thispaper tried to factor out as many components ofsemantic similarity as possible.
The effort of fac-torization resulted in the 18 classes of semantic(un)relatedness to be explained in ?2.3.1.
Suchfactorization is a necessary step for a full valida-tion of the hypothesis.
To meet the criterion oftesting the hypothesis at a very large scale, weanalyzed 300,000 pairs of distributionally simi-lar terms.
Details of the data we used are givenin ?2.2.This paper is organized as follows.
In ?2, wepresent our method and data we used.
In ?3, wepresent the results and subsequent analysis.
In?4, we address a few remaining problems.
In ?5,we state tentative conclusions.402 Method and Data2.1 MethodThe question we need to address is how manysubtypes of semantic relation we can identify inthe highly similar terms.
We examined the ques-tion in the following procedure:(1) a.
Select a set of ?base?
terms B.b.
Use a similarity measure M to con-struct a list of n terms T = [ti,1, ti,2,.
.
.
,ti, j, .
.
.
, ti,n] where ti, j denotes the j-th most similarity term in T againstbi ?
B. P(k) are pairs of bi and ti,k, i.e.,the k-th most similar term to bi.c.
Human raters classify a portion Q ofthe pairs in P(k) with reference toa classification guideline prepared forthe task.Note that the selection of base set B can beindependent of the selection of T .
Note also thatT is indexed by terms in B.
To encode this, wewrite: T [bi] = [ti,1, ti,2,.
.
.
, ti, j, .
.
.
, ti,n].2.2 DataFor T , we used Kazama?s nominal term cluster-ing (Kazama and Torisawa, 2008; Kazama et al,2009).
In this data, base set B for T is one mil-lion terms defined by the type counts of depen-dency relations, which is roughly equated withthe ?frequencies?
of the terms.
Each base termin B is associated with up to 500 of the most dis-tributionally similar terms.
This defines T .For M, we used the Jensen-Shannon diver-gence (JS-divergence) base on the probabilitydistributions derived by an EM-based soft clus-tering (Kazama and Torisawa, 2008).
For con-venience, some relevant details of the data con-struction are described in Appendix A, but in anutshell, we used dependency relations as dis-tributional information.
This makes our methodcomparable to that used in Hindle (1990).
Thestatistics of the distributional data used were asfollows: roughly 920 million types of depen-dency relations1) were automatically acquired1)The 920 million types come in two kinds of contexttriples: 590 million types of (t, p,v) and 320 million typesfrom a large-scale Japanese Web-corpus calledthe Tsubaki corpus (Shinzato et al, 2008) whichconsists of roughly 100 million Japanese pageswith six billion sentences.
After excluding hapaxnouns, we had about 33 million types of nouns(in terms of string) and 27 million types of verbs.These nouns were ranked by type count of thetwo context triples, i.e., (t, p,v) and (n?, p?, t).
Bwas determined by selecting the top one millionterms with the most variations of context triples.2.2.1 Sample of T [b]For illustration, we present examples of theWeb-derived distributional similar terms.
(2)shows the 10 most distributionally similar terms(i.e., [t1070,1, t1070,2, .
.
.
, t1070,10] in T (b1070))where b1070 = ?????
(piano) is the 1070-thterm in B.
Likewise, (3) shows the 10 most dis-tributionally similar terms [t38555,1, t38555,2, .
.
.
,t38555,10] in T (b38555)) where b38555 = ??????????
(Tchaikovsky) is the 38555-th term in B.
(2) 10 most similar to ?????1.
??????
(Electone; electronic or-gan) [-0.322]2.
?????
(violin) [-0.357]3.
??????
(violin) [-0.358]4.
???
(cello) [-0.358]5.
??????
(trumpet) [-0.377]6.
???
(shamisen) [-0.383]7.
????
(saxophone) [-0.39]8.
????
(organ) [-0.392]9.
??????
(clarinet) [-0.394]10.
??
(erh hu) (-0.396)(3) 10 most similar to ??????????1.
?????
(Brahms) [-0.152]2.
?????
(Schumann) [-0.163]3.
????????
(Mendelssohn) [-0.166]4.
?????????
(Shostakovich) [-0.178]5.
?????
(Sibelius) [-0.18]of (t, p?,n?
), where t denotes the target nominal term, p apostposition, v a verb, and n?
a nominal term that follows tand p?, i.e., ?t-no?
analogue to the English ?of t.?416.
????
(Haydn) [-0.181]7.
????
(Ha?ndel) [-0.181]8.
????
(Ravel) [-0.182]9.
??????
(Schubert) [-0.187]10.
???????
(Beethoven) [-0.19]For construction of P(k), we had the follow-ing settings: i) k = 1,2; and ii) for each k, weselected the 150,000 most frequent terms (out ofone million terms) with some filtering specifiedbelow.
Thus, Q was 300,000 pairs whose baseterms are roughly the most frequent 150,000terms in B with filtering and targets are termsk = 1 or k = 2.2.2.2 Filtering of terms in BFor filtering, we excluded the terms of B withone of the following properties: a) they are in aninvalid form that could have resulted from parseerrors; b) they have regular ending (e.g., -?
?, -?
[event], -?
[time or when], -??
[thing orperson], -?
[thing], -?
[person]).
The reasonfor the second is two-fold.
First, it was desir-able to reduce the ratio of the class of ?class-mates with common morpheme,?
which is ex-plained in ?2.3.2, whose dominance turned out tobe evident in the preliminary analysis.
Second,the semantic property of the terms in this classis relatively predictable from their morphology.That notwithstanding, this filtering might havehad an undesirable impact on our results, at leastin terms of representativeness.
Despite of this,we decided to place priority on collecting morevarieties of classes.The crucial question is, again, whether dis-tributionally similar terms can really be equatedwith semantically similar terms.
Put differently,what kinds of terms can we find in the sets con-structed using distributionally similarity?
Wecan confirm the hypothesis if the most of theterm pairs are proved to be semantically simi-lar for most sets of terms constructed based onthe distributional hypothesis.
To do this, how-ever, we need to clarify what constitutes seman-tic similarity.
We will deal with this prerequisite.2.3 Classification2.3.1 Factoring out ?semantic similarity?Building on lexicographic works like Fell-baum (1998) and Murphy (2003), we assumethat the following are the four major classesof semantic relation that contribute to semanticsimilarity between two terms:(4) a.
?synonymic?
relation (one can substi-tute for another on an identity basis).Examples are (Microsoft, MS).b.
?hypernym-hyponym?
relation be-tween two terms (one can substitutefor another on un underspecifica-tion/abstraction basis).
Examples are(guys, players)c. ?meronymic?
(part-whole) relation be-tween two terms (one term can be asubstitute for another on metonymicbasis).
Examples are (bodies, players)[cf.
All the players have strong bodies]d. ?classmate?
relation between twoterms, t1 and t2, if and only if (i) theyare not synonymous and (ii) there is aconcrete enough class such that both t1and t2 are instances (or subclasses).2)For example, (China, South Korea)[cf.
(Both) China and South Koreaare countries in East Asia], (Ford, Toy-ota) [cf.
(Both) Ford and Toyota aretop-selling automotive companies] and(tuna, cod) [cf.
(Both) tuna and codare types of fish that are eaten in theEurope] are classmates.For the substitution, the classmate class behavessomewhat differently.
In this case, one term can-not substitute for another for a pair of terms.
Itis hard to find out the context in which pairs like(China, South Korea), (Ford, Toyota) and (tuna,cod) can substitute one another.
On the otherhand, substitution is more or less possible in theother three types.
For example, a synonymic pairof (MS, Microsoft) can substitute for one anotherin contexts like Many people regularly complain2)The proper definition of classmates is extremely hardto form.
The authors are aware of the incompleteness oftheir definition, but decided not to be overly meticulous.42pair of formspair ofmeaningfultermsx: pair with ameaninglessformu: pair of termsin no conceivablesemantic relationr: pair of terms ina conceivablesemantic relations:* synonymouspair in thebroadest sensea: acronymicpairv: allographicpairn: alias paire: erroneouspairf: quasi-erroneous pairv*: notationalvariation of thesame termm: misuse pairo: pair in other,unindentifiedrelationh: hypernym-hyponym pairk**: classmatein the broadestsensek*: classmatewithout obviouscontrastivenessc*: contrastivepairs d: antonymicpairc: contrastivepair withoutantonymityp: meronymicpairt: pair of termswith inherenttemporal ordery: undecidablek: classmatewithout sharedmorphemew: classmatewith sharedmorphemes: synonymouspair of differenttermsFigure 1: Classification tree for semantic relations usedabout products { i. MS; ii.
Microsoft }.
Ahypernym-hyponym pair of (guys, players) cansubstitute in contexts like We have dozens of ex-cellent { i. guys; ii.
players } on our team.
Ameronymic pair of (bodies, players) can substi-tute for each other in contexts like They had a fewof excellent { i. bodies; ii.
players} last year.2.3.2 Classification guidelinesThe classification guidelines were specifiedbased on a preliminary analysis of 5,000 ran-domly selected examples.
We asked four annota-tors to perform the task.
The guidelines were fi-nalized after several revisions.
This revision pro-cess resulted in a hierarchy of binary semanticrelations as illustrated in Figure 1, which sub-sumes 18 types as specified in (5).
The essen-tial division is made at the fourth level wherewe have s* (pairs of synonyms in the broadestsense) with two subtypes, p (pairs of terms inthe ?part-whole?
relation), h (pairs of terms inthe ?hypernym-hyponym?
relation), k** (pairsof terms in the ?classmate?
relation), and o (pairsof terms in any other relation).
Note that thissystem includes the four major types describedin (4).
The following are some example pairs ofJapanese terms with or without English transla-tions:(5) s: synonymic pairs (subtype of s*) inwhich the pair designates the same en-tity, property, or relation.
Examplesare: (?
?, ??)
[both mean root], (??????,????)
[(supporting mem-ber, cooperating member)], (????
?, ?????)
[(invoker of the pro-cess, parent process)], (????????
?, ?????)
[(venture business, ven-ture)], (???
?, ???????)
[(op-posing hurler, opposing pitcher)], (?
?, ???)
[(medical history, anamne-ses)],n: alias pairs (subtype of s*) in whichone term of the pair is the ?alias?
ofthe other term.
Examples are (SteveJobs, founder of Apple, Inc.), (BarakObama, US President), (???,???????
), (??
?, ????
)43a: acronymic pair (subtype of s*) inwhich one term of the pair is theacronym of of the other term.
Ex-amples are: (DEC, Digital Equip-ment), (IBM, International BusinessMachine) (Microsoft ?, MS ?
), (??
?, ????
), (???
?, ??
),v: allographic pairs (subtype of s*) inwhich the pair is the pair of two formsof the same term.
Examples are:(convention centre, convention cen-ter), (colour terms, color terms), (????
?, ????
), (???
?, ????),(?????????
?, ???????????
), (?
?, ??
), (???
?, ????
), (??
?, ??
), (???
?, ?????),(?
?, ??
), (???
?, ????
)h: hypernym-hyponym pair in which oneterm of the pair designates the ?class?of the other term.
Examples (or-der is irrelevant) are: (thesaurus, Ro-get?s), (????
?, ?????)
[(searchtool, search software)], (???
?, ????)
[(unemployment measures, em-ployment measures)], (?
?, ????)
[(business conditions, employmentconditions)], (??????
?, ???
)[(festival, music festival)], (??
?, ?????)
[(test agent, pregnancy test)],(?????
?, ???)
[(cymbidium, or-chid)], (????,?????)
[(companylogo, logo)], (????,????)
[(mys-tical experiences, near-death experi-ences)]p: meronymic pair in which one term ofthe pair designates the ?part?
of theother term.
Examples (order is ir-relevant) are: (???
?, ??)
[(earth,sea)], (?
?, ??)
[(affirmation, ad-mission)], (?
?, ????)
[(findings,research progress)], (????????
?, ?????)
[(solar circuit system,exterior thermal insulation method)],(????
?, ??)
[(Provence, SouthFrance)],k: classmates not obviously contrastivewithout common morpheme (subtypeof k*).
Examples are: (???
?, ????)
[(self-culture, training)], (???
?, ??)
[(sub-organs, services)], (?????,??????)
[(Dongba alphabets,hieroglyphs)], (Tom, Jerry)w: classmates not obviously contrastivewith common morpheme (subtype ofk*).
Examples are: (???
?, ????
)[(gas facilities, electric facilities)], (????,???)
[(products of other com-pany, aforementioned products)], (??
?, ???)
[(affiliate station, local sta-tion)], (???,????)
[(Niigata City,Wakayama City)], (????
?, ?????)
[(Sinai Peninsula, Malay Penin-sula)],c: contrastive pairs without antonymity(subtype of c*).
Examples are: (????
?, ????)
[(romanticism, natural-ism)], (???????
?, ???????????)
[(mobile user, internet user)],(??
?, PS2?
), [(bootleg edition, PS2edition)]d: antonymic pairs = contrastive pairswith antonymity (subtype of c*).
Ex-amples are: (?
?, ??)
[(bond-ing, disintegration)], (??
?, ???
)[(gravel road, pavement)], (?
?, ??)
[(west walls, east walls)], (???,????)
[(daughter and son-in-law,son and daughter-in-law)], (?
?, ??)
[(tax-exclusive prices, tax-inclusiveprices)], (?????
?, ????????)
[(front brake, rear brake)], (?????
?, ???????)
[(tag-team match,solo match)], (??
?, ???)
[(wip-ing with dry materials, wiping withwet materials)], (?????
?, ??
)[(sleeveless, long-sleeved)]t: pairs with inherent temporal order(subtype of c*).
Examples are: (??
?, ???)
[(harvesting of rice, plant-ing of rice)], (???
?, ????)
[(dayof departure, day of arrival)], (???
?, ????)
[(career decision, careerselection)], (??
?, ????)
[(catnap,stay up)], (?
?, ??)
[(poaching, con-44traband trade)], (?
?, ??)
[(surren-der, dispatch of troops)], (??
?, ???)
[(2nd-year student, 3rd-year stu-dent)]e: erroneous pairs are pairs in whichone term of the pair seems to sufferfrom character-level input errors, i.e.?mistypes.?
Examples are: (??
?, ???
), (??????
?, ???????),(??
?, ???
)f: quasi-erroneous pair is a pair of termswith status somewhat between v and e.Examples (order is irrelevant) are: (???
?, ????)
[(supoito, supoido)],(?????
?, ??????)
[(goru-fubaggu, gorufugakku)], (?????,?????)
[(biggu ban, bikku ban)],m: misuse pairs in which one term of thepair seems to suffer from ?mistake?
or?bad memory?
of a word (e is causedby mistypes but m is not).
Examples(order is irrelevant) are: (??
?, ???
), (????
?, ?????
), (?
?, ??),(??
?, ???
), (?
?, ??
)o: pairs in other unidentified relation inwhich the pair is in some semantic re-lation other than s*, k**, p, h, andu.
Examples are: (?
?, ???)
[(ul-terior motives, possessive feeling)], (?????,?????)
[(theoretical back-ground, basic concepts)], (???????
?, ????)
[(Alexandria, Sira-cusa)],u: unrelated pairs in which the pair is inno promptly conceivable semantic re-lation.
Examples are: (??
?, ????)
[(noncontact, high resolution)], (?
?, ????)
[(imitation, overinterpreta-tion)],x: nonsensical pairs in which either of thepair is not a proper term of Japanese.
(but it can be a proper name with verylow familiarity).
Examples are: (???
?, ???
), (???
?, ??
), (?
?, ???
), (??
?, ??
), (ma, ?????
)y: unclassifiable under the allowed timelimit.3) Examples are: (??
?, ??????
), (fj, ???
), (?
?, ??
),Note that some relation types are symmetricand others are asymmetric: a, n, h, p, and t (ande, f, and m, too) are asymmetric types.
Thismeans that the order of the pair is relevant, but itwas not taken into account during classification.Annotators were asked to ignore the direction ofpairs in the classification task.
In the finaliza-tion, we need to reclassify these to get them inthe right order.2.3.3 Notes on implicational relationsThe overall implicational relation in the hier-archy in Figure 1 is the following:(6) a. s, k**, p, h, and o are supposed to bemutually exclusive, but the distinctionis sometimes obscure.4)b. k** has two subtypes: k* and c*.c.
k and w are two subtypes k*.d.
c, d and t three subtypes of c*.To resolve the issue of ambiguity, priority wasset among the labels so that e, f < v < a < n <p < h < s < t < d < c < w < k < m < o < u <x < y, where the left label is more preferred overthe right.
This guarantees preservation of the im-plicational relationship among labels.2.3.4 Notes on quality of classificationWe would like to add a remark on the quality.After a quick overview, we reclassified o and w,because the first run of the final task ultimatelyproduced a resource of unsatisfactory quality.Another note on inter-annotator agreement:originally, the classification task was designedand run as a part of a large-scale language re-source development.
Due to its overwhelmingsize, we tried to make our development as effi-cient as possible.
In the final phase, we asked3)We did not ask annotators to check for unknown terms.4)To see this, consider pairs like (large bowel, bowel),(small bowel, bowel).
Are they instances of p or h?
Thedifficulty in the distinction between h and p becomes harderin Japanese due to the lack of plurality marking: caseslike (Mars, heavenly body) (a case of h) and (Mars, heav-enly bodies) (a p case) cannot be explicitly distinguished.In fact, the Japanese term ??
can mean both ?heavenlybody?
(singular) and ?heavenly bodies?
(plural).45Table 1: Distribution of relation typesrank count ratio (%) cum.
(%) class label1 108,149 36.04 36.04 classmates without common morpheme k2 67,089 22.35 58.39 classmates with common morpheme w3 26,113 8.70 67.09 synonymic pairs s4 24,599 8.20 75.29 hypernym-hyponym pairs h5 20,766 6.92 82.21 allographic pairs v6 18,950 6.31 88.52 pairs in other ?unidentified?
relation o7 12,383 4.13 92.65 unrelated pairs u8 8,092 2.70 95.34 contrastive pairs without antonymity c9 3,793 1.26 96.61 pairs with inherent temporal order t10 3,038 1.01 97.62 antonymic pairs d11 2,995 1.00 98.62 meronymic pairs p12 1,855 0.62 99.23 acronymic pairs a13 725 0.24 99.48 alias pairs n14 715 0.24 99.71 erroneous pairs e15 397 0.13 99.85 misuse pairs m16 250 0.08 99.93 nonsensical pairs x17 180 0.06 99.99 quasi-erroneous pairs f18 33 0.01 100.00 unclassified y17 annotators to classify the data with no over-lap.
Ultimately we obtained results that deservea detailed report.
This history, however, broughtus to an undesirable situation: no inter-annotatoragreement is calculable because there was nooverlap in the task.
This is why no inter-rateragreement data is now available.3 ResultsTable 1 summarizes the distribution of relationtypes with their respective ranks and proportions.The statistics suggests that classes of e, f, m, x,and y can be ignored without risk.3.1 ObservationsWe noticed the following.
Firstly, the largestclass is the class of classmates, narrowly definedor broadly defined.
The narrow definition of theclassmates is the conjunction of k and w, whichmakes 58.39%.
The broader definition of class-mates, k**, is the union of k, w, c, d and t, whichmakes 62.10%.
This confirms the distributionalhypothesis.The second largest class is the narrowly de-fined synonymous pairs s. This is 8.7% of thetotal, but the general class of synonymic pairs,s* as the union of s, a, n, v, e, f, and m, makes16.91%.
This comes next to h and w. Noticealso that the union of k** and s* makes 79.01%.The third largest is the class of terms inhypernym-hyponym relations.
This is 8.20% ofthe total.
We are not sure if this is large or small.These results look reasonable and can beseen as validation of the distributional hypothe-sis.
But there is something uncomfortable aboutthe the fourth and fifth largest classes, pairs in?other?
relation and ?unrelated?
pairs, whichmake 6.31% and 4.13% of the total, respectively.Admittedly, 6.31% are 4.13% are not very largenumbers, but it does not guarantee that we canignore them safely.
We need a closer examina-tion of these classes and return to this in ?4.3.2 Note on allography in JapaneseThere are some additional notes: the rate of al-lographic pairs [v] (6.92%) is rather high.5) Wesuspect that this ratio is considerably higher thanthe similar results that are to be expected in other5)Admittedly, 6.92% is not a large number in an absolutevalue, but it is quite large for the rate of allographic pairs.46languages.
In fact, the range of notational varia-tions in Japanese texts is notoriously large.
Manyresearchers in Japanese NLP became to be awareof this, by experience, and claim that this is oneof the causes of Japanese NLP being less effi-cient than NLP in other (typically ?segmented?)languages.
Our result revealed only the allogra-phy ratio in nominal terms.
It is not clear to whatextent this result is applied to the notional varia-tions on predicates, but it is unlikely that predi-cates have a lesser degree of notational variationthan nominals.
At the least, informal analysissuggests that the ratio of allography is more fre-quent and has more severe impacts in predicatesthan in nominals.
So, it is very unlikely that wehad a unreasonably high rate of allography in ourdata.3.3 Summary of the resultsOverall, we can say that the distributional hy-pothesis was to a great extent positively con-firmed to a large extent.
Classes of classmatesand synonymous pairs are dominant.
If the sideeffects of filtering described in ?2.2.2 are ig-nored, nearly 88% (all but o, u, m, x, and y)of the pairs in the data turned out to be ?se-mantically similar?
in the sense they are clas-sified into one of the regular semantic relationsdefined in (5).
While the status of the inclusionof hypernym-hyponym pairs in classes of seman-tically similar terms could be controversial, thisresult cannot be seen as negative.One aspect somewhat unclear in the results weobtained, however, is that highly similar termsin our data contain such a number of pairs inunidentifiable relation.
We will discuss this inmore detail in the following section.4 Discussion4.1 Limits induced by parametersOur results have certain limits.
We specify thosehere.First, our results are based on the case ofk = 1, 2 for P(k).
This may be too small andit is rather likely that we did not acquire resultswith enough representativeness.
For more com-plete results, we need to compare the present re-sults under larger k, say k = 4, 8, 16, .
.
.. We didnot do this, but we have a comparable result inone of the preliminary studies.
In the prepara-tion stage, we classified samples of pairs whosebase term is at frequency ranks 13?172, 798?1,422 and 12,673?15,172 where k = 1, 2, 3, .
.
.
,9, 10.6) Table 2 shows the ratios of relation typesfor this sample (k = 1, 2, 4, 8, 10).Table 2: Similarity rank = 1, 2, 4, 8, 10rank 1 2 4 8 10v 18.13 10.48 3.92 2.51 1.04o 17.08 21.24 26.93 28.24 29.56w 13.65 13.33 14.30 12.19 12.75s 11.74 9.14 7.05 4.64 4.06u 11.07 16.48 17.63 20.79 20.87h 10.50 10.29 11.17 12.96 10.20k 7.82 8.38 7.84 7.74 8.22d 2.58 2.00 1.57 1.16 0.85p 2.00 1.14 1.08 1.35 1.79c 1.43 1.05 1.27 1.35 1.89a 1.05 1.33 0.88 0.39 0.57x 1.05 1.14 1.27 1.64 2.08t 0.29 0.19 0.20 0.39 0.47f 0.10 0.10 0.00 0.10 0.09m 0.00 0.10 0.20 0.00 0.19#item 1,048 1,050 1,021 1,034 1,059From Table 2, we notice that: as similarityrank decreases, (i) the ratios of v, s, a, and ddecrease monotonically, and the ratios of v and sdecrease drastically; (ii) the ratios of o, u, and xincreases monotonically, and the ratio of o and uincreases considerably; and while (iii) the ratiosof h, k, p, w, m, and f seem to be constant.
Butit is likely that the ratios of h, k, p, w, m, and fchange at larger k, say 128, 256.Overall, however, this suggests that the differ-ence in similarity rank has the greatest impacton s* (recall that s and v are subtypes of s*),o, and u, but not so much on others.
Two ten-dencies can be stated: first, terms at lower sim-ilarity ranks become less synonymous.
Second,6)The frequency/rank in B was measured in terms of thecount of types of dependency relation.47the relationships among terms at lower similar-ity ranks become more obscure.
Both are quiteunderstandable.There are, however, two caveats concerningthe data in Table 2, however.
First, the 15 la-bels used in this preliminary task are a subset ofthe 18 labels used in the final task.
Second, thedefinitions of some labels are not completely thesame even if the same labels are used (this is whywe have this great of a ratio of o in Table 2.
Wemust admit, therefore, that no direct comparisonis possible between the data in Tables 1 and 2.Second, it is not clear if we made the bestchoices for clustering algorithm and distribu-tional data.
For the issue of algorithm, thereare too many clustering algorithms and it is hardto reasonably select candidates for comparison.We do, however, plan to extend our evaluationmethod to other clustering algorithms.
Cur-rently, one of such options is Bayesian cluster-ing.
We are planning to perform some compar-isons.For the issue of what kind of distributional in-formation to use, many kinds of distributionaldata other than dependency relation are avail-able.
For example, simple co-occurrences withina ?window?
are a viable option.
With a lackof comparison, however, we cannot tell at thepresent what will come about if another kind ofdistributional data was used in the same cluster-ing algorithm.4.2 Possible overestimation of hypernymsA closer look suggests that the ratio ofhypernym-hyponym pairs was somewhat overes-timated.
This is due to the algorithm used in ourdata construction.
It was often the case that headnouns were extracted as bare nouns from com-plex, much longer noun phrases, sometimes dueto the extraction algorithms or parse errors.
Thisresulted in accidental removal of modifiers be-ing attached to head nouns in their original uses.We have not yet checked how often this was thecase.
We are aware that this could have resultedin the overestimation of the ratio of hypernymicrelations in our data.4.3 Remaining issuesAs stated, the fourth largest class, roughly 6.31%of the total, is that of the pairs in the ?other?unidentified relation [o].
In our setting, ?other?means that it is in none among the synonymous,classmate, part-whole or hypernym-hyponym re-lation.
A closer look into some examples ofo suggest that they are pairs of terms with ex-tremely vague association or contrast.Admittedly, 6.31% is not a large number, butits ratio is comparable with that of the allo-graphic pairs [v], 6.92%.
We have no explana-tion why we have this much of an unindenfiablekind of semantic relation distinguished from un-related pairs [u].
All we can say now is that weneed further investigation into it.u is not as large as o, but it has a status similarto o.
We need to know why this much amount ofthis kind of pairs.
A possible answer would bethat they are caused by parse errors, directly orindirectly.5 ConclusionWe analyzed the details of the Japanese nominalterms automatically constructed under the ?dis-tributional hypothesis,?
as in Harris (1954).
Wehad two aims.
One aim was to examine to seeif what we acquire under the hypothesis is ex-actly what we expect, i.e., if distributional sim-ilarity can be equated with semantic similarity.The other aim was to see what kind of seman-tic relations comprise a class of distributionallysimilar terms.For the first aim, we obtained a positive result:nearly 88% of the pairs in the data turned out tobe semantically similar under the 18 criteria de-fined in (5), which include hypernym-hyponym,meronymic, contrastive, and synonymic rela-tions.
Though some term pairs we evaluatedwere among none of these relations, the ratio ofo and u in sum is about 14% and within the ac-ceptable range.For the second aim, our result revealed thatthe ratio of the classmates, synonymous, rela-tion, hypernym-hyponym, and meronymic rela-tions are respectively about 62%, 17%, 8% and1% of the classified data.48Overall, these results suggest that automaticacquisition of terms under the distributional hy-pothesis give us reasonable results.A Clustering of one million nominalsThis appendix provides some details on how theclustering of one million nominal terms was per-formed.To determine the similarity metric of a pair ofnominal terms (t1, t2), Kazama et al (2009) usedthe Jensen-Shannon divergence (JS-divergence)DJS(p||q) = 12D(p||M) + 12D(q||M), where pand q are probability distributions, and D =?i p(i)log p(i)q(i) (Kullback-Leibler divergence, orKL-divergence) of p and q, and M = 12(p+ q).We obtained p and q in the following way.Instead of using raw distribution, Kazama etal.
(2009) applied smoothing using EM algo-rithm (Rooth et al, 1999; Torisawa, 2001).
InTorisawa?s model (2001), the probability of theoccurrence of the dependency relation ?v,r,n?
isdefined as:P(?v,r, t?)
=def ?a?AP(?v,r?|a)P(t|a)P(a),where a denotes a hidden class of ?v,r?
and termt.
In this equation, the probabilities P(?v,r?|a),P(t|a), and P(a) cannot be calculated directlybecause class a is not observed in a given depen-dency data.
The EM-based clustering methodestimates these probabilities using a given cor-pus.
In the E-step, the probability P(a|?v,r?
)is calculated.
In the M-step, the probabilitiesP(?v,r?|a), P(t|a), and P(a) are updated untilthe likelihood is improved using the results ofthe E-step.
From the results of this EM-basedclustering method, we can obtain the probabili-ties P(?v,r?|a), P(t|a), and P(a) for each ?v,r?, t,and a.
Then, P(a|t) is calculated by the follow-ing equation:P(a|t) = P(t|a)P(a)?a?AP(t|a)P(a) .The distributional similarity between t1 and t2was calculated by the JS divergence betweenP(a|t1) and P(a|t2).ReferencesFellbaum, C., ed.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Grefenstette, G. 1993.
Automatic thesaurus gener-ation from raw text using knowledge-poor tech-niques.
In In Making Sense of Words: The 9thAnnual Conference of the UW Centre for the NewOED and Text Research.Harris, Z. S. 1954.
Distributional structure.
Word,10(2-3):146?162.
Reprinted in Fodor, J.
A andKatz, J. J.
(eds.
), Readings in the Philosophyof Language, pp.
33?49.
Englewood Cliffs, NJ:Prentice-Hall.Hindle, D. 1990.
Noun classification from predicate-argument structures.
In Proceedings of ACL-90,pp.
268?275, Pittsburgh, PA.Kazama, J. and K. Torisawa.
2008.
Inducinggazetteers for named entity recognition by large-scale clustering of dependency relations.
In Pro-ceedings of ACL-2008: HLT, pp.
407?415.Kazama, J., S. De Saeger, K. Torisawa, and M. Mu-rata.
2009.
Generating a large-scale analogy listusing a probabilistic clustering based on noun-verb dependency profiles.
In Proceedings of the15th Annual Meeting of the Association for Natu-ral Language Processing.
[in Japanese].Lee, L. 1997.
Similarity-Based Approaches to Natu-ral Language Processing.
Unpublished Ph.D. the-sis, Harvard University.Lin, D. 1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING/ACL-98, Montreal, Canda, pages 768?774.Murphy, M. L. 2003.
Semantic Relations and theLexicon.
Cambridge University Press, Cambridge,UK.Rooth, M., S. Riezler, D. Presher, G. Carroll, andF.
Beil.
1999.
Inducing a semantically annotatedlexicon via em-based clustering.
In Proceedingsof the 37th Annual Meeting of the Association forComputational Linguistics, pp.
104?111.Shinzato, K., T. Shibata, D. Kawahara, C. Hashimoto,and S. Kurohashi.
2008.
TSUBAKI: An opensearch engine infrastructure for developing newinformation access.
In Proceedings of IJCNLP2008.Torisawa, K. 2001.
An unsupervised method forcanonicalization of Japanese postpositions.
InProceedings of the 6th Natural Language Process-ing Pacific Rim Symposium (NLPRS), pp.
211?218.49
