TECHNIQUES TO ACHIEVE AN ACCURATE REAL-TIME LARGE-VOCABULARY SPEECH RECOGNITION SYSTEMHy Murveit, Peter Monaco, Vassilios Digalakis, John ButzbergerSRI InternationalSpeech Technology and Research Laboratory333 Ravenswood AvenueMenlo Park, Califoraia 94025-3493ABSTRACTIn addressing the problem of achieving high-accuracy real-timespeech recognition systems, we focus on recognizing speech fromARPA's 20,000-word Wall Street Journal (WSJ) task, usingcurrent UNIX workstations.
We have found that our standardapproach--using a narrow beam width in a Viterbi search forsimple discrete-density hidden Markov models (HMMs)--worksin real time with only very low accuracy.
Our most accuratealgorithms recognize speech many times slower than real time.Our (yet unattained) goal is to recognize speech in real time at ornear full accuracy.We describe the speed/accuracy trade-offs associated with severaltechniques u ed in a one-pass speech recognition framework:?
Trade-offs associated with reducing the acoustic modelingresolution of the HMMs (e.g., output-distribution type,number of parameters, cross-word modeling)?
Trade-offs associated with using lexicon tree,s, andtechniques for implementing full and partial bigramgrammars with those trees?
Computation fGaussian probabilities are the most ime-consuming aspect of our highest accuracy system, andtechniques allowing us to reduce the number of Gaussianprobabilities computed with little or no impact on speechrecognition accuracy.Our results how that tree-based modeling techniques used withappropriate acoustic modeling approaches achieve real-timeperformance on current UNIX workstations atabout a 30% errorrate for the WSJ task.
The results also show that we candramatically reduce the computational complexity of our moreaccurate but slower modeling alternatives sothat hey are near thespeed necessary for real-time performance in a multipass earch.Our near-future goal is to combine these two technologies sothatreal-time, high-accuracy large-vocabulary speech recognition canbe achieved.
(WSJ) speech corpus.
All of the speed and performance datagiven in this paper are results of recognizing 169 sentences fromthe four male speakers that comprise ARPA's November 199220,000-word vocabulary evaluation set.
Our best performance onthese data is 8.9% (10.3% using bigram language models).
Ourstandard implementation f rthis system would run approximately100 times slower than real time.
1 Both these systems use beam-search techniques for finding the highest-scoring recognitionhypothesis.Our most accurate systems are those that use HMMs with genonicmixtures as observation distributions \[3\].
Genonic mixturessample the continuum between fully continuous and fled-mixtureHMMs at an arbitrary point and therefore can achieve anoptimum recognition performance given the available trainingdata and computational resources.
In brief, genonic systems aresimilar to fully continuous Gaussian-mixture HMMs, except hatinstead of each state having its own set of Gaussian densities,states are clustered into genones that share these Gaussiancodebooks.
Each state, however, can have its own set of mixtureweights used with the Gaussian codebook to form its own uniqueobservation distribution.
All the genonie systems discussed inthispaper use a single 39-dimensional observation composed of thespeech cepstrum and its first and second erivatives, and thespeech energy and its first and second erivatives.
All Gaussianshave diagonal covariance matrices.2.
MODELING TRADE-OFFSThe speed/accuracy trade-off of our speech recognition systemscan be adjusted in several ways.
The standard approaches are toadjust the beam width of the Viterbi search and to change the1.
INTRODUCTIONOur techniques for achieving real-time, high-accuracy large-vocabulary continuous peech recognition systems focus on thetask of recognizing speech from ARPA's Wall Street Journal i.
We define real-time systems as those that process 1 second of speech per second.393output-distribution modeling technique.
Table 1 shows, for 3.
LEXICON TREESSystem Type Cross-Word ModelingWordError(%)LatticeSearchSpeedGenone yes 11.6 50.4Genone no 13.4 19.8PhoneticaUy Tied yes 13.9 43.9/~SxturesPhoneticaUy Tied no 16.6 6.8MixturesVQ no 19.2 ~1Table 1: Effect of model type on speed and accuracyinstance, that eliminating cross-word modeling can significantlyimprove the speed of our recognition systems at about a 20% costin word error.
In this table, lattice speed refers to recognitionaccuracy when decoding from precomputed word lattices \[8\].
Thatis, this is only performing a subset of the search.
Actual fullgrammar recognition time could be from a factor of 3 to an orderof magnitude higher.
However, it is useful to compare relativelattice decoding speeds from the various techniques.A technique frequently used at SRI to achieve relatively fastspeech recognition demonstrations is to downgrade our acousticmodeling by implementing a discrete density (VQ) HMM systemwithout cross-word acoustic modeling.
This system is thensearched using a Viterbi search with a small beam width.
Table 2shows the fuU grammar speed accuracy trade-off when modifyingthe beam width if a Silicon Graphics Incorporated 2 (SGI) UNIXworkstation with a 150-MHz MIPS R4400 CPU 3 is used toBeamWidth600700800Table 2:WordError(%)29.521.519.2Hypothesesper Frame98130897764FullSearchSpeed3.28,316.0Speed/accuracy trade-off or a beam searchperform the computation.We have found that this technique gives an unsatisfactory speed/accuracy trade-off for this task and we have investigated othertechniques as described below.2.
All product names mentioned in this paper are thetrademark of their respective holders.a.
This workstation scores 85 and 93 for the SPECint92and SPECfp92 benchmarks.
For our tests it is roughly50% faster than an SGI R4000 Indigo, and 50% fasterthan a SPARC 10/51.
It should be between 1/2 and 2/3the speed of an HP735.
SGI R4400 systems cost about$12,000.We explored the use of lexicon trees as a technique for speedingup the decoding times for all modeling techniques.
Lexicon treesrepresent the phonetics of the recognition vocabulary as a treeinstead of as a list of pronunciations (lists of phones).
With a treerepresentation, words starting with the same phonetic units sharethe computation ofphonetic models.
This technique has been usedby others, including the IBM \[10\], Phillips \[7\], and CMU groups,and it is also currently used at LIMSI.
Because of the large amountof sharing, trees can drastically reduce the amount of computationrequired by a speech recognition system.
However, lexicon treeshave several possible drawbacks:?
Phonetic trees are not able to use triphone modeling in allpositions ince the right phonetic ontext of a node in a treecan be ambiguous.?
One cannot implement an admissible Viterbi search for asingle lexicon tree when using a bigram language model,because the word being decoded (w2 in the bigramequation P(w2/wl)) may not be known until a leaf in thetree is reached--long after certain Viterbi decisions aretypically made.The first concern can be addressed by replicating nodes in the freeto disambiguate triphone contexts.
However, even this may not benecessary because the large majority of right contexts in the treeare unambiguous (that is, most nodes have only one child).
This isshown in Table 3, where the concentrations of triphone andbiphone models are compared for tree- and linear-lexiconschemes.Lexicon Tfiphone BiphoneModels ModelsType (%) (%)Tree 73 27Linear 85 15Table 3: Model allocation for the SRI WSJ system withand without lexicon treesThe second concern, the ability to model bigram language modelsusing an admissible search strategy, is a problem.
As shown inTable 4, moving from a bigram to a unigrarn language model morethan doubles our error rate.
Ney \[7\] has proposed a scheme wherelexicon trees are replicated for each bigram context.
It is possiblethat this scheme would generalize to our application as well.
Forthe three recognition systems in Table 2, on average 7, 13, and 26different words end each frame.
This is the minimum averagenumber of copies of the lexicon tree that the system would need tomaintain.394We have decided to pursue a different approach, which is shownin the figure below.
We refer to this technique as approximatebigram trees.Q_ w _  t/ \[ Transitions \[ l \[lexicon I ~ / Bigram I@.
.
.
.
i--,v-/Section/.IFigure 1: Approximate bigram treesIn an approximate bigram tree, the aim is to model the salientportion of the backed-off bigrarn language model \[11\] inuse.
In anapproximate bigram tree, a standard lexicon tree (incorporatingunigram word probabilities) is combined with a bigram sectionthat maintains alinear (non-tree) representation f the vocabulary.Bigram and backoff language model transitions are added to theleaves of the tree and to the word-final nodes of the bigramsection.
4When the entire set of bigram is represented, then thisnetwork implements a full backed-off bigram language modelwith an efficient ree-based backoff section.
In fact, for VQHMMsystems, this scheme halves our typical decoding time for little orno cost in accuracy.
Typically, however, we need further eductionin the computational requirement.
To achieve this we representonly a subset of the group of bigram transitions (and adjust hebackoff probabilities appropriately).
This degrades the accuracy ofour original bigram language model, but reduces its computationalrequirements.
The choice of which bigrarns to represent is the keydesign decision for approximate bigram trees.
We haveexperimented with four techniques for choosing bigrarn subsets tosee which make the best speed/accuracy trade-offs:Count x means only use bigrarns where P(wl) * P(w2/wl) > 10 x.Prob x means only use bigrarns where P(w2/wl) > 10 x.Improve X means only use bigrams where P(w2/wl )  >Backoff(wl ) * P(w2) 110 x.Top x means only use bigrarns P(w2/wl) where w2 is one of themost frequent x words.Table 4 shows speed/accuracy trade-offs for approximate bigrarnNumber of Word Full SearchTree Type Bigrams Used Error Speed(thousands) (%) ( x RT)Unigram 0 42.3 0.6tree(non-tree) 3500 21.5 8.5Bigramcount, -6 93 30.4 1.5count, -5 10 35.8 0.9count, -4 ,6 39.2 0.7prob, -3 1250 28.2 0.9prob, -2.5 671 29.2 0.8prob, -2 219 31.5 0.7prob,-1 20 36.6 0 .7improve, 2 908 29.7 1.6improve, 3 191 37.1 0.8top 10 113 39.5 0.7top 50 320 36.0 0.7top 100 35.2 0.7top 1000 1500 31.4 1.1top 5000 2624 25.3 1.9top 20000 3500 2t.0 ~3Table 4: Performance of "approximate bigram" treestrees.The top two lines of the table show that the bigram languagemodel improves perforrnanee from 42.3% word error to 21.5% ascompared with a unigram language model.
The rest of the tableshows how approximate bigram trees can trade off theperformance and speed of the bigram model.
For instance, inseveral techniques shown--such as prob 2&--that maintain morethan half of the benefit of bigrarns for little computational cost,CPU usage goes from 0.6 to 0.8, when the error rate goes from42.3% to 29.2%.
The rest of the improvement, reducing the errorrate from 29.2% to 21%, increases the required computation rateby a factor of four.Table 4 also shows that he number of bigrams represented doesnot predict he computation rate.4.
In the actual implementation, word-final nodes in thebigrarn section are merged with their counterparts in thetree so that the bigram transitions need be representedonly once.
For simplicity, however, we show the systemwith two sets of bigram probabilities.395The square root of the perplexity of these language models eemsto predict he recognition performance asshown in Table 5.Perplexity WordFop X Perplexity Square errorRoot (%)0 1248 35.3 42.310 954 30.9 39.550 727 27.0 36.0100 631 25.1 35.21000 401 20.0 31.4~0000 237 15.4 21TaMe 5: Grammar Perplexity for top X trees4.
REDUCING GAUSSIANCOMPUTATIONSSRI's most accurate recognition systems, using genonie mixtures,require the evaluation of very large numbers of Gaussiandistributions, and are therefore very slow to compute.
Thebaseline system referenced here uses 589 genonic mixtures(genones), each with 48 Gaussian distributions, for a total of28,272 39-dimensional Gaussians.
On ARPA's November 199220,000-word Evaluation Test Set, this noncrossword, bigramsystem performs at 13.43% word error.
Decoding time from wordlattices is 12.2 times lower than real time on an R4400 processor.Full grammar decoding time would be much slower.
Since thedecoding time of a genonic recognition system such as this one isdominated by Gaussian evaluation, one major thrust of our effortto achieve real-time recognition has been to reduce the number ofGaussians requiring evaluation each frame.
We have exploredthree methods of reducing Gaussian computation: Gaussianclustering, Gaussian shortlists, and genonic approximations.4.1.
Gaussian ClusteringThe number of Gaussians per genone can be reduced usingclustering.
Specifically, we used an agglomerative procedure tocluster the component densities within each genone.
The criteriathat we considered were an entropy-based distance and ageneralized likelihood-based distance \[6\].
We found that theentropy-based distance worked better.
This criterion is thecontinuous-density analog of the weighted-by-counts entropy ofthe discrete HMM state distributions, often used for clusteringHMM state distributions \[5\], \[3\].Our experiments showed that the number of Gaussians per genonecan be reduced by a factor of three by first clustering and thenperforming one additional iteration of the Baum-Welch algorithmas shown in Table 6.
The table also shows that clustering followedby additional training iterations gives better accuracy than directlytraining a system with a smaller number of Gaussians (Table 6,Baseline2).
This is especially true as the number of Gaussians pergenone decreases.SystemGaussiansper GenoneWord Erro~(%)Baseline1 - - 48 13.43Baselinel+Clustering 18 14.17above+Retraining 18 13.64Baseline2 25 14.35Table 6: Improved training of systems with fewerGaussians by clustering from a larger number ofGaussians4.2.
Gaussian ShortlistsWe have developed a method for eliminating large numbers ofGaussians before they are computed.
Our method is to build a"Gaussian shorflist" \[2\], \[4\], which uses vector quantization tosubdivide the acoustic space into regions, and lists the Gaussiansworth evaluating within each region.
Applied to unclusteredgenonic recognition systems, this technique has allowed us toreduce by more than a factor of five the number of Gaussiansconsidered each frame.
Here we apply Gaussian shortlists to theclustered system described in Section 4.1.
Several methods forgenerating improved, smaller Gaussian shorflists are discussed andapplied to the same system.Table 7 shows the word error rates for shortlists generated by avariety of methods.
Through a series of methods, we have reducedthe average number of Gaussian distributions evaluated for eachgenone from 18 to 2.48 without compromising accuracy.
Thevarious hortlists tested were generated in the following ways:?
None: No shortlist was used.
This is the baseline case fromthe clustered system described above.
All 18 Gaussians areevaluated whenever a genone is active.?
12D-256: Our original shortlist method was used.
Thismethod uses a cepstral vector quantization codebook (12-dimensions, 256 codewords) to partition the acousticspace.
With unclustered systems, this method generallyachieves a 5:1 reduction i  Gaussian computation.
I  thisclustered system, only a 3:1 reduction was achieved, mostlikely because the savings from clustering and Gaussianshortlists overlap.?
39D-256: The cepstral codebook that partitions theacoustic space in the previous method ignores 27 of the 39feature dimensions.
By using a 39-dimensional, 256-codeword VQ eodebook, we created better-differentiatedacoustic regions, and reduced the average shortlist lengthto 4.93.?
39D-4096: We further decreased the number of Gaussiansper region by shrinking the size of the regions.
Here weused a single-feature VQ codebook with 4096 eodewords.396For such a large codebook, vector quantizafion isaccelerated using a binary tree VQ fastmateh.39D-4096-minl: When generating a Gaussian shortlist,certain region/genone pairs with low probabilities areassigned very few or even no Ganssians densities.
Whenwe were using 48 Gaussians/genone, w  found it importantto ensure that each list contains aminimum of threeGaussian densities.
With our current clustered systems wefound that we can achieve similar recognition accuracy byensuring only one Gaussian per list.
As shown in Table 7,this technique results in lists with an average of 2.48Gaussians per genone, without hurting accuracy.Gaussians Shortlist Word Error Evaluated (%) Shortlist Length per Framenone 18 5459 13.6412D-256 6.08 1964 13.5339D-256 4.93 1449 13.4639D-4096 3.68 1088 13.6439D-4096-minl 2.48 732 13.50Table 7: Word error  rates and Gaussians evaluated, fora variety of Gaussian shortlistsThus, with the methods in Sections 4.1 and 4.2, we have usedclustering, retraining, and new Gaussian shortlist echniques toreduce computation from 48 to an average of 2.48 Gaussians pergenone without affecting accuracy.4.3.
Genonic ApproximationsWe have successfuUy employed one other method for reducingGaussian computation.
For certain pairs of genones and acousticregions, even the evaluation of one or two Gaussian distributionsmay be excessive.
These are cases where the output probability iseither very low or very uniform across an acoustic region.
Here auniform probability across the region (i.e., requiring no Gaussianevaluations) may be sufficient to model the output probability.To provide these regional flat probabilities, we implemented adiscrete-density HMM, but one whose output probabilities were aregion-by-region approximation of the probabilities of ourgenonic system.
Since the two systems' outputs are calibrated, wecan use them interchangeably, using a variety of criteria to decidewhich system's output o use for any given frame, state, acousticregion, or hypothesis.
This technique, using variable resolutionoutput models for HMMs is similar to what has been suggested byAlleva et al \[1\].We train this genonic approximation by averaging, for eachacoustic region, the output of each genone across a set ofobservations.
The resulting system can be used either by itself orin combination with the continuous ystem from which it wastrained.Table 8 shows the performance ofthe discrete approximate g nonesystems as a function of the number of regions used.GenonicSystemContinuousNumber ofAcousticRegionsWord Error(%)N/A 13.64Discrete 256 31.72Discrete 1024 23.62Discrete 4096 20.3216384 Discrete 18.40Table 8: Accuracy of genonic approximation systemsEven with 16384 acoustic regions, the discrete genonicapproximation has an error rate of 18.40%, compared with thebaseline continuous ystem at 13.64%.
However, when thesediscrete systems are used selectively in combination with acontinuous genonie system, the results are more encouraging.
Ourmost successful merger combines the 4096-region discreteapproximation system (20.32% error) with the 39D-4096-minlgenone system from Table 7 (13.50% error).
In combining the two,instead of ensuring that a single Gaussian density was available forall shortlists, the genonic approximation was used for cases whereno densities existed.
In this way, we were able to eliminate another25% of the Gaussian computations, reducing our lattice-basedcomputation burden to 564 Gaussians per frame, with a word errorof 13.36%.In summary, we started with a speech recognition system with28,272 Gaussian distributions that computed 14,538 Gaussiandistributions per frame and achieved a 13.43% word error raterunning 12.2 times slower than real time on word lattices.
Usingthe techniques described in Section 4, we have reduced thesystem's computational requirements o 564 Gaussians per frame,resulting in a system with word error of 13.36%, running at 2.0times real time on our word lattices.5.
MULT IPASS APPROACHESThe techniques for improving the speed of single-pass speechrecognition systems can be combined to achieve other speed/accuracy trade-offs (e.g., trees using genone systems with reducedGaussian computation rates).
Furthermore, with multipassapproaches \[8,9\] many of these techniques can be usedindependently asthe different passes of the speech recognitionsystem.
For instance, adiscrete density tree search may be used ina lattice building or a forward pass, and a Gaussian system may beused in the lattice and/or backward passes.We have performed preliminary evaluations of several of the tree-based systems presented inSection 3 to evaluate their performanceas forward passes for a forward-backward search \[9\].
Preliminaryresults show that forward tree-based systems with 30% word errorwould add at most 3% to the word error rate of a fuU accuracybackward pass (i.e., at most increase the error rate from397approximately 10% to approximately 13%).
More detail on thiswork wiU be presented at the HLT conference and will be includedin the final version of this paper.6.
CONCLUSIONSTree-based techniques, combined with appropriate modelingalternatives, can achieve real-time performance at about 30% errorrate for ARPA's 20,000-word Wall Street Journal task.
We haveshown techniques that reduce the computational complexity ofmore accurate but slower modeling alternatives so that they arenear the speed necessary for real-time performance in a multipasssearch.
Our near-future goal is to combine these two technologiesso that real-time, high-accuracy large-vocabulary speechrecognition can be achieved.ACKNOWLEDGMENTWe gratefully acknowledge support for this work from ARPAthrough Office of Naval Research Contract N00014-92-C-0154.The Government has certain fights in this material.
Any opinions,findings, and conclusions or recommendations expressed in thismaterial are those of the authors and do not necessarily reflect heviews of the Government funding agencies.Continuous Speech Recognition," Proc.
ICASSP, pp.
I-9 - 1-12, March 1992.. H. Murveit, J. Butzberger, V.Digalakis and M. Weintraub,"Large Vocabulary Dictation using SRI's DECIPHER TMSpeech Recognition System: Progressive SearchTechniques," Proc.
ICASSP, pp.
H-319 - II-322, April 1993.. L. Nguyen, R. Schwartz, E Kubala and P. Placeway,"Search Algorithms for Software-Only Real-'I~meRecognition with Very Large Vocabularies," Proc.
ARPAHuman Language Technology Workshop, March 1993.10.
L. Bahl, S. De Gennaro, P. Gopalakrishnan d R. Mercer,"A Fast Approximate Acoustic Match for Large VocabularySpeech Recognition," Proc.
Eurospeech 1989.11.
S. Katz, "Estimation of Probabilities from Sparse Data forthe Language Model Component of a Speech Recognizer,"ASSP-35 pp.
400-401, March 1987...4...7.R E F E R E N C E SE Alleva, X. D. Huang and M.-Y.
Hwang, "An ImprovedSearch Algorithm Using Incremental Knowledge forContinuous Speech Recognition," Proc.
1CASSP, pp.
II-307- H-310, April 1993.E.
Bocchieri, "Vector Quantization for the EfficientComputation of Continuous Density Likelihoods," Proc.ICASSP, pp.
H-692 - H-695, April 1993.V.
Digalakis and H. Murveit, "Genones: Optimizing theDegree of Tying in a Large Vocabulary HMM-basedSpeech Recognizer," toappear in Proc.
ICASSP, 1994.V.
Digalakis, E Monaco and H. Murveit, "AcousticCalibration and Search in SRI's Large Vocabulary HMM-based Speech Recognition System," Proc.
IEEE ASRWorkshop, Snowbird, Dec. 1993.M.-Y.
Hwang and X. D. Huang, "Subphonefic Modelingwith Markov States - Senone," Proc.
ICASSP, pp.
1-33-36,March 1992.A.
Kannan, M. Ostendorf and J. R. Rohlicek, "MaximumLikelihood Clustering of Gaussians for SpeechRecognition," in IEEE Transactions Speech and AudioProcessing, to appear July 1994.H.
Ney, R. Haeb-Umbach, B. Tran and M. Oerder,"Improvements in Beam Search for 10,000-word398
