Topic Identification in DiscourseKuang-hua ChenDepartment of Computer Science and Information EngineeringNational Taiwan UniversityTaipei, Taiwan, R.O.C.khchen@nlg.csie.ntu.edu.twAbstractThis paper proposes a corpus-based languagemodel for topic identification.
We analyzethe association of noun-noun and noun-verbpairs in LOB Corpus.
The word associationnorms are based on three factors: 1) wordimportance, 2) pair co-occurrence, and 3)distance.
They are trained on the paragraphand sentence l vels for noun-noun and noun-verb pairs, respectively.
Under the topiccoherence postulation, the nouns that havethe strongest connectivities with the othernouns and verbs in the discourse form thepreferred topic set.
The collocationalsemantics then is used to identify the topicsfrom paragraphs and to discuss the topicshift phenomenon among paragraphs.1 IntroductionAlthough only speakers and writers instead of textshave topics (Brown and Yule, 1983, p. 68), naturallanguage researchers always want to identify a topicor a set of possible topics from a discourse for furtherapplications, such as anaphora resolution,information retrieval and so on.
This paper adopts acorpus-based approach to process discourseinformation.
We postulate that:(1) Topic is coherent and has strong relation-ships with the events in the discourse.Now, consider the following example quoted fromthe Lancaster-Oslo/Bergen (LOB) Corpus(Johansson, 1986).
The topics in this example are"problem" and "dislocation".
The two words aremore strongly related to the verbs ("explain", "fell","placing" and "suppose") and nouns ("theories","explanations", roll", "codex", "disorder", "order","disturbance" and "upheaval").There is a whole group of theories whichattempt to explain the problems of the FourthGospel by explanations based on assumedtextual dislocations.
The present state of theGospel is the result of an accident-pronehistory.
The original was written on a roll, orcodex, which fell into disorder or wasaccidentally damaged.
An editor, who was notthe author, made what he could of the chaos byplacing the fragments, or sheets, or pages, inorder.
Most of those who expound a theory oftextual dislocation take it for granted that theGospel was written entirely by one authorbefore the disturbance took place but a fewleave it open to suppose that the original bookhad been revised even before the upheaval.We also postulate that(2) Noun-verb is a predicate-argument relation-ship on the sentence level and noun-nounrelationship is associated on discourse level.The postulation (2) could be also observed from theabove example.
These relationships may berepresented implicitly by collocational semantics.Collocation has been applied successfully to manypossible applications (Church et al , 1989), e.g,lexicography (Church and Hanks, 1990), informationretrieval (Salton, 1986a), text input (Yamashina ndObashi, 1988), etc.
This paper will touch on itsfeasibility in topic identification.This paper is organized as follows.
Section 2presents a corpus-based language model and discusshow to train this model.
Section 3 touches on topicidentification i discourse.
Section 4 shows a seriesof experiments based on the proposed model anddiscusses the results.
Section 5 gives short remarks.2 A Language ModelBrown and Yue (1983) pointed out there are twokinds of topics: one is sentence topic and the other isdiscourse topic.
The discourse topic is usually theform of topic sentence.
We postulate, further, thatthe noun in the topic sentence play important roles inthe whole discourse.
Thus nouns play the core partin the underlying language model.
The associationsof a noun with other nouns and verbs are supportingfactors for it to be a topic.267The importance of a specific verb or noun isdefined by Inverse Document Frequency (IDF)(Salton, 1986b):IDF(W) = log((P - O(W)) / O(W)) + c (1)where P is the number of documents in LOB Corpus,i.e.
500, O(I4/) is the number of documents with wordW, and c is a threshold value.
LOB Corpus is amillion-word collection of present-day BritishEnglish texts.
It contains 500 texts of approximately2,000 words distributed over 15 text categories(Johansson, 1986).
These categories includereportage, editorial, reviews, religion, skills, trades,popular lore, belles lettres, biography, essays, learnedand scientific writings, fictions, humour, adventureand western fiction, love story, etc.
That is to say,LOB Corpus is a balanced corpus.
The tag set ofLOB Corpus is based on the philosophy of that ofBrown Corpus (Francis and Kucera, 1979), but somemodifications are made.
This is to achieve greaterdelicacy, while preserving comparability with theBrown Corpus.Those words that appear more than one haft ofthe documents in LOB Corpus have negative log((P-.
O(W))/O(W)) shown below.Noun:Verb:time(-3.68) way(-1.92) year(-1.71)man(-1.47) day(-1.12) part(-0.76)people(-0.75) thing(-0.73) hand(-0.54)life(-0.51) fact(-0.40) place(-0.40)work(-0.35) end(-0.12) case(-0.09)point(-0.05)make(-5.01) take(-3.56) give(-2.95)come(-2.45) find(-2.30) see(-2.26)know(-2.20) say(-2.18) go(-2.11)seem(-l.30) show(-l.20) think(-l.18)use(-1.07) get(-l.06) become(-0.95)bring(-0.73) put(-0.68) leave(-0.62)1ook(-0.48) call(-0.43) tell(-0.41)keep(-0.32) hold(-0.18) ask(-0.23)begin(-0.08)The threshold values for nouns and verbs are set to0.77 and 2.46 respectively.
The two values are usedto screen out the unimportant words, whose 1DFvalues are negative.
That is, their 1DF values arereset to zero.
The strength of one occurrence of averb-noun pair or a noun-noun pair is computed bythe importance of the words and their distances:SNV(N~,~) =IDF(N~).IDF(V~)I D(N~,Vj) (2)SNN(N~, Nk) = IDF(N~).IDF(Nk) / D(N~, Nk) (3)where SNV denotes the strength of a noun-verb pair,SNN the strength of a noun-noun pair, and D(X,Y)represents the distance between X and Y.
When iequals to k, the SNN(Ni,Nk) is set to zero.
Includingthe distance factor is motivated by the fact that therelated events are usually located in the sametexthood.
This is the spatial ocality of events in adiscourse.The distance is measured by the differencebetween cardinal numbers of two words.
We assigna cardinal number to each verb and noun insentences.
The cardinal numbers are keptcontinuous across sentences in the same paragraph.For example,With so many problems 1 to solve2, it wouldbe a great helP3 to select 4 some one problem 5which might be the key 6 to all the others, andbegin 7 there.
If there is any such key-problem 8,then it is undoubtedly the problem 9of the unitYlo of the Gospelll.
There arethree viewsl2 of the Fourth Gospell3 whichhave been held14.Therefore, the cardinal number of problems,C(problems), equals to 1 and C(held) is 14.
Thedistance can be defined to beD(Z,Y) = abs( C(X)-C( Y) ) (4)The association orms of verb-noun and noun-nounpairs are summation of the strengths of all theiroccurrences in the corpus:ANV(Nj, V~) = Z SNV(Ni' Vs) (5)ANN(Ni, N k) = Z SNN(N~, N k ) (6)where ANV denotes the association orm of a noun-verb pair, and ANN the association orm of a noun-noun pair.
The less frequent word has a higher IDFvalue so that the strength SNV and SNN of oneoccurrence may be larger.
However, the number ofterms to be summed is smaller.
Thus, the formulaeIDF and ANV (ANN) are complementary.
LOBCorpus of approximately one million words is usedto train the basic association orms.
They are basedon different levels: the paragraph and sentence l velsfor noun-noun and noun-verb pairs respectively.Table 1 shows the statistics of the training corpus.The words with tags NC, NNU and NNUS and Dittotags are not considered.
Here NC means cited words,and NNU (NNUS) denotes abbreviated (plural) unitof measurement unmarked for number.
Ditto tagsare those words whose senses in combination differfrom the role of the same words in other context.For example, "as to", "each other", and "so as to"(Johansson, 1986).268Table 1.
Statistics for LOB CorpusnumberDocument 500Paragraph 18678Sentences 54297Nouns 23399Verbs 4358N-N Pairs 3476842V-N Pairs 422945Under the topic coherence postulation in aparagraph, we compute the connectivities of thenouns in each sentence with the verbs and nouns inthe paragraph.
For example, 439 verbs in LOBCorpus have relationships with the word "problem"in different degrees.
Some of them are listed belowin descending order by the strength.solve(225.21), face(84.64) ..... specify(16.55) .....explain(6.47), ..., fal1(2.52) ..... suppose(1.67) ....For the example in Section 1, the word "problem"and "dislocation" are coherent with the verbs andnouns in the discourse.
The nouns with the strongestconnectivity form the preferred topic set.
Considerthe interference effects.
The constituents far aparthave less relationship.
Distance D(X,Y) is used tomeasure such effects.
Assume there are m nouns andn verbs in a paragraph.
The connective strength of anoun Ni (1 < i < m) is defined to be:CSNN(N~) = Z (ANN(N,  N k) / D(Ni, Ark) (7)kCSNV(N~) = Z (ANN(N~, V k) / D(N,, V k)) (8)kCS(N~) = (PN.
CSNN(N,) + PV.
CSNV(N~)) / c (9)where CS denotes the connective strength, and PArand PV are parameters for CSNN and CSNV andPN+PV=I.The determination of par and PV is via deletedinterpolation (Jelinek, 1985).
Using equation PN +PV = 1 and equation 9, we could derive PAr and PVas equation 10 and equation 11 show.CS-  CSNVPN = (10)CSNN - CSNVCS - CSNNPV - (11)CSNV - CSNNLOB corpus are separated into two parts whosevolume ratio is 3:1.
Both PN and PV are initializedto 0.5 and then are trained by using the 3/4 corpus.Alter the first set of parameters i generated, theremain 1/4 LOB corpus is run until par and PVconverge using equations 9, 10 and 11.
Finally, theparameters, PN and PV, converge to 0.675844 and0.324156 respectively.3 Topic Identification in a ParagraphThe test data are selected from the first text of thefiles LOBT-DI, LOBT-F1, LOBT-G1, LOBT-H1,LOBT-KI, LOBT-M1 and LOBT-N1 of horizontalversion of LOB Tagged Corpus for inside test(hereafter, we will use D01, F01, G01, H01, K01,M01, and N01 to represent these texts respectively).Category D denotes religion, Category F denotespopular lore, Category G denotes belles lettres,biography and essays, Category H denotesMiscellaneous texts, Category K denotes generalfiction, Category M denotes science fiction, andCategory N denotes adventure and western fiction.Each paragraph has predetermined topics (calledassumed topics) which are determined by a linguist.Because a noun with basic form N may appear morethan once in the paragraph, say k times, its strengthis normalized by the following recursive formula:NCS( N m) ) = CS( N o(~) ) (12)NCS( No(o) = NCS( No(,_,) +(1 - NCS(No(,_,))).CS(No(o) (13)where NCS represents the net connective strength,o(k) denotes the cardinal number of the k'thoccurrence of the same N such that C(NoO)) <C(No(2)) < C(No(3)) <... < C(No(k-l)) < C(No(k)).The possible topic N* has the high probabilityNCS(N*).
Here, a topic set whose members are thefirst 20% of the candidates is formed.
Theperformance an be measured as the Table 2 shows.4 The Preliminary Experimental ResultsAccording to the language model mentioned inSection 2, we build the ANN and ANV values foreach noun-noun pair and noun-verb pair.
Then, weapply recursive formula of NCS shown in equations12 and 13 to identifying the topic set for test texts.Table 3 shows experimental results.
Symbols tx andc denotes mean and standard eviation.
(+) denotescorrect number, (-) denotes error number and (?
)denotes undecidable number in topic identification.The undecidable case is that the assumed topic is apronoun.
Figure 1 shows correct rate, error rate, andundecidable rate.Row (1) in Table 3 shows the difficulty in findingtopics from many candidates.
In general, there aremore than 20 candidates in a paragraph, It isimpossible to select opics at random.
Row (2) gives269the rank of assumed topic.
The assumed topics areassigned by a linguist.
Comparing row (1) and row(2), the average number of candidates are muchlarger than the rank of assumed topic.
Since it isimpossible to randomly select candidates as topics,we know topic identification isvaluable.Rows (3), (4) and (5) list the frequencies ofcandidates, assumed topics and computed topic.
Theresults intensify the viewpoint that the repeatedwords make persons impressive, and these words arelikely to be topics.
Our topic identificationalgorithm demonstrates the similar behavior (seerows (4) and (5)).
The average frequencies ofassumed topics and computed topics are close andboth of them are larger than average frequency ofcandidates.
Figure 2 clearly demonstrates this point.Row (6) reflects an interesting phenomenon.
Thetopic shifted by authors from paragraph to paragraphis demonstrated through comparison of data shownin this row and row (2).
The rank value of previoustopics do obviously increase.
Recall that large rankvalue denotes low rank.Table 2.
Metrics for Performance1 average #of candidates2 average rank of assumed topic3 frequency of candidates4 frequency of assumed topic5 frequency of computed topic6 average rank of topic in previous paragraphE # of nouns in basic form in paragraph i / # of paragraphsE rank of assumed topic in paragraph i/# of paragraphsy.
# of nouns / E # of nouns in basic form in paragraph iE occurrences ofassumed topic / # of paragraphsE occurrences ofcomputed topic / # of paragraphsE rank of topic in previous paragraph / (# of paragraph - 1)(F, '~)Table 3.
Experimental ResultsD01 F01 G01 H01 K01 M01 N01(1) (21.59, 9.96) (10.57, 18.42) (62.43, 18.42) (19.77, 8.39) (31.71, 23.80) (15.22, 6.44) (12.21, 6.73)(2) (4.56, 5.98) (5.25, 5.51) (7.29, 10.35) (4.55, 4.13) (7.08, 16.02) (2.61, 2.11) (3.68, 3.87)(3) (1.32, 0.88) (1.39, 0.89) (1.21, 0.56) (1.33, 0.82) (1.11, 0.39) (1.11, 0.32) (1.06, 0.25)(4) (2.61, 1.60) (1.27, 1.21) (2.57, 1.18) (2.46, 1,62) (1.77, 1.05) (1.50, 0.69) (1.28, 0.60)(5) (3.33, 1.97) (2.39, 1.84) (3.43, 1.40) (2.91, 1.56) (1.86, 0.99) (1.48, 0.50) (1.29, 0.52)(6) (6.29, 7.84) (5.48, 5.09) (19.67, 16.64) (5.71, 6.06) (17.23, 18.51) (7.92, 6.28) (9.36, 6.62)(+) 12 13 6 12 9 13 15(-) 6 15 1 10 4 5 10(?)
0 0 0 0 1 9 9TextN01M01K01H01G01F01D0110 2ONumber of ParagraphsFigure 1.
The Results of Topic Identificationible (?
)+)I3O 40270TextN01M01K01H01G01F01D01)picl)ic1 2MeanFigure 2.
Comparison of Frequencym l45 Conc lud ing  RemarksDiscourse analysis is a very difficult problem innatural anguage processing.
This paper proposes acorpus-based language model to tackle topi.cidentification.
The word association norms of noun-noun pairs and noun-verb pairs which model themeanings of texts are based on three factors: 1) wordimportance, 2) pair occurrence, and 3) distance.
Thenouns that have the stronger connectivities withother nouns and verbs in a discourse could form apreferred topic set.
Inside test of this proposedalgorithm shows 61.07% correct rate (80 of 131paragraphs).Besides topic identification, the algorithm coulddetect topic shift phenomenon.
The meaningtransition from paragraph to paragraph could bedetected by the following way.
The connectivestrengths of the topics in the previous paragraphwith the nouns and the verbs in the currentparagraph are computed, and compared with thetopics in the current paragraph.
As our experimentsshow, the previous topics have the tendency todecrease their strengths in the current paragraph.AcknowledgmentWe are thankful to Yu-Fang Wang and Yue-Shi Leefor their help in this work.ReferencesG.
Brown and G. Yule.
1983.
Discourse Analysis.Cambridge University Press.K.W.
Church, W. Gale, P. Hanks and D. Hindle.1989.
Parsing, Word Associations and TypicalPredicate-Argument Relations.
In Proceedingsof International Workshop on ParsingTechnologies, 389-398.K.W.
Church and P. Hanks.
1990.
Word AssociationNorms, Mutual Information, and Lexicography.Computational Linguistics, 16(1), 22-29.W.N.
Francis and H. Kucera.
1979.
Manual ofInformation to Accompany a Standard Sampleof Present-day Edited American English , forUse with Digital Computers.
Original ed.
1964,revised 1971, revised and augmented 1979.Department of Linguistics, Brown University,Providence, R.I.F.
Jelinek.
1985.
Markov Source Modeling of TextGeneration.
In J.K. Skwirzynski (ed.
), TheImpact of Processing Techniques onCommunication, Nijhoff, Dordrecht, TheNetherlands.S.
Johansson.
1986.
The Tagged LOB Corpus: Users'Manual.
Bergen: Norwegian Computing Centrefor the Humanities.G.
Salton.
1986a.
On the Use of Term Associationsin Automatic Information Retrieval.
InProceedings of l lth COLING, 380-386.G.
Salton.
1986b.
Another Look at Automatic Text-Retrieval Systems.
Communications of ACM,29(7), 648-656,M.
Yamashina and S. Obashi.
1988.
CollocationalAnalysis in Japanese Text Input.
InProceedings of l 2th COLING, 770-772.271
