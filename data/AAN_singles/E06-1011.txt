Online Learning of Approximate Dependency Parsing AlgorithmsRyan McDonald Fernando PereiraDepartment of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104{ryantm,pereira}@cis.upenn.eduAbstractIn this paper we extend the maximumspanning tree (MST) dependency parsingframework of McDonald et al (2005c)to incorporate higher-order feature rep-resentations and allow dependency struc-tures with multiple parents per word.We show that those extensions can makethe MST framework computationally in-tractable, but that the intractability can becircumvented with new approximate pars-ing algorithms.
We conclude with ex-periments showing that discriminative on-line learning using those approximate al-gorithms achieves the best reported pars-ing accuracy for Czech and Danish.1 IntroductionDependency representations of sentences (Hud-son, 1984; Me?lc?uk, 1988) model head-dependentsyntactic relations as edges in a directed graph.Figure 1 displays a dependency representation forthe sentence John hit the ball with the bat.
Thissentence is an example of a projective (or nested)tree representation, in which all edges can bedrawn in the plane with none crossing.
Sometimesa non-projective representations are preferred, asin the sentence in Figure 2.1 In particular, forfreer-word order languages, non-projectivity is acommon phenomenon since the relative positionalconstraints on dependents is much less rigid.
Thedependency structures in Figures 1 and 2 satisfythe tree constraint: they are weakly connectedgraphs with a unique root node, and each non-rootnode has a exactly one parent.
Though trees are1Examples are drawn from McDonald et al (2005c).more common, some formalisms allow for wordsto modify multiple parents (Hudson, 1984).Recently, McDonald et al (2005c) have shownthat treating dependency parsing as the searchfor the highest scoring maximum spanning tree(MST) in a graph yields efficient algorithms forboth projective and non-projective trees.
Whencombined with a discriminative online learning al-gorithm and a rich feature set, these models pro-vide state-of-the-art performance across multiplelanguages.
However, the parsing algorithms re-quire that the score of a dependency tree factorsas a sum of the scores of its edges.
This first-orderfactorization is very restrictive since it only allowsfor features to be defined over single attachmentdecisions.
Previous work has shown that condi-tioning on neighboring decisions can lead to sig-nificant improvements in accuracy (Yamada andMatsumoto, 2003; Charniak, 2000).In this paper we extend the MST parsing frame-work to incorporate higher-order feature represen-tations of bounded-size connected subgraphs.
Wealso present an algorithm for acyclic dependencygraphs, that is, dependency graphs in which aword may depend on multiple heads.
In both casesparsing is in general intractable and we providenovel approximate algorithms to make these casestractable.
We evaluate these algorithms withinan online learning framework, which has beenshown to be robust with respect approximate in-ference, and describe experiments displaying thatthese new models lead to state-of-the-art accuracyfor English and the best accuracy we know of forCzech and Danish.2 Maximum Spanning Tree ParsingDependency-tree parsing as the search for themaximum spanning tree (MST) in a graph was81root John saw a dog yesterday which was a Yorkshire TerrierFigure 2: An example non-projective dependency structure.roothitJohn ball withthe battheroot0 John1 hit2 the3 ball4 with5 the6 bat7Figure 1: An example dependency structure.proposed byMcDonald et al (2005c).
This formu-lation leads to efficient parsing algorithms for bothprojective and non-projective dependency treeswith the Eisner algorithm (Eisner, 1996) and theChu-Liu-Edmonds algorithm (Chu and Liu, 1965;Edmonds, 1967) respectively.
The formulationworks by defining the score of a dependency treeto be the sum of edge scores,s(x,y) =?
(i,j)?ys(i, j)where x = x1 ?
?
?
xn is an input sentence and ya dependency tree for x.
We can view y as a setof tree edges and write (i, j) ?
y to indicate anedge in y from word xi to word xj .
Consider theexample from Figure 1, where the subscripts indexthe nodes of the tree.
The score of this tree wouldthen be,s(0, 2) + s(2, 1) + s(2, 4) + s(2, 5)+ s(4, 3) + s(5, 7) + s(7, 6)We call this first-order dependency parsing sincescores are restricted to a single edge in the depen-dency tree.
The score of an edge is in turn com-puted as the inner product of a high-dimensionalfeature representation of the edge with a corre-sponding weight vector,s(i, j) = w ?
f(i, j)This is a standard linear classifier in which theweight vector w are the parameters to be learnedduring training.
We should note that f(i, j) can bebased on arbitrary features of the edge and the in-put sequence x.Given a directed graph G = (V,E), the maxi-mum spanning tree (MST) problem is to find thehighest scoring subgraph of G that satisfies thetree constraint over the vertices V .
By defininga graph in which the words in a sentence are thevertices and there is a directed edge between allwords with a score as calculated above, McDon-ald et al (2005c) showed that dependency pars-ing is equivalent to finding the MST in this graph.Furthermore, it was shown that this formulationcan lead to state-of-the-art results when combinedwith discriminative learning algorithms.Although the MST formulation applies to anydirected graph, our feature representations and oneof the parsing algorithms (Eisner?s) rely on a linearordering of the vertices, namely the order of thewords in the sentence.2.1 Second-Order MST ParsingRestricting scores to a single edge in a depen-dency tree gives a very impoverished view of de-pendency parsing.
Yamada and Matsumoto (2003)showed that keeping a small amount of parsinghistory was crucial to improving parsing perfor-mance for their locally-trained shift-reduce SVMparser.
It is reasonable to assume that other pars-ing models might benefit from features over previ-ous decisions.Here we will focus on methods for parsingsecond-order spanning trees.
These models fac-tor the score of the tree into the sum of adjacentedge pair scores.
To quantify this, consider againthe example from Figure 1.
In the second-orderspanning tree model, the score would be,s(0,?, 2) + s(2,?, 1) + s(2,?, 4) + s(2, 4, 5)+ s(4,?, 3) + s(5,?, 7) + s(7,?, 6)Here we use the second-order score functions(i, k, j), which is the score of creating a pair ofadjacent edges, from word xi to words xk and xj .For instance, s(2, 4, 5) is the score of creating theedges from hit to with and from hit to ball.
Thescore functions are relative to the left or right ofthe parent and we never score adjacent edges thatare on different sides of the parent (for instance,82there is no s(2, 1, 4) for the adjacent edges fromhit to John and ball).
This independence betweenleft and right descendants allow us to use a O(n3)second-order projective parsing algorithm, as wewill see later.
We write s(xi,?, xj) when xj isthe first left or first right dependent of word xi.For example, s(2,?, 4) is the score of creating adependency from hit to ball, since ball is the firstchild to the right of hit.
More formally, if the wordxi0 has the children shown in this picture,xi0xi1 .
.
.
xij xij+1 .
.
.
ximthe score factors as follows:?j?1k=1 s(i0, ik+1, ik) + s(i0,?, ij)+ s(i0,?, ij+1) +?m?1k=j+1 s(i0, ik, ik+1)This second-order factorization subsumes thefirst-order factorization, since the score functioncould just ignore the middle argument to simulatefirst-order scoring.
The score of a tree for second-order parsing is nows(x,y) =?
(i,k,j)?ys(i, k, j)where k and j are adjacent, same-side children ofi in the tree y.The second-order model allows us to conditionon the most recent parsing decision, that is, the lastdependent picked up by a particular word, whichis analogous to the the Markov conditioning of inthe Charniak parser (Charniak, 2000).2.2 Exact Projective ParsingFor projective MST parsing, the first-order algo-rithm can be extended to the second-order case, aswas noted by Eisner (1996).
The intuition behindthe algorithm is shown graphically in Figure 3,which displays both the first-order and second-order algorithms.
In the first-order algorithm, aword will gather its left and right dependents in-dependently by gathering each half of the subtreerooted by its dependent in separate stages.
Bysplitting up chart items into left and right com-ponents, the Eisner algorithm only requires 3 in-dices to be maintained at each step, as discussed indetail elsewhere (Eisner, 1996; McDonald et al,2005b).
For the second-order algorithm, the keyinsight is to delay the scoring of edges until pairs2-order-non-proj-approx(x, s)Sentence x = x0 .
.
.
xn, x0 = rootWeight function s : (i, k, j) ?
R1.
Let y = 2-order-proj(x, s)2. while true3.
m = ?
?, c = ?1, p = ?14.
for j : 1 ?
?
?n5.
for i : 0 ?
?
?n6.
y?
= y[i ?
j]7. if ?tree(y?)
or ?k : (i, k, j) ?
y continue8.
?
= s(x,y?)
?
s(x,y)9. if ?
> m10.
m = ?, c = j, p = i11.
end for12.
end for13.
if m > 014. y = y[p ?
c]15. else return y16.
end whileFigure 4: Approximate second-order non-projective parsing algorithm.of dependents have been gathered.
This allows forthe collection of pairs of adjacent dependents ina single stage, which allows for the incorporationof second-order scores, while maintaining cubic-time parsing.The Eisner algorithm can be extended to anarbitrary mth-order model with a complexity ofO(nm+1), for m > 1.
An mth-order parsing al-gorithm will work similarly to the second-order al-gorithm, except that we collect m pairs of adjacentdependents in succession before attaching them totheir parent.2.3 Approximate Non-projective ParsingUnfortunately, second-order non-projective MSTparsing is NP-hard, as shown in appendix A. Tocircumvent this, we designed an approximate al-gorithm based on the exact O(n3) second-orderprojective Eisner algorithm.
The approximationworks by first finding the highest scoring projec-tive parse.
It then rearranges edges in the tree,one at a time, as long as such rearrangements in-crease the overall score and do not violate the treeconstraint.
We can easily motivate this approxi-mation by observing that even in non-projectivelanguages like Czech and Danish, most trees areprimarily projective with just a few non-projectiveedges (Nivre and Nilsson, 2005).
Thus, by start-ing with the highest scoring projective tree, we aretypically only a small number of transformationsaway from the highest scoring non-projective tree.The algorithm is shown in Figure 4.
The ex-pression y[i ?
j] denotes the dependency graphidentical to y except that xi?s parent is xi instead83FIRST-ORDERh1h3?h1 r r+1 h3(A)h1h3h1 h3(B)SECOND-ORDERh1h2 h2 h3?h1 h2 h2 r r+1 h3(A)h1h2 h2 h3?h1 h2 h2 h3(B)h1h3h1 h3(C)Figure 3: A O(n3) extension of the Eisner algorithm to second-order dependency parsing.
This figureshows how h1 creates a dependency to h3 with the second-order knowledge that the last dependent ofh1 was h2.
This is done through the creation of a sibling item in part (B).
In the first-order model, thedependency to h3 is created after the algorithm has forgotten that h2 was the last dependent.of what it was in y.
The test tree(y) is true iff thedependency graph y satisfies the tree constraint.In more detail, line 1 of the algorithm sets y tothe highest scoring second-order projective tree.The loop of lines 2?16 exits only when no fur-ther score improvement is possible.
Each iterationseeks the single highest-scoring parent change toy that does not break the tree constraint.
To thateffect, the nested loops starting in lines 4 and 5enumerate all (i, j) pairs.
Line 6 sets y ?
to the de-pendency graph obtained from y by changing xj?sparent to xi.
Line 7 checks that the move from yto y?
is valid by testing that xj?s parent was not al-ready xi and that y?
is a tree.
Line 8 computes thescore change from y to y?.
If this change is largerthan the previous best change, we record how thisnew tree was created (lines 9-10).
After consid-ering all possible valid edge changes to the tree,the algorithm checks to see that the best new treedoes have a higher score.
If that is the case, wechange the tree permanently and re-enter the loop.Otherwise we exit since there are no single edgeswitches that can improve the score.This algorithm allows for the introduction ofnon-projective edges because we do not restrictany of the edge changes except to maintain thetree property.
In fact, if any edge change is evermade, the resulting tree is guaranteed to be non-projective, otherwise there would have been ahigher scoring projective tree that would have al-ready been found by the exact projective parsingalgorithm.
It is not difficult to find examples forwhich this approximation will terminate withoutreturning the highest-scoring non-projective parse.It is clear that this approximation will alwaysterminate ?
there are only a finite number of de-pendency trees for any given sentence and each it-eration of the loop requires an increase in scoreto continue.
However, the loop could potentiallytake exponential time, so we will bound the num-ber of edge transformations to a fixed value M .It is easy to argue that this will not hurt perfor-mance.
Even in freer-word order languages suchas Czech, almost all non-projective dependencytrees are primarily projective, modulo a few non-projective edges.
Thus, if our inference algorithmstarts with the highest scoring projective parse, thebest non-projective parse only differs by a smallnumber of edge transformations.
Furthermore, itis easy to show that each iteration of the loop takesO(n2) time, resulting in a O(n3 + Mn2) runtimealgorithm.
In practice, the approximation termi-nates after a small number of transformations andwe do not need to bound the number of iterationsin our experiments.We should note that this is one of many possibleapproximations we could have made.
Another rea-sonable approach would be to first find the highestscoring first-order non-projective parse, and thenre-arrange edges based on second order scores ina similar manner to the algorithm we described.We implemented this method and found that theresults were slightly worse.3 Danish: Parsing Secondary ParentsKromann (2001) argued for a dependency formal-ism called Discontinuous Grammar and annotateda large set of Danish sentences using this formal-ism to create the Danish Dependency Treebank(Kromann, 2003).
The formalism allows for a84root Han spejder efter og ser elefanterneHe looks for and sees elephantsFigure 5: An example dependency tree fromthe Danish Dependency Treebank (from Kromann(2003)).word to have multiple parents.
Examples includeverb coordination in which the subject or object isan argument of several verbs, and relative clausesin which words must satisfy dependencies both in-side and outside the clause.
An example is shownin Figure 5 for the sentence He looks for and seeselephants.
Here, the pronoun He is the subject forboth verbs in the sentence, and the noun elephantsthe corresponding object.
In the Danish Depen-dency Treebank, roughly 5% of words have morethan one parent, which breaks the single parent(or tree) constraint we have previously requiredon dependency structures.
Kromann also allowsfor cyclic dependencies, though we deal only withacyclic dependency graphs here.
Though lesscommon than trees, dependency graphs involvingmultiple parents are well established in the litera-ture (Hudson, 1984).
Unfortunately, the problemof finding the dependency structure with highestscore in this setting is intractable (Chickering etal., 1994).To create an approximate parsing algorithmfor dependency structures with multiple parents,we start with our approximate second-order non-projective algorithm outlined in Figure 4.
We usethe non-projective algorithm since the Danish De-pendency Treebank contains a small number ofnon-projective arcs.
We then modify lines 7-10of this algorithm so that it looks for the change inparent or the addition of a new parent that causesthe highest change in overall score and does notcreate a cycle2.
Like before, we make one changeper iteration and that change will depend on theresulting score of the new tree.
Using this sim-ple new approximate parsing algorithm, we train anew parser that can produce multiple parents.4 Online Learning and ApproximateInferenceIn this section, we review the work of McDonaldet al (2005b) for online large-margin dependency2We are not concerned with violating the tree constraint.parsing.
As usual for supervised learning, we as-sume a training set T = {(xt,yt)}Tt=1, consist-ing of pairs of a sentence xt and its correct depen-dency representation yt.The algorithm is an extension of the Margin In-fused Relaxed Algorithm (MIRA) (Crammer andSinger, 2003) to learning with structured outputs,in the present case dependency structures.
Fig-ure 6 gives pseudo-code for the algorithm.
An on-line learning algorithm considers a single traininginstance for each update to the weight vector w.We use the common method of setting the finalweight vector as the average of the weight vec-tors after each iteration (Collins, 2002), which hasbeen shown to alleviate overfitting.On each iteration, the algorithm considers asingle training instance.
We parse this instanceto obtain a predicted dependency graph, and findthe smallest-norm update to the weight vector wthat ensures that the training graph outscores thepredicted graph by a margin proportional to theloss of the predicted graph relative to the traininggraph, which is the number of words with incor-rect parents in the predicted tree (McDonald et al,2005b).
Note that we only impose margin con-straints between the single highest-scoring graphand the correct graph relative to the current weightsetting.
Past work on tree-structured outputs hasused constraints for the k-best scoring tree (Mc-Donald et al, 2005b) or even all possible trees byusing factored representations (Taskar et al, 2004;McDonald et al, 2005c).
However, we have foundthat a single margin constraint per example leadsto much faster training with a negligible degrada-tion in performance.
Furthermore, this formula-tion relates learning directly to inference, which isimportant, since we want the model to set weightsrelative to the errors made by an approximate in-ference algorithm.
This algorithm can thus beviewed as a large-margin version of the perceptronalgorithm for structured outputs Collins (2002).Online learning algorithms have been shownto be robust even with approximate rather thanexact inference in problems such as word align-ment (Moore, 2005), sequence analysis (Daume?and Marcu, 2005; McDonald et al, 2005a)and phrase-structure parsing (Collins and Roark,2004).
This robustness to approximations comesfrom the fact that the online framework setsweights with respect to inference.
In other words,the learning method sees common errors due to85Training data: T = {(xt,yt)}Tt=11.
w(0) = 0; v = 0; i = 02. for n : 1..N3.
for t : 1..T4.
min??
?w(i+1) ?
w(i)???s.t.
s(xt,yt; w(i+1))?s(xt,y?
; w(i+1)) ?
L(yt,y?
)where y?
= arg maxy?
s(xt,y?
; w(i))5. v = v + w(i+1)6. i = i + 17. w = v/(N ?
T )Figure 6: MIRA learning algorithm.
We writes(x,y; w(i)) to mean the score of tree y usingweight vector w(i).approximate inference and adjusts weights to cor-rect for them.
The work of Daume?
and Marcu(2005) formalizes this intuition by presenting anonline learning framework in which parameter up-dates are made directly with respect to errors in theinference algorithm.
We show in the next sectionthat this robustness extends to approximate depen-dency parsing.5 ExperimentsThe score of adjacent edges relies on the defini-tion of a feature representation f(i, k, j).
As notedearlier, this representation subsumes the first-orderrepresentation of McDonald et al (2005b), so wecan incorporate all of their features as well as thenew second-order features we now describe.
Theold first-order features are built from the parentand child words, their POS tags, and the POS tagsof surrounding words and those of words betweenthe child and the parent, as well as the directionand distance from the parent to the child.
Thesecond-order features are built from the followingconjunctions of word and POS identity predicatesxi-pos, xk-pos, xj-posxk-pos, xj-posxk-word, xj-wordxk-word, xj-posxk-pos, xj-wordwhere xi-pos is the part-of-speech of the ith wordin the sentence.
We also include conjunctions be-tween these features and the direction and distancefrom sibling j to sibling k. We determined the use-fulness of these features on the development set,which also helped us find out that features such asthe POS tags of words between the two siblingswould not improve accuracy.
We also ignored fea-EnglishAccuracy Complete1st-order-projective 90.7 36.72nd-order-projective 91.5 42.1Table 1: Dependency parsing results for English.CzechAccuracy Complete1st-order-projective 83.0 30.62nd-order-projective 84.2 33.11st-order-non-projective 84.1 32.22nd-order-non-projective 85.2 35.9Table 2: Dependency parsing results for Czech.tures over triples of words since this would ex-plode the size of the feature space.We evaluate dependencies on per word accu-racy, which is the percentage of words in the sen-tence with the correct parent in the tree, and oncomplete dependency analysis.
In our evaluationwe exclude punctuation for English and include itfor Czech and Danish, which is the standard.5.1 English ResultsTo create data sets for English, we used the Ya-mada and Matsumoto (2003) head rules to ex-tract dependency trees from the WSJ, setting sec-tions 2-21 as training, section 22 for developmentand section 23 for evaluation.
The models relyon part-of-speech tags as input and we used theRatnaparkhi (1996) tagger to provide these forthe development and evaluation set.
These datasets are exclusively projective so we only com-pare the projective parsers using the exact projec-tive parsing algorithms.
The purpose of these ex-periments is to gauge the overall benefit from in-cluding second-order features with exact parsingalgorithms, which can be attained in the projectivesetting.
Results are shown in Table 1.
We can seethat there is clearly an advantage in introducingsecond-order features.
In particular, the completetree metric is improved considerably.5.2 Czech ResultsFor the Czech data, we used the predefined train-ing, development and testing split of the PragueDependency Treebank (Hajic?
et al, 2001), and theautomatically generated POS tags supplied withthe data, which we reduce to the POS tag setfrom Collins et al (1999).
On average, 23% ofthe sentences in the training, development andtest sets have at least one non-projective depen-dency, though, less than 2% of total edges are ac-86DanishPrecision Recall F-measure2nd-order-projective 86.4 81.7 83.92nd-order-non-projective 86.9 82.2 84.42nd-order-non-projective w/ multiple parents 86.2 84.9 85.6Table 3: Dependency parsing results for Danish.tually non-projective.
Results are shown in Ta-ble 2.
McDonald et al (2005c) showed a substan-tial improvement in accuracy by modeling non-projective edges in Czech, shown by the differencebetween two first-order models.
Table 2 showsthat a second-order model provides a compara-ble accuracy boost, even using an approximatenon-projective algorithm.
The second-order non-projective model accuracy of 85.2% is the highestreported accuracy for a single parser for these data.Similar results were obtained by Hall and No?va?k(2005) (85.1% accuracy) who take the best out-put of the Charniak parser extended to Czech andrerank slight variations on this output that intro-duce non-projective edges.
However, this systemrelies on a much slower phrase-structure parseras its base model as well as an auxiliary rerank-ing module.
Indeed, our second-order projectiveparser analyzes the test set in 16m32s, and thenon-projective approximate parser needs 17m03sto parse the entire evaluation set, showing that run-time for the approximation is completely domi-nated by the initial call to the second-order pro-jective algorithm and that the post-process edgetransformation loop typically only iterates a fewtimes per sentence.5.3 Danish ResultsFor our experiments we used the Danish Depen-dency Treebank v1.0.
The treebank contains asmall number of inter-sentence and cyclic depen-dencies and we removed all sentences that con-tained such structures.
The resulting data set con-tained 5384 sentences.
We partitioned the datainto contiguous 80/20 training/testing splits.
Weheld out a subset of the training data for develop-ment purposes.We compared three systems, the standardsecond-order projective and non-projective pars-ing models, as well as our modified second-ordernon-projective model that allows for the introduc-tion of multiple parents (Section 3).
All systemsuse gold-standard part-of-speech since no trainedtagger is readily available for Danish.
Results areshown in Figure 3.
As might be expected, the non-projective parser does slightly better than the pro-jective parser because around 1% of the edges arenon-projective.
Since each word may have an ar-bitrary number of parents, we must use precisionand recall rather than accuracy to measure perfor-mance.
This also means that the correct trainingloss is no longer the Hamming loss.
Instead, weuse false positives plus false negatives over edgedecisions, which balances precision and recall asour ultimate performance metric.As expected, for the basic projective and non-projective parsers, recall is roughly 5% lower thanprecision since these models can only pick up atmost one parent per word.
For the parser that canintroduce multiple parents, we see an increase inrecall of nearly 3% absolute with a slight drop inprecision.
These results are very promising andfurther show the robustness of discriminative on-line learning with approximate parsing algorithms.6 DiscussionWe described approximate dependency parsing al-gorithms that support higher-order features andmultiple parents.
We showed that these approxi-mations can be combined with online learning toachieve fast parsing with competitive parsing ac-curacy.
These results show that the gain from al-lowing richer representations outweighs the lossfrom approximate parsing and further shows therobustness of online learning algorithms with ap-proximate inference.The approximations we have presented are verysimple.
They start with a reasonably good baselineand make small transformations until the scoreof the structure converges.
These approximationswork because freer-word order languages we stud-ied are still primarily projective, making the ap-proximate starting point close to the goal parse.However, we would like to investigate the benefitsfor parsing of more principled approaches to ap-proximate learning and inference techniques suchas the learning as search optimization frameworkof (Daume?
and Marcu, 2005).
This frameworkwill possibly allow us to include effectively moreglobal features over the dependency structure than87those in our current second-order model.AcknowledgmentsThis work was supported by NSF ITR grants0205448.ReferencesE.
Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proc.
NAACL.D.M.
Chickering, D. Geiger, and D. Heckerman.
1994.Learning bayesian networks: The combination ofknowledge and statistical data.
Technical ReportMSR-TR-94-09, Microsoft Research.Y.J.
Chu and T.H.
Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.M.
Collins and B. Roark.
2004.
Incremental parsingwith the perceptron algorithm.
In Proc.
ACL.M.
Collins, J.
Hajic?, L. Ramshaw, and C. Tillmann.1999.
A statistical parser for Czech.
In Proc.
ACL.M.
Collins.
2002.
Discriminative training methodsfor hidden Markov models: Theory and experimentswith perceptron algorithms.
In Proc.
EMNLP.K.
Crammer and Y.
Singer.
2003.
Ultraconservativeonline algorithms for multiclass problems.
JMLR.H.
Daum?e and D. Marcu.
2005.
Learning as search op-timization: Approximate large margin methods forstructured prediction.
In Proc.
ICML.J.
Edmonds.
1967.
Optimum branchings.
Journalof Research of the National Bureau of Standards,71B:233?240.J.
Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proc.
COL-ING.J.
Hajic?, E. Hajicova, P. Pajas, J. Panevova, P. Sgall, andB.
Vidova Hladka.
2001.
The Prague DependencyTreebank 1.0 CDROM.
Linguistics Data Consor-tium Cat.
No.
LDC2001T10.K.
Hall and V. N?ov?ak.
2005.
Corrective modeling fornon-projective dependency parsing.
In Proc.
IWPT.R.
Hudson.
1984.
Word Grammar.
Blackwell.M.
T. Kromann.
2001.
Optimaility parsing and localcost functions in discontinuous grammars.
In Proc.FG-MOL.M.
T. Kromann.
2003.
The danish dependency tree-bank and the dtag treebank tool.
In Proc.
TLT.R.
McDonald, K. Crammer, and F. Pereira.
2005a.Flexible text segmentation with structured multil-abel classifi cation.
In Proc.
HLT-EMNLP.R.
McDonald, K. Crammer, and F. Pereira.
2005b.
On-line large-margin training of dependency parsers.
InProc.
ACL.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.2005c.
Non-projective dependency parsing usingspanning tree algorithms.
In Proc.
HLT-EMNLP.I.A.
Me?lc?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press.R.
Moore.
2005.
A discriminative framework for bilin-gual word alignment.
In Proc.
HLT-EMNLP.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective de-pendency parsing.
In Proc.
ACL.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proc.
EMNLP.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proc.
EMNLP.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InProc.
IWPT.A 2nd-Order Non-projective MSTParsing is NP-hardProof by a reduction from 3-D matching (3DM).3DM: Disjoint sets X,Y,Z each withm distinct elementsand a set T ?
X?Y ?Z.
Question: is there a subset S ?
Tsuch that |S| = m and each v ?
X?Y ?Z occurs in exactlyone element of S.Reduction: Given an instance of 3DM we defi ne a graphin which the vertices are the elements from X ?
Y ?
Z aswell as an artifi cial root node.
We insert edges from root toall xi ?
X as well as edges from all xi ?
X to all yi ?
Yand zi ?
Z.
We order the words s.t.
the root is on the leftfollowed by all elements of X, then Y , and fi nally Z. Wethen defi ne the second-order score function as follows,s(root, xi, xj) = 0, ?xi, xj ?
Xs(xi,?, yj) = 0, ?xi ?
X, yj ?
Ys(xi, yj , zk) = 1, ?
(xi, yj , zk) ?
TAll other scores are defi ned to be ?
?, including for edgespairs that were not defi ned in the original graph.Theorem: There is a 3D matching iff the second-orderMST has a score of m. Proof: First we observe that no treecan have a score greater thanm since that would require morethan m pairs of edges of the form (xi, yj , zk).
This can onlyhappen when some xi has multiple yj ?
Y children or mul-tiple zk ?
Z children.
But if this were true then we wouldintroduce a??
scored edge pair (e.g.
s(xi, yj , y?j)).
Now, ifthe highest scoring second-order MST has a score of m, thatmeans that every xi must have found a unique pair of chil-dren yj and zk which represents the 3D matching, since therewould be m such triples.
Furthermore, yj and zk could notmatch with any other x?i since they can only have one incom-ing edge in the tree.
On the other hand, if there is a 3DM, thenthere must be a tree of weight m consisting of second-orderedges (xi, yj , zk) for each element of the matching S. Sinceno tree can have a weight greater than m, this must be thehighest scoring second-order MST.
Thus if we can fi nd thehighest scoring second-order MST in polynomial time, then3DM would also be solvable.
88
