Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 742?751,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLexically-Triggered Hidden Markov Modelsfor Clinical Document CodingSvetlana Kiritchenko Colin CherryInstitute for Information TechnologyNational Research Council Canada{Svetlana.Kiritchenko,Colin.Cherry}@nrc-cnrc.gc.caAbstractThe automatic coding of clinical documentsis an important task for today?s healthcareproviders.
Though it can be viewed asmulti-label document classification, the cod-ing problem has the interesting property thatmost code assignments can be supported bya single phrase found in the input docu-ment.
We propose a Lexically-Triggered Hid-den Markov Model (LT-HMM) that leveragesthese phrases to improve coding accuracy.
TheLT-HMM works in two stages: first, a lexicalmatch is performed against a term dictionaryto collect a set of candidate codes for a docu-ment.
Next, a discriminative HMM selects thebest subset of codes to assign to the documentby tagging candidates as present or absent.By confirming codes proposed by a dictio-nary, the LT-HMM can share features acrosscodes, enabling strong performance even onrare codes.
In fact, we are able to recovercodes that do not occur in the training set atall.
Our approach achieves the best ever per-formance on the 2007Medical NLP Challengetest set, with an F-measure of 89.84.1 IntroductionThe clinical domain presents a number of interestingchallenges for natural language processing.
Con-ventionally, most clinical documentation, such asdoctor?s notes, discharge summaries and referrals,are written in a free-text form.
This narrative formis flexible, allowing healthcare professionals to ex-press any kind of concept or event, but it is notparticularly suited for large-scale analysis, search,or decision support.
Converting clinical narrativesinto a structured form would support essential activi-ties such as administrative reporting, quality control,biosurveillance and biomedical research (Meystreet al, 2008).
One way of representing a docu-ment is to code the patient?s conditions and the per-formed procedures into a nomenclature of clinicalcodes.
The International Classification of Diseases,9th and 10th revisions, Clinical Modification (ICD-9-CM, ICD-10-CM) are the official administrativecoding schemes for healthcare organizations in sev-eral countries, including the US and Canada.
Typi-cally, coding is performed by trained coding profes-sionals, but this process can be both costly and error-prone.
Automated methods can speed-up the cod-ing process, improve the accuracy and consistencyof internal documentation, and even result in higherreimbursement for the healthcare organization (Ben-son, 2006).Traditionally, statistical document coding isviewed as multi-class multi-label document classifi-cation, where each clinical free-text document is la-belled with one or several codes from a pre-defined,possibly very large set of codes (Patrick et al, 2007;Suominen et al, 2008).
One classification model islearned for each code, and then all models are ap-plied in turn to a new document to determine whichcodes should be assigned to the document.
Thedrawback of this approach is poor predictive perfor-mance on low-frequency codes, which are ubiqui-tous in the clinical domain.This paper presents a novel approach to documentcoding that simultaneously models code-specific aswell as general patterns in the data.
This allows742us to predict any code label, even codes for whichno training data is available.
Our approach, thelexically-triggered HMM (LT-HMM), is based onthe fact that a code assignment is often indicatedby short lexical triggers in the text.
Consequently,a two-stage coding method is proposed.
First, theLT-HMM identifies candidate codes by matchingterms from a medical terminology dictionary.
Then,it confirms or rejects each of the candidates by ap-plying a discriminative sequence model.
In this ar-chitecture, low-frequency codes can still be matchedand confirmed using general characteristics of theirtrigger?s local context, leading to better predictionperformance on these codes.2 Document Coding and Lexical TriggersDocument coding is a special case of multi-classmulti-label text classification.
Given a fixed set ofpossible codes, the ultimate goal is to assign a set ofcodes to documents, based on their content.
Further-more, we observe that for each code assigned to adocument, there is generally at least one correspond-ing trigger term in the text that accounts for thecode?s assignment.
For example, if an ICD-9-CMcoding professional were to see ?allergic bronchitis?somewhere in a clinical narrative, he or she wouldimmediately consider adding code 493.9 (Asthma,unspecified) to the document?s code set.
The pres-ence of these trigger terms separates document cod-ing from text classification tasks, such as topic orgenre classification, where evidence for a particularlabel is built up throughout a document.
However,this does not make document coding a term recogni-tion task, concerned only with the detection of trig-gers.
Codes are assigned to a document as a whole,and code assignment decisions within a documentmay interact.
It is an interesting combination of sen-tence and document-level processing.Formally, we define the document coding taskas follows: given a set of documents X and a setof available codes C, assign to each document xia subset of codes Ci ?
C. We also assume ac-cess to a (noisy) mechanism to detect candidate trig-gers in a document.
In particular, we will assumethat an (incomplete) dictionary D(c) exists for eachcode c ?
C, which lists specific code terms asso-ciated with c.1 To continue our running example:D(493.9) would include the term ?allergic bron-chitis?.
Each code can have several correspondingterms while each term indicates the presence of ex-actly one code.
A candidate code c is proposed eachtime a term from D(c) is found in a document.2.1 From triggers to codesThe presence of a term from D(c) does not automat-ically imply the assignment of code c to a document.Even with extremely precise dictionaries, there arethree main reasons why a candidate code may notappear in a document?s code subset.1.
The context of the trigger term might indicatethe irrelevancy of the code.
In the clinical do-main, such irrelevancy can be specified by anegative or speculative statement (e.g., ?evalu-ate for pneumonia?)
or a family-related context(e.g., ?family history of diabetes?).
Only defi-nite diagnosis of the patient should be coded.2.
There can be several closely related candidatecodes; yet only one, the best fitted code shouldbe assigned to the document.
For example, thetriggers ?left-sided flank pain?
(code 789.09)and ?abdominal pain?
(code 789.00) may bothappear in the same clinical report, but only themost specific code, 789.09, should end up inthe document code set.3.
The domain can have code dependency rules.For example, the ICD-9-CM coding rules statethat no symptom codes should be given toa document if a definite diagnosis is present.That is, if a document is coded with pneumo-nia, it should not be coded with a fever orcough.
On the other hand, if the diagnosis isuncertain, then codes for the symptoms shouldbe assigned.This suggests a paradigm where a candidate code,suggested by a detected trigger term, is assessedin terms of both its local context (item 1) and thepresence of other candidate codes for the document(items 2 and 3).1Note that dictionary-based trigger detection could be re-placed by tagging approaches similar to those used in named-entity-recognition or information extraction.7432.2 ICD-9-CM CodingAs a specific application we have chosen the taskof assigning ICD-9-CM codes to free-form clinicalnarratives.
We use the dataset collected for the 2007Medical NLP Challenge organized by the Compu-tational Medicine Center in Cincinnati, Ohio, here-after refereed to as ?CMC Challenge?
(Pestian et al,2007).
For this challenge, 1954 radiology reportson outpatient chest x-ray and renal procedures werecollected, disambiguated, and anonymized.
The re-ports were annotated with ICD-9-CM codes by threecoding companies, and the majority codes were se-lected as a gold standard.
In total, 45 distinct codeswere used.For this task, our use of a dictionary to detect lex-ical triggers is quite reasonable.
The medical do-main is rich with manually-created and carefully-maintained knowledge resources.
In particular, theICD-9-CM coding guidelines come with an indexfile that contains hundreds of thousands of termsmapped to corresponding codes.
Another valuableresource is Metathesaurus from the Unified MedicalLanguage System (UMLS) (Lindberg et al, 1993).It has millions of terms related to medical problems,procedures, treatments, organizations, etc.
Often,hospitals, clinics, and other healthcare organizationsmaintain their own vocabularies to introduce con-sistency in their internal and external documenta-tion and to support reporting, reviewing, and meta-analysis.This task has some very challenging properties.As mentioned above, the ICD-9-CM coding rulescreate strong code dependencies: codes are assignedto a document as a set and not individually.
Fur-thermore, the code distribution throughout the CMCtraining documents has a very heavy tail; that is,there are a few heavily-used codes and a largenumber of codes that are used only occasionally.An ideal approach will work well with both high-frequency and low-frequency codes.3 Related workAutomated clinical coding has received much atten-tion in the medical informatics literature.
Stanfill etal.
reviewed 113 studies on automated coding pub-lished in the last 40 years (Stanfill et al, 2010).
Theauthors conclude that there exists a variety of toolscovering different purposes, healthcare specialties,and clinical document types; however, these toolsare not generalizable and neither are their evaluationresults.
One major obstacle that hinders the progressin this domain is data privacy issues.
To overcomethis obstacle, the CMC Challenge was organized in2007.
The purpose of the challenge was to providea common realistic dataset to stimulate the researchin the area and to assess the current level of perfor-mance on the task.
Forty-four teams participated inthe challenge.
The top-performing system achievedmicro-averaged F1-score of 0.8908, and the meanscore was 0.7670.Several teams, including the winner, built puresymbolic (i.e., hand-crafted rule-based) systems(e.g., (Goldstein et al, 2007)).
This approach is fea-sible for the small code set used in the challenge,but it is questionable in real-life settings where thou-sands of codes need to be considered.
Later, thewinning team showed how their hand-crafted rulescan be built in a semi-automatic way: the initial setof rules adopted from the official coding guidelineswere automatically extended with additional syn-onyms and code dependency rules generated fromthe training data (Farkas and Szarvas, 2008).Statistical systems trained on only text-derivedfeatures (such as n-grams) did not show good per-formance due to a wide variety of medical languageand a relatively small training set (Goldstein et al,2007).
This led to the creation of hybrid systems:symbolic and statistical classifiers used together inan ensemble or cascade (Aronson et al, 2007; Cram-mer et al, 2007) or a symbolic component provid-ing features for a statistical component (Patrick etal., 2007; Suominen et al, 2008).
Strong competi-tion systems had good answers for dealing with neg-ative and speculative contexts, taking advantage ofthe competition?s limited set of possible code com-binations, and handling of low-frequency codes.Our proposed approach is a combination systemas well.
We combine a symbolic component thatmatches lexical strings of a document against a med-ical dictionary to determine possible codes (Lussieret al, 2000; Kevers and Medori, 2010) and a sta-tistical component that finalizes the assignment ofcodes to the document.
Our statistical componentis similar to that of Crammer et al (2007), in thatwe train a single model for all codes with code-744specific and generic features.
However, Crammeret al (2007) did not employ our lexical trigger stepor our sequence-modeling formulation.
In fact, theyconsidered all possible code subsets, which can beinfeasible in real-life settings.4 MethodTo address the task of document coding, ourlexically-triggered HMM operates using a two-stageprocedure:1.
Lexically match text to the dictionary to get aset of candidate codes;2.
Using features derived from the candidates andthe document, select the best code subset.In the first stage, dictionary terms are detected in thedocument using exact string matching.
All codescorresponding to matches become candidate codes,and no other codes can be proposed for this docu-ment.In the second stage, a single classifier is trained toselect the best code subset from the matched candi-dates.
By training a single classifier, we use all ofthe training data to assign binary labels (present orabsent) to candidates.
This is the key distinction ofour method from the traditional statistical approachwhere a separate classifier is trained for each code.The LT-HMM allows features learned from a doc-ument coded with ci to transfer at test time to pre-dict code cj , provided their respective triggers ap-pear in similar contexts.
Training one common clas-sifier improves our chances to reliably predict codesthat have few training instances, and even codes thatdo not appear at all in the training data.4.1 Trigger DetectionWe have manually assembled a dictionary of termsfor each of the 45 codes used in the CMC chal-lenge.2 The dictionaries were built by collecting rel-evant medical terminology from UMLS, the ICD-9-CM coding guidelines, and the CMC training data.The test data was not consulted during dictionaryconstruction.
The dictionaries contain 440 terms,with 9.78 terms per code on average.
Given thesedictionaries, the exact-matching of terms to input2Online at https://sites.google.com/site/colinacherry/ICD9CM ACL11.txtdocuments is straightforward.
In our experiments,this process finds on average 1.83 distinct candidatecodes per document.The quality of the dictionary significantly affectsthe prediction performance of the proposed two-stage approach.
Especially important is the cover-age of the dictionary.
If a trigger term is missingfrom the dictionary and, as the result, the code is notselected as a candidate code, it will not be recov-ered in the following stage, resulting in a false neg-ative.
Preliminary experiments show that our dictio-nary recovers 94.42% of the codes in the training setand 93.20% in the test set.
These numbers providean upper bound on recall for the overall approach.4.2 Sequence ConstructionAfter trigger detection, we view the input documentas a sequence of candidate codes, each correspond-ing to a detected trigger (see Figure 1).
By taggingthese candidates in sequence, we can label each can-didate code as present or absent and use previoustagging decisions to model code interactions.
Thefinal code subset is constructed by collecting all can-didate codes tagged as present.Our training data consists of [document, code set]pairs, augmented with the trigger terms detectedthrough dictionary matching.
We transform this intoa sequence to be tagged using the following steps:Ordering: The candidate code sequence is pre-sented in reverse chronological order, according towhen their corresponding trigger terms appear in thedocument.
That is, the last candidate to be detectedby the dictionary will be the first code to appear inour candidate sequence.
Reverse order was chosenbecause clinical documents often close with a final(and informative) diagnosis.Merging: Each detected trigger corresponds toexactly one code; however, several triggers may bedetected for the same code throughout a document.If a code has several triggers, we keep only the lastoccurrence.
When possible, we collect relevant fea-tures (such as negation information) of all occur-rences and associate them with this last occurrence.Labelling: Each candidate code is assigned a bi-nary label (present or absent) based on whether itappears in the gold-standard code set.
Note that this745Cough,?fever?in?9-??year-??old?male.?IMPRESSION:?1.?Right?middle?lobe?pneumonia.?2.?Minimal?pleural?thickening?on?the?right?may?represent?small?pleural?effusion.??486?pneumonia?context=pos?sem=disease??N?Y?
N?N?511.9?pleural?effusion?context=neg?sem=disease??780.6?fever?context=pos?sem=symptom??786.2?cough?context=pos?sem=symptom??Gold?code?set:?
{486}?Figure 1: An example document and its corresponding gold-standard tag sequence.
The top binary layer is the correctoutput tag sequence, which confirms or rejects the presence of candidate codes.
The bottom layer shows the candidatecode sequence derived from the text, with corresponding trigger phrases and some prominent features.process can not introduce gold-standard codes thatwere not proposed by the dictionary.The final output of these steps is depicted in Fig-ure 1.
To the left, we have an input text with un-derlined trigger phrases, as detected by our dictio-nary.
This implies an input sequence (bottom right),which consists of detected codes and their corre-sponding trigger phrases.
The gold-standard codeset for the document is used to infer a gold-standardlabel sequence for these codes (top right).
At testtime, the goal of the classifier is to correctly predictthe correct binary label sequence for new inputs.
Wediscuss the construction of the features used to makethis prediction in section 4.3.4.3 ModelWe model this sequence data using a discriminativeSVM-HMM (Taskar et al, 2003; Altun et al, 2003).This allows us to use rich, over-lapping features ofthe input while also modeling interactions betweenlabels.
A discriminative HMM has two major cate-gories of features: emission features, which charac-terize a candidate?s tag in terms of the input docu-ment x, and transition features, which characterizea tag in terms of the tags that have come before it.We describe these two feature categories and thenour training mechanism.
All feature engineering dis-cussed below was carried out using 10-fold cross-validation on the training set.Transition FeaturesThe transition features are modeled as simple in-dicators over n-grams of present codes, for values ofn up to 10, the largest number of codes proposed byour dictionary in the training set.3 This allows thesystem to learn sequences of codes that are (and arenot) likely to occur in the gold-standard data.We found it useful to pad our n-grams with ?be-ginning of document?
tokens for sequences whenfewer than n codes have been labelled as present,but found it harmful to include an end-of-documenttag once labelling is complete.
We suspect that thesmall training set for the challenge makes the systemprone to over-fit when modeling code-set length.Emission FeaturesThe vast majority of our training signal comesfrom emission features, which carefully model boththe trigger term?s local context and the document asa whole.
For each candidate code, three types offeatures are generated: document features, ConTextfeatures, and code-semantics features (Table 1).Document: Document features include indicatorson all individual words, 2-grams, 3-grams, and 4-grams found in the document.
These n-gram fea-tures have the candidate code appended to them,making them similar to features traditionally usedin multiclass document categorization.ConText: We take advantage of the ConText algo-rithm?s output.
ConText is publicly available soft-ware that determines the presence of negated, hypo-thetical, historical, and family-related context for agiven phrase in a clinical text (Harkema et al, 2009).3We can easily afford such a long history because input se-quences are generally short and the tagging is binary, resultingin only a small number of possible histories for a document.746Features gen. spec.Documentn-gram xConTextcurrent matchcontext x xonly in context x xmore than once in context x xother matchespresent x xpresent in context = pos x xcode present in context x xCode Semanticscurrent matchsem type xother matchessem type, context = pos x xTable 1: The emission features used in LT-HMM.Typeset words represent variables replaced with spe-cific values, i.e.
context ?
{pos,neg}, sem type ?
{symptom,disease}, code is one of 45 challenge codes,n-gram is a document n-gram.
Features can come ingeneric and/or code-specific version.The algorithm is based on regular expression match-ing of the context to a precompiled list of contextindicators.
Regardless of its simplicity, the algo-rithm has shown very good performance on a vari-ety of clinical document types.
We run ConText foreach trigger term located in the text and produce twotypes of features: features related to the candidatecode in question and features related to other candi-date codes of the document.
Negated, hypothetical,and family-related contexts are clustered into a sin-gle negative context for the term.
Absence of thenegative context implies the positive context.We used the following ConText derived indicatorfeatures: for the current candidate code, if there is atleast one trigger term found in a positive (negative)context, if all trigger terms for this code are foundin a positive (negative) context, if there are morethan one trigger terms for the code found in a posi-tive (negative) context; for other candidate codes ofthe document, if there is at least one other candidatecode, if there is another candidate code with at leastone trigger term found in a positive context, if thereis a trigger term for candidate code ci found in a pos-itive (negative) context.Code Semantics: We include features that indi-cate if the code itself corresponds to a disease or asymptom.
This assignment was determined basedon the UMLS semantic type of the code.
Like theConText features, code features come in two types:those regarding the candidate code in question andthose regarding other candidate codes from the samedocument.Generic versus Specific: Most of our featurescome in two versions: generic and code-specific.Generic features are concerned with classifying anycandidate as present or absent based on characteris-tics of its trigger or semantics.
Code-specific fea-tures append the candidate code to the feature.
Forexample, the feature context=pos represents thatthe current candidate has a trigger term in a positivecontext, while context=pos:486 adds the infor-mation that the code in question is 486.
Note thatn-grams features are only code-specific, as they arenot connected to any specific trigger term.To an extent, code-specific features allow usto replicate the traditional classification approach,which focuses on one code at a time.
Using thesefeatures, the classifier is free to build complex sub-models for a particular code, provided that this codehas enough training examples.
Generic versions ofthe features, on the other hand, make it possible tolearn common rules applicable to all codes, includ-ing low-frequency ones.
In this way, even in the ex-treme case of having zero training examples for aparticular code, the model can still potentially assignthe code to new documents, provided it is detectedby our dictionary.
This is impossible in a traditionaldocument-classification setting.TrainingWe train our SVM-HMM with the objective ofseparating the correct tag sequence from all othersby a fixed margin of 1, using a primal stochasticgradient optimization algorithm that follows Shalev-Shwartz et al (2007).
Let S be a set of trainingpoints (x, y), where x is the input and y is the cor-responding gold-standard tag sequence.
Let ?
(x, y)be a function that transforms complete input-outputpairs into feature vectors.
We also use ?
(x, y?, y)as shorthand for the difference in features between747beginInput: S, ?, nInitialize: Set w0 to the 0 vectorfor t = 1, 2 .
.
.
, n|S|Choose (x, y) ?
S at randomSet the learning rate: ?t = 1?tSearch:y?
= argmaxy??
[?
(y, y??)
+ wt ?
?
(x, y??
)]Update:wt+1 = wt + ?t(?
(x, y, y?)
?
?wt)Adjust:wt+1 = wt+1 ?
min[1, 1/???wt+1?
]endOutput: wn|S|+1endFigure 2: Training an SVM-HMMtwo outputs: ?
(x, y?, y) = ?
(x, y?)
?
?
(x, y).
Withthis notation in place, the SVM-HMM minimizesthe regularized hinge-loss:minw?2w2 +1|S|?
(x,y)?S`(w; (x, y)) (1)where`(w; (x, y)) = maxy?[?
(y, y?)
+ w ?
?
(x, y?, y)](2)and where ?
(y, y?)
= 0 when y = y?
and 1 oth-erwise.4 Intuitively, the objective attempts to finda small weight vector w that separates all incorrecttag sequences y?
from the correct tag sequence y bya margin of 1. ?
controls the trade-off between reg-ularization and training hinge-loss.The stochastic gradient descent algorithm usedto optimize this objective is shown in Figure 2.
Itbears many similarities to perceptron HMM train-ing (Collins, 2002), with theoretically-motivated al-terations, such as selecting training points at ran-dom5 and the explicit inclusion of a learning rate ?4We did not experiment with structured versions of ?
thataccount for the number of incorrect tags in the label sequencey?, as a fixed margin was already working very well.
We intendto explore structured costs in future work.5Like many implementations, we make n passes through S,shuffling S before each pass, rather than sampling from S withreplacement n|S| times.training test# of documents 978 976# of distinct codes 45 45# of distinct code subsets 94 94# of codes with < 10 ex.
24 24avg # of codes per document 1.25 1.23Table 2: The training and test set characteristics.and a regularization term ?.
The search step can becarried out with a two-best version of the Viterbi al-gorithm; if the one-best answer y?1 matches the gold-standard y, that is ?
(y, y?1) = 0, then y?2 is checkedto see if its loss is higher.We tune two hyper-parameters using 10-foldcross-validation: the regularization parameter ?
anda number of passes n through the training data.
Us-ing F1 as measured by 10-fold cross-validation onthe training set, we found values of ?
= 0.1 withn = 5 to prove optimal.
Training time is less thanone minute on modern hardware.5 Experiments5.1 DataFor testing purposes, we use the CMC Challengedataset.
The data consists of 978 training and 976test medical records labelled with one or more ICD-9-CM codes from a set of 45 codes.
The data statis-tics are presented in Table 2.
The training and testsets have similar, very imbalanced distributions ofcodes.
In particular, all codes in the test set have atleast one training example.
Moreover, for any codesubset assigned to a test document there is at leastone training document labelled with the same codesubset.
Notably, more than half of the codes haveless than 10 instances in both training and test sets.Following the challenge?s protocol, we use micro-averaged F1-measure for evaluation.5.2 BaselineAs the first baseline for comparison, we built aone-classifier-per-code statistical system.
A docu-ment?s code subset is implied by the set of classi-fiers that assign it a positive label.
The classifiersuse a feature set designed to mimic our LT-HMMas closely as possible, including n-grams, dictionarymatches, ConText output, and symptom/disease se-748mantic types.
Each classifier is trained as an SVMwith a linear kernel.Unlike our approach, this baseline cannot sharefeatures across codes, and it does not allow codingdecisions for a document to inform one another.
Italso cannot propose codes that have not been seen inthe training data as it has no model for these codes.However, one should note that it is a very strongbaseline.
Like our proposed system, it is built withmany features derived from dictionary matches andtheir contexts, and thus it shares many of our sys-tem?s strengths.
In fact, this baseline system outper-forms all published statistical approaches tested onthe CMC data.Our second baseline is a symbolic system, de-signed to evaluate the quality of our rule-based com-ponents when used alone.
It is based on the samehand-crafted dictionary, filtered according to theConText algorithm and four code dependency rulesfrom (Farkas and Szarvas, 2008).
These rules ad-dress the problem of overcoding: some symptomcodes should be omitted when a specific diseasecode is present.6This symbolic system has access to the samehand-crafted resources as our LT-HMM and, there-fore, has a good chance of predicting low-frequencyand unseen codes.
However, it lacks the flexibility ofour statistical solution to accept or reject code candi-dates based on the whole document text, which pre-vents it from compensating for dictionary or Con-Text errors.
Similarly, the structure of the code de-pendency rules may not provide the same flexibilityas our features that look at other detected triggersand previous code assignments.5.3 Coding AccuracyWe evaluate the proposed approach on both thetraining set (using 10-fold cross-validation) and thetest set (Table 3).
The experiments demonstrate thesuperiority of the proposed LT-HMM approach overthe one-per-code statistical scheme as well as oursymbolic baseline.
Furthermore, the new approachshows the best results ever achieved on the dataset,beating the top-performing system in the challenge,a symbolic method.6Note that we do not match the performance of the Farkasand Szarvas system, likely due to our use of a different (andsimpler) dictionary.Cross-fold TestSymbolic baseline N/A 85.96Statistical baseline 87.39 88.26LT-HMM 89.39 89.84CMC Best N/A 89.08Table 3: Micro-averaged F1-scores for statistical andsymbolic baselines, the proposed LT-HMM approach,and the best CMC hand-crafted rule-based system.System Prec.
Rec.
F1Full 90.91 88.80 89.84-ConText 88.54 85.89 87.19-Document 89.89 88.55 89.21-Code Semantics 90.10 88.38 89.23-Append code-specific 88.96 88.30 88.63-Transition 90.79 88.38 89.57-ConText & Transition 86.91 85.39 86.14Table 4: Results on the CMC test data with each majorcomponent removed.5.4 AblationOur system employs a number of emission featuretemplates.
We measure the impact of each by re-moving the template, re-training, and testing on thechallenge test data, as shown in Table 4.
By far themost important component of our system is the out-put of the ConText algorithm.We also tested a version of the system that doesnot create a parallel code-specific feature set by ap-pending the candidate code to emission features.This system tags code-candidates without any code-specific components, but it still does very well, out-performing the baselines.Removing the sequence-based transition featuresfrom our system has only a small impact on accu-racy.
This is because several of our emission fea-tures look at features of other candidate codes.
Thisprovides a strong approximation to the actual tag-ging decisions for these candidates.
If we removethe ConText features, the HMM?s transition featuresbecome more important (compare line 2 of Table 4to line 7).5.5 Low-frequency codesAs one can see from Table 2, more than half of theavailable codes appear fewer than 10 times in the749System Prec.
Rec.
F1Symbolic baseline 42.53 56.06 48.37Statistical baseline 73.33 33.33 45.83LT-HMM 70.00 53.03 60.34Table 5: Results on the CMC test set, looking only at thecodes with fewer than 10 examples in the training set.System Prec.
Rec.
F1Symbolic baseline 60.00 80.00 68.57All training data 72.92 74.47 73.68One code held out 79.31 48.94 60.53Table 6: Results on the CMC test set when all instancesof a low-frequency code are held-out during training.training documents.
This does not provide muchtraining data for a one-classifier-per-code approach,which has been a major motivating factor in the de-sign of our LT-HMM.
In Table 5, we compare oursystem to the baselines on the CMC test set, con-sidering only these low-frequency codes.
We showa 15-point gain in F1 over the statistical baselineon these hard cases, brought on by an substantialincrease in recall.
Similarly, we improve over thesymbolic baseline, due to a much higher precision.In this way, the LT-HMM captures the strengths ofboth approaches.Our system also has the ability to predict codesthat have not been seen during training, by labellinga dictionary match for a code as present according toits local context.
We simulate this setting by drop-ping training data.
For each low-frequency code c,we hold out all training documents that include c intheir gold-standard code set.
We then train our sys-tem on the reduced training set and measure its abil-ity to detect c on the unseen test data.
11 of the 24low-frequency codes have no dictionary matches inour test data; we omit them from our analysis as weare unable to predict them.
The micro-averaged re-sults for the remaining 13 low-frequency codes areshown in Table 6, with the results from the symbolicbaseline and from our system trained on the com-plete training data provided for comparison.We were able to recover 49% of the test-time oc-currences of codes withheld from training, whilemaintaining our full system?s precision.
Consider-ing that traditional statistical strategies would leadto recall dropping uniformly to 0, this is a vast im-provement.
However, the symbolic baseline recalls80% of occurrences in aggregate, indicating that weare not yet making optimal use of the dictionary forcases when a code is missing from the training data.By holding out only correct occurrences of a codec, our system becomes biased against it: all triggerterms for c that are found in the training data mustbe labelled absent.
Nonetheless, out of the 13 codeswith dictionary matches, there were 9 codes that wewere able to recall at a rate of 50% or more, and 5codes that achieved 100% recall.6 ConclusionWe have presented the lexically-triggered HMM, anovel and effective approach for clinical documentcoding.
The LT-HMM takes advantage of lexicaltriggers for clinical codes by operating in two stages:first, a lexical match is performed against a triggerterm dictionary to collect a set of candidates codesfor a document; next, a discriminative HMM se-lects the best subset of codes to assign to the docu-ment.
Using both generic and code-specific features,the LT-HMM outperforms a traditional one-per-code statistical classification method, with substan-tial improvements on low-frequency codes.
Also,it achieves the best ever performance on a commontestbed, beating the top-performer of the 2007 CMCChallenge, a hand-crafted rule-based system.
Fi-nally, we have demonstrated that the LT-HMM cancorrectly predict codes never seen in the training set,a vital characteristic missing from previous statisti-cal methods.In the future, we would like to augment ourdictionary-based matching component with entity-recognition technology.
It would be interesting tomodel triggers as latent variables in the documentcoding process, in a manner similar to how latentsubjective sentences have been used in document-level sentiment analysis (Yessenalina et al, 2010).This would allow us to employ a learned matchingcomponent that is trained to compliment our classi-fication component.AcknowledgementsMany thanks to Berry de Bruijn, Joel Martin, andthe ACL-HLT reviewers for their helpful comments.750ReferencesY.
Altun, I. Tsochantaridis, and T. Hofmann.
2003.
Hid-den Markov support vector machines.
In ICML.A.
R. Aronson, O. Bodenreider, D. Demner-Fushman,K.
W. Fung, V. K. Lee, J. G. Mork, A. Nvol, L. Peters,and W. J. Rogers.
2007.
From indexing the biomed-ical literature to coding clinical text: Experience withMTI and machine learning approaches.
In BioNLP,pages 105?112.S.
Benson.
2006.
Computer assisted coding softwareimproves documentation, coding, compliance and rev-enue.
Perspectives in Health Information Manage-ment, CAC Proceedings, Fall.M.
Collins.
2002.
Discriminative training methods forHidden Markov Models: Theory and experiments withperceptron algorithms.
In EMNLP.K.
Crammer, M. Dredze, K. Ganchev, P. P. Talukdar, andS.
Carroll.
2007.
Automatic code assignment to med-ical text.
In BioNLP, pages 129?136.R.
Farkas and G. Szarvas.
2008.
Automatic construc-tion of rule-based ICD-9-CM coding systems.
BMCBioinformatics, 9(Suppl 3):S10.I.
Goldstein, A. Arzumtsyan, and Uzuner.
2007.
Threeapproaches to automatic assignment of ICD-9-CMcodes to radiology reports.
In AMIA, pages 279?283.H.
Harkema, J. N. Dowling, T. Thornblade, and W. W.Chapman.
2009.
Context: An algorithm for determin-ing negation, experiencer, and temporal status fromclinical reports.
Journal of Biomedical Informatics,42(5):839?851, October.L.
Kevers and J. Medori.
2010.
Symbolic classifica-tion methods for patient discharge summaries encod-ing into ICD.
In Proceedings of the 7th InternationalConference on NLP (IceTAL), pages 197?208, Reyk-javik, Iceland, August.D.
A. Lindberg, B. L. Humphreys, and A. T. McCray.1993.
The Unified Medical Language System.
Meth-ods of Information in Medicine, 32(4):281?291.Y.
A. Lussier, L. Shagina, and C. Friedman.
2000.
Au-tomating ICD-9-CM encoding using medical languageprocessing: A feasibility study.
In AMIA, page 1072.S.
M. Meystre, G. K. Savova, K. C. Kipper-Schuler, andJ.
F. Hurdle.
2008.
Extracting information from tex-tual documents in the electronic health record: a re-view of recent research.
Methods of Information inMedicine, 47(Suppl 1):128?144.J.
Patrick, Y. Zhang, and Y. Wang.
2007.
Developingfeature types for classifying clinical notes.
In BioNLP,pages 191?192.J.
P. Pestian, C. Brew, P. Matykiewicz, D. J. Hovermale,N.
Johnson, K. B. Cohen, and W. Duch.
2007.
Ashared task involving multi-label classification of clin-ical free text.
In BioNLP, pages 97?104.S.
Shalev-Shwartz, Y.
Singer, and N. Srebro.
2007.
Pega-sos: Primal Estimated sub-GrAdient SOlver for SVM.In ICML, Corvallis, OR.M.
H. Stanfill, M. Williams, S. H. Fenton, R. A. Jenders,and W. R. Hersh.
2010.
A systematic literature re-view of automated clinical coding and classificationsystems.
JAMIA, 17:646?651.H.
Suominen, F. Ginter, S. Pyysalo, A. Airola,T.
Pahikkala, S. Salanter, and T. Salakoski.
2008.Machine learning to automate the assignment of di-agnosis codes to free-text radiology reports: a methoddescription.
In Proceedings of the ICML Workshopon Machine Learning for Health-Care Applications,Helsinki, Finland.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginmarkov networks.
In Neural Information ProcessingSystems Conference (NIPS03), Vancouver, Canada,December.A.
Yessenalina, Y. Yue, and C. Cardie.
2010.
Multi-level structured models for document-level sentimentclassification.
In EMNLP.751
