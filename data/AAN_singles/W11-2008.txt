Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59?66,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsSpoken Dialogue System based on Information Extractionusing Similarity of Predicate Argument StructuresKoichiro Yoshino, Shinsuke Mori and Tatsuya KawaharaSchool of Informatics, Kyoto UniversitySakyo-ku, Kyoto, 606-8501, JapanAbstractWe present a novel scheme of spoken dialoguesystems which uses the up-to-date informa-tion on the web.
The scheme is based on in-formation extraction which is defined by thepredicate-argument (P-A) structure and real-ized by semantic parsing.
Based on the in-formation structure, the dialogue system canperform question answering and also proac-tive information presentation.
Feasibility ofthis scheme is demonstrated with experimentsusing a domain of baseball news.
In order toautomatically select useful domain-dependentP-A templates, statistical measures are intro-duced, resulting to a completely unsupervisedlearning of the information structure given acorpus.
Similarity measures of P-A structuresare also introduced to select relevant infor-mation.
An experimental evaluation showsthat the proposed system can make more rel-evant responses compared with the conven-tional ?bag-of-words?
scheme.1 IntroductionRecently, a huge amount of information is accumu-lated and distributed on the web day by day.
As aresult, many people get information via web ratherthan the conventional mass media.
On the otherhand, the amount of information on the web is sohuge that we often encounter the difficulty in findinginformation we want.
Keyword search is the mostwidely-used means for the web information access.However, this style is not necessarily the best forinformation demands of all users who do not havedefinite goals or just want to know what would beinteresting.
To cope with user?s vague informationdemands is an important mission for interactive spo-ken dialogue systems.
Moreover, supporting user?sinformation collection in a small-talk style is one ofthe new directions of spoken dialogue systems.Existing spoken dialogue systems can be clas-sified into two types (T.Kawahara, 2009): thoseusing relational databases (RDB) such as the Air-line Travel Information System (ATIS) (D.A.Dahl,1994), and those using information retrieval tech-niques based on statistical document matching(T.Misu and T.Kawahara, 2010).
The first schemecan achieve a well-defined task by using a struc-tural database, but this scheme cannot be applied tothe web information in which the structure and taskare not well defined.
The second scheme has beenstudied to handle large-scale texts such as web, butmost of the conventional systems adopt a ?bag-of-words?
model, and naive statistical matching oftengenerates irrelevant responses which have nothingto do with the user?s requests.
Our proposed schemesolves this problem by using information extractionbased on semantic parsing from web texts, with-out constructing an RDB.
We adopt the predicate-argument (P-A) structure generated by a parser asa baseline, but every P-A structure is not useful forinformation extraction and retrieval(Y.Kiyota et al,2002; M.O.Dzikovska et al, 2003; S.Harabagiu etal., 2005).
In fact, the useful information structureis dependent on domains.
Conventionally, the tem-plates for information extraction were hand-crafted(R.Grishman, 2003), but this heuristic process is socostly that it cannot be applied to a variety of do-mains on the web.
In this paper, therefore, we pro-59		 !!
"#!$	%!
"&'()"!*!%Figure 1: System overview.pose a filtering method of predicate-argument (P-A)patterns generated by the parser, in order to automat-ically define the domain-dependent useful informa-tion structure.We also address flexible matching based on the P-A structure, because the exact matching often failsand does not generate any outputs.
In order to re-trieve most relevant information, we define similar-ity measures of predicates and arguments, which arealso learned from a domain corpus.In this paper, the proposed scheme is applied to adomain of baseball news, and implemented as a spo-ken dialogue system which can reply to the user?squestion as well as make proactive information pre-sentation using a news website.
An overview of thissystem is described in Section 2, and the templatefiltering method is presented in Section 3.
Then, sys-tem response generation based on flexible matchingis explained in Section 4.
Finally, an evaluation ofthe system is presented in Section 5.2 System Overview2.1 ArchitectureThe architecture of the proposed spoken dialoguesystem is depicted in Fig.
1.
First, information ex-traction is conducted by parsing web texts in ad-vance.
A user?s query is also parsed to extract thesame information structure, and the system matchesthe extracted information against the web informa-tion.
According to the matching result, the systemeither answers the user?s question or makes proac-tive presentation of information which should bemost relevant to the user?s request.If the system finds some information which com-	 !
"#$%%$#$ !
"#$$ &'&(&&&)* + 	#$,$,-$!
$ .*+ $,	#$$ /011)2$*$,!1$!
"#$Figure 2: Example of information extraction and dia-logue.pletely matches the user?s query, the system makesa response using the corresponding web text.
Whenthe system cannot find exact information, it searchesfor some information which matches partially.
Forexample, in Fig.
2, when a user asked ?Did Ichirohit a home-run?
?, the system cannot find exact in-formation ?
[Ichiro (agent), home-run (object), hit]?,but finds ?
[Lopez (agent), three-run homer (object),hit]?
which is partially matched and most relevant.This information is used to generate a relevant re-sponse that the user would expect.In the conventional RDB-based dialogue scheme,the system hardly makes relevant responses if itfinds no matched entries, thus usually replies ?Thereis no matched entries?.
In the conventional question-answering scheme, the same situation often hap-pens.
Occasionally, a set of close-matched answersmay be found by statistical matching, but the foundanswers may not be relevant to the user?s query.
Inthe proposed scheme, we guarantee that the answeris at least partially matched to the user?s query interms of the information structure.2.2 Information Extraction based on P-AStructureWe use the predicate argument (P-A) structure to de-fine the information structure from web texts.
TheP-A structure represents a sentence with a predicate,arguments and their semantic cases, as shown in theprevious examples.
There are some required se-mantic cases depending on the type of the predicate(verb), and also arbitrary semantic cases like time,place, and other modifications.
This structure is aclassic concept in natural language processing, butrecently, automatic semantic parsing has reached apractical level thanks to corpus-based learning tech-60niques (D.Kawahara and S.Kurohashi, 2006) andhas been used for several large-scale tasks (D.Shenand M.Lapata, 2007; R.Wang and Y.Zhang, 2009;D.Wu and P.Fung, 2009).
We use KNP1 as a syntac-tic and semantic parser.3 Extraction of Domain-Dependent P-ATemplatesThe P-A structure automatically generated by thesemantic parser provides useful information struc-ture as a baseline.
However, every P-A pair is notmeaningful in information navigation; actually, onlya fraction of the patterns are useful, and they aredomain-dependent.
For example, in the baseball do-main, key patterns include ?
[A (agent) beat B (ob-ject)]?
and ?
[A (agent) hit B (object)]?, and in thebusiness domain, ?
[A (agent) sell B (object)]?
and?
[A (agent) acquire B (object)]?.
We propose amethod to automatically extract these useful patternsgiven a domain corpus.
We assume each article inthe newspaper corpus/websites is annotated with adomain such as sports-baseball and economy-stock.The method is to filter P-A structure patterns(=templates) based on some statistical measurewhich accounts for the domain.
The filtering pro-cess is also expected to eliminate inappropriate pat-terns caused by parsing errors.
Moreover, in spokendialogue systems, errors in automatic speech recog-nition (ASR) may result in erroneous matching.
Byeliminating irrelevant patterns, we expect robust in-formation extraction for spoken input.Specifically, the following two significance mea-sures are investigated in this work.3.1 TF-IDF MeasureFirst, we use the TF-IDF measure to evaluate impor-tance of word wi in a particular domain or topic t.tfidf(wi, t) = P (wi|t) logC(d)C(d : wi ?
d)(1)The TF term is the occurrence probability of wordwi, defined as:P (wi|t) ?C(wi, t) + ?
?j(C(wj , t) + ?)
(2)1http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.htmlwhere C(wi, t) is the occurrence count of word wiin the domain t in the corpus, and ?
is a smooth-ing factor given by the Dirichlet process prior.
TheIDF term is the inverse log probability of documentscontaining word wi:C(d)C(d : wi ?
d)?
C(d) + ?C(d : wi ?
d) + ?
(3)where C(d) is the number of documents (=newspa-per articles) in the corpus and C(d : wi ?
d) isthe number of documents which contain wi.
?
isa smoothing factor given by the Dirichlet processprior.
We estimate ?
and ?
by a likelihood functionusing the training corpus.
We compute the TF-IDFvalue for a predicate and each argument, and thencompute their geometric mean to define the evalua-tion measure for a P-A template.3.2 Naive Bayes (NB) ModelThe second measure is based on the Naive Bayesmodel.P (t|wi) =C(wi, t) + Dt?C(wi) + ?
(4)Here, ?
is a smoothing factor and Dt is a normal-ization coefficient of the corpus size of the domaint.Dt =?j C(wj , t)?k C(wk).
(5)The evaluation measure for a P-A pattern is ob-tained by taking a geometric mean of the componentwords.3.3 Clustering of Named EntitiesThe statistical learning often falls in the data sparse-ness problem, especially for proper nouns, for ex-ample, name of persons.
Moreover, there may bemismatch in the set of named entities between thetraining corpus and the test phase.
For robust estima-tion, we introduce classes for named entities (nameof persons, organizations, places).
Note that unify-ing all named entities in the corpus before comput-ing the evaluation measure would weaken the sig-nificance of these entities.
Thus, we compute statis-tics for every proper noun before clustering, and sumup values for the class afterwards.
For example, thescore for ?
[[person](agent), hit]?
is computed as asum over all persons of this pattern.61Table 1: Evaluation of template filtering.model feature Precision Recall FBaseline - 0.444 1 0.615TF-IDF Predicate 0.587 0.840 0.691Argument 0.658 0.730 0.692P + A 0.513 0.843 0.638NB Predicate 0.601 0.879 0.714Argument 0.661 0.794 0.722P + A 0.878 0.726 0.7953.4 Evaluation of Template FilteringWe performed an experimental evaluation to com-pare the effectiveness of the two significance mea-sures (TF-IDF and Naive Bayes (NB)) in theJapanese professional baseball domain.
The mod-els are trained with the Mainichi Newspaper cor-pus 2008.
The clustering of named entities is ap-plied to both methods.
The P-A templates hav-ing larger significance scores are selected.
We de-termined a threshold for selecting templates usinga development set which was held out from thetest set by 10%.
The test set was made fromMainichi newspaper?s website which talks aboutgames played between April 21-23, 2010.
Manualannotation was made on typical predicates and se-mantic cases which can be used for question answer-ing and proactive presentation.
The filtering wasperformed on the test set by matching the patternsdefined by each measure, and evaluated against theannotated answers in terms of recall, precision andF-measure (F).
Table 1 lists the result for the twomeasures using predicate-only, argument-only, andboth of them.In this result, using both predicates and argumentsin the Naive Bayes (NB) model performs the best.Compared with the baseline without any filtering,the proposed methods significantly improved pre-cision with some degradation of recall.
This prop-erty is important in realizing informative responsegeneration robust against ASR and parsing errors.Among the selected templates, we can find typicaland important patterns like ?have a win?, ?come intopitch?, and ?make it consecutive wins?.
Most of re-call errors are infrequent patterns, and majority ofprecision errors are those patterns that are frequentlyobserved but not useful for presentation.4 Presentation of Relevant InformationWhen the system fails to find exact information thatmatches the user?s query, or the user does not speakfor a while, the system tries to make proactive in-formation presentation.
It is based on the partiallymatched entries of the current or latest query.
Thefall-back is similar to collaborative response gener-ation in the conventional spoken dialogue systems(D.Sadek, 1999), but it is intended for proactive in-formation presentation using general documents.4.1 Response generation based on partialmatchingFor preference among multiple components in theP-A pattern of the user query, we make use of thesignificance measure defined in Section 3.
Specifi-cally, we relax (=ignore) the component of the leastsignificance score, then search for relevant informa-tion.
If any entry is not still matched, we relax thenext less significant component.
If multiple entriesare found with this matching, we need to select themost relevant entry.
Thus, we introduce two scoresof relevance.
The relevance measure is defined indifferent manners for predicates (=verbs) and argu-ments (=nouns).
The measure for arguments is de-fined based on the co-occurrence statistics in the cor-pus.
The measure for predicate is defined based ondistributional analysis of arguments.4.2 Relevance measure of argumentsThe relevance of argument words (=nouns) wi andwj is defined assimarg(wi, wj) ={C(wi, wj)}2C(wi) ?
C(wj).
(6)Here, wi is in the original query, and relaxed (ig-nored) in the partial matching, and wj of the bestrelevance score is retrieved for response generation.In the example of Fig.
2, wi is ?Ichiro?
and wj is?Lopez?.4.3 Relevance measure of predicatesDistributional analysis (Z.Harris, 1951; Lin, 1998)has been used to define similarity of words, assum-ing that similar words have similar contexts.
In thispaper, we use the distribution of arguments whichhave a modification relation to predicates (Fig.
3)62		 Figure 3: Distribution analysis of P-A structure.
(T.Shibata et al, 2008; P.Pantel et al, 2009).
Therelevance of predicate words wprei and wprej is de-fined as a cosine distance of occurrence vectors ofthe modifying arguments(J.Mitchell and M.Lapata,2008; S.Thater et al, 2010).
Here, argument en-tries are distinguished by their semantic cases suchas Agent and Object, as shown in Fig 3.
As the dis-tribution of arguments is sparse and its reliable esti-mation is difficult, we introduce smoothing by usinganother distributional analysis of arguments, whichis similar to the one in the previous section.4.4 Bag-of-Words (BOW) ModelIf no entry is matched with all possible partialmatching, we resort to the naive ?bag-of-words?
(BOW) model, in which a sentence is representedwith a vector of word occurrence and matching isdone based on this vector.
This method is widelyused for document retrieval.
We count only contentwords.
In this method, we make use of the signifi-cance score for preference of the words when multi-ple candidates are matched for a short query.The overall matching strategy of the proposedscheme is summarized in Fig.
4.4.5 Selection of Relevant Information fromSentenceAnswer or information presentation is generatedbased on the matched sentence in a newspaper ar-ticle.
As a sentence is often complex or made ofmultiple predicates, simple presentation of the sen-tence would be redundant or even irrelevant.
There-fore, we select the portion of the matched P-A struc-ture, to generate a concise response relevant to theuser?s query.
For example, when a sentence ?Ichirohit a three-run homer in the seventh inning andMariners won the game?
is matched by the pattern 	 		  	Figure 4: Strategy for flexible matching in steps.?
[Ichiro(agent), hit]?, we select the former portionof the sentence which exactly answers the user?squery, and generate a response ?Ichiro hit a three-run homer in the seventh inning.
?5 System EvaluationWe have implemented a spoken dialogue systembased on the significance measure (Naive Bayesmodel) and the relevance measures, which werelearned using the Mainichi Newspaper corpus of tenyears (2000-2009).
For evaluation of the system, weprepared 201 questions from news articles (Septem-ber 19-26, 2010) seen at the website of MainichiNewspaper2.
Correct answers to the test querieswere annotated manually.
Evaluation was done withthe text input as well as speech input.
A word N-gram language model for ASR dedicated to the do-main was trained using the relevant newspaper arti-cle corpus.
The word error rate was approximately24%.The system responses for the test queries are cat-egorized into one of the following four: correct an-swer only (?Correct?
), case which includes the cor-rect answer but also other redundant answers (?Am-biguous?
), incorrect answer (?Incorrect?
), and (?NoAnswer?).
The ambiguous cases occur when mul-tiple sentences or predicates are matched.
We alsocalculate recall, precision and F-measure by count-ing individual answers separately even when multi-ple answers are output.
The results based on theseevaluation measures are summarized in Table 2 andTable 3 for text input and speech input.In the tables, the proposed method is broken downinto three phases as shown in Fig.
4: exact match-ing of P-A structure (Section 3), incorporation ofthe partial matching (Section 4.1), and back-off tothe ?bag-of-words?
(BOW) model (Section 4.4).
Forcomparison, we also tested the BOW model and2http://www.mainichi.jp63Table 2: Evaluation of system response.Input Model Correct Ambiguous Incorrect No AnswerText Exact 29.9% 0.5% 1.5% 68.1%Exact+Partial 66.2% 5.0% 20.3% 8.5%Exact+Partial+BOW 69.7% 5.0% 25.3% 0.0%(cf) Bag-of-words (BOW) 46.8% 13.9% 39.3% 0.0%(cf) Sequence-of-words (SOW) 54.2% 11.4% 34.3% 0.0%Speech Exact 19.4% 1.0% 0.5% 79.1%(ASR) Exact+Partial 57.2% 6.0% 18.9% 17.9%Exact+Partial+BOW 64.1% 6.5% 28.9% 0.0%(cf) Bag-of-words (BOW) 39.8% 9.4% 48.8% 0.0%(cf) Sequence-of-words (SOW) 46.3% 10.4% 43.3% 0.0%Table 3: Accuracy of system response.Input Model Precision Recall FText Exact 93.8% 30.3% 45.8%Exact+Partial 72.5% 71.1% 71.8%Exact+Partial+BOW 70.1% 74.6% 72.3%(cf) Bag-of-words (BOW) 49.8% 60.7% 54.7%(cf) Sequence-of-words (SOW) 55.2% 65.6% 60.0%Speech Exact 89.1% 20.4% 33.2%(ASR) Exact+Partial 65.8% 63.2% 64.5%Exact+Partial+BOW 61.7% 70.6% 65.9%(cf) Bag-of-words (BOW) 42.9% 49.3% 45.9%(cf) Sequence-of-words (SOW) 48.3% 56.7% 52.2%?sequence-of-words?
(SOW) model, which considerthe sequence order in the BOW model.
The exactmatching assumes strong constraint of P-A patterns,so the generated answers are almost correct, but noanswers are generated very often.
By incorporat-ing the partial matching and BOW model, the sys-tem can output more relevant answers.
Comparedwith the BOW model, the proposed method achievesmuch higher ratio or precision of correct answers.
F-measure is also higher by 17% absolute.A similar tendency is observed for speech input,although the overall accuracy is degraded becauseof the ASR errors.
However, degradation is rela-tively small considering the word accuracy of 76%.The partial matching works effectively even if theexact matching fails due to ASR errors.
Moreover,the back-off to the BOW model is effective in ASRinput.The proposed method generates concise re-sponses by selecting the relevant portion as de-scribed in Section 4.5, while the BOW method of-ten generates long responses which includes manyredundant portions.
This property is particularly im-portant in the speech interface.We show a dialogue example in Fig.
5 which isin Japanese and translated to English for reference(=Italic).6 Domain PortabilityIn the proposed scheme, we construct a domain-dependent system in a pre-determined domain.
Itis straightforward to port the system to other do-mains just by preparing a domain corpus.
This sys-tem uses a newspaper corpus for training domain-dependent statistical models.
Newspaper articles areclassified according to domain categories such assports/baseball and business/finance.
Therefore, it isstraightforward to select relevant articles for a par-ticular domain.
In the proposed scheme, we do notneed any further annotations and all statistical mod-els are trained in an unsupervised manner.64 	 	   	          	 	   	!"#	 	      	!$	%&Figure 5: Dialogue example (original in Japanese, trans-lated to English).7 ConclusionsWe have presented a new scheme of spoken dialoguesystems which can talk about web texts in an in-teractive manner.
The information extraction tech-nique is adopted to conduct question answering aswell as proactive information presentation.
Filteringbased on a statistical significant measure is intro-duced to automatically select useful templates in agiven domain.
Relevance measures are also definedfor predicate and argument in order to retrieve rele-vant entries when the exact matching does not suc-ceed.
In experimental evaluations, we have demon-strated that the filtering works effectively and thesystem generates more relevant responses than theconventional method.Ongoing works include application to other do-mains to demonstrate generality of the scheme.ReferencesD.A.Dahl.
1994.
Expanding the scope of the ATIS task:The ATIS-3 corpus.
In Proc.
ARPA Human LanguageTechnology Workshop, pages 43?48.D.Kawahara and S.Kurohashi.
2006.
A fully-lexicalizedprobabilistic model for japanese syntactic and casestructure analysis.
In Proc.
HLT-NAACL, pages 176?183.D.Sadek.
1999.
Design consideration on dialogue sys-tems: From theory to technology - the case of Artimis-.
In Proc.
ESCA workshop on Interactive Dialogue inMulti-Modal Systems, pages 173?187.D.Shen and M.Lapata.
2007.
Using semantic roles to im-prove question answering.
In Proc.
EMNLP-CoNLL,pages 12?21.D.Wu and P.Fung.
2009.
Can semantic role labeling im-prove SMT?
In Proc.
EAMT, pages 218?225.J.Mitchell and M.Lapata.
2008.
Vector-based models ofsemantic composition.
In Proc.
ACL-HLT, pages 236?244.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proc.
ACL and COLING, pages768?774.M.O.Dzikovska, M.D.Swift, and J.F.Allen.
2003.
Inte-grating linguistic and domain knowledge for spokendialogue systems in multiple domains.
In Proc.
ofIJCAI-03 Workshop on Knowledge and Reasoning inPractical Dialogue Systems.P.Pantel, E.Crestan, A.Borkovsky, A.-M.Popescu, andV.Vayas.
2009.
Web-scale distributional similarityand entity set expansion.
In Proc.
EMNLP, pages 938?947.R.Grishman.
2003.
Discovery methods for informa-tion extraction.
In Proc.
ISCA & IEEE Workshopon Spontaneous Speech Processing and Recognition,pages 243?247.R.Pieraccini, E.Tzoukermann, Z.Gorelov, J-L.Gauvain,E.Levin, C.-H Lee, and J.G.Wilpon.
1992.
A speechunderstanding system based on statistical representa-tion of semantics.
In Proc.
IEEE-ICASSP, volume 1,pages 193?196.R.Wang and Y.Zhang.
2009.
Recognizing textual re-latedness with predicate-argument structure.
In Proc.EMNLP, pages 784?792.S.Harabagiu, A.Hickl, J.Lehmann, and D.Moldovan.2005.
Experiments with interactive question-answering.
In Proc.
ACL, pages 205?214.S.Thater, H.Fu?rstenau, and M.Pinkal.
2010.
Contextual-izing semantic representations using syntactically en-riched vector models.
In Proc.
ACL, pages 948?957.T.Kawahara.
2009.
New perspectives on spoken lan-guage understanding: Does machine need to fully un-derstand speech?
In Proc.
IEEE-ASRU, pages 46?50.T.Misu and T.Kawahara.
2010.
Bayes risk-based dia-logue management for document retrieval system withspeech interface.
Speech Communication, 52(1):61?71.65T.Shibata, M.Odani, J.Harashima, T.Oonishi, andS.Kurohashi.
2008.
Syngraph: A flexible match-ing method based on synonymous expression extrac-tion from an ordinary dictionary and a web corpus.
InProc.
IJCNLP, pages 787?792.Y.Kiyota, S.Kurohashi, and F.Kido.
2002.
?dialog nav-igator?
: A question answering system based on largetext knowledge base.
In Proc.
COLING, pages 460?466.Z.Harris.
1951.
Structural Linguistics.
University ofChicago Press.66
