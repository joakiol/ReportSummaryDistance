Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1296?1306,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsSequence-to-Sequence Learningas Beam-Search OptimizationSam Wiseman and Alexander M. RushSchool of Engineering and Applied SciencesHarvard UniversityCambridge, MA, USA{swiseman,srush}@seas.harvard.eduAbstractSequence-to-Sequence (seq2seq) modelinghas rapidly become an important general-purpose NLP tool that has proven effective formany text-generation and sequence-labelingtasks.
Seq2seq builds on deep neural languagemodeling and inherits its remarkable accuracyin estimating local, next-word distributions.
Inthis work, we introduce a model and beam-search training scheme, based on the workof Daume?
III and Marcu (2005), that extendsseq2seq to learn global sequence scores.
Thisstructured approach avoids classical biases as-sociated with local training and unifies thetraining loss with the test-time usage, whilepreserving the proven model architecture ofseq2seq and its efficient training approach.
Weshow that our system outperforms a highly-optimized attention-based seq2seq system andother baselines on three different sequence tosequence tasks: word ordering, parsing, andmachine translation.1 IntroductionSequence-to-Sequence learning with deep neuralnetworks (herein, seq2seq) (Sutskever et al, 2011;Sutskever et al, 2014) has rapidly become a veryuseful and surprisingly general-purpose tool for nat-ural language processing.
In addition to demon-strating impressive results for machine translation(Bahdanau et al, 2015), roughly the same modeland training have also proven to be useful for sen-tence compression (Filippova et al, 2015), parsing(Vinyals et al, 2015), and dialogue systems (Ser-ban et al, 2016), and they additionally underlie othertext generation applications, such as image or videocaptioning (Venugopalan et al, 2015; Xu et al,2015).The dominant approach to training a seq2seq sys-tem is as a conditional language model, with trainingmaximizing the likelihood of each successive tar-get word conditioned on the input sequence and thegold history of target words.
Thus, training uses astrictly word-level loss, usually cross-entropy overthe target vocabulary.
This approach has proven tobe very effective and efficient for training neural lan-guage models, and seq2seq models similarly obtainimpressive perplexities for word-generation tasks.Notably, however, seq2seq models are not used asconditional language models at test-time; they mustinstead generate fully-formed word sequences.
Inpractice, generation is accomplished by searchingover output sequences greedily or with beam search.In this context, Ranzato et al (2016) note that thecombination of the training and generation schemejust described leads to at least two major issues:1.
Exposure Bias: the model is never exposed toits own errors during training, and so the in-ferred histories at test-time do not resemble thegold training histories.2.
Loss-Evaluation Mismatch: training uses aword-level loss, while at test-time we targetimproving sequence-level evaluation metrics,such as BLEU (Papineni et al, 2002).We might additionally add the concern of labelbias (Lafferty et al, 2001) to the list, since word-probabilities at each time-step are locally normal-ized, guaranteeing that successors of incorrect his-1296tories receive the same mass as do the successors ofthe true history.In this work we develop a non-probabilistic vari-ant of the seq2seq model that can assign a scoreto any possible target sequence, and we proposea training procedure, inspired by the learning assearch optimization (LaSO) framework of Daume?III and Marcu (2005), that defines a loss functionin terms of errors made during beam search.
Fur-thermore, we provide an efficient algorithm to back-propagate through the beam-search procedure dur-ing seq2seq training.This approach offers a possible solution to eachof the three aforementioned issues, while largelymaintaining the model architecture and training ef-ficiency of standard seq2seq learning.
Moreover,by scoring sequences rather than words, our ap-proach also allows for enforcing hard-constraints onsequence generation at training time.
To test out theeffectiveness of the proposed approach, we developa general-purpose seq2seq system with beam searchoptimization.
We run experiments on three very dif-ferent problems: word ordering, syntactic parsing,and machine translation, and compare to a highly-tuned seq2seq system with attention (Luong et al,2015).
The version with beam search optimizationshows significant improvements on all three tasks,and particular improvements on tasks that requiredifficult search.2 Related WorkThe issues of exposure bias and label bias have re-ceived much attention from authors in the structuredprediction community, and we briefly review someof this work here.
One prominent approach to com-bating exposure bias is that of SEARN (Daume?
IIIet al, 2009), a meta-training algorithm that learns asearch policy in the form of a cost-sensitive classifiertrained on examples generated from an interpolationof an oracle policy and the model?s current (learned)policy.
Thus, SEARN explicitly targets the mis-match between oracular training and non-oracular(often greedy) test-time inference by training on theoutput of the model?s own policy.
DAgger (Rosset al, 2011) is a similar approach, which differs interms of how training examples are generated andaggregated, and there have additionally been impor-tant refinements to this style of training over the pastseveral years (Chang et al, 2015).
When it comesto training RNNs, SEARN/DAgger has been appliedunder the name ?scheduled sampling?
(Bengio et al,2015), which involves training an RNN to generatethe t+ 1?st token in a target sequence after consum-ing either the true t?th token, or, with probability thatincreases throughout training, the predicted t?th to-ken.Though technically possible, it is uncom-mon to use beam search when training withSEARN/DAgger.
The early-update (Collins andRoark, 2004) and LaSO (Daume?
III and Marcu,2005) training strategies, however, explicitly ac-count for beam search, and describe strategies forupdating parameters when the gold structure be-comes unreachable during search.
Early update andLaSO differ primarily in that the former discards atraining example after the first search error, whereasLaSO resumes searching after an error from a statethat includes the gold partial structure.
In the con-text of feed-forward neural network training, earlyupdate training has been recently explored in a feed-forward setting by Zhou et al (2015) and Andoret al (2016).
Our work differs in that we adopta LaSO-like paradigm (with some minor modifica-tions), and apply it to the training of seq2seq RNNs(rather than feed-forward networks).
We also notethat Watanabe and Sumita (2015) apply maximum-violation training (Huang et al, 2012), which is sim-ilar to early-update, to a parsing model with recur-rent components, and that Yazdani and Henderson(2015) use beam-search in training a discriminative,locally normalized dependency parser with recurrentcomponents.Recently authors have also proposed alleviatingexposure bias using techniques from reinforcementlearning.
Ranzato et al (2016) follow this ap-proach to train RNN decoders in a seq2seq model,and they obtain consistent improvements in perfor-mance, even over models trained with scheduledsampling.
As Daume?
III and Marcu (2005) note,LaSO is similar to reinforcement learning, exceptit does not require ?exploration?
in the same way.Such exploration may be unnecessary in supervisedtext-generation, since we typically know the goldpartial sequences at each time-step.
Shen et al(2016) use minimum risk training (approximated by1297sampling) to address the issues of exposure bias andloss-evaluation mismatch for seq2seq MT, and showimpressive performance gains.Whereas exposure bias results from training ina certain way, label bias results from properties ofthe model itself.
In particular, label bias is likelyto affect structured models that make sub-structurepredictions using locally-normalized scores.
Be-cause the neural and non-neural literature on thispoint has recently been reviewed by Andor et al(2016), we simply note here that RNN models aretypically locally normalized, and we are unaware ofany specifically seq2seq work with RNNs that doesnot use locally-normalized scores.
The model weintroduce here, however, is not locally normalized,and so should not suffer from label bias.
We alsonote that there are some (non-seq2seq) exceptionsto the trend of locally normalized RNNs, such asthe work of Sak et al (2014) and Voigtlaender et al(2015), who train LSTMs in the context of HMMsfor speech recognition using sequence-level objec-tives; their work does not consider search, however.3 Background and NotationIn the simplest seq2seq scenario, we are given a col-lection of source-target sequence pairs and taskedwith learning to generate target sequences fromsource sequences.
For instance, we might view ma-chine translation in this way, where in particular weattempt to generate English sentences from (corre-sponding) French sentences.
Seq2seq models arepart of the broader class of ?encoder-decoder?
mod-els (Cho et al, 2014), which first use an encodingmodel to transform a source object into an encodedrepresentation x.
Many different sequential (andnon-sequential) encoders have proven to be effec-tive for different source domains.
In this work weare agnostic to the form of the encoding model, andsimply assume an abstract source representation x.Once the input sequence is encoded, seq2seqmodels generate a target sequence using a decoder.The decoder is tasked with generating a target se-quence of words from a target vocabulary V .
Inparticular, words are generated sequentially by con-ditioning on the input representation x and on thepreviously generated words or history.
We use thenotation w1:T to refer to an arbitrary word sequenceof length T , and the notation y1:T to refer to the gold(i.e., correct) target word sequence for an input x.Most seq2seq systems utilize a recurrent neuralnetwork (RNN) for the decoder model.
Formally,a recurrent neural network is a parameterized non-linear function RNN that recursively maps a se-quence of vectors to a sequence of hidden states.
Letm1, .
.
.
,mT be a sequence of T vectors, and let h0be some initial state vector.
Applying an RNN toany such sequence yields hidden states ht at eachtime-step t, as follows:ht ?
RNN(mt,ht?1;?
),where ?
is the set of model parameters, which areshared over time.
In this work, the vectors mtwill always correspond to the embeddings of a tar-get word sequence w1:T , and so we will also writeht ?
RNN(wt,ht?1;?
), with wt standing in forits embedding.RNN decoders are typically trained to act as con-ditional language models.
That is, one attempts tomodel the probability of the t?th target word con-ditioned on x and the target history by stipulatingthat p(wt|w1:t?1,x) = g(wt,ht?1,x), for some pa-rameterized function g typically computed with anaffine layer followed by a softmax.
In computingthese probabilities, the state ht?1 represents the tar-get history, and h0 is typically set to be some func-tion of x.
The complete model (including encoder)is trained, analogously to a neural language model,to minimize the cross-entropy loss at each time-stepwhile conditioning on the gold history in the train-ing data.
That is, the model is trained to minimize?
ln?Tt=1 p(yt|y1:t?1,x).Once the decoder is trained, discrete se-quence generation can be performed by approx-imately maximizing the probability of the tar-get sequence under the conditional distribution,y?1:T = argbeamw1:T?Tt=1 p(wt|w1:t?1,x), wherewe use the notation argbeam to emphasize that thedecoding process requires heuristic search, since theRNN model is non-Markovian.
In practice, a simplebeam search procedure that explores K prospectivehistories at each time-step has proven to be an effec-tive decoding approach.
However, as noted above,decoding in this manner after conditional language-model style training potentially suffers from the is-1298sues of exposure bias and label bias, which moti-vates the work of this paper.4 Beam Search OptimizationWe begin by making one small change to theseq2seq modeling framework.
Instead of predictingthe probability of the next word, we instead learnto produce (non-probabilistic) scores for ranking se-quences.
Define the score of a sequence consistingof history w1:t?1 followed by a single word wt asf(wt,ht?1,x), where f is a parameterized functionexamining the current hidden-state of the relevantRNN at time t?
1 as well as the input representa-tion x.
In experiments, our f will have an identi-cal form to g but without the final softmax transfor-mation (which transforms unnormalized scores intoprobabilities), thereby allowing the model to avoidissues associated with the label bias problem.More importantly, we also modify how this modelis trained.
Ideally we would train by comparingthe gold sequence to the highest-scoring completesequence.
However, because finding the argmaxsequence according to this model is intractable,we propose to adopt a LaSO-like (Daume?
III andMarcu, 2005) scheme to train, which we will re-fer to as beam search optimization (BSO).
In par-ticular, we define a loss that penalizes the gold se-quence falling off the beam during training.1 Theproposed training approach is a simple way to ex-pose the model to incorrect histories and to matchthe training procedure to test generation.
Further-more we show that it can be implemented efficientlywithout changing the asymptotic run-time of train-ing, beyond a factor of the beam size K.4.1 Search-Based LossWe now formalize this notion of a search-based lossfor RNN training.
Assume we have a set St of Kcandidate sequences of length t. We can calculate ascore for each sequence in St using a scoring func-tion f parameterized with an RNN, as above, and wedefine the sequence y?
(K)1:t ?St to be the K?th ranked1Using a non-probabilistic model further allows us to incurno loss (and thus require no update to parameters) when the goldsequence is on the beam; this contrasts with models based on aCRF loss, such as those of Andor et al (2016) and Zhou et al(2015), though in training those models are simply not updatedwhen the gold sequence remains on the beam.sequence in St according to f .
That is, assumingdistinct scores,|{y?
(k)1:t ?St | f(y?
(k)t , h?
(k)t?1) > f(y?
(K)t , h?
(K)t?1)}| = K ?
1,where y?
(k)t is the t?th token in y?
(k)1:t , h?
(k)t?1 is the RNNstate corresponding to its t?
1?st step, and where wehave omitted the x argument to f for brevity.We now define a loss function that gives loss eachtime the score of the gold prefix y1:t does not exceedthat of y?
(K)1:t by a margin:L(f) =T?t=1?(y?
(K)1:t )[1?
f(yt,ht?1) + f(y?
(K)t , h?
(K)t?1)].Above, the ?(y?
(K)1:t ) term denotes a mistake-specificcost-function, which allows us to scale the loss de-pending on the severity of erroneously predictingy?
(K)1:t ; it is assumed to return 0 when the margin re-quirement is satisfied, and a positive number other-wise.
It is this term that allows us to use sequence-rather than word-level costs in training (addressingthe 2nd issue in the introduction).
For instance,when training a seq2seq model for machine trans-lation, it may be desirable to have ?(y?
(K)1:t ) be in-versely related to the partial sentence-level BLEUscore of y?
(K)1:t with y1:t; we experiment along theselines in Section 5.3.Finally, because we want the full gold sequence tobe at the top of the beam at the end of search, whent=T we modify the loss to require the score of y1:Tto exceed the score of the highest ranked incorrectprediction by a margin.We can optimize the loss L using a two-step pro-cess: (1) in a forward pass, we compute candidatesets St and record margin violations (sequences withnon-zero loss); (2) in a backward pass, we back-propagate the errors through the seq2seq RNNs.
Un-like standard seq2seq training, the first-step requiresrunning search (in our case beam search) to findmargin violations.
The second step can be doneby adapting back-propagation through time (BPTT).We next discuss the details of this process.4.2 Forward: Find ViolationsIn order to minimize this loss, we need to specify aprocedure for constructing candidate sequences y?
(k)1:t1299at each time step t so that we find margin viola-tions.
We follow LaSO (rather than early-update 2;see Section 2) and build candidates in a recursivemanner.
If there was no margin violation at t?1,then St is constructed using a standard beam searchupdate.
If there was a margin violation, St is con-structed as the K best sequences assuming the goldhistory y1:t?1 through time-step t?1.Formally, assume the function succ maps a se-quence w1:t?1 ?Vt?1 to the set of all valid se-quences of length t that can be formed by appendingto it a valid word w?V .
In the simplest, uncon-strained case, we will havesucc(w1:t?1) = {w1:t?1, w | w ?
V}.As an important aside, note that for some prob-lems it may be preferable to define a succ func-tion which imposes hard constraints on successorsequences.
For instance, if we would like to useseq2seq models for parsing (by emitting a con-stituency or dependency structure encoded into a se-quence in some way), we will have hard constraintson the sequences the model can output, namely, thatthey represent valid parses.
While hard constraintssuch as these would be difficult to add to standardseq2seq at training time, in our framework they cannaturally be added to the succ function, allowing usto train with hard constraints; we experiment alongthese lines in Section 5.3, where we refer to a modeltrained with constrained beam search as ConBSO.Having defined an appropriate succ function, wespecify the candidate set as:St = topK{succ(y1:t?1) violation at t?1?Kk=1 succ(y?
(k)1:t?1) otherwise,where we have a margin violation at t?1 ifff(yt?1,ht?2) < f(y?
(K)t?1 , h?
(K)t?2) + 1, and wheretopK considers the scores given by f .
This searchprocedure is illustrated in the top portion of Figure 1.In the forward pass of our training algorithm,shown as the first part of Algorithm 1, we run thisversion of beam search and collect all sequences andtheir hidden states that lead to losses.2We found that training with early-update rather than (de-layed) LaSO did not work well, even after pre-training.
Giventhe success of early-update in many NLP tasks this was some-what surprising.
We leave this question to future work.a red dog smells home todaythe dog dog barks quickly Fridayred blue cat barks straight nowruns todaya red dog runs quickly todayblue dog barks home todayFigure 1: Top: possible y?
(k)1:t formed in training with abeam of size K = 3 and with gold sequence y1:6 = ?ared dog runs quickly today?.
The gold sequence is high-lighted in yellow, and the predicted prefixes involved inmargin violations (at t= 4 and t= 6) are in gray.
Notethat time-step T = 6 uses a different loss criterion.
Bot-tom: prefixes that actually participate in the loss, ar-ranged to illustrate the back-propagation process.4.3 Backward: Merge SequencesOnce we have collected margin violations we canrun backpropagation to compute parameter updates.Assume a margin violation occurs at time-step t be-tween the predicted history y?
(K)1:t and the gold his-tory y1:t. As in standard seq2seq training we mustback-propagate this error through the gold history;however, unlike seq2seq we also have a gradient forthe wrongly predicted history.Recall that to back-propagate errors through anRNN we run a recursive backward procedure ?denoted below by BRNN ?
at each time-step t,which accumulates the gradients of next-step and fu-ture losses with respect to ht.
We have:?htL ?
BRNN(?htLt+1,?ht+1L),where Lt+1 is the loss at step t+ 1, deriving, forinstance, from the score f(yt+1,ht).
Running thisBRNN procedure from t=T ?
1 to t= 0 is knownas back-propagation through time (BPTT).In determining the total computational cost ofback-propagation here, first note that in the worstcase there is one violation at each time-step, whichleads to T independent, incorrect sequences.
Sincewe need to call BRNN O(T ) times for each se-quence, a naive strategy of running BPTT for eachincorrect sequence would lead to an O(T 2) back-ward pass, rather than the O(T ) time required forthe standard seq2seq approach.Fortunately, our combination of search-strategyand loss make it possible to efficiently shareBRNN operations.
This shared structure comes1300naturally from the LaSO update, which resets thebeam in a convenient way.We informally illustrate the process in Figure 1.The top of the diagram shows a possible sequenceof y?
(k)1:t formed during search with a beam of size 3for the target sequence y = ?a red dog runs quicklytoday.?
When the gold sequence falls off the beamat t= 4, search resumes with S5 = succ(y1:4), andso all subsequent predicted sequences have y1:4 as aprefix and are thus functions of h4.
Moreover, be-cause our loss function only involves the scores ofthe gold prefix and the violating prefix, we end upwith the relatively simple computation tree shownat the bottom of Figure 1.
It is evident that we canbackpropagate in a single pass, accumulating gradi-ents from sequences that diverge from the gold at thetime-step that precedes their divergence.
The secondhalf of Algorithm 1 shows this explicitly for a singlesequence, though it is straightforward to extend thealgorithm to operate in batch.35 Data and MethodsWe run experiments on three different tasks, com-paring our approach to the seq2seq baseline, and toother relevant baselines.5.1 ModelWhile the method we describe applies to seq2seqRNNs in general, for all experiments we use theglobal attention model of Luong et al (2015)?
which consists of an LSTM (Hochreiter andSchmidhuber, 1997) encoder and an LSTM decoderwith a global attention model ?
as both the base-line seq2seq model (i.e., as the model that computesthe g in Section 3) and as the model that computesour sequence-scores f(wt,ht?1,x).
As in Luonget al (2015), we also use ?input feeding,?
whichinvolves feeding the attention distribution from theprevious time-step into the decoder at the currentstep.
This model architecture has been found tobe highly performant for neural machine translationand other seq2seq tasks.3We also note that because we do not update the parametersuntil after the T ?th search step, our training procedure differsslightly from LaSO (which is online), and in this aspect is essen-tially equivalent to the ?delayed LaSO update?
of Bjo?rkelundand Kuhn (2014).Algorithm 1 Seq2seq Beam-Search Optimization1: procedure BSO(x,Ktr, succ)2: /*FORWARD*/3: Init empty storage y?1:T and h?1:T ; init S14: r ?
0; violations?
{0}5: for t = 1, .
.
.
, T do6: K =Ktr if t 6=T else argmaxk:y?
(k)1:t 6=y1:tf(y?
(k)t , h?
(k)t?1)7: if f(yt,ht?1) < f(y?
(K)t , h?
(K)t?1) + 1 then8: h?r:t?1 ?
h?
(K)r:t?19: y?r+1:t ?
y?
(K)r+1:t10: Add t to violations11: r ?
t12: St+1 ?
topK(succ(y1:t))13: else14: St+1 ?
topK(?Kk=1 succ(y?
(k)1:t ))15: /*BACKWARD*/16: grad hT ?
0; grad h?T ?
017: for t = T ?
1, .
.
.
, 1 do18: grad ht?BRNN(?htLt+1, grad ht+1)19: grad h?t?BRNN(?h?tLt+1, grad h?t+1)20: if t?
1 ?
violations then21: grad ht ?
grad ht + grad h?t22: grad h?t ?
0To distinguish the models we refer to our systemas BSO (beam search optimization) and to the base-line as seq2seq.
When we apply constrained training(as discussed in Section 4.2), we refer to the modelas ConBSO.
In providing results we also distinguishbetween the beam size Ktr with which the modelis trained, and the beam size Kte which is used attest-time.
In general, if we plan on evaluating with abeam of size Kte it makes sense to train with a beamof size Ktr = Kte + 1, since our objective requiresthe gold sequence to be scored higher than the lastsequence on the beam.5.2 MethodologyHere we detail additional techniques we found nec-essary to ensure the model learned effectively.
First,we found that the model failed to learn when trainedfrom a random initialization.4 We therefore foundit necessary to pre-train the model using a standard,word-level cross-entropy loss as described in Sec-4This may be because there is relatively little signal in thesparse, sequence-level gradient, but this point requires furtherinvestigation.1301tion 3.
The necessity of pre-training in this instanceis consistent with the findings of other authors whotrain non-local neural models (Kingsbury, 2009; Saket al, 2014; Andor et al, 2016; Ranzato et al,2016).5Similarly, it is clear that the smaller the beam usedin training is, the less room the model has to makeerroneous predictions without running afoul of themargin loss.
Accordingly, we also found it use-ful to use a ?curriculum beam?
strategy in training,whereby the size of the beam is increased graduallyduring training.
In particular, given a desired train-ing beam size Ktr, we began training with a beamof size 2, and increased it by 1 every 2 epochs untilreaching Ktr.Finally, it has been established that dropout (Sri-vastava et al, 2014) regularization improves the per-formance of LSTMs (Pham et al, 2014; Zarembaet al, 2014), and in our experiments we run beamsearch under dropout.6For all experiments, we trained both seq2seq andBSO models with mini-batch Adagrad (Duchi et al,2011) (using batches of size 64), and we renormal-ized all gradients so they did not exceed 5 beforeupdating parameters.
We did not extensively tunelearning-rates, but we found initial rates of 0.02for the encoder and decoder LSTMs, and a rate of0.1 or 0.2 for the final linear layer (i.e., the layertasked with making word-predictions at each time-step) to work well across all the tasks we consid-ered.
Code implementing the experiments describedbelow can be found at https://github.com/harvardnlp/BSO.75.3 Tasks and ResultsOur experiments are primarily intended to evaluatethe effectiveness of beam search optimization overstandard seq2seq training.
As such, we run exper-iments with the same model across three very dif-5Andor et al (2016) found, however, that pre-training onlyincreased convergence-speed, but was not necessary for obtain-ing good results.6However, it is important to ensure that the same mask ap-plied at each time-step of the forward search is also applied atthe corresponding step of the backward pass.
We accomplishthis by pre-computing masks for each time-step, and sharingthem between the partial sequence LSTMs.7Our code is based on Yoon Kim?s seq2seq code, https://github.com/harvardnlp/seq2seq-attn.ferent problems: word ordering, dependency pars-ing, and machine translation.
While we do not in-clude all the features and extensions necessary toreach state-of-the-art performance, even the baselineseq2seq model is generally quite performant.WordOrdering The task of correctly ordering thewords in a shuffled sentence has recently gainedsome attention as a way to test the (syntactic) capa-bilities of text-generation systems (Zhang and Clark,2011; Zhang and Clark, 2015; Liu et al, 2015;Schmaltz et al, 2016).
We cast this task as seq2seqproblem by viewing a shuffled sentence as a sourcesentence, and the correctly ordered sentence as thetarget.
While word ordering is a somewhat synthetictask, it has two interesting properties for our pur-poses.
First, it is a task which plausibly requiressearch (due to the exponentially many possible or-derings), and, second, there is a clear hard constrainton output sequences, namely, that they be a permu-tation of the source sequence.
For both the baselineand BSO models we enforce this constraint at test-time.
However, we also experiment with constrain-ing the BSO model during training, as described inSection 4.2, by defining the succ function to only al-low successor sequences containing un-used wordsin the source sentence.For experiments, we use the same PTB dataset(with the standard training, development, and testsplits) and evaluation procedure as in Zhang andClark (2015) and later work, with performance re-ported in terms of BLEU score with the correctly or-dered sentences.
For all word-ordering experimentswe use 2-layer encoder and decoder LSTMs, eachwith 256 hidden units, and dropout with a rate of 0.2between LSTM layers.
We use simple 0/1 costs indefining the ?
function.We show our test-set results in Table 1.
We seethat on this task there is a large improvement at eachbeam size from switching to BSO, and a further im-provement from using the constrained model.Inspired by a similar analysis in Daume?
III andMarcu (2005), we further examine the relationshipbetween Ktr and Kte when training with ConBSOin Table 2.
We see that larger Ktr hurt greedy in-ference, but that results continue to improve, at leastinitially, when using a Kte that is (somewhat) biggerthan Ktr ?
1.1302Word Ordering (BLEU)Kte = 1 Kte = 5 Kte = 10seq2seq 25.2 29.8 31.0BSO 28.0 33.2 34.3ConBSO 28.6 34.3 34.5LSTM-LM 15.4 - 26.8Table 1: Word ordering.
BLEU Scores of seq2seq, BSO,constrained BSO, and a vanilla LSTM language model(from Schmaltz et al 2016).
All experiments above haveKtr = 6.Word Ordering Beam Size (BLEU)Kte = 1 Kte = 5 Kte = 10Ktr = 2 30.59 31.23 30.26Ktr = 6 28.20 34.22 34.67Ktr = 11 26.88 34.42 34.88seq2seq 26.11 30.20 31.04Table 2: Beam-size experiments on word ordering devel-opment set.
All numbers reflect training with constraints(ConBSO).Dependency Parsing We next apply our modelto dependency parsing, which also has hard con-straints and plausibly benefits from search.
Wetreat dependency parsing with arc-standard transi-tions as a seq2seq task by attempting to map froma source sentence to a target sequence of sourcesentence words interleaved with the arc-standard,reduce-actions in its parse.
For example, we attemptto map the source sentenceBut it was the Quotron problems that ...to the target sequenceBut it was @L SBJ @L DEP the Quotronproblems @L NMOD @L NMOD that ...We use the standard Penn Treebank dataset splitswith Stanford dependency labels, and the standardUAS/LAS evaluation metric (excluding punctua-tion) following Chen and Manning (2014).
Allmodels thus see only the words in the source and,when decoding, the actions it has emitted so far;no other features are used.
We use 2-layer encoderand decoder LSTMs with 300 hidden units per layerDependency Parsing (UAS/LAS)Kte = 1 Kte = 5 Kte = 10seq2seq 87.33/82.26 88.53/84.16 88.66/84.33BSO 86.91/82.11 91.00/87.18 91.17/87.41ConBSO 85.11/79.32 91.25/86.92 91.57/87.26Andor 93.17/91.18 - -Table 3: Dependency parsing.
UAS/LAS of seq2seq,BSO, ConBSO and baselines on PTB test set.
Andor isthe current state-of-the-art model for this data set (Andoret al 2016), and we note that with a beam of size 32 theyobtain 94.41/92.55.
All experiments above have Ktr = 6.and dropout with a rate of 0.3 between LSTM lay-ers.
We replace singleton words in the training setwith an UNK token, normalize digits to a singlesymbol, and initialize word embeddings for bothsource and target words from the publicly availableword2vec (Mikolov et al, 2013) embeddings.
Weuse simple 0/1 costs in defining the ?
function.As in the word-ordering case, we also experimentwith modifying the succ function in order to trainunder hard constraints, namely, that the emitted tar-get sequence be a valid parse.
In particular, we con-strain the output at each time-step to obey the stackconstraint, and we ensure words in the source areemitted in order.We show results on the test-set in Table 3.
BSOand ConBSO both show significant improvementsover seq2seq, with ConBSO improving most onUAS, and BSO improving most on LAS.
We achievea reasonable final score of 91.57 UAS, which lagsbehind the state-of-the-art, but is promising for ageneral-purpose, word-only model.Translation We finally evaluate our model on asmall machine translation dataset, which allows usto experiment with a cost function that is not 0/1,and to consider other baselines that attempt to mit-igate exposure bias in the seq2seq setting.
We usethe dataset from the work of Ranzato et al (2016),which uses data from the German-to-English por-tion of the IWSLT 2014 machine translation eval-uation campaign (Cettolo et al, 2014).
The datacomes from translated TED talks, and the datasetcontains roughly 153K training sentences, 7K devel-opment sentences, and 7K test sentences.
We use thesame preprocessing and dataset splits as Ranzato et1303Machine Translation (BLEU)Kte = 1 Kte = 5 Kte = 10seq2seq 22.53 24.03 23.87BSO, SB-?
23.83 26.36 25.48XENT 17.74 20.10 20.28DAD 20.12 22.25 22.40MIXER 20.73 21.81 21.83Table 4: Machine translation experiments on test set; re-sults below middle line are from MIXER model of Ran-zato et al (2016).
SB-?
indicates sentence BLEU costsare used in defining ?.
XENT is similar to our seq2seqmodel but with a convolutional encoder and simpler at-tention.
DAD trains seq2seq with scheduled sampling(Bengio et al, 2015).
BSO, SB-?
experiments abovehave Ktr = 6.al.
(2016), and like them we also use a single-layerLSTM decoder with 256 units.
We also use dropoutwith a rate of 0.2 between each LSTM layer.
We em-phasize, however, that while our decoder LSTM is ofthe same size as that of Ranzato et al (2016), our re-sults are not directly comparable, because we use anLSTM encoder (rather than a convolutional encoderas they do), a slightly different attention mechanism,and input feeding (Luong et al, 2015).For our main MT results, we set ?(y?
(k)1:t ) to1?SB(y?
(K)r+1:t, yr+1:t), where r is the last marginviolation and SB denotes smoothed, sentence-levelBLEU (Chen and Cherry, 2014).
This setting of ?should act to penalize erroneous predictions with arelatively low sentence-level BLEU score more thanthose with a relatively high sentence-level BLEUscore.
In Table 4 we show our final results and thosefrom Ranzato et al (2016).8 While we start with animproved baseline, we see similarly large increasesin accuracy as those obtained by DAD and MIXER,in particular when Kte > 1.We further investigate the utility of thesesequence-level costs in Table 5, which compares us-ing sentence-level BLEU costs in defining ?
withusing 0/1 costs.
We see that the more sophisti-cated sequence-level costs have a moderate effect onBLEU score.8Some results from personal communication.Machine Translation (BLEU)Kte = 1 Kte = 5 Kte = 100/1-?
25.73 28.21 27.43SB-?
25.99 28.45 27.58Table 5: BLEU scores obtained on the machine trans-lation development data when training with ?(y?
(k)1:t ) = 1(top) and ?(y?
(k)1:t ) = 1?SB(y?
(K)r+1:t, yr+1:t) (bottom), andKtr = 6.Timing Given Algorithm 1, we would expecttraining time to increase linearly with the size ofthe beam.
On the above MT task, our highly tunedseq2seq baseline processes an average of 13,038 to-kens/second (including both source and target to-kens) on a GTX 970 GPU.
For beams of size Ktr= 2, 3, 4, 5, and 6, our implementation processeson average 1,985, 1,768, 1,709, 1,521, and 1,458 to-kens/second, respectively.
Thus, we appear to payan initial constant factor of ?
3.3 due to the morecomplicated forward and backward passes, and thentraining scales with the size of the beam.
Becausewe batch beam predictions on a GPU, however, wefind that in practice training time scales sub-linearlywith the beam-size.6 ConclusionWe have introduced a variant of seq2seq and an as-sociated beam search training scheme, which ad-dresses exposure bias as well as label bias, andmoreover allows for both training with sequence-level cost functions as well as with hard constraints.Future work will examine scaling this approach tomuch larger datasets.AcknowledgmentsWe thank Yoon Kim for helpful discussions and forproviding the initial seq2seq code on which our im-plementations are based.
We thank Allen Schmaltzfor help with the word ordering experiments.
Wealso gratefully acknowledge the support of a GoogleResearch Award.ReferencesDaniel Andor, Chris Alberti, David Weiss, AliakseiSeveryn, Alessandro Presta, Kuzman Ganchev, Slav1304Petrov, and Michael Collins.
2016.
Globally normal-ized transition-based neural networks.
ACL.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In ICLR.Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and NoamShazeer.
2015.
Scheduled sampling for sequence pre-diction with recurrent neural networks.
In Advances inNeural Information Processing Systems, pages 1171?1179.Anders Bjo?rkelund and Jonas Kuhn.
2014.
Learningstructured perceptrons for coreference Resolution withLatent Antecedents and Non-local Features.
ACL,Baltimore, MD, USA, June.Mauro Cettolo, Jan Niehues, Sebastian Stu?ker, LuisaBentivogli, and Marcello Federico.
2014.
Report onthe 11th iwslt evaluation campaign.
In Proceedings ofIWSLT, 20014.Kai-Wei Chang, Hal Daume?
III, John Langford, andStephane Ross.
2015.
Efficient programmable learn-ing to search.
In Arxiv.Boxing Chen and Colin Cherry.
2014.
A system-atic comparison of smoothing techniques for sentence-level bleu.
ACL 2014, page 362.Danqi Chen and Christopher D Manning.
2014.
Afast and accurate dependency parser using neural net-works.
In EMNLP, pages 740?750.KyungHyun Cho, Bart van Merrienboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014.
On the propertiesof neural machine translation: Encoder-decoder ap-proaches.
Eighth Workshop on Syntax, Semantics andStructure in Statistical Translation.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, page 111.
Association forComputational Linguistics.Hal Daume?
III and Daniel Marcu.
2005.
Learning assearch optimization: approximate large margin meth-ods for structured prediction.
In Proceedings of theTwenty-Second International Conference on MachineLearning (ICML 2005), pages 169?176.Hal Daume?
III, John Langford, and Daniel Marcu.
2009.Search-based structured prediction.
Machine Learn-ing, 75(3):297?325.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive Subgradient Methods for Online Learningand Stochastic Optimization.
The Journal of MachineLearning Research, 12:2121?2159.Katja Filippova, Enrique Alfonseca, Carlos A Col-menares, Lukasz Kaiser, and Oriol Vinyals.
2015.Sentence compression by deletion with lstms.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 360?368.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural Comput., 9:1735?1780.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, pages 142?151.Association for Computational Linguistics.Brian Kingsbury.
2009.
Lattice-based optimizationof sequence classification criteria for neural-networkacoustic modeling.
In Acoustics, Speech and SignalProcessing, 2009.
ICASSP 2009.
IEEE InternationalConference on, pages 3761?3764.
IEEE.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of the Eighteenth International Confer-ence on Machine Learning (ICML 2001), pages 282?289.Yijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin.2015.
Transition-based syntactic linearization.
InProceedings of NAACL.Thang Luong, Hieu Pham, and Christopher D. Manning.2015.
Effective approaches to attention-based neuralmachine translation.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP 2015, pages 1412?1421.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositionality.In Advances in neural information processing systems,pages 3111?3119.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th annual meeting on association for computationallinguistics, pages 311?318.
Association for Computa-tional Linguistics.Vu Pham, The?odore Bluche, Christopher Kermorvant,and Je?ro?me Louradour.
2014.
Dropout improves re-current neural networks for handwriting recognition.In Frontiers in Handwriting Recognition (ICFHR),2014 14th International Conference on, pages 285?290.
IEEE.Marc?Aurelio Ranzato, Sumit Chopra, Michael Auli, andWojciech Zaremba.
2016.
Sequence level trainingwith recurrent neural networks.
ICLR.Ste?phane Ross, Geoffrey J. Gordon, and Drew Bagnell.2011.
A reduction of imitation learning and structuredprediction to no-regret online learning.
In Proceedingsof the Fourteenth International Conference on Artifi-cial Intelligence and Statistics, pages 627?635.Hasim Sak, Oriol Vinyals, Georg Heigold, Andrew W.Senior, Erik McDermott, Rajat Monga, and Mark Z.1305Mao.
2014.
Sequence discriminative distributed train-ing of long short-term memory recurrent neural net-works.
In INTERSPEECH 2014, pages 1209?1213.Allen Schmaltz, Alexander M Rush, and Stuart MShieber.
2016.
Word ordering without syntax.
arXivpreprint arXiv:1604.08633.Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio,Aaron C. Courville, and Joelle Pineau.
2016.
Build-ing end-to-end dialogue systems using generative hier-archical neural network models.
In Proceedings of theThirtieth AAAI Conference on Artificial Intelligence,pages 3776?3784.Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu,Maosong Sun, and Yang Liu.
2016.
Minimum risktraining for neural machine translation.
In Proceed-ings of the 54th Annual Meeting of the Association forComputational Linguistics, ACL 2016.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Ilya Sutskever, James Martens, and Geoffrey E Hinton.2011.
Generating text with recurrent neural networks.In Proceedings of the 28th International Conferenceon Machine Learning (ICML), pages 1017?1024.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Sys-tems (NIPS), pages 3104?3112.Subhashini Venugopalan, Marcus Rohrbach, JeffreyDonahue, Raymond J. Mooney, Trevor Darrell, andKate Saenko.
2015.
Sequence to sequence - videoto text.
In ICCV, pages 4534?4542.Oriol Vinyals, ?ukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Grammaras a foreign language.
In Advances in Neural Informa-tion Processing Systems, pages 2755?2763.Paul Voigtlaender, Patrick Doetsch, Simon Wiesler, RalfSchluter, and Hermann Ney.
2015.
Sequence-discriminative training of recurrent neural networks.In Acoustics, Speech and Signal Processing (ICASSP),2015 IEEE International Conference on, pages 2100?2104.
IEEE.Taro Watanabe and Eiichiro Sumita.
2015.
Transition-based neural constituent parsing.
Proceedings of ACL-IJCNLP.Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,Aaron C. Courville, Ruslan Salakhutdinov, Richard S.Zemel, and Yoshua Bengio.
2015.
Show, attend andtell: Neural image caption generation with visual at-tention.
In ICML, pages 2048?2057.Majid Yazdani and James Henderson.
2015.
Incremen-tal recurrent neural network dependency parser withsearch-based discriminative training.
In Proceedingsof the 19th Conference on Computational Natural Lan-guage Learning, (CoNLL 2015), pages 142?152.Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.2014.
Recurrent neural network regularization.CoRR, abs/1409.2329.Yue Zhang and Stephen Clark.
2011.
Syntax-basedgrammaticality improvement using ccg and guidedsearch.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, pages1147?1157.
Association for Computational Linguis-tics.Yue Zhang and Stephen Clark.
2015.
Discriminativesyntax-based word ordering for text generation.
Com-putational Linguistics, 41(3):503?538.Hao Zhou, Yue Zhang, and Jiajun Chen.
2015.
Aneural probabilistic structured-prediction model fortransition-based dependency parsing.
In Proceedingsof the 53rd Annual Meeting of the Association forComputational Linguistics, pages 1213?1222.1306
