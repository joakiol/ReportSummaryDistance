Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 68?75,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSemantically Rich Human-Aided Machine AnnotationMarjorie McShane, Sergei Nirenburg, Stephen Beale, Thomas O?HaraDepartment of Computer Science and Electrical EngineeringUniversity of Maryland Baltimore County1000 Hilltop Circle, Baltimore, Maryland, 21250   USA{marge, sergei, sbeale, tomohara}@umbc.eduAbstractThis paper describes a semantically rich,human-aided machine annotation systemcreated within the Ontological Semantics(OntoSem) environment using theDEKADE toolset.
In contrast to main-stream annotation efforts, this method ofannotation provides more information at alower cost and, for the most part, shiftsthe maintenance of consistency to the sys-tem itself.
In addition, each tagging effortnot only produces knowledge resourcesfor that corpus, but also leads to im-provements in the knowledge environ-ment that will better support subsequenttagging efforts.1 IntroductionCorpus tagging is a prerequisite for many machinelearning methods in NLP but has the drawbacks ofhigh cost, inter-annotator inconsistency and theinsufficient treatment of meaning.
A tagging ap-proach that strives to ameliorate all of these draw-backs is semantically rich, human-aided machineannotation (HAMA), implemented in the OntoSem(Ontological Semantics) environment using a tool-set called DEKADE: the Development, Evaluation,Knowledge Acquisition and Demonstration Envi-ronment of OntoSem.In brief, the OntoSem text analyzer takes as in-put open text and outputs a text-meaning represen-tation (TMR) that represents its meaning using anontologically grounded, language-independentmetalanguage (see Nirenburg and Raskin 2004).Since the processing leading up to the productionof TMR includes, in addition to semantic analysisproper, preprocessing (roughly, segmentation,treatment of named entities and morphology) andsyntactic analysis, the overall annotation of text inthis approach includes tags relating to all of theabove levels.
Since the typical input for analysis inour practice is genuine sentences, which are onaverage 25 words long and contain all manner ofcomplex phenomena, it is not uncommon for theautomatically generated TMRs to contain errors.These errors?which can occur at the level of pre-processing, syntactic analysis or semantic analy-sis?can be corrected manually using theDEKADE environment, yielding ?gold standard?output.
Making a human the final arbiter in theprocess means that such long-term complexities astreatment of metaphor, metonymy, PP-attachment,difficult cases of reference resolution and otherscan be resolved locally while we work on funda-mental, implementable automatic solutions.In this paper we describe the Onto-Sem/DEKADE environment for the creation ofgold standard TMRs, which supports the first everannotation effort that:?
produces structures that can be used as inputfor both text generators and general reason-ing systems: semantically rich representa-tions of the meaning of text written in alanguage-independent metalanguage; theserepresentations cover entities, propositions,relations, attributes, speaker attitudes, mo-dalities, polarity, discourse relations, time,reference relations, and more;?
produces semantic tagging of text largelyautomatically, thus making more realisticand affordable the tagging of large amountsof text in finite time;?
almost fully circumvents the pitfalls of man-ual tagging, including human tagger errorsand inconsistencies;?
produces richer semantic annotations thanmanual tagging realistically could, since ma-nipulating large and complex static knowl-68edge sources would be impossible for hu-mans if starting from scratch (i.e., our meth-odology effectively turns an essay questioninto a multiple choice one, with most of thecorrect answers already provided);?
incorporates humans as final arbiters for out-put of three stages of text analysis (preproc-essing, syntactic analysis and semanticanalysis), thus maximally leveraging theautomated capacity of the system but not re-quiring of it blanket coverage at this point inits development;?
promises to reduce, over time, the depend-ence on human input because an importantside effect of the operation of the human-assisted machine annotation approach is en-hancement of the static knowledge resources?
the lexicon and the ontology ?
underlyingthe OntoSem analyzer, so that the quality ofautomatic text analysis will grow as theHAMA system operates, leading to an everimproving quality of raw, unedited TMRs;?
(as a corollary to the previous point) be-comes more cost-efficient over time; and?
can be cost-effectively extended to otherlanguages (including less commonly taughtlanguages), with much less work than wasrequired for the first language since many ofthe necessary resources are language-independent.Our approach to text analysis is a hybrid ofknowledge-based and corpus-based, stochasticmethods.In the remainder of the paper we will briefly de-scribe the lay of the land in text annotation (Sec-tion 2), the OntoSem environment (Section 3), theDEKADE environment for creating gold-standardTMRs from automatically generated ones (Section4), the portability of OntoSem to other languages(Section 5), and the broader implications of thisR&D effort (Section 6).2 The Lay of the Land in AnnotationIn addition to the well-known bottlenecks of costand inconsistency, it is widely assumed that low-level (only syntactic or ?light semantic?)
tagging iseither sufficient or inevitable due to the complexityof semantic tagging.
Past and ongoing tagging ef-forts share this point of departure.Numerous projects have striven to achieve textannotation via a simpler task, like translation,sometimes assuming that one language has alreadybeen tagged (e.g., Pianta and Bentivogli 2003, andreferences therein).
But results of such efforts areeither of low quality, light semantic depth, or re-main to be reported.
Of significant interest is theporting of annotations across languages: for exam-ple, Yarowsky et al 2001 present a method forautomatic tagging of English and the projection ofthe tags to other languages; however, these tags donot include semantics.Post-editing of automatic annotation has beenpursued in various projects (e.g., Brants 2000, andMarcus et al 1993).
The latter group did an ex-periment early on in which they found that ?man-ual tagging took about twice as long as correcting[automated tagging], with about twice the inter-annotator disagreement rate and an error rate thatwas about 50% higher?
(Marcus et al 1993).
Thisconclusion supports the pursuit of automated tag-ging methods.
The difference between our workand the work in the above projects, however, isthat syntax for us is only a step in the progressiontoward semantics.Interesting time- and cost-related observationsare provided in Brants 2000 with respect to themanual correction of automated POS and syntactictagging of a German corpus (semantics is not ad-dressed).
Although these tasks took approximately50 seconds per sentence, with sentences averaging17.5 tokens, the actual cost in time and money putseach sentence at 10 minutes, by the time two tag-gers carry out the task, their results are compared,difficult issues are resolved, and taggers are trainedin the first place.
Notably, however, this effortused students as taggers, not professionals.
We, bycontrast, use professionals to check and correctTMRs and thus reduce to practically zero the train-ing time, the need for multiple annotators (pro-vided the size of a typical annotation task iscommensurate with those in current projects), andcostly correction of errors.Among past projects that have addressed se-mantic annotation are the following:1.
Gildea and Jurafsky (2002) created a stochas-tic system that labels case roles of predicates witheither abstract (e.g., AGENT, THEME) or domain-specific (e.g., MESSAGE, TOPIC) roles.
The system69trained on 50,000 words of hand-annotated text(produced by the FrameNet project).
When taskedto segment constituents and identify their semanticroles (with fillers being undisambiguated textualstrings) the system scored in the 60?s in precisionand recall.
Limitations of the system include itsreliance on hand-annotated data, and its reliance onprior knowledge of the predicate frame type (i.e., itlacks the capacity to disambiguate productively).Semantics in this project is limited to case-roles.2.
The goal of the ?Interlingual Annotation ofMultilingual Text Corpora?
project(http://aitc.aitcnet.org/nsf/iamtc/) is to create a syn-tactic and semantic annotation representationmethodology and test it out on six languages (Eng-lish, Spanish, French, Arabic, Japanese, Korean,and Hindi).
The semantic representation, however,is restricted to those aspects of syntax and seman-tics that developers believe can be consistentlyhandled well by hand annotators for many lan-guages.
The current stage of development includesonly syntax and light semantics ?
essentially, the-matic roles.3.
In the ACE project(http://www.ldc.upenn.edu/Projects/ACE/intro.html), annotators carry out manual semantic annotationof texts in English, Chinese and Arabic to createtraining and test data for research task evaluations.The downside of this effort is that the inventory ofsemantic entities, relations and events is very smalland therefore the resulting semantic representa-tions are coarse-grained: e.g., there are only fiveevent types.
The project description promises morefine-grained descriptors and relations amongevents in the future.4.
Another response to the insufficiency of syn-tax-only tagging is offered by the developers ofPropBank, the Penn Treebank semantic extension.Kingsbury et al 2002 report: ?It was agreed thatthe highest priority, and the most feasible type ofsemantic annotation, is coreference and predicateargument structure for verbs, participial modifiersand nominalizations?, and this is what is includedin PropBank.To summarize, previous tagging efforts thathave addressed semantics at all have covered onlya relatively small subset of semantic phenomena.OntoSem, by contrast, produces a far richer anno-tation, carried out largely automatically, within anenvironment that will improve over time and withuse.3 A Snapshot of OntoSemOntoSem is a text-processing environment thattakes as input unrestricted raw text and carries outpreprocessing, morphological analysis, syntacticanalysis, and semantic analysis, with the results ofsemantic analysis represented as formal text-meaning representations (TMRs) that can then beused as the basis for many applications (for details,see, e.g., Nirenburg and Raskin 2004, Beale et al2003).
Text analysis relies on:?
The OntoSem language-independent ontology,which is written using a metalanguage of de-scription and currently contains around 6,000concepts, each of which is described by an aver-age of 16 properties.?
An OntoSem lexicon for each language proc-essed, which contains syntactic and semanticzones (linked using variables) as well as calls forprocedural semantic routines when necessary.The semantic zone most frequently refers to on-tological concepts, either directly or with prop-erty-based modifications, but can also describeword meaning extra-ontologically, for example,in terms of modality, aspect, time, etc.
The cur-rent English lexicon contains approximately25,000 senses, including most closed-class itemsand many of the most frequent and polysemousverbs, as targeted by corpus analysis.
(An exten-sive description of the lexicon, formatted as a tu-torial, can be found at http://ilit.umbc.edu.)?
An onomasticon, or lexicon of proper names,which contains approximately 350,000 entries.?
A fact repository, which contains real-worldfacts represented as numbered ?remembered in-stances?
of ontological concepts (e.g., SPEECH-ACT-3366 is the 3366th instantiation of the con-cept SPEECH-ACT in the world model constructedduring the processing of some given text(s)).?
The OntoSem syntactic-semantic analyzer,which covers preprocessing, syntactic analysis,semantic analysis, and the creation of TMRs.
In-stead of using a large, monolithic grammar of alanguage, which leads to ambiguity and ineffi-ciency, we use a special lexicalized grammarcreated on the fly for each input sentence (Beale,et.
al.
2003).
Syntactic rules are generated fromthe lexicon entries of each of the words in thesentence, and are supplemented by a small in-ventory of generalized rules.
We augment this70basic grammar with transformations triggered bywords or features present in the input sentence.?
The TMR language, which is the metalanguagefor representing text meaning.Creating gold standard TMRs involves runningtext through the OntoSem processors and check-ing/correcting the output after three stages ofanalysis: preprocessing, syntactic analysis, andsemantic analysis.
These outputs canbe viewed and edited as text or as vis-ual representations through theDEKADE interface.
Although the goldstandard TMR itself does not reflectthe results of preprocessing or syntacticanalysis, the gold standard results ofthose stages of processing are stored inthe system and can be converted into amore traditional annotation format.4 TMRs in DEKADETMRs represent propositions con-nected by discourse relations (sincespace permits only the briefest of descriptions, in-terested readers are directed to Nirenburg andRaskin 2004, Chapter 6 for details).
Propositionsare headed by instances of ontological concepts,parameterized for modality, aspect, propositiontime, overall TMR time, and style.
Each proposi-tion is related to other instantiated concepts usingontologically defined relations (which include caseroles and many others) and attributes.
Coreferencelinks form an additional layer of linking betweeninstantiated concepts.
OntoSem microtheories de-voted to modality, aspect, time, style, reference,etc., undergo iterative extensions and improve-ments in response to system needs as diagnosedduring the processing of actual texts.We use the following sentence to walk throughthe processes of automatically generating TMRsand viewing/editing those TMRs to create a gold-standard annotated corpus.The Iraqi government has agreed to letU.S.
Representative Tony Hall visit thecountry to assess the humanitarian crisis.Preprocessor.
The preprocessor identifies theroot word, part of speech and morphological fea-tures of each word; recognizes sentence bounda-ries, named entities, dates, times and numbers; andfor named entities, determines the ontological type(i.e.
HUMAN, PLACE, ORGANIZATION, etc.)
of theentity as well as its subparts (e.g., the first, last,and middle names of a person).
For the semi-automatic creation of gold standard TMRs, muchambiguity can be removed at small cost by allow-ing people to correct spurious part-of-speech tags,number and date boundaries, etc., through theDEKADE environment at the preprocessor stage(see Figure 1).
Clicking on w+ permits a new POStag/analysis, and clicking on w-, the more commonaction, removes spurious analyses.
Preprocessorcorrection is a conceptually simple and logisticallyfast task that can be carried out by less trained, andtherefore less expensive, annotators.Figure 1.
Preprocesor Output Editor.Syntax.
Syntax output can be viewed and ed-ited in text or graphic form.
The graphicviewer/editor presents the sentence using the tradi-tional metaphor of color-coded labeled arcs.Mouse clicks show the components of arcs, permitarcs to be deleted along with the orphans theywould leave, allow for the edges of arcs to bemoved, etc.
(no graphic of the syntax or semanticsbrowsers/editors are provided due to space con-straints).One common error in syntax output is spuriousparses due to contextually incorrect POS or featureanalysis.
As shown above, this can be fixed fromthe outset by correcting the preprocessor.
How-ever, since the preprocessor will always containspurious analyses that can usually be removedautomatically by the syntactic analyzer, it is notnecessarily most time efficient to always start withpreprocessor editing.
A more difficult, long-termresearch issue is genuine ambiguity caused, forexample, by PP-attachments.
While such issues are71not likely to be solved computationally in the shortterm, they can be easily resolved when humans areused as the final arbiters in the creation of goldstandard TMRs.When the correct parse is not included in thesyntactic output, either the necessary lexicalknowledge is lacking (i.e.
there is an unknownword or word sense), or an unknown grammaticalconstruction has been used.
While the syntax-editing interface permits spot-correction of theproblem by the addition of the necessary arc(s), amore fundamental knowledge-building approach isgenerally preferred ?
except when the input is non-standard, in which case systemic modifications areavoided.Semantics.
Within the OntoSem environment,there are two stages of text-meaning representa-tions (TMRs): basic and extended.
The basic TMRshows the basic ontological mappings and depend-ency structure, whereas the extended TMR showsthe results of procedural semantics, including ref-erence resolution, reasoning about time relations,etc.
The basic and extended stages of TMR crea-tion can be viewed and edited separately withinDEKADE.TMRs can be viewed and edited in text formator graphically.
In the latter, concepts are shown asnodes and properties are shown as lines connectingthem.
A pretty-printed view of the textual extendedTMR for our sample sentence, repeated for con-venience, is as follows (concept names are in smallcaps; instance numbers are appended to them).The Iraqi government has agreed to let U.S.Representative Tony Hall visit the country toassess the humanitarian crisis.AGREE-268textpointer agreeTHEME   MODALITY-200AGENT   GOVERNMENTAL-ORGANIZATION-41TIME   (< FIND-ANCHOR-TIME)GOVERNMENTAL-ORGANIZATION-41textpointer governmentRELATION  NATION-56AGENT-OF  AGREE-268NATION-56textpointer IraqRELATION  GOVERNMENTAL-ORGANIZATION-41MODALITY-200textpointer letTYPE    permissiveSCOPE   TRAVEL-EVENT-272VALUE       1TRAVEL-EVENT-272textpointer visitAGENT   SENATOR-4471DESTINATION NATION-57PURPOSE  EVALUATE-69SCOPE-OF  MODALITY-200SENATOR-447textpointer Representative Tony Hall2REPRESENTATIVE-OF NATION-40NATION-40textpointer U.S.REPRESENTED-BY SENATOR-447NATION-57textpointer countryCOREFER  NATION-56EVALUATE-69AGENT   SENATOR-447THEME   DISASTER-EVENT-2DISASTER-EVENT-2BENEFICIARY SET-23THEME-OF  EVALUATE-69SET-23MEMBER-TYPE HUMAN-1342BENEFICIARY-OF DISASTER-EVENT-2Within the graphical browser, clicking on conceptnames or properties permits them to be deleted,edited, or permits new ones to be added.
It alsoshows the expansion of any concept in text format.Evaluating and editing the semantic output isthe most challenging aspect of creating gold stan-dard TMRs, since creating formal semantic repre-sentations is arguably one of the most difficulttasks in all of NLP.
If a knowledge engineer de-termines that some aspect of the semantic repre-sentation is incorrect, the problems can becorrected locally or by editing the knowledge re-sources and rerunning the analyzer.
Local correc-tions are used, for example, in cases of metaphorand metonymy, which we do not record in ourknowledge resources (we are working on a mi-crotheory of tropes but it is not yet implemented).In all other cases, resource supplementation is pre-ferred; it can be carried out either immediately orthe problem can be fixed locally, in which case arequest will be sent to a knowledge acquirer tocarry out the necessary resource enhancements.1 The concept SENATOR is defined as a member of a legislativeassembly.2 Collocations of SOCIAL-ROLE + personal name are handled bythe preprocessor.72Striking the balance between short-term goals(a gold standard TMR for the given text) and long-term goals (better analysis of any text in the future)is always a challenge.
For example, if a text con-tained the word grass in the sense of ?marijuana?,and if the lexicon lacked the word ?grass?
alto-gether, we would want to acquire the meaning?green lawn cover?
as well; however, doing thiswithout constraint could mean getting boggeddown by knowledge acquisition (as with the doz-ens of idiomatic uses of ?have?)
at the expense ofactually producing gold-standard TMRs.
There arealso cases in which a local solution to semanticrepresentation is very easy whereas a fundamental,machine-reproducible solution is very difficult.Consider the case of relative expressions, like re-spective and respectively, as used in Smith andMatthews pleaded innocent and guilty, respec-tively.
Manually editing a TMR such that the ap-propriate properties are linked to their heads isquite simple, whereas writing a program for thisnon-trivial case of reference resolution is not.Thus, in some cases we push through gold standardTMR production while keeping track of ?
and de-veloping as time permits ?
the more difficult as-pects of text processing that will enhance TMRoutput in the future.The gold standard TMR for the sentence dis-cussed at length here was produced with only afew manual corrections: changing two part ofspeech tags and selecting the correct sense for oneword.
Work took less than the 10 minutes reportedby Brants 2000 for their non-semantic tagging.5 Porting to Other LanguagesRecently the need for tagged corpora for lesscommonly taught languages has received muchattention.
While our group is not currently pursu-ing such languages, it has in the past: TMRs havebeen automatically generated for languages such asChinese, Georgian, Arabic and Persian.
We take ashort tangent to explain how OntoSem/DEKADEcan be extended, at relatively low cost, to the anno-tation of other languages ?
showing yet anotherway in which this approach to annotation reachesbeyond the results for any given text or corpus.Whereas it is typical to assume that lexicons arelanguage-specific whereas ontologies are lan-guage-independent, most aspects of the semanticstructures (sem-strucs) of OntoSem lexicon entriesare actually language-independent, apart from thelinking of specific variables to their counterparts inthe syntactic structure.
Stated differently, if weconsider sem-strucs ?
no matter what lexicon theyoriginate from ?
to be building blocks of the repre-sentation of word meaning (as opposed to conceptmeaning, as is done in the ontology), then we un-derstand why building a large OntoSem lexicon forEnglish holds excellent promise for future portingto other languages: most of the work is alreadydone.
This conception of cross-linguistic lexicondevelopment derives in large part from the Princi-ple of Practical Effability (Nirenburg and Raskin2004), which states that what can be expressed inone language can somehow be expressed in allother languages, be it by a word, a phrase, etc.
(Ofcourse, it is not necessary that every nuancedmeaning be represented in the lexicon of everylanguage and, as such, there will be some differ-ences in the lexical stock of each language: e.g.,whereas German has a word for white horse whichwill be listed in its lexicon, English will not havesuch a lexical entry, the collocation white horsebeing treated compositionally.)
We do not intendto trivialize the fact that creating a new lexicon is alot of work.
It is, however, compelling to considerthat a new lexicon of the same quality of our On-toSem English one could be created with littlemore work than would be required to build a typi-cal translation dictionary.
In fact, we recently car-ried out an experiment on porting the Englishlexicon to Polish and found that a) much of it couldbe done semi-automatically and b) the manualwork for a second language is considerably lessthan for the first language (for further discussion,see McShane et al 2004).To sum up, the OntoSem ontology and theDEKADE environment are equally suited to anylanguage, and the OntoSem English lexicon andanalyzer can be configured to new languages withmuch less work required than for their initial de-velopment.
In short, semantic-rich tagging throughTMR creation could be a realistic option for lan-guages other than English.6 DiscussionLack of interannotator agreement presents a sig-nificant problem in annotation efforts (see, e.g.,Marcus et al 1993).
With the OntoSem semi-automated approach, there is far less possibility of73interannotator disagreement since people only cor-rect the output of the analyzer, which is responsi-ble for consistent and correct deployment of thelarge and complex static resources:  if the knowl-edge bases are held constant, the analyzer will pro-duce the same output every time, ensuringreproducibility of the annotation.Evaluation of annotation has largely centeredupon the demonstration of interannotator agree-ment, which is at best a partial standard for evalua-tion.
On the one hand, agreement amongannotators does not imply the correctness of theannotations: all annotators could be mistaken, par-ticularly as students are most typically recruited forthe job.
On the other hand, there are cases of genu-ine ambiguity, in which more than one annotationis equally correct.
Such ambiguity is particularlycommon with certain classes of referring expres-sions, like this and that, which can refer to chunksof text ranging from a noun phrase to many para-graphs.
Genuine ambiguity in the context of corpustagging has been investigated by Poesio and Art-stein (ms.), among others, who conclude, reasona-bly, that a system of tags must permit multiplepossible correct coreference relations and that it isuseful to evaluate coreference based on corefer-ence chains rather than individual entities.The abovementioned evidence suggests the needfor ever more complex evaluation metrics whichare costly to develop and deploy.
In fact, evalua-tion of a complex tagging effort will be almost ascomplex as the core work itself.
In our case, TMRsneed to be evaluated not only for their correctnesswith respect to a given state of knowledge re-sources but also in the abstract.
Speed of goldstandard TMR creation must also be evaluated, aswell as the number of mistakes at each stage ofanalysis, and the effect that the correction of outputat one stage has on the next stage.
No methods orstandards for such evaluation are readily availablesince no work of this type has ever been carriedout.In the face of the usual pressures of time andmanpower, we have made the programmatic deci-sion not to focus on all types of evaluation but,rather, to concentrate our evaluation metrics on thecorrectness of the automated output of the system,the extent to which manual correction is needed,and the depth and robustness of  our knowledgeresources (see Nirenburg et al 2004 for our firstevaluation effort).
We do not deny the ultimatedesirability of additional aspects of evaluation inthe future.The main source of variation among knowledgeengineers within our approach lies not in review-ing/editing annotations as such, but in building theknowledge sources that give rise to them.
To takean actual example we encountered: one member ofour group described the phrase weapon of massdestruction in the lexicon as BIOLOGICAL-WEAPONor CHEMICAL-WEAPON, while another described itas a WEAPON with the potential to kill a very largenumber of people/animals.
While both of these arecorrect, they focus on different salient aspects ofthe collocation.
Another example of potential dif-ferences at the knowledge level has to do withgrain size: whereas one knowledge engineer re-viewing a TMR might consider the current lexicalmapping of neurosurgeon to SURGEON perfectlyacceptable,  another might consider that this grainsize is too rough and that, instead, we need a newconcept NEUROSURGEON, whose special propertiesare ontologically defined.
Such cases are to be ex-pected especially as we work on new specializeddomains which put greater demands on the depthof knowledge encoded about relevant concepts.There has been some concern that manual edit-ing of automated annotation can introduce bias.Unfortunately, completely circumventing bias insemantic annotation is and will remain impossiblesince the process involves semantic interpretation,which often differs among individuals from theoutset.
As such, even agreements among annota-tors can be questioned by a third (fourth, etc.
)party.At the present stage of development, the TMRtogether with the static (ontology, lexicons) anddynamic (analyzer) knowledge sources that areused in generating and manipulating it, alreadyprovide substantial coverage for a broad variety ofsemantic phenomena and represent in a compactway practically attainable solutions for most issuesthat have concerned the computational linguisticsand NLP community for over fifty years.
OurTMRs have been used as the substrate for ques-tion-answering, MT, knowledge extraction, andwere also used as the basis for reasoning in thequestion-answering system AQUA, where theysupplied knowledge to enable the operation of theJTP (Fikes et al, 2003) reasoning module.We are creating a database of TMRs pairedwith their corresponding sentences that we believe74will be a boon to machine learning research.
Re-peatedly within the ML community, the creation ofa high quality dataset (or datasets) for a particulardomain has sparked development of applications,such as  learning semantic parsers, learning lexicalitems, learning about the structure of the underly-ing domain of discourse, and so on.
Moreover, asthe quality of the raw TMRs increases due to gen-eral improvements to the static resources (in part,as side effects of the operation of the HAMA proc-ess) and processors (a long-term goal), the netbenefit of this approach will only increase, as theproduction rate of gold-standard TMRs will in-crease thus lowering the costs.TMRs are a useful medium for semantic repre-sentation in part because they can capture any con-tent in any language, and even content notexpressed in natural language.
They can, for ex-ample, be used for recording the interim and finalresults of reasoning by intelligent agents.
We fullyexpect that, as the actual coverage in the ontologyand the lexicons and the quality of semantic analy-sis grows, the TMR format will be extended to ac-commodate these improvements.
Such anextension, we believe, will largely involve move-ment toward a finer grain size of semantic descrip-tion, which the existing formalism should readilyallow.
The metalanguage of TMRs is quite trans-parent, so that the task of converting them into adifferent representation language (e.g., OWL)should not be daunting.ReferencesStephen Beale, Sergei Nirenburg and MarjorieMcShane.
2003.
Just-in-time grammar.
Proceedingsof the 2003 International Multiconference in Com-puter Science and Computer Engineering.
Las Ve-gas, Nevada.Thorsten Brants.
2000.
Inter-annotator agreement for aGerman newspaper corpus.
LREC-2000.
Athens,Greece.Richard Fikes, Jessica Jenkins and Gleb Frank.
2003.JTP: A system architecture and component library forhybrid reasoning.
Proceedings of the Seventh WorldMulticonference on Systemics, Cybernetics, and In-formatics.
Orlando, Florida, USA.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguistics28:3, 245-288.Paul Kingsbury, Martha Palmer and Mitch Marcus.2002.
Adding semantic annotation to the Penn Tree-Bank.
(http://www.cis.upenn.edu/~ace/HLT2002-propbank.pdf.
)Marcus, Mitchell P., Beatrice Santorini and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics 19.Marjorie McShane, Margalit Zabludowski, Sergei Ni-renburg and Stephen Beale.
2004.
OntoSem andSIMPLE: Two multi-lingual world views.
Proceed-ings of ACL-2004 Workshop on Text Meaning andInterpretation.
Barcelona, Spain.Sergei Nirenburg, Stephen Beale and MarjorieMcShane.
2004.
Evaluating the performance of theOntoSem semantic analyzer.
Proceedings of the ACLWorkshop on Text Meaning Representation.
Barce-lona, Spain.Sergei Nirenburg and Victor Raskin.
2004.
OntologicalSemantics.
The MIT Press.Emanuele Pianta and Luisa Bentivogli.
2003.
Transla-tion as annotation.
Proceedings of the AI*IA 2003Workshop "Topics and Perspectives of Natural Lan-guage Processing in Italy."
Pisa, Italy.Massimo Poesio and Ron Artstein.
2005.
The reliabilityof anaphoric annotation, reconsidered: Taking ambi-guity into account.
Proceedings of the ACL 2005Workshop ?Frontiers in Corpus Annotation II, Pie inthe Sky?.David Yarowsky, Grace Ngai and Richard Wicen-towski.
2001.
Inducing multilingual text analysistools via robust projection across aligned corpora.Proceedings of HLT 2001, First International Con-ference on Human Language Technology Research,San Diego, California, USA.75
