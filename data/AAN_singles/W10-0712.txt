Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 71?79,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAnnotating Large Email Datasets for Named Entity Recognition withMechanical TurkNolan Lawson, Kevin Eustice,Mike PerkowitzMeliha Yetisgen-YildizKiha Software Biomedical and Health Informatics100 South King Street, Suite 320 University of WashingtonSeattle, WA 98104 Seattle, WA 98101{nolan,kevin,mikep}@kiha.com melihay@u.washington.eduAbstractAmazon's  Mechanical  Turk service  has  beensuccessfully applied to many natural languageprocessing tasks.
However, the task of namedentity recognition presents unique challenges.In  a  large  annotation  task  involving  over20,000 emails, we demonstrate that a compet?itive bonus system and inter?annotator agree?ment  can  be  used  to  improve the  quality  ofnamed  entity  annotations  from  MechanicalTurk.
We also build several statistical namedentity  recognition  models  trained  with  theseannotations, which compare favorably to sim?ilar models trained on expert annotations.1    IntroductionIt  is well known that the performance of manymachine learning systems is heavily determinedby the size and quality of the data used as inputto the training algorithms.
Additionally, for cer?tain applications in natural language processing(NLP),  it  has been noted that  the particular  al?gorithms or feature sets used tend to become ir?relevant  as  the  size  of  the  corpus  increases(Banko and Brill 2001).
It is therefore not sur?prising that obtaining large annotated datasets isan  issue  of  great  practical  importance  for  theworking  researcher.
Traditionally,  annotatedtraining data have been provided by experts  inthe field or the researchers themselves, often atgreat  costs  in  terms  of  time  and  money.
Re?cently,  however,  attempts  have  been  made  toleverage  non?expert  annotations  provided  byAmazon's Mechanical  Turk (MTurk) service tocreate large training corpora at a fraction of theusual costs (Snow et al 2008).
The initial resultsseem promising, and a new avenue for enhancingexisting  sources  of  annotated  data  appears  tohave been opened.Named entity recognition (NER) is one of themany fields of NLP that rely on machine learn?ing  methods,  and  therefore  large  training  cor?pora.
Indeed, it is a field where more is almostalways better, as indicated by the traditional useof named entity gazetteers (often culled from ex?ternal sources) to simulate data that would havebeen inferred from a larger training set (Minkovet al 2005; Mikheev et al 1999).
Therefore, itappears to be a field that could profit from theenormous  bargain?price  workforce  availablethrough MTurk.It  is  not  immediately  obvious,  though, thatMTurk is well?suited for the task of NER annota?tion.
Commonly,  MTurk has been used for theclassification  task  (Snow et  al.
2008) or  forstraightforward data entry.
However, NER doesnot fit well into either of these formats.
As poin?ted out by Kozareva (2006), NER can be thoughtof as a composition of two subtasks: 1) determin?ing the start and end boundaries of a textual en?tity, and 2) determining the label of the identifiedspan.
The  second  task  is  the  well?understoodclassification  task,  but  the  first  task  presentssubtler  problems.
One  is  that  MTurk's  form?based user interface is inappropriate for the taskof identifying textual spans.
Another problem isthat  MTurk's fixed?fee payment system encour?ages  low  recall  on  the  part  of  the  annotators,since they receive the same pay no matter howmany entities they identify.This  paper addresses  both of these problemsby  describing  a custom  user interface and com?petitive  payment  system  that  together  create  afluid user  experience  while  encouraging  high?quality  annotations.
Further,  we demonstratethat MTurk successfully scales to the task of an?notating  a  very  large  set  of  documents  (over20,000), with  each document annotated by mul?tiple  workers.
We  also present  a  system  forresolving  inter?annotator  conflicts  to  create  thefinal  training  corpus,  and  determine  the  idealagreement threshold to maximize precision andrecall  of  a  statistical  named  entity  recognitionmodel.
Finally,  we  demonstrate  that  a  model71trained on our  corpus is on par with one trainedfrom  expert  annotations,  when  applied  to  alabeled test set.2    Related WorkMechanical Turk is a virtual market in which anyrequester can post tasks that are simple for hu?mans  but  difficult  for  computers.
MTurk hasbeen adopted  for  a  variety of  uses  both in  in?dustry and academia from user studies (Kittur etal.
2008) to image labeling (Sorokin and Forsyth2008).
In March 2007, Amazon claimed the userbase of  MTurk consisted of over 100,000 usersfrom 100 countries (Pontin 2007).In  the  scope  of  this  paper,  we  examine  thefeasibility of  MTurk in creating large?scale cor?pora for training statistical named entity recogni?tion models.
However, our work was not the firstapplication of MTurk in the NLP domain.
Snowet al (2008) examined the quality of labels cre?ated by MTurk workers for various NLP tasks in?cluding word sense disambiguation, word simil?arity,  text  entailment,  and  temporal  ordering.Since  the  publication  of  Snow  et  al.
?s  paper,MTurk has  become increasingly  popular  as  anannotation tool for NLP research.
Examples in?clude Nakov?s work on creating a manually an?notated resource for noun?noun compound inter?pretation based on paraphrasing verbs by MTurk(Nakov  2008)  and  Callison?Burch?s  machinetranslation evaluation study with MTurk (Callis?on?Burch 2009).
In  contrast  to  the  existing  re?search, we both evaluated the quality of corporagenerated by MTurk in different named entity re?cognition  tasks  and explored  ways  to  motivatethe workers to do higher quality work.
We be?lieve  the  experiences  we  present  in  this  paperwill  contribute  greatly  to  other  researchers  asthey design similar large?scale annotation tasks.3    General Problem DefinitionNamed entity recognition (NER) is a well?knownsubtask of information extraction.
Traditionally,the task has been based on identifying words andphrases that refer to various entities of interest,including persons,  locations,  and  organizations,(Nadeau and Sekine 2007).
The problem is usu?ally posed as a sequence labeling task similar tothe  part?of?speech  (POS) tagging  or  phrase?chunking tasks,  where  each  token in  the  inputtext corresponds to a label in the output, and  issolved with  sequential  classification algorithms(such as CRF, SVMCMM, or MEMM).Previous works have tackled NER within thebiomedical domain (Settles 2004), newswire do?main (Grishman and Sundheim 1996), and emaildomain (Minkov et al 2005).
In this paper, wefocus on extracting entities from email text.It  should  be noted that  email  text  has  manydistinctive features that create a unique challengewhen applying NER.
For one, email text tends tobe more informal  than either  newswire  or  bio?medical  text,  which  reduces  the  usefulness  oflearned features that depend on patterns of capit?alization and spelling.
Also,  the choice of cor?pora in email text is  particularly important.
Asemail corpora tend to come from either a singlecompany (e.g., the  Enron Email  Dataset1)  or  asmall group of people (e.g., the Sarah Palin emailset2), it is easy to build a classifier that overfitsthe data.
For instance, a classifier trained to ex?tract  personal  names  from Enron emails  mightshow an especially high preference to words suchas ?White,?
?Lay,?
and ?Germany,?
because theycorrespond to the names of Enron employees.Within the newswire and biomedical domains,such overfitting may be benign or actually bene?ficial, since documents in those domains tend todeal with a relatively small and pre?determinedset of named entities (e.g., politicians and largecorporations for newswire text, gene and proteinnames  for  biomedical  text).
For  NER  in  theemail domain, however, such overfitting is unac?ceptable.
The personal nature of emails ensuresthat they will almost always contain references topeople, places, and organizations not covered bythe training data.
Therefore, for the classifier tobe useful on any spontaneous piece of email text,a large, heterogeneous training set is desired.To achieve this effect, we chose four differentsources of unlabeled email text to be annotatedby the Mechanical  Turk workers for input  intothe training algorithms:1.
The Enron Email Dataset.2.
The  2005  TREC   Public  Spam Corpus(non?spam only).33.
The 20 Newsgroups Dataset.44.
A private mailing list for synthesizer afi?cionados called ?Analogue Heaven.
?4   Mechanical Turk for NERAs described previously, MTurk is not explicitlydesigned for NER tasks.
Because of this, we de?1 http://www.cs.cmu.edu/~enron/2 http://palinemail.crivellawest.net/3 http://plg.uwaterloo.ca/~gvcormac/treccorpus/4 http://people.csail.mit.edu/jrennie/20Newsgroups/72cided to build a custom user interface and bonuspayment system that largely circumvents the de?fault  MTurk web interface and instead performsits operations through the MTurk Command LineTools.5  Additionally, we built a separate set oftools designed to determine the ideal number ofworkers to assign per email.4.1    User InterfaceIn order to adapt the task of NER annotation tothe  Mechanical  Turk  format,  we  developed  aweb?based graphical user interface using JavaS?cript that allowed the user to select a span of textwith the mouse cursor and choose different cat?egories of entities from a dropdown menu.
Theinterface also used simple tokenization heuristicsto divide the text into highlightable spans and re?solve  partial  overlaps  or  double?clicks  into  thenext largest span.
For instance, highlighting theword ?Mary?
from ?M?
to ?r?
would result in theentire word being selected.Each Human Intelligence Task (or HIT, a unitof payable work in the Mechanical Turk system)presented the entire subject and body of an emailfrom one of the four corpora.
To keep the HITsat a reasonable size, emails with bodies havingless than 60 characters or more than 900 charac?ters were omitted.
The average email length, in?cluding both subject and body, was 405.39 char?acters.For the labeling task, we chose three distinctentity  types  to  identify:  PERSON,  LOCATION,  andORGANIZATION.
To reduce potential worker confu?sion  and  make  the  task  size  smaller,  we  alsobroke up each individual HIT by entity type, sothe user only had to concentrate on one at a time.For the  PERSON and  LOCATION entity types, wenoticed during initial tests that there was a user5 http://mturkclt.sourceforge.nettendency to  conflate  unnamed references  (suchas  ?my  mom?
and  ?your  house?)
with  truenamed references.
Because NER is intended tobe limited only to named entities (i.e., referencesthat contain proper nouns), we asked the users todistinguish  between  ?named?
and  ?unnamed?persons and locations, and to tag both separately.The inclusion of unnamed entities was intendedto keep their named counterparts pure and undi?luted; the unnamed entities were discarded afterthe annotation process was complete.
The samemechanism  could  have  been  used  for  theORGANIZATION entity type, but the risk of unnamedreferences seemed smaller.Initially, we ran a small trial with a base rateof $0.03 for each HIT.
However, after compilingthe results  we noticed that  there was a generaltendency for the workers to under?tag the entit?ies.
Besides outright freeloaders (i.e., workerswho  simply  clicked  ?no  entities?
each  time),there were also many who would highlight  thefirst one or two entities, and then ignore the restof the email.This may have been due to a misunderstandingof the HIT instructions, but we conjectured that adeeper reason was that  we were paying a baserate regardless of the number of entities identi?fied.
Ideally, a HIT with many entities to high?light  should pay more than  a  HIT with fewer.However, the default fixed?rate system was pay?ing the same for both, and the workers were re?sponding to such an inflexible incentive systemaccordingly.
To remedy this  situation,  we  setabout to create a payment system that would mo?tivate higher recall  on entity?rich emails,  whilestill  discouraging  the  opposite  extreme  of  ran?dom over?tagging.Fig.
1: Sample of the interface presented to workers.734.2    Bonus Payment SystemMechanical Turk provides two methods for pay?ing  workers:  fixed  rates  on  each  HIT  and  bo?nuses to individual workers for especially goodwork.
We chose to  leverage these  bonuses  toform the core of our payment system.
Each HITwould pay a base rate of $0.01, but each taggedentity   could  elicit  a  bonus  of  $0.01?$0.02.PERSON entities  paid  $0.01,  while  LOCATION andORGANIZATION entities paid $0.02 (since they wererarer).To ensure quality and discourage over?tagging,bonuses for each highlighted entity were limitedbased  on  an  agreement  threshold  with  otherworkers.
This threshold was usually set such thata majority agreement was required, which was anarbitrary  decision we made  in  order  to  controlcosts.
The terms of the bonuses were explainedin detail in the instructions page for each HIT.Additionally, we decided to leverage this bo?nus system to encourage improvements in work?er performance over time.
Since the agreed?uponspans that elicited bonuses were assumed to bemostly correct, we realized we could give feed?back to the workers on these entities to encour?age similar performance in the future.In general,  worker  bonuses  are  a  mysteriousand poorly understood motivational mechanism.Our  feedback  system  attempted  to  make  thesebonuses more predictable and transparent.
Thesystem we built uses Amazon's ?NotifyWorkers?REST  API  to  send  messages  directly  to  theworkers' email accounts.
Bonuses were batchedon a daily basis, and the notification emails gavea summary description of the day's bonuses.Both the UI and the bonus/notification systemwere works in progress that were continually re?fined based on comments from the worker com?munity.
We  were  pleasantly  surprised  to  findthat,  throughout  the  annotation  process,  theMechanical Turk workers were generally enthu?siastic about the HITs, and also interested in im?proving the quality of their annotations.
Out of169,156 total HITs, we received 702 commentsfrom 140 different workers, as well as over 50email responses and a dedicated thread at Turk?erNation.com6.
Most of the feedback was posit?ive, and negative feedback was almost solely dir?ected at the UI.
Based on their comments, wecontinually  tweaked  and  debugged  the  UI  andHIT instructions, but kept the basic structure ofthe bonus system.4.3    Worker DistributionWith the bonus system in place, it was still ne?cessary to determine the ideal number of workersto  assign  per  email.
Previously,  Snow  et al(2008) used expert annotations to find how manyMechanical  Turk workers could ?equal?
an ex?pert in terms of annotation quality.
Because welacked expert  annotations,  we developed an al?ternative system to determine the ideal number ofworkers  based  purely  on  inter?annotator  agree?ment.As described in the previous section, the mostsignificant problem faced with our HITs was thatof low recall.
Low precision was generally notconsidered to be a problem, since, with enoughannotators,  inter?annotator  agreement  could  al?ways be set arbitrarily high in order to weed outfalse positives.
Recall, on the other hand, couldbe consistently expected to improve as more an?notators were added to the worker pool.
There?fore, the only problem that remained was to cal?culate the marginal utility (in terms of recall) ofeach additional annotator assigned to an email.In order to estimate this marginal recall  gainfor each entity type, we first ran small initial testswith a relatively large number of workers.
Fromthese results, we took all the entities identified byat least two workers and set  those aside as thegold standard annotations;  any  overlapping an?notations  were  collapsed  into  the  larger  one.Next, for each  n number of workers between 2and the size of the entire worker pool, we ran?domly sampled n workers from the pool, re?cal?culated the entities based on agreement from atleast two workers within that group, and calcu?lated the recall relative to the gold standard an?notation.
The threshold of 2 was chosen arbitrar?ily for the purpose of this experiment.From this data we generated a marginal recallcurve  for  each  entity  type,  which  roughly  ap?proximates how many workers are required peremail  before  recall  starts  to  drop  off  signific?antly.
As expected, each graph shows a plateau?like behavior as the number of workers increases,but some entity types reach their plateau earlier6 http://turkers.proboards.com/index.cgi?action=display&board=everyoneelse&thread=3177In?recognition?of?your?performance,?you?were?awarded?a?bonus?of?$0.5?($0.02x25)?for?catching?the?following?span(s):?['ve?gas',?'Mt.?Hood',?'Holland',?[...]Fig.
2: Example bonus notification.74than others.
Most saliently, Person entities seemto require only a few workers to reach a relat?ively  high  recall,  compared  to  LOCATION orORGANIZATION entities.Based on the expected diminishing returns foreach entity type, we determined some number ofworkers to assign per email that we felt  wouldmaximize entity recall while staying within ourbudgetary limits.
After some tinkering and ex?perimentation with marginal recall curves, we ul?timately settled on 4 assignments for PERSON en?tities,  6  for  LOCATION entities,  and  7  forORGANIZATION entities.5    Corpora and ExperimentsWe ran our Mechanical Turk tasks over a periodof  about  three  months,  from  August  2008  toNovember  2008.
We typically  processed 500?1,500 documents per day.
In the end, the work?ers annotated 20,609 unique emails which totaled7.9 megabytes, including both subject and body.All in all, we were pleasantly surprised by thespeed at which each HIT series was completed.Out of 39 total HIT series, the average comple?tion time (i.e.
from when the HITs were first pos?ted to MTurk.com until  the last HIT was com?pleted)  was  3.13  hours,  with  an  average  of715.34 emails per HIT series.
The fastest com?pletion time per number of emails was 1.9 hoursfor a 1,000?email task, and the slowest was 5.13hours  for  a  100?email  task.
We noticed,  that,paradoxically, larger HIT series were often com?pleted  more  quickly  ?
most  likely  becauseAmazon promotes  the  larger  tasks  to  the  frontpage.5.1    Corpora AnnotationIn Table 1, we present several statistics regard?ing the annotation tasks, grouped by corpus andentity type.
Here, ?Cost?
is the sum of all bo?nuses and base rates for the HITs, ?Avg.
Cost?is  the  average amount  we paid in bonuses andbase rates per email, ?Avg.
# Workers?
is the av?erage  number  of  workers  assigned  per  email,?Avg.
Bonus?
is  the  average  bonus  per  HIT,?Avg.
# Spans?
is the average number of entitieshighlighted per HIT, and ?Avg.
Time?
is the av?erage  time  of  completion  per  HIT  in  seconds.Precision and recall  are reported relative to the?gold standards?
determined by the bonus agree?ment thresholds.
None of the reported costs in?clude fees paid to Amazon, which varied basedon how the bonuses were batched.A  few  interesting  observations  emerge  fromthese data.
For one, the average bonus was usu?ally a bit more than the base rate of $0.01.
Theimplication  is  that  bonuses  actually  comprisedthe majority of the compensation, somewhat call?ing into question their role as a ?bonus.
?Also noteworthy is  that  ORGANIZATION entitiestook  less  time  per  identified  span  to  completethan either location or person entities.
However,we suspect that this is due to the fact that we ranthe  ORGANIZATION tasks  last  (after  PERSON andLOCATION),  and by that  time we had ironed outseveral bugs in the UI, and our workers had be?come more adept at using it.5.2    Worker PerformanceIn the end, we had 798 unique workers complete169,156  total  HITs.
The  average  number  ofHITs per worker was 211.97, but the median wasonly  30.
Ten workers  who  tagged no  entitieswere  blocked,  and  the  1,029  HITs  they  com?pleted were rejected without payment.For the most part, a small number of dedicatedworkers completed the majority of the tasks.
Outof all non?rejected HITs, the top 10 most prolificworkers  completed  22.51%,  the  top  25  com?pleted  38.59%,  the  top  50  completed  55.39%,and the top 100 completed 74.93%.Callison?Burch  (2009)  found  in  their  ownMechanical  Turk  system that  the  workers  whocontributed more tended to show lower quality,2 3 400.20.40.60.812 3 4 5 6 7 800.20.40.60.812 3 4 5 6 7 8 9 1000.20.40.60.81Fig.
3: Marginal recall curves for PERSON, LOCATION, and ORGANIZATION entity types, from a trial run of900?1,000 emails.
Recall is plotted on the y?axis, the number of annotators on the x?axis.75as measured by agreement with an expert.
Wehad hoped that our bonus system, by rewardingquality  work  with  higher  pay,  would  yield  theopposite effect, and in practice, our most prolificworkers did indeed tend to show the highest en?tity recall.0 1000 2000 3000 4000 5000 6000 7000 800000.10.20.30.40.50.60.70.80.91Fig.
4: # HITs Completed vs. RecallFigure 4 shows how each of the non?rejectedworkers fared in terms of entity recall (relative tothe  ?gold  standard?
determined  by  the  bonusagreement threshold), compared to the number ofHITs completed.
As the chart shows, out of the10 most productive workers, only one had an av?erage recall score below 60%, and the rest all hadscores above 80%.
While there are still quite afew  underperforming  workers  within  the  coregroup of high?throughput annotators, the generaltrend seemed to be that the more HITs a workercompletes, the more likely he/she is to agree withthe other annotators.
This chart may be directlycompared  to  a  similar  one  in  Callison?Burch(2009), where the curve takes largely the oppos?ite shape.
One interpretation of this is that ourbonus system had the desired effect on annotatorquality.5.3    Annotation Quality ExperimentsTo  evaluate  the  quality  of  the  worker  annota?tions, one would ideally like to have at least  asubset annotated by an expert, and then comparethe expert's judgments with the Mechanical Turkworkers'.
However, in our case we lacked expertannotations  for  any  of  the  annotated  emails.Thus, we devised an alternative method to evalu?ate the annotation quality, using the NER systembuilt into the open?source MinorThird toolkit.7MinorThird is a popular machine learning andnatural language processing library that has pre?viously been applied to the problem of NER withsome success (Downey et al 2007).
For our pur?poses, we wanted to minimize the irregularity in?troduced by deviating from the core features andalgorithms  available  in  MinorThird,  and  there?fore did not apply any feature selection or featureengineering in our experiments.
We chose to useMinorThird's default  ?CRFLearner,?
which is amodule that learns feature weights using the IITBCRF library8 and then applies them to a condi?tional Markov model?based extractor.
All of theparameters were set to their default value, includ?ing  the  built?in  ?TokenFE?
feature  extractor,which extracts features for the lowercase value ofeach  token  and  its  capitalization  pattern.
Theversion of MinorThird used was 13.7.10.8.In order to convert  the  Mechanical  Turk an?notations to a format that could be input as train?ing data to the NER system, we had to resolvethe conflicting annotations of the multiple work?ers into a unified set of labeled documents.
Sim?ilarly to the bonus system, we achieved this usinga simple voting scheme.
In contrast to the bonussystem, though, we experimented with multipleinter?annotator  agreement  thresholds  between 1and 4.
For the PERSON corpora this meant a relat?ively stricter threshold than for the  LOCATION or7 http://minorthird.sourceforge.net8 http://crf.sourceforge.netTable 1: Statistics by corpus and entity type (omitting rejected work).Corpus Entity Cost #Emails Avg.
Cost Avg.
#Workers Avg.
Bonus Avg.
#Spans Avg.
Precision Avg.
Recall Avg.
Time20N.
315.68 1999 0.1579 6 0.0163 1.6885 0.5036 0.7993 144.34A.H.
412.2 2500 0.1649 6.4 0.0158 1.1924 0.6881 0.8092 105.34323.54 3000 0.1078 6.23 0.0073 1.0832 0.3813 0.7889 105.25TREC 274.88 2500 0.1100 6 0.0083 1.1847 0.3794 0.7864 122.9720N.
438.44 3500 0.1253 7 0.0079 1.2396 0.3274 0.6277 105.68A.H.
396.48 2500 0.1586 7 0.0127 1.2769 0.4997 0.7062 92.01539.19 2500 0.2157 8.6 0.0151 1.3454 0.5590 0.7415 80.55TREC 179.94 1500 0.1200 7 0.0071 0.8923 0.4414 0.6992 84.2320N.
Per.
282.51 2500 0.1130 4 0.0183 2.8693 0.7267 0.9297 152.77A.H.
Per.
208.78 2500 0.0835 4 0.0109 1.6529 0.7459 0.9308 112.4Per.
54.11 400 0.1353 6.14 0.0120 2.7360 0.8343 0.8841 111.23TREC Per.
214.37 2500 0.0857 4 0.0114 1.5918 0.7950 0.9406 103.73Loc.Loc.Enron Loc.Loc.Org.Org.Enron Org.Org.Enron76ORGANIZATION corpora,  since the  PERSON corporatypically had only 4 annotations per document.Mail subjects and bodies were split into separatedocuments.Four separate experiments were run with thesecorpora.
The first was a 5?fold cross?evaluation(i.e.,  a  80%/20% split)  train/test  experiment oneach of the twelve corpora.
Because this test didnot  rely on  any expert  annotations  in  the  goldstandard,  our  goal  here  was  only  to  roughlymeasure the ?cohesiveness?
of the corpus.
Lowprecision  and  recall  scores  should  indicate  amessy corpus, where annotations in the trainingportion do not  necessarily help the extractor todiscover  annotations  in  the  test  portion.
Con?versely, high precision and recall  scores shouldindicate a more cohesive corpus ?
one that is atleast  somewhat  internally  consistent  across  thetraining and test portions.The second test was another train/test experi?ment, but with the entire Mechanical Turk corpusas  training  data,  and  with  a  small  set  of  182emails, of which 99 were from the W3C EmailCorpus9 and 83 were from emails belonging tovarious  Kiha Software employees,  as  test  data.These 182 test  emails  were hand?annotated forthe three entity types by the authors.
Althoughthis  test  data  was  small,  our  goal  here  was  todemonstrate  how  well  the  trained  extractorscould fare against email text from a completelydifferent source than the training data.The third test  was similar  to the second,  butused as its test data 3,116 Enron emails annotatedfor  PERSON entities.10  The labels were manuallycorrected by the authors before testing.
The goalhere  was the same as  with the  second test,  al?though it must be acknowledged that the  PERSONtraining data did make use of 400 Enron emails,and therefore the test data was not from a com?pletely separate domain.The fourth test  was intended to  increase  thecomparability of our own results with those thatothers have shown in NER on email text.
For thetest  data,  we  chose  two  subsets  of  the  EnronEmail  Corpus  used  in  Minkov  et  al.
(2005).11The first, ?Enron?Meetings,?
contains 244 train?ing documents, 242 tuning documents, and 247test documents.
The second, ?Enron?Random,?contains 89 training documents, 82 tuning docu?ments,  and  83  test  documents.
For  each,  we9 http://tides.umiacs.umd.edu/webtrec/trecent/parsed_w3c_corpus.html10 http://www.cs.cmu.edu/~wcohen/repository.tgzand http://www.cs.cmu.edu/~einat/datasets.html.11 http://www.cs.cmu.edu/~einat/datasets.html.tested our statistical recognizers against all threedivisions combined as well as the test set alne.6    ResultsThe results from these four tests are presented inTables 2?5.
In these tables, ?Agr.?
refers to in?ter?annotator agreement, ?TP?
to token precision,?SP?
to  span  precision,  ?TR?
to  token  recall,?SR?
to span recall,  ?TF?
to token F?measure,and ?SF?
to span F?measure.
?Span?
scores donot award partial credit for entities, and are there?fore a stricter measure than ?token?
scores.Table 2: Cross?validation test results.The cross?validation test results seem to indic?ate that, in general, an inter?annotator agreementthreshold of 2 produces the most cohesive cor?pora  regardless  of  the  number  of  workers  as?signed  per  email.
In  all  cases,  the  F?measurepeaks at 2 and then begins to drop afterwards.The  results  from  the  second  test,  using  theW3C and Kiha emails as test data, tell a slightlydifferent story, however.
One predictable obser?vation from these data is that precision tends toincrease as more inter?annotator agreement is re?quired, while recall decreases.
We believe thatTable 3: Results from the second test.Entity TP TR TF1 65.90% 37.52% 47.82%2 83.33% 56.28% 67.19%3 84.05% 48.12% 61.20%4 84.21% 26.10% 39.85%1 41.03% 35.54% 38.09%2 62.89% 30.77% 41.32%3 66.00% 15.23% 24.75%4 84.21% 9.85% 17.63%Per.
1 85.48% 70.81% 77.45%2 69.93% 69.72% 69.83%3 86.95% 64.40% 73.99%4 95.02% 43.29% 59.49%Agr.Loc.Org.Entity TP TR TF1 60.07% 54.65% 57.23%2 75.47% 70.51% 72.90%3 71.59% 60.99% 65.86%4 59.50% 41.40% 48.83%1 70.79% 49.34% 58.15%2 77.98% 55.97% 65.16%3 38.96% 57.87% 46.57%4 64.68% 50.19% 56.52%Per.
1 86.67% 68.27% 76.38%2 89.97% 77.36% 83.19%3 87.58% 76.19% 81.49%4 75.19% 63.76% 69.00%Agr.Loc.Org.77this is due to the fact that entities that were con?firmed by more workers tended to be less contro?versial  or  ambiguous  than  those  confirmed  byfewer.
Most  surprising  about  these  results  isthat, although F?measure peaks with the 2?agree?ment corpora for both LOCATION and ORGANIZATIONentities,  PERSON entities actually show the worstprecision when using the 2?agreement corpus.
Inthe case of  PERSON entities, the corpus generatedusing no inter?annotator agreement at all, i.e., an?notator  agreement  of  1,  actually  performs  thebest in terms of F?measure.Table 4: Results from the third test.Data TP TR TF SP SR SFE?M 1 100% 57.16% 72.74% 100% 50.10% 66.75%(All) 2 100% 64.31% 78.28% 100% 56.11% 71.88%3 100% 50.44% 67.06% 100% 45.11% 62.18%4 100% 31.41% 47.81% 100% 27.91% 43.64%E?M 1 100% 62.17% 76.68% 100% 51.30% 67.81%(Test) 2 100% 66.36% 79.78% 100% 54.28% 70.36%3 100% 55.72% 71.56% 100% 45.72% 62.76%4 100% 42.24% 59.39% 100% 36.06% 53.01%E?R 1 36.36% 59.91% 45.25% 40.30% 53.75% 46.07%(All) 2 70.83% 65.32% 67.96% 67.64% 57.68% 62.26%3 88.69% 58.63% 70.60% 82.93% 54.38% 65.68%4 93.59% 43.68% 59.56% 89.33% 41.22% 56.41%E?R 1 100% 60.87% 75.68% 100% 54.82% 70.82%(Test) 2 100% 64.70% 78.56% 100% 59.05% 74.26%3 100% 63.06% 77.34% 100% 58.38% 73.72%4 100% 43.04% 60.18% 100% 40.10% 57.25%Agr.Table 5: Results from the fourth test.With  the  third  test,  however,  the  results  aremore in line with those from the cross?validationtests: F?measure peaks with the 2?agreement cor?pus  and  drops  off  as  the  threshold  increases.Most likely these results can be considered moresignificant than those from the second test, sincethis  test  corpus  contains  almost  20  times  thenumber of documents.For the fourth test, we report both token?levelstatistics  and  span?level  statistics  (i.e.,  wherecredit  for  partially  correct  entity  boundaries  isnot awarded) in order to increase comparabilitywith Minkov et al  (2005).
With one exception,these tests seem to show again that the highest F?measure comes from the annotator created usingan agreement level of 2, confirming results fromthe first and third tests.The fourth test may also be directly comparedto the results in Minkov et al  (2005), which re?port span F?measure scores of 59.0% on Enron?Meetings  and  68.1% on  Enron?Random,  for  aCRF?based recognizer using the ?Basic?
featureset  (which  is  identical  to  ours)  and  using  the?train?
division for training and the ?test?
divi?sion for testing.
In both cases, our best?perform?ing annotators exceed these scores ?
an 11.5%improvement  on  Enron?Meetings  and  a  6.16%improvement on Enron?Random.
This is an en?couraging  result,  given  that  our  training  datalargely come from a different source than the testdata, and that the labels come from non?experts.We see this as confirmation that very large cor?pora annotated by Mechanical Turk workers cansurpass the quality of smaller corpora annotatedby experts.7    ConclusionIn order to quickly and economically build alarge annotated dataset  for  NER,  we leveragedAmazon?s Mechanical Turk.
MTurk allowed usto  build a dataset of 20,609 unique emails with169,156  total  annotations  in  less  than  fourmonths.
The  MTurk worker population respon?ded well to NER tasks, and in particular respon?ded well to the bonus and feedback scheme weput into place to improve annotation quality.
Thebonus feedback system was designed to improvethe transparency of the compensation system andmotivate higher quality work over time.
Encour?agingly, our results indicate that the workers whocompleted the most documents also had consist?ently high entity recall, i.e., agreement with otherworkers, indicating that the system achieved thedesired effect.Given a large body of MTurk annotated docu?ments, we were able to leverage inter?annotatoragreement to control the precision and recall of aCRF?based recognizer trained on the data.
Im?portantly,  we  also  showed  that  inter?annotatoragreement can be used to predict the appropriatenumber of workers to assign to a given email inorder to maximize entity recall and reduce costs.Finally, a direct comparison of the entity re?cognizers generated from  MTurk annotations tothose  generated  from  expert  annotations  wasvery promising, suggesting that Mechanical Turkis  appropriate  for  NER annotation  tasks,  whencare is taken to manage annotator error.AcknowledgmentsThanks to Dolores Labs for the initial version ofthe UI, and thanks to Amazon and the 798 Mech?anical Turk workers for making this work pos?sible.TP TR TF1 80.56% 62.55% 70.42%2 85.08% 67.66% 75.37%3 93.25% 57.13% 70.86%4 95.61% 39.67% 56.08%Agr.78ReferencesMichele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigu?ation.
In ACL '01:26?33.Chris Callison?Burch.
2009.
Fast, cheap, and creative:evaluating  translation  quality  using  Amazon'sMechanical Turk.
In EMNLP '09:286?295.Doug  Downey,  Matthew  Broadhead,  and  Oren  Et?zioni.
2007.
Locating  complex  named  entities  inweb text.
In IJCAI '07.Ralph Grishman and Beth Sundheim.
1996.
MessageUnderstanding  Conference?6:  a  brief  history.
InProceedings of  the 16th conference on Computa?tional Linguistics:466?471.Aniket Kittur, Ed H. Chi, and Bongwon Suh.
2008.Crowdsourcing user studies with Mechanical Turk.In Proceedings of CHI 2008.Zornitsa Kozareva.
2006.
Bootstrapping named entityrecognition with automatically generated gazetteerlists.
In Proceedings of the Eleventh Conference ofthe European Chapter of the Association for Com?putational Linguistics:15?21.Andrei  Mikheev,  Marc  Moens,  and  Claire  Grover.1999.
Named entity recognition without gazetteers.In  Proceedings  of  the  Ninth  Conference  of  theEuropean chapter of the Association for Computa?tional Linguistics:1?8.Einat Minkov, Richard C. Wang, and William W. Co?hen.
2005.
Extracting personal names from email:applying named entity recognition to informal text.In HTL '05:443?450.David Nadeau and Satoshi Sekine.
2007.
A survey ofnamed  entity  recognition  and  classification.
Lin?guisticae Investigationes, 30(1):3?26.Preslav Nakov.
2008.
Noun compound interpretationusing paraphrasing verbs: Feasibility study.
In Pro?ceedings  of  the 13th international  conference  onArtificial  Intelligence:  Methodology,  Systems andApplications (AIMSA 2008):103?117.Jason Pontin.
2007.
Artificial Intelligence, With HelpFrom the Humans.
In New York Times (March 25,2007).Burr Settles.
2004.
Biomedical named entity recogni?tion using conditional random fields and rich fea?ture sets.
In Proceedings of the COLING 2004 In?ternational Joint Workshop on Natural LanguageProcessing in Biomedicine and its Applications.Rion Snow, Brendan O'Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast  ?
but is  itgood?
: evaluating non?expert annotations for natur?al language tasks.
In EMNLP '08:254?263.Alexander Sorokin and David Forsyth.
Utility data an?notation with Amazon MTurk.
In  Proceedings ofComputer  Vision  and Pattern  Recognition Work?shop at CVPR?08.79
