Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 763?773,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsVector-space calculation of semantic surprisal for predicting wordpronunciation durationAsad Sayeed, Stefan Fischer, and Vera DembergComputational Linguistics and Phonetics/M2CI Cluster of ExcellenceSaarland University66123 Saarbr?ucken, Germany{asayeed,sfischer,vera}@coli.uni-saarland.deAbstractIn order to build psycholinguistic mod-els of processing difficulty and evaluatethese models against human data, we needhighly accurate language models.
Here wespecifically consider surprisal, a word?spredictability in context.
Existing ap-proaches have mostly used n-gram modelsor more sophisticated syntax-based pars-ing models; this largely does not accountfor effects specific to semantics.
We buildon the work by Mitchell et al (2010) andshow that the semantic prediction modelsuggested there can successfully predictspoken word durations in naturalistic con-versational data.An interesting finding is that the trainingdata for the semantic model also playsa strong role: the model trained on in-domain data, even though a better lan-guage model for our data, is not able topredict word durations, while the out-of-domain trained language model does pre-dict word durations.
We argue that this atfirst counter-intuitive result is due to theout-of-domain model better matching the?language models?
of the speakers in ourdata.1 IntroductionThe Uniform Information Density (UID) hypothe-sis holds that speakers tend to maintain a relativelyconstant rate of information transfer during speechproduction (e.g., Jurafsky et al, 2001; Aylett andTurk, 2006; Frank and Jaeger, 2008).
The rateof information transfer is thereby quantified usingas each words?
Surprisal (Hale, 2001), that is, aword?s negative log probability in context.Surprisal(wi) = ?
logP (wi|w1..wi?1)This work makes use of an existing measure ofsemantic surprisal calculated from a distributionalspace in order to test whether this measure ac-counts for an effect of UID on speech production.Our hypothesis is that a word in a semanticallysurprising context is pronounced with a slightlylonger duration than the same word in a seman-tically less-expected context.
In this way, a moreuniform rate of information transfer is achieved,because the higher information content of the un-expected word is stretched over a slightly longertime.
To our knowledge, the use of this form ofsurprisal as a pronunciation predictor has neverbeen investigated.The intuition is thus: in a sentence like thesheep ate the long grass, the word grass will haverelatively high surprisal if the context only con-sists of the long.
However, a distributional repre-sentation that retains the other content words in thesentence, thus representing the contextual similar-ity of grass to sheep ate, would able to capture therelevant context for content word prediction moreeasily.
In the approach taken here, both types ofmodels are combined: a standard language modelis reweighted with semantic similarities in order tocapture both short- and more long-distance depen-dency effects within the sentence.The semantic surprisal model, a re-implementation of Mitchell (2011), uses aword vector w and a history or context vector h tocalculate the language model p(w|h), defining thisprobability in vector space via cosine similarity.Words that have a higher distributional similarityto their context are thus represented as having ahigher probability than words that do not.
Thus,we calculate probabilities for words in the contextof a sentence in a framework of distributionalsemantics.Regarding our main hypothesis?that speakersadapt their speech rate as a function of a word?s in-formation content?it is particularly important to763us to test this hypothesis on fully ?natural?
conver-sational data.
Therefore, we use the AMI corpus,which contains transcripts of English-languageconversations with orthographically correct tran-scriptions and precise word pronunciation bound-aries in terms of time.We will explain the calculation of semantic sur-prisal in section 4 (this is so far only described inMitchell?s 2011 PhD thesis), and then evaluate theeffect of an in-domain semantic surprisal model insection 7.
Next, we will compare this to the ef-fect of an out-of-domain semantic surprisal modelin section 8.
The hypothesis is only confirmed forthe out-of-domain model, which we argue is dueto this model being more similar to the speaker?sinternal ?model?
than the in-domain model.2 Background2.1 Surprisal and UIDSurprisal is defined in terms of the negativelogarithm of the probability of a word in con-text: S(w) = ?
logP (w|context), whereP (w|context) is the probability of a word givenits previous (linguistic) context.
It is a measureof information content in which a high surprisalimplies low predictability.
The use of surprisalin psycholinguistic research goes back to Hale(2001), who used a probabilistic Earley Parser tomodel the difficulty in parsing so-called gardenpath sentences (e.g.
?The horse raced past the barnfell?
), wherein the unexpectedness of an upcom-ing word or structure influences the language pro-cessor?s difficulty.
Recent work in psycholinguis-tics has provided increasing support (e.g., Levy(2008); Demberg and Keller (2008); Smith andLevy (2013); Frank et al (2013)) for the hypoth-esis that the surprisal of a word is proportionalto the processing difficulty (measured in terms ofreading times and EEG event-related potentials) itcauses to a human.The Uniform Information Density (UID) hy-pothesis (Frank and Jaeger, 2008) holds thatspeakers tend distribute information uniformlyacross an utterance (in the limits of grammatical-ity).
Information density is quantified in terms ofthe surprisal of each word (or other linguistic unit)in the utterance.
These notions go back to Shan-non (1948), who showed that conveying informa-tion uniformly close to channel capacity is optimalfor communication through a (noisy) communica-tion channel.Frank and Jaeger (2008) investigated UID ef-fects in the SWITCHBOARD corpus at a mor-phosyntactic level wherein speakers avoid usingEnglish contracted forms (?you are?
vs.
?you?re?
)when the contractible phrase is also transmittinga high degree of information in context.
In thiscase, n-gram surprisal was used as the informationdensity measure.
Related hypotheses have beensuggested by Jurafsky et al (2001), who relatedspeech durations to bigram probabilities on theSwitchboard corpus, and Aylett and Turk (2006),who investigated information density effects at thesyllable level.
They used a read-aloud Englishspeech synthesis corpus, and they found that thereis an inverse relationship between the pronuncia-tion duration and the N-gram predictability.
Dem-berg et al (2012) also use the AMI corpus usedin this work, and show that syntactic surprisal(i.e., the surprisal estimated from Roark?s (2009)PCFG parser) can predict word durations in natu-ral speech.Our work expands upon the existing efforts indemonstrating the UID hypothesis by applyingsurprisal to the level of lexical semantics.2.2 Distributional semanticsGiven a means of evaluating the similarity of lin-guistic units (e.g., words, sentences, texts) in somenumerical space that represents the contexts inwhich they appear, it is possible to approximatethe semantics in distributional terms.
This is usu-ally done by collecting statistics from a corpus us-ing techniques developed for information retrieval.Using these statistics as a model of semantics isjustified in terms of the ?distributional hypothe-sis?, which holds that words used in similar con-texts have similar meanings (Harris, 1954).A simple and widely-used type of distributionalsemantic model is the vector space model (Tur-ney and Pantel, 2010).
In such a model, all wordsare represented each in terms of vectors in a sin-gle high-dimensional space.
The semantic simi-larity of words can then be calculated via the co-sine of the angle between the vectors in this man-ner: cos(?)
=~a?~b|~a||~b|.
Closed-class function wordsare usually excluded from this calculation.
Untilrelatively recently (Erk, 2012), distributional se-mantic models did not take into account the fine-grained details of syntactic and semantic structureconstrued in formal terms.7643 CorpusThe AMI Meeting Corpus (Carletta, 2007) is amultimodal English-language corpus.
It containsvideos and transcripts of simulated workgroupmeetings accompanied by various kinds of anno-tations.
The corpus is available along with its an-notations under a free license1.Two-thirds of the videos contain simulatedmeetings of 4-person design teams assigned to talkabout the development of a fictional television re-mote control.
The remaining meetings discuss var-ious other topics.
The majority of speakers werenon-native speakers of English, although all theconversations were held in English.
The corpuscontains about 100 hours of material.An important characteristic of this corpus forour work is that the transcripts make use of con-sistent English orthography (as opposed to beingphonetic transcripts).
This enables the use of nat-ural language processing techniques that requirethe reliable identification of words.
Grammaticalerrors, however, remain in the corpus.
The corpusincludes other annotations such as gesture and dia-log acts.
Most important for our work are the timespans of word pronunciation, which are precise tothe hundredth of a second.We removed interjections, incomplete words,and transcriptions that were still misspelled fromthe corpus, and we took out all incomplete sen-tences.
This left 951,769 tokens (15,403 types) re-maining in the corpus.4 Semantic surprisal modelWe make use of a re-implementation of the se-mantic surprisal model presented in Mitchell et al(2010).
As this paper does not provide a detaileddescription of how to calculate semantic surprisal,our re-implementation is based on the descriptionin Mitchell?s PhD thesis (2011).In order to calculate surprisal, we need to beable to obtain a good estimate of a word givenprevious context.
Mitchell uses the following con-cepts in his model:?
hn?1is the history and represents all the pre-vious words in the sentence.
If wnis the cur-rent word, then hn?1= w1.
.
.
wn?1.
Thevector-space semantic representation of hn?11http://groups.inf.ed.ac.uk/ami/download/is calculated from the composition of individ-ual word vectors, which we call~hn?1.?
context words represent the dimensions of theword vectors.
The value of a word vector?scomponent is the co-occurrence of that wordwith a context word.
The context words con-sist of the most frequent words in the corpus.?
we use word class and distinguish betweencontent words and function words, for whichwe use open and closed classes as a proxy.4.1 Computing the vector componentsThe proportion between two probabilitiesp(ci|w)p(ci)is used for calculating vector components, whereciis the ith context dimension and w is the givenword in the current position.
We can calculateeach vector component vifor a word vector ~v ac-cording to the following equation:vi=p(ci|w)p(ci)=fciwftotalfwfci(1)where fciwis the cooccurrence frequency ofw andcitogether, ftotalis the total corpus size, and cirepresents the unigram frequencies of w. All fu-ture steps in calculating our language model relyon this definition of vi.4.2 Semantic probabilitiesFor the goal of computing p(w|h), we use the ba-sic idea that the more ?semantically coherent?
aword is with its history, the more likely it is.
Co-sine similarity is a common way to define thissimilarity mathematically in a distributional space,producing a value in the interval [?1, 1].
We usethe following definitions, wherein ?
is the anglebetween ~w and~h:cos(?)
=~w ?~h|~w||~h|(2)~w ?~h =?iwihi(3)Mitchell notes that there are at least three prob-lems with using cosine similarity in connectionwith the construction of a probabilistic model:(a) the sum of all cosine values is not unity, (b)word frequency does not pay a role in the cal-culation, such that a rare synonym of a frequentword might get a high similarity rating, despitelow predictability, and (c) the calculation can re-sult in negative values.765This problem is addressed by two changes to thenotion of dot product used in the calculation of thecosine:~w ?~h =?ip(ci|w)p(ci)p(ci|h)p(ci)(4)The influence of word frequencies is then restoredusing p(w) and p(ci):p(w|h) = p(w)?ip(ci|w)p(ci)p(ci|h)p(ci)p(ci) (5)This expression reweights the new scalar productwith the likelihood of the given words and the con-text words.
We refer the reader to Mitchell (2011)in order to see that this is a true probability.
Theapplication of Bayes?
Rule allows us to rewrite theformula as p(w|h) =?ip(w|ci)p(ci|h).
Never-theless, equation (5) is better suited to our task, asit operates directly over our word vectors.4.3 Incremental processingEquation (5) provides a conditional probability fora word w and its history h. To calculate the prod-uctp(ci|w)p(ci)p(ci|h)p(ci), we need the components of thevectors for w and h at the current position in thesentence.
We can get ~w from directly from thevector space of words.
However,~h does not havea direct representation in that space, and it must beconstructed compositionally:~h1= ~w1Initialization (6)~hn= f(~hn?1, ~wn) Composition (7)f is a vector composition function that can be cho-sen independently from the model.
The history isinitialized using the vector of the first word andcombined step-by-step with the vectors of the fol-lowing words.
History vectors that arise from thecomposition step are normalized2:hi=?hi?j?hjp(cj)Normalization (8)The equations (5), (6), (7), and (8) represent a sim-ple language model, assuming calculation of vec-tor components with equation (1).2This equation is slightly different from what appears inMitchell (2011).
We present here a corrected formula basedon private communication with the author.4.4 Accounting for word orderThe model described so far is based on semanticcoherence and mostly ignores word order.
Conse-quently, it has poor predictive power.
In this sec-tion, we describe how a notion of word order isincluded in the model through the integration ofan n-gram language model.Specifically, equation (5) can be represented asthe product of two factors:p(w|h) = p(w)?
(w, h) (9)?
(w, h) =?ip(ci|w)p(ci)p(ci|h)p(ci)p(ci) (10)where ?
is the semantic component that scalesp(w) in function of the context.
A word w that hasa close semantic similarity to a history h shouldreceive higher or lower probability depending onwhether ?
is higher or lower than 1.
In order tomake this into a prediction, p(w) is replaced witha trigram probability.p?
(wn, hn?1, wn?1n?2) = p(wn|wn?1n?2)?
(wn, hn?1)(11)However, this change means that the result is nolonger a true probability.
Instead, equation 11 canbe seen as an estimate of semantic similarity.
Inorder to restore its status as a probability, Mitchellincludes another normalization step:p(wn|hn?3, wn?1n?2) =????????????????????
?p(wn|wn?1n?2)Function wordsp?(wn,hn?3,wn?1n?2)?wcp?
(wc,hn?3,wn?1n?2)?wcp(wc|wn?1n?2)Content words(12)The model hence simply uses the trigram modelprobability for function words, making the as-sumption that the distributional representation ofsuch words does not include useful information.On the other hand, content words obtain a por-tion of the probability mass whose size dependson its similarity estimate p?
(wn, hn?3, wn?1n?2) rel-ative to the similarity estimates of all otherwords?wcp?
(wc, hn?3, wn?1n?2).
The factor?wcp(wc|wn?1n?2) ensures that not all of the proba-bility mass is divided up among the content wordswc; rather, only the mass assigned by the n-grammodel at position wn?1n?2is re-distributed.
The766probability mass of the function words remainsunchanged.Mitchell (2011) restricts the history so that onlywords outside the trigram window are taken intoaccount in order to keep the n-gram model and thesemantic similarity model independent.
Thus, then-gram model represents local dependencies, andthe semantic model represents longer-distance de-pendencies.The final model that we use in our experimentconsists of equations (1), (6), (7), (8) and (12).5 Evaluation MethodsOur goal is to test whether semanticallyreweighted surprisal can explain spoken worddurations over and above more simple factors thatare known to influence word durations, such asword length, frequency and predictability usinga simpler language model.
Our first experimenttests whether semantic surprisal based on a modeltrained using in-domain data is predictive ofword pronunciation duration, considering theUID hypothesis.
For our in-domain model, weestimate surprisal using 10-fold cross-validationover the AMI corpus: we divide the corpus intoten equally-sized segments and produce surprisalvalues for each word in each segment based on amodel trained from the other nine segments.
Wethen use linear mixed effects modeling (LME) viathe lme4 package in R (Pinheiro and Bates, 2000;Bates et al, 2014) in order to account for wordpronunciation length.
We follow the approach ofDemberg et al (2012).Linear mixed effects modelling is a generaliza-tion of linear regression modeling and includesboth fixed effects and random effects.
This is par-ticularly useful when we have a statistical units(e.g., speakers) each with their own set of repeatedmeasures (e.g., word duration), but each such unithas its own particular characteristics (e.g., somespeakers naturally speak more slowly than others).These are the random effects.
The fixed effects arethose characteristics that are expected not to varyacross such units.
LME modeling learns coeffi-cients for all of the predictors, defining a regres-sion equation that should account for the data inthe dependent variable (in our case, word pronun-ciation duration).
The variance in the data that amodel cannot explain is referred to as the residual.We denote statistical significances in the followingway: *** means a p-value ?
0.001, ** means p ?0.01, * means p ?
0.05, and no stars means thatthe predictor is not significant (p > 0.05).In our regression models, all the variables arecentered and scaled to reduce effects of correla-tions between predictors.
Furthermore, we log-transformed the response variable (actual spokenword durations from the corpus) as well as the du-ration estimates from the MARY speech synthesissystem to obtain more normal distributions, whichare prerequisite for applying the LME models.
Allconclusions drawn here also hold for versions ofthe model where no log transformation is used.From the AMI corpus, we filter out data points(words) that have a pronunciation duration of zeroor those that are longer than two seconds, the latterin order to avoid including such things as pausesfor thought.
We also remove items that are notrepresented in Gigaword.
That leaves us with790,061 data points for further analysis.
How-ever, in our semantic model, function words arenot affected by the ?
semantic similarity adjust-ment and are therefore not analyzable for the ef-fect of semantically-weighted trigram predictabil-ity.
That leaves 260k data points for analysis in themodels.6 Baseline modelAs a first step, we estimate a baseline modelwhich does not include the in-domain semanticsurprisal.
The response variable in this modelare the word durations observed in the corpus.Predictor variables include DMARY(the context-dependent spoken word duration as estimated bythe MARY speech synthesis system), word fre-quency estimates from the same domain as wellas the GigaWord corpus (FAMIand FGiga, bothas log relative frequencies), the interaction be-tween estimated word durations and in-domainfrequency, (DMARY:FAMI) and a domain-generaltrigram model (SAMI-3).
Our model also includes arandom intercept for each speaker, as well as ran-dom slopes under speaker for DMARYand SAMI-3.The baseline model is shown in Table 1.All predictors in the baseline model shown inTable 1 significantly improve model fit.
We cansee that the MARY-TTS estimated word durationsare a positive highly significant predictor in themodel.
Furthermore, the word frequency esti-mates from the domain general corpus as well asthe in-domain frequency estimates are significantnegative predictors of word durations, this means767Predictor Coefficient t-value Sig.
(Intercept) 0.034 4.90 ***DMARY0.427 143.97 ***FAMI-0.137 -60.26 ***FGiga-0.051 -18.92 ***SGiga-3gram0.032 10.94 ***DMARY:FAMI-0.003 -2.12 *Table 1: Fixed effects of a baseline model includ-ing the data points for which we could calculatesemantic surprisal.that as expected, words durations are shorter formore frequent words.
We can furthermore seethat n-gram surprisal is a significant positive pre-dictor of spoken word durations; i.e., more unex-pected words have longer durations than otherwisepredicted.
Finally, there is also a significant in-teraction between estimated word durations andin-domain word frequency, which means that theduration of long and frequent words is correctedslightly downward.7 Experiment 1: in-domain modelThe AMI corpus contains spoken conversations,and is thus quite different from the written cor-pora we have available.
When we train an n-gram model in domain (using 10-fold cross valida-tion), perplexities for the in-domain model (67.9)are much lower than for a language model trainedon gigaword (359.7), showing that the in-domainmodel is a better language model for the data3.In order to see the effect of semantic surprisalestimated based on the in-domain language modeland reweighted for semantic similarity within thesame sentence as described in Section 3, we thenexpand the baseline model, adding SSemanticsasa predictor.
Table 2 shows the fixed effects ofthis expanded model.
The predictor for semanticsurprisal is significant, but the coefficient is neg-ative.
This apparently contradicts our hypothesisthat semantic surprisal has a UID effect on pronun-ciation duration, so that higher SSemanticsmeanshigher DAMI.
We found that these results are verystable?in particular, the same results also hold ifwe estimate a separate model with SSemanticsas apredictor and residuals of the baseline model as a3Low perplexity estimates are reflective of the spokenconversational domain.
Perplexities on content words aremuch higher: 357.3 for the in-domain model and 2169.8 forthe out of domain model.Predictor Coefficient t-value Sig.
(Intercept) 0.031 4.53 **DMARY0.428 144.06 ***FAMI-0.148 -59.15 ***FGiga-0.043 -15.10 ***SGiga-3gram0.047 14.60 ***SSemantics-0.028 -9.78 ***DMARY:FAMI-0.003 -2.27 *Table 2: Fixed effects of the baseline model withsemantic surprisal (including also a random slopefor semantic surprisal under subject).Figure 1: GAM-calculated spline for SSemanticsforthe in-domain model.response variable, and when we include in-domainsemantic surprisal in a model where there ngramsurprisal on the out of domain corpus is not in-cluded as a predictor variable.In order to understand the unexpected behaviourof SSemantics, we make use of a generalized additivemodel (GAM) with the R package mgcv.
Com-pared to LME models, GAMs are parameter-freeand do not assume a linear form of the predic-tors.
Instead, for every predictor, GAMs can fit aspline.
We learn a GAM using the residuals of thebaseline model as a response variable and fittingsemantic surprisal based on the in-domain model;see Table 2.In figure 1, we see that SSemanticsis poorly fitby a linear function.
In particular, there are twointervals in the curve.
Between surprisal values 0768and 1.5, the curve falls, but between 1.5 and 4, itrises.
(For high surprisal values, there are too fewdata points from which to draw conclusions.
)Therefore, we decided to divide the data up intodatapoints with SSemanticsabove 1.5 and below 1.5.We then modelled the effect of SSemanticson theresiduals of the baseline model, with SSemanticsasa random effect.
This is to remove a possible effectof collinearity between SSemanticsand the otherpredictors.Interval ofPredictor Coef.
t-value Sig.SSemantics[0,?
[(Intercept) 0 0SSemantics-0.013 -7.01 ***[0, 1.5[(Intercept) 0 0SSemantics-0.06 -18.56 ***[1.5,?
[(Intercept) 0 0SSemantics0.013 5.50 ***Table 3: Three models of SSemanticsas a random ef-fect over the residuals of baseline models learnedfrom the remaining fixed effects.
The first modelis over the entire range.Table 3 shows that the random effect of se-mantic surprisal is positive and significant in therange of semantic surprisal above 1.5.
That lowsurprisals have the opposite effect compared towhat we expect suggests to us that using theAMI corpus as an in-domain source of trainingdata presents a problem.
The observed resultfor the relationship between semantic surprisaland spoken word durations does not only holdfor the semantic surprisal model, but also for thestandard non-weight-adjusted in-domain trigrammodel.
We therefore hypothesize that our seman-tic surprisal model is producing surprisal valuesthat are low because they are common in this do-main (both higher frequency and higher similari-ties), but speakers are coming to the AMI task with?models?
trained on out-of-domain data.
Thus,words that are apparently very low-surprisal dis-play longer pronunciation durations as an artifactof the model.
To test this, we conducted a secondexperiment, for which we built a model with out-of-domain data.8 Experiment 2: out-of-domain trainingIn order to test for the effect of possible under-estimation of surprisal due to in-domain training,we also tested the semantic surprisal model whentrained on more domain-general text.
As train-ing data for our semantic model, we use a ran-domly selected 1% (by sentence) of the EnglishGigaword 5.0 corpus.
This is lowercased, with ha-pax legomena treated as unknown words.
We testthe model against the entire AMI corpus.
Further-more, we also compare our semantic surprisal val-ues to the syntactic surprisal values calculated byDemberg et al (2012) for the AMI corpus, whichwe obtained from the authors.
As noted above,the out-of-domain language model has higher per-plexity on the AMI corpus?that is, it is a lower-performing language model.
On the other hand, itmay represent overall speaker experience more ac-curately than the in-domain model; in other words,it may be a better model of the speaker.8.1 ResultsOnce again, the semantic surprisal model is onlydifferent from a general n-gram model on contentwords.
We therefore first compare whether themodel that is reweighted for semantic surprisal canexplain more of the variance than the same modelwithout semantic reweighting.We again use the same baseline model as for thein-domain experiment, see table 1.
As the seman-tic surprisal model represents a reweighted trigrammodel, there is a high correlation between thetrigram model and the semantic surprisal model.We thus need to know whether the semanticallyreweighted model is better than the simple tri-gram model.
When we compare a model that con-tains both trigram surprisal and semantic surprisalas a predictor, we find that this model is signifi-cantly better than the model including only trigramsurprisal (AIC of baseline model: 618427; AICof model with semantic surprisal: 618394; ?2=35.8; p < 0.00001).
On the other hand, the modelincluding both predictors is only marginally betterthan the model including semantic surprsial (AICof semantic surprisal model: 618398).
This meansthat the simpler trigram surprisal model does notcontribute anything over the semantic model, andthat the semantic model fits the word duration databetter.
Table 4 shows the model with semantic sur-prisal as a predictor.Furthermore, we wanted to check whether ourhypothesis about the negative result for the in-domain model was indeed due to an under-estimation of surprisal of in-domain words for the769Predictor Coefficient t-value Sig.
(Intercept) 0.034 4.90 ***DMARY0.427 144.36 ***FAMI-0.135 -58.76 ***FGiga-0.053 -19.99 ***SSemantics0.034 11.70 ***DMARY:FAMI-0.003 -2.09 *Table 4: Model of spoken word durations,with random intercept and random slopes forDMARYand SSemanticsunder speaker.Figure 2: GAM-calculated spline for SSemanticsforthe ouf-of-domain model.in-domain model.
We again calculate a GAMmodel showing the effect of out-of-domain seman-tic surprisal in a model containing also the base-line predictors, see figure 2.We can see that word durations increase withincreasing semantic surprisal, and that there is inparticular no effect of longer word durations forlow surprisal words.
This result is also confirmedby LME models splitting up the data in small andlarge surprisal values, as done for the in-domainmodel in Table 3; semantic surprisal based on theout-of-domain model is a significant positive pre-dictor in both data ranges.Next, we tested whether the semantic similaritymodel improves model fit over and above a modelalso containing syntactic surprisal as a predictor.We find that syntactic surprisal improves model fitover and above the model including semantic sur-Predictor Coefficient t-value Sig.
(Intercept) -0.058 -6.58 ***DMARY0.425 144.04 ***FAMI-0.131 -57.04 ***FGiga-0.051 -19.41 ***SSyntax0.011 17.61 ***SSemantics0.015 4.99 ***DMARY:FAMI-0.007 -4.44 ***Table 5: Linear mixed effects model for spokenword durations in the AMI corpus, for a model in-cluding both syntactic and semantic surprisal as apredictor as well as a random intercept and slopefor DMARYand SSemanticsunder speaker.prisal (?2= 309.5; p < 0.00001), and that seman-tic surprisal improves model fit over and abovea model including syntactic surprisal and trigramsurprisal (?2= 28.5; p < 0.00001).
Table 5shows the model containing both syntactic basedon the Roark parser ((Roark et al, 2009); see alsoDemberg et al (2012) for use of syntactic surprisalfor estimating spoken word durations) and seman-tic surprisal.Finally, we split our dataset into data from na-tive and non-native speakers of English (305 na-tive speakers, vs. 376 non-native speakers).
Ta-ble 6 shows generally larger effects for native thannon-native speakers.
In particular, the interac-tion between duration estimates and word frequen-cies, and semantic surprisal were not significantpredictors in the non-native speaker model (how-ever, random slopes for semantic surprisal un-der speaker still improved model fit very strongly,showing that non-native speakers differ in whetherand how they take into account semantic surprisalduring language production).9 DiscussionOur analysis shows that high information densityat one linguistic level of description (for exam-ple, syntax or semantics) can lead to a compen-satory effect at a different linguistic level (here,spoken word durations).
Our data also shows how-ever, that the choice of training data for the mod-els is important.
A language model trained exclu-sively in a specific domain, while a good languagemodel, may not be representative of speaker?soverall language experience.
This is particularlyrelevant for the AMI corpus, in which groups of770Native Speaker Non-native SpeakerPredictor Coefficient t-value Sig.
Coefficient t-value Sig.
(Intercept) -0.1706 -13.76 *** 0.035 3.42 ***DMARY0.4367 105.43 *** 0.415 104.09 ***FAMI-0.1407 -42.54 *** -0.122 -38.66 ***FGiga-0.0421 -11.07 *** -0.063 -18.70 ***SSyntax0.0132 14.22 *** 0.009 11.96 ***SSemantics0.0246 5.89 *** ***DMARY:FAMI-0.0139 -6.12 *** ***Table 6: Linear mixed effects models for spoken word durations in the AMI corpus, for native as well asnon-native speakers of English separately.
The models include both syntactic and semantic surprisal asa fixed effect, and a random intercept and slope for DMARYand SSemanticsunder speaker.researchers are discussing the design of a remotecontrol, but where it is not necessarily the casethat these people discuss remote controls very fre-quently.
Furthermore, none of the speakers werepresent in the whole corpus, and most of the> 600speakers participated only in very few meetings.This means that the in-domain language modelstrongly over-estimates people?s familiarity withthe domain.Words that are highly predictable for the in-domain model (but which are not highly pre-dictable in general) were not pronounced faster,as evident in our first analysis.
When seman-tic surprisal is however estimated based on amore domain-general text like Gigaword, we finda significant positive effect of semantic surprisalon spoken word durations across the completespectrum from very predictable to unpredictablewords.These results also point to an interesting sci-entific question: to what extent to people usetheir domain-general model for adapting their lan-guage and speech production in a specific situa-tion, and to what extent do they use a domain-specific model for adaptation?
Do people adaptduring a conversation, such that in-domain mod-els would be more relevant for language produc-tion in situations where speakers are more versedin the domain?10 Conclusions and future workWe have described a method by which it is pos-sible to connect a semantic level of representation(estimated using a distributional model) to obser-vations about speech patterns at the word level.From a language science or psycholinguistic per-spective, we have shown that semantic surprisalaffects spoken word durations in natural conversa-tional speech, thus providing additional supportiveevidence for the uniform information density hy-pothesis.
In particular, we find evidence that UIDeffects connect linguistic levels of representation,providing more information about the architectureof the human processor or generator.This work also has implications for designersof speech synthesis systems: our results point to-wards using high-level information about the rateof information transfer measured in terms of sur-prisal for estimating word durations in order tomake artificial word pronunciation systems soundmore natural.Finally, the strong effect of training data domainraises scientific questions about how speakers usedomain-general and -specific knowledge in com-municative cooperation with listeners at the wordpronunciation level.One possible next step would be to expand thiswork to more complex semantic spaces which in-clude stronger notions of compositionality, seman-tic roles, and so on, such as the distributional ap-proaches of Baroni and Lenci (2010), Sayeed andDemberg (2014), and Greenberg et al (2015) thatcontain grammatical information but rely on vec-tor operations.AcknowledgementsThis research was funded by the German ResearchFoundation (DFG) as part of SFB 1102 ?Informa-tion Density and Linguistic Encoding?.771ReferencesAylett, M. and Turk, A.
(2006).
Language redun-dancy predicts syllabic duration and the spectralcharacteristics of vocalic syllable nuclei.
TheJournal of the Acoustical Society of America,119(5):3048?3058.Baroni, M. and Lenci, A.
(2010).
Distributionalmemory: A general framework for corpus-based semantics.
Comput.
Linguist., 36(4):673?721.Bates, D., M?achler, M., Bolker, B. M., and Walker,S.
C. (2014).
Fitting linear mixed-effects mod-els using lme4.
ArXiv e-print; submitted toJournal of Statistical Software.Carletta, J.
(2007).
Unleashing the killer corpus:experiences in creating the multi-everythingAMI meeting corpus.
Language Resources andEvaluation, 41(2):181?190.Demberg, V. and Keller, F. (2008).
Data fromeye-tracking corpora as evidence for theoriesof syntactic processing complexity.
Cognition,109(2):193?210.Demberg, V., Sayeed, A., Gorinski, P., and En-gonopoulos, N. (2012).
Syntactic surprisal af-fects spoken word duration in conversationalcontexts.
In Proceedings of the 2012 JointConference on Empirical Methods in NaturalLanguage Processing and Computational Nat-ural Language Learning, pages 356?367, JejuIsland, Korea.
Association for ComputationalLinguistics.Erk, K. (2012).
Vector space models of wordmeaning and phrase meaning: A survey.
Lan-guage and Linguistics Compass, 6(10):635?653.Frank, A. F. and Jaeger, T. F. (2008).
Speaking ra-tionally: Uniform information density as an op-timal strategy for language production.
In Love,B.
C., McRae, K., and Sloutsky, V. M., editors,Proceedings of the 30thAnnual Conference ofthe Cognitive Science Society, pages 939?944.Cognitive Science Society.Frank, S. L., Otten, L. J., Galli, G., and Vigliocco,G.
(2013).
Word surprisal predicts n400 ampli-tude during reading.
In ACL (2), pages 878?883.Greenberg, C., Sayeed, A., and Demberg, V.(2015).
Improving unsupervised vector-spacethematic fit evaluation via role-filler prototypeclustering.
In Proceedings of the 2015 Confer-ence of the North American Chapter of the As-sociation for Computational Linguistics HumanLanguage Technologies (NAACL HLT).Hale, J.
(2001).
A probabilistic Earley parser as apsycholinguistic model.
In Proceedings of theSecond Meeting of the North American Chapterof the Association for Computational Linguis-tics on Language Technologies, NAACL ?01,pages 1?8, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Harris, Z. S. (1954).
Distributional structure.Word, 10(2-3):146?162.Jurafsky, D., Bell, A., Gregory, M., and Ray-mond, W. D. (2001).
Probabilistic relations be-tween words: Evidence from reduction in lexi-cal production.
Typological studies in language,45:229?254.Levy, R. (2008).
Expectation-based syntacticcomprehension.
Cognition, 106(3):1126?1177.Mitchell, J., Lapata, M., Demberg, V., and Keller,F.
(2010).
Syntactic and semantic factors inprocessing difficulty: An integrated measure.In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics,pages 196?206.
Association for ComputationalLinguistics.Mitchell, J. J.
(2011).
Composition in distribu-tional models of semantics.
PhD thesis, TheUniversity of Edinburgh.Pinheiro, J. C. and Bates, D. M. (2000).
Mixed-Effects Models in S and S-PLUS.
Statistics andComputing.
Springer.Roark, B., Bachrach, A., Cardenas, C., and Pal-lier, C. (2009).
Deriving lexical and syntacticexpectation-based measures for psycholinguis-tic modeling via incremental top-down parsing.In Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Process-ing, pages 324?333, Singapore.
Association forComputational Linguistics.Sayeed, A. and Demberg, V. (2014).
Combin-ing unsupervised syntactic and semantic mod-els of thematic fit.
In Proceedings of the firstItalian Conference on Computational Linguis-tics (CLiC-it 2014).Shannon, C. E. (1948).
A mathematical theory ofcommunication.
Bell System Technical Journal,27(379-423):623?656.772Smith, N. J. and Levy, R. (2013).
The effect ofword predictability on reading time is logarith-mic.
Cognition, 128(3):302?319.Turney, P. D. and Pantel, P. (2010).
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Re-search, 37:141?188.773
