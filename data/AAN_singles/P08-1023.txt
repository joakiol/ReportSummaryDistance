Proceedings of ACL-08: HLT, pages 192?199,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsForest-Based TranslationHaitao Mi?
Liang Huang?
Qun Liu?
?Key Lab.
of Intelligent Information Processing ?Department of Computer & Information ScienceInstitute of Computing Technology University of PennsylvaniaChinese Academy of Sciences Levine Hall, 3330 Walnut StreetP.O.
Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA{htmi,liuqun}@ict.ac.cn lhuang3@cis.upenn.eduAbstractAmong syntax-based translation models, thetree-based approach, which takes as input aparse tree of the source sentence, is a promis-ing direction being faster and simpler thanits string-based counterpart.
However, currenttree-based systems suffer from a major draw-back: they only use the 1-best parse to directthe translation, which potentially introducestranslation mistakes due to parsing errors.
Wepropose a forest-based approach that trans-lates a packed forest of exponentially manyparses, which encodes many more alternativesthan standard n-best lists.
Large-scale exper-iments show an absolute improvement of 1.7BLEU points over the 1-best baseline.
Thisresult is also 0.8 points higher than decodingwith 30-best parses, and takes even less time.1 IntroductionSyntax-based machine translation has witnessedpromising improvements in recent years.
Depend-ing on the type of input, these efforts can be di-vided into two broad categories: the string-basedsystems whose input is a string to be simultane-ously parsed and translated by a synchronous gram-mar (Wu, 1997; Chiang, 2005; Galley et al, 2006),and the tree-based systems whose input is already aparse tree to be directly converted into a target treeor string (Lin, 2004; Ding and Palmer, 2005; Quirket al, 2005; Liu et al, 2006; Huang et al, 2006).Compared with their string-based counterparts, tree-based systems offer some attractive features: theyare much faster in decoding (linear time vs. cubictime, see (Huang et al, 2006)), do not require abinary-branching grammar as in string-based mod-els (Zhang et al, 2006), and can have separate gram-mars for parsing and translation, say, a context-freegrammar for the former and a tree substitution gram-mar for the latter (Huang et al, 2006).
However, de-spite these advantages, current tree-based systemssuffer from a major drawback: they only use the 1-best parse tree to direct the translation, which po-tentially introduces translation mistakes due to pars-ing errors (Quirk and Corston-Oliver, 2006).
Thissituation becomes worse with resource-poor sourcelanguages without enough Treebank data to train ahigh-accuracy parser.One obvious solution to this problem is to take asinput k-best parses, instead of a single tree.
This k-best list postpones some disambiguation to the de-coder, which may recover from parsing errors bygetting a better translation from a non 1-best parse.However, a k-best list, with its limited scope, of-ten has too few variations and too many redundan-cies; for example, a 50-best list typically encodesa combination of 5 or 6 binary ambiguities (since25 < 50 < 26), and many subtrees are repeatedacross different parses (Huang, 2008).
It is thus inef-ficient either to decode separately with each of thesevery similar trees.
Longer sentences will also aggra-vate this situation as the number of parses grows ex-ponentially with the sentence length.We instead propose a new approach, forest-basedtranslation (Section 3), where the decoder trans-lates a packed forest of exponentially many parses,11There has been some confusion in the MT literature regard-ing the term forest: the word ?forest?
in ?forest-to-string rules?192VPPPPyu?x1:NPBVPBVVju?x??ngASlex2:NPB?
held x2 with x1Figure 1: An example translation rule (r3 in Fig.
2).which compactly encodes many more alternativesthan k-best parses.
This scheme can be seen asa compromise between the string-based and tree-based methods, while combining the advantages ofboth: decoding is still fast, yet does not commit toa single parse.
Large-scale experiments (Section 4)show an improvement of 1.7 BLEU points over the1-best baseline, which is also 0.8 points higher thandecoding with 30-best trees, and takes even less timethanks to the sharing of common subtrees.2 Tree-based systemsCurrent tree-based systems perform translation intwo separate steps: parsing and decoding.
A parserfirst parses the source language input into a 1-besttree T , and the decoder then searches for the bestderivation (a sequence of translation steps) d?
thatconverts source tree T into a target-language stringamong all possible derivations D:d?
= argmaxd?DP(d|T ).
(1)We will now proceed with a running exampletranslating from Chinese to English:(2) ?Bu`sh??Bush?yu?with/and??Sha?lo?ngSharon1>Lju?x?
?nghold?lepass.hu?`ta?ntalk2?Bush held a talk2 with Sharon1?Figure 2 shows how this process works.
The Chi-nese sentence (a) is first parsed into tree (b), whichwill be converted into an English string in 5 steps.First, at the root node, we apply rule r1 preservingtop-level word-order between English and Chinese,(r1) IP(x1:NPB x2:VP)?
x1 x2(Liu et al, 2007) was a misnomer which actually refers to a setof several unrelated subtrees over disjoint spans, and should notbe confused with the standard concept of packed forest.
(a) Bu`sh??
[yu?
Sha?lo?ng ]1 [ju?x?
?ng le hu?`ta?n ]2?
1-best parser(b) IPNPBNRBu`sh??VPPPPyu?NPBNRSha?lo?ngVPBVVju?x?
?ngASleNPBNNhu?`ta?nr1 ?
(c) NPBNRBu`sh??VPPPPyu?NPBNRSha?lo?ngVPBVVju?x?
?ngASleNPBNNhu?`ta?nr2 ?
r3 ?
(d) Bush held NPBNNhu?`ta?nwith NPBNRSha?lo?ngr4 ?
r5 ?
(e) Bush [held a talk]2 [with Sharon]1Figure 2: An example derivation of tree-to-string trans-lation.
Shaded regions denote parts of the tree that ispattern-matched with the rule being applied.which results in two unfinished subtrees in (c).
Thenrule r2 grabs the Bu`sh??
subtree and transliterate it(r2) NPB(NR(Bu`sh??))?
Bush.Similarly, rule r3 shown in Figure 1 is applied tothe VP subtree, which swaps the two NPBs, yieldingthe situation in (d).
This rule is particularly interest-ing since it has multiple levels on the source side,which has more expressive power than synchronouscontext-free grammars where rules are flat.193More formally, a (tree-to-string) translation rule(Huang et al, 2006) is a tuple ?t, s, ?
?, where t is thesource-side tree, whose internal nodes are labeled bynonterminal symbols in N , and whose frontier nodesare labeled by source-side terminals in ?
or vari-ables from a set X = {x1, x2, .
.
.
}; s ?
(X ??)?
isthe target-side string where ?
is the target languageterminal set; and ?
is a mapping from X to nonter-minals in N .
Each variable xi ?
X occurs exactlyonce in t and exactly once in s. We denote R to bethe translation rule set.
A similar formalism appearsin another form in (Liu et al, 2006).
These rules arein the reverse direction of the original string-to-treetransducer rules defined by Galley et al (2004).Finally, from step (d) we apply rules r4 and r5(r4) NPB(NN(hu?`ta?n))?
a talk(r5) NPB(NR(Sha?lo?ng))?
Sharonwhich perform phrasal translations for the two re-maining subtrees, respectively, and get the Chinesetranslation in (e).3 Forest-based translationWe now extend the tree-based idea from the previ-ous section to the case of forest-based translation.Again, there are two steps, parsing and decoding.In the former, a (modified) parser will parse the in-put sentence and output a packed forest (Section 3.1)rather than just the 1-best tree.
Such a forest is usu-ally huge in size, so we use the forest pruning algo-rithm (Section 3.4) to reduce it to a reasonable size.The pruned parse forest will then be used to directthe translation.In the decoding step, we first convert the parse for-est into a translation forest using the translation ruleset, by similar techniques of pattern-matching fromtree-based decoding (Section 3.2).
Then the decodersearches for the best derivation on the translationforest and outputs the target string (Section 3.3).3.1 Parse ForestInformally, a packed parse forest, or forest in short,is a compact representation of all the derivations(i.e., parse trees) for a given sentence under acontext-free grammar (Billot and Lang, 1989).
Forexample, consider the Chinese sentence in Exam-ple (2) above, which has (at least) two readings de-pending on the part-of-speech of the word yu?, whichcan be either a preposition (P ?with?)
or a conjunc-tion (CC ?and?).
The parse tree for the prepositioncase is shown in Figure 2(b) as the 1-best parse,while for the conjunction case, the two proper nouns(Bu`sh??
and Sha?lo?ng) are combined to form a coordi-nated NPNPB0,1 CC1,2 NPB2,3NP0,3 (*)which functions as the subject of the sentence.
Inthis case the Chinese sentence is translated into(3) ?
[Bush and Sharon] held a talk?.Shown in Figure 3(a), these two parse trees canbe represented as a single forest by sharing commonsubtrees such as NPB0,1 and VPB3,6.
Such a foresthas a structure of a hypergraph (Klein and Manning,2001; Huang and Chiang, 2005), where items likeNP0,3 are called nodes, and deductive steps like (*)correspond to hyperedges.More formally, a forest is a pair ?V,E?, where Vis the set of nodes, and E the set of hyperedges.
Fora given sentence w1:l = w1 .
.
.
wl, each node v ?
Vis in the form of X i,j , which denotes the recogni-tion of nonterminal X spanning the substring frompositions i through j (that is, wi+1 .
.
.
wj).
Each hy-peredge e ?
E is a pair ?tails(e), head(e)?, wherehead(e) ?
V is the consequent node in the deduc-tive step, and tails(e) ?
V ?
is the list of antecedentnodes.
For example, the hyperedge for deduction (*)is notated:?
(NPB0,1, CC1,2, NPB2,3), NP0,3?.There is also a distinguished root node TOP ineach forest, denoting the goal item in parsing, whichis simply S0,l where S is the start symbol and l is thesentence length.3.2 Translation ForestGiven a parse forest and a translation rule setR, wecan generate a translation forest which has a simi-lar hypergraph structure.
Basically, just as the depth-first traversal procedure in tree-based decoding (Fig-ure 2), we visit in top-down order each node v in the194(a)IP0,6NP0,3NPB0,1NR0,1Bu`sh?
?CC1,2yu?VP1,6PP1,3P1,2 NPB2,3NR2,3Sha?lo?ngVPB3,6VV3,4ju?x??ngAS4,5leNPB5,6NN5,6hu?`ta?n?
translation rule setR(b)IP0,6NP0,3NPB0,1 CC1,2VP1,6PP1,3P1,2 NPB2,3VPB3,6VV3,4 AS4,5 NPB5,6e5e2e6e4 e3e1(c)translation hyperedge translation rulee1 r1 IP(x1:NPB x2:VP)?
x1 x2e2 r6 IP(x1:NP x2:VPB)?
x1 x2e3 r3 VP(PP(P(yu?)
x1:NPB) VPB(VV(ju?x?
?ng) AS(le) x2:NPB))?
held x2 with x1e4 r7 VP(PP(P(yu?)
x1:NPB) x2:VPB)?
x2 with x1e5 r8 NP(x1:NPB CC(yu?)
x2:NPB)?
x1 and x2e6 r9 VPB(VV(ju?x?
?ng) AS(le) x1:NPB)?
held x1Figure 3: (a) the parse forest of the example sentence; solid hyperedges denote the 1-best parse in Figure 2(b) whiledashed hyperedges denote the alternative parse due to Deduction (*).
(b) the corresponding translation forest afterapplying the translation rules (lexical rules not shown); the derivation shown in bold solid lines (e1 and e3) correspondsto the derivation in Figure 2; the one shown in dashed lines (e2, e5, and e6) uses the alternative parse and correspondsto the translation in Example (3).
(c) the correspondence between translation hyperedges and translation rules.parse forest, and try to pattern-match each transla-tion rule r against the local sub-forest under node v.For example, in Figure 3(a), at node VP1,6, two rulesr3 and r7 both matches the local subforest, and willthus generate two translation hyperedges e3 and e4(see Figure 3(b-c)).More formally, we define a function match(r, v)which attempts to pattern-match rule r at node v inthe parse forest, and in case of success, returns alist of descendent nodes of v that are matched to thevariables in r, or returns an empty list if the matchfails.
Note that this procedure is recursive and may195Pseudocode 1 The conversion algorithm.1: Input: parse forest Hp and rule setR2: Output: translation forest Ht3: for each node v ?
Vp in top-down order do4: for each translation rule r ?
R do5: vars ?
match(r, v) ?
variables6: if vars is not empty then7: e?
?vars, v, s(r)?8: add translation hyperedge e to Htinvolve multiple parse hyperedges.
For example,match(r3,VP1,6) = (NPB2,3, NPB5,6),which covers three parse hyperedges, while nodesin gray do not pattern-match any rule (although theyare involved in the matching of other nodes, wherethey match interior nodes of the source-side treefragments in a rule).
We can thus construct a transla-tion hyperedge from match(r, v) to v for each nodev and rule r. In addition, we also need to keep trackof the target string s(r) specified by rule r, which in-cludes target-language terminals and variables.
Forexample, s(r3) = ?held x2 with x1?.
The subtrans-lations of the matched variable nodes will be sub-stituted for the variables in s(r) to get a completetranslation for node v. So a translation hyperedge eis a triple ?tails(e), head(e), s?
where s is the targetstring from the rule, for example,e3 = ?
(NPB2,3, NPB5,6),VP1,6, ?held x2 with x1?
?.This procedure is summarized in Pseudocode 1.3.3 Decoding AlgorithmsThe decoder performs two tasks on the translationforest: 1-best search with integrated language model(LM), and k-best search with LM to be used in min-imum error rate training.
Both tasks can be done ef-ficiently by forest-based algorithms based on k-bestparsing (Huang and Chiang, 2005).For 1-best search, we use the cube pruning tech-nique (Chiang, 2007; Huang and Chiang, 2007)which approximately intersects the translation forestwith the LM.
Basically, cube pruning works bottomup in a forest, keeping at most k +LM items at eachnode, and uses the best-first expansion idea from theAlgorithm 2 of Huang and Chiang (2005) to speedup the computation.
An +LM item of node v has theform (va?b), where a and b are the target-languageboundary words.
For example, (VP held ?
Sharon1,6 ) is an+LM item with its translation starting with ?held?and ending with ?Sharon?.
This scheme can be eas-ily extended to work with a general n-gram by stor-ing n?
1 words at both ends (Chiang, 2007).For k-best search after getting 1-best derivation,we use the lazy Algorithm 3 of Huang and Chiang(2005) that works backwards from the root node,incrementally computing the second, third, throughthe kth best alternatives.
However, this time we workon a finer-grained forest, called translation+LM for-est, resulting from the intersection of the translationforest and the LM, with its nodes being the +LMitems during cube pruning.
Although this new forestis prohibitively large, Algorithm 3 is very efficientwith minimal overhead on top of 1-best.3.4 Forest Pruning AlgorithmWe use the pruning algorithm of (Jonathan Graehl,p.c.
; Huang, 2008) that is very similar to the methodbased on marginal probability (Charniak and John-son, 2005), except that it prunes hyperedges as wellas nodes.
Basically, we use an Inside-Outside algo-rithm to compute the Viterbi inside cost ?
(v) and theViterbi outside cost ?
(v) for each node v, and thencompute the merit ??
(e) for each hyperedge:??
(e) = ?
(head(e)) +?ui?tails(e)?
(ui) (4)Intuitively, this merit is the cost of the best derivationthat traverses e, and the difference ?
(e) = ??
(e) ??
(TOP) can be seen as the distance away from theglobally best derivation.
We prune away a hyper-edge e if ?
(e) > p for a threshold p. Nodes withall incoming hyperedges pruned are also pruned.4 ExperimentsWe can extend the simple model in Equation 1 to alog-linear one (Liu et al, 2006; Huang et al, 2006):d?
= argmaxd?DP(d | T )?0 ?
e?1|d| ?
Plm(s)?2 ?
e?3|s|(5)where T is the 1-best parse, e?1|d| is the penalty termon the number of rules in a derivation, Plm(s) is thelanguage model and e?3|s| is the length penalty term196on target translation.
The derivation probability con-ditioned on 1-best tree, P(d | T ), should now bereplaced by P(d | Hp) where Hp is the parse forest,which decomposes into the product of probabilitiesof translation rules r ?
d:P(d | Hp) =?r?dP(r) (6)where each P(r) is the product of five probabilities:P(r) = P(t | s)?4 ?
Plex(t | s)?5 ?P(s | t)?6 ?
Plex(s | t)?7 ?
P(t | Hp)?8 .
(7)Here t and s are the source-side tree and target-side string of rule r, respectively, P(t | s) andP(s | t) are the two translation probabilities, andPlex(?)
are the lexical probabilities.
The only extraterm in forest-based decoding is P(t | Hp) denot-ing the source side parsing probability of the currenttranslation rule r in the parse forest, which is theproduct of probabilities of each parse hyperedge epcovered in the pattern-match of t against Hp (whichcan be recorded at conversion time):P(t | Hp) =?ep?Hp, ep covered by tP(ep).
(8)4.1 Data preparationOur experiments are on Chinese-to-English transla-tion, and we use the Chinese parser of Xiong et al(2005) to parse the source side of the bitext.
Follow-ing Huang (2008), we modify the parser to output apacked forest for each sentence.Our training corpus consists of 31,011 sentencepairs with 0.8M Chinese words and 0.9M Englishwords.
We first word-align them by GIZA++ refinedby ?diagand?
from Koehn et al (2003), and applythe tree-to-string rule extraction algorithm (Galley etal., 2006; Liu et al, 2006), which resulted in 346Ktranslation rules.
Note that our rule extraction is stilldone on 1-best parses, while decoding is on k-bestparses or packed forests.
We also use the SRI Lan-guage Modeling Toolkit (Stolcke, 2002) to train atrigram language model with Kneser-Ney smooth-ing on the English side of the bitext.We use the 2002 NIST MT Evaluation test set asour development set (878 sentences) and the 20050.2300.2320.2340.2360.2380.2400.2420.2440.2460.2480.2500  5  10  15  20  25  30  35BLEUscoreaverage decoding time (secs/sentence)1-bestp=5p=12k=10k=30k=100k-best treesforests decodingFigure 4: Comparison of decoding on forests with decod-ing on k-best trees.NIST MT Evaluation test set as our test set (1082sentences), with on average 28.28 and 26.31 wordsper sentence, respectively.
We evaluate the transla-tion quality using the case-sensitive BLEU-4 met-ric (Papineni et al, 2002).
We use the standard min-imum error-rate training (Och, 2003) to tune the fea-ture weights to maximize the system?s BLEU scoreon the dev set.
On dev and test sets, we prune theChinese parse forests by the forest pruning algo-rithm in Section 3.4 with a threshold of p = 12, andthen convert them into translation forests using thealgorithm in Section 3.2.
To increase the coverageof the rule set, we also introduce a default transla-tion hyperedge for each parse hyperedge by mono-tonically translating each tail node, so that we canalways at least get a complete translation in the end.4.2 ResultsThe BLEU score of the baseline 1-best decoding is0.2325, which is consistent with the result of 0.2302in (Liu et al, 2007) on the same training, develop-ment and test sets, and with the same rule extrac-tion procedure.
The corresponding BLEU score ofPharaoh (Koehn, 2004) is 0.2182 on this dataset.Figure 4 compares forest decoding with decodingon k-best trees in terms of speed and quality.
Us-ing more than one parse tree apparently improves theBLEU score, but at the cost of much slower decod-ing, since each of the top-k trees has to be decodedindividually although they share many common sub-trees.
Forest decoding, by contrast, is much faster19705101520250  10  20  30  40  50  60  70  80  90  100Percentageof sentences(%)i (rank of the parse tree picked by the decoder)forest decoding30-best treesFigure 5: Percentage of the i-th best parse tree beingpicked in decoding.
32% of the distribution for forest de-coding is beyond top-100 and is not shown on this plot.and produces consistently better BLEU scores.
Withpruning threshold p = 12, it achieved a BLEUscore of 0.2485, which is an absolute improvementof 1.6% points over the 1-best baseline, and is statis-tically significant using the sign-test of Collins et al(2005) (p < 0.01).We also investigate the question of how often theith-best parse tree is picked to direct the translation(i = 1, 2, .
.
.
), in both k-best and forest decodingschemes.
A packed forest can be roughly viewed asa (virtual)?-best list, and we can thus ask how of-ten is a parse beyond top-k used by a forest, whichrelates to the fundamental limitation of k-best lists.Figure 5 shows that, the 1-best parse is still preferred25% of the time among 30-best trees, and 23% ofthe time by the forest decoder.
These ratios decreasedramatically as i increases, but the forest curve has amuch longer tail in large i.
Indeed, 40% of the treespreferred by a forest is beyond top-30, 32% is be-yond top-100, and even 20% beyond top-1000.
Thisconfirms the fact that we need exponentially large k-best lists with the explosion of alternatives, whereasa forest can encode these information compactly.4.3 Scaling to large dataWe also conduct experiments on a larger dataset,which contains 2.2M training sentence pairs.
Be-sides the trigram language model trained on the En-glish side of these bitext, we also use another tri-gram model trained on the first 1/3 of the Xinhuaportion of Gigaword corpus.
The two LMs have dis-approach \ ruleset TR TR+BP1-best tree 0.2666 0.293930-best trees 0.2755 0.3084forest (p = 12) 0.2839 0.3149Table 1: BLEU score results from training on large data.tinct weights tuned by minimum error rate training.The dev and test sets remain the same as above.Furthermore, we also make use of bilingualphrases to improve the coverage of the ruleset.
Fol-lowing Liu et al (2006), we prepare a phrase-tablefrom a phrase-extractor, e.g.
Pharaoh, and at decod-ing time, for each node, we construct on-the-fly flattranslation rules from phrases that match the source-side span of the node.
These phrases are called syn-tactic phrases which are consistent with syntacticconstituents (Chiang, 2005), and have been shown tobe helpful in tree-based systems (Galley et al, 2006;Liu et al, 2006).The final results are shown in Table 1, where TRdenotes translation rule only, and TR+BP denotesthe inclusion of bilingual phrases.
The BLEU scoreof forest decoder with TR is 0.2839, which is a 1.7%points improvement over the 1-best baseline, andthis difference is statistically significant (p < 0.01).Using bilingual phrases further improves the BLEUscore by 3.1% points, which is 2.1% points higherthan the respective 1-best baseline.
We suspect thislarger improvement is due to the alternative con-stituents in the forest, which activates many syntac-tic phrases suppressed by the 1-best parse.5 Conclusion and future workWe have presented a novel forest-based translationapproach which uses a packed forest rather than the1-best parse tree (or k-best parse trees) to direct thetranslation.
Forest provides a compact data-structurefor efficient handling of exponentially many treestructures, and is shown to be a promising direc-tion with state-of-the-art translation results and rea-sonable decoding speed.
This work can thus beviewed as a compromise between string-based andtree-based paradigms, with a good trade-off betweenspeed and accuarcy.
For future work, we would liketo use packed forests not only in decoding, but alsofor translation rule extraction during training.198AcknowledgementPart of this work was done while L. H. was visit-ing CAS/ICT.
The authors were supported by Na-tional Natural Science Foundation of China, Con-tracts 60736014 and 60573188, and 863 State KeyProject No.
2006AA010108 (H. M and Q. L.), andby NSF ITR EIA-0205456 (L. H.).
We would alsolike to thank Chris Quirk for inspirations, YangLiu for help with rule extraction, Mark Johnson forposing the question of virtual ?-best list, and theanonymous reviewers for suggestions.ReferencesSylvie Billot and Bernard Lang.
1989.
The structure ofshared forests in ambiguous parsing.
In Proceedingsof ACL ?89, pages 143?151.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine-grained n-best parsing and discriminative rerank-ing.
In Proceedings of the 43rd ACL.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL, pages 263?270, Ann Arbor, Michigan, June.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Comput.
Linguist., 33(2):201?228.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL, pages 531?540,Ann Arbor, Michigan, June.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependency in-sertion grammars.
In Proceedings of ACL, pages 541?548, Ann Arbor, Michigan, June.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In HLT-NAACL, pages 273?280, Boston, MA.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of COLING-ACL, pages 961?968, Sydney, Aus-tralia, July.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of Ninth International Work-shop on Parsing Technologies (IWPT-2005), Vancou-ver, Canada.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of ACL, pages 144?151, Prague, CzechRepublic, June.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA, Boston,MA, August.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL, Columbus, OH.Dan Klein and Christopher D. Manning.
2001.
Parsingand Hypergraphs.
In Proceedings of the Seventh In-ternational Workshop on Parsing Technologies (IWPT-2001), 17-19 October 2001, Beijing, China.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT-NAACL, Edmonton, AB, Canada.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of AMTA, pages 115?124.Dekang Lin.
2004.
A path-based transfer model for ma-chine translation.
In Proceedings of the 20th COLING,Barcelona, Spain.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of COLING-ACL, pages 609?616, Sydney, Australia, July.Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007.Forest-to-string statistical translation rules.
In Pro-ceedings of ACL, pages 704?711, Prague, Czech Re-public, June.Franz J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of ACL,pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL,pages 311?318, Philadephia, USA, July.Chris Quirk and Simon Corston-Oliver.
2006.
The im-pact of parse quality on syntactically-informed statis-tical machine translation.
In Proceedings of EMNLP.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of ACL, pages 271?279,Ann Arbor, Michigan, June.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP, vol-ume 30, pages 901?904.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin.2005.
Parsing the Penn Chinese Treebank with seman-tic knowledge.
In Proceedings of IJCNLP 2005, pages70?81, Jeju Island, South Korea.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for ma-chine translation.
In Proceedings of HLT-NAACL,New York, NY.199
