Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
228?237, Prague, June 2007. c?2007 Association for Computational LinguisticsSemi-Supervised Classification for Extracting Protein Interaction Sentencesusing Dependency ParsingGu?nes?
ErkanUniversity of Michigangerkan@umich.eduArzucan ?Ozgu?rUniversity of Michiganozgur@umich.eduDragomir R. RadevUniversity of Michiganradev@umich.eduAbstractWe introduce a relation extraction method toidentify the sentences in biomedical text thatindicate an interaction among the proteinnames mentioned.
Our approach is based onthe analysis of the paths between two proteinnames in the dependency parse trees of thesentences.
Given two dependency trees, wedefine two separate similarity functions (ker-nels) based on cosine similarity and edit dis-tance among the paths between the proteinnames.
Using these similarity functions, weinvestigate the performances of two classesof learning algorithms, Support Vector Ma-chines and k-nearest-neighbor, and the semi-supervised counterparts of these algorithms,transductive SVMs and harmonic functions,respectively.
Significant improvement overthe previous results in the literature is re-ported as well as a new benchmark datasetis introduced.
Semi-supervised algorithmsperform better than their supervised ver-sion by a wide margin especially when theamount of labeled data is limited.1 IntroductionProtein-protein interactions play an important rolein vital biological processes such as metabolic andsignaling pathways, cell cycle control, and DNAreplication and transcription (Phizicky and Fields,1995).
A number of (mostly manually curated)databases such as MINT (Zanzoni et al, 2002),BIND (Bader et al, 2003), and SwissProt (Bairochand Apweiler, 2000) have been created to store pro-tein interaction information in structured and stan-dard formats.
However, the amount of biomedicalliterature regarding protein interactions is increas-ing rapidly and it is difficult for interaction databasecurators to detect and curate protein interaction in-formation manually.
Thus, most of the protein in-teraction information remains hidden in the text ofthe papers in the biomedical literature.
Therefore,the development of information extraction and textmining techniques for automatic extraction of pro-tein interaction information from free texts has be-come an important research area.In this paper, we introduce an information extrac-tion approach to identify sentences in text that in-dicate an interaction relation between two proteins.Our method is different than most of the previousstudies (see Section 2) on this problem in two as-pects: First, we generate the dependency parses ofthe sentences that we analyze, making use of thedependency relationships among the words.
Thisenables us to make more syntax-aware inferencesabout the roles of the proteins in a sentence com-pared to the classical pattern-matching informationextraction methods.
Second, we investigate semi-supervised machine learning methods on top of thedependency features we generate.
Although therehave been a number of learning-based studies in thisdomain, our methods are the first semi-supervisedefforts to our knowledge.
The high cost of label-ing free text for this problem makes semi-supervisedmethods particularly valuable.We focus on two semi-supervised learning meth-ods: transductive SVMs (TSVM) (Joachims, 1999),228and harmonic functions (Zhu et al, 2003).
We alsocompare these two methods with their supervisedcounterparts, namely SVMs and k-nearest neigh-bor algorithm.
Because of the nature of these al-gorithms, we propose two similarity functions (ker-nels in SVM terminology) among the instances ofthe learning problem.
The instances in this problemare natural language sentences with protein names inthem, and the similarity functions are defined on thepositions of the protein names in the correspondingparse trees.
Our motivating assumption is that thepath between two protein names in a dependencytree is a good description of the semantic relationbetween them in the corresponding sentence.
Weconsider two similarity functions; one based on thecosine similarity and the other based on the edit dis-tance among such paths.2 Related WorkThere have been many approaches to extract pro-tein interactions from free text.
One of them isbased on matching pre-specified patterns and rules(Blaschke et al, 1999; Ono et al, 2001).
How-ever, complex cases that are not covered by thepre-defined patterns and rules cannot be extractedby these methods.
Huang et al (2004) proposed amethod where patterns are discovered automaticallyfrom a set of sentences by dynamic programming.Bunescu et al (2005) have studied the performanceof rule learning algorithms.
They propose two meth-ods for protein interaction extraction.
One is basedon the rule learning method Rapier and the otheron longest common subsequences.
They show thatthese methods outperform hand-written rules.Another class of approaches is using more syntax-aware natural language processing (NLP) tech-niques.
Both full and partial (shallow) parsingstrategies have been applied in the literature.
Inpartial parsing the sentence structure is decomposedpartially and local dependencies between certainphrasal components are extracted.
An example ofthe application of this method is relational parsingfor the inhibition relation (Pustejovsky et al, 2002).In full parsing, however, the full sentence structureis taken into account.
Temkin and Gilder (2003)used a full parser with a lexical analyzer and a con-text free grammar (CFG) to extract protein-proteininteraction from text.
Another study that uses full-sentence parsing to extract human protein interac-tions is (Daraselia et al, 2004).
Alternatively,Yakushiji et al (2005) propose a system based onhead-driven phrase structure grammar (HPSG).
Intheir system protein interaction expressions are pre-sented as predicate argument structure patterns fromthe HPSG parser.
These parsing approaches con-sider only syntactic properties of the sentences anddo not take into account semantic properties.
Thus,although they are complicated and require many re-sources, their performance is not satisfactory.Machine learning techniques for extracting pro-tein interaction information have gained interest inthe recent years.
The PreBIND system uses SVM toidentify the existence of protein interactions in ab-stracts and uses this type of information to enhancemanual expert reviewing for the BIND database(Donaldson et al, 2003).
Words and word bigramsare used as binary features.
This system is alsotested with the Naive Bayes classifier, but SVM isreported to perform better.
Mitsumori et al (2006)also use SVM to extract protein-protein interac-tions.
They use bag-of-words features, specificallythe words around the protein names.
These sys-tems do not use any syntactic or semantic informa-tion.
Sugiyama et al (2003) extract features fromthe sentences based on the verbs and nouns in thesentences such as the verbal forms, and the part ofspeech tags of the 20 words surrounding the verb(10 before and 10 after it).
Further features are usedto indicate whether a noun is found, as well as thepart of speech tags for the 20 words surroundingthe noun, and whether the noun contains numeri-cal characters, non-alpha characters, or uppercaseletters.
They construct k-nearest neighbor, decisiontree, neural network, and SVM classifiers by usingthese features.
They report that the SVM classifierperforms the best.
They use part-of-speech informa-tion, but do not consider any dependency or seman-tic information.The paper is organized as follows.
In Section 3 wedescribe our method of extracting features from thedependency parse trees of the sentences and defin-ing the similarity between two sentences.
In Section4 we discuss our supervised and semi-supervisedmethods.
In Section 5 we describe the data sets andevaluation metrics that we used, and present our re-229sults.
We conclude in Section 6.3 Sentence Similarity Based onDependency ParsingIn order to apply the semi-supervised harmonicfunctions and its supervised counterpart kNN, andthe kernel based TSVM and SVM methods, we needto define a similarity measure between two sen-tences.
For this purpose, we use the dependencyparse trees of the sentences.
Unlike a syntactic parse(which describes the syntactic constituent structureof a sentence), the dependency parse of a sentencecaptures the semantic predicate-argument relation-ships among its words.
The idea of using depen-dency parse trees for relation extraction in generalwas studied by Bunescu and Mooney (2005a).
Toextract the relationship between two entities, theydesign a kernel function that uses the shortest path inthe dependency tree between them.
The motivationis based on the observation that the shortest path be-tween the entities usually captures the necessary in-formation to identify their relationship.
They showthat their approach outperforms the dependency treekernel of Culotta and Sorensen (2004), which isbased on the subtree that contains the two entities.We adapt the idea of Bunescu and Mooney (2005a)to the task of identifying protein-protein interactionsentences.
We define the similarity between twosentences based on the paths between two proteinsin the dependency parse trees of the sentences.In this study we assume that the protein nameshave already been annotated and focus instead onthe task of extracting protein-protein interaction sen-tences for a given protein pair.
We parse the sen-tences with the Stanford Parser1 (de Marneffe et al,2006).
From the dependency parse trees of each sen-tence we extract the shortest path between a proteinpair.For example, Figure 1 shows the dependency treewe got for the sentence ?The results demonstratedthat KaiC interacts rhythmically with KaiA, KaiB,and SasA.?
This example sentence illustrates thatthe dependency path between a protein pair capturesthe relevant information regarding the relationshipbetween the proteins better compared to using thewords in the unparsed sentence.
Consider the pro-1http://nlp.stanford.edu/software/lex-parser.shtmltein pair KaiC and SasA.
The words in the sentencebetween these proteins are interacts, rhythmically,with, KaiA, KaiB, and and.
Among these wordsrhythmically, KaiA, and and KaiB are not directlyrelated to the interaction relationship between KaiCand SasA.
On the other hand, the words in the depen-dency path between this protein pair give sufficientinformation to identify their relationship.In this sentence we have four proteins (KaiC,KaiA, KaiB, and SasA).
So there are six pairs ofproteins for which a sentence may or may not be de-scribing an interaction.
The following are the pathsbetween the six protein pairs.
In this example thereis a single path between each protein pair.
However,there may be more than one paths between a pro-tein pair, if one or both appear multiple times in thesentence.
In such cases, we select the shortest pathsbetween the protein pairs.ccompprep_withresults interactsTheKaiA KaiBrhytmically SasAthat KaiCdemonstratednsubjcomplm nsubjadvmodconj_and conj_anddetFigure 1: The dependency tree of the sentence ?Theresults demonstrated that KaiC interacts rhythmi-cally with KaiA, KaiB, and SasA.?1.
KaiC - nsubj - interacts - prep with - SasA2.
KaiC - nsubj - interacts - prep with - SasA - conj and -KaiA3.
KaiC - nsubj - interacts - prep with ?
SasA - conj and -KaiB4.
SasA - conj and - KaiA5.
SasA - conj and - KaiB6.
KaiA ?
conj and ?
SasA - conj and - KaiBIf a sentence contains n different proteins, thereare(n2)different pairs of proteins.
We use machinelearning approaches to classify each sentence as aninteraction sentence or not for a protein pair.
A sen-tence may be an interaction sentence for one protein230pair, while not for another protein pair.
For instance,our example sentence is a positive interaction sen-tence for the KaiC and SasA protein pair.
However,it is a negative interaction sentence for the KaiA andSasA protein pair, i.e., it does not describe an inter-action between this pair of proteins.
Thus, beforeparsing a sentence, we make multiple copies of it,one for each protein pair.
To reduce data sparseness,we rename the proteins in the pair as PROTX1 andPROTX2, and all the other proteins in the sentenceas PROTX0.
So, for our example sentence we havethe following instances in the training set:1.
PROTX1 - nsubj - interacts - prep with - PROTX22.
PROTX1 - nsubj - interacts - prep with - PROTX0 -conj and - PROTX23.
PROTX1 - nsubj - interacts - prep with ?
PROTX0 -conj and - PROTX24.
PROTX1 - conj and - PROTX25.
PROTX1 - conj and - PROTX26.
PROTX1 ?
conj and ?
PROTX0 - conj and - PROTX2The first three instances are positive as they describean interaction between PROTX1 and PROTX2.
Thelast three are negative, as they do not describe aninteraction between PROTX1 and PROTX2.We define the similarity between two instancesbased on cosine similarity and edit distance basedsimilarity between the paths in the instances.3.1 Cosine SimilaritySuppose pi and pj are the paths between PROTX1and PROTX2 in instance xi and instance xj , respec-tively.
We represent pi and pj as vectors of termfrequencies in the vector-space model.
The cosinesimilarity measure is the cosine of the angle betweenthese two vectors and is calculated as follows:cos sim(pi, pj) = cos(pi,pj) =pi ?
pj?pi??pj?
(1)that is, it is the dot product of pi and pj divided bythe lengths of pi and pj.
The cosine similarity mea-sure takes values in the range [0, 1].
If all the termsin pi and pj are common, then it takes the maximumvalue of 1.
If none of the terms are common, then ittakes the minimum value of 0.3.2 Similarity Based on Edit DistanceA shortcoming of cosine similarity is that it onlytakes into account the common terms, but does notconsider their order in the path.
For this reason, wealso use a similarity measure based on edit distance(also called Levenshtein distance).
Edit distance be-tween two strings is the minimum number of op-erations that have to be performed to transform thefirst string to the second.
In the original character-based edit distance there are three types of opera-tions.
These are insertion, deletion, or substitutionof a single character.
We modify the character-basededit distance into a word-based one, where the oper-ations are defined as insertion, deletion, or substitu-tion of a single word.The edit distance between path 1 and path 2 ofour example sentence is 2.
We insert PROTX0 andconj and to path 1 to convert it to path 2.1.
PROTX1 - nsubj - interacts - prep with - insert (PROTX0)- insert (conj and) ?
PROTX22.
PROTX1 - nsubj - interacts - prep with - PROTX0 -conj and - PROTX2We normalize edit distance by dividing it by thelength (number of words) of the longer path, so thatit takes values in the range [0, 1].
We convert the dis-tance measure into a similarity measure as follows.edit sim(pi, pj) = e??
(edit distance(pi,pj)) (2)Bunescu and Mooney (2005a) propose a similarmethod for relation extraction in general.
However,their similarity measure is based on the number ofthe overlapping words between two paths.
Whentwo paths have different lengths, they assume thesimilarity between them is zero.
On the other hand,our edit distance based measure can also account fordeletions and insertions of words.4 Semi-Supervised Machine LearningApproaches4.1 kNN and Harmonic FunctionsWhen a similarity measure is defined among the in-stances of a learning problem, a simple and naturalchoice is to use a nearest neighbor based approachthat classifies each instance by looking at the labelsof the instances that are most similar to it.
Per-haps the simplest and most popular similarity-based231learning algorithm is the k-nearest neighbor classifi-cation method (kNN).
Let U be the set of unlabeledinstances, and L be the set of labeled instances ina learning problem.
Given an instance x ?
U , letNLk (x) be the set of top k instances in L that aremost similar to x with respect to some similaritymeasure.
The kNN equation for a binary classifi-cation problem can be written as:y(x) =?z?NLk (x)sim(x, z)y(z)?z?
?NLk (x)sim(x, z?)
(3)where y(z) ?
{0, 1} is the label of the instance z.2Note that y(x) can take any real value in the [0, 1]interval.
The final classification decision is made bysetting a threshold in this interval (e.g.
0.5) and clas-sifying the instances above the threshold as positiveand others as negative.
For our problem, each in-stance is a dependency path between the proteins inthe pair and the similarity function can be one of thefunctions we have defined in Section 3.Equation 3 can be seen as averaging the labels (0or 1) of the nearest neighbors of each unlabeled in-stance.
This suggests a generalized semi-supervisedversion of the same algorithm by incorporating un-labeled instances as neighbors as well:y(x) =?z?NL?Uk (x)sim(x, z)y(z)?z?
?NL?Uk (x)sim(x, z?)
(4)Unlike Equation 3, the unlabeled instances are alsoconsidered in Equation 4 when finding the nearestneighbors.
We can visualize this as an undirectedgraph, where each data instance (labeled or unla-beled) is a node that is connected to its k nearestneighbor nodes.
The value of y(?)
is set to 0 or 1for labeled nodes depending on their class.
For eachunlabeled node x, y(x) is equal to the average of they(?)
values of its neighbors.
Such a function thatsatisfies the average property on all unlabeled nodesis called a harmonic function and is known to existand have a unique solution (Doyle and Snell, 1984).Harmonic functions were first introduced as a semi-supervised learning method by Zhu et al (2003).There are interesting alternative interpretations of2Equation 3 is the weighted (or soft) version of the kNNalgorithm.
In the classical voting scheme, x is classified in thecategory that the majority of its neighbors belong to.a harmonic function on a graph.
One of them canbe explained in terms of random walks on a graph.Consider a random walk on a graph where at eachtime point we move from the current node to one ofits neighbors.
The next node is chosen among theneighbors of the current node with probability pro-portional to the weight (similarity) of the edge thatconnects the two nodes.
Assuming we start the ran-dom walk from the node x, y(x) in Equation 4 isthen equal to the probability that this random walkwill hit a node labeled 1 before it hits a node labeled0.4.2 Transductive SVMSupport vector machines (SVM) is a supervised ma-chine learning approach designed for solving two-class pattern recognition problems.
The aim is tofind the decision surface that separates the positiveand negative labeled training examples of a classwith maximum margin (Burges, 1998).Transductive support vector machines (TSVM)are an extension of SVM, where unlabeled data isused in addition to labeled data.
The aim now isto assign labels to the unlabeled data and find a de-cision surface that separates the positive and nega-tive instances of the original labeled data and the(now labeled) unlabeled data with maximum mar-gin.
Intuitively, the unlabeled data pushes the deci-sion boundary away from the dense regions.
How-ever, unlike SVM, the optimization problem nowis NP-hard (Zhu, 2005).
Pointers to studies forapproximation algorithms can be found in (Zhu,2005).In Section 3 we defined the similarity betweentwo instances based on the cosine similarity andthe edit distance based similarity between the pathsin the instances.
Here, we use these path similar-ity measures as kernels for SVM and TSVM andmodify the SV M light package (Joachims, 1999) byplugging in our two kernel functions.A well-defined kernel function should be sym-metric positive definite.
While cosine kernel is well-defined, Cortes et al (2004) proved that edit kernelis not always positive definite.
However, it is pos-sible to make the kernel matrix positive definite byadjusting the ?
parameter, which is a positive realnumber.
Li and Jiang (2005) applied the edit kernelto predict initiation sites in eucaryotic mRNAs and232obtained improved results compared to polynomialkernel.5 Experimental Results5.1 Data SetsOne of the problems in the field of protein-proteininteraction extraction is that different studies gen-erally use different data sets and evaluation met-rics.
Thus, it is difficult to compare their re-sults.
Bunescu et al (2005) manually developed theAIMED corpus3 for protein-protein interaction andprotein name recognition.
They tagged 199 Medlineabstracts, obtained from the Database of InteractingProteins (DIP) (Xenarios et al, 2001) and known tocontain protein interactions.
This corpus is becom-ing a standard, as it has been used in the recent stud-ies by (Bunescu et al, 2005; Bunescu and Mooney,2005b; Bunescu and Mooney, 2006; Mitsumori etal., 2006; Yakushiji et al, 2005).In our study we used the AIMED corpus and theCB (Christine Brun) corpus that is provided as a re-source by BioCreAtIvE II (Critical Assessment forInformation Extraction in Biology) challenge eval-uation4.
We pre-processed the CB corpus by firstannotating the protein names in the corpus automat-ically and then, refining the annotation manually.
Asdiscussed in Section 3, we pre-processed both of thedata sets as follows.
We replicated each sentencefor each different protein pair.
For n different pro-teins in a sentence,(n2)new sentences are created,as there are that many different pairs of proteins.In each newly created sentence we marked the pro-tein pair considered for interaction as PROTX1 andPROTX2, and all the remaining proteins in the sen-tence as PROTX0.
If a sentence describes an inter-action between PROTX1 and PROTX2, it is labeledas positive, otherwise it is labeled as negative.
Thesummary of the data sets after pre-processing is dis-played in Table 15.Since previous studies that use AIMED corpusperform 10-fold cross-validation.
We also per-formed 10-fold cross-validation in both data sets andreport the average results over the runs.3ftp://ftp.cs.utexas.edu/pub/mooney/bio-data/4http://biocreative.sourceforge.net/biocreative 2.html5The pre-processed data sets are available athttp://belobog.si.umich.edu/clair/biocreativeData Set Sentences + Sentences - SentencesAIMED 4026 951 3075CB 4056 2202 1854Table 1: Data Sets5.2 Evaluation MetricsWe use precision, recall, and F-score as our metricsto evaluate the performances of the methods.
Preci-sion (pi) and recall (?)
are defined as follows:pi = TPTP + FP ; ?
=TPTP + FN (5)Here, TP (True Positives) is the number of sen-tences classified correctly as positive; FP (FalsePositives) is the number of negative sentences thatare classified as positive incorrectly by the classifier;and FN (False Negatives) is the number of positivesentences that are classified as negative incorrectlyby the classifier.F-score is the harmonic mean of recall and precision.F -score = 2pi?pi + ?
(6)5.3 Results and DiscussionWe evaluate and compare the performances ofthe semi-supervised machine learning approaches(TSVM and harmonic functions) with their super-vised counterparts (SVM and kNN) for the task ofprotein-protein interaction extraction.
As discussedin Section 3, we use cosine similarity and edit dis-tance based similarity as similarity functions in har-monic functions and kNN, and as kernel functionsin TSVM and SVM.
Our instances consist of theshortest paths between the protein pairs in the de-pendency parse trees of the sentences.
In our ex-periments, we tuned the ?
parameter of the editdistance based path similarity function to 4.5 withcross-validation.
The results in Table 2 and Table 3are obtained with 10-fold cross-validation.
We re-port the average results over the runs.Table 2 shows the results obtained for the AIMEDdata set.
Edit distance based path similarity functionperforms considerably better than the cosine sim-ilarity function with harmonic functions and kNNand usually slightly better with SVM and TSVM.We achieve our best F-score performance of 59.96%with TSVM with edit kernel.
While SVM with edit233kernel achieves the highest precision of 77.52%, itperforms slightly worse than SVM with cosine ker-nel in terms of F-score measure.
TSVM performsslightly better than SVM, both of which perform bet-ter than harmonic functions.
kNN is the worst per-forming algorithm for this data set.In Table 2, we also show the results obtained pre-viously in the literature by using the same data set.Yakushiji et al (2005) use an HPSG parser to pro-duce predicate argument structures.
They utilizethese structures to automatically construct proteininteraction extraction rules.
Mitsumori et al (2006)use SVM with the unparsed text around the pro-tein names as features to extract protein interac-tion sentences.
Here, we show their best result ob-tained by using the three words to the left and to theright of the proteins.
The most closely related studyto ours is that by Bunescu and Mooney (2005a).They define a kernel function based on the short-est path between two entities of a relationship inthe dependency parse tree of a sentence (the SPKmethod).
They apply this method to the domainof protein-protein interaction extraction in (Bunescuand Mooney, 2006).
Here, they also test the meth-ods ELCS (Extraction Using Longest Common Sub-sequences) (Bunescu et al, 2005) and SSK (Sub-sequence Kernel) (Bunescu and Mooney, 2005b).We cannot compare our results to theirs directly,because they report their results as a precision-recall graph.
However, the best F-score in theirgraph seems to be around 0.50 and definitely lowerthan the best F-scores we have achieved (?
0.59).Bunescu and Mooney (2006) also use SVM as theirlearning method in their SPK approach.
They definetheir similarity based on the number of overlappingwords between two paths and assign a similarity ofzero if the two paths have different lengths.
Ourimproved performance with SVM and the shortestpath dependency features may be due to the edit-distance based kernel, which takes into account notonly the overlapping words, but also word order andaccounts for deletions and insertions of words.
Ourresults show that, SVM, TSVM, and harmonic func-tions achieve better F-score and recall performancesthan the previous studies by Yakushiji et al (2005),Mitsumori et al (2006), and the SSK and ELCS ap-proaches of Bunescu and Mooney (2006).
SVM andTSVM also achieve higher precision scores.
Since,Mitsumori et al (2006) also use SVM in their study,our improved results with SVM confirms our moti-vation of using dependency paths as features.Table 3 shows the results we got with the CBdata set.
The F-score performance with the editdistance based similarity function is always betterthan that of cosine similarity function for this dataset.
The difference in performances is considerablefor harmonic functions and kNN.
Our best F-scoreis achieved with TSVM with edit kernel (85.22%).TSVM performs slightly better than SVM.
Whencosine similarity function is used, kNN performsbetter than harmonic functions.
However, when editdistance based similarity is used, harmonic functionsachieve better performance.
SVM and TSVM per-form better than harmonic functions.
But, the gap inperformance is low when edit distance based simi-larity is used with harmonic functions.Method Precision Recall F-ScoreSVM-edit 77.52 43.51 55.61SVM-cos 61.99 54.99 58.09TSVM-edit 59.59 60.68 59.96TSVM-cos 58.37 61.19 59.62Harmonic-edit 44.17 74.20 55.29Harmonic-cos 36.02 67.65 46.97kNN-edit 68.77 42.17 52.20kNN-cos 40.37 49.49 44.36(Yakushiji et al, 2005) 33.70 33.10 33.40(Mitsumori et al, 2006) 54.20 42.60 47.70Table 2: Experimental Results ?
AIMED Data SetMethod Precision Recall F-ScoreSVM-edit 85.15 84.79 84.96SVM-cos 87.83 81.45 84.49TSVM-edit 85.62 84.89 85.22TSVM-cos 85.67 84.31 84.96Harmonic-edit 86.69 80.15 83.26Harmonic-cos 72.28 70.91 71.56kNN-edit 72.89 86.95 79.28kNN-cos 65.42 89.49 75.54Table 3: Experimental Results ?
CB Data SetSemi-supervised approaches are usually more ef-fective when there is less labeled data than unlabeleddata, which is usually the case in real applications.To see the effect of semi-supervised approaches weperform experiments by varying the amount of la-23400.10.20.30.40.50.60.70.80.91300020001000500200100502010F-ScoreNumber of Labeled SentenceskNNHarmonicSVMTSVMFigure 2: The F-score on the AIMED dataset withvarying sizes of training databeled training sentences in the range [10, 3000].
Foreach labeled training set size, sentences are selectedrandomly among all the sentences, and the remain-ing sentences are used as the unlabeled test set.
Theresults that we report are the averages over 10 suchrandom runs for each labeled training set size.
Wereport the results for the algorithms when edit dis-tance based similarity is used, as it mostly performsbetter than cosine similarity.
Figure 2 shows theresults obtained over the AIMED data set.
Semi-supervised approaches TSVM and harmonic func-tions perform considerably better than their super-vised counterparts SVM and kNN when we havesmall number of labeled training data.
It is inter-esting to note that, although SVM is one of the bestperforming algorithms with more training data, it isthe worst performing algorithm with small amountof labeled training sentences.
Its performance startsto increase when number of training data is largerthan 200.
Eventually, its performance gets close tothat of the other algorithms.
Harmonic functions isthe best performing algorithm when we have lessthan 200 labeled training data.
TSVM achieves bet-ter performance when there are more than 500 la-beled training sentences.Figure 3 shows the results obtained over the CBdata set.
When we have less than 500 labeled sen-00.10.20.30.40.50.60.70.80.91300020001000500200100502010F-ScoreNumber of Labeled SentenceskNNHarmonicSVMTSVMFigure 3: The F-score on the CB dataset with vary-ing sizes of training datatences, harmonic functions and TSVM perform sig-nificantly better than kNN, while SVM is the worstperforming algorithm.
When we have more than500 labeled training sentences, kNN is the worst per-forming algorithm, while the performance of SVMincreases and gets similar to that of TSVM andslightly better than that of harmonic functions.6 ConclusionWe introduced a relation extraction approach basedon dependency parsing and machine learning toidentify protein interaction sentences in biomedicaltext.
Unlike syntactic parsing, dependency parsingcaptures the semantic predicate argument relation-ships between the entities in addition to the syntac-tic relationships.
We extracted the shortest paths be-tween protein pairs in the dependency parse trees ofthe sentences and defined similarity functions (ker-nels in SVM terminology) for these paths based oncosine similarity and edit distance.
Supervised ma-chine learning approaches have been applied to thisdomain.
However, they rely only on labeled trainingdata, which is difficult to gather.
To our knowledge,this is the first effort in this domain to apply semi-supervised algorithms, which make use of both la-beled and unlabeled data.
We evaluated and com-pared the performances of two semi-supervised ma-235chine learning approaches (harmonic functions andTSVM), with their supervised counterparts (kNNand SVM).
We showed that, edit distance based sim-ilarity function performs better than cosine simi-larity function since it takes into account not onlycommon words, but also word order.
Our 10-foldcross validation results showed that, TSVM per-forms slightly better than SVM, both of which per-form better than harmonic functions.
The worst per-forming algorithm is kNN.
We compared our resultswith previous results published with the AIMEDdata set.
We achieved the best F-score performancewith TSVM with the edit distance kernel (59.96%)which is significantly higher than the previously re-ported results for the same data set.In most real-world applications there are muchmore unlabeled data than labeled data.
Semi-supervised approaches are usually more effective inthese cases, because they make use of both the la-beled and unlabeled instances when making deci-sions.
To test this hypothesis for the applicationof extracting protein interaction sentences from text,we performed experiments by varying the numberof labeled training sentences.
Our results showthat, semi-supervised algorithms perform consider-ably better than their supervised counterparts, whenthere are small number of labeled training sentences.An interesting result is that, in such cases SVM per-forms significantly worse than the other algorithms.Harmonic functions achieve the best performancewhen there are only a few labeled training sentences.As number of labeled training sentences increasesthe performance gap between supervised and semi-supervised algorithms decreases.AcknowledgmentsThis work was supported in part by grants R01-LM008106 and U54-DA021519 from the US Na-tional Institutes of Health.ReferencesG.
Bader, D. Betel, and C. Hogue.
2003.
Bind - thebiomolecular interaction network database.
NucleicAcids Research, 31(1):248?250.A.
Bairoch and R. Apweiler.
2000.
The swiss-prot pro-tein sequence database and its supplement trembl in2000.
Nucleic Acids Research, 28(1):45?48.C.
Blaschke, M. A. Andrade, C. A. Ouzounis, and A. Va-lencia.
1999.
Automatic extraction of biological in-formation from scientific text: Protein-protein interac-tions.
In Proceedings of the AAAI Conference on In-telligent Systems for Molecular Biology (ISMB 1999),pages 60?67.R.
C. Bunescu and R. J. Mooney.
2005a.
A shortestpath dependency kernel for relation extraction.
In Pro-ceedings of the Human Language Technology Confer-ence and Conference on Empirical Methods in Natu-ral Language Processing, pages 724?731, Vancouver,B.C, October.R.
C. Bunescu and R. J. Mooney.
2005b.
Subsequencekernels for relation extraction.
In Proceedings of the19th Conference on Neural Information ProcessingSystems (NIPS), Vancouver, B.C, December.R.
C. Bunescu and R. J. Mooney, 2006.
Text Mining andNatural Language Processing, chapter Extracting Re-lations from Text: From Word Sequences to Depen-dency Paths.
forthcoming book.R.
Bunescu, R. Ge, J. R. Kate, M. E. Marcotte, R. J.Mooney, K. A. Ramani, and W. Y. Wong.
2005.
Com-parative experiments on learning information extrac-tors for proteins and their interactions.
Artificial Intel-ligence in Medicine, 33(2):139?155, February.C.
J. C. Burges.
1998.
A tutorial on support vectormachines for pattern recognition.
Data Mining andKnowledge Discovery, 2(2):121?167.C.
Cortes, P. Haffner, and M. Mohri.
2004.
Rationalkernels: Theory and algorithms.
Journal of MachineLearning Research, (5):1035?1062, August.A.
Culotta and J. Sorensen.
2004.
Dependency tree ker-nels for relation extraction.
In Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics (ACL-04), Barcelona, Spain, July.N.
Daraselia, A. Yuryev, S. Egorov, S. Novichkova,A.
Nikitin, and I. Mazo.
2004.
Extracting humanprotein interactions from medline using a full-sentenceparser.
Bioinformatics, 20(5):604?611.M-C. de Marneffe, B. MacCartney, and C. D. Manning.2006.
Generating Typed Dependency Parses fromPhrase Structure Parses.
In Proceedings of the IEEE /ACL 2006 Workshop on Spoken Language Technology.The Stanford Natural Language Processing Group.I.
Donaldson, J. Martin, B. de Bruijn, C. Wolting,V.
Lay, B. Tuekam, S. Zhang, B. Baskin, G. D. Bader,K.
Michalockova, T. Pawson, and C. W. V. Hogue.2003.
Prebind and textomy - mining the biomedicalliterature for protein-protein interactions using a sup-port vector machine.
BMC Bioinformatics, 4:11.236P.
G. Doyle and J. L. Snell.
1984.
Random Walksand Electric Networks.
Mathematical Association ofAmerica.M.
Huang, X. Zhu, Y. Hao, D. G. Payan, K. Qu, andM.
Li.
2004.
Discovering patterns to extract protein-protein interactions from full texts.
Bioinformatics,20(18):3604?3612.T.
Joachims.
1999.
Transductive inference for textclassification using support vector machines.
In IvanBratko and Saso Dzeroski, editors, Proceedings ofICML-99, 16th International Conference on MachineLearning, pages 200?209.
Morgan Kaufmann Publish-ers, San Francisco, US.H.
Li and T. Jiang.
2005.
A class of edit kernels forsvms to predict translation initiation sites in eukaryoticmrnas.
Journal of Computational Biology, 12(6):702?718.T.
Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.2006.
Extracting protein-protein interaction informa-tion from biomedical text with svm.
IEICE Trans-actions on Information and Systems, E89-D(8):2464?2466.T.
Ono, H. Hishigaki, A. Tanigami, and T. Takagi.2001.
Automated extraction of information onprotein-protein interactions from the biological liter-ature.
Bioinformatics, 17(2):155?161.E.
M. Phizicky and S. Fields.
1995.
Protein-protein in-teractions: methods for detection and analysis.
Micro-biol.
Rev., 59(1):94?123, March.J.
Pustejovsky, J. Castano, J. Zhang, M. Kotecki, andB.
Cochran.
2002.
Robust relational parsing overbiomedical literature: Extracting inhibit relations.
InProceedings of the seventh Pacific Symposium on Bio-computing (PSB 2002), pages 362?373.K.
Sugiyama, K. Hatano, M. Yoshikawa, and S. Uemura.2003.
Extracting information on protein-protein in-teractions from biological literature based on machinelearning approaches.
Genome Informatics, 14:699?700.J.
M. Temkin and M. R. Gilder.
2003.
Extraction of pro-tein interaction information from unstructured text us-ing a context-free grammar.
Bioinformatics, 19:2046?2053.I.
Xenarios, E. Fernandez, L. Salwinski, X. J. Duan, M. J.Thompson, E. M. Marcotte, and D. Eisenberg.
2001.Dip: The database of interacting proteins: 2001 up-date.
Nucleic Acids Res., 29:239 ?
241, January.A.
Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii.
2005.Biomedical information extraction with predicate-argument structure patterns.
In Proceedings of TheEleventh Annual Meeting of The Association for Natu-ral Language Processing, pages 93?96.A.
Zanzoni, L. Montecchi-Palazzi, M. Quondam,G.
Ausiello, M. Helmer-Citterich, and G. Cesareni.2002.
Mint: A molecular interaction database.
FEBSLetters, 513:135?140.X.
Zhu, Z. Ghahramani, and J. D. Lafferty.
2003.
Semi-supervised learning using gaussian fields and har-monic functions.
In T. Fawcett and N. Mishra, editors,ICML, pages 912?919.
AAAI Press.X.
Zhu.
2005.
Semi-supervised learning lit-erature survey.
Technical Report 1530, Com-puter Sciences, University of Wisconsin-Madison.http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.237
