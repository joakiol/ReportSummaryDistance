The Hub and Spoke Paradigm for CSR EvaluationFrancis KubalaJerome BellegardaJordan CohenDavid PallettDoug PaulMike PhillipsRaja RajasekaranFred RichardsonMichael RileyRoni RosenfeldBob RothMitch WeintraubBBN Systems and TechnologiesIBM T. J. Watson Research CenterInstitute for Defense AnalysesNational Institute of Standards and TechnologyMIT Lincoln LaboratoryMIT Laboratory for Computer ScienceTexas InstrumentsBoston UniversityAT&T Bell LaboratoriesCarnegie Mellon UniversityDragon Systems, Inc.SRI International1993 Members of the CSR Corpus Coordinating Committee (CCCC)e-mail: cccc@bbn.comABSTRACTIn this paper, we introduce the new paradigm used in the most re-cent ARPA-sponsored Continuous Speech Recognition (CSR) eval-uation and then discuss the important features of the test design.The 1993 CSR evaluation was organized in a novel fashion in anattempt to accomodate r search over a broad variety of importantproblems in CSR while maintaining a clear program-wide r searchfocus.
Furthermore, ach test component in the evaluation was de-signed as an experiment toextract as much information as possiblefrom the results.The evaluation was centered around a large vocabulary speaker-independent (SI) baseline test, which was required of every partic-ipating site.
This test was dubbed the 'Hub' since it was commonto all sites and formed the basis for controlled inter-system com-parisons.The Hub test was augmented with a variety of problem-specificoptional tests designed to explore avariety of important problems inCSR, mostly involving some kind of mismatch between the trainingand test conditions.
These tests were known as the 'Spokes' sincethey all could be informatively compared to the Hub, but wereotherwise independent.In the first trial of this evaluation paradigm in November, 1993, 11research groups participated, yielding a rich array of comparativeand contrastive r sults, all calibrated to the current state of the artin large vocabulary CSR.1.
IntroductionSince 1986, ARPA has sponsored periodic formal evaluations ofCSR technology.
From the beginning, these evaluations were dis-tinguished by a well-defined test that was required of all partici-pants within a specified window of time.
Most importantly, thetest definition remained stable over several years so that sustainedeffort could be made toward improved performance and so that anyimprovement made over time could be demonstrated convincingly.These were important features of a series of evaluations based onthe well-known Resource Management (RM) corpus.
Those evalu-ations were highly regarded for the competitive stimulus they pro-duced, resulting in the rapid assimilation ofnew techniques acrossthe CSR community worldwide.When the ARPA CSR community began designing a testbed cor-pus for large-vocabulary ecognition to replace RM, one of theshortcomings addressed was its lack of support for the variety ofimportant research interests that existed within the community atthe time.
Active research was already underway in adaptation tospeaker, domain, dialect, and in compensation formismatch in mi-crophone, environment, and speaking style, but none of this wassupported by the RM corpus.The ARPA-chartered CSR Corpus Coordinating Committee(CCCC) was given the task of defining a corpus and specifyingan evaluation scheme that would take advantage of this diversityand drive it to produce nabling technology for eventual pplicationto real-world CSR problems.The Hub and Spoke evaluation paradigm was conceived to accomo-date the research requirements of this diverse community and pro-duce convincing demonstrations of technological capability.
Testswere defined, to exercise the primary interests of all participants,and to include important comparisons needed to make informeddescisions about he efficacy of a particular algorithm or generalapproach.
At the same time, the evaluation preserved the impor-tant controlled baseline test, characteristic of past ARPA-sponsoredevaluations, that permitted irect comparison of CSR technologyacross different systems.In the next section, we describe the general design of the Huband Spoke evaluation paradigm.
Each component test in the 1993evaluation is then described in detail in section 3.2.
The Hub and Spoke Evaluation ParadigmThe Hub and Spoke appelation is intended to characterize the orga-nization of the evaluation as a suite of fairly independent tests (theSpokes) coupled to a central test (the Hub) in some informative37fashion.
The Hub test is further distinguished by being an abstractreprese, ntation of a fundamentally important problem in CSR andby being the only test required of all participants in the evaluation.It forms the basis for all informative inter-system comparisons.The Spoke tests, on the other hand, are abstractions of problems ofsomewhat less central importance in CSR and evaluation on themis optional.
They are the research sandbox, if you will, wherenew problems and methods can be introduced and evaluated spec-ulatively, without requiring agreement of the entire ARPA CSRcommunity.
They are specifically designed to permit intra-systemcomparisons ofalgorithms and methods for problems that often in-volve a mismatch between training and test data.
The Spoke testscan all be informatively compared to the Hub test but they areotherwise independent.Every Hub or Spoke test was structured toproduce aprimary resultfor each system evaluated as well as one or more contrastive con-ditions designed to measure the effect of an algorithm or approachon the problem under study.
For instance, the primary result mighthave featured a noise compensation algorithm while a contrastivetest, on the same data, might have required that the compensationbe disabled.
Comparing these two results demonstrates the efficacyof the compensation.The primary conditions are designated the P0 condition.
In gen-eral, they are unconstrained with respect o the lexicon and acousticor language model (LM) training allowed.
The contrastive condi-tion(s) are designated asCX (X = 1,2,..).
The first contrast test (des-ignated C1) normally specifies that an adaptive or unconstrainedfeature of the primary test be disabled or constrained so that theeffect of the primary feature can be measured in isolation.
Thiscontrast is usually required.
Additional contrastive t sts (either e-quired or optional) may be specified to calibrate the data or evaluatethe featured algorithm on additional data.The CI contrast condition in the Hub test has special importancein the overall evaluation design.
This condition specifies exactlythe acoustic training data allowed and the precise LM to be used.These easily-varied parameters of the test are held fixed in thisone condition in order to focus attention solely upon the acousticmodeling power of each system tested.
Although fixed, the amountof training data permitted and the quality of the LM used in theC1 test are near state-of-the-art.
This controlled test therefore s-tablishes a convincing baseline which allows a direct comparisonbetween all systems in the evaluation.The P0 primary condition in the Hub also occupies an exaltedposition within the evaluation framework.
It is designed to testthe current capability on a central problem in CSR.
In a singlenumber, esults from this test quantify the meaning of state-of-the-art in large-vocabulary continuous speech recognition.TerminologySeveral useful terms are defined here that describe important fea-tures of the evaluation.A session implies that the speaker, microphone, and acoustic envi-ronment all remain constant for a group of utterances.A static SI test does not offer session boundaries orutterance orderas side information to the system, and therefore implies that thespeaker, microphone, and environment may change from utteranceto utterance.
Functionally, it implies that each utterance must berecognized independently of all others, yielding the same answersfor any utterance order (or the same expectation, in the case of anon-deterministic re ognizer).Unsupervised incremental daptation means that the system is al-lowed to use any information it can extract from test data thatit has already recognized.
It implies that session boundaries andutterance order are known to the system as side information.Supervised incremental daptation means that the correct ranscrip-tion is made available to the system after each utterance has beenrecognized.
It also implies that session boundaries and utteranceorder are known to the system as side information.
This recog-nition mode models the scenario in which the user incrementallycorrects the system response after each utterance.3.
The 1993 Hub and Spoke EvaluationThe Hub and Spoke evaluation paradigm was first used for ARPACSR evaluation i  November, 1993.
The entire test suite for thisevaluation consisted of 2 Hub tests and 9 Spoke tests.
Each ofthese tests is described in detail below.Designation VocabularyTHE HUBH1.
Read WSJ Baseline 64KH2.
5K-Word Read WSJ Baseline 5KTHE SPOKESS 1.
Language Model Adaptation unlimited$2.
Domain-Independence unlimited$3.
SI Recognition Oufliers 5K$4.
Incremental Speaker Adaptation 5K$5.
Microphone-Independence 5K$6.
Known Alternate Microphone 5K$7.
Noisy Environments 5K$8.
Calibrated Noise Sources 5K$9.
Spontaneous WSJ Dictation unlimitedThe abstract problem represented byall the tests in the 1993 evalua-tion was the dictation of news stories, with an emphasis on financialnews stories.
Most of the tests in the 1993 evaluation used speechdata from subjects reading selected articles from the Wall StreetJournal.
The prompting texts for the WSJ-based tests came fromthe pre-defined evaluation test text pools specified in the WSJ0corpus \[3\] which consists of articles from the Wall Street Joumalpublished uring the years 1987-1989.Typical tests used 10 subjects reading 20-40 sentences each.Each test had equal numbers of male and female subjects.
TheSennheiser HMD-410 close-talking, noise-canceling microphonewas the primary one used.Unless otherwise noted, the default side information given to thesystem was as follows.
Speaking style, general environment con-ditions (quiet or noisy), and microphone identity were known.Speaker gender, specific environment conditions (room identity),session boundaries and utterance order were unknown unless notedotherwise.
Collectively, these defaults imply that static SI condi-tions were the default.3.1.
The 1993 HubThe Hub for 1993 was split into two tests differing in vocabularysize (64K and 5K-words).
The smaller test was included to provide38a computationally tractable test for sites that were not prepared atthe time to handle the larger vocabulary at the time.HI.
Read WSJ BaselineThe paramount Hub test (H1) was designed to measure state-of-the-art performance on a large-vocabulary SI test, using clean testdata well-matched to the training data.
The prompting texts forthe HI test came from the pre-defined 64K-word WSJ0 text pools.These texts excluded paragraphs that contained words outside the64K most frequent from the WSJ0 corpus.The primary HI (H1-P0) test allowed any language model (LM) oracoustic training data to be used.
In addition, the temporal order ofthe utterances and the location of subject-session boundaries in theutterance s quence was given to encourage the use of unsupervisedincremental daptation techniques.To permit direct comparisons of acoustic modeling technology be-tween different systems, the HI test contained a required con-trastive test (H1-C1) that controlled the amount of training dataand specified the LM statistics.
This contrast was run as a staticSI test, so utterance order and session boundaries were not givento the system.For H1-C1, the acoustic training data was limited to 37.2K utter-ances (about 62 hours of speech) drawn from one of two segmentsof the combined WSJ0 and WSJ1 corpora.
One segment was madeup of speech data from 284 subjects (SI-284) who produced 100-150 utterances ach.
The other segment had 37 subjects (SI-37)who produced either 600 or 1200 utterances ach.
Evaluating siteswere free to choose ither acoustic training corpus.The common required LM specified for the H1-C1 test was pro-duced by Doug Paul at MIT Lincoln Laboratory.
It was a 3-grambackoff LM estimated from approximately 35M words of text inthe 1987-89 WSJ0 text corpus.
Its lexicon was defined as the 20Kmost frequent words in the corpus, hence, the test contained somewords outside the vocabulary.
For the closed-vocabulary versionof the 20K trigram LM, the perplexity is about 160.An optional contrast, H1-C2, was specified with a companion 20K-word bigram LM produced by Doug Paul.
All other conditionswere identical to H1-CI.H2.
5K-Word Read WSJ BaselineThe smaller 5K Hub test used prompting texts from the 5K-wordtext pools specified in the WSJ0 corpus.
These articles were filteredto discard paragraphs with more than one word outside of the 5Kmost frequent words in the corpus.Similar to the H1-P0 test, the primary H2 test (H2-P0) allowed anylanguage model (LM) or acoustic training data to be used and alsoallowed unsupervised incremental daptation.The required H2-C1 was scaled down, however, to reduce the com-putational burden of participation.
The acoustic training data waslimited to 7.2K utterances (about 12 hours of speech) drawn fromeither the short-term or long-term subject segments ofthe combinedWSJ0 and WSJI corpora.
Here, the short-term subjects numbered84 (SI-84), compared to 12 (SI-12) subjects for the long-term seg-ment.A common 5K-word bigram LM, produced at MIT Lincoln Labo-ratory was required for H2-CI.
This LM was nominally a closed-vocabulary grammar.
The lexicon was constructed by includingall the words from the test truth texts and then adding words fromWSJ0 word-frequency-list un il 5K words were accumulated.
Dueto subject variability in reading the prompting texts, a few wordswere produced that were outside the specified vocabulary.
Theperplexity of the standard 5K closed-vocabulary LM is about 80for the bigram and 45 for the tdgram.3.2.
The 1993 SpokesThere were 9 Spoke tests in the 1993 evaluation that were designedto support the major interests of the participating sites at the time.Most of them are designed to study problems involving amismatchbetween the training and test data.Spokes S1 and $2 supported problems in LM adaptation primarily.$3 and $4 were targeted at speaker adaptation methods.
Adapta-tion to microphone was the focus of Spokes $5 and $6.
Ambientnoise was considered in $7 and $8.
Spoke $9 looked at data froma potential application for large-vocabulary CSR - spontaneousdictation of news stories from print-media journalists.All Spokes except S1, $2, and $9 used read-speech from the WSJ05K-word prompting texts.
The channel and noise compensationSpokes, $5-$8, used data from a variety of secondary microphones.All other Spokes used data from the Sennheiser microphone.In each Spoke below, the primary test represents he abstract prob-lem of interest.
The system is generally the least constrained forthe primary condition but run on data that is somehow mismatchedto the training data.
The contrastive t sts then attempt to exposeinformation by constraining some feature of the primary system ordata.
Most often the first contrast (C1) constrains the system toshow the efficacy of an algorithm used in the primary test.
Othercontrasts, which may vary in number from Spoke to Spoke, willoften be run on the matched ata to calibrate the problem andestimate upper-bound performance.Since the purpose of the Spoke tests is to calibrate the effectivenessof an algorithm or approach within a single system, sites are freeto choose their system parameters for the primary condition as theysee fit.
That means, however, that direct comparisons between sys-tems cannot be made without suitable caution.
Unless the systemdetails are well understood, the reader could unknowingly end upcomparing a system using a bigram LM to one using a trigram, forinstance, and thereby draw a completely inappropriate conclusion.In general it is advisable to make only intra-system comparisonsbetween results in all Spoke tests.Sl.
Language Model AdaptationThis Spoke was concerned with the problem of within-domain sub-language adaptation.
The data was read-speech prompted from un-filtered texts of 1990 WSJ publications.
Articles were selected witha minimum of 20 sentences.For the primary test (S1-P0) the vocabulary was closed and therecognition mode was specified as supervised incremental dapta-tion, so a system was allowed to know all words that could occurand was given the correct answer after each utterance was rec-ognized.
The sequential order of the utterances and the articleboundaries were known.
The LM training was restricted to the1987-1989 WSJ0 texts so that it predated any of the test data.The required contrast, S1-C1, repeated the same test with the LMadaptation disabled.
An optional contrast, S1-C2, specified unsu-pervised incremental LM adaptation (the vocabulary was dosed39but the correct answer was not given).For reporting results, the test data was partitioned into four partsthat were distinguished by their position within the articles.
Sep-arate word error rates were given for utterances I-5, 6-10, 11-15,and 16 or greater in a given article.
This was done to observe theeffect of the LM adaptation as a function of the amount of contextavailable to it.$2.
Domain-IndependenceThe purpose of this Spoke was to evaluate techniques for dealingwith a newspaper domain different from the training.
The data wasprompted from articles drawn from the San Jose Mercury (SJM)newspaper.The primary test allowed any acoustic or LM training with therestrictions that no training be used from the San Jose Mercuryitself and that no use could be made of knowledge of the paper'sidentity.
Two required contrastive tests then calibrated the S2-P0 system on the WSJ-based H1 data, and conversely, the H1-P0system on the SJM-based $2 data.As it happened, no site evaluated on this Spoke in 1993.$3.
SI Recognition OutliersThis Spoke was designed to study speaker adaptation for non-nativespeakers of American English, a group for which recognition per-formance isoften very degraded.
The goal of the test was to reducethis degradation by using limited enrollment data from each of thesespeakers to adapt he system to them.The data was read-speech from the 5K-word WSJ texts similar tothe H2 data.
It was produced by ten subjects whose first languagesincluded, French, Spanish, German, Danish, Japanese, Hebrew, andBritish English.The primary test allowed use of 40 utterances of rapid-enrollmentdata collected from each of the test speakers.
The required $3-CI contrast was then run with the adaptation disabled to measureits effectiveness.
A second required contrast test ran the S3-P0adaptive system on the H2 data to see the effect of the adaptation onnative speakers of the language.
All tests were static SI recognitiontests.$4.
Incremental Speaker AdaptationSpoke $4 was directed specifically toward incremental speakeradaptation for native speakers.
The goal was to improve on thebaseline SI performance by adapting to each test speaker individ-ually.The 5K-word WSJ read data for this Spoke differed from the H2data only in that each speaker produced 100 utterances sothat theconvergence of the adaptation algorithms could be observed overa longer session from each speaker.The primary test specified unsupervised incremental daptation.The required $4-C1 contrast was run with the adaptation disabled.An optional test, $4-C2, allowed supervised incremental daptationfor comparison with the primary test.For reporting results, the test data was partitioned into four partsthat were distinguished bytheir position within the sessions.
Sepa-rate word error rates were given for utterances 1-25, 26-50, 51-75,and 76 or greater within a given session.
This was done to observethe effect of the speaker adaptation as a function of the amount ofcontext available to it.To measure the cost of the adaptation relative to the recognition,the ratio of the total runtime of S4-P0 to $4-C1 was also given asan auxilliary performance measure.S5.
Microphone-IndependenceSpoke $5 exposed the system to a variety (10) of unknown micro-phones.
Only unsupervised compensation algorithms were allowedfor this Spoke.
It was a static SI test as well, so that each utterancehad to be considered in isolation.The data was the same speech as the H2 data (5K-word WSJ), butcollected in stereo recordings through the various econdary micro-phones.
The selection of microphones included telephone (handsetand speakerphone), lapel, hand-held, stand-mounted, and monitor-mounted ones.
They included professional quality microphones awell as consumer grade devices.The primary test demonstrated the unsupervised compensation al-gorithm.
The required $5-C1 test ran with the compensation dis-abled to show the degradation due to the mismatched channel.
Thiscondition calibrates the mismatched channel problem.The $5-C2 test required the S5-P0 system (compensation e abled)to be run on the matching stereo channel from the Sennheiser mi-crophone to observe the effect of the compensation data that wasmatched to the training.
An optional $5-C3 contrast calibrated the$5-C1 system (compensation disabled) on the stereo Sennheiserdata to set the upper-bound matched-channel p rformance l vel.These 4 conditions define the space of the mismatched channelproblem completely.
The ideal result is that the compensated mis-matched condition (S5-P0) works as well as the uncompensatedmatched condition ($5-C3), and that the compensation doesn't hurtwhen used on the matched ata ($3-C2).$6.
Known Alternate MicrophoneSpoke $6 exposed the system to two microphones whose identi-fies were known, but that differed from the microphone used inthe training.
One was a telephone handset and the other was ahigh-quality directional stand-mounted microphone placed about18 inches from the subject's mouth.
The goal of the test was tomake the performance of the mismatched (but known) channel thesame as the matched channel data.The test data was produced from the 5K-word WSJ prompts.
Thetest mode was static SI recognition.
Simultaneous stereo recordingswere also made through the Sennheiser microphone tocalibrate thechannel mismatch.To facilitate adaptation tothe known microphone, an additional setof stereo recordings were made for both microphone types pairedwith the Sennheiser f om I0 training speakers.
This adaptationdata was the only data allowed from the target microphones.The primary test demonstrated an adaptation algorithm and allowedthe use of the stereo microphone-adaptation data.
There were tworequired contrasts.
S6-C1 ran with the adaptation disabled for com-parison to the primary test.
$6-C2 ran the S6-C1 system (adaptationdisabled) on stereo Sennheiser data to set the basefine performancefor the matched-channel condition.
Separate rror rates are reportedfor the two microphones.The telephone data was routed through the local Palo Alto digi-tal network, but sampled on wide-band analog lines in the inter-40nal phone system at SRI.
Just prior to the 1993 evaluation, prob-lems with this collection procedure were discovered that resultedin marked differences between the microphone-adaptation data andthe development test data for this Spoke.
Any algorithms thatused the adaptation data may have been adversely affected by thisdifference.S7.
Noisy EnvironmentsThis Spoke featured the same two microphones used in S6 butplaced them in noisy environments with a background A-weightednoise level ranging over 55-68 dB.
Sennheiser data from stereorecordings was also produced for calibration tests.
The goal ofthe test was to minimize the difference in performance betweenthe noise-cancelling Sennheiser microphone and the alternate mi-crophones (telephone handset and far-field stand-mounted micro-phone).The two environments sampled were large rooms housing an opencomputer laboratory and a mechanical equipment laboratory.
Noisein the computer lab came from the surrounding equipment andnormal human traffic around the lab.
In the mechanical lab, thenoises were generated by parcel sorting equipment.The test data was from the 5K-word WSJ prompts.
The test modewas static SI recognition.
As in Spoke S6, use of the stereomicrophone-adaptation data was permitted.The primary test demonstrated a noise compensation algorithmand the required S7-C1 contrast ran with the compensation dis-abled.
A second required test, S7-C2, ran the S7-PO system on thestereo Sennheiser data to calibrate the baseline performance on thematched channel with most of the noise suppressed.All data was run for each condition, but separate error rates arereported for each of four microphone-environment combinations.S8.
Calibrated Noise SourcesThis Spoke exposed the system to two calibrated noise sourcesthrough the stand-mounted directional microphone used in S6 andS7.
Stereo Sennheiser data was also collected for calibration.
Thetwo noise sources were pre-recorded music and talk-radio, set atSNR levels roughly corresponding to 20, 10, and 0 dB, measuredthrough the stand-mounted microphone.The test data was from the 5K-word WSJ prompts.
The test modewas static SI recognition.
As in Spoke $6, use of the stereomicrophone-adaptation data was permitted to adapt to the chan-nel.The primary test demonstrated a noise compensation algorithm andthe required S8-C1 contrast ran the same system with the compen-sation disabled.
A second required test, S8-C2, ran the S8-POsystem (compensation enabled) on the stereo Sennheiser data tocalibrate the compensation algorithm on the matched channel withmost of the noise suppressed.
An additional optional test, S8-C3,ran on the Sennheiser data again but with the S8-C1 system (com-pensation disabled) to set the baseline.All data was run for each noise source and SNR, but separate errorrates are reported for each of six noise-level combinations.The data sets were calibrated by adjusting the noise source lev-els with the subject in place until a sample set of utterances pro-duced the specified SNR levels as measured by software suppliedby NIST.
But due to the variability of the sources themselves andthe observed tendency of the subjects to drift in the level of theirresponse over the session, the SNR levels measured on the test datadiffer considerably in places from the target SNR.S9.
Spontaneous WSJ DictationThe purpose of Spoke S9 was to simulate the application of large-vocabulary CSR to the dictation of news stories.
The data wascollected from practicing print-media journalists who composedstories spontaneously as described in [I].
News topic were chosenat the discretion of the subject after a priming review of WSJnewspapers current at the time.The primary test allowed any acoustic or LM training.
There weretwo required contrasts.
S9-C1 tested the S9-PO system on theH1 data, measuring the change in performance on read WSJ dueto generalizing toward spontaneous data.
The S9-C2 test ran theHI-C1 (controlled Hub baseline) on the S9 data to calibrate thedifficulty of the spontaneous data.4.
SummaryIn the first trial of the Hub and Spoke evaluation paradigm inNovember, 1993, 11 research sites participated, including 5 sitesoutside the ARPA community.
The result was a rich array ofcomparative and contrastive results on several important problemsin large vocabulary CSR, all calibrated to the current state-of-the-art performance levels.
A complete listing of the numerical resultscan be found in 121.
For interpretive results, the interested readershould consult the comtemporary papers of the participating sites.%o cautions are in order when attempting to interpret these re-sults.
First, since the acoustic training and development test datawere distributed quite late, and since the Hub and Spoke paradigmwas under development up to two months prior to the evaluation,a considerable burden was imposed on the participants who wererushed through the data processing and system training steps andwere often denied a complete understanding of the rules.
Someanomalies in the results did occur due to these undesirable circum-stances.Secondly, it's important to remember that the only tests for whichfair and informative direct comparisons can be made across systems(and sites) are the controlled C1 contrasts for either of the twoHub tests.
All other tests are designed to produce informativecomparisons only within a given system run in two contrastivemodes.
So in general, only intra-system comparisons should bemade on the Spoke tests.The Hub and Spoke evaluation paradigm appears to have met thecompeting requirements of supporting the variety of important re-search interests within the ARPA CSR community while providinga mechanism to focus that work into well-defined and competi-tively charged evaluations of enabling technology.
It is a flexibleframework that encourages work in diverse problems in CSR.It is also a very structured framework that treats all tests conductedin an evaluation as if they were scientific experiments, specifyingcontrols where appropriate to maximize the amount of informationcontained in the results.
This structure also helps keep the effort ofthe participating research community focused around a productivecore of problems.
If the Hub and Spoke paradigm is to be trulysuccessful, however, it will need to sustain that focus over time ina manner analogous to the very successful Resource Managementbased evaluations of the late 1980's.Contact InformationThe complete specification of the 1993 evaluation is contained ina documentation file included with the 1993 evaluation data dis-tributed by NIST.
It can also be obtained by sending a request toFrancis Kubala via e-mail to flcubala@bbn.com.Sites that are interested in participating in future ARPA-sponsored CSR evaluations can notify David Pallett at the Na-tional Institute of Standards and Technology (NIST).
E-mail:dave@jaguar.ncsl.nist.gov.The evaluation test data, as well as the training and developmenttest data used in this evaluation, and all accompanying documen-tation, are available from the Linguistic Data Consortium (LDC).E-mail: Idc@unagi.cis.upenn.edu.AcknowledgementFor their dedication in producing the test data required to supportthe 1993 Hub and Spoke evaluation, the CCCC would like to thankDenise Danielson and Tom Kuhn at SRI as well as Jack Godfreyand Dave Graff at LDC.
Without their committed work this ambi-tious evaluation would not have been possible.References1.
Bernstein, J., D. Danielson, "Spontaneous Speech Collectionfor the CSR Corpus", Proceedings of the DARPA Speech andNatural Language Workshop, Morgan Kaufmann Publishers,Feb.
1992, pp.
373-378.2.
Pallett, D., J. Fiscus, W. Fisher, J. Garofolo, B. Lund, andM.
Pryzbocki, "1993 Benchmark Tests for the ARPA spokenLanguage Program", Proceedings of the ARPA Human Lan-guage Technology Workshop, Morgan Kaufmann Publishers,Mar.
1994, elsewhere these proceedings.3.
Paul, D., J. Baker, ''The Design for the Wall Street Journal-based CSR Corpus", Proceedings of the DARPA Speech andNatural Language Workshop, Morgan Kaufmann Publishers,Feb.
1992, pp.
357-362.
