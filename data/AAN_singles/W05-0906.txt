Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translationand/or Summarization, pages 41?48, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsEvaluating Summaries and Answers: Two Sides of the Same Coin?Jimmy Lin1,3 and Dina Demner-Fushman2,31College of Information Studies2Department of Computer Science3Institute for Advanced Computer StudiesUniversity of MarylandCollege Park, MD 20742, USAjimmylin@umd.edu, demner@cs.umd.eduAbstractThis paper discusses the convergencebetween question answering and multi-document summarization, pointing outimplications and opportunities for knowl-edge transfer in both directions.
As acase study in one direction, we discussthe recent development of an automaticmethod for evaluating definition questionsbased on n-gram overlap, a commonly-used technique in summarization evalua-tion.
In the other direction, the move to-wards topic-oriented summaries requiresan understanding of relevance and topi-cality, issues which have received atten-tion in the question answering literature.It is our opinion that question answeringand multi-document summarization repre-sent two complementary approaches to thesame problem of satisfying complex userinformation needs.
Although this pointsto many exciting opportunities for system-building, here we primarily focus on im-plications for system evaluation.1 IntroductionRecent developments in question answering (QA)and multi-document summarization point to manyinteresting convergences that present exciting oppor-tunities for collaboration and cross-fertilization be-tween these largely independent communities.
Thisposition paper attempts to draw connections be-tween the task of answering complex natural lan-guage questions and the task of summarizing mul-tiple documents, the boundaries between which arebeginning to blur, as anticipated half a decadeago (Carbonell et al, 2000).Although the complementary co-evolution ofquestion answering and document summarizationpresents new directions for system-building, thispaper primarily focuses on implications for evalu-ation.
Although assessment of answer and sum-mary quality employs different methodologies, thereare many lessons that each community can learnfrom the other.
The summarization community hasextensive experience in intrinsic metrics based onn-gram overlap for automatically scoring systemoutputs against human-generated reference texts?these techniques would help streamline aspects ofquestion answering evaluation.
In the other direc-tion, because question answering has its roots ininformation retrieval, much work has focused onextrinsic metrics based on relevance and topical-ity, which may be valuable to summarization re-searchers.This paper is organized as follows: In Section 2,we discuss the evolution of question answering re-search and how recent trends point to the conver-gence of question answering and multi-documentsummarization.
In Section 3, we present a casestudy of automatically evaluating definition ques-tions by employing metrics based on n-gram over-lap, a general technique widely used in summariza-tion and machine translation evaluations.
Section 4highlights some opportunities for knowledge trans-fer in the other direction: how the notions of rele-41vance and topicality, well-studied in the informationretrieval literature, can guide the evaluation of topic-oriented summaries.
We conclude with thoughtsabout the future in Section 5.2 Convergence of QA and SummarizationQuestion answering was initially conceived as es-sentially a fine-grained information retrieval task.Much research has focused on so-called factoidquestions, which can typically be answered bynamed entities such as people, organizations, loca-tions, etc.
As an example, a system might return?Bee Gees?
as the answer to the question ?Whatband did the music for the 1970?s film ?SaturdayNight Fever???.
For such well-specified informationneeds, question answering systems represent an im-provement over traditional document retrieval sys-tems because they do not require a user to manu-ally browse through a ranked list of ?hits?.
Since1999, the NIST-organized question answering tracksat TREC (see, for example, Voorhees 2003a) haveserved as a focal point of research in the field, pro-viding an annual forum for evaluating systems de-veloped by teams from all over the world.
Themodel has been duplicated and elaborated on byCLEF in Europe and NTCIR in Asia, both of whichhave also introduced cross-lingual elements.Recently, research in question answering hasshifted away from factoid questions to more com-plex information needs.
This new direction can becharacterized as a move towards answers that canonly be arrived at through some form of reason-ing and answers that require drawing informationfrom multiple sources.
Indeed, there are many typesof questions that would require integration of bothcapabilities: extracting raw information ?nuggets?from potentially relevant documents, reasoning overthese basic facts to draw additional inferences, andsynthesizing an appropriate answer based on thisknowledge.
?What is the role of the Libyan gov-ernment in the Lockerbie bombing??
is an exampleof such a complex question.Commonalities between the task of answeringcomplex questions and summarizing multiple doc-uments are evident when one considers broader re-search trends.
Both tasks require the ability todraw together elements from multiple sources andcope with redundant, inconsistent, and contradic-tory information.
Both tasks require extracting finer-grained (i.e., sub-document) segments, albeit basedon different criteria.
These observations point tothe convergence of question answering and multi-document summarization.Complementary developments in the summariza-tion community mirror the aforementioned shiftsin question answering research.
Most notably, theDUC 2005 task requires systems to generate an-swers to natural language questions based on a col-lection of known relevant documents: ?The systemtask in 2005 will be to synthesize from a set of 25?50 documents a brief, well-organized, fluent answerto a need for information that cannot be met by juststating a name, date, quantity, etc.?
(DUC 2005guidelines).
These guidelines were modeled afterthe information synthesis task suggested by Amigo?et al (2004), which they characterize as ?the processof (given a complex information need) extracting,organizing, and inter-relating the pieces of informa-tion contained in a set of relevant documents, in or-der to obtain a comprehensive, non-redundant reportthat satisfies the information need?.
One of the ex-amples they provide, ?I?m looking for informationconcerning the history of text compression both be-fore and with computers?, looks remarkably like auser information need current question answeringsystems aspire to satisfy.
The idea of topic-orientedmulti-document summarization isn?t new (Goldsteinet al, 2000), but only recently have the connectionsto question answering become explicit.
Incidentally,it appears that the current vision of question answer-ing is more ambitious than the information synthesistask because in the former, the set of relevant doc-uments is not known in advance, but must first bediscovered within a larger corpus.There is, however, an important difference be-tween question answering and topic-focused multi-document summarization: whereas summaries arecompressible in length, the same cannot be said ofanswers.1 For question answering, it is difficult tofix the length of a response a priori: there may becases where it is impossible to fit a coherent, com-plete answer into an allotted space.
On the other1Wewould like to thank an anonymous reviewer for pointingthis out.421 vital american composer2 vital musical achievements ballets symphonies3 vital born brooklyn ny 19004 okay son jewish immigrant5 okay american communist6 okay civil rights advocate7 okay had senile dementia8 vital established home for composers9 okay won oscar for ?the Heiress?10 okay homosexual11 okay teacher tanglewood music center boston symphonyTable 1: The ?answer key?
to the question ?Who is Aaron Copland?
?hand, summaries are condensed representations ofcontent, and should theoretically be expandable andcompressible based on the level of detail desired.What are the implications, for system evaluations,of this convergence between question answering andmulti-document summarization?
We believe that thetwo fields have much to benefit from each other.
Inone direction, the question answering communitycurrently lacks experience in automatically evalu-ating unstructured answers, which has been the fo-cus of much research in document summarization.In the other direction, the question answering com-munity, due to its roots in information retrieval, hasa good grasp on the notions of relevance and topi-cality, which are critical to the assessment of topic-oriented summaries.
In the next section, we presenta case study in leveraging summarization evaluationtechniques to automatically evaluate definition ques-tions.
Following that, we discuss how lessons fromquestion answering (and more broadly, informationretrieval) can be applied to assist in evaluating sum-marization systems.3 Definition Questions: A Case StudyDefinition questions represent complex informationneeds that involve integrating facts from multipledocuments.
A typical definition question is ?Whatis the Cassini space probe?
?, to which a systemmight respond with answers that include ?interplan-etary probe to Saturn?, ?carries the Huygens probeto study the atmosphere of Titan, Saturn?s largestmoon?, and ?a joint project between NASA, ESA,and ASI?.
The goal of the task is to return asmany interesting ?nuggets?
of information as possi-ble about the target entity being defined (the Cassinispace probe, in this case) while minimizing theamount of irrelevant information retrieved.
In thetwo formal evaluations of definition questions thathave been conducted at TREC (in 2003 and 2004),an information nugget is operationalized as a fact forwhich an assessor could make a binary decision as towhether a response contained that nugget (Voorhees,2003b).
Additionally, information nuggets are clas-sified as either vital or okay.
Vital nuggets rep-resent facts central to the target entity, and shouldbe present in a ?good?
definition.
Okay nuggetscontribute worthwhile information about the target,but are not essential.
As an example, assessors?nuggets for the question ?Who is Aaron Copland?
?are shown in Table 1.
The distinction between vi-tal and okay nuggets is consequential for the scorecalculation, which we will discuss below.In the TREC setup, a system response to a defi-nition question is comprised of an unordered set ofanswer strings paired with the identifier of the doc-ument from which it was extracted.
Each of theseanswer strings is presumed to have one or more in-formation nuggets contained within it.
Althoughthere is no explicit limit on the length of each answerstring and the number of answer strings a system isallowed to return, verbosity is penalized against, aswe shall see below.To evaluate system output, NIST gathers answerstrings from all participants, hides their association43[NYT19990708.0196] Once past a rather routine apprenticeship, which included three years of studywith Nadia Boulanger in Paris, Copland became one of the few American composers to make a livingfrom composition.Nugget present: 1[NYT20000107.0305] A passionate advocate of civil rights, Copland conducted a performance of the?Lincoln Portrait?
with Coretta Scott King as narrator.Nuggets present: 6[NYT19991117.0369] after four prior nominations, he won an Oscar in 1949 for his music for ?TheHeiress?Nugget present: 9Figure 1: Examples of judging actual system responses.with the runs that produced them, and presents allanswer strings to a human assessor.
Using these re-sponses and research performed during the originaldevelopment of the question (with an off-the-shelfdocument retrieval system), the assessor creates an?answer key?
; Table 1 shows the official answer keyfor the question ?Who is Aaron Copland?
?.After this answer key has been created, NIST as-sessors then go back over each run and manuallyjudge whether or not each nugget is present in a par-ticular system?s response.
Figure 1 shows a few ex-amples of real system output and the nuggets thatwere found in them.The final score of a particular answer is com-puted as an F-measure, the harmonic mean betweennugget precision and recall.
The ?
parameter con-trols the relative importance of precision and recall,and is heavily biased towards the latter to model thenature of the task.
Nugget recall is calculated solelyas a function of the vital nuggets, which means thata system receives no ?credit?
(in terms of recall) forreturning okay nuggets.
Nugget precision is approx-imated by a length allowance based on the numberof vital and okay nuggets returned; a response longerthan the allowed length is subjected to a verbositypenalty.
Using answer length as a proxy to precisionappears to be a reasonable compromise because apilot study demonstrated that it was impossible forhumans to consistently enumerate the total numberof nuggets in a response, a necessary step in calcu-lating nugget precision (Voorhees, 2003b).The current TREC setup for evaluating definitionLetr # of vital nuggets returned in a responsea # of okay nuggets returned in a responseR # of vital nuggets in the answer keyl # of non-whitespace characters in the entireanswer stringThenrecall (R) = r/Rallowance (?)
= 100?
(r + a)precision (P) ={1 if l < ?1?
l?
?l otherwiseFinally, F (?)
= (?2 + 1)?
P ?R?2 ?
P +R?
= 5 in TREC 2003, ?
= 3 in TREC 2004.Figure 2: Official definition of F-measure.questions necessitates having a human ?in the loop?.Even though answer keys are available for questionsfrom previous years, determining if a nugget was ac-tually retrieved by a system currently requires hu-man judgment.
Without a fully-automated evalu-ation method, it is difficult to consistently and re-producibly assess the performance of a system out-side the annual TREC cycle.
Thus, researchers can-not carry out controlled laboratory experiments torapidly explore the solution space.
In many otherfields in computational linguistics, the ability to con-duct evaluations with quick turnaround has lead torapid progress in the state of the art.
Question an-44swering for definition questions appears to be miss-ing this critical ingredient.To address this evaluation gap, we have re-cently developed POURPRE, a method for automat-ically evaluating definition questions based on idf-weighted unigram co-occurrences (Lin and Demner-Fushman, 2005).
This idea of employing n-gramco-occurrence statistics to score the output of a com-puter system against one or more desired referenceoutputs has its roots in the BLEU metric for ma-chine translation (Papineni et al, 2002) and theROUGE (Lin and Hovy, 2003) metric for summa-rization.
Note that metrics for automatically eval-uating definitions should be, like metrics for eval-uating summaries, biased towards recall.
Fluency(i.e., precision) is not usually of concern becausemost systems employ extractive techniques to pro-duce answers.
Our study reports good correlationbetween the automatically computed POURPRE met-ric and official TREC system ranks.
This measurewill hopefully spur progress in definition questionanswering systems.The development of automatic evaluation metricsbased on n-gram co-occurrence for question answer-ing is an example of successful knowledge transferfrom summarization to question answering evalua-tion.
We believe that there exist many more op-portunities for future exploration; as an example,there are remarkable similarities between informa-tion nuggets in definition question answering andrecently-proposed methods for assessing summariesbased on fine-grained semantic units (Teufel and vanHalteren, 2004; Nenkova and Passonneau, 2004).Another promising direction of research in defini-tion question answering involves applying the Pyra-mid Method (Nenkova and Passonneau, 2004) tobetter model the vital/okay nuggets distinction.
Asit currently stands, the vital/okay dichotomy is trou-blesome because there is no way to operationalizesuch a classification scheme within a system; seeHildebrandt et al (2004) for more discussion.
Yet,the effects on score are significant: a system that re-turns, for example, all the okay nuggets but none ofthe vital nuggets would receive a score of zero.
Intruth, the vital/okay distinction is a poor attempt atmodeling the fact that some nuggets about a targetare more important than others?this is exactly whatthe Pyramid Method is designed to capture.
?Build-ing pyramids?
for definition questions is an avenueof research that we are currently pursuing.In the next section, we discuss opportunities forknowledge transfer in the other direction; i.e., howsummarization evaluation can benefit from work inquestion answering evaluation.4 Putting the Relevance in SummarizationThe definition of a meaningful extrinsic evalua-tion metric (e.g., a task-based measure) is an issuethat the summarization community has long grap-pled with (Mani et al, 2002).
This issue has beenone of the driving factors towards summaries thatare specifically responsive to complex informationneeds.
The evaluation of such summaries hinges onthe notions of relevance and topicality, two themesthat have received much research attention in the in-formation retrieval community, from which questionanswering evolved.Debates about the nature of relevance are al-most as old as the field of information retrieval it-self (Cooper, 1971; Saracevic, 1975; Harter, 1992;Barry and Schamber, 1998; Mizzaro, 1998; Spinkand Greisdorf, 2001).
Theoretical discussions aside,there is evidence suggesting that there exist sub-stantial inter-assessor differences in document-levelrelevance judgments (Voorhees, 2000; Voorhees,2002); in the TREC ad hoc tracks, for example,overlap between two humans can be less than 50%.For factoid question answering, it has also beenshown that the notion of answer correctness is lesswell-defined than one would expect (Voorhees andTice, 2000; Lin and Katz, 2005 in press).
Thisinescapable fact about the nature of informationneeds represents a fundamental philosophical differ-ence between research in information retrieval andcomputational linguistics.
Information retrieval re-searchers accept the fact that the notion of ?groundtruth?
is not particularly meaningful, and any pre-scriptive attempt to dictate otherwise would result inbrittle and overtrained systems of limited value.
Aretrieval system must be sensitive to the inevitablevariations in relevance exhibited by different users.This philosophy represents a contrast from com-putational linguistics research, where ground truthdoes in fact exist.
For example, there is a single cor-rect parse of a natural language sentence (modulo45truly ambiguous sentences), there is the notion of acorrect word sense (modulo granularity issues), etc.This view also pervades evaluation in machine trans-lation and document summarization, and is implic-itly codified in intrinsic metrics, except that there isnow the notion of multiple correct answers (i.e., thereference texts).Faced with the inevitability of variations in hu-mans?
notion of relevance, how can informationretrieval researchers confidently draw conclusionsabout system performance and the effectiveness ofvarious techniques?
Meta-evaluations have shownthat while some measures such as recall are rela-tively meaningless in absolute terms (e.g., the to-tal number of relevant documents cannot be knownwithout exhaustive assessment of the entire corpus,which is impractical for current document collec-tions), relative comparisons between systems are re-markably stable.
That is, if system A performs bet-ter than system B (by a metric such as mean averageprecision, for example), system A is highly likelyto out-perform system B with any alternative sets ofrelevance judgments that represent different notionsof relevance (Voorhees, 2000; Voorhees, 2002).Thus, it remains possible to determine the relativeeffectiveness of different retrieval techniques, anduse evaluation results to guide system development.We believe that this philosophical starting pointfor conducting evaluations is an important point thatsummarization researchers should take to heart, con-sidering that notions such as relevance and topicalityare central to the evaluation of the information syn-thesis task.
What concrete implications of this vieware there?
We outline some thoughts below:First, we believe that summarization metricsshould embrace variations in human judgment as aninescapable part of the evaluation process.
Mea-sures for automatically assessing the quality of asystem?s output such as ROUGE implicitly assumethat the ?best summary?
is a statistical agglomera-tion of the reference summaries, which is not likelyto be true.
Until recently, ROUGE ?hard-coded?
theso-called ?jackknifing?
procedure to estimate aver-age human performance.
Fortunately, it appears re-searchers have realized that ?model averaging?
maynot be the best way to capture the existence of many?equally good?
summaries.
As an example, thePyramid Method (Nenkova and Passonneau, 2004),represents a good first attempt at a realistic model ofhuman variations.Second, the view that variations in judgment arean inescapable part of extrinsic evaluations wouldlead one to conclude that low inter-annotator agree-ment isn?t necessarily bad.
Computational linguis-tics research generally attaches great value to highkappa measures (Carletta, 1996), which indicatehigh human agreement on a particular task.
Lowagreement is seen as a barrier to conducting repro-ducible research and to drawing generalizable con-clusions.
However, this is not necessarily true?lowagreement in information retrieval has not been ahandicap for advancing the state of the art.
Whendealing with notions such as relevance, low kappavalues can most likely be attributed to the natureof the task itself.
Attempting to raise agreementby, for example, developing rigid assessment guide-lines, may do more harm than good.
Prescriptiveattempts to define what a good answer or summaryshould be will lead to systems that are not usefulin real-world settings.
Instead, we should focus re-search on adaptable, flexible systems.Third, meta-evaluations are important.
The infor-mation retrieval literature has an established tradi-tion of evaluating evaluations post hoc to insure thereliability and fairness of the results.
The aforemen-tioned studies examining the impact of different rel-evance judgments are examples of such work.
Dueto the variability in human judgments, systems areessentially aiming at a moving target, which neces-sitates continual examination as to whether evalu-ations are accurately answering the research ques-tions and producing trustworthy results.Fourth, a measure for assessing the quality of au-tomatic scoring metrics should reflect the philosoph-ical starting points that we have been discussing.As a specific example, the correlation between anautomatically-calculated metric and actual humanpreferences is better quantified by Kendall?s ?
thanby the coefficient of determination R2.
Since rela-tive system comparisons are more meaningful thanabsolute scores, we are generally less interested incorrelations among the scores than in the rankings ofsystems produced by those scores.
Kendall?s ?
com-putes the ?distance?
between two rankings as theminimum number of pairwise adjacent swaps neces-sary to convert one ranking into the other.
This value46is normalized by the number of items being rankedsuch that two identical rankings produce a correla-tion of 1.0; the correlation between a ranking and itsperfect inverse is ?1.0; and the expected correlationof two rankings chosen at random is 0.0.
Typically,a value of greater than 0.8 is considered ?good?, al-though 0.9 represents a threshold researchers gener-ally aim for.5 ConclusionWhat?s in store for the ongoing co-evolution of sum-marization and question answering?
Currently, def-inition questions exercise a system?s ability to inte-grate information from multiple documents.
In theprocess, it needs to automatically recognize similarinformation units to avoid redundant information,much like in multi-document summarization.
Theother research direction in advanced question an-swering, integration of reasoning capabilities to gen-erate answers that cannot be directly extracted fromtext, remains more elusive for a variety of reasons.Finer-grained linguistic analysis at a large scale andsufficiently-rich domain ontologies to support po-tentially long inference chains are necessary pre-requisites?both of which represent open researchproblems.
Furthermore, it is unclear how exactlyone would operationalize the evaluation of such ca-pabilities.Nevertheless, we believe that advanced reasoningcapabilities based on detailed semantic analyses oftext will receive much attention in the future.
Therecent flurry of work on semantic analysis, basedon resources such as FrameNet (Baker et al, 1998)and PropBank (Kingsbury et al, 2002), provide thesubstrate for reasoning engines.
Developments inthe automatic construction, adaptation, and merg-ing of ontologies will supply the knowledge nec-essary to draw inferences.
In order to jump-startthe knowledge acquisition process, we envision thedevelopment of domain-specific question answeringsystems, the lessons from which will be applied tosystems that operate on broader domains.
In termsof operationalizing evaluations for these advancedcapabilities, the field has already made importantfirst steps, e.g., the Pascal Recognising Textual En-tailment Challenge.What effect will these developments have on sum-marization research?
We believe that future sys-tems will employ more detailed linguistic analysis.As a simple example, the ability to reason aboutpeople?s age based on their birthdates would un-doubtedly be useful for answering particular typesof questions, but may also play a role in redundancydetection, for example.
In general, we anticipate amove towards more abstractive techniques in multi-document summarization.
Fluent, cohesive, and top-ical summaries cannot be generated solely usingan extractive approach?sentences are at the wronglevel of granularity, a source of problems rangingfrom dangling anaphoric references to verbose sub-ordinate clauses.
Only through more detailed lin-guistic analysis can information from multiple doc-uments be truly synthesized.
Already, there arehybrid approaches to multi-document summariza-tion that employ natural language generation tech-niques (McKeown et al, 1999; Elson, 2004), andresearchers have experimented with sentential op-erations to improve the discourse structure of sum-maries (Otterbacher et al, 2002).The primary purpose of this paper was to identifysimilarities between multi-document summarizationand complex question answering, pointing out po-tential synergistic opportunities in the area of systemevaluation.
We hope that this is merely a small partof a sustained dialogue between researchers fromthese two largely independent communities.
An-swering complex questions and summarizing mul-tiple documents are essentially opposite sides of thesame coin, as they represent different approaches tothe common problem of addressing complex user in-formation needs.6 AcknowledgementsWe would like to thank Donna Harman and EllenVoorhees for many insights about the intricacies ofIR evaluation, Bonnie Dorr for introducing us toDUC and bringing us into the summarization com-munity, and Kiri for her kind support.ReferencesEnrique Amigo?, Julio Gonzalo, Victor Peinado, AnselmoPen?as, and Felisa Verdejo.
2004.
An empirical studyof information synthesis task.
In Proceedings of ACL2004.47Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of COLING/ACL 1998.Carol Barry and Linda Schamber.
1998.
Users?
crite-ria for relevance evaluation: A cross-situational com-parison.
Information Processing and Management,34(2/3):219?236.Jaime Carbonell, Donna Harman, Eduard Hovy, SteveMaiorano, John Prange, and Karen Sparck-Jones.2000.
Vision statement to guide research in Question& Answering (Q&A) and Text Summarization.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
Computational Lin-guistics, 22(2):249?254.William S. Cooper.
1971.
A definition of relevance forinformation retrieval.
Information Storage and Re-trieval, 7:19?37.David K. Elson.
2004.
Categorization of narrative se-mantics for use in generative multidocument summa-rization.
In Proceedings of INLG 2004, pages 192?197.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, andJamie Callan.
2000.
Creating and evaluating multi-document sentence extract summaries.
In Proceedingsof CIKM 2000.Stephen P. Harter.
1992.
Psychological relevance andinformation science.
Journal of the American Societyfor Information Science, 43(9):602?615.Wesley Hildebrandt, Boris Katz, and Jimmy Lin.
2004.Answering definition questions with multiple knowl-edge sources.
In Proceedings of HLT/NAACL 2004.Paul Kingsbury, Martha Palmer, and Mitch Marcus.2002.
Adding semantic annotation to the Penn Tree-Bank.
In Proceeding of HLT 2002.Jimmy Lin and Dina Demner-Fushman.
2005.
Au-tomatically evaluating answers to definition ques-tions.
Technical Report LAMP-TR-119/CS-TR-4695/UMIACS-TR-2005-04, University of Maryland,College Park.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statis-tics.
In Proceedings of HLT/NAACL 2003.Jimmy Lin and Boris Katz.
2005, in press.
Building areusable test collection for question answering.
Jour-nal of the American Society for Information Scienceand Technology.Inderjeet Mani, Therese Firmin, David House, GaryKlein, Beth Sundheim, and Lynette Hirschman.
2002.The TIPSTER SUMMAC text summarization evalua-tion.
Natural Language Engineering, 8(1):43?68.Kathleen R. McKeown, Judith L. Klavans, VasileiosHatzivassiloglou, Regina Barzilay, and Eleazar Eskin.1999.
Towards multidocument summarization by re-formulation: Progress and prospects.
In Proceedingsof AAAI-1999.Stefano Mizzaro.
1998.
How many relevances in in-formation retrieval?
Interacting With Computers,10(3):305?322.Ani Nenkova and Rebecca Passonneau.
2004.
Evaluat-ing content selection in summarization: The PyramidMethod.
In Proceedings of HLT/NAACL 2004.Jahna C. Otterbacher, Dragomir R. Radev, and AirongLuo.
2002.
Revisions that improve cohesion in multi-document summaries: A preliminary study.
In Pro-ceedings of the ACL 2002 Workshop on AutomaticSummarization.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of ACL2002.Tefko Saracevic.
1975.
Relevance: A review of and aframework for thinking on the notion in informationscience.
Journal of the American Society for Informa-tion Science, 26(6):321?343.Amanda H. Spink and Howard Greisdorf.
2001.
Regionsand levels: Mapping and measuring users relevancejudgments.
Journal of the American Society for Infor-mation Science and Technology, 52(2):161?173.Simone Teufel and Hans van Halteren.
2004.
Evaluatinginformation content by factoid analysis: Human anno-tation and stability.
In Proceedings of EMNLP 2004.Ellen M. Voorhees and Dawn M. Tice.
2000.
Building aquestion answering test collection.
In Proceedings ofSIGIR 2000.Ellen M. Voorhees.
2000.
Variations in relevance judg-ments and the measurement of retrieval effectiveness.Information Processing and Management, 36(5):697?716.Ellen M. Voorhees.
2002.
The philosophy of informationretrieval evaluation.
In Evaluation of Cross-LanguageInformation Retrieval Systems, Springer-Verlag LNCS2406.Ellen M. Voorhees.
2003a.
Evaluating the evaluation: Acase study using the TREC 2002 question answeringtrack.
In Proceedings of HLT/NAACL 2003.Ellen M. Voorhees.
2003b.
Overview of the TREC 2003question answering track.
In Proceedings of TREC2003.48
