Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 97?107,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsGraph-Based Translation Via Graph SegmentationLiangyou Li and Andy Way and Qun LiuADAPT Centre, School of ComputingDublin City University, Ireland{liangyouli,away,qliu}@computing.dcu.ieAbstractOne major drawback of phrase-basedtranslation is that it segments an input sen-tence into continuous phrases.
To sup-port linguistically informed source discon-tinuity, in this paper we construct graphswhich combine bigram and dependencyrelations and propose a graph-based trans-lation model.
The model segments aninput graph into connected subgraphs,each of which may cover a discontinuousphrase.
We use beam search to combinetranslations of each subgraph left-to-rightto produce a complete translation.
Experi-ments on Chinese?English and German?English tasks show that our system issignificantly better than the phrase-basedmodel by up to +1.5/+0.5 BLEU scores.By explicitly modeling the graph segmen-tation, our system obtains further improve-ment, especially on German?English.1 IntroductionStatistical machine translation (SMT) starts fromsequence-based models.
The well-known phrase-based (PB) translation model (Koehn et al, 2003)has significantly advanced the progress of SMT byextending translation units from single words tophrases.
By using phrases, PB models can cap-ture local phenomena, such as word order, worddeletion, and word insertion.
However, one of thesignificant weaknesses in conventional PB modelsis that only continuous phrases are used, so gen-eralizations such as French ne .
.
.
pas to Englishnot cannot be learned.
To solve this, syntax-basedmodels (Galley et al, 2004; Chiang, 2005; Liuet al, 2006; Marcu et al, 2006) take tree struc-tures into consideration to learn translation pat-terns by using non-terminals for generalization.Model C D S(Koehn et al, 2003) ?
sequence(Galley and Manning, 2010) ?
?
sequence(Quirk et al, 2005) and?
tree(Menezes and Quirk, 2005)This work ?
?
graphTable 1: Comparison between our work and pre-vious work in terms of three aspects: keepingcontinuous phrases (C), allowing discontinuousphrases (D), and input structures (S).However, the expressiveness of these models isconfined by hierarchical constraints of the gram-mars used (Galley and Manning, 2010) since thesepatterns still cover continuous spans of an inputsentence.By contrast, Quirk et al (2005), Menezes andQuirk (2005) and Xiong et al (2007) take treeletsfrom dependency trees as the basic translationunits.
These treelets are connected and maycover discontinuous phrases.
However, their mod-els lack the ability to handle continuous phraseswhich are not connected in trees but could in factbe extremely important to system performance(Koehn et al, 2003).
Galley and Manning (2010)directly extract discontinuous phrases from inputsequences.
However, without imposing additionalrestrictions on discontinuity, the amount of ex-tracted rules can be very large and unreliable.Different from previous work (as shown in Ta-ble 1), in this paper we use graphs as input struc-tures and propose a graph-based translation modelto translate a graph into a target string.
The ba-sic translation unit in this model is a connectedsubgraph which may cover discontinuous phrases.The main contributions of this work are summa-rized as follows:?
We propose to use a graph structure to com-bine a sequence and a tree (Section 3.1).
The97graph contains both local relations betweenwords from the sequence and long-distancerelations from the tree.?
We present a translation model to translate agraph (Section 3).
The model segments thegraph into subgraphs and uses beam searchto generate a complete translation from leftto right by combining translation options ofeach subgraph.?
We present a set of sparse features to explic-itly model the graph segmentation (Section4).
These features are based on edges in theinput graph, each of which is either insidea subgraph or connects the subgraph with aprevious subgraph.?
Experiments (Section 5) on Chinese?Englishand German?English tasks show that ourmodel is significantly better than the PBmodel.
After incorporating the segmentationmodel, our system achieves still further im-provement.2 Review: Phrase-based TranslationWe first review the basic PB translation approach,which will be extended to our graph-based trans-lation model.
Given a pair of sentences ?S, T ?, theconventional PB model is defined as Equation (1):p(tI1| sI1) =I?i=1p(ti|sai)d(sai, sai?1) (1)The target sentence T is broken into I phrasest1?
?
?
tI, each of which is a translation of a sourcephrase sai.
d is a distance-based reordering model.Note that in the basic PB model, the phrase seg-mentation is not explicitly modeled which meansthat different segmentations are treated equally(Koehn, 2010).The performance of PB translation relies on thequality of phrase pairs in a translation table.
Con-ventionally, a phrase pair ?s, t?
has two proper-ties: (i) s and t are continuous phrases.
(ii) ?s, t?is consistent with a word alignment A (Och andNey, 2004): ?
(i, j) ?
A, si?
s ?
tj?
t and?si?
s, tj?
t, (i, j) ?
A.PB decoders generate hypotheses (partial trans-lations) from left to right.
Each hypothesis main-tains a coverage vector to indicate which sourcewords have been translated so far.
A hypothe-sis can be extended on the right by translating an0 1???2?
?????3?????????4???????????
?Figure 1: Beam search for phrase-based MT.
?
de-notes a covered source position while indicatesan uncovered position (Liu and Huang, 2014).uncovered source phrase.
The translation processends when all source words have been translated.Beam search (as in Figure 1) is taken as an ap-proximate search strategy to reduce the size of thedecoding space.
Hypotheses which cover the samenumber of source words are grouped in a stack.Hypotheses can be pruned according to their par-tial translation cost and an estimated future cost.3 Graph-Based TranslationOur graph-based translation model extends PBtranslation by translating an input graph ratherthan a sequence to a target string.
The graph is seg-mented into a sequence of connected subgraphs,each of which corresponds to a target phrase, as inEquation (2):(2)p(tI1| G(s?I1))=I?i=1p(ti|G(s?ai))d(G(s?ai), G(s?ai?1))?I?i=1p(ti|G(s?ai))d(s?ai, s?ai?1)where G(s?i) denotes a connected source subgraphwhich covers a (discontinuous) phrase s?i.3.1 Building GraphsAs a more powerful and natural structure for sen-tence modeling, a graph can model various kindsof word-relations together in a unified represen-tation.
In this paper, we use graphs to combinetwo commonly used relations: bigram relationsand dependency relations.
Figure 2 shows an ex-ample of a graph.
Each edge in the graph denoteseither a dependency relation or a bigram relation.Note that the graph we use in this paper is directed,connected, node-labeled and may contain cycles.Bigram relations are implied in sequences andprovide local and sequential information on pairs9820102010NianFIFAFIFAWorld CupShijiebeiinZaiSouth AfricaNanfeisuccessfullyChenggongheldJuxingFigure 2: An example graph for a Chinese sen-tence.
Each node includes a Chinese word and itsEnglish meaning.
Dashed red lines are bigram re-lations.
Solid lines are dependency relations.
Dot-ted blue lines are shared by bigram and depen-dency relations.of continuous words.
Phrases connected by bi-gram relations (i.e.
continuous phrases) are knownto be useful to improve phrase coverage (Hanne-man and Lavie, 2009).
By contrast, dependencyrelations come from dependency structures whichmodel syntactic and semantic relations betweenwords.
Phrases whose words are connected bydependency relations (also known as treelets) arelinguistic-motivated and thus more reliable (Quirket al, 2005).By combining these two relations together ingraphs, we can make use of both continuous andlinguistic-informed discontinuous phrases as longas they are connected subgraphs.3.2 TrainingDifferent from PB translation, the basic translationunits in our model are subgraphs.
Thus, duringtraining, we extract subgraph?phrase pairs insteadof phrase pairs on parallel graph?string sentencesassociated with word alignments.1An example ofa translation rule is as follows:FIFA Shijiebei JuxingFIFA World Cup was heldNote that the source side of a rule in our model is agraph which can be used to cover either a continu-ous phrase or a discontinuous phrase according toits match in an input graph during decoding.The algorithm for extracting translation rules isshown in Algorithm 1.
This algorithm traverseseach phrase pair ?s?, t?, which is within a lengthlimit and consistent with a given word alignment1Different from translation rules in conventional syntax-based MT, rules in our model are not learned based on syn-chronous grammars and so non-terminals are disallowed.Algorithm 1: Algorithm for extracting trans-lation rules from a graph-string pair.Data: A word-aligned graph?string pair(G(S), T, A)Result: A set of translation pairs R1 for each phrase t in T : | t |?
L do2 find the minimal (may be discontinuous)phrase s?
in S so that | s?
|?
L and ?s?, t?
isconsistent with A ;3 Queue Q = {s?
};4 while Q is not empty do5 pop an element s?
off;6 if G(s?)
is connected then7 add ?G(s?
), t?
to R;8 end9 if | s?
|< L then10 for each unaligned word siadjacent to s?
do11 s?
?= extend s?
with si;12 add s?
?to Q;13 end14 end15 end16 end(lines 1?2), and outputs ?G(s?
), t?
if s?
is covered bya connected subgraph G(s?)
(lines 6?8).
A sourcephrase can be extended with unaligned sourcewords which are adjacent to the phrase (lines 9?14).
We use a queue Q to store all phrases whichare consistently aligned to the same target phrase(line 3).3.3 Model and DecodingWe define our model in the log-linear framework(Och and Ney, 2002) over a derivation D =r1r2?
?
?
rN, as in Equation (3):p(D) ?
?i?i(D)?i(3)where riare translation rules, ?iare features de-fined on derivations and ?iare feature weights.In our experiments, we use the standard 9 fea-tures: two translation probabilities p(G(s)|t) andp(t|G(s)), two lexical translation probabilitiesplex(s|t) and plex(t|s), a language model lm(t)over a translation t, a rule penalty, a word penalty,an unknown word penalty and a distortion featured for distance-based reordering.The calculation of the distortion feature d in our99Ss2s1s2s31 2 3 4 5 6 7Td1=2 d2=5d2+=3d3=0Figure 3: Distortion calculation for both continu-ous and discontinuous phrases in a derivation..model is different from the one used in conven-tional PB models, as we need to take disconti-nuity into consideration.
In this paper, we use adistortion function defined in Galley and Manning(2010) to penalize discontinuous phrases that haverelatively long gaps.
Figure 3 shows an example ofcalculating distortion for discontinuous phrases.Our graph-based decoder is very similar to thePB decoder except that, in our decoder, each hy-pothesis is extended by translating an uncoveredsubgraph instead of a phrase.
Positions coveredby the subgraph are then marked as translated.4 Graph Segmentation ModelEach derivation in our graph-based translationmodel implies a sequence of subgraphs (alsocalled a segmentation).
By default, similar toPB translation, our model treats each segmenta-tion equally as shown in Equation (2).
However,previous work on PB translation has suggestedthat such segmentations provide useful informa-tion which can improve translation performance.For example, boundary information in a phrasesegmentation can be used for reordering models(Xiong et al, 2006; Cherry, 2013).In this paper, we are interested in directly mod-eling the segmentation using information fromgraphs.
By making the assumption that each sub-graph is only dependent on previous subgraphs,we define a generative process over a graph seg-mentation as in Equation (4):(4)p(G(s?1) ?
?
?G(s?I))=I?i=1P (G(s?i)|G(s?1) ?
?
?G(s?i?1))Instead of training a stand-alone discriminativesegmentation model to assign each subgraph aprobability given previous subgraphs, we imple-ment the model via sparse features, each of whichis extracted at run-time during decoding and thenZH?EN #SentsTrain 1.5M+MT02 (Dev) 878MT04 1,597MT05 1,082DE?EN #SentsTrain 2M+WMT11 (Dev) 3,003WMT12 3,003WMT13 3,000Table 2: The number of sentences in our corpora.directly added to the log-linear framework, so thatthese features can be tuned jointly with other fea-tures (of Section 3.3) to directly maximize thetranslation quality.Since a segmentation is obtained by breaking upthe connectivity of an input graph, it is intuitiveto use edges to model the segmentation.
Accord-ing to Equation (4), for a current subgraph Gi, weonly consider those edges which are either insideGior connectGiwith a previous subgraph.
Basedon these edges, we extract sparse features for eachnode in the subgraph.
The set of sparse features isdefined as follows:{n.wn.c}?{n?.wn?.c}????CPH????
{inout}where n.w and n.c are the word and class of thecurrent node n, and n?.w and n?.c are the wordand class of a node n?connected to n. C, P , andH denote that the node n?is in the current sub-graph Gior the adjacent previous subgraph Gi?1or other previous subgraphs, respectively.
Notethat we treat the adjacent previous subgraph differ-ently from others since information from the lastprevious unit is quite useful (Xiong et al, 2006;Cherry, 2013).
in and out denote that the edge isan incoming edge or outgoing edge for the currentnode n. Figure 4 shows an example of extractingsparse features for a subgraph.Inspired by success in using sparse features inSMT (Cherry, 2013), in this paper we lexicalizeonly on the top-100 most frequent words.
In ad-dition, we group source words into 50 classes byusing mkcls which should provide useful general-ization (Cherry, 2013) for our model.5 ExperimentWe conduct experiments on Chinese?English(ZH?EN) and German?English (DE?EN) transla-tion tasks.
Table 2 provides a summary of our cor-pra.
Our ZH?EN training corpus contains 1.5M+sentences from LDC.
NIST 2002 (MT02) is takenas a development set to tune weights, and NIST10020102010NianFIFAFIFAWorld CupShijiebeiinZaiSouth AfricaNanfeisuccessfullyChenggongheldJuxing2010FIFA World Cup was held successfully in South Africar1r2r3Sparse features for r3:W:Zai W:Nanfei C inW:Zai W:Nanfei C outW:Zai W:Shijiebei P outW:Zai W:Juxing P inW:Nanfei W:Zai C inW:Nanfei W:Zai C outW:Nanfei W:Chenggong C inW:Chenggong W:Nanfei C outW:Chenggong W:Juxing P inC:4 W:Nanfei C inC:4 W:Nanfei C outC:4 W:Shijiebei P outC:4 W:Juxing P inC:5 W:Zai C inC:5 W:Zai C outC:5 W:Chenggong C inC:6 W:Nanfei C outC:6 W:Juxing P inW:Zai C:5 C inW:Zai C:5 C outW:Zai C:3 P outW:Zai C:7 P inW:Nanfei C:4 C inW:Nanfei C:4 C outW:Nanfei C:6 C inW:Chenggong C:5 C outW:Chenggong C:7 P inC:4 C:5 C inC:4 C:5 C outC:4 C:3 P outC:4 C:7 P inC:5 C:4 C inC:5 C:4 C outC:5 C:6 C inC:6 C:5 C outC:6 C:7 P inFigure 4: An illustration of extracting sparse features for each node in a subgraph during decoding.
Thedecoder segments the graph in Figure 2 into three subgraphs (solid rectangles) and produces a completetranslation by combining translations of each subgraph (dashed rectangles).
In this figure, the class of aword is randomly assigned.2004 (MT04) and NIST 2005 (MT05) are two testsets used to evaluate the systems.
The StanfordChinese word segmenter (Chang et al, 2008) isused to segment Chinese sentences.
The Stan-ford dependency parser (Chang et al, 2009) parsesa Chinese sentence into a projective dependencytree which is then converted to a graph by addingbigram relations.The DE?EN training corpus is from WMT2014, including Europarl V7 and News Commen-tary.
News-Test 2011 (WMT11) is taken as a de-velopment set while News-Test 2012 (WMT12)and News-Test 2013 (WMT13) are test sets.
Weuse mate-tools2to perform morphological analy-sis and parse German sentences (Bohnet, 2010).Then, MaltParser3converts a parse result into aprojective dependency tree (Nivre and Nilsson,2005).5.1 SettingsIn this paper, we mainly report results from fivesystems under the same configuration.
PBMT isbuilt by the PB model in Moses (Koehn et al,2http://code.google.com/p/mate-tools/3http://www.maltparser.org/2007).
Treelet extends PBMT by taking treeletsas the basic translation units (Quirk et al, 2005;Menezes and Quirk, 2005).
We implement aTreelet model in Moses which produces transla-tions from left to right and uses beam search fordecoding.
DTU extends the PB model by allow-ing discontinuous phrases (Galley and Manning,2010).
We implement DTU with source disconti-nuity in Moses.4GBMT is our basic graph-basedtranslation system while GSM adds the graph seg-mentation model into GBMT.
Both systems areimplemented in Moses.Word alignment is performed by GIZA++ (Ochand Ney, 2003) with the heuristic function grow-diag-final-and.
We use SRILM (Stolcke, 2002)to train a 5-gram language model on the Xinhuaportion of the English Gigaword corpus 5th edi-tion with modified Kneser-Ney discounting (Chenand Goodman, 1996).
Batch MIRA (Cherry andFoster, 2012) is used to tune weights.
BLEU (Pa-pineni et al, 2002), METEOR (Denkowski andLavie, 2011), and TER (Snover et al, 2006) areused for evaluation.4The re-implementation of DTU in Moses makes it easierto meaningfully compare systems under the same settings.101Metric SystemZH?EN DE?ENMT04 MT05 WMT12 WMT13BLEU ?PBMT 33.2 31.8 19.5 21.9Treelet 33.8?31.7 19.6 22.1?DTU 34.5?32.3?19.8?22.3?GBMT 34.7?32.4?19.8?22.4?GSM 34.9?+32.7?+20.3?+22.9?+METEOR ?PBMT 32.1 32.3 28.0 29.2Treelet 31.9 31.8 28.0 29.1DTU 32.3?32.4 28.2?29.5?GBMT 32.4?+32.5?28.2?29.4?GSM 32.7?+32.6?+28.5?+29.8?+TER ?PBMT 60.6 61.6 63.7 60.2Treelet 60.1?61.4 63.2?59.6?DTU 60.0?61.5 63.5?59.8?GBMT 59.8?+61.3?63.5?59.8?GSM 60.5 62.1 63.1?+59.3?+Table 3: Metric scores for all systems on Chinese?English (ZH?EN) and German?English (DE?EN).Each score is an average over three MIRA runs (Clark et al, 2011).
?
means a system is significantlybetter than PBMT at p ?
0.01.
Bold figures mean a system is significantly better than Treelet at p ?
0.01.+ means a system is significantly better than DTU at p ?
0.01.
In this table, we mark a system bycomparing it with previous ones.5.2 Results and DiscussionTable 3 shows our evaluation results.
We findthat our GBMT system is significantly better thanPBMT as measured by all three metrics across alltest sets.
Specifically, the improvements are up to+1.5/+0.5 BLEU, +0.3/+0.2 METEOR, and -0.8/-0.4 TER on ZH?EN and DE?EN, respectively.This improvement is reasonable as our system al-lows discontinuous phrases which can reduce datasparsity and handle long-distance relations (Gal-ley and Manning, 2010).
Another argument fordiscontinuous phrases is that they allow the de-coder to use larger translation units which tend toproduce better translations (Galley and Manning,2010).
However, this argument was only verifiedon ZH?EN.
Therefore, we are interested in seeingwhether we have the same observation in our ex-periments on both language pairs.We count the used translation rules in MT02 andWMT11 based on different target lengths.
The re-sults are shown in Figure 5.
We find that both DTUand GBMT indeed tend to use larger translationunits on ZH?EN.
However, more smaller transla-tion units are used on DE?EN.5We presume thisis because long-distance reordering is performedmore often on ZH?EN than on DE?EN.
Based onthe fact that the distortion function d measures thereordering distance, we find that the average dis-tortion value in PB on ZH?EN MT02 is 18.4 and5We have the same finding on all test sets.System# RulesZH?EN DE?ENDTU 224M+ 352M+GBMT 99M+ 153M+Table 4: The number of rules in DTU and GBMT.3.5 on DE?EN WMT11.
Our observations suggestthat the argument that discontinuous phrases allowdecoders to use larger translation units should beconsidered with caution when we explain the ben-efit of discontinuity on different language pairs.Compared to PBMT, the Treelet system doesnot show consistent improvements.
Our systemachieves significantly better BLEU and METEORscores than Treelet on both ZH?EN and DE?EN,and a better TER score on DE?EN.
This suggeststhat continuous phrases are essential for system ro-bustness since it helps to improve phrase coverage(Hanneman and Lavie, 2009).
Lower phrase cov-erage in Treelet results in more short phrases be-ing used, as shown in Figure 5.
In addition, wefind that both DTU and our systems do not achieveconsistent improvements over Treelet in terms ofTER.
We observed that both DTU and our systemstend to produce longer translations than Treelet,which might cause unreliable TER evaluation inour experiments as TER favours shorter sentences(He and Way, 2010).Since discontinuous phrases produced by us-ing syntactic information are fewer in number but1021 2 3 4 5 6 751015MT021 2 3 4 5 6 751015WMT11[log2]PhraseCountNumber of English Words per PhrasePBMT Treelet DTU GBMTFigure 5: Phrase Length Histogram for MT02 and WMT11.more reliable (Koehn et al, 2003), our GBMT sys-tem achieves comparable performance with DTUbut uses significantly fewer rules, as shown in Ta-ble 4.
After integrating the graph segmentationmodel to help subgraph selection, GBMT is fur-ther improved and the resulted system G2S hassignificantly better evaluation scores than DTUon both language pairs.
However, our segmenta-tion model is more helpful on DE?EN than ZH?EN.
We find that the number of features learnedon ZH?EN (25K+) is much less than on DE?EN(49K+).
This may result in a lower feature cov-erage during decoding.
The lower number of fea-tures in ZH?EN could be caused by the fact thatthe development set MT02 has many fewer sen-tences than WMT11.
Accordingly, we suggestto use a larger development set during tuning toachieve better translation performance when thesegmentation model is integrated.Our current model is more akin to addressingproblems in phrase-based and treelet-based mod-els by segmenting graphs into pieces rather thanextracting a recursive grammar.
Therefore, simi-lar to those models, our model is weak at phrasereordering as well.
However, we are interesting inthe potential power of our model by incorporatinglexical reordering (LR) models and comparing itwith syntax-based models.Table 5 shows BLEU scores of the hierarchi-cal phrase-based (HPB) system (Chiang, 2005) inMoses6and GBMT combined with a word-based6For a fairer comparison, we disallow target discontinuityin HPB rules.
This means that a non-terminal on the targetside is either the first symbol or the last symbol.SystemZH?EN DE?ENMT04 MT05 WMT12 WMT13GBMT+LR 36.0 33.9 20.6 23.6HPB 36.1 34.1 20.3 22.8Table 5: BLEU scores of a Moses hierarchi-cal phrase-based system (HPB) and our system(GBMT) with a word-based lexical reorderingmodel (LR).LR model (Koehn et al, 2005).
We find thatthe LR model significantly improves our system.GBMT+LR is comparable with the Moses HPBmodel on Chinese?English and better than HPBon German?English.5.3 ExamplesFigure 6 shows three examples from MT04 to bet-ter explain the differences of each system.
Exam-ple 1 shows that systems which allow discontin-uous phrases (namely Treelet, DTU, GBMT, andGSM) successfully translate a Chinese colloca-tion ?Yu .
.
.
Wuguan?
to ?have nothing to do with?while PBMT fails to catch the generalization sinceit only allows continuous phrases.In Example 2, Treelet translates a discontinu-ous phrase ?Dui .
.
.
Zuofa?
(to .
.
.
practice) only as?to?
where an important target word ?practice?
isdropped.
By contrast, bigram relations allow oursystems (GBMT and GSM) to find a better phraseto translate: ?De Zuofa?
to ?of practice?.
In ad-dition, DTU translates a discontinuous phrase ?DeZuofa .
.
.
Buman?
to ?dissatisfaction with the ap-proach of?.
However, the phrase is actually not103Example 1PBMT: the unitedstates has indicatedthat the united statesand north korea dele-gation has visitedTreelet: the unitedstates has indicatedthat it has nothing todo with the us del-egation visited thenorth koreaDTU: the unitedstates has indicatedthat it has nothing todo with the us dele-gation visited northkoreaGBMT: the unitedstates has indicatedthat it has nothing todo with the us dele-gation visited northkoreaGSM: the unitedstates has indicatedthat it has nothing todo with the us dele-gation visited northkoreaREF: the american government said that it has nothing to do with the american delegation to visit north koreaamerican government said with visit north korea of american delegation no tieMeiguo Zhengfu Biaoshi Yu Zoufang BeiHan De Meiguo Daibiaotuan Wuguanthe united states has indicated that ithas nothing to do with the us delegationvisited north koreaExample 2PBMT: the unitedstates government tobrazil has repeatedlyexpressed its dissatis-faction .Treelet: the govern-ment of brazil to theunited states has onmany occasions ex-pressed their discon-tent .DTU: the unitedstates has repeat-edly expressed itsdissatisfaction withthe approach of thegovernment to brazil .GBMT: the us gov-ernment has repeat-edly expressed dis-satisfaction with thepractice of brazil .GSM: the us govern-ment has repeatedlyexpressed dissatisfac-tion with the practiceof brazil .REF: the us government has expressed their resentment against this practice of brazil on many occasions .US government to Brazil of practice already many times express dissatisfaction .Meiguo Zhengfu Dui Baxi De Zuofa Yijing Duo Ci Biaoshi Buman .the us government has repeatedly expressed dissatisfaction with the practice ofbrazil.Example 3PBMT: the govern-ment and all sectorsof society shouldcontinue to explorein depth and draw oncollective wisdom .Treelet: the govern-ment must continueto make in-depth dis-cussions with varioussectors of the com-munity and the col-lective wisdom .DTU: the govern-ment must continueto work together withvarious sectors of thecommunity to makean in-depth study anddraw on collectivewisdom .GBMT: the govern-ment must continueto work together withvarious sectors of thecommunity in-depthstudy and draw oncollective wisdom .GSM: the govern-ment must continueto make in-depth dis-cussions with varioussectors of the com-munity and draw oncollective wisdom .REF: the government must continue to hold thorough discussions with all walks of life to pool the wisdom of the masses .government must continue with society each community make in-depth discussion , draw collective wisdom .Zhengfu Wubi Jixu Yu Shehui Ge Jie Zuo Shengru Taolun , Jisi Guangyi .the governmentmustcontinue to makein-depth discussionswithvarious sectorsof the communityand draw oncollective wisdom .Figure 6: Translation examples from MT04 produced by different systems.
Each source sentence isannotated by dependency relations and additional bigram relations (dotted red edges).
We also annotatephrase alignments produced by our system GSM.104linguistically motivated and could be unreliable.By disallowing phrases which are not connectedin the input graph, GBMT and GSM produce bet-ter translations.Example 3 illustrates that our graph segmenta-tion model helps to select better subgraphs.
Af-ter obtaining a partial translation ?the governmentmust?, GSM chooses to translate a subgraph whichcovers a discontinuous phrase ?Jixu .
.
.
Zuo?
to?continue to make?
while GBMT translates ?JixuYu?
(continue .
.
.
with) to ?continue to work to-gether with?.
By selecting the proper subgraph totranslate, GSM performs a better reordering on thetranslation.6 Related WorkStarting from sequence-based models, SMT hasbeen benefiting increasingly from complex struc-tures.Sequence-based MT: Since the breakthroughmade by IBM on word-based models in the 1990s(Brown et al, 1993), SMT has developed rapidly.The PB model (Koehn et al, 2003) advanced thestate-of-the-art by translating multi-word units,which makes it better able to capture local phe-nomena.
However, a major drawback in PBMTis that only continuous phrases are considered.Galley and Manning (2010) extend PBMT by al-lowing discontinuity.
However, without linguis-tic structure information such as syntax trees,sequence-based models can learn a large amountof phrases which may be unreliable.Tree-based MT: Compared to sequences, treesprovide recursive structures over sentences andcan handle long-distance relations.
Typically,trees used in SMT are either phrasal structures(Galley et al, 2004; Liu et al, 2006; Marcu etal., 2006) or dependency structures (Menezes andQuirk, 2005; Xiong et al, 2007; Xie et al, 2011;Li et al, 2014).
However, conventional tree-based models only use linguistically well-formedphrases.
Although they are more reliable in the-ory, discarding all phrase pairs which are not lin-guistically motivated is an overly harsh decision.Therefore, exploring more translation rules usu-ally can significantly improve translation perfor-mance (Marcu et al, 2006; DeNeefe et al, 2007;Wang et al, 2007; Mi et al, 2008).Graph-basedMT: Compared to sequences andtrees, graphs are more general and can representmore relations between words.
In recent years,graphs have been drawing quite a lot of attentionfrom researchers.
Jones et al (2012) propose ahypergraph-based translation model where hyper-graphs are taken as a meaning representation ofsentences.
However, large corpora with annotatedhypergraphs are not readily available for MT.
Liet al (2015) use an edge replacement grammar totranslate dependency graphs which are convertedfrom dependency trees by labeling edges.
How-ever, their model only focuses on subgraphs whichcover continuous phrases.7 ConclusionIn this paper, we extend the conventional phrase-based translation model by allowing discontinuousphrases.
We use graphs which combine bigramand dependency relations together as inputs andpresent a graph-based translation model.
Exper-iments on Chinese?English and German?Englishshow our model to be significantly better than thephrase-based model as well as other more sophisti-cated models.
In addition, we present a graph seg-mentation model to explicitly guide the selectionof subgraphs.
In experiments, this model furtherimproves our system.In the future, we will extend this model to allowdiscontinuity on target sides and explore the possi-bility of directly encoding reordering informationin translation rules.
We are also interested in usinggraphs for neural machine translation to see how itcan translate and benefit from graphs.AcknowledgmentsThis research has received funding from the Peo-ple Programme (Marie Curie Actions) of the Euro-pean Union?s Framework Programme (FP7/2007-2013) under REA grant agreement no317471and the European Union?s Horizon 2020 researchand innovation programme under grant agreementno645452 (QT21).
The ADAPT Centre for Dig-ital Content Technology is funded under the SFIResearch Centres Programme (Grant 13/RC/2106)and is co-funded under the European Regional De-velopment Fund.
The authors thank all anony-mous reviewers for their insightful comments andsuggestions.ReferencesBernd Bohnet.
2010.
Very High Accuracy and FastDependency Parsing is Not a Contradiction.
In Pro-ceedings of the 23rd International Conference on105Computational Linguistics, pages 89?97, Beijing,China, August.Peter F Brown, Vincent J Della Pietra, Stephen A DellaPietra, and Robert L Mercer.
1993.
The Mathe-matics of Statistical Machine Translation: ParameterEstimation.
Computational Linguistics, 19(2):263?311.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese Word Seg-mentation for Machine Translation Performance.
InProceedings of the Third Workshop on StatisticalMachine Translation, pages 224?232, Columbus,Ohio, June.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D. Manning.
2009.
Discriminative Re-ordering with Chinese Grammatical Relations Fea-tures.
In Proceedings of the Third Workshop on Syn-tax and Structure in Statistical Translation, pages51?59, Boulder, Colorado, June.Stanley F. Chen and Joshua Goodman.
1996.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
In Proceedings of the 34th AnnualMeeting on Association for Computational Linguis-tics, ACL ?96, pages 310?318, Santa Cruz, Califor-nia, June.Colin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 427?436, Montreal, Canada, June.Colin Cherry.
2013.
Improved Reordering for Phrase-Based Translation using Sparse Features.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages22?31, Atlanta, Georgia, June.David Chiang.
2005.
A Hierarchical Phrase-basedModel for Statistical Machine Translation.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 263?270, AnnArbor, Michigan, June.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better Hypothesis Testing forStatistical Machine Translation: Controlling for Op-timizer Instability.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies: ShortPapers - Volume 2, pages 176?181, Portland, Ore-gon, June.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What Can Syntax-Based MT Learnfrom Phrase-Based MT?
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 755?763, Prague, Czech Republic, June.Michael Denkowski and Alon Lavie.
2011.
Me-teor 1.3: Automatic Metric for Reliable Optimiza-tion and Evaluation of Machine Translation Sys-tems.
In Proceedings of the Sixth Workshop onStatistical Machine Translation, pages 85?91, Ed-inburgh, Scotland, July.Michel Galley and Christopher D. Manning.
2010.Accurate Non-hierarchical Phrase-Based Transla-tion.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 966?974, Los Angeles, California, June.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a Translation Rule?In Proceedings of the Human Language TechnologyConference of the North American Chapter of theAssociation for Computational Linguistics: HLT-NAACL 2004, page 273280, Boston, Massachusetts,USA, May.Greg Hanneman and Alon Lavie.
2009.
Decoding withSyntactic and Non-syntactic Phrases in a Syntax-based Machine Translation System.
In Proceed-ings of the Third Workshop on Syntax and Structurein Statistical Translation, pages 1?9, Boulder, Col-orado, June.Yifan He and Andy Way.
2010.
Metric and refer-ence factors in minimum error rate training.
Ma-chine Translation, 24(1):27?38.Bevan Jones, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, and Kevin Knight.
2012.Semantics-Based Machine Translation with Hyper-edge Replacement Grammars.
In COLING 2012,24th International Conference on ComputationalLinguistics, Proceedings of the Conference: Tech-nical Papers, pages 1359?1376, Mumbai, India,December.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, pages 48?54, Edmonton, Canada, July.Philipp Koehn, Amittai Axelrod, Alexandra Birch,Chris Callison-Burch, Miles Osborne, and DavidTalbot.
2005.
Edinburgh System Description forthe 2005 IWSLT Speech Translation Evaluation.
InProceedings of the International Workshop on Spo-ken Language Translation 2005, pages 68?75, Pitts-burgh, PA, USA, October.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open Source Toolkit for Statistical Machine Trans-lation.
In Proceedings of the 45th Annual Meetingof the ACL on Interactive Poster and Demonstration106Sessions, pages 177?180, Prague, Czech Republic,June.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, New York, NY, USA,1st edition.Liangyou Li, Jun Xie, Andy Way, and Qun Liu.
2014.Transformation and Decomposition for EfficientlyImplementing and Improving Dependency-to-StringModel In Moses.
In Proceedings of SSST-8, EighthWorkshop on Syntax, Semantics and Structure inStatistical Translation, October.Liangyou Li, Andy Way, and Qun Liu.
2015.
De-pendency Graph-to-String Translation.
In Proceed-ings of the 2015 Conference on Empirical Methodsin Natural Language Processing, Lisbon, Portugal,September.Lemao Liu and Liang Huang.
2014.
Search-AwareTuning for Machine Translation.
In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 1942?1952, Doha, Qatar, October.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string Alignment Template for Statistical Ma-chine Translation.
In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics, pages 609?616, Sydney,Australia, July.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical Ma-chine Translation with Syntactified Target LanguagePhrases.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Process-ing, pages 44?52, Sydney, Australia.Arul Menezes and Chris Quirk.
2005.
DependencyTreelet Translation: The Convergence of Statisticaland Example-Based Machine-translation?
In Pro-ceedings of the Workshop on Example-based Ma-chine Translation at MT Summit X, September.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-Based Translation.
In Proceedings of the 46th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages192?199, Columbus, Ohio, USA, June.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-Projective Dependency Parsing.
In Proceedings ofthe 43rd Annual Meeting on Association for Com-putational Linguistics, pages 99?106, Ann Arbor,Michigan, June.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive Training and Maximum Entropy Models for Sta-tistical Machine Translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, pages 295?302, Philadelphia, PA,USA, July.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och and Hermann Ney.
2004.
The Align-ment Template Approach to Statistical MachineTranslation.
Computational Linguistics, 30(4):417?449, December.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, July.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency Treelet Translation: Syntactically In-formed Phrasal SMT.
In Proceedings of the 43rdAnnual Meeting of the Association for Computa-tional Linguistics (ACL?05), pages 271?279, AnnArbor, Michigan, June.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A Study of Translation Edit Ratewith Targeted Human Annotation.
In Proceedingsof Association for Machine Translation in the Amer-icas, pages 223?231, Cambridge, Massachusetts,USA, August.Andreas Stolcke.
2002.
SRILM An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the In-ternational Conference Spoken Language Process-ing, pages 901?904, Denver, CO.Wei Wang, Kevin Knight, and Daniel Marcu.
2007.Binarizing Syntax Trees to Improve Syntax-BasedMachine Translation Accuracy.
In Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 746?754, Prague, Czech Republic,June.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A NovelDependency-to-string Model for Statistical MachineTranslation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 216?226, Edinburgh, United Kingdom,July.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Max-imum Entropy Based Phrase Reordering Model forStatistical Machine Translation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 521?528, Sydney, Australia, July.Deyi Xiong, Qun Liu, and Shouxun Lin.
2007.
A De-pendency Treelet String Correspondence Model forStatistical Machine Translation.
In Proceedings ofthe Second Workshop on Statistical Machine Trans-lation, pages 40?47, Prague, June.107
