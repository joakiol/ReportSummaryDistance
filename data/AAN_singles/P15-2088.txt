Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 536?541,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsContext-Dependent Translation Selection Using Convolutional NeuralNetworkBaotian Hu?Zhaopeng Tu?
?Zhengdong Lu?Hang Li?Qingcai Chen?
?Intelligent Computing Research?Noah?s Ark LabCenter, Harbin Institute of Technology Huawei Technologies Co. Ltd.Shenzhen Graduate School tu.zhaopeng@huawei.combaotianchina@gmail.com lu.zhengdong@huawei.comqingcai.chen@hitsz.edu.cn hangli.hl@huawei.comAbstractWe propose a novel method for translationselection in statistical machine translation,in which a convolutional neural network isemployed to judge the similarity betweena phrase pair in two languages.
The specif-ically designed convolutional architectureencodes not only the semantic similarityof the translation pair, but also the con-text containing the phrase in the sourcelanguage.
Therefore, our approach isable to capture context-dependent seman-tic similarities of translation pairs.
Weadopt a curriculum learning strategy totrain the model: we classify the trainingexamples into easy, medium, and difficultcategories, and gradually build the abil-ity of representing phrases and sentence-level contexts by using training examplesfrom easy to difficult.
Experimental re-sults show that our approach significantlyoutperforms the baseline system by up to1.4 BLEU points.1 IntroductionConventional statistical machine translation(SMT) systems extract and estimate translationpairs based on their surface forms (Koehn et al.,2003), which often fail to capture translationpairs which are grammatically and semanticallysimilar.
To alleviate the above problems, severalresearchers have proposed learning and utilizingsemantically similar translation pairs in a contin-uous space (Gao et al., 2014; Zhang et al., 2014;Cho et al., 2014).
The core idea is that the twophrases in a translation pair should share the samesemantic meaning and have similar (close) featurevectors in the continuous space.?
* Corresponding authorThe above methods, however, neglect the infor-mation of local contexts, which has been provento be useful for disambiguating translation candi-dates during decoding (He et al., 2008; Marton andResnik, 2008).
The matching scores of translationpairs are treated the same, even they are in dif-ferent contexts.
Accordingly, the methods fail toadapt to local contexts and lead to precision issuesfor specific sentences in different contexts.To capture useful context information, we pro-pose a convolutional neural network architectureto measure context-dependent semantic similari-ties between phrase pairs in two languages.
Foreach phrase pair, we use the sentence contain-ing the phrase in source language as the context.With the convolutional neural network, we sum-marize the information of a phrase pair and its con-text, and further compute the pair?s matching scorewith a multi-layer perceptron.
We discriminatelytrain the model using a curriculum learning strat-egy.
We classify the training examples accordingto the difficulty level of distinguishing the positivecandidate from the negative candidate.
Then wetrain the model to learn the semantic informationfrom easy (basic semantic similarities) to difficult(context-dependent semantic similarities).Experimental results on a large-scale transla-tion task show that the context-dependent convo-lutional matching (CDCM) model improves theperformance by up to 1.4 BLEU points over astrong phrase-based SMT system.
Moreover,the CDCM model significantly outperforms itscontext-independent counterpart, proving that it isnecessary to incorporate local contexts into SMT.Contributions.
Our key contributions include:?
we introduce a novel CDCM model to cap-ture context-dependent semantic similaritiesbetween phrase pairs (Section 2);?
we develop a novel learning algorithm totrain the CDCM model using a curriculumlearning strategy (Section 3).536/           /        	      : tagged words    : untagged wordsrepresentation?/            /            the        key       point        is representation?Matching))Score)Layer&1(Layer&2(poolingconvolutionmatching(model(convolu6onal(sentence(model(Figure 1: Architecture of the CDCM model.
The convolutional sentence model (bottom) summarizes themeaning of the tagged sentence and target phrase, and the matching model (top) compares the represen-tations using a multi-layer perceptron.
?/?
indicates all-zero padding turned off by the gating function.2 Context-Dependent ConvolutionalMatching ModelThe model architecture, shown in Figure 1, is avariant of the convolutional architecture of Hu etal.
(2014).
It consists of two components:?
convolutional sentence model that summa-rizes the meaning of the source sentence andthe target phrase;?
matching model that compares the tworepresentations with a multi-layer percep-tron (Bengio, 2009).Let e?
be a target phrase and f be the source sen-tence that contains the source phrase aligning to e?.We first project f and e?
into feature vectors x andy via the convolutional sentence model, and thencompute the matching score s(x,y) by the match-ing model.
Finally, the score is introduced into aconventional SMT system as an additional feature.Convolutional sentence model.
As shown in Fig-ure 1, the model takes as input the embeddings ofwords (trained beforehand elsewhere) in f and e?.It then iteratively summarizes the meaning of theinput through layers of convolution and pooling,until reaching a fixed length vectorial representa-tion in the final layer.In Layer-1, the convolution layer takes slidingwindows on f and e?
respectively, and models allthe possible compositions of neighbouring words.The convolution involves a filter to produce a newfeature for each possible composition.
Given ak-sized sliding window i on f or e?, for example,the jth convolution unit of the composition of thewords is generated by:ci(1,j)= g(?ci(0)) ?
?(w(1,j)?
?ci(0)+ b(1,j)) (1)where?
g(?)
is the gate function that determineswhether to activate ?(?);?
?(?)
is a non-linear activation function.
Inthis work, we use ReLu (Dahl et al., 2013)as the activation function;?
w(1,j)is the parameters for the jth convolu-tion unit on Layer-1, with matrix W(1)=[w(1,1), .
.
.
,w(1,J)];?
?ci(0)is a vector constructed by concatenatingword vectors in the k-sized sliding widow i;?
b(1,j)is a bias term, with vector B(1)=[b(1,1), .
.
.
,b(1,J)].To distinguish the phrase pair from its con-text, we use one additional dimension in wordembeddings: 1 for words in the phrase pair and0 for the others.
After transforming words to537their tagged embeddings, the convolutional sen-tence model takes multiple choices of compositionusing sliding windows in the convolution layer.Note that sliding windows are allowed to crossthe boundary of the source phrase to exploit bothphrasal and contextual information.In Layer-2, we apply a local max-pooling innon-overlapping 1 ?
2 windows for every convo-lution unitc(2,j)i= max{c(1,j)2i, c(1,j)2i+1} (2)In Layer-3, we perform convolution on outputfrom Layer-2:ci(3,j)= g(?ci(2)) ?
?(w(3,j)?
?ci(2)+ b(3,j)) (3)After more convolution and max-pooling opera-tions, we obtain two feature vectors for the sourcesentence and the target phrase, respectively.Matching model.
The matching score of a sourcesentence and a target phrase can be measuredas the similarity between their feature vectors.Specifically, we use the multi-layer perceptron(MLP), a nonlinear function for similarity, to com-pute their matching score.
First we use one layerto combine their feature vectors to get a hiddenstate hc:hc= ?(wc?
[x?fi: ye?j] + bc) (4)Then we get the matching score from the MLP:s(x,y) = MLP (hc) (5)3 TrainingWe employ a discriminative training strategy witha max-margin objective.
Suppose we are giventhe following triples (x,y+,y?)
from the ora-cle, where x,y+,y?are the feature vectors forf , e?+, e??respectively.
We have the ranking-basedloss as objective:L?(x,y+,y?)
= max(0, 1+s(x,y?
)?s(x,y+))(6)where s(x,y) is the matching score function de-fined in Eq.
5, ?
consists of parameters for boththe convolutional sentence model and MLP.
Themodel is trained by minimizing the above ob-jective, to encourage the model to assign highermatching scores to positive examples and to as-sign lower scores to negative examples.
We usestochastic gradient descent (SGD) to optimize themodel parameters ?.
We train the CDCM modelwith a curriculum strategy to learn the context-dependent semantic similarity at the phrase levelfrom easy (basic semantic similarities betweenthe source and target phrase pair) to difficult(context-dependent semantic similarities for thesame source phrase in varying contexts).3.1 Curriculum TrainingCurriculum learning, first proposed by Bengio etal.
(2009) in machine learning, refers to a se-quence of training strategies that start small, learneasier aspects of the task, and then gradually in-crease the difficulty level.
It has been shownthat the curriculum learning can benefit the non-convex training by giving rise to improved gener-alization and faster convergence.
The key point isthat the training examples are not randomly pre-sented but organized in a meaningful order whichillustrates gradually more concepts, and graduallymore complex ones.For each positive example (f , e?+), we have threetypes of negative examples according to the diffi-culty level of distinguishing the positive examplefrom them:?
Easy: target phrases randomly chosen fromthe phrase table;?
Medium: target phrases extracted from thealigned target sentence for other non-overlapsource phrases in the source sentence;?
Difficult: target phrases extracted from othercandidates for the same source phrase.We want the CDCM model to learn the followingsemantic information from easy to difficult:?
the basic semantic similarity between thesource sentence and target phrase from theeasy negative examples;?
the general semantic equivalent betweenthe source and target phrase pair from themedium negative examples;?
the context-dependent semantic similaritiesfor the same source phrase in varying con-texts from the difficult negative examples.Alg.
1 shows the curriculum training algorithmfor the CDCM model.
We use different portions ofthe overall training instances for different curricu-lums (lines 2-11).
For example, we only use the538Algorithm 1 Curriculum training algorithm.
HereT denotes the training examples, W the initialword embeddings, ?
the learning rate in SGD, nthe pre-defined number, and t the number of train-ing examples.1: procedure CURRICULUM-TRAINING(T , W )2: N1?
easy negative(T )3: N2?
medium negative(T )4: N3?
difficult negative(T )5: T ?
N16: CURRICULUM(T , n ?
t) .
CUR.
easy7: T ?MIX([N1, N2])8: CURRICULUM(T , n ?
t) .
CUR.
medium9: for step?
1 .
.
.
n do10: T ?MIX([N1, N2, N3], step)11: CURRICULUM(T , t) .
CUR.
difficult12: procedure CURRICULUM(T , K)13: iterate until reaching a local minima or K iterations14: calculate L?for a random instance in T15: ?
= ??
?
??L???.
update parameters16: W = W ?
?
?
0.01 ??L??W.
update embeddings17: procedure MIX(N, s = 0)18: len?
length of N19: if len < 3 then20: T ?
sampling with [0.5, 0.5] from N21: else22: T ?
sampling with [1s+2,1s+2,ss+2] from Ntraining instances that consist of positive examplesand easy negative examples in the easy curriculum(lines 5-6).
For the latter curriculums, we gradu-ally increase the difficulty level of the training in-stances (lines 7-12).For each curriculum (lines 12-16), we computethe gradient of the loss objective L?and learn ?using the SGD algorithm.
Note that we mean-while update the word embeddings to better cap-ture the semantic equivalence across languagesduring training.
If the loss function L?reachesa local minima or the iterations reach the pre-defined number, we terminate this curriculum.4 Related WorkOur research builds on previous work in the fieldof context-dependent rule matching and bilingualphrase representations.There is a line of work that employs local con-texts over discrete representations of words orphrases.
For example, He et al.
(2008), Liu etal.
(2008) and Marton and Resnik (2008) em-ployed within-sentence contexts that consist ofdiscrete words to guide rule matching.
Wu etal.
(2014) exploited discrete contextual features inthe source sentence (e.g.
words and part-of-speechtags) to learn better bilingual word embeddings forSMT.
In this study, we take into account all thephrase pairs and directly compute phrasal similari-ties with convolutional representations of the localcontexts, integrating the strengths associated withthe convolutional neural networks (Collobert andWeston, 2008).In recent years, there has also been growinginterest in bilingual phrase representations thatgroup phrases with a similar meaning across dif-ferent languages.
Based on that translation equiv-alents share the same semantic meaning, they cansupervise each other to learn their semantic phraseembeddings in a continuous space (Gao et al.,2014; Zhang et al., 2014).
However, these mod-els focused on capturing semantic similarities be-tween phrase pairs in the global contexts, and ne-glected the local contexts, thus ignored the use-ful discriminative information.
Alternatively, weintegrate the local contexts into our convolutionalmatching architecture to obtain context-dependentsemantic similarities.Meng et al.
(2015) and Zhang (2015) haveproposed independently to summary source sen-tences with convolutional neural networks.
How-ever, they both extend the neural network jointmodel (NNJM) of Devlin et al.
(2014) to includethe whole source sentence, while we focus on cap-turing context-dependent semantic similarities oftranslation pairs.5 Experiments5.1 SetupWe carry out our experiments on the NISTChinese-English translation tasks.
Our trainingdata contains 1.5M sentence pairs coming fromLDC dataset.1We train a 4-gram language modelon the Xinhua portion of the GIGAWORD corpususing the SRI Language Toolkit (Stolcke, 2002)with modified Kneser-Ney Smoothing (Kneserand Ney, 1995).
We use the 2002 NIST MTevaluation test data as the development data, andthe 2004, 2005 NIST MT evaluation test data asthe test data.
We use minimum error rate train-ing (Och, 2003) to optimize the feature weights.For evaluation, case-insensitive NIST BLEU (Pa-pineni et al., 2002) is used to measure translationperformance.
We perform a significance test usingthe sign-test approach (Collins et al., 2005).1The corpus includes LDC2002E18, LDC2003E07,LDC2003E14, Hansards portion of LDC2004T07,LDC2004T08 and LDC2005T06.539Models MT04 MT05 AllBaseline 34.86 33.18 34.40CICM 35.82?33.51?34.95?CDCM135.87?33.58 35.01?CDCM235.97?33.80?35.21?CDCM336.26??33.94??35.40?
?Table 1: Evaluation of translation quality.CDCMkdenotes the CDCM model trained in thekth curriculum in Alg.
1 (i.e., three levels ofcurriculum training), CICM denotes its context-independent counterpart, and ?All?
is the com-bined test sets.
The superscripts ?
and ?
indicatestatistically significant difference (p < 0.05) fromBaseline and CICM, respectively.For training the neural networks, we use 4 con-volution layers for source sentences and 3 convo-lution layers for target phrases.
For both of them, 4pooling layers (pooling size is 2) are used, and allthe feature maps are 100.
We set the sliding win-dow k = 3, and the learning rate ?
= 0.02.
Allthe parameters are selected based on the develop-ment data.
We train the word embeddings using abilingual strategy similar to Yang et al.
(2013), andset the dimension of the word embeddings be 50.To produce high-quality bilingual phrase pairs totrain the CDCM model, we perform forced decod-ing on the bilingual training sentences and collectthe used phrase pairs.5.2 Evaluation of Translation QualityWe have two baseline systems:?
Baseline: The baseline system is an open-source system of the phrase-based model ?Moses (Koehn et al., 2007) with a set of com-mon features, including translation models,word and phrase penalties, a linear distortionmodel, a lexicalized reordering model, and alanguage model.?
CICM (context-independent convolutionalmatching) model: Following the previousworks (Gao et al., 2014; Zhang et al., 2014;Cho et al., 2014), we calculate the match-ing degree of a phrase pair without consider-ing any contextual information.
Each uniquephrase pair serves as a positive example anda randomly selected target phrase from thephrase table is the corresponding negative ex-ample.
The matching score is also introducedinto Baseline as an additional feature.Table 1 summaries the results of CDCMstrained from different curriculums.
No matterfrom which curriculum it is trained, the CDCMmodel significantly improves the translation qual-ity on the overall test data (with gains of 1.0BLEU points).
The best improvement can be up to1.4 BLEU points on MT04 with the fully trainedCDCM.
As expected, the translation performanceis consistently increased with curriculum grow-ing.
This indicates that the CDCM model indeedcaptures the desirable semantic information by thecurriculum learning from easy to difficult.Comparing with its context-independent coun-terpart (CICM, Row 2), the CDCM model showssignificant improvement on all the test data con-sistently.
We contribute this to the incorporationof useful discriminative information embedded inthe local context.
In addition, the performance ofCICM is comparable with that of CDCM1.
This isintuitive, because both of them try to capture thebasic semantic similarity between the source andtarget phrase pair.One of the hypotheses we tested in the course ofthis research was disproved.
We thought it likelythat the difficult curriculum (CDCM3that distin-guishs the correct translation from other candi-dates for a given context) would contribute most tothe improvement, since this circumstance is moreconsistent with the real decoding procedure.
Thisturned out to be false, as shown in Table 1.
Onepossible reason is that the ?negative?
examples(other candidates for the same source phrase) mayshare the same semantic meaning with the posi-tive one, thus give a wrong guide in the supervisedtraining.
Constructing a reasonable set of nega-tive examples that are more semantically differentfrom the positive one is left for our future work.6 ConclusionIn this paper, we propose a context-dependent con-volutional matching model to capture semanticsimilarities between phrase pairs that are sensitiveto contexts.
Experimental results show that our ap-proach significantly improves the translation per-formance and obtains improvement of 1.0 BLEUscores on the overall test data.Integrating deep architecture into context-dependent translation selection is a promising wayto improve machine translation.
In the future, wewill try to exploit contextual information at the tar-get side (e.g., partial translations).540AcknowledgmentsThis work is supported by China National 973project 2014CB340301.
Baotian Hu and QinghaiChen are supported by National Natural ScienceFoundation of China 61173075 and 61473101.
Wethank Junhui Li, and the anonymous reviewers fortheir insightful comments.ReferencesYoshua Bengio, J?er?ome Louradour, Ronan Collobert,and Jason Weston.
2009.
Curriculum learning.
InICML 2009.Yoshua Bengio.
2009.
Learning deep architectures forai.
Foundations and TrendsR?
in Machine Learning,2(1):1?127.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Fethi Bougares, Holger Schwenk, and YoshuaBengio.
2014.
Learning phrase representationsusing rnn encoder-decoder for statistical machinetranslation.
In EMNLP 2014.M.
Collins, P. Koehn, and I. Ku?cerov?a.
2005.
Clauserestructuring for statistical machine translation.
InACL 2005.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In ICML2008.George E Dahl, Tara N Sainath, and Geoffrey E Hinton.2013.
Improving deep neural networks for lvcsr us-ing rectified linear units and dropout.
In ICASSP2013.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In ACL 2014.Jianfeng Gao, Xiaodong He, Wen-tau Yih, andLi Deng.
2014.
Learning continuous phrase rep-resentations for translation modeling.
In ACL 2014.Zhongjun He, Qun Liu, and Shouxun Lin.
2008.
Im-proving statistical machine translation using lexical-ized rule selection.
In COLING 2008.Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network archi-tectures for matching natural language sentences.
InNIPS 2014.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
InICASSP 1995.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL 2003.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InACL 2007.Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.2008.
Maximum entropy based rule selection modelfor syntax-based statistical machine translation.
InEMNLP 2008.Yuval Marton and Philip Resnik.
2008.
Soft syntac-tic constraints for hierarchical phrased-based trans-lation.
In ACL 2008.Fandong Meng, Zhengdong Lu, Mingxuan Wang,Hang Li, Wenbin Jiang, and Qun Liu.
2015.
Encod-ing source language with convolutional neural net-work for machine translation.
In ACL 2015.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL 2003.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In ACL 2002.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proceedings of Seventh Inter-national Conference on Spoken Language Process-ing, volume 3, pages 901?904.
Citeseer.Haiyang Wu, Daxiang Dong, Xiaoguang Hu, Dian-hai Yu, Wei He, Hua Wu, Haifeng Wang, and TingLiu.
2014.
Improve statistical machine transla-tion with context-sensitive bilingual semantic em-bedding model.
In EMNLP 2014.Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and NenghaiYu.
2013.
Word Alignment Modeling with ContextDependent Deep Neural Network.
In ACL 2013.Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, andChengqing Zong.
2014.
Bilingually-constrainedphrase embeddings for machine translation.
In ACL2014.Jiajun Zhang.
2015.
Local translation prediction withglobal sentence representation.
In IJCAI 2015.541
