Proceedings of NAACL-HLT 2013, pages 878?887,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsFinding What Matters in QuestionsXiaoqiang Luo, Hema Raghavan, Vittorio Castelli, Sameer Maskey and Radu FlorianIBM T.J. Watson Research Center1101 Kitchawan Road, Yorktown Heights, NY 10598{xiaoluo,hraghav,vittorio,smaskey,raduf}@us.ibm.comAbstractIn natural language question answering (QA)systems, questions often contain terms andphrases that are critically important for re-trieving or finding answers from documents.We present a learnable system that can ex-tract and rank these terms and phrases (dubbedmandatory matching phrases or MMPs), anddemonstrate their utility in a QA system on In-ternet discussion forum data sets.
The systemrelies on deep syntactic and semantic analysisof questions only and is independent of rele-vant documents.
Our proposed model can pre-dict MMPs with high accuracy.
When used ina QA system features derived from the MMPmodel improve performance significantly overa state-of-the-art baseline.
The final QA sys-tem was the best performing system in theDARPA BOLT-IR evaluation.1 IntroductionIn most question answering (QA) systems andsearch engines term-weights are assigned in a con-text independent fashion using simple TF-IDF likemodels (Robertson and Walker, 1994; Ponte andCroft, 1998).
Even the more recent advancesin information retrieval techniques for query termweighting (Bendersky et al 2010; Bendersky, 2011)typically rely on bag-of-words models and cor-pus statistics, such as inverse-document-frequency(IDF), to assign weights to terms in questions.
Whilesuch solutions may work for keyword queries of thetype common on search engines such as Google,they do not exploit syntactic and semantic informa-tion when it comes to well formed natural languagequestions.
In this paper we propose a new modelthat identifies important terms and phrases in a natu-ral language question, providing better query analy-sis that ultimately leads to significant improvementsin a QA system.To motivate the work presented here, consider thequery ?How does one apply for a New York day carelicense??.
A bag-of-words model would likely as-sign a high score to ?New licenses for day care cen-ters in York county, PA?
because of high word over-lap, but it does not answer the question, and alsothe state is wrong.
A matching component that usesthe phrases ?New York,?
?day care,?
and ?license?is likely to do better.
However, a better matchingcomponent will understand that in the context of thisquery all three phrases ?New York,?
?day care?
and?license?
are important, and that ?New York?
needsto modify ?day care.?
A snippet that does not con-tain1 these important phrases, is unlikely an answer.We call these important phrases mandatory match-ing phrases (MMPs).In this paper, we explore deep syntactic and se-mantic analyses of questions to determine and rankMMPs.
Unlike existing work (Zhao and Callan,2010; Bendersky et al 2010; Bendersky, 2011),where term/concept weights are learned from a setof questions and judged documents based on corpus-based statistics, we annotate questions and build atrainable system to select and score MMPs.
Thismodel relies heavily on existing syntactic parsersand semantic-oriented named-entity recognizers, butdoes not need question answer pairs.
This is espe-1?contain?
here means semantic equivalence or entailment,not necessarily the exact words or phrases.878cially attractive at the initial system-building stagewhen no or little answer data is available.The main contributions of this paper are: firstly,we propose a framework to select and rank impor-tant question phrases (MMPs) for question answer-ing in Section 3.
This framework seamlessly incor-porates lexical, syntactic and semantic information,resulting in an MMP prediction F-measure as highas 88.6%.
Secondly, we show that features derivedfrom identified MMPs improve significantly a rele-vance classification model, in Section 4.2.
Thirdly,we show that using the improved relevance modelinto our QA system results in a statistically signifi-cant 5 point improvement in F-measure, in Section5.
This finding is further corroborated by the resultson the official 2012 BOLT IR (IR, 2012) task wherethe combined system yielded the best performancein the evaluation.2 Related WorkPopular information retrieval systems likeBM25 (Robertson and Walker, 1994) and languagemodels (Ponte and Croft, 1998) use unsupervisedtechniques based on corpus statistics for termweighting.
Many of these techniques are variantsof the one proposed by (Luhn, 1958).
Recently,several researchers have studied approaches for termweighting using supervised learning techniques.However, much of this research has focused oninformation retrieval task rather than on questionanswering problems of the nature addressed inthis paper.
(Bendersky and Croft, 2008) restrictedthemselves to predicting key noun phrases, whichis perhaps sufficient for a retrieval task.
However,for questions like ?Find comments about howAmerican hedge funds legally avoid taxes,?
the verb?avoid?
is perhaps as important as the noun phrase?American hedge funds?
and ?taxes?.
Works likethat of (Lease et al 2009) and (Zhao and Callan,2010) predict importance at the word level.
Whileword level importance is perhaps sufficient foran IR task, predicting the importance of phrases,especially those derived from a parse tree, givesa much richer representation that might also beuseful for better question understanding and thusgenerate more relevant answers.
Both (Lease et al2009; Zhao and Callan, 2010) propose supervisedmethods that learn from a large set of queries andrelevance judgments on their answers.
While this ispossible in a TREC Ad-hoc-retrieval-like task, sucha large training corpus of question-answer pairs isunavailable for most scenarios.
(Monz, 2007) learnsterm weights for the IR component of a questionanswering task.
His work unlike ours does not aimto find the answers to the questions.Most QA systems in the literature have dealtwith answering factoid questions, where the an-swer is a noun phrase in response to questions ofthe form ?Who,?
?Where,?
?When.?
Most sys-tems have a question analysis component that rep-resents the question as syntactic relations in a parseor as deep semantic relations in a handcrafted on-tology (Hermjakob et al 2000; Chu-carroll et al2003; Moldovan et al 2003).
In addition certainsystems (Bunescu and Huang, 2010) aim to find the?focus?
of the question, that is, the noun-phrases inthe question that would co-refer with answers.
Ad-ditionally, much past work has focused on findingthe lexical answer type (Pinchak, 2006; Li and Roth,2002).
Since these papers considered a small num-ber of answer types, rules over the detected relationsand answer types could be applied to find the rel-evant answer.
However, since our system answersnon-factoid questions that can have answer of arbi-trary types, we want to use as few rules as possible.The MMPs therefore become a critical componentof our system, both for question analysis and for rel-evance detection.3 Question Data and MMP ModelTo train the MMP model, we first create a set ofquestions and label their MMPs.
The labeled datais then used to train a statistical model to predictMMPs for new questions as discussed next.3.1 Question CorpusWe use a subset of the DARPA BOLT corpus (seeSection 5.1) containing forum postings in English.Four annotators use a search tool to explore thisdocument collection.
They can perform keywordsearches and retrieve forum threads from which theygenerate questions.
The program participants de-cided a basic set of question types that are out-of-scope of the current research agenda.
Accordingly,879annotators cannot generate questions (1) that requirereasoning or calculation over the data to computethe answers; (2) that are vague or ambiguous; (3)that can be broken into multiple disjoint questions;(4) that are multiple choice questions; (5) that arefactoid questions?the kinds that have already beenwell studied in TREC (Voorhees, 2004).
Any otherkind of question is allowed.
Two other annotators,who have neither browsed the corpus nor generatedthe questions, mark selected spans of the questionsinto one of two categories?MMP-Must and MMP-maybe.
The annotation tool allows arbitrary spansto be highlighted and the annotators are instructed toselect spans corresponding to the smallest semanticunits.
The phrases that are very likely to appear con-tiguously in a relevant answer are marked as MMP-Must.
Annotators can mark multiple spans per ques-tion, but not overlapping spans.
We generated 201annotated questions using this process.Figure 1 contains an example, where ?American,?
?hedge fund,?
and ?legally avoid taxes?
are requiredelements to find answers and are thus marked asMMP-Musts (signified by enclosing rectangles).
Wepurposely annotate MMPs at the word level and notin the parse tree, because this requires minimal lin-guistic knowledge.
We do, however, employ anautomatic procedure to attach MMPs to parse treenodes when generating MMP training instances.3.2 MMP TrainingQuestions annotated in Section 3.1 are first pro-cessed by an information extraction (IE) pipelineconsisting of syntactic parsing, mention detectionand coreference resolution (Florian et al 2004; Luoet al 2004; Luo and Zitouni, 2005).
After IE, wehave access to the syntactic structure represented bya parse tree and semantic information representedby coreferenced mentions (including those of namedentities).To take advantage of the availability of the syn-tactic and semantic information, we first attach theMMP annotations to parse tree nodes of a question,and, if necessary, we augment the parse tree.There are several reasons why we want to embedthe MMPs into a parse tree.
First, many constituentsin parse trees correspond to important phrases wewant to capture, especially proper names.
Second,after an MMP is attached to a tree node, the problemVP0VBFind commentsNNSaboutINSBARPPVPhowWRB NNP NNlegally taxesNNSfundsRB NNSNPSavoidhedgeNPWHADVPVBPAmericanGPENP NP1NP2Figure 1: MMPs are aligned with tree nodes: MMPsare shown in rectangular boxes along with their alignednodes (with slanted labels); augmented parse tree nodes(i.e., NP1, NP2) in dashed nodes.
Dotted edges underNP0 are the structure before the tree is augmented.of predicting MMPs reduces to classifying parse treenodes, and syntactic information can be naturallybuilt into the MMP classifier.
Lastly, and more im-portantly, associating MMPs with tree nodes opensthe door to explore features derived from the syn-tactic parse tree.
For instance, it is easy to readbilexical dependencies from a parse tree (providedthat head information is propagated); with MMPsaligned with the parse tree, bilexical dependenciescan be ranked by examining whether or not an MMPphrase is a head or a dependent.
This way, notonly are the dependencies in a question captured, butMMP scores or ranks can be propagated to depen-dencies as well.
We will discuss more how MMPfeatures are computed in Section 4.2.2.Annotators can mark MMPs that are not perfectlyaligned with a tree node.
Hence, care has to be takenwhen generating MMP training instances.
As an ex-ample, In Figure 1, ?American?
and ?hedge funds?are marked as two separate MMPs, but the Penn-Tree-style parse tree has a flat ?NP0?
constituentspanning directly on ?American hedge fund,?
illus-trated in Figure 1 as dotted edges.To anchor MMPs in the parse tree, we augmentit by combining the IE output and the MMP anno-tation.
In the aforementioned example, ?American?is a named mention with the entity type GPE (geo-political entity) and there is no non-terminal nodespanning it: so, a new node ?NP1?
is created; ?hedgefunds?
is marked as an MMP: so, a second node(?NP2?)
is created to anchor it.880A training instance for building the MMP modelis defined as a span along with an MMP label.
Forinstance, ?hedge funds?
in Figure 1 will generate apositive training instance as ?
(5,6), +1?, where(5,6) is the span of ?hedge funds?
in the questionsentence, and +1 signifies that it is a positive train-ing instance.
For the purpose of this paper we useonly binary labels, mapping all MMP-Must to +1and MMP-Skip and MMP-Maybe to ?1.Formally, we use the following procedure to gen-erate training instances:Algorithm 1 Pseudo code to generate MMP traininginstances.Input: An input question tree with detected men-tions and marked MMPsOutput: A list of MMP training instances1: Foreach mention m in the question2: if no node spans m, and m does not cross bracket3: Find lowest node N dominating m4: Insert a child node of N that spans exactly m5: Foreach mention p in marked MMPs6: Find lowest non-terminal Np dominating p7: Generate a positive training example for Np8: Mark Np as visited9: Recursively generate instances for Np?s children10: Generate a negative training instance for all un-visited nodes in Step 5-9Steps 1 to 4 augment the question tree by creatinga node for each named mention, provided that no ex-isting node spans exactly the mention and the men-tion does not cross-bracket tree constituents.
Steps 5to 8 generate positive training instances for markedMMPs; step 9 recursively generates positive traininginstances 2 for tree nodes dominated by Np, whereNp is the lowest non-terminal node dominating themarked MMP p.After MMP training instances are generated wedesign and compute features for each instance, anduse them to train a classifier.3.3 MMP Features and ClassifierWe compute four types of features that will be usedin a statistical classifier.
These features are designedto characterize a phrase from the lexical, syntactic,2One exception to this step is that if a node spans a singlestop word, then a negative training instance is generated.semantic and corpus-level aspect.
The weights asso-ciated with these features are automatically learnedfrom training data.We will use ?
(NP1 American)?
in Figure 1 as therunning example below.Lexical Features: Lexical features are motivated bythe observation that spellings in English sometimesoffer important cues about word significance.
Forexample, an all-capitalized word often signifies anacronym; an all-digit word in a question is likely ayear, etc.
We compute the following lexical featuresfor a candidate MMP:CaseFeatures: is the first word of an MMPupper-case?
Is it all capital letters?
Does it containnumeric letters?
For ?
(NP American)?
in Figure 1,the upper-case feature fires.CommonQWord: Does the MMP contain questionwords, including ?What,?
?When,?
?Who,?
etc.Syntactic Features: The second group of featuresare computed from syntactic parse trees after anno-tated MMPs are aligned with question parse-treesas described previously.PhraseLabel: this feature returns the phrasal labelof the MMP.
For ?
(NP American)?
in Figure 1, thefeature value is ?NP.?
This captures that an NP ismore likely an MMP than, say, an ADVP.NPUnique: this Boolean feature fires if a phraseis the only NP in a question, indicating that thisconstituent probably should be matched.
For ?(NPAmerican),?
the feature value would be false.PosOfPTN: these features characterize the positionof the parse tree node to which an MMP is anchored.They compute: (1) the position of the left-mostword of the node; (2) whether the left-most word isthe beginning of the question; (3) the depth of theanchoring node, defined as the length of the path tothe root node.
For ?
(NP American)?
in Figure 1, thefeatures state that it is the 5th word in the sentence;it is not the first word of the sentence; and the depthof the node is 6 (where root has depth 0).PhrLenToQLenRatio: This feature computes thenumber of words in an MMP, and its relative ratio tothe sentence length.
This feature controls the lengthof MMPs at decoding time, since most of MMPsare short.Semantic Features (NETypes): The third group offeatures are computed from named entities and aimto capture semantic information.
The feature tests if881a phrase is or contains a named entity, and, if thisis the case, the value is the entity type.
For ?(NPAmerican)?
in Figure 1, the feature value would be?GPE.
?Corpus-based Features ( AvgCorpusIDF): Thisgroup of features computes the average of the IDFsof the words in this phrase.
From the corpus IDF,we also compute the ratio between the number ofstop words and the total number of words in theMMP, and use it as another feature.3.4 MMP Classification ResultsWe now show that we can reliably predict MMPs ofquestions.
We split our set of 201 annotated ques-tions into a training set consisting of 174 questionsand a test set with the remaining 27 questions.
Weuse the procedure and features described in Sec-tion 3 to train a logistic regression binary classifierusing WEKA.
Then, the trained MMP classifier isapplied to the test set question trees.
Since the classbias is quite skewed (only 16% of the phrases aremarked as MMP-Must) we also use re-sampling attraining time to balance the prior probability of thetwo classes.
At testing time, a parser and a men-tion detection algorithm (Florian et al 2004; Luo etal., 2004; Luo and Zitouni, 2005) are run on eachquestion.
The detected mentions are then used toaugment the question parse trees.
The MMP classi-fier achieves an 88.6% F-measure (cf.
Table 1, with91.6% precision).
This is a respectable number, con-sidering the limited amount of training data.
We ex-perimented with decision trees and bagging as wellbut found logistic regression to work the best.Feature P R F1AvgCorpusIDF 0.849 0.634 0.725+NPUnique 0.868 0.634 0.732+NETypes 0.867 0.662 0.750+PhraseLabel 0.890 0.705 0.783+CaseFeatures 0.829 0.820 0.824+PosOfPTN 0.911 0.852 0.880+PhrLenToQLenRatio 0.915 0.855 0.883+commonQWord 0.916 0.858 0.886Table 1: The performances of the MMP classifier whileincrementally adding features.The examples in Table 2 illustrate the top threeMMPs produced by the model on two questions.These results are encouraging: in the first exam-ple the word AIDS is clearly the most ?important?word, but IDF alone is not adequate to place it in thetop since AIDS is also a common verb (words arelower-cased before IDF look-up).
Similarly, in thethird example, the phrase ?the causes?
has a muchhigher MMP score than the phrase ?the concerns?
(MMP score of 0.109), even though the words ?con-cerns?
has a slightly higher IDF, 2.80, than the word?causes?(2.68).
However, in this question, under-standing that the word ?causes?
is critical to themeaning of the question is critical and is capturedby the MMP model.We analyzed feature importance for MMP classi-fication by incrementally adding each feature groupto the model.
The result is tabulated in Table 1.
Notsurprisingly, syntactical (i.e., ?NPUnique,?
?Phrase-Label?
and ?PosOfPTN?)
and semantic features(i.e., ?NETypes?)
are complementary to the corpus-based statistics features (i.e., average IDF).
Lexicalfeatures also improve recall: the addition of ?Case-Features?
boosts the F-measure by 4 points.
At firstsight, it is surprising that the feature group ?PosOf-PTN,?
which characterize the position of a candi-date MMP relative to the sentence and relative to theparse tree, has such a large impact?it improves theF-measure by 5.6 points.
However, a cursory brows-ing of the training questions reveals that most MMPsare short and concentrate towards the end of the sen-tence.
So this feature group helps by directing themodel to predict MMPs at the end of the sentenceand to prefer short phrases versus long ones.4 Relevance Model with MMPsWe now validate our second hypothesis that MMPsare effective for open domain question answering.We demonstrate this through the improvement inperformance on relevance prediction.
More specif-ically, given a natural language question, the taskis one of finding relevant sentences in posts on on-line forums.
The relevance prediction componentis critical for question answering as has been seenin TREC(Ittycheriah and Roukos, 2001) and morerecently in the Jeopardy challenge(Gondek et al2012).
The improved relevance model further im-proves our question answering system as seen inSection 5.882Question Top 3 MMPs MMP-scoreTop wordsby IDFList statistics about changes in the de-mographics of AIDS.1: AIDS 0.955 demographics2: changes 0.525 AIDS3: the demographics 0.349 statisticsWhat are the concerns about thecauses of autism?1: autism 0.989 autism2: the causes 0.422 concerns3: the causes of autism 0.362 causesTable 2: Example questions and the top-3 phrases ranked by the MMP model.4.1 Data for Relevance ModelThe data to train and test the relevance model is ob-tained as follows.
First, a rudimentary version (i.e.,key word search) of a QA system using Lucene isbuilt.
The Lucene index comprised of a large num-ber of threads in online forums released to the par-ticipants of the BOLT-IR task(IR, 2012) for devel-opment of our systems.
The corpus is described inmore detail in Sec.
5.
Top snippets returned by thesearch engine are judged for relevancy by our an-notators.
The initial (small) batch of data is usedto train a relevance model which is deployed in thesystem.
The new model is in turn used to createmore answers for new questions.
When more datais collected, the relevance model is retrained and re-deployed to collect more data.
The process is iter-ated for several months, and at the end of this pro-cess, a total of 390 training questions are created andabout 28,915 snippets are judged by human annota-tors, out of which about 6,528 are relevant answers.These question-answers pairs are used to train the fi-nal relevance model used in our question-answeringsystem.
A separate held-out test set of 59 questionsis created and its system output is also judged by hu-mans.
This data set is our test set.4.2 Relevance PredictionA key component in our question-answering sys-tem is the snippet relevance model, which is usedto compute the probability that a snippet is relevantto a question.
The relevance model is a conditionaldistribution P (r|q, s;D), where r is a binary ran-dom variable indicating if the candidate snippet s isrelevant to the question q.
D is the document wherethe snippet s is found.In our question answering system, MMPs ex-tracted from questions are used to compute the fea-tures for the relevance model.
To test their effective-ness, we conduct a controlled experiment by com-paring the system with MMP features with 2 base-lines: (1) a system without MMP features; (2) abaseline with each word as an MMP and the word?sIDF as the MMP score.4.2.1 Baseline FeaturesWe list the features used in our baseline system,where no MMP feature is used.
The features canbe categorized into the following types.
(1) TextMatch Features: One set of features are the cosinescores between different representations of the queryand the snippet.
In one version the query and snip-pet words are used as is; in another version the queryand snippet are stemmed using porter stemmer; inyet another the words are morphed to their roots bya table extracted from WordNet.
We also computethe inclusion scores (the proportion of query wordsfound in the snippet) and other word overlap fea-tures.
(2) Answer Type Features: The top 3 pre-dictions of a statistical classifier trained to predictanswer categories were used as features.
(3) Men-tion Match Features compute whether a named en-tity in the query occurs in the snippet.
The matchingtakes into consideration the results from within andcross document coreference resolution componentsfor nominal and pronominal mentions.
(4) Eventmatch features use several hand-crafted dictionar-ies containing terms exclusive to various types ofevents like ?violence?, ?legal?, ?election?.
Accord-ingly a set of features that take a value of ?1?
ifboth the query and snippet contain the same eventtype were designed.
(5) Snippet Statistics: Severalfeatures based on snippet length, the position of thesnippet in the post etc were created.8834.2.2 Features Derived from MMPThe MMPs extracted from questions are used tocompute features in the following ways.As MMPs are aligned with a question?s syntactictree, they can be used to find answers by matchinga question constituent with that of a candidate snip-pet.
The MMP model also returns a score for eachphrase, which can be used to compute the degree towhich a question matches a candidate snippet.In this section, we use s = wn1 to denote a snip-pet with words w1, w2, ?
?
?
, wn, and m to denotea phrase from the MMP model along with a scoreM(m).
The features are listed below:HardMatch: Let I(m ?
s) be a 1 or 0 functionindicating if a snippet contains the MMP m, thenthe hard match score is computed as:HM(q, s) =?m?q M(m)I(m ?
s)?m?q M(m).SoftLMMatch: The SoftLMMatch score is alanguage-model (LM) based score, similar to thatused in (Bendersky and Croft, 2008), except thatMMPs play the role of concepts.
The snippet-sidelanguage model score LM(v|s) is computed as:LM(v|s) =?ni=1 I(wi = v) + 0.05n + 0.05|V | ,where wi is the ith in snippet s; I(wi = v) is anindicator function, taking value 1 if wi is v and 0otherwise; |V | is the vocabulary size.The soft match score between a question q and asnippet s is then:SM(q, s) =?m?q(M(m)?w?m LM(w|s))?m?q M(m),where m ?
q denotes all MMPs in question q, andsimilarly, w ?
m signifying words in m.MMPInclScore: An MMP m?s inclusion score is:IS(m, s) =?w?m I(l(w, s) > ?
)IDF (w)?w?m IDF (w),where w ?
m are the words in m; I(?)
is the in-dicator function taking value 1 when the argumentis true and 0 otherwise; ?
is a constant threshold;IDF (w) is the IDF of word w. l(w, s) is the sim-ilarity of word w to the snippet s as: l(w, s) =maxv?sJW (w, v), where JW (w, v) is the JaroWinkler similarity score between words w and v.The MMP weighted inclusion score between thequestion q and snippet s is computed as:IS(q, s) =?m?q M(m)IS(m, s)?m?q M(m)MMPRankDep: This feature, RD(q, s) first testsif there exists a matched bilexcial dependency be-tween q and s; if yes, it further tests if the head ordependent in the matched dependency is the head ofany MMP.Let m(i) be the ith ranked MMP; let ?wh, wd|q?and ?uh, ud|s?
be bilexical dependencies from q ands, respectively, where wh and uh are the heads andwd and ud are the dependents; let EQ(w, u) be afunction testing if the question word w and snip-pet word u are a match.
In our implementation,EQ(w, u) is true if either w and u are exactly thesame, or their morphs are the same, or they headthe same entity, or their synset in WordNet overlap.With these notations, RD(q, s) is true if and only ifEQ(wh, uh) ?
EQ(wd, ud) ?wh ?
m(i) ?wd ?
m(j)is true for some ?wh, wd|q?, for some ?uh, ud|s?
andfor some i and j.EQ(wh, uh)?EQ(wd, ud) requires that the ques-tion dependency ?wh, wd|q?
and the snippet depen-dency ?uh, ud|s?
match; wh ?
m(i) ?wd ?
m(j) re-quires that the head word and dependent word are inthe ith-rank and jth rank MMP, respectively.
There-fore, RD(q, s) is a dependency feature enhancedwith MMPs.To test the effectiveness of the MMP features, wetrained 3 snippet classifiers on the data describedin Section 4.1: one baseline system without MMPfeatures (henceforth ?no-MMP?
); a second baselinewith words as MMPs and their IDFs as the scoresin the MMP model(henceforth ?IDF-as-MMP?
); thethird system uses the MMPs generated by the modelfrom Section 3 and all MMP features described inthis section.
We used two types of classifiers: deci-sion tree (DTree) and logistic regression (Logit).The classification results on a set of 59 questionsdisjoint from the training set are shown in Table 3.The numbers in the table are F-measure on answersnippets (or positive snippets).
Within a machine884LearnerModel DTree LogitnoMMP 0.426 0.458IDF-as-MMP 0.413 0.455MMP 0.451 0.470Table 3: F-measure for Relevance Prediction.learning method, the model with MMP features isalways the best.
Between the two classifiers, the lo-gistic regression models are consistently better thanthe decision tree ones.
The results show that MMPfeatures are very helpful to the relevance model.5 End-to-End System ResultsThe question-answering system is used in the 2012BOLT IR evaluation (IR, 2012).
The task is to an-swer questions against a corpus of posts collectedfrom Internet discussion forums in 3 languages:Arabic, Chinese and English.
There are 499K, 449Kand 262K threads in each of these languages.
TheArabic and Chinese posts were first translated intoEnglish before being processed.
We now describeour experiments on the set of 59 questions devel-oped internally and demonstrate the effectiveness ofan MMP based relevance model in the end-to-endsystem.
In the next subsection we discuss our per-formance in the BOLT-IR evaluation done by NISTfor DARPA.We now briefly describe the question-answeringsystem we developed for the DARPA BOLT IR task,where we applied the MMP classifier and its fea-tures.
Users submit questions to the system in natu-ral language; the BOLT program mandates that thesequestions comply with the restrictions described inSection 3.1.
Questions are analyzed by a query pre-processing stage that includes our MMP extractionclassifier.
The preprocessed queries are convertedto search queries.
These are sent to an Indri-basedsearch engine (Strohman et al 2005), which re-turns candidate passages, typically spanning numer-ous sentences.
Each sentence of the retrieved pas-sages is analyzed by a relevance detection module,consisting of a statistical classifier that uses, amongothers, features computed from the MMPs extractedfrom the questions.
Sentences or spans that aredeemed relevant to the question by the relevance de-tection module are further grouped into equivalenceclasses that provide different information about theanswers.
The system generates a single answer foreach equivalence class, since elements of the sameclass are redundant with respect to each other.
Theelements of each equivalence class are convertedinto citations that support the corresponding answer.The ultimate goal of the MMP model is to im-prove the performance of our question-answeringsystem.
To test the effectiveness of the MMP model,we contrast the model trained in Section3 with anIDF baseline, where each non-stop word in a ques-tion is an MMP and its score is the corpus IDF.
TheIDF baseline is what a typical question answeringsystem would do in absence of deep question analy-sis.
To have a fair comparison, the two systems aretested on the same set of 59 questions as the rele-vance model.The results of the IDF baseline and MMP systemare tabulated in Table 4.
Note that the recalls areless than 1.0 because (1) annotated snippets comefrom both systems; (2) the annotation is done for allsnippets in a window surrounding system snippets.As can be seen from Table 4, the MMP system isabout 5 points better than the baseline system.
Theprecision is notably better by 2 points, and the re-call is far better (by 7.7%) than that of the baseline.We also compute the question-level F-measures andconduct a Wilcoxon signed-rank test for paired sam-ples.
The test indicates that the MMP system is bet-ter than the baseline system at p < 0.00066.
There-fore, the MMP system has a clear advantage over thebaseline system.System Prec Recall F1baseline .4228 .3679 .3935MMP .4425 .4452 .4438Table 4: End-to-End system result on 59 questions.5.1 BOLT Evaluation ResultsThe BOLT evaluation consists of 146 questions,mostly event- or topic- related, e.g., ?What are peo-ple saying about the ending of NASA?s space shuttleprogram??.
A system answer, if correct, is mappedmanually to a facet, which is one semantic unit thatanswers the question.
For each question, facetsare collected across all participants?
submission.
A885facet-based F-measure is computed for each partic-ipating site.
The recall from which the official F-measure is computed is weighted by snippet cita-tions (a citation is a reference to the original docu-ment that supports the correct facet).
In other words,a snippet with more citations leads to a higher recallthan one with less citations.
The performances of4 participating sites are listed in Table 5.
Note thatthe F-measure is weighted and is not necessarily anumber between the precision and the recall.Facet MetricSite Precision Recall (Weighted) FSITE 1 0.2713 0.1595 0.1713SITE 2 0.1500 0.1316 0.1109SITE 3 0.1935 0.2481 0.1734Ours 0.2729 0.2195 0.2046Table 5: Official BOLT 2012 IR evaluation results..Among 4 participating sites, our system has thehighest performance.
SITE 1 has about the samelevel of precision, with lower recall, while SITE 3has the best recall, but lower precision.
The resultsvalidate that the MMP question analysis techniquepresented in this paper is quite effective.6 ConclusionsWe propose a framework to select and rank manda-tory matching phrases (MMP) for question answer-ing.
The framework makes full use of the lexical,syntactic and semantic information in a question anddoes not require answer data.The proposed MMP framework is tested at 3 lev-els in a full QA system and is shown to be very effec-tive to improve its performance: first, we show thatit is possible to reliably predict MMPs from ques-tions alone: the MMP classifier can achieve an F-measure as high as 88.6%; second, phrases proposedby the MMP model are incorporated into a snippetrelevance model and we show that it improves itsperformance; third, the MMP framework is used inan question answering system which achieved thebest performance in the official 2012 BOLT IR (IR,2012) evaluation.AcknowledgmentsThis work was partially supported by the DefenseAdvanced Research Projects Agency under contractNo.
HR0011-12-C-0015.
The views and findingscontained in this material are those of the authorsand do not necessarily reflect the position or policyof the U.S. government and no official endorsementshould be inferred.ReferencesMichael Bendersky and W. Bruce Croft.
2008.
Discov-ering key concepts in verbose queries.
Proceedings ofthe 31st annual international ACM SIGIR conferenceon research and development in information retrieval- SIGIR ?08, page 491.Michael Bendersky, Donald Metzler, and W. Bruce Croft.2010.
Learning concept importance using a weighteddependence model.
Proceedings of the third ACM in-ternational conference on Web search and data mining- WSDM ?10, page 31.Michael Bendersky.
2011.
Parameterized conceptweighting in verbose queries.
Proceedings of the 34thannual international ACM SIGIR conference on re-search and development in information retrieval.Razvan Bunescu and Yunfeng Huang.
2010.
Towards ageneral model of answer typing: Question focus iden-tification.
In Proceedings of the 11th InternationalConference on Intelligent Text Processing and Com-putational Linguistics (CICLing).Jennifer Chu-carroll, John Prager, Christopher Welty,Krzysztof Czuba, and David Ferrucci.
2003.
A multi-strategy and multi-source approach to question an-swering.
In In Proceedings of Text REtrieval Confer-ence.R Florian, H Hassan, A Ittycheriah, H Jing, N Kamb-hatla, X Luo, N Nicolov, and S Roukos.
2004.
Astatistical model for multilingual entity detection andtracking.
In Daniel Marcu Susan Dumais and SalimRoukos, editors, HLT-NAACL 2004: Main Proceed-ings, pages 1?8, Boston, Massachusetts, USA, May 2- May 7.
Association for Computational Linguistics.D.
C. Gondek, A. Lally, A. Kalyanpur, J. W. Murdock,P.
A. Duboue, L. Zhang, Y. Pan, Z. M. Qiu, andC.
Welty.
2012.
A framework for merging and rank-ing of answers in DeepQA.
IBM Journal of Researchand Development, 56(3.4):14:1 ?14:12, may-june.Ulf Hermjakob, Eduard H. Hovy, and Chin yew Lin.2000.
Knowledge-based question answering.
In InProceedings of the 6th World Multiconference on Sys-tems, Cybernetics and Informatics (SCI-2002, pages772?781.886BOLT IR.
2012.
Broad operational language translation(BOLT).
www.darpa.mil/Our_Work/I2O/Programs/Broad_Operational_Language_Translat%ion_(BOLT).aspx.
[Online; ac-cessed 10-Dec-2012].Abraham Ittycheriah and Salim Roukos.
2001.
IBM?sstatistical question answering system - TREC-11.
InProceedings of the Text REtrieval Conference.Matthew Lease, James Allan, and W. Bruce Croft.
2009.Advances in Information Retrieval, volume 5478 ofLecture Notes in Computer Science.
Springer BerlinHeidelberg, Berlin, Heidelberg, April.Xin Li and Dan Roth.
2002.
Learning question classi-fiers.
In Proceedings of the 19th international confer-ence on Computational linguistics - Volume 1, COL-ING ?02, pages 1?7, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.H.
P. Luhn.
1958.
A business intelligence system.
IBMJ.
Res.
Dev., 2(4):314?319, October.Xiaoqiang Luo and Imed Zitouni.
2005.
Multi-lingual coreference resolution with syntactic fea-tures.
In Proc.
of Human Language Technology(HLT)/Empirical Methods in Natural Language Pro-cessing (EMNLP).Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the bell tree.
In Proc.
of ACL.Dan Moldovan, Christine Clark, Sanda Harabagiu, andSteve Maiorano.
2003.
Cogex: a logic prover forquestion answering.
In Proceedings of the 2003 Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technology - Volume 1, NAACL ?03, pages 87?93.Christof Monz.
2007.
Model tree learning for queryterm weighting in question answering.
In Proceed-ings of the 29th European conference on IR re-search, ECIR?07, pages 589?596, Berlin, Heidelberg.Springer-Verlag.Christopher Pinchak.
2006.
A probabilistic answer typemodel.
In In EACL, pages 393?400.Jay M. Ponte and W. Bruce Croft.
1998.
A languagemodeling approach to information retrieval.
In Pro-ceedings of the 21st annual international ACM SIGIRconference on research and development in informa-tion retrieval, SIGIR ?98, pages 275?281, New York,NY, USA.
ACM.S.
E. Robertson and S. Walker.
1994.
Some simpleeffective approximations to the 2-poisson model forprobabilistic weighted retrieval.
In Proceedings ofthe 17th annual international ACM SIGIR conferenceon research and development in information retrieval,SIGIR ?94, pages 232?241, New York, NY, USA.Springer-Verlag New York, Inc.Trevor Strohman, Donald Metzler, Howard Turtle, andW.
Bruce Croft.
2005.
Indri: a language-model basedsearch engine for complex queries.
Technical report,in Proceedings of the International Conference on In-telligent Analysis.Ellen M. Voorhees.
2004.
Overview of the TREC 2004question answering track.
In TREC.Le Zhao and Jamie Callan.
2010.
Term necessity predic-tion.
Proceedings of the 19th ACM international con-ference on Information and knowledge management -CIKM ?10, page 259.887
