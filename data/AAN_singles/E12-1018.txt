Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 174?184,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsTree Representations in Probabilistic Models for Extended NamedEntities DetectionMarco DinarelliLIMSI-CNRSOrsay, Francemarcod@limsi.frSophie RossetLIMSI-CNRSOrsay, Francerosset@limsi.frAbstractIn this paper we deal with Named En-tity Recognition (NER) on transcriptions ofFrench broadcast data.
Two aspects makethe task more difficult with respect to previ-ous NER tasks: i) named entities annotatedused in this work have a tree structure, thusthe task cannot be tackled as a sequence la-belling task; ii) the data used are more noisythan data used for previous NER tasks.
Weapproach the task in two steps, involvingConditional Random Fields and Probabilis-tic Context-Free Grammars, integrated in asingle parsing algorithm.
We analyse theeffect of using several tree representations.Our system outperforms the best system ofthe evaluation campaign by a significantmargin.1 IntroductionNamed Entity Recognition is a traditinal task ofthe Natural Language Processing domain.
Thetask aims at mapping words in a text into seman-tic classes, such like persons, organizations or lo-calizations.
While at first the NER task was quitesimple, involving a limited number of classes (Gr-ishman and Sundheim, 1996), along the yearsthe task complexity increased as more complexclass taxonomies were defined (Sekine and No-bata, 2004).
The interest in the task is related toits use in complex frameworks for (semantic) con-tent extraction, such like Relation Extraction ap-plications (Doddington et al 2004).This work presents research on a Named EntityRecognition task defined with a new set of namedentities.
The characteristic of such set is in thatnamed entities have a tree structure.
As conce-quence the task cannot be tackled as a sequencelabelling approach.
Additionally, the use of noisydata like transcriptions of French broadcast data,makes the task very challenging for traditionalNLP solutions.
To deal with such problems, weadopt a two-steps approach, the first being real-ized with Conditional Random Fields (CRF) (Laf-ferty et al 2001), the second with a ProbabilisticContext-Free Grammar (PCFG) (Johnson, 1998).The motivations behind that are:?
Since the named entities have a tree struc-ture, it is reasonable to use a solution com-ing from syntactic parsing.
However pre-liminary experiments using such approachesgave poor results.?
Despite the tree-structure of the entities,trees are not as complex as syntactic trees,thus, before designing an ad-hoc solution forthe task, which require a remarkable effortand yet it doesn?t guarantee better perfor-mances, we designed a solution providinggood results and which required a limited de-velopment effort.?
Conditional Random Fields are models ro-bust to noisy data, like automatic transcrip-tions of ASR systems (Hahn et al 2010),thus it is the best choice to deal with tran-scriptions of broadcast data.
Once wordshave been annotated with basic entity con-stituents, the tree structure of named entitiesis simple enough to be reconstructed withrelatively simple model like PCFG (Johnson,1998).The two models are integrated in a single pars-ing algorithm.
We analyze the effect of the use of174Zahraname.firstAbouchname.lastpers.indZahrnme .n faisnthn Anhbomh.mtuomnh.nAahcAatlpu.AFigure 1: Examples of structured named entities annotated on thedata used in this workseveral tree representations, which result in differ-ent parsing models with different performances.We provide a detailed evaluation of our mod-els.
Results can be compared with those obtainedin the evaluation campaign where the same datawere used.
Our system outperforms the best sys-tem of the evaluation campaign by a significantmargin.The rest of the paper is structured as follows: inthe next section we introduce the extended namedentities used in this work, in section 3 we describeour two-steps algorithm for parsing entity trees,in section 4 we detail the second step of our ap-proach based on syntactic parsing approaches, inparticular we describe the different tree represen-tations used in this work to encode entity treesin parsing models.
In section 6 we describe andcomment experiments, and finally, in section 7,we draw some conclusions.2 Extended Named EntitiesThe most important aspect of the NER task weinvestigated is provided by the tree structure ofnamed entities.
Examples of such entities aregiven in figure 1 and 2, where words have been re-move for readability issues and are: (?90 personsare still present at Atambua.
It?s there that 3 employ-ees of the High Conseil of United Nations for refugeeshave been killed yesterday morning?
):90 personnes toujours pre?sentes a`Atambua c?
est la` qu?
hier matin onte?te?
tue?s 3 employe?s du haut commis-sariat des Nations unies aux re?fugie?s ,le HCRWords realizing entities in figure 2 are in bold,and they correspond to the tree leaves in thepicture.
As we see in the figures, entitiescan have complex structures.
Beyond the useof subtypes, like individual in person (to givepers.ind), or administrative in organization(to give org.adm), entities with more specific con-tent can be constituents of more general enti-ties to form tree structures, like name.first andZah rnme.fairstfhr.AabiAfrot taie f uiecirbuluepfuieAbafeApehZah dutb taierpAabilst.A.rhhrnme.fairstfFigure 2: An example of named entity tree corresponding to en-tities of a whole sentence.
Tree leaves, corresponding to sentencewords have been removed to keep readabilityQuaero training dev# sentences 43,251 112words entities words entities# tokens 1,251,432 245,880 2,659 570# vocabulary 39,631 134 891 30# components ?
133662 ?
971# components dict.
?
28 ?
18# OOV rate [%] ?
?
17.15 0Table 1: Statistics on the training and development sets of theQuaero corpusname.last for pers.ind or val (for value) and ob-ject for amount.These named entities have been annotated ontranscriptions of French broadcast news comingfrom several radio channels.
The transcriptionsconstitute a corpus that has been split into train-ing, development and evaluation sets.The evalu-ation set, in particular, is composed of two setof data, Broadcast News (BN in the table) andBroadcast Conversations (BC in the table).
Theevaluation of the models presented in this workis performed on the merge of the two data types.Some statistics of the corpus are reported in ta-ble 1 and 2.
This set of named entities has beendefined in order to provide more fine semantic in-formation for entities found in the data, e.g.
aperson is better specified by first and last name,and is fully described in (Grouin, 2011) .
In or-der to avoid confusion, entities that can be associ-ated directly to words, like name.first, name.last,val and object, are called entity constituents, com-ponents or entity pre-terminals (as they are pre-terminals nodes in the trees).
The other entities,like pers.ind or amount, are called entities or non-terminal entities, depending on the context.3 Models Cascade for Extended NamedEntitiesSince the task of Named Entity Recognition pre-sented here cannot be modeled as sequence la-belling and, as mentioned previously, an approach175Quaero test BN test BC# sentences 1704 3933words entities words entities# tokens 32945 2762 69414 2769# vocabulary 28 28# components ?
4128 ?
4017# components dict.
?
21 ?
20# OOV rate [%] 3.63 0 3.84 0Table 2: Statistics on the test set of the Quaero corpus, divided inBroadcast News (BN) and Broadcast Conversations (BC)Figure 3: Processing schema of the two-steps approach proposedin this work: CRF plus PCFGcoming from syntactic parsing to perform namedentity annotation in ?one-shot?
is not robust onthe data used in this work, we adopt a two-steps.The first is designed to be robust to noisy data andis used to annotate entity components, while thesecond is used to parse complete entity trees andis based on a relatively simple model.
Since weare dealing with noisy data, the hardest part of thetask is indeed to annotate components on words.On the other hand, since entity trees are relativelysimple, at least much simpler than syntactic trees,once entity components have been annotated in afirst step, for the second step, a complex model isnot required, which would also make the process-ing slower.
Taking all these issues into account,the two steps of our system for tree-structurednamed entity recognition are performed as fol-lows:1.
A CRF model (Lafferty et al 2001) is usedto annotate components on words.2.
A PCFG model (Johnson, 1998) is usedto parse complete entity trees upon compo-nents, i.e.
using components annotated byCRF as starting point.This processing schema is depicted in figure 3.Conditional Random Fields are described shortlyin the next subsection.
PCFG models, constitutingthe main part of this work together with the analy-sis over tree representations, is described more indetails in the next sections.3.1 Conditional Random FieldsCRFs are particularly suitable for sequence la-belling tasks (Lafferty et al 2001).
Beyond thepossibility to include a huge number of featuresusing the same framework as Maximum Entropymodels (Berger et al 1996), CRF models en-code global conditional probabilities normalizedat sentence level.Given a sequence of N words WN1 =w1, ..., wN and its corresponding components se-quence EN1 = e1, ..., eN , CRF trains the condi-tional probabilitiesP (EN1 |WN1 ) =1ZNYn=1expMXm=1?m ?
hm(en?1, en, wn+2n?2)!
(1)where ?m are the training parameters.hm(en?1, en, wn+2n?2) are the feature functionscapturing dependencies of entities and words.
Zis the partition function:Z =Xe?N1NYn=1H(e?n?1, e?n, wn+2n?2) (2)which ensures that probabilities sum up to one.e?n?1 and e?n are components for previous and cur-rent words, H(e?n?1, e?n, wn+2n?2) is an abbreviationfor?Mm=1 ?m ?
hm(en?1, en, wn+2n?2), i.e.
the setof active feature functions at current position inthe sequence.In the last few years different CRF implemen-tations have been realized.
The implementationwe refer in this work is the one described in(Lavergne et al 2010), which optimize the fol-lowing objective function:?log(P (EN1 |WN1 )) + ?1??
?1 +?22??
?22 (3)??
?1 and ??
?22 are the l1 and l2 regulariz-ers (Riezler and Vasserman, 2004), and togetherin a linear combination implement the elastic netregularizer (Zou and Hastie, 2005).
As mentionedin (Lavergne et al 2010), this kind of regulariz-ers are very effective for feature selection at train-ing time, which is a very good point when dealingwith noisy data and big set of features.1764 Models for Parsing TreesThe models used in this work for parsing en-tity trees refer to the models described in (John-son, 1998), in (Charniak, 1997; Caraballo andCharniak, 1997) and (Charniak et al 1998), andwhich constitutes the basis of the maximum en-tropy model for parsing described in (Charniak,2000).
A similar lexicalized model has been pro-posed also by Collins (Collins, 1997).
All thesemodels are based on a PCFG trained from dataand used in a chart parsing algorithm to find thebest parse for the given input.
The PCFG modelof (Johnson, 1998) is made of rules of the form:?
Xi ?
XjXk?
Xi ?
wwhere X are non-terminal entities and w areterminal symbols (words in our case).1 The prob-ability associated to these rules are:pi?j,k =P (Xi ?
Xj , Xk)P (Xi)(4)pi?w =P (Xi ?
w)P (Xi)(5)The models described in (Charniak, 1997;Caraballo and Charniak, 1997) encode probabil-ities involving more information, such as headwords.
In order to have a PCFG model made ofrules with their associated probabilities, we ex-tract rules from the entity trees of our corpus.
Thisprocessing is straightforward, for example fromthe tree depicted in figure 2, the following rulesare extracted:S?
amount loc.adm.town time.dat.rel amountamount?
val objecttime.date.rel?
name time-modifierobject?
func.collfunc.coll?
kind org.admorg.adm?
nameUsing counts of these rules we then computemaximum likelihood probabilities of the RightHand Side (RHS) of the rule given its Left HandSide (LHS).
Also binarization of rules, applied to1These rules are actually in Chomsky Normal Form, i.e.unary or binary rules only.
A PCFG, in general, can have anyrule, however, the algorithm we are discussing convert thePCFG rules into Chomsky Normal Form, thus for simplicitywe provide directly such formulation.Figure 4: Baseline tree representations used in the PCFG parsingmodelFigure 5: Filler-parent tree representations used in the PCFG pars-ing modelhave all rules in the form of 4 and 5, is straight-forward and can be done with simple algorithmsnot discussed here.4.1 Tree Representations for ExtendedNamed EntitiesAs discussed in (Johnson, 1998), an importantpoint for a parsing algorithm is the representationof trees being parsed.
Changing the tree represen-tation can change significantly the performancesof the parser.
Since there is a large difference be-tween entity trees used in this work and syntac-tic trees, from both meaning and structure pointof view, it is worth performing an analysis withthe aim of finding the most suitable representa-tion for our task.
In order to perform this analy-sis, we start from a named entity annotated on thewords de notre president , M. Nicolas Sarkozy(ofour president, Mr. Nicolas Sarkozy).
The corre-sponding named entity is shown in figure 4.
Asdecided in the annotation guidelines, fillers can bepart of a named entity.
This can happen for com-plex named entities involving several words.
Therepresentation shown in figure 4 is the default rep-resentation and will be referred to as baseline.
Aproblem created by this representation is the factthat fillers are present also outside entities.
Fillersof named entities should be, in principle, distin-guished from any other filler, since they may beinformative to discriminate entities.Following this intuition, we designed two dif-ferent representations where entity fillers are con-177Figure 6: Parent-context tree representations used in the PCFGparsing modelFigure 7: Parent-node tree representations used in the PCFG pars-ing modeltextualized so that to be distinguished from theother fillers.
In the first representation we give tothe filler the same label of the parent node, whilein the second representation we use a concatena-tion of the filler and the label of the parent node.These two representations are shown in figure 5and 6, respectively.
The first one will be referredto as filler-parent, while the second will be re-ferred as parent-context.
A problem that may beintroduced by the first representation is that someentities that originally were used only for non-terminal entities will appear also as components,i.e.
entities annotated on words.
This may intro-duce some ambiguity.Another possible contextualization can be toannotate each node with the label of the parentnode.
This representation is shown in figure 7and will be referred to as parent-node.
Intuitively,this representation is effective since entities an-notated directly on words provide also the en-tity of the parent node.
However this representa-tion increases drastically the number of entities,in particular the number of components, whichin our case are the set of labels to be learned bythe CRF model.
For the same reason this repre-sentation produces more rigid models, since labelsequences vary widely and thus is not likely tomatch sequences not seen in the training data.Finally, another interesting tree representationis a variation of the parent-node tree, where en-tity fillers are only distinguished from fillers notin an entity, using the label ne-filler, but they arenot contextualized with entity information.
Thisrepresentation is shown in figure 8 and it will beFigure 8: Parent-node-filler tree representations used in the PCFGparsing modelreferred to as parent-node-filler.
This representa-tion is a good trade-off between contextual infor-mation and rigidity, by still representing entitiesas concatenation of labels, while using a commonspecial label for entity fillers.
This allows to keeplower the number of entities annotated on words,i.e.
components.Using different tree representations affects boththe structure and the performance of the parsingmodel.
The structure is described in the next sec-tion, the performance in the evaluation section.4.2 Structure of the ModelLexicalized models for syntactic parsing de-scribed in (Charniak, 2000; Charniak et al 1998)and (Collins, 1997), integrate more informationthan what is used in equations 4 and 5.
Consider-ing a particular node in the entity tree, not includ-ing terminals, the information used is:?
s: the head word of the node, i.e.
the mostimportant word of the chunk covered by thecurrent node?
h: the head word of the parent node?
t: the entity tag of the current node?
l: the entity tag of the parent nodeThe head word of the parent node is definedpercolating head words from children nodes toparent nodes, giving the priority to verbs.
Theycan be found using automatic approaches basedon words and entity tag co-occurrence or mutualinformation.
Using this information, the modeldescribed in (Charniak et al 1998) is P (s|h, t, l).This model being conditioned on several piecesof information, it can be affected by data sparsityproblems.
Thus, the model is actually approxi-mated as an interpolation of probabilities:P (s|h, t, l) =?1P (s|h, t, l) + ?2P (s|ch, t, l)+?3P (s|t, l) + ?4P (s|t) (6)178where ?i, i = 1, ..., 4, are parameters of themodel to be tuned, and ch is the cluster of headwords for a given entity tag t. With such model,when not all pieces of information are available toestimate reliably the probability with more con-ditioning, the model can still provide a proba-bility with terms conditioned with less informa-tion.
The use of head words and their percola-tion over the tree is called lexicalization.
Thegoal of tree lexicalization is to add lexical infor-mation all over the tree.
This way the probabil-ity of all rules can be conditioned also on lexi-cal information, allowing to define the probabili-ties P (s|h, t, l) and P (s|ch, t, l).
Tree lexicaliza-tion reflects the characteristics of syntactic pars-ing, for which the models described in (Charniak,2000; Charniak et al 1998) and (Collins, 1997)were defined.
Head words are very informativesince they constitute keywords instantiating la-bels, regardless if they are syntactic constituentsor named entities.
However, for named entityrecognition it doesn?t make sense to give prior-ity to verbs when percolating head words over thetree, even more because head words of named en-tities are most of the time nouns.
Moreover, itdoesn?t make sense to give priority to the headword of a particular entity with respect to the oth-ers, all entities in a sentence have the same im-portance.
Intuitively, lexicalization of entity treesis not straightforward as lexicalization of syntac-tic trees.
At the same time, using not lexicalizedtrees doesn?t make sense with models like 6, sinceall the terms involve lexical information.
Instead,we can use the model of (Johnson, 1998), whichdefine the probability of a tree ?
as:P (?)
=YX?
?P (X ?
?)C?
(X??)
(7)here the RHS of rules has been generalized with?, representing RHS of both unary and binaryrules 4 and 5.
C?
(X ?
?)
is the number of timesthe rule X ?
?
appears in the tree ?
.
The model7 is instantiated when using tree representationsshown in Fig.
4, 5 and 6.
When using representa-tions given in Fig.
7 and 8, the model is:P (?
|l) (8)where l is the entity label of the parent node.Although non-lexicalized models like 7 and 8have shown less effective for syntactic parsingthan their lexicalized couter-parts, there are evi-dences showing that they can be effective in ourtask.
With reference to figure 4, considering theentity pers.ind instantiated by Nicolas Sarkozy,our algorithm detects first name.first for Nicolasand name.last for Sarkozy using the CRF model.As mentioned earlier, once the CRF model has de-tected components, since entity trees have not acomplex structure with respect to syntactic trees,even a simple model like the one in equation 7or 8 is effective for entity tree parsing.
For ex-ample, once name.first and name.last have beendetected by CRF, pers.ind is the only entity hav-ing name.first and name.last as children.
Am-biguities, like for example for kind or qualifier,which can appear in many entities, can affect themodel 7, but they are overcome by the model 8,taking the entity tag of the parent node into ac-count.
Moreover, the use of CRF allows to in-clude in the model much more features than thelexicalized model in equation 6.
Using featureslike word prefixes (P), suffixes (S), capitalization(C), morpho-syntactic features (MS) and otherfeatures indicated as F2, the CRF model encodesthe conditional probability:P (t|w,P, S, C,MS, F ) (9)where w is an input word and t is the corre-sponding component.The probability of the CRF model, used in thefirst step to tag input words with components,is combined with the probability of the PCFGmodel, used to parse entity trees starting fromcomponents.
Thus the structure of our model is:P (t|w,P, S, C,MS, F ) ?
P (?)
(10)orP (t|w,P, S, C,MS, F ) ?
P (?
|l) (11)depending if we are using the tree representa-tion given in figure 4, 5 and 6 or in figure 7 and 8,respectively.
A scale factor could be used to com-bine the two scores, but this is optional as CRFscan provide normalized posterior probabilities.2The set of features used in the CRF model will be de-scribed in more details in the evaluation section.1795 Related WorkWhile the models used for named entity detectionand the set of named entities defined along theyears have been discussed in the introduction andin section 2, since CRFs and models for parsingconstitute the main issue in our work, we discusssome important models here.Beyond the models for parsing discussed insection 4, together with motivations for using ornot in our work, another important model for syn-tactic parsing has been proposed in (Ratnaparkhi,1999).
Such model is made of four MaximumEntropy models used in cascade for parsing atdifferent stages.
Also this model makes use ofhead words, like those described in section 4, thusthe same considerations hold, moreover it seemsquite complex for real applications, as it involvesthe use of four different models together.
Themodels described in (Johnson, 1998), (Charniak,1997; Caraballo and Charniak, 1997), (Charniaket al 1998), (Charniak, 2000), (Collins, 1997)and (Ratnaparkhi, 1999), constitute the main in-dividual models proposed for constituent-basedsyntactic parsing.
Later other approaches basedon models combination have been proposed, likee.g.
the reranking approach described in (Collinsand Koo, 2005), among many, and also evolutionsor improvements of these models.More recently, approaches based on log-linearmodels have been proposed (Clark and Curran,2007; Finkel et al 2008) for parsing, called also?Tree CRF?, using also different training criteria(Auli and Lopez, 2011).
Using such models in ourwork has basically two problems: one related toscaling issues, since our data present a large num-ber of labels, which makes CRF training problem-atic, even more when using ?Tree CRF?
; anotherproblem is related to the difference between syn-tactic parsing and named entity detection tasks,as mentioned in sub-section 4.2.
Adapting ?TreeCRF?
to our task is thus a quite complex work, itconstitutes an entire work by itself, we leave it asfeature work.Concerning linear-chain CRF models, theone we use is a state-of-the-art implementation(Lavergne et al 2010), as it implements themost effective optimization algorithms as well asstate-of-the-art regularizers (see sub-section 3.1).Some improvement of linear-chain CRF havebeen proposed, trying to integrate higher ordertarget-side features (Tang et al 2006).
An inte-gration of the same kind of features has been triedalso in the model used in this work, without giv-ing significant improvements, but making modeltraining much harder.
Thus, this direction has notbeen further investigated.6 EvaluationIn this section we describe experiments performedto evaluate our models.
We first describe the set-tings used for the two models involved in the en-tity tree parsing, and then describe and commentthe results obtained on the test corpus.6.1 SettingsThe CRF implementation used in this work is de-scribed in (Lavergne et al 2010), named wapiti.3We didn?t optimize parameters ?1 and ?2 of theelastic net (see section 3.1), although this im-proves significantly the performances and leadsto more compact models, default values lead inmost cases to very accurate models.
We used awide set of features in CRF models, in a windowof [?2,+2] around the target word:?
A set of standard features like word prefixesand suffixes of length from 1 to 6, plus someYes/No features like Does the word start withcapital letter?, etc.?
Morpho-syntactic features extracted fromthe output of the tool tagger (Allauzen andBonneau-Maynard, 2008)?
Features extracted from the output of the se-mantic analyzer (Rosset et al (2009)) pro-vided by the tool WMatch (Galibert, 2009).This analysis morpho-syntactic information aswell as semantic information at the same levelof named entities.
Using two different sets ofmorpho-syntactic features results in more effec-tive models, as they create a kind of agreementfor a given word in case of match.
Concerningthe PCFG model, grammars, tree binarization andthe different tree representations are created withour own scripts, while entity tree parsing is per-formed with the chart parsing algorithm describedin (Johnson, 1998).43available at http://wapiti.limsi.fr4available at http://web.science.mq.edu.au/?mjohnson/Software.htm180CRF PCFGModel # features # labels # rulesbaseline 3,041,797 55 29,611filler-parent 3,637,990 112 29,611parent-context 3,605,019 120 29,611parent-node 3,718,089 441 31,110parent-node-filler 3,723,964 378 31,110Table 3: Statistics showing the characteristics of the differentmodels used in this work6.2 Evaluation MetricsAll results are expressed in terms of Slot ErrorRate (SER) (Makhoul et al 1999) which has asimilar definition of word error rate for ASR sys-tems, with the difference that substitution errorsare split in three types: i) correct entity type withwrong segmentation; ii) wrong entity type withcorrect segmentation; iii) wrong entity type withwrong segmentation; here, i) and ii) are given halfpoints, while iii), as well as insertion and deletionerrors, are given full points.
Moreover, results aregiven using the well known F1 measure, definedas a function of precision and recall.6.3 ResultsIn this section we provide evaluations of the mod-els described in this work, based on combinationof CRF and PCFG and using different tree repre-sentations of named entity trees.6.3.1 Model StatisticsAs a first evaluation, we describe some statis-tics computed from the CRF and PCFG modelsusing the tree representations.
Such statistics pro-vide interesting clues of how difficult is learningthe task and which performance we can expectfrom the model.
Statistics for this evaluation arepresented in table 3.
Rows corresponds to the dif-ferent tree representations described in this work,while in the columns we show the number of fea-tures and labels for the CRF models (# featuresand # labels), and the number of rules for PCFGmodels (# rules).As we can see from the table, the numberof rules is the same for the tree representationsbaseline, filler-parent and parent-context, andfor the representations parent-node and parent-node-filler.
This is the consequence of the con-textualization applied by the latter representa-tions, i.e.
parent-node and parent-node-fillercreate several different labels depending fromthe context, thus the corresponding grammarDEV TESTModel SER F1 SER F1baseline 20.0% 73.4% 14.2% 79.4%filler-parent 16.2% 77.8% 12.5% 81.2%parent-context 15.2% 78.6% 11.9% 81.4%parent-node 6.6% 96.7% 5.9% 96.7%parent-node-filler 6.8% 95.9% 5.7% 96.8%Table 4: Results computed from oracle predictions obtained withthe different models presented in this workDEV TESTModel SER F1 SER F1baseline 33.5% 72.5% 33.4% 72.8%filler-parent 31.3% 74.4% 33.4% 72.7%parent-context 30.9% 74.6% 33.3% 72.8%parent-node 31.2% 77.8% 31.4% 79.5%parent-node-filler 28.7% 78.9% 30.2% 80.3%Table 5: Results obtained with our combined algorithm based onCRF and PCFGwill have more rules.
For example, the rulepers.ind?
name.first name.last canappear as it is or contextualized with func.ind,like in figure 8.
In contrast the other tree repre-sentations modify only fillers, thus the number ofrules is not affected.Concerning CRF models, as shown in table 3,the use of the different tree representations resultsin an increasing number of labels to be learned byCRF.
This aspect is quite critical in CRF learn-ing, as training time is exponential in the numberof labels.
Indeed, the most complex models, ob-tained with parent-node and parent-node-fillertree representations, took roughly 8 days for train-ing.
Additionally, increasing the number of labelscan create data sparseness problems, however thisproblem doesn?t seem to arise in our case since,apart the baseline model which has quite less fea-tures, all the others have approximately the samenumber of features, meaning that there are actu-ally enough data to learn the models, regardlessthe number of labels.6.3.2 Evaluations of Tree RepresentationsIn this section we evaluate the models in termsof the evaluation metrics described in previoussection, Slot Error Rate (SER) and F1 measure.In order to evaluate PCFG models alone, weperformed entity tree parsing using as input ref-erence transcriptions, i.e.
manual transcriptionsand reference component annotations taken fromdevelopment and test sets.
This can be consid-ered a kind of oracle evaluations and provides usan upper bound of the performance of the PCFGmodels.
Results for this evaluation are reported in181Participant SERP1 48.9P2 41.0parent-context 33.3parent-node 31.4parent-node-filler 30.2Table 6: Results obtained with our combined algorithm based onCRF and PCFGtable 4.
As it can be intuitively expected, addingmore contextualization in the trees results in moreaccurate models, the simplest model, baseline,has the worst oracle performance, filler-parentand parent-context models, adding similar con-textualization information, have very similar ora-cle performances.
Same line of reasoning appliesto models parent-node and parent-node-filler,which also add similar contextualization and havevery similar oracle predictions.
These last twomodels have also the best absolute oracle perfor-mances.
However, adding more contextualizationin the trees results also in more rigid models, thefact that models are robust on reference transcrip-tions and based on reference component annota-tions, doesn?t imply a proportional robustness oncomponent sequences generated by CRF models.This intuition is confirmed from results re-ported in table 5, where a real evaluation of ourmodels is reported, using this time CRF out-put components as input to PCFG models, toparse entity trees.
The results reported in ta-ble 5 show in particular that models using base-line, filler-parent and parent-context tree repre-sentations have similar performances, especiallyon test set.
Models characterized by parent-nodeand parent-node-filler tree representations haveindeed the best performances, although the gainwith respect to the other models is not as muchas it could be expected given the difference inthe oracle performances discussed above.
In par-ticular the best absolute performance is obtainedwith the model parent-node-filler.
As we men-tioned in subsection 4.1, this model represents thebest trade-off between rigidity and accuracy usingthe same label for all entity fillers, but still distin-guishing between fillers found in entity structuresand other fillers found in words not instantiatingany entity.6.3.3 Comparison with Official ResultsAs a final evaluation of our models, we pro-vide a comparison of official results obtained atthe 2011 evaluation campaign of extended namedentity recognition (Galibert et al 2011; 2) Re-sults are reported in table 6, where the other twoparticipants to the campaign are indicated as P1and P2.
These two participants P1 and P2, useda system based on CRF, and rules for deep syn-tactic analysis, respectively.
In particular, P2 ob-tained superior performances in previous evalua-tion campaign on named entity recognition.
Thesystem we proposed at the evaluation campaignused a parent-context tree representation.
Theresults obtained at the evaluation campaign arein the first three lines of Table 6.
We comparesuch results with those obtained with the parent-node and parent-node-filler tree representations,reported in the last two rows of the same table.
Aswe can see, the new tree representations describedin this work allow to achieve the best absolute per-formances.7 ConclusionsIn this paper we have presented a Named EntityRecognition system dealing with extended namedentities with a tree structure.
Given such represen-tation of named entities, the task cannot be mod-eled as a sequence labelling approach.
We thusproposed a two-steps system based on CRF andPCFG.
CRF annotate entity components directlyon words, while PCFG apply parsing techniquesto predict the whole entity tree.
We motivatedour choice by showing that it is not effective toapply techniques used widely for syntactic pars-ing, like for example tree lexicalization.
We pre-sented an analysis of different tree representationsfor PCFG, which affect significantly parsing per-formances.We provided and discussed a detailed evalua-tion of all the models obtained by combining CRFand PCFG with the different tree representationproposed.
Our combined models result in betterperformances with respect to other models pro-posed at the official evaluation campaign, as wellas our previous model used also at the evaluationcampaign.AcknowledgmentsThis work has been funded by the project Quaero,under the program Oseo, French State agency forinnovation.182ReferencesRalph Grishman and Beth Sundheim.
1996.
Mes-sage Understanding Conference-6: a brief history.In Proceedings of the 16th conference on Com-putational linguistics - Volume 1, pages 466?471,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Satoshi Sekine and Chikashi Nobata.
2004.
Defini-tion, Dictionaries and Tagger for Extended NamedEntity Hierarchy.
In Proceedings of LREC.G.
Doddington, A. Mitchell, M. Przybocki,L.
Ramshaw, S. Strassel, and R. Weischedel.2004.
The Automatic Content Extraction (ACE)Program?Tasks, Data, and Evaluation.
Proceedingsof LREC 2004, pages 837?840.Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,Karn Fort, Olivier Galibert, Ludovic Quintard.2011.
Proposal for an extension or traditionalnamed entities: From guidelines to evaluation, anoverview.
In Proceedings of the Linguistic Annota-tion Workshop (LAW).J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Pro-ceedings of the Eighteenth International Confer-ence on Machine Learning (ICML), pages 282?289,Williamstown, MA, USA, June.Mark Johnson.
1998.
Pcfg models of linguistictree representations.
Computational Linguistics,24:613?632.Stefan Hahn, Marco Dinarelli, Christian Raymond,Fabrice Lefe`vre, Patrick Lehen, Renato De Mori,Alessandro Moschitti, Hermann Ney, and GiuseppeRiccardi.
2010.
Comparing stochastic approachesto spoken language understanding in multiple lan-guages.
IEEE Transactions on Audio, Speech andLanguage Processing (TASLP), 99.Adam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
COMPU-TATIONAL LINGUISTICS, 22:39?71.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings the 48th Annual Meeting of the Association forComputational Linguistics (ACL), pages 504?513.Association for Computational Linguistics, July.Stefan Riezler and Alexander Vasserman.
2004.
In-cremental feature selection and l1 regularizationfor relaxed maximum-entropy modeling.
In Pro-ceedings of the International Conference on Em-pirical Methods for Natural Language Processing(EMNLP).Hui Zou and Trevor Hastie.
2005.
Regularization andvariable selection via the Elastic Net.
Journal of theRoyal Statistical Society B, 67:301?320.Eugene Charniak.
1997.
Statistical parsing witha context-free grammar and word statistics.
InProceedings of the fourteenth national conferenceon artificial intelligence and ninth conference onInnovative applications of artificial intelligence,AAAI?97/IAAI?97, pages 598?603.
AAAI Press.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st NorthAmerican chapter of the Association for Computa-tional Linguistics conference, pages 132?139, SanFrancisco, CA, USA.
Morgan Kaufmann Publish-ers Inc.Sharon A. Caraballo and Eugene Charniak.
1997.New figures of merit for best-first probabilistic chartparsing.
Computational Linguistics, 24:275?298.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Com-putational Linguistics and Eighth Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, ACL ?98, pages 16?23, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Eugene Charniak, Sharon Goldwater, and Mark John-son.
1998.
Edge-based best-first chart parsing.
InIn Proceedings of the Sixth Workshop on Very LargeCorpora, pages 127?133.
Morgan Kaufmann.Alexandre Allauzen and He?le?ne Bonneau-Maynard.2008.
Training and evaluation of pos taggers on thefrench multitag corpus.
In Proceedings of the SixthInternational Language Resources and Evaluation(LREC?08), Marrakech, Morocco, may.Olivier Galibert.
2009.
Approches et me?thodologiespour la re?ponse automatique a` des questionsadapte?es a` un cadre interactif en domaine ouvert.Ph.D.
thesis, Universite?
Paris Sud, Orsay.Rosset Sophie, Galibert Olivier, Bernard Guillaume,Bilinski Eric, and Adda Gilles.
The LIMSI mul-tilingual, multitask QAst system.
In Proceed-ings of the 9th Cross-language evaluation forumconference on Evaluating systems for multilin-gual and multimodal information access, CLEF?08,pages 480?487, Berlin, Heidelberg, 2009.
Springer-Verlag.Azeddine Zidouni, Sophie Rosset, and Herve?
Glotin.2010.
Efficient combined approach for named en-tity recognition in spoken language.
In Proceedingsof the International Conference of the Speech Com-munication Assosiation (Interspeech), Makuhari,JapanJohn Makhoul, Francis Kubala, Richard Schwartz,and Ralph Weischedel.
1999.
Performance mea-sures for information extraction.
In Proceedings ofDARPA Broadcast News Workshop, pages 249?252.Adwait Ratnaparkhi.
1999.
Learning to Parse NaturalLanguage with Maximum Entropy Models.
Journalof Machine Learning, vol.
34, issue 1-3, pages 151?175.183Michael Collins and Terry Koo.
2005.
DiscriminativeRe-ranking for Natural Language Parsing.
Journalof Machine Learning, vol.
31, issue 1, pages 25?70.Clark, Stephen and Curran, James R. 2007.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
Journal of Computational Lin-guistics, vol.
33, issue 4, pages 493?552.Finkel, Jenny R. and Kleeman, Alex and Manning,Christopher D. 2008.
Efficient, Feature-based,Conditional Random Field Parsing.
Proceedingsof the Association for Computational Linguistics,pages 959?967, Columbus, Ohio.Michael Auli and Adam Lopez 2011.
Training a Log-Linear Parser with Loss Functions via Softmax-Margin.
Proceedings of Empirical Methods forNatural Language Processing, pages 333?343, Ed-inburgh, U.K.Tang, Jie and Hong, MingCai and Li, Juan-Zi andLiang, Bangyong.
2006.
Tree-Structured Con-ditional Random Fields for Semantic Annotation.Proceedgins of the International Semantic WebConference, pages 640?653, Edited by Springer.Olivier Galibert; Sophie Rosset; Cyril Grouin; PierreZweigenbaum; Ludovic Quintard.
2011.
Struc-tured and Extended Named Entity Evaluation in Au-tomatic Speech Transcriptions.
IJCNLP 2011.Marco Dinarelli, Sophie Rosset.
Models Cascade forTree-Structured Named Entity Detection IJCNLP2011.184
