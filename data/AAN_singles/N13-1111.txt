Proceedings of NAACL-HLT 2013, pages 907?917,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsGlobal Inference for Bridging Anaphora ResolutionYufang Hou1, Katja Markert2, Michael Strube11 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany(yufang.hou|michael.strube)@h-its.org2School of Computing, University of Leeds, UKscskm@leeds.ac.ukAbstractWe present the first work on antecedent se-lection for bridging resolution without restric-tions on anaphor or relation types.
Our modelintegrates global constraints on top of a richlocal feature set in the framework of Markovlogic networks.
The global model improvesover the local one and both strongly outper-form a reimplementation of prior work.1 IntroductionIdentity coreference is a relatively well understoodand well-studied instance of entity coherence.
How-ever, entity coherence can rely on more complex,lexico-semantic, frame or encyclopedic relationsthan identity.
Anaphora linking distinct entities orevents this way are called bridging or associativeanaphora and have been widely discussed in the lin-guistic literature (Clark, 1975; Prince, 1981; Gundelet al 1993).1 In Example 1, the phrases the win-dows, the carpets and walls can be felicitously usedbecause they are semantically related via a part-ofrelation to their antecedent the Polish center.2(1) .
.
.
as much as possible of the Polish center willbe made from aluminum, steel and glass recycledfrom Warsaw?s abundant rubble.
.
.
.
The windowswill open.
The carpets won?t be glued down andwalls will be coated with non-toxic finishes.1Poesio and Vieira (1998) include cases where antecedentand anaphor are coreferent but do not share the same head noun.We restrict bridging to non-coreferential cases.
We also excludecomparative anaphora (Modjeska et al 2003)2Examples are from OntoNotes (Weischedel et al 2011).Bridging anaphora are typed in boldface; antecedents in italics.Bridging is frequent amounting to between 5%(Gardent and Manue?lian, 2005) and 20% (Caselliand Prodanof, 2006) of definite descriptions (bothstudies limited to NPs starting with the or non-English equivalents).
Bridging resolution is neededto fill gaps in entity grids based on coreference only(Barzilay and Lapata, 2008).
Example 1 does not ex-hibit any coreferential entity coherence.
Coherencecan only be established when the bridging anaphoraare resolved.
Bridging resolution may also be im-portant for textual entailment (Mirkin et al 2010).Bridging resolution can be divided into two tasks,recognizing that a bridging anaphor is present andfinding the correct antecedent among a list of candi-dates.
These two tasks have frequently been handledin a pipeline with most research concentrating on an-tecedent selection only.
We also handle only the taskof antecedent selection.Previous work on antecedent selection for bridg-ing anaphora is restricted.
It makes strong untestedassumptions about bridging anaphora types or rela-tions, limiting it to definite NPs (Poesio and Vieira,1998; Poesio et al 2004; Lassalle and Denis, 2011)or to part-of relations between anaphor and an-tecedent (Poesio et al 2004; Markert et al 2003;Lassalle and Denis, 2011).
We break new groundby considering all relations and anaphora/antecedenttypes and show that the variety of bridging anaphorais much higher than reported previously.Following work on coreference resolution, we ap-ply a local pairwise model (Soon et al 2001) for an-tecedent selection.
We then develop novel semantic,syntactic and salience features for this task, show-ing strong improvements over one of the best known907prior models (Poesio et al 2004).However, this local model classifies eachanaphor-antecedent candidate pair in isolation.Thus, it neglects that bridging anaphora referring toa single antecedent often occur in clusters (see Ex-ample 1).
It also neglects that once an entity is anantecedent for a bridging anaphor it is more likely tobe used again as antecedent.
In addition, such localmodels construct the list of possible antecedent can-didates normally relying on a window size constraintto restrict the set of candidates: is the window toosmall, we miss too many correct antecedents; is ittoo large, we include so many incorrect antecedentsas to lead to severe data imbalance in learning.To remedy these flaws we change to a globalMarkov logic model that allows us to:?
model constraints that certain anaphora arelikely to share the same antecedent;?
model the global semantic connectivity of asalient potential antecedent to all anaphora in atext;?
consider the union of potential antecedents forall anaphora instead of a static window-sizedconstraint.We show that this global model with the same lo-cal features but enhanced with global constraints im-proves significantly over the local model.2 Related WorkPrior corpus-linguistic studies on bridging are be-set by three main problems.
First, reliability is notmeasured or low (Fraurud, 1990; Poesio, 2003; Gar-dent and Manue?lian, 2005; Riester et al 2010).3Second, annotated corpora are small (Poesio et al2004; Korzen and Buch-Kromann, 2011).
Third,they are often based on strong untested assumptionsabout bridging anaphora types, antecedent types orrelations, such as limiting it to definite NP anaphora(Poesio and Vieira, 1998; Poesio et al 2004; Gar-dent and Manue?lian, 2005; Caselli and Prodanof,2006; Riester et al 2010; Lassalle and Denis,2011), to NP antecedents (all prior work) or to part-3Although the overall information status scheme in Riesteret al(2010) achieved high agreement, their confusion matrixshows that the anaphoric bridging category (BRI) is frequentlyconfused with other categories so that the two annotators agreedon only less than a third of bridging anaphors.of relations between anaphor and antecedent (Mark-ert et al 2003; Poesio et al 2004).
In our ownwork (Markert et al 2012) we established a corpusthat circumvents these problems, i.e.
human bridg-ing recognition was reliable, it contains a mediumnumber of bridging cases that allows generalisablestatistics and we did not limit bridging anaphora orantecedents according to their syntactic type or re-lations between them.
However, we only discussedhuman agreement on bridging recognition in Mark-ert et al(2012), disregarding antecedent annotation.We also did not discuss the different types of bridg-ing in the corpus.
We will remedy this in Section 3.Automatic work on bridging distinguishes be-tween recognition (Vieira and Poesio, 2000; Rah-man and Ng, 2012; Cahill and Riester, 2012; Mark-ert et al 2012) and antecedent selection.
Work onantecedent selection suffers from focusing on sub-problems, e.g.
only part-of bridging (Poesio et al2004; Markert et al 2003) or definite NP anaphora(Lassalle and Denis, 2011).
Most relevant for us isLassalle and Denis (2011) who restrict anaphora todefinite descriptions but have no other restrictionson relations or antecedent NPs (in a French corpus)with an accuracy of 23%.
Also the evaluation set-up is sometimes not clear: The high results in Poe-sio et al(2004) cannot be used for comparison asthey test unrealistically: they distinguish only be-tween the correct antecedent and one or three falsecandidates (baseline of 50% for the former).
Theyalso restrict the phenomenon to part-of relations.There is a partial overlap between bridging andimplicit noun roles (Ruppenhofer et al 2010).However, work on implicit noun roles is mostlyfocused on few predicates (e.g.
Gerber and Chai(2012)).
We consider all bridging anaphors in run-ning text.
The closest work to ours interpreting im-plicit role filling as anaphora resolution is Silbererand Frank (2012).3 Corpus for Bridging: An OverviewWe use the dataset we created in Markert et al(2012) with almost 11,000 NPs annotated for infor-mation status including 663 bridging NPs and theirantecedents in 50 texts taken from the WSJ portionof the OntoNotes corpus (Weischedel et al 2011).Bridging anaphora can be any noun phrase.
They908are not limited to definite NPs as in previous work.In contrast to Nissim et al(2004), antecedents areannotated and can be noun phrases, verb phrases oreven clauses.
Our bridging annotation is also notlimited with regards to semantic relations betweenanaphor and antecedent.In Markert et al(2012) we achieved high agree-ment for the overall information status annotationscheme between three annotators (?
between 75 and80, dependent on annotator pairs) as well as for allsubcategories, including bridging (?
over 60 for allannotator pairings, over 70 for two expert annota-tors).
Here, we add the following new results:?
Agreement for selecting bridging antecedentswas around 80% for all annotator pairings.?
Surprisingly, only 255 of the 663 (38%) bridg-ing anaphors are definite NPs, which calls intoquestion the strategy of prior approaches to limitthemselves to these types of bridging.?
NPs are the most frequent antecedents by farwith only 42 of 663 (6%) bridging anaphora hav-ing a non-NP antecedent (mostly verb phrases).?
Bridging is a relatively local phenomenon with71% of NP antecedents occurring in the same orup to 2 sentences prior to the anaphor.
However,farther away antecedents are common when theantecedent is the global focus of a document.?
The semantic relations between anaphor and an-tecedent are extremely diverse with only 92 of663 (14%) anaphors having a part-of/attribute-of antecedent (see Example 1) and only 45 (7%)anaphors standing in a set relationship to the an-tecedent (see Example 2).
This contrasts withGardent and Manue?lian?s (2005) finding that52% of bridging cases had meronymic relations.We find many different types of relations in ourcorpus, including encyclopedic relations such asrestaurant ?
the waiter as well as, frequently,relational person nouns as bridging anaphorssuch as friend, husband, president.?
There are only a few cases of bridging wheresurface cues may indicate the antecedent.
First,some bridging anaphors are modified by a smallnumber of adjectives that have more than onerole filler, with the bridging relation often beingtemporal or spatial sequence between two enti-ties of the same semantic type as in Example 3(see also Lassalle and Denis (2011) for a dis-cussion of such cases).
Second, some anaphorsare compounds where the nominal premodifiermatches the antecedent head as in Example 4.
(2) Still employees do occasionally try to smuggleout a gem or two.
One man wrapped several dia-monds in the knot of his tie.
Another poked a holein the heel of his shoe.
None made it past the bodysearches .
.
.
(3) His truck is parked across the field .
.
.
Thefarmer at the next truck shouts .
.
.
(4) .
.
.
it doesn?t make the equipment needed toproduce those chips.
And IBM worries that theJapanese will take over that equipment market.4 Models for Bridging Resolution4.1 Pairwise mention-entity modelThe pairwise model is widely used in coreferenceresolution (Soon et al 2001).
We adapt it for bridg-ing resolution4: Given an anaphor mention m andthe set of antecedent candidate entities Em whichappear before m, we create a pairwise instance(m, e) for every e ?
Em.
A binary decision whetherm is bridged to e is made for each instance (m, e)separately.
A post-processing step to choose one an-tecedent is necessary (closest first or best first arecommon strategies).
This model causes three prob-lems for bridging resolution: First, the ratio betweenpositive and negative instances is 1 to 17 even if onlyantecedent candidates from the current and the im-mediately preceding two sentences are considered.The ratio will be even worse with a larger win-dow size.
Therefore, usually a fixed window size isused restricting the set of candidates.
This, however,causes a second problem: antecedents which are be-yond the window cannot be found.
In our data, only81% of NP antecedents appear within the previous 5sentences, and only 71% of NP antecedents appearwithin the previous 2 sentences.
The third problemis a shortcoming of the pairwise model itself: deci-sions are made for each instance separately, ignoring4Different from coreference, we treat an anaphor as a men-tion and an antecedent as an entity.
The anaphor is the firstmention of the corresponding entity in the document.909relations between instances.
We resolve these prob-lems by employing a global model based on Markovlogic networks.4.2 Markov Logic NetworksBridging can be considered a document global phe-nomenon, where globally salient entities are pre-ferred as antecedents and two or more anaphors hav-ing the same antecedent should be related or similar.Motivated by this observation, we explore Markovlogic networks (Domingos and Lowd, 2009, MLNs)to model bridging resolution on the global discourselevel.MLNs are a powerful representation for jointinference with uncertainty.
An MLN consistsof a set of pairs (Fi, wi), where Fi is a formulain first-order logic and wi is its associated realnumbered weight.
It can be viewed as a template forconstructing Markov networks.
Given different setsof constants, an MLN will produce different groundMarkov networks which may vary in size but havethe same structure and parameters.
For a groundMarkov network, the probability distribution overpossible worlds x is given byP (X = x) = 1Z exp(?iwini(x))(1)where ni(x) is the number of true groundings of Fiin x.
The normalization factor Z is the partitionfunction.MLNs have been applied to many NLP tasks andachieved good performance by leveraging rich re-lations among objects (Poon and Domingos, 2008;Meza-Ruiz and Riedel, 2009; Fahrni and Strube,2012, inter alia).
We use thebeast5 to learn weightsfor the formulas and to perform inference.
thebeastemploys cutting plane inference (Riedel, 2008) toimprove the accuracy and efficiency of MAP infer-ence for Markov logic.With MLNs, we model bridging resolution glob-ally on the discourse level: given the set M of allanaphors and sets of local antecedent candidates Emfor each anaphor m ?
M , we select antecedents forall anaphors from E =?m?M Em at the same time.Table 1 shows the hidden predicates and formulasused.
Each formula is associated with a weight.
The5http://code.google.com/p/thebeastpolarity of the weights is indicated by the leading+ or ?.
The weight value (except for hard con-straints) is learned from training data.
For some for-mulas the final weight consists of a learned weightw multiplied by a score d (e.g.
inverse distance be-tween antecedent and anaphor).
In these cases thefinal weight for a formula in a ground Markov net-work does not just depend on the respective formula,but also on the specific constants.
We indicate suchcombined weights by the term w ?
d.We tackle the previously mentioned problems ofthe pairwise model: (1) We construct hard con-straints to specify that each anaphor has at mostone antecedent entity (Table 1: f1) and that the an-tecedent must precede the anaphor (f2).
This elim-inates the need for the post-processing step in thepairwise model.
(2) We select the antecedent en-tity for each anaphor from the antecedent candidateentities pool E which alleviates the missing trueantecedent problem in the pairwise model.
Basedon (1) and (2), MLNs allow us to express relationsbetween anaphor-anaphor and anaphor-antecedentpairs ((m,n) or (m,e)) on the global discourse levelimproving accuracy by performing joint inference.5 Features5.1 Local features5.1.1 Poesio et als feature setTable 2 shows the feature set proposed by Poesio etal.
(2004) for part-of bridging.
Google distance isthe inverse value of Google hit counts for the ofPat-tern query (e.g.
the windows of the center).
Word-Net distance is the inverse value of the shortest pathlength between an anaphor and an antecedent candi-date among all synset combinations.
These featuresare supposed to capture the meronymy relation be-tween anaphor and antecedent.
The other ones mea-sure the salience of the antecedent candidate.Group Feature Valuelexical Google distance numericWordNet distance numericsalience utterance distance numericlocal first mention booleanglobal first mention booleanTable 2: Poesio et als feature set910Hidden predicatesp1 isBridging(m, e)p2 hasSameAntecedent (m,n)FormulasHard constraintsf1 ?m ?
M : |e ?
E : isBridging(m, e)| ?
1f2 ?m ?
M?e ?
E : hasPairDistance(e,m, d) ?
d < 0 ?
?isBridging(m, e)f3 ?m,n ?
M : m 6= n ?
hasSameAntecedent (m,n)?
hasSameAntecedent (n,m)f4 ?m,n, l ?
M : m 6= n ?m 6= l ?
n 6= l ?
hasSameAntecedent (m,n)?
hasSameAntecedent (n, l) ?
hasSameAntecedent (m, l)f5 ?m,n ?
M?e ?
E : m 6= n ?
hasSameAntecedent (m,n) ?
isBridging(m, e)?
isBridging(n, e)f6 ?m,n ?
M?e ?
E : m 6= n ?
isBridging(m, e) ?
isBridging(n, e)?
hasSameAntecedent (m,n)Discourse level formulasf7 + (w) ?m ?
M?e ?
E : predictedGlobalAnte(e) ?
hasPairDistance(e,m, d)?
d > 0 ?
isBridging(m, e)f8 + (w) ?m,n ?
M conjunction(m,n) ?
hasSameAntecedent (m,n)f9 + (w) ?m,n ?
M sameHead(m,n) ?
hasSameAntecedent (m,n)f10 + (w) ?m,n ?
M similarTo(m,n) ?
hasSameAntecedent (m,n)f11 + (w) ?m ?
M?e ?
E : hasSemanticClass (m, ?rolePerson?)?
hasSemanticClass(e, ?org|gpe?)
?
hasPairDistance(e,m, d) ?
d > 0?
isBridging(m, e)f12 + (w ?
d) ?m ?
M?e ?
E : hasSemanticClass (m, ?relativePerson?)?
hasSemanticClass(e, ?otherPerson?)
?
hasPairDistanceInverse(e,m, d)?
isBridging(m, e)f13 + (w ?
d) ?m ?
M?e ?
E : hasSemanticClass (m, ?date?)?
hasSemanticClass(e, ?date?)
?
hasPairDistanceInverse(e,m, d)?
isBridging(m, e)Local formulasf14 + (w) ?m ?
M ?e ?
Em : isTopRelativeRankPrepPattern (m, e) ?
isBridging(m, e)f15 + (w) ?m ?
M ?e ?
Em : isTopRelativeRankVerbPattern(m, e) ?
isBridging(m, e)f16 + (w ?
d) ?m ?
M ?e ?
Em : isPartOf (m, e) ?
hasPairDistanceInverse(e,m, d)?
isBridging(m, e)f17 + (w) ?m ?
M ?e ?
Em : isTopRelativeRankDocSpan (m, e) ?
isBridging(m, e)f18 ?
(w) ?m ?
M ?e ?
Em : isSameHead(m, e) ?
isBridging(m, e)f19 + (w) ?m ?
M ?e ?
Em : isPremodOverlap(m, e) ?
isBridging(m, e)f20 ?
(w) ?m ?
M ?e ?
Em : isCoArgument(m, e) ?
isBridging(m, e)Table 1: Hidden predicates and formulas used for bridging resolution (m,n, l represent mentions, M the set of bridginganaphora mentions in the whole document, e the antecedent candidate entity, Em the set of local antecedent candidateentities for m, and E =?m?M Em )9115.1.2 Other featuresSince Poesio et al(2004) deal exclusively withmeronymy bridging, we have to extend the fea-ture set to capture more diverse relations betweenanaphor and antecedent.
All numeric features in Ta-ble 3 are normalized among all antecedent candi-dates of one anaphor.
For anaphor mi and its an-tecedent candidates Emi (eij ?
Emi), the numericscore for pair {mi, eik} is Sik.
Then the valueNormSik for this pair is normalized (set to valuesbetween 0 and 1) as below:NormSik =Sik ?minj Sijmaxj Sij ?minj Sij(2)A second variant of numeric features tells whetherthe score of an anaphor-antecedent candidate pair isthe highest among all pairs for this anaphor.Group Feature Valuesemantic feat1 preposition pattern numericfeat2 verb pattern numericfeat3 WordNet partOf booleanfeat4 semantic class nominalsalience feat5 document span numericsurface feat6 isSameHead booleanfeat7 isPremodOverlap booleansyntactic feat8 isCoArgument booleanTable 3: Local features we developedPreposition pattern (feat1).
The ofPattern pro-posed by Poesio et al(2004) is useful for part-ofand attribute-of relations but cannot cover all bridg-ing relations (such as sanctions against a country).We extend the ofPattern to a generalised prepositionpattern by using the Gigaword (Parker et al 2011)and the Tipster (Harman and Liberman, 1993) cor-pora (both automatically POS tagged and NP chun-ked for improving query match precision).First, we extract the three most highly associ-ated prepositions for each anaphor.
Then for eachanaphor-antecedent candidate pair, we use their headwords to create the query ?anaphor preposition an-tecedent?.
To improve recall, we take lowercase,uppercase, singular and plural forms of the headword into account, and replace proper names byfine-grained named entity types (using a gazetteer).All raw hit counts are converted into the DunningRoot Loglikelihood association measure,6 then nor-malized using Formula 2 within all antecedent can-didates of one anaphor.Verb pattern (feat2).
A set-membership rela-tion between anaphor and antecedent is often hardto capture by the preposition pattern because theanaphor often has no common noun head (see Ex-ample 2 in Section 3).
Hence, we measure the com-patibility of the antecedent candidates with the verbthe anaphor depends on.First, we hypothesise that anaphors whose lexi-cal head is a pronoun or a number are potential setbridging cases and then extract the verb the anaphordepends on.
In example 2, for the set anaphor An-other, poked is the verb.
Then for each antecedentcandidate, subject-verb or verb-object queries areapplied to the Web 1T 5-gram corpus (Brants andFranz, 2006).
In this case, employees poked and di-amonds poked are example queries.
The hit countsare transformed into PMI and all pairs for oneanaphor are normalized as described in Formula 2.WordNet partOf relation (feat3).
To capturepart-of bridging, we extract whether the anaphor ispart of the antecedent candidate in WordNet.
To im-prove recall, we use hyponym information of theantecedent.
If an antecedent e is a hypernym of xand an anaphor m is a meronym of x, then m is ameronym of e.Semantic class (feat4).
The anaphor and the an-tecedent candidate are assigned one of 16 coarse-grained semantic classes, e.g.
location, organiza-tion, GPE, roleperson, relativePerson, otherPerson7,product, language, NORP (nationalities, religiousor political groups) and several classes for numbers(such as date, money or percent).Salience feature (feat5).
Salient entities are pre-ferred as antecedents.
We capture salience super-ficially by computing the ?antecedent documentspan?
of an antecedent candidate.
We compute the6http://tdunning.blogspot.de/2008/03/surprise-and-coincidence.html7We use WordNet to extract lists for rolePerson (persons likepresident or teacher playing a role in an organization) and rela-tivePerson (persons like father or son indicating that they havea relation with another person).
Persons not in these two listsare counted as otherPerson.912span of text (measured in sentences) in which theantecedent candidate entity is mentioned.
This is di-vided by the number of sentences in the whole doc-ument.
This score is normalized using Formula 2 forall antecedent candidates of one anaphor.Surface features (feat6-feat7).
isSameHead(feat6) checks whether antecedent candidates havethe same head as the anaphor: this is rarely thecase in bridging anaphora (except in some casesof set bridging and spatial/temporal sequence, seeExample 3) and can therefore be used to excludeantecedent candidates.
isPremodOverlap (feat7)determines the antecedent for compound nounanaphors whose head is prenominally modified bythe antecedent head (see Example 4).Syntactic feature (feat8) The isCoArgument fea-ture is based on the intuition that the subject can-not be the bridging antecedent of the object inthe same clause.
This feature excludes (some)close antecedent candidates.
In Example 4, the an-tecedent candidate the Japanese isCoArgument withthe anaphor that equipment market.5.2 Global features for MLNsf1-f13 in Table 1 are discourse level constraints.All antecedent candidates come from the antecedentcandidates pool E in the whole document.Global salience (Table 1: f3-f10).
The saliencefeature in the pairwise model only measures thesalience for candidates within the local window.However, globally salient antecedents are preferredeven if they are far away from the anaphor.
Wemodel this from two perspectives:f7 models the preference for globally salient an-tecedents, which we derive for each document.
Form ?
M and e ?
E, let score(m, e) be the prepo-sition pattern score for pair (m,e).
Calculate patternsemantic salience score esal for each e ?
E asesal =?m?Mscore(m, e) (3)If e appears in the title and also has the highestpattern semantic salience score esal among all e inE, then e is the predicted globally salient antecedentfor this document.
Note that global salience here isbased on semantic connectivity to all anaphors in thedocument and that not every document has a glob-ally salient antecedent.f3-f6 and f8-f10 model that similar or relatedanaphors in one document are likely to have thesame antecedent.
To make the ground Markov net-work more sparse for more efficient inference, weadd the hidden predicate (p2) and hard constraints(f3-f6) specifying relations among similar/relatedanaphors m, n and l (reflexivity and transitivity).Formulas f8-f10 explore three different ways (syn-tactic and semantic) to compute the similarity be-tween two anaphors.
In f10, we use SVMlight (simi-larity scores from WordNet plus sentence distance asfeatures) to predict whether two anaphors not shar-ing the same head are similar or not.Frequent bridging relations (Table 1: f11-f13).Three common bridging relations are restricted bysemantic class of anaphor and antecedent (see alsoSection 3).
It is worth noting that in formula f11(modeling that a role person mention like presi-dent or chairman prefers organization or GPE an-tecedents), we do not penalize the antecedents faraway from the anaphor.
In formula f12 (modelingthat a relativePerson mention such as mother or hus-band prefers close person antecedents) and f13, weprefer close antecedents by including the distancebetween antecedent and anaphor into the weights.MLN formulation of local features (Table 1: f14-f20).
Corresponding to features of the pairwisemodel (Table 3) ?
we exclude only semantic classas this is modelled globally via features f11-f13.These local features are only used for an anaphor mand its local antecedent candidate e from Em.6 Experiments and Results6.1 Experimental setupWe perform experiments on our gold standard cor-pus via 10-fold cross-validation on documents.
Weuse gold standard mentions, true coreference infor-mation, and the OntoNotes named entity and syntac-tic annotation layers for feature extraction.6.2 Improved baselineWe reimplement the algorithm from Poesio et al(2004) as baseline.
Since they did not explain913whether they used the mention-mention or mention-entity model, we assume they treated antecedents asentities and use a 2 and 5 sentence window for can-didates8.
Since the GoogleAPI is not available anymore, we use the Web 1T 5-gram corpus (Brants andFranz, 2006) to extract the Google distance feature.We improve it by taking all information about en-tities via coreference into account as well as by re-placing proper names.
All other features (Table 2in Section 5.1.1) are extracted as Poesio et aldid.A Naive Bayes classifier with standard settings inWEKA (Witten and Frank, 2005) is used.
In orderto evaluate their model in the more realistic settingof our experiment, we apply the best first strategy toselect the antecedent for each anaphor.6.3 Pairwise modelsPairwise model I: We use the preposition patternfeature (feat1) plus Poesio et als salience features(Table 2).
We use a 2 sentence window as it per-formed on a par with the 5 sentence window in thebaseline.
We replace Naive Bayes with SVMlightbecause it can deal better with imbalanced data9.Pairwise model II: Based on Pairwise model I.Local features feat2-feat8 from Table 2 are added.Pairwise model III: Based on Pairwise model II.We apply a more advanced antecedent candidate se-lection strategy, which allows to include 77% of NPantecedents compared to 71% in Pairwise model II.For each anaphor, we add the top k salient enti-ties measured through the length of the coreferencechains (k is set to 10%) as additional antecedent can-didates.
For potential set anaphors (as automaticallydetermined by pronoun or number heads), singu-lar antecedent candidates are filtered out.
We com-piled a small set of adjectives (using FrameNet andthesauri) that indicate spatial or temporal sequences(see Example 3).
For anaphors modified by such ad-jectives we consider only antecedent candidates thathave the same semantic class as the anaphor.8They use a 5 sentence window, because all antecedents intheir corpus are within the previous 5 sentences.9The SVMlight parameter is set according to the ratio be-tween positive and negative instances in the training set.6.4 MLN modelsMLN model I: MLN system using local formu-las f1-f2 and f14-f20.
The same strategy as inPairwise model III is used to select local antecedentcandidates Em for each anaphor m.MLN model II: Based on MLN model I, all for-mulas in Table 1 are used.6.5 ResultsTable 4 shows the comparison of our models to base-lines.
Significance tests are conducted using McNe-mar?s test on overall accuracy at the level of 1%.accimproved baseline 2 sent.
+ NB 18.855 sent.
+ NB 18.40pairwise model pairwise model I 29.11pairwise model II 33.94pairwise model III 36.35MLN model MLN model I 35.60MLN model II 41.32Table 4: Results for MLN models compared to pairwisemodels and baselines.MLN model II, which is inspired by the linguis-tic observation that globally salient entities are pre-ferred as antecedents, performs significantly betterthan all other systems.
The gains come from threeaspects.
First, by selecting the antecedent for eachanaphor from the antecedent candidate pool E in thewhole document 91% of NP antecedents are acces-sible compared to 77% in pairwise model III.
Sec-ond, we leverage semantics and salience by usinglocal formulas and discourse level formulas.
Lo-cal formulas are used to capture semantic relationsfor bridging pairs as well as surface and syntacticconstraints.
Global formulas resolve several bridg-ing anaphors together, often to a globally salient an-tecedent beyond the local window.
Third, the modelallows us to express specific relations among bridg-ing anaphors and their antecedents (f11-f13).However, our pairwise model I already outper-forms improved baselines by about 10%, which sug-gests that our preposition pattern feature can capturemore diverse semantic relations.
The continuous im-provements shown in pairwise model II and pair-wise model III verify the contribution of our other914features and advanced antecedent candidate selec-tion strategy.
pairwise model III would become toocomplex if we tried to integrate discourse level for-mulas f7, f11-f13 into antecedent candidate selec-tion.
MLN model II solves this task elegantly.6.6 Discussion and error analysisWe analyse our best model (MLN model II) andcompare it to the best local one (pairwise model III).Anaphors with long distance antecedents areharder to resolve.
Table 5 shows the compari-son of correctly resolved anaphors with regard toanaphor-antecedent distance.
We can see that theglobal model is equal or better to the local modelfor all anaphor types but that the difference is espe-cially large for anaphora with antecedents that are3 or more sentences away due to the use of globalsalience and accessibility of possible antecedentsbeyond a fixed window-size.# pairs MLN II pairwise IIIsent.
distance0 175 48.57 45.141 260 34.62 352 90 47.78 43.33?3 158 35.44 16.46Table 5: Comparison of the percentage of correctly re-solved anaphors with regard to anaphor-antecedent dis-tance.
Significance tests are conducted using McNemar?stest at the level of 1%.We now distinguish between ?sibling anaphors?
(anaphors that share an antecedent with other bridg-ing anaphors) and ?non-siblings?
(anaphors that donot share an antecedent with any other anaphor).The performance of our MLN model II is 54%on sibling anaphors but only 24% on non-siblinganaphors.
This shows that our use of global salienceand links between related anaphors does indeed helpto capture the behaviour of sibling anaphors.However, our global model is good at predictingthe right antecedent for sibling anaphors where theantecedent is globally salient but not as good for sib-ling anaphors where the (shared) antecedent is a lo-cally salient subtopic.
Thus, in the future we needto model equivalent constraints for local salienceof antecedents, taking into account topic segmen-tation/shifts to improve over the 54% for siblinganaphors.The semantic knowledge we employ is still in-sufficient.
Typical cases where we have problemsare: (i) cases with very context-specific bridging re-lations.
For example, in one text about the stealingof Sago Palms in California we found the thievesas a bridging anaphor with the antecedent palms,which is not a very usual semantic link.
(ii) morefrequently, we have cases where several good an-tecedents from a semantic perspective can be found.For example, two laws are discussed and a lateranaphor the veto could be the veto of either bills.Integration of the wider context apart from the twoNPs is necessary in these cases.
This includes the se-mantics of modification, whereas we currently con-sider only head noun knowledge.
An example is thatthe anaphor the local council would preferably beinterpreted as the council of a village instead of thecouncil of a state due to the occurrence of local.Finally, 6% of the anaphors in our corpus have anon-NP antecedent.
These cases are not correctlyresolved in our current model as we only extract NPphrases as potential candidate antecedents.7 ConclusionsWe provide the first reasonably sized and reliablyannotated English corpus for bridging resolution.
Itcovers a diverse set of relations between anaphor andantecedent as well as all anaphor/antecedent types.We developed novel semantic, syntactic and saliencefeatures based on linguistic intuition.
Inspired bythe observation that salient entities are preferred asantecedents, we implemented a global model for an-tecedent selection within the framework of Markovlogic networks.
We show that our global model sig-nificantly outperforms other local models and base-lines.
This work is ?
to our knowledge ?
the firstbridging resolution algorithm that tackles the unre-stricted phenomenon in a real setting.Acknowledgements.
Yufang Hou is funded by a PhDscholarship from the Research Training Group Coher-ence in Language Processing at Heidelberg University.Katja Markert receives a Fellowship for Experienced Re-searchers by the Alexander-von-Humboldt Foundation.We thank HITS gGmbH for hosting Katja Markert andfunding the annotation.
We thank our colleague AngelaFahrni for advice on using Markov logic networks.915ReferencesRegina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34(1):1?34.Thorsten Brants and Alex Franz.
2006.
Web 1t 5-gramversion 1.
LDC2006T13, Philadelphia, Penn.
: Lin-guistic Data Consortium.Aoife Cahill and Arndt Riester.
2012.
Automatically ac-quiring fine-grained information status distinctions inGerman.
In Proceedings of the SIGdial 2012 Confer-ence: The 13th Annual Meeting of the Special InterestGroup on Discourse and Dialogue, Seoul, Korea, 5?6July 2012, pages 232?236.Tommaso Caselli and Irina Prodanof.
2006.
Annotat-ing bridging anaphors in Italian: In search of reliabil-ity.
In Proceedings of the 5th International Conferenceon Language Resources and Evaluation, Genoa, Italy,22?28 May 2006.Herbert H. Clark.
1975.
Bridging.
In Proceedings of theConference on Theoretical Issues in Natural LanguageProcessing, Cambridge, Mass., June 1975, pages 169?174.Pedro Domingos and Daniel Lowd.
2009.
MarkovLogic: An Interface Layer for Artificial Intelligence.Morgan Claypool Publishers.Angela Fahrni and Michael Strube.
2012.
Jointlydisambiguating and clustering concepts and entitieswith Markov logic.
In Proceedings of the 24th In-ternational Conference on Computational Linguistics,Mumbai, India, 8?15 December 2012, pages 815?832.Kari Fraurud.
1990.
Definiteness and the processing ofnoun phrases in natural discourse.
Journal of Seman-tics, 7:395?433.Claire Gardent and He?le`ne Manue?lian.
2005.
Cre?ationd?un corpus annote?
pour le traitement des descrip-tions de?finies.
Traitement Automatique des Langues,46(1):115?140.Matthew Gerber and Joyce Chai.
2012.
Semantic rolelabeling of implicit arguments for nominal predicates.Computational Linguistics, 38(4):756?798.Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.1993.
Cognitive status and the form of referring ex-pressions in discourse.
Language, 69:274?307.Donna Harman and Mark Liberman.
1993.
TIPSTERComplete.
LDC93T3A, Philadelphia, Penn.
: Linguis-tic Data Consortium.Iorn Korzen and Matthias Buch-Kromann.
2011.Anaphoric relations in the Copenhagen dependencytreebanks.
In S. Dipper and H. Zinsmeister, edi-tors, Corpus-based Investigations of Pragmatic andDiscourse Phenomena, volume 3 of Bochumer Lin-guistische Arbeitsberichte, pages 83?98.
University ofBochum, Bochum, Germany.Emmanuel Lassalle and Pascal Denis.
2011.
Leverag-ing different meronym discovery methods for bridgingresolution in French.
In Proceedings of the 8th Dis-course Anaphora and Anaphor Resolution Colloquium(DAARC 2011), Faro, Algarve, Portugal, 6?7 October2011, pages 35?46.Katja Markert, Malvina Nissim, and Natalia N. Mod-jeska.
2003.
Using the web for nominal anaphoraresolution.
In Proceedings of the EACL Workshop onthe Computational Treatment of Anaphora.
Budapest,Hungary, 14 April 2003, pages 39?46.Katja Markert, Yufang Hou, and Michael Strube.
2012.Collective classification for fine-grained informationstatus.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics, Jeju Is-land, Korea, 8?14 July 2012, pages 795?804.Ivan Meza-Ruiz and Sebastian Riedel.
2009.
Jointlyidentifying predicates, arguments and senses usingMarkov logic.
In Proceedings of Human LanguageTechnologies 2009: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, Boulder, Col., 31 May ?
5 June2009, pages 155?163.Shachar Mirkin, Ido Dagan, and Sebastian Pado?.
2010.Assessing the role of discourse references in entail-ment inference.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, Uppsala, Sweden, 11?16 July 2010, pages 1209?1219.Natalia M. Modjeska, Katja Markert, and Malvina Nis-sim.
2003.
Using the web in machine learning forother-anaphora resolution.
In Proceedings of the 2003Conference on Empirical Methods in Natural Lan-guage Processing, Sapporo, Japan, 11?12 July 2003,pages 176?183.Malvina Nissim, Shipara Dingare, Jean Carletta, andMark Steedman.
2004.
An annotation scheme for in-formation status in dialogue.
In Proceedings of the 4thInternational Conference on Language Resources andEvaluation, Lisbon, Portugal, 26?28 May 2004, pages1023?1026.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English Gigaword Fifth Edi-tion.
LDC2011T07.Massimo Poesio and Renata Vieira.
1998.
A corpus-based investigation of definite description use.
Com-putational Linguistics, 24(2):183?216.Massimo Poesio, Rahul Mehta, Axel Maroudas, andJanet Hitzeman.
2004.
Learning to resolve bridgingreferences.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics,Barcelona, Spain, 21?26 July 2004, pages 143?150.Massimo Poesio.
2003.
Associate descriptions andsalience: A preliminary investigation.
In Proceedings916of the EACL Workshop on the Computational Treat-ment of Anaphora.
Budapest, Hungary, 14 April 2003,pages 31?38.Hoifung Poon and Pedro Domingos.
2008.
Joint un-supervised coreference resolution with Markov Logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, Waikiki,Honolulu, Hawaii, 25?27 October 2008, pages 650?659.Ellen F. Prince.
1981.
Towards a taxonomy of given-newinformation.
In P. Cole, editor, Radical Pragmatics,pages 223?255.
Academic Press, New York, N.Y.Altaf Rahman and Vincent Ng.
2012.
Learning the fine-grained information status of discourse entities.
InProceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, Avignon, France, 23?27 April 2012, pages 798?807.Sebastian Riedel.
2008.
Improving the accuracy and ef-ficiency of MAP inference for Markov logic.
In Pro-ceedings of the 24th Conference on Uncertainty in Ar-tificial Intelligence, Helsinki, Finland, 9?12 July 2008,pages 468?475.Arndt Riester, David Lorenz, and Nina Seemann.
2010.A recursive annotation scheme for referential informa-tion status.
In Proceedings of the 7th InternationalConference on Language Resources and Evaluation,La Valetta, Malta, 17?23 May 2010, pages 717?722.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin Baker, and Martha Palmer.
2010.
SemEval-2010 Task 10: Linking events and their participantsin discourse.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluations (SemEval-2), Up-psala, Sweden, 15?16 July 2010, pages 45?50.Carina Silberer and Anette Frank.
2012.
Castingimplicit role linking as an anaphora resolution task.In Proceedings of STARSEM 2012: The First JointConference on Lexical and Computational Semantics,Montre?al, Que?bec, Canada, 7?8 June 2012, pages 1?10.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Renata Vieira and Massimo Poesio.
2000.
Anempirically-based system for processing definite de-scriptions.
Computational Linguistics, 26(4):539?593.Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-anwen Xue, Ann Taylor, Jeff Kaufman, MichelleFranchini, Mohammed El-Bachouti, Robert Belvin,and Ann Houston.
2011.
OntoNotes release 4.0.LDC2011T03, Philadelphia, Penn.
: Linguistic DataConsortium.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann, San Francisco, Cal., 2nd edition.917
