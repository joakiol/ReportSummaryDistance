Proceedings of the Fourth International Natural Language Generation Conference, pages 81?88,Sydney, July 2006. c?2006 Association for Computational LinguisticsNoun Phrase Generation for Situated DialogsLaura Stoia and Darla Magdalene Shockley and Donna K. Byron and Eric Fosler-LussierThe Ohio State UniversityComputer Science and Engineering2015 Neil Ave., Columbus, Ohio 43210stoia|shockley|dbyron|fosler@cse.ohio-state.eduAbstractWe report on a study examining the gener-ation of noun phrases within a spoken di-alog agent for a navigation domain.
Thetask is to provide real-time instructionsthat direct the user to move between a se-ries of destinations within a large interiorspace.
A subtask within sentence plan-ning is determining what form to choosefor noun phrases.
This choice is driven byboth the discourse history and spatial con-text features derived from the direction-follower?s position, e.g.
his view angle,distance from the target referent and thenumber of similar items in view.
The al-gorithm was developed as a decision treeand its output was evaluated by a group ofhuman judges who rated 62.6% of the ex-pressions generated by the system to be asgood as or better than the language origi-nally produced by human dialog partners.1 IntroductionIn today?s world of mobile, context-aware com-puting, intelligent software agents are being de-ployed in a wide variety of domains to aid hu-mans in performing navigation tasks.
Exam-ples include hand-held tourist information por-tals (Johnston et al, 2002) campus tour guides(Yang et al, 1999; Long et al, 1996; Striegnitzet al, 2005), direction-giving avatars for visitorsto a building (Cassell et al, 2002; Chou et al,2005), in-car driving direction systems (Dale et al,2003; Wahlster et al, 2001), and pedestrian navi-gation systems (Muller, 2002).
These applicationspresent an exciting and challenging new frontierfor dialog agents, since attributes of the real-worldsetting must be combined with other contextualfactors for the agent to communicate successfully.In the current work, we focus on a scenarioin which the system provides incremental direc-tions to a mobile user who is following the instruc-tions as they are produced.
Unlike the rigid di-rections produced by applications like Mapquest,1which describes the entire route from start to fin-ish, this task requires realtime instructions issuedwhile monitoring the user?s progress.
Instructionsare based on dynamic local context variables suchas the visibility of and distance to reference points.In referring to items in the setting, human speak-ers produce a wide variety of noun phrase forms,including descriptions that are headed by a com-mon noun and that employ a definite, indefinite, ordemonstrative determiner, one anaphors, and pro-nouns such as it, this and that.
Our goal in thecurrent work is to model that entire space of varia-tion, which makes the task more difficult than thenoun phrase generation task defined in many pre-vious studies that simplify the alternatives down todescription or pronoun.In order to study this process, we developed atask domain in which a human partner is directedthrough an interior space (a graphically-presented3D virtual world) to perform a sequence of ma-nipulation tasks.
In the first stages of the work, wecollected and annotated a corpus of human-humandialogs from this domain.
Then, using this data,we trained a decision-tree classifier to utilize con-text variables such as distance, target object visi-bility, discourse history, etc., to determine lexicalproperties of referring expressions to be producedby the generation component of our dialog system.2 Generation for Situated TasksMany previous projects, such as (Lauria et al,2001; Moratz and Tenbrink, 2003; Skubic et al,2002), inter alia, study interpretation of situatedlanguage, e.g.
for giving directions to a robot.
Thefocus of our work is rather on generating naviga-tion instructions for a human partner to follow.Linguistic studies have shown that speakers se-lect noun phrase forms to refer to entities based ona variety of factors.
Some of the factors are intrin-sic to the object being described, while others arefeatures of the context in which the expression isspoken.
The entity?s status within the discourse,1www.mapquest.com81spatial position, and the presence of similar itemsfrom which the target referent must be distin-guished, have all been found to cause changes tothe lexical properties chosen for a particular refer-ring expression (i.e.
(Gundel et al, 1993; Prince,1981; Grosz et al, 1995)).
This variation is ex-pressed in terms of the determiner chosen (e.g.that/a), the head noun (e.g.
that/door/one), andthe presence of additional modifiers such as pre-nominal adjectives or prepositional phrases.In natural language generation, the process ofgenerating referring expressions occurs in stages(Reiter and Dale, 1992).
The process we explorein this paper is the sentence planning stage, whichdetermines whether the context supports generat-ing a particular referring expression as a pronoun,description, one-anaphor, etc.There has been extensive research in both au-tomatic route description and on general nounphrase (NP) generation, but few projects considerextra-linguistic information as part of the contextthat influences dialog behavior.
(Poesio et al,1999) applies statistical techniques for the prob-lem of NP generation.
However, even though thecorpus used in that study contains descriptions ofmuseum items visually accessible to the user, thefeatures used in generation were mostly linguis-tic, and included little information about the vi-sual or spatial properties of the referent.
Anotherrelated study in statistical NP generation (Cheng etal., 2001) focuses on choosing the modifiers to beincluded.
Again, no features derived from the sit-uated world were used in that study.
(Maass et al,1995) use features from the world, including ob-jects?
color, height, width, and visibility, as well asthe user?s direction of travel and distance from ob-jects, for generating instructions in a situated task.However, their focus is on selecting landmarks anddescriptions under time pressure, rather than se-lecting the linguistic form to be produced.3 Data CollectionOur task setup is designed to elicit natural, spon-taneous situated language examples from humanpartners.
The experimental platform employs avirtual-reality (VR) world in which one partner,the direction-follower (DF), moves about to per-form a series of tasks, such as pushing buttons tore-arrange objects in the room, finding and pickingup treasures, etc.
The simulated world was pre-sented from first-person perspective on a desk-topcomputer monitor.
The DF had no knowledge ofthe world map or tasks.DG: you can currently see three buttons... there?sactually a fourth button that?s kind of hiddenDF: yeahDG: by this cabinet on the rightDF: I know, yeahDG: ok, um, so what you wanna do is you want togo in and you?re gonna press one of the buttonsthat?s on the right hand wall, so you wanna goall the way straight into the room and then facethe wallDF: mhmDG: there with the two buttonsDF: yepDG: um and you wanna push the one that?s on the leftFigure 1: Sample dialog fragment and accompanying videoframeHis partner, the direction-giver (DG), had a pa-per 2D map of the world and a list of tasks tocomplete.
As they performed the task, the DGhad instant feedback about the DF?s location inthe VR world, via mirroring of the DF?s computerscreen on the DG?s computer monitor.
The part-ners communicated through headset microphones.Our paid participants were self-identified nativespeakers of North American English.
Figure 1shows an example view of the world and the ac-companying dialog fragment.The video output of DF?s computer was cap-tured to a camera, along with the audio from bothmicrophones.
A logfile created by the VR soft-ware recorded the DF?s coordinates, gaze angle,and the position of objects in the world 10 timesper second.
These data sources were synchronizedusing calibration markers.
A technical report isavailable that describes the recording equipmentand software used (Byron, 2005).3.1 Corpus and Annotation SchemeUsing the above-described setup, we created a cor-pus consisting of 15 dialogs containing a total of221 minutes of speech.
It was transcribed andword-aligned using Praat 2 and SONIC.3 The di-alogs were further annotated using the Anvil soft-ware (Kipp, 2004) to identify a set of target refer-ring expressions in the corpus.
Because we are in-2http://www.praat.org3http://cslr.colorado.edu/beginweb/speech recognition/sonic.html/82terested in the spatial properties of the referents ofthese target referring expressions, the items of in-terest in this experiment were restricted to objectswith a defined spatial position.Each object in the virtual world was assigned asymbolic id, and the id of each target referring ex-pression was added to the annotation.
Referringexpressions with plural referents were marked asSet, and were labeled with a list of the membersin the set.
Expressions were also annotated as ei-ther vague when the referent was not clear at thetime of utterance or abandoned in case the utter-ance was cut short.
Items that did not contain asurface realization of the head of the NP (e.g., onthe left), were marked with the tag empty.The corpus contains 1736 target expressions, ofwhich 221 were Vague, 45 were Empty, and 228were Sets.
The remaining 1242 form the set of testitems in the experiment described below.
Vagueitems were excluded since we do not wish for thealgorithm we develop to reproduce this behavior.Set items were excluded in order to avoid the morecomplex calculation of spatial properties associ-ated with plural entities.The data used in the experiments is a consensusversion on which both annotators, two of the au-thors, agreed on the set of target expressions andtheir properties.
Due to the constraints introducedby the task, referent annotation achieved almostperfect agreement.
The data used in this study isonly the DG?s language.4 Algorithm DevelopmentOur ultimate goal is to provide input to a surfacerealization component for NP generation, giventhe ID of a target referent and a vector of contextfeatures.
It is desirable for these context featuresto be automatically derived, to limit the relianceon human annotation, so we restricted out study tofeatures that either were derived automatically, orrequired minimal human annotation.One impact of this decision is that even thoughthe linguistic literature predicts that syntactic fea-tures such as grammatical role are important inselecting NP forms, these features were difficultto obtain.
Our corpus contains spontaneous spo-ken discourse, which has no sentence boundariesand relaxed structural constraints.
Thus, automaticparsing was problematic.
With improved parsingtechniques, we may include syntactic informationin the decision process for NP generation in future,but this was not included in the current study.Following (Poesio et al, 1999), we consider thedet a, the, that, nonehead it, that, one, noun, nonemod +, -The possible values of each NP frame slot[det : nonehead : itmod : ?]
[det : thathead : nounmod : +]it that button on the rightNP frames for it and that button on the rightFigure 2: NP frame slot values and examplesinformation conveyed by an NP to be divided intofour slots which must be filled to be able to gen-erate the NP form: a determiner/quantifier, a preor post-modifier and a head noun slot.
There werevery few examples of premodifiers in the corpus,so we collapsed the modifier feature.
Therefore,the output from our algorithm is an NP frame spec-ifying values for the three slots for each target ex-pression.
Figure 2 shows the possible values ineach slot and example slot values for two NPs.
Thenumber of occurrences in the entire corpus for theNP frame slot values are shown in Table 2.4In the experimental VR world developed for thisstudy, all items from the same category were de-signed to look identical.
This was intended to en-courage the subjects to use referring expressionsthat rely on spatial attributes or deictic referencesuch as that one.
The spatial properties of targetreferents and distractors are used as inputs to thecontent planning algorithm.
Their values in thisstudy were calculated automatically based on ge-ometric properties of the virtual world.To form the training dataset, we processed eachtarget expression with a syntactic chunker.5 Thepartial parse it produced was further processedwith a regular-expression matcher to isolate thevalues corresponding to the three slots.
Parser er-rors caused some low-count NP frame values, sowe retained only items that occurred at least 10times in the entire corpus.
Any parser errors thatremained in the data were not hand corrected, inorder to minimize human intervention.4.1 Context FeaturesGiven the restrictions that we impose over whatis accessible to the learning algorithm, we devel-oped a set of features for each referring expressionthat characterize both the referent and the contextin which the expression was spoken.
The context4The two possible tags for Mod occurred in almost equalproportion (49%/51%)5http://www.ltg.ed.ac.uk/software/chunk/index.html83Dialog history features1.
Count and chainCount the mention counts for the referent over the dialog and inside a reference chaina2.
DeltaTime and DeltaTimeChain the time elapsed since it was last mentioned in the dialog overall or in a chain3.
PrevSpeaker the previous speaker that mentioned the ID (either DG or DF)4.
Modi?1, Deti?1, Headi?1the values of the slots of the NP frame of the prior mention of the same referent5.
Modi?2, Deti?2, Headi?2the previous-1 values of the slots6.
WordDistance and the number of words spoken by both speakers since the last mention of the IDchainWordDistance overall or in the chain7.
Typei?1indicates if the previous mention was in a Set, was Vague, or was a test itemSpatial/Visual featuresb8.
Distance the distance between the referent and the DF?s VR coordinates9.
Angle the angle between the center of the DF?s view angle and the center of the referent10.
Visible a boolean value which indicates if the object is visibleRelation to other objects in the world11.
Visible Distractors the number of other objects besides the target referent in the field of view12.
SameCatVisDistractors the number of visible distractors of the same type as the referentObject category and its information status13.
Cat the semantic category of the referent: door/cabinet/button14.
First Locate indicates if this is the first expression that allowed the DF to identify the objectin the world.
The point where joint spatial reference is accomplished.Table 1: The Context Features Used by the Algorithmamention counts are not considered over vague or ambiguous tags, or over sets.bnote that an Angle value smaller than 500 ensures the object is visibleDet HeadValue Count Percent Value Count Percentthe 364 39% noun 558 60%that/this 264 29% one 166 18%none 253 27% it 116 13%a 46 5% that 57 6%none 30 3%Table 2: Distribution of Det and Head values in the corpusv = Visible area(100o)?
= Angle to targetd = distance to targetIn this scene:VisDistr =3 {B2, B3, C1}VisSemDistr =2 {B2, B3}Figure 3: An example configuration with spatial contextfeatures.
The target object is B4.features are not only linguistic but also derivedfrom the extralinguistic situation, including spatialrelations between the referent and the DF?s posi-tion and orientation at that instant.
The contextfeature for each target expression includes theseautomatically-calculated attributes as well as fea-tures from the annotation described above.
Table 1describes the full set of context features, and Fig-ure 3 shows a schematic of the spatial context fea-tures.The mention history of any target referent is im-portant for determining the form to use in a subse-quent referring expression.
Ideally, the discoursehistory feature should indicate whether a refer-ent has already been discussed, and the distancebetween a new mention and its antecedent.
Butdetermining the discourse status of items in thisworld was complicated by two factors.
All ob-jects in the world of the same semantic categoryhad identical visual features, and the VR worldin which the task is conducted is a maze, whichrequired the subjects to perform tasks, move to adifferent portion of the maze, and possibly returnto a previously visited room.
Due to the visualand spatial confusion possible in this setting, thereis no guarantee that our subjects could accuratelycalculate whether they were discussing the sameobject they had encountered before, or rememberwhether that object had been discussed.
Whilethe subjects were focused on a task in a particularroom, however, it is reasonable to expect that theycould remember which items had been discussed.Therefore, the discourse histories of target objectswere calculated using a re-initialization process.Each time the subjects left a VR room to pursue adifferent task, if more than 25s elapsed before thenext mention of objects in that room, those sub-sequent expressions were considered to be in newcoreference chains.
This time constant was estab-lished by examining pronominal referring expres-sions in the training dialogs.These features were used as input to develop aclassifier to determine NP frames for unseen tar-get referents in context.
We chose decision treesdue to their ease of interpretation, but we plan totest other machine learning techniques in the fu-ture.
5 dialogs were held out as unseen data andthe remaining 10 were used to train and adjust theparameters of the decision process.
The first pro-cedure was to test whether the three slot valuesare interdependent.
In contrast to previous work,84which focused on predicting the values for one ofthe slots at a time, we hold that due to their inter-dependence, these decisions should not be madeseparately.
For example, a noun form that has thepronoun it as the head will never have a modifieror a determiner.
If the three slots are independent,training three separate classifiers and then com-bining their decisions will yield better results.
Onthe other hand, if they are dependent, better resultswill be obtained through training a single classifieron the combined label.
Unfortunately, combiningthe labels is problematic due to data sparsity.
Totest these dependencies, we trained several deci-sion trees, varying the independence assumptions:Independent - a decision tree was trained for eachslot and their outputs combined at the end.Joint - a decision tree was trained for the com-bined label for all three slotsConditional - three decision trees were trainedin sequence, each having access to the output ofthe previous tree.
For example, Mod-Det-Headmeans that first the Mod tree was trained, then atree to classify Det, using the output from Mod,and finally a tree for Head , using both the Detand Mod values.All possible orderings between Mod, Head andDet were tested.
The best result obtained was fromthe ordering Mod-Det-Head, but the differencesbetween the orderings were not significant.
The10 fold cross-validation results are shown in Ta-ble 3.
There were 632 items in the data set.
TheConditional trees outperformed the Independenttrees by 9%, which is significant at the level of(p < .0002).As our training data suggests, we test the Mod-Det-Head trees against our held out data.
Wedecided to use a leave one out method of train-ing/testing due to the sparsity of data.Independent Joint Mod-Det-Head22.0 % 28.8 % 31.0 %Table 3: Testing independence of the slot valuesDecision tree classifiers offer the opportunity toexamine the relevance of particular features in thefinal decision.
Algorithm 1 and 2 show exampletrees derived for the Mod and Det features (theHead tree is not shown due to space limitations).We found that there are significant dependenciesbetween the slots in the NP form.
Each time one ofthe slots?
values was available to the decision pro-cess, it was selected as most informative feature inthe next tree.
The spatial features were selected asinformative in all the trees, most prevelantly in theAlgorithm 1 An example decision tree for Modif FirstLocate = True thenif V isibleDistractors = 0 thenif Distance ?
116 thenreturn Mod: -elsereturn Mod:+elseif SameCatV isDistractors = 0 thenif V isibleDistractors ?
2 thenif Angle ?
38 thenreturn Mod: -elsereturn Mod: +elsereturn Mod: +elsereturn Mod: +elseif chainWordDistance = 0 thenif prevMention 6= Set/AllV ague thenif firstMention = True thenreturn Mod: +elseif Angle ?
27 thenreturn Mod: -elsereturn Mod: +elseif noprevMention thenreturn Mod: +else {prevMention = Set/AllV ague}return Mod: -elsereturn Mod: -Algorithm 2 An example decision tree for Detif Mod : ?
thenif FirstLocate = True thenreturn Det:thatelseif prevMention 6= Set/AllV ague thenif notV isible thenif Cat = Button/Cabinet thenreturn Det:noneelse {Cat = Door}return Det:thatelse {isV isible}if Headi?1= it thenreturn Det:noneelse if Headi?1= noun thenif DeltaT ime ?
6.3 thenif Cat = Button/Cabinet thenreturn Det:noneelse {Cat = Door}return Det:thatelsereturn Det:theelse if Headi?1= one/none/low thenreturn Det:thatelse {Headi?1= that}return Det:noneelse if noprevMention thenreturn Det:thatelse {prevMention = Set/AllV ague}return Det:noneelse {target has modifier}return Det:the85decision tree for Mod, suggesting that the decisionof including extra information is driven largely bythe spatial configuration.
The information statusfeatures and discourse history, such as First Lo-cate, Type, and attributes of the prior mention,were selected as good predictors for the Det slot.5 EvaluationWe report several methods of evaluating the NPframes produced using the process given by thedecision trees.
First, we report the results of astrict evaluation in which the system?s output mustexactly match expressions produced by the hu-man subjects.
We also compare this result witha hand-crafted Centering-style generation algo-rithm.
Requiring the algorithm to exactly matchhuman performance is an overly-strict criterion,since in many contexts several possible referringexpression forms could be equally felicitous in agiven context, so we also conducted a human judg-ment study.
The 5 test dialogs contain 295 targetexpressions.5.1 Exact Match EvaluationThe output of the decision tree classifier was com-pared to the expressions observed in the test dia-log.
Table 4 reports the results of this evaluation.The accuracy obtained was 31.2%.
The most fre-quent tag gives a 20.0% baseline performance us-ing this strict match criterion.Exact match resultsPredicted All three features det mod headCorrect 31% 48% 72% 56%Exact match: head feature per valuePredicted noun it none one thatCorrect 65% 64% 0% 30% 38%Exact match: det feature per valuePredicted a none that theCorrect 0% 49% 36% 66%Table 4: Classifier results using Exact-match criterion5.2 Comparison to CenteringFor purposes of comparing the performance of ourgeneration algorithm to existing work on genera-tion of NPs, we performed a manual evaluation ofthe centering-style generation algorithm describedin (Kibble and Power, 2000) against our dialogcorpus.
Algorithms developed according to thecentering framework use discourse coherence tomake decisions about pronominalization (Grosz etal., 1995), where coherence is measured in termsof topical continuity from one sentence to the next.Centering designates the backward-looking cen-ter (Cb) as the item in the current sentence thatwas most topical in the previous sentence.
There-fore, to perform a centering-style evaluation, thedialogs must be broken into sentence-like units,and a ranking procedure must be devised for theitems mentioned in each unit.The current evaluation corpus, being a spo-ken dialog, has not been parsed to automaticallydetermine the syntactic or dependency structure,but rather was manually segmented into utteranceunits, where each unit contained a main predicateand its satellites.
The items mentioned in each unitwere ranked according to thematic roles, using theranking {AGENT > PATIENT > COMP > AD-JUNCT}, and excluding references to the speakersthemselves, which often appear in AGENT posi-tion (Byron and Stent, 1998).
The Cb in each unit,if there is one, is the highest-ranked item from theprior unit?s list that is repeated in the current unit?slist.
Following a procedure similar to that reportedby Kibble and Power, our decision procedure rec-ommends pronominalizing an item if it is the Cbof its unit and if it is in Subject position, otherwisea description is generated.
Based on this rule, allitems that are being mentioned for the first timein the discourse are predicted to require a descrip-tion.Although most prior studies take the recom-mendation to pronominalize to mean that a per-sonal pronoun (e.g.
it) should be generated, dueto the demonstrative nature of our domain, the de-cision to produce a pronoun can result in either ademonstrative or a personal pronoun.
Therefore,we considered the algorithm?s output to match hu-man production when the target expression in thehuman corpus was either a personal or demonstra-tive pronoun, and the algorithm generated eithercategory of pronoun.
Table 5 shows the compari-son of our system?s output and the output from thecentering algorithm on anaphoric mentions.
The 5dialogs used for testing in this study contained 145such items.
Both algorithms obtained a similar ac-curacy (64.8% our system vs. 64.1% centering)and over-generated pronoouns.
Although our al-gorithm does not outperform centering, it assumesless structural analysis of the input text.5.3 Human Judgment EvaluationEvaluating generation studies by calculating theirsimilarity to human spontaneous speech may notbe the ideal performance metric, since several dif-ferent realizations may be equally felicitous in a86Pron Desc TotalHuman Production 28 117 145Predicted by Our Algorithm 55 90Predicted by Centering 64 81Table 5: Comparison to Coherence-based GenerationFigure 4: The Anvil software tool used for judginggiven context.
Therefore, we also performed ahuman judgement evaluation.
In this evaluation,judges compared the NPs generated by our algo-rithm to the NPs produced by human subjects, andto NPs with randomly generated feature assign-ments.
Judges viewed test NPs in the context ofthe original test corpus.To re-create the context in which the originalexpression was produced, the video, audio, anddialog transcript were played for the judges us-ing the Anvil annotation tool (Kipp, 2004).
Thejudges could play or pause the video as theywished.
Using the word-alignments establishedduring the data annotation phase, the audio of thetest NPs was replaced by silence, and the wordswere removed in the transcript shown in the time-line viewer.
For each test item, the judges werepresented with a selection box showing two pos-sible referring expressions that they were asked tocompare using a qualitative ranking (option 1 isbetter, option 2 is better, or they are equal), givena particular target ID and the context.
Figure 4shows a screen-shot of the judges?
annotation tool.The judges did not know the source of the expres-sions they evaluated (system, human production,or random).
The 10 judges were volunteers fromthe university community who were self-identifiednative speakers of English.
They were not com-pensated for their time.The decision tree selected NP-frame slot val-ues which were converted into realized NPs.
TheDet and Head choices were directly translated intosurface forms (for Head=noun we chose a consis-tent common noun for each semantic class: but-All ItemsSystem compared to Human Trials: 577equal 45.9%system preferred 16.6%(system equal or preferred to human) (62.6%)human preferred 37.4%System compared to Random Trials: 289equal 24.2%system preferred 53.3%(system equal or preferred to random) (77.5%)random preferred 22.5%Random compared to Human Trials: 292equal 23.3%random preferred 13.0%(random equal or preferred to human) (36.3%)human preferred 65.7%Items with two judges & judges agreedSystem against Human Trials: 197equal 37.3%system preferred 19.8%(system equal or preferred to human) (57.1%)human preferred 36.6%Table 6: Results of Human Judgington, door or cabinet.
If the system?s selection ofMod feature matched the value from the corpus,we used the expression produced by the originalspeaker.
If the original expression did not includea modifier, but the system selected Mod:+, we lex-icalized this feature to a simple but correct spatialdescription like on the right, on the left or in front.Table 6 shows the results of human judging.The system?s output was either equal or preferredto the original spontaneous language in 62.6%of cases where these two choices were compareddirectly.
Interestingly, the randomly-generatedchoice was preferred over the original spontaneouslanguage in 13.0% of trials, and was preferred overthe system?s output in 22.5% of trials.
Aggregat-ing over all judges, the system?s performance wasjudged to be much better than random, but not asgood as the original human language.Trials were balanced among judges so that eachtarget item was seen by four judges: with twocomparing the system?s response to the originalhuman language, one comparing the system torandom, and one comparing the human to random.There were 282 trials for which 2 judges saw theidentical pair of choices.
Out of these, the twojudges?
responses agreed in 197 cases, producingan inter-annotator reliability (kappa score) of 0.51,with raw agreement of 69% and expected agree-ment of 37%.
Although this is a relatively lowkappa value, we believe that the aggregate judg-ments of all of the judges over all of the test itemsare still informative, since the scores of items forwhich we have two judgements follow a very sim-87ilar pattern to the overal distribution of responses.The low inter-annotator agreement may be due tothe substitutability of the expressions.6 Conclusions and Future WorkIn this paper we describe a generation study forsituated dialog and a novel evaluation setup of thesystem?s output.
The algorithm decides upon thedeterminer, head and modifier values to be pro-duced in a noun phrase describing an object ina particular moment in the dialog.
The decisionis influenced by dialog history, spatial and visualrelations and information status of the ID to bedescribed.
Our algorithm achieved 31.2% exactmatch with human language, but human evalua-tors judged the output as good as or better than theoriginal human language 62.6% of the time.For our future work, we intend to develop thegeneration module of a dialog system that per-forms the direction giver?s role.
We plan to incor-porate the results of this study in an extension of(Reiter and Dale, 1992) algorithm that would takeinto account other types of properties of the ob-jects like visual salience, temporal attributes (forexample time elapsed between mentions), if it par-ticipated in an action (like the case of a door open-ing, or a button being pushed) or its importance tothe overall task completion.AcknowledgmentsThe authors would like to thank our undergradu-ate RA, Bradley Mellen, for building the virtualworld, the 11 judges who rated the system output,and the anonymous reviewers.ReferencesD.
Byron and A. Stent.
1998.
A preliminary model of center-ing in dialog.
In Proceedings of ACL ?98, pp.
1475?1477.D.
Byron.
2005.
The OSU Quake 2004 corpus of two-partysituated problem-solving dialogs.
Technical Report OSU-CISRC-805-TR57, The Ohio State University ComputerScience and Engineering Department, September.J.
Cassell, T. Stocky, T. Bickmore, Y. Gao, Y. Nakano,K.
Ryokai, D. Tversky, C. Vaucelle, and H. Vilhjalmsson.2002.
MACK: Media lab Autonomous ConversationalKiosk.
In Proceedings of IMAGINA?02, Monte Carlo, Jan-uary.H.
Cheng, M. Poesio, R. Henschel, and C. Mellish.
2001.Corpus-based NP modifier generation.
In NAACL ?01,pp.
1?8, Morristown, NJ, USA.
Association for Compu-tational Linguistics.S.
Chou, W. Hsieh, F. Gandon, and N. Sadeh.
2005.
Se-mantic web technologies for context-aware museum tourguide applications.
In Proceedings of the 2005 Interna-tional Workshop on Web and Mobile Information Systems.R.
Dale, S. Geldof, and J. Prost.
2003.
CORAL: Usingnatural language generation for navigational assistance.In M. Oudshoorn, editor, Proceedings of the 26th Aus-tralasian Computer Science Conference, Adelaide, Aus-tralia.B.
Grosz, A. Joshi, and S. Weinstein.
1995.
Centering: Aframework for modeling the local coherence of discourse.Computational Linguistics, 21(2):203?226.J.
Gundel, N. Hedberg, and R. Zacharski.
1993.
Cognitivestatus and the form of referring expressions in discourse.Language, 69(2):274?307.M.
Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen,M.
Walker, S. Whittaker, and P. Maloor.
2002.
MATCH:An architecture for multimodal dialogue systems.
In Pro-ceedings of ACL ?02, pp.
376?383.R.
Kibble and R. Power.
2000.
An integrated framework fortext planning and pronominalisation.
In Proceedings ofINLG?2000, pp.
77?84.M.
Kipp.
2004.
Gesture Generation by Imitation - FromHuman Behavior to Computer Character Animation.
Dis-sertation.com.S.
Lauria, G. Bugmann, T. Kyriacou, J. Bos, and E. Klein.2001.
Training personal robots using natural langauge in-struction.
IEEE Intelligent Systems, 16(5):2?9.S.
Long, R. Kooper, G. Abowd, and C. Atkesonet.
1996.Rapid prototyping of mobile context-aware applications:The cyberguide case study.
In 2nd ACM InternationalConference on Mobile Computing and Networking (Mo-biCom?96), November 10-12.W.
Maass, J. Baus, and J. Paul.
1995.
Visual grounding ofroute descriptions in dynamic environments.R.
Moratz and T. Tenbrink.
2003.
Instruction modes for jointspatial reference between naive users and a mobile robot.In Proc.
RISSP 2003 IEEE International Conference onRobotics, Intelligent Systems and Signal Processing, Spe-cial Session on NewMethods in Human Robot Interaction.C.
Muller.
2002.
Multimodal dialog in a pedestrian navi-gation system.
In Proceedings of ISCA Tutorial and Re-search Workshop on Multi-Modal Dialogue in Mobile En-vironments.M.
Poesio, R. Henschel, J. Hitzeman, and R. Kibble.
1999.Statistical NP generation: A first report.
Utrecht, August.E.
Prince.
1981.
On the inferencing of indefinite this NPs.In Aravind K. Joshi, Bonnie Lynn Webber, and Ivan Sag,editors, Elements of Discourse Understanding, pp.
231?250.
Cambridge University Press.E.
Reiter and R. Dale.
1992.
A fast algorithm for the gen-erations referring expressions.
In Proceedings of COL-ING ?92, pp.
232?238.M.
Skubic, D. Perzanowski, A. Schultz, and W. Adams.2002.
Using spatial language in a human-robot dialog.In 2002 IEEE International Conference on Robotics andAutomation, Washington, D.C.K.
Striegnitz, P. Tepper, A. Lovett, and J. Cassell.
2005.Knowledge representation for generating locating gesturesin route directions.
In Proceedings of Workshop on Spa-tial Language and Dialogue (5th Workshop on Languageand Space), Delmenhorst, Germany, October.W.
Wahlster, N. Reithinger, and A. Blocher.
2001.Smartkom: Towards multimodal dialogues with anthropo-morphic interface agents.
In International Status Confer-ence: Lead Projects HumanComputer -Interaction, Saar-bruecken, Germany.J.
Yang, W. Yang, M. Denecke, and A. Waibel.
1999.
Smartsight: a tourist assistant system.
In Proceedings of the3rd International Symposium on Wearable Computers, pp.73?78, San Francisco, California, 18-19 October.88
