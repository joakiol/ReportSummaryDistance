Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 44?54,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsDistinguishing Past, On-going, and Future Events: The EventStatus CorpusRuihong HuangTexas A&M Universityhuangrh@cse.tamu.eduIgnacio CasesStanford Universitycases@stanford.eduDan JurafskyStanford Universityjurafsky@stanford.eduCleo CondoravdiStanford Universitycleoc@stanford.eduEllen RiloffUniversity of Utahriloff@cs.utah.eduAbstractDetermining whether a major societal eventhas already happened, is still on-going, or mayoccur in the future is crucial for event pre-diction, timeline generation, and news sum-marization.
We introduce a new task and anew corpus, EventStatus, which has 4500 En-glish and Spanish articles about civil unrestevents labeled as PAST, ON-GOING, or FU-TURE.
We show that the temporal status ofthese events is difficult to classify because lo-cal tense and aspect cues are often lacking,time expressions are insufficient, and the lin-guistic contexts have rich semantic composi-tionality.
We explore two approaches for eventstatus classification: (1) a feature-based SVMclassifier augmented with a novel induced lex-icon of future-oriented verbs, such as ?threat-ened?
and ?planned?, and (2) a convolutionalneural net.
Both types of classifiers improveevent status recognition over a state-of-the-artTempEval model, and our analysis offers lin-guistic insights into the semantic composition-ality challenges for this new task.1 IntroductionWhen a major societal event is mentioned in thenews (e.g., civil unrest, terrorism, natural disaster), itis important to understand whether the event has al-ready happened (PAST), is currently happening (ON-GOING), or may happen in the future (FUTURE).
Weintroduce a new task and corpus for studying thetemporal/aspectual properties of major events.
TheEventStatus corpus consists of 4500 English andSpanish news articles about civil unrest events, suchas protests, demonstrations, marches, and strikes, inwhich each event is annotated as PAST, ON-GOING,or FUTURE (sublabeled as PLANNED, ALERT orPOSSIBLE).
This task bridges event extraction re-search and temporal research in the tradition ofTIMEBANK (Pustejovsky et al, 2003) and TempE-val (Verhagen et al, 2007; Verhagen et al, 2010;UzZaman et al, 2013).
Previous corpora have be-gun this association: TIMEBANK, for example, in-cludes temporal relations linking events with Doc-ument Creation Times (DCT).
But the EventStatustask and corpus offers several new research direc-tions.First, major societal events are often discussed be-fore they happen, or while they are still happening,because they have the potential to impact a largenumber of people.
News outlets frequently reporton impending natural disasters (e.g., hurricanes), an-ticipated disease outbreaks (e.g., Zika virus), threatsof terrorism, and plans or warnings of potential civilunrest (e.g., strikes and protests).
Traditional eventextraction research has focused primarily on recog-nizing events that have already happened.
Further-more, the linguistic contexts of on-going and futureevents involve complex compositionality, and fea-tures like explicit time expressions are less useful.Our results demonstrate that a state-of-the-art Tem-pEval system has difficulty identifying on-going andfuture events, mislabeling examples like these:(1) The metro workers?
strike in Bucharest has enteredthe fifth day.
(On-Going)(2) BBC unions demand more talks amid threat of newstrikes.
(Future)(3) Pro-reform groups have called for nationwideprotests on polling day.
(Future)44Second, we intentionally created the EventSta-tus corpus to concentrate on one particular eventframe (class of events): civil unrest.
In contrast,previous temporally annotated corpora focus on awide variety of events.
Focusing on one frame (se-mantic depth instead of breadth) makes this corpusanalogous to domain-specific event extraction datasets, and therefore appropriate for evaluating richtasks like event extraction and temporal question an-swering, which require more knowledge about eventframes and schemata than might be represented inlarge broad corpora like TIMEBANK (UzZaman etal., 2012; Llorens et al, 2015).Third, the EventStatus corpus focuses on specificinstances of high-level events, in contrast to the low-level and often non-specific or generic events thatdominate other temporal datasets.1 Mentions of spe-cific events are much more likely to be realized innon-finite form (as nouns or infinitives, such as ?thestrike?
or ?to protest?)
than randomly selected eventkeywords.
In breadth-based corpora like the Event-CorefBank (ECB) corpus (Bejan and Harabagiu,2008), 34% of the events have non-finite realization;in TIMEBANK, 45% of the events have non-finiterealization.
By contrast, in a frame-based corpuslike ACE2005 (ACE, 2005), 59% of the events havenon-finite forms.
In the EventStatus corpus, 80% ofthe events have non-finite forms.
Whether this is dueto differences in labeling or to intrinsic properties ofthese events, the result is that they are much harderto label because tense and aspect are less availablethan for events realized as finite verbs.Fourth, the EventStatus data set is multilingual:we collected data from both English and Spanishtexts, allowing us to compare events representingthe same event frame across two languages that areknown to differ in their typological properties for de-scribing events (Talmy, 1985).Using the new EventStatus corpus, we investigatetwo approaches for recognizing the temporal statusof events.
We create a SVM classifier that incor-porates features drawn from prior TempEval work(Bethard, 2013; Chambers et al, 2014; Llorens etal., 2010) as well as a new automatically induced1For example in TIMEBANK almost half the annotatedevents (3720 of 7935) are hypothetical or generic, i.e., PERCEP-TION, REPORTING, ASPECTUAL, I ACTION, STATE or I STATErather than the specific OCCURRENCE.lexicon of 411 English and 348 Spanish ?future-oriented?
matrix verbs?verbs like ?threaten?
and?fear?
whose complement clause or nominal directobject argument is likely to describe a future event.We show that the SVM outperforms a state-of-the-art TempEval system and that the induced lexiconfurther improves performance for both English andSpanish.
We also introduce a Convolutional Neu-ral Network (CNN) to detect the temporal status ofevents.
Our analysis shows that it successfully mod-els semantic compositionality for some challengingtemporal contexts.
The CNN model again improvesperformance in both English and Spanish, providingstrong initial results for this new task and corpus.2 The EventStatus CorpusFor major societal events, it can be very impor-tant to know whether the event has ended or if itis still in progress (e.g., are people still rioting inthe streets?).
And sometimes events are anticipatedbefore they actually happen, such as labor strikes,marches and parades, social demonstrations, politi-cal events (e.g., debates and elections), and acts ofwar.
The EventStatus corpus represents the tempo-ral status of an event as one of five categories:Past: An event that has started and has ended.
Thereshould be no reason to believe that it may still be inprogress.On-going: An event that has started and is still inprogress or likely to resume2 in the immediate fu-ture.
There should be no reason to believe that it hasended.Future Planned: An event that has not yet started,but a person or group has planned for or explicitlycommitted to an instance of the event in the future.There should be near certainty it will happen.Future Alert: An event that has not yet started, buta person or group has been threatening, warning, oradvocating for a future instance of the event.Future Possible: An event that has not yet started,but the context suggests that its occurrence is a livepossibility (e.g., it is anticipated, feared, hinted at,or is mentioned conditionally).The three subtypes of future events are important2For example, demonstrators have gone home for the daybut are expected to return in the morning.45Past[EN] Today?s demonstration ended without violence.An estimated 2,000 people protested against the government in Peru.
[SP] Termino?
la manifestacio?n de los kurdos en la UNESCO de Par?
?s.On-going[EN] Negotiations continue with no end in sight for the 2 week old strike.Yesterday?s rallies have caused police to fear more today.
[SP] Pacifistas latinoamericanos no cesan sus protestas contra guerra en Irak.Future Planned[EN] 77 percent of German steelworkers voted to strike to raise their wages.Peace groups have already started organizing mass protests in Sydney.
[SP] Miedo en la City en v?
?spera de masivas protestas que la toman por blanco.Future Alert[EN] Farmers have threatened to hold demonstrations on Monday.Nurses are warning they intend to walkout if conditions don?t improve.
[SP] Indigenas honduren?os amenazan con declararse en huelga de hambre.Future Possible[EN] Residents fear riots if the policeman who killed the boy is acquitted.The military is preparing for possible protests at the G8 summit.
[SP] Polic?
?a Militar analiza la posibilidad de decretar una huelga nacional.Table 1: Examples of event status categories for civil unrest events, showing two examples in English [EN] and one in Spanish[SP].in marking not just temporal status but also what wemight call predictive status.
Events very likely to oc-cur are distinguished from events whose occurrencedepends on other contingencies (Future Planned vs.Alert/Possible).
Warnings or mentions of a potentialevent by a likely actor are further distinguished fromevents whose occurrence is more open-ended (Fu-ture Alert vs. Possible).
The status of future eventsis not due just to lexical semantics or local contextbut also other qualifiers in the sentence (e.g.
?may?
),the larger discourse context, and world knowledge.The annotation guidelines are formulated with thatin mind.
The categories for future events are notincompatible with one another but are meant to beinformationally ordered (e.g.
?future alert?
implies?future possible?).
Annotators are instructed to gofor the strongest implication supported by the over-all context.
Table 1 presents examples of each cate-gory in news reports about civil unrest events, withthe event keywords in italics.2.1 EventStatus AnnotationsThe EventStatus dataset consists of English andSpanish news articles.
We manually identified 6English words3 and 13 Spanish words4 and phrasesassociated with civil unrest events, and added theirmorphological variants.
We then randomly selected2954 and 14915 news stories from the English Gi-gaword 5th Ed.
(Parker et al, 2011) and SpanishGigaword 3rd Ed.
(Mendon et al, 2011) corpora,respectively, that contain at least one civil unrestphrase.
Events of a specific type are very sparselydistributed in a large corpus like the Gigaword, sowe used keyword matching just as a first pass toidentify candidate event mentions.3The English keywords are ?protest?, ?strike?, ?march?,?rally?, ?riot?
and ?occupy?.
These correspond to the most fre-quent words in the relevant frame in the Media Frames corpus(Card et al, 2015).
Because ?march?
most commonly refers tothe month, we removed the word itself and only kept its othermorphological variations.4Spanish keywords: ?marchar?, ?protestar?, ?amoti-nar(se)?, ?manifestar(se)?, ?huelga?, ?manifestacio?n?, ?distur-bio?, ?mot?
?n?, ?ocupar * la calle?, ?tomar * la calle?, ?salir *las calles?, ?lanzarse a las calles?, ?cacerolas vac?
?as?, ?cacero-lazo?, ?cacerolada?.
Asterisks could be replaced by up to 4words.
The last three terms are common expressions for protestmarches in many countries of Latin America and Spain.546 (out of 3000) and 9 (out of 1500) stories were removeddue to keyword errors.46Future NotPast Ongoing (Plan,Alert,Possible) Multiple EventEN 1735 583 292 (197,48,47) 28 186SP 1545 739 360 (279,61,30) 21 72Table 2: Counts of Temporal Status Labels in EventStatus.Because many keyword instances don?t refer toa specific event, primarily due to lexical ambiguityand generic descriptions (e.g., ?Protests are oftenfacilitated by ...?
), we used a two-stage annotationprocess.
First, we extracted sentences containing atleast one key phrase, and had three human anno-tators judge whether the sentence describes a spe-cific civil unrest event.
Next, for each sentence thatmentions a specific event, the annotators assigned anevent status to every civil unrest key phrase in thatsentence.
In both annotation phases, we asked theannotators to consider the context of the entire arti-cle.In the first annotation phase, the average pairwiseinter-annotator agreement (Cohen?s ?)
among theannotators was ?
= 0.84 on the English data and 0.70on the Spanish data.
We then assigned the majoritylabel among the three annotators to each sentence.In the English data, of the 5085 sentences with atleast one key phrase, 2492 (49%) were judged tobe about a specific civil unrest event.
In the Span-ish data, 3249 sentences contained at least one keyphrase and 2466 (76%) described a specific event.In the second phase, the annotators assigned oneof the five temporal status categories listed in Sec-tion 2 to each event keyword in a relevant sentence.In addition, we provided a Not Event label.6 Occa-sionally, a single instance of a keyword can refer tomultiple events (e.g., ?Both last week?s and today?sprotests...?
), so we permitted multiple labels to beassigned to an event phrase.
However this happenedfor only 28 cases in English and 21 cases in Spanish.The average pairwise inter-annotator agreementamong the three human annotators for the tempo-ral status labels was ?=.78 for English and ?=.80for Spanish.
We used the majority label among thethree annotators as the gold status.
In total, 2907English and 2807 Spanish event phrases exist in therelevant sentences and were annotated.
However6A sentence can contain multiple keyword instances.
Soeven in a relevant sentence, some instances may not refer toa specific event.there were 83 English cases (?2.9%) and 70 Span-ish cases (?2.5%) where the labels among the threeannotators were all different, so we discarded thesecases.
Table 2 shows the final distribution of labelsin the EventStatus corpus.
The EventStatus corpus7is available through the LDC.2.2 Linguistic Properties of Event MentionsNext, we investigated the linguistic properties of theevent status categories, lumping together the 3 fu-ture subcategories.
Table 3 shows the distributionof syntactic forms of the event mentions in two com-monly used event datasets, ACE2005 (ACE, 2005)and EventCorefBank (Bejan and Harabagiu, 2008),and our new EventStatus corpus.
In the introduction,we mentioned the high frequency of non-finite eventexpressions; Table 3 provides the evidence: non-finite forms (nouns and infinitives) constitute 59% inACE2005, 34% in EventCorefBank, and a very high80% of the events in the EventStatus dataset.
Thedistribution is even more skewed for future events,which are 95% (English) and 96% (Spanish) real-ized by non-finite surface forms.Finite Inf.Verbs Nouns Verbs OtherACE Dataset2201 (41) 2566 (48) 352 (7) 243 (5)ECB Dataset1151 (66) 488 (28) 77 (4) 25 (1)EventStatus, English SectionPA 331 (19) 1295 (75) 103 (6) 6 (0)OG 58 (10) 476 (82) 29 (5) 20 (3)FU 15 (5) 245 (84) 32 (11) 0 (0)EventStatus, Spanish SectionPA 315 (20) 1145 (74) 84 (5) 1 (0)OG 41 (6) 685 (93) 12 (2) 1 (0)FU 14 (4) 309 (86) 36 (10) 1 (0)Table 3: Number and % (in parentheses) of event mentions bysyntactic form.
PA = Past; OG = On-going; FU = Future2.3 Future Oriented VerbsWe observed that many future event mentions arepreceded by a set of lexical (non-aux) verbs that wecall future oriented verbs, such as ?threatened?
in (4)and ?fear?
in (5).
These verbs project the events inthe lower clause into the future.7http://faculty.cse.tamu.edu/huangrh/EventStatus_corpus.html47(4) They threatened to protest if Kmart does not ac-knowledge their request for a meeting.
(5) People fear renewed rioting during the comingdays.Categories of future oriented verbs include mentalactivity (?anticipate?, ?expect?
), affective (?fear?,?worry?
), planning (?plan?, ?prepare?, ?schedule?
),threatening (?threaten?, ?advocate?, ?warn?
), andinchoative verbs (?start?, ?initiate?, and ?launch?
).We found that these categories correlate with thepredictive status of the events they embed.
We drewon these insights to induce a lexicon of future ori-ented verbs.We harvested matrix verbs whose complementunambiguously describes a future event using twoheuristics.
One heuristic looks for examples witha tense conflict between the matrix verb and itscomplement: a matrix verb in the past tense (like?planned?
below) whose complement event is an in-finitive verb or deverbal noun modified by a futuretime expression (like ?tomorrow?
or ?next week?
),hence in the future (e.g., ?strike?
below): 8(6) The union planned to strike next week.Future events are often marked by conditionalclauses, so the second heuristic considers an eventto be future if it was post-modified by a conditionalclause (beginning with ?if?
or ?unless?
):(7) The union threatened to strike if their appealwas rejected.Finally, to increase precision, we only harvesteda verb as future-oriented if it functioned as a matrixboth in sentences with an embedded future time ex-pression and in sentences with a conditional clause.Future Oriented Verb Categories: We ran thealgorithm on the English and Spanish Gigaword cor-pora (Parker et al, 2011; Mendon et al, 2011), ob-taining 411 English verbs and 348 Spanish verbs.To better understand the structure of the learned lex-icon, we mapped each English verb to Framenet(Baker et al, 1998); 86% (355) of the English verbsoccurred in Framenet, in 306 unique frames.
We8For English, we extract events linked by the ?xcomp?
de-pendency using the Stanford dependency parser (Marneffe etal., 2006), with a future time expression attached to the secondevent with the ?tmod?
relation.
For Spanish, we consider twoevents related if they are at most 5 words apart, and the secondevent is modified by a time expression, at most 5 words apart.clustered these into 102 frames9 and grouped theSpanish verbs following English Framenet, identi-fying 67 categories.
(Some learned verbs, such as?poise?
, ?slate?
, ?compel?
and ?hesitate?, had aclear future orientation but didn?t exist in Framenet.
)Table 4 shows examples of learned verbs for En-glish and their categories.Commitment: threaten, vow, promise, pledge,commit, declare, claim, volunteer, anticipateComing to be: enter, emerge, plunge, kick,mount reach, edge, soar, promote, increase,climb, doublePurpose: plan, intend, project, aim, object, targetPermitting: allow, permit, approve, subpoenaExperiencer subj: fear, scare, hateWaiting: expect, waitScheduling: arrange, scheduleDeciding: decide, opt, elect, pick, select, settleRequest: ask, urge, order, encourage, demand,appeal, request, summon, implore, advise, inviteEvoking: raise, press, back, recall, pressure,force, rush, pull, drag, respondTable 4: Examples from Future Oriented Verb LexiconIn the next sections we propose two classifiers,an SVM classifier using standard TempEval featuresplus our new future-oriented lexicon, and a Convo-lutional Neural Net, as a pilot exploration of whatfeatures and architecture work well for the EventSta-tus task.
For these studies we combine the FuturePlanned, Future Alert and Future Possible categoriesinto a single Future event status because we firstwanted to establish how well classifiers can detectthe primary temporal distinctions between Past vs.Ongoing vs. Future.
The future subcategories are,of course, relatively smaller and we expect that themost effective approach will be to design a classifierthat sits on top of the primary classifier to furthersubcategorize the Future instances.
We leave thetask of subcategorizing future events for later work.9By merging frames that share frame elements (e.g., ?Pur-pose?
and ?Project?
share the frame element ?plan?
)483 SVM Event Status ModelOur first classifier is a linear SVM classifier.10 Wetrained three binary classifiers (one per class) us-ing one-vs.-rest, and label an event mention withthe class that assigned the highest score to the men-tion.
We used features inspired by prior TempEvalwork and by the previous analysis, including words,tense and aspect features, time expressions, and thenew future-oriented verb lexicon.
We also experi-mented with other features used by TempEval sys-tems (including bigrams, POS tags, and two-hop de-pendency features), but they did not improve perfor-mance.11Bag-Of-Words Features: For bag-of-words uni-gram features we used a window size of 7 (7 left and7 right) for the English data and 6 for the Spanishdata; this size was optimized on the tuning sets.Tense, Aspect and Time Expressions: Becausethese features are known to be the most impor-tant for relating events to document creation time(Bethard, 2013; Llorens et al, 2010), we usedTIPSem (Llorens et al, 2010) to generate the tenseand aspect of events and find time expressions inboth languages.
TIPSem infers the tense and as-pect of nominal and infinitival event mentions usingheuristics without relying on syntactic dependen-cies.
For the English data set, we also generated syn-tactic dependencies using Stanford CoreNLP (Marn-effe et al, 2006) and applied several rules to cre-ate additional tense and aspect features based on thegoverning words of event mentions12.
Time indi-cation features are created by comparing documentcreation time to time expressions linked to an eventmention detected by TIPSem.
If TIPSem detects nolinked time expressions for an event mention, wetake the nearest time expression in the same sen-tence.Governing Words: Governing words have beenuseful in prior work.
Our version of the feature10Trained using LIBSVM (Chang and Lin, 2011) with linearkernels (polynomial kernels yielded worse performance).11Previous TempEval work reported that those additional fea-tures were useful when computing temporal relations betweentwo events but not when relating an event to the Document Cre-ation Time, for which tense, aspect, and time expression fea-tures were the most useful (Llorens et al, 2010; Bethard, 2013).12We did not imitate this procedure for Spanish because thequality of our generated Spanish dependencies is poor.pairs the governing word of an event mention withthe dependency relation in between.
We used Stan-ford CoreNLP (Marneffe et al, 2006) to generatedependencies for the English data.
For the Spanishdata, we used Stanford CoreNLP to generate Part-of-Speech tags13 and then applied the MaltParser(Nivre et al, 2004) to generate dependencies.4 Convolutional Neural Network ModelConvolutional neural networks (CNNs) have beenshown to be effective in modeling natural languagesemantics (Collobert et al, 2011).
We were espe-cially keen to find out whether the convolution op-erations of CNNs can model the semantic composi-tionality needed to detect temporal-aspectual status.For our experiments, we trained a simple CNN withone convolution layer followed by one max poolinglayer (Kim, 2014; Collobert et al, 2011),The convolution layer has 300 hidden units.
Ineach unit, the same affine transformation is appliedto every consecutive 5 words (a filter instance) inthe input sequence of words.
A different affinetransformation is applied to each hidden unit.
Aftereach affine transformation, a Rectified Linear Units(ReLU) (Nair and Hinton, 2010) non-linearity is ap-plied.
For each hidden unit, the max pooling layerselects the maximum value from the pool of real val-ues generated from each filter instance.After the max pooling layer, a softmax classifierpredicts probabilites for each of the three classes,Past, Ongoing and Future.
To alleviate overfittingof the CNN model, we applied dropout (Hinton etal., 2012) on the convolution layer and the followingpooling layer with a keeping rate of 0.5.Our experiments used the 300-dimension En-glish word2vec embeddings14 trained on 100 billionwords of Google News.
We trained our own 300-dimension Spanish embeddings, running word2vec(Mikolov et al, 2013) over both Spanish Giga-word (Mendon et al, 2011)?
tokenized using Stan-ford CoreNLP SpanishTokenizer (Manning et al,2014)?
and the pre-tokenized Spanish Wikipediadump (Al-Rfou et al, 2013).
The vectors were thentuned during backpropagation for our specific task.13Stanford CoreNLP has no support for generating syntacticdependencies for Spanish.14docs.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM.49Row Method PA OG FU Macro Micro1 TIPSem 26/80/39 8/32/13 4/23/7 13/45/20 20/68/312 TIPSem with transitivity 75/76/75 14/22/17 4/21/7 31/40/35 55/67/613 SVM with all features 91/81/86 33/47/39 45/58/51 56/62/59 75/75/754 SVM with BOW features only 88/80/84 37/46/41 40/53/45 55/60/57 72/72/725 +Tense/Aspect/Time 89/81/85 40/50/44 42/52/46 57/61/59 73/73/736 +Governing Word 90/81/85 43/56/48 42/55/47 58/64/61 75/75/757 +Future Oriented Lexicon 90/82/86 44/56/49 48/62/54 61/66/63 76/76/768 Convolutional Neural Net 91/83/87 46/57/51 49/67/57 62/69/65 77/77/77Table 6: Experimental Results on English Data.
Each cell shows Recall/Precision/F-score.Row Method PA OG FU Macro Micro1 TIPSem 19/84/31 14/38/20 4/53/8 12/58/20 16/65/252 TIPSem with transitivity 69/70/70 40/35/37 12/62/20 40/56/47 54/59/563 SVM with all features 84/77/80 48/51/49 42/57/48 58/62/60 69/69/694 SVM with BOW features only 82/75/78 53/56/54 34/52/41 56/61/59 68/68/685 +Tense/Aspect/Time 82/77/79 55/57/56 45/61/52 61/65/63 70/70/706 +Governing Word 83/75/79 51/56/53 42/58/49 59/63/61 69/69/697 +Future Oriented Lexicon 82/77/79 55/57/56 47/63/54 61/65/63 70/70/708 Convolutional Neural Net 84/80/82 60/58/59 44/59/50 62/66/64 72/72/72Table 7: Experimental Results on Spanish Data.
Each cell shows Recall/Precision/F-score.PA OG FUEnglish 1385 (68%) 427 (21%) 233 (11%)Spanish 1251 (59%) 589 (28%) 280 (13%)Table 5: Label Distributions in the Test Set5 EvaluationsFor all subsequent evaluations, we use gold eventmentions.
We randomly sampled around 20% of theannotated documents as the parameter tuning set andused the rest as the test set.
Rather than training onceon a distinct training set, all our experiment resultsare based on 10-fold cross validation on the test set,(1191 Spanish documents, 2364 English documents;see Table 5 for the distribution of event mentions).5.1 Comparing with a TempEval SystemWe begin with a baseline: applying a TempEval sys-tem to classify each event.
Most of our features arealready drawn from TempEval, but our goal was tosee if an off-the-shelf system could be directly ap-plied to our task.
We chose TIPSem (Llorens et al,2010), a CRF system trained on TimeBank that useslinguistic features, has achieved top performance inTempEval competitions for both English and Span-ish (Verhagen et al, 2010), and can compute therelation of each event with the Document CreationTime.
We applied TIPSem to our test set, mappingthe DCT relations to our three event status classes15.Row 1 of Tables 6 and 7 shows TIPSem re-sults.
The columns show results for each categoryseparately, as well as macro-average and micro-average results across the three categories.
Each cellshows the Recall/Precision/F-score numbers.
SinceTIPSem linked relatively few event mentions to theDCT, we next leveraged the transitivity of tempo-ral relations (UzZaman et al, 2012; Llorens et al,2015), linking an event to a DCT if the temporal re-lation between another event in the same sentenceand the DCT is transferable.
For instance, if eventA is AFTER its DCT, and event B is AFTER event A,then event B is also AFTER the DCT.16 Row 2 showsthe results of TIPSem with temporal transitivity.Even augmented by transitivity, TIPSem fails todetect many Ongoing (OG) and Future (FU) events;most mislabeled OG and FU events were nominal.Confusion matrices (Table 8) show that most of the15We used the obvious mappings from TIPSem relations:?BEFORE?
to ?PA?, ?AFTER?
to ?FU?
, and ?INCLUDES?
(for English) and ?OVERLAP?
(for Spanish) to ?OG?.16Some transitivity rules are ambiguous: if event A is AF-TER DCT, event B INCLUDES event A, event B can be AFTERor INCLUDES DCT.
We ran experiments and chose rules thatimproved performance the most for TipSem.50missed OG events were labeled as Past (PA) whileFU events were commonly mislabeled as both PAand OG.
Below are some examples of OG and FUevents mislabeled as PA:(8) Jego said Sunday on arriving in Guadeloupe that hewould stay as long as it took to bring an end to thestrike organised by the Collective against ExtremeExploitation (LKP).
(OG)(9) A massive protest planned for Kathmandu on Tues-day has been re-baptised a victory parade.
(FU)Predicted (EN) Predicted (SP)PA OG FU PA OG FUGold PA 718 96 15 653 231 6Gold OG 156 35 11 196 160 10Gold FU 72 30 7 78 72 26Table 8: Confusion Matrices for TIPSem (with transitivity).SVM Results Next, we compare TIPSem?s resultswith our SVM classifier.
An issue is that TIPSemidentifies only 72% and 78% of the gold event men-tions, for English and Spanish respectively17.
Tohave a fair comparison, we applied the SVM to onlythe event mentions that TipSem recognized.
Row3 shows these results for the SVM classifier usingits full feature set.
The SVM outperforms TipSemon all three categories, for both languages, with thelargest improvements on Future events.Next, we ran ablation experiments with the SVMto evaluate the impact of different subsets of its fea-tures.
For these experiments, we applied the SVM toall gold event mentions, thus Rows 1-3 of Tables 6and 7 report on fewer event mentions than rows 4-8.Row 4 shows results using only bag-of-words fea-tures18.
Row 5 shows results when additionally in-cluding the tense, aspect, and time features providedby TIPSem (Llorens et al, 2010).
Unsurprisingly,in both languages19 these features improve over justbag-of-word features.Row 6 further adds governing word features.These improve English performance, especially forOn-Going events.
For Spanish, governing word fea-17We were not able to decouple TipSem?s event recognitioncomponent and force it to process all event mentions.18Replacing each word feature with a word2vec embeddingresulted in slightly worse performance.19We always obtain even recall and precision for the microaverage metric because we only apply classifiers to event men-tions that refer to a civil unrest event.tures slightly decrease performance, likely due to thepoor quality of the Spanish dependencies.Row 7 adds the future oriented lexicon features20.For both English and Spanish, the future orientedlexicon increased overall performance, and (as ex-pected) especially for Future events.CNN Results Row 8 shows the results using CNNmodels.
For English and Spanish, the same window(7 words for English, 6 words for Spanish) was usedto compute bag-of-word features for SVMs as fortraining the CNN models.
For English, the CNNmodel further increased recall and precision acrossall three classes.
The CNN improved Spanish per-formance on both Past and On-going events, but theSVM outperformed the CNN for Future events whenthe future oriented lexicon features were included.6 AnalysisTo better understand whether the CNN model?sstrong performance was related to handling com-positionality, we examined some English examplesthat were correctly recognized by the CNN modelbut mislabeled by the SVM classifier with bag-of-words features.
The examples below (event men-tions are in italics) suggest that the CNN may becapturing the compositional impact of local cues like?possibility?
or ?since?
:(10) Raising the possibility of a strike on New Year?s Eve,the president of New York City?s largest union iscalling for a 30 percent raise over three years.
(FU)(11) The lockout was announced in the wake of a go-slowand partial strike by the union since July 12 aftermanagement turned down its demand.
(OG)We also conducted an error analysis by randomlysampling and then analyzing 50 of the 473 errorsby the CNN model.
Many cases (26/50) are am-biguous from the sentence alone, requiring discourseinformation.
The first example below is caused bythe well-known ?double access?
ambiguity of thecomplement of a communication verb (Smith, 1978;Abusch, 1997; Giorgi, 2010).
(12) Chavez also said he discussed the strike with UNSecretary General Kofi Annan and told him the strikeorganizers were ?terrorists.?
(OG)20For Spanish, we removed the governing word features be-cause of the poor quality of the Spanish dependencies.51(13) Students and teachers protest over education budget(PA)In 9/50 cases, the contexts that imply temporal statusare complex and fall out of our ?7 word range, e.g.,:(14) Protesters on Saturday also occupied two gymnas-tics halls near Gorleben which are to be used as ac-commodation for police.
They were later forciblydispersed by policemen.
(PA)The remaining 15/50 cases contain enough localcues to be solvable by humans, but both the CNNand SVM models nonetheless failed:(15) Eastern leaders have grown weary of the protestmovement led mostly by Aymara.
(OG)7 Related WorkOur work overlaps with two communities of tasksand corpora: the task of classifying temporal or-der between event mentions and Document CreationTime (DCT) in TempEval (Verhagen et al, 2007;Verhagen et al, 2010; UzZaman et al, 2013), andthe task of extracting events, associated with cor-pora such as ACE2005 (ACE, 2005) and the Event-CorefBank (ECB) (Bejan and Harabagiu, 2008).
Bystudying the events in a particular frame (civil un-rest), but focusing on their temporal status, our workhas the potential to draw these communities to-gether.
Most event extraction work (Freitag, 1998;Appelt et al, 1993; Ciravegna, 2001; Chieu and Ng,2002; Riloff and Jones, 1999; Roth and Yih, 2001;Zelenko et al, 2003; Bunescu and Mooney, 2007)has focused on extracting event slots or frames forpast events and assigning dates.
The TempEval taskof linking events to DCT has not focused on eventsthat tend to have non-finite realizations, nor has itfocused on subtypes of future events.
Our work, in-cluding the corpus and the future-oriented verb lex-icon, has the potential to benefit related tasks likegenerating event timelines from news articles (Allanet al, 2000; Yan et al, 2011) or social media sources(Li and Cardie, 2014; Ritter et al, 2012), or explor-ing the psychological implications of future orientedlanguage (Nie et al, 2015; Schwartz et al, 2015).8 ConclusionsWe have proposed a new task of recognizing thepast, on-going, or future temporal status of ma-jor events, introducing a new resource for study-ing events in two languages.
Besides its importancefor studying time and aspectuality, the EventStatusdataset offers a rich resource for any future investi-gation of information extraction from major societalevents.The strong performance of the convolutional netsystem suggests the power of latent representationsto model temporal compositionality, and points toextensions of our work using deeper and more pow-erful networks.Finally, our investigation of the role of contextand semantic composition in conveying temporal in-formation also has implications for our understand-ing of temporality and aspectuality and their linguis-tic expression.
Many of the errors made by our CNNsystem are complex ambiguities, like the double ac-cess readings, that cannot be solved without infor-mation from the wider discourse context.
Our workcan thus also be seen as a call for the further useof rich discourse information in the computationalstudy of temporal processing.9 AcknowledgmentsWe want to thank the Stanford NLP group and espe-cially Danqi Chen for valuable inputs, and MichaelZeleznik for helping us refine the categories and formasterfully orchestrating the annotation efforts.
Wealso thank Luke Zettlemoyer and all our reviewersfor providing useful comments.
This work was par-tially supported by the National Science Foundationvia NSF Award IIS-1514268, by the Defense Ad-vanced Research Projects Agency (DARPA) DeepExploration and Filtering of Text (DEFT) Programunder Air Force Research Laboratory (AFRL) con-tract no.
FA8750-13-2-0040, and by the IntelligenceAdvanced Research Projects Activity (IARPA) viaDepartment of Interior National Business Center(DoI/NBC) contract number D12PC00337.
TheU.S.
Government is authorized to reproduce and dis-tribute reprints for Governmental purposes notwith-standing any copyright annotation thereon.
Dis-claimer: the views and conclusions contained hereinare those of the authors and should not be inter-preted as necessarily representing the official poli-cies or endorsements, either expressed or implied, ofDARPA, NSF, IARPA, DoI/NBC, or the U.S. Gov-ernment.52ReferencesDorit Abusch.
1997.
Sequence of tense and temporal dere.
Linguistics & Philosophy, 20:1?50.ACE.
2005.
NIST ACE evaluation website.
Inhttp://www.nist.gov/speech/tests/ace/2005.Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013.Polyglot: Distributed word representations for multi-lingual nlp.
In Proceedings of the Seventeenth Confer-ence on Computational Natural Language Learning,pages 183?192, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.J.
Allan, V. Lavrenko, D. Malin, and R. Swan.
2000.
De-tections, Bounds, and Timelines: Umass and TDT-3.In Proceedings of Topic Detection and Tracking Work-shop.D.
Appelt, J. Hobbs, J.
Bear, D. Israel, and M. Tyson.1993.
FASTUS: a Finite-state Processor for Informa-tion Extraction from Real-world Text.
In Proceedingsof the Thirteenth International Joint Conference on Ar-tificial Intelligence (IJCAI).Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In In Proceed-ings of COLING/ACL, pages 86?90.C.
Bejan and S. Harabagiu.
2008.
A Linguistic Resourcefor Discovering Event Structures and Resolving EventCoreference.
In Proceedings of the Sixth InternationalConference on Language Resources and Evaluation(LREC).S.
Bethard.
2013.
ClearTK-TimeML: A minimalist ap-proach to TempEval 2013.
In Proceedings of SecondJoint Conference on Lexical and Computational Se-mantics (*SEM).R.
Bunescu and R. Mooney.
2007.
Learning to ExtractRelations from the Web using Minimal Supervision.In Proceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics.Dallas Card, Amber E. Boydstun, Justin H. Gross, PhilipResnik, and Noah A. Smith.
2015.
The media framescorpus: Annotations of frames across issues.
In Pro-ceedings of the 53rd Annual Meeting of the Associa-tion for Computational Linguistics and the 7th Inter-national Joint Conference on Natural Language Pro-cessing (Volume 2: Short Papers).Nathanael Chambers, Bill McDowell, Taylor Cassidy,and Steve Bethard.
2014.
Dense event ordering with amulti-pass architecture.
In Transactions of the Associ-ation for Computational Linguistics (TACL).Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.H.L.
Chieu and H.T.
Ng.
2002.
A Maximum En-tropy Approach to Information Extraction from Semi-Structured and Free Text.
In Proceedings of the 18thNational Conference on Artificial Intelligence.F.
Ciravegna.
2001.
Adaptive Information Extractionfrom Text by Rule Induction and Generalisation.
InProceedings of the 17th International Joint Confer-ence on Artificial Intelligence.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuglu, and P. Kuksa.
2011.
Natural Lan-guage Processing (Almost) from Scratch.
In Journalof Machine Learning Research.Dayne Freitag.
1998.
Toward General-Purpose Learningfor Information Extraction.
In Proceedings of the 36thAnnual Meeting of the Association for ComputationalLinguistics.Alessandra Giorgi.
2010.
About the speaker: towards asyntax of indexicality.
Oxford University Press, Ox-ford.G.
E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever,and R. R. Salakhutdinov.
2012.
Improving NeuralNetworks by Preventing Co-adaptation of Feature De-tectors.
In arXiv preprint arXiv:1207.0580.Y.
Kim.
2014.
Convolutional neural networks for sen-tence classification.
In Proceedings of 2014 the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-2014).J.
Li and C. Cardie.
2014.
Timeline Generation: Track-ing Individuals on Twitter.
In Proceedings of the 23rdInternational Conference on World Wide Web.H.
Llorens, E. Saquete, and B. Navarro.
2010.
TIPSem(English and Spanish): Evaluating CRFs and Seman-tic Roles in TempEval-2.
In Proceedings of the 5thInternational Workshop on Semantic Evaluation.H.
Llorens, N. Chambers, N. UzZaman, MostafazadehN., J. Allen, and J. Pustejovsky.
2015.
Semeval-2015Task 5: QA TempEval - Evaluating Temporal Infor-mation Understanding with Question Answering.
InProceedings of the 9th International Workshop on Se-mantic Evaluation (SemEval 2015).Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The stanford corenlp natural language process-ing toolkit.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 55?60.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the Fifth Conference on Language Re-sources and Evaluation (LREC-2006).Angelo Mendon ca, Daniel Jaquette, David Graff, andDenise DiPersio.
2011.
Spanish Gigaword Third Edi-tion.
In Linguistic Data Consortium.53T.
Mikolov, I. Sutskever, K. Chen, G. Corrado, andJ.
Dean.
2013.
Distributed Representations of Wordsand Phrases and their Compositionality.
In Proceed-ings of NIPS.V.
Nair and G. E. Hinton.
2010.
Rectified Linear UnitsImprove Restricted Boltzmann Machines.
In Pro-ceedings of 27th International Conference on MachineLearning.A.
Nie, J. Shepard, J. Choi, B. Copley, and P. Wolff.2015.
Computational Exploration of the LinguisticStructures of Future-Oriented Expression: Classifica-tion and Categorization.
In Proceedings of the NAACLStudent Research Workshop (NAACL-SRW?15).J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-Based Dependency Parsing.
In Proceedings of theEighth Conference on Computational Natural Lan-guage Learning (CoNLL).Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English Gigaword.
In Lin-guistic Data Consortium.J.
Pustejovsky, P. Hanks, R. Saur, A.
See, R. Gaizauskas,A.
Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,and M. Lazo.
2003.
The TIMEBANK Corpus.
InProceedings of Corpus Linguistics.E.
Riloff and R. Jones.
1999.
Learning Dictionaries forInformation Extraction by Multi-Level Bootstrapping.In Proceedings of the Sixteenth National Conferenceon Artificial Intelligence.A.
Ritter, Mausam, O. Etzioni, and S. Clark.
2012.
OpenDomain Event Extraction from Twitter.
In The 18thACM SIGKDD Knowledge Discovery and Data Min-ing Conference.D.
Roth and W. Yih.
2001.
Relational Learning viaPropositional Algorithms: An Information ExtractionCase Study.
In Proceedings of the Seventeenth In-ternational Joint Conference on Artificial Intelligence,pages 1257?1263, Seattle, WA, August.A.
Schwartz, G. Park, M. Sap, E. Weingarten, J. Eich-staedt, M. Kern, J. Berger, M. Seligman, and L. Un-gar.
2015.
Extracting Human Temporal Orientation inFacebook Language.
In Proceedings of the The 2015Conference of the North American Chapter of the As-sociation for Computational Linguistics - Human Lan-guage Technologies.Carlota Smith.
1978.
The syntax and interpretation oftemporal expressions in English.
Linguistics & Phi-losophy, 2:43?99.Leonard Talmy.
1985.
Lexicalization patterns: Semanticstructure in lexical forms.
In Timothy Shopen, editor,Language Typology and Syntactic Description, Volume3.
Cambridge University Press.N.
UzZaman, H. Llorens, and J. Allen.
2012.
EvaluatingTemporal Information Understanding with TemporalQuestion Answering.
In Proceedings of IEEE Inter-national Conference on Semantic Computing.N.
UzZaman, H. Llorens, J. Allen, L. Derczynski,M.
Verhagen, and J. Pustejovsky.
2013.
SemEval-2013 task 1: TempEval-3 evaluating time expressions,events, and temporal relations.
In Proceedings of the7th International Workshop on Semantic Evaluation(SemEval 2013).M.
Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,G.
Katz, and J. Pustejovsky.
2007.
SemEval-2007task 15: TempEval temporal relation identification.
InProceedings of the 4th International Workshop on Se-mantic Evaluations.M.
Verhagen, R. Sauri, T. Caselli, and J. Pustejovsky.2010.
SemEval-2010 task 13: TempEval-2.
In Pro-ceedings of the 5th International Workshop on Seman-tic Evaluation.R.
Yan, L. Kong, C. Huang, X. Wan, X. Li, and Y. Zhang.2011.
Timeline Generation through EvolutionaryTrans-temporal Summarization.
In Proceedings of the2011 Conference on Empirical Methods in NaturalLanguage Processing.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel Methods for RelationExtraction.
Journal of Machine Learning Research, 3.54
