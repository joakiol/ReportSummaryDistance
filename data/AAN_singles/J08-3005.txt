Identifying Semitic Roots: Machine Learningwith Linguistic ConstraintsEzra Daya?University of HaifaDan Roth?
?University of IllinoisShuly Wintner?University of HaifaWords in Semitic languages are formed by combining two morphemes: a root and a pattern.
Theroot consists of consonants only, by default three, and the pattern is a combination of vowelsand consonants, with non-consecutive ?slots?
into which the root consonants are inserted.Identifying the root of a given word is an important task, considered to be an essential partof the morphological analysis of Semitic languages, and information on roots is important forlinguistics research as well as for practical applications.
We present a machine learning approach,augmented by limited linguistic knowledge, to the problem of identifying the roots of Semiticwords.
Although programs exist which can extract the root of words in Arabic and Hebrew, theyare all dependent on labor-intensive construction of large-scale lexicons which are components offull-scale morphological analyzers.
The advantage of our method is an automation of this process,avoiding the bottleneck of having to laboriously list the root and pattern of each lexeme in thelanguage.
To the best of our knowledge, this is the first application of machine learning to thisproblem, and one of the few attempts to directly address non-concatenative morphology usingmachine learning.
More generally, our results shed light on the problem of combining classifiersunder (linguistically motivated) constraints.1.
IntroductionThe standard account of word-formation processes in Semitic languages describeswords as combinations of two morphemes: a root and a pattern.1 The root consists ofconsonants only, by default three (although longer roots are known), called radicals.The pattern is a combination of vowels and, possibly, consonants too, with ?slots?
intowhich the root consonants can be inserted.
Words are created by interdigitating roots?
Department of Computer Science, University of Haifa, 31905 Haifa, Israel.
E-mail: edaya@cs.haifa.ac.il.??
Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801.
E-mail:danr@cs.uiuc.edu.?
Department of Computer Science, University of Haifa, 31905 Haifa, Israel.
E-mail: shuly@cs.haifa.ac.il1 An additional morpheme, vocalization, is used to abstract the pattern further; for the present purposes,this distinction is irrelevant.Submission received: 19 June 2006; revised submission received: 30 May 2007; accepted for publication:12 October 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 3into patterns: The first radical is inserted into the first consonantal slot of the pattern,the second fills the second slot, and the third fills the last slot.
See Shimron (2003) for asurvey.We present a machine learning approach, augmented by limited linguistic knowl-edge, to the problem of identifying the roots of Semitic words.
To the best of ourknowledge, this is the first application of machine learning to this problem, and oneof the few attempts to directly address the non-concatenative morphology of Semiticlanguages using machine learning.
Although there exist programs which can extract theroots of words in Arabic (Beesley 1998a, 1998b) andHebrew (Choueka 1990), they are alldependent on labor-intensive construction of large-scale lexicons which are componentsof full-scale morphological analyzers.
Note that the Arabic morphological analyzer ofBuckwalter (2002, software documentation) only uses ?word stems?rather than rootand pattern morphemes?to identify lexical items.?
Buckwalter further notes that ?Theinformation on root and pattern morphemes could be added to each stem entry ifthis were desired.?
The challenge of our work is to automate this process, avoidingthe bottleneck of having to laboriously list the root and pattern of each lexeme in thelanguage.Identifying the root of a given word is a non-trivial problem, due to the complexnature of Semitic derivational and inflectional morphology and the peculiarities of theorthography.
It is also an important task.
Although existingmorphological analyzers forHebrew only provide a lexeme (which is a combination of a root and a pattern), for otherSemitic languages, notably Arabic, the root is an essential part of any morphologicalanalysis simply because traditional dictionaries are organized by root, rather than bylexeme (Owens 1997).
Information on roots is important for linguistic research, becauseroots can shed light on etymological processes, both within a single language and acrosslanguages.
Furthermore, roots are known to carry some meaning, albeit vague.
Thisinformation can be useful for computational applications: For example, several studiesshow that indexing Arabic documents by root improves the performance of informationretrieval systems (Al-Kharashi and Evens 1994; Abu-Salem, Al-Omari, and Evens 1999;Larkey, Ballesteros, and Connell 2002).2The contributions of this article are manifold.
First and foremost, we report ona practical system which can be used to extract roots in Hebrew and Arabic (thesystem is freely available; an on-line demo is provided at http://cl.haifa.ac.il/projects/roots/index.shtml).
The system can be used for practical applications orfor scientific (linguistic) research, and constitutes an important addition to the grow-ing set of resources dedicated to Semitic languages.
It is one of the few attempts todirectly address the non-concatenative morphology of Semitic languages and extractnon-contiguousmorphemes from surface forms.
As amachine learning application, thiswork describes a set of experiments in combination of classifiers under constraints.
Theresulting insights can be used for other applications of the same techniques for similarproblems (see, e.g., Habash and Rambow 2005).
Furthermore, this work demonstratesthat providing a data-driven classifier with limited linguistic knowledge significantlyimproves the classification results.We focus on Hebrew in the first part of this article.
After sketching the linguisticdata in Section 2 and our methodology in Section 3, we discuss in Section 4 a simple,baseline, learning approach.We then propose several methods for combining the results2 These results are challenged by Darwish and Oard (2002), who conclude that roots are inferior tocharacter n-grams for this task.430Daya, Roth, and Wintner Identifying Semitic Rootsof interdependent classifiers in Section 5 and demonstrate the benefits of using limitedlinguistic knowledge in the inference procedure.
Then, the same technique is appliedto Arabic in Section 6 and we demonstrate comparable improvements.
In Section 7we discuss the influence of global constraints on local classifiers.
We conclude withsuggestions for future research.2.
Linguistic BackgroundRoot and pattern morphology is the major word formation device of Semitic languages.As an example, consider the Hebrew roots g.d.l, k.t.b, and r.$.m and the patternshaCCaCa, hitCaCCut, and miCCaC, where the ?C?s indicate the slots.3 When theroots combine with these patterns the resulting lexemes are hagdala, hitgadlut, migdal,haktaba, hitkatbut, miktab, har$ama, hitra$mut, and mir$am, respectively.
After theroot combines with the pattern, some morpho-phonological alternations take place,which may be non-trivial: For example, the hitCaCCut pattern triggers assimilationwhen the first consonant of the root is t or d : thus, d.r.$+hitCaCCut yields hiddar$ut.The same pattern triggers metathesis when the first radical is s or $ : s.d.r+hitCaCCutyields histadrut rather than the expected *hitsadrut.
Semi-vowels such as w or y in theroot are frequently combined with the vowels of the pattern, so that q.w.m+haCCaCayields haqama, and so on.
Frequently, root consonants such as w or y are altogethermissing from the resulting form.These matters are complicated further due to two sources: First, the standardHebrew orthography leaves most of the vowels unspecified.4 It does not explicate a ande vowels, does not distinguish between o and u vowels and leaves many of the i vowelsunspecified.
Furthermore, the single letter w is used both for the vowels o and u and forthe consonant v, whereas i is similarly used both for the vowel i and for the consonant y.On top of that, the script dictates thatmany particles, including four of themost frequentprepositions, the definite article, the coordinating conjunction, and some subordinatingconjunctions, all attach to the words which immediately follow them.
Thus, a form suchas mhgr can be read as a lexeme (?immigrant?
), as m-hgr (?from Hagar?
), or even asm-h-gr (?from the foreigner?).
Note that there is no deterministic way to tell whether thefirst m of the form is part of the pattern, the root, or a prefixing particle (the prepositionm ?from?
).The Hebrew script has 22 letters, all of which can be considered consonants.
Thenumber of tri-consonantal roots is thus theoretically bounded by 223, although severalphonological constraints limit this number to a much smaller value.
For example,although roots whose second and third radicals are identical abound in Semitic lan-guages, roots whose first and second radicals are identical are extremely rare (seeMcCarthy 1981 for a theoretical explanation).
To estimate the number of roots in Hebrewwe compiled a list of roots from two sources: a dictionary (Even-Shoshan 1993) and theverb paradigm tables of Zdaqa (1974).
The union of these yields a list of 2,152 roots.53 To facilitate readability we use a straight-forward transliteration of Hebrew using ASCII characters,where the characters (in Hebrew alphabetic order) are: ?bgdhwzxviklmnsypcqr$t.4 In this work we consider only texts in undotted, or unvocalized script.
This is the standard script of bothHebrew and Arabic.5 Only tri-consonantal roots are counted.
Ornan (2003) mentions 3,407 roots, whereas the number of rootsin Arabic is estimated to be 10,000 (Darwish 2002).
We do not know why Arabic should have so manymore roots than Hebrew.431Computational Linguistics Volume 34, Number 3Whereas most Hebrew roots are regular, many belong to weak paradigms, whichmeans that root consonants undergo changes in some patterns.
Examples include i or nas the first root consonant, w or i as the second, i as the third, and roots whose secondand third consonants are identical.
For example, consider the pattern hCCCh.
Regularroots such as p.s.q yield forms such as hpsqh.
However, the irregular roots n.p.l, i.c.g,q.w.m, and g.n.n in this pattern yield the seemingly similar forms hplh, hcgh, hqmh,and hgnh, respectively.
Note that in the first and second examples, the first radical (n ori ) is missing, in the third the second radical (w) is omitted, and in the last example oneof the two identical radicals is omitted.
Consequently, a form such as hC1C2h can haveany of the roots n.C1.C2, C1.w.C2, C1.i.C2, C1.C2.C2 and even, in some cases, i.C1.C2.Although root and pattern morphology is the major word formation device of Se-mitic languages, both Hebrew and Arabic have words which are not generated throughthis mechanism, and therefore have no root.
These are either loan words (which areoftentimes longer than originally Semitic words, in particular in the case of propernames) or short functional or frequent wordswhose origin is more ancient.
For example,the most frequent token in Hebrew texts is the accusative preposition ?t, which is notformed through root and pattern processes.
Of course, theremay be surface formswhichare ambiguous: one reading based on root and pattern morphology, the other a loanword, for example, npl (either ?Nepal?
or ?fall?
(past, 3rd person masculine singular)).Although the Hebrew script is highly ambiguous, ambiguity is somewhat reducedfor the task we consider here, as many of the possible lexemes of a given form sharethe same root.
Still, in order to correctly identify the root of a given word, contextmust be taken into consideration.
For example, the form $mnh has more than a dozenreadings, including the adjective ?fat?
(feminine singular), which has the root $.m.n,and the verb ?count,?
whose root is m.n.i, preceded by a subordinating conjunction.In the experiments we describe herein we ignore context completely, so our resultsare handicapped by design.
Adding contextual information renders the problem verysimilar to that of word sense disambiguation (as different roots denote distinct senses),and we opted to focus only on morphology here.3.
Data and Methodology3.1 Machine-Learning FrameworkIn this work we apply several machine-learning techniques to the problem of rootidentification.
In all the experiments described in this article we use SNoW (Roth1998; Carlson et al 1999) as the learning environment, with Winnow as the updaterule (using Perceptron yielded comparable results).
SNoW is a multi-class classifierthat is specifically tailored for learning in domains in which the potential number ofinformation sources (features) taking part in decisions is very large.
It is an on-linelinear classifier, as are most of the classifiers currently used in NLP, over a variablenumber of expressive features.
In addition to the ?standard?
perceptron-like algorithms,SNoW has a number of extensions such as regularization and good treatment of multi-class classification.
SNoW provides, in addition to classification, a reliable confidence inthe instance prediction which facilitates its use in an inference algorithm that combinespredictors to produce a coherent inference.SNoW has already been used successfully as the learning vehicle in a large col-lection of natural language related tasks, including part-of-speech tagging, shallowparsing, information extraction tasks, and so forth, and compared favorably with otherclassifiers (Roth 1998; Golding and Roth 1999; Banko and Brill 2001; Punyakanok and432Daya, Roth, and Wintner Identifying Semitic RootsRoth 2001; Florian 2002; Punyakanok, Roth, and Yih 2005).
The choice of SNoW asthe learning algorithm in this work is motivated by its good performance on other,similar tasks and on the availability in SNoW of easy to tune state-of-the-art versionsof three linear algorithms (Perceptron, Winnow, and naive Bayes) in a single package.As was shown by Roth (1998), Golding and Roth (1999), and in countless experimentalpapers thereafter, most algorithms used today, from on-line variations of Winnow andPerceptron to maximum entropy algorithms to SVMs, perform comparably if tunedproperly, and the eventual performance depends mostly on the selection of features.3.2 Data and EvaluationFor training and testing, a Hebrew linguist manually tagged a corpus of 15,000 words(from a set of newspaper articles).
Of these, only 9,752 were annotated; the reason forthe gap is that some Hebrew words, mainly borrowed but also some frequent wordssuch as prepositions, are not formed by the root and pattern paradigm.
Such words areexcluded from our experiments in this work; in an application, such words have to beidentified and handled separately.
This can be rather easily done using simple heuristicsand a small list of frequent closed-class words, because words which do not conformto the root and pattern paradigm are either (short, functional) closed-class words, orloan words which tend to be longer and, in many cases, involve ?foreign?
characters(typically proper names).
This problem is orthogonal to the problem of identifying theroot, and hence a pipeline approach is reasonable.We further eliminated 168 roots with more than three consonants and were leftwith 5,242 annotated word types, exhibiting 1,043 different roots.
Table 1 shows thedistribution of word types according to root ambiguity.Table 2 provides the distribution of the roots of the 5,242 word types in our corpusaccording to root type, where Ri is the ith radical (note that some roots may belong tomore than one group).Table 1Root ambiguity in the corpus.Number of roots 1 2 3 4Number of word types 4,886 335 18 3Table 2Distribution of root paradigms.Paradigm Number PercentageR1 = i 414 7.90R1 = w 28 0.53R1 = n 419 7.99R2 = i 297 5.66R2 = w 517 9.86R3 = h 18 0.19R3 = i 677 12.92R2 = R3 445 8.49Regular 3,061 58.41433Computational Linguistics Volume 34, Number 3As assurance for statistical reliability, in all the experiments discussed in the sequel(unless otherwise mentioned) we performed 10-fold cross validation runs for everyclassification task during evaluation.
We also divided the annotated corpus into twosets: a training set of 4,800 words and a test set of 442 words.
Only the training setwas used for most experiments, and the results reported here refer to these data unlessstated otherwise.
We used the training set to tune the parameter ?
(see Section 5.4), andonce ?
was set we report on results obtained by training on the training set and testingon test data (Table 8).A given example is a word type with all its (manually tagged) possible roots.
Inthe experiments we describe subsequently, our system produces one or more rootcandidates for each example.
For each example, we define tp as the number of correctcandidates produced by the system; fp as the number of candidates which are not cor-rect roots; and fn as the number of roots the system did not produce.
As usual, we definerecall as tptp+fp, precision as tptp+fnand F-score as 2?recall?precisionrecall+precision; we then (macro-) averageover all words to obtain the system?s overall F-score.To estimate the difficulty of this task, we asked six human subjects to performit.
Participants were asked to identify all the possible roots of all the words in a listof 200 words (without context), randomly chosen from the training corpus.
All par-ticipants were computer science graduates, native Hebrew speakers with no linguisticbackground.
The average precision of humans on this task is 83.52%, and with recall at80.27%, F-score is 81.86%.
We conjecture that the main reasons for the low performanceof our subjects are the lack of context (people tend to pick the most prominent root andignore the less salient ones) and the ambiguity of some of the weak paradigms (Hebrewspeakers are unaware of the correct root in many of the weak paradigms, even whenonly one exists).3.3 Feature DesignAll the experiments we describe in this work share the same features and differ only inthe target classifiers.
One of the advantages of SNoW is that it makes use of variable-sized feature vectors, represented as the list of the active (present) features in a giveninstance, rather than the fixed-sized Boolean vectors.
This facilitates the use of very long(theoretically, unbounded) feature lists, which are typically very sparse.
The featuresthat are used to characterize a word are both grammatical and statistical: Position of letters (e.g., the third letter of the word is b).
We limit wordlength to 20, as the longest string generated by a Hebrew morphologicalgenerator (Yona and Wintner 2008) is 18.
We thus obtain up to 440features6 of this type (recall that the size of the alphabet is 22). Bigrams of letters, independently of their location (e.g., the substring gdoccurs in the word).
This yields up to 484 features. Prefixes (e.g., the word is prefixed by k$h, ?when the?).
We have 292features of this type, corresponding to 17 prefixes and sequences thereof. Suffixes (e.g., the word ends with im, a plural suffix).
There are 26 suchfeatures.6 Some of these features are never active and are thus never represented.434Daya, Roth, and Wintner Identifying Semitic RootsThe lists of suffixes and of prefix sequences were compiled from a morphologicalgrammar of Hebrew (Yona and Wintner 2008).
In the general case, such features can beelicited from (non-expert) native speakers or extracted from amorphologically analyzedcorpus, if one exists.3.4 Linguistic ResourcesOne of our goals in this work is to demonstrate the contribution of limited linguisticknowledge to a machine-learning approach to an NLP task.
Specifically, we used thefollowing resources for Hebrew and Arabic: A list of roots (Section 2) Lists of common prefixes and suffixes (Section 3.3) Corpora annotated with roots (Section 3.2) Knowledge of word-formation processes, and in particular the behavior ofthe weak roots in certain paradigms (see Section 5.4)It is important to note that these resources do not constitute a method for identifying,even approximately, the root of a given word.
We are unaware of any set of rules whichattempts to address this task, or of the chances of solving this problem deterministically.4.
Naive Classification Methods4.1 Direct PredictionTo establish a baseline, we first performed two experiments with simple, baseline clas-sifiers.
In the first of the two experiments, referred to as Experiment A, we trained aclassifier to learn roots as a single unit.
The two obvious drawbacks of this approachare the large set of targets and the sparseness of the training data.
Of course, defining amulti-class classification task with 2,152 targets, when only half of them are manifestedin the training corpus, does not leave much hope for ever learning to identify themissing targets.
There is no generalization when the whole root is predicted as a singleunit with a simple classifier.In Experiment A, themacro-average precision of 10-fold cross validation runs of thisclassification problem is 45.72%; recall is 44.37%, yielding an F-score of 45.03%.
In orderto demonstrate the inadequacy of this method, we repeated the same experiment witha different organization of the training data.
We chose 30 roots and collected all theiroccurrences in the corpus into a test file.
We then trained the classifier on the remainderof the corpus and tested on the test file.
As expected, the accuracy was close to 0%.4.2 Decoupling the ProblemIn the second experiment, referred to as Experiment B, we separated the problem intothree different tasks.
We trained three classifiers to learn each of the root consonantsin isolation and then combined the results in the straightforward way (a conjunctionof the decisions of the three classifiers).
This is still a multi-class classification but the435Computational Linguistics Volume 34, Number 3number of targets in every classification task is only 22 (the number of letters in theHebrew alphabet) and data sparseness is no longer a problem.
Although each classifierperforms well in isolation, the clear limitation of this method is that it completelyignores interdependencies between different targets: The decision on the first radicalis completely independent of the decision on the second and the third.We observed a difference between recognizing the first and third radicals andrecognizing the second one, as can be seen in Table 3.
These results correspond wellto our linguistic intuitions: The most difficult cases for humans are those in whichthe second radical is w or i, and those where the second and the third consonants areidentical.
Combining the three classifiers using logical conjunction yields an F-score of52.84%.
Repeating the same experiment, but testing only on unseen roots, yielded 18.1%accuracy.To demonstrate the difficulty of the problem, we conducted yet another experiment.Here, we trained the system as described but we tested it on different wordswhose rootswere known to be in the training set.
The results of Experiment A here were 46.35%,whereas Experiment B was accurate in 57.66% of the cases.
Evidently, even when testingonly on previously seen roots, both naive methods are unsuccessful.5.
Combining Interdependent Classifiers5.1 Adding Linguistic ConstraintsThe experiments discussed previously are completely devoid of linguistic knowledge.In particular, Experiment B inherently assumes that any sequence of three consonantscan be the root of a given word.
This is obviously not the case: With few exceptions, allradicals must be present in any inflected form.
In fact, when roots and patterns combine,the first radical can be deleted only when it is w, i, n, and in an exceptional case l ; thesecond radical can only be deleted if it is w or i ; and the third, only if it is i.
We thereforetrained the classifiers to consider as targets, during training and testing, only letters thatoccurred in the observedword, plus w, i, n, and l (depending on the radical), rather thanany of the alphabet letters.
The average number of targets is now 7.2 for the first radical,5.7 for the second, and 5.2 for the third (compared to 22 each in the previous setup).In this model, known as the sequential model (Even-Zohar and Roth 2001), SNoW?sperformance improved slightly, as can be seen in Table 4 (compare to Table 3).
Com-bining the results in the straight-forward way yields an F-score of 58.89%, a smallimprovement over the 52.84% performance of the basic method.
This new result shouldbe considered the baseline.
In what follows we always employ the sequential modelfor training and testing the classifiers, using the same constraints.
However, we employmore linguistic knowledge for a more sophisticated combination of the classifiers.Table 3Accuracy of identifying the correct radical.R1 R2 R3 rootPrecision 82.25 72.29 81.85 53.60Recall 80.13 70.00 80.51 52.09F-score 81.17 71.13 81.18 52.84436Daya, Roth, and Wintner Identifying Semitic RootsTable 4Accuracy of identifying the correct radical, sequential model.R1 R2 R3 rootPrecision 83.06 72.52 83.88 59.83Recall 80.88 70.20 82.50 57.98F-score 81.96 71.34 83.18 58.895.2 Sequential CombinationEvidently, simple combination of the results of the three classifiers leaves much roomfor improvement.
We therefore explore other ways for combining these results.
We canrely on the fact that SNoW provides insight into the decisions of the classifiers?it listsnot only the selected target, but rather all candidates, with an associated confidencemeasure.
Apparently, the correct radicals are chosen among SNoW?s top-n candidateswith high accuracy, as shown in Table 5.
This observation calls for a different wayof combining the results of the classifiers which takes into account not only the firstcandidate but also others, along with their confidence scores.Given the sequential nature of the data and the fact that our classifier returnsa distribution over the possible outcomes for each radical, a natural approach is tocombine SNoW?s outcomes via a Markovian approach.
Variations of this approachare used in the context of several natural language problems, including part-of-speechtagging (Schu?tze and Singer 1994), shallow parsing (Punyakanok and Roth 2001), andnamed entity recognition (Tjong Kim Sang and De Meulder 2003).However, perhaps not surprisingly given the difficulty of the problem, this model istoo simplistic.
In fact, performance deteriorated to an F-score of 37.79%.
We conjecturethat the static probabilities (the model) are too biased and cause the system to abandongood choices obtained from SNoW in favor of worse candidates whose global behavioris better.
For example, the root q.r.n was correctly generated by SNoW as the bestcandidate for the word mqrn, but because P(R3 = r | R2 = r), which is 0.066, is higherthan P(R3 = n | R2 = r), which is 0.025, the root q.r.r was produced instead.Similar examples of interdependencies among radicals abound.
In Hebrew andArabic, some letters cannot appear in a sequence, mostly due to phonetic restrictions.For example, if the first radical is s, the second radical cannot be z, c, or e. Taking intoaccount the dependency between the root radicals is an interesting learning experimentwhich may provide a better results.
We therefore extend the naive HMM approach toaccount for such dependencies, following the PMM model of Punyakanok and Roth(2001).Table 5Recall of identifying the correct radical among top-n candidatesR1 R2 R3top-1: 80.88 70.20 82.50top-2: 92.98 86.99 93.85top-5: 99.14 99.38 99.68top-10: 99.69 99.90 99.70437Computational Linguistics Volume 34, Number 3Consider a specific example of some word w. We already trained a classifier forR1 (the first root radical), so we can look at the predictions of the R1 classifier for w.Assume that this classifier predicts a1, a2, a3, .
.
.
, ak with confidence scores c1, c2, .
.
.
, ckrespectively (the maximum value of k is 22).
For each value ai (1 ?
i ?
k) predicted bythe R1 classifier, we run the R2 classifier where the value of the feature for R1 is ai.
Thatis, we run the R2 classifier k times for each word w (k depends on w).
Then, we checkwhich value of i (where i runs from 1 to k) gives the best sum of the two classifiers, boththe confidence measure of the R1 classifier on ai and the confidence measure of the R2classifier.
This gives a confidence ranking for R2.
We perform the same evaluation for R3,using the results of the R2 classifier as the value of the R3 added feature.
We then selectthe root which maximizes the confidence of R3; selecting the root which maximizes allthree classifiers yields similar results, as shown in Table 6.As the results demonstrate, this is a promising approach which we believe can yieldeven better results with more training data.
However, as we show subsequently, addingmore linguistic knowledge can improve performance even further.5.3 Learning BigramsIn the first experiments of this research, we trained a classifier to learn roots as a singleunit.
As already mentioned, the drawbacks of this approach are the large set of targetsand the sparseness of the training data.
Then, we decoupled the problem into threedifferent classifiers to learn each of the root consonants in isolation and then combinedthe results in various ways.
Training the classifiers in the sequential model, consideringas targets only letters that occurred in the observed word, plus w, i, n, and l, reducedthe number of targets from 22 to approximately 7.
This facilitates a different experimentwhereby bigrams of the root radicals, rather than each radical in isolation, are learned,taking advantage of the reduced number of targets for each radical.
On one hand, theaverage number of targets is still relatively large (about 50), but on the other, we onlyhave to deal with a combination of two classifiers.
In this method, each of the classifiersshould predict two letters at once.
For example, we define one classifier to learn thefirst and second radicals (R1R2), and a second classifier to learn the second and thirdradicals (R2R3).
Alternatively, the first and third radicals can be learned as a single unitby a different classifier.
In this case, we only need to combine this classifier with one ofthe above mentioned classifiers to obtain the complete root.It should be noted that the number of potential roots for a given word examplein combining three different classifiers (for each of the root radicals) is determined bymultiplying the number of targets of each of the classifiers.
In this classification problem,each classifier predicts two root radicals, meaning that the classifiers overlap in oneradical.
This common radical should be identical in the combination (e.g., R1R2 andTable 6Results: Combining dependent classifiers.Maximizing all radicals Maximizing R3 Baseline (Table 4)Precision 76.99 76.87 59.83Recall 84.78 84.78 57.98F-score 80.70 80.63 58.89438Daya, Roth, and Wintner Identifying Semitic RootsTable 7Results: Combining classifiers of root radicals bigram.R1R2&R2R3 R1R2&R1R3 R2R3&R1R3Precision 82.11 79.71 79.11Recall 85.28 86.40 86.64F-score 83.67 82.92 82.71R2R3 overlap in R2), and thus the number of potential roots is significantly reduced.
Theresults of these experiments are depicted in Table 7.5.4 Combining Classifiers using Linguistic KnowledgeSNoW provides a ranking on all possible roots.
We now describe the use of linguis-tic constraints to re-rank this list.
We implemented a function, dubbed the scoringfunction, which uses knowledge pertaining to word-formation processes in Hebrewin order to estimate the likelihood of a given candidate being the root of a given word.The function practically classifies the candidate roots into one of three classes: goodcandidates, which are likely to be the root of the word; bad candidates, which are highlyunlikely; and average cases.It is important to note that the scoring function alone is not a function for extractingroots from Hebrew words.
First, it only scores a given root candidate against a givenword, rather than yield a root given a word.
Although we could have used it exhaus-tively on all possible roots in this case, in a general setting of a number of classifiersthe number of classes might be too high for this solution to be practical.
Second, thefunction only produces three different values; when given a number of candidate rootsit may return more than one root with the highest score.
In the extreme case, whencalled with all 223 potential roots, it returns on average more than 11 candidates whichscore highest (and hence are ranked equally).
The linguistic knowledge employed by thesystem, although significant to the improved performance, is far from being sufficientfor devising a deterministic root extraction algorithm.We now discuss the constraints employed by the scoring function in detail.
In whatfollows, a root r = r1r2r3 is said to be in Paradigm P1 if r1 ?
{w, i,n}; in Paradigm P2 ifr2 ?
{w, i}; in Paradigm P3 if r3 ?
{h, i}; in Paradigm P4 if r2 = r3; and regular if noneof these holds.
The constraints are deterministic, in the sense that they always hold inthe training data.
They can be easily evaluated because determining the paradigm(s) agiven root belongs to is deterministic and efficient.
In the examples, root consonants aretypeset in boldface.1.
If r is regular then r1, r2, r3 must occur in the word in this order.Furthermore, either r1r2r3 are consecutive in the target word, or a singleletter intervenes between r1 and r2 or between r2 and r3 (or both).
Theintervening letter between r1 and r2 can only be w, i, t (if r1 is $ or s),d (if r1 is z), or v (if r1 is c).
The intervening letter between r2 and r3can only be w or i.
For example, hgrywm hmsxri gdl bkt$yh ?xwzim;hhstdrwt hzdrzh lhctlm wlhcvdq.439Computational Linguistics Volume 34, Number 32.
If r is in Paradigm P1 and not in Paradigm P2, P3, or P4, then r2, r3 mustoccur in the word in this order.
Furthermore, either r2r3 are consecutivein the target word, or a single letter intervenes between r2 and r3.
Theintervening letter can only be w or i.
Examples: l$bt (i.$.b); hwdiyh (i.d.y);mpilim (n.p.l).3.
If r is in Paradigm P2 and not in Paradigm P1 or P3, then r1, r3 mustoccur in the word in this order.
Furthermore, either r1r3 are consecutivein the target word, or a single letter intervenes between r1 and r3.
Theintervening letter can only be w, i, t (if r1 is $ or s), d (if r1 is z), or v (if r1 isc).
Examples: hqmt (q.w.m) hmlwn hcviirh (c.i.r) kmbi?h (b.w.?)
rwwxim.4.
If r is in Paradigm P3 and not in Paradigm P1 or P2, then r1, r2 mustoccur in the word in this order.
Furthermore, either r1r2 are consecutivein the target word, or a single letter intervenes between r1 and r2.
Theintervening letter can only be w, i, t (if r1 is $ or s), d (if r1 is z), or v (if r1is c).
Examples: tgliwt (g.l.i); hzdkw (z.k.i).5.
If r is in Paradigm P4 and not in Paradigm P1 or P2, then r1, r2 mustoccur in the word in this order.
Furthermore, either r1r2 are consecutivein the target word, or a single letter intervenes between r1 and r2.
Theintervening letter can only be w, i, t (if r1 is $ or s), d (if r1 is z), or v (if r1is c).
Examples: mgilh (g.l.l); hmginim (g.n.n).6. r must occur in the pre-compiled list of roots.The decision of the function is based on the observation that when a root is regularit either occurs in a word consecutively or with a certain single letter between any twoof its radicals (constraint 1).
The scoring function checks, given a root and a word,whether this is the case.
If the condition holds, the scoring function returns a highvalue.
The weak paradigm constraints (2?5) are assigned a middle score, because insuch paradigms we are limited to a partial check on the root as we only check forthe occurrence of two root radicals in the word.
For roots that are in more than oneparadigm, the scoring function returns an average score as a default value.
We alsomake use in this function of our pre-compiled list of roots.
A root candidate which doesnot occur in the list (constraint 6) is assigned the low score.The actual values that the function returns were chosen empirically by counting thenumber of occurrences of each class in the training data.
Thus, ?good?
candidates makeup 74.26% of the data, hence the value the function returns for ?good?
roots is set to0.7426.
Similarly, the middle value is set to 0.2416 and the low to 0.0155.As an example, consider ki$lwn, whose only possible root is k.$.l.
Here, the correctcandidate will be assigned the high score, because k.$.l is a regular root and its radicalsoccur consecutively in the word with a single intervening letter i between k and $(constraint 1).
The candidate root $.l.i will be assigned a middle score, because this rootis in paradigm P3 and constraint 4 holds.
The candidate root $.l.n will score low, as itdoes not occur in the list of roots (constraint 6).In addition to the scoring function we implemented a simple edit distance functionwhich returns, for a given root and a given word, the inverse of the edit distancebetween the two.
Edit distance (Levenshtein 1965) is the minimum number of characterinsertions and deletions required to transform one string to another.
For example, forhipilw, the (correct) root n.p.l scores 1/5 whereas h.p.l scores 1/3.
The inverse edit440Daya, Roth, and Wintner Identifying Semitic Rootsdistance increases with the similarity between two strings; finer tuning of this similaritymeasure is of course possible, but was not the focus of this work.We then run SNoW on the test data and rank the results of the three classifiers glob-ally, where the order is determined by the product of the three different classifiers.
Thisinduces an order on roots, which are combinations of the decisions of three independentclassifiers.
Each candidate root is assigned three scores: the product of the confidencemeasures of the three classifiers; the result of the scoring function; and the inverseedit distance between the candidate and the observed word.
We rank the candidatesaccording to the product of the three scores (i.e., we give each score an equal weight inthe final ranking).Recall that a given word formmay have several possible roots; our system thereforehas to determine how many roots to produce for each example.
We observed that in the?difficult?
examples, the top ranking candidates are assigned close scores, whereas inthe easier cases, the top candidate is usually scored much higher than the next one.We therefore decided to produce all those candidates whose scores are not much lowerthan the score of the top ranking candidate.
The drop in the score, ?, was determinedempirically on the training set and was set to ?
= 0.4.
With this value for ?, results forthe test data are presented in Table 8.The results clearly demonstrate the added benefit of the linguistic knowledge.Interestingly, even when testing the system on a set of roots which do not occur in thetraining corpus, we obtain an F-score of 65.60%.
This result demonstrates the robustnessof our method.The additional linguistic knowledge is not merely eliminating illegitimate rootsfrom the ranking produced by SNoW.
Using the linguistic constraints encoded in thescoring function only to eliminate roots, while maintaining the ranking proposed bySNoW, yields much lower accuracy.
Specifically, when we use only the list of rootsas the single constraint when combining the three classifiers, thereby implementingonly a filter of infeasible results, we obtain a precision of 65.24%, recall of 73.87%,and an F-score of 69.29%.
Clearly, our linguistically motivated scoring does more thanelimination, and actually re-ranks the roots.
We conclude that it is only the combinationof the classifiers with the linguistically motivated scoring function which boosts theperformance on this task.5.5 Error AnalysisLooking at the questionnaires filled in by our subjects (Section 3.2), it is obvious thathumans have problems identifying the correct roots in two general cases: when theroot paradigm is weak (i.e., when the root is irregular) and when the word can beread in more than one way and the subject chooses only one (presumably, the mostprominent one).
Our system suffers from similar problems: First, its performance onTable 8Results: Using linguistic constraints for inference.System BaselinePrecision 80.90 59.83Recall 88.16 57.98F-score 84.38 58.89441Computational Linguistics Volume 34, Number 3the regular paradigms is far superior to its overall performance; second, it sometimescannot distinguish between several roots which are in principle possible, but only oneof which happens to be the correct one.To demonstrate the first point, we evaluated the performance of the system on adifferent organization of the data.
We tested separately words whose roots are all reg-ular, versus words all of whose roots are irregular.
We also tested words which have atleast one regular root (this group is titled ?mixed?
herein).
As an additional experiment,we extracted from the corpus a sample of 200 ?hard?
words: these are surface forms inwhich either one of the root characters is missing, or two root characters are transposeddue to metathesis.
The results are presented in Table 9, and clearly demonstrate thedifficulty of the system on the weak paradigms, compared to almost 95% on the easier,regular roots.A more refined analysis reveals differences between the various weak paradigms.Table 10 lists F-score for words whose roots are irregular, classified by paradigm.
Ascan be seen, the system has great difficulty in the cases of R2 = R3 and R3 = i.
Refer toTable 2 for the sizes of the different root classes.Finally, we took a closer look at some of the errors, and in particular at cases wherethe system produces several roots where fewer (usually only one) are correct.
Such casesinclude, for example, the word hkwtrt (?the title?
), whose root is the regular k.t.r; butthe system produces, in addition, also w.t.r, mistaking the k to be a prefix.
These are thekinds of errors which are most difficult to fix.However, in many cases the system?s errors are relatively easy to overcome.
Con-sider, for example, the word hmtndbim (?the volunteers?)
whose root is the irregularn.d.b.
Our system produces as many as five possible roots for this word: n.d.b, i.t.d,d.w.b, i.h.d, and i.d.d.
Clearly some of these could be eliminated.
For example, i.t.dshould not be produced, because if this were the root, nothing could explain the pres-ence of the b in the word; i.h.d should be excluded because of the location of the h.Similar phenomena abound in the errors the system makes; they indicate that a moreTable 9Error analysis: Performance of the system on different cases.Regular Irregular Mixed HardNumber of words 2,598 2,019 2,781 200Precision 92.79 60.02 92.54 47.87Recall 96.92 73.45 94.28 55.11F-score 94.81 66.06 93.40 51.23Table 10Error analysis: The weak paradigms.Paradigm F-scoreR1 = i 70.57R1 = n 71.97R2 = i/w 76.33R3 = i 58.00R2 = R3 47.42442Daya, Roth, and Wintner Identifying Semitic RootsTable 11Arabic root ambiguity in the corpus.Number of roots 1 2 3 4 5 6Number of words 28,741 2,258 664 277 48 3careful design of the scoring function can yield still better results, and this is a directionwe intend to pursue in the future.6.
Extension to ArabicAlthough Arabic and Hebrew have a very similar morphological system, being bothSemitic languages, the task of learning roots in Arabic is more difficult than in Hebrew,for the following reasons. There are 28 letters in Arabic which are represented usingapproximately 40 characters in the transliteration of Modern StandardArabic orthography of Buckwalter (2002).
Thus, the learning problem ismore complicated due to the increased number of targets (potential rootradicals) as well as the number of characters available in a word. The number of roots in Arabic is significantly higher.
We pre-compiled alist of 3,822 trilateral roots from Buckwalter?s list of roots, 2,517 of whichoccur in our corpus.
According to our lists, Arabic has almost twice asmany roots as Hebrew. Not only is the number of roots high, the number of patterns in Arabic isalso much higher than in Hebrew. Whereas in Hebrew the only possible letters which can intervene betweenroot radicals in a word are i and w, in Arabic there are more possibilities.The possible intervening letter sequences between r1 and r2 are y, w, A, t,and wA, and between r2 and r3 y, w, A, and A}.7We applied the same methods discussed previously to the problem of learning(Modern Standard) Arabic roots.
For training and testing, we produced a corpusof 31,991 word types (we used the morphological analyzer of Buckwalter 2002 to ana-lyze a corpus of 152,666 word tokens from which our annotated corpus was produced).Table 11 shows the distribution of word types according to root ambiguity.We then trained stand-alone classifiers to identify each radical of the root in iso-lation, using features of the same categories as for Hebrew: location of letters, letterbigrams (independently of their location), and prefixes and suffixes compiled manu-ally from a morphological grammar (Buckwalter 2002).
Despite the rather pessimisticstarting point, each classifier provides satisfying results, as shown in Table 12, probablyowing to the significantly larger training corpus.
The first three columns present theresults of each of the three classifiers, and the fourth column is a straightforwardcombination of the three classifiers.7 ?}?
is a character in Buckwalter?s transliteration.443Computational Linguistics Volume 34, Number 3Table 12Accuracy of identifying the correct radical in Arabic.R1 R2 R3 rootPrecision 86.02 70.71 82.95 54.08Recall 89.84 80.29 88.99 68.10F-score 87.89 75.20 85.86 60.29We combined the classifiers using linguistic knowledge pertaining to word-formation processes in Arabic, by implementing a function that approximates the like-lihood of a given candidate to be the root of a given word.
The function actually checksthe following cases: If a root candidate is indeed the root of a given word, then we expectit to occur in the word consecutively or with one of {y, w, A, t, wA}intervening between R1 and R2, or with one of { y, w, A, A} } betweenR2 and R3 (or both). If a root candidate does not occur in our pre-compiled list of roots, itcannot be a root of any word in the corpus.We suppressed the constraints of weak paradigms in the Arabic experiments, be-cause in such paradigms we are limited to a partial check on the root as we only checkfor the occurrence of two root radicals instead of three in the word.
This limitation seemsto be crucial in Arabic, considering the fact that the number of roots is much higherand, in addition, there are more possible intervening letter sequences between the rootradicals.
Consequently, more incorrect roots are wrongly extracted as correct ones.
Ofcourse, this is an over-simplistic account of the linguistic facts, but it serves our purposeof using very limited and very shallow linguistic constraints on the combination ofspecialized ?expert?
classifiers.
Table 13 shows the final results.The Arabic results are slightly worse than the Hebrew ones.
One reason is that inHebrew the number of roots is smaller than in Arabic (2,152 vs. 3,822), which leavesmuch room for wrong root selection.
Another reason might be the fact that in Arabicword formation is amore complicated process, for example by allowingmore charactersto occur in the word between the root letters as previously mentioned.
This may havecaused the scoring function to wrongly tag some root candidates as possible roots.7.
Improving Local Classifiers by Applying Global ConstraintsIn Section 5 we presented several methods addressing the problem of learning roots.
Ingeneral, we trained stand-alone classifiers, each predicting a different root component,Table 13Results: Arabic root identification.Precision 78.21Recall 82.80F-score 80.44444Daya, Roth, and Wintner Identifying Semitic Rootsin which the decision for the complete root depends on the outcomes of these differentbut mutually dependent classifiers.
The classifiers?
outcomes need to respect someconstraints that arise from the dependency between the root radicals, requiring a levelof inference on top of the predictions, which is implemented by the scoring function(Section 5.4).In this section we show that applying global constraints, in the form of the scor-ing function, not only improves global decisions but also significantly improves thelocal classification task.
Specifically, we show that the performance of identifying eachradical in isolation improves after the scoring function is applied.
In this experimentwe trained each of the three radical classifiers as previously described, and thenapplied inference to re-rank the results.
The combined classifier now predicts thecomplete root, and in particular, induces a new local classifier decision on each ofthe radicals which, due to re-ranking, may differ from the original prediction of thelocal classifiers.Table 14 shows the results of each of the radical classifiers after inference with thescoring function.
There is a significant improvement in each of the three classifiers afterapplying the global constraints (Table 14; cf.
Table 4).
The most remarkable improve-ment is of the R2 classifier.
The gap between R2 and other classifiers, as stand-aloneclassifiers with no external knowledge, is 10?12%, due to linguistic reasons.
Now, afteremploying the global constraints, the gap is reduced to only 4%.
In such scenarios,global constraints can significantly aid local classification.Because the most dominant constraint is the occurrence of the candidate root inthe pre-compiled list of roots, we examined the results of applying only this constrainton each of the three classifiers, as a single global constraint.
Although there is animprovement in all classifiers, as shown in Table 15, applying this single constraint stillperforms worse than applying all the constraints mentioned in Section 5.4.
Again, weconclude that re-ranking the candidates produced by the local classifiers is essential forimproving the accuracy, and filtering out infeasible results is not sufficient.Finally, to further emphasize the contribution of global inference to local classifi-cation, we repeated the same experiment, measuring accuracy of each of the radicalclassifiers induced by the root identification system, for Arabic.
The results are listedTable 14Accuracy of each classifier after applying global constraints.R1 R2 R3Precision 89.67 84.7 89.27Recall 93.08 90.17 93.16F-score 91.34 87.35 91.17Table 15Accuracy of each classifier applying the list of roots as a single constraint.R1 R2 R3Precision 86.33 78.58 83.63Recall 88.59 83.75 88.82F-score 87.45 81.08 86.15445Computational Linguistics Volume 34, Number 3Table 16Accuracy of each classifier after applying global constraints (Arabic).R1 R2 R3Precision 90.41 84.40 87.92Recall 92.90 89.59 92.19F-score 91.64 86.92 90.01in Table 16, and show a significant improvement over the basic classifiers (compare toTable 12).8.
ConclusionsWe have shown that combining machine learning with limited linguistic knowledgecan produce state-of-the-art results on a difficult morphological task, the identificationof roots of Semitic words.
Our best result, over 80% accuracy, was obtained using simpleclassifiers for each of the root?s consonants, and then combining the outputs of theclassifiers using a linguistically motivated, yet extremely coarse and simplistic, scoringfunction.This work can be improved in a variety of ways.
As is well known from other learn-ing tasks, fine-tuning of the feature set can produce additional accuracy; we expect thisto be the case in this task, too.
In particular, introducing features that capture contextualinformation is likely to improve the results.
Similarly, our scoring function is simplisticandwe believe that it can be improved.
The edit-distance function can be improved suchthat the cost of replacing characters reflect phonological and orthographic constraints(Kruskal 1999).
Other, learning-based, re-ranking methods can also be used to improvethe results.There are various other ways in which different inter-related classifiers can becombined.
Here we only used a simple multiplication of the three classifiers?
confidencemeasures, which is then combined with the linguistically motivated functions.
Weintend to investigate more sophisticated methods for this combination.Finally, we plan to extend these results to more complex cases of learning tasks witha large number of targets, in particular such tasks in which the targets are structured.Weare currently working on morphological disambiguation in languages with non-trivialmorphology, which can be viewed as a part-of-speech tagging problem with a largenumber of tags on which structure can be imposed using the various morphologicaland morpho-syntactic features that morphological analyzers produce.AcknowledgmentsPrevious versions of this work werepublished as Daya, Roth, and Winter (2004,2007).
This work was supported by TheCaesarea Edmond Benjamin de RothschildFoundation Institute for InterdisciplinaryApplications of Computer Science at theUniversity of Haifa and the Israeli Ministryof Science and Technology, under theauspices of the Knowledge Center forProcessing Hebrew.
Dan Roth is supportedby NSF grants CAREER IIS-9984168, ITRIIS-0085836, and ITR-IIS 00-85980.
Weare grateful to Ido Dagan, Alon Lavie,and Idan Szpektor for useful comments.We benefitted greatly from useful andinstructive comments by three reviewers.ReferencesAbu-Salem, Hani, Mahmoud Al-Omari,and Martha W. Evens.
1999.
Stemmingmethodologies over individual querywords for an Arabic information retrieval446Daya, Roth, and Wintner Identifying Semitic Rootssystem.
Journal of the American Society forInformation Science, 50(6):524?529.Al-Kharashi, Ibrahim A. and Martha W.Evens.
1994.
Comparing words, stems,and roots as index terms in an Arabicinformation retrieval system.
Journal of theAmerican Society for Information Science,45(8):548?560.Banko, Michele and Eric Brill.
2001.
Scalingto very very large corpora for naturallanguage disambiguation.
In Proceedings ofthe 39th Annual Meeting of the Association forComputational Linguistics, pages 26?33,Morristown, NJ.Beesley, Kenneth R. 1998a.
Arabicmorphological analysis on the internet.In Proceedings of the 6th InternationalConference and Exhibition on Multi-lingualComputing, Cambridge, UK.Beesley, Kenneth R. 1998b.
Arabicmorphology using only finite-stateoperations.
Proceedings of the Workshopon Computational Approaches to SemiticLanguages, pages 50?57, Montreal, Quebec.Buckwalter, Tim.
2002.
Buckwalter Arabicmorphological analyzer.
Linguistic DataConsortium (LDC) catalog numberLDC2002L49 and ISBN 1-58563-257-0.Carlson, Andrew J., Chad M. Cumby, Jeff L.Rosen, and Dan Roth.
1999.
The SNoWlearning architecture.
Technical ReportUIUCDCS-R-99-2101, UIUC ComputerScience Department.Choueka, Yaacov.
1990.
MLIM?A system forfull, exact, on-line grammatical analysisof Modern Hebrew.
In Proceedings of theAnnual Conference on Computers inEducation, page 63, Tel Aviv.
[In Hebrew.
]Darwish, Kareem.
2002.
Building a shallowArabic morphological analyzer in oneday.
In Computational Approaches toSemitic Languages, an ACL?02 Workshop,pages 47?54, Philadelphia, PA.Darwish, Kareem and Douglas W. Oard.2002.
Term selection for searching printedArabic.
In SIGIR ?02: Proceedings of the 25thAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval, pages 261?268, New York, NY.Daya, Ezra, Dan Roth, and Shuly Wintner.2004.
Learning Hebrew roots: Machinelearning with linguistic constraints.
InProceedings of EMNLP?04, pages 357?364,Barcelona, Spain.Daya, Ezra, Dan Roth, and Shuly Wintner.2007.
Learning to identify Semitic roots.In Abdelhadi Soudi, Guenter Neumann,and Antal van den Bosch, editors, ArabicComputational Morphology: Knowledge-basedand Empirical Methods, volume 38 of Text,Speech and Language Technology.
Springer,New York, pages 143?158.Even-Shoshan, Abraham.
1993.
HaMillonHaXadash (The New Dictionary).
KiryatSefer, Jerusalem.
In Hebrew.Even-Zohar, Yair and Dan Roth.
2001.A sequential model for multi classclassification.
In EMNLP-2001, the SIGDATConference on Empirical Methods in NaturalLanguage Processing, pages 10?19,Pittsburgh, PA.Florian, Radu.
2002.
Named entityrecognition as a house of cards: Classifierstacking.
In Proceedings of CoNLL-2002,pages 175?178, Taiwan.Golding, Andrew R. and Dan Roth.1999.
A Winnow based approach tocontext-sensitive spelling correction.Machine Learning, 34(1?3):107?130.Habash, Nizar and Owen Rambow.2005.
Arabic tokenization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.In Proceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics (ACL?05), pages 573?580,Ann Arbor, MI.Kruskal, Joseph.
1999.
An overview ofsequence comparison.
In David Sankoffand Joseph Kruskal, editors, Time Warps,String Edits and Macromolecules: The Theoryand Practice of Sequence Comparison.
CSLIPublications, Stanford, CA, pages 1?44.Larkey, Leah S., Lisa Ballesteros, andMargaret E. Connell.
2002.
Improvingstemming for Arabic information retrieval:Light stemming and co-occurrenceanalysis.
In SIGIR ?02: Proceedings of the25th Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval, pages 275?282,New York, NY.Levenshtein, Vladimir I.
1965.
Binary codescapable of correcting deletions, insertionsand reversals.
Doklady Akademii NaukSSSR, 163(4):845?848.McCarthy, John J.
1981.
A prosodic theory ofnonconcatenative morphology.
LinguisticInquiry, 12(3):373?418.Ornan, Uzzi.
2003.
The Final Word.
Universityof Haifa Press, Haifa, Israel.
[In Hebrew.
]Owens, Jonathan.
1997.
The Arabicgrammatical tradition.
In Robert Hetzron,editor, The Semitic Languages.
Routledge,London and New York, chapter 3,pages 46?58.Punyakanok, Vasin and Dan Roth.
2001.
Theuse of classifiers in sequential inference.
In447Computational Linguistics Volume 34, Number 3NIPS-13; The 2000 Conference on Advances inNeural Information Processing Systems 13,pages 995?1001, Denver, CO.Punyakanok, Vasin, Dan Roth, and Wen-TauYih.
2005.
The necessity of syntacticparsing for semantic role labeling.
InProceedings of IJCAI 2005, pages 1117?1123,Edinburgh.Roth, Dan.
1998.
Learning to resolve naturallanguage ambiguities: A unified approach.In Proceedings of AAAI-98 and IAAI-98,pages 806?813, Madison, WI.Schu?tze, Hinrich and Yoram Singer.
1994.Part-of-speech tagging using a variablememory Markov model.
In Proceedingsof the 32nd Annual Meeting of theAssociation for Computational Linguistics,pages 181?187, Las Cruses, NM.Shimron, Joseph, editor.
2003.
LanguageProcessing and Acquisition in Languages ofSemitic, Root-Based, Morphology.
Number 28in Language Acquisition and LanguageDisorders.
John Benjamins, Amsterdam.Tjong Kim Sang, Erik F. and FienDe Meulder.
2003.
Introductionto the CoNLL-2003 shared task:Language-independent named entityrecognition.
In Proceedings of CoNLL-2003,pages 142?147, Edmonton, Canada.Yona, Shlomo and Shuly Wintner.
2008.
Afinite-state morphological grammar ofHebrew.
Natural Language Engineering,14(2):173?190.Zdaqa, Yizxaq.
1974.
Luxot HaPoal [TheVerb Tables].
Kiryath Sepher, Jerusalem.
[In Hebrew.
]448
