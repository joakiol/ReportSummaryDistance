Named Entity Learning and Verification:Expectation Maximization in Large CorporaUwe QUASTHOFF, Christian BIEMANN, Christian WOLFFCS Institute, Leipzig UniversityAugustusplatz 10/11Leipzig, Germany, 04109AbstractThe regularity of named entities is used to learnnames and to extract named entities.
Having onlya few name elements and a set of patterns the al-gorithm learns new names and its elements.
Averification step assures quality using a largebackground corpus.
Further improvement isreached through classifying the newly learntelements on character level.
Moreover, unsuper-vised rule learning is discussed.1 IntroductionThe task of recognizing person names in textcorpora is well examined in the field of Infor-mation Extraction.
Most of the approaches, us-ing machine-learning or statistical methodsmake excessive use of annotated data or largegazetteers.
We present a method that needs littleinput knowledge and performs unsupervisedlearning on unlabeled data, restricting ourselvesto person names.In most languages, named entities form regularpatterns.
Usually, a surname has a precedingfirst name, which in turn might have a precedingtitle or profession.
Similar rules hold for differ-ent kinds of named entities consisting of morethan one word.
Moreover, some elements ofsuch multiwords (like president) are high fre-quency items, well known and of high signifi-cance for identifying a named entity.
On theother hand, there are elements that often appearin named entities, but are not characteristic forthem (like the first name Isra l).Therefore, a verification step is included in thelearning algorithm.2 The AlgorithmOur learning algorithm starts with a set of pat-terns and initial name elements.
A large corpusof more then 10 million sentences [cf.
Quasthoff& Wolff 2000], taken from newspapers of thelast 10 years is used for both, the identificationof candidates for new name elements as well asfor verifying the candidates found.
The algo-rithm stops, if no more new name elements arefound.The algorithm implements xpectation maximi-zation (EM) [cf.
Dempster, 1977, Collins, 1999]in the following way: The combination of alearning step and a verification step are iterated.If more name elements are found, the recall ofthe verification step increases.
The key propertyof this algorithm is to assure high precision andstil  get massive recall.From another point of view our algorithm im-plements bootstrapping [cf.
Riloff 99], as itstarts from a small number of seed words anduses knowledge found during the run to findmore candidates.2.1 Patterns and Pattern RulesIn a first step the text to be analysed is tagged inhe ollowing way: We have two types of tags.The first type is problem dependent.
In the caseof persons, we have tags for title or profession(TI), first name (FN) and surname (LN).
Thesecond tag set is problem independent, but lan-guage dependent.
In our experiments, wemarked words as lower case (LC) or upper case(UC) depending on the first letter.
Punctuationmarks are marked as PM, determiners as DET.Words can have multiple tags, e.g.
UC and FNat the same time.The next step is to find tag sequences which aretypical for names, like TI-FN-LN .
From here,we can create rules likeTI-UC-LN ?
TI-FN-LN ,which means that an upper case word betweentitle and last name is a candidate for a first name.An overview of handmade start rules is given inappendix 1.Looking at the rules, it is possible to argue that arule like UC-LN ?
FN-LN  is a massive over-generalization.
This would be true if we wouldlearn new name elements simply by applyingrules.
However, the verification step ensures thatfalse friends are eliminated at high rate.2.2 The Outer LoopThe algorithm is described as follows:Load pattern rules.Let unused name elements = in i tialset of name elementsLoop:For each unused name entityDo the learning step andcollect new cand i datesFor each new candidateDo the verification stepOutput verified candidatesLet unused name elements =ver i fied candidates2.3 The Learning Step: Finding Candi-datesUsing the pattern rules and the current nameelements, new candidates are searched.
Here weuse the corpus.Search 255 random sentences co n-taining the unused name entity (orall, if <255).Use the pattern rules to identifynew candidates as d escribed above.2.4 The Verification StepIn a verification step, each candidate is testedbefore it is used to generate new candidates.
Wetest the following property: Does the candidateappear often enough together with verified nameelements?
Again, we use the corpus.Search 30 random sentences contai n-ing the name element to be ver i -fied (or all, if <30).If the ratio fulfilling at leastone right side of a pattern ruleis above some threshold, the ca n-didate is a ccepted.3 The Exhaustion CycleThe overall performance of the algorithm can beestimated as follows: For simplicity let us as-sume that the average number of items (in ourtask: name elements) findable by any unuseditem equals N. Then the number of items startsto grow exponentially.
Sooner or later, the totalnumber of unseen entities decreases.
Hence,most of the N items found are known already.The numbers of new items found in each turndecreases, until no more items can be reached.So we discriminate between a phase of growthand a phase of exhaustion.The following figures visualize the number ofnew items per turn and the accumulated totalnumber of items for each turn.
Data was takenfrom an experiment with 19 items of knowledge(see appendix 2).
The test was performed on theGerman corpus and designed to find first andlast names only.
The phase of growth lasts untilthe 5th cycle, then exhaustion takes over, as canbe seen in figure 11.0500010000150002000025000300000 1 2 3 4 5 6 7 8 9 1 0 1 1cycleitemsNew Items Total ItemsFigure 1: total and new items vs. cycles21 Additional runs with more start items produced thesame amount of  total items in less cycles.2 Note that 25'000 name elements are responsible forthe detection of over 150'000 full names.Natural growth in the number of items takesplace under the following conditions:?
Sufficient size of corpus?
Sufficient frequency of start items?
Suitable relation, e.g.
namesIf the corpus is not large enough or it is not pos-sible to find enough candidates from the startitems, exhaustion takes place immediately.4 ExamplesLet us closely examine the learning of items byexample: From the known first name John, thecandidate for being a last name H uberg wasfound in the fragment "...by John Hauberg and.."by the rule FN-UC-LC => FN-LN-LC  andverified in occurrences like "Robert Hauberg,...", "Robert Hauberg urges..." using the alreadyknown first name Robert.Errors occur in the case of words, which aremainly used in positions which are often occu-pied by first names.
In German, the algorithmextracts and verifies "?ra" (e ) and "Transport-panzer" (Army transportation tank) because ofthe common usage "?ra Kohl" and the propername "Transportpanzer Fuchs" (fox tank).
In thecase of "?ra", this false first name supports theclassifications of the proper last names Hinrichs,Strau?, Bangemann, Albrecht, Gorbatchow,Jelzin and many more.5 Precision and Recall5.1 PrecisionNote that precision will be different for the dif-ferent types of name elements.
Usually surnamesare recognized with high precision.
First namesmay be confused with titles, for instance.Moreover, precision is language dependentmainly due to the different usage of capital let-ters: In German, nouns start with capital lettersand can much easier be confused with names.For German first names in the run mentionedabove, the algorithm yields a precision of84.1%.
Noise items mainly are titles and profes-sion names, which are spelled with a capitalletter in German.
Using the additional fact thatfirst names usually do not exceed 10 letters inlength, the precision for first names rose to92.7%.For last names, results were excellent with aprecision of more than 99%.
The same holds fortitles, as further experiments showed.The ratio number of first names vs. number oflast names happens to be about 1:3, overall pre-cision for German scored 97.5%.Because of the fewer capitalized words in Eng-lish the precision for English first names ishigher, scoring 92.6% without further filtering.Overall precision for English first and last nameswas 98.7%.5.2 RecallRecall mainly depends on the pattern rules used.The experiments were performed with the 14handmade rules given in appendix 1, whichsurely are not sufficient.Calculating the recall is not at all straightfor-ward, because we do not know how many namesare contained in our corpora and experiments onsmall corpora fail to show the natural growth ofitems described in the previous section.
Further,recall will rise with a growing knowledge size.So we modified the algorithm in a way that ittakes plain text as input, applies the rules to findcandidates and checks them in the big corpus.Providing a large set of knowledge items, in anexperiment processing 1000 sentences, 71.4% ofthe person names were extracted correctly.To increase the coverage of the rules it is possi-ble to add rules manually or start a process ofrule learning as described below.5.3.
Propagation of ErrorsDuring the run the error rate increases due tofinding candidates and verification through mis-classified items.
However, as the "era" example(see section 4) illustrates, misclassified itemssupport the classification of goal items.The amount of deterioration highly depends onthe pattern rules.
Strict rules mean low recall buthigh precision, whereas general rules havegreater coverage but find too much, resulting ina trade-off between precision and recall.Table 1 shows the error rate for first names forthe illustrated run (see section 3) over the courseof time.From this we conclude that the algorithm is ro-bust against errors and the quality of the classifi-cations remains relatively stable during the runwhen using appropriate rules.total itemsintervalPrecision forFN withoutlength filterPrecision forFN withlength filter1-1000 87.1% 93.8%1001-2000 90.0% 95.3%4001-5000 88.1% 97.1%9001-10000 83.2% 94.4%19001-20000 83.7% 91.2%21001-22000 86.2% 92.4%24001-25000 83.0% 87.9%Table 1: Propagation of Errors6 Classification on character levelIn German, most words misclassified as firstnames were titles and professions.
While theycannot be distinguished by the rules used, theydiffer strongly from the morphological view.German titles are usually longer because theyare compounds, and parts of compounds areused very frequently.In this section, we introduce a m thod to distin-guish between titles and first names at characterlevel, using the fact that the formation of wordsfollows language-dependent rules.This procedure is implemented in the followingclassifier A: Assume the property we are inter-ested in is visible at the ending of a word (this isbasically true for different word classes in lan-guages like English, French or German).
Webuild a decision tree [cf.
McCarthy & Lehnert1995] reading the words character-by-character,starting from the end.
We stop if the feature isuniquely determined.Moreover, we could as well start from the be-ginning of a word (classifier B).
Finally, we canuse any connected substring of the word insteadof substrings containing the end or the beginning(classifier C).If the training set is large enough and the algo-rithm of the classifier is appropriate, it will coverboth general rules as well as many exceptions.Classifier A and B only differ on the direction aword is analyzed.
We build decision trees withadditional default child nodes as follows.6.1 Classifier A: Considering PrefixesStep 1:Building the ordinary decision tree:Given the training word list we constructa prefix tree [cf.
Gusfield 1999, Navarro2001:38ff].
The leaves in the tree corre-spond to word endings; here we store thefeature of the corresponding word.Step 2:Reduction of the decision tree: If allchildren of a given node have the samefeature, this feature is lifted to the parentnode and the children are deleted.Step 3:Insertion of default features: If a nodedoes not yet have a feature, but one ofthe features is very dominant (say, pres-ent in 80% of the children), this featurewill be assigned as default feature.For classification, the decision tree is used asfollows:Step 1:Searching the tree: Reading the givenword from left to right we follow thetree as far as possible.
The reading proc-ess stops in a certain node N.Step 2:Classification: If the node N has an as-signed feature F then return F. Other-wise return no decision.Figure 2 shows a part of the decision tree builtusing first names Theoardis, Theobald, Theo-derich, Theodor, Theresa, Therese, ?
and thesingular title Theologe (which should be the onlytitle in our training list starting with Theo).
As aresult, all children of Theo will be first names;hence they get the feature firstname.
The nodeTheologe gets the feature titl .This turns out to be singular; hence their parentTheo gets the default feature firstname.
As aconsequence, Theophil will correctly be classi-fied as firstname, while the exception Theologewill still be classified as title.Theologe(TI)Theodor(FN)Theobald(FN)Theo [default](FN)......T A Z(root)Figure 2: Prefix Decision Tree for Proper NamesAs mentioned above, algorithm B works thesame way as algorithm A, using suffixes insteadof prefixes for the decision tree.6.2 Classifier C: Considering SubstringsInstead of concentrating on prefixes or suffixes,we consider all relevant continuous substrings ofa given word.
Unfortunately, there is no naturaltree structure for this set.
Hence, we will con-struct a decision list without default features.Given is a training list containing pairs (word,feature):Construction of the decision listStep 1:Collect all substring information.
Weproduce the following list L: For allpairs (wordN, featureN) from the train-ing list we generate all possible pairs ofthe kind (continuous substring ofwordN, featureN).
If wordN has lengthn, we have n(n+1)/2 continuous sub-strings.
Finally the list is sorted alpha-betically and duplicates are removed.Step 2:Removing contradictions: If a substringoccurs with more then one feature, theselines are deleted from L.Step 3:Removing tails: If a certain string nowhas a unique feature, all extensions ofthis string should have the same featureand the corresponding entries are re-moved from L.For classification, the decision list is used asfollows:Step 1:Look-up of substrings: For a word to beclassified we generate its continuoussubstrings and collect their features fromL.Step 2:Classification: If all collected featuresare equal, then return this feature.
Oth-erwise, return no decision.6.3 Properties of the classifiersIn the following, we assume that the classifiersare trained with non-contradictory data.
Theclassifiers now have the following properties:?
The classifiers reproduce the results given inthe training set.
Hence, they can also betrained with rare exceptions.?
It is necessary to have a training set coveringall aspects of the data, otherwise the deci-sion tree will be confused.?
It is appropriate to return no decision if theclassifier stops in the decision tree at a pointwhere children have mixed features.Bagging [cf.
Breiman 1996] the three classifiers,we achieved a precision of 94.7% with 94.5%recall, using merely a training set of 1368 exam-ples on a test set of 683 items, distinguishingbetween the three classes:?
First name (FN)?
Title (TI)?
None of these.This method of postprocessing is applicable toall features visible by the three classifiers, whichare:?
Features represented by word suffixes orprefixes like inflection and some word for-mation rules.?
Words carrying the same feature if they aresimilar as strings.
Candidates are all kinds ofproper names, as well as distinguishingparts-of-speech.?
Words of languages for special purposes,which are often built by combining partswhere some of them are very typical for agiven domain.
Examples are chemical sub-stances, professions and titles, or industrialgoods.7 Rule LearningUnlike most tasks in Inductive Logic Program-ming (ILP) [cf.
Dzeroski, 2001], our methodneeds rules-of-thumb that find many candidateslike in boosting [cf.
Collins, 1999], rather then arule precision of 100%.For automatic rule induction we used a trainingset of 236 sentences found automatically bytaking sentences containing known first namesand last names from the corpus.
After excessiveannotation, all possible rules were built accord-ing to the contexts of known items and after-wards tested on the training set.
To avoid rulestoo general like UC-UC?FN-UC, the patternshad to contain at least one problem specific tag(i.e.
FN, LN, TIT).
The rules performing above acertain precision threshold (in our experimentswe used 0.7) were taken as input for our algo-rithm.We obtained 106 rules for first names, 67 forlast names and 4 for titles, ranging from veryspecific rules likePM-PM-UC-LN ?
PM-PM-FN-LNto very general ones likeTI-UC ?
TI-FN.In the table below some rules found by auto-matic induction are shown.Rule example contextFN-UC-LN?
FN-FN-LNHerbert ArchibaldMillerFN-LC-FN-UC?
FN-LC-FN-LNIlse und Maria Bode-mannUC-UC-LN?
UC-FN-LNPr?sident Bill ClintonFN-FN-UC?
FN-FN-LNHans Christian A -dersonTI-PM-UC-UC?
TI-FS-FN-UCDr.
Helmut KohlTable 2: Rules Found by Automatic InductionUsing those rules as input for our algorithm, wegained both, higher recall as well as higher pre-cision compared to the handmade rules whenstarting with the same knowledge.
Table 3shows precision rates for the three classes ofname elements, data was taken from a run with19 start elements, the length filter for first nameswas applied, and the string classifiers were not.Due to less strict rules, precision decreases.total itemsintervalPrec.
FN Prec.
LN Prec.
TIT1-1000 94,6% 99,6% 100%1001-2000 94,8% 98,6% 100%2001-3000 94,7% 98,4% 100%4001-5000 84,7% 99,1% 100%9001-10000 86,6% 98,6% 100%24001-2500074,0% 89,7% 100%Table 2: Propagation of errors for inferred rulesPercentage of first name items from the numberof total items was 23,3%, last name items made75,2% of total items and title items yielded only1,4%, because to the low number of title rules.8 Future workDespite of the good results when using inferredrules as described above for our algorithm, wehope to improve the method as a whole withrespect to the size of the input knowledge.Natural growth behaviour can be observed fromsome 10 frequent start items, the string classifierrequires a couple of hundred words for trainingwhereas rule learning needs some 200 fully an-notated sentences containing names.
Experi-ments with sparsely annotated training sentences(100 knowledge items) yielded too specific andtoo weak rules with poor performance w.r.t.recall.Another possibility would be to start with asmall set of seed rules [cf.
Riloff 1999] and toconstruct-by-example and rate rules during theclassification task.Another interesting issue is the understanding ofrelations suitable for this method from a the-retical viewpoint.9 AcknowledgementsThe authors would like to thank Martin L?uterfor providing, implementing and testing thethree string classifiers.10 ReferencesApte, C.; Damerau, F.; Weiss, S. M. (1998) TextMining with Decision Trees and Decision Rules.Proc.
Conference on Automated Learning and Dis-covery, Carnegie-Mellon University, June 1998.Breiman, L. (1996) Bagging Predictors, MachineLearning, Vol.
24, No.
2, pp.
123-140Califf, M. E.; Mooney, R. J.
(1997) RelationalLearning of Pattern-match Rules for InformationExtraction.
Working Papers of the ACL-97 Work-shop in NLP, 1997, 6-11.Collins, M.; Singer, Y.
(1999) Unsupervised Modelsfor Named Entity Classification.
In: Proc.
Of theJoint SIGDAT Conference on Empirical Methodsin Natural Language Processing and very LargeCorpora.Dempster, A.P.
; Laird, N. M.; Rubin, D.B.
(1977)Maximum Likelihood from Incomplete Data via theEM Algorithm, Journal of the Royal Statistical So-ciety, Ser B, 39, 1-38.Dzeroski, S.; Lavrac, N. (2001) Introduction to In-ductive Logic Programming.
I  Saso Dzeroski andNada Lavrac, editors, Relational Data Mining,pages 48-73.
Springer-Verlag, BerlinFreitag, D. (1998) Multistrategy Learning for Info-mation Extraction.
Proc.
15th International Conf.on Machine Learning, 161-169.Gusfield, Dan (1999) Algorithms on Strings, Trees,and Sequences.
Cambridge University Press, UK.McCarthy, J.; Lehnert, W. (1995) Using DecisionTrees for Coreference Resolution.
In: Mellish, C.(ed.)
(1995).
Proc.
Fourteenth International Con-ference on Artificial Intelligence, 1050-1055.Nahm, U. Y.; Mooney, R. J.
(2002) Text Mining withInformation Extraction.
To appear in AAAI 2002Spring Symposium on Mining Answers from Textsand Knowledge Bases, Stanford, CA.Navarro, G. (2001) A guided tour to approximatestring matching.
ACM Computing Surveys 33(1)(2001), 31-88.Ng, H.; Lee, H. (1996) Integrating Multiple Knowl-edge Sources to Disambiguate Word Sense: An Ex-emplar-Based Approach.
Proc.
of the 34th AnnualMeeting of the ACL, 40-47.Quasthoff, U.; Wolff, Ch.
(2000) An Infrastructurefor Corpus-Based Monolingual Dictionaries.
Proc.LREC-2000.
Second International Conference onLanguage Resources and Evaluation.
Athens, May/ June 2000, Vol.
I, 241-246.Riloff, E.; Jones, R. (1999) Learning Dictionaries forInformation Extraction by Multi-Level Bootstrap-ping.
Proceedings of the sixteenth National Con-ference on Artificial Intellig nce (AAAI-99)Roth, D. (1998) Learning to Resolve Natural Lan-guage Ambiguities: A Unified Approach.
Pro .
ofthe American Association of Artificial Intelligence,806-813.Witten, I. H.; Frank, E. (1999) Data Mining: Practi-cal Machine Learning Tools and Techniques withJava Implementations.
San Francisco, CA: MorganKaufman.Appendix 1: Initial Handmade Rule SetUC-LN ?
FN-LNPM-FN-PM-UC ?
PM-FN-PM-FNTI-PM-UC-LN ?
TI-PM-FN-LNFN-LN-PM-UC-LN ?
FN-LN-PM-FN-LNFN-UC-PM ?
FN-LN-PMFN-UC-LC ?
FN-LN-LCTI-UC-LC ?
TI-LN-LCTI-PM-UC-LC ?
TI-PM-LN-LCLN-PM-FN-UC-PM ?
LN-PM-FN-LN-PMUC-PM-FN-LN ?
TI-PM-FN-LNUC-PM-LN ?
TI-PM-LNDET-UC-FN-LN ?
DET-TI-FN-LNDET-UC-FN-FN-LN ?
DET-TI-FN-FN-LNDET-UC-LN ?
DET-TI-LNNote that the last three rules are specific for Germanbecause titles are in upper case in this language.Appendix 2: 19 Start items used in theexperimentsName elment ClassSchmidt LNReuter LNWagner LNSch?uble LNVogts LNHoffmann LNSchulz LNM?ller LNMeyer LNBeck LNMichael FNThomas FNKlaus FNWolfgang FNHans FNWerner FNMartin FNWalter FNKarl FN
