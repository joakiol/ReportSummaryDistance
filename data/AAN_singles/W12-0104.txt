Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 30?37,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsAn Empirical Evaluation of Stop Word Removalin Statistical Machine TranslationChong Tze YuangSchool of Computer Engi-neering, Nanyang Technolo-gical University, 639798Singaporetychong@ntu.edu.sgRafael E. BanchsInstitute for Infocomm Re-search, A*STAR, 138632,Singaporerembanchs@i2r.a-star.edu.sgChng Eng SiongSchool of Computer Engi-neering Nanyang Technolo-gical University, 639798Singapore.aseschng@ntu.edu.sgAbstractIn this paper we evaluate the possibility ofimproving the performance of a statisticalmachine translation system by relaxing thecomplexity of the translation task by remov-ing the most frequent and predictable termsfrom the target language vocabulary.
After-wards, the removed terms are inserted backin the relaxed output by using an n-grambased word predictor.
Empirically, we havefound that when these words are omittedfrom the text, the perplexity of the text de-creases, which may imply the reduction ofconfusion in the text.
We conducted somemachine translation experiments to see ifthis perplexity reduction produced a bettertranslation output.
While the word predic-tion results exhibits 77% accuracy in pre-dicting 40% of the most frequent words inthe text, the perplexity reduction did nothelp to produce better translations.1 IntroductionIt is a characteristic of natural language that alarge proportion of running words in a corpuscorresponds to a very small fraction of the voca-bulary.
An analysis of the Brown Corpus hasshown that the hundred most frequent wordsaccount for 42% of the corpus, while only 0.1%in the vocabulary.
On the other hand, words oc-curring only once account merely 5.7% in thecorpus but 58% in the vocabulary (Bell et al1990).
This phenomenon can be explained interms of Zipf?s Law, which states that the prod-uct of word ranks and their frequencies approx-imates a constant, i.e.
word-frequency plot isclose to a hyperbolic function, and hence the fewtop ranked words would account for a great por-tion of the corpus.
Also, it appears that the topranked words are mainly function words.
Forinstance, the eight most frequent words in theBrown Corpus are the, of, and, to, a, in, that andis (Bell et al 1990).It is a common practice in Information Re-trieval (IR) to filter the most frequent words outfrom processed documents (which are referred toas stop words), as these function words are se-mantically non-informative and constitute weakindexing terms.
By removing this great amountof stop words, not only space and time complexi-ties can be reduced, but document content can bebetter discriminated by the remaining contentwords (Fox, 1989; Rijsbergen, 1979; Zou et al,2006; Dolamic & Savoy 2009).Inspired by the concept of stop word removalin Information Retrieval, in this work we studythe feasibility of stop word removal in StatisticalMachine Translation (SMT).
Different from In-formation Retrieval, that ranks or classifies doc-uments; SMT hypothesizes sentences in targetlanguage.
Therefore, without explicitly removingfrequent words from the documents, we proposedto ignore such words in the target language vo-cabulary, i.e.
by replacing those words with anull token.
We term this process as ?relaxation?and the omitted words as ?relaxed words?.Relaxed SMT here refers to a translation taskin which target vocabulary words are intentional-ly omitted from the training dataset for reducingtranslation complexity.
Since the most frequentwords are targeted to be relaxed, as a result, therewill be vast amount of null tokens in the outputtext, which later shall be recovered in a postprocessing stage.
The idea of relaxation in SMTis motivated by one of our experimental findings,in which the perplexity measured over a test setdecreases when most frequent words are relaxed.For instance, a 15% of perplexity reduction isobserved when the twenty most frequent wordsare relaxed in the English EPPS dataset.
Thereduction of perplexity allows us to conjecture30about the decrease of confusion in the text, fromwhich a SMT system might be benefited.After applying relaxed SMT, the resultingnull tokens in the translated sentences have to bereplaced by the corresponding words from the setof relaxed words.
As relaxed words are chosenfrom the top ranked words, which possess highoccurrences in the corpus, their n-gram probabili-ties could be reliably trained to serve for wordprediction.
Also, these words are mainly functionwords and, from the human perspective, functionwords are usually much easier to predict fromtheir neighbor context than content words.
Con-sider for instance the sentence the house of thepresident is very nice.
Function words like the,of, and is, are certainly easier to be predicted thancontent words such as house, president, and nice.The rest of the paper is organized into foursections.
In section 2, we discuss the relaxationstrategy implemented for a SMT system, whichgenerates translation outputs that contain null to-kens.
In section 3, we present the word predic-tion mechanism used to recover the null tokensoccurring in the relaxed translation outputs.
Insection 4, we present and discuss the experimen-tal results.
Finally, in section 5 we present themost relevant conclusion of this work.2 Relaxation for Machine TranslationIn this paper, relaxation refers to the replacementof the most frequent words in text by a null to-ken.
In the practice, a set of frequent words isdefined and the cardinality of such set is referredto as the relaxation order.
For example, lets therelaxation order be two and the two words on thetop rank are the and is.
By relaxing the samplesentence previously presented in the introduc-tion, the following relaxed sentence will be ob-tained: NULL house of NULL President NULLvery beautiful.From some of our preliminary experimentalresults with the EPPS dataset, we did observethat a relaxation order of twenty leaded to a per-plexity reduction of about a 15%.
To see whetherthis contributes to improving the translation per-formance, we trained a translation system byrelaxing the top ranked words in the vocabularyof the target language.
In this way, there will be alarge number of words in the source languagethat will be translated to a null token.
For exam-ple: la (the in Spanish) and es (is in Spanish) willbe both translated to a null token in English.This relaxation of terms is only applied to thetarget language vocabulary, and it is conductedafter the word alignment process but before theextraction of translation units and the computa-tion of model probabilities.
The main objectiveof this relaxation procedure is twofold: on theone hand, it attempts to reduce the complexity ofthe translation task by reducing the size of thetarget vocabulary while affecting a large propor-tion of the running words in the text; on the otherhand, it should also help to reduce model sparse-ness and improve model probability estimates.Of course, different from the Information Re-trieval case, in which stop words are not used atall along the search process, in the consideredmachine translation scenario, the removed wordsneed to be recovered after decoding in order toproduce an acceptable translation.
The relaxedword replacement procedure, which is based onan n-gram based predictor, is implemented as apost-processing step and applied to the relaxedmachine translation output in order to producethe final translation result.Our bet here is that the gain in the translationstep, which is derived from the relaxation strate-gy, should be enough to compensate the errorrate of the word prediction step, producing, inoverall, a significant improvement in translationquality with respect to the non-relaxed baselineprocedure.The next section describes the implementedword prediction model in detail.
It constitutes afundamental element of our proposed relaxedSMT approach.3 Frequent Word PredictionWord prediction has been widely studied andused in several different tasks such as, for exam-ple, augmented and alternative communication(Wandmacher and Antoine, 2007) and spellingcorrection (Thiele et al, 2000).
In addition to thecommonly used word n-gram, various languagemodeling techniques have been applied, such asthe semantic model (Lu?s and Rosa, 2002; Wand-macher and Antoine, 2007) and the class-basedmodel (Thiele et al, 2000; Zohar and Roth,2000; Ruch et al, 2001).The role of such a word predictor in our con-sidered problem is to recover the null tokens inthe translation output by replacing them with thewords that best fit the sentence.
This task is es-sentially a classification problem, in which themost suitable relaxed word for recovering a giv-en null token must be selected.
In other words,  maxsentence??, wheresentence?  is the probabilistic model, e.g.
n-31gram, that estimates the likelihood of a sentencewhen a null token is recovered with word  ,drawn from the set of relaxed words .
The car-dinality || is referred to as the relaxation order,e.g.
||  5 implies that the five most frequentwords have been relaxed and are candidates to berecovered.Notice that the word prediction problem inthis task is quite different from other works in theliterature.
This is basically because the relaxedwords to be predicted in this case are mainlyfunction words.
Firstly, it may not be effective topredict a function word semantically.
For exam-ple, we are more certain in predicting equity thanfor given the occurrence of share in the sentence.Secondly, although class-based modeling is com-monly used for prediction, its original intentionis to tackle the sparseness problem, whereas ourtask focuses only on the most frequent words.In this preliminary work, our predicting me-chanism is based on an n-gram model.
It predictsthe word that yields the maximum a posterioriprobability, conditioned on its predecessors.
Forthe case of the trigram model, it can be expressedas follows:  max|  (1)Often, there are cases in which more than onenull token occur consecutively.
In such casespredicting a null token is conditioned on the pre-vious recovered null tokens.
To prevent a predic-tion error from being propagated, one possibilityis to consider the marginal probability (summedover the relaxed word set) over the words thatwere previously null tokens.
For example, if is a relaxed word, which has been recoveredfrom earlier predictions, then the prediction of should no longer be conditioned by  .
Thiscan be computed as follows:  max |max?
|(2)The traditional n-gram model, as discussedpreviously, can be termed as the forward n-grammodel as it predicts the word ahead.
Additional-ly, we also tested the backward n-gram to predictthe word behind (i.e.
on the left hand side of thetarget word), which can be formulated as:  max|  (3)and the bidirectional n-gram to predict the wordin middle, which can be formulated as follows:  max|,  (4)Notice that the backward n-gram model canbe estimated from the word counts as:|   !  !(5)or, it can be also approximated from the forwardn-gram model, as follows:| " !| " |"" !| " (6)Similarly, the bidirectional n-gram model canbe estimated from the word counts:| " |"|"? "
 |"|"#	$(7)or approximated from the forward model:| " |"|"? "
 |"|"#	$(8)The word prediction results of using the for-ward, backward, and bidirectional n-gram mod-els will be presented and discussed in the ex-perimental section.The three n-gram models discussed so farpredict words based on the local word ordering.There are two main drawbacks to this: first, onlythe neighboring words can be used for predictionas building higher order n-gram models is costly;and, second, prediction may easily fail when con-secutive null tokens occur, especially when allwords conditioning the prediction probability arerecovered null tokens.
Hence, instead of predict-ing words by maximizing the local probability,predicting words by maximizing a global score(i.e.
a sentence probability in this case), may be abetter alternative.At the sentence level, the word predictor con-siders all possible relaxed word permutations andsearchs for the one that yields the maximum aposteriori sentence probability.
For the trigrammodel, a relaxed word that maximizes the sen-tence probability can be predicted as follows:  max?
|&'  (9)32where, ( is the number of words in the sentence.Although the forward, backward, and interpo-lated models have been shown to be applicablefor local word prediction, they make no differ-ence at sentence level predictions as they pro-duce identical sentence probabilities.
It is nothard to prove the following identity:?
|&'  ?
|&'(10)4 Experimental Results and DiscussionIn this section, we first highlight the Zipfian dis-tribution in the corpus and the reduction of per-plexity after removing the top ranked words.
Then-gram probabilities estimated were then usedfor word prediction, and we report the resultingprediction accuracy at different relaxation orders.The performance of the SMT system with a re-laxed vocabulary is presented and discussed inthe last subsection of this section.4.1 Corpus AnalysisThe data used in our experiments is taken fromthe EPPS (Europarl Corpus).
We used the ver-sion available through the shared task of the2007's ACL Workshops on Statistical MachineTranslation (Burch et al, 2007).
The training setcomprises 1.2M sentences with 35M words whilethe development set and test sets contains 2Ksentences each.From the train set, we computed the twentymost frequent words and ranked them according-ly.
We found them to be mainly function words.Their counts follow closely a Zipfian distribution(Figure 1) and account for a vast proportion ofthe text (Figure 2).
Indeed, the 40% of the run-ning words is made up by these twenty words.Figure 1.
The twenty top ranked words and theirrelative frequenciesFigure 2.
Cumulative relative frequencies of thetop ranked words (up to order 20)We found that when the most frequent wordswere relaxed from the vocabulary, which meansbeing replaced by a null token, the perplexity(measured with a trigram model) decreased up to15% for the case of a relaxation order of twenty(Figure 3).
This implies that the relaxation causesthe text becoming less confusing, which mightbenefit natural language processing tasks such asmachine translation.Figure 3.
Perplexity decreases with the relaxationof the most frequent words4.2 Word Prediction AccuracyIn order to evaluate the quality of the differentprediction strategies, we carried out some expe-riments for replacing null tokens with relaxedwords.
For this, frequent words were droppedmanually from text (i.e.
replaced with null to-kens) and were recovered later by using the wordpredictor.
As discussed earlier, a word can bepredicted locally, to yield maximum n-gramprobability, or globally, to yield maximum sen-012345678thecommaperiod of to and inthat a is wefor ithis on itbe are ashaveRelativefrequency(%)Top Ranked WordsTESTDEV5101520253035400 5 10 15 20Cumulativefrequency(%)Word RankTESTDEV45474951535557596163650 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20PerplexityRelaxation OrderDEVTEST33tence probability.
In a real application, a textmay comprise up to 40% of null tokens that mustbe recovered from the twenty top ranked words.For n-gram level prediction (local), we eva-luated word accuracy at different orders of relax-ation.
More specifically, we tested the forwardtrigram model, the backward trigram model, thebidirectional model, and the linear interpolationbetween forward and backward trigram models(with weight 0.5).
The accuracy was computed asthe percentage of null tokens recovered success-fully with respect to the original words.
Theseresults are shown in Figure 4.
Notice that theaccuracy of the relaxation of order one is 100%,so it has not been depicted in the plots.Notice from the figure how the forward andbackward models performed very alike through-out the different relaxation orders.
This can beexplained in terms of their similar perplexities(both models exhibit a perplexity of 58).
Betteraccuracy was obtained by the interpolated model,which demonstrates the advantage of incorporat-ing the left and right contexts in the prediction.Different from the interpolated model, whichsimply adds probabilities from the two models,the bidirectional model estimates the probabilityfrom the left and right contexts simultaneouslyduring the training phase; thus it produces a bet-ter result.
However, due to its heavy computa-tional cost (Equation 8), it is infeasible to apply itat orders higher than five.Figure 4.
Accuracy of word prediction at n-gramlevel.
Models incorporating left and right contextyield about a 20% improvement over one-sidedmodels.Better accuracy has been obtained for sentence-level prediction by using a bigram model and atrigram model.
These results are shown in Figure5.
From the cumulative frequency showed inFigure 2, we could see that 40% of the words intext could now be predicted with an accuracy ofabout 77%.Figure 5.
Accuracy of word prediction at thesentence level.For predicting words by maximizing the sen-tence probability, two methods have been tried:first, a brute force method that attempts all possi-ble relaxed word permutations for the occurringnull tokens within a sentence and finds the oneproducing the largest probability estimate; and,second, applying Viterbi decoding over a wordlattice, which is built from the sentence by re-placing the arcs of null tokens with a parallelnetwork of all relaxed words.All the arcs in the word lattice have to beweighted with n-gram probabilities in order tosearch for the best route.
In the case of the tri-gram model, we expand the lattice with the aid ofthe SRILM toolkit (Stolcke 2002).
Both methodsyield almost identical prediction accuracy.
How-ever, we discarded the brute force approach forthe later experiments because of its heavy com-putational burden.Figure 4 and 5 have been plotted with thesame scale on the vertical axis for easing theirvisual comparison.
The global prediction strate-gy, which optimizes the overall sentence perplex-ity, is much more useful for prediction as com-pared to the local predictions.
Furthermore, asseen from Figure 5, the global prediction hasbetter resistance against higher order relaxations.We also observed that the local bidirectionalmodel performed closely to the global bigrammodel, up to relaxation order five, for which the4050607080901000 5 10 15 20AccuracyRelaxation OrderTrigram(Forward)Trigram(Backward)InterpolationBidirectional4050607080901000 5 10 15 20Acuuracy(%)Relaxation OrderBigramTrigram34computation of the bidirectional model is stillfeasible.
In Figure 6 we present a scaled versionof the plots to focus on the lower orders for com-parison purposes.
Although the global bigramprediction makes use of all words in the sentencein order to yield the prediction, locally, a givenword is only covered by two consecutive bi-grams.
Thus, the prediction of a word does notdepend on the second word before or the secondword after.
In other words, we could see the bidi-rectional model as a global model that is appliedto a ?short segment?
(in this case, a three wordsegment).
The only difference here is that thelocal bidirectional model estimates the probabili-ties from the corpus and keeps all seen ?shortsegment?
probabilities in the language model (inour case, it is derived from forward bigram),while the global bigram model optimizes theprobabilities by searching for the best two con-secutive bigrams.
The global prediction mightonly show its advantage when predicting two ormore consecutive null tokens.Figure 6.
Comparison among local bidirectional,local interpolation, and global bigram models.Hence, we believe that, if the bidirectional bi-gram model is computed from counts and stored,it could perform as good as global bigram modelat much faster speed (as it involves only query-ing the language model).
Similarly, a local bidi-rectional trigram model (actually a 5-gram) maybe comparable to a global trigram model.Deriving bidirectional n-gram probabilitiesfrom a forward model is computationally expen-sive.
In the worst case scenario, where bothcompanion words are relaxed words, the compu-tation complexity is in the order of )|*|||+,where |*| is the vocabulary size and || is thenumber of relaxed words in *.
Building a bidi-rectional bigram/trigram model from scratch isworth to be considered.
As all known languagemodel toolkits do not offer this function (eventhe backward n-gram model is built by first re-versing the sentences manually), the discount-ing/smoothing of the trigram has to be derived.The methods of Good-Turing, Kneser Ney, Ab-solute discounting, etc.
(Chen and Goodman,1998) can be imitated.4.3 Translation PerformanceAs frequent words have been ignored in the tar-get language vocabulary of the machine transla-tion system, the translation outputs will contain agreat number of null tokens.
The amount of nulltokens should approximate the cumulative fre-quencies shown in Figure 2.In this experiment, a word predictor was usedto recover all null tokens in the translation out-puts, and the recovered translations were eva-luated with the BLEU metric.
All BLEU scoreshave been computed up to trigram precision.The word predictor used was the global tri-gram model, which was the best performing sys-tem in word prediction experiments previouslydescribed.
In this case, the predictor was used torecover the null tokens in the translation outputs.In order to apply the prediction mechanism as apost-processing step, a word lattice was builtfrom each translation output, for which the nullword arcs were expanded with the words of therelaxed set.
Finally, the lattice was decoded toproduce the best word list as the complete trans-lation output.To evaluate whether a SMT system benefitsfrom the relaxation strategy, we set up a baselinesystem in which the relaxed words in the transla-tion output were replaced manually with nulltokens.
After that, we used the same word pre-dictor as in the relaxed SMT case (global trigrampredictor) for recovering the null tokens andregenerating the complete sentences.
We thencompared the translation output of the relaxedSMT system to the baseline system.The results for the baseline (BL) and the re-laxed (RX) systems are shown in Figure 7.
Weevaluated the translation performance for relaxa-tion orders of five and twenty.From the results shown in Figure 7, it be-comes evident that the translation task is notgaining any advantage from relaxation strategyand did not outperform the baseline translator,neither at low nor at high orders of relaxation.878991939597992 3 4 5Accuracy(%)Relaxation OrderBidirectional(local)Interpolation(local)Bigram (global)35Notice how the BLEU score of the baseline sys-tems are better than those of the relaxed systems.Figure 7.
BLEU scores for baseline (BL) andrelaxed (RX) translation systems at relaxationorders of five and twenty.We further analyzed these results by compu-ting BLEU scores for the translation outputsbefore and after the word prediction step.
Theseresults are shown in Figure 8.
Notice from Figure8 that the relaxed translators did not produce anybetter BLEU score than the corresponding base-line systems, even before word recovery.
Al-though the text after relaxation is less confusing(perplexity decreases about 15% after the twentymost frequent words are relaxed), the resultingperplexity drop was not translated into a BLEUscore improvement.Figure 8.
The BLEU scores before and afterword predictionIn terms of the word predictions shown inFigure 8, we can see that this post-processingstep performed consistently for the relaxed SMTsystems, as for the baseline systems (for whichthe null tokens were inserted manually into sen-tences).
Since the word prediction is based on ann-gram model, we may deduce that the relaxedSMT system preserves the syntactic structure ofthe sentence as the null tokens in the translationoutput could be recovered as accurate as in thecase of the baseline system.5 Conclusion and Future WorkWe have looked into the problem of predictingthe most frequently occurring words in a text.The best of the studied word predictors, which isbased on an n-gram language model and a globalsearch at the sentence level, has achieved 77% ofaccuracy when predicting 40% words in the text.We also proposed the idea of relaxed SMT,which consists of replacing top ranked words inthe target language vocabulary with a null token.This strategy was originally inspired by the con-cept of stop word removal in Information Re-trieval, and later motivated by the finding thattext will become less confusing after relaxation.However, when relaxation is applied to the ma-chine translation system, our results indicate thatthe relaxed translation task is performing poorerthan the conventional non-relaxed system.
Inother words, the SMT system does not seem tobe benefiting from the word relaxation strategy,at least in the case of the specific implementationstudied here.As future work, we will attempt to re-tailorthe set of relaxed words by, for instance, impos-ing some constraints to also include some lessfrequent function words, which may not be infor-mative to the translation system or, alternatively,excluding some frequent semantically importantwords from the relaxed set.
This remark is basedon the observation of the fifty most frequentwords in the EPPS dataset, such as president,union, commission, European, and parliament,which could be harmful when ignored by thetranslation system but also easy to predict.
Hencethere is a need to study the effects of differentsets of relaxed words on translation performance,as it have already been done for the search prob-lem by researchers in the area of InformationRetrieval (Fox, 1990; Ibrahim, 2006).AcknowledgementsThe authors would like to thank their correspond-ing institutions: the Nanyang Technological Uni-0.30.310.320.330.340.350.360.370.38BL5 BL20 RX5 RX20BLEUTranslatorDEVTEST00.050.10.150.20.250.30.350.4BL5 BL20 RX5 RX20BLEUTranslatorBeforeAfter36versity and the Institute for Infocomm Research,for their support regarding the development andpublishing of this work.ReferencesAndreas Stolcke, 2002, SRILM - An Extensible Lan-guage Modeling Toolkit, in Proceedings of ICSLP,901-904.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz and Josh Schroeder, 2007,(Meta-)evaluation of machine translation, in Pro-ceedings of the SMT Workshop, 136-158Christopher Fox, 1990, A stop list for general text,SIGIR Forum, 24:19-35.Cornelis Joost van Rijsbergen, 1979, InformationRetrieval, Butterworth-Heinemann.Feng Zou, Fu Lee Wang, Xiaotie Deng and Song Han,2006, Automatic identification of Chinese stopwords, Research on Comp.
Science, 18:151-162.Frank Thiele, Bernhard Rueber and Dietrich Klakow,2000, Long range language models for free spel-ling recognition, in Proceeding of the 2000 IEEEICASSP, 3:1715-1718.Ibrahim Abu El-Khair, 2006, Effects of stop wordselimination for Arabic information retrieval: acomparative study, International Journal of Com-puting & Information Sciences, 4(3):119-133.Jo?o Lu?s and Garcia Rosa, 2002, Next word predic-tion in a connectionist distributed representationsystem, in Proceedings of the 2002 IEEE Int.
Con-ference on Systems, Man and Cybernetics, 6-11.Keith Trnka, John McCaw, Debra Yarrington, Kath-leen F. McCoy, User interaction with word predic-tion: the effects of prediction quality, ACMTransaction on Accessible Computing, 1(17):1-34.Ljiljana Dolamic and Jacques Savoy., 2009, Whenstopword lists make the difference, Journal of theAmerican Society for Information Science andTechnology, 61(1):1-4.Patrick Ruch, Robert Baud and Antoine Geissbuhler,2001, Toward filling the gap between interactiveand fully-automatic spelling correction using thelinguistic context, in Proceedings of the 2001 IEEEInternational Conference on Systems, Man andCybernetics, 199-204.Stanley F. Chen and Joshua Goodman, 1998, Anempirical study of smoothing techniques for lan-guage modeling, in Proceedings of the 34th AnnualMeeting of the Association for Computational Lin-guistics, 310-318.Timothy C. Bell, John G. Cleary and Ian H. Witten.,1990, Text Compression, Prentice Hall.Tonio Wandmacher and Jean-Yves Antoine, 2007,Methods to integrate a language model with se-mantic information for a word prediction compo-nent, in Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, 506-513.Yair Even-Zohar and Dan Roth, 2000, A classifica-tion approach to word prediction, in Proceedings ofthe 1st North American chapter of the Associationfor Computational Linguistics, 124-131.37
