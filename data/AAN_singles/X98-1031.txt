THE TEXT RETRIEVAL CONFERENCES (TRECS)Ellen M. Voorhees, Donna HarmanNat iona l  Ins t i tu te  o f  S tandards  and  Techno logyGa i thersburg ,  MD 208991 INTRODUCTIONPhase III of the T IPSTER project included threeworkshops for evaluating document detection (infor-mation retrieval) projects: the fifth, sixth and sev-enth Text REtrieval Conferences (TRECs).
Thiswork was co-sponsored by the National Instituteof Standards and Technology (NIST), and includedevaluation ot only of the T IPSTER contractors, butalso of many information retrieval groups outside ofthe T IPSTER project.
The conferences were run asworkshops that provided a forum for participatinggroups to discuss their system results on the retrievaltasks done using the T IPSTER/TREC collection.
Aswith the first four TRECs, the goals of these work-shops were:?
to encourage research in text retrieval based onlarge test collections;?
to increase communication among industry,academia, and government by creating an openforum for the exchange of research ideas;?
to speed the transfer of technology from researchlabs into commercial products by demonstratingsubstantial improvements in retrieval method-ologies on real-world problems;?
to increase the availability of appropriate val-uation techniques for use by industry andacademia, including development of new evalu-ation techniques more applicable to current sys-tems; and?
to serve as a showcase for state-of-the-art e-trieval systems for DARPA and its clients.For each TREC, NIST provides a test set of docu-ments and questions.
Participants run their retrievalsystems on the data, and return to NIST a list of theretrieved top-ranked ocuments.
NIST pools the in-dividual results, judges the retrieved ocuments forcorrectness, and evaluates the results.
The TRECcycle ends with a workshop that is a forum for par-ticipants to share their experiences.
The most recentworkshop in the series, TREC-7, was held at NISTin November 1998.The number of participating systems has grownfrom 25 in TREC-1 to 38 in TREC-5 (Table 1), 51in TREC-6 (Table 1), and 56 in TREC-7 (Table 1).The groups include representatives from 16 differentcountries and 32 companies.TREC provides a common test set to focus researchon a particular etrieval task, yet actively encouragesparticipants to do their own experiments within theumbrella task.
The individual experiments broadenthe scope of the research that is done within TRECand make TREC more attractive to individual par-ticipants.
This marshaling of research efforts has suc-ceeded in improving the state of the art in retrievaltechnology, both in the level of basic performance (seeFigure 1) and in the ability of these systems to func-tion well in diverse environments, uch as retrievalin a filtering operation or retrieval against multiplelanguages.Each of the TREC conferences has centered aroundtwo main tasks: the routing task (not run in TREC-7) and the ad hoc task (these tasks are described inmore detail in Section 2.3).
In addition, starting inTREC-4 a set of "tracks" or tasks that focus on par-ticular subproblems of text retrieval was introduced.These tracks include tasks that concentrate on a spe-cific part of the retrieval process (such as the inter-active track which focuses on user-related issues), ortasks that tackle research in related areas, such as theretrieval of spoken "documents" from news broad-casts.The  graph in Figure i shows that retrieval effective-ness has approximately doubled since the beginningof TREC.
This means,  for example,  that retrieval en-gines that could retrieve three good  documents  withinthe top ten documents  retrieved in 1992 are now likelyto retrieve six good  documents  in the top ten docu-ments  retrieved for the same search.
The  figure plotsretrieval effectiveness for one wel l -known retrieval en-gine, the SMART system of Cornell University.
TheSMART system has consistently been one of the moreeffective systems in TREC,  but other systems are241Apple ComputerAustralian National UniversityCLARITECH CorporationCity UniversityComputer Technology InstituteCornell UniversityDublin City UniversityFS ConsultingGE/NYU/Rutgers/Lockheed MartinGSI-ErliGeorge Mason UniversityIBM CorporationIBM T.J. Watson Research CenterInformation Technology Institute, SingaporeInstitut de Recherche n Informatique de ToulouseIntext SystemsLexis-NexisMDS at RMITMITREMonash UniversityNew Mexico State University (two groups)Open Text CorporationQueens College, CUNYRank Xerox Research CenterRutgers University (two groups)Swiss Federal Institute of Technology (ETH)Universite de NeuchatelUniversity of California, BerkeleyUniversity of California, San DiegoUniversity of GlasgowUniversity of Illinois at Urbana-ChampaignUniversity of KansasUniversity of MarylandUniversity of Massachusetts, AmherstUniversity of North CarolinaUniversity of WaterlooTable 1:TREC-5 participantsApple ComputerAT&T Labs ResearchAustralian National UniversityCEA (France)Carnegie Mellon UniversityCenter for Information Research, RussiaCity University, LondonCLARITECH CorporationCornell U./SaBIR Research, IncCSIRO (Australia)Daimler Benz Research Center UlmDublin City UniversityDuke U./U.
of Colorado/BellcoreFS Consulting, Inc.GE Corp./Rutgers U.George Mason U./NCR Corp.Harris Corp.IBM T.J. Watson Research (2 groups)ITI (Singapore)MSI/IRIT/U.
Toulouse (France)ISS (Singapore)APL, Johns Hopkins UniversityLexis-NexisMDS at RMIT, AustraliaMIT/IBM Almaden Research CenterNEC CorporationNew Mexico State U.
(2 groups)NSA (Speech Research Branch)Open Text CorporationOregon Health Sciences U.Queens College, CUNYRutgers University (2 groups)Siemens AGSRI InternationalSwiss Federal Inst.
of Tech.
(ETH)TwentyOne (TNO/U-Tente/DFKI/Xerox/U-Tuebingen)U. of California, BerkeleyU.
of California, San DiegoU.
of GlasgowU.
of Maryland, College ParkU.
of Massachusetts, AmherstU.
of MontrealU.
of North Carolina (2 groups)U. of Sheffield/U.
of CambridgeU.
of WaterlooVerity, Inc.Xerox Research Centre EuropeTable 2 :TREC-6  participants242ACSys Cooperative Research CentreAT&T Labs ResearchAvignon CS Laboratory/BertinBBN TechnologiesCanadian Imperial Bank of CommerceCarnegie Mellon UniversityCommissariat ~ l'Energie AtomiqueCLARITECH CorporationCornell University/SabIR Research, Inc.Defense Evaluation and Research AgencyEurospiderFondazione Ugo BordoniFS Consulting, Inc.Fujitsu Laboratories, Ltd.GE/Rutgers/SICS/HelsinkiHarris Information Systems DivisionIBM - -  Almaden Research CenterIBM T.J. Watson Research Center (2 groups)Illinois Institute of TechnologyImperial College of Science, Technology and MedicineInstitut de Recherche n Informatique de ToulouseThe Johns Hopkins University - -  APLKasetsart UniversityKDD R&D LaboratoriesKeio UniversityLexis-NexisLos Alamos National LaboratoryManagement Information Technologies, Inc.Massachusetts Institute of TechnologyNational Tsing Hua UniversityNEC Corp. and Tokyo Institute of TechnologyNew Mexico State UniversityNTT DATA CorporationOkapi Group (City U./U.
of Sheffield/Micr osoft)Oregon Health Sciences UniversityQueens College, CUNYRMIT/Univ.
of Melbourne/CSIRORutgers University (2 groups)Seoul National UniversitySwiss Federal Institute of Technology (ETH)TextWise, Inc.TNO-TPD TU-DelftTwentyOneUniversite de MontrealUniversity of California, BerkeleyUniversity of CambridgeUniversity of IowaUniversity of MarylandUniversity of Massachusetts, AmherstUniversity of North Carolina, Chapel HillUniv.
of Sheffield/Cambridge/SoftSoundUniversity of TorontoUniversity of WaterlooU.S.
Department ofDefenseTable 3:TREC-7 participantscomparable with it, so the graph is representative ofthe increase in effectiveness for the field as a whole.Researchers at Cornell ran the version of SMARTused in each of the seven TREC conferences againsteach of the seven ad hoc test sets (Buckley, Mitra,Walz, & Cardie, 1999).
Each line in the graph con-nects the mean average precision scores produced byeach version of the system for a single test.
For eachtest, the TREC-7 system has a markedly higher meanaverage precision than the TREC-1 system.
The re-cent decline in the absolute scores reflects the evolu-tion towards more realistic, and difficult, test ques-tions, and also possibly a dilution of effort because ofthe many tracks being run in TRECs 5, 6, and 7.The seven TREC conferences represent hun-dreds of retrieval experiments.
The Proceedingsof each conference captures the details of the in-dividual experiments, and the Overview paper ineach Proceedings ummarizes the main findings ofeach conference.
A special issue on TREC-6 willbe published in Information Processing and Man-agement (Voorhees, in press), which includes anOverview of TREC-6 (Voorhees & Harman, in press)as well as an analysis of the TREC effort by SparckJones (in press).2 THE TASKSEach of the TREC conferences has centered aroundtwo main tasks, the routing task and the ad hoc task.In addition, starting in TREC-4 a set of "tracks,"tasks that focus on particular subproblems of textretrieval, was introduced.
This section describes thegoals of the two main tasks.
Details regarding thetracks are given in Section 6.2.1 The  Rout ing  TaskThe routing task in the TREC workshops investigatesthe performance ofsystems that use standing queriesto search new streams of documents.
These searchesare similar to those required by news clipping ser-vices and library profiling systems.
A true routing243Ogo<0.45000.40000.35000.30000.25000.20000.15000.10000.05000.0000'92 SI - -  TREC-1 task.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
TREC-2 task~ _  --- TREC-3 taskn TREC-4 task- -  TREC-5 task- - -  TREC-6 task~ : - ' - ~ .
.
- .
i  ~.
: ?
.
: .
- .~ .
.
-  TREC-7 task: .
'2I I I I I I,stem '93 System '94 System '95 System '96 System '97 System '98 SystemFigure 1: Retrieval effectiveness improvement for Cornell's SMART system, TREC-1 - TREC-7.environment is simulated in TREC by using ques-tions (called topics in TREC) for which the right setof documents to be retrieved is known for one docu-ment set, and then testing the systems' performancewith those questions on a completely new documentset.The training for the routing task is shown in theleft-hand column of Figure 2.
Participants are given aset of topics and a document set that includes knownrelevant documents for those topics.
The topics con-sist of natural anguage text describing a user's infor-mation need (see sec.
3.2 for details).
The topics areused to create a set of queries (the actual input tothe retrieval system) that are then used against hetraining documents.
This is represented by Q1 in thediagram.
Many Q1 query sets might be built to helpadjust the retrieval system to the task, to create bet-ter weighting algorithms, and to otherwise preparethe system for testing.
The result of the training isquery set Q2, routing queries derived from the rout-ing topics and run against he test documents.The testing phase of the routing task is shown inthe middle column of Figure 2.
The output of run-ning Q2 against he test documents i the official testresult for the routing task.2.2 The  Ad  Hoc  TaskThe ad hoc task investigates the performance of sys-tems that search a static set of documents using newtopics.
This task is similar to how a researcher mightuse a library--the collection is known but the ques-tions likely to be asked are not known.
The right-hand column of Figure 2 depicts how the ad hoc taskis accomplished in TREC.
Participants are given adocument collection consisting of approximately 2 gi-gabytes of text and 50 new topics.
The set of relevantdocuments for these topics in the document set is notknown at the time the participants receive the top-ics.
Participants produce a new query set, Q3, fromthe ad hoc topics and run those queries against head hoc documents.
The output from this run is theofficial test result for the ad hoc task.2.3 Task  Gu ide l inesIn addition to the task definitions, TREC partici-pants are given a set of guidelines outlining accept-able methods of indexing, knowledge base construc-tion, and generating queries from the supplied top-ics.
In general, the guidelines are constructed to re-flect an actual operational environment and to allowfair comparisons among the diverse query construc-tion approaches.
The allowable query constructionmethods in TRECs 5, 6, and 7 were divided into au-244TopicsQ1TrainingQueries= 3.5 GBTrainingDocuments50RoutingTopicsQ250 RoutingQueriesRoutingDocuments50Ad HoctopicsQ350 Ad HocQueries=2GBDocumentsFigure 2: TREC main tasks.tomatic methods, in which queries are derived com-pletely automatically from the topic statements, andmanual methods, which includes queries generated byall other methods.
This definition of manual queryconstruction methods permitted users to look at indi-vidual documents retrieved by the ad hoc queries andthen reformulate the queries based on the documentsretrieved.3 THE TEST  COLLECT IONSLike most traditional retrieval test collections, thereare three distinct parts to the collections used inTREC: the documents, the questions or topics, andthe relevance judgments or "right answers."
This sec-tion describes each of these pieces for the collectionsused in the main tasks in TRECs 5, 6, and 7.
Manyof the tracks have used the same data or used dataconstructed in a similar method but in a differentenvironment, such as in multiple languages or usingdifferent guidelines (such as high precision searching).3 .1 DocumentsTREC documents are distributed on CD-ROM's withapproximately 1 GB of text on each, compressed tofit.
Table 3.1 shows the statistics for all the Englishdocument collections used in TREC.
TREC-5 useddisks 2 and 4 for the ad hoc testing, while TRECs6 and 7 used disks 4 and 5 for ad hoc testing.
TheFBIS on disk 5 (FBIS-1) was used for testing in theTREC-5 routing task and for training in the TREC-6routing task, with new FBIS (FBIS-2) being used fortesting in TREC-6.
There was no routing task inTREC-7.Documents are tagged using SGML to allow easyparsing (see Fig.
3).
The documents in the differentdatasets have been tagged with identical major struc-tures, but they have different minor structures.
Thephilosophy in the formatting at NIST is to leave thedata as close to the original as possible.
No attemptis made to correct spelling errors, sentence fragments,strange formatting around tables, or similar faults.3 .2  Top icsIn designing the TREC task, there was a consciousdecision made to provide "user need" statementsrather than more traditional queries.
Two major is-sues were involved in this decision.
First, there wasa desire to allow a wide range of query constructionmethods by keeping the topic (the need statement)distinct from the query (the actual text submittedto the system).
The second issue was the ability toincrease the amount of information available abouteach topic, in particular to include with each topica clear statement of what criteria make a documentrelevant.The topics used in TREC-1 and TREC-2 (topics1-150) were very detailed, containing multiple fieldsand lists of concepts related to the subject of thetopics.
The ad hoc topics used in TREC-3 (151-200)245Disk 1Wall Street Journal, 1987-1989Associated Press newswire, 1989Computer Selects articles, Ziff-DavisFederal Register, 1989abstracts of U.S. DOE publicationsDisk 2Wall Street Journal, 1990-1992 (WSJ)Associated Press newswire (1988) (AP)Computer Selects articles, Ziff-Davis (ZIFF)Federal Register (1988) (FR88)Disk 3San Jose Mercury News, 1991Associated Press newswire, 1990Computer Selects articles, Ziff-DavisU.S.
patents, 1993Disk 4the Financial Times, 1991-1994 (FT)Federal Register, 1994 (FR94)Congressional Record, 1993 (CR)Disk 5Foreign Broadcast Information Service (FBIS-1)the LA TimesTREC-6 Routing Test DataForeign Broadcast Information Service (FBIS-2)Size # Median # Mean #(megabytes) Docs Words/Doc Words/Doc267254242260184242237175209287237345243564395235470475490Table 4: Document collection statistics.
Words are strings ofremoved and no stemming was performed.98,73284,67875,18025,960226,08774,52079,91956,92019,86090,25778,321161,0216,711245446200391111210,15855,63027,922130,471131,896120,6533014381823963794511224445434.0473.9473.01315.9120.4508.4468.7451.91378.1453.0478.4295.45391.0316588288322351348412.7644.71373.5543.6526.5581.3alphanumeric characters.
No stop words werewere much shorter and did not contain the complexstructure of the earlier topics.
Nonetheless, partici-pants in TREC-3 felt that the topics were still toolong compared with what users normally submit tooperational retrieval systems.
Therefore the TREC-4topics (201-250) were made even shorter: a singlefield consisting of a one sentence description of theinformation eed.
Figure 4 gives a sample topic fromeach of these sets.One of the conclusions reached in TREC-4 wasthat the much shorter topics caused both manual andautomatic systems trouble, and that there were is-sues associated with using short topics in TREC thatneeded further investigation (Harman, 1996).
Ac-cordingly, the TREC-5 ad hoc topics re-introducedthe title and narrative fields, making the topics sim-ilar in format to the TREC-3 topics.
TREC-6 andTREC-7 topics used this same format, as shownin Figure 5.
While having the same format asthe TREC-3 topics, on average the later topics areshorter (contain fewer words) than the TREC-3 top-ics.
Table 3.2 shows the lengths of the various sec-tions in the TREC topics as they have evolved overthe 7 TRECs.Since TREC-3, the ad hoc topics have been createdby the same person (or assessor) who performed therelevance assessments for that topic.
Each assessorcomes to NIST with ideas for topics based on his orher own interests, and searches the ad hoc collection(looking at approximately 100 documents per topic)to estimate the likely number of relevant documentsper candidate topic.
NIST personnel select the fi-nal 50 topics from among these candidates, based onhaving both a reasonable range of estimated numberof relevant documents across topics and on balancingthe load across assessors.3.3 Re levance  AssessmentsRelevance judgments are of critical importance to atest collection.
For each topic it is necessary to com-pile a list of relevant documents--as comprehensive246<DOC><DOCNO>FT911-3</DOCNO><PROFILE>AN-BEOATAAIFT</PROFILE><DATE>910514</DATE><HEADLINE>FT 14 MAY 91 / International Company News: Contigas plans DM9OOm east Germanproject</HEADLINE><BYLINE>By DAVID GOODHART</BYLINE><DATELINE>BONN</DATELINE><TEXT>CONTIGAS, the German gas group 81 per cent owned by the utility Bayernwerk, saidyesterday that it intends to invest DM9OOm (Dollars 522m) in the next four yearsto build a new gas distribution system in the east German state of Thuringia .
.
.
.</TEXT></DOC>Figure 3: A document extract from the Financial Times.a list as possible.
All TRECs have used the poolingmethod (Sparck Jones ~ van Rijsbergen, 1975) toassemble the relevance assessments.
In this methoda pool of possible relevant documents i created bytaking a sample of documents selected by the variousparticipating systems.
This pool is then shown to thehuman assessors.
The particular sampling methodused in TREC is to take the top 100 documents re-trieved in each submitted run for a given topic andmerge them into the pool for assessment.
This isa valid sampling technique since all the systems usedranked retrieval methods, with those documents mostlikely to be relevant returned first.
On average, anassessor judges approximately 1500 documents pertopic.Given the vital role relevance judgments play ina test collection, it is important o assess the qual-ity of the judgments created in TREC.
In particular,both the completeness and the consistency of the rel-evance judgments are of interest.
Completeness mea-sures the degree to which all the relevant documentsfor a topic have been found; consistency measuresthe degree to which the assessor has marked all the"truly" relevant documents relevant and the "truly"irrelevant documents irrelevant.The completeness of the TREC relevance judg-ments has been investigated both at NIST (Harman,1996) and independently at the Royal Melbourne In-stitute of Technology (RMIT) (Zobel, 1998).
Bothstudies found that the completeness for most top-ics is adequate, though topics with many relevantdocuments are likely to have yet more relevant doc-uments that have not been found through pooling.For this reason, NIST has deliberately chosen moretightly focused topics in recent TRECs.
Both studiesalso found that any lack of completeness did not biasthe results of particular systems.
Indeed, the RMITstudy showed that systems that did not contributedocuments to the pool can still be evaluated fairlywith the resulting judgments.The consistency of the TREC judgments was inves-tigated at NIST by obtaining multiple independentassessments for a set of topics and evaluating systemsusing each of the different judgment sets (Voorhees,1998).
The study confirmed that the comparative r -sults for different runs remains table despite changesin the underlying judgments.
Taken together, thesestudies validate the use of the TREC collections forretrieval research.4 EVALUATIONAn important element of TREC is to provide a com-mon evaluation forum.
A standard evaluation pack-247<num> Number : 051<dom> Domain: International Economics<title> Topic: Airbus Subsidies<desc> Description:Document will discuss government assistance to Airbus Industrie, or mentiona trade dispute between Airbus and a U.S. aircraft producer over the issue ofsubsidies.<narr> Narrative:A relevant document will cite or discuss assistance to Airbus Industrie by theFrench, German, British or Spanish government(s), or will discuss a trade disputebetween Airbus or the European governments and a U.S. aircraft producer, mostlikely Boeing Co. or McDonnell Douglas Corp., or the U.S. government, overfederal subsidies to Airbus.<con> Concept(s):1.
Airbus Industr ie2.
European a i rc ra f t  consortium, Messerschmitt-Boelkow-BlohmGmbH, Br i t i shAerospace PLC, Aerospatiale,  Construcciones Aeronauticas S.A.3.
federa l  subsidies,  government ass istance,  aid, loan, f inancing4.
trade dispute,  trade controversy, trade tension5.
General Agreement on Tar i f f s  and Trade (GATT) a i rc ra f t  code6.
Trade Pol icy Review Group (TPKG)7. complaint, object ion8.
re ta l ia t ion ,  anti-dumping duty pet i t ion ,  countervai l ing duty pet i t ion ,sanctions<hum> Number: 168<title> Topic: Financing AMTRAK<desc> Description:A document will address the role of the Federal Government in financing theoperation of the National Railroad Transportation Corporation (AMTRAK).<narr> Narrative:A relevant document must provide information on the government's responsibilityto make AMTRAK an economically viable entity.
It could also discuss theprivatization of AMTRAK as an alternative to continuing government subsidies.Documents comparing government subsidies given to air and bus transportation withthose provided to AMTRAE would also be relevant.<num> Number: 207<desc> What are the prospects of the Quebec separatists achieving independencefrom the rest of Canada?Figure 4: The evolution of TREC topic statements.
Sample topic statement from TRECs 1 and 2 (top),TREC-3 (middle), and TREC-4 (bottom).248<num> Number: 312<title> Hydroponics<desc> Description:Document will discuss the science of growing plants in water or some substanceother than soil.<hart> Narrative:A relevant document will contain specific information on the necessary nutrients,experiments, types of substrates, and/or any other pertinent facts related to thescience of hydroponics.
Related information includes, but is not limited to, thehistory of hydroponics, advantages over standard soil agricultural practices,or the approach of suspending roots in a humid enclosure and spraying themperiodically with a nutrient solution to promote plant growth.Figure 5: A sample TREC-6 topic.M in iMax  MeanTREC-1 (51-100) 44 250 107.4title 1 11 3.8description 5 41 17.9narrative 23 209 64.5concepts 4 111 21.2TREC-2 (101-150) 54 231 130.8title 2 9 4.9description 6 41 18.7narrative 27 165 78.8concepts 3 88 28.5TREC-3 (151-200) 49 180 103.4title 2 20 6.5description 9 42 22.3narrative 26 146 74.6TREC-4 (201-250) 8 33 16.3description 8 33 16.3TREC-5 (251-300) 29 213 82.7title 2 10 3.8description 6 40 15.7narrative 19 168 63.2TREC-6 (301-350) 47 156 88.4title 1 5 2.7description 5 62 20.4narrative 17 142 65.3TREC-7 (351-400) 31 114 57.6title 1 3 2.5description 5 34 14.3narrative 14 92 40.8Table 5: Topic length statistics by topic section.Lengths count number of tokens in topic statementincluding stop words.age, called trec_eval,  is used to evaluate ach ofthe submitted runs.
trec_eval was developed byChris Buckley at Cornell University and is availableby anonymous ftp from f tp .
cs.
corne l l ,  edu in thepub/smart directory.
TREC reports a variety ofrecall- and precision-based valuation measures foreach run to give a broad picture of the run.Since TREC-3 there has been a histogram for eachsystem showing performance on each topic.
In gen-eral, more emphasis has been placed in later TRECson a "per topic analysis" in an effort o get beyond theproblems of averaging across topics.
Work has beendone, however, to find statistical differences amongthe systems (see paper "A Statistical Analysis of theTREC-3 Data" by Jean Tague-Sutcliffe and JamesBlustein in the TREC-3 proceedings.)
Additionallycharts have been published in the proceedings thatconsolidate information provided by the systems de-scribing features and system timing, allowing someprimitive comparison of the amount of effort neededto produce the results.Figure 4 shows two typical recall/precision curves.The x axis plots a fixed set of recall levels wherenumber o/ relevant items retrievedRecall =total number o/ relevant items in the collection"The y axis plots precision values at the given recalllevel, where precision is calculated bynumber o/ relevant items retrievedPrecision --total number o\] items retrievedThese curves represent averages over the 50 top-ics.
The averaging method was developed many yearsago (Salton & McGill, 1983) and is well acceptedby the information retrieval community.
The curves2490.800.60O"50.400.200.000.00 0.20 0.40 0.60 0.80 1.00Recall_._ System A+ System Bl.O0Figure 6: A sample Recall-Precision graph.show system performance across the full range of re-trieval, i.e., at the early stage of retrieval where thehighly-ranked documents give high accuracy or preci-sion, and at the final stage of retrieval where there isusually a low accuracy, but more complete retrieval.The use of these curves assumes a ranked output froma system.
Systems that provide an unranked set ofdocuments are known to be less effective and there-fore were not tested in the TREC program.The curves in Figure 4 show that system A hasa much higher precision at the low recall end of thegraph and therefore is more accurate.
System B how-ever has higher precision at the high recall end of thecurve and therefore will give a more complete set ofrelevant documents, assuming that the user is willingto look further in the ranked list.The single-valued evaluation measure most fre-quently used in TREC is the mean (non-interpolated)average precision.
The average precision for a singletopic is the mean of the precision obtained after eachrelevant document is retrieved (using zero as the pre-cision for relevant documents that are not retrieved).The mean average precision for a run consisting ofmultiple topics is the mean of the average precisionscores of each of the individual topics in the run.
Theaverage precision measure has a recall component inthat it reflects the performance of a retrieval runacross all relevant documents, and a precision com-ponent in that it weights documents retrieved earliermore heavily than documents retrieved later.
Geo-metrically, mean average precision is the area under-neath a non-interpolated recall-precision curve.5 RETRIEVAL  RESULTSOne of the important goals of the TREC conferencesis that the participating roups freely devise theirown experiments within the TREC task(s).
For somegroups, particularly the groups new to TREC, thismeans doing the ad hoc and/or routing task with thegoal of achieving high retrieval effectiveness perfor-mance.
Other groups use TREC as an opportunityto run experiments especially tuned to their own en-vironment, either taking part in the organized tracksor performing associated tasks that can be evaluatedeasily within the TREC framework.
The experimen-tal work performed for TRECs 5, 6, and 7 is thereforeboth too broad and too extensive to be summarizedwithin this paper.
What is presented is some analy-sis of the trends within the ad hoc and routing tasks,plus a summary of the various tracks that have beenrun in these three TRECs.
In all cases, readers arereferred to the full TREC proceedings for papers fromthe various groups that give more details of their ex-periments.5 .1 The  Ad  Hoc  Resu l tsThe basic TREC ad hoc paradigm has presentedthree major challenges to search engine technologyfrom the beginning.
The first is the vast scale-up interms of number of documents to be searched, fromseveral megabytes ofdocuments to 2 gigabytes of doc-uments.
This system engineering problem occupiedmost systems in TREC-1, and has continued to bethe initial work for most new groups entering TREC.The second challenge is that these documents aremostly full-text and therefore much longer than mostalgorithms in TREC-1 were designed to handle.
Thedocument length issue has resulted in major changesto the basic term weighting algorithms, starting inTREC-2.
The third challenge has been the idea thata test question or topic contains multiple fields, eachrepresenting either facets of a user's question or thevarious lengths of text that question could be repre-sented in.
The particular fields, and the lengths ofthese fields, have changed across the various TRECs,resulting in different research issues as the basic en-vironment has changed.Because TREC-1 required significant system re-building by most participating roups due to the hugeincrease in the size of the document collection, the250TREC-1 results should be viewed as only very pre-liminary due to severe time constraints.
TREC-2 oc-curred in August of 1993, less than 10 months afterthe first conference, and the TREC-2 results can beseen as both a validation of the earlier experiments onthe smaller test collections and as an excellent base-line for the more complex experimentation that hastaken part in later TRECs.Table 5.1 summarizes the ad hoc task across the6 TRECs that have occurred since 1992.
It illus-trates some of the common issues that have affectedall groups, and also shows the initial use and subse-quent spread of some of the now-standard techniquesthat have emerged from TREC.Five different research areas are shown in the ta-ble, with research in many of these areas triggered bychanges in the TREC evaluation environment.
Forexample, the use of subdocuments or passages wascaused by the initial difficulties in handling full textdocuments, particularly excessively ong ones.
Theuse of better term weighting, including correct lengthnormalization procedures, made this technique lessused in TREC's 4 and 5, but it resurfaced in TREC-6to facilitate better input to relevance feedback.The first research area shown in the table is thatof term weighting.
Most of the initial participants inTREC used term weighting that had been developedand tested on very small test collections with shortdocuments (abstracts).
Many of these algorithmswere modified to handle longer documents in simpleways, however some algorithms were not amenableto this approach, resulting in some new fundamentalresearch.
The group from the Okapi system, CityUniversity, London (Robertson, Walker, Hancock-Beaulieu, & Gatford, 1994) decided to experimentwith a completely new term weighting algorithm thatwas both theoretically and practically based on termdistribution within longer documents.
By TREC-3this algorithm had been "perfected" into the BM25algorithm now in use by many of the systems inTRECs 5, 6 and 7.
Continuing along this same rowin table 5.1, three other systems (the SMART sys-tem from Cornell (Singhal, Buckley, & Mitra, 1996),the PIRCS system from CUNY (Kwok, 1996) andthe INQUERY system from the University of Mas-sachusetts (Allan, Ballesteros, Callan, Croft, & Lu,1996) changed their weighting algorithms in TREC-4based on analysis comparing their old algorithms tothe new BM25 algorithm.
By TREC-5 many of thegroups had adopted these new weighting algorithms,with the early adopters being those systems with sim-ilar structural models.TREC-6 saw even further expansion of the use ofthese new weighting algorithms (alternatively calledthe Okapi/SMART algorithm, or the Cornell imple-mentation of the Okapi algorithm).
In particular,many groups adapted these algorithms to new mod-els, often involving considerable experimentation tofind the correct fit.
For example IRIT (Boughanem& Soul6-Dupuy, 1998) modified the Okapi algorithmto fit a spreading activation model, IBM (Brown& Chong, 1998) modified it to deal with unigramsand trigrams, and the Australian National Uni-versity (Hawking, Thistlewaite, & Craswell, 1998)and the University of Waterloo (Cormack, Clarke,Palmer, & To, 1998) used it in conjunction with var-ious types of proximity measures.
Of major noteis the fact that City University also ran major ex-periments (Walker, Robertson, Boughanem, Jones,& Sparck Jones, 1998) with the BM25 weighting al-gorithm in TREC-6, including extensive xplorationof the various existing parameters, and addition ofsome new ones involving the use of non-relevant doc-uments!It could be expected that 6 years of term weight-ing experiments would lead to a convergence of thealgorithms.
However, a snapshot of the top 8 sys-tems in TREC-7 (see Table 5.1) shows that thesesystems are derived from many models and use dif-ferent erm weighting algorithms and similarity mea-sures.
Of particular note here is that new models andterm weighting algorithms are still being developed,and that these are competitive with the more estab-lished methods.
This applies both to new variationson old weighting algorithms, uch as the double log tfweighting from AT&T (Singhal, Choi, Hindle, Lewis,& Pereira, 1999) and to more major variations such asthe new weighting algorithm from TNO (Hiemstra &Kraaij, 1999), and the completely new retrieval modelfrom BBN (Miller, Leek, & Schwartz, 1999).The second new technique started back in TREC-2(the second line of table 5.1) was the use of smallersections of documents, called subdocuments, by thePIRCS system at City University of New York (Kwok& Grunfeld, 1994).
Again this issue was forced bythe difficulty of using the PIRCS spreading activationmodel for documents having a wide variety of lengths.By TREC-3 many of the groups were also using sub-documents, or passages, to help with retrieval.
But,as mentioned before, TREC's 4 and 5 saw far less useof this technique as many groups dropped the use ofpassages due to minimal added improvements in per-formance.TREC-6 saw a revival in the use of passages,but generally only for specific uses.
Whereas thePIRCS system continued to use 550-word subdocu-251?Z O~Z2~oE~r~3?D ?d o~laOooho~.~:o"~o~~D~ao0 ~g ~  .~er~GOo ~~ ?~o ~ ??
~v  .~ ~?
~ o~ ~:~~ ~ .
~r~3h0h02~03bD.~2?d09C1)Table 6: Use of new techniques in the ad hoc task252Organization Model Weighting/Similarity Phrase Imp.
CommentsOkapi groupAT&T Labs ResearchU.
MassRMIT/UM/CSIROBBNTwentyOneCUNYCornell/SabIRprobabilisticvectorinference netvectorHMMvectorspread, act.vectorBM25pivot*belief unctionBM25/cosineprobabilisticnew probabilisticavtf/RSVpivotminimal*3.6%2%2%*last reported in TREC-5*byte normalizationphrases usedbigram phrasesno phrases usedphrases used for rerankingTable 7: Models and term weight in TREC-7.ments for all its processing, most systems used pas-sages only in the topic expansion phase.
The Aus-tralian National University (Hawking et al, 1998)worked with "hot spots" of 500 characters surround-ing the original topic terms to locate new expansionterms.
AT&T (Singhal, 1998) used overlapping win-dows of 50 words to help rerank the top 50 documentsbefore selecting the final documents for use in expan-sion.
The University of Waterloo (Cormack et al,1998) used passages of maximum length 64 words toselect expansion terms, whereas Verity (Pedersen, Sil-verstein, & Vogt, 1998) used their automatic summa-rizer for this purpose.
Two groups ( Lexis-Nexis (Lu,Meier, Rao, Miller, & Pliske, 1998) and MDS (Fulleret al, 1998)) performed major experiments in theuse of passages, particularly when employed in con-junction with other methods as input to data fusion.This diverse use of passages continued in TREC-7,with passages clearly becoming one of the standardtools for experimentation.The query expansion/modification techniquesshown in the third and fourth lines of the table 5.1were started when the topics were substantially short-ened in TREC-3.
As described in section 3.2, the for-mat of the topics was modified to remove a valuablesource of keywords: the concept section.
In the searchfor some technique that would automatically expandthe topic, several groups revived an old technique ofassuming that the top retrieved ocuments are rel-evant, and then using them in relevance feedback.This technique, which had not worked on smaller col-lections, turned out to work very well in the TRECenvironment.By TREC-6 almost all groups were using variationson expanding queries using information from the topretrieved documents (often called pseudo-relevancefeedback).
There are many parameters needed forsuccess here, such as how many top documents touse for mining terms, how many terms to select, andhow to weight those terms.
There has been gen-eral convergence on some of these parameters.
Ta-ble 5.1 shows the characteristics of the expansiontools used in the top 8 systems in TREC-7.
Thesecond column gives the basic expansion model, withthe vector-based systems using the Rocchio expan-sion and other systems using expansion models moresuitable to their retrieval model.
For example, theLocal Context Analysis (LCA) method eveloped bythe INQUERY group (Xu & Croft, 1996) has beensuccessfully used by other groups.
The third columnshows the number of top-ranked documents (P if pas-sages were used), and the number of terms addedfrom these documents.
It should be noted that thesenumbers are more similar than in earlier TRECs, al-though they are still being investigated by new sys-tems adopting these techniques as there can be subtledifferences between systems that strongly influenceparameter selection.
The fourth column shows thesource of the documents being mined for terms, whichhas generally moved to the use of as much informa-tion as possible, i.e.
all the TREC disks as opposedto only those being used for testing purposes.TRECs 5, 6, and 7 saw many additional exper-iments in the query expansion area.
The OpenText Corporation (Fitzpatrick & Dent, 1997) gath-ered terms for expansion by looking at relevant doc-uments from past topics that were loosely similar tothe TREC-5 topics.
Several groups ( (Lu, Ayoub, &Dong, 1997; Namba, Igata, Horai, Nitta, & Matsui,1999)) have tried clustering the top retrieved ocu-ments in order to more accurately select expansionterms, and in TREC-6 three groups (City University,AT&T, and IRIT) successfully got information fromnegative feedback, i.e.
using non-relevant documentsto modify the expansion process.TREC-7 contained even more experiments in au-tomatic query expansion, such as the group (Man-dala, Tokunaga, Tanaka, Okumura ,  ~ Satoh, 1999)that compared the use of three different thesaurifor expansion (WordNet, a simple co-occurrance the-253Organization Expansion/Feedback Top Docs/Terms added Disks used CommentsOkapi group probabilistic Full-15/30 1-5T+D-10/30T only-6/20+titleAT&T Labs Research Rocchio 10/20+5 phrases 1-5 conservative n-richmentU.
Mass LCA 30P/50 1-5 reranking usingtitle terms be-fore expansionRMIT/UM/CSIRO Rocchio 10/40+5 phrases ?
additional ex-periments withpassagesBBN HMM-based 6/?
7 differentialweighting ontopic partsTwentyOne Rocchio 3/200 ?CUNY LCA 200P/?
1-5Cornell/SabIR Rocchio 30/25 4-5 clustering,rerankingTable 8: Characterization f query expansion used in best automatic ad hoc TREC-7 runs.saurus and an automatically built thesaurus usingpredicate-argument structures).
Of particular noteis the AT&T (Singhal et al, 1999) investigationinto "conservative enrichment" to avoid the addi-tional noise caused by using larger corpora (all fivedisks) for query expansion.Groups that build their queries manually alsolooked into better query expansion techniques start-ing in TREC-3 (see fourth line of table 5.1).
At firstthese xpansions involved using other sources to man-ually expand the initial query.
However the rules gov-erning manual query building changed in TREC-5to allow unrestricted interactions with the systems.This change caused a major evolution in the manual ?query expansion, with most systems not only manu-ally expanding the initial queries, but then looking atretrieved ocuments in order to further expand thequeries, much in the manner that users of these sys-tems could operate.
Two types of experiments werenotable in TREC-5: those that could be labelled as"manual exploration" runs and those that involveda more complex type of human-machine interaction.The first type is exemplified by the GE group (Strza-lkowski et al, 1997), where the task was to ask usersto pick out phrases and sentences from the retrieved ?documents oadd to the query, in hopes that this pro-cess could be imitated by automatic methods.
TheCLARITECH group (Milic-Frayling, Evans, Tong, &Zhai, 1997) is a good example of the second type of ?manual TREC-5 runs.
They examined a multi-stageprocess of query construction, where the goal was toinvestigate better sets of tools that allow users toimprove their queries, including different sources forsuggestions ofexpansion terms and also various levelsof user-added constraints o the expansion process.Many of the manual experiments seen in bothTREC-6 andTREC-7, however, hark back to the sim-pler scenario f having users edit the automaticaily-generated query, or having users select documentsto be used in automatic relevance feedback.
Severalof the groups had specific user strategies that theytested.GE Corporate R&D/Rutgers University (Strza-lkowski, Lin, & Perez-Carballo, 1998) usedautomatically-generated summaries of the top30 documents retrieved as sources of manually-selected terms and phrases.CLARITECH Corp. (Evans, Huettner, Tong,Jansen, & Bennett, 1999) performed a userexperiment measuring the difference in per-formance between two presentation modes: aranked list vs a clustered set of documents.University of Toronto (Bodner & Chignell, 1999)used their dynamic hypertext model to build thequeries.Lexis-Nexis (Rao, Humphrey, Parhizgar, Wilson,& Pliske, 1999) experimented with human rele-254vance feedback as opposed to automatic feedbackfrom the top 20 documents.The final line in table 5.1 shows some of the otherareas that have seen concentrated research in thead hoc task.
Data fusion has been used in TRECby many groups in various ways, but has increasedin complexity over the years.
For example, a projectinvolving four teams led by Tomek Strzalkowski hascontinued the investigation of merging results frommultiple streams of input using different indexingmethods ((Strzalkowski et al, 1997, 1998, 1999).In TREC-6, several groups such as Lexis-Nexis (Luet al, 1998) and MDS (Fuller et al, 1998) used mul-tiple stages of data fusion, including merging resultsfrom different erm weighting schemes, various mix-tures of documents and passages, and different queryexpansion schemes.The INQUERY system from the University of Mas-sachusetts has worked in all TREC's to automati-cally build more structure into their queries, basedon information they have "mined" from the top-ics (Brown, 1995).
Starting in TREC-5, there havebeen experiments by other groups to use more in-formation from the initial topic.
Lexis-Nexis (Luet al, 1997) used the inter-term distance betweennouns in the topic.
Several other groups have madeuse of term proximity features (Australian NationalUniversity (Hawking, Thistlewaite, & Bailey, 1997),University of Waterloo (Clarke & Cormack, 1997) ,and IBM) to improve retrieval scores, while others(CUNY (Kwok & Grunfeld, 1997), AT&T (Singhal,1998), and INQUERY (Allan, Callan, Sanderson, Xu,& Wegmann, 1999)) have used the initial topic tolook for clues that would suggest a need for moreemphasis on certain topic terms.
TREC-7 had twoadditional groups working with the use of term co-occurrance and proximity as alternative methods forranking (see (Braschler, Wechsler, Mateev, Mitten-dorf, & Sch~iuble, 1999) and (Nakajima, Takaki, Hi-rao, & Kitauchi, 1999)).A final theme that has continued throughout allthe TREC conferences has been the investigation ofthe use of phrases in addition to single terms.
Thishas long been a topic for research in the informa-tion retrieval community, with generally unsuccess-ful results.
However there was initially hope thatuse of phrases in these much larger collections wouldbecome critical and almost all groups have experi-mented with phrases.
In general these experimentshave been equally unsuccessful.The fourth column of table 5.1 shows thewidespread use of phrases in addition to single termsin TREC-7, but the minimal improvement from theiruse.
The biggest improvement reported in the paperswas 3.6% from the INQUERY group at the Univer-sity of Massachusetts (Allan et al, 1999).
Whereasmost of the other groups are also using phrases, manydid not bother to test for differences due to minimalresults in earlier years.
Cornell/SabIR reported 7.7%improvement in TREC-6, but this is the improve-ment on top of the initial baseline, not the improve-ment after expansion.
Private conversations with sev-eral of these groups indicate that these improvementsare likely to be much less if measured after expan-sion.
As is often the case, these minimal changesin the averages cover a wide variation in phrase per-formance across topics.
A special run by the Okapigroup (many thanks) showed less than a 1% averagedifference in performance, but 19 topics helped byphrases, 14 hurt, and the rest unchanged.
Whereasthe benefit of phrases is not proven, they are likely toremain a permanent tool in the retrieval systems in amanner similar to the earlier adoption of stemming.It is interesting to note that many of these groupsare using different phrase "gathering" techniques.The Okapi group has a manually-built phrase listwith synonym classes that has slowly grown overthe years based on mostly past TREC topics.
Theautomatically-produced INQUERY phrase list wasnew for TREC-6 (Allan et al, 1998), the Cornell listwas basically unchanged from early TRECs, and theBBN list was based on a new bigram model.The creation of two formal topic lengths in TREC-5has inspired many experiments comparing results us-ing those different opic lengths, and the addition ofa formal "title" in TREC-6 increased these investiga-tions.
Table 5.1 shows the results (official and unof-ficial as reported in the papers) of the top 8 TREC-7groups howing their use of different topic parts.
Thesecond column gives the various topic parts used byeach group (T = title, D = description, N = narra-tive).
The third column gives the average precisionusing only the description and title.
The fourth andfifth columns give the corresponding performance ofthe systems using either only the title or using thefull topic (all topic parts).Note that most of the best runs use the full topic.However there is now a smaller performance differ-ence between runs that use the full topic and runsthat use only the title and description sections thanwas seen in earlier TRECs.
This is most likely dueto improved query expansion methods, but could bedue to variations across topic sets.
It should be notedthat the improvement going to the full topic is only1% for several groups.
The decrease in performanceusing only the title is more marked, ranging from 4%255Long Desc TitleOkapi 28 13 9CUNY 27 10 13Cornell 22 17 11Table 10: Number of TREC-7 topics performing bestby topic length.to 22%.
The TREC-7 title results should be a truermeasure of the effects of using the title only thanTREC-6, where the descriptions were often missingkey terms.
However, it is not clear how representativethese titles are with respect o very short user inputsand therefore title results should best be viewed ashow well these systems could perform on very short,but very good user input.Looking at individual topic results hows a less con-sistent picture.
Table 5.1 shows the number of topicsthat had the best performance from among a group'sthree runs using different input lengths.
Not onlyis there a wide variation across topics, there is alsoa wide variation across systems in that topics thatwork best at a particular length for one group didnot necessarily work best at that length for the othergroups.5 .2  The  Rout ing  Resu l tsThe routing evaluation used a specifically selectedsubset of the training topics against a new set of testdocuments, but there have always been difficulties inlocating appropriate t sting data for the routing task.TREC-3 was forced to re-use some of the trainingdata, and TREC-4 performed routing tests using theFederal Register (with new data) for 25 of the topics,and using training data and "net trash" for testingthe other 25 topics.
This situation was clearly notideal and for TREC-5 NIST held back decisions onthe routing topics until a new data source could befound.When the FBIS data became available, it was de-cided to pick topics that had many relevant docu-ments in the Associated Press data, on the assump-tion that the FBIS data would be similar to AP.
Be-cause of delays in getting and processing the data,this assumption could not be checked out, and prob-lems arose that will be discussed later.It should be noted that the routing task in TREChas always served two purposes.
The first is its in-tended purpose: to test systems in their abilities touse training data to build effective filters or profiles.The second purpose, which has become equally im-portant in the more recent TRECs, is to serve as alearning environment for more effective retrieval tech-niques in general.
Groups use the relevance judg-ments to explore the characteristics of relevant doc-uments, such as which features are most effective touse for retrieval or how to best merge results frommultiple queries.
This is more profitable than simplyusing the previous TREC results in a retrospectivemanner because of the use of completely new testingdata for evaluation.A focus on using the training data as a learningenvironment was particularly prevalent in TREC-5.Cornell (Buckley, Singhal, & Mitra, 1997) used therelevant and non-relevant documents for investiga-tions of Rocchio feedback algorithms, including morecomplex processes of expansion and weighting.
TheUniversity of Waterloo (Clarke & Cormack, 1997) in-teractively searched the training data for co-occurringsubstrings and GE (Strzalkowski et al, 1997) ranmajor experiments in data fusion to test their newstream-based architecture.
In each of these cases theexperiments are assumed to lead to better ways ofdoing the routing task, and also to new approachesfor the ad hoc task.Three experimental themes dominate most routingexperiments in TREC-5.
The first is the discovery ofoptimal features (usually single terms) for use in thequery or filter.
The Okapi System from City Univer-sity, London (Beaulieu et al, 1997) continued its ex-periments in repeatedly trying various combinationsof terms to discover the optimal set, but for TREC-5used subsets of the training data.
The University ofCalifornia at Berkeley (Gey, Chen, He, Xu, & Meggs,1997) concentrated on further investigations of theuse of the chi-square discrimination measure to lo-cate large numbers of good terms, and the Swiss Fed-eral Institute of Techology (ETH) (Ballerini et al,1997) tried three different feature selection methods,including the chi-square method, the RSV (OKAPI)method, and a new method, the U measure.
Xe-rox (Hull et al, 1997) also investigated a new featureselection method, the binomial ikelihood ratio test.The second theme was the use of co-occurring termpairs in the training data to "expand" the query.Four groups experimented with locating and incorpo-rating co-occurring pairs of terms, including the IN-QUERY group from the University of Massachusettsin both TREC-4 and TREC-5 (Allan et al, 1996,1997), and Cornell University in TREC-5 (Buckleyet ai., 1997).
As mentioned before, Waterloo interac-tively looked for word-pairs or co-occurring strings tomanually add to their query.
ETH used the OKAPIRSV values to formally motivate a series of experi-256OrganizationOkapi groupAT&T Labs ResearchU.
MassRMIT/UM/CSIROBBNTwentyOneCUNYCornell/SabIRT,D,NT,D,NT,D,NT,D,NTopic Parts D + TT,D,N 0.281T,D 0.2960.252T,D 0.281T,D,N 0.2540.254*T only I Full Topic Comments0.253 (-10%)0.249 (-16%)0.220 (-22%)0.243 (-4%)0.239 (-6%)0.284 (1%)0.274 (9%)0.285 (1%)0.2800.2790.266 (5%)0.267 (5%)fused run-0.296title filtered run-0.282with phrases-0.272*description onlyTable 9:TREC-7 Performance using variations in topic length.meats using co-occurring terms within different por-tions of the document (within sentence, within para-graph, etc.)
as different methods of constructingqueries.
These multiple representations of the querywere then linearly combined, with the parameters forthat combination discovered using logistic regressionon the training data.The  third theme in the routing experiments wasthe continuing effort to use only subsets of the train-ing data.
The  number  of judged documents  per topicis on the order of 2000 or more, and this can be eom-putationally difficult for complex  techniques.
Effi-ciency has motivated CUNY experiments (the P IRCSsystem) since TREC-3  where they tried using onlythe "short" documents  for training.
In TREC-5  thisgroup (Kwok  & Grunfeld, 1997) used genetic algo-rithms to select the optimal set of training docu-ments.
Cornell (in TREC-5)  used a new "query zone"technique to subset the training documents  so thatnot all non-relevant documents  were used for train-ing.
The  goal was not just improved efficiency, butalso improved effectiveness in that training was moreconcentrated on documents  that the Cornell systemwas likely to retrieve.There is another issue that suggests the use of sub-sets: the prob lem of overfitting the queries/methodsto the training data.
This was specifically emphasizedin the City system, where  they used different subsetsof the training data for locating features, and usedcombinations of runs for their final results.
Xeroxused subsets to reduce overfitting, with their subsetsbased on finding documents  within a "local zone" tothe query (a predecessor to the query zoning tech-nique used by Cornell).
The  Xerox  paper providesmore  discussion of the overfitting prob lem and sug-gests some additional techniques to avoid it.As  in the ad hoc task, there is a heavy adoptionrate across groups for successful techniques.
For thead hoc task these techniques revolve around betterways of handling the initial topic, or use of the top Xdocuments for relevance f edback.
Because of the ex-istence of training data in routing, the routing experi-ments have generally not used the topic itself heavily,but constructed queries mainly based on the train-ing data.
The success of these techniques thereforerevolves around how well the test data matches thetraining data, and also on how tuned the techniquesare to the particular training data.TREC-5 used AP documents as training data, withFBIS material for test data.
Whereas the types ofdocuments are similar, the domains of the documentsdid not always match.
For some topics there was agood match of training and test data, but for othersthe match was very poor, and very few relevant doc-uments were found for those topics.
Four topics hadzero relevant documents in the test set, and an addi-tional six topics had only one or two relevant docu-ments.
Additionally there was a serious mismatch onthe number of relevant documents for a topic in thetraining data and in the test data.
Even after drop-ping the four topics with no relevant documents fromthe evaluation, the results are still heavily affected bythe mismatch.
The overall results for TREC-5 werenot better than for TREC-4 (or TREC-3.In TREC-6 an attempt was made to have a closematch between the training and test data.
Sincethe TREC-5 routing task had used a documentstream from the Foreign Broadcast Information Ser-vice (FBIS) as its test set, a new stream of FBISdocuments was selected as the TREC-6 test set.
TheTREC-6 routing topics consisted of 38 topics used inTREC-5 that had at least 5 relevant documents in theoriginal FBIS stream, plus nine new topics (that hadminimal training data on the original FBIS stream).The histogram in Figure 7 shows that the trainingand test data do have similar numbers of relevantdocuments for most topics.The following gives the various experiments hatwere run by the 8 top performing systems in theTREC-6 routing task.2579008007006OO500~D400Z300200100Number  Re levant  Tra in ing  vs.  Test  FB IS//001 1 ii1 3 4 5 15 l l  12 23 24 44 54 58 77 3'8 82 94 95 100108111114118119123125126142154161173185187189192194202228240Topic Number\ [ \ ]  FBIS Training Rels\ [ \ ]  FBIS Test RelsFigure 7: Comparison of the number of relevant documents in the training and test FBIS collections.?
AT&T Labs Research (Singhal, 1998) added themachine learning technique of boosting to thequery refinement phase of the Cornell TREC-5routing algorithm (which includes the use ofword pairs, DFO optimization, and query zones).?
City University, London (Walker et al, 1998) ex-plored iterative methods of term weighting withthe goal of avoiding overfitting.?
Cornell/SaBIR esearch (Buckley, Mitra, Walz,& Cardie, 1998) also used a variant of the basicCornell TREC-5 routing approach, adding Su-perConcepts o the routing query.?
Queens College, CUNY (Kwok, Grunfeld, & Xu,1998) combined results from five separate com-ponent runs; this combined result is superior toeach of the individual components.?
University of Waterloo (Cormack et al, 1998)interactively refined a set of Boolean queries intoa single tiered Boolean query for each topic.?
Claxitech Corporation (Milic-Frayling, Zhai,Tong, Jansen, & Evans, 1998) explored the ben-efits of using different term selection methods indifferent parts of the query refinement process.For this run they developed ifferent queries us-ing different erm selection strategies and then,for each topic, selected the query that performedthe best on the training data.?
MSI/IRIT/SIG/CERISS (Boughanem & Soul&Dupuy, 1998) continued their work with aspreading activation model by expanding querieswith the top 30 terms from relevance backprop-agation.?
Swiss Federal Institute of Technology (ETH)(Mateev, Munteanu, Sheridan, Wechsler,Sch~iuble, 1998) also performed a combinationrun where one component run selected querywords and phrases based on the U-measure.The best mean average precision for a routingrun in TREC-6 was .420, a 9% improvement overTREC-5's best of .386.
However, given that theTREC-6 task was designed to use a homogeneousdata set whereas the TREC-5 test data were differ-ent from the training data, a greater improvementwas expected.
At this point, it is unclear why thedifference was not greater.
It is possible that whilethe numbers of relevant documents in the trainingand test set are comparable, the relevant documents258in each set don't "look like" each other.
However, thisis unlikely since both sets of documents come from acommon source.
It is also possible that the mismatchbetween training and test sets is not as significant afactor as was thought.Another hypothesis uggested by (Singhal, 1998)is that the relevance judgments are less consistentfor routing than they are for the ad hoc task, andthat this inconsistency prevents the machine learningmethods that are prevalent in the task from perform-ing well.
Since some routing topics have been usedmany times, and therefore have relevance judgmentsspanning many years, the judgments are likely to beless consistent than for the ad hoc task.
It may be in-structive to explore the stability of the routing tech-niques in the face of different relevance judgments,especially given that real user judgments are knownto be extremely volatile (Schamber, 1994).Because of operational constraints on the overallTREC program, it was decided to pursue further in-vestigations in routing within the very closely relatedfiltering track.
For this reason there was no routingtask in TREC-7, but there was a routing option inthe filtering track.6 THE TRACKSOne of the goals of TREC is to provide a commontask evaluation that allows cross-system comparisons,and this has proven to be a key strength in TREC.A second major strength is the loose definition ofthe ad hoc task, which allows a wide range of ex-periments.
The addition of secondary tasks (calledtracks) in TREC-4 combined these strengths by cre-ating a common evaluation for retrieval subproblems.TREC participants are free to turn in results for any,or all, or none, of the tracks.The tracks have had a significant impact on TRECparticipation.
Figure 8 shows the number of exper-iments performed in each TREC, where the set ofruns submitted for a track by one group is countedas one experiment.
The number of experiments in-creased each year through TREC-6 then decreased inTREC-7, mostly due to the elimination of the rout-ing main task and the Chinese track.
The numberof participants performing the ad hoc task continuesto grow, with 42 groups taking part in TREC-7 com-pared to 31 in TREC-6.
The number of participantsin each of the TREC-7 tracks and the correspondingTREC-6 participation is given below.CLIRfilteringHPinteractivequerySDRVLCTREC-6 \[ TREC-713I0590137912482106The set of tracks run in any particular year de-pends on the interests of the participants and spon-sors, as well as on the suitability of the problem tothe TREC environment.
Some initial tracks havebeen discontinued because the goals of the track weremet.
For example, the Spanish track, an ad hoc taskin which both topics and documents are in Spanish,was discontinued when the results demonstrated thatcurrent retrieval systems can retrieve Spanish doc-uments as effectively as English documents.
Othertracks, such as the interactive track, have been runeach year, but have changed their focus in differentyears.
Each track has a set of guidelines developedunder the direction of the track coordinator.
The setof tracks and their primary goals are listed below.See the track reports in the various TREC proceed-ings for a more complete description of each trackand its results.6 .1 The  Span ish  and  Ch inese  TracksTrack reports- (Smeaton & Wilkinson, 1997; Wilkin-son, 1998)The first non-English track was started in TREC-3.Four groups worked with 25 topics in Spanish, us-ing a document collection consisting of about 200megabytes (58,000 documents) of a Mexican news-paper from Monterey (El Norte).
Since there was notraining data for testing (similar to the startup prob-lems for TREC-1), the groups used simple techniques.The major result from this very preliminary experi-ment in a second language was the ease of porting theretrieval techniques across languages.
Cornell (Buck-ley, Salton, Allan, & Singhal, 1995) reported thatonly 5 to 6 hours of system changes were necessary(beyond creation of any stemmers or stopword lists).In TREC-4 10 groups took part, using the samedocument collection and 25 new topics.
The finalround of Spanish retrieval took place in TREC-5,again with 25 new topics and also with additionaltext (1994 newswire from Agence France Presse, in-cluding 308 megabytes or 173,950 documents).
Sevengroups took part in Spanish, with several of thembuilding more elaborate procedures for testing, such259140r~?
~,,-i12010080604020TREC 1 TREC 2 TREC 3 TREC 4Figure 8: Number of TRECTREC 5 TREC 6 TREC 7experiments by TREC task~i Ad Hoc\ [ \ ]  RoutingInteractive~ Spanish\ [ \ ]  ConfusionDB MergingFilteringi"~:!i ChineseNLPSpeech\ [ \ ]  X LingualHigh Precision~ VLC\ [ \ ]  Queryas Spanish POS taggers.
But in the main these didnot improve performance and the major outcome ofthe Spanish track was that most of the techniquesused in English retrieval, including the advanced onesused in the ad hoc task, can be successfully appliedto Spanish.The purpose of the Chinese track was to investi-gate retrieval performance for a language whose or-thographics are not word-oriented.
Participants per-formed an ad hoc search in which both the topics andthe documents were in Chinese.
The document setwas a collection of articles selected from the PeoplesDaily newspaper and the Xinhua newswire, a total of168,811 documents in 170 megabytes.
Twenty-eighttopics were created for the track in TREC-5 and anadditional 26 topics for TREC-6.Nine groups submitted Chinese runs in TREC-5,and since it was the first year for Chinese in TREC,most groups concentrated on segmentation issues.In TREC-6 there were 12 participating roups, andagain the majority of the experiments compared if-ferent methods of segmenting the text into retrievalfeatures.
In general, approaches that used singlecharacters or bi-grams as features were competitivewith word-based approaches and had the advantageof not requiring complicated segmentation schemes.A confounding factor in the analysis of the retrievalresults was that the retrieval effectiveness was quitehigh (the median mean average precision was greaterthan 0.5), and was similar across systems.
It was dif-ficult to distinguish more effective techniques whenall techniques appear to work equally as well.
With-out more testing, it was not possible to determinewhether the TREC-6 topics were simply easy, or ifthere is something inherent in Chinese that facilitatesretrieval.
Further testing was postponed until newChinese data could be assembled.6 .2  The  Cross  Language (CL IR)T rackTrack reports- (Sch~uble ~ Sheridan, 1998;Braschler, Krause, Peters, & Sch~uble, 1999)The CLIR task focuses on searching for documentsin one language using topics in a different language.The first CLIR track was held in TREC-6 (Sch~uble& Sheridan, 1998).
Three document sets were used: a250 MB set of French documents from the Swiss newsagency Sehweizerisehe Depesehen Agentur (SDA); a330 MB set of German documents from SDA plusa set of articles (200 MB) from the newspaper NewZurich Newspaper (NZZ); and a 750 mB set of Englishdocuments from the AP newswire.
All of the doc-ument sets contain news stories from approximatelythe same time period, but are not aligned or speciallycoordinated with one another.
A set of 25 topics that260were translated into each of the languages was alsoprovided.
Participants earched for documents in onetarget language using topics written in a different lan-guage.
In addition, participants were asked to per-form a monolingual run in the target language to actas a baseline.Thirteen groups participated in the TREC-6 CLIRtrack.
Three major approaches to cross-language r -trieval were represented: machine translation, whereeither the topics or the documents were translatedinto the target language; the use of machine-readablebilingual dictionaries or other existing linguistic re-sources; and the use of corpus resources to train orotherwise enable the cross-language r trieval mech-anism.
The approaches all behaved similarly inthat some group obtained good cross-language per-formance for each method.
In general, the best cross-language performance was between 50%-75% as ef-fective as a quality monolingual run.The TREC-7 task expanded on this beginning.The document set for the TREC-7 track consistedof all the documents used in the TREC-6 track plusthe Italian version of the SDA for the same time pe-riod.
Participants were provided with a new set of 28topics (with translations available in English, French,German, and Italian), and used one topic languageto search the combined document set.
That is, asingle run retrieved documents written in differentlanguages.
To enable participation in the track bymore groups, a second task was also defined in whichEnglish topics were run against he combined Frenchand English document set.The TREC-7 track also defined an optional sub-task.
The subtask used a different document collec-tion, a 31,000 document structured atabase (format-ted as SGML fielded text data) from the field of socialscience plus the NZZ articles, and a separate set of28 topics.
The rational of the subtask was to studyCLIR in a vertical domain (i.e.
social science) wherea German/English thesaurus is available.Nine groups participated in the TREC-7 CLIRtrack, with five groups performing the test on thefull four-language collection, and seven groups per-forming the test on the English and French collec-tion.
No runs were submitted for the optional sub-task; however this subtask is planned to be repeatedin TREC-8 now that groups have more experiencewith cross language retrieval.
The results of the trackdemonstrate that very different approaches to cross-language retrieval can lead to comparable retrievaleffectiveness.The construction of the cross language test col-lection differs from the way the other TREC collec-tions have been created.
The set of topics createdfor the track were developed at four different institu-tions: NIST (English); EPFL Lausanne, Switzerland(French); University Bonn, Germany (German); andCNR, Pisa, Italy (Italian).
Each institution createdtopics that would target documents in their corre-sponding language.
The relevance judgments for alltopics for a particular document language were alsomade at the site responsible for that language.
This isthe first time that TREC has used multiple relevanceassessors for a single topic.6 .3  The  F i l te r ing  TrackTrack reports-  (Lewis, 1997; Hull, 1998, 1999)As mentioned before, the routing task investigatesthe performance of systems that use standing queriesto search new streams of documents.
As the routingtask is defined in TREC, participants use old top-ics with existing relevance judgments to form rout-ing queries.
These queries are then run against apreviously unseen document collection to produce aranked document list.
However, real routing appli-cations generally require a system to make a binarydecision whether or not to retrieve the current docu-ment, not to form a ranking of a document set.
Thefiltering track was started in TREC-4 to address thismore difficult version of the routing task.The question of how to evaluate filtering runs hasbeen a focus of the filtering track since its inception.Since filtering results are an unordered set of docu-ments, the rank-based measures used in the ad hocand routing tasks are not appropriate.
The  main  ap-proach has been to try utility functions as measuresof the quality of the retrieved set--the quality is com-puted as a function of the benefit of retrieving a rele-vant document  and  the cost of retrieving an irrelevantdocument .In TREC-5 ,  a family of three functions was  triedin an investigation of how retrieval was  affected bychanges in the relative worth  of retrieving a relevantdocument  versus not retrieving a nonrelevant docu-ment.
There  were seven participating groups, but themajor  outcome was  the awareness of the difficulty ofdefining an appropriate utility measure.In TREC-6  two different utility functions wereused:F1 = 3R + - 2N +F2 = 3R +-N +-R-261where R + is the number of relevant documents thatare retrieved, R -  is the number of relevant documentsthat are not retrieved, and N + is the number of non-relevant documents that are retrieved.A problem with utilities as measures i  that differ-ent topics have widely varying possible utility values,and these utilities cannot be normalized.
Thus, util-ities cannot be meaningfully averaged or comparedacross topics.
A second measure, average set preci-sion (ASP) defined as the product of recall and preci-sion, was therefore introduced in TREC-6.
Unfortu-nately, ASP suffers from its own drawback.
When norelevant documents are retrieved, ASP is 0 regardlessof how many non-relevant documents are retrieved.This is a problem in filtering evaluation since know-ing when to NOT retrieve documents i an importantpart of the filtering task.The F1 utility measure defined above rewards asystem with 3 "points" for every relevant documentdocument it retrieves, and penalizes the system two"points" for every nonrelevant document it retrieves.While these benefit and cost values seem reasonable,they define a level of performance that is quite chal-lenging for current systems to meet.
Ten groups par-ticipated in the TREC-6 filtering track and submitteda total of 17 runs that were optimized for the F1 mea-sure.
The best of these runs had a positive utility for33 (of 47) topics, and the median of the 17 runs hada positive utility for just 20 topics.
Since retrievingno documents has an F1 utility of 0, retrieving nodocuments would result in a better F1 utility thancurrent systems obtain on average.The F2 utility measure is even more demandingsince systems are penalized for not retrieving relevantdocuments (and thus retrieving no documents alsoresults in a negative utility).
Of the 17 runs optimizedfor the F2 utility, only 10 topics had a positive medianF2 utility.The TREC-7 filtering track contained three tasksof increasing difficulty (and realism).
For each task,topics 1-50 and the AP newswire collection on Disks1-3 were used (with different splits into training andtest sets, depending on the task).
The first task wasthe traditional routing task.
The second task was abatch filtering task in which systems are given topicsand relevance judgments as in the routing task, andmust then decide whether or not to retrieve ach doc-ument in the test portion of the collection.
This taskis what previous filtering tracks in TRECs 5 and 6had performed.The third task, and the focus of the TREC-7 track,was an adaptive filtering task.
In this task, a filteringsystem starts with just the query derived from thetopic statement, and  processes documents  one at at ime in date order.
If the system decides to retrievea document ,  it obtains the relevance judgment  for it,and  can modi fy  its query as desired.In TREC-7  two different utility functions wereused:F1 = 3R + - 2N +F3 = 4R + - N +where R + and N + are the number of relevant andnon-relevant documents retrieved, respectively.
Anapproach to scaling and normalizing utilities was in-troduced in this year's track(Hull, 1999)Twelve groups submitted at least one TREC-7 fil-tering run.
A total of 46 runs were submitted, con-sisting of 10 routing runs, 12 batch filtering runs,and 24 adaptive filtering runs.
The track resultsdemonstrated that adaptive filtering is a challeng-ing problem for current systems.
Indeed, when usingthe F1 utility measure to evaluate performance, the"baseline" system which retrieves no documents wasthe most effective system overall.
Comparison withbatch filtering results show that setting an appropri-ate threshhold for when to retrieve a document is acritical, and difficult, task in adaptive filtering.6 .4  The  H igh  Prec i s ion  TrackTrack reports-  (Buckley, 1998, 1999a)TREC-6 was the first running of the high preci-sion track.
The task in the track was to retrieve tenrelevant documents for a topic within five minutes(wall clock time).
Users could not collaborate on asingle topic, nor could the system (or user) have pre-vious knowledge of the topic.
Otherwise, the userwas free to use any available resources as long as thefive minute time limit was observed.
The task is anabstraction of a common retrieval problem: quicklyfind a few good documents to get a feel for the topicarea.Since the track guidelines put no limits on who theuser could be, an implicit assumption of the track isthat the runs were performed by system experts.
Assuch, the track provides an upper-bound on the ef-fectiveness obtainable by the systems.
The 5-minutetime limit was selected so that the intrinsic effective-ness of the system, the system efficiency, and the userinterface would all be tested by the task.The TREC-6 high precision track used the same50 topics and document set at used in the TREC-6ad hoc task.
Five groups participated in the HP262track, submitting a total of 13 runs.
The mean over50 topics of the precision after ten documents were re-trieved ranged from a high 0.6020 to a low of 0.3360.The least effective runs were a set of completely au-tomatic runs submitted to see how automatic runswould fare; the results confirm that user involvementis indeed beneficial.
However, the best result was arun in which the user simply provided yes/no rele-vance judgments as input for a sophisticated (auto-matic) relevance feedback algorithm.
This suggeststhat user involvement does not need to be extensiveThe TREC-7 high precision track used the same50 topics and document set as used in the TREC-7ad hoc task.
Four groups participated, submitting atotal of seven runs.
One finding of the track was thatretrieving 15 good documents i a simple enough taskfor current retrieval systems that disagreements be-tween the searcher and the assessor egarding whatconstitutes a relevant document bounds performance.However, new time-based evaluation measures intro-duced in the track offered a possible solution.6 .5  The  In teract ive  TrackTrack reports-  (Over, 1997, 1998, 1999)One of the first tracks to be started in TREC, theinteractive track studies text retrieval systems in in-teraction with users and is interested in the processas well as the results.
Effectively supporting the usersof a retrieval system has become an increasingly im-portant problem as more and more text is made elec-tronically accessible, and larger numbers of end users(as opposed to a relatively small group of trainedintermediaries) perform searches.
Yet designing re-trieval experiments that can be fairly evaluated andthat produce interpretable results when humans areincluded in the loop is especially challenging since itis difficult to isolate the effects of the different factorsthat contribute to overall effectiveness.Interactive xperiments include a third factor, thesearcher, to the topic and retrieval system factors in-herent in all retrieval experiments.
An ideal experi-mental design tests all combinations of all settings ofall factors with repetitions, but with human subjectssuch a design is not feasible within a single site andcertainly not across sites.
For example, the same usercannot perform a search for a topic more than oncebe.cause the experience gained during the first searchbiases the second search, but logistics prevents ran-domly assigning searchers from one site to performsearches on another site's experimental system.
Find-ing a sufficient number of subjects is also difficult: ex-perience indicates that reliably detecting significantsystem effects requires relatively many searches.
Un-fortunately, reducing the number of required searchesby narrowing the focus of the investigation makesgeneralizing any conclusions difficult.Based on the lessons learned from the TREC-4track on how difficult it was to fairly compare resultsin interactive xperiments, the track concentrated onexperimental design in TREC-5.
Unfortunately, thefinal design was not decided until late in the TRECcycle, and only two groups were able to participate.However the same design was used in TREC-6.The goal of the TREC-6 interactive track was tocompare systems across sites.
To this end, the trackdeveloped and employed a new method for compar-ing interactive IR systems across different sites.
Themethod involved comparing the particular retrievalsystem used at a site (an experimental system) toa common control system that was also run at eachsite.
The direct comparison between the experimen-tal and control systems was used to derive a measureof how much better the experimental system was thanthe control, independent of topic, searcher, and anyother site-specific effects.
Different experimental sys-tems could then be indirectly compared across sitesrelative to the common control.The experiment used six slightly modified ad hoctopics and the Financial Times newspaper data asthe document collection.
The searcher task involvedsix searches (three on control, three on the exper-imental system) to find and save documents whichtaken together contained as many answers as possi-ble to the question stated or implied by the topic.Nine participants used this experimental frameworkto pursue their own research goals, and to contributedata to a cross-site comparison of systems.
The eval-uation measures used were recall and precision de-fined in terms of the set of all possible answers asdetermined by NIST assessors.
Participants also re-ported extensive data on the characteristics of eachsearcher and of each searcher's interactions with boththe control and experimental system.As a first step in analyzing the cross-site data, thebest model for each site's results in terms of whichfactors and interactions to include was determined.Then a cross-site analysis of variance (ANOVA) wasperformed, which indicated that there was a signif-icant difference between some systems.
However, amultiple comparisons test (Tukey's), run to deter-mine which systems differed, found no significantpair-wise differences.The effectiveness of using a control system to re-move the site effect from cross-site comparisons was263an assumption of the track design and so could not betested by it.
Additional experiments before and af-ter TREC-6 did address the effectiveness of the con-trol (i.e., the equivalence of the direct and indirectcomparison of systems) but neither confirmed nor re-futed its effectiveness (Lagergren & Over, 1998; Swan& Allan, 1998).
As a practical matter, it is difficultto justify the cost of adding a control system to anexperimental design in the absence of clear positiveevidence for its effectiveness.The TREC-7 track used a similar experimentalframework, but without the requirement to use thesingle control system.
The framework both defineda common task for participants to perform and pre-scribed an experimental matrix.
The search task usedthe title and description sections plus a special "In-stances" section of eight ad hoc topics; the documentssearched were the Financial Times collection fromDisk 4.
The topics each described a need for infor-mation of a particular type such that multiple distinctexamples or instances of that information were con-tained in the document collection.
The searchers jobwas to save documents covering as many distinct an-swers to the question as possible in a 15-minute timelimit.
The NIST assessor for the topic made a com-prehensive list of instances from the documents sub-mitted by the track.
The effectiveness of the searchwas evaluated by the fraction of total instances forthat topic covered by the search (instance recall) andthe fraction of the documents retrieved in the searchthat contained an instance (instance precision).
Par-ticipants were also required to collect demographicand psychometric data from the searchers, and to re-port extensive data on each searcher's interactionswith the search systems.The experimental matrix defined how searchersand topics were to be divided among the experimen-tal and control systems.
(Participants were free tochoose whatever systems they wanted to serve as ex-perimental and control.
That is, the track did notattempt o coordinate cross-site comparisons or testparticular hypotheses.)
The matrix was based on alatin square design, which provides the desired uncon-taminated estimate of the difference between the sys-tems.
The minimum experiment defined by the de-sign required eight searchers, with each searcher per-forming four searches with each of the two systems.The eight-searcher minimum was imposed since theresults of the TREC-6 track suggested that with eighttopics at least eight searchers are required to obtainstatistically significant results.Eight groups participated in the interactive track,performing a total of ten experiments.
Since compar-ison of systems across sites was  not supported by theexperimental design, the results of the track need tounderstood in the context of the particular researchgoals of the individual research groups.6.6  The  Query  Track(Track report-  (Buckley, 1999b))The query track was a new track in TREC-7 whosegoal was to create a large query collection.
Thevariability in topic performance makes it impossi-ble to reach meaningful conclusions regarding query-dependent processing strategies unless there is a verylarge query set--much larger than the sets of 50 top-ics used in the TREC collections.
The query trackwas designed as a means for creating a large set ofdifferent queries for an existing TREC topic set, top-ics 1-50.Participants in the track created ifferent ypes ofqueries from the topic statements and/or relevancejudgments.
A query of a given type was created foreach of the 50 topics, forming one query set.
Fivedifferent query types were used:Very  short :  two or three words extracted from thetopic statement.Sentence:  an English sentence based on the topicstatement and the relevant documents.Manua l  feedback:  an English sentence based onreading 5-10 relevant documents only (by some-one who doesn't know the topic statement).Manua l  s t ruc tured  query:  a manually con-structed query based on the topic statementand relevant documents.
The use of operatorssupported by the participant's system wasencouraged.
The  T IPSTER DN2 format wasused to represent the query structure.Automat ic  s t ruc tured  query :  a query con-structed automatically f rom the topic statementand  relevance judgments.
T IPSTER DN2format used to represent the query structure.Participants exchanged the query sets they createdwith all other participants in the track, and  all partic-ipants ran all query sets their system could support.The  document  set used for the runs was  the docu-ments  on Disk 2 plus the AP  collection on Disk 3.The  retrieval results were submitted to N IST  whereall runs were judged and  evaluated.Since the track design included all groups runningall query sets, a number  of direct compar isons were264possible.
First, participants could see how effectivetheir system was using their own queries.
Second,they could see how effective their search componentwas when using other queries, and finally, partici-pants could evaluate how effective their query con-struction strategies were by seeing how other groupsfared with their queries.Unfortunately, only two groups participated in thequery track, too few to make any meaningful compar-isons.
The track will run again in TREC-8, with thehope that heightened awareness of the problems thequery track is addressing will generate participation.6 .7  The  Confus ion  Track(Track report-  (Kantor & Voorhees, 1997))A confusion (or data corruption) track was run inTREC-4 and TREC-5 to investigate the problemswith using "corrupted" data such as would come fromOCR or speech input.
The TREC-4 track followedthe ad hoc task, but using only the category B data.This data was randomly corrupted at NIST usingcharacter deletions, substitutions, and additions tocreate data with a 10% and 20% error rate (i.e., 10%or 20% of the characters were affected).
Note thatthis process is neutral in that it does not model OCRor speech input.
Four groups used the baseline and10% corruption level; only two groups tried the 20%level.
As was somewhat expected, the 10% error ratedid not hurt performance in general and the trackresults were somewhat inconclusive.In TREC-5, the test data was actual OCR outputof scanned images of the 1994 Federal Register.
Fivegroups participated in the experiment designed to ex-plore the effect different levels of OCR error has on re-trieval performance.
This time a new task was tried:known-item searching.
In this task the participantssearched for particular previously identified docu-ments in three versions of documents.
The three ver-sions of the documents were the original documents,the documents that resulted after the originals weresubjected to an optical character recognition (OCR)process with a character error rate of approximately5%, and the documents produced through OCR witha 20% error rate (caused by down-sampling the im-age before doing the OCR).
The five groups tried verydifferent methods, with the group from the Swiss Fed-eral Institute of Technology (ETH) (Ballerini et al,1997) performing the best, using a type of expan-sion of possible candidate words to improve the bestmatch score.It was decided to migrate the confusion track tothe speech area in TREC-6, where it was called theSpoken Document Retrieval (SDR) track.
The SDRtrack is a successor to the confusion track in that itrepresents a different form of "corrupted" documents.Instead of retrieving documents that are the resultof OCR, systems retrieved ocuments that were theresult of speech recognition systems.6 .8  The  Spoken Document  Ret r ieva l(SDR)  T rackTrack reports- (Garofolo, Voorhees, Stanford, &Jones, 1998; Garofolo, Voorhees, Auzanne, Stanford,& Lund, 1999)The SDR track fosters research on retrievalmethodologies for spoken documents (i.e., recordingsof speech).
It was run in both TRECs 6 and 7, usingdifferent document sets and different asks.The TREC-6 document set was a set of transcriptsfrom 50 hours of broadcast news originally collectedby the Linguistic Data Consortium for DARPA Hub-4 speech recognition evaluations (Garofolq, Fiscus, &Fisher, 1997).
Three versions of the transcripts wereused: a "truth" transcript hat was hand-produced;a transcript produced by an IBM baseline speechrecognition system; and a transcript produced by theparticipant's own speech recognition system.
Doc-ument boundaries were given in the hand-producedtranscript, and the same boundaries were used in theother two versions.
While recognizing fifty hours ofnews presented a serious challenge to the speech sys-tems, the resulting document set was small by re-trieval standards, consisting of only 1451 stories.Like the earlier confusion tracks, the task in theTREC-6 SDR track was a known-item search.
In aknown-item search, the goal was to retrieve a sin-gle specific document, rather than a set of relevantdocuments.
The search simulates a user seeking aparticular, half-remembered document.
NIST cre-ated 50 topics, each designed to describe preciselyone document.
Half of the topics were created totarget speech conditions, and half to target retrievalconditions.
Within each half, half were designed tobe easy and half difficult.
Difficult speech conditionsincluded background noise, non-native speakers, low-bandwidth channels, and the like.
Difficult retrievalconditions included the use of synonyms (e.g., cinemafor movie theater) and rare senses of common words(e.g., looking for the document describing cigarettepants when many stories were about cigarette smok-ing).Thirteen groups submitted SDR track runs.
Theresults uggested that speech recognition and IR tech-265nologies are sufficiently advanced to do a credible jobof retrieving specific documents.
The better systemswere able to retrieve the target document at rank1 over 70% of the time using their own recognizertranscripts, compared to the best performance on thetruth transcripts of 78.7%.
Search performance wasa bigger factor in the overall results than recognitionaccuracy, although the best results were obtained bygroups that included both speech and IR experts.The TI:tEC-7 track implemented a full ranked re-trieval task.
The document collection consisted oftranscripts of approximately 100 hours of broadcastnews programs, representing about 3000 news stories.Participants worked with four different versions ofthe transcripts: the reference transcripts, which werehand-produced and assumed to be perfect; the firstbaseline transcripts, which were produced by a base-line speech recognition system running at about 35%word error rate; a second set of baseline transcripts,produced by the baseline recognizer running at about50% word error rate; and the recognizer transcripts,which were produced by the participant's own recog-nizer system.
Document boundaries were given in thehand-produced transcripts, and the same boundarieswere used in the other versions.NIST created a set of 23 topics, which were used tosearch each of the versions of the transcripts.
The dif-ferent versions of the transcripts allowed participantsto observe the effect of recognizer errors on their re-trieval strategy.
The different recognizer runs providea comparison of how different recognition strategiesaffect retrieval.
To make this comparison as com-plete as possible, participants were encouraged to re-trieve using other groups' recognizer transcripts aswell.
These runs are called cross-recognizer runs.Eleven groups participated in the TREC-7 SDRtrack.
The results of the track displayed a linearcorrelation between the error rate of the recognitionand a decrease in retrieval effectiveness, a correlationthat was not present in last year's track that used aknown-item search task.
Not surprisingly, the corre-lation is stronger when recognizer error rate is com-puted over content-based words (e.g., named entities)rather than all words.6 .9  The  Very  Large  Corpus  (VLC)T rackTrack reports- (Hawking & Thistlewaite, 1998;Hawking, Craswell, & Thistlewaite, 1999)The VLC track explores how well retrieval algo-rithms scale to larger document collections.
In con-trast to the ad hoc task that uses a 2 GB docu-ment coliection, the first running of the VLC track inTREC-6 used a 20 GB collection, while the TREC-7track used a 100 GB document collection.The TREC-6 track's corpus consisted of 7.5 milliontexts for a total of 20.14 GB of data, including thefive TREC CDs; USENET news postings; Canadianand Australian Hansards; HTML-formatted docu-ments including university websites, and laws andjudgments from the Australian Attorney General'sDepartment; and the Glasgow Herald and FinancialTimes newspapers.
The TREC-6 ad hoc topics wereused.Because of the difficulty of obtaining sufficient rele-vance judgments for recall-based measures, the maineffectiveness measure used for VLC runs was preci-sion after 20 documents were retrieved.
Also reportedwere query response time; data structure (e.g., in-verted index) building time; and a cost measure ofnumber of queries processed per minute per hard-ware dollar.
Participants were required to submittwo runs: one run over the entire VLC corpus and asecond run over a baseline collection that consisted ofa random 10% sample of the full corpus.
The focusof the evaluation was on the ratio of the measuresbetween the baseline and full corpus runs.Seven groups submitted VLC track runs.
All of theparticipants were able to complete the VLC task withthe hardware available to them (i.e., no special hard-ware purchases were made for the track).
Indeed, themajor conclusion of the track is that current systemsare able to obtain good (high precision) retrieval ef-fectiveness on a 20 GB collection with reasonable re-sources.
For example, one of the best runs, from theUniversity of Waterloo (Cormack et al, 1998), re-trieved an average of 12.8 relevant documents in thetop twenty processing at the rate of 2678 queries perhour using a cluster of four commodity PCs.The TREC-7 collection consisted of World WideWeb data that was collected by the Internet Archive(ht tp : / /www.arch ive .org) .
The track used theTREC-7 ad hoc topics, and a set of relevance judg-ments produced by assessors at the Australian Na-tional University.
Because of the difficulty of gettingsufficient relevance judgments to accurately measurerecall, the main effectiveness measure used for VLCruns was precision after 20 documents were retrieved.To more accurately measure the effect size has onthe retrieval systems used by the participants, thetrack provided 3 collections: the original 100 GB col-lections plus 1% and 10% subsamples.
Participantsindexed each of the three collections and ran the en-tire topic set on each.
They then reported timing266figures for each phase as well as the top 20 retrieved.The main evaluation measures were precision after20 documents retrieved (the effectiveness measure);query response time (elapsed time as seen by theuser); data structure (e.g., inverted index) buildingtime (elapsed time as seen by the user); plus a com-bination timing measure that factored in the expenseof the hardware used.Seven groups participated in the TREC-7 VLCtrack, with six groups processing the entire 100GBcorpus.
The track demonstrated that processing a100GB corpus is well within the capabilities of to-day's retrieval systems.
Of particular note was theMultitext group that achieved sub-second query pro-cessing time while maintaining ood retrieval effec-tiveness using hardware that cost under US$100,000.6 .10  The  Natura l  Language Process -ing  (NLP)  T rack(Track report-  (Strzalkowski & Jones, 1997))The NLP track was started in TREC-5 to explorewhether the natural language processing techniquesavailable today are mature enough to have an im-pact on IR, and specifically whether they can offeran advantage over more conventional methods.
Fourgroups participated in the initial running of the nat-ural language processing track.The TREC-6 track used the 50 TREC-6 ad hoctopics and a reduced ocument set consisting of justthe Financial Times newspaper data.
The track hadlimited participation, with just two groups submit-ting NLP runs.To date, specific NLP processing has not proved es-sential to obtaining effective retrieval in TREC.
Themost useful NLP techniques for text retrieval gener-ally have been methods that recognize and normal-ize names and other multi-word terms.
However, theTREC topics do not require processing at this level ofdetail.
Other information seeking tasks such as factextraction or story summarization may be a more ap-propriate test of current NLP technology.6 .11  The  Database  Merg ing  Track(Track report-  (Voorhees, 1997))The database merging track had the goal of in-vestigating techniques for merging results from thevarious TREC subcollections (as opposed to treat-ing the collections as a single entity).
This type ofinvestigation is important for real-world collections,and also to allow researchers to take advantage ofpossible variations in retrieval techniques for hetero-geneous collections.The track" was started in TREC-4, with 3 partici-pating groups, running the ad hoc topics separatelyon each of the 10 subcollections, merging the results,and then submitting these, along with a baseline runtreating the subcollections as a single collection.
The10 subcollections were defined corresponding to thevarious dates of the data, i.e., the three different yearsof the Wall Street Journal, the two different years ofthe AP newswire, the two sets of Ziff documents (oneon each disk), and the three single subcollections (theFederal Register, the San Jose Mercury News, and theU.S.
Patents).If results are produced without use of collectioninformation, then the merging process is trivial.
Cer-tainly this is one method of handling the problemsof merging results from different databases.
Howeverthis precludes using information about the collectionto modify the various algorithms in the search en-gine, and, even more importantly, it does not dealwith the issue about which collection to select.
Animplied question in this track was the hypothesis thatone might want to bias searching towards certain col-lections.There was a second running of the database merg-ing track in TREC-5, again with only three groupsparticipating.
This time the data was split into manymore (98) databases, to allow testing of database se-lection methods.
Unfortunately this proved to be ahigh-overhead track and thus did not attract muchparticipation despite a general interest in the prob-lem.
The track has not been run since TREC-5.7 THE FUTUREThe final session of each TREC workshop is a plan-ning session for future TRECs--especially to decideon the set of tracks for the next TREC.
Two newtracks are planned for TREC-8, a question answeringtrack and a Web track.
The question answering trackis designed to encourage research on methods for in-formation retrieval as opposed to document retrieval.The goal in the track will be for systems to produceshort text extracts that contain the answer for eachof a set of 200 questions.
The goal in the Web trackwill be to investigate whether links can be used toenhance retrieval.
The track will use a 2GB subsetof the data collected for the VLC track and a typicalTREC ad hoc task.
Also, participation in the querytrack is encouraged, since the benefits of that trackincrease with increased participation.267AcknowledgmentsThe authors gratefully acknowledge the continuedsupport of the TREC conferences by the IntelligentSystems Office of the Defense Advanced ResearchProjects Agency.
Thanks also go to the TREC pro-gram committee and the staff at NIST.
The TRECtracks could not happen without the efforts of thetrack coordinators; our special thanks to them.ReferencesAllan, J., Ballesteros, L., Callan, J., Croft, B., &Lu, Z.
(1996).
Recent Experiments with IN-QUERY.
In D. K. Harman (Ed.
), (pp.
49-63).
(NIST Special Publication 500-236.
)Allan, J., Callan, J., Croft, B., Bellesteros, L.,Broglio, J., Xu, J., & Shu, H. (1997).
IN-QUERY at TREC-5.
In E. Voorhees & D. Har-man (Eds.
), (pp.
119-132).
(NIST SpecialPublication 500-238.
)Allan, J., Callan, J., Croft, W. B., Ballesteros,L., Byrd, D., Swan, R., & Xu, J.
(1998).INQUERY does battle with TREC-6.
InE.
Voorhees & D. Harman (Eds.
), (pp.
169-206).
(NIST Special Publication 500-240.
)Allan, J., Callan, J., Sanderson, M., Xu, J., & Weg-mann, S. (1999).
INQUERY and TREC-7.
InE.
Voorhees & D. Harman (Eds.
), (p.
TBD).
(NIST Special Publication 500-242.
)Ballerini, J.-P., Bfichel, M., Domenig, R., Knaus, D.,Mateev, B., Mittendorf, E., Schiiuble, P., Sheri-dan, P., & Wechsler, M. (1997).
SPIDER Re-trieval System at TREC-5.
In E. Voorhees &D. Harman (Eds.
), (pp.
217-228).
(NIST Spe-cial Publication 500-238.
)Beaulieu, M., Gatford, M., Huang, X., Robertson,S., Walker, S., & Williams, P. (1997).
Okapi atTREC-5.
In E. Voorhees & D. Harman (Eds.),(pp.
143-166).
(NIST Special Publication 500-238.
)Bodner, R., & Chignell, M. (1999).
ClickIR: TextRetrieval using a Dynamic Hypertext Interface.In E. Voorhees & D. Harman (Eds.
), (p.
TBD).
(NIST Special Publication 500-242.
)Boughanem, M., & Soul6-Dupuy, C. (1998).
Mercureat Trec6.
In E. Voorhees & D. Harman (Eds.),(pp.
321-328).
(NIST Special Publication 500-240.
)Braschler, M., Krause, J., Peters, C., & Sch~uble,P.
(1999).
Cross-Language Information Re-trieval (CLIR) Track Overview.
In E. Voorhees& D. Havman (Eds.
), (p. TBD).
(NIST SpecialPublication 500-242.
)Braschler, M., Wechsler, M., Mateev, B., Mittendorf,E., & Schauble, P. (1999).
SPIDER RetrievalSystem at TREC7.
In E. Voorhees & D. Har-man (Eds.
), (p. TBD).
(NIST Special Publi-cation 500-242.
)Brown, E. (1995).
Fast evaluation of structuredqueries for information retrieval.
In Proceed-ings of the 18th annual international ACM SI-GIR conference on research and development iinformation retrieval (pp.
30-38).Brown, E. W., & Chong, H. A.
(1998).
The GURUsystem in TREC-6.
In E. Voorhees & D. Har-man (Eds.
), (pp.
535-540).
(NIST SpecialPublication 500-240.
)Buckley, C. (1998).
TREC-6 High-Precision Track.In E. Voorhees & D. Harman (Eds.
), (p.
69-72).
(NIST Special Publication 500-240.
)Buckley, C. (1999a).
TREC-7 High-Precision Track.In E. Voorhees & D. Harman (Eds.
), (p.
TBD).
(NIST Special Publication 500-242.
)Buckley, C. (1999b).
TREC-7 Query Track.
InE.
Voorhees & D. Harman (Eds.
), (p.
TBD).
(NIST Special Publication 500-242.
)Buckley, C., Mitra, M., Walz, J., & Cardie, C.(1998).
Using Clustering and SuperConceptswithin SMART: TREC-6.
In E. Voorhees &D. Harman (Eds.
), (pp.
107-124).
(NIST Spe-cial Publication 500-240.
)Buckley, C., Mitra, M., Walz, J., & Cardie, C.(1999).
SMART High Precision: TREC 7.
InE.
Voorhees & D. Harman (Eds.
), (p.
TBD).
(NIST Special Publication 500-242.
)Buckley, C., Salton, G., Allan, J., & Singhal, A.(1995).
Automatic Query Expansion UsingSMART: TREC-3.
In D. K. Harman (Ed.
), (pp.69-80).
(NIST Special Publication 500-225.
)Buckley, C., Singhal, A., & Mitra, M. (1997).
UsingQuery Zoning and Correlation Within SMART:TREC 5.
In E. Voorhees & D. Havman (Eds.),(pp.
105-118).
(NIST Special Publication 500-238.
)269Clarke, C. L., & Cormack, G. V. (1997).
Inter-active Substring Retrieval (MultiText Experi-ments for TREC-5.
In E. Voorhees & D. Har-man (Eds.
), (pp.
267-278).
(NIST SpecialPublication 500-238.
)Cormack, G. V., Clarke, C. L., Palmer, C. R., & To,S.
S. L. (1998).
Passage-based refinement (Mul-tiText experiments for TREC-6.
In E. Voorhees& D. Harman (Eds.
), (pp.
303-319).
(NISTSpecial Publication 500-240.
)Evans, D., Huettner, A., Tong, X., Jansen, P., & Ben-nett, J.
(1999).
Effectiveness of Clustering inAd Hoc Retrieval.
In E. Voorhees & D.
Harman(Eds.
), (p. TBD).
(NIST Special Publication500-242.
)Fitzpatrick, L., & Dent, M. (1997).
Automaticfeedback using past queries: Social searching?In Proceedings of the 20th annual internationalACM SIGIR conference on research and devel-opment in information retrieval (pp.
306-313).Fuller, M., Kaszkiel, M., Ng, C. L., Vines, P., Wilkin-son, R., & Zobel, J.
(1998).
MDS TREC6 re-port.
In E. Voorhees & D. Harman (Eds.
), (pp.241-257).
(NIST Special Publication 500-240.
)Garofolo, J., Fiscus, J., & Fisher, W. (1997).
Designand preparation of the 1996 Hub-4 broadcastnews benchmark test corpora.
In Proceedingsof the DARPA speech recognition workshop (pp.15-21).Garofolo, J., Voorhees, E., Auzanne, C., Stanford,V., & Lund, B.
(1999).
1998 TREC-7 SpokenDocument Retrieval Track Overview and Re-sults.
In E. Voorhees & D. Harman (Eds.
), (p.TBD).
(NIST Special Publication 500-242.
)Garofolo, J., Voorhees, E., Stanford, V., & Jones,K.S.
(1998).
1997 TREC-6 Spoken Docu-ment Retrieval Track Overview and Results.
InE.
Voorhees & D. Harman (Eds.
), (p.
83-92).
(NIST Special Publication 500-240.
)Gey, F. C., Chen, A., He, J., Xu, L., & Meggs, J.(1997).
Term importance, Boolean conjuncttraining, negative terms, and foreign languageretrieval: probabilistic algorithms for TREC-5.In E. Voorhees & D. Harman (Eds.
), (pp.
181-190).
(NIST Special Publication 500-238.
)Harman, D. (1996).
Overview of the fourth Text RE-trieval Conference (TREC-4).
In D. K.
Harman(Ed.
), (pp.
1-23).
(NIST Special Publication500-236.
)Harman, D. K.
(Ed.).
(1994, March).
Proceedings ofthe second text REtrieval conference (TREC-2).
(NIST Special Publication 500-215.
)Harman, D. K.
(Ed.).
(1996, October).
Proceedings o\]the fourth text REtrieval conference (TREC-~).
(NIST Special Publication 500-236.
)Hawking, D., Craswell, N., & Thistlewaite, P. (1999).Overview of TREC-7 Very Large CollectionTrack.
In E. Voorhees & D. Harman (Eds.
), (p.TBD).
(NIST Special Publication 500-242.
)Hawking, D., & Thistlewaite, P. (1998).
Overviewof TREC-6 Very Large Collection Track.
InE.
Voorhees & D. Harman (Eds.
), (p.
93-106).
(NIST Special Publication 500-240.
)Hawking, D., Thistlewaite, P., & Bailey, P.(1997).
ANU/ACSys TREC-5 Experiments.
InE.
Voorhees & D. Harman (Eds.
), (pp.
359-376).
(NIST Special Publication 500-238.
)Hawking, D., Thistlewaite, P., & Craswell, N.(1998).
ANU/ACSys TREC-6 experiments.
InE.
Voorhees & D. Harman (Eds.
), (pp.
275-290).
(NIST Special Publication 500-240.
)Hiemstra, D., & Kraaij, W. (1999).
Twenty-One atTREC-7: Ad-hoc and Cross-language Track.
InE.
Voorhees & D. Harman (Eds.
), (p.
TBD).
(NIST Special Publication 500-242.
)Hull, D., Grefenstette, G., Schulze, B., Gaussier,E., Schiitze, H., & Pedersen, J.
(1997).
Xe-rox TREC-5 Site Report: Routing, Filtering,NLP, and the Spanish Tracks.
In E. Voorhees& D. Harman (Eds.
), (pp.
167-180).
(NISTSpecial Publication 500-238.
)Hull, D. A.
(1998).
The TREC-6 Filtering Track:Description and Analysis.
In E. VoorheesD.
Harman (Eds.
), (p. 45-68).
(NIST SpecialPublication 500-240.
)Hull, D. A.
(1999).
The TREC-7 Filtering Track:Description and Analysis.
In E. Voorhees &D. Harman (Eds.
), (p. TBD).
(NIST SpecialPublication 500-242.
)Kantor, P., & Voorhees, E. (1997).
Report on theTREC-5 Confusion Track.
In E. Voorhees &D. Harman (Eds.
), (pp.
65-74).
(NIST SpecialPublication 500-238.
)270Kwok, K. (1996).
A new method of weighting queryterms.
In Proceedings of the 19th annual in-ternational ACM SIGIR conference on researchand development in information retrieval (pp.187-196).Kwok, K., & Grunfeld, L. (1994).
TREC-2 DocumentRetrieval Experiments using PIRCS.
In D. K.Harman (Ed.
), (pp.
233-242).
(NIST SpecialPublication 500-215.
)Kwok, K., ~ Grunfeld, L. (1997).
TREC-5 En-glish and Chinese Retrieval Experiments usingPIRCS.
In E. Voorhees & D. Harman (Eds.),(pp.
133-142).
(NIST Special Publication 500-238.
)Kwok, K., Grunfeld, L., & Xu, J.
(1998).
TREC-6 English and Chinese retrieval experimentsusing PIRCS.
In E. Voorhees & D.
Harman(Eds.
), (pp.
207-214).
(NIST Special Publica-tion 500-240.
)Lagergren, E., & Over, P. (1998).
Comparing interac-tive information retrieval systems across sites:The trec-6 interactive track matrix experiment.In Proceedings of the 21th annual internationalACM SIGIR conference on research and devel-opment in information retrieval (pp.
164-172).Lewis, D. (1997).
The TREC-5 Filtering Track.
InE.
Voorhees & D. Harman (Eds.
), (pp.
75-96).
(NIST Special Publication 500-238.
)Lu, A., Ayoub, M., & Dong, J.
(1997).
Ad HocExperiments using EUREKA.
In E. Voorhees& D. Harman (Eds.
), (pp.
229-240).
(NISTSpecial Publication 500-238.
)Lu, A., Meier, E., Rao, A., Miller, D., & Pliske,D.
(1998).
Query processing in TREC6.
InE.
Voorhees & D. Harman (Eds.
), (pp.
567-576).
(NIST Special Publication 500-240.
)Mandala, R., Tokunaga, T., Tanaka, H., Okumura,A., & Satoh, K. (1999).
Ad Hoc RetrievalExperiments Using WordNet and Automati-cally Constructed Thesauri.
In E. Voorhees &D. Harman (Eds.
), (p. TBD).
(NIST SpecialPublication 500-242.
)Mateev, B., Munteanu, E., Sheridan, P., Wechsler,M., & Sch~uble, P. (1998).
ETH TREC-6:Routing, Chinese, cross-language, and spokendocument retrieval.
In E. Voorhees ~ D. Har-man (Eds.
), (pp.
623-635).
(NIST SpecialPublication 500-240.
)Milic-Frayling, N., Evans, D., Tong, X., & Zhai,C.
(1997).
CLARIT Compound Queries andConstraint-Controlled Feedback in TREC-5.
InE.
Voorhees & D. Harman (Eds.
), (pp.
315-334).
(NIST Special Publication 500-238.
)Milic-Frayling, N., Zhai, C., Tong, X., Jansen, P., &Evans, D. A.
(1998).
Experiments in query op-timization: The CLARIT system TREC-6 re-port.
In E. Voorhees & D. Harman (Eds.
), (pp.415-454).
(NIST Special Publication 500-240.
)Miller, D., Leek, T., & Schwartz, R. (1999).
A hid-den markov model information retrieval system.In Proceedings of the 22th annual internationalA CM SIGIR conference on research and devel-opment in information retrieval (p. TBD).Nakajima, H., Takaki, T., Hirao, T., & Kitauchi, A.(1999).
NTT DATA at TREC-7: system ap-proach for ad hoc and filtering.
In E. Voorhees& D. Haxman (Eds.
), (p. TBD).
(NIST SpecialPublication 500-242.
)Namba, I., Igata, N., Horai, H., Nitta, K., & Matsui,K.
(1999).
Fujitsu Laboratories TREC7 Re-port.
In E. Voorhees & D. Harman (Eds.
), (p.TBD).
(NIST Special Publication 500-242.
)Over, P. (1997).
TREC-5 Interactive Track Report.In E. Voorhees & D. Harman (Eds.
), (pp.
29-56).
(NIST Special Publication 500-238.
)Over, P. (1998).
TREC-6 Interactive Track Report.In E. Voorhees ~: D. Harman (Eds.
), (pp.
73-81).
(NIST Special Publication 500-240.
)Over, P. (1999).
TREC-7 Interactive Track Report.In E. Voorhees & D. Harman (Eds.
), (p.
TBD).
(NIST Special Publication 500-242.
)Pedersen, J. O., Silverstein, C., & Vogt, C. C. (1998).Verity at TREC-6: Out-of-the-box and beyond.In E. Voorhees ,~ D. Harman (Eds.
), (pp.
259-273).
(NIST Special Publication 500-240.
)Rao, A., Humphrey, T., Parhizgar, A., Wilson, C.,& Pliske, D. (1999).
Experiments in QueryProcessing at LEXIS-NEXIS for TREC-7.
InE.
Voorhees & D. Harman (Eds.
), (p.
TBD).
(NIST Special Publication 500-242.
)Robertson, S., Walker, S., Hancock-Beaulieu, M., &Gatford, M. (1994).
Okapi and TREC-2.
InD.
K. Harman (Ed.
), (pp.
21-34).
(NIST Spe-cial Publication 500-215.
)271Salton, G., & McGill, M.
(Eds.).
(1983).
Introductionto modern information retrieval.
McGraw-HillBook Co., New York, NY.Schamber, L. (1994).
Relevance and information be-havior.
Annual Review of Information Scienceand Technology, 29, 3-48.Sch~uble, P., & Sheridan, P. (1998).
Cross-LanguageInformation Retrieval (CLIR) Track Overview.In E. Voorhees & D. Harman (Eds.
), (pp.
31-43).
(NIST Special Publication 500-240.
)Singhal, A.
(1998).
AT&T at TREC-6.
InE.
Voorhees & D. Harman (Eds.
), (pp.
215-225).
(NIST Special Publication 500-240.
)Singhal, A., Buckley, C., & Mitra, M. (1996).
Pivoteddocument length normalization.
In Proceedingsof the 19th annual international ACM SIGIRconference on research and development in in-formation retrieval (pp.
21-29).Singhal, A., Choi, J., Hindle, D., Lewis, D., &Pereira, F. (1999).
AT&T at TREC-7.
InE.
Voorhees & D. Harman (Eds.
), (p.
TBD).
(NIST Special Publication 500-242.
)Smeaton, A., & Wilkinson, R. (1997).
Spanish andChinese Document Retrieval in TREC-5.
InE.
Voorhees & D. Harman (Eds.
), (pp.
57-64).
(NIST Special Publication 500-238.
)Sparck Jones, K. (in press).
Further Reflections onTREC.
Information Processing and Manage-ment.Sparck Jones, K., & van Rijsbergen, C. (1975).Report on the need for and provision of an"ideal" information retrieval test collection.British Library Research and Development Re-port 5266.
Computer Laboratory, University ofCambridge.Strzalkowski, T., & Jones, K. S. (1997).
NLP Trackat TREC-5.
In E. Voorhees & D.
Harman(Eds.
), (pp.
97-102).
(NIST Special Publi-cation 500-238.
)Strzalkowski, T., Lin, F., & Perez-Carballo, J.(1998).
Natural Language Information Re-trieval: TREC-6 Report.
In E. Voorhees &D. Harman (Eds.
), (pp.
347-366).
(NIST Spe-cial Publication 500-240.
)Strzalkowski, T., Lin, F., Wang, J., Guthrie,L., Leistensnider, J., Wilding, J., Karlgren,J., Straszheim, T., & Perez-Carballo, J.(1997).
Natural Language Information Re-trieval: TREC-5 Report.
In E. Voorhees &D. Harman (Eds.
), (pp.
291-314).
(NIST Spe-cial Publication 500-238.
)Strzalkowski, T., Stein, G., Wise, G. B., Perez-Carballo, J., Tapananinen, P., Jarvinen, T.,Voutilainen, A., & Karlgren, J.
(1999).
Nat-ural Language Information Retrieval: TREC-7Report.
In E. Voorhees & D. Harman (Eds.),(p.
TBD).
(NIST Special Publication 500-242.
)Swan, R., & Allan, J.
(1998).
Aspect windows, 3-dvisualizations, and indirect comparisons of in-formation retrieval systems.
In Proceedings ofthe 21th annual international ACM SIGIR con-ference on research and development in infor-mation retrieval (pp.
173-181).Voorhees, E. (1997).
The TREC-5 Database MergingTrack.
In E. Voorhees & D. Harman (Eds.),(pp.
103-104).
(NIST Special Publication 500-238.
)Voorhees, E. (in press).
Special issue: The sixth TextREtrieval Conference (TREC-6).
InformationProcessing and Management.Voorhees, E., & Harman, D.
(Eds.).
(1997, Novem-ber).
Proceedings of the fifth Text REtrievalConference (TREC-5).
(NIST Special Publica-tion 500-238.
)Voorhees, E., & Harman, D.
(Eds.).
(1998, August).Proceedings of the sixth Text REtrieval Con-ference (TREC-6).
(NIST Special Publication500-240.
)Voorhees, E., & Harman, D.
(Eds.).
(1999, April).Proceedings of the seventh Text REtrieval Con-ference (TREC-7).
(NIST Special Publication500-242.
)Voorhees, E., & Harman, D. (in press).
Overviewof the Sixth Text REtrieval Conference(TREC-6).
Information Processing and Man-agement.Voorhees, E. M. (1998).
Variations in relevance judg-ments and the measurement of retrieval effec-tiveness.
In Proceedings of the 21th annual in-ternational ACM SIGIR conference on researchand development in information retrieval (p.315-323).272Walker, S., Robertson, S., Boughanem, M., Jones, G.,& Sparck Jones, K. (1998).
Okapi at TREC-6:Automatic ad hoc, VLC, routing, filtering andQSDR.
In E. Voorhees & D. Harman (Eds.),(pp.
125-136).
(NIST Special Publication 500-240.
)Wilkinson, R. (1998).
Chinese Document Retrieval atTREC-6.
In E. Voorhees & D. Harman (Eds.),(pp.
25-30).
(NIST Special Publication 500-240.
)Xu, J., & Croft, W. (1996).
Query expansion usinglocal and global document analysis.
In Pro-ceedings of the 19th annual international ACMSIGIR conference on research and developmentin information retrieval (pp.
4-11).Zobel, J.
(1998).
How reliable are the results of large-scale information retrieval experiments.
In Pro-ceedings of the 21th annual international ACMSIGIR conference on research and developmentin information retrieval (pp.
307-314).273
