Toward a Scoring Function for Quality-Driven Machine TranslationDouglas A. Jones ~ Gregory M. RuskDepartment of Defense RABA Technologies9800 Savage Road, Suite 6514 10500 Little Patuxent ParkwayFort Meade, MD 20755-6514 Colulnbia, MD 21044AbstractWe describe how we constructed an automatic scoring function for machine translation quality;this function makes use of arbitrarily many pieces of natural anguage processing software thathas been designed to process English language text.
By machine-learning values of fnnctionsavailable inside the software and by constructing functions that yield values based upon thesoftware output, we are able to achieve preliminary, positive results in machine-learning thedifference between human-produced English and machine-translation E glish.
We suggesthow the scoring ftmction may be used for MT system development.Introduction to the MT PlateauWe believe it is fair to say that the field ofmachine translation has been on a plateau for atleast the past decade.
2 Traditional, band-builtMT systems held up very well in the ARPAMT evaluation (White and O'Connell 1994).These systems are relatively expensive to buildand generally require a trained staff workingfor several years to produce a mature system.This is the current commercial state of the art:hand-building specialized lexicons andtranslation rules.
A completely different ype ofsystem was competitive in this evaluation,namely, the purely statistical CANDIDEsystem built at IBM.
It was generally felt thatthis system had also reached a plateau in thatmore data and more training was not likely toimprove the quality of the output.Low Density Machine TranslationHowever, in the case of "Low Density MachineTranslation" (see Nirenburg and Raskin 1998,Jones and Havrilla 1998) commercial marketforces are not likely to provide significantincentives for machine translation systems forLow Density (Non-Major) languages any timesoon.
Two noteworthy efforts to break past thedata and labor bottlenecks for high-qualitymachine translation development are thefollowing.
The NSF Summer Workshop oni Douglas Jones is now at National Institute ofStandards & Technology, Gaithersburg, MD 20899,Douglas.Jones @NIST.gova A sensible, plateau-fi'iendly strategy may be toaccumulate translation memory to improve both thelong-term efficiency of human translators and thequality of machine translation systems.
If weimagine that the plateau is really a kind oflogarithmic function tending ever upwards, we needonly be patient.Statistical Machine Translation held at JohnsHopkins University summer 1999 developed apublic-domain version intended as a platformfor further development of a CANDIDE-styleMT system.
Part of the goal here is to improvethe trauslation by adding levels of linguisticanalysis beyond the word N-gram.
An effortaddressing the labor bottleneck is theExpedition Project at New Mexico StateUniversity where a preliminary elicitationenvironlnent for a computational fieldlinguistics ystem has been developed (the Boasinterface; see Nirenburg and Raskin 1998)A Scoring Function for MT qualityOur contribution toward working beyond thisplateau is to look for a way to define a scoringfunction for the quality of the English outputsuch that we can use it to machine-learn a goodtranslation grammar.
The novelty of our ideafor this function is that we do not have to definethe internals of it ourselves per se.
We are ableto define a successful function for two reasons.First, there is a growing body of softwareworldwide that has been designed to consumeEnglish; all we need is for each piece ofsoftware to provide a metric as to how English-like its input is.
Second, we can tell whether thesoftware had trouble with the input, either bysystem-internal diagnosis or by diagnosing thesoftware's output.
A good illustration is thefacility in current word-processing software toput red squiggly lines underneath text it thinksshould be revised.
We know fi'om experiencethat this feature is often only annoying.Nevertheless, imagine that it is correct somepercentage of the time, and that each piece ofsoftware we use for this purpose is correct solnepercentage of the time.
Our strategy is to376extract or create nurneric wflues fl'om eachpiece of software that corresponds to the degreeto which the software was happy with the input.That array of numbers is tile heart of ourscorim, function for En~lishness ~- we arecalling these numeric values "indicators" ofEnglishness.
We then use that array ofindicators to drive the machine translationdevelopment.
In this paper we will report onhow we have constructed a prototype of thisfunction; in separate work we discuss how toinsert this function into a machine-learningregimen designed to maximize the overallquality of the rnachine translation output.A Reverse Turing TestPeople can generally tell the difference betweenhuman-produced English and machinetranslation Englisll~ assuming all tile obviousconstraints uch as that tile reader and writerhave command of the language.
Whether ornot a machine can tell the difference depends ofcourse, on how good tim MT system is.
Can weget a machine to tell tile difference?
Of courseit depends on how good the MT system is: if itwere perfecL neither we nor the machinesought to be able to distinguish them.
MTquality being what it is, that is not a problemfor us now.
An essential first step towardQ1)MT is what we are calling a "ReverseTuring Test".
In the ordinary Turing Test, wewant to fool a person into thinking the machineis a person.
Here, we are turning that on itshead.
We want to define a function that can telltile difference between English that a humanbeing has produced versus English that themachine has produced) To construct he test,we use a bilingual parallel aligned corpus: wetake tile foreign language side and send thatthrough the MT system; then we see if we candefine a scoring function that can distinguishthe two w:rsions (original English and MTEnglish).
With our current indicators andcorpus, we can machiue-leam a function thatbehaves as follows: if you hand it a humansentence, it conectly classifies it as human 74%of the time.
If you hand it a machine sentence,it correctly classifies it as a machine sentence57% of the time.
In tile remainder of the paper,we will step through the details of tileexperiment; we will also discuss why we3Obviously the end goal here is to fail this ReverseTuring Test for a "perfect" machine translationsystem.
We are very far away from this, but wewould like to use this function to drive the processtoward that eventual alld ti)rtunate failure.neither expect nor require 100% accuracy forthis function.
Our boundary tests behave asexpected and are shown ill the final section --we use tile same test to distinguish betweenEnglish and (a) English word salad, (b) Englishalphabet soup, (c) Japanese, and (d) the identitycase of more human-produced English.Case Study: Japanese-EnglishIn this paper, we report on results using a smallcorpus of 2,340 sentences drawn from theKenkyusha New Japanese-English Dictionary.It was important in this particular experiment touse a very clean corpus (perfectly aligned andminimally formatted).
This case study issituated in a broader context: we haveconducted exploratory experiments on samplesfrom several corpora, for example the ARPAMT Evaluation corpus, samples from EuropeanCorpus Initiative Data corpus (ECI-I) andothers.
Since we found that the scoringfunction was quite sensitive to forrnattingproblems (for example, the presence of tablesand sentence segmentation enors causeproblems) we are examining a small corpus thatis free flom these issues.
The sentences are onaverage relatively short (7.0 words persentence; 37.6 characters/sentence), this makesour task both easier and harder.
It is easierbecause we have overcome tile forlnattingproblems.
It is harder because the MT systemis able to perform much better on the shorter,cleaner sentences than it was on longersentences with formatting problems.
Since theoutput is better, it is more difficult to define afunction that can tell the difference between theoriginal English and the machine translationEnglish.
On balance, this corpus is a good oneto illustrate our technique.i(l) #208 .~j~0)  ~z:~: {j~: {.a ff~:3;\]~ b j3';'\]-2 ~ x _3 \]-7_o}tie beauty ballled descnptu nMT It described he, beauty and the abno,malplay applied, She was radiant with happinessMT she had shone happilyIn terror the child seized his father's\]/ a l l l l .!
MT !Becoming fearful, the child , \]I : ",c a,m fa!h e'- IlFigure 1.
Subjective Quality Ranking \]377Figure 1 shows a range of output quality.
(1) isthe worst -- it is obviously MT output.
For usthis output is only partially intelligible.
(2) isnot so bad, but it is still not perfect English.
But(3)is nearly perfect.
We want to design asystem that can tell the difference.
We willnow walk through our suite of indicators; thegoal is to get the machine to see what we see interms of quality.Suite of IndicatorsWe have defined a suite of functions thatoperate at various levels of linguistic analysis:syntactic, semantic, and phonological(orthographic).
For each of these levels, wehave integrated at least one tool for which weconstruct an indicator function.
The task is touse these indicators to generate an array ofvalues which we can use to capture thesubjective quality we see wheu we read thesentences.
We will step through these indicatorfunctions one by one.
In some cases, in orderto get numbers, we take what amounts todebugging information from the tool (lnany ofthe tools have very nice API's that give accessto a variety of information about how itprocessed input).
In other cases, we define afunction that yields an output based oil theoutput of the tool (for example, we defined afunction that indicated the degree to which aparse tree was balanced; it turned out that abalanced tree was a negative indicator ofEnglishness, probably because English is right-branching).Syntactic IndicatorsTwo sources of local syntactic information are(a) parse trees and (b) N-grams.
Within tileparsers, we looked at internal processinginformation as well as output structures.
Forexample, we measured the probability of aparse and number of edges in the parse from theCollins parser.
The Apple Pie Parser providedvarious weights which we used.
The Appendixlists all of the indicator functions that we used.N-Gram Language Model (Cross-Perplexity)An easy number to calculate is the cross-perplexity of a given text, as calculated usingan N-gram language model.
44 We used the Cambridge/CMU language modelingtoolkit, trained on the Wall Street Journal (4/1990through 3/1992), (hn parameters: n=4, Good-Turingsmoothing)- C I 'OSS-perplexity__ (  1 ) 2439 It described her beauty and the_ abnormal play applied(2) ~ 2185 She had shone happily(3~ 1836 Becoming fearful, the childgrasped the arm of the fathertightlyFigure 2.
Cross-Perplexity IndicatorNotice that the subjective order is mirrored bythe cross-perl?lexity scores in Figure 2.Collins ParserThe task here is to write functions that processthe parse trees and return a number.
We haveexperimented with lnore elaborate functionsthat indicate how balanced the parse tree is andless complicated functions uch as the level ofembedding, number of parentheses, and so oil.Interestingly, the number of parentheses in theparse was a helpful indicator in conjunctionwith other indicators.Indicators of Semantic CohesivenessFor the semantic indicators, we want someindication as to how nmch the words in a textare related to each other by virtue of theirmeaning.
Which words belong together,regardless of exactly how they are used in theseutence?
Two resources we have begun tointegrate for this purpose are WordNet and theTrigger Toolkit (measuring mutualinformation).
The overall experimental designis roughly the same in both cases.
Our methodwas to remove stop words, lemmatize the text,and then take a measurement of pairwisesemantic cohesiveness of the iemmatizedwords 5.
For WordNet, we are counting howmany ways two words are related by thehyponylny relation (future indicators will besnore sophisticated).
For the Trigger Toolkit,we weighted the connections (by mutualinformation).OrthographicWe had two motivations for an orthographiclevel: one was methodological (we wanted tolook at each of tile traditional levels oflinguistic analysis).
The other was driven byour experience in looking at the MT output.Some MT systems leave untranslated words5The following parameters were used to build andcalculate mutual information using the TriggerToolkit: (1) All uppercase l tters were converted 1olowercase (2) All numbers were converted to a"NUMBER" token (3) Punctuation stripped (4)Stopwords removed (5) Words lcnunatized.378alone, or transliterate them, or insert a dummysynlbol, such as "X".
These cities wereadequate to give us apl)ropriate hints as towhether the text was produced by human or bymachine.
But some of our tools missed theseclues because of how they were designed.Robust parsers often treat uukllowu words asUOUlIS,; SO if we got au uut raus la ted  tel i l l  or  an"X", the parser simply treats it as a noun.
FiveX's in a row might be a noun phrase followedby a verb.
a Smoothed N-gram models of wordsusually treat any string of letters as a possibleword.MToutputworst(1)mid(2)best(3)Word N- Num.gram EdgesCross Per-iplexity2439 i1522185 27i1836 1654PieParser~core247139302A1;plo Sumof \[Char=mutual !N-graminfer- !crossina/ ion per-!plexily0 '8.10 16.31.7E-4 9;3Figure 3.
Subjective and Objective RanidngsBecause the parsers and N-gram models weredesigned to be very robust, they are notnecessarily sensitive to these obvious clues.
Inorder to get at these hints, we built a character-based N-gram model of English.
Althoughthese indicators were not very informative ontheir own for distinguishing htunan frolnmachine English, they boosted l)erforlnancc inconjunction with the syntactic aud semanticindicators.Combined IndicatorsLet's come back to the three sentences t'romFigure 1: we want to capture our subjectiveranking of tile sentences with appropriateindicator willies.
In other words, we want themachine to be able to see differences which ahuman might see.For these three examples, some scores correhttewell with our subjective ranking of Englishuess(e.g.
cross-perplexity, Edges).
However, theother scores on their own only partiallycorrelate.
The expectation is that an indicatoron its own will not be sutTicient o score tileEnglishness.
It is the combined effect of allindicators which ultimately decides the6We found that we cot, ld often guess the "del'ault"behavior that a parser used and we have begun todesign indicators that can tell when a parser hasdefaulled to these.Englishness.
Now we have enough raw data tobegin machine-learning a way to distinguishthese kinds of sentences.Simple Machine Learning RegimenWe have started out with very simple memory-based machine learning techniques.
Since weare del'ining a range of functions, we wanted tokeep things relatively simple for debugging anddidactic purposes.KNNOne of the simplest methods we can use forclassification is to collect values of the Nindicators for a set of training cases and for thetest cases, to find tile K nearest raining cases(using Euclidean distance in N-dimensionalspace).
For K, we used 5 for our generalCXl)el'iments (but see below fol" sonicvariations).
For a concrete example in twodimensions, imagine that wc use the cross-perplexity of an N-granl language model for theY-axis and the probability of a parse from theCollins parser for tile X-axis.
Human sentencestended to have bettor (lower) cross-perplexitynumbers and better (higher) parse probabilities.If the 5 nearest neighbors to a data point were(h,h,h,h,m) four human sentences and ollemachine our KNN function guesses that it is ahuman sentence.Figure 4 lists some of the parameters we usedfor KNN.
The vahles for cross perplexityranged fronl around 100 to 10,000 and theCollins parse probability (log) ranged fromaround -1000 to 0.
These wlhles weren0rmalizcd torange fi'om 0-1.All columns were scaled between 0 and 1.- Value for K in KNN was set to 5.- Value for L in KNN was set to 0 (L is thei minimum number of positive neighborsI required for a confident classificationi.e.
L=5 means all neighbors must be ofi one class)i- Distance calculation is Euclidean'- We used 10-fold cross-validation andi calculated the average classificationl accuracy for the overall score.t:Figure 4.
KNN l~arametel'sTo get an indication of how much guessingfigured into tile classification, we wwied L fl'om3 to 5, keeping K at 5.
We found that we get thesame overall shape for tile classification, withfewer guesses made.
Of course the penalty fornot guessing as nmch is that more cases are leftunclassified.
When we reduced guessing bysetting L to 4, we correctly classified 47% ofthe human sentences as human and incorrectlychlssified 9% of the human sentences as379machine (the remaining 44% were notclassified).
By setting L to 5 (eliminatingguessing) these numbers dropped to 18% and2% respectively.
When we varied K (forexample, trying K of 101) we found that we canincrease the performance of the humanclassifier to nearly 90%.
Performance of KNNtended to top out at around 74% with theparameters in Figure 4.Indicator MonotonicityThere is no guarantee that classification willperform better with more dimensions in KNN.However, we found that we generally got amonotonically increasing performance inclassification when we added indicatorfunctions.
A helpful analogy might be toconsider the blind men and the elephant.
In ourcase, "English" is the elephant, and each of ourindicator functions is one blind man grasping atthe elephant.
One is grasping at semantics, oneat syntax, and so on.
Figure 5 shows howclassification improves with more indicators(the back of the elephant, so to speak).BenchmarksTo calibrate' the indicator functions we haveused to classify text into human- or machine-produced, we tested our method with someboundary cases, shown in Figure 6.
The mostextreme case was to learn the differencel Bu~in  I( MTo/\ ]r ,oB~Top: Truth is human; machine guesses humanBottom: Truth is MT; machine guesses MTFigure 5between Japanese text (in native characterencoding) and English.Truth is:MachineGuesses:human machineJapanese 99.6 99.6A!phabet Soup 99.4 99.2Word Salad 95\[95.4.4 91.1MT Output \[74.0 56.1Identity Case 52.3 49.4Figure 6.
BaselinesIn other words, we have come up with a verycomputationally expensive method forLanguage Identification.
Next less extreme waswhat we called "Alphabet Soup"; we tookEnglish sentences from the English side of theKenkyusha corpus: for each alphabeticcharacter, we substituted a randomly-selectedalphabetic character, preserving case andpunctuation.
7 For "Word Salad", we took theEnglish sentences from the Kenkyusha corpusand scrambled their order.
MT Output is thecase we discussed in detail above.
The IdentityCase is to divide the English sentences from thecorpus into two piles and then try to tell themapart.
As Figure 6 shows, the pathologicalbaseline cases all work out very well: ourmachine can ahnost always tell that Japanese,Alphabet Soup, and Word Salad are notEnglish.
Nor can it distinguish betweentwo arbitrarily divided piles of humanEnglish.Other Classification AlgorithmsWe have performed some initialexperiments with Support VectorMachines (SVM) as a classificationmethod.
SVM attempts to divide up an n-dimensional space using a set of supportvectors defining a hyperplane.
The basicapproach of the SVM algorithm isdifferent from KNN in that it actuallydeduces a classification model from thetraining data.
KNN is a memory-basedlearning algorithm wherein the model isessentially a replica of the trainingexamples.The initial trials using SVM are yieldingclassification accuracies of correctlyclassifying 83% of the human sentencesand 64% of the machine sentences (single7We found that it was often easy to crash someof the software when we fed it arbitrary binarydata, so we used "Alphabet Soup" instead ofarbitrary binary data.380randonl sample of 10% withheld -- no n-foldcross-validation).
These accuracies representiml~rovenaents of 11% for truman test sentencesand 14% for tile machine test sentences.Further tests on this and other classificationmethods will be investigated to maximizeperformance ill terms of accuracy andexecution time.Next StepsThere are two general areas we are cominuiugto work on: (a) to increase the scope andreliability of our indicators and (b) to insert tilescoring function into a machine-learningregimen for producing translation grammars.
Inthe first area, we have begun to explore tiledegree to which we might recapitulate tileARPA MT Evaluation.
The data from theseevaluations are freely available, a Of course ifall we did was recapitulate the data in somenon-explanatory way, we would be doingsomething analogous to using the ChicagoBears to predict the stock market.
The realwork here is to map the objective scoringfunction numbers back to reliable subjectiveevaluation of the machine-produced texts.
Acrucial task t'or us here is to get a deeperunderstanding of how each of the pieces ofsoftware behaves with various types of inputtext.
We are cnrrently at a quite preliminarystage in terms of the number of indicators weare using and the degree to which each is fine-tuned to out" \]mrpose.
For machine-learning atranslation gramnmr, we have begun to exploreusing our scoring function to drive theconstruction of a prototype Low Densitymachine translation grammar compatible with aprevious ystem built by hand.
We have foundthat the scoring function is sensitive to the wordorder difference between the target Englishtranslation and the glosses for the sourcelanguage.
We would like to re-create acompatible knowledge base of the English halfof the translation grammar using only theglosses as input.
Such a technique wouldreduce the labor requirements in constructing atranslation knowledge base.Reverse Turing Scores for MachineLearning GrammarsTo illustrate how we can use tile ReverseTuring scores to machine learn a grammar, letus consider a simple case of learning lexicalfeatures for a unification-based phrase structuregrammar of the sort discussed ill Jones &SFrom ursula.gcorgctown.cdu/mt_wcl~.Havrilla 1998.
The working assumption there isthat an adequate translation grammar can becreated that conforms to the constraiut that theonly reordering that is allowed is thepermutation of nodes in a biuary-branching tree(as in Wu 1995, among others).
How might welearn that postpositions and verbs generallytrigger inversion?
Consider the followingexample as shown ill Figure 7 fronl Jones &Havrilla 1998; let us use +T to indicate that thelexical item triggers inversion; -T means that itdoes not.
Let the initial state of the lexiconmark all lexical items as "-T".Shobl)a~ karate-men \]baiThii hal~OS N N O IV V!
-T -T +?'
I+T ,, \[+TI 'Sh?bha l the_room-in \],,.sitting _ J,is~igurc 7.
Shobha is sitting in tile room.Ore" machine learning process marks lexicalitems as "+T" when the Reverse Tnringclassification score for the bilingual corpusimproves.ConclusionWe are capitalizing on two historical accideuts:(1) that English is a major world language and(2) that we want to translate into English.
Inaddition to a variety of modern, standard NLPtechniques and ideas, we have drawn fi'om twounlikely sources of intellectual capital: (l)philosophy of language and (2) the currentubiquity of hmguagc cnginecring software.What we have taken from (1) is that we haveassumed that lhere is such a thing as "English".That might not seem like much of anassmnption, but we are treading near some verythorny problems in the philosophy of language.We can no nlore point to English than we canpoint to tile perfect riangle.
And like the blindmen grasping at tile elephant, how wecharacterize it depends on how we are exploringit.
What is ilnportant is the helpful aggregate ofnumeric values that we use for the scoring.What does this mean for machine translation?We want to "Begin with the End in Mind"; inother words, we want the machine translationsystem to create output hat scores well on ourindicators of Englishness.
The rest would bedetails, so to speak.AcknowledgmentsThis project was funded ill part by theAdvanced Research and Development Activity.We would like to thank our colleagues at DoDfor very helpful discussions and insights.381AppendixList of current Indicators1.
Word N-Granl (CM U/Cambridge Language Tk)2.
Ntunber of edges in parse (Collins Parser)3.
Log probability (Collins Parser)4.
1,2xecution time (Collins Parser)5.
Paren count (Collins Parse,')6.
Mean leaf node size of parse live (Collins Parser)7.
Mean NN sequence l ngth (Collins Parser)8.
Overall scorn (Apple Pie Parser)9.
Word level score (Apple Pie Parser)I 0.
Node coun!
(Apple Pie Parser)11.
User execution lime (Apple Pie Parser)12.
CD node count (Apple Pie Parser)13.
Mean CD sequence l ngth (Apple Pie Parser)14.
Mean leading spaces in outline tree (fi'om Collins Parse)15.
Tree balance ratio (fl'om Collins Parse)16.
Tree depth (fi'om Collins Parse)17.
Average minimtun hypernym path length in WordNet18.
Average number hypernym paths in WordNel19.
Path found ratio in WordNel20.
Percent words with sense in WordNet21.
Sum of count of relations (Trigger Toolkit)22.
Mean of eotinl of 1elations (T,igger Toolkit)23.
Sum of mutual information (Trigger Toolkil)24.
Mean of mutual information (Trigger Toolkil)25.
Pairs with mutual information (Trigger Toolkit)26.
Weighled pair sum of mutual information (Trigger Toolkit)27.
Number of target paired words (Trigger Toolkil27.
.
N-Gram Cross-perplexity (Cambridge/CMU Lang Tk.
)ToolsTiMBL: Tilburg Memory Based Learner 2.0.
ILKResearch Group.
http:/lilk.kub.nl/software.html.PCKIMMO 2.0.
Summer Institute of Linguistics.MXTERMINATOR.
Adwait Ratnaparkhi.WEKA 3.0.
University of Waikato.flp://ftp.cs.waikato.ac.nz/pubhnl/weka-3-O.jarCollins Parser 98.Brill Tagger 1.14R Statistical Package 0.65.0. http://cran.r-project.org/Apple Pie Parser 5.8.
New York University.http://cs.nyu.edu/cs/projects/proteus/appWordNet 1.6. ftp://ftp.cogsci.princeton.edu/-wn/Trigger Toolkit 1.0.
CMU.http://www.cs.cmu.edu/~aberger/software.ReferencesAfifi, A.A., Virginia Clark.
1996.
Computer-AidedMultivariate Analysis, 3rd ed.. New York, NY:Chapman and Hall.Brill, Eric.
1995.
Transformation-Based Error-Driven Learning and Natural LanguageProcessing: A Case Study in Part Of SpeechTagging.
(ACL).Chambers, John M., William S. Cleveland, BeatKleiner, and Paul A. Tukey.
1983.
GraphicalMethods for Data Analysis.
Boston, MA: DuxburyPress.Clarkson, Philip, Ronald Rosenfeld.
1997.Statistical Language Modeling Using the CMU-Cambridge Toolkit.
Eurospeech97.
Rhodes,GreeceCollins, Michael.
1997.
Three Generative,Lexicalised Models for Statistical Parsing.Proceedings of the 35th Annual Meeting of theACL/EACL), Madrid.Daelemans, Walter, Jakub Zavrel and Ko van derSleet.
1998.
TiMBL: Tilbm'g Memory BasedLearner, version 2.0, Reference Guide.
Availablel'ronl http://ilk.kub.nl/software.html.Everitt, Brian S. and Graham Dunn.
1992.
AppliedMultivariate Data Analysis.
New York, NY:Oxford University Press.Fellbaum, Christiane (ed.).
1998.
WordNet: AnElectronic Lexical )atabase.
Cambridge, MA: TheMIT Press.Fhu'y, Bernhard and Hans Riedwyl.
1988.Multivariate Statistics.
New York, NY: Chapmanand Hall.Hornik, Kurt.
1999.
"The R FAQ".
Available athttp://www.ci.tuwien.ac.al/Miornik/R/.Jones, Doug and Rick Haw'ilia.
1998.
Twisted PairGrammar: Sul~port for Rapid Development ofMachine Translation for Low Density Languages.AMTA-98.
Langhorn, PA.Knight, Kevin, Ishwar Chandler, Matthew Haines,Vasilcios, Hatzivassigloglou, Eduard Hovy,Masayo Ida, Steve Luk, Richard, Whimey, andKenji Yamada.
1994.
Integrating KnowledgeBases and Statistics in MT (AMTA-94)Manning, Christopher D. and Hinrich Schutze.
1999.Foundations of Statistical Natural LanguageProcessing.
Cambridge, MA: The MIT Press.Masuda, Koh (ed).
1974.
Kcnkyusha's NewJapanese-English Dictionary, 4th Ed.
Tokyo:Kenkyusha.Michalski, Ryszard S., Ivan Bratko, and MiroslavKubat (cds.).
1998.
Machine Learning and DataMining.
John Wiley & Son.Mitchell, Tom M. 1997.
Machine Learning.
Boston,MA: McGraw-Hill.Nirenburg, S. and V. Raskin.
1998.
UniversalGrammar and Lcxis for Quick l~,amp-Ui ~ of MTSystems.
Proceedings of ACL/COL1NG "98.Montrdal: University el' Montreal (in press).Reynar, Jeffrey C. and Adwait Ratnaparkhi.
1997.
AMaximum Entropy Approach to ideutifyingSentence Boundaries.
ANLP-97.
Washington,D.C.Rosenfeld, Ronald.
1996.
A Maximum EntropyApproach to Adaptive Statistical LanguageModeling.
Computer, Speech and Language.Witten, Ian H. and Eibe Frank.
1999.
Data Mining:Practical Machine Learning Tools and Techniqueswith Java Implementations.
Morgan Kaufmann.White, J. and T.A.
O'Connell.
1994.
The ARPA MTEvahmtion Methodologies: Evolution, Lessons,and Future Approaches.
Proceedings of AMTA-94Wu, Dekai and Xuanyin Xia.
1995.
Large-ScaleAutomatic Extraction of an English-ChineseTranslation Lexicon.
Machine Tranlsation.
9:3, 1-28.382
