Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 484?493,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsEvaluating (and Improving) Sentence Alignment under Noisy ConditionsOmar ZaidanMicrosoft Research, USAozaidan@cs.jhu.eduVishal ChowdharyMicrosoft Research, USAvishalc@microsoft.comAbstractSentence alignment is an important stepin the preparation of parallel data.
Mostaligners do not perform very well whenthe input is a noisy, rather than a highly-parallel, document pair.
Evaluating align-ers under noisy conditions would seem torequire creating an evaluation dataset bymanually annotating a noisy document forgold-standard alignments.
Such a costlyprocess hinders our ability to evaluatean aligner under various types and lev-els of noise.
In this paper, we proposea new evaluation framework for sentencealigners, which is particularly suitable fornoisy-data evaluation.
Our approach isunique as it requires no manual labeling,instead relying on small parallel datasets(already at the disposal of MT researchers)to generate many evaluation datasets thatmimic a variety of noisy conditions.
Weuse our framework to perform a compre-hensive comparison of three aligners un-der noisy conditions.
Furthermore, ourframework facilitates the fine-tuning of astate-of-the-art sentence aligner, allowingus to substantially increase its recall ratesby anywhere from 5% to 14% (absolute)across several language pairs.1 IntroductionVirtually all training pipelines of statistical ma-chine translation systems expect training data tobe in the form of a sequence of parallel sentencepairs.
This means that a pair of parallel documentsmust first be segmented into a sequence of alignedsentence pairs, discarding or combining sentenceswhen needed, and aligning sentences as appropri-ate.
The performance and output of an SMT sys-tem is directly dependent on the amount and qual-ity of available training data.
Therefore, it is crit-ical to perform this sentence alignment step prop-erly, ensuring both high recall (to have as muchtraining data as possible) and high precision (toavoid noisy training data).While sentence aligners achieve excellent per-formance on highly-parallel, clean data, the task ismuch more difficult under noisy conditions.
Someprior work has investigated evaluation under noisyconditions (see section 6), but the major focus ofprior work has been the clean-data scenario, whereaccuracy rates exceed 98% (e.g.
Simard et al(1993), Moore (2002)).
For one thing, this meantthat the various sentence alignment algorithms dif-fer only slightly in absolute terms.
Similarly, fine-tuning any one of those algorithms might not seemto have an impact on performance.
More impor-tantly, this also meant that we do not have a clearunderstanding of how well these algorithms wouldperform under noisy conditions.Arguably, there was little need to examine sen-tence alignment of noisy datasets in early MT re-search, since almost all training data came fromhigh-quality, highly-parallel sources, such as UNdocuments or parliamentary proceedings.1 How-ever, recent efforts have attempted to utilize webresources and non-perfectly-parallel texts, such asWikipedia articles and news stories (e.g.
Resnikand Smith (2003), Utiyama and Isahara (2003),Munteanu and Marcu (2005), and Smith et al(2010)).
Such resources naturally contain signifi-cantly more noise, at a level that would render sen-tence alignment a much less straightforward task.Because sentence alignment algorithms hadusually been evaluated under a clean-data sce-nario, there are fewer empirical results to guidethose who wish to extract parallel data from noisy1Also, parallel datasets created explicitly for MT research(by having a source corpus translated into the target language)would be already sentence-aligned by mere construction ifthe source side is split into sentences beforehand.484sources.
Furthermore, there is also no easy wayto fine-tune an aligner of interest.
For buildingthe Microsoft Translation service, we are con-tinuously mining inherently-noisy web resources,from which we extract MT training data for dozensof the world?s languages.
Therefore, having aprincipled method to evaluate and fine-tune ouraligner was critical.In this paper, we describe our framework forevaluating sentence alignment under noisy con-ditions.
We use this framework to examine andevaluate the Moore alignment algorithm (Moore,2002), which was empirically shown to be state-of-the-art under clean conditions, and which weregularly use to extract parallel data from web re-sources to create training data.
We perform a com-prehensive comparison of this aligner against twoother algorithms, and furthermore use our frame-work to fine-tune the algorithm along dimensionsof interest (such as the aligner?s search parame-ters) by quantitatively evaluating how the aligner?sperformance is affected by such changes.The paper is organized as follows.
We brieflydefine sentence alignment and existing approachesin section 2.
We then discuss the evaluation ofalignment algorithms in section 3, and present ourevaluation framework.
In section 4, we perform acomparative assessment of three alignment algo-rithms using our framework, illustrating the dif-ferences between them under noisy conditions.
Insection 5, we present two additional applicationsof our framework, namely fine-tuning an alignerand performing training data cleanup.
Finally, wegive an overview in section 6 of prior work thathas tackled the specific issue of evaluating sen-tence aligners.2 Sentence AlignmentSentence alignment is the process by which a pairof parallel documents lacking explicit sentencelinks are used to extract a parallel dataset consist-ing of sentence pairs that are translations of eachother.
Specifically, let S and T be the documentpair to be aligned, with S composed of the sen-tence sequence s1, s2, ..., sm, and T composed ofthe sentence sequence t1, t2, ..., tn.
A sentencealignment of S and T is a segmentation of eachof S and T into p sequences s?1, s?2, ..., s?p andt?1, t?2, ..., t?p such that the following holds about thesegmentation of S: (a similar set of conditions ex-ist that correspond to T )?
s?i = CS [a, b) for some 1 ?
a ?
b ?
m ?i?
s?1 = CS [1, b) for some b >= 1?
s?p = CS [a,m) for some a <= m?
If s?i = CS [a, b), then s?i+1 = CS [b, c)?
If s?i = CS [x, x), then t?i = CT [y, z) suchthat y 6= zAbove, CS [a, b) is the concatenation ofsa, sa+1, ..., sb?1, which indicates the possibilityof aligning multiple source sentences to a singlesentence (or combined sequence of sentences) onthe target side.
Note that CS [a, a) is the emptystring, which indicates deletion on the target side(i.e.
a target sentence is aligned to the emptystring).
The last condition disallows aligning anempty string to another empty string, thus elimi-nating the possibility for an infinite segmentationsequence.Note that the result of this segmentation is q(non-empty) sentence pairs, where q <= p (andnaturally q <= m and q <= n).
The deleted sen-tences, each aligned with an empty string, are leftout of the resulting parallel corpus.2.1 Approaches to Sentence AlignmentTiedemann (2007) and Santos (2011) each pro-vide a broad overview of sentence alignment, giv-ing a timeline of relevant research and discussingalgorithms and performance metrics for sentencealignment.
In general, there are two main ap-proaches to sentence alignment: length-based andlexical-based.In length-based alignment approaches (e.g.Brown et al(1991), Gale and Church (1991), andKay and Ro?scheisen (1993)), the aligner relies ona probabilistic model that describes the source-to-target sentence length ratio for a pair of corre-sponding sentences.
Such a model would accountboth for the average or typical length ratio as wellas its variance.
The aligner proceeds to align sen-tence pairs such that the output would be highlylikely under the length ratio model.In lexical-based alignment approaches (e.g.Chen (1993), Melamed (1997), Simard and Pla-mondon (1998), Menezes and Richardson (2001),and the LDC alignment tool, Champollion (Ma,2006)), the aligner relies on a probabilistic modelthat describes the lexical similarity between a pairof sentences.
The model could either be a fully-trained translation model, or a simpler bilingual485lexicon that finds corresponding word pairs.
Incontrast to length-based algorithms, lexical-basedapproaches typically require external bilingual re-sources, and usually perform better.Previous work on sentence alignment variesacross a few other dimensions as well.
Somelexical-based algorithms build the needed bilin-gual resources from the very dataset that is tobe aligned, whereas other approaches assume thatsuch resources are externally provided.
Anotherdimension is the need to provide anchor pointswithin the text to be aligned, such as in the formof paragraph-level alignment.
Such anchor pointsare typically needed to restrict the search space toa manageable size.Another group of aligners take a hybrid ap-proach, relying both on sentence length and lexicalsimilarity (e.g.
Zhao and Vogel (2002)).
One no-table example is the algorithm by Moore (2002),which has the benefit of relying only on the in-put data when training the lexical similarity model,rather than needing external resources (bilinguallexicon or parallel training data) for that purpose.The Moore algorithm is a state-of-the-art algo-rithm, and has been used, for example, to alignthe data for the Europarl corpus (Koehn, 2005),and is often a strong baseline in papers propos-ing new alignment algorithms (e.g.
Braune andFraser (2010)).
In section 4, we use our pro-posed framework to evaluate Moore?s algorithm,and compare it against two other aligners, illustrat-ing our framework?s utility as a comparative tool.3 Evaluating Sentence AlignmentAlgorithms under Noisy ConditionsIn much of the prior work mentioned above in 2.1,and in other comparative evaluation work (e.g.Simard et al(1993), Langlais et al(1998), andVe?ronis and Langlais (2000)), sentence align-ment algorithms were evaluated using a manually-created gold-standard dataset.
This is done bytaking a parallel dataset, and manually annotatingsentence pairs that are translations of each other(and should therefore be aligned).
This evaluationdataset is provided as input to the aligner, whichis evaluated based on the precision and recall ofits output, as measured against the set of hand-annotated sentence pairs.While this is a reasonable approach that mir-rors the evaluation model in many other taskswithin machine learning (i.e.
to manually createan evaluation set with gold-standard labels, basedon which the learner?s output is judged), it suffersfrom some drawbacks.For one thing, all the difficulties of creating anevaluation dataset apply here as well.
Most signif-icantly, manually labeling sentence pairs is costlyand time-consuming.
This problem is magnifiedin the context of machine translation, since oneshould ideally evaluate a sentence alignment algo-rithm under several language pairs, rather than asingle one, requiring the creation of several evalu-ation sets, rather than a single one.Furthermore, prior work usually used a fairlyclean dataset to annotate, on which it is relativelyeasy for an aligner to achieve very high precisionand recall rates.
This means that differences be-tween algorithms are sometimes fairly small in ab-solute terms, making it difficult to attribute suchdifferences to the algorithms themselves or to sta-tistical noise.The noisy-data scenario is extremely importantin the web domain.
The web is a huge repositoryof parallel documents that machine translationsystems leverage for training data, and we continu-ally extract content from noisy online sources.
Un-like the above evaluation setup, we are concernedwith scenarios where the data has a relatively highdegree of noise, where by ?noise?
we mean bothnon-perfect translations but also additional contenton one side that is not translated at all.
Both kindsof noise should be dealt with appropriately: thefirst introduces imperfect training data, while thesecond could eliminate good translations, or mightsend word alignment into a frenzy.Because prior work mostly focused on theclean-data scenario, it is unknown whether previ-ous evaluations would hold for noisy input.
Thismakes it difficult to judge how these algorithmswould compare to each other under more noisyconditions, or when any other experimental di-mension is varied, such as domain and the lan-guage pair in question.3.1 Creating Noisy Datasets for EvaluationPurposesHow can we create a noisy-data scenario underwhich to evaluate a sentence alignment algorithm?One approach is to mimic prior work: in a datasetthat is known to be noisy, have an annotator selectthe sentence pairs that should be aligned to eachother.
However, this approach would be expensive486and time-consuming.We propose a completely different approach.Rather than attempting to annotate correspondingsentences in a dataset that is known to be noisy,we deliberately introduce noise into a dataset thatis already perfectly-aligned (and for which, as aconsequence, we already know the sentence cor-respondence).Specifically, we start with a parallel datasetD that we know to be perfectly-aligned.
Suchdatasets are abundant and readily available for MTresearchers in the form of a myriad of tuning andtest datasets across many language pairs and do-mains.
We introduce noise into D (using any ofthe methods described below and detailed in sub-section 4.2) to obtain a modified dataset D?.
Thesource side of D?
is a subset of the source side ofD (possibly reordered), and the same holds for thetarget side.
Since we know what the correct sen-tence alignments are in D, we also know, by mereconstruction, what the correct alignments in D?
areas well.
This allows us to easily compute precisionand recall of a sentence alignment algorithm whenit is given D?
as input, without the need to collecta single annotation.We employ several methods to create a noisydataset D?
from a perfectly-aligned dataset D:2?
Clean dataset.
The source and target sides ofD?
are exactly the unaltered source and targetsides of D. This represents the easiest test setfor a sentence aligner, as the test set consistsentirely of 1-to-1 mappings, all of which fallexactly along the search matrix diagonal.?
Random deletions.
The source side of D?
isa subset of the source side of D, where thenumber of discarded sentences is determinedby a source deletion rate dels.
For example,for a dataset D with 1000 sentences on thesource side and dels = 0.10, the source sideof D?
consists of 900 randomly-chosen sen-tences from the source side of D (with no re-ordering).
The target side of D?
is createdsimilarly, using a target deletion rate delt.Note that the deletion on the target side isdone independently from the deletion on the2In a few of our experiments, we make use of two datasets(that are non-overlapping and non-related), say D1 and D2,to createD?.
The way we frame the creation ofD?, as a map-ping from a single dataset D, still applies here: D is simplythe concatenation of D1 and D2.source side.
That is, the probability of delet-ing the ith sentence on the target side is delt,regardless of whether the ith sentence on thesource side was deleted or not.?
Random combinations.
The source and tar-get sides of D?
are the same as those fromD, but with random consecutive pairs of sen-tences combined into a single sentence.
Thedegree to which sentences are combined isdetermined by source and target combinationrates combs and combt.
For example, for adataset D with 1000 sentences on the sourceside and combs = 0.10, 100 sentence pairs(each consisting of consecutive sentences)are chosen randomly, and each pair is com-bined into a single sentence, yielding a setof 900 source sentences in D?.
The goal ofthis scenario is to test the aligner?s ability torecover 1-to-many and many-to-1 mappings,rather than focusing solely on 1-to-1 map-pings.3 As with random deletions, the combi-nation processes on the source side and on thetarget side are independent from each other.?
Randomized order.
The source side of D?consists of the source side of D, but in ran-dom order.
The target side of D is also ran-domized.?
Length-aligned from same dataset.
Thesource side of D?
is exactly the same as thesource side of D. The noise is introducedinto the target side, where all the target sen-tences from D are preserved, but they are re-ordered.
The reordering is not completelystochastic.
Rather, an attempt is made to havethe sentences length-aligned as much as pos-sible.
This is somewhat of an adversarial sce-nario, since a length-based alignment methodwould align too many sentences that are com-pletely unrelated to each other.?
Different datasets.
The dataset D?
is formedby taking two datasets D1 and D2, and align-ing the source side of D1 with the target sideof D2, and vice versa.
A good sentencealigner would deem that the source and tar-get sides are unrelated, yielding a very lowalignment rate.3With high enough combination rates, many-to-manymappings arise as well.4874 Experimental ResultsEven though this paper is not mainly concernedwith comparing aligners to each other, we utilizeour proposed framework and apply it to three dif-ferent aligners as a demonstration.
In this section,we describe the aligners to be compared, and pro-vide specific details about how our test sets weregenerated.
We then describe the metrics we use,and present results based on these metrics.4.1 Sentence AlignersThe first aligner (LEN) is a length-based alignerbased on the algorithm described in Brown et al(1991).
It segments the source and target sidesby finding the highest-likelihood segmentation ac-cording to a model describing the relationship be-tween source sentence length and target sentencelength.
In particular, this relationship is modeledusing a Poisson distribution that has as its meanthe length ratio observed in the dataset to align.4The second aligner (MRE) is based on Moore?salgorithm (Moore, 2002), which makes use of thelength-based aligner?s output to build a tentativemodel 1.
Moore?s algorithm takes the output fromthis ?first phase?
and builds a bilingual lexicon thatallows it to compute translation model scores.
Fora given pair of sentences, the likelihood that theyare translations of each other is now computedbased not only on their lengths, but also on theirlexical similarity.The third aligner (MRE+) is similar to the sec-ond aligner, but uses a much stronger translationmodel.
The stronger translation model is simplythe translation system that has already been builtfor that particular language pair and now helpsaligning new data.
While this requires the avail-ability of external resources, this setup closely re-sembles the resources we have, given our paralleltraining datasets.
We note here that our evaluationdatasets have no overlap with the data used to trainthe translation models used by MRE+.4.2 Noisy Dataset GenerationFor random deletions, we use six different dele-tion rates (from 0.00 to 0.25, with 0.05 incre-ments), both on the source side and the target side,for a total of 35 test sets.
For random combi-nations, we use four different combination rates(from 0.00 to 0.15, with 0.05 increments), both4Note that we follow Moore (2002) in using a Poissondistribution instead of the Gaussian of Brown et alon the source side and the target side, for a to-tal of 15 test sets.
Note that we do not considerthe case when both deletion/combination rates are0.00, since that mimics the clean-dataset scenario.For the length-aligned scenario, we align eachsource sentence with a randomly-selected sen-tence from the target side that is closest in lengthto that source sentence.
(We take the target-to-source length ratio into consideration, and multi-ply the source length by that ratio before trying tofind the closest-length target sentence.)
If severaltarget sentences have lengths that are equally closeto the desired length, we pick one at random.We note here that if the source sentences areprocessed sequentially, there will be a clusteringof overly long target sentences at the bottom ofthe dataset, since such sentences are never chosenbased on length ?
they are simply too long.
There-fore, we process the source sentences in randomorder rather than sequentially, to avoid this clus-tering of long sentences.4.3 Performance MetricsWe report the following metrics for quantitativelyevaluating and describing the output of the sen-tence aligner:?
Precision: of the sentence pairs producedby the aligner, what percentage are sentencepairs in the gold-standard dataset D??
Recall: of the sentence pairs in the gold-standard dataset D, what percentage are pro-duced by the aligner??
Alignment rate: what proportion of the sen-tences in the input dataset D?
were alignedby the aligner?
Due to the possibility that thesource and target sides of D?
have differentsizes, there are two alignment rates, and wereport their average.5Higher precision and higher recall are, by defi-nition, indicators of better performance.
This can-not be said of the alignment rate.
For instance,consider the noisy deletion scenario of 3.1 above.By mere construction of D?, there will be source(resp.
target) sentences that should not be alignedto anything on the target (resp.
source) side, sincewe deliberately deleted the corresponding sen-tence.
In such cases, an alignment rate of 100%5Of course, the dataset returned by the aligner always hassource and target sides of equal sizes.488Language Test Scenario LEN MRE MRE+PairClean (no noise) 100%, 82%, 82% 100%, 85%, 85% 100%, 99%, 99%dels = delt = 0.05 100%, 46%, 44% 99%, 71%, 68% 100%, 96%, 91%EN-ES combs = combt = 0.05 100%, 39%, 38% 99%, 66%, 64% 100%, 92%, 89%Randomized 0%, 0%, 1% 0%, 0%, 4% 34%, 1%, 4%Length-aligned 0%, 0%, 82% 0%, 0%, 15% 0%, 0%, 7%Clean (no noise) 100%, 55%, 55% 100%, 60%, 60% 100%, 89%, 89%dels = delt = 0.05 99%, 27%, 26% 99%, 44%, 42% 100%, 82%, 78%EN-AR combs = combt = 0.05 99%, 22%, 21% 99%, 41%, 39% 99%, 77%, 74%Randomized N/A, 0%, 0% 17%, <1%, <1% 26%, <1%, 1%Length-aligned 0%, 0%, 59% 0%, 0%, 9% 5%, <1%, 2%Clean (no noise) 100%, 66%, 66% 100%, 72%, 72% 100%, 97%, 97%dels = delt = 0.05 100%, 40%, 39% 99%, 56%, 55% 100%, 92%, 88%EN-CH combs = combt = 0.05 99%, 35%, 34% 99%, 52%, 50% 99%, 87%, 82%Randomized 0%, 0%, <1% 0%, 0%, <1% 29%, <1%, 2%Length-aligned 0%, 0%, 62% 0%, 0%, 13% 2%, <1%, 5%Clean (no noise) 100%, 68%, 68% 100%, 72%, 72% 100%, 95%, 95%Average dels = delt = 0.05 100%, 38%, 36% 99%, 57%, 55% 100%, 90%, 86%(over the combs = combt = 0.05 99%, 32%, 31% 99%, 53%, 51% 99%, 85%, 82%3 LP?s) Randomized 0%, 0%, <1% 6%, <1%, 2% 30%, 1%, 2%Length-aligned 0%, 0%, 68% 0%, 0%, 12% 2%, <1%, 5%Table 1: Results of the comparative experiment of the three aligners.
For brevity, we report the resultsfor only five scenarios (per language pair and aligner) out of the more than fifty scenarios we propose.Each cell contains three percentages: precision, recall, and alignment rate.
The N/A precision value forLEN in the EN-AR randomized scenario indicates the aligner produced no output.for example (i.e.
all input sentences were alignedto some other sentence) is indicative of pervasivealignment rather than good performance.6Hence, alignment rate is not a performancemeasure in the conventional sense, as it is not anobjective to be maximized or minimized.
Still, it isa useful descriptor that sheds light on the aligner?sbehavior, as we see in the next subsection.4.4 ResultsWe carried out experiments covering three lan-guage pairs: English-Spanish, English-Arabic,and English-Chinese.
The comparative experi-ment is quite telling, and the results (Table 1) pointto consistent and noticeable differences betweenthe three examined aligners.
While all alignershave very high alignment precision rates in non-adversary scenarios, always exceeding 99%, thedifference is in how well they recover sentencepairs that should be aligned to each other, illus-6Even an oracle aligner with perfect precision and recallwill almost surely have an alignment rate less than 100% (oreven 90%) when D?
is constructed using high deletion rates.trated by significant differences in recall rates.The clearest trend is that the length-based al-gorithm (LEN) performs worse than Moore?s al-gorithm (MRE), which in turn benefits quite a bitwhen it?s aided by an external strong translationmodel (MRE+).
It is worth pointing out that the gapbetween MRE and MRE+ is typically larger than thegap between LEN and MRE, suggesting the impor-tant of external bilingual resources to aid the sen-tence aligner.The results of the adversary scenarios (random-ized and length-aligned) are particularly interest-ing.
Looking at precision and recall alone, it mightseem that there is not much to separate the threealgorithms.
For example, they all have 0% pre-cision and 0% recall in the length-aligned EN-ESscenario (fifth row of Table 1).
However, lookingat the alignment rate, we find that LEN was proneto over-aligning the data, having an (unnecessarilyvery high) alignment rate of 82%.
On the otherhand, MRE and MRE+, have much lower alignmentrates of 15% and 7%, respectively.
This meansthat they would introduce only a fraction of the489bad data that LEN would, which is a great advan-tage for MRE and especially MRE+.5 Applications of the EvaluationFrameworkIn the previous section, we utilized our frameworkto perform a comparison between three differentaligners, by evaluating them under various noisy-data circumstances.
In this section, we use ourframework in two more applications relevant tosentence alignment and machine translation.5.1 Fine-tuning Aligner ParametersWe explore using the evaluation setup to fine-tunethe parameters of the MRE+ algorithm.
Lackinga principled way to evaluate the aligner?s output,it was not possible to fine-tune the aligner?s var-ious parameters.
Now, equipped with our eval-uation framework, it is possible to quantitativelydetermine the effect of changing the value of anyparameter, and pick the best value.
This is prefer-able to accepting whatever default parameters arein already place, which are more than likely suit-able for a specific domain, dataset, or low-to-nonexistent noise.5.1.1 Experimental DesignWe fine-tune the parameters of the MRE+ algo-rithm by optimizing its performance on a tuningdataset generated using the noisy deletion setup,and then measure its performance on a differentevaluation set that was also generated using thenoisy deletion setup.
We investigate two cases,one with dels = delt = 0.05, and one withdels = delt = 0.20, to examine the benefit offine-tuning both under a relatively low noise leveland under a relatively high noise level.We optimize the performance of the MRE+ algo-rithm along three dimensions:?
Prior probabilities (PRIOR).
As explainedin section 2, sentence alignment is essentiallya segmentation of the source and target sidesof the parallel dataset.
In addition to relyingon length similarity and lexical correspon-dence, the MRE+ aligner also relies on a set ofprior probabilities for each insert/delete/alignaction it could take.
By default, the probabil-ity assigned to deletion and insertion was setat 0.02.
It is reasonable to assume that thismight be too low, especially for highly-noisyinput data, and so this is the first dimensionthat we optimize.?
Search beam size (SIZE).
The algorithm alsopays attention to the location of a candidatesentence pair.
While positional similaritydoes not play a direct role in computing thealignment probability, the aligner does prunethe search space based on location.
For ex-ample, when considering a sentence half-waythrough the source side, only sentences thatare close to the half-way point in the targetside will be considered.
How far the aligneris willing to deviate from the diagonal7 is atunable parameter, making it our second di-mension.?
Alignment threshold (THRESHOLD).
Thealigner assigns a probability to each sentencepair it considers for alignment, reflecting itsconfidence that the sentence pair should bealigned.
By default, the aligner eliminatesany sentence pair that fails to meet a thresh-old of 0.99.
This alignment threshold is thethird dimension we optimize, as it should belowered or increased to reflect our confidencein the translation model and/or the variabilityof the length-correspondence model.5.1.2 Experimental ResultsThe results in Tables 2 and 3 show the benefit ofoptimizing the aligner?s parameters.
It is bene-ficial to optimize the prior probabilities and thealignment threshold, as indicated by higher recallrates compared to the default values.
On the otherhand, the tuning of the search beam size had mini-mal impact.
This indicates that the mistakes madeby the sentence aligner are usually model errorsrather than search error.The effect of optimizing the prior probabilitiesis more pronounced in the high-noise scenario (Ta-ble 3), where it proves to provide the most gainover the baseline.
Contrast this with the low-noisescenario (Table 2), where optimizing the align-ment threshold is at least equally important, if notmore so.
This is to be expected, since the de-fault prior of 0.02 in the high-noise scenario sig-nificantly underestimates the amount of deletionthat has actually taken place, making the prior themost important parameter to optimize.7If we were to create a grid of alignment probabilities, thispruning of the search space means that grid cells far off thediagonal of this grid are never considered.490Tuned EN-ES EN-AR EN-CHparameter(s)None 95.7% 82.4% 92.0%PRIOR 96.2% 85.6% 93.5%SIZE 95.8% 82.8% 92.0%THRESHOLD 96.8% 86.7% 92.9%All 97.1% 87.5% 93.7%Table 2: Results of the MRE+ fine-tuning experi-ment for the 0.05 deletion rate scenario.
For clar-ity, we show only recall rates ?
all precision ratesare 99% or higher.Tuned EN-ES EN-AR EN-CHparameter(s)None 87.8% 68.1% 81.9%PRIOR 92.7% 81.5% 88.4%SIZE 88.0% 68.8% 82.3%THRESHOLD 89.3% 70.4% 84.3%All 93.0% 82.8% 90.6%Table 3: Results of the MRE+ fine-tuning experi-ment for the 0.20 deletion rate scenario.
For clar-ity, we show only recall rates ?
all precision ratesare 98% or higher.It is worth pointing out the work of Yu et al(2012), who perform a comparative study of sen-tence aligners, and show that Moore?s algorithmdoes not perform as well as other aligners on anoisy dataset.
As they provide no details regardingthe values of the various parameters of Moore?salgorithm, one can assume that they used defaultvalues and performed no tuning.
Of course, suchtuning would not have been easy to perform, giventhe lack of a tuning dataset.
This is exactly whywe propose our evaluation framework, so that fu-ture researchers would not have to guess parame-ter values or accept default values if they believethat would lead to suboptimal performance.
Giventhe results of our experiments, it is conceivablethat the performance of Moore?s algorithm in Yuet als work (and other algorithms they examinedas well) might have been improved had their pa-rameters been optimized.5.2 Using Sentence Alignment to FilterTraining DataMuch of our training data comes from noisysources, both online and otherwise.
Due to the vastamount of data, it is not possible to go through it todiscard noisy sentence pairs.
Now, equipped witha better understanding of our sentence aligner andits performance, we use it to trim down our train-ing data by eliminating sentence pairs to which thealigner does not assign a high weight.5.2.1 Experimental DesignWe provide our current training data as input to thesentence aligner, and treat the output of the aligneras a filtered version of our data, since sentencesthat are discarded (not aligned) by the aligner tendto be noisy data.
To evaluate the effectivenessof this process, we compare models trained withpre-filtered data vs. ones trained with the filtereddata.
We examine how the filtering affects thedata and model size, since trimming those downwould speed up training and translation.
This isespecially relevant for us given the large numberof language pairs for which we train models.
Toensure the translation quality doesn?t degrade, wemeasure the effect on translation quality for twoin-house evaluation datasets.We consider three scenarios:?
No filtering.
As a baseline, we use our train-ing data as-is to train the MT system, withoutany filtering.?
Uniform filtering.
We provide our trainingdata as input to the sentence aligner, and usethe aligner?s output as the training data totrain the MT system.
(We refer to this as?uniform?
filtering in contrast to the next sce-nario.)?
Filtering ?web?
datasets.
Here, we applysentence alignment filtering only to certainhand-picked datasets that we believe to con-tain a relatively high level of noise.
Thedatasets are not picked by inspecting theircontent, but simply by deciding that anydataset that came from online sources (aka?web?
data) should undergo filtering.5.2.2 Experimental ResultsWe performed our filtering experiments on twosystems, Arabic-English and Urdu-English, withthe results displayed in Tables 4 and 5, respec-tively.
In all cases but one, the BLEU score wentup or down by less than a quarter of a point, indi-cating general stability in performance quality.This line of experiments is still in progress.
Weplan to carry out another set of experiments where491Scenario Data Model Test1 Test2Size Size BLEU BLEUNo filtering 100% 100% 31.44 30.57All filtered 94.8% 96.7% 31.29 30.34Web only 96.6% 96.0% 31.54 30.52Table 4: Results of the data filtering experimentsfor the Arabic-English system.Scenario Data Model Test1 Test2Size Size BLEU BLEUNo filtering 100% 100% 38.03 13.32All filtered 81.6% 85.9% 38.19 13.13Web only 99.1% 99.1% 37.80 12.78Table 5: Results of the data filtering experimentsfor the Urdu-English system.the prior deletion probability is customized foreach portion of our training data, based on ourbelief of how noisy that portion of the dataset is.We are also expanding the experiments to includemore language pairs.6 Related WorkSingh and Husain (2005) evaluate several sentencealignment algorithms.
Their work does have a hintof proposing a fuller evaluation framework, in thatthey have one test scenario where noise is added totheir test set (in the form of adding sentences fromanother, unrelated dataset).
Another major differ-ence from our work is that they rely on manualevaluation of the output, as is the case for much ofprior work.Moore does point out that the error rates ob-tained by his algorithm are very low partly becausethe data being aligned is highly parallel, there-fore making it ?fairly easy data to align?
(Moore(2002), p. 142).
He therefore presents one ad-ditional experiment where a single block of sen-tences is deleted from one side of the input tomimic a noisy condition.
While this is similar inspirit to our noisy deletions scenario, it introducesonly a very small amount of noise in practice.
Thisis because the deleted sentences are all sequentialrather than being at different positions in the cor-pus, are all on one side of the corpus, and sincethe deletion rate was very low (varied up to only3.0%).
Case in point, the resulting dataset was stillvery easy to align, with error rates that remainedbelow 2.0% even for the baseline aligner.Yu et al(2012) use the BAF dataset (Simard,2006) as an evaluation dataset, since it is knownto contain a relatively high degree of 0-1 and 1-0beads (what they call ?null links?
), and use thatdataset specifically to evaluate an alignment al-gorithm customized to handle noisy data.
Simi-larly, Rosen (2005) evaluates several aligners us-ing three datasets, one of which is characterizedas being more noisy than the others.Abdul-Rauf et al(2012) compare several algo-rithms to each other, across several datasets, in-cluding the noisy BAF dataset.
However, they donot propose a full framework for evaluating sen-tence alignment itself, and instead emphasize thedifferences in performance of MT systems trainedon the aligned data.There is a good amount of prior work deal-ing with filtering noisy data from parallel datasets.Taghipour et al(2010) propose a discriminativeframework to filter noisy sentence pairs from par-allel data, and apply it to a Farsi-English dataset.Denkowski et al(2012) briefly describe a filter-ing method to clean up training data for a French-English system submitted to WMT 2010, relyingon deviations from typical values for certain sta-tistical measures to identify noisy sentence pairs.7 ConclusionIn this paper, we proposed a new evaluation frame-work for sentence aligners, which is specificallydesigned with noisy-data conditions in mind.
Ourapproach is unique in that it requires absolutelyno manual labeling, and relies on parallel datasetsthat are already in existence.
We provide sev-eral methods to deliberately introduce noise into adataset that is already perfectly-aligned, thus cre-ating a whole host of evaluation test sets quicklyand at no cost.Our framework allows us and other researchersto easily compare and contrast several aligners toeach other.
Furthermore, our framework can beused to improve the performance of an aligner byfacilitating the fine-tuning of any or all of its hy-perparameters.ReferencesSadaf Abdul-Rauf, Mark Fishel, Patrik Lambert, San-dra Noubours, and Rico Sennrich.
2012.
Extrin-sic evaluation of sentence alignment systems.
InProceedings of LREC Workshop on Creating Cross-492language Resources for Disconnected Languagesand Styles, CREDISLAS, pages 6?10.Fabienne Braune and Alexander Fraser.
2010.
Im-proved unsupervised sentence alignment for sym-metrical and asymmetrical parallel corpora.
In Pro-ceedings of COLING: Poster Volume, pages 81?89.Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer.1991.
Aligning sentences in parallel corpora.
InProceedings of ACL, pages 169?176.Stanley F. Chen.
1993.
Aligning sentences in bilingualcorpora using lexical information.
In Proceedings ofACL, pages 9?16.Michael Denkowski, Greg Hanneman, and Alon Lavie.2012.
The CMU-Avenue French-English transla-tion system.
In Proceedings of the NAACL Work-shop on Statistical Machine Translation, pages 261?266.William A. Gale and Kenneth W. Church.
1991.
Aprogram for aligning sentences in bilingual corpora.In Proceedings of ACL, pages 177?184.Martin Kay and Martin Ro?scheisen.
1993.
Text-translation alignment.
Computational Linguistics,19(1):121?142.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof MT Summit, pages 79?86.Philippe Langlais, Michel Simard, and Jean Ve?ronis.1998.
Methods and practical issues in evaluatingalignment techniques.
In ACL/COLING, pages 711?717.Xiaoyi Ma.
2006.
Champollion: A robust parallel textsentence aligner.
In Proceedings of LREC, pages489?492.I.
Dan Melamed.
1997.
A portable algorithm for map-ping bitext correspondence.
In Proceedings of ACL,pages 305?312.Arul Menezes and Stephen D. Richardson.
2001.
Abest-first alignment algorithm for automatic extrac-tion of transfer mappings from bilingual corpora.
InProceedings of the ACL Workshop on Data-DrivenMethods in Machine Translation, pages 39?46.Robert C. Moore.
2002.
Fast and accurate sen-tence alignment of bilingual corpora.
In Stephen D.Richardson, editor, AMTA 2002: From Research toReal Users, pages 135?144.
Springer Berlin Heidel-berg.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguis-tics, 31(4):477?504.Philip Resnik and Noah A. Smith.
2003.
The webas a parallel corpus.
Computational Linguistics,29(3):349?380.Alexandr Rosen.
2005.
In search of the best methodfor sentence alignment in parallel texts.
In Proceed-ings of SLOVKO.Andre?
Santos.
2011.
A survey on parallel corporaalignment.
In Proceedings of MI-Star, pages 117?128.Michel Simard and Pierre Plamondon.
1998.
Bilin-gual sentence alignment: Balancing robustness andaccuracy.
Machine Translation, 13:59?80.Michel Simard, George F. Foster, and Pierre Isabelle.1993.
Using cognates to align sentences in bilin-gual corpora.
In Proceedings of the Conference ofthe Centre for Advanced Studies on CollaborativeResearch: Distributed Computing - Volume 2, pages1071?1082.Michel Simard.
2006.
The BAF: A corpus of English-French bitext.
In Proceedings of LREC, pages 489?494.Anil Kumar Singh and Samar Husain.
2005.
Com-parison, selection and use of sentence alignment al-gorithms for new language pairs.
In Proceedings ofthe ACL Workshop on Building and Using ParallelTexts, pages 99?106.Jason R. Smith, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from compara-ble corpora using document level alignment.
In Pro-ceedings of NAACL, pages 403?411.Kaveh Taghipour, Nasim Afhami, Shahram Khadivi,and Saeed Shiry.
2010.
A discriminative approachto filter out noisy sentence pairs from bilingual cor-pora.
In Proceedings of International Symposium onTelecommunications, pages 537?541.Jo?rg Tiedemann.
2007.
Improved sentence alignmentfor movie subtitles.
In Proceedings of Recent Ad-vances in Natural Language Processing.Masao Utiyama and Hitoshi Isahara.
2003.
Reliablemeasures for aligning japanese-english news articlesand sentences.
In Proceedings of ACL, pages 72?79.Jean Ve?ronis and Philippe Langlais.
2000.
Evaluationof parallel text alignment systems: The ARCADEproject.
In Jean Ve?ronis, editor, Parallel Text Pro-cessing: Alignment and Use of Translation Corpora,pages 369?388.
Kluwer Academic Publishers.Qian Yu, Aure?lien Max, and Franc?ois Yvon.
2012.Revisiting sentence alignment algorithms for align-ment visualization and evaluation.
In Proceedingsof the LREC Workshop on Building and Using Com-parable Corpora, pages 10?16.Bing Zhao and Stephan Vogel.
2002.
Adaptive paral-lel sentences mining from web bilingual news col-lection.
In IEEE International Conference on DataMining, pages 745?748.493
