Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 177?180, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSparse Bayesian Classification of Predicate ArgumentsRichard Johansson and Pierre NuguesLUCAS, Department of Computer Science, Lund UniversityBox 118SE-221 00 Lund, Sweden{richard, pierre}@cs.lth.seAbstractWe present an application of SparseBayesian Learning to the task of semanticrole labeling, and we demonstrate that thismethod produces smaller classifiers thanthe popular Support Vector approach.We describe the classification strategy andthe features used by the classifier.
In par-ticular, the contribution of six parse treepath features is investigated.1 IntroductionGeneralized linear classifiers, in particular SupportVector Machines (SVMs), have recently been suc-cessfully applied to the task of semantic role iden-tification and classification (Pradhan et al, 2005),inter alia.Although the SVM approach has a number ofproperties that make it attractive (above all, excel-lent software packages exist), it also has drawbacks.First, the resulting classifier is slow since it makesheavy use of kernel function evaluations.
This isespecially the case in the presence of noise (sinceeach misclassified example has to be stored as abound support vector).
The number of support vec-tors typically grows with the number of training ex-amples.
Although there exist optimization methodsthat speed up the computations, the main drawbackof the SVM approach is still the classification speed.Another point is that it is necessary to tune theparameters (typically C and ?).
This makes it nec-essary to train repeatedly using cross-validation tofind the best combination of parameter values.Also, the output of the decision function of theSVM is not probabilistic.
There are methods to mapthe decision function onto a probability output usingthe sigmoid function, but they are considered some-what ad-hoc (see (Tipping, 2001) for a discussion).In this paper, we apply a recent learningparadigm, namely Sparse Bayesian learning, ormore specifically the Relevance Vector learningmethod, to the problem of role classification.
Itsprincipal advantages compared to the SVM ap-proach are:?
It typically utilizes fewer examples comparedto the SVM, which makes the classifier faster.?
It uses no C parameter, which reduces the needfor cross-validation.?
The decision function is adapted for probabilis-tic output.?
Arbitrary basis functions can be used.Its significant drawback is that the training pro-cedure relies heavily on dense linear algebra, and isthus difficult to scale up to large training sets andmay be prone to numerical difficulties.For a description of the task and the data, see (Car-reras and M?rquez, 2005).2 Sparse Bayesian Learning and theRelevance Vector MachineThe Sparse Bayesian method is described in detail in(Tipping, 2001).
Like other generalized linear learn-ing methods, the resulting binary classifier has theformsignf(x) = signm?i=1?ifi(x) + b177where the fiare basis functions.
Training themodel then consists of finding a suitable ?
=(b, ?1, .
.
.
, ?m) given a data set (X,Y ).Analogous with the SVM approach, we can letfi(x) = k(x, xi), where xiis an example from thetraining set and k a function.
We have then arrivedat the Relevance Vector Machine (RVM).
There arehowever no restrictions on the function k (such asMercer?s condition for SVM).
We use the Gaussiankernel k(x, y) = exp(??
?x ?
y?2) throughout thiswork.We first model the probability of a positive ex-ample as a sigmoid applied to f(x).
This can beused to write the likelihood function P (Y |X,?
).Instead of a conventional ML approach (maximiz-ing the likelihood with respect to ?, which wouldgive an overfit model), we now adopt a Bayesianapproach and encode the model preferences usingpriors on ?.
For each ?i, we introduce a parame-ter siand assume that ?i?
N(0, s?1i) (i.e.
Gaus-sian).
This is in effect an ?Occam penalty?
that en-codes our preference for sparse models.
We shouldfinally specify the distributions of the si.
However,we make the simplifying assumption that their dis-tribution is flat (noninformative).We now find the maximum of the marginal likeli-hood, or ?evidence?, with respect to s, that isp(Y |X, s) =?P (Y |X,?
)p(?|s)d?.This integral is not tractable, hence we approximatethe integrand using a Gaussian centered at the modeof the integrand (Laplace?s approximation).
Themarginal likelihood can then be differentiated withrespect to s, and maximized using iterative methodssuch as gradient descent.The algorithm thus proceeds iteratively as fol-lows: First maximize the penalized likelihood func-tion P (Y |X,?
)p(?|s) with respect to ?
(for ex-ample via the Newton-Raphson method), then up-date the parameters si.
This goes on until a con-vergence criterion is met, for example that the sichanges are small enough.
During iteration, the siparameters for redundant examples tend to infinity.They (and the corresponding columns of the kernelmatrix) are then removed from the model.
This isnecessary because of numerical stability and also re-duces the training time considerably.We implemented the RVM training method usingthe ATLAS (Whaley et al, 2000) implementationof the BLAS and LAPACK standard linear algebraAPIs.
To make the algorithm scale up, we used aworking-set strategy that used the results of partialsolutions to train the final classifier.
Our implemen-tation is based on the original description of the al-gorithm (Tipping, 2001) rather than the greedy opti-mized version (Tipping and Faul, 2003), since pre-liminary experiments suggested a decrease in clas-sification accuracy.
Our current implementation canhandle training sets up to about 30000 examples.We used the conventional one-versus-one methodfor multiclass classification.
Although the SparseBayesian paradigm is theoretically not limited to bi-nary classifiers, this is of little use in practice, sincethe size of the Hessian matrix (used while maximiz-ing the likelihood and updating s) grows with thenumber of classes.3 System DescriptionLike previous systems for semantic role identifica-tion and classification, we used an approach basedon classification of nodes in the constituent tree.To simplify training, we used the soft-prune ap-proach as described in (Pradhan et al, 2005), whichmeans that before classification, the nodes were fil-tered through a binary classifier that classifies themas having a semantic role or not (NON-NULL orNULL).
The NULL nodes missed by the filter wereincluded in the training set for the final classifier.Since our current implementation of the RVMtraining algorithm does not scale up to large trainingsets, training on the whole PropBank was infeasible.We instead trained the multiclass classifier on sec-tions 15 ?
18, and used an SVM for the soft-pruningclassifier, which was then trained on the remainingsections.
The excellent LIBSVM (Chang and Lin,2001) package was used to train the SVM.The features used by the classifiers can begrouped into predicate and node features.
Of thenode features, we here pay most attention to theparse tree path features.3.1 Predicate FeaturesWe used the following predicate features, all ofwhich first appeared in (Gildea and Jurafsky, 2002).178?
Predicate lemma.?
Subcategorization frame.?
Voice.3.2 Node Features?
Head word and head POS.
Like most previouswork, we used the head rules of Collins to ex-tract this feature.?
Position.
A binary feature that describes if thenode is before or after the predicate token.?
Phrase type (PT), that is the label of the con-stituent.?
Named entity.
Type of the first contained NE.?
Governing category.
As in (Gildea and Juraf-sky, 2002), this was used to distinguish subjectsfrom objects.
For an NP, this is either S or VP.?
Path features.
(See next subsection.
)For prepositional phrases, we attached the prepo-sition to the PT and replaced head word and headPOS with those of the first contained NP.3.3 Parse Tree Path FeaturesPrevious studies have shown that the parse tree pathfeature, used by almost all systems since (Gildea andJurafsky, 2002), is salient for argument identifica-tion.
However, it is extremely sparse (which makesthe system learn slowly) and is dependent on thequality of the parse tree.
We therefore investigatedthe contribution of the following features in orderto come up with a combination of path features thatleads to a robust system that generalizes well.?
Constituent tree path.
As in (Gildea and Ju-rafsky, 2002), this feature represents the path(consisting of step directions and PTs of thenodes traversed) from the node to the predicate,for example NP?VP?VB for a typical object.Removing the direction (as in (Pradhan et al,2005)) improved neither precision nor recall.?
Partial path.
To reduce sparsity, we introduceda partial path feature (as in (Pradhan et al,2005)), which consists of the path from thenode to the lowest common ancestor.?
Dependency tree path.
We believe that la-beled dependency paths provide more informa-tion about grammatical functions (and, implic-itly, semantic relationships) than the raw con-stituent structure.
Since the grammatical func-tions are not directly available from the parsetrees, we investigated two approximations ofdependency arc labels: first, the POSs of thehead tokens; secondly, the PTs of the head nodeand its immediate parent (such labels were usedin (Ahn et al, 2004)).?
Shallow path.
Since the UPC shallow parserswere expected to be more robust than the fullparsers, we used a shallow path feature.
Wefirst built a parse tree using clause and chunkbracketing, and the shallow path feature wasthen constructed like the constituent tree path.?
Subpaths.
All subpaths of the constituent path.We used the parse trees from Charniak?s parser toderive all paths except for the shallow path.4 Results4.1 Comparison with SVMThe binary classifiers that comprise the one-versus-one multiclass classifier were 89% ?
98% smallerwhen using RVM compared to SVM.
However, theperformance dropped by about 2 percent.
The rea-son for the drop is possibly that the classifier uses anumber of features with extremely sparse distribu-tions (two word features and three path features).4.2 Path Feature ContributionsTo estimate the contribution of each path feature, wemeasured the difference in performance between asystem that used all six features and one where oneof the features had been removed.
Table 2 showsthe results for each of the six features.
For the finalsystem, we used the dependency tree path with PTpairs, the shallow path, and the partial path.4.3 Final System ResultsThe results of the complete system on the test setsare shown in Table 1.
The smaller training set (asmentioned above, we used only sections 15 ?
18179Precision Recall F?=1Development 73.40% 70.85% 72.10Test WSJ 75.46% 73.18% 74.30Test Brown 65.17% 60.59% 62.79Test WSJ+Brown 74.13% 71.50% 72.79Test WSJ Precision Recall F?=1Overall 75.46% 73.18% 74.30A0 84.56% 85.18% 84.87A1 73.40% 73.35% 73.37A2 61.99% 57.30% 59.55A3 71.43% 46.24% 56.14A4 72.53% 64.71% 68.39A5 100.00% 40.00% 57.14AM-ADV 58.13% 51.58% 54.66AM-CAU 70.59% 49.32% 58.06AM-DIR 59.62% 36.47% 45.26AM-DIS 81.79% 71.56% 76.33AM-EXT 72.22% 40.62% 52.00AM-LOC 54.05% 55.10% 54.57AM-MNR 54.33% 52.91% 53.61AM-MOD 98.52% 96.73% 97.62AM-NEG 96.96% 96.96% 96.96AM-PNC 36.75% 37.39% 37.07AM-PRD 0.00% 0.00% 0.00AM-REC 0.00% 0.00% 0.00AM-TMP 76.00% 70.19% 72.98R-A0 83.33% 84.82% 84.07R-A1 68.75% 70.51% 69.62R-A2 57.14% 25.00% 34.78R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 100.00% 25.00% 40.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 92.31% 57.14% 70.59R-AM-MNR 40.00% 33.33% 36.36R-AM-TMP 75.00% 69.23% 72.00V 98.82% 98.82% 98.82Table 1: Overall results (top) and detailed results onthe WSJ test (bottom).for the role classifier) causes the result to be signifi-cantly lower than state of the art (F-measure of 79.4,reported in (Pradhan et al, 2005)).5 Conclusion and Future WorkWe have provided an application of Relevance Vec-tor Machines to a large-scale NLP task.
The re-sulting classifiers are drastically smaller that thoseproduced by the SV training methods.
On the otherhand, the classification accuracy is lower, probablybecause of the use of lexicalized features.The results on the Brown test set shows that thegenre has a significant impact on the performance.An evaluation of the contribution of six parse treeP R F?=1Const.
tree -0.2% -0.6% -0.4Partial -0.4% +0.4% 0Dep.
w/ POSs -0.1% -0.4% -0.3Dep.
w/ PT pairs +0.4% +0.4% +0.4Shallow -0.1% +0.4% +0.1Const.
subpaths -10.9% +2.5% -4.5Table 2: Contribution of path featurespath features suggests that dependency tree paths aremore useful for semantic role labeling than the tra-ditional constituent tree path.In the future, we will investigate if it is possibleto incorporate the ?
parameter into the probabilitymodel, thus eliminating the need for cross-validationcompletely.
In addition, the training algorithm willneed to be redesigned to scale up to larger trainingsets.
The learning paradigm is still young and op-timized methods (such as for SVM) have yet to ap-pear.
One possible direction is the greedy methoddescribed in (Tipping and Faul, 2003).ReferencesDavid Ahn, Sisay Fissaha, Valentin Jijkoun, and Maartende Rijke.
2004.
The university of Amsterdam atSenseval-3: Semantic roles and logic forms.
In Pro-ceedings of SENSEVAL-3.Xavier Carreras and Llu?s M?rquez.
2005.
Introductionto the CoNLL-2005 Shared Task: Semantic Role La-beling.
In Proceedings of CoNLL-2005.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: alibrary for support vector machines.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James Martin, and Dan Jurafsky.
2005.Support vector learning for semantic argument classi-fication.
Machine Learning.
To appear.Michael E. Tipping and Anita Faul.
2003.
Fast marginallikelihood maximisation for sparse bayesian models.In 9th International Workshop on AI and Statistics.Michael E. Tipping.
2001.
Sparse bayesian learningand the relevance vector machine.
Journal of MachineLearning Research, 1:211 ?
244.R.
Clint Whaley, Antoine Petitet, and Jack J. Dongarra.2000.
Automated empirical optimizations of softwareand the ATLAS project.180
