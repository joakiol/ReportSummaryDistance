Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 361?364,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPThe Impact of Query Refinement in the Web People Search TaskJavier ArtilesUNED NLP & IR groupMadrid, Spainjavart@bec.uned.esJulio GonzaloUNED NLP & IR groupMadrid, Spainjulio@lsi.uned.esEnrique Amig?oUNED NLP & IR groupMadrid, Spainenrique@lsi.uned.esAbstractSearching for a person name in a WebSearch Engine usually leads to a numberof web pages that refer to several peoplesharing the same name.
In this paper westudy whether it is reasonable to assumethat pages about the desired person can befiltered by the user by adding query terms.Our results indicate that, although in mostoccasions there is a query refinement thatgives all and only those pages related toan individual, it is unlikely that the user isable to find this expression a priori.1 IntroductionThe Web has now become an essential resourceto obtain information about individuals but, at thesame time, its growth has made web people search(WePS) a challenging task, because every singlename is usually shared by many different peo-ple.
One of the mainstream approaches to solvethis problem is designing meta-search engines thatcluster search results, producing one cluster perperson which contains all documents referring tothis person.Up to now, two evaluation campaigns ?
WePS 1in 2007 (Artiles et al, 2007) and WePS 2 in 2009(Artiles et al, 2009) ?
have produced datasets forthis clustering task, with over 15 research groupssubmitting results in each campaign.
Since the re-lease of the first datasets, this task is becoming anincreasingly popular research topic among Infor-mation Retrieval and Natural Language Process-ing researchers.For precision oriented queries (for instance,finding the homepage, the email or the phone num-ber of a given person), clustered results might helplocating the desired data faster while avoiding con-fusion with other people sharing the same name.But the utility of clustering is more obvious for re-call oriented queries, where the goal is to mine theweb for information about a person.
In a typicalhiring process, for instance, candidates are eval-uated not only according to their cv, but also ac-cording to their web profile, i.e.
information aboutthem available in the Web.One question that naturally arises is whethersearch results clustering can effectively help usersfor this task.
Eventually, a query refinement madeby the user ?
for instance, adding an affiliation ora location ?
might have the desired disambigua-tion effect without compromising recall.
The hy-pothesis underlying most research on Web PeopleSearch is that query refinement is risky, because itcan enhance precision but it will usually harm re-call.
Adding the current affiliation of a person, forinstance, might make information about previousjobs disappear from search results.This hypothesis has not, up to now, been em-pirically confirmed, and it is the goal of this pa-per.
We want to evaluate the actual impact of us-ing query refinements in the Web People Search(WePS) clustering task (as defined in the frame-work of the WePS evaluation).
For this, we havestudied to what extent a query refinement can suc-cessfully filter relevant results and which type ofrefinements are the most successful.
In our ex-periments we have considered the search resultsassociated to one individual as a set of relevantdocuments, and we have tested the ability of dif-ferent query refinement strategies to retrieve thosedocuments.
Our results are conclusive: in mostoccasions there is a ?near-perfect?
refinement thatfilters out most relevant information about a givenperson, but this refinement is very hard to predictfrom a user?s perspective.In Section 2 we describe the datasets that whereused for our experiments.
The experimentalmethodology and results are presented in Section3.
Finally we present our conclusions in 4.3612 Dataset2.1 The WePS-2 corpusFor our experiments we have used the WePS-2testbed (Artiles et al, 2009)1.
It consists of 30datasets, each one related to one ambiguous name:10 names were sampled from the US Census, 10from Wikipedia, and 10 from the Computer Sci-ence domain (Programme Committee members ofthe ACL 2008 Conference).
Each dataset consistsof, at most, 100 web pages written in English andretrieved as the top search results of a web searchengine, using the (quoted) person name as query2.Annotators were asked to organize the webpages from each dataset in groups where all docu-ments refer to the same person.
For instance, the?James Patterson?
web results were gruped in fourclusters according to the four individuals men-tioned with that name in the documents.
In caseswhere a web page refers to more than one personusing the same ambiguous name (e.g.
a web pagewith search results from Amazon), the documentis assigned to as many groups as necessary.
Doc-uments were discarded when there wasn?t enoughinformation to cluster them correctly.2.2 Query refinement candidatesIn order to generate query refinement candidates,we extracted several types of features from eachdocument.
First, we applied a simple preprocess-ing to the HTML documents in the corpus, con-verting them to plain text and tokenizing.
Then,we extracted tokens and word n-grams for eachdocument (up to four words lenght).
A list of En-glish stopwords was used to remove tokens and n-grams beginning or ending with a stopword.
Usingthe Stanford Named Entity Recognition Tool3weobtained the lists of persons, locations and organi-zations mentioned in each document.Additionally, we used attributes manually an-notated for the WePS-2 Attribute Extraction Task(Sekine and Artiles, 2009).
These are personattributes (affiliation, occupation, variations ofname, date of birth, etc.)
for each individual shar-ing the name searched.
These attributes emulatethe kind of query refinements that a user might tryin a typical people search scenario.1http://nlp.uned.es/weps2We used the Yahoo!
search service API.3http://nlp.stanford.edu/software/CRF-NER.shtmlfield F prec.
recall cover.ae affiliation 0.99 0.98 1.00 0.46ae award 1.00 1.00 1.00 0.04ae birthplace 1.00 1.00 1.00 0.09ae degree 0.85 0.80 1.00 0.10ae email 1.00 1.00 1.00 0.11ae fax 1.00 1.00 1.00 0.06ae location 0.99 0.99 1.00 0.27ae major 1.00 1.00 1.00 0.07ae mentor 1.00 1.00 1.00 0.03ae nationality 1.00 1.00 1.00 0.01ae occupation 0.95 0.93 1.00 0.48ae phone 0.99 0.99 1.00 0.13ae relatives 0.99 0.98 1.00 0.15ae school 0.99 0.99 1.00 0.15ae work 0.96 0.95 1.00 0.07stf location 0.96 0.95 1.00 0.93stf organization 1.00 1.00 1.00 0.98stf person 0.98 0.97 1.00 0.82tokens 1.00 1.00 1.00 1.00bigrams 1.00 1.00 1.00 0.98trigrams 1.00 1.00 1.00 1.00fourgrams 1.00 1.00 1.00 0.98fivegrams 1.00 1.00 1.00 0.98Table 1: Results for clusters of size 1field F prec.
recall cover.ae affiliation 0.76 0.99 0.65 0.40ae award 0.67 1.00 0.50 0.02ae birthplace 0.67 1.00 0.50 0.10ae degree 0.63 0.87 0.54 0.15ae email 0.74 1.00 0.60 0.16ae fax 0.67 1.00 0.50 0.09ae location 0.77 1.00 0.66 0.32ae major 0.71 1.00 0.56 0.09ae mentor 0.75 1.00 0.63 0.04ae nationality 0.67 1.00 0.50 0.01ae occupation 0.76 0.98 0.65 0.52ae phone 0.75 1.00 0.63 0.13ae relatives 0.78 0.96 0.68 0.15ae school 0.68 0.96 0.56 0.17ae work 0.81 1.00 0.72 0.17stf location 0.83 0.97 0.77 0.98stf organization 0.89 1.00 0.83 1.00stf person 0.83 0.99 0.74 0.98tokens 0.96 0.99 0.94 1.00bigrams 0.95 1.00 0.92 1.00trigrams 0.94 1.00 0.92 1.00fourgrams 0.91 1.00 0.86 0.99fivegrams 0.89 1.00 0.84 0.99Table 2: Results for clusters of size 2field F prec.
recall cover.ae affiliation 0.51 0.96 0.39 0.81ae award 0.26 1.00 0.16 0.20ae birthplace 0.33 0.99 0.24 0.28ae degree 0.37 0.90 0.26 0.36ae email 0.35 0.96 0.23 0.33ae fax 0.30 1.00 0.19 0.15ae location 0.34 0.96 0.23 0.64ae major 0.30 0.97 0.20 0.22ae mentor 0.23 0.95 0.15 0.22ae nationality 0.36 0.88 0.26 0.16ae occupation 0.52 0.93 0.40 0.80ae phone 0.34 0.96 0.23 0.33ae relatives 0.32 0.95 0.22 0.16ae school 0.40 0.95 0.29 0.43ae work 0.45 0.94 0.34 0.38stf location 0.62 0.87 0.53 1.00stf organization 0.67 0.96 0.56 1.00stf person 0.59 0.95 0.47 1.00tokens 0.87 0.90 0.86 1.00bigrams 0.79 0.95 0.70 1.00trigrams 0.75 0.96 0.65 1.00fourgrams 0.67 0.97 0.55 1.00fivegrams 0.62 0.96 0.50 1.00Table 3: Results for clusters of size >=33 ExperimentsIn our experiments we consider each set of doc-uments (cluster) related to one individual in theWePS corpus as a set of relevant documents fora person search.
For instance the James Patter-362field F prec.
recall cover.best-ae 1.00 0.99 1.00 0.74best-all 1.00 1.00 1.00 1.00best-ner 1.00 1.00 1.00 0.99best-nl 1.00 1.00 1.00 1.00Table 4: Results for clusters of size 1field F prec.
recall cover.best-ae 0.77 1.00 0.65 0.79best-all 0.95 1.00 0.93 1.00best-ner 0.92 0.99 0.88 1.00best-nl 0.96 1.00 0.94 1.00Table 5: Results for clusters of size 2field F prec.
recall cover.best-ae 0.60 0.97 0.47 0.92best-all 0.89 0.96 0.85 1.00best-ner 0.74 0.95 0.63 1.00best-nl 0.89 0.95 0.85 1.00Table 6: Results for clusters of size >=3son dataset in the WePS corpus contains a total of100 documents, and 10 of them belong to a Britishpolitician named James Patterson.
The WePS-2corpus contains a total of 552 clusters that wereused to evaluate the different types of QRs.For each person cluster, our goal is to find thebest query refinements; in an ideal case, an expres-sion that is present in all documents in the clus-ter, and not present in documents outside the clus-ter.
For each QR type (affiliation, e-mail, n-gramsof various sizes, etc.)
we consider all candidatesfound in at least one document from the cluster,and pick up the one that leads to the best harmonicmean (F?=.5) of precision and recall on the clusterdocuments (there might be more than one).For instance, when we evaluate a set of tokenQR candidates for the politician in the James Pat-terson dataset we find that among all the tokensthat appear in the documents of its cluster, ?repub-lican?
gives us a perfect score, while ?politician?obtains a low precision (we retrieve documents ofother politicians named James Patterson).In some cases a cluster might not have any can-didate for a particular type of QR.
For instance,manual person attributes like phone number aresparse and won?t be available for every individual,whereas tokens and ngrams are always present.We exclude those cases when computing F, andinstead we report a coverage measure which rep-resents the number of clusters which have at leastone candidate of this type of QR.
This way weknow how often we can use an attribute (coverage)field 1 2 >=3ae affiliation 20.96 17.88 29.41ae occupation 20.25 21.79 24.60ae work 3.23 8.38 8.56ae location 12.66 12.29 8.02ae school 7.03 6.70 6.42ae degree 3.23 3.91 5.35ae email 5.34 6.15 4.28ae phone 6.19 5.03 3.21ae nationality 0.28 0.00 3.21ae relatives 7.03 5.03 2.67ae birthplace 4.22 5.03 1.60ae fax 2.95 1.68 1.60ae major 3.52 3.91 1.07ae mentor 1.41 2.23 0.00ae award 1.69 0.00 0.00Table 7: Distribution of the person attributes usedfor the ?best-ae?
strategyand how useful it is when available (F measure).These figures represent a ceiling for each typeof query refinement: they represent the efficiencyof the query when the user selects the best possiblerefinement for a given QR type.We have split the results in three groups depend-ing on the size of the target cluster: (i) rare people,mentioned in only one document (335 clusters ofsize 1); (ii)people that appear in two documents(92 clusters of size 2), often these documents be-long to the same domain, or are very similar; and(iii) all other cases (125 clusters of size >=3).We also report on the aggregated results for cer-tain subsets of QR types.
For instance, if we wantto know what results will get a user that picks thebest person attribute, we consider all types of at-tributes (e-mail, affiliation, etc.)
for every cluster,and pick up the ones that lead to the best results.We consider four groups: (i) best-all selects thebest QR among all the available QR types (ii) best-ae considers all manually annotated attributes (iii)best-ner considers automatically annotated NEs;and (iv) best-ng uses only tokens and ngrams.3.1 ResultsThe results of the evaluation for each cluster size(one, two, more than two) are presented in Ta-bles 1, 2 and 3.
These tables display results foreach QR type.
Then Tables 4, 5 and 6 show theresults for aggregated QR types.Two main results can be highlighted: (i) Thebest overall refinement is, in average, very good(F = .89 for clusters of size ?
3).
In other words,there is usually at least one QR that leads to (ap-proximately) the desired set of results; (ii) this best363refinement, however, is not necessarily an intu-itive choice for the user.
One would expect usersto refine the query with a person?s attribute, suchas his affiliation or location.
But the results forthe best (manually extracted) attribute are signifi-cantly worse (F = .60 for clusters of size ?
3),and they cannot always be used (coverage is .74,.79 and .92 for clusters of size 1, 2 and ?
3).The manually tagged attributes from WePS-2are very precise, although their individual cover-age over the different person clusters is generallylow.
Affiliation and occupation, which are themost frequent, obtain the largest coverage (0.81and 0.80 for sizes ?
3).
Also the recall of thistype of QRs is low in clusters of two, three or moredocuments.
When evaluating the ?best-ae?
strat-egy we found that in many clusters there is at leastone manual attribute that can be used as QR withhigh precision.
This is the case mostly for clustersof three or more documents (0.92 coverage) and itdecreases with smaller clusters, probably becausethere is less information about the person and thusless biographical attributes are to be found.In Table 7 we show the distribution of the actualQR types selected by the ?best-ae?
strategy.
Thebest type is affiliation, which is selected in 29%of the cases.
Affiliation and occupation togethercover around half of the cases (54%), and the restis a long tail where each attribute makes a smallcontribution to the total.
Again, this is a strongindication that the best refinement is probably verydifficult to predict a priori for the user.Automatically recognized named entities in thedocuments obtain better results, in general, thanmanually tagged attributes.
This is probably dueto the fact that they can capture all kinds of relatedentities, or simply entities that happen to coocurwith the person name.
For instance, the pages of auniversity professor that is usually mentioned to-gether with his PhD students could be refined withany of their names.
This goes to show that a goodQR can be any information related to the person,and that we might need to know the person verywell in advance in order to choose this QR.Tokens and ngrams give us a kind of ?upperboundary?
of what is possible to achieve usingQRs.
They include almost anything that is foundin the manual attributes and the named entities.They also frequently include QRs that are not re-alistic for a human refinement.
For instance, inclusters of only two documents it is not uncom-mon that both pages belong to the same domainor that they are near duplicates.
In those cases to-kens and ngram QR will probably include non in-formative strings.
In some cases the QRs foundare neither directly biographical or related NEs,but topical information (e.g.
the term ?soccer?
inthe pages of a football player or the ngram ?align-ment via structured multilabel?
that is the title of apaper written by a Computer Science researcher).These cases widen even more the range of effec-tive QRs.
The overall results of using tokens andngrams are almost perfect for all clusters, but atthe cost of considering every possible bit of infor-mation about the person or even unrelated text.4 ConclusionsIn this paper we have studied the potential effectsof using query refinements to perform the WebPeople Search task.
We have shown that althoughin theory there are query refinements that performwell to retrieve the documents of most individuals,the nature of these ideal refinements varies widelyin the studied dataset, and there is no single in-tuitive strategy leading to robust results.
Even ifthe attributes of the person are well known before-hand (which is hardly realistic, given that in mostcases this is precisely the information needed bythe user), there is no way of anticipating whichexpression will lead to good results for a particu-lar person.
These results confirm that search re-sults clustering might indeed be of practical helpfor users in Web people search.ReferencesJavier Artiles, Julio Gonzalo, and Satoshi Sekine.2007.
The semeval-2007 weps evaluation: Estab-lishing a benchmark for the web people search task.In Proceedings of the Fourth International Work-shop on Semantic Evaluations (SemEval-2007).ACL.Javier Artiles, Julio Gonzalo, and Satoshi Sekine.2009.
Weps 2 evaluation campaign: overview ofthe web people search clustering task.
In WePS 2Evaluation Workshop.
WWW Conference 2009.Satoshi Sekine and Javier Artiles.
2009.
Weps2 at-tribute extraction task.
In 2nd Web People SearchEvaluation Workshop (WePS 2009), 18th WWWConference.364
