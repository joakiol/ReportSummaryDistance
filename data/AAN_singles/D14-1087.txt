Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 799?809,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsUnsupervised Template Mining for Semantic Category UnderstandingLei Shi1,2?, Shuming Shi3, Chin-Yew Lin3, Yi-Dong Shen1, Yong Rui31State Key Laboratory of Computer Science,Institute of Software, Chinese Academy of Sciences2University of Chinese Academy of Sciences3Microsoft Research{shilei,ydshen}@ios.ac.cn{shumings,cyl,yongrui}@microsoft.comAbstractWe propose an unsupervised approach toconstructing templates from a large collec-tion of semantic category names, and usethe templates as the semantic representa-tion of categories.
The main challenge isthat many terms have multiple meanings,resulting in a lot of wrong templates.
Sta-tistical data and semantic knowledge areextracted from a web corpus to improvetemplate generation.
A nonlinear scoringfunction is proposed and demonstrated tobe effective.
Experiments show that ourapproach achieves significantly better re-sults than baseline methods.
As an imme-diate application, we apply the extractedtemplates to the cleaning of a category col-lection and see promising results (preci-sion improved from 81% to 89%).1 IntroductionA semantic category is a collection of items shar-ing common semantic properties.
For example,all cities in Germany form a semantic categorynamed ?city in Germany?
or ?German city?.
InWikipedia, the category names of an entity aremanually edited and displayed at the end of thepage for the entity.
There have been quite a lot ofapproaches (Hearst, 1992; Pantel and Ravichan-dran, 2004; Van Durme and Pasca, 2008; Zhang etal., 2011) in the literature to automatically extract-ing category names and instances (also called is-aor hypernymy relations) from the web.Most existing work simply treats a categoryname as a text string containing one or multiplewords, without caring about its internal structure.In this paper, we explore the semantic structureof category names (or simply called ?categories?).
?This work was performed when the first author was vis-iting Microsoft Research Asia.For example, both ?CEO of General Motors?
and?CEO of Yahoo?
have structure ?CEO of [com-pany]?.
We call such a structure a category tem-plate.
Taking a large collection of open-domaincategories as input, we construct a list of categorytemplates and build a mapping from categories totemplates.
Figure 1 shows some example semanticcategories and their corresponding templates.Templates can be treated as additional featuresof semantic categories.
The new features can beexploited to improve some upper-layer applica-tions like web search and question answering.
Inaddition, by linking categories to templates, it ispossible (for a computer program) to infer the se-mantic meaning of the categories.
For example inFigure 1, from the two templates linking to cat-egory ?symptom of insulin deficiency?, it is rea-sonable to interpret the category as: ?a symptomof a medical condition called insulin deficiencywhich is about the deficiency of one type of hor-mone called insulin.?
In this way, our knowledgeabout a category can go beyond a simple stringand its member entities.
An immediate applicationof templates is removing invalid category namesfrom a noisy category collection.
Promising re-sults are observed for this application in our ex-periments.An intuitive approach to this task (i.e., extract-ing templates from a collection of category names)national holiday of South Africa(instances: Heritage Day, Christmas?
)national holiday of Brazil(instances: Carnival, Christmas?)
national holiday of [country]symptom of cortisol deficiency(instances: low blood sugar?
)symptom of insulin deficiency(instances: nocturia, weight loss?)
symptom of [hormone] deficiencysymptom of [medical condition]school in Denverschool in Houston school in [place]school in [city]Semantic Categories Category templatesfootball playerbasketball player [sport] playerFigure 1: Examples of semantic categories andtheir corresponding templates.799contains two stages: category labeling, and tem-plate scoring.Category labeling: Divide a category nameinto multiple segments and replace some key seg-ments with its hypernyms.
As an example, as-sume ?CEO of Delphinus?
is divided to three seg-ments ?CEO + of + Delphinus?
; and the last seg-ment (Delphinus) has hypernyms ?constellation?,?company?, etc.
By replacing this segment withits hypernyms, we get candidate templates ?CEOof [constellation]?
(a wrong template), ?CEO of[company]?, and the like.Template scoring: Compute the score of eachcandidate template by aggregating the informationobtained in the first phase.A major challenge here is that many segments(like ?Delphinus?
in the above example) have mul-tiple meanings.
As a result, wrong hypernymsmay be adopted to generate incorrect candidatetemplates (like ?CEO of [constellation]?).
In thispaper, we focus on improving the template scor-ing stage, with the goal of assigning lower scoresto bad templates and larger scores to high-qualityones.There have been some research efforts (Third,2012; Fernandez-Breis et al., 2010; Quesada-Mart?nez et al., 2012) on exploring the structure ofcategory names by building patterns.
However, weautomatically assign semantic types to the patternvariables (or called arguments) while they do not.For example, our template has the form of ?cityin [country]?
while their patterns are like ?city in[X]?.
More details are given in the related worksection.A similar task is query understanding, includingquery tagging and query template mining.
Querytagging (Li et al., 2009; Reisinger and Pasca,2011) corresponds to the category labeling stagedescribed above.
It is different from template gen-eration because the results are for one query only,without merging the information of all queries togenerate the final templates.
Category templateconstruction are slightly different from query tem-plate construction.
First, some useful featuressuch as query click-through is not available in cat-egory template construction.
Second, categoriesshould be valid natural language phrases, whilequeries need not.
For example, ?city Germany?
isa query but not a valid category name.
We discussin more details in the related work section.Our major contributions are as follows.1) To the best of our knowledge, this is the firstwork of template generation specifically for cate-gories in unsupervised manner.2) We extract semantic knowledge and statisti-cal information from a web corpus for improvingtemplate generation.
Significant performance im-provement is obtained in our experiments.3) We study the characteristics of the scoringfunction from the viewpoint of probabilistic evi-dence combination and demonstrate that nonlinearfunctions are more effective in this task.4) We employ the output templates to cleanour category collection mined from the web, andget apparent quality improvement (precision im-proved from 81% to 89%).After discussing related work in Section 2, wedefine the problem and describe one baseline ap-proach in Section 3.
Then we introduce our ap-proach in Section 4.
Experimental results are re-ported and analyzed in Section 5.
We conclude thepaper in Section 6.2 Related workSeveral kinds of work are related to ours.Hypernymy relation extraction: Hypernymyrelation extraction is an important task in text min-ing.
There have been a lot of efforts (Hearst, 1992;Pantel and Ravichandran, 2004; Van Durme andPasca, 2008; Zhang et al., 2011) in the literature toextract hypernymy (or is-a) relations from the web.Our target here is not hypernymy extraction, butdiscovering the semantic structure of hypernyms(or category names).Category name exploration: Category namepatterns are explored and built in some ex-isting research work.
Third (2012) pro-posed to find axiom patterns among categorynames on an existing ontology.
For ex-ample, infer axiom pattern ?SubClassOf(AB,B)?
from ?SubClassOf(junior school school)?and ?SubClassOf(domestic mammal mammal)?.Fernandez-Breis et al.
(2010) and Quesada-Mart?nez et al.
(2012) proposed to find lexical pat-terns in category names to define axioms (in med-ical domain).
One example pattern mentioned intheir papers is ?
[X] binding?.
They need man-ual intervention to determine what X means.
Themain difference between the above work and oursis that we automatically assign semantic types tothe pattern variables (or called arguments) whilethey do not.800Template mining for IE: Some research workin information extraction (IE) involves patterns.Yangarber (2003) and Stevenson and Greenwood(2005) proposed to learn patterns which were inthe form of [subject, verb, object].
The categorynames and learned templates in our work are notin this form.
Another difference between our workand their work is that, their methods need a super-vised name classifer to generate the candidate pat-terns while our approach is unsupervised.
Cham-bers and Jurafsky (2011) leverage templates to de-scribe an event while the templates in our work arefor understanding category names (a kind of shorttext).Query tagging/labeling: Some research workin recent years focuses on segmenting web searchqueries and assigning semantic tags to key seg-ments.
Li et al.
(2009) and Li (2010) employedCRF (Conditional Random Field) or semi-CRFmodels for query tagging.
A crowdsourcing-assisted method was proposed by Han et al.
(2013)for query structure interpretation.
These super-vised or semi-supervised approaches require muchmanual annotation effort.
Unsupervised meth-ods were proposed by Sarkas et al.
(2010) andReisinger and Pasca (2011).
As been discussedin the introduction section, query tagging is onlyone of the two stages of template generation.
Thetagging results are for one query only, without ag-gregating the global information of all queries togenerate the final templates.Query template construction: Some existingwork leveraged query templates or patterns forquery understanding.
A semi-supervised randomwalk based method was proposed by Agarwal etal.
(2010) to generate a ranked templates list whichare relevant to a domain of interest.
A predefineddomain schema and seed information is needed forthis method.
Pandey and Punera (2012) proposedan unsupervised method based on graphical mod-els to mine query templates.
The above methodsare either domain-specific (i.e., generating tem-plates for a specific domain), or have some degreeof supervision (supervised or semi-supervised).Cheung and Li (2012) proposed an unsupervisedmethod to generate query templates by the aid ofknowledge bases.
An approach was proposed in(Szpektor et al., 2011) to improve query recom-mendation via query templates.
Query session in-formation (which is not available in our task) isneeded in this approach for templates generation.Li et al.
(2013) proposed an clustering algorithmto group existing query templates by search intentsof users.Compared to the open-domain unsupervisedmethods for query template construction, our ap-proach improves on two aspects.
First, we proposeto incorporate multiple types of semantic knowl-edge (e.g., term peer similarity and term clusters)to improve template generation.
Second, we pro-pose a nonlinear template scoring function whichis demonstrated to be more effective.3 Problem Definition and Analysis3.1 Problem definitionThe goal of this paper is to construct a list of cat-egory templates from a collection of open-domaincategory names.Input: The input is a collection of categorynames, which can either be manually compiled(like Wikipedia categories) or be automatically ex-tracted.
The categories used in our experimentswere automatically mined from the web, by fol-lowing existing work (Hearst, 1992, Pantel andRavichandran 2004; Snow et al., 2005; Talukdaret al., 2008; Zhang et al., 2011).
Specifically,we applied Hearst patterns (e.g., ?NP [,] (suchas | including) {NP, }?
{and|or} NP?)
and is-a patterns (?NP (is|are|was|were|being) (a|an|the)NP?)
to a large corpus containing 3 billion En-glish web pages.
As a result, we obtained aterm?hypernym bi-partite graph containing 40million terms, 74 million hypernyms (i.e., cate-gory names), and 321 million edges (e.g., oneexample edge is ?Berlin??
?city in Germany?,where ?Berlin?
is a term and ?city in Germany?
isthe corresponding hypernym).
Then all the multi-word hypernyms are used as the input categorycollection.Output: The output is a list of templates, eachhaving a score indicating how likely it is valid.
Atemplate is a multi-word string with one headwordand at least one argument.
For example, in tem-plate ?national holiday of [country]?, ?holiday?
isthe headword, and ?[country]?
is the argument.We only consider one-argument templates in thispaper, and the case of multiple arguments is left asfuture work.
A template is valid if it is syntacti-cally and semantically correct.
?CEO of [constel-lation]?
(wrongly generated from ?CEO of Del-phinus?, ?CEO of Aquila?, etc.)
is not valid be-cause it is semantically unreasonable.8013.2 Baseline approachAn intuitive approach to this task contains twostages: category labeling and template scoring.Figure 2 shows its workflow with simple exam-ples.3.2.1 Phase-1: Category labelingAt this stage, each category name is automaticallysegmented and labeled; and some candidate tem-plate tuples (CTTs) are derived based on the la-beling results.
This can be done in the followingsteps.Category segmentation: Divide each cate-gory name into multiple segments (e.g., ?holi-day of South Africa?
to ?holiday + of + SouthAfrica?).
Each segment is one word or a phraseappearing in an entity dictionary.
The dictionaryused in this paper is comprised of all Freebase(www.freebase.com) entities.Segment to hypernym: Find hypernyms forevery segment (except for the headword and sometrivial segments like prepositions and articles), byreferring to a term?hypernym mapping graph.Following most existing query labeling work, wederive the term?hypernym graph from a dump ofFreebase.
Below are some examples of Freebasetypes (hypernyms),German city (id: /location/de city)Italian province (id: /location/it province)Poem character (id: /book/poem character)Book (id: /book/book)To avoid generating too fine-grained templateslike ?mayor of [Germany city]?
and ?mayor of[Italian city]?
(semantically ?mayor of [city]?is more desirable), we discard type modifiersand map terms to the headwords of Freebasetypes.
For example, ?Berlin?
is mapped to?city?.
In this way, we build our basic version ofterm?hypernym mapping which contains 16.13million terms and 696 hypernyms.
Since ?SouthAfrica?
is both a country and a book name in Free-base, hypernyms ?country?, ?book?, and othersare assigned to the segment ?South Africa?
in thisstep.CTT generation: Construct CTTs by choosingone segment (called the target segment) each timeand replacing the segment with its hypernyms.
AnCTT is formed by the candidate template (withone argument), the target segment (as an argumentvalue), and the tuple score (indicating tuple qual-ity).
Below are example CTTs obtained after thelast segment of ?holiday + of + South Africa?
isprocessed,U1: (holiday of [country], South Africa, w1)U2: (holiday of [book], South Africa, w2)3.2.2 Phase-2: Template scoringThe main objective of this stage is to merge allthe CTTs obtained from the previous stage and tocompute a final score for each template.
In thisstage, the CTTs are first grouped by the first ele-ment (i.e., the template string).
For example, tu-ples for ?holiday of [country]?
may include,U1: (holiday of [country], South Africa, w1)U2: (holiday of [country], Brazil, w2)U3: (holiday of [country], Germany, w3)...Then a scoring function is employed to calcu-late the template score from the tuple scores.
For-mally, given n tuples~U=(U1, U2..., Un) for a tem-plate, the goal is to find a score fusion functionF (~U) which yields large values for high-qualitytemplates and small (or zero) values for invalidones.Borrowing the idea of TF-IDF from informationretrieval, a reasonable scoring function is,F (~U) =n?i=1wi?
IDF (h) (1)where h is the argument type (i.e., the hypernymof the argument value) of each tuple.
TF meansthe ?term frequency?
and IDF means the ?inversedocument frequency?.
An IDF function assignslower scores to common hypernyms (like personand music track which contain a lot of entities).Let DF (h) be the number of entities having hy-pernym h, we test two IDF functions in our exper-iments,IDF1(h) = log1 +N1 +DF (h)IDF2(h) = 1/sqrt(DF (h))(2)where N is total number of entities in the entitydictionary.The next problem is estimating tuple score wi.Please note that there is no weight or score infor-mation in the term?hypernym mapping of Free-base.
So we have to set wito be constant in thebaseline,wi= 1 (3)802Wikipediaholiday of Brazilholiday of South Africa?Brazil ?
countryBrazil ?
bookSouth Africa ?
countrySouth Africa ?
book?holiday of [country], Brazil, w 1holiday of [book], Brazil, w 2holiday of [country], South Africa, w 3holiday of [book], South Africa, w 4?holiday of [country], S 1holiday of [book], S 2?Phase-1: Category labelingPhase-2: Template scoringPhase-1 Phase-2Input: Category namesTerm-hypernym mappingOutput: Category templateshead argumentargumentargument valuetuple scoreCandidate template tuples (CTTs)Figure 2: Problem definition and baseline approach.4 Approach: Enhancing TemplateScoringIn our approach, we follow the same frameworkas in the above baseline approach, and focus onimproving the template scoring phase (i.e., phase-2).We try three techniques: First, a better tuplescorewiis calculated in Section 4.1 by performingstatistics on a large corpus.
The corpus is a collec-tion of 3 billion web pages crawled in early 2013by ourselves.
During this paper, we use ?our webcorpus?
or ?our corpus?
to refer to this corpus.Second, a nonlinear function is adopted in Sec-tion 4.2 to replace the baseline tuple fusion func-tion (Formula 1).
Third, we extract term peer sim-ilarity and term clusters from our corpus and usethem as additional semantic knowledge to refinetemplate scores.4.1 Enhancing tuple scoringLet?s examine the following two template tuples,U1: (holiday of [country], South Africa, w1)U2: (holiday of [book], South Africa, w2)Intuitively, ?South Africa?
is more likely to bea country than a book when it appears in text.
Sofor a reasonable tuple scoring formula, we shouldhave w1> w2.The main idea is to automatically calculatethe popularity of a hypernym given a term, byreferring to a large corpus.
Then by addingthe popularity information to (the edges of) theterm?hypernym graph of Freebase, we obtain aweighted term?hypernym graph.
The weightedgraph is then employed to enhance the estimationof wi.For popularity calculation, we apply Hearstpatterns (Hearst, 1992) and is-a patterns (?NP(is|are|was|were|being) (a|an|the) NP?)
to everysentence of our web corpus.
For a (term, hyper-nym) pair, its popularity F is calculated as thenumber of sentences in which the term and the hy-pernym co-occur and also follow at least one ofthe patterns.For a template tuple Uiwith argument type hand argument value v, we test two ways of esti-mating the tuple score wi,wi= log (1 + F (v, h)) (4)wi=F (v, h))?+?hj?HF (v, hj)(5)where F (v, h) is the popularity of the (v, h) pairin our corpus, H is the set of all hypernyms for vin the weighted term?hypernym graph.
Parame-ter ?
(=1.0 in our experiments) is introduced forsmoothing purpose.
Note that the second formulais the conditional probability of hypernym h giventerm v.Since it is intuitive to estimate tuple scores withtheir frequencies in a corpus, we treat the approachwith the improved wias another baseline (ourstrong baseline).4.2 Enhancing tuple combination functionNow we study the possibility of improving the tu-ple combination function (Formula 1), by examin-ing the tuple fusion problem from the viewpointof probabilistic evidence combination.
We firstdemonstrate that the linear function in Formula 1corresponds to the conditional independence as-sumption of the tuples.
Then we propose to adopta series of nonlinear functions for combining tuplescores.We define the following events:T : Template T is a valid template;T : T is an invalid template;Ei: The observation of tuple Ui.803Let?s compute the posterior odds of event T ,given two tuples U1and U2.
Assuming E1andE2are conditionally independent given T or T ,according to the Bayes rule, we have,P (T |E1, E2)P (T |E1, E2)=P (E1, E2|T ) ?
P (T )P (E1, E2|T ) ?
P (T )=P (E1|T )P (E1|T )?P (E2|T )P (E2|T )?P (T )P (T )=P (T |E1) ?
P (T )P (T |E1) ?
P (T )?P (T |E2) ?
P (T )P (T |E2) ?
P (T )?P (T )P (T )(6)Define the log-odds-gain of T given E as,G(T |E) = logP (T |E)P (T |E)?
logP (T )P (T )(7)Here G means the gain of the log-odds of T af-ter E occurs.
By combining formulas 6 and 7, wegetG(T |E1, E2) = G(T |E1) +G(T |E2) (8)It is easy to prove that the above conclusionholds true when n > 2, i.e.,G(T |E1, ..., En) =n?i=1G(T |Ei) (9)If we treat G(T |Ei) as the score of template Twhen only Uiis observed, andG(T |E1, ..., En) asthe template score after the n tuples are observed,then the above equation means that the combinedtemplate score should be the sum of wi?
IDF (h),which is exactly Formula 1.
Please keep in mindthat Equation 9 is based on the assumption that thetuples are conditional independent.
This assump-tion, however, may not hold in reality.
The caseof conditional dependence was studied in (Zhanget al., 2011), where a group of nonlinear combina-tion functions were proposed and achieved goodperformance in their task of hypernymy extrac-tion.
We choose p-Norm as our nonlinear fusionfunctions, as below,F (~U) =p????n?i=1wpi?
IDF (h) (p > 1) (10)where p (=2 in experiments) is a parameter.Experiments show that the above nonlinearfunction performs better than the linear functionof Formula 1.
Let?s use an example to show theintuition.
Consider a good template ?city of [coun-try]?
corresponding to CTTs~UAand a wrong tem-plate ?city of [book]?
having tuples~UB.
Sup-pose |~UA| = 200 (including most countries inthe world) and |~UB| = 1000 (considering thatmany place names have already been used as booknames).
We observe that each tuple score corre-sponding to ?city of [country]?
is larger than thetuple score corresponding to ?city of [book]?.
Forsimplicity, we assume each tuple in~UAhas score1.0 and each tuple in~UBhas score 0.2.
With thelinear and nonlinear (p=2) fusion functions, wecan get,Linear:F (~UA) = 200 ?
1.0 = 200F (~UB) = 1000 ?
0.2 = 200(11)Nonlinear:F (~UA) = 14.1F (~UB) = 6.32(12)In the above settings the nonlinear functionyields a much higher score for the good template(than for the invalid template), while the linear onedoes not.4.3 Refinement with term similarity andterm clustersThe above techniques neglect the similarity amongterms, which has a high potential to improve thetemplate scoring process.
Intuitively, for a toy set{?city in Brazil?, ?city in South Africa?,?city inChina?, ?city in Japan?
}, since ?Brazil?, ?SouthAfrica?, ?China?
and ?Japan?
are very similar toeach other and they all have a large probability tobe a ?country?, so we have more confidence that?city in [country]?
is a good template.
In this sec-tion, we propose to leverage the term similarityinformation to improve the template scoring pro-cess.We start with building a large group of smalland overlapped clusters from our web corpus.4.3.1 Building term clustersTerm clusters are built in three steps.Mining term peer similarity: Two terms arepeers if they share a common hypernym and theyare semantically correlated.
For example, ?dog?and ?cat?
should have a high peer similarity score.Following existing work (Hearst, 1992; Kozareva804et al., 2008; Shi et al., 2010; Agirre et al., 2009;Pantel et al., 2009), we built a peer similarity graphcontaining about 40.5 million nodes and 1.33 bil-lion edges.Clustering: For each term, choose its top-30neighbors from the peer similarity graph and run ahierarchical clustering algorithm, resulting in oneor multiple clusters.
Then we merge highly du-plicated clusters.
The algorithm is similar to thefirst part of CBC (Pantel and Lin, 2002), with thedifference that a very high merging threshold isadopted here in order to generate small and over-lapped clusters.
Please note that one term may beincluded in many clusters.Assigning top hypernyms: Up to two hyper-nyms are assigned for each term cluster by major-ity voting of its member terms, with the aid of theweighted term?hypernym graph of Section 4.1.To be an eligible hypernym for the cluster, it hasto be the hypernym of at least 70% of terms in thecluster.
The score of each hypernym is the aver-age of the term?hypernym weights over all themember terms.4.3.2 Template score refinementWith term clusters at hand, now we describe thescore refinement procedure for a template T hav-ing argument type h and supporting tuples~U=(U1,U2..., Un).
Denote V = {V1, V2, ..., Vn} to be theset of argument values for the tuples (where Viisthe argument value of Ui).By computing the intersection of V and everyterm cluster, we can get a distribution of the argu-ment values in the clusters.
We find that for a goodtemplate like ?holiday in [country]?, we can oftenfind at least one cluster (one of the country clus-ters in this example) which has hypernym h andalso contains many elements in V .
However, forinvalid templates like ?holiday of [book]?, everycluster having hypernym h (=?book?
here) onlycontains a few elements in V .
Inspired by suchan observation, our score refinement algorithm fortemplate T is as follows,Step-1.
Calculating supporting scores: Foreach term cluster C having hypernym h, computeits supporting score to T as follows:S(C, T ) = k(C, V ) ?
w(C, h) (13)where k(C, V ) is the number of elements sharedby C and V , and w(C, h) is hypernym score of hto C (computed in the last step of building clus-ters).Step-2.
Calculating the final template score:Let term cluster C?has the maximal supportingscore to T , the final template score is computedas,S(T ) = F (~U) ?
S(C?, T ) (14)where F (~U) is the template score before refine-ment.5 Experiments5.1 Experimental setup5.1.1 Methods for comparisonWe make a comparison among 10 methods.SC: The method is proposed in (Cheung and Li,2012) to construct templates from queries.
Themethod firstly represents a query as a matrix basedon Freebase data.
Then a hierarchical clusteringalgorithm is employed to group queries having thesame structure and meaning.
Then an intent sum-marization algorithm is employed to create tem-plates for each query group.Base: The linear function in Formula 1 isadopted to combine the tuple scores.
We useIDF2here because it achieves higher precisionthan IDF1in this setting.LW: The linear function in Formula 1 isadopted to combine the tuple scores generated byFormula 4.
IDF1is used rather than IDF2forbetter performance.LP: The linear function in Formula 1 is adoptedto combine the tuple scores generated by Formula5.
IDF2is used rather than IDF1for better per-formance.NLW: The nonlinear fusion function in For-mula 10 is used.
Other settings are the same asLW.NLP: The nonlinear fusion function in Formula10 is used.
Other settings are the same as LP.LW+C, LP+C, NLW+C, NLP+C: All the set-tings of LW, LP, NLW, NLP respectively, with therefinement technology in Section 4.3 applied.5.1.2 Data sets, annotation and evaluationmetricsThe input category names for experiments are au-tomatically extracted from a web corpus (Section3.1).
Two test-sets are built for evaluation from theoutput templates of various methods.Subsets: In order to conveniently compare theperformance of different methods, we create 20sub-collections (called subsets) from the whole in-put category collection.
Each subset contains all805the categories having the same headword (e.g.,?symptom of insulin deficiency?
and ?depressionsymptom?
are in the same subset because theyshare the same headword ?symptom?).
To choosethe 20 headwords, we first sample 100 at ran-dom from the set of all headwords; then manu-ally choose 20 for diversity.
The headwords in-clude symptom, school, food, gem, hero, weapon,model, etc.
We run the 10 methods on these sub-sets and sort the output templates by their scores.Top-30 templates from each method on each sub-set are selected and mixed together for annotation.Fullset: We run method NLP+C (which hasthe best performance according to our subsetsexperiments) on the input categories and sortthe output templates by their scores.
Then wesplit the templates into 9 sections accordingto their ranking position.
The sections are:[1?100], (100?1K], (1K?10K], (10K?100K],(100K,120K], (120K?140K], (140K?160K],(160K?180K], (180K?200K].
Then 40 templatesare randomly chosen from each section and mixedtogether for annotation.The selected templates (from subsets and thefullset) are annotated by six annotators, with eachtemplate assigned to two annotators.
A template isassigned a label of ?good?, ?fair?, or ?bad?
by anannotator.
The percentage agreement between theannotators is 80.2%, with kappa 0.624.For the subset experiments, we adoptPrecision@k (k=10,20,30) to evaluate thetop templates generated by each method.
Thescores for ?good?, ?fair?, and ?bad?
are 1, 0.5,and 0.
The score of each template is the averageannotation score over two annotators (e.g., if atemplate is annotated ?good?
by one annotator and?fair?
by another, its score is (1.0+0.5)/2=0.75).The evaluation score of a method is the averageover the 20 subsets.
For the fullset experiments,we report the precision for each section.5.2 Experimental results5.2.1 Results for subsetsThe results of each method on the 20 subsetsare presented in Table 1.
A few observationscan be made.
First, by comparing the per-formance of baseline-1 (Base) and the methodsadopting term?hypernym weight (LW and LP),we can see big performance improvement.
Thebad performance of baseline-1 is mainly due tothe lack of weight (or frequency) information onMethod P@10 P@20 P@30Base (baseline-1) 0.359 0.361 0.358SC (Cheung and Li, 2012) 0.382 0.366 0.371Weighted LW 0.633 0.582 0.559(baseline-2) LP 0.771 0.734 0.707Nonlinear NLW 0.711 0.671 0.638NLP 0.818 0.791 0.765LW+C 0.813 0.786 0.754Term cluster NLW+C 0.854 0.833 0.808LP+C 0.818 0.788 0.778NLP+C 0.868 0.839 0.788Table 1: Performance comparison among themethods on subset.term?hypernym edges.
The results demonstratethat edge scores are critical for generating highquality templates.
Manually built semantic re-sources typically lack such kinds of scores.
There-fore, it is very important to enhance them by de-riving statistical data from a large corpus.
Sinceit is relatively easy to have the idea of adopt-ing a weighted term?hypernym graph, we treatLW and LP as another (stronger) baseline namedbaseline-2.As the second observation, the results show thatthe nonlinear methods (NLP and NLW) achieveperformance improvement over their linear ver-sions (LW and LP).Third, let?s examine the methods with templatescores refined by term similarity and term clus-ters (LW+C, NLW+C, LP+C, NLP+C).
It is shownthat the refine-by-cluster technology brings addi-tional performance gains on all the four settings(linear and nonlinear, two different ways of calcu-lating tuple scores).
So we can conclude that thepeer similarity and term clusters are quite effectivein improving template generation.Fourth, the best performance is achievedwhen the three techniques (i.e., term?hypernymweight, nonlinear fusion function, and refine-by-cluster) are combined together.
For instance, bycomparing the P@20 scores of baseline-2 andNLP+C, we see a performance improvement of14.3% (from 0.734 to 0.839).
Therefore everytechnique studied in this paper has its own meritin template generation.Finally, by comparing the method SC (Cheungand Li, 2012) with other methods, we can see thatSC is slightly better than baseline-1, but has muchlower performance than others.
The major reasonmay be that this method did not employ a weightedterm?hypernym graph or term peer similarity in-formation in template construction.806Base SC LP NLP LP+CSC ?LP > ??
> ?
?P@10 NLP > ??
> ??
>LP+C > ??
> ??
> ??
?NLP+C > ??
> ??
> ??
> ??
>Base SC LP NLP LP+CSC ?LP > ??
> ?
?P@20 NLP > ??
> ??
> ?
?LP+C > ??
> ??
> ??
?NLP+C > ??
> ??
> ??
> ??
> ?
?Base SC LP NLP LP+CSC ?LP > ??
> ?
?P@30 NLP > ??
> ??
> ?
?LP+C > ??
> ??
> ??
?NLP+C > ??
> ??
> ??
> ?Table 2: Paired t-test results on subsets.Base SC LW NLW LW+CSC ?LW > ??
> ?
?P@10 NLW > ??
> ??
> ?LW+C > ??
> ??
> ??
> ?
?NLW+C > ??
> ??
> ??
> ??
> ?Base SC LW NLW LW+CSC ?LW > ??
> ?
?P@20 NLW > ??
> ??
> ?
?LW+C > ??
> ??
> ??
> ?
?NLW+C > ??
> ??
> ??
> ??
> ?
?Base SC LW NLW LW+CSC ?LW > ??
> ?
?P@30 NLW > ??
> ??
> ?
?LW+C > ??
> ??
> ??
> ?
?NLW+C > ??
> ??
> ??
> ??
> ?
?Table 3: Paired t-test results on subsets.Are the performance differences between meth-ods significant enough for us to say that one is bet-ter than the other?
To answer this question, we runpaired two-tailed t-test on every pair of methods.We report the t-test values among methods in ta-bles 2, 3 and 4.The meaning of the symbols in the tables are,?
: The method on the row and the one on thecolumn have similar performance.>: The method on the row outperforms themethod on the column, but the performance dif-ference is not statistically significant (0.05 ?
P <0.1 in two-tailed t-test).> ?
: The performance difference is statisticallysignificant (P < 0.05 in two-tailed t-test).> ??
: The performance difference is statisti-cally highly significant (P < 0.01 in two-tailedt-test).P@10 P@20 P@30LP V.S.
LW > ??
> ??
> ?
?NLP V.S.
NLW > ??
> ??
> ?
?LP+C V.S.
LW+C ?
?
?NLP+C V.S.
NLW+C ?
?
?Table 4: Paired t-test results on subsets.00.10.20.30.40.50.60.70.80.911 2 3 4 5 6 7 8 9PrecisionSection IDFigure 3: Precision by section in the fullset.5.2.2 Fullset resultsAs described in the Section 5.1.2, for the fullsetexperiments, we conduct a section-wise evalua-tion, selecting 40 templates from each of the 9 sec-tions of the NLP+C results.
The results are shownin Figure 3.
It can be observed that the precisionfor each section decreases when the section ID in-creases.
The results indicate the effectiveness ofour approach, since it can rank good templates intop sections and bad templates in bottom sections.According to the section-wise precision data, weare able to determine the template score thresholdfor choosing different numbers of top templates indifferent applications.5.2.3 Templates for category collectioncleaningSince our input category collection is automati-cally constructed from the web, some wrong orinvalid category names is inevitably contained.
Inthis subsection, we apply our category templatesto clean the category collection.
The basic idea isthat if a category can match a template, it is morelikely to be correct.
We compute a new score forevery category name H as follows,Snew(H) = log(1 + S(H)) ?
S(T?)
(15)where S(H) is the existing category score, deter-mined by its frequency in the corpus.
Here S(T?
)is the score of template T?, the best template (i.e.,the template with the highest score) for the cate-gory.Then we re-rank the categories according totheir new scores to get a re-ranked category list.We randomly sampled 150 category names fromthe top 2 million categories of each list (the old listand the new list) and asked annotators to judge the807quality of the categories.
The annotation resultsshow that, after re-ranking, the precision increasesfrom 0.81 to 0.89 (i.e., the percent of invalid cate-gory names decreases from 19% to 11%).6 ConclusionIn this paper, we studied the problem of build-ing templates for a large collection of categorynames.
We tested three techniques (tuple scor-ing by weighted term?hypernym mapping, non-linear score fusion, refinement by term clusters)and found that all of them are very effective andtheir combination achieves the best performance.By employing the output templates to clean ourcategory collection mined from the web, we getapparent quality improvement.
Future work in-cludes supporting multi-argument templates, dis-ambiguating headwords of category names and ap-plying our approach to general short text templatemining.AcknowledgmentsWe would like to thank the annotators for their ef-forts in annotating the templates.
Thanks to theanonymous reviewers for their helpful commentsand suggestions.
This work is supported in part byChina National 973 program 2014CB340301 andNSFC grant 61379043.ReferencesGanesh Agarwal, Govind Kabra, and Kevin Chen-Chuan Chang.
2010.
Towards rich query interpreta-tion: walking back and forth for mining query tem-plates.
In Proceedings of the 19th international con-ference on World wide web, pages 1?10.
ACM.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 19?27.
Association for Computational Lin-guistics.Nathanael Chambers and Dan Jurafsky.
2011.Template-based information extraction without thetemplates.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies-Volume 1,pages 976?986.
Association for Computational Lin-guistics.Jackie Chi Kit Cheung and Xiao Li.
2012.
Sequenceclustering and labeling for unsupervised query intentdiscovery.
In Proceedings of the fifth ACM interna-tional conference on Web search and data mining,pages 383?392.
ACM.Jesualdo Tomas Fernandez-Breis, Luigi Iannone, Ig-nazio Palmisano, Alan L Rector, and RobertStevens.
2010.
Enriching the gene ontology via thedissection of labels using the ontology pre-processorlanguage.
In Knowledge Engineering and Manage-ment by the Masses, pages 59?73.
Springer.Jun Han, Ju Fan, and Lizhu Zhou.
2013.Crowdsourcing-assisted query structure interpreta-tion.
In Proceedings of the Twenty-Third inter-national joint conference on Artificial Intelligence,pages 2092?2098.
AAAI Press.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th conference on Computational linguistics -Volume 2, COLING ?92, pages 539?545, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy.2008.
Semantic class learning from the web withhyponym pattern linkage graphs.
In ACL, volume 8,pages 1048?1056.Xiao Li, Ye-Yi Wang, and Alex Acero.
2009.
Extract-ing structured information from user queries withsemi-supervised conditional random fields.
In Pro-ceedings of the 32nd international ACM SIGIR con-ference on Research and development in informationretrieval, pages 572?579.
ACM.Yanen Li, Bo-June Paul Hsu, and ChengXiang Zhai.2013.
Unsupervised identification of synonymousquery intent templates for attribute intents.
In Pro-ceedings of the 22nd ACM international conferenceon Conference on information & knowledge man-agement, pages 2029?2038.
ACM.Xiao Li.
2010.
Understanding the semantic struc-ture of noun phrase queries.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 1337?1345.
Associationfor Computational Linguistics.Sandeep Pandey and Kunal Punera.
2012.
Unsuper-vised extraction of template structure in web searchqueries.
In Proceedings of the 21st internationalconference on World Wide Web, pages 409?418.ACM.Patrick Pantel and Dekang Lin.
2002.
Discoveringword senses from text.
In Proceedings of the eighthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 613?619.ACM.Patrick Pantel and Deepak Ravichandran.
2004.
Au-tomatically labeling semantic classes.
In HLT-NAACL, volume 4, pages 321?328.808Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume2-Volume 2, pages 938?947.
Association for Com-putational Linguistics.Manuel Quesada-Mart?nez, Jesualdo Tom?asFern?andez-Breis, and Robert Stevens.
2012.Enrichment of owl ontologies: a method for defin-ing axioms from labels.
In Proceedings of the FirstInternational Workshop on Capturing and RefiningKnowledge in the Medical Domain (K-MED 2012),Galway, Ireland, pages 1?10.Joseph Reisinger and Marius Pasca.
2011.
Fine-grained class label markup of search queries.
InACL, pages 1200?1209.Nikos Sarkas, Stelios Paparizos, and PanayiotisTsaparas.
2010.
Structured annotations of webqueries.
In Proceedings of the 2010 ACM SIGMODInternational Conference on Management of data,pages 771?782.
ACM.Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-Rong Wen.
2010.
Corpus-based semantic classmining: distributional vs. pattern-based approaches.In Proceedings of the 23rd International Conferenceon Computational Linguistics, pages 993?1001.
As-sociation for Computational Linguistics.Mark Stevenson and Mark A Greenwood.
2005.
Asemantic approach to ie pattern induction.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 379?386.
As-sociation for Computational Linguistics.Idan Szpektor, Aristides Gionis, and Yoelle Maarek.2011.
Improving recommendation for long-tailqueries via templates.
In Proceedings of the 20thinternational conference on World wide web, pages47?56.
ACM.Allan Third.
2012.
Hidden semantics: what can welearn from the names in an ontology?
In Proceed-ings of the Seventh International Natural LanguageGeneration Conference, pages 67?75.
Associationfor Computational Linguistics.Benjamin Van Durme and Marius Pasca.
2008.
Find-ing cars, goddesses and enzymes: Parametrizableacquisition of labeled instances for open-domain in-formation extraction.
In AAAI, volume 8, pages1243?1248.Roman Yangarber.
2003.
Counter-training in discov-ery of semantic patterns.
In Proceedings of the 41stAnnual Meeting on Association for ComputationalLinguistics-Volume 1, pages 343?350.
Associationfor Computational Linguistics.Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, andChin-Yew Lin.
2011.
Nonlinear evidence fusionand propagation for hyponymy relation mining.
InACL, volume 11, pages 1159?1168.809
