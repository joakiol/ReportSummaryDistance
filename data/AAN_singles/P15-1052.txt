Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 534?542,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Strategic Reasoning Model for Generating Alternative AnswersJon Scott StevensCenter for General Linguistics, Berlinstevens@zas.gwz-berlin.deAnton BenzCenter for General Linguistics, BerlinSebastian Reu?eRuhr University Bochumsebastian.reusse@rub.deRalf KlabundeRuhr University BochumAbstractWe characterize a class of indirect an-swers to yes/no questions, alternative an-swers, where information is given that isnot directly asked about, but which mightnonetheless address the underlying moti-vation for the question.
We develop amodel rooted in game theory that gener-ates these answers via strategic reasoningabout possible unobserved domain-leveluser requirements.
We implement themodel within an interactive question an-swering system simulating real estate dia-logue.
The system learns a prior probabil-ity distribution over possible user require-ments by analyzing training dialogues,which it uses to make strategic deci-sions about answer selection.
The systemgenerates pragmatically natural and inter-pretable answers which make for more ef-ficient interactions compared to a baseline.1 IntroductionIn natural language dialogue, questions are oftenanswered indirectly.
This is particularly apparentfor yes/no questions, where a wide range of re-sponses beyond literal ?yes?
and ?no?
answers isavailable.
Sometimes indirect answers serve to an-ticipate the next step of the hearer?s plan, as in (1)(Allen and Perrault, 1980), where the literal an-swer is entailed by the supplied answer, and some-times indirect answers leave it to the hearer to in-fer the literal answer from common contextual as-sumptions, as in (2) (de Marneffe et al, 2009).
(1) Q: Has the train to Windsor left yet?A: It?s leaving soon from gate 7.
(2) Q: Is Sue at work?A: She?s sick with the flu.But other times there is no semantic link betweenthe question and the supplied answer.
Rather, theanswer must be interpreted in light of the task-specific goals of the interlocutors.
Consider (3)in a context where a customer is posing questionsto a real estate agent with the aim of renting anapartment.
(3) Q: Does the apartment have a garden?A: Well, it has a large balcony.Whether there is a balcony has no logical bear-ing on whether there is a garden.
Intuitively, therealtor is inferring that the customer?s questionmight have been motivated by a more general re-quirement (perhaps the customer wants a placeto grow flowers) and supplying an alternative at-tribute to satisfy that requirement.
In this casethe answerer must reason about which attributes ofan apartment might satisfy a customer who wouldask about a garden.
Note that multiple motivatingrequirements are possible (perhaps the customerjust wants to relax outside), such that the answerermight just as easily have said, ?It has a large bal-cony, and there is a park close by.?
In either case,the hearer can infer from the lack of a direct an-swer that the apartment must not have a garden,because if it did, to say so would have been moreobviously helpful.This paper focuses on this class of answers,which we call alternative answers.
We character-ize these as indirect answers to yes/no questionsthat offer attributes of an object under discussionwhich might satisfy an unobserved domain-levelrequirement of the questioner.
We conceive of arequirement as a set of satisfying conditions, suchthat a particular domain-related need would be metby any one member of the set.
For example, in thecontext of (3) we can encode a possible customerrequirement of a place to grow flowers in an apart-ment, FLOWERS = {GARDEN, BALCONY}, suchthat either GARDEN or BALCONY would sufficeto satisfy the requirement.534In order to generate alternative answers auto-matically, we must first solve two problems: (i)how does one learn and represent a space of likelyuser requirements?, and (ii) how does one use sucha space to select indirect answers?
To do thisin a natural, pragmatically interpretable way, wemust not only derive answers like in (3), but cru-cially, also rule out infelicitous responses like thefollowing, where a logically possible alternativeleads to incoherence due to the low probability ofan appropriate requirement like {GARDEN, BASE-MENT}.
(In other words, wanting a garden haslittle effect on the probability of wanting a base-ment.
)(4) Q: Does the apartment have a garden?A: #Well, it has a large basement.To solve these problems, we propose an approachrooted in decision-theoretic and game-theoreticanalyses of indirectness in natural language (vanRooij, 2003; Benz and van Rooij, 2007; Benz etal., 2011; Stevens et al, 2014) whereby a systemuses strategic reasoning to derive an optimal re-sponse to a yes/no question given certain domainassumptions.
The model operates by assumingthat both the questioner and the answerer are ratio-nal, i.e.
that both participants want to further theirown goals, and will behave so as to maximize theprobability of success at doing so.One appeal of the strategic approach is its rela-tive simplicity: the model utilizes a learned prob-ability distribution over possible domain-level re-quirements of the questioner and applies simpleprobabilistic reasoning to feed content selectionduring online answer generation.
Unlike plan in-ference approaches, we do not need to representany complex taxonomies of stimulus conditions(Green and Carberry, 1994) or coherence relations(Green and Carberry, 1999; Asher and Lascarides,2003).By implementing the strategic reasoning modelwithin a simple interactive question answeringsystem (Konstantinova and Orasan, 2012), simu-lating real estate dialogues with exchanges like in(3), we are able to evaluate the current approachquantitatively in terms of dialogue efficiency, per-ceived coherence of the supplied answers, andability of users to draw natural pragmatic infer-ences.
We conclude that strategic reasoning pro-vides a promising framework for developing an-swer generation methods by starting with princi-pled theoretical analyses of human dialogue.The following section presents the model, in-cluding a concrete content selection algorithmused for producing answers to questions, and thenwalks through a simple illustrative example.
Sec-tion 3 describes our implementation, addresses theproblem of learning requirement probabilities, andpresents the results of our evaluation, providingquantitative support for our approach.
Section 4concludes with a general summary.2 Model2.1 OverviewWe derive our model beginning with a simpledescription of the discourse situation.
In ourcase, this is an exchange of questions and answerswhere a user poses questions to be answered byan expert who has access to a database of in-formation that the user wants.
The expert hasno advance knowledge of the database, and thusmust look up information as needed.
Each userquestion is motivated by a requirement, conceivedof as a (possibly singleton) set of database at-tributes (restricted for current purposes to booleanattributes), any one of which satisfies a user need(e.g.
{GARDEN, BALCONY} in the previous sec-tion).
Only the user has direct access to her ownrequirements, and only the expert can query thedatabase to inform the user whether her require-ments can be satisfied.
For current purposes weassume that each question and answer in the di-alogue pertains to a specific object o from thedatabase which is designated as the object un-der discussion.
This way we can represent an-swers and question denotations with attributes,like GARDEN, where the queried/supplied attributeis assumed to predicate over o.
In these terms, theexpert can either ASSERT an attribute (if it holdsof o) or DENY an attribute (if it does not hold of o)in response to a user query.Now we describe the goals of the interlocutors.The user wants her requirements to be satisfied,and will not accept an object until she is sure thisis the case.
If it is clear that an object cannot sat-isfy one or more requirements, the user will ask todiscuss a different object from the database.
Wecan thus characterize the set of possible user re-sponses as follows: the user may ACCEPT the ob-ject as one that meets all requirements, the usermay REJECT the object and ask to see somethingelse, or the user may FOLLOW UP, continuing topose questions about the current object.
The user?s535goal, then, is ultimately to accept an object that infact satisfies her requirements, and to reject anyobject that does not.The expert?s goal is to help the user find an op-timal object as efficiently as possible.
Given thisgoal, the expert does better to provide alternativeattributes (like BALCONY for GARDEN in (3)) inplace of simple ?no?
answers only when those at-tributes are relevant to the user?s underlying re-quirements.
To use some economic terminology,we can define the benefit (B) of looking up a po-tential alternative attribute a in the database as abinary function indicating whether a is relevant to(i.e.
a member of) the user requirement ?
whichmotivated the user?s question.
For example, in (3),if the user?s question is motivated by requirement{GARDEN, BALCONY}, then the benefit of lookingup whether there is a balcony is 1, because if thatattribute turns out to hold of o, then the customer?srequirement is satisfied.
If, on the other hand, thequestioner has requirement {GARDEN}, then thebenefit of looking up BALCONY is 0, because thisattribute cannot satisfy this requirement.B(a|?)
= 1 if a ?
?
and 0 otherwise (1)Regardless of benefit, the expert incurs a cost bylooking up information.
To fully specify what costmeans in this context, first assume a small, fixedeffort cost associated with looking up an attribute.Further assume a larger cost incurred when theuser has to ask a follow-up question to find outwhether a requirement is satisfied.
What reallymatters are not the raw cost amounts, which maybe very small, but rather the relative cost of look-ing up an attribute compared to that of receiving afollow-up.
We can represent the ratio of look-upcost to follow-up cost as a constant ?, which en-codes the reluctance of the expert to look up newinformation.
Intuitively, if ?
is close to 1 (i.e.
iffollow-ups are not much more costly than simplelook-ups), the expert should give mostly literal an-swers, and if ?
is close to 0, (i.e.
if relative follow-up cost is very high), the expert should look up allpotentially beneficial attributes.
With this, let theutility (U ) of looking up a be the benefit of lookingup a minus the relative cost.U(a|?)
= B(a|?)?
?
(2)The expert is utility-maximizing under game-theoretic assumptions, and (assuming a baselineutility of zero for doing nothing) should aim tolook up attributes for which U is positive, i.e.
forwhich benefit outweighs cost.
But the expert has aproblem: ?, on which U depends, is known only tothe user.
Therefore, the best the expert can do is toreason probabilistically, based on the user?s ques-tion, to maximize expected utility, or the weightedaverage of U(a|?)
for all possible values of ?.
Theexpected utility of looking up an attribute a can bewritten as the expected benefit of a?the weightedaverage of B(a|?)
for all ?
?minus the relativecost.
Let REQS be the set of all possible user re-quirements and let q be the user?s question.EU(a|q, REQS) = EB(a|q, REQS)?
?
(3)EB(a|q, REQS) =??
?REQSP (?|q)?B(a|?)
(4)The probability of a user requirement P (?|q) iscalculated via Bayes?
rule, assuming that userswill choose their questions randomly from the setof questions whose denotations are in their re-quirement set.
This yields the following.P (?|q) =P (q|?)?
P (?)???
?REQSP (q|??)?
P (??
)(5)P (q|?)
=1|?|if JqK ?
?
and 0 otherwise (6)The prior probability of a user requirement, P (?
),is given as input to the model.
We will see in thenext section that it is possible to learn a prior prob-ability distribution from training dialogues.We have now fully characterized the expectedbenefit (EB) of looking up an attribute in thedatabase.
As per Eq.3, the expert should onlybother looking up an attribute if EB is greaterthan the relative cost ?, since that is when EUis positive.
The final step is to give the experta sensible way to iteratively look up attributes topotentially produce multiple alternatives.
To thisend, we first point out that if an alternative hasbeen found which satisfies a certain requirement,then it no longer adds any benefit to consider thatrequirement when selecting further alternatives.For example, in the context of example (3), whenthe realtor queries the database to find the apart-ment has a balcony, she no longer needs to con-sider the probability of a requirement {BALCONY,GARDEN} when considering additional attributes,since that is already satisfied.
Given this consid-eration, the order in which database attributes are536looked up can make a difference to the outcome.So, we need a consistent and principled criterionfor determining the order in which to look up at-tributes.
The most efficient method is to start withthe attribute with the highest possible EB valueand then iteratively move down to the next best at-tribute until EB is less than or equal to cost.Note that the attribute that was asked about willalways have an EB value of 1.
Consider againthe QA exchange in (3).
Recall that the expert as-sumes that the user?s query is relevant to an un-derlying requirement ?.
This means that ?
mustcontain the attribute GARDEN.
Therefore, by defi-nition, supplying GARDEN will always yield posi-tive benefit.
We can use this fact to explain how al-ternative answers are interpreted by the user.
Theuser knows that the most beneficial attribute tolook up (in terms ofEB) is the one asked about.
Ifthat attribute is not included in the answer, the useris safe to assume that it does not hold of the objectunder discussion.
By reasoning about the expert?sreasoning, the user can derive the implicature thatthe literal answer to her question is ?no?.
In fact,this is what licenses the expert to leave the nega-tion of the garden attribute out of the answer: theexpert knows that the user knows that the expertwould have included it if it were true.
This type of?I know that you know?
reasoning is characteristicof game-theoretic analysis.12.2 Algorithm and exampleOur algorithm for generating alternative answers(Algorithm 1), which simulates strategic reason-ing by the expert in our dialogue situation, iscouched in a simple information state update(ISU) framework (Larsson and Traum, 2000;Traum and Larsson, 2003), whereby the answererkeeps track of the current object under discussion(o) as well as a history of attributes looked up foro (HISTo).
The output of the algorithm takes theform of a dialogue move, either an assertion (or setof assertions) or denial that an attribute holds of o.These dialogue moves can then be translated intonatural language with simple sentence templates.The answerer uses HISToto make sure redundantalternatives aren?t given across QA exchanges.
If1It can be shown that the answer selection algorithm pre-sented in this section, combined with a simple user inter-pretation model, constitutes a perfect Bayesian equilibrium(Harsanyi, 1968; Fudenberg and Tirole, 1991) in a signalinggame (Lewis, 1969) with private hearer types which formallydescribes this kind of dialogue.Requirement set P (?)
P (q|?)
P (?|q)?G={GARDEN} 0.5 1 0.67?F={GARDEN, BALCONY} 0.25 0.5 0.17?P={GARDEN, PARK} 0.2 0.5 0.13?S={GARDEN, BASEMENT} 0.05 0.5 0.03Table 1: A toy example of a customer requirementspace with probabilities for q = ?Does the apart-ment have a garden?
?all possible answers are redundant, the answererfalls back on a direct yes/no response.To illustrate how the algorithm works, considera simple toy example.
Table 1 gives a hypotheticalspace of possible requirements along with a dis-tribution of priors, likelihoods and Bayesian pos-teriors.
We imagine that a customer might wanta garden (?G), or more generally a place to growflowers (?F), a place for their child to play outside(?P), or, in rare cases, either a garden or a base-ment to use as storage space (?S).
The rather oddnature of ?Sis reflected in its low prior.
Considera variant of (3) where HISTois empty, and whereDBocontains BALCONY, PARK and BASEMENT.
(5) Q: Does the apartment have a garden?A: It has a balcony, and there is a parkvery close by.To start, let REQS contain the requirements inTable 1, and let ?
= 0.1.
The algorithm derivesthe answer as follows.
First, the algorithm looksup whether GARDEN holds of o.
It does not hold,so GARDEN is not added to the answer; it is onlyadded to the history of looked up attributes.a = GARDEN; EB(GARDEN) = 1;HISTo= {GARDEN}Then, the system finds the next best attribute, BAL-CONY, which does hold of o, appends it to the an-swer as well as the history, and removes the rele-vant requirement from consideration.a = BALCONY; EB(BALCONY) = 0.17;HISTo= {GARDEN, BALCONY};ANSWER = {BALCONY};REQS = {?G, ?P, ?S}The attribute PARK is similarly added.a = PARK; EB(PARK) = 0.13;HISTo= {GARDEN, BALCONY, PARK};ANSWER = {BALCONY, PARK};REQS = {?G, ?S}The attribute BASEMENT is next in line.
However,its EB value is below the threshold of 0.1 due537Algorithm 1 An algorithm for generating alternative answersInput: A set of attributes ?, an object under discussion o, a database DBoof attributes which hold of o, a history HISToofattributes that have been looked up in the database, a set of possible user requirements REQS, a prior probability distributionover REQS, a user-supplied question q with denotation JqK and a relative cost threshold ?
?
(0, 1)Initialize: ANSWER = {}; LOOKUP = TRUE1: while LOOKUP do2: ?
?= (?
\ HISTo) ?
{JqK} .
Only consider alternatives once per object per dialogue.3: a = arg max???
?EB(?|q, REQS) .
Find the best candidate answer.4: if EB(a|q, REQS) > ?
then .
Check whether expected benefit outweighs cost.5: HISTo= HISTo?
{a} .
Log which attribute has been looked up.6: if a ?
DBothen7: ANSWER = ANSWER ?
{a} .
Add to answer if attribute holds.8: REQS = REQS \ {?
?
REQS | ?
?
ANSWER 6= ?}.
Don?t consider requirements that are already satisfied.9: end if10: else11: LOOKUP = FALSE .
Stop querying the database when there are no promising candidates left.12: end if13: end while14: if ANSWER 6= ?
then ASSERT(ANSWER),15: else DENY(JqK)16: end ifto its low prior probability, and thus the iterationstops there, and BASEMENT is never looked up.a = BASEMENT; EB(BASEMENT) = 0.03;EB < ?
; exit loop3 Implementation and evaluation3.1 SetupA simple interactive question answering sys-tem was built using a modified version of thePyTrindiKit toolkit2with a database back end im-plemented using an adapted version of PyKE, aHorn logic theorem prover.3The system was setup to emulate the behavior of a real estate agentanswering customers?
yes/no questions about arange of attributes pertaining to individual apart-ments.
A set of 12 attributes was chosen for thecurrent evaluation experiment.
The system gen-erates answers by first selecting a discourse move(i.e.
assertion or denial of an attribute) and thentranslating the move into natural language withsimple sentence templates like, ?It has a(n) X?
or?There is a(n) X nearby?.
When answers are in-direct (i.e.
not asserting or denying the attributeasked about), the system begins its reply with thediscourse connective ?well?
as in example (3).42https://code.google.com/p/py-trindikit3http://pyke.sourceforge.net/4Early feedback indicated that alternative answers weremore natural when preceded by such a discourse connective.To assess this effect, we ran a separate evaluation experimentwith an earlier version of the system that produced alterna-tive answers without ?well?.
Dialogue lengths and coherencescores were not very different from what is reported in thisSubjects interacted with our system by means ofan online text-based interface accessible remotelythrough a web browser.
At the outset of the exper-iment, subjects were told to behave as if they werefinding an apartment for a hypothetical friend, andgiven a list of requirements for that friend.
Thetask required them to identify which from amonga sequence of presented apartments would satisfythe given set of requirements.
One out of fourlists, each containing three requirements (one ofwhich was a singleton), was assigned to subjectsat random.
The requirements were constructed bythe researchers to be plausible desiderata for userslooking for a place to rent or buy (e.g.
connectionto public transit, which could be satisfied either bya nearby bus stop, or by a nearby train station).The apartments presented by the system wereindividually generated for each experiment suchthat there was an apartment satisfying one attributefor each possible combination of the three require-ments issued to subjects, plus two additional apart-ments that each satisfied two of the conditions(23+ 2 = 10 apartments overall).
Attributes out-side a subject?s requirement sets were added atrandom to assess the effect of ?unhelpful?
alter-native answers.Subject interacted with one of two answer gen-eration models: a literal model, which only pro-duced direct yes/no answers, and the strategicsection; however, in contrast with the current evaluation, wefound a large effect of model type (a 69% decrease for strate-gic vs. literal) on whether the subjects successfully completedthe task (z=-2.19, p=0.03).
This is consistent with the earlyfeedback.538model as outlined above.
Crucially, in both con-ditions, the sequence in which objects were pre-sented was fixed so that the last apartment of-fered would be the sole object satisfying all ofthe desired criteria.
Also, we set the strategicmodel?s ?
parameter high enough (1/7) that onlysingle-attribute answers were ever given.
Thesetwo properties of the task, taken together, allowus to obtain an apples-to-apples comparison of themodels with respect to average dialogue length.
Ifsubjects failed to accept the optimal solution, theinteraction was terminated.
After completing in-teraction with our system, subjects were asked tocomplete a short survey designed to get at the per-ceived coherence of the system?s answers.
Sub-jects were asked to rate, on a seven-point Likertscale, the relevance of the system?s answers to thequestions asked, overall helpfulness, the extent towhich questions seemed to be left open, and theextent to which the system seemed evasive.We predict that the strategic system will im-prove overall efficiency of dialogue over that of theliteral system by (i) offering helpful alternatives tosatisfy the customer?s needs, and (ii) allowing cus-tomers to infer implicit ?no?
answers from alterna-tive answers, leading to rejections of sub-optimalapartments.
If, contrary to our hypothesis, sub-jects fail to draw inferences/implicatures from al-ternative answers, then we expect unhelpful alter-natives (i.e.
alternatives not in the user?s require-ment set) to prompt repeated questions and/or fail-ures to complete the task.With respect to the questionnaire items, the lit-eral system is predicted to be judged maximallycoherent, since only straightforward yes/no an-swers are offered.
The question is whether thepragmatic system also allows for coherent dia-logue.
If subjects judge alternative answers to beincoherent, then we expect any difference in aver-age Likert scale ratings between strategic and lit-eral system to reflect the proportion of alternativeanswers that are given.3.2 Learning prior probabilitiesBefore presenting our results, we explain howprior probabilities can be learned within thisframework.
One of the assumptions of the strate-gic reasoning model is that users ask questions thatare motivated by specific requirements.
Moreover,we should assume that users employ a reason-able questioning strategy for finding out whetherS: An apartment in the north of town might suit you.
Ihave an additional offer for you there.U: Does the apartment have a garden?S: The apartment does not have a garden.U: Does the apartment have a balcony?S: The apartment does not have a balcony.U: I?d like to see something elseFigure 1: An example of the negation-rejection se-quence ?GARDEN, BALCONY?requirements hold, which is tailored to the sys-tem they are interacting with.
For example, if auser interacts with a system that only produces lit-eral yes/no answers, the user should take all an-swers at face value, not drawing any pragmaticinferences.
In such a scenario, we expect theuser?s questioning strategy to be roughly as fol-lows: for a1, a2, ?
?
?
, anin requirement ?, askabout a1, then if a1is asserted, accept (or moveon to the next requirement if there are multiple re-quirements), and if not, ask about a2; if a2is as-serted, accept, and if not, ask about a3, and so on,until anis asked about.
If anis denied, then rejectthe object under discussion.
If you need a placeto grow flowers, ask if there is a balcony or gar-den, then, if the answer is no, ask about the otherattribute.
If no ?yes?
answers are given, reject.Such a strategy predicts that potential user re-quirements should be able to be gleaned from dia-logues with a literal system by analyzing negation-rejection sequences (NRSs).
A negation-rejectionsequence is a maximal observed sequence of ques-tions which all receive ?no?
answers, without anyintervening ?yes?
answers or any other interven-ing dialogue moves, such that at the end of thatsequence of questions, the user chooses to rejectthe current object under discussion.
Such a se-quence is illustrated in Fig.1.
By hypothesis, theNRS ?GARDEN, BALCONY?
indicates a possibleuser requirement {GARDEN, BALCONY}.By considering NRSs, the system can learnfrom training data a reasonable prior probabilitydistribution over possible customer requirements.This obviates the need to pre-supply the systemwith complex world knowledge.
If customer re-quirements can in principle be learned, then thestrategic approach could be expanded to dialoguesituations where the distribution of user require-ments could not sensibly be pre-supplied.
Whilethe system in its current form is not guaranteed toscale up in this way, its success here provides uswith a promising proof of concept.539Using the dialogues with the literal system astraining data, we were able to gather frequen-cies of observed negation-rejection sequences.
Bytransforming the sequences into unordered setsand then normalizing the frequencies of those sets,we obtained a prior probability distribution overpossible customer requirements.
In the training di-alogues, subjects were given the same lists of re-quirements as was given for the evaluation of thestrategic model.
If successful, the system shoulduse the yes/no dialogue data to learn high prob-abilities for requirements which customers actu-ally had, and low probabilities for any others, al-lowing us to evaluate the system without giving itany prior clues as to which customer requirementswere assigned.Because we know in advance which require-ments the subjects wanted to fulfill, we havea gold standard against which we can comparethe question-alternative answer pairs that differentvariants of the model are able to produce.
For ex-ample, we know that if a subject asked whetherthe apartment had a balcony and received an an-swer about a nearby caf?, that answer could nothave been beneficial, since no one was assignedthe requirement {CAF?, BALCONY}.Table 2 compares three variant models: (i) thesystem we use in our evaluation, which sets priorprobabilities proportional to NRS frequency, (ii) asystem with flat priors, where probability is zeroif NRS frequency is zero, but where all observedNRSs are taken to correspond to equiprobable re-quirements, and finally (iii) a baseline which doesnot utilize an EB threshold, but rather simply ran-domly selects alternatives which were observed atleast once in an NRS with the queried attribute.These models are compared by the maximum ben-efit of their possible outputs using best-case val-ues for ?.
We see that there is a good match be-tween the answers given by the strategic modelwith learned priors and the actual requirementsthat users were told to fulfill.Though it remains to be seen whether this wouldscale up to more complex requirement spaces, thisresult suggests that NRSs can in fact be indicativeof disjunctive requirement sets, and can indeed beuseful in learning what possible alternatives mightbe.
For purposes of our evaluation, we will seethat the method was successful.Model Precision Recall F1Frequency-based 1 0.92 0.96Flat 0.88 0.92 0.90Baseline 0.23 1 0.37Table 2: Comparison of best-case output withrespect to potential benefit of alternative answertypes to subjects.
Precision = hits / hits+misses,and Recall = hits / possible hits.
A ?hit?
is a QApair which is a possible output of the model, suchthat A could be a beneficial answer to a customerasking Q, and a ?miss?
is such a QA pair such thatA is irrelevant to Q.3.3 Evaluation resultsWe obtained data from a total of 115 subjects viaAmazon Mechanical Turk; 65 subjects interactedwith the literal comparison model, and 50 sub-jects interacted with the strategic model.
We ex-cluded a total of 13 outliers across both condi-tions who asked too few or too many questions(1.5 interquartile ranges below the 1st or abovethe 3rd quartile).
These subjects either quit thetask early or simply asked all available questionseven for apartments that were obviously not agood fit for their requirements.
Two subjects wereexcluded for not filling out the post-experimentquestionnaire.
This left 100 subjects (59 literal/41strategic), of which 86 (49/37) successfully com-pleted the task, accepting the object which metall assigned requirements.
There was no statisti-cally significant difference between the literal andstrategic models with respect to task success.We first compare the literal and strategic modelswith regard to dialogue length, looking only at thesubjects who successfully completed the task.
Dueto the highly structured nature of the experimentit was always the case that a successful dialogueconsisted of 10 apartment proposals, some num-ber of QA pairs, where each question was given asingle answer, 9 rejections and, finally, one accep-tance.
This allows us to use the number of ques-tions asked as a proxy for dialogue length.
Fig-ure 2 shows the comparison.
The strategic modelyields 27.4 questions on average, more than fourfewer than the literal model?s 31.6.
Standard sta-tistical tests show the effect to be highly signif-icant, with a one-way ANOVA yielding F=16.2,p = 0.0001, and a mixed effects regression modelwith a random slope for item (the items in this casebeing the set of requirements assigned to the sub-5400102030LiteralStrategicModelNumberof questionsFigure 2: Avg.
number of QA pairs by modelS: How about an apartment in the east of the city?
Ihave an offer for you there.U: Does the apartment have a caf?
nearby?S: Well, there is a restaurant nearby.U: I?d like to see something elseFigure 3: A QA exchange from a dialogue wherethe user was instructed to find an apartment with acaf?
nearbyject) yielding t=4, p=0.0001.We now ask whether the observed effect isdue only to the presence of helpful alternativeswhich preclude the need for follow-up questions,or whether the ability of users to draw pragmaticinferences from unhelpful alternatives (i.e.
alterna-tives that don?t actually satisfy the user?s require-ment) also contributes to dialogue efficiency.
Fig-ure 3, taken from a real dialogue with our system,illustrates such an inference.
The subject specifi-cally wants a caf?
nearby, and infers from the al-ternative answer that this requirement cannot besatisfied, and therefore rejects.
The subject couldhave asked the question again to get a direct an-swer, which would have had a negative effect ondialogue efficiency, but this did not happen.
Wewant to know if subjects?
aggregate behavior re-flects this example.First, take the null hypothesis to be that subjectsdo not reliably draw such negative implicatures.
Inthat case we would expect a certain proportion ofquestions to be repeated.
Subjects are allowed toask questions multiple times, and alternatives arenever presented twice, such that repeating ques-tions will ultimately lead to a direct yes/no answer.We do see some instances of this behavior in the0.000.050.100.150.200.0 0.1 0.2Unhelpful alternative answers / Total answersRepeated questions/ Total questionsFigure 4: Proportion unhelpful alternatives vs.proportion repeated questionsdialogues.
If this is indicative of an overall diffi-culty in drawing pragmatic inferences from an on-line dialogue system, then we expect the numberof such repetitions to reflect the number of unhelp-ful alternatives that are offered.
Instead, we findthat when we plot a linear regression of repeatedquestions vs. unhelpful alternatives, we get a flatline with no observable correlation (Fig.4).
More-over, we also find no effect of unhelpful alterna-tives on whether the task was successfully com-pleted.
This suggests that the correct inferencesare being drawn, as in Fig.3.We now look at the perceived coherence ofthe dialogues as assessed by our post-experimentquestionnaire.
We obtain a composite coher-ence score from all coherence-related items on theseven point Likert scale by summing all per-itemscores for each subject and normalizing them to aunit interval, where 1 signifies the upper bound ofperceived coherence.
Although there is a differ-ence in mean coherence score between the strate-gic and literal models, with the strategic model ex-hibiting 88% perceived coherence and the literalmodel 93%, the difference is not statistically sig-nificant.
Moreover, we can rule out the possibilitythat the strategic model is judged to be coherentonly when the number of alternative answers islow.
To rule this out, we calculate the expectedcoherence score under the null hypothesis that co-herence is directly proportional to the proportionof literal answers.
Taking the literal model?s av-erage score of 0.93 as a ceiling, we multiply thisby the proportion of literal answers to obtain a541null hypothesis expected score of about 0.75 forthe strategic model.
This null hypothesis is dis-confirmed (F=12.5, t=30.6, p<0.01).
The strate-gic model is judged, by the criteria assessed byour post-experiment questionnaire, to be pragmat-ically coherent independently of the rate of indi-rect answers given.4 ConclusionWe have characterized the class of alternative an-swers to yes/no questions and proposed a contentselection model for generating these answers indialogue.
The model is based on strategic rea-soning about unobserved user requirements, andis based on work in game-theoretic pragmatics(Benz and van Rooij, 2007; Stevens et al, 2014).The model was implemented as an answer selec-tion algorithm within an interactive question an-swering system in a real estate domain.
We havepresented an evaluation of this system against abaseline which produces only literal answers.
Theresults show that the strategic reasoning approachleads to efficient dialogues, allows pragmatic in-ferences to be drawn, and does not dramaticallyreduce the overall perceived coherence or natural-ness of the produced answers.
Although the strate-gic model requires a form of world knowledge?knowledge of possible user requirements and theirprobabilities?we have shown that there is a sim-ple method, the analysis of negation-rejection se-quences in yes/no QA exchanges, that can be usedto learn this knowledge with positive results.
Fur-ther research is required to address issues of scala-bility and generalizability, but the current modelrepresents a promising step in the direction ofpragmatically competent dialogue systems withsolid basis in formal pragmatic theory.AcknowledgmentsThis work has been supported by the DeutscheForschungsgemeinschaft (DFG) (Grant nrs.
BE4348/3-1 and KL 1109/6-1, project ?PragmaticRequirements for Answer Generation in a SalesDialogue?
), and by the Bundesministerium f?rBildung und Forschung (BMBF) (Grant nr.01UG0711).ReferencesJames F. Allen and C. Raymond Perrault.
1980.
Ana-lyzing intention in utterances.
Artificial Intelligence,15(3):143?178.N.
Asher and A. Lascarides.
2003.
Logics of Con-versation.
Studies in Natural Language Processing.Cambridge University Press.Anton Benz and Robert van Rooij.
2007.
Optimal as-sertions, and what they implicate.
a uniform gametheoretic approach.
Topoi, 26(1):63?78.Anton Benz, Nuria Bertomeu, and AlexandraStrekalova.
2011.
A decision-theoretic approachto finding optimal responses to over-constrainedqueries in a conceptual search space.
In Proceed-ings of the 15th Workshop on the Semantics andPragmatics of Dialogue, pages 37?46.Marie-Catherine de Marneffe, Scott Grimm, andChristopher Potts.
2009.
Not a simple yes or no.
InProceedings of the SIGDIAL 2009 Conference: The10th Annual Meeting of the Special Interest Groupon Discourse and Dialogue, pages 136?143.Dan Fudenberg and Jean Tirole.
1991.
PerfectBayesian equilibrium and sequential equilibrium.Journal of Economic Theory, 53(2):236?260.Nancy Green and Sandra Carberry.
1994.
Generatingindirect answers to yes-no questions.
In Proceed-ings of the Seventh International Workshop on Nat-ural Language Generation, pages 189?198.Nancy Green and Sandra Carberry.
1999.
Interpret-ing and generating indirect answers.
ComputationalLinguistics, 25(3):389?435.John C. Harsanyi.
1968.
Games of incomplete infor-mation played by ?Bayesian?
players, part II.
Man-agement Science, 14(5):320?334.Natalia Konstantinova and Constantin Orasan.
2012.Interactive question answering.
Emerging Appli-cations of Natural Language Processing: Conceptsand New Research, pages 149?169.Staffan Larsson and David R. Traum.
2000.
Informa-tion state and dialogue management in the TRINDIdialogue move engine toolkit.
Natural LanguageEngineering, 6(3&4):323?340.David Lewis.
1969.
Convention: A PhilosophicalStudy.
Cambridge University Press, Cambridge.Jon Scott Stevens, Anton Benz, Sebastian Reu?e,Ronja Laarmann-Quante, and Ralf Klabunde.
2014.Indirect answers as potential solutions to decisionproblems.
In Proceedings of the 18th Workshop onthe Semantics and Pragmatics of Dialogue, pages145?153.David R. Traum and Staffan Larsson.
2003.
The in-formation state approach to dialogue management.In Jan van Kuppevelt and Ronnie W. Smith, edi-tors, Current and new directions in discourse anddialogue, pages 325?353.
Springer.Robert van Rooij.
2003.
Questioning to resolvedecision problems.
Linguistics and Philosophy,26(6):727?763.542
