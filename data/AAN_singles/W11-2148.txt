Proceedings of the 6th Workshop on Statistical Machine Translation, pages 399?404,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsThe Value of Monolingual Crowdsourcing in a Real-World TranslationScenario: Simulation using Haitian Creole Emergency SMS MessagesChang Hu?, Philip Resnik?
?, Yakov Kronrod?Vladimir Eidelman?, Olivia Buzek?
?, Benjamin B.
Bederson?
?UMIACS and Department of Linguistics?UMIACS and Department of Computer ScienceUniversity of Maryland, College Park{changhu,bederson}@cs.umd.edu{resnik,vlad,buzek}@umiacs.umd.eduyakov@umd.eduAbstractMonoTrans2 is a translation system that com-bines machine translation (MT) with humancomputation using two crowds of monolin-gual source (Haitian Creole) and target (En-glish) speakers.
We report on its use in theWMT 2011 Haitian Creole to English trans-lation task, showing that MonoTrans2 trans-lated 38% of the sentences well compared toGoogle Translate?s 25%.1 IntroductionOne of the most remarkable success stories to comeout of the January 2010 earthquake in Haiti in-volved translation (Munro, 2010).
While otherforms of emergency response and communicationchannels were failing, text messages were still get-ting through, so a number of people came together tocreate a free phone number for emergency text mes-sages, which allowed earthquake victims to reportthose who were trapped or in need of medical atten-tion.
The problem, of course, was that most peoplewere texting in Haitian Creole (Kreyol), a languagenot many of the emergency responders understood,and few, if any, professional translators were avail-able.
The availability of usable translations literallybecame a matter of life and death.In response to this need, Stanford University grad-uate student Rob Munro coordinated the rapid cre-ation of a crowdsourcing framework, which allowedvolunteers ?
including, for example, Haitian expa-triates and French speakers ?
to translate messages,providing responders with usable information in aslittle as ten minutes.
Translations may not have beenperfect, but to a woman in labor, it had to have madea big difference for English-speaking responders tosee Undergoing children delivery Delmas 31 insteadof Fanm gen tranche pou fe` yon pitit nan Delmas 31.What about a scenario, though, in which even am-ateur bilingual volunteers are hard to find, or toofew in number?
What about a scenario, e.g.
theMarch 2011 earthquake and tsunami in Japan, inwhich there are many people worldwide who wishto help but are not fluent in both the source and tar-get languages?For the last few years, we have been exploring theidea of monolingual crowdsourcing for translation?
that is, technology-assisted collaborative transla-tion involving crowds of participants who know onlythe source or target language (Buzek et al, 2010;Hu, 2009; Hu et al, 2010; Hu et al, 2011; Resniket al, 2010).
Our MonoTrans2 framework has pre-viously shown very promising results on children?sbooks: on a test set where Google Translate pro-duced correct translations for only 10% of the inputsentences, monolingual German and Spanish speak-ers using our framework produced translations thatwere fully correct (as judged by two independentbilinguals) nearly 70% of the time (Hu et al, 2011).We used the same framework in the WMT 2011Haitian-English translation task.
For this experi-ment, we hired Haitian Creole speakers located inHaiti, and recruited English speakers located in theU.S., to serve as the monolingual crowds.2 SystemMonoTrans2 is a translation system that combinesmachine translation (MT) with human computation(Quinn et al, 2011) using two ?crowds?
of mono-lingual source (Haitian Creole) and target (English)399speakers.1 We summarize its operation here; see Huet al (2011) for details.The Haitian Creole sentence is first automaticallytranslated into English and presented to the Englishspeakers.
The English speakers then can take any ofthe following actions for candidate translations:?
Mark a phrase in the candidate as an error?
Suggest a new translation candidate?
Vote candidates up or downIdentifying likely errors and voting for candidatesare things monolinguals can do reasonably well:even without knowing the intended interpretation,you can often identify when some part of a sentencedoesn?t make sense, or when one sentence seemsmore fluent or plausible than another.
Sometimesrather than identifying errors, it is easier to suggestan entirely new translation candidate based on theinformation available on the target side, a variantof monolingual post-editing (Callison-Burch et al,2004).Any new translation candidates are then back-translated into Haitian Creole, and any spans markedas translation errors are projected back to identifythe corresponding spans in the source sentence, us-ing word alignments as the bridge (cf.
Hwa et al(2002), Yarowsky et al (2001)).2 The Haitian Cre-ole speakers can then:?
Rephrase the entire source sentence (cf.
(Morita and Ishida, 2009))?
?Explain?
spans marked as errors?
Vote candidates up or down (based on the back-translation)Source speakers can ?explain?
error spans by of-fering a different way of phrasing that piece of thesource sentence (Resnik et al, 2010), in order toproduce a new source sentence, or by annotating thespans with images (e.g.
via Google image search)or Web links (e.g.
to Wikipedia).
The protocol thencontinues: new source sentences created via partial-1For the work reported here, we used Google Translate asthe MT component via the Google Translate Research API.2The Google Translate Research API provides alignmentswith its hypotheses.or full-sentence paraphrase pass back through MTto the English side, and any explanatory annota-tions are projected back to the corresponding spansin the English candidate translations (where the er-ror spans had been identified).
The process is asyn-chronous: participants on the Haitian Creole andEnglish sides can work independently on whateveris available to them at any time.
At any point, thevoting-based scores can be used to extract a 1-besttranslation.In summary, the MonoTrans2 framework usesnoisy MT to cross the language barrier, and supportsmonolingual participants in doing small tasks thatgain leverage from redundant information, the hu-man capacity for linguistic and real-world inference,and the wisdom of the crowd.3 ExperimentWe recruited 26 English speakers and 4 Haitian Cre-ole speakers.
The Haitian Creole speakers were re-cruited from Haiti and do not speak English.
Five ofthe 26 English speakers were paid UMD undergrad-uates; the other 21 were volunteer researchers, grad-uate students, and staff unrelated to this research.
3Over a 13 day period, Haitian Creole and Englishspeaker efforts totaled 15 and 29 hours, respectively.4 Data SetsOur original goal of fully processing the entire SMSclean test and devtest sets could not be realized in theavailable time, owing to unanticipated reshuffling ofthe data by the shared task organizers and logisticalchallenges working with participants in Haiti.
Ta-ble 1 summarizes the data set sizes before and afterreshuffling.
We put 1,224 sentences from the pre-before aftertest 1,224 1,274devtest 925 900Table 1: SMS clean data sets before and after reshufflingreshuffling test set, interspersed with 123 of the 925sentences from the pre-reshuffling devtest set, intothe system ?
1,347 sentences in total.
We report3These, obviously, did not include any of the authors.400results on the union of pre- and post-reshuffling de-vtest sentences (Set A, |A| = 1516), and the post-reshuffling test set (Set B, |B| = 1274 ).5 EvaluationOf the 1,347 sentences available for processing inMonoTrans2, we define three subsets:?
Touched: Sentences that were processed by atleast one person (657 sentences)?
Each-side: Sentences that were processed by atleast one English speaker followed by at leastone Haitian Creole speaker (431 sentences)?
Full: Sentences that have at least three trans-lation candidates, of which the most voted-forone received at least three votes (207 sentences)We intersect these three sets with sets A and B in or-der to evaluate MonoTrans2 output against the pro-vided references (Table 2).4Set S |S| |S ?A| |S ?B|Touched 657 162 168Each-side 431 127 97Full 207 76 60Table 2: Data sets for evaluation and their sizesTables 3 and 4 report two automatic scoring met-rics, uncased BLEU and TER, comparing Mono-Trans2 (M2) against Google Translate (GT) as abaseline.Set Condition BLEU TERTouched ?AGT 21.75 56.99M2 23.25 57.27Each-side ?AGT 21.44 57.51M2 21.47 58.98Full ?AGT 25.05 54.15M2 27.59 52.78Table 3: BLEU and TER results for different levels of com-pletion on the devtest set ASince the number of sentences in each evaluatedset is different (Table 2), we cannot directly compare4Note that according to these definitions, Touched containsboth Each-side and Full, but Each-side does not contain Full.Set Condition BLEU TERTouched ?BGT 19.78 59.88M2 24.09 58.15Each-side ?BGT 21.15 56.88M2 23.80 57.19Full ?BGT 22.51 54.51M2 28.90 52.22Table 4: BLEU and TER results for different levels of com-pletion on the test set Bscores between the sets.
However, Table 4 showsthat when the MonoTrans2 process is run on testitems ?to completion?, in the sense defined by ?Full?(i.e.
Full?B), we see a dramatic BLEU gain of 6.39,and a drop in TER of 2.29 points.
Moreover, evenwhen only target-side or only source-side monolin-gual participation is available we see a gain of 4.31BLEU and a drop of 1.73 TER points (Touched?B).By contrast, the results on the devtest data are en-couraging, but arguably mixed (Table 3).
In order tostep away from the vagaries of single-reference au-tomatic evaluations, therefore, we also conducted anevaluation based on human judgments.
Two nativeEnglish speakers unfamiliar with the project wererecruited and paid for fluency and adequacy judg-ments: for each target translation paired with its cor-responding reference, each evaluator rated the tar-get sentence?s fluency and adequacy on a 5-pointscale, where fluency of 5 indicates complete fluencyand adequacy of 5 indicates complete preservationof meaning (Dabbadie et al, 2002).5Sentences N Google MonoTrans2Full ?A 76 18 (24%) 30 (39%)Full ?B 60 15 (25%) 23 (38%)Table 5: Number of sentences with maximum possibleadequacy (5) in Full ?A and Full ?B, respectively.Similar to Hu et al (2011), we adopt the very con-servative criterion that a translation output is consid-ered correct only if both evaluators independentlygive it a rating of 5.
Unlike Hu et al (2011), forwhom children?s book translation requires both flu-ency and adequacy, we make this a requirement only5Presentation order was randomized.401for adequacy, since in this scenario what matters toaid organizations is not whether a translation is fullyfluent, but whether it is correct.
On this criterion,the Google Translate baseline of around 25% cor-rect improves to around 40% for Monotrans, con-sistently for both the devtest and test data (Table 5).Nonetheless, Figures 1 and 2 make it clear that theimprovements in fluency are if anything more strik-ing.5.1 Statistical AnalysisVariable Adequacy FluencyPositivemostSingleCandidateVote ** ***candidateCount ** **numOfAnswers * NSNegativeroundTrips *** ***voteCount * .Table 6: Effects of independent variables in linear regres-sion for 330 touched sentences(Signif.
codes: ?***?
0.001, ?**?
0.01, ?*?
0.05, ?.?
0.1)In addition to the main evaluation, we investi-gated the relationship between tasks performed inthe MonoTrans2 system and human judgments us-ing linear regression and an analysis of variance.We evaluate the set of all 330 touched sentences inTouched?A and Touched?B in order to under-stand which properties of the MonoTrans2 processcorrelate with better translation outcomes.Our analysis focused on improvement over theGoogle Translate baseline, looking specifically atthe improvement based on the human evaluators?
av-eraged fluency and adequacy scores.Table 6 summarizes the positive and negativeeffects for five of six variables we considered thatcame out significant for at least one of the measures.6The positive results were as expected.
Havingmore votes for the winning candidate (mostSingle-CandidateVote) made it more successful, since thismeans that more people felt it was a good represen-tative translation.
Having more candidates to choose6A sixth, numOfVoters, was not significant in the linear re-gression for either adequacy or fluency.from (candidateCount) meant that more people hadtaken the time to generate alternatives, reflecting at-tention paid to the sentence.
Also, the amount ofattention paid to target speakers?
requests for clarifi-cation (numOfAnswers) is as expected related to theadequacy of the final translation, and perhaps as ex-pected does not correlate with fluency of the outputsince it helps with meaning and not actual target-sidewording.We were, however, confused at first by the neg-ative influence of the roundTrips measure and vote-Count measures.
We conjecture that the first effectarises due to a correlation between roundTrips andtranslation difficulty; much harder sentences wouldhave led to many more paraphrase requests, andhence to more round trips.
We attempted to inves-tigate this hypothesis by testing correlation with anaive measure of sentence difficulty, length, but thiswas not fruitful.
We suspect that inspecting use ofabbreviations, proper nouns, source-side mistakes,and syntactic complexity would give us more insightinto this issue.As for voteCount, the negative correlation is un-derstandable when considered side by side withthe other vote-based measure, mostSingleCandidat-eVote.
Having a higher number of votes for the win-ning candidate leads to improvement (strongly sig-nificant for both adequacy and fluency), so a highergeneral vote count means that people were also vot-ing more times for other candidates.
Hence, once thepositive winning vote count is taken into account,the remaining votes actually represent disagreementon the candidates, hence correlating negatively withoverall improvement over baseline.It is important to note that when these measuresare all considered together, they show that there is aclear correlation between the MonoTrans2 system?shuman processing and the eventual increase in bothquality and fluency of the sentences.
As people givemore attention to sentences, these sentences showbetter performance, as judged by increase over base-line.6 DiscussionOur experiment did not address acquisition of, andincentives for, monolingual participants.
In fact, get-ting time from Haitian Creole speakers, even for pay,4020?10?20?30?40?50?60?1?
2?
3?
4?
5?#?of?sentences?Google?MonoTrans2?
(a) Fluency Distribution0?10?20?30?40?50?1?
2?
3?
4?
5?#?of?Sentences?Google?MonoTrans2?
(b) Adequacy DistributionFigure 1: Human judgments for fluency and adequacy in fully processed devtest items (Full ?A)0?10?20?30?40?50?1?
2?
3?
4?
5?#?of?sentences?Google?MonoTrans2?
(a) Fluency Distribution0?10?20?30?40?50?1?
2?
3?
4?
5?#?of?Sentences?Google?MonoTrans2?
(b) Adequacy DistributionFigure 2: Human judgments for fluency and adequacy in fully processed test items (Full ?B)created a large number of logistical challenges, andwas a contributing factor as to why we did not obtaintranslations for the entire test set.
However, avail-ability of monolingual participants is not the issuebeing addressed in this experiment: we are confi-dent that in a real-world scenario like the Haitianor Japanese earthquakes, large numbers of monolin-gual volunteers would be eager to help, certainly inlarger total numbers than bilingual volunteers.
Whatmatters here, therefore, is not how much of the testset was translated in total, but how much the trans-lations improved for the sentences where monolin-gual crowdsourcing was involved, compared to theMT baseline, and what throughput might be like ina real-world scenario.We also were interested in throughput, particu-larly in comparison to bilingual translators.
In previ-ous experimentation (Hu et al, 2011), throughput inMonoTrans2 extrapolated to roughly 800 words perday, a factor of 2.5 slower than professional trans-lators?
typical speed of 2000 words per day.
Inthis experiment, overall translation speed averagedabout 300 words per day, a factor of more than 6times slower.
However, this is an extremely pes-simistic estimate, for several reasons.
First, our pre-vious experiment had more than 20 users per side,while here our Haitian crowd consisted of only fourpeople.
Second, we discovered after beginning theexperiment that the translation of our instructionsinto Haitian Creole had been done somewhat slop-pily.
And, third, we encountered a range of tech-nical and logistical problems with our Haitian par-ticipants, ranging from finding a location with In-ternet access to do the work (ultimately an InternetCafe?
turned out to be the best option), to slow andsporadic connections (even in an Internet Cafe?
), torelative lack of motivation for part-time rather thanfull-time work.
It is fair to assume that in a real-world scenario, some unanticipated problems likethese might crop up, but it also seems fair to assumethat many would not; for example, most people fromthe Haitian Creole and French-speaking communi-ties who volunteered using Munro et al?s systemin January 2010 were not themselves located in the403third world.Finally, regarding quality, the results here arepromising, albeit not as striking as those Hu et al(2011) obtained for Spanish-German translation ofchildren?s books.
The nature of SMS messagesthemselves may have been a contributing factor tothe lower translation adequacy: even in clean form,these are sometimes written using shorthand (e.g.?SVP?
), and are sometimes not syntactically correct.The text messages are seldom related to each other,unlike sentences in larger bodies of text where evenpartially translated sentences can be related to eachother to provide context, as is the case for children?sbooks.
One should also keep in mind that the under-lying machine translation engine, Google Translatebetween Haitian Creole and English, is still in an al-pha phase.Those considerations notwithstanding, it is en-couraging to see a set of machine translations getbetter without the use of any human bilingual exper-tise.
We are optimistic that with further refinementsand research, monolingual translation crowdsourc-ing will make it possible to harness the vast num-ber of technologically connected people who wantto help in some way when disaster strikes.7 AcknowledgmentsThis research is supported by NSF contract#BCS0941455 and by a Google Research Award.ReferencesOlivia Buzek, Philip Resnik, and Benjamin B. Bederson.2010.
Error driven paraphrase annotation using me-chanical turk.
In NAACL 2010 Workshop on CreatingSpeech and Text Language Data With Amazon?s Me-chanical Turk.Chris Callison-Burch, Colin Bannard, , and JoshSchroeder.
2004.
Improving statistical translationthrough editing.
In Workshop of the European Asso-ciation for Machine Translation.Marianne Dabbadie, Anthony Hartley, Margaret King,Keith J. Miller, Widad Mustafa El Hadi, AndreiPopescu-Belis, Florence Reeder, and Michelle Vanni.2002.
A hands-on study of the reliability and coher-ence of evaluation metrics.
In Workshop at the LREC2002 Conference, page 8.
Citeseer.Chang Hu, Benjamin B. Bederson, and Philip Resnik.2010.
Translation by iterative collaboration betweenmonolingual users.
In Proceedings of Graphics Inter-face 2010 on Proceedings of Graphics Interface 2010,pages 39?46, Ottawa, Ontario, Canada.
Canadian In-formation Processing Society.Chang Hu, Ben Bederson, Philip Resnik, and Yakov Kro-nrod.
2011.
Monotrans2: A new human computationsystem to support monolingual translation.
In HumanFactors in Computing Systems (CHI 2011), Vancou-ver, Canada, May.
ACM, ACM.Chang Hu.
2009.
Collaborative translation by monolin-gual users.
In Proceedings of the 27th internationalconference extended abstracts on Human factors incomputing systems, pages 3105?3108, Boston, MA,USA.
ACM.Rebecca Hwa, Philip Resnik, Amy Weinberg, and OkanKolak.
2002.
Evaluating translational correspon-dence using annotation projection.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, pages 392?399, Philadelphia, Penn-sylvania.
Association for Computational Linguistics.Daisuke Morita and Toru Ishida.
2009.
Designing pro-tocols for collaborative translation.
In PRIMA ?09:Proceedings of the 12th International Conference onPrinciples of Practice in Multi-Agent Systems, pages17?32, Berlin, Heidelberg.
Springer-Verlag.Robert Munro.
2010.
Crowdsourced translation foremergency response in haiti: the global collaborationof local knowledge.
In AMTA Workshop on Collabo-rative Crowdsourcing for Translation.
Keynote.Alexander J. Quinn, Bederson, and Benjamin B. Beder-son.
2011.
Human computation: A survey and tax-onomy of a growing field.
In Human Factors in Com-puting Systems (CHI 2011), Vancouver, Canada, May.ACM, ACM.Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,Alexander J. Quinn, and Benjamin B. Bederson.
2010.Improving translation via targeted paraphrasing.
InEMNLP.David Yarowsky, Grace Ngai, and Richard Wicentowski.2001.
Inducing multilingual text analysis tools viarobust projection across aligned corpora.
In HLT?01: Proceedings of the first international conferenceon Human language technology research, pages 1?8,Morristown, NJ, USA.
Association for ComputationalLinguistics.404
