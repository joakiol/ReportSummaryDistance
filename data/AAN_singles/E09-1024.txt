Proceedings of the 12th Conference of the European Chapter of the ACL, pages 202?210,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsRe-Ranking Models For Spoken Language UnderstandingMarco DinarelliUniversity of TrentoItalydinarelli@disi.unitn.itAlessandro MoschittiUniversity of TrentoItalymoschitti@disi.unitn.itGiuseppe RiccardiUniversity of TrentoItalyriccardi@disi.unitn.itAbstractSpoken Language Understanding aims atmapping a natural language spoken sen-tence into a semantic representation.
Inthe last decade two main approaches havebeen pursued: generative and discrimi-native models.
The former is more ro-bust to overfitting whereas the latter ismore robust to many irrelevant features.Additionally, the way in which these ap-proaches encode prior knowledge is verydifferent and their relative performancechanges based on the task.
In this pa-per we describe a machine learning frame-work where both models are used: a gen-erative model produces a list of ranked hy-potheses whereas a discriminative modelbased on structure kernels and SupportVector Machines, re-ranks such list.
Wetested our approach on the MEDIA cor-pus (human-machine dialogs) and on anew corpus (human-machine and human-human dialogs) produced in the Euro-pean LUNA project.
The results show alarge improvement on the state-of-the-artin concept segmentation and labeling.1 IntroductionIn Spoken Dialog Systems, the Language Under-standing module performs the task of translatinga spoken sentence into its meaning representationbased on semantic constituents.
These are theunits for meaning representation and are often re-ferred to as concepts.
Concepts are instantiated bysequences of words, therefore a Spoken LanguageUnderstanding (SLU) module finds the associationbetween words and concepts.In the last decade two major approaches havebeen proposed to find this correlation: (i) gener-ative models, whose parameters refer to the jointprobability of concepts and constituents; and (ii)discriminative models, which learn a classifica-tion function to map words into concepts basedon geometric and statistical properties.
An ex-ample of generative model is the Hidden VectorState model (HVS) (He and Young, 2005).
Thisapproach extends the discrete Markov model en-coding the context of each state as a vector.
Statetransitions are performed as stack shift operationsfollowed by a push of a preterminal semantic cat-egory label.
In this way the model can capture se-mantic hierarchical structures without the use oftree-structured data.
Another simpler but effec-tive generative model is the one based on FiniteState Transducers.
It performs SLU as a transla-tion process from words to concepts using FiniteState Transducers (FST).
An example of discrim-inative model used for SLU is the one based onSupport Vector Machines (SVMs) (Vapnik, 1995),as shown in (Raymond and Riccardi, 2007).
Inthis approach, data are mapped into a vector spaceand SLU is performed as a classification problemusing Maximal Margin Classifiers (Shawe-Taylorand Cristianini, 2004).Generative models have the advantage to bemore robust to overfitting on training data, whilediscriminative models are more robust to irrele-vant features.
Both approaches, used separately,have shown a good performance (Raymond andRiccardi, 2007), but they have very different char-acteristics and the way they encode prior knowl-edge is very different, thus designing models ableto take into account characteristics of both ap-proaches are particularly promising.In this paper we propose a method for SLUbased on generative and discriminative models:the former uses FSTs to generate a list of SLU hy-potheses, which are re-ranked by SVMs.
Theseexploit all possible word/concept subsequences(with gaps) of the spoken sentence as features (i.e.all possible n-grams).
Gaps allow for the encod-202ing of long distance dependencies between wordsin relatively small n-grams.
Given the huge sizeof this feature space, we adopted kernel methodsand in particular sequence kernels (Shawe-Taylorand Cristianini, 2004) and tree kernels (Raymondand Riccardi, 2007; Moschitti and Bejan, 2004;Moschitti, 2006) to implicitly encode n-grams andother structural information in SVMs.We experimented with different approaches fortraining the discriminative models and two dif-ferent corpora: the well-known MEDIA corpus(Bonneau-Maynard et al, 2005) and a new corpusacquired in the European project LUNA1 (Ray-mond et al, 2007).
The results show a greatimprovement with respect to both the FST-basedmodel and the SVM model alone, which are thecurrent state-of-the-art for concept classificationon such corpora.
The rest of the paper is orga-nized as follows: Sections 2 and 3 show the gener-ative and discriminative models, respectively.
Theexperiments and results are reported in Section 4whereas the conclusions are drawn in Section 5.2 Generative approach for conceptclassificationIn the context of Spoken Language Understanding(SLU), concept classification is the task of asso-ciating the best sequence of concepts to a givensentence, i.e.
word sequence.
A concept is a classcontaining all the words carrying out the same se-mantic meaning with respect to the application do-main.
In SLU, concepts are used as semantic unitsand are represented with concept tags.
The associ-ation between words and concepts is learned froman annotated corpus.The Generative model used in our work for con-cept classification is the same used in (Raymondand Riccardi, 2007).
Given a sequence of wordsas input, a translation process based on FST isperformed to output a sequence of concept tags.The translation process involves three steps: (1)the mapping of words into classes (2) the mappingof classes into concepts and (3) the selection of thebest concept sequence.The first step is used to improve the generaliza-tion power of the model.
The word classes at thislevel can be both domain-dependent, e.g.
?Hotel?in MEDIA or ?Software?
in the LUNA corpus, ordomain-independent, e.g.
numbers, dates, months1Contract n. 33549etc.
The class of a word not belonging to any classis the word itself.In the second step, classes are mapped into con-cepts.
The mapping is not one-to-one: a classmay be associated with more than one concept, i.e.more than one SLU hypothesis can be generated.In the third step, the best or the m-best hy-potheses are selected among those produced in theprevious step.
They are chosen according to themaximum probability evaluated by the ConceptualLanguage Model, described in the next section.2.1 Stochastic Conceptual Language Model(SCLM)An SCLM is an n-gram language model built onsemantic tags.
Using the same notation proposedin (Moschitti et al, 2007) and (Raymond and Ric-cardi, 2007), our SCLM trains joint probabilityP (W,C) of word and concept sequences from anannotated corpus:P (W,C) =k?i=1P (wi, ci|hi),where W = w1..wk, C = c1..ck andhi = wi?1ci?1..w1c1.
Since we use a 3-gramconceptual language model, the history hi is{wi?1ci?1, wi?2ci?2}.All the steps of the translation process describedhere and above are implemented as Finite StateTransducers (FST) using the AT&T FSM/GRMtools and the SRILM (Stolcke, 2002) tools.
Inparticular the SCLM is trained using SRILM toolsand then converted to an FST.
This allows the useof a wide set of stochastic language models (bothback-off and interpolated models with several dis-counting techniques like Good-Turing, Witten-Bell, Natural, Kneser-Ney, Unchanged Kneser-Ney etc).
We represent the combination of all thetranslation steps as a transducer ?SLU (Raymondand Riccardi, 2007) in terms of FST operations:?SLU = ?W ?
?W2C ?
?SLM ,where ?W is the transducer representation of theinput sentence, ?W2C is the transducer mappingwords to classes and ?SLM is the Semantic Lan-guage Model (SLM) described above.
The bestSLU hypothesis is given byC = projectC(bestpath1(?SLU )),where bestpathn (in this case n is 1 for the 1-besthypothesis) performs a Viterbi search on the FST203and outputs the n-best hypotheses and projectCperforms a projection of the FST on the output la-bels, in this case the concepts.2.2 Generation of m-best concept labelingUsing the FSTs described above, we can generatem best hypotheses ranked by the joint probabilityof the SCLM.After an analysis of the m-best hypotheses ofour SLU model, we noticed that many times thehypothesis ranked first by the SCLM is not theclosest to the correct concept sequence, i.e.
its er-ror rate using the Levenshtein alignment with themanual annotation of the corpus is not the low-est among the m hypotheses.
This means thatre-ranking the m-best hypotheses in a convenientway could improve the SLU performance.
Thebest choice in this case is a discriminative model,since it allows for the use of informative features,which, in turn, can model easily feature dependen-cies (also if they are infrequent in the training set).3 Discriminative re-rankingOur discriminative re-ranking is based on SVMsor a perceptron trained with pairs of conceptuallyannotated sentences.
The classifiers learn to selectwhich annotation has an error rate lower than theothers so that the m-best annotations can be sortedbased on their correctness.3.1 SVMs and Kernel MethodsKernel Methods refer to a large class of learningalgorithms based on inner product vector spaces,among which Support Vector Machines (SVMs)are one of the most well known algorithms.
SVMsand perceptron learn a hyperplane H(~x) = ~w~x +b = 0, where ~x is the feature vector represen-tation of a classifying object o, ~w ?
Rn (avector space) and b ?
R are parameters (Vap-nik, 1995).
The classifying object o is mappedinto ~x by a feature function ?.
The kernel trickallows us to rewrite the decision hyperplane as?i=1..l yi?i?(oi)?
(o) + b = 0, where yi is equalto 1 for positive and -1 for negative examples,?i ?
R+, oi?i ?
{1..l} are the training instancesand the product K(oi, o) = ??(oi)?(o)?
is the ker-nel function associated with the mapping ?.
Notethat we do not need to apply the mapping ?, wecan use K(oi, o) directly (Shawe-Taylor and Cris-tianini, 2004).
For example, next section shows akernel function that counts the number of word se-quences in common between two sentences, in thespace of n-grams (for any n).3.2 String KernelsThe String Kernels that we consider count thenumber of substrings containing gaps shared bytwo sequences, i.e.
some of the symbols of theoriginal string are skipped.
Gaps modify theweight associated with the target substrings asshown in the following.Let ?
be a finite alphabet, ??
= ?
?n=0 ?n is theset of all strings.
Given a string s ?
?
?, |s| denotesthe length of the strings and si its compoundingsymbols, i.e s = s1..s|s|, whereas s[i : j] selectsthe substring sisi+1..sj?1sj from the i-th to thej-th character.
u is a subsequence of s if thereis a sequence of indexes ~I = (i1, ..., i|u|), with1 ?
i1 < ... < i|u| ?
|s|, such that u = si1 ..si|u|or u = s[~I] for short.
d(~I) is the distance betweenthe first and last character of the subsequence u ins, i.e.
d(~I) = i|u| ?
i1 + 1.
Finally, given s1, s2?
?
?, s1s2 indicates their concatenation.The set of all substrings of a text corpus forms afeature space denoted by F = {u1, u2, ..} ?
?
?.To map a string s in R?
space, we can use thefollowing functions: ?u(s) =P~I:u=s[~I] ?d(~I) forsome ?
?
1.
These functions count the num-ber of occurrences of u in the string s and assignthem a weight ?d(~I) proportional to their lengths.Hence, the inner product of the feature vectors fortwo strings s1 and s2 returns the sum of all com-mon subsequences weighted according to theirfrequency of occurrences and lengths, i.e.SK(s1, s2) =Xu???
?u(s1) ?
?u(s2) =Xu??
?X~I1:u=s1[~I1]?d( ~I1)X~I2:u=s2[~I2]?d( ~I2) =Xu??
?X~I1:u=s1[~I1]X~I2:u=s2[~I2]?d( ~I1)+d( ~I2),where d(.)
counts the number of characters in thesubstrings as well as the gaps that were skipped inthe original string.
It is worth noting that:(a) longer subsequences receive lower weights;(b) some characters can be omitted, i.e.
gaps;and(c) gaps determine a weight since the exponentof ?
is the number of characters and gaps be-tween the first and last character.204Characters in the sequences can be substitutedwith any set of symbols.
In our study we pre-ferred to use words so that we can obtain wordsequences.
For example, given the sentence: Howmay I help you ?
sample substrings, extracted bythe Sequence Kernel (SK), are: How help you ?,How help ?, help you, may help you, etc.3.3 Tree kernelsTree kernels represent trees in terms of their sub-structures (fragments).
The kernel function de-tects if a tree subpart (common to both trees) be-longs to the feature space that we intend to gen-erate.
For such purpose, the desired fragmentsneed to be described.
We consider two importantcharacterizations: the syntactic tree (STF) and thepartial tree (PTF) fragments.3.3.1 Tree Fragment TypesAn STF is a general subtree whose leaves can benon-terminal symbols.
For example, Figure 1(a)shows 10 STFs (out of 17) of the subtree rooted inVP (of the left tree).
The STFs satisfy the con-straint that grammatical rules cannot be broken.For example, [VP [V NP]] is an STF, whichhas two non-terminal symbols, V and NP, as leaveswhereas [VP [V]] is not an STF.
If we relaxthe constraint over the STFs, we obtain more gen-eral substructures called partial trees fragments(PTFs).
These can be generated by the applicationof partial production rules of the grammar, con-sequently [VP [V]] and [VP [NP]] are validPTFs.
Figure 1(b) shows that the number of PTFsderived from the same tree as before is still higher(i.e.
30 PTs).3.4 Counting Shared SubTreesThe main idea of tree kernels is to compute thenumber of common substructures between twotrees T1 and T2 without explicitly considering thewhole fragment space.
To evaluate the above ker-nels between two T1 and T2, we need to define aset F = {f1, f2, .
.
.
, f|F|}, i.e.
a tree fragmentspace and an indicator function Ii(n), equal to 1if the target fi is rooted at node n and equal to 0otherwise.
A tree-kernel function over T1 and T2is TK(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2),where NT1 and NT2 are the sets of the T1?sand T2?s nodes, respectively and ?
(n1, n2) =?|F|i=1 Ii(n1)Ii(n2).
The latter is equal to the num-ber of common fragments rooted in the n1 andn2 nodes.
In the following sections we report theequation for the efficient evaluation of ?
for STand PT kernels.3.5 Syntactic Tree Kernels (STK)The ?
function depends on the type of fragmentsthat we consider as basic features.
For example,to evaluate the fragments of type STF, it can bedefined as:1. if the productions at n1 and n2 are differentthen ?
(n1, n2) = 0;2. if the productions at n1 and n2 are thesame, and n1 and n2 have only leaf children(i.e.
they are pre-terminals symbols) then?
(n1, n2) = 1;3. if the productions at n1 and n2 are the same,and n1 and n2 are not pre-terminals then?
(n1, n2) =nc(n1)?j=1(?
+ ?
(cjn1 , cjn2)) (1)where ?
?
{0, 1}, nc(n1) is the number of chil-dren of n1 and cjn is the j-th child of the noden.
Note that, since the productions are the same,nc(n1) = nc(n2).
?
(n1, n2) evaluates the num-ber of STFs common to n1 and n2 as proved in(Collins and Duffy, 2002).Moreover, a decay factor ?
can be added bymodifying steps (2) and (3) as follows2:2.
?
(n1, n2) = ?,3.
?
(n1, n2) = ?
?nc(n1)j=1 (?
+ ?
(cjn1 , cjn2)).The computational complexity of Eq.
1 isO(|NT1 | ?
|NT2 |) but as shown in (Moschitti,2006), the average running time tends to be lin-ear, i.e.
O(|NT1 | + |NT2 |), for natural languagesyntactic trees.3.6 The Partial Tree Kernel (PTK)PTFs have been defined in (Moschitti, 2006).Their computation is carried out by the following?
function:1. if the node labels of n1 and n2 are differentthen ?
(n1, n2) = 0;2. else ?
(n1, n2) =1+?~I1,~I2,l(~I1)=l(~I2)?l(~I1)j=1 ?
(cn1(~I1j), cn2(~I2j))2To have a similarity score between 0 and 1, we also applythe normalization in the kernel space, i.e.:K?
(T1, T2) = TK(T1 ,T2)?TK(T1 ,T1)?TK(T2 ,T2) .205NPD NacatNPD NNPD NaNPD NNPD NVPVbroughtacatcatNPD NVPVacatNPD NVPVNcatDaVbroughtNMary?
(a) Syntactic Tree fragments (STF)NPD NVPVbroughtacatNPD NVPVacatNPD NVPacatNPD NVPaNPDVPaNPDVPNPNVPNPNNP NPD N DNP?VP(b) Partial Tree fragments (PTF)Figure 1: Examples of different classes of tree fragments.where ~I1 = ?h1, h2, h3, ..?
and ~I2 =?k1, k2, k3, ..?
are index sequences associated withthe ordered child sequences cn1 of n1 and cn2 ofn2, respectively, ~I1j and ~I2j point to the j-th childin the corresponding sequence, and, again, l(?)
re-turns the sequence length, i.e.
the number of chil-dren.Furthermore, we add two decay factors: ?
forthe depth of the tree and ?
for the length of thechild subsequences with respect to the original se-quence, i.e.
we account for gaps.
It follows that?
(n1, n2) =?(?2+?~I1,~I2,l(~I1)=l(~I2)?d(~I1)+d(~I2)l(~I1)?j=1?
(cn1(~I1j), cn2(~I2j))),(2)where d(~I1) = ~I1l(~I1) ?
~I11 and d(~I2) = ~I2l(~I2) ?~I21.
This way, we penalize both larger trees andchild subsequences with gaps.
Eq.
2 is more gen-eral than Eq.
1.
Indeed, if we only consider thecontribution of the longest child sequence fromnode pairs that have the same children, we imple-ment the STK kernel.3.7 Re-ranking models using sequencesThe FST generates the m most likely concept an-notations.
These are used to build annotationpairs,?si, sj?, which are positive instances if sihas a lower concept annotation error than sj , withrespect to the manual annotation in the corpus.Thus, a trained binary classifier can decide if siis more accurate than sj .
Each candidate anno-tation si is described by a word sequence whereeach word is followed by its concept annotation.For example, given the sentence:ho (I have) un (a) problema (problem) con(with) la (the) scheda di rete (network card) ora(now)a pair of annotations?si, sj?could besi: ho NULL un NULL problema PROBLEM-B conNULL la NULL scheda HW-B di HW-I rete HW-I oraRELATIVETIME-Bsj: ho NULL un NULL problema ACTION-B conNULL la NULL scheda HW-B di HW-B rete HW-B oraRELATIVETIME-Bwhere NULL, ACTION, RELATIVETIME,and HW are the assigned concepts whereas B andI are the usual begin and internal tags for conceptsubparts.
The second annotation is less accuratethan the first since problema is annotated as an ac-tion and ?scheda di rete?
is split in three differentconcepts.Given the above data, the sequence kernelis used to evaluate the number of common n-grams between si and sj .
Since the string ker-nel skips some elements of the target sequences,the counted n-grams include: concept sequences,word sequences and any subsequence of wordsand concepts at any distance in the sentence.Such counts are used in our re-ranking functionas follows: let ei be the pair?s1i , s2i?we evaluatethe kernel:KR(e1, e2) = SK(s11, s12) + SK(s21, s22) (3)?
SK(s11, s22)?
SK(s21, s12)This schema, consisting in summing four differ-ent kernels, has been already applied in (Collinsand Duffy, 2002) for syntactic parsing re-ranking,where the basic kernel was a tree kernel instead ofSK and in (Moschitti et al, 2006), where, to re-rank Semantic Role Labeling annotations, a treekernel was used on a semantic tree similar to theone introduced in the next section.3.8 Re-ranking models using treesSince the aim in concept annotation re-ranking isto exploit innovative and effective source of infor-mation, we can use the power of tree kernels togenerate correlation between concepts and wordstructures.Fig.
2 describes the structural association be-tween the concept and the word level.
This kind oftrees allows us to engineer new kernels and con-sequently new features (Moschitti et al, 2008),206Figure 2: An example of the semantic tree used for STK or PTKCorpus Train set Test setLUNA words concepts words conceptsDialogs WOZ 183 67Dialogs HH 180 -Turns WOZ 1.019 373Turns HH 6.999 -Tokens WOZ 8.512 2.887 2.888 984Tokens WOZ 62.639 17.423 - -Vocab.
WOZ 1.172 34 - -Vocab.
HH 4.692 49 - -OOV rate - - 3.2% 0.1%Table 1: Statistics on the LUNA corpusCorpus Train set Test setMedia words concepts words conceptsTurns 12,922 3,518# of tokens 94,912 43,078 26,676 12,022Vocabulary 5,307 80 - -OOV rate - - 0.01% 0.0%Table 2: Statistics on the MEDIA corpuse.g.
their subparts extracted by STK or PTK, likethe tree fragments in figures 1(a) and 1(b).
Thesecan be used in SVMs to learn the classification ofwords in concepts.More specifically, in our approach, we use treefragments to establish the order of correctnessbetween two alternative annotations.
Therefore,given two trees associated with two annotations, are-ranker based on tree kernel, KR, can be builtin the same way of the sequence-based kernel bysubstituting SK in Eq.
3 with STK or PTK.4 ExperimentsIn this section, we describe the corpora, param-eters, models and results of our experiments ofword chunking and concept classification.
Ourbaseline relates to the error rate of systems basedon only FST and SVMs.
The re-ranking modelsare built on the FST output.
Different ways ofproducing training data for the re-ranking modelsdetermine different results.4.1 CorporaWe used two different speech corpora:The corpus LUNA, produced in the homony-mous European project is the first Italian corpusof spontaneous speech on spoken dialog: it isbased on the help-desk conversation in the domainof software/hardware repairing (Raymond et al,2007).
The data are organized in transcriptionsand annotations of speech based on a new multi-level protocol.
Data acquisition is still in progress.Currently, 250 dialogs acquired with a WOZ ap-proach and 180 Human-Human (HH) dialogs areavailable.
Statistics on LUNA corpus are reportedin Table 1.The corpus MEDIA was collected withinthe French project MEDIA-EVALDA (Bonneau-Maynard et al, 2005) for development and evalu-ation of spoken understanding models and linguis-tic studies.
The corpus is composed of 1257 di-alogs, from 250 different speakers, acquired witha Wizard of Oz (WOZ) approach in the contextof hotel room reservations and tourist information.Statistics on transcribed and conceptually anno-tated data are reported in Table 2.4.2 Experimental setupWe defined two different training sets in theLUNA corpus: one using only the WOZ train-ing dialogs and one merging them with the HHdialogs.
Given the small size of LUNA corpus, wedid not carried out parameterization on a develop-ment set but we used default or a priori parameters.We experimented with LUNA WOZ and six re-rankers obtained with the combination of SVMsand perceptron (PCT) with three different typesof kernels: Syntactic Tree Kernel (STK), PartialTree kernels (PTK) and the String Kernel (SK) de-scribed in Section 3.3.Given the high number and the cost of these ex-periments, we ran only one model, i.e.
the one207Corpus LUNA WOZ+HH MEDIAApproach (STK) MT ST MTFST 18.2 18.2 12.6SVM 23.4 23.4 13.7RR-A 15.6 17.0 11.6RR-B 16.2 16.5 11.8RR-C 16.1 16.4 11.7Table 3: Results of experiments (CER) using FSTand SVMs with the Sytntactic Tree Kernel (STK)on two different corpora: LUNA WOZ + HH, andMEDIA.based on SVMs and STK3 , on the largest datasets,i.e.
WOZ merged with HH dialogs and Media.We trained all the SCLMs used in our experimentswith the SRILM toolkit (Stolcke, 2002) and weused an interpolated model for probability esti-mation with the Kneser-Ney discount (Chen andGoodman, 1998).
We then converted the model inan FST as described in Section 2.1.The model used to obtain the SVM baselinefor concept classification was trained using Yam-CHA (Kudo and Matsumoto, 2001).
For the re-ranking models based on structure kernels, SVMsor perceptron, we used the SVM-Light-TK toolkit(available at dit.unitn.it/moschitti).
For ?
(see Sec-tion 3.2), cost-factor and trade-off parameters, weused, 0.4, 1 and 1, respectively.4.3 Training approachesThe FST model generates the m-best annotations,i.e.
the data used to train the re-ranker basedon SVMs and perceptron.
Different training ap-proaches can be carried out based on the use of thecorpus and the method to generate the m-best.
Weapply two different methods for training: Mono-lithic Training and Split Training.In the former, FSTs are learned with the wholetraining set.
The m-best hypotheses generated bysuch models are then used to train the re-rankerclassifier.
In Split Training, the training data aredivided in two parts to avoid bias in the FST gen-eration step.
More in detail, we train FSTs on part1 and generate the m-best hypotheses using part 2.Then, we re-apply these procedures inverting part1 with part 2.
Finally, we train the re-ranker on themerged m-best data.
At the classification time, wegenerate the m-best of the test set using the FSTtrained on all training data.3The number of parameters, models and training ap-proaches make the exhaustive experimentation expensive interms of processing time, which approximately requires 2 or3 months.Monolithic TrainingWOZ SVM PCTSTK PTK SK STK PTK SKRR-A 18.5 19.3 19.1 24.2 28.3 23.3RR-B 18.5 19.3 19.0 29.4 23.7 20.3RR-C 18.5 19.3 19.1 31.5 30.0 20.2Table 4: Results of experiments, in terms of Con-cept Error Rate (CER), on the LUNA WOZ corpususing Monolithic Training approach.
The baselinewith FST and SVMs used separately are 23.2%and 26.7% respectively.Split TrainingWOZ SVM PCTSTK PTK SK STK PTK SKRR-A 20.0 18.0 16.1 28.4 29.8 27.8RR-B 19.0 19.0 19.0 26.3 30.0 25.6RR-C 19.0 18.4 16.6 27.1 26.2 30.3Table 5: Results of experiments, in terms of Con-cept Error Rate (CER), on the LUNA WOZ cor-pus using Split Training approach.
The baselinewith FST and SVMs used separately are 23.2%and 26.7% respectively.Regarding the generation of the training in-stances?si, sj?, we set m to 10 and we choose oneof the 10-best hypotheses as the second element ofthe pair, sj , thus generating 10 different pairs.The first element instead can be selected accord-ing to three different approaches:(A): si is the manual annotation taken from thecorpus;(B) si is the most accurate annotation, in termsof the edit distance from the manual annotation,among the 10-best hypotheses of the FST model;(C) as above but si is selected among the 100-best hypotheses.
The pairs are also inverted togenerate negative examples.4.4 Re-ranking resultsAll the results of our experiments, expressed interms of concept error rate (CER), are reported inTable 3, 4 and 5.In Table 3, the corpora, i.e.
LUNA (WOZ+HH)and Media, and the training approaches, i.e.Monolithic Training (MT) and Split Training (ST),are reported in the first and second row.
Column1 shows the concept classification model used, i.e.the baselines FST and SVMs, and the re-rankingmodels (RR) applied to FST.
A, B and C referto the three approaches for generating training in-stances described above.
As already mentionedfor these large datasets, SVMs only use STK.208We note that our re-rankers relevantly improveour baselines, i.e.
the FST and SVM concept clas-sifiers on both corpora.
For example, SVM re-ranker using STK, MT and RR-A improves FSTconcept classifier of 23.2-15.6 = 7.6 points.Moreover, the monolithic training seems themost appropriate to train the re-rankers whereasapproach A is the best in producing training in-stances for the re-rankers.
This is not surprisingsince method A considers the manual annotationas a referent gold standard and it always allowscomparing candidate annotations with the perfectone.Tables 4 and 5 have a similar structure of Ta-ble 3 but they only show experiments on LUNAWOZ corpus with respect to the monolithic andsplit training approach, respectively.
In these ta-bles, we also report the result for SVMs and per-ceptron (PCT) using STK, PTK and SK.
We notethat:First, the small size of WOZ training set (only1,019 turns) impacts on the accuracy of the sys-tems, e.g.
FST and SVMs, which achieved aCER of 18.2% and 23.4%, respectively, using alsoHH dialogs, with only the WOZ data, they obtain23.2% and 26.7%, respectively.Second, the perceptron algorithm appears to beineffective for re-ranking.
This is mainly due tothe reduced size of the WOZ data, which clearlyprevents an on line algorithm like PCT to ade-quately refine its model by observing many exam-ples4.Third, the kernels which produce higher numberof substructures, i.e.
PTK and SK, improves thekernel less rich in terms of features, i.e.
STK.
Forexample, using split training and approach A, STKis improved by 20.0-16.1=3.9.
This is an interest-ing result since it shows that (a) richer structuresdo produce better ranking models and (b) kernelmethods give a remarkable help in feature design.Next, although the training data is small, the re-rankers based on kernels appear to be very effec-tive.
This may also alleviate the burden of anno-tating a lot of data.Finally, the experiments of MEDIA show a notso high improvement using re-rankers.
This is dueto: (a) the baseline, i.e.
the FST model is veryaccurate since MEDIA is a large corpus thus there-ranker can only ?correct?
small number of er-rors; and (b) we could only experiment with the4We use only one iteration of the algorithm.less expensive but also less accurate models, i.e.monolithic training and STK.Media also offers the possibility to comparewith the state-of-the-art, which our re-rankersseem to improve.
However, we need to considerthat many Media corpus versions exist and thismakes such comparisons not completely reliable.Future work on the paper research line appearsto be very interesting: the assessment of our bestmodels on Media and WOZ+HH as well as othercorpora is required.
More importantly, the struc-tures that we have proposed for re-ranking arejust two of the many possibilities to encode bothword/concept statistical distributions and linguis-tic knowledge encoded in syntactic/semantic parsetrees.5 ConclusionsIn this paper, we propose discriminative re-ranking of concept annotation to capitalize fromthe benefits of generative and discriminative ap-proaches.
Our generative approach is the state-of-the-art in concept classification since we usedthe same FST model used in (Raymond and Ric-cardi, 2007).
We could improve it by 1% pointin MEDIA and 7.6 points (until 30% of relativeimprovement) on LUNA, where the more limitedavailability of annotated data leaves a larger roomfor improvement.It should be noted that to design the re-rankingmodel, we only used two different structures,i.e.
one sequence and one tree.
Kernel meth-ods show that combinations of feature vectors, se-quence kernels and other structural kernels, e.g.on shallow or deep syntactic parse trees, appearto be a promising research line (Moschitti, 2008).Also, the approach used in (Zanzotto and Mos-chitti, 2006) to define cross pair relations may beexploited to carry out a more effective pair re-ranking.
Finally, the experimentation with auto-matic speech transcriptions is interesting to test therobustness of our models to transcription errors.AcknowledgmentsThis work has been partially supported by the Eu-ropean Commission - LUNA project, contract n.33549.209ReferencesH.
Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,and D. Mostefa.
2005.
Semantic annotation of thefrench media dialog corpus.
In Proceedings of In-terspeech2005, Lisbon, Portugal.S.
F. Chen and J. Goodman.
1998.
An empirical studyof smoothing techniques for language modeling.
InTechnical Report of Computer Science Group, Har-vard, USA.M.
Collins and N. Duffy.
2002.
New Ranking Al-gorithms for Parsing and Tagging: Kernels overDiscrete structures, and the voted perceptron.
InACL02, pages 263?270.Y.
He and S. Young.
2005.
Semantic processing us-ing the hidden vector state model.
Computer Speechand Language, 19:85?106.T.
Kudo and Y. Matsumoto.
2001.
Chunkingwith support vector machines.
In Proceedings ofNAACL2001, Pittsburg, USA.A.
Moschitti and C. Bejan.
2004.
A semantic ker-nel for predicate argument classification.
In CoNLL-2004, Boston, MA, USA.A.
Moschitti, D. Pighin, and R. Basili.
2006.
Seman-tic role labeling via tree kernel joint inference.
InProceedings of CoNLL-X, New York City.A.
Moschitti, G. Riccardi, and C. Raymond.
2007.Spoken language understanding with kernels forsyntactic/semantic structures.
In Proceedings ofASRU2007, Kyoto, Japan.A.
Moschitti, D. Pighin, and R. Basili.
2008.
Treekernels for semantic role labeling.
ComputationalLinguistics, 34(2):193?224.A.
Moschitti.
2006.
Efficient Convolution Kernelsfor Dependency and Constituent Syntactic Trees.
InProceedings of ECML 2006, pages 318?329, Berlin,Germany.A.
Moschitti.
2008.
Kernel methods, syntax and se-mantics for relational text categorization.
In CIKM?08: Proceeding of the 17th ACM conference on In-formation and knowledge management, pages 253?262, New York, NY, USA.
ACM.C.
Raymond and G. Riccardi.
2007.
Generative anddiscriminative algorithms for spoken language un-derstanding.
In Proceedings of Interspeech2007,Antwerp,Belgium.C.
Raymond, G. Riccardi, K. J. Rodrigez, and J. Wis-niewska.
2007.
The luna corpus: an annotationscheme for a multi-domain multi-lingual dialoguecorpus.
In Proceedings of Decalog2007, Trento,Italy.J.
Shawe-Taylor and N. Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge Univer-sity Press.A.
Stolcke.
2002.
Srilm: an extensible language mod-eling toolkit.
In Proceedings of SLP2002, Denver,USA.V.
Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer.F.
M. Zanzotto and A. Moschitti.
2006.
Automaticlearning of textual entailments with cross-pair simi-larities.
In Proceedings of the 21st Coling and 44thACL, pages 401?408, Sydney, Australia, July.210
