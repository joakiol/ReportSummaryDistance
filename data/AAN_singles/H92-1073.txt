The Design for the Wall Street Journal-basedCSR Corpus*Douglas B. PaulLincoln Laboratory, MITLexington, Ma.
02173and Janet M. BakerDragon Systems, Inc.320 Nevada St.Newton, Ma.
02160ABSTRACTThe DARPA Spoken Language System (SLS) communityhas long taken a leadership osition in designing, imple-menting, and globally distributing significant speech corporawidely used for advancing speech recognition research.
TheWall Street Journal (WSJ) CSR Corpus described here isthe newest addition to this valuable set of resources.
Incontrast o previous corpora, the WSJ corpus will provideDARPA its first general-purpose English, large vocabulary,natural anguage, high perplexity, corpus containing signif-icant quantities of both speech data (400 hrs.)
and textdata (47M words), thereby providing a means to integratespeech recognition and natural anguage processing in ap-plication domains with high potential practical value.
Thispaper presents the motivating oals, acoustic data design,text processing steps, lexicons, and testing paradigms incor-porated into the multi-faceted WSJ CSR Corpus.INTRODUCTIONAs spoken language technology progresses and goals expand,progressively larger, and more challenging corpora need tobe created to support advanced research.
The SLS DARPA1994 goals are ambitious, focusing on cooperative speak-ers, generating goal-directed, spontaneous continuous speech,in speaker-adaptive and speaker-independent modes, for ex-pandable vocabularies (5000 or more words active), moder-ate perplexity (100-200), with integrated speech and naturallanguage processing, for speakers in a moderate noise envi-ronment, using multiple types of microphones, engaged incommand/database nd dictation applications.
In contrastto typical command/database pplications, dictation (i.e.
in-teractive speech-driven word processing) tasks focus on coop-erative speakers (e.g.
speaker dependent/adaptlve sustainedusage) who generate continuous peech (usually in a some-what careful fashion to facilitate accurate transcription) ver-balizing their words and sentence punctuation.
The existingResource Management\[15\] andsubsequent Air Travel Infor-mation System\[16\] corpora target specific database inquirytasks, characterized by medium vocabularies (<1500 words)with language model perplexities ranging from 9 to 60.
TheWSJ corpus described here is designed to advance CSR tech-nology and support he 1994 SLS research goals.
A similarread speech corpus in the French language has been success-*This work was sponsored by the Defense Advanced ResearchProjects Agency.
The views expressed are those of the authors anddo not reflect he official policy or position of the U.S. Government.fully completed using text from the newspaper Le Monde\[5\].Commencing with serious contractor concerns regarding suit-able CSR corpora\[12\] starting in the mid 1980's, the DARPASLS Coordinating Committee started considering new cor-pora requirements in early 1990, with the subsequent for-mation of the CSR Corpus Committee, culminating in theWSJ Corpus design.
The CSR Corpus Committee mem-bers include J.M.
Baker (Dragon, chair), F. Kubala (BBN),D. Pallett (NIST), D. Paul (LL), M. Phillips (MIT), M.Picheny (IBM), R. Rajasekran (TI), B. Weide (CMU), M.Weintraub (SRI), and 3.
Wilpon (ATT).
A survey takenof the DARPA contractors for CSR research interests dis-closed highly diverse, often opposing views of research in-terest.
All contractors, however, cited a common inter-est in pursuing research on "Domain-independent Acous-tic Models", "Domain-independent Language Models", and"Speaker-adaptation".The outcome of lively meetings and discussions resultedin the definition and preliminary authorization of a ma-jor (>400 hrs.)
corpus with materials based primarily onWSJ material (backed by WSJ text from 1987-89 providedby the ACL/DCI\[9\] to enable statistical language modeling)and supplemented by other material (spontaneous dictation,Hansard, etc., shown in Table 1).
This corpus will providea uniquely rich resource, in a carefully crafted structure de-signed to elicit a highly productive flow of diagnostic researchinformation with an array of comparative t st paradigms.Although this WSJ corpus is large relative to many otheravailable corpora, it should be cautioned that insofar as mostresearch experiments continue to show marked improvementwith the increased availability of training data, it is likelythat this corpus also will fail to allow us to find or achieveasymptotic performance.
Most systems continue to be under-trained or constrained to work in suboptimal lower dimen-sional spaces, due to their data-starvation.
I deed, this resultis not really surprising in light of the much larger amounts ofspeech data to which young children must be exposed beforegaining recognition proficiency of even modest size vocabu-laries.The structure, features, and dimensions of this corpus con-stitute the outcome of a heavily debated consensus process,which satisfies the basic (though certainly not all) differentrequirements of the different research loci of all parties in-volved.
There are significant portions of this corpus which357will be more heavily used by one or more research groups,and not at all by others.
Nonetheless, the common basis andcareful structuring of these materials hould allow for highlyinformative intra- and inter-group comparisons.
The mem-bers of this committee are to be commended and should takepride in their success in jointly exercising a rare "statesman-like" cooperation to support he legitimate diversity of expertresearch interests in this field (often overcoming strong pres-sures of both personal and political convictions to supportonly their own narrower esearch interests).THE WSJ -CORPUS STRUCTUREAND CAPABIL IT IESSpecifically, the WSJ corpus is scalable and built toaccommodate variable size large vocabularies (SK, 20K,and larger), variable perplexities (80, 120, 160, 240, andlarger), speaker dependent (SD) and independent (SI) train-ing with variable amounts of data (ranging from 100 to9600 sentences/speaker), including equal portions of ver-balized and non-verbalized punctuation (to reflect bothdictation-mode and non-dictation-mode applications), sepa-rate speaker adaptation materials (40 phonetically rich sen-tences/speaker), simultaneous tandard close talking andmultiple secondary microphones, variable moderate noise en-vironments, equal numbers of male and female speakers cho-sen for diversity of voice quality and dialect.
In order tocollect large quantities of speech data very cost-effectively, itwas decided to collect the majority of the recorded speechin a "read" speech mode, whereby speakers are promptedby newspaper text paragraphs.
The presentation of coher-ent paragraph blocks of text provides emantically meaning-ful material, thereby facilitating the production of realisticspeech prosodies.
Small amounts of unprompted "sponta-neous" speech are provided for comparison (utilizing somenaive speakers as well as some who are experienced at dicta-tion for human transcription).Testing paradigms were carefully constructed to accommo-date efficient comparisons of SI and SD performance andvariable size vocabulary "open" and "closed" tests to per-mit evaluation both with and without "out-of-vocabulary"lexical items.
The value of variable amounts of training setmaterials can be directly assessed both within and acrossspeakers.
Well-trained speaker-dependent performance pro-vides an upper bound against which the success of differentspeaker-independent modeling and speaker-adaptive m thod-ologies may be rigorously compared.Adaptive acoustic and language modeling is easily sup-ported through the following simple though rigorous auto-matic paradigm: 1) Recognition of a sentence is performedand assessed as usual against existing system acoustic andlanguage models.
2) The system commences to adapt using(supervised) or not using (unsupervised) the correct "cleartext" to modify its internal acoustic and language modelsautomatically before proceeding to recognition of the nextutterance.Recognition performance with this kind of automatic axlapta-tion is assessable with standard scoring routines.
This modeprovides an easy means to maximize performance for speakersby tracking and accommodating to speaker and environmen-tal changes in a dynamic fashion, also simulating (in a repro-ducible fashion) an interactive system mode where speakerscorrect system recognition errors, and using systems whichcan utilize this feedback to improve performance, in a contin-uous automatic fashion.
The results of automatic adaptationcan be assessed in an on-going "dynamic" fashion, or stoppedafter varying amounts of adaptation, for subsequent "static"testing on materials to which the system is not subsequentlyadapted\[I,2,3\].The availability of large amounts of machine-readable textfrom nearly three years of the Wall Street Journal enablesmeaningful statistical benchmark language models (includingbigrams and trigrams) to be generated, and the results fromthese to be easily contrasted.
By varying the types of lan-guage models chosen, the effect on recognition performanceof variable perplexities for the same textual materials can beassessed.
The availability of this text provides a valuable re-source enabling novel language models and language modelsadapted from other tasks to be developed and evaluated aswell.THE WSJ -P ILOT DATABASEIt was judged to be too ambitious to immediately record a400 hour recognition database.
Therefore, a smaller pilotdatabase built around the WSJ task was designed.
A jointBBN/Lincoln proposal for the pilot was adopted by the CSRcommittee.
In an attempt o "share the shortage" this pro-posal provided equal amounts of training data for each ofthree popular training paradigms.
This proposal was alsorich enough that it provided for "multi-mode" use of the datato allow many more than just three paradigms to be explored.The original plan was for about a 45 hour database, butthe three recording sites, (MIT, SRI, and TI), each recordedabout a half share for a total of 80 hours.
The resultantdatabase is shown in Table 4 and described below.
(About1.5K additional SI training sentences are not shown in thetable.
)THE WSJ  TEXT PREPROCESSINGIt is important o be able to train a language model thatis well matched to the (text) source to be used as a controlcondition to isolate the performance of the acoustic model-ing from the language modeling\[12\].
(It is always possibleto train a mismatched language model, but its effects cannotbe adequately assessed without a control matched languagemodel.)
Ideally, one would have access to many (tens tohundreds of millions of words) of accurately transcribed spo-ken speech.
Such was not available to us.
Therefore, thiscondition was simulated by preprocessing the WSJ text ina manner that removed the ambiguity in the word sequencethat a reader might choose.
(This preprocessing is similarto that which might be used in a text-to-speech system\[4\].
)This ensures that the unread (and unchecked) text used totrain the language model is representative of the spoken test358material.The original WSJ text data were supplied by Dow Jones,Inc.
to the ACL/DCI\[9\] which organized the data and dis-tributed it to the research community in CD-ROM format.The WSJ text data were supplied as 313 1MB files from theyears 1987, 1988 and 1989.
The data consisted of articlesthat were paragraph and sentence marked by the ACL/DCI.
(Since automatic marking methods were used, some of theparagraphs and sentence marks are erroneous.)
The articleheaders contained a WSJ-supplied ocument-control number.The preprocessing began with integrity checks: one file from1987 and 38 from 1988 were discarded due to duplicationof articles in the same file (1987) or duplication of datafound in other files (1988).
274 files were retained, whichyielded 47M with-verbalized-punctuation w rds from 1.8Msentences.
(The yield is on the order of 10% fewer words inthe non-verbalized-punctuation version.)
Each file contain ascatter of dates, usually within a few days, but sometimes upto six months apart.
Each file was characterized by its mostfrequent date (used later to temporally order the files).Since the CSR Committee had decided to support both withand without verbalized punctuation modes, it was necessaryto produce four versions of each text: with/without verbal-ized punctuation x prompt/truth texts.
(A prompt text isthe version read by the speaker and the truth text is theversion used by the training, recognition, and scoring algo-rithms.)
The preprocessing consisted of a general prepro-cessor (GP) followed by four customizing preprocessors toconvert he GP output in the four specific outputs.
The tra-ditional computer definition of a word is used--any white-space separated object is a word.
Thus, a word followed by acomma becomes a word unless that comma is separated fromthe word.
(Resolution of the role of a period or an apostro-phe/single quote can be a very difficult problem requiring fullunderstanding of the text.
)The general preprocessor started by labeling all paragraphsand sentences using an SGML-Iike scheme based upon the filename, document-control number, paragraph number withinthe article, and sentence number within the paragraph.
Thismarking scheme, which was carried transparently though allof the processing, made it very easy to locate any of the textat any stage of the processing.
A few bug fixes were appliedfor such things as common typos or misspellings.
Next thenumbers are converted into orthographics.
"Magic numbers"(numbers uch as 386 and 747 which are not pronounced nor-mally because they have a special meaning) are pronouncedfrom an exceptions table.
The remaining numbers are pro-nounced by rule--the algorithms cover money, time, dates,"serial numbers" (mixed digits and letters), fractions, feet-inches, real numbers, and integers.
Next sequences of let-ters are separated: U.S.---~U.
S., Roman numerals are writtenout as cardinals or ordinals depending on the left context,acronyms are spelled out or left as words according to thecommon pronunciation, and abbreviations (except for Mr.,Mrs., Ms., and Messrs.) are expanded to the full word.
Fi-nally, single letters are followed by a "."
to distinguish themfrom the words "a" and "I".
This output is the input to thefour specific preprocessors.The punctuation processor is used in several modes.
Inits normal mode, it is used to produce the with-verbalized-punctuation texts.
It resolves apostrophes from single quotes(an apostrophe is part of the word, a single quote is not),resolves whether a period indicates an abbreviation or is apunctuation mark, and separates punctuation into individ-ual marks separate from the words.
This punctuation iswritten out in a word-like form (eg.
,GOMMA) to ensurethat the speaker will pronounce it.
This output is the with-punctuation prompting text.
Until this point, the text retainsthe original case as suppled on the CD-ROM.
If one wishesto perform case-sensitive r cognition (ie.
the language modelpredicts the case of the word), this same text can be usedas the with-punctuation truth text or if one wishes to per-form case-insensitive recognition, the text may be mapped toupper-case.
(A post-processor is supplied with the databaseto perform the case mapping without altering the sentencemarkings.)
Initial use of the database will center on case-insensitive recognition.The without-punctuation prompting text is very similar tothe GP output.
Only a few things, such as mapping "%"to "percent", need to be performed.
This text contains themixed case and normal punctuation to help the subject speakthe sentence.
(The subject is instructed not to pronounce anyof the punctuation in this mode.)
The punctuation processoris used in a special mode to produce the without-punctuationtruth-text.
It performs all of the same processing as describedabove to locate the punctuation marks, but now, rather thanspelling them out, eliminates them from the output.
(Sincethe punctuation marks do not appear explicitly in the acous-tics, they must be eliminated from the truth texts.
Pre-dicting punctuation from the acoustics has been shown tobe impractical--human transcribers don't punctuate consis-tently, and, in an attempt to perform punctuation predic-tion by the language model in a CSR, IBM found a highpercentage of their errors to be due to incorrectly predictedpunctuation\]14\].
People dictating to a human transcriberverbalize the punctuation if they feel that correct punctua-tion is important: e.g.
lawyers.
They also verbally spell un-common words and issue formatting commands where appro-priate.)
This without-punctuation truth text is again mixedcase and can be mapped to upper case if the user desires.WSJ  TEXT SELECT ION INTODATABASE PARTSNext it was necessary to divide the text into sections forthe various parts of the database.
Since the plan called forthe pilot to become a portion of the full database, all textprocessing and selection were performed according to criteriathat were consistent with the full database.Ninety percent of the text, including all of the PennTreebank\[17\] (about 2M words) were reserved for training,5% for development testing, and the remaining 53~ for eval-uation testing.
The non-treebank text files were temporally359ordered (see above) and 28 were selected for test ing--the oddordinal files for development testing and the even ordinal fliesfor evaluation testing.
(The Treebank included the 21 mostrecent files so it was not possible to simulate the real case--train on the past and test on the "present").All of the non-test data, with the exception of the sentencesrecorded for acoustic training, is available for training lan-guage models.
The acoustic training data is eliminated toallow a standard sanity check: CSR testing on the acoustictraining data without also performing a closed test on thelanguage model.WSJ  TEXT SELECT ION FORRECORDINGNext the recording sentences were selected.
Separate sen-tence "pools" were selected from the appropriate text sec-tions for SI train (10K sentences), SD train (20K sentences),20K-word vocabulary test (4K development test and 4K eval-uation test sentences), and 5K-word vocabulary test (2K de-velopment est and 2K evaluation test).
It was originallyhoped that the 5K vocabulary test set could be formed as asubset of the 20K test set, but this was not possible---thusthe 4 test sets are completely independent.The recording texts were filtered for readability.
(The WSJuses a lot of uncommon words and names and uses com-plex sentence structures that were never intended to be readaloud.)
The first step was to form a. word-frequency list(WFL) (ie.
a frequency-ordered unigram list) from all ofthe upper-case with-punctuation truth texts.
This yielded alist of 173K words.
(For comparison, mixed case yields 210Kwords).
Next, a process of automated "quality filtering" wasdevised to filter out the majority of the erroneous and un-readable paragraphs.
This filtering is applied only to therecorded texts, not to the general anguage model trainingtexts.
Since many typos, misspellings and processing (bothACL-DCI and preprocessing) errors map into low frequencywords, any paragraph which contained an out-of-top-64K-WFL word or was shorter than 3 words was rejected.
(Thetop 64K WFL words cover 99.6% of the frequency-weightedwords in the database.)
Any paragraph containing less thanthree sentences or more than eight sentences was rejectedto maintain reasonable selection unit sizes.
Any paragraphcontaining a sentence longer than 30 words was rejected astoo difficult to read 1.
Because the WSJ contains many in-stances of certain "boiler-plate" figure captions which wouldbe pathologically over represented in the test data, duplicatesentences were removed from the test sentence pools.
Finallyhuman checks verified the high overall quality of the chosensentences.
Note that this does not mean perfect--there wereerrors in both the source material and the preprocessing.1 One of the authors (dbp) has recorded about 2500 WSJ sen-tences.
The most difficult sentences to record were the longestones.
After a little practice, verbalized punctuation sentences wereonly slightly harder to read than the non-verbalized punctuationones.
This slight additional difficulty can be accounted for by thefact that the verbalized punctuation sentences average about 10%longer than the non-verbalized punctuation ones.The 20K test pools were produced by randomly selectedquality-filtered paragraphs until 8K (4K dev.
test and 4Keval.
test) sentences were selected.
This produced a realizedvocabulary of 13K words.
Since this data set was producedin a vocabulary insensitive manner, it can be used withoutbias for open and closed recognition vocabulary testing atany vocabulary size up to 64K words.
(However, using itfor open vocabulary testing at any vocabulary size less than20K will yield a large number of out-of-vocabulary errors--the top-20K of the WFL (the 20K open vocabulary) has afrequency weighted coverage of 97.8% of the data.
)Attempts to produce the 5K vocabulary test pools by thesame method produced too few sentences to be useful(,-,1200).
Thus it was necessary to use a vocabulary sen-sitive procedure--paragraphs were allowed to have up to 1out-of-top-5.6K-WFL words.
This produced the highest yield(~4K sentences with a realized vocabulary of 5K words) andreduces, but does not completely eliminate the tail of theword frequency distribution.
This test set alows open andclosed vocabulary testing at a 5K-word vocabulary, but wouldbe expected to yield somewhat biased test results if used atlarger test vocabularies\[10,14\].
The top-5K of the WFL (the5K open vocabulary) has a frequency weighted coverage of91.7% of the data.Finally, the evaluation test paragraphs were broken into fourseparate groups.
This was done to provide four independentevaluation test sets.The recording sites selected a randomly chosen subset of theparagraphs from the pool corresponding to the database sec-tion being recorded (with replacement between subjects) foreach subjects to read.
The sentences were recorded one peraudio file.
All subjects recorded one set of the 40 adaptationsentences.OTHER WSJ  DATABASECOMPONENTSThe above describes the selection and recording of theacoustic portion of the WSJ-pi lot database.
Additionalcomponents--such as a dictionary and language models--are required to perform recognition experiments.
DragonSystems Inc., under a joint license agreement with Ran-dom House, has provided a set of pronouncing dictionaries--totaling 33K words--to cover the training and 5K and 20K-word open and closed test conditions.
This dictionary alsoincludes the 1K-word Resource Management\[15\] vocabularyto allow cross-task tests with an existing database.
MIT Lin-coln Laboratory, as part of its text selection and preprocess-ing effort, has provided baseline open and closed test vocab-ularies based upon the test-set realized-vocabularies and theWFL for the 5K and 20K test sets.
Lincoln has also provided8 baseline bigram back-off\[8,11\] language models (5K/20Kwords ?
open/closed vocab.
?
verbalized/non-verbalizedpunct.)
for research and cross-site comparative valuationtesting.
Finally language model training data and utilitiesfor manipulating the processed texts have been made avail-able to the recording and CSR research sites.360NIST compiled the data from the three recording sites (MIT,SRI, and TI), formatted it, and shipped it to MIT whereWORM CD-ROMS were produced for rapid distribution tothe CSR development sites.CONCLUSIONThe WSJ Corpus and its supporting components have beenvery carefully and efficiently designed by the joint effortsof the DARPA SLS CSR Committee to support advancedstrategic CSR research of many different ypes.
It is hopedthat eventually, these materials will be instrumental in facil-itating the speech recognition research community to createspoken language technology capabilities suited to broad prac-tical application.REFERENCES1.
J. M. Baker, "DragonDictate-30K: Natural Lan-guage Speech Recognition with 30,000 Words," EU-ROSPEECH 89, Paris, September 1989.2.
J. M. Baker, Presentation at ESCA Workshop on Per-formance Evaluation and Databases Noordwijkerhout,Netherlands, September.
1989.3.
J. M. Baker, Presentation at the Kobe Workshopon Performance Evaluation and Databases,Kobe JapanNovember 19904.
J.
Allen., M. S. Hunnicutt, and D. Klatt, "From Textto Speech: The MITalk System, Cambridge UniversityPress, New York, 1987.5.
J. L. Gauvain, L. F. Lamel, and M. Esk~nazi, "DesignConsiderations and Text Selection for BREF, a largeFrench Read-Speech Corpus," ICSLP 90, Kobe, Japan,November 1990.6.
H. W. Hon and K. F. Lee, "On Vocabulary-IndependentSpeech Modeling," Proc.
ICASSPg0, Albuquerque, NewMexico, April 1990.7.
F. Jelinek and R. Mercer, personal communication.8.
S. M. Katz, "Estimation of Probabilities from SparseData for the Language Model Component of a SpeechRecognizer," ASSP-35, pp 400-401, March 1987.9.
M. Liberman, "Text on Tap: the ACL/DCI," Proceed-ings October, 1989 DARPA Speech and Natural Lan-guage Workshop, Morgan Kanfmann Publishers, Octo-ber, 1989.10.
R. Mercer, personal communication.11.
D. B. Paul, "Experience with a Stack Decoder-BasedHMM CSR and Back-Off N-Gram Language Models,"Proc.
DARPA Speech and Natural Language Workshop,Morgan Kaufmann Publishers, Feb. 1991.12.
D. B. Paul, J. K. Baker, and J. M. Baker, "On the In-teraction Between True Source, Training, and TestingLanguage Models," Proceedings June 1990 Speech andNatural Language Workshop, Morgan Kaufmann Pub-lishers, June, 1990.13.
D. B. Paul, CSR results presented at the October 91SLS Mid-Term Workshop, CMU, October 1991.14.
M. Picheny, personal communication.15.
P. Price, W. Fisher, J. Bernstein, and D. Pallett, "TheDARPA 1000-Word Resource Management Databasefor Continuous Speech Recognition," ICASSP 88, NewYork, April 1988.16.
P. Price, The ATIS Common Task: Selection andOverview," Proceedings June 1990 Speech and Natu-rM Language Workshop, Morgan Kaufmann Publishers,June, 1990.17.
B. Santorini, "Annotation Manual for the Penn Tree-bank Project," Technical Report, CIS Department, Uni-versity of Pennsylvania, May 1990.WSJ -spon  Spontaneously spoken data.
The subjectssimulate dictating a short WSJ-like article.
(Included in the WSJ-pilot database.
)Hansard - read  Read data from the Hansard database.Rad io logy- read  Read radiology (medical) reports.CALS- read  (Computer-aided Acquisition & LogisticSupport) Read repair manuals.DART- read  (Database Query for Material Routing)Read database queries.USENET- read  Read computer bulletin board mes-sages.NPR- read  (National Public Radio) Read transcrip-tions of radio programs.B l ind - read  Taped recordings for the blind.Table 1.
Proposed complementary datasets for the LargeVocabulary CSR database.I Vocab \[ Word Coverage5K 91.7%20K 97.7%64K 99.6%173K 100.0%Table 2.
Frequency-weighted upper-case word coverage(from the word-frequency list).361TRAINING: SI-160 SI-16/SD-2400 LSD-9600Train 160a .
260 = 41600 16b * 2400 = 38400Adaptation 160a* 40 = 6400 16b* 40 = 6404b' * 7200 = 28800Est.
total training data: SI-160:86 hrs, SI-16:79 hrs, SD-2400:5 hrs/spkr, LSD-9600:20 hrs/spkrDEVELOPMENT TEST: SI SDRead text, 5K 32c .
100 = 3200 16b * 100 = 1600Read text, 20K 32c .
100 = 3200 16b * 100 = 1600Spontaneous 32c .
100 = 3200 16b * 100 = 1600Read spontaneous 32c .
100 = 3200 16b * 100 = 1600Adaptation 32c .
40 = 1280EVALUATION TEST: SI SDRead text, 5K 32d .
100 = 3200 16b * 100 = 1600Read text, 20K 32d * 100 = 3200 16b .
100 = 1600Spontaneous 32d .
100 = 3200 16b .
100 = 1600Read spontaneous 32d * 100 = 3200 16b * 100 = 1600Adaptation 32d* 40 = 1280Table 3.
The plan for the WSJ portion of the full database.
Format: no.
spkr * no.
sent = total no.
sent.
The lettersfollowing the number of speakers indicate the speaker sets (b t is a subset of b).
The data in all sections, except foradaptation, is half verbalized punctuation and half non-verbalized punctuation.
Training times do not include theadaptation data.
Times based on 7.4 sec/sentence.
Total database size: 157K sentences=323 hrs=37 GB.TRAINING: SI-84 SI-12/SD-600 LSD-2400Train 84a .
100~ = 7240 12b * 600 = 7200Adaptation 84a .
40 = 3660 8b*  40 = 3203b ~ .
1800 = 5400Total training data: SI-84:15.3 hrs, SI-12:14.3 hrs, SD-600 ..~1.2 hrs/spkr, SD-2400:--~4.8 hrs/spkrtSome speakers recorded 50 sentences.DEVELOPMENT TEST:Read text, 5KRead text, 20KSpontaneousRead spontaneousAdaptationSI SD10c .
80 = 80010c * 80 = 80010c * 80 = 80010c * 80 = 800lOc * 40 = 40012b .
80 = 96012b .
80 = 96012b .
80 = 96012b .
80 = 960EVALUATION TEST: SI SDRead text, 5K 10d * 80 = 800 12b .
80 = 960Read text, 20K 10d .
80 = 800 12b .
80 = 960Spontaneous 10d * 80 = 800 12b * 80 = 960Read spontaneous 10d * 80 = 800 12b * 80 = 960Adaptation 10d * 40 = 400Table 4.
The WSJ-Pilot database.
Format: no.
spkr .
no.
sent = total no.
sent.
The letters following the numberof speakers indicate the speaker sets (b ~ is a subset of b).
The average sentence length is .-,7.4 sec.
(Verbalizedpunctuation sentences tend to be somewhat longer than average and non-verbalized punctuation sentences somewhatshorter.)
The data in all sections, except for adaptation, is half verbalized punctuation and half non-verbalizedpunctuation.
Training times do not include adaptation data.
Total database size: 39K sent=80 hrs=9.2 GB.362
