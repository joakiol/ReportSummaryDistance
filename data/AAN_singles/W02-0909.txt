Acquiring Collocations for Lexical Choice between Near-SynonymsDiana Zaiu Inkpen and Graeme HirstDepartment of Computer ScienceUniversity of Toronto{dianaz,gh}@cs.toronto.eduAbstractWe extend a lexical knowledge-base ofnear-synonym differences with knowl-edge about their collocational behaviour.This type of knowledge is useful in theprocess of lexical choice between near-synonyms.
We acquire collocations forthe near-synonyms of interest from a cor-pus (only collocations with the appropri-ate sense and part-of-speech).
For eachword that collocates with a near-synonymwe use a differential test to learn whetherthe word forms a less-preferred collo-cation or an anti-collocation with othernear-synonyms in the same cluster.
Forthis task we use a much larger corpus(the Web).
We also look at associations(longer-distance co-occurrences) as a pos-sible source of learning more about nu-ances that the near-synonyms may carry.1 IntroductionEdmonds and Hirst (2002 to appear) developed alexical choice process for natural language gener-ation (NLG) or machine translation (MT) that candecide which near-synonyms are most appropriatein a particular situation.
The lexical choice processhas to choose between clusters of near-synonyms (toconvey the basic meaning), and then to choose be-tween the near-synonyms in each cluster.
To groupnear-synonyms in clusters we trust lexicographers?judgment in dictionaries of synonym differences.For example task, job, duty, assignment, chore, stint,hitch all refer to a one-time piece of work, but whichone to choose depends on the duration of the work,the commitment and the effort involved, etc.In order to convey desired nuances of mean-ing and to avoid unwanted implications, knowledgeabout the differences among near-synonyms is nec-essary.
I-Saurus, a prototype implementation of (Ed-monds and Hirst, 2002 to appear), uses a small num-ber of hand-built clusters of near-synonyms.Our goal is to automatically acquire knowledgeabout distinctions among near-synonyms from adictionary of synonym differences and from othersources such as free text, in order to build a new lex-ical resource, which can be used in lexical choice.Preliminary results on automatically acquiring a lex-ical knowledge-base of near-synonym differenceswere presented in (Inkpen and Hirst, 2001).
We ac-quired denotational (implications, suggestions, de-notations), attitudinal (favorable, neutral, or pejo-rative), and stylistic distinctions from Choose theRight Word (Hayakawa, 1994) (hereafter CTRW)1.We used an unsupervised decision-list algorithm tolearn all the words used to express distinctions andthen applied information extraction techniques.Another type of knowledge that can help in theprocess of choosing between near-synonyms is col-locational behaviour, because one must not choosea near-synonym that does not collocate well withthe other word choices for the sentence.
I-Saurusdoes not include such knowledge.
The focus ofthe work we present in this paper is to add knowl-edge about collocational behaviour to our lexicalknowledge-base of near-synonym differences.
Thelexical choice process implemented in I-Saurus gen-1We are grateful to HarperCollins Publishers, Inc. for per-mission to use CTRW in this project.July 2002, pp.
67-76.
Association for Computational Linguistics.ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,Unsupervised Lexical Acquisition: Proceedings of the Workshop of theerates all the possible sentences with a given mean-ing, and ranks them according to the degree to whichthey satisfy a set of preferences given as input (theseare the denotational, attitudinal, and stylistic nu-ances mentioned above).
We can refine the rank-ing so that it favors good collocations, and penal-izes sentences containing words that do not collocatewell.We acquire collocates of all near-synonyms inCTRW from free text.
We combine several statis-tical measures, unlike other researchers who rely ononly one measure to rank collocations.Then we acquire knowledge about less-preferredcollocations and anti-collocations2.
For exam-ple daunting task is a preferred collocation, whiledaunting job is less preferred (it should not be usedin lexical choice unless there is no better alternative),and daunting duty is an anti-collocation (it must notbe used in lexical choice).
Like Church et al(1991),we use the t-test and mutual information.
Unlikethem we use the Web as a corpus for this task, wedistinguish three different types of collocations, andwe apply sense disambiguation to collocations.Collocations are defined in different ways by dif-ferent researchers.
For us collocations consist ofconsecutive words that appear together much moreoften than by chance.
We also include words sep-arated by a few non-content words (short-distanceco-occurrence in the same sentence).We are interested in collocations to be used in lex-ical choice.
Therefore we need to extract lexicalcollocations (between open-class words), not gram-matical collocations (which could contain closed-class words, for example put on).
For now, we con-sider only two-word fixed collocations.
In futurework we will consider longer and more flexible col-locations.We are also interested in acquiring words thatstrongly associate with our near-synonyms, espe-cially words that associate with only one of the near-synonyms in the cluster.
Using these strong asso-ciations, we plan to learn about nuances of near-synonyms in order to validate and extend our lexicalknowledge-base of near-synonym differences.In our first experiment, described in sections 2and 3 (with results in section 4, and evaluation in2This term was introduced by Pearce (2001).section 5), we acquire knowledge about the collo-cational behaviour of the near-synonyms.
In step 1(section 2), we acquire potential collocations fromthe British National Corpus (BNC)3, combining sev-eral measures.
In section 3 we present: (step2) se-lect collocations for the near-synonyms in CTRW;(step 3) filter out wrongly selected collocations us-ing mutual information on the Web; (step 4) for eachcluster we compose new collocations by combin-ing the collocate of one near-synonym with the theother near-synonym, and we apply the differential t-test to classify them into preferred collocations, less-preferred collocations, and anti-collocations.
Sec-tion 6 sketches our second experiment, involvingword associations.
The last two sections present re-lated work, and conclusions and future work.2 Extracting collocations from free textFor the first experiment we acquired collocations fornear-synonyms from a corpus.
We experimentedwith 100 million words from the Wall Street Journal(WSJ).
Some of our near-synonyms appear very fewtimes (10.64% appear fewer than 5 times) and 6.87%of them do not appear at all in WSJ (due to its busi-ness domain).
Therefore we need a more generalcorpus.
We used the 100 million word BNC.
Only2.61% of our near-synonyms do not occur; and only2.63% occur between 1 and 5 times.Many of the near-synonyms appear in more thanone cluster, with different parts-of-speech.
We ex-perimented on extracting collocations from raw text,but we decided to use a part-of-speech tagged corpusbecause we need to extract only collocations rele-vant for each cluster of near-synonyms.
The BNC isa good choice of corpus for us because it has beentagged (automatically by the CLAWS tagger).We preprocessed the BNC by removing all wordstagged as closed-class.
To reduce computation time,we also removed words that are not useful for ourpurposes, such as proper names (tagged NP0).
If wekeep the proper names, they are likely to be amongthe highest-ranked collocations.There are many statistical methods that can beused to identify collocations.
Four general meth-ods are presented by Manning and Schu?tze (1999).The first one, based on frequency of co-occurrence,3http://www.hcu.ox.ac.uk/BNC/does not consider the length of the corpus.
Part-of-speech filtering is needed to obtain useful colloca-tions.
The second method considers the means andvariance of the distance between two words, and cancompute flexible collocations (Smadja, 1993).
Thethird method is hypothesis testing, which uses sta-tistical tests to decide if the words occur togetherwith probability higher than chance (it tests whetherwe can reject the null hypothesis that the two wordsoccurred together by chance).
The fourth methodis (pointwise) mutual information, an information-theoretical measure.We use Ted Pedersen?s Bigram Statistics Pack-age4.
BSP is a suite of programs to aid in analyz-ing bigrams in a corpus (newer versions allow N-grams).
The package can compute bigram frequen-cies and various statistics to measure the degree ofassociation between two words: mutual information(MI), Dice, chi-square (?2), log-likelihood (LL), andFisher?s exact test.The BSP tools count for each bigram in a corpushow many times it occurs, and how many times thefirst word occurs.We briefly describe the methods we use in our ex-periments, for the two-word case.
Each bigram xycan be viewed as having two features represented bythe binary variables X and Y .
The joint frequencydistribution of X and Y is described in a contingencytable.
Table 1 shows an example for the bigramdaunting task.
n11 is the number of times the bi-gram xy occurs; n12 is the number of times x occursin bigrams at the left of words other than y; n21 isthe number of times y occurs in bigrams after wordsother that x; and n22 is the number of bigrams con-taining neither x nor y.
In Table 1 the variable Xdenotes the presence or absence of daunting in thefirst position of a bigram, and Y denotes the pres-ence or absence of task in the second position of abigram.
The marginal distributions of X and Y arethe row and column totals obtained by summing thejoint frequencies: n+1 = n11 + n21, n1+ = n11 + n12,and n++ is the total number of bigrams.The BSP tool counts for each bigram in a corpushow many times it occurs, how many times the firstword occurs at the left of any bigram (n+1), and howmany times the second words occurs at the right of4http://www.d.umn.edu/?tpederse/code.htmly ?yx n11 = 66 n12 = 54 n1+ = 120?x n21 = 4628 n22 = 15808937 n2+ = 15813565n+1 = 4694 n+2 = 15808991 n++ = 15813685Table 1: Contingency table for daunting task(x = daunting, y = task).any bigram (n1+).Mutual information, I(x;y), compares the prob-ability of observing words x and word y together (thejoint probability) with the probabilities of observingx and y independently (the probability of occurringtogether by chance) (Church and Hanks, 1991).I(x;y) = log2P(x,y)P(x)P(y)The probabilities can be approximated by: P(x) =n+1/n++, P(y) = n1+/n++, P(x,y) = n11/n++.Therefore:I(x;y) = log2n++n11n+1n1+The Dice coefficient is related to mutual informa-tion and it is calculated as:Dice(x,y) =2P(x,y)P(x)+ P(y)=2n11n+1 + n1+The next methods fall under hypothesis test-ing methods.
Pearson?s Chi-square and Log-likelihood ratios measure the divergence of ob-served (ni j) and expected (mi j) sample counts (i =1,2, j = 1,2).
The expected values are for the modelthat assumes independence (assumes that the nullhypothesis is true).
For each cell in the contingencytable, the expected counts are: mi j = ni+n+ jn++ .
Themeasures are calculated as (Pedersen, 1996):?2 = ?i, j(ni j?mi j)2mi jLL = 2 ?i, jlog2 n2i jmi jLog-likelihood ratios (Dunning, 1993) are moreappropriate for sparse data than chi-square.Fisher?s exact test is a significance test that isconsidered to be more appropriate for sparse andskewed samples of data than statistics such as thelog-likelihood ratio or Pearson?s Chi-Square test(Pedersen, 1996).
Fisher?s exact test is computedby fixing the marginal totals of a contingency tableand then determining the probability of each of thepossible tables that could result in those marginal to-tals.
Therefore it is computationally expensive.
Theformula is:P =n1+!n2+!n+1!n+2!n++!n11!n12!n21!n22!Because these five measures rank collocations indifferent ways (as the results in the Appendix willshow), and have different advantages and draw-backs, we decided to combine them in choosing col-locations.
We choose as potential collocations foreach near-synonym a collocation that is selected byat least two of the measures.
For each measurewe need to choose a threshold T , and consider asselected collocations only the T highest-ranked bi-grams (where T can differ for each measure).
Bychoosing higher thresholds we increase the precision(reduce the chance of accepting wrong collocations).By choosing lower thresholds we get better recall.If we opt for low recall we may not get many col-locations for some of the near-synonyms.
Becausethere is no principled way of choosing these thresh-olds, we prefer to choose lower thresholds (the first200,000 collocations selected by each measure, ex-cept Fisher?s measure for which we take all 435,000collocations ranked 1) and to filter out later (in step2) the bigrams that are not true collocations, usingmutual information on the Web.3 Differential collocationsFor each cluster of near-synonyms, we now havethe words that occur in preferred collocations witheach near-synonym.
We need to check whether thesewords collocate with the other near-synonyms in thesame cluster.
For example, if daunting task is a pre-ferred collocation, we check whether daunting col-locates with the other near-synonyms of task.We use the Web as a corpus for differential col-locations.
We don?t use the BNC corpus to rankless-preferred and anti-collocations, because theirabsence in BNC may be due to chance.
We can as-sume that the Web (the portion retrieved by searchengines) is big enough that a negative result can betrusted.We use an interface to AltaVista search engine tocount how often a collocation is found.
(See Table 2for an example.5) A low number of co-occurrencesindicates a less-preferred collocation.
But we alsoneed to consider how frequent the two words in thecollocation are.
We use the differential t-test to findcollocations that best distinguish between two near-synonyms (Church et al, 1991), but we use the Webas a corpus.
Here we don?t have part-of-speech tagsbut this is not a problem because in the previousstep we selected collocations with the right part-of-speech for the near-synonym.
We approximate thenumber of occurrences of a word on the Web withthe number of documents containing the word.The t-test can also be used in the hypothesis test-ing method to rank collocations.
It looks at the meanand variance of a sample of measurements, wherethe null hypothesis is that the sample was drawnfrom a normal distribution with mean ?.
It measuresthe difference between observed (x?)
and expectedmeans, scaled by the variance of the data (s2), whichin turn is scaled by the sample size (N).t =x???
?s2NWe are interested in the Differential t-test, whichcan be used for hypothesis testing of differences.
Itcompares the means of two normal populations:t =x?1?
x?2?s21N +s22NHere the null hypothesis is that the average differ-ence is ?
= 0.Therefore x??
?
= ?
= x?1?
x?2.
In thedenominator we add the variances of the two popu-lations.If the collocations of interest are xw and yw (orsimilarly wx and wy), then we have the approxima-tions x?1 = s21 = P(x,w) and x?2 = s22 = P(y,w); there-fore:t =P(x,w)?P(y,w)?P(x,w)+P(y,w)n++=nxw?nyw?nxw + nywIf w is a word that collocates with one of the near-synonyms in a cluster, and x is each of the near-5The search was done on 13 March 2002.synonyms, we can approximate the mutual informa-tion relative to w:P(w,x)P(x)=nwxnxwhere P(w) was dropped because it is the same forvarious x (we cannot compute if we keep it, becausewe don?t know the total number of bigrams on theWeb).We use this measure to eliminate collocationswrongly selected in step 1.
We eliminate those withmutual information lower that a threshold.
We de-scribe the way we chose this threshold (Tmi) in sec-tion 5.We are careful not to consider collocations of anear-synonym with a wrong part-of-speech (our col-locations are tagged).
But there is also the case whena near-synonym has more than one major sense.
Inthis case we are likely to retrieve collocations forsenses other than the one required in the cluster.
Forexample, for the cluster job, task, duty, etc., the col-location import/N duty/N is likely to be for a differ-ent sense of duty (the customs sense).
Our way ofdealing with this is to disambiguate the sense usedin each collocations (we assume one sense per collo-cation), by using a simple Lesk-style method (Lesk,1986).
For each collocation, we retrieve instances inthe corpus, and collect the content words surround-ing the collocations.
This set of words is then in-tersected with the context of the near-synonym inCTRW (that is the whole entry).
If the intersectionis not empty, it is likely that the collocation and theentry use the near-synonym in the same sense.
If theintersection is empty, we don?t keep the collocation.In step 3, we group the collocations of each near-synonym with a given collocate in three classes,based on the t-test values of pairwise collocations.We compute the t-test between each collocation andthe collocation with maximum frequency, and thet-test between each collocation and the collocationwith minimum frequency (see Table 2 for an exam-ple).
Then, we need to determine a set of thresholdsthat classify the collocations in the three groups:preferred collocations, less preferred collocations,and anti-collocations.
The procedure we use in thisstep is detailed in section 5.x Hits MI t max t mintask 63573 0.011662 - 252.07job 485 0.000022 249.19 22.02assignment 297 0.000120 250.30 17.23chore 96 0.151899 251.50 9.80duty 23 0.000022 251.93 4.80stint 0 0 252.07 -hitch 0 0 252.07 -Table 2: The second column shows the number ofhits for the collocation daunting x, where x is oneof the near-synonyms in the first column.
The thirdcolumn shows the mutual information, the fourthcolumn, the differential t-test between the colloca-tion with maximum frequency (daunting task) anddaunting x, and the last column, the t-test betweendaunting x and the collocation with minimum fre-quency (daunting hitch).4 ResultsWe obtained 15,813,685 bigrams.
From these,1,350,398 were distinct and occurred at least 4times.We present some of the top-ranked collocationsfor each measure in the Appendix.
We present therank given by each measure (1 is the highest), thevalue of the measure, the frequency of the colloca-tion, and the frequencies of the words in the collo-cation.We selected collocations for all 914 clusters inCTRW (5419 near-synonyms in total).
An exampleof collocations extracted for the near-synonym taskis:daunting/A task/N-- MI 24887 10.8556-- LL 5998 907.96-- X2 16341 122196.8257-- Dice 2766 0.0274repetitive/A task/N-- MI 64110 6.7756-- X2 330563 430.4004where the numbers are, in order, the rank given bythe measure and the value of the measure.We filtered out the collocations using MI on theWeb (step 2), and then we applied the differentialt-test (step 3).
Table 2 shows the values of MIbetween daunting x and x, where x is one of thenear-synonyms of task.
It also shows t-test val-Near-synonyms daunting particular toughtask?
?
?job ?
?
?assignment ??
?chore ?
?
?duty ??
?stint ?
?
?hitch ?
?
?Table 3: Example of results for collocations.ues between (some) pairs of collocations.
Table 3presents an example of results for differential col-locations, where?marks preferred collocations, ?marks less-preferred collocations, and ?
marks anti-collocations.Before proceeding with step 3, we filtered out thecollocations in which the near-synonym is used ina different sense, using the Lesk method explainedabove.
For example, suspended/V duty/N is keptwhile customs/N duty/N and import/N duty/N are re-jected.
The disambiguation part of our system wasrun only for a subset of CTRW, because we have yetto evaluate it.
The other parts of our system were runfor the whole CTRW.
Their evaluation is describedin the next section.5 EvaluationOur evaluation has two purposes: to get a quanti-tative measure of the quality of our results, and tochoose thresholds in a principled way.As described in the previous sections, in step 1we selected potential collocations from BNC (theones selected by at least two of the five measures).Then, we selected collocations for each of the near-synonyms in CTRW (step 2).
We need to evaluatethe MI filter (step 3), which filters out the bigramsthat are not true collocations, based on their mutualinformation computed on the Web.
We also need toevaluate step 4, the three way classification based onthe differential t-test on the Web.For evaluation purposes we selected three clustersfrom CTRW, with a total of 24 near-synonyms.
Forthese, we obtained 916 collocations from BNC ac-cording to the method described in section 2.We had two human judges reviewing these collo-cations to determine which of them are true colloca-tions and which are not.
We presented the colloca-tions to the judges in random order, and each collo-cation was presented twice.
The first judge was con-sistent (judged a collocation in the same way bothtimes it appeared) in 90.4% of the cases.
The secondjudge was consistent in 88% of the cases.
The agree-ment between the two judges was 67.5% (computedin a strict way, that is we considered agreement onlywhen the two judges had the same opinion includingthe cases when they were not consistent).
The con-sistency and agreement figures show how difficultthe task is for humans.We used the data annotated by the two judges tobuild a standard solution, so we can evaluate theresults of our MI filter.
In the standard solutiona bigram was considered a true collocation if bothjudges considered it so.
We used the standard solu-tion to evaluate the results of the filtering, for variousvalues of the threshold Tmi.
That is, if a bigram hadthe value of MI on the Web lower than a thresholdTmi, it was filtered out.
We choose the value of Tmi sothat the accuracy of our filtering program is the high-est.
By accuracy we mean the number of true collo-cations (as given by the standard solution) identifiedby our program over the total number of bigrams weused in the evaluation.
The best accuracy was 70.7%for Tmi = 0.0017.
We used this value of the thresholdwhen running our programs for all CTRW.As a result of this first part of the evaluation, wecan say that after filtering collocations based on MIon the Web, approximately 70.7% of the remainingbigrams are true collocation.
This value is not ab-solute, because we used a sample of the data for theevaluation.
The 70.7% accuracy is much better thana baseline (approximately 50% for random choice).Table 4 summarizes our evaluation results.Next, we proceeded with evaluating the differ-ential t-test three-way classifier.
For each cluster,for each collocation, new collocations were formedfrom the collocate and all the near-synonyms in thecluster.
In order to learn the classifier, and to evalu-ate its results, we had the two judges manually clas-sify a sample data into preferred collocations, less-preferred collocations, and anti-collocations.
Weused 2838 collocations obtained for the same threeclusters from 401 collocations (out of the initial 916)that remained after filtering.
We built a standard so-lution for this task, based on the classifications ofStep Baseline Our systemFilter (MI on the Web) 50% 70.7%Dif.
t-test classifier 71.4% 84.1%Table 4: Accuracy of our main steps.both judges.
When the judges agreed, the class wasclear.
When they did not agree, we designed sim-ple rules, such as: when one judge chose the classpreferred collocation, and the other judge chose theclass anti-collocation, the class in the solution wasless-preferred collocation.
The agreement betweenjudges was 80%; therefore we are confident that thequality of our standard solution is high.
We usedthis standard solution as training data to learn a de-cision tree6 for our three-way classifier.
The fea-tures in the decision tree are the t-test between eachcollocation and the collocation from the same groupthat has maximum frequency on the Web, and thet-test between the current collocation and the col-location that has minimum frequency (as presentedin Table 2).
We could have set aside a part of thetraining data as a test set.
Instead, we did 10-foldcross validation to quantify the accuracy on unseendata.
The accuracy on the test set was 84.1% (com-pared with a baseline that chooses the most frequentclass, anti-collocations, and achieves an accuracy of71.4%).
We also experimented with including MIas a feature in the decision tree, and with manuallychoosing thresholds (without a decision tree) for thethree-way classification, but the accuracy was lowerthan 84.1%.The three-way classifier can fix some of the mis-takes of the MI filter.
If a wrong collocation re-mained after the MI filter, the classifier can classifyit in the anti-collocations class.We can conclude that the collocational knowledgewe acquired has acceptable quality.6 Word AssociationWe performed a second experiment, where welooked for long distance co-occurrences (words thatco-occur in a window of size K).
We call these as-sociations, and they include the lexical collocationswe extracted in section 2.6We used C4.5, http://www.cse.unsw.edu.au/?quinlanWe use BSP with the option of looking for bi-grams in a window larger than 2.
For exampleif the window size is 3, and the text is vaccine/Ncure/V available/A, the extracted bigrams are vac-cine/N cure/V, cure/V available/A, and vaccine/Navailable/A.
We would like to choose a large (4?15) window size; the only problem is the increasein computation time.
We look for associations of aword in the paragraph, not only in the sentence.
Be-cause we look for bigrams, we may get associationsthat occur to the left or to the right of the word.
Thisis an indication of strong association.We obtained associations similar to those pre-sented by Church et al(1991) for the near-synonymsship and boat.
Church et al suggest that a lexicog-rapher looking at these associations can infer that aboat is generally smaller than a ship, because theyare found in rivers and lakes, while the ships arefound in seas.
Also, boats are used for small jobs(e.g., fishing, police, pleasure), whereas ships areused for serious business (e.g., cargo, war).
Our in-tention is to use the associations to automatically in-fer this kind of knowledge and to validate acquiredknowledge.For our purpose we need only very strong associ-ations, and we don?t want words that associate withall near-synonyms in a cluster.
Therefore we test foranti-associations using the same method we used insection 3, with the difference that the query asked toAltaVista is: x NEAR y (where x and y are the wordsof interest).Words that don?t associate with a near-synonymbut associate with all the other near-synonyms ina cluster can tell us something about its nuancesof meaning.
For example terrible slip is an anti-association, while terrible associates with mistake,blunder, error.
This is an indication that slip is aminor error.Table 5 presents some preliminary results weobtained with K = 4 (on half the BNC and thenon the Web), for the differential associations ofboat (where ?
marks preferred associations, ?marks less-preferred associations, and ?
marks anti-associations).
We used the same thresholds as forour experiment with collocations.Near-synonyms fishing club rowingboat?
?
?vessel??
?craft ?
?
?ship ?
?
?Table 5: Example of results for associations.7 Related workThere has been a lot of work done in extracting col-locations for different applications.
We have alreadymentioned some of the most important contributors.Like Church et al(1991), we use the t-test andmutual information, but unlike them we use the Webas a corpus for this task (and a modified form ofmutual information), and we distinguish three typesof collocations (preferred, less-preferred, and anti-collocations).We are concerned with extracting collocations foruse in lexical choice.
There is a lot of work onusing collocations in NLG (but not in the lexicalchoice sub-component).
There are two typical ap-proaches: the use of phrasal templates in the formof canned phrases, and the use of automatically ex-tracted collocations for unification-based generation(McKeown and Radev, 2000).Statistical NLG systems (such as Nitrogen(Langkilde and Knight, 1998)) make good use of themost frequent words and their collocations.
But sucha system cannot choose a less-frequent synonym thatmay be more appropriate for conveying desired nu-ances of meaning, if the synonym is not a frequentword.Finally, there is work related to ours from thepoint of view of the synonymy relation.Turney (2001) used mutual information to detectthe best answer to questions about synonyms fromTest of English as a Foreign Language (TOEFL) andEnglish as a Second Language (ESL).
Given a prob-lem word (with or without context), and four alter-native words, the question is to choose the alterna-tive most similar in meaning with the problem word.His work is based on the assumption that two syn-onyms are likely to occur in the same document (onthe Web).
This can be true if the author needs toavoid repeating the same word, but not true whenthe synonym is of secondary importance in a text.The alternative that has the highest PMI-IR (point-wise mutual information for information retrieval)with the problem word is selected as the answer.
Weused the same measure in section 3 ?
the mutualinformation between a collocation and a collocatethat has the potential to discriminate between near-synonyms.
Both works use the Web as a corpus, anda search engine to estimate the mutual informationscores.Pearce (2001) improves the quality of retrievedcollocations by using synonyms from WordNet(Pearce, 2001).
A pair of words is considered acollocation if one of the words significantly prefersonly one (or several) of the synonyms of the otherword.
For example, emotional baggage is a goodcollocation because baggage and luggage are in thesame synset and ?emotional luggage is not a col-location.
As in our work, three types of colloca-tions are distinguished: words that collocate well;words that tend to not occur together, but if theydo the reading is acceptable; and words that mustnot be used together because the reading will be un-natural (anti-collocations).
In a similar manner with(Pearce, 2001), in section 3, we don?t record collo-cations in our lexical knowledge-base if they don?thelp discriminate between near-synonyms.
A differ-ence is that we use more than frequency counts toclassify collocations (we use a combination of t-testand MI).Our evaluation was partly inspired by Evert andKrenn (2001).
They collect collocations of the formnoun-adjective and verb-prepositional phrase.
Theybuild a solution using two human judges, and usethe solution to decide what is the best threshold fortaking the N highest-ranked pairs as true colloca-tions.
In their experiment MI behaves worse thatother measures (LL, t-test), but in our experimentMI on the Web achieves good results.8 Conclusions and Future WorkWe presented an unsupervised method to acquireknowledge about the collocational behaviour ofnear-synonyms.Our future work includes improving the way wecombine the five measures for ranking collocations,maybe by giving more weight to the collocations se-lected by the log-likelihood ratio.
We also plan toexperiment more with disambiguating the senses ofthe words in a collocation.Our long-term goal is to acquire knowledge aboutnear-synonyms from corpora and other sources, bybootstrapping with our initial lexical knowledge-base of near-synonym differences.
This includesvalidating the knowledge already asserted and learn-ing more distinctions.AcknowledgmentsWe thank Gerald Penn, Olga Vechtomova, and three anonymousreviewers for their helpful comments on previous drafts of thispaper.
We thank Eric Joanis and Tristan Miller for helping withthe judging task.
Our work is financially supported by the Nat-ural Sciences and Engineering Research Council of Canada andthe University of Toronto.AppendixThe first 10 collocations selected by each mea-sure are presented below.
Note that some ofthe measures rank many collocations equally atrank 1: MI 358 collocations; LL one collocation;?2 828 collocations; Dice 828 collocations; andFisher 435,000 collocations (when the measure iscomputed with a precision of 10 digits ?
higherprecision is recommended, but the computationtime becomes a problem).
The rest of the columnsare: the rank assigned by the measure, the valueof the measure, the frequency of the collocation inBNC, the frequency of the first word in the firstposition in bigrams, and the frequency of the secondword in the second position in bigrams.Some of the collocations ranked 1 by MI:source-level/A debugger/N 1 21.9147 4 4 4prosciutto/N crudo/N 1 21.9147 4 4 4rumpy/A pumpy/A 1 21.9147 4 4 4thrushes/N blackbirds/N 1 21.9147 4 4 4clickity/N clickity/N 1 21.9147 4 4 4bldsc/N microfilming/V 1 21.9147 4 4 4chi-square/A variate/N 1 21.9147 4 4 4long-period/A comets/N 1 21.9147 4 4 4tranquillizers/N sedatives/N 1 21.9147 4 4 4one-page/A synopsis/N 1 21.9147 4 4 4First 10 collocations selected by LL:prime/A minister/N 1 123548 9464 11223 18825see/V p./N 2 83195 8693 78213 10640read/V studio/N 3 67537 5020 14172 5895ref/N no/N 4 62486 3630 3651 4806video-taped/A report/N 5 52952 3765 3765 15886secretary/N state/N 6 51277 5016 10187 25912date/N award/N 7 48794 3627 8826 5614hon./A friend/N 8 47821 4094 10345 10566soviet/A union/N 9 44797 3894 8876 12538report/N follows/V 10 44785 3776 16463 6056Some of the collocations ranked 1 by ?2:lymphokine/V activated/A 1 15813684 5 5 5config/N sys/N 1 15813684 4 4 4levator/N depressor/N 1 15813684 5 5 5nobile/N officium/N 1 15813684 11 11 11line-printer/N dot-matrix/A 1 15813684 4 4 4dermatitis/N herpetiformis/N 1 15813684 9 9 9self-induced/A vomiting/N 1 15813684 5 5 5horoscopic/A astrology/N 1 15813684 5 5 5mumbo/N jumbo/N 1 15813684 12 12 12long-period/A comets/N 1 15813684 4 4 4Some of the collocations ranked 1 by Dice:clarinets/N bassoons/N 1 1.00 5 5 5email/N footy/N 1 1.00 4 4 4tweet/V tweet/V 1 1.00 5 5 5garage/parking/N vehicular/A 1 1.00 4 4 4growing/N coca/N 1 1.00 5 5 5movers/N seconders/N 1 1.00 5 5 5elliptic/A integrals/N 1 1.00 8 8 8viscose/N rayon/N 1 1.00 15 15 15cause-effect/A inversions/N 1 1.00 5 5 5first-come/A first-served/A 1 1.00 6 6 6Some of the collocations ranked 1 by Fisher:roman/A artefacts/N 1 1.00 4 3148 108qualitative/A identity/N 1 1.00 16 336 1932literacy/N education/N 1 1.00 9 252 20350disability/N pension/N 1 1.00 6 470 2555units/N transfused/V 1 1.00 5 2452 12extension/N exceed/V 1 1.00 9 1177 212smashed/V smithereens/N 1 1.00 5 194 9climbing/N frames/N 1 1.00 5 171 275inclination/N go/V 1 1.00 10 53 51663trading/N connections/N 1 1.00 6 2162 736ReferencesKenneth Church and Patrick Hanks.
1991.
Word asso-ciation norms, mutual information and lexicography.Computational Linguistics, 16(1):22?29.Kenneth Church, William Gale, Patrick Hanks, and Don-ald Hindle.
1991.
Using statistics in lexical analy-sis.
In Uri Zernik, editor, Lexical Acquisition: UsingOn-line Resources to Build a Lexicon, pages 115?164.Lawrence Erlbaum.Ted Dunning.
1993.
Accurate methods for statistics ofsurprise and coincidence.
Computational Linguistics,19(1):61?74.Philip Edmonds and Graeme Hirst.
2002 (to appear).Near-synonymy and lexical choice.
ComputationalLinguistics, 28(2).Stefan Evert and Brigitte Krenn.
2001.
Methods forthe qualitative evaluation of lexical association mea-sures.
In Proceedings of the 39th Annual Meeting ofthe of the Association for Computational Linguistics(ACL?2001), Toulouse, France.S.
I. Hayakawa.
1994.
Choose the Right Word.
Harper-Collins Publishers.Diana Zaiu Inkpen and Graeme Hirst.
2001.
Build-ing a lexical knowledge-base of near-synonym differ-ences.
In Proceedings of the Workshop on WordNetand Other Lexical Resources, Second Meeting of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL?2001), Pittsburgh.Irene Langkilde and Kevin Knight.
1998.
The practi-cal value of N-grams in generation.
In Proceedings ofthe International Natural Language Generation Work-shop, Niagara-on-the-Lake, Ontario.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: How to tell a pinecone from an ice cream cone.
In Proceedings of SIG-DOC Conference, Toronto.Christopher Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.The MIT Press, Cambridge, Massachusetts.Kathleen McKeown and Dragomir Radev.
2000.
Col-locations.
In R. Dale, H. Moisl, and H. Somers, edi-tors, Handbook of Natural Language Processing.
Mar-cel Dekker.Darren Pearce.
2001.
Synonymy in collocation extrac-tion.
In Proceedings of the Workshop on WordNetand Other Lexical Resources, Second meeting of theNorth American Chapter of the Association for Com-putational Linguistics, Pittsburgh.Ted Pedersen.
1996.
Fishing for exactness.
In Proceed-ings of the South-Central SAS Users Group Confer-ence (SCSUG-96), Austin, Texas.Frank Smadja.
1993.
Retrieving collocations from text:Xtract.
Computational Linguistics, 19(1):143?177.Peter Turney.
2001.
Mining the web for synonyms: PMI-IR versus LSA on TOEFL.
In Proceedings of theTwelfth European Conference on Machine Learning(ECML-2001), pages 491?502, Freiburg, Germany.
