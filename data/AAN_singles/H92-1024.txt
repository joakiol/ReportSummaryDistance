INS IDE-OUTSIDE REEST IMATION FROMPARTIALLY  BRACKETED CORPORAFernando Pereira Yves Schabes2D-447, AT&T Bell LaboratoriesPO Box 636, 600 Mountain AveMurray Hill, NJ 07974-0636Dept.
of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104-6389ABSTRACTThe inside-outside algorithm for inferring the parameters ofa stochastic context-free grammar is extended to take advan-tage of constituent information in a partially parsed corpus.Experiments on formal and natural anguage parsed corporashow that the new algorithm can achieve faster convergenceand better modelling of hierarchical structure than the orig-inal one.
In particular, over 90% of the constituents in themost likely analyses of a test set are compatible with testset constituents for a grammar trMned on a corpus of 700hand-parsed part-of-speech strings for ATIS sentences.1.
MOTIVATIONGrammar inference is a challenging problem for statis-tical approaches to natural-language processing.
Themost successful grammar inference techniques involvestochastic finite-state language models such as hiddenMarkov models (HMMs) \[1\].
However, finite-state lan-guage models fail to represent he hierarchical struc-ture of natural anguage.
Therefore, stochastic versionsof grammar formalisms tructurally more expressive areworth investigating.
Baker \[2\] generalized the parameterestimation methods for HMMs to stochastic ontext-freegrammars (SCFGs) \[3\] as the inside-outside algorithm.Unfortunately, the application of SCFGs and the inside-outside algorithm to natural-language modeling \[4, 5, 6\]has so far been inconclusive.Several reasons can be adduced for the difficulties.
First,each iteration of the inside-outside algorithm on a gram-mar with n nonterminals may require O(nalwl 3) time pertraining sentence w, while each iteration of its finite-statecounterpart training an HMM with s states requires atworst O(s2lwD time per training sentence.
Second, theconvergence properties of the algorithm sharply deteri-orate as the number of nonterminal symbols increases.This fact can be intuitively understood by observing thatthe algorithm searches for the maximum of a functionwhose number of local maxima grows with the numberof nonterminMs.
Finally, although SCFGs provide a hi-erarchical model of the language, that structure is unde-termined by raw text and only by chance will the inferredgrammar agree with qualitative linguistic judgments ofsentence structure.
For example, since in English textspronouns are very likely to immediately precede a verb,a grammar inferred from raw text will tend to togetherthe subject pronoun with the verb.We describe here an extension of the inside-outside algo-rithm that infers the parameters of a stochastic ontext-free grammar from a partially parsed corpus, thus pro-viding a tighter connection between the hierarchicalstructure of the inferred SCFG and that of the trainingcorpus.
The Mgorithm takes advantage of whatever con-stituent information is provided by the training corpusbracketing, ranging from a complete constituent analysisof the training sentences to the unparsed corpus used forthe original inside-outside algorithm.
In the latter case,the new algorithm reduces to the original one.Using a partiMly parsed corpus has several importantadvantages.
We empirically show that the use of par-tially parsed corpus can decrease the number of itera-tions needed to reach a solution.
We also exhibit caseswhere a good solution is found from partially parsed cor-pus but not from raw text.
Most importantly, the useof partially parsed corpus enables the Mgorithm to infergrammars that derive constituent boundaries that can-not be inferred from raw text.We first outline our extension of the inside-outside al-gorithm to partially parsed text, and then report pre-liminary experiments illustrating the advantages of theextended algorithm.2.
PART IALLY  BRACKETED TEXTInformally, a partially bracketed corpus is a set of sen-tences annotated with parentheses marking constituentboundaries that any analysis of the corpus should re-spect.
More precisely, we start from a corpus C con-sisting of bracketed strings, which are pairs c = (w, B)where w is a string and B is a bracketing of w. For con-venience, we will define the length of the bracketed stringcby \ [c \ [=\ [w I.Given a string w = wl ...wlw \[, a span ofw is a pair ofintegers (i, j )  with 0 _~ i < j _~ \[w\[.
By convention, span(i,j) delimits substring iwj = wi+l .
.
.wj  of w. We also122use the abbreviation itv for iWlto I.A bracketing B of a string w is a finite set of spans on w(that is, a finite set of pairs or integers (i, j )  with 0 _< i <j _< \[wl) satisfying a consistency condition that ensuresthat each span (i, j )  can be seen as delimiting a (sequenceof) constituents iwi.
The consistency condition is simplythat no two spans in a bracketing may overlap, where twospans (i, j )  and (k, I) overlap if either i < k < j < i ork < i < l < j.
We also say that two bracketings of thesame string are compatible if their union is consistent.Note that there is no requirement that a bracketing ofw describe fully the constituent structure of w. In fact,some or all sentences in a corpus may have empty brack-etings, in which case the new algorithm behaves like theoriginal one.To present he notion of compatibility between a deriva-tion and a bracketed string, we need first to define thespan of a symbol occurrence in a context-free derivation.Let (w, B) be a bracketed string, and a0 ~ al  =::> .
.
.am = w be a derivation of w for (S)CFG G. The spanof a symbol occurrence in aj  is defined inductively asfollows:?
I f j  = m, aj  = w E E*, and the span ofwi in aj is(i - 1, i).?
If j < m, then aj = /fAT, a j+l  = ~X1.
.
.XkT,where A ~ Xi ..- Xk is a production of G. Then thespan of A in aj is ( i l , jk),  where for each 1 < 1 < k,(iz,jt) is the span of Xz in aj+l.
The spans in aj ofthe symbol occurrences in/~ and 7 are the same asthose of the corresponding symbols in otj+t.A derivation of w is then compatible with a bracketing Bof w if no span of a symbol occurrence in the derivationoverlaps a span in B.3.
THE INS IDE-OUTSIDEALGORITHMThe inside-outside algorithm \[2\] is a reestimation proce-dure for the rule probabilities of a Chomsky normal-form(CNF) SCFG.
It takes as inputs an initial CNF SCFGand a training corpus of sentences and it iteratively rees-timates rule probabilities to maximize the probabilitythat the grammar used as a stochastic generator wouldproduce the corpus.A reestimation algorithm can be used both to refine theparameter estimates for a CNF SCFG derived by othermeans \[7\] or to infer a grammar from scratch.
In thelatter case, the initial grammar for the inside-outside al-gorithm consists of all possible CNF rules over given setsN of nonterminals and E of terminals, with suitable as-signed nonzero probabilities.
In what follows, we willtake N, E as fixed, n = \[NI, t = I~1, and assume enu-merations N = {A1, .
.
.
,An} and E = {bl , .
.
.
,bt},  withA1 the grammar start symbol.
A CNF SCFG over N, Ecan then be specified by the n s + nt probabilities Bp,q,rof each possible binary rule Ap ~ Aq Ar and Up,m ofeach possible unary rule A n ~ bin.
Since for each p theparameters Bp.q,r and Up,m are supposed to be the prob-abilities of different ways of expanding Ap, we must havethe for all 1 _< p_< n+ = I (1)qjrFor grammar inference, we give random initial values tothe parameters Bp,q,r and Up,m subject o the constraints(I).The intended meaning of rule probabilities in a SCFGis directly tied to the intuition of context-freeness: aderivation is assigned a probability which is the prod-uct of the probabilities of the rules used in each step ofthe derivation.
Context-freeness together with the com-mutativity of multiplication thus allow us to identify allderivations associated to the same parse tree, and wewill speak indifferently below of derivation and analy-sis (parse tree) probabilities.
Finally, the probability ofa sentence or sentential form is the sum of the proba-bilities of all its analyses (equivalently, the sum of theprobabilities of all of its leftmost derivations from thestart symbol).The basic idea of the inside-outside algorithm is to usethe current rule probabilities to estimate from the train-ing text the expected frequencies of certain derivationsteps, and then compute new rule probability estimatesas appropriate frequency ratios.
Therefore, each itera-tion of the algorithm starts by calculating estimates ofthe number of occurrences of the relevant configurationsin each of the sentences tv in the training corpus W.Because the frequency estimates are most convenientlycomputed as ratios of other frequencies, they are a bitloosely referred to as inside and outside probabilities.In the original inside-outside algorithm, for each tv EW, the inside probability I~(i, j)  estimates the likeli-hood that Ap derives iwj, while the outside probabilityO~(i,j) estimates the likelihood of deriving sententialform owi Apjw from the start symbol A1.
In adaptingthe algorithm to partially bracketed strings we must takeinto account he constraints that the bracketing imposeson possible derivations, and thus on possible phrases.Clearly, nonzero values for I~(i, j)  or O~(i,j) shouldonly be allowed if iwj is compatible with the bracketingof w, or, equivalently, if (i, j )  does not overlap any span123in the bracketing of w. Therefore, we will in the fol-lowing assume a bracketed corpus C, which as describedabove is a set of bracketed strings c = (w, B), and willmodify the standard formulae for the inside and outsideprobabilities and rule probability reestimation \[2, 4, 5\]to involve only constituents whose spans are compatiblewith string bracketings.
For this purpose, for each brack-eted string c = (w, B) we define the auxiliary function1 if (i,j) does not overlap any b 6 B6(i,j)= 0 otherwiseFor each bracketed sentence c in the training corpus, theinside probabilities of longer spans of c can be computedfrom those for shorter spans by the following recurrenceequations:I~(i- 1, i) = Up,~ where c = (w, B) and bm = wi (2)I;(i'k) = 5(i'k) E E Bp,q,rI~(i,j)I,~(j,k) (3)q,r i<j<kEquation (3) computes the expected relative frequencyof derivations of ~wk from Ap compatible with the brack-eting B of c = (w, B).
The multiplier 5(i, k) is 0 just incase (i, k) overlaps ome span in B, which is exactly whenAp cannot derive iwk compatibly with B.Similarly, the outside probabilities for shorter spans of ccan be computed from the inside probabilities and theoutside probabilities for longer spans by the followingrecurrence:1 i fp=lO$(0,1cl) = 0 otherwise.
(4)Or(i, k) =i--1+q~rkj--k+x(5)Once the inside and outside probabilities computed foreach sentence in the corpus, the reestimated probabilityof binary rules, /Jp,q,r, and the reestimated probabilityof unary rules, (/p,m, are computed using the followingreestimation formulae, which are just like the standardones \[2, 5, 4\] except for the use of bracketed strings in-stead of unbracketed ones:l ( Bp,q,rX~(i,j) )Z ?Bp,q,r : c6C O~i<j<k_<\[w\[ I,~(j, k)O~(i, k) (6)EP;/POcEC1~p,rn ~ cEC: l<i<\[c\[,?-(w,B),wi-b.
(7)E e;/e?c?Cwhere Pc is the probability assigned by the currentmodel to bracketed string cpc = I~(0, \[el)and P~ is the probability assigned by the current modelto the set of derivations compatible with c involving someinstance of nonterminal ApP;=0<i<j<lc\[The denominator of ratios (6) and (7) estimates theprobability that a compatible derivation of a bracketedstring in C will involve at least one expansion of nonter-minal Av.
The numerator of (6) estimates the probabil-ity that a compatible derivation of a bracketed string inC will involve rule Ap --~ Aq At, while the numerator of(7) estimates the probability that a compatible deriva-tion of a string in C will rewrite Ap to bin.
Thus (6)estimates the probability that a rewrite of Ap in a com-patible derivation of a bracketed string in C will use ruleAp ~ Aq At, and (7) estimates the probability that anoccurrence of Ap in a compatible derivation of a string inin C will be rewritten to bin.
Clearly, these are the bestcurrent estimates for the binary and unary rule proba-bilities.The process is then repeated with the reestimated prob-abilities until the increase in the estimated probabilityof the training text given the model becomes negligible,or, what amounts to the same, the decrease in the crossentropy estimate (log probability)ElogProb(c) E log I\[(0, Icl)He(C) = tee = tee (8)eEC cECbecomes negligible.
Note that for comparisons with theoriginal algorithm, we should use the cross entropy ofthe unbracketed text with respect o the grammar, not(8).4.
EXPERIMENTAL  EVALUATIONThe following experiments, although preliminary, givesome support to our earlier suggested advantages of theinside-outside algorithm for partially bracketed corpora.We start with a formal-language example used by Lariand Young \[4\] in a previous evaluation of the inside-outside algorithm.
In this case, training on a bracketed124corpus can lead to a good solution while no reasonablesolution is found training on raw text only.Then, using a naturally occurring corpus and its par-tially bracketed version provided by the Penn Treebank,we compare the bracketings assigned by grammars in-ferred from raw and from bracketed training materialwith the Penn Treebank bracketings.Together, the experiments support he view that train-ing on bracketed corpora can lead to better convergence,and the resulting rammars agree better with linguisticjudgments of sentence structure.4.1.
In fe r r ing  the  Pa l indrome LanguageWe consider first an artificial language discussed by Lariand Young \[4\].
Our training corpus consists of 100 sen-tences in the palindrome language L over two symbols aand bL = {ww a I we  {a,b}'}.randomly generated with the SCFGS?~ACS?~BDS?~AAS ?~ BBC~SAD -~ SB1 A --~ aB -L bThe initial grammar consists of all possible CNF rulesover five nonterminals and the terminals a and b (135rules), with a random assignment of initial probabilities.As shown in Figure 1, with an unbracketed training setthe log probability remains almost unchanged after 40iterations (from 1.57 to 1.43) and no useful solution isfound.
In contrast, with the same training set fullybracketed, the log probability of the inferred grammarcomputed on the raw text decreases rapidly (1.57 ini-tially, 0.87 after 22 iterations).
Similarly, the cross en-tropy estimate of the bracketed text with respect o thegrammar improves rapidly (2.85 initially, 0.87 after 22iterations).The inferred grammar models correctly the palindromelanguage.
Its high probability rules (p > 0.1, p/p '  > 30=o1.61.51.41.31.2I.I10.90.8 !
i i i RawBracketed - - "IIIIIi i i i i I i5 I0 15 20 25 30 35I terat ion40Figure 1: Convergence for the Palindrome Corpusfor any excluded rule pl) areS ~ ADS - -~CBB ~ SCD ~ SAA ~ bB ~ aC ~ aD ~ bwhich is a close to optimal CNF CFG for the palindromelanguage.The results on this grammar are quite sensitive to thesize and statistics of the training corpus and the ini-tial rule probability assignment.
In fact, for a couple ofchoices of initial grammar and corpus, the original algo-rithm yields somewhat better esults than the new one.However, in no experiment did the training on unparsedtext achieve nearly as good a result as that shown abovefor parsed text.4.2.
Exper iments  on  the  AT IS  CorpusWe also conducted an experiment on inferring rammarsfor the language consisting of part-of-speech sequences ofspoken-language transcriptions in the Texas Instrumentssubset of the Air Travel Information System (ATIS) cor-pus \[8\].
We take advantage of the availability of thehand-parsed version of the ATIS corpus provided by thePenn Treebank project \[9\] and use the correspondingbracketed corpus over parts of speech as training data.Out of the 770 bracketed sentences (7812 words) in thecorpus, we used 700 as training data and 70 (901 words)as test set.
The following is an example training string( ( ( VB ( DT ~NS ( Im ( ( ~ ) ( nCD)  ) ) ) ) ) .
)125corresponding to the parsed sentence(((\[List (the fares (for ((flight)(number 891)))))) .
)The initial grammar consists of all possible CNF rules(4095 rules) over 15 nonterminals (the same number asin the tree bank) and 48 terminals corresponding to theparts of speech used in the tree bank.We trained a random initial grammar twice, on the un-bracketed version of the training corpus yielding gram-mar GR, and on the bracketed training set, yieldinggrammar GB.Figure 2 shows that the convergence toGB is faster thanthe convergence to GR.
Even though the cross-entropyestimates for the raw training text with both grammarsare not that different after 50 iterations (3.0 for GB, 3.02for GR), the analyses assigned by the resulting rammarsto the test set are drastically different.To evaluate objectively the quality of the analysesyielded by a grammar G, we used a Viterbi-style parserto find the most likely analyses of the test set accordingto G, and computed the proportion of phrases in thoseanalyses that are compatible in the sense defined in Sec-tion 2 with the tree bank bracketings ofthe test set.
Thiscriterion is closely related to the "crossing parentheses"score of Black et al \[10\].
We found that that only 35%of the constituents in the most likely GR analyses of thetest set are compatible with tree bank bracketing, incon-trast to 88% of the constituents in the most likely GBanalysis.As a first example, GB gives the following bracketings:( ( ( I  (would (like (to (take (((Delta(flight number)) 83) (toAt lanta) ) ) ) ) ) )  .
)((What (is (((the cheapest) fare) (I(can get))))) ?
))Although the constituent ( he cheapest) is linguisti-cally wrong, the only constituent not compatible withthe tree bank bracketing is (Delta f l ight  number):(I would (like (to (take (Delta((flight number) 83)) (toAtlanta)))).
)(What ((is (the cheapest fare (I canget)))) ?
)In contrast, GR gives the following analyses for the samestrings, with 16 constituents incompatible with the treebank:(I (would (like ((to ((take (Deltaflight)) (number (83 ((to Atlanta).
)))))((What (((is the) cheapest) fare)) ((Ican) (get ?))))
) ) )Another example analysis for GB is((Tell (me (about (((the public)transportation) ((from SFO) (to(San Francisco))))))) .
)which is compatible with the tree bank one:~o34.64.44.243.83.63.43.23!
m !
!
!
!
!
!
!R a w  - -Bracketed - - -5 i0 15 20 25 30 35 40 45 50I terat ionFigure 2: Convergence for the ATIS CorpusIt is interesting to look at some the differences betweenGR and GB, as seen from the most likely analyses theyassign to certain sentences.
For readability, we give theanalyses in terms of the original words rather than partof speech tags.
((Tell me (about (the publictransportation ((from SF0) (to SanFrancisco))))).
)However, the most likely GR analysis has nine con-stituents incompatible with the tree bank:(Tell ((me (((about the) public)tramsportation)) ((from SF0) ((toSam) (Francisco . )
) ) ) )In this analysis, a Francisco and the final punctuationare places in a lowest-level constituent.
Since final punc-tuation is quite often preceded by a noun, a grammarinferred from raw text will tend to bracket he noun withthe punctuation mark.Even better results can be obtained by continuing thereestimation  bracketed text.
After 78 iterations, 91%of the constituents of the most likely parse of the testset are compatible with the tree bank bracketing.126This experiment illustrates the fact that althoughSCFGs provide a hierarchical model of the language,that structure is undetermined by raw text and only bychance will the inferred grammar agree with qualitativelinguistic judgments of sentence structure.
This prob-lem has also been previously observed with linguisticstructure inference methods based on mutual informa-tion.
Magerman and Marcus \[11\] propose to alleviatethis behavior by enforcing that a predetermined list ofpairs of words (such as verb-preposition, pronoun-verb)are never embraced by a constituent.
However, theseconstraints are stipulated in advance rather than beingautomatically derived from the training material, in con-trast with what we have shown to be possible with theinside-outside algorithm for partially bracketed corpora.5.
CONCLUSIONS AND FURTHERWORKWe have introduced a modification of the well-knowninside-outside algorithm for inferring the parameters ofastochastic ontext-free grammar that can take advantageof constituent information (constituent bracketing) in apartially bracketed corpus.The method has been successfully applied to SCFG in-ference for formal languages and for part-of-speech se-quences derived from the ATIS spoken-language corpus.The use of partially bracketed corpus can reduce thenumber of iterations required for convergence ofparame-ter reestimation.
In some cases, a good solution is foundfrom a bracketed corpus but not from raw text.
Most im-portantly, the use of partially bracketed natural corpusenables the algorithm to infer grammars pecifying lin-guistically reasonable constituent boundaries that can-not be inferred by the inside-outside algorithm on rawtext.These preliminary investigations could be extended inseveral ways.
First, it is important o determine thesensitivity of the training algorithm to the initial proba-bility assignments and training corpus, as well as to lackor misplacement of brackets.
We have started experi-ments in this direction, but reasonable statistical modelsof bracket elision and misplacement are lacking.Second, we would like to extend our experimvnts tolarger terminal vocabularies.
As is well-known, thisraises both computational nd data sparseness problems,so clustering of terminal symbols will be essential.Finally, this work does not address a central weaknessof SCFGs, their inability to represent lexical influenceson distribution except by a statistically and computa-tionally impractical proliferation of nonterminal sym-bols.
One might instead look into versions of the currentalgorithm for more lexically-oriented formalisms uch asstochastic lexicalized tree-adjoining grammars \[12\].ACKNOWLEGMENTSWe thank Aravind Joshi and Stuart Shieber for usefuldiscussions.
The second author is partially supported byDARPA Grant N0014-90-31863, ARO Grant DAAL03-89-C-0031 and NSF Grant IRI90-16592.REFERENCES1.
Rabiner, L. R. A tutorial on hidden Markov models andselected applications in speech recognition.
Proceedingso\] the IEEE, 77(2):257-285, February 1989.2.
Baker, J. Trainable grammars for speech recognition.In Wolf, J. J. and Klatt, D. H., editors, Speech com-munication papers presented at the 97 th Meeting of theAcoustical Society of America, MIT, Cambridge, MA,June 1979.3.
Booth, T. Probabilistic representation f formal lan-guages.
In Tenth Annual/EEE Symposium on Switchingand Automata Theory, October 1969.4.
Lari, K. and Young, S. J.
The estimation of stochas-tic context-free grammars using the Inside-Outside algo-rithm.
Computer Speech and Language, 4:35-56, 1990.5.
Jelinek, F., Lafferty, J. D., and Mercer, R. L. Basicmethods of probabilistic ontext free grammars.
Tech-nical Report RC 16374 (72684), IBM, Yorktown Heights,New York 10598, 1990.6.
Lari, K. and Young, S. J.
Applications of stochasticcontext-free grammars using the Inside-Outside algo-rithm.
Computer Speech and Language, 5:237-257, 1991.7.
Fujisaki, T., Jelinek, F., Cocke, J., Black, E., andNishino, T. A probabilistic parsing method for sen-tence disambiguation.
In Proceedings of the Interna-tional Workshop on Parsing Technologies, Pittsburgh,August 1989.8.
Hemphill, C. T., Godfrey, J. J., and Doddington, G. R.The ATIS spoken language systems pilot corpus.
InDARPA Speech and Natural Language Workshop, Hid-den Valley, Pennsylvania, June 1990.9.
Brill, E., Magerman, D., Marcus, M., and Santorini,B.
Deducing linguistic structure from the statistics oflarge corpora.
In DARPA Speech and Natural LanguageWorkshop.
Morgan Kaufmann, Hidden Valley, Pennsyl-vania, June 1990.10.
Black, E., Abney, S., Flickenger, D., Grishman, R., har-rison, P., Hindle, D., Ingria, R., Jelinek, F., Klavans,J., Liberman, M., Marcus, M., Roukos, S., Santorini,B., and Strzalkowski, T. A procedure for quantita-tively comparing the syntactic overage of english gram-mars.
In DARPA Speech and Natural Language Work-shop, pages 306-311, Pacific Grove, California, 1991.Morgan Kaufmann.11.
Magerman, D. and Marcus, M. Parsing a natural an-guage using mutual information statistics.
In AAAI-90,Boston, MA, 1990.12.
Schabes, Y. Stochastic lexicalized tree-adjoining gram-mars.
Also in these proceedings, 1992.127
