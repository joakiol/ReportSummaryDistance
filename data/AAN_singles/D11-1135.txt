Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1456?1466,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsStructured Relation Discovery using Generative ModelsLimin Yao?
Aria Haghighi+ Sebastian Riedel?
Andrew McCallum??
Department of Computer Science, University of Massachusetts at Amherst+ CSAIL, Massachusetts Institute of Technology{lmyao,riedel,mccallum}@cs.umass.edu{aria42}@csail.mit.eduAbstractWe explore unsupervised approaches to rela-tion extraction between two named entities;for instance, the semantic bornIn relation be-tween a person and location entity.
Con-cretely, we propose a series of generativeprobabilistic models, broadly similar to topicmodels, each which generates a corpus of ob-served triples of entity mention pairs and thesurface syntactic dependency path betweenthem.
The output of each model is a cluster-ing of observed relation tuples and their as-sociated textual expressions to underlying se-mantic relation types.
Our proposed modelsexploit entity type constraints within a relationas well as features on the dependency path be-tween entity mentions.
We examine effective-ness of our approach via multiple evaluationsand demonstrate 12% error reduction in preci-sion over a state-of-the-art weakly supervisedbaseline.1 IntroductionMany NLP applications would benefit from largeknowledge bases of relational information aboutentities.
For instance, knowing that the entitySteve Balmer bears the leaderOf relation to theentity Microsoft, would facilitate question answer-ing (Ravichandran and Hovy, 2002), data mining,and a host of other end-user applications.
Due tothese many potential applications, relation extrac-tion has gained much attention in information ex-traction (Kambhatla, 2004; Culotta and Sorensen,2004; Mintz et al, 2009; Riedel et al, 2010; Yao etal., 2010).
We propose a series of generative prob-abilistic models, broadly similar to standard topicmodels, which generate a corpus of observed triplesof entity mention pairs and the surface syntactic de-pendency path between them.
Our proposed mod-els exploit entity type constraints within a relationas well as features on the dependency path betweenentity mentions.
The output of our approach is aclustering over observed relation paths (e.g.
?X wasborn in Y?
and ?X is from Y?)
such that expressionsin the same cluster bear the same semantic relationtype between entities.Past work has shown that standard supervisedtechniques can yield high-performance relation de-tection when abundant labeled data exists for afixed inventory of individual relation types (e.g.leaderOf ) (Kambhatla, 2004; Culotta and Sorensen,2004; Roth and tau Yih, 2002).
However, less ex-plored are open-domain approaches where the setof possible relation types are not fixed and little tono labeled is given for each relation type (Banko etal., 2007; Banko and Etzioni, 2008).
A more re-lated line of research has explored inducing rela-tion types via clustering.
For example, DIRT (Linand Pantel, 2001) aims to discover different repre-sentations of the same semantic relation using dis-tributional similarity of dependency paths.
Poonand Domingos (2008) present an Unsupervised se-mantic parsing (USP) approach to partition depen-dency trees into meaningful fragments (or ?parts?to use their terminology).
The combinatorial natureof this dependency partition model makes it difficultfor USP to scale to large data sets despite severalnecessary approximations during learning and infer-1456ence.
Our work is similar to DIRT and USP in thatwe induce relation types from observed dependencypaths, but our approach is a straightforward andprincipled generative model which can be efficientlylearned.
As we show empirically, our approach out-performs these related works when trained with thesame amount of data and further gains are observedwhen trained with more data.We evaluate our approach using ?intrinsic?
clus-tering evaluation and ?extrinsic?
evaluation settings.1The former evaluation is performed using subset ofinduced clusters against Freebase relations, a largemanually-built entity and relational database.
Wealso show some clusters which are not included asFreebase relations, as well as some entity clustersfound by our approach.
The latter evaluation usesthe clustering induced by our models as features forrelation extraction in distant supervision framework.Empirical results show that we can find coherentclusters.
In relation extraction, we can achieve 12%error reduction in precision over a state-of-the-artweakly supervised baseline and we show that usingfeatures from our proposed models can find morefacts for a relation without significant accuracy loss.2 Problem and Experimental SetupThe task of relation extraction is mapping surfacetextual relations to underlying semantic relations.For instance, the textual expression ?X was born inY?
indicates a semantic relation bornIn between en-tities ?X?
and ?Y?.
This relation can be expressedtextually in several ways: for instance, ?X, a nativeof Y?
or ?X grew up in Y?.
There are several com-ponents to a coherent relation type, including a tightsmall number of textual expressions as well as con-straints on the entities involved in the relation.
Forinstance, in the bornIn relation ?X?
must be a personentity and ?Y?
a location (typically a city or nation).In this work, we present an unsupervised probabilis-tic generative model for inducing clusters of relationtypes and recognizing their textual expressions.
Theset of relation types is not pre-specified but inducedfrom observed unlabeled data.
See Table 4 for ex-amples of learned semantic relations.Our observed data consists of a corpus of docu-ments and each document is represented by a bag1See Section 4 for a fuller discussion of evaluation.of relation tuples.
Each tuple represents an ob-served syntactic relationship between two NamedEntities (NE) and consists of three components: thedependency path between two NE mentions, thesource argument NE, and the destination argumentNE.
A dependency path is a concatenation of depen-dency relations (edges) and words (nodes) along apath in a dependency tree.
For instance, the sentence?John Lennnon was born in Liverpool?
would yieldthe relation tuple (Lennon, [?
?nsubjpass, born, ?
?in], Liverpool).
This relation tuple reflects a se-mantic bornIn relation between the John Lennon andLiverpool entities.
The dependency path in this ex-ample corresponds to the ?X was born in Y?
textualexpression given earlier.
Note that for the above ex-ample, the bornIn relation can only occur between aperson and a location.
The relation tuple is the pri-mary observed random variable in our model and weconstruct our models (see Section 3) so that clustersconsist of textual expressions representing the sameunderlying relation type.3 ModelsWe propose three generative models for modelingtuples of entity mention pairs and the syntactic de-pendency path between them (see Section 2).
Thefirst two models, Rel-LDA and Rel-LDA1 are sim-ple extensions of the standard LDA model (Blei etal., 2003).
At the document level, our model is iden-tical to standard LDA; a multinomial distributionis drawn over a fixed number of relation types R.Changes lie in the observations.
In standard LDA,the atomic observation is a word drawn from a la-tent topic distribution determined by a latent topicindicator variable for that word position.
In our ap-proach, a document consists of an exchangeable setof relation tuples.
Each relation tuple is drawn froma relation type ?topic?
distribution selected by a la-tent relation type indicator variable.
Relation tuplesare generated using a collection of independent fea-tures drawn from the underlying relation type distri-bution.
These changes to standard LDA are intendedto have the effect that instead of representing seman-tically related words, the ?topic?
latent variable rep-resents a relation type.Our third model exploits entity type constraintswithin a relation and induces clusters of relations1457and entities jointly.
For each tuple, a set of rela-tion level features and two latent entity type indica-tors are drawn independently from the relation typedistribution; a collection of entity mention featuresfor each argument is drawn independently from theentity type distribution selected by the entity typeindicator.Path X, made by YSource Gamma KnifeDest ElektaTrigger makeLex , made by the Swedishmedical technology firmPOS , VBN IN DT JJ JJ NN NNNER pair MISC-ORGSync pair partmod-pobjTable 1: The features of tuple ?
(Gamma Knife, madeby, Elekta)?
in sentence ?Gamma Knife, made by theSwedish medical technology firm Elekta, focuses low-dosage gamma radiation ...?3.1 Rel-LDA ModelThis model is an extension to the standard LDAmodel.
At the document level, a multinomial dis-tribution over relations ?doc is drawn from a priorDir(?).
To generate a relation tuple, we first draw arelation ?topic?
r from Multi(?).
Then we generateeach feature f of a tuple independently from a multi-nomial distribution Multi(?rf ) selected by r. In thismodel, each tuple has three features, i.e.
its threecomponents, shown in the first three rows in Table 1.Figure 1 shows the graphical representation of Rel-LDA.
Table 2 lists all the notation used in describingour models.The learning process of the models is an EM pro-cess.
The procedure is similar to that used by thestandard topic model.
In the variational E-step (in-ference), we sample the relation type indicator foreach tuple using p(r|f):P (r|f(p, s, d)) ?
p(r)?f p(f |r)?
(?r + nr|d)?f?f+nf |rPf ?
(?f ?+nf ?|r)|R| Number of relations|D| Number of documentsr A relationdoc A documentp, s, d Dep path, source and dest argsf A feature/feature typeT Entity type of one argument?
Dirichlet prior for ?doc?x Dirichlet prior for ?rx?
Dirichlet prior for ?t?doc p(r|doc)?rx p(x|r)?t p(fs|T ), p(fd|T )Table 2: The notation used in our models|R|......Nrf??rf?
?ff|D|Figure 1: Rel-LDA model.
Shaded circles are observa-tions, and unshaded ones are hidden variables.
A docu-ment consists of N tuples.
Each tuple has a set of fea-tures.
Each feature of a tuple is generated independentlyfrom a hidden relation variable r.p(r) and p(f |r) are estimated in the M-step:?doc =?+ nr|doc?r?
(?+ nr?|doc)?rf =?f + nf |r?f ?
(?f ?
+ nf ?|r)where nf |r indicates the number of times a feature fis assigned with r.3.2 Rel-LDA1Looking at results of Rel-LDA, we find the clus-ters sometimes are in need of refinement, and wecan address this by adding more features.
For in-stance, adding trigger features can encourage spar-sity over dependency paths.
We define trigger wordsas all the words on the dependency path except stopwords.
For example, from path ?X, based in Y?,?base?
is extracted as a trigger word.
The intuition1458for using trigger words is that paths sharing the sameset of trigger words should go to one cluster.
Addingnamed entity tag pair can refine the cluster too.
Forexample, a cluster found by Rel-LDA contains ?Xwas born in Y?
and ?X lives in Y?
; but it also con-tains ?X, a company in Y?.
In this scenario, addingfeatures ?PER-LOC?
and ?ORG-LOC?
can push themodel to split the clusters into two and put the thirdcase into a new cluster.Hence we propose Rel-LDA1.
It is similar toRel-LDA, except that each tuple is represented withmore features.
Besides p, s, and d, we introducetrigger words, lexical pattern, POS tag pattern, thenamed entity pair and the syntactic category pair fea-tures for each tuple.
Lexical pattern is the word se-quence between the two arguments of a tuple andPOS tag pattern is the POS tag sequence of the lexi-cal pattern.
See Table 1 as an example.Following typical EM learning(Charniak and El-sner, 2009), we start with a much simpler genera-tive model, expose the model to fewer features first,and iteratively add more features.
First, we train aRel-LDA model, i.e.
the model only generates thedependency path, source and destination arguments.After each interval of 10 iterations, we introduce oneadditional feature.
We add the features in the orderof trigger, lexical pattern, POS, NER pair, and syn-tactic pair.3.3 Type-LDA modelWe know that relations can only hold betweencertain entity types, known as selectional prefer-ences (Ritter et al, 2010; Seaghdha, 2010; Kozarevaand Hovy, 2010).
Hence we propose Type-LDAmodel.
This model can capture the selectional pref-erences of relations to their arguments.
In the meantime, it clusters tuples into relational clusters, andarguments into different entity clusters.
The entityclusters could be interesting in many ways, for ex-ample, defining fine-grained entity types and findingnew concepts.We split the features of a tuple into relation levelfeatures and entity level features.
Relation level fea-tures include the dependency path, trigger, lex andPOS features; entity level features include the entitymention itself and its named entity tag.The generative storyline is as follows.
At the doc-ument level, a multinomial distribution over rela-N|D||R|rffs??rf?tfd|R|?rt2??t2?
?fT1T2|T||R|?rt1?t1Figure 2: Type-LDA model.
Each document consists ofN tuples.
Each tuple has a set of features, relation levelfeatures f and entity level features of source argument fsand destination argument fd.
Relation level features andtwo hidden entity types T1 and T2 are generated fromhidden relation variable r independently.
Source entityfeatures are generated from T1 and destination featuresare generated from T2.tions ?doc is drawn from a Dirichlet prior.
A doc-ument consists of N relation tuples.
Each tuple isrepresented by relation level features (f ) and entitylevel features of source argument (fs) and destina-tion argument (fd).
For each tuple, a relation r isdrawn from Multi(?doc).
The relation level featuresand two hidden entity types T1 and T2 are indepen-dently generated from r. Features fs are generatedfrom T1 and fd from T2.
Figure 2 shows the graphi-cal representation of this model.At inference time, we sample r, T1 and T2 foreach tuple.
For efficient inference, we first initializethe model without T1 and T2, i.e.
all the features aregenerated directly from r. Here the model degener-ates to Rel-LDA1.
After some iterations, we intro-duce T1 and T2.
We sample the relation variable (r)and two mention types variables (T1,T2) iterativelyfor each tuple.
We can sample them together, butthis is not very efficient.
In addition, we found thatit does not improve performance.4 ExperimentsOur experiments are carried out on New York Timesarticles from year 2000 to 2007 (Sandhaus, 2008).We filter out some noisy documents, for example,1459obituary content, lists and so on.
Obituary arti-cles often contain syntax that diverges from stan-dard newswire text.
This leads to parse errors withWSJ-trained parsers and in turn, makes extractionharder.
We also filter out documents that containlists or tables of items (such as books, movies) be-cause this semi-structured information is not the fo-cus of our current work.
After filtering we are leftwith approximately 428K documents.
They are pre-processed in several steps.
First we employ Stanfordtools to tokenize, sentence-split and Part-Of-Speechtag (Toutanova et al, 2003) a document.
Next werecognize named entities (Finkel et al, 2005) bylabelling tokens with PERSON, ORGANIZATION,LOCATION, MISC and NONE tags.
Consecutivetokens which share the same category are assembledinto entity mentions.
They serve as source and des-tination arguments of the tuples we seek to model.Finally we parse each sentence of a document usingMaltParser (Nivre et al, 2004) and extract depen-dency paths for each pair of named entity mentionsin one sentence.Following DIRT (Lin and Pantel, 2001), we fil-ter out tuples that do not satisfy the following con-straints.
First, the path needs to be shorter than10 edges, since longer paths occur less frequently.Second, the dependency relations in the path shouldconnect two content words, i.e.
nouns, verbs, ad-jectives and adverbs.
For example, in phrase ?solvea problem?, ?obj(solve, problem)?
is kept, while?det(problem, a)?
is discarded.
Finally, the de-pendency labels on the path must not be: ?conj?,?ccomp?, ?parataxis?, ?xcomp?, ?pcomp?, ?advcl?,?punct?, and ?infmod?.
This selection is based on theobservation that most of the times the correspondingdependency relations do not explicitly state a rela-tion between two candidate arguments.After all entity mentions are generated and pathsare extracted, we have nearly 2.5M tuples.
Afterclustering (inference), each of these tuple will be-long to one cluster/relation and is associated with itsclusterID.We experimented with the number of clusters andfind that in a range of 50-200 the performance doesnot vary significantly with different numbers.
In ourexperiments, we cluster the tuples into 100 relationclusters for all three models.
For Type-LDA model,we use 50 entity clusters.We evaluate our models in two ways.
The firstaims at measuring the clustering quality by mappingclusters to Freebase relations.
The second seeks toassess the utility of our predicted clusters as featuresfor relation extraction.4.1 Relations discovered by different modelsLooking closely at the clusters we predict, we findthat some of them can be mapped to Freebase rela-tions.
We discover clusters that roughly correspondto the parentCom (parent company relation), filmDi-rector, authorOf, comBase (base of a company rela-tion) and dieIn relations in Freebase.
We treat Free-base annotations as ground truth and measure recall.We count each tuple in a cluster as true positive ifFreebase states the corresponding relation betweenits argument pair.
We find that precision numbersagainst Freebase are low, below 10%.
However,these numbers are not reliable mainly because manycorrect instances found by our models are missingin Freebase.
One reason why our predictions aremissing in Freebase is coreference.
For example,we predict parentCom relation between ?Linksys?and ?Cisco?, while Freebase only considers ?CiscoSystems, Inc.?
as the parent company of ?Linksys?.It does not corefer ?Cisco?
to ?Cisco Systems, Inc.?.Incorporating coreference in our model may fix thisproblem and is a focus of future work.
Instead ofmeasuring precision against Freebase, we ask hu-mans to label 50 instances for each cluster and reportprecision according to this annotated data.
Table 3shows the scores.We can see that in most cases Rel-LDA1 andType-LDA substantially outperform the Rel-LDAmodel.
This is due to the fact that both models canexploit more features to make clustering decisions.For example, in Rel-LDA1 model, the NER pair fea-ture restricts the entity types the two arguments cantake.In the following, we take parentCom relation asan example to analyze the behaviors of differentmodels.
Rel-LDA includes spurious instances suchas ?A is the chief executive of B?, while Rel-LDA1has fewer such instances due to the NER pair fea-ture.
Similarly, by explicitly modeling entity typeconstraints, Type-LDA makes fewer such errors.
Allour models make mistakes when sentences have co-ordination structures on which the parser has failed.1460Rel.
Sys.
Rec.
Prec.parentComRel-LDA 51.4 76.0Rel-LDA1 49.5 78.0Type-LDA 55.3 72.0filmDirectorRel-LDA 42.5 32.0Rel-LDA1 70.5 40.0Type-LDA 74.2 26.0comBaseRel-LDA 31.5 12.0Rel-LDA1 54.2 22.0Type-LDA 57.1 30.0authorOfRel-LDA 25.2 84.0Rel-LDA1 46.9 86.0Type-LDA 20.2 68.0dieInRel-LDA 26.5 34.0Rel-LDA1 55.9 40.0Type-LDA 50.2 28.0Table 3: Clustering quality evaluation (%), Rec.
is mea-sured against Freebase, Prec.
is measured according tohuman annotatorsFor example, when a sentence has the following pat-tern ?The winners are A, a part of B; C, a part ofD; E, a part of F?, our models may predict parent-Com(A,F), because the parser connects A with F viathe pattern ?a part of?.Some clusters found by our models cannot bemapped to Freebase relations.
Consider the Free-base relation worksFor as one example.
This re-lation subsumes all types of employment relation-ships, irrespective of the role the employee plays forthe employer.
By contrast, our models discover clus-ters such as leaderOf, editorOf that correspond tomore specific roles an employee can have.
We showsome example relations in Table 4.
In the table, the2nd row shows a cluster of employees of news mediacompanies; the 3rd row shows leaders of companies;the last one shows birth and death places of persons.We can see that the last cluster is noisy since wedo not handle antonyms in our models.
The argu-ments of the clusters have noise too.
For example,?New York?
occurs as a destination argument in the2nd cluster.
This is because ?New York?
has highfrequency in the corpus and it brings noise to theclustering results.
In Table 5 some entity clustersfound by Type-LDA are shown.
We find differenttypes of companies, such as financial companies andnews companies.
We also find subclasses of person,for example, reviewer and politician, because thesedifferent entity classes participate in different rela-tions.
The last cluster shown in the table is a mix-ture of news companies and government agencies.This may be because this entity cluster is affectedby many relations.4.2 Distant Supervision based RelationExtractionOur generative models detect clusters of dependencypaths and their arguments.
Such clusters are inter-esting in their own right, but we claim that they canalso be used to help a supervised relation extractor.We validate this hypothesis in the context of relationextraction with distant supervision using predictedclusters as features.Following previous work (Mintz et al, 2009), weuse Freebase as our distant supervision source, andalign related entity pairs to the New York Times arti-cles discussed earlier.
Our training and test instancesare pairs of entities for which both arguments appearin at least one sentence together.
Features of eachinstance are extracted from all sentences in whichboth entities appear together.
The gold label for eachinstance comes from Freebase.
If a pair of entitiesis not related according to Freebase, we consider ita negative example.
Note that this tends to createsome amount of noise: some pairs may be related,but their relationships are not yet covered in Free-base.After filtering out relations with fewer than 10 in-stances we have 65 relations and an additional ?O?label for unrelated pairs of entities.
We call relatedinstances positive examples and unrelated instancesnegative examples.We train supervised classifiers using maximumentropy.
The baseline classifier employs featuresthat Mintz et al (2009) used.
To extract featuresfrom the generative models we proceed as follows.For each pair of entities, we collect all tuples asso-ciated with it.
For each of these tuples we extract itsclusterID, and use this ID as a binary feature.The baseline system without generative modelfeatures is called Distant.
The classifiers with ad-ditional features from generative models are namedafter the generative models.
Thus we have Rel-LDA,Rel-LDA1 and Type-LDA classifiers.
We compare1461Source New York, Euro RSCG Worldwide, BBDO Worldwide, American, DDB WorldwidePath X, a part of Y; X, a unit of Y; X unit of Y; X, a division of Y; X is a part of YDest Omnicom Group, Interpublic Group of Companies, WPP Group, Publicis GroupeSource Supreme Court, Anna Wintour, William Kristol, Bill Keller, Charles McGrathPath X, an editor of Y; X, a publisher of Y; X, an editor at Y; X, an editor in chief of Y; X is an editor of Y;Dest The Times, The New York Times, Vogue, Vanity Fair, New YorkSource Kenneth L. Lay, L. Dennis Kozlowski, Bernard J. Ebbers, Thomas R. Suozzi, Bill GatesPath X, the executive of Y; X, Y?s executive; X, Y executive; X, the chairman of Y; X, Y?s chairmanDest Enron, Microsoft, WorldCom, Citigroup, Nassau CountySource Paul J. Browne, John McArdle, Tom Cocola, Claire Buchan, Steve SchmidtPath X, a spokesman for Y; X, a spokeswoman for Y; X, Y spokesman; X, Y spokeswoman; X, a commissioner of YDest White House, Justice Department, Pentagon, United States, State DepartmentSource United Nations, Microsoft, Intel, Internet, M. D. AndersonPath X, based in Y; X, which is based in Y; X, a company in Y; X, a company based in Y; X, a consultant in YDest New York, Washington, Manhattan, Chicago, LondonSource Army, Shiite, Navy, John, DavidPath X was born in Y; X die at home in Y; X die in Y; X, son of Y; X die at YDest Manhattan, World War II, Brooklyn, Los Angeles, New YorkTable 4: The path, source and destination arguments of some relations found by Rel-LDA1.Company Microsoft, Enron, NBC, CBS, DisneyFinanceCom Merrill Lynch, Morgan Stanley, Goldman Sachs, Lehman Brothers, Credit Suisse First BostonNews Notebook, New Yorker, Vogue, Vanity Fair, NewsweekSportsTeam Yankees, Mets, Giants, Knicks, JetsUniversity University of California, Harvard, Columbia University, New York University, University of Penn.Art Reviewer Stephen Holden, Ken Johnson, Roberta Smith, Anthony Tommasini, Grace GlueckGames World Series, Olympic, World Cup, Super Bowl, OlympicsPolitician Eliot Spitzer, Ari Fleischer, Kofi Annan, Scott McClellan, Karl RoveGov.
Agency Congress, European Union, NATO, Federal Reserve, United States Court of AppealsNews/Agency The New York Times, The Times, Supreme Court, Security Council, Book ReviewTable 5: The entity clusters found by Type-LDAthese against Distant and the DIRT database.
Forthe latter we parse our data using Minipar (Lin,1998) and extract dependency paths between pairsof named entity mentions.
For each path, the top 3similar paths are extracted from DIRT database.
TheMinipar path and the similar paths are used as addi-tional features.For held-out evaluation, we construct the trainingdata from half of the positive examples and half ofthe negative examples.
The remaining examples areused as test data.
Note that the number of negativeinstances is more than 10 times larger than the num-ber of positive instances.
At test time, we rank thepredictions by the conditional probabilities obtainedfrom the Maximum Entropy classifier.
We reportprecision of top ranked 50 instances for each relationin table 6.
From the table we can see that all systemsusing additional features outperform the Distant sys-tem.
In average, our best model achieves 4.1%improvement over the distant supervision baseline,12% error reduction.
The precision of bornIn is lowbecause in most cases we predict bornIn instancesas liveIn.We expect systems using generative model fea-tures to have higher recall than the baseline.
Thisis difficult to measure, but precision in the high re-call area is a signal.
We look at top ranked 1000instances of each system and show the precision inthe last row of the table.
We can see that our bestmodel Type-LDA outperforms the distant supervi-sion baseline by 4.5%.Why do generative model features help to im-1462Relation Dist Rel Rel1 Type DIRTworksFor 80.0 92.0 86.0 90.0 84.0authorOf 98.0 98.0 98.0 98.0 98.0containedBy 92.0 96.0 96.0 92.0 96.0bornIn 16.0 18.0 22.0 24.0 10.0dieIn 28.0 30.0 28.0 24.0 24.0liveIn 50.0 52.0 54.0 54.0 56.0nationality 92.0 94.0 90.0 90.0 94.0parentCom 94.0 96.0 96.0 96.0 90.0founder 65.2 76.3 61.2 64.0 68.3parent 52.0 54.0 50.0 52.0 52.0filmDirector 54.0 60.0 60.0 64.0 62.0Avg 65.6 69.7 67.4 68.0 66.8Prec@1K 82.8 85.8 85.3 87.3 82.8Table 6: Precision (%) of some frequent relationsprove relation extraction?
One reason is that gen-erative models can transfer information from knownpatterns to unseen patterns.
For example, given?Sidney Mintz, the great food anthropologist atJohns Hopkins University?, we want to predict therelation between ?Sidney Mintz?
and ?Johns HopkinsUniversity?.
The distant supervision system incor-rectly predicts the pair as ?O?
since it has not seenthe path ?X, the anthropologist at Y?
in the trainingdata.
By contrast, Rel-LDA can predict this pair cor-rectly as worksFor because the dependency path ofthis pair is in a cluster which contains the path ?X, aprofessor at Y?.In addition to held-out evaluation we also carryout manual evaluation.
To this end, we use all thepositive examples and randomly select five timesthe number of positive examples as negative ex-amples to train a classifier.
The remaining nega-tive examples are candidate instances.
We rank thepredicted instances according to their classificationscores.
For each relation, we ask human annotatorsto judge its top ranked 50 instances.Table 7 lists the manual evaluation results forsome frequent relations.
We also list how many in-stances are found for each relation.
For almost allthe relations, systems using generative model fea-tures find more instances.
In terms of precision, ourmodels perform comparatively to the baseline, evenbetter for some relations.We also notice that clustering quality is not con-sistent with distant supervision performance.
Rel-LDA1 can find better clusters than Rel-LDA but ithas lower precision in held-out evaluation.
Type-LDA underperforms Rel-LDA in average precisionbut it gets higher precision in a higher recall area, i.e.precision at 1K.
One possible reason for the incon-sistency is that the baseline distant supervision sys-tem already employs features that are used in Rel-LDA1.
Another reason may be that the clusters donot overlap with Freebase relations very well, seesection 4.1.4.3 Comparing against USPWe also try to compare against USP (Poon andDomingos, 2008).
Due to memory requirements ofUSP, we are only able to run it on a smaller dataset consisting of 1,000 NYT documents; this is threetimes the amount of data Poon and Domingos (2008)used to train USP.2 For distant supervision based re-lation extraction, we only match about 500 Freebaseinstances to this small data set.USP provides a parse tree for each sentence andfor each mention pair we can extract a path fromthe tree.
Since USP provides clusters of words andphrases, we use the USP clusterID associated withthe words on the path as binary features in the clas-sifier.All models are less accurate when trained on thissmaller dataset; we can do as well as USP does,even a little better.
USP achieves 8.6% in F1, Rel-LDA 8.7%, Rel-LDA1 10.3%, Type-LDA 8.9% andDistant 10.3%.
Of course, given larger datasets,the performance of Rel-LDA, Rel-LDA1, and Type-LDA improves considerably.
In summary, compar-ing against USP, our approach scales much moreeasily to large data.5 Related WorkMany approaches have been explored in relation ex-traction, including bootstrapping, supervised classi-fication, distant supervision, and unsupervised ap-proaches.Bootstrapping employs a few labeled examplesfor each relation, iteratively extracts patterns fromthe labeled seeds, and uses the patterns to extract2Using the publicly released USP code, training a modelwith 1,000 documents resulted in about 45 gigabytes of heapspace in the JVM.1463Relation Top 50 (%) #InstancesDist Rel Type Dist Rel TypeworksFor 100.0 100.0 100.0 314 349 349authorOf 94.0 94.0 96.0 185 208 229containedBy 98.0 98.0 98.0 670 714 804bornIn 82.6 88.2 88.0 46 36 56dieIn 100.0 100.0 100.0 167 176 231liveIn 98.0 98.0 94.0 77 86 109nationality 78.0 82.0 76.0 84 92 114parentCom 79.2 77.4 85.7 24 31 28founder 80.0 80.0 50.0 5 5 14parent 97.0 92.3 94.7 33 39 38filmDirector 92.6 96.9 97.1 27 32 34Table 7: Manual evaluation, Precision and recall of some frequent relationsmore seeds (Brin, 1998).
This approach may sufferfrom low recall since the patterns can be too specific.Supervised learning can discover more generalpatterns (Kambhatla, 2004; Culotta and Sorensen,2004).
However, this approach requires labeled data,and most work only carry out experiments on smalldata set.Distant supervision for relation extraction re-quires no labeled data.
The approach takes someexisting knowledge base as supervision source,matches its relational instances against the text cor-pus to build the training data, and extracts new in-stances using the trained classifiers (Mintz et al,2009; Bunescu and Mooney, 2007; Riedel et al,2010; Yao et al, 2010).All these approaches can not discover new rela-tions and classify instances which do not belong toany of the predefined relations.
Other past work hasexplored inducing relations using unsupervised ap-proaches.For example, DIRT (Lin and Pantel, 2001) aimsto discover different representations of the same se-mantic relation, i.e.
similar dependency paths.
Theyemploy the distributional similarity based approachwhile we use generative models.
Both DIRT and ourapproach take advantage of the arguments of depen-dency paths to find semantic relations.
Moreover,our approach can cluster the arguments into differ-ent types.Unsupervised semantic parsing (USP) (Poon andDomingos, 2008) discovers relations by mergingpredicates which have similar meanings; it proceedsto recursively cluster dependency tree fragments (or?parts?)
to best explain the observed sentence.
It isnot focused on capturing any particular kind of re-lation between sentence constituents, but to capturerepeated patterns.
Our approach differs in that weare focused on capturing a narrow range of binaryrelations between named entities; some of our mod-els (see Section 3) utilize entity type information toconstraint relation type induction.
Also, our modelsare built to be scalable and trained on a very largecorpus.
In addition, we use a distant supervisionframework for evaluation.Relation duality (Bollegala et al, 2010) employsco-clustering to find clusters of entity pairs and pat-terns.
They identify each cluster of entity pairs as arelation by selecting representative patterns for thatrelation.
This approach is related to our models,however, it does not identify any entity clusters.Generative probabilistic models are widely em-ployed in relation extraction.
For example, they areused for in-domain relation discovery while incorpo-rating constraints via posterior regularization (Chenet al, 2011).
We are focusing on open domain re-lation discovery.
Generative models are also ap-plied to selectional preference discovery (Ritter etal., 2010; Seaghdha, 2010).
In this scenario, theauthors assume relation labels are given while weautomatically discover relations.
Generative modelsare also used in unsupervised coreference (Haghighiand Klein, 2010).1464Clustering is also employed in relation extraction.Hasegawa et al (2004) cluster pairs of named en-tities according to the similarity of context wordsintervening between them.
Their approach is notprobabilistic.
Researchers also use topic models toperform dimension reduction on features when theycluster relations (Hachey, 2009).
However, they donot explicitly model entity types.Open information extraction aims to discover re-lations independent of specific domains and rela-tions (Banko et al, 2007; Banko and Etzioni, 2008).A self-learner is employed to extract relation in-stances but the systems do not cluster the instancesinto relations.
Yates and Etzioni (2009) present RE-SOLVER for discovering relational synonyms as apost processing step.
Our approach integrates entityand relation discovery in a probabilistic model.6 ConclusionWe have presented an unsupervised probabilisticgenerative approach to relation extraction betweentwo named entities.
Our proposed models exploitentity type constraints within a relation as wellas features on the dependency path between entitymentions to cluster equivalent textual expressions.We demonstrate the effectiveness of this approachby comparing induced relation clusters against alarge knowledge base.
We also show that using clus-ters of our models as features in distant supervisedframework yields 12% error reduction in precisionover a weakly supervised baseline and outperformsother state-of-the art relation extraction techniques.AcknowledgmentsThis work was supported in part by the Centerfor Intelligent Information Retrieval and the Uni-versity of Massachusetts gratefully acknowledgesthe support of Defense Advanced Research ProjectsAgency (DARPA) Machine Reading Program un-der Air Force Research Laboratory (AFRL) primecontract no.
FA8750-09-C-0181, ITR#1, and NSFMALLET.
Any opinions, findings, and conclusionor recommendations expressed in this material arethose of the author(s) and do not necessarily reflectthe view of the DARPA, AFRL, or the US govern-ment.
Any opinions, findings and conclusions orrecommendations expressed in this material are theauthors?
and do not necessarily reflect those of thesponsor.ReferencesMichele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InProceedings of ACL-08: HLT.Michele Banko, Michael J Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open in-formation extraction from the web.
In Proceedings ofIJCAI2007.David Blei, Andrew Ng, and Michael Jordan.
2003.
La-tent Dirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022, January.Danushka Bollegala, Yutaka Matsuo, and MitsuruIshizuka.
2010.
Relational duality: Unsupervised ex-traction of semantic relations between entities on theweb.
In Proceedings of WWW.Sergey Brin.
1998.
Extracting patterns and relationsfrom the world wide web.
In Proc.
of WebDB Work-shop at 6th International Conference on ExtendingDatabase Technology.Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to extract relations from the web using min-imal supervision.
In Proceedings of the 45rd AnnualMeeting of the Association for Computational Linguis-tics (ACL ?07).Eugene Charniak and Micha Elsner.
2009.
Em works forpronoun anaphora resolution.
In Proceedings of ACL.Harr Chen, Edward Benson, Tahira Naseem, and ReginaBarzilay.
2011.
In-domain relation discovery withmeta-constraints via posterior regularization.
In Pro-ceedings of ACL.Aron Culotta and Jeffery Sorensen.
2004.
Dependencytree kernels for relation extraction.
In 42nd AnnualMeeting of the Association for Computational Linguis-tics, Barcelona, Spain.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics (ACL?05), pages 363?370, June.Benjamin Hachey.
2009.
Towards Generic Relation Ex-traction.
Ph.D. thesis, University of Edinburgh.Aria Haghighi and Dan Klein.
2010.
Coreference resolu-tion in a modular, entity-centered model.
In Proceed-ings of HLT-NAACL.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.2004.
Discovering relations among named entitiesfrom large corpora.
In ACL.Nanda Kambhatla.
2004.
Combining lexical, syntactic,and semantic features with maximum entropy modelsfor extracting relations.
In Proceedings of ACL.1465Zornitsa Kozareva and Eduard Hovy.
2010.
Learningarguments and supertypes of semantic relations usingrecursive patterns.
In Proceedings of ACL 10.Dekang Lin and Patrick Pantel.
2001.
Dirt - discovery ofinference rules from text.
In Proceedings of KDD.Dekang Lin.
1998.
Dependency-based evaluation ofminipar.
In Proceedings of the Workshop on the Eval-uation of Parsing Systems.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In ACL-IJCNLP.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-baseddependency parsing.
In Proceedings of CoNLL, pages49?56.Hoifung Poon and Pedro Domingos.
2008.
Unsuper-vised semantic parsing.
In Proceedings of the Confer-ence on Empirical methods in natural language pro-cessing (EMNLP).Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In Proceedings of ACL.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions withoutlabeled text.
In Proceedings of the European Confer-ence on Machine Learning and Knowledge Discoveryin Databases (ECML PKDD ?10).Alan Ritter, Mausam, and Oren Etzioni.
2010.
A latentdirichlet alocation method for selectional preferences.In Proceedings of ACL10.Dan Roth and Wen tau Yih.
2002.
Probabilistic reason-ing for entity and relation recognition.
In Proceedingsof Coling.Evan Sandhaus, 2008.
The New York Times AnnotatedCorpus.
Linguistic Data Consortium, Philadelphia.Diarmuid O Seaghdha.
2010.
Latent variable models ofselectional preference.
In Proceedings of ACL 10.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In HLT-NAACL, pages 252?259.Limin Yao, Sebastian Riedel, and Andrew McCallum.2010.
Collective cross-document relation extractionwithout labelled data.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1013?1023, Cambridge, MA, Oc-tober.
Association for Computational Linguistics.Alexander Yates and Oren Etzioni.
2009.
Unsupervisedmethods for determining object and relation synonymson the web.
Journal of Artificial Intelligence Research,34:255?296.1466
