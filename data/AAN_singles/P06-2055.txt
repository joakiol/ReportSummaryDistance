Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 420?427,Sydney, July 2006. c?2006 Association for Computational LinguisticsAnalysis and Repair of Name Tagger ErrorsHeng Ji Ralph GrishmanDepartment of Computer ScienceNew York UniversityNew York, NY, 10003, USAhengji@cs.nyu.edu grishman@cs.nyu.eduAbstractName tagging is a critical early stage inmany natural language processing pipe-lines.
In this paper we analyze the typesof errors produced by a tagger, distin-guishing name classification and varioustypes of name identification errors.
Wepresent a joint inference model to im-prove Chinese name tagging by incorpo-rating feedback from subsequent stages inan information extraction pipeline: namestructure parsing, cross-documentcoreference, semantic relation extractionand event extraction.
We show throughexamples and performance measurementhow different stages can correct differenttypes of errors.
The resulting accuracyapproaches that of individual human an-notators.1 IntroductionHigh-performance named entity (NE) tagging iscrucial in many natural language processing tasks,such as information extraction and machinetranslation.
In 'traditional' pipelined system archi-tectures, NE tagging is one of the first steps inthe pipeline.
NE errors adversely affect subse-quent stages, and error rates are often com-pounded by later stages.However, (Roth and Yi 2002, 2004) and ourrecent work have focused on incorporating richerlinguistic analysis, using the ?feedback?
fromlater stages to improve name taggers.
We ex-panded our last year?s model (Ji and Grishman,2005) that used the results of coreference analy-sis and relation extraction, by adding ?feedback?from more information extraction components ?name structure parsing, cross-document corefer-ence, and event extraction ?
to incrementally re-rank the multiple hypotheses from a baselinename tagger.While together these components produced afurther improvement on last year?s model, ourgoal in this paper is to look behind the overallperformance figures in order to understand howthese varied components contribute to the im-provement, and compare the remaining systemerrors with the human annotator?s performance.To this end, we shall decompose the task of nametagging into two subtasks?
Name Identification ?
The process of iden-tifying name boundaries in the sentence.?
Name Classification ?
Given the correctname boundaries, assigning the appropri-ate name types to them.and observe the effects that different componentshave on errors of each type.
Errors of identifica-tion will be further subdivided by type (missingnames, spurious names, and boundary errors).We believe such detailed understanding of thebenefits of joint inference is a prerequisite forfurther improvements in name tagging perform-ance.After summarizing some prior work in thisarea, describing our baseline NE tagger, and ana-lyzing its errors, we shall illustrate, through aseries of examples, the potential for feedback toimprove NE performance.
We then present somedetails on how this improvement can be achievedthrough hypothesis reranking in the extractionpipeline, and analyze the results in terms of dif-ferent types of identification and classificationerrors.2 Prior WorkSome recent work has incorporated global infor-mation to improve the performance of name tag-gers.For mixed case English data, name identifica-tion is relatively easy.
Thus some researchershave focused on the more challenging task ?classifying names into correct types.
In (Roth and420Yi 2002, 2004), given name boundaries in thetext, separate classifiers are first trained for nameclassification and semantic relation detection.Then, the output of the classifiers is used as aconditional distribution given the observed data.This information, along with the constraintsamong the relations and entities (specific rela-tions require specific classes of names), is used tomake global inferences by linear programmingfor the most probable assignment.
They obtainedsignificant improvements in both name classifi-cation and relation detection.In (Ji and Grishman 2005) we generated N-best NE hypotheses and re-ranked them aftercoreference and semantic relation identification;we obtained a significant improvement in Chi-nese name tagging performance.
In this paper weshall use a wider range of linguistic knowledgesources, and integrate cross-document techniques.3 Baseline Name TaggerWe apply a multi-lingual (English / Chinese)bigram HMM tagger to identify four namedentity types: Person, Organization, GPE (?geo-political entities?
?
locations which are alsopolitical units, such as countries, counties, andcities) and Location.
The HMM tagger generallyfollows the Nymble model (Bikel et al 1997),and uses best-first search to generate N-Besthypotheses for each input sentence.In mixed-case English texts, most propernames are capitalized.
So capitalization providesa crucial clue for name boundaries.In contrast, a Chinese sentence is composed ofa string of characters without any word bounda-ries or capitalization.
Even after word segmenta-tion there are still no obvious clues for the nameboundaries.
However, we can apply the followingcoarse ?usable-character?
restrictions to reducethe search space.Standard Chinese family names are generallysingle characters drawn from a set of 437 familynames (there are also 9 two-character familynames, although they are quite infrequent) andgiven names can be one or two characters (Gao etal., 2005).
Transliterated Chinese person namesusually consist of characters in three relativelyfixed character lists (Begin character list, Middlecharacter list and End character list).
Person ab-breviation names and names including title wordsmatch a few patterns.
The suffix words (if thereare any) of Organization and GPE names belongto relatively fixed lists too.However, this ?usable-character?
restriction isnot as reliable as the capitalization informationfor English, since each of these special characterscan also be part of common words.3.1 Identification and Classification ErrorsWe begin our error analysis with an investigationof the English and Chinese baseline taggers, de-composing the errors into identification and clas-sification errors.
In Figure 1 we report theidentification F-Measure for the baseline (thefirst hypothesis), and the N-best upper bound, thebest of the N hypotheses1, using different models:English MonoCase (EN-Mono, without capitali-zation), English Mixed Case (EN-Mix, with capi-talization), Chinese without the usable characterrestriction (CH-NoRes) and Chinese with theusable character restriction (CH-WithRes).Figure 1.
Baseline and Upper Bound ofName IdentificationFigure 1 shows that capitalization is a crucialclue in English name identification (increasingthe F measure by 7.6% over the monocase score).We can also see that the best of the top N (N <=30) hypotheses is very good, so reranking a smallnumber of hypotheses has the potential of pro-ducing a very good tagger.The ?usable?
character restriction plays a ma-jor role in Chinese name identification, increas-ing the F-measure 4%.
With this restriction, theperformance of the best-of-N-best is again verygood.
However, it is evident that, even with thisrestriction, identification is more challenging forChinese, due to the absence of capitalization andword boundaries.Figure 2 shows the classification accuracy ofthe above four models.
We can see that capitali-zation does not help English name classification;1 These figures were obtained using training and test corporadescribed later in this paper, and a value of N ranging from1 to 30 depending on the margin of the HMM tagger, as alsodescribed below.
All figures are with respect to the officialACE keys prepared by the Linguistic Data Consortium.421and the difficulty of classification is similar forthe two languages.Figure 2.
Baseline and Upper Bound ofName Classification3.2 Identification Errors in ChineseFor the remainder of this paper we shall focus onthe more difficult problems of Chinese tagging,using the HMM system with character restric-tions as our baseline.
The name identificationerrors of this system can be divided into missednames (21%), spurious names (29%), and bound-ary errors, where there is a partial overlap be-tween the names in the key and the systemresponse (50%).
Confusion between names andnominals (phrases headed by a common noun) isa major source of both missed and spuriousnames (56% of missed, 24% of spurious).
In alanguage without capitalization, this is a hardtask even for people; one must rely largely onworld knowledge to decide whether a phrase(such as the "criminal-processing team") is anorganization name or merely a description of anorganization.
The other major source of missednames is words not seen in the training data, gen-erally representing minor cities or other locationsin China (28%).
For spurious names, the largestsource of error is names of a type not included inthe key (44%) which are mistakenly tagged asone of the known name types.2  As we shall see,different types of knowledge are required for cor-recting different types of errors.4 Mutual Inferences between Informa-tion Extraction Stages4.1 Extraction PipelineName tagging is typically one of the first stages2 If the key included an 'other' class of names, these wouldbe classification errors; since it does not -- since these namesare not tagged in the key -- the automatic scorer treats themas spurious names.in an information extraction pipeline.
Specifically,we will consider a system which was developedfor the ACE (Automatic Content Extraction)task 3  and includes the following stages: namestructure parsing, coreference, semantic relationextraction and event extraction (Ji et al, 2006).All these stages are performed after name tag-ging since they take names as input ?objects?.However, the inferences from these subsequentstages can also provide valuable constraints toidentify and classify names.Each of these stages connects the name candi-date to other linguistic elements in the sentence,document, or corpus, as shown in Figure 3.Sentence    DocumentBoundary  BoundaryName        Local    Related   Event              CoreferringCandidate Context Mention  trigger&arg     MentionsLinguistic Elements Supporting InferenceFigure 3.
Name candidate and its global contextThe baseline name tagger (HMM) uses verylocal information; feedback from later extractionstages allows us to draw from a wider context inmaking final name tagging decisions.In the following we use two related (translated)texts as examples, to give some intuition of howthese different types of linguistic evidence im-prove name tagging.4Document 1: Yugoslav election[?]
More than 300,000 people rushed the <beier ge le>0 congress building, forcing <yugo-slav>1 president <mi lo se vi c>2 to admitfrankly that in the Sept. 24 election he wasbeaten by his opponent <ke shi tu ni cha>3.<mi lo se vi c>4 was forced to flee <bei er gele>5; the winning opposition party's <sai er weiya>6 <anti-democracy committee>7 on themorning of the 6th formed a <crisis-handling3 The ACE task description can be found athttp://www.itl.nist.gov/iad/894.01/tests/ace/  and the ACEguidelines at http://www.ldc.upenn.edu/Projects/ACE/4 Rather than offer the most fluent translation, we have pro-vided one that more closely corresponds to the Chinese textin order to more clearly illustrate the linguistic issues.Transliterated names are rendered phonetically, character bycharacter.supporting  inferenceinformation422committee>8, to deal with transfer-of-power is-sues.This crisis committee includes police, supply,economics and other important departments.In such a crisis, people cannot think throughthis question: has the <yugoslav>9 president <milo se vi c>10 used up his skills?According to the official voting results in thefirst round of elections, <mi lo se vi c>11 wasbeaten by <18 party opposition committee>12candidate <ke shi tu ni cha>13.
[?
]Document 2: Biography of these two leaders[?
]<ke shi tu ni cha>14 used to pursue an aca-demic career, until 1974, when due to his opposi-tion position he was fired by <bei er ge le>15<law school>16 and left the academic community.<ke shi tu ni cha>17 also at the beginning of the1990s joined the opposition activity, and in 1992founded <sai er wei ya>18 <opposition party>19.This famous new leader and his previousclassmate at law school, namely his wife <zuo lika>20 live in an apartment in <bei er ge le>21.The vanished <mi lo se vi c>22 was born in<sai er wei ya>23 ?s central industrial city.
[?
]4.1 Inferences for Correcting Name Errors4.2.1 Internal Name StructureConstraints and preferences on the structure ofindividual names can capture local informationmissed by the baseline name tagger.
They cancorrect several types of identification errors, in-cluding in particular boundary errors.
For exam-ple, ?<ke shi tu ni cha>3?
is more likely to becorrect than ?<shi tu ni cha>3?
since ?shi?
(?
)cannot be the first character of a transliteratedname.Name structures help to classify names too.For example, ?anti-democracy committee7?
isparsed as ?
[Org-Modifier anti-democracy] [Org-Suffix committee]?, and the first character is nota person last name or the first character of atransliterated person name, so it is more likely tobe an organization than a person name.4.2.2 PatternsInformation about expected sequences of con-stituents surrounding a name can be used to cor-rect name boundary errors.
In particular, eventextraction is performed by matching patterns in-volving a "trigger word" (typically, the main verbor nominalization representing the event) and aset of arguments.
When a name candidate is in-volved in an event, the trigger word and otherarguments of the event can help to determine thename boundaries.
For example, in the sentence?The vanished mi lo se vi c was born in sai er weiya ?s central industrial city?, ?mi lo se vi c?
ismore likely to be a name than ?mi lo se?, ?sai erwei ya?
is more likely be a name than ?er wei?,because these boundaries will allow us to matchthe event pattern ?
[Adj] [PER-NAME] [Triggerword for 'born' event] in [GPE-NAME]?s [GPE-Nominal]?.4.2.3 SelectionAny context which can provide selectional con-straints or preferences for a name can be used tocorrect name classification errors.
Both semanticrelations and events carry selectional constraintsand so can be used in this way.For instance, if the ?Personal-Social/Business?relation (?opponent?)
between ?his?
and ?<ke shitu ni cha>3?
is correctly identified, it can help toclassify ?<ke shi tu ni cha>3?
as a person name.Relation information is sometimes crucial toclassifying names.
?<mi lo se vi c>10?
and ?<keshi tu ni cha>13?
are likely person names becausethey are ?employees?
of ?<yugoslav>9?
and?<18 party opponent committee>12?.
Also the?Personal-Social/Family?
relation (?wife?)
be-tween ?his?
and ?<zuo li ka>20?
helps to classify<zuo li ka>20 as a person name.Events, like relations, can provide effective se-lectional preferences to correctly classify names.For example, ?<mi lo se vi c>2,4,10,11,22?
are likelyperson names because they are involved in thefollowing events: ?claim?, ?escape?, ?built?,?beat?, ?born?, while ?<sai er wei ya>23?can beeasily tagged as GPE because it?s a ?birth-place?in the event ?born?.4.2.4 CoreferenceNames which are introduced in an article arelikely to be referred to again, either by repeatingthe same name or describing it with nominalmentions (phrases headed by common nouns).These mentions will have the same spelling(though if a name has several parts, some may bedropped) and same semantic type.
So if theboundary or type of one mention can be deter-mined with some confidence, coreference can beused to disambiguate other mentions.For example, if ?< mi lo se vi c>2?
is con-firmed as a name, then ?< mi lo se vi c>10?
ismore likely to be a name than ?< mi lo se>10?, by423refering to ?< mi lo se vi c>2?.
Also ?This crisiscommittee?
supports the analysis of ?<crisis-handling committee>8?
as an organization namein preference to the alternative name candidate?<crisis-handling>8?.For a name candidate, high-confidence infor-mation about the type of one mention can be usedto determine the type of other mentions.
For ex-ample, for the repeated person name ?< mi lo sevi c>2,4,10,11,22?
type information based on theevent context of one mention can be used to clas-sify or confirm the type of the others.
The personnominal ?This famous new leader?
confirms?<ke shi tu ni cha>17?
as a person name.5 Incremental Re-Ranking Algorithm5.1 Overall ArchitectureIn this section we will present the algorithms tocapture the intuitions described in Section 4.
Theoverall system pipeline is presented in Figure 4.Figure 4.
System ArchitectureThe baseline name tagger generates N-Bestmultiple hypotheses for each sentence, and alsocomputes the margin ?
the difference betweenthe log probabilities of the top two hypotheses.This is used as a rough measure of confidence inthe top hypothesis.
A large margin indicatesgreater confidence that the first hypothesis is cor-rect.5 It generates name structure parsing resultstoo, such as the family name and given name ofperson, the prefixes of the abbreviation names,the modifiers and suffixes of organization names.Then the results from subsequent componentsare exploited in four incremental re-rankers.From each re-ranking step we output the bestname hypothesis directly if the re-ranker has highconfidence in its decisions.
Otherwise the sen-tence is forwarded to the next re-ranker, based onother features.
In this way we can adjust the rank-ing of multiple hypotheses and select the besttagging for each sentence gradually.The nominal mention tagger (noun phrasechunker) uses a maximum entropy model.
Entitytype assignment for the nominal heads is done bytable look-up.
The coreference resolver is a com-bination of high-precision heuristic rules andmaximum entropy models.
In order to incorpo-rate wider context we use cross-documentcoreference for the test set.
We cluster the docu-ments using a cross-entropy metric and then treatthe entire cluster as a single document.The relation tagger uses a K-nearest-neighboralgorithm.We extract event patterns from the ACE05training corpus for personnel, contact, life, busi-ness, and conflict events.
We also collect addi-tional event trigger words that appear frequentlyin name contexts, from a syntactic dictionary, asynonym dictionary and Chinese PropBank V1.0.Then the patterns are generalized and testedsemi-automatically.5.2 Supervised Re-Ranking ModelIn our name re-ranking model, each hypothesis isan NE tagging of the entire sentence, for example,?The vanished <PER>mi lo se vi c</PER> wasborn in <GPE>sai er wei ya</GPE>?s centralindustrial city?
; and each pair of hypotheses (hi,hj) is called a ?sample?.5 The margin also determines the number of hypotheses (N)generated by the baseline tagger.
Using cross-validation onthe training data, we determine the value of N required toinclude the best hypothesis, as a function of the margin.
Wethen divide the margin into ranges of values, and set a valueof N for each range, with a maximum of 30.High-ConfidenceRankingBest NameHypothesisEvent basedRe-RankingCross-documentCoreference basedRe-RankingCorefResolverEventPatternsRaw SentenceHMM NameTagger and NameStructure ParserMultiple namehypothesesName Structurebased Re-RankingRelationTaggerMentionsRelation basedRe-RankingNominalTagger424Re-Ranker Property for comparing names Nik and NjkHMMMargin scaled margin value from HMMIdiomik -1 if Nik is part of an idiom; otherwise 0PERContextik the number of PER context words if Nik and Njk  are both PER; otherwise 0ORGSuffixik 1 if Nik is tagged as ORG and it includes a suffix word; otherwise 0PERCharac-terik-1 if Nik is tagged as PER without family name, and it does not consist entirely oftransliterated person name characters; otherwise 0Titlestructureik -1 if Nik = title word + family name while Njk = title word + family name + givenname; otherwise 0Digitik -1 if Nik is  PER or GPE and it includes digits or punctuation; otherwise 0AbbPERik -1 if Nik = little/old + family name + given name while Njk = little/old + familyname; otherwise 0SegmentPERik -1 if Nik is GPE (PER)* GPE , while Njk is PER*; otherwise 0Votingik the voting rate among all the candidate hypotheses6NameStructureBasedFamous-Nameik1 if Nik is tagged as the same type in one of the famous name lists7; otherwise 0Probability1i scaled ranking probability for (hi, hj) from name structure based re-rankerRelationConstraintikIf Nik is in relation R (Nik = EntityType1, M2 = EntityType2), computeProb(EntityType1|EntityType2, R) from training data and scale it; otherwise 0RelationBasedConjunction ofInRelation i &Probability1iInrelationik is 1 if Nik and Njk  have different name types, and Nik is in a definite re-lation while Njk  is not; otherwise 0.
?kiki InrelationInrelation?Probability2i scaled ranking probability for (hi, hj) from relation based re-rankerEventConstrainti1 if all entity types in hi match event pattern, -1 if some do not match, and 0 if theargument slots are emptyEventBasedEventSubType Event subtype if the patterns are extracted from ACE data, otherwise?None?Probability3i scaled ranking probability for (hi, hj) from event based re-rankerHeadik 1 if ikN includes the head word of name; otherwise 0CorefNumik the number of mentions corefered to NikWeightNumik the sum of all link weights between Nik and its corefered mentions, 0.8 for name-name coreference; 0.5 for apposition;  0.3 for other name-nominal coreferenceCross-documentCorefer-enceBasedNumHigh-Corefithe number of mentions which corefer to Nik and output by previous re-rankers withhigh confidenceTable 3.
Re-Ranking PropertiesComponent DataBaseline name tagger 2978 texts from the People?s Daily in 1998 and 1300 texts fromACE03, 04, 05 training dataNominal tagger Chinese Penn TreeBank V5.1Coreference resolver 1300 texts from ACE03, 04, 05 training dataRelation tagger 633 ACE 05 texts, and 546 ACE 04 texts with types/subtypesmapped into 05 setEvent pattern 376 trigger words, 661 patternsName structure, coreferenceand relation based re-rankers1,071,285 samples (pairs of hypotheses) from ACE 03, 04 and05 training dataTrainingEvent based re-ranker 325,126 samples from ACE sentences including event triggerwordsTest 100 texts from ACE 04 training corpus, includes 2813 names:1126 persons, 712 GPEs, 785 organizations and 190 locations.Table 4.
Data Description6 The method of counting the voting rate refers to (Zhai, 04) and (Ji and Grishman, 05)7 Extracted from the high-frequency name lists from the training corpus, and country/province/state/ city lists from Chinesewikipedia.425The goal of each re-ranker is to learn a rankingfunction f of the following form: for each pair ofhypotheses (hi, hj), f : H ?
H ?
{-1, 1}, such thatf(hi, hj) = 1 if hi is better than hj; f (hi, hj) = -1 if hiis worse than hj.
In this way we are able to con-vert ranking into a classification problem.
Andthen a maximum entropy model for re-rankingthese hypotheses can be trained and applied.During training we use F-measure to measurethe quality of each name hypothesis against thekey.
During test we get from the MaxEnt classi-fier the probability (ranking confidence) for eachpair: Prob (f (hi, hj) = 1).
Then we apply a dy-namic decoding algorithm to output the best hy-pothesis.
More details about the re-rankingalgorithm are presented in (Ji et al, 2006).5.3 Re-Ranking FeaturesFor each sample (hi, hj), we construct a featureset for assessing the ranking of hi and hj.
Basedon the information obtained from inferences, wecompute (for each property) the property scorePSik for each individual name candidate Nik in hi;some of these properties depend also on the cor-responding name tags in hj.
Then we sum overall names in each hypothesis hi: ?=kiki PSPSFinally we use the quantity (PSi?PSj) as thefeature value for the sample (hi, hj).
Table 3summarizes the property scores PSik used in thedifferent re-rankers; space limitations prevent usfrom describing them in further detail.6 Experimental Results and AnalysisTable 4 shows the data used to train each stage,drawn from the ACE training data and othersources.
The training samples of the re-rankersare obtained by running the name tagger in cross-validation.
100 ACE 04 documents were held outfor use as test data.In the following we evaluate the contributionsof re-rankers in name identification and classifi-cation separately.Identification ModelPrecision Recall F-MeasureBaseline 93.2 93.4 93.3+name structure 94.0 93.5 93.7+relation 93.9 93.7 93.8+event 94.1 93.8 93.9+cross-doccoreference95.1 93.9 94.5Table 5.
Name IdentificationIdentification+ClassificationModelClassifi-cationAccuracy P R FBaseline 93.8 87.4 87.6 87.5+name structure 94.3 88.7 88.2 88.4+relation 95.2 89.4 89.2 89.3+event 95.7 90.1 89.8 89.9+cross-doccoreference96.5 91.8 90.6 91.2Table 6.
Name ClassificationTables 5 and 6 show the performance on iden-tification, classification, and the combined task aswe add each re-ranker to the system.The gain is greater for classification (2.7%)than for identification (1.2%).
Furthermore, wecan see that the gain in identification is producedprimarily by the name structure and coreferencecomponents.
As we noted earlier, the name struc-ture analysis can correct boundary errors by pre-ferring names with complete internal components,while coreference can resolve a boundary ambi-guity for one mention of a name if another men-tion is unambiguous.
The greatest gains weretherefore obtained in boundary errors: the stagestogether eliminated over 1/3 of boundary errorsand about 10% of spurious names; only a fewmissing names were corrected, and some correctnames were deleted.Both relations and events contribute substan-tially to classification performance through theirselectional constraints.
The lesser contribution ofevents is related to their lower frequency.
Only11% of the sentences in the test data contain in-stances of the original ACE event types.
To in-crease the impact of the event patterns, webroadened their coverage to include additionalfrequent event types, so that finally 35% of sen-tences contain event "trigger words".We used a simple cross-document coreferencemethod in which the test documents were clus-tered based on their cross-entropy and documentsin the same cluster were treated as a singledocument for coreference.
This produced smallgains in both identification (0.6% vs. 0.4%) andclassification (0.8% vs. 0.4%) over single-document coreference.7 DiscussionThe use of 'feedback' from subsequent stages ofanalysis has yielded substantial improvements inname tagging accuracy, from F=87.5 with thebaseline HMM to F=91.2.
This performancecompares quite favorably with the performanceof the human annotators who prepared the ACE4262005 training data.
The annotator scores (whenmeasured against a final key produced by reviewand adjudication of the two annotations) wereF=92.5 for one annotator and F=92.7 for theother.As in the case of the automatic tagger, humanclassification accuracy (97.2 - 97.6%) was betterthan identification accuracy (F = 95.0 - 95.2%).In Figure 5 we summarize the error rates forthe baseline system, the improved system withoutcoreference based re-ranker, the final systemwith re-ranking, and a single annotator.8Figure 5.
Error DistributionFigure 5 shows that the performance im-provement reflects a reduction in classificationand boundary errors.
Compared to the system,the human annotator?s identification accuracywas much more skewed (52.3% missing, 13.5%spurious), suggesting that a major source of iden-tification error was not difference in judgementbut rather names which were simply overlookedby one annotator and picked up by the other.This further suggests that through an extension ofour joint inference approach we may soon be ableto exceed the performance of a single manualannotator.Our analysis of the types of errors, and the per-formance of our knowledge sources, gives someindication of how these further gains may beachieved.
The selectional force of event extrac-tion was limited by the frequency of event pat-terns ?
only about 1/3 of sentences had a pattern8  Here spurious errors are names in the system responsewhich do not overlap names in the key; missing errors arenames in the key which do not overlap names in the systemresponse; and boundary errors are names in the system re-sponse which partially overlap names in the key plus namesin the key which partially overlap names in the system re-sponse.instance.
Even with this limitation, we obtaineda gain of 0.5% in name classification.
Capturinga broader range of selectional patterns shouldyield further improvements.
Nearly 70% of thespurious names remaining in the final outputwere in fact instances of 'other' types of names,such as book titles and building names; creatingexplicit models of such names should improveperformance.
Finally, our cross-documentcoreference is currently performed only withinthe (small) test corpus.
Retrieving related articlesfrom a large collection should increase the likeli-hood of finding a name instance with a disam-biguating context.AcknowledgmentThis material is based upon work supported bythe Defense Advanced Research Projects Agencyunder Contract No.
HR0011-06-C-0023, and theNational Science Foundation under Grant IIS-00325657.
Any opinions, findings and conclu-sions expressed in this material are those of theauthors and do not necessarily reflect the viewsof the U. S. Government.ReferencesDaniel M. Bikel, Scott Miller, Richard Schwartz, andRalph Weischedel.
1997.
Nymble: a high-performance Learning Name-finder.
Proc.ANLP1997.
pp.
194-201., Washington, D.C.Jianfeng Gao, Mu Li, Andi Wu and Chang-NingHuang.
2005.
Chinese Word Segmentation andNamed Entity Recognition: A Pragmatic Approach.Computational Linguistics 31(4).
pp.
531-574Heng Ji and Ralph Grishman.
2005.
Improving NameTagging by Reference Resolution and Relation De-tection.
Proc.
ACL2005.
pp.
411-418.
Ann Arbor,USA.Heng Ji, Cynthia Rudin and Ralph Grishman.
2006.Re-Ranking Algorithms for Name Tagging.
Proc.HLT/NAACL 06 Workshop on ComputationallyHard Problems and Joint Inference in Speech andLanguage Processing.
New York, NY, USADan Roth and Wen-tau Yih.
2004.
A Linear Pro-gramming Formulation for Global Inference inNatural Language Tasks.
Proc.
CONLL2004.Dan Roth and Wen-tau Yih.
2002.
Probabilistic Rea-soning for Entity & Relation Recognition.
Proc.COLING2002.Lufeng Zhai, Pascale Fung, Richard Schwartz, MarineCarpuat, and Dekai Wu.
2004.
Using N-best Listsfor Named Entity Recognition from ChineseSpeech.
Proc.
NAACL 2004 (Short Papers)427
