Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 1?8,Rochester, New York, April 2007. c?2007 Association for Computational LinguisticsChunk-Level Reordering of Source Language Sentences withAutomatically Learned Rules for Statistical Machine TranslationYuqi Zhang and Richard Zens and Hermann NeyHuman Language Technology and Pattern RecognitionLehrstuhl fu?r Informatik 6 ?
Computer Science DepartmentRWTH Aachen University, D-52056 Aachen, Germany{yzhang,zens,ney}@cs.rwth-aachen.deAbstractIn this paper, we describe a source-side reordering method based on syntac-tic chunks for phrase-based statistical ma-chine translation.
First, we shallow parsethe source language sentences.
Then, re-ordering rules are automatically learnedfrom source-side chunks and word align-ments.
During translation, the rules areused to generate a reordering lattice foreach sentence.
Experimental results arereported for a Chinese-to-English task,showing an improvement of 0.5%?1.8%BLEU score absolute on various test setsand better computational efficiency thanreordering during decoding.
The exper-iments also show that the reordering atthe chunk-level performs better than at thePOS-level.1 IntroductionIn machine translation, reordering is one of the ma-jor problems, since different languages have differ-ent word order requirements.
Many reordering con-straints have been used for word reorderings, suchas ITG constraints (Wu, 1996), IBM constraints(Berger et al, 1996) and local constraints (Kanthaket al, 2005).
These approaches do not make use ofany linguistic knowledge.Several methods have been proposed to use syn-tactic information to handle the reordering problem,e.g.
(Wu, 1997; Yamada and Knight, 2001; Gildea,2003; Melamed, 2004; Graehl and Knight, 2004;Galley et al, 2006).
One approach makes use ofbitext grammars to parse both the source and tar-get languages.
Another approach makes use of syn-tactic information only in the target language.
Notethat these models have radically different structuresand parameterizations than phrase-based models forSMT.Another kind of approaches is to use syntactic in-formation in rescoring methods.
(Koehn and Knight,2003) apply a reranking approach to the sub-taskof noun-phrase translation.
(Och et al, 2004) and(Shen et al, 2004) describe the use of syntactic fea-tures in reranking the output of a full translation sys-tem, but the syntactic features give very small gains.In this paper, we present a strategy to reordera source sentence using rules based on syntacticchunks.
It is possible to integrate reordering rules di-rectly into the search process, but here, we considera more modular approach: easy to exchange reorder-ing strategy.
To avoid hard decisions before SMT,we generate a source-reordering lattice instead of asingle reordered source sentence as input to the SMTsystem.
Then, the decoder uses the reordered sourcelanguage model as an additional feature function.
Alanguage model trained on the reordered source-sidechunks gives a score for each path in the lattice.
Thenovel ideas in this paper are:?
reordering of the source sentence at the chunklevel,?
representing linguistic chunks-reorderings in alattice.1The rest of this paper is organized as follows.
Sec-tion 2 presents a review of related work.
In Sec-tions 3, we review the phrase-based translation sys-tem used in this work and propose the frameworkof the new reordering method.
In Section 4, we in-troduce the details of the reordering rules, how theyare defined and how to extract them.
In Section 5,we explain how to apply the rules and how to gen-erate reordering lattice.
In Section 6, we presentsome results that show that the chunk-level sourcereordering is helpful for phrase-based statistical ma-chine translation.
Finally, we conclude this paperand discuss future work in Section 7.2 Related WorkBeside the reordering methods during decoding, analternative approach is to reorder the input sourcesentence to match the word order of the target sen-tence.Some reordering methods are carried out on syn-tactic source trees.
(Collins et al, 2005) describea method for reordering German for German-to-English translation, where six transformations areapplied to the surface string of the parsed sourcesentence.
(Xia and McCord, 2004) propose an ap-proach for translation from French-to-English.
Thisapproach automatically extracts rewrite patterns byparsing the source and target sides of the trainingcorpus.
These rewrite patterns can be applied to anyinput source sentence so that the rewritten sourceand target sentences have similar word order.
Bothmethods need a parser to generate trees of sourcesentences and are applied only as a preprocessingstep.Another kind of source reordering methods be-sides full parsing is based on Part-Of-Speech (POS)tags or word classes.
(Costa-jussa` and Fonollosa,2006) view the source reordering as a translationtask that translate the source language into a re-ordered source language.
Then, the reordered sourcesentence is taken as the single input to the standardSMT system.
(Chen et al, 2006) automatically extract rulesfrom word alignments.
These rules are defined atthe POS level and the scores of matching rules areused as additional feature functions during rescor-ing.
(Crego and Marin?o, 2006) integrate source-sidereordering into SMT decoding.
They automaticallylearn rewrite patterns from word alignment and rep-resent the patterns with POS tags.
To our knowledgeno work is reported on the reordering with shallowparsing.Decoding lattices were already used in (Zens etal., 2002; Kanthak et al, 2005).
Those approachesused linguistically uninformed word-level reorder-ings.3 System OverviewIn this section, we will describe the phrase-basedSMT system which we use for the experiments.Then, we will give an outline of the extentions withthe chunk-level source reordering model.3.1 The Baseline Phrase-based SMT SystemIn statistical machine translation, we are given asource language sentence fJ1 = f1 .
.
.
fj .
.
.
fJ ,which is to be translated into a target language sen-tence eI1 = e1 .
.
.
ei .
.
.
eI .
Among all possible tar-get language sentences, we will choose the sentencewith the highest probability:e?I?1 = argmaxI,eI1{Pr(eI1|fJ1 )} (1)= argmaxI,eI1{Pr(eI1) ?
Pr(fJ1 |eI1)} (2)This decomposition into two knowledge sourcesis known as the source-channel approach to sta-tistical machine translation (Brown et al, 1990).It allows an independent modeling of the targetlanguage model Pr(eI1) and the translation modelPr(fJ1 |eI1).
The target language model describesthe well-formedness of the target language sentence.The translation model links the source language sen-tence to the target language sentence.
The argmaxoperation denotes the search problem, i.e., the gen-eration of the output sentence in the target language.A generalization of the classical source-channelapproach is the direct modeling of the posteriorprobability Pr(eI1|fJ1 ).
Using a log-linear model2(Och and Ney, 2002), we obtain:Pr(eI1|fJ1 ) =exp(?Mm=1 ?mhm(eI1, fJ1 ))?I?,e?I?1exp(?Mm=1 ?mhm(e?I?1 , fJ1 ))(3)The denominator represents a normalization factorthat depends only on the source sentence fJ1 .
There-fore, we can omit it during the search process.
As adecision rule, we obtain:e?I?1 = argmaxI,eI1{ M?m=1?mhm(eI1, fJ1 )}(4)The log-linear model has the advantage that addi-tional models h(?)
can be easily integrated into theoverall system.
The model scaling factors ?M1 aretrained according to the maximum entropy principle,e.g., using the GIS algorithm.
Alternatively, one cantrain them with respect to the final translation qualitymeasured by an error criterion (Och, 2003).The log-linear model is a natural framework to in-tegrate many models.
The baseline system uses thefollowing models:?
phrase translation model?
phrase count features?
word-based translation model?
word and phrase penalty?
target language model (6-gram)?
distortion model (assigning costs based on thejump width)All the experiments in the paper are evaluated with-out rescoring.
More details about the baseline sys-tem can be found in (Mauser et al, 2006)3.2 Source Sentence Reordering FrameworkEncouraged by the work of (Xia and McCord, 2004)and (Crego and Marin?o, 2006), we also reorder thesource language side.
Compared to reordering onthe target language side, one advantage is the effi-ciency since the reordering lattice can be translatedmonotonically as in (Zens et al, 2002).
Another ad-vantage is that there is correct sentence informationPOS taggingshallow chunkingTranslation ProcessStandard Translation Proceswith Source Reorderingsource text sentencesreordering rulesSMT systemtranslation output translation outputsource text sentencesSMT systemsource reordering latticeFigure 1: Illustration of the translation process withand without source reordering.for the reordering methods, because the source sen-tences are always given.
Syntactic reordering on tar-get language is difficult, since the methods will de-grade much because of the errors in hypothesis.We apply reordering at the syntactic chunk levelwhich can been seen as an intermediate level be-tween full parsing and POS tagging.
Figure 1 showsthe differences between the new translation frame-work and the standard translation process.
A re-ordering lattice replaces the original source sentenceas the input to the translation system.
The use of alattice avoids hard decisions before translation.
Togenerate the reordering lattice, the source sentence isfirst POS tagged and chunk parsed.
Then, reorder-ing rules are applied to the chunks to generate thereordering lattice.Reordering rules are the key information forsource reordering.
They are automatically learnedfrom the training data.
The details of these two mod-ules will be introduced in Section 5.4 Reordering RulesThere has been much work on learning and apply-ing reordering rules on source language, such as(Nie?en and Ney, 2001; Xia and McCord, 2004;Collins et al, 2005; Chen et al, 2006; Crego andMarin?o, 2006; Popovic?
and Ney, 2006).
The re-ordering rules could be composed of words, POStags or syntactic tags of phrases.
In our work, a ruleis composed of chunk tags and POS tags.
There is3Table 1: Examples of reordering rules.
(lhs: chunkand POS tag sequence, rhs: permutation )no.
lhs rhs1.
NP0 PP1 u2 n3 0 1 2 32.
NP0 PP1 u2 n3 3 0 1 23.
DNP0 NP1 V P2 0 1 24.
DNP0 NP1 V P2 1 0 25.
DNP0 NP1 m2 0 1 26.
DNP0 NP1 m2 ad3 3 0 1 27.
DNP0 NP1 m2 ad3 v4 4 3 0 1 2no hierarchical structure in a rule.4.1 Definition of Reordering RulesFirst, we show some rule examples in Table 1.
A re-ordering rule consists of a left-hand-side (lhs) and aright-hand-side (rhs).
The left-hand-side is a syn-tactic rule (chunk or POS tags), while the right-hand-side is the reordering positions of the rule.
Dif-ferent rules can share the same left-hand-side, suchas rules no.
1, 2 and no.
3, 4.
The rules recordnot only the real reordered chunk sequence, but alsothe monotone chunk sequences, like no.
1, 3 and5.
Note that the same tag sequence can appear mul-tiple times according to different contexts, such asDNP0 NP1 m2 # 0 1 2 in rules no.
5, 6, 7.4.2 Extraction of Reordering RulesThe extraction of reordering rules is based on theword alignment and the source sentence chunks.Here, we train word alignments in both directionswith GIZA++ (Och and Ney, 2003).
To get algn-ment with high accuracy, we use the intersectionalignment here.For a given word-aligned sentence pair(fJ1 , eI1, aJ1 ), the source word sequence fJ1 isfirst parsed into a chunk sequence FK1 .
Accord-ingly, the word-to-word alignment aJ1 is changedto a chunk-to-word alignment a?K1 which is thecombination of the target words aligned to thesource words in a chunk.
It is defined as:a?k = {i|i = aj ?
j ?
[jk, jk+1 ?
1]}Figure 2: Illustration of three kinds of phrases:(a)monotone phrase, (b)reordering phrase, (c)crossphrase.
The black box is a word-to-word alignment.The gray box is a chunk-to-word alignment.Here, jk denotes the position of the first source wordin kth chunk.
The new alignment is 1 : m fromsource chunks to target words.
It also means a?k is aset of positions of target words.We apply the standard phrase extraction algorithm(Zens et al, 2002) to (FK1 , eI1, a?K1 ).
Discarding thecross phrases, we keep the other phrases as rules.
Ina cross phrase, at least two chunk-word alignmentsoverlap on the target language side.
An exampleof a cross phrase is illustrated in Figure 2(c).
Fig-ure 2(a) and (b) illustrate the phrases for reorderingrules, which could be monotone phrases or reorder-ing phrases.5 Reordering Lattice Generation5.1 Parsing the Source SentenceThe first step of chunk parsing is word segmentation.Then, a POS tagger is usually needed for furthersyntactic analysis.
In our experiments, we use thetool of ?Inst.
of Computing Tech., Chinese LexicalAnalysis System (ICTCLAS)?
(Zhang et al, 2003),which does the two tasks in one pass.Referring to the description of the chunking taskin CoNLL-20001, instead of English, a Chinesechunker is processed and evaluated.
Each word isassigned a chunk tag, which contains the name of thechunk type and ?B?
for the first word of the chunkand ?I?
for each other word in the chunk.
The ?O?chunk tag is used for tokens which are not part ofany chunk.
We use the maximum entropy tool YAS-1http://www.cnts.ua.ac.be/conll2000/chunking/4Figure 3: Example of applying rules.
The left part is the used rules.
The right part is the generated neworders of source words.MET2 to learn the chunking model.
The model isbased on a combination of word and POS tags.
Sincespecific training and test data are not available forChinese chunking, we convert subtrees of the Chi-nese treebank (LDC2005T01) into chunks.
As thereare many ways to choose a subtree, we uses the min-imum subtree with the following constraints:?
a subtree has more than one child,?
the children of a subtree are all leaves.Compared to chunking of English as in CoNLL-2000, there are more chunk types (24 instead of 6)and no single-word chunks.
These two aspects makechunking for Chinese harder.5.2 Applying Reordering RulesFirst, we search the reordering rules, in which thechunk sequence matches any tag sequence in the in-put sentence.
A source sentence has many pathsgenerated by the rules .
For a word uncovered by anyrules, its POS tag is used.
Each path corresponds toone sentence permutation.The left part of the Figure 3 shows seven possiblecoverages, the right part is the reordering for eachcoverage.
Some of the reorderings are identical, likethe permutations in line 1, 3 and 5.
That is becauseone word sequence is memorized by several rules indifferent contexts.5.3 Lattice WeightingAll reorderings of an input sentence S are com-pressed and stored in a lattice.
Each path is a possi-2http://www-i6.informatik.rwth-aachen.de/web/Software/index.htmlble reordering S?
and is given a weight W .
In thispaper, the weight is computed using a source lan-guage model p(S?).
The weight is used directly inthe decoder, integrated into Equation (4).
There isalso a scaling factor for this weight, which is op-timized together with other scaling factors on thedevelopment data.
The probability of the reorderedsource sentence is calculated as follows: for a re-ordered source sentence w1w2...wn, the trigram lan-guage model is:p(S?)
=N?n=1p(wn|wn?2, wn?1) (5)Beside a word N-gram language model, a POS tagN-gram model or a chunk tag N-gram model couldbe used as well.In this paper, we use a word trigram model.
Themodel is trained on reordered training source sen-tences.
A training source sentence is parsed intochunks.
In the same way as described in Section4.2, word-to-word alignments is converted to chunk-to-word alignments.
We reorder the source chunksto monotonize the chunk-to-word alignments.
Thechunk boundaries are kept when this reordering isdone.6 Experiments6.1 Chunking ResultIn this section, we report results for chunk parsing.The annotation of the data is derived from the Chi-nese treebank (LDC2005T01).
The corpus is splitinto two parts: 1000 sentences are randomly se-5Table 2: Statistics of training and test corpus forchunk parsing.train testsentences 17 785 1 000words 486 468 21 851chunks 105 773 4 680words out of chunks 244 416 10 282Table 3: Chunk parsing result on 1000 sentences.accuracy precision recall F-measure74.51% 65.2% 61.5% 63.3lected as test data.
The remaining part is used fortraining.
The corpus is from the newswire domain.Table 2 shows the corpus statistics.
For the 4 680chunks in the test set, the chunker has found 4 414chunks, of which 2 879 are correct.
Following thecriteria of CoNLL-2000, the chunker is evaluatedusing the F-score, which is a combination of pre-cision and recall.
The result is shown in Table 3.The accuracy is evaluated at the word level, theother three metrics are evaluated at the chunk level.The results at the chunk level are worse than at theword level, because a chunk is counted as correctonly if the chunk tag and the chunk boundaries areboth correct.6.2 Translation ResultsFor the translation experiments, we report the twoaccuracy measures BLEU (Papineni et al, 2002)and NIST (Doddington, 2002) as well as the twoerror rates word error rate (WER) and position-independent word error rate (PER).We perform translation experiments on the Ba-sic Traveling Expression Corpus (BTEC) for theChinese-English task.
It is a speech translation taskin the domain of tourism-related information.
Wereport results on the IWSLT 2004, 2005 and 2006evaluation test sets.
There are 16 reference trans-lations for the IWSLT 2004 and 2005 tasks and 7reference translations for the IWSLT 2006 task.Table 4 shows the corpus statistics of the task.
Atraining corpus is used to train the translation model,the language model and to obtain the reorderingTable 4: Statistics of training and test corpora for theIWSLT tasks.Chinese EnglishTrain Sentences 40kWords 308k 377kDev Sentences 489Words 5 478 6 008Test Sentences 500IWSLT04 Words 3 866 3 581Test Sentences 506IWSLT05 Words 3 652 3 579Test Sentences 500IWSLT06 Words 5 846 ?rules.
A development corpus is used to optimize thescaling factors for the BLEU score.
The English textis processed using a tokenizer.
The Chinese text pro-cessing uses word segmentation with the ICTCLASsegmenter (Zhang et al, 2003).
The translation isevaluated case-insensitive and without punctuationmarks.The translation results are presented in Table 5.The baseline system is a non-monotone translationsystem, in which the decoder does reordering onthe target language side.
Compared to the base-line system, the source reordering method improvesthe BLEU score by 0.5% ?
1.8% absolute.
It alsoachieves a better WER.
Note that the used chun-ker here is out-of-domain 3.
An improvement isachieved even with a low F-measure for chunking.So, we could hope that larger improvement is possi-ble using a high-accuracy chunker.Though the input is a lattice, the source reorderingis still faster than the reordering during decoding,e.g.
for the IWSLT 2006 test set, the baseline systemtook 17.5 minutes and the source reordering systemtook 12.3 minutes.
The result also indicates that thenon-monotone decoding hurts the performance in asource reordering framework.
A similar conclusionis also presented in (Xia and McCord, 2004).Additional experiments we carried out to comparePOS-level and chunk-level reorderings.
We deletethe chunk information and keep the POS tags.
Then,3The chunker is trained on newswire data, but the test datais from the tourism domain.6Table 5: Translation performance for the Chinese-English IWSLT taskWER[%] PER[%] NIST BLEU[%]IWSLT04 baseline 47.3 38.2 7.78 39.1source reordering 46.3 37.2 7.70 40.9IWSLT05 baseline 45.0 37.3 7.40 41.8source reordering 44.6 36.8 7.51 42.3IWSLT06 baseline 67.4 50.0 6.65 22.4source reordering 65.6 50.4 6.46 23.3source reordering+non-monotone decoder 66.5 50.3 6.52 22.4Table 6: Translation performance of reorderingmethods on IWSLT 2004 test setWER PER NIST BLEU[%] [%] [%]Baseline 47.3 38.2 7.78 39.1POS 46.9 37.5 7.38 39.7Chunk 46.3 37.2 7.70 40.9Table 7: Lattice information for the Chinese-EnglishIWSLT 2004 test dataavg.
density used translationpro sent rules time [min/sec]POS 15.7 6 868 7:08Chunk 8.2 3 685 3:47we rerun the source reordering system on the IWSLT2004 test set.
The translation results are shown inTable 6.
Though the accuracy of chunking is low,the chunk-level method gets better results than POS-level method.
With POS tags, we get more reorder-ing rules and more paths in the lattice, since the sen-tence length is longer than with chunks.
The statis-tics are shown in Table 7.7 Conclusions and Future WorkThis paper presents a source-side reordering methodwhich is based on syntactic chunks.
The reorderingrules are automatically learned from bilingual data.To avoid hard decision before decoding, a reorder-ing lattice representing all possible reorderings isused instead of single source sentence for decoding.The experiments demonstrate that even with a verypoor chunker, the chunk-level source reordering isstill helpful for a state-of-the-art statistical transla-tion system and it has better performance than thePOS-level source reordering and target-side reorder-ing.There are some directions for future work.
First,we would like to try this method on larger data setsand other language pairs.
Second, we are going toimprove the chunking accuracy.
Third, we wouldreduce the number of rules and prune the lattice.AcknowledgmentsThis material is partly based upon work sup-ported by the Defense Advanced Research ProjectsAgency (DARPA) under Contract No.
HR0011-06-C-0023, and was partially funded by the DeutscheForschungsgemeinschaft (DFG) under the project?Statistische Textu?bersetzung?
(Ne572/5)ReferencesA.
L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996.
Amaximum entropy approach to natural language processing.Computational Linguistics, 22(1):39?72, March.P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,F.
Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.1990.
A statistical approach to machine translation.
Com-putational Linguistics, 16(2):79?85, June.B.
Chen, M. Cettolo, and M. Federico.
2006.
Reorderingrules for phrase-based statistical machine translation.
InInt.
Workshop on Spoken Language Translation EvaluationCampaign on Spoken Language Translation, pages 1?15,Kyoto, Japan, November.M.
Collins, P. Koehn, and I. Kucerova.
2005.
Clause restructur-ing for statistical machine translation.
In Proc.
of the 43rdAnnual Meeting of the Association for Computational Lin-guistics (ACL), pages 531?540, Ann Arbor, Michigan, June.7M.
R. Costa-jussa` and J.
A. R. Fonollosa.
2006.
Statistical ma-chine reordering.
In Proc.
of the Conf.
on Empirical Meth-ods in Natural Language Processing, pages 70?76, Sydney,Australia, July.J.
M. Crego and J.
B. Marin?o.
2006.
Integration of postag-based source reordering into SMT decoding by an extendedsearch graph.
In Proc.
of AMTA06, pages 29?36, Mas-sachusetts, USA, August.G.
Doddington.
2002.
Automatic evaluation of machine trans-lation quality using n-gram co-occurrence statistics.
In Proc.ARPA Workshop on Human Language Technology.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thayer.
2006.
Scalable inference and train-ing of context-rich syntactic translation models.
In Proc.
ofthe 21st Int.
Conf.
on Computational Linguistics and 44thAnnual Meeting of the Association for Computational Lin-guistics, pages 961?968, Sydney, Australia, July.D.
Gildea.
2003.
Loosely tree-based alignment for machinetranslation.
In Proc.
of the 41th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages 80?87,Sapporo, Japan, July.J.
Graehl and K. Knight.
2004.
Training tree transducers.In HLT-NAACL 2004: Main Proc., pages 105?112, Boston,Massachusetts, USA, May 2 - May 7.S.
Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005.Novel reordering approaches in phrase-based statistical ma-chine translation.
In 43rd Annual Meeting of the Assoc.
forComputational Linguistics: Proc.
Workshop on Building andUsing Parallel Texts: Data-Driven Machine Translation andBeyond, pages 167?174, Ann Arbor, Michigan, June.P.
Koehn and K. Knight.
2003.
Empirical methods for com-pound splitting.
In Proc.
10th Conf.
of the Europ.
Chapterof the Assoc.
for Computational Linguistics (EACL), pages347?354, Budapest, Hungary, April.A.
Mauser, R. Zens, E. Matusov, S. Hasan, and H. Ney.
2006.The RWTH Statistical Machine Translation System for theIWSLT 2006 Evaluation.
In Proc.
of the Int.
Workshopon Spoken Language Translation, pages 103?110, Kyoto,Japan.I.
Melamed.
2004.
Statistical machine translation by parsing.In The Companion Volume to the Proc.
of 42nd Annual Meet-ing of the Association for Computational Linguistics, pages653?660.S.
Nie?en and H. Ney.
2001.
Morpho-syntactic analysis forreordering in statistical machine translation.
In Proc.
of MTSummit VIII, pages 247?252.F.
J. Och and H. Ney.
2002.
Discriminative training and max-imum entropy models for statistical machine translation.
InProc.
of the 40th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 295?302, Philadelphia,PA, July.F.
J. Och and H. Ney.
2003.
A systematic comparison of vari-ous statistical alignment models.
Computational Linguistics,29(1):19?51, March.F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,A.
Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,Z.
Jin, and D. Radev.
2004.
A smorgasbord of features forstatistical machine translation.
In Proc.
2004 Human Lan-guage Technology Conf.
/ North American Chapter of theAssociation for Computational Linguistics Annual Meeting(HLT-NAACL), pages 161?168, Boston,MA.F.
J. Och.
2003.
Minimum error rate training in statistical ma-chine translation.
In Proc.
of the 41th Annual Meeting ofthe Association for Computational Linguistics (ACL), pages160?167, Sapporo, Japan, July.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.
Bleu: amethod for automatic evaluation of machine translation.
InProc.
of the 40th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 311?318, Philadelphia,PA, July.M.
Popovic?
and H. Ney.
2006.
POS-based word reorderingsfor statistical machine translation.
In Proc.
of the Fifth Int.Conf.
on Language Resources and Evaluation (LREC).L.
Shen, A. Sarkar, and F. J. Och.
2004.
Discriminative rerank-ing for machine translation.
In HLT-NAACL 2004: MainProc., pages 177?184, Boston, Massachusetts, USA, May 2- May 7.C.
Tillmann, S. Vogel, H. Ney, and A. Zubiaga.
1997.
A DP-based search using monotone alignments in statistical trans-lation.
In Proc.
35th Annual Conf.
of the Association forComputational Linguistics, pages 289?296, Madrid, Spain,July.D.
Wu.
1996.
A polynomial-time algorithm for statistical ma-chine translation.
In Proc.
34th Annual Meeting of the As-soc.
for Computational Linguistics, pages 152?158, SantaCruz, CA, June.D.
Wu.
1997.
Stochastic inversion transduction grammars andbilingual parsing of parallel corpora.
Computational Lin-guistics, 23(3):377?403, September.F.
Xia and M. McCord.
2004.
Improving a statistical MT sys-tem with automatically learned rewrite patterns.
In Proc.
ofCOLING04, pages 508?514, Geneva, Switzerland, Aug 23?Aug 27.K.
Yamada and K. Knight.
2001.
A syntax-based statisticaltranslation model.
In Proc.
of the 39th Annual Meeting ofthe Association for Computational Linguistics (ACL), pages523?530, Toulouse, France, July.R.
Zens, F. J. Och, and H. Ney.
2002.
Phrase-based statisticalmachine translation.
In M. Jarke, J. Koehler, and G. Lake-meyer, editors, 25th German Conf.
on Artificial Intelligence(KI2002), volume 2479 of Lecture Notes in Artificial Intel-ligence (LNAI), pages 18?32, Aachen, Germany, September.Springer Verlag.H.
P. Zhang, Q. Liu, X. Q. Cheng, H. Zhang, and H. K. Yu.2003.
Chinese lexical analysis using hierarchical hiddenmarkov model.
In Proc.
of the second SIGHAN workshopon Chinese language processing, pages 63?70, Morristown,NJ, USA.8
