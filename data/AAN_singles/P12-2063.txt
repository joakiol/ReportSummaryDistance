Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322?327,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsUnsupervised Morphology Rivals Supervised Morphology for Arabic MTDavid Stallard Jacob DevlinMichael KayserBBN Technologies{stallard,jdevlin,rzbib}@bbn.comYoong Keok Lee Regina BarzilayCSAILMassachusetts Institute of Technology{yklee,regina}@csail.mit.eduAbstractIf unsupervised morphological analyzerscould approach the effectiveness of super-vised ones, they would be a very attractivechoice for improving MT performance onlow-resource inflected languages.
In thispaper, we compare performance gains forstate-of-the-art supervised vs. unsupervisedmorphological analyzers, using a state-of-the-art Arabic-to-English MT system.
We applymaximum marginal decoding to the unsu-pervised analyzer, and show that this yieldsthe best published segmentation accuracyfor Arabic, while also making segmentationoutput more stable.
Our approach givesan 18% relative BLEU gain for Levantinedialectal Arabic.
Furthermore, it gives highergains for Modern Standard Arabic (MSA), asmeasured on NIST MT-08, than does MADA(Habash and Rambow, 2005), a leadingsupervised MSA segmenter.1 IntroductionIf unsupervised morphological segmenters could ap-proach the effectiveness of supervised ones, theywould be a very attractive choice for improving ma-chine translation (MT) performance in low-resourceinflected languages.
An example of particular cur-rent interest is Arabic, whose various colloquial di-alects are sufficiently different from Modern Stan-dard Arabic (MSA) in lexicon, orthography, andmorphology, as to be low-resource languages them-selves.
An additional advantage of Arabic for studyis the availability of high-quality supervised seg-menters for MSA, such as MADA (Habash andRambow, 2005), for performance comparison.
TheMT gain for supervised MSA segmenters on dialectestablishes a lower bound, which the unsupervisedsegmenter must exceed if it is to be useful for dialect.And comparing the gain for supervised and unsuper-vised segmenters on MSA tells us how useful theunsupervised segmenter is, relative to the ideal casein which a supervised segmenter is available.In this paper, we show that an unsupervised seg-menter can in fact rival or surpass supervised MSAsegmenters on MSA itself, while at the same timeproviding superior performance on dialect.
Specifi-cally, we compare the state-of-the-art morphologicalanalyzer of Lee et al (2011) with two leading super-vised analyzers for MSA, MADA and Sakhr1, eachserving as an alternative preprocessor for a state-of-the-art statistical MT system (Shen et al, 2008).
Wemeasure MSA performance on NIST MT-08 (NIST,2010), and dialect performance on a Levantine di-alect web corpus (Zbib et al, 2012b).To improve performance, we apply maximummarginal decoding (Johnson and Goldwater, 2009)(MM) to combine multiple runs of the Lee seg-menter, and show that this dramatically reduces thevariance and noise in the segmenter output, whileyielding an improved segmentation accuracy thatexceeds the best published scores for unsupervisedsegmentation on Arabic Treebank (Naradowsky andToutanova, 2011).
We also show that it yields MT-08 BLEU scores that are higher than those obtainedwith MADA, a leading supervised MSA segmenter.For Levantine, the segmenter increases BLEU scoreby 18% over the unsegmented baseline.1http://www.sakhr.com/Default.aspx3222 Related WorkMachine translation systems that process highly in-flected languages often incorporate morphologicalanalysis.
Some of these approaches rely on mor-phological analysis for pre- and post-processing,while others modify the core of a translation systemto incorporate morphological information (Habash,2008; Luong et al, 2010; Nakov and Ng, 2011).
Forinstance, factored translation Models (Koehn andHoang, 2007; Yang and Kirchhoff, 2006; Avramidisand Koehn, 2008) parametrize translation probabili-ties as factors encoding morphological features.The approach we have taken in this paper isan instance of a segmented MT model, which di-vides the input into morphemes and uses the de-rived morphemes as a unit of translation (Sadat andHabash, 2006; Badr et al, 2008; Clifton and Sarkar,2011).
This is a mainstream architecture that hasbeen shown to be effective when translating from amorphologically rich language.A number of recent approaches have exploredthe use of unsupervised morphological analyzersfor MT (Virpioja et al, 2007; Creutz and Lagus,2007; Clifton and Sarkar, 2011; Mermer and Ak?n,2010; Mermer and Saraclar, 2011).
Virpioja et al(2007) apply the unsupervised morphological seg-menter Morfessor (Creutz and Lagus, 2007), andapply an existing MT system at the level of mor-phemes.
The system does not outperform the wordbaseline partially due to the insufficient accuracy ofthe automatic morphological analyzer.The work of Mermer and Ak?n (2010) and Mer-mer and Saraclar (2011) attempts to integrate mor-phology and MT more closely than we do, by in-corporating bilingual alignment probabilities into aGibbs-sampled version of Morfessor for Turkish-to-English MT.
However, the bilingual strategy showsno gain over the monolingual version, and nei-ther version is competitive for MT with a super-vised Turkish morphological segmenter (Oflazer,1993).
By contrast, the unsupervised analyzer wereport on here yields MSA-to-English MT perfor-mance that equals or exceed the performance ob-tained with a leading supervised MSA segmenter,MADA (Habash and Rambow, 2005).3 Review of Lee Unsupervised SegmenterThe segmenter of Lee et al (2011) is a probabilis-tic model operating at word-type level.
It is di-vided into four sub-model levels.
Model 1 preferssmall affix lexicons, and assumes that morphemesare drawn independently.
Model 2 generates a la-tent POS tag for each word type, conditioning theword?s affixes on the tag, thereby encouraging com-patible affixes to be generated together.
Model 3incorporates token-level contextual information, bygenerating word tokens with a type-level HiddenMarkov Model (HMM).
Finally, Model 4 modelsmorphosyntactic agreement with a transition proba-bility distribution, encouraging adjacent tokens withthe same endings to also have the same final suffix.4 Applying Maximum Marginal Decodingto Reduce Variance and NoiseMaximum marginal decoding (Johnson and Gold-water, 2009) (MM) is a technique which assignsto each latent variable the value with the high-est marginal probability, thereby maximizing theexpected number of correct assignments (Rabiner,1989).
Johnson and Goldwater (2009) extend MMto Gibbs sampling by drawing a set of N indepen-dent Gibbs samples, and selecting for each word themost frequent segmentation found in them.
Theyfound that MM improved segmentation accuracyover the mean, consistent with its maximization cri-terion.
However, for our setting, we find that MMprovides several other crucial advantages as well.First, MM dramatically reduces the output vari-ance of Gibbs sampling (GS).
Table 1 documents theseverity of this variance for the MT-08 lexicon, asmeasured by the average exact-match accuracy andsegmentation F-measure between different runs.
Itshows that on average, 13% of the word tokens, and25% of the word types, are segmented differentlyfrom run to run, which obviously makes the input toMT highly unstable.
By contrast the ?MM?
columnof Table 1 shows that two different runs of MM, eachderived by combining separate sets of 25 GS runs,agree on the segmentations of over 95% of the wordtoken ?
a dramatic improvement in stability.Second, MM reduces noise from the spurious af-fixes that the unsupervised segmenter induces forlarge lexicons.
As Table 2 shows, the segmenter323Decoding Level Rec Prec F1 AccGibbs Type 82.9 83.2 83.1 74.5Token 87.5 89.1 88.3 86.7MM Type 95.9 95.8 95.9 93.9Token 97.3 94.0 95.6 95.1Table 1: Comparison of agreement in outputs between25 runs of Gibbs sampling vs. 2 runs of MM on thefull MT-08 data set.
We give the average segmentationrecall, precision, F1-measure, and exact-match accuracybetween outputs, at word-type and word-token levels.ATB MT-08GS GS MM MorfUnique prefixes 17 130 93 287Unique suffixes 41 261 216 241Top-95 prefixes 7 7 6 6Top-95 suffixes 14 26 19 19Table 2: Affix statistics of unsupervised segmenters.
Forthe ATB lexicon, we show statistics for the Lee seg-menter with regular Gibbs sampling (GS).
For the MT-08 lexicon, we also show the output of the Lee segmenterwith maximum marginal decoding (MM).
In addition, weshow statistics for Morfessor.induces 130 prefixes and 261 suffixes for MT-08(statistics for Morfessor are similar).
This phe-nomenon is fundamental to Bayesian nonparamet-ric models, which expand indefinitely to fit the datathey are given (Wasserman, 2006).
But MM helpsto alleviate it, reducing unique prefixes and suffixesfor MT-08 by 28% and 21%, respectively.
It also re-duces the number of unique prefixes/suffixes whichaccount for 95% of the prefix/suffix tokens (Top-95).Finally, we find that in our setting, MM increasesaccuracy not just over the mean, but over even thebest-scoring of the runs.
As shown in Table 3, MMincreases segmentation F-measure from 86.2% to88.2%.
This exceeds the best published results onATB (Naradowsky and Toutanova, 2011).These results suggest that MM may be worth con-sidering for other GS applications, not only for theaccuracy improvements pointed out by Johnson andGoldwater (2009), but also for its potential to pro-vide more stable and less noisy results.Model Mean Min Max MMM1 80.1 79.0 81.5 81.8M2 81.4 80.2 83.0 82.0M3 81.4 80.1 82.8 83.2M4 86.2 85.4 87.2 88.2Table 3: Segmentation F-scores on ATB dataset for Leesegmenter, shown for each Model level M1?M4 on theArabic segmentation dataset used by (Poon et al, 2009):We give the mean, minimum, and maximum F-scores for25 independent runs of Gibbs sampling, together with theF-score from running MM over that same set of runs.5 MT Evaluation5.1 Experimental DesignMT System.
Our experiments were performedusing a state-of-the-art, hierarchical string-to-dependency-tree MT system, described in Shen etal.
(2008).Morphological Analyzers.
We compare the Leesegmenter with the supervised MSA segmenterMADA, using its ?D3?
scheme.
We also comparewith Sakhr, an intensively-engineered, supervisedMSA segmenter which applies multiple NLP tech-nologies to the segmentation problem, and whichhas given the best results for our MT system in pre-vious work (Zbib et al, 2012a).
We also comparewith Morfessor.MT experiments.
We apply the appropriate seg-menter to split words into morphemes, which wethen treat as words for alignment and decoding.
Fol-lowing Lee et al (2011), we segment the test andtraining sets jointly, estimating separate translationmodels for each segmenter/dataset combination.Training and Test Corpora.
Our ?Full MSA?
cor-pus is the NIST MT-08 Constrained Data Track Ara-bic training corpus (35M total, 336K unique words);our ?Small MSA?
corpus is a 1.3M-word subset.Both are tested on the MT-08 evaluation set.
Fordialect, we use a Levantine dialectal Arabic cor-pus collected from the web with 1.5M total, 160Kunique words and 18K words held-out for test (Zbibet al, 2012b)PerformanceMetrics.
We evaluate MTwith BLEUscore.
To calculate statistical significance, we usethe boot-strap resampling method of Koehn (2004).3245.2 Results and DiscussionTable 4 summarizes the BLEU scores obtained fromusing various segmenters, for three training/test sets:Full MSA, Small MSA, and Levantine dialect.As expected, Sakhr gives the best results forMSA.
Morfessor underperforms the other seg-menters, perhaps because of its lower accuracy onArabic, as reported by Poon et al (2009).
TheLee segmenter gives the best results for Levantine,inducing valid Levantine affixes (e.g ?hAl+?
forMSA?s ?h*A-Al+?, English ?this-the?)
and yieldingan 18% relative gain over the unsegmented baseline.What is more surprising is that the Lee segmentercompares favorably with the supervised MSA seg-menters on MSA itself.
In particular, the Lee seg-menter with MM yields higher BLEU scores thandoes MADA, a leading supervised segmenter, whilepreserving almost the same performance as GS ondialect.
On Small MSA, it recoups 93% of evenSakhr?s gain.By contrast, the Lee segmenter recoups only 79%of Sakhr?s gain on Full MSA.
This might result fromthe phenomenon alluded to in Section 4, where addi-tional data sometimes degrades performance for un-supervised analyzers.
However, the Lee segmenter?sgain on Levantine (18%) is higher than its gain onSmall MSA (13%), even though Levantine has moredata (1.5M vs. 1.3M words).
This might be be-cause dialect, being less standardized, has more or-thographic and morphological variability, which un-supervised segmentation helps to resolve.These experiments also show that while Model 4gives the best F-score, Model 3 gives the best MTscores.
Comparison of Model 3 and 4 segmentationsshows that Model 4 induces a much larger num-ber of inflectional suffixes, especially the femininesingular suffix ?-p?, which accounts for a plurality(16%) of the differences by token.
While such suf-fixes improve F-measure on the segmentation refer-ences, they do not correspond to any English lexicalunit, and thus do not help alignment.An interesting question is how much performancemight be gained from a supervised segmenter thatwas as intensively engineered for dialect as Sakhrwas for MSA.
Assuming a gain ratio of 0.93, similarto Small MSA, the estimated BLEU score would be20.38, for a relative gain of just 5% over the unsuper-System Small Full LevMSA MSA DialUnsegmented 38.69 43.45 17.10Sakhr 43.99 46.51 19.60MADA 43.23 45.64 19.29Morfessor 42.07 44.71 18.38Lee GSM1 43.12 44.80 19.70M2 43.16 45.45 20.15+M3 43.07 44.82 19.97M4 42.93 45.06 19.55Lee MMM1 43.53 45.14 19.75M2 43.45 45.29 19.75M3 43.64+ 45.84 20.09M4 43.56 45.16 19.93Table 4: BLEU scores for all experiments.
Full MSA isthe the full MT-08 corpus, Small MSA is a 1.3M-wordsubset, Lev Dial our Levantine dataset.
For each of these,the highest Lee segmenter score is in bold, with ?+?
ifstatistically significant vs. MADA at the 95% confidencelevel or higher.
The highest overall score is in bold italic.vised segmenter.
Given the large engineering effortthat would be required to achieve this gain, the un-supervised segmenter may be a more cost-effectivechoice for dialectal Arabic.6 ConclusionWe compare unsupervised vs. supervised morpho-logical segmentation for Arabic-to-English machinetranslation.
We add maximum marginal decodingto the unsupervised segmenter, and show that itsurpasses the state-of-the-art segmentation perfor-mance, purges the segmenter of noise and variabil-ity, yields BLEU scores on MSA competitive withthose from supervised segmenters, and gives an 18%relative BLEU gain on Levantine dialectal Arabic.AcknowledgementsThis material is based upon work supported byDARPA under Contract Nos.
HR0011-12-C00014and HR0011-12-C00015, and by ONR MURI Con-tract No.
W911NF-10-1-0533.
Any opinions, find-ings and conclusions or recommendations expressedin this material are those of the author(s) and do notnecessarily reflect the views of the US government.We thank Rabih Zbib for his help with interpretingLevantine Arabic segmentation output.325ReferencesEleftherios Avramidis and Philipp Koehn.
2008.
Enrich-ing morphologically poor languages for statistical ma-chine translation.
In Proceedings of ACL-08: HLT.Ibrahim Badr, Rabih Zbib, and James Glass.
2008.
Seg-mentation for English-to-Arabic statistical machinetranslation.
In Proceedings of ACL-08: HLT, ShortPapers.Ann Clifton and Anoop Sarkar.
2011.
Combin-ing morpheme-based machine translation with post-processing morpheme prediction.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies.Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM Trans.
Speech Lang.
Process., 4:3:1?3:34, February.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In Proceedings ofACL.Nizar Habash.
2008.
Four techniques for online handlingof out-of-vocabulary words in Arabic-English statisti-cal machine translation.
In Proceedings of ACL-08:HLT, Short Papers.Mark Johnson and Sharon Goldwater.
2009.
Improv-ing nonparametric bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics.Philipp Koehn and Hieu Hoang.
2007.
Factored transla-tion models.
In Proceedings of EMNLP-CoNLL, pages868?876.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP 2004.Yoong Keok Lee, Aria Haghighi, and Regina Barzi-lay.
2011.
Modeling syntactic context improvesmorphological segmentation.
In Proceedings of theFifteenth Conference on Computational Natural Lan-guage Learning.Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.2010.
A hybrid morpheme-word representationfor machine translation of morphologically rich lan-guages.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing.Cos?kun Mermer and Ahmet Afs?
?n Ak?n.
2010.
Unsuper-vised search for the optimal segmentation for statisti-cal machine translation.
In Proceedings of the ACL2010 Student Research Workshop, pages 31?36, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Cos?kun Mermer and Murat Saraclar.
2011.
Unsuper-vised Turkish morphological segmentation for statis-tical machine translation.
In Workshop on MachineTranslation and Morphologically-rich languages, Jan-uary.Preslav Nakov and Hwee Tou Ng.
2011.
Trans-lating from morphologically complex languages: Aparaphrase-based approach.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies.Jason Naradowsky and Kristina Toutanova.
2011.
Unsu-pervised bilingual morpheme segmentation and align-ment with context-rich hidden semi-Markov models.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies.NIST.
2010.
NIST 2008 Open Machine Translation(Open MT) Evaluation.
http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2010T21/.Kemal Oflazer.
1993.
Two-level description of Turkishmorphology.
In Proceedings of the Sixth Conferenceof the European Chapter of the Association for Com-putational Linguistics.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentation withlog-linear models.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics.Lawrence R. Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.
In Proceedings of the IEEE, pages 257?286.Fatiha Sadat and Nizar Habash.
2006.
Combinationof Arabic preprocessing schemes for statistical ma-chine translation.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Computa-tional Linguistics.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-08: HLT.Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz, andMarkus Sadeniemi.
2007.
Morphology-aware statisti-cal machine translation based on morphs induced in anunsupervised manner.
In Proceedings of the MachineTranslation Summit XI.LarryWasserman.
2006.
All of Nonparametric Statistics.Springer.326Mei Yang and Katrin Kirchhoff.
2006.
Phrase-basedbackoff models for machine translation of highly in-flected languages.
In Proceedings of EACL.Rabih Zbib, Michael Kayser, Spyros Matsoukas, JohnMakhoul, Hazem Nader, Hamdy Soliman, and RamiSafadi.
2012a.
Methods for integrating rule-based andstatistical systems for Arabic to English machine trans-lation.
Machine Translation, 26(1-2):67?83.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, JohnMakhoul, Omar F. Zaidan, and Chris Callison-Burch.2012b.
Machine translation of Arabic dialects.
InNAACL 2012: Proceedings of the 2012 Human Lan-guage Technology Conference of the North AmericanChapter of the Association for Computational Linguis-tics, Montreal, Quebec, Canada, June.
Association forComputational Linguistics.327
