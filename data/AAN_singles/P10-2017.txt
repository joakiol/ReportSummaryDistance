Proceedings of the ACL 2010 Conference Short Papers, pages 92?97,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsExemplar-Based Models for Word Meaning In ContextKatrin ErkDepartment of LinguisticsUniversity of Texas at Austinkatrin.erk@mail.utexas.eduSebastian Pado?Institut fu?r maschinelle SprachverarbeitungStuttgart Universitypado@ims.uni-stuttgart.deAbstractThis paper describes ongoing work on dis-tributional models for word meaning incontext.
We abandon the usual one-vector-per-word paradigm in favor of an exemplarmodel that activates only relevant occur-rences.
On a paraphrasing task, we findthat a simple exemplar model outperformsmore complex state-of-the-art models.1 IntroductionDistributional models are a popular frameworkfor representing word meaning.
They describea lemma through a high-dimensional vector thatrecords co-occurrence with context features over alarge corpus.
Distributional models have been usedin many NLP analysis tasks (Salton et al, 1975;McCarthy and Carroll, 2003; Salton et al, 1975), aswell as for cognitive modeling (Baroni and Lenci,2009; Landauer and Dumais, 1997; McDonald andRamscar, 2001).
Among their attractive propertiesare their simplicity and versatility, as well as thefact that they can be acquired from corpora in anunsupervised manner.Distributional models are also attractive as amodel of word meaning in context, since they donot have to rely on fixed sets of dictionary sensewith their well-known problems (Kilgarriff, 1997;McCarthy and Navigli, 2009).
Also, they canbe used directly for testing paraphrase applicabil-ity (Szpektor et al, 2008), a task that has recentlybecome prominent in the context of textual entail-ment (Bar-Haim et al, 2007).
However, polysemyis a fundamental problem for distributional models.Typically, distributional models compute a single?type?
vector for a target word, which contains co-occurrence counts for all the occurrences of thetarget in a large corpus.
If the target is polyse-mous, this vector mixes contextual features for allthe senses of the target.
For example, among thetop 20 features for coach, we get match and team(for the ?trainer?
sense) as well as driver and car(for the ?bus?
sense).
This problem has typicallybeen approached by modifying the type vector fora target to better match a given context (Mitchelland Lapata, 2008; Erk and Pado?, 2008; Thater etal., 2009).In the terms of research on human concept rep-resentation, which often employs feature vectorrepresentations, the use of type vectors can be un-derstood as a prototype-based approach, which usesa single vector per category.
From this angle, com-puting prototypes throws away much interestingdistributional information.
A rival class of mod-els is that of exemplar models, which memorizeeach seen instance of a category and perform cat-egorization by comparing a new stimulus to eachremembered exemplar vector.We can address the polysemy issue through anexemplar model by simply removing all exem-plars that are ?not relevant?
for the present con-text, or conversely activating only the relevantones.
For the coach example, in the context ofa text about motorways, presumably an instancelike ?The coach drove a steady 45 mph?
would beactivated, while ?The team lost all games since thenew coach arrived?
would not.In this paper, we present an exemplar-based dis-tributional model for modeling word meaning incontext, applying the model to the task of decid-ing paraphrase applicability.
With a very simplevector representation and just using activation, weoutperform the state-of-the-art prototype models.We perform an in-depth error analysis to identifystable parameters for this class of models.2 Related WorkAmong distributional models of word, there aresome approaches that address polysemy, eitherby inducing a fixed clustering of contexts intosenses (Schu?tze, 1998) or by dynamically modi-92fying a word?s type vector according to each givensentence context (Landauer and Dumais, 1997;Mitchell and Lapata, 2008; Erk and Pado?, 2008;Thater et al, 2009).
Polysemy-aware approachesalso differ in their notion of context.
Some use abag-of-words representation of words in the cur-rent sentence (Schu?tze, 1998; Landauer and Du-mais, 1997), some make use of syntactic con-text (Mitchell and Lapata, 2008; Erk and Pado?,2008; Thater et al, 2009).
The approach that wepresent in the current paper computes a representa-tion dynamically for each sentence context, usinga simple bag-of-words representation of context.In cognitive science, prototype models predictdegree of category membership through similar-ity to a single prototype, while exemplar theoryrepresents a concept as a collection of all previ-ously seen exemplars (Murphy, 2002).
Griffiths etal.
(2007) found that the benefit of exemplars overprototypes grows with the number of available ex-emplars.
The problem of representing meaning incontext, which we consider in this paper, is closelyrelated to the problem of concept combination incognitive science, i.e., the derivation of representa-tions for complex concepts (such as ?metal spoon?
)given the representations of base concepts (?metal?and ?spoon?).
While most approaches to conceptcombination are based on prototype models, Voor-spoels et al (2009) show superior results for anexemplar model based on exemplar activation.In NLP, exemplar-based (memory-based) mod-els have been applied to many problems (Daele-mans et al, 1999).
In the current paper, we use anexemplar model for computing distributional repre-sentations for word meaning in context, using thecontext to activate relevant exemplars.
Comparingrepresentations of context, bag-of-words (BOW)representations are more informative and noisier,while syntax-based representations deliver sparserand less noisy information.
Following the hypothe-sis that richer, topical information is more suitablefor exemplar activation, we use BOW representa-tions of sentential context in the current paper.3 Exemplar Activation ModelsWe now present an exemplar-based model formeaning in context.
It assumes that each targetlemma is represented by a set of exemplars, wherean exemplar is a sentence in which the target occurs,represented as a vector.
We use lowercase lettersfor individual exemplars (vectors), and uppercaseSentential context ParaphraseAfter a fire extinguisher is used, it mustalways be returned for recharging andits use recorded.bring back (3),take back (2),send back (1),give back (1)We return to the young woman who isreading the Wrigley?s wrapping paper.come back (3),revert (1), revisit(1), go (1)Table 1: The Lexical Substitution (LexSub) dataset.letters for sets of exemplars.We model polysemy by activating relevant ex-emplars of a lemma E in a given sentence contexts.
(Note that we use E to refer to both a lemmaand its exemplar set, and that s can be viewed asjust another exemplar vector.)
In general, we defineactivation of a set E by exemplar s asact(E, s) = {e ?
E | sim(e, s) > ?
(E, s)}where E is an exemplar set, s is the ?point of com-parison?, sim is some similarity measure such asCosine or Jaccard, and ?
(E, s) is a threshold.
Ex-emplars belong to the activated set if their similarityto s exceeds ?
(E, s).1 We explore two variants ofactivation.
In kNN activation, the k most simi-lar exemplars to s are activated by setting ?
to thesimilarity of the k-th most similar exemplar.
Inq-percentage activation, we activate the top q%of E by setting ?
to the (100-q)-th percentile of thesim(e, s) distribution.
Note that, while in the kNNactivation scheme the number of activated exem-plars is the same for every lemma, this is not thecase for percentage activation: There, a more fre-quent lemma (i.e., a lemma with more exemplars)will have more exemplars activated.Exemplar activation for paraphrasing.
A para-phrases is typically only applicable to a particularsense of a target word.
Table 1 illustrates this ontwo examples from the Lexical Substitution (Lex-Sub) dataset (McCarthy and Navigli, 2009), bothfeaturing the target return.
The right column listsappropriate paraphrases of return in each context(given by human annotators).
2 We apply the ex-emplar activation model to the task of predictingparaphrase felicity: Given a target lemma T in aparticular sentential context s, and given a list of1In principle, activation could be treated not just as binaryinclusion/exclusion, but also as a graded weighting scheme.However, weighting schemes introduce a large number ofparameters, which we wanted to avoid.2Each annotator was allowed to give up to three para-phrases per target in context.
As a consequence, the numberof gold paraphrases per target sentence varies.93potential paraphrases of T , the task is to predictwhich of the paraphrases are applicable in s.Previous approaches (Mitchell and Lapata, 2008;Erk and Pado?, 2008; Erk and Pado?, 2009; Thateret al, 2009) have performed this task by modify-ing the type vector for T to the context s and thencomparing the resulting vector T ?
to the type vec-tor of a paraphrase candidate P .
In our exemplarsetting, we select a contextually adequate subsetof contexts in which T has been observed, usingT ?
= act(T, s) as a generalized representation ofmeaning of target T in the context of s.Previous approaches used all of P as a repre-sentation for a paraphrase candidate P .
However,P includes also irrelevant exemplars, while for aparaphrase to be judged as good, it is sufficient thatone plausible reading exists.
Therefore, we useP ?
= act(P, s) to represent the paraphrase.4 Experimental EvaluationData.
We evaluate our model on predicting para-phrases from the Lexical Substitution (LexSub)dataset (McCarthy and Navigli, 2009).
This datasetconsists of 2000 instances of 200 target words insentential contexts, with paraphrases for each tar-get word instance generated by up to 6 participants.Paraphrases are ranked by the number of annota-tors that chose them (cf.
Table 1).
Following Erkand Pado?
(2008), we take the list of paraphrase can-didates for a target as given (computed by poolingall paraphrases that LexSub annotators proposedfor the target) and use the models to rank them forany given sentence context.As exemplars, we create bag-of-words co-occurrence vectors from the BNC.
These vectorsrepresent instances of a target word by the otherwords in the same sentence, lemmatized and POS-tagged, minus stop words.
E.g., if the lemmagnurge occurs twice in the BNC, once in the sen-tence ?The dog will gnurge the other dog?, andonce in ?The old windows gnurged?, the exemplarset for gnurge contains the vectors [dog-n: 2, other-a:1] and [old-a: 1, window-n: 1].
For exemplarsimilarity, we use the standard Cosine similarity,and for the similarity of two exemplar sets, theCosine of their centroids.Evaluation.
The model?s prediction for an itemis a list of paraphrases ranked by their predictedgoodness of fit.
To evaluate them against aweighted list of gold paraphrases, we follow Thateret al (2009) in using Generalized Average Preci-para- actT actPmeter kNN perc.
kNN perc.10 36.1 35.5 36.5 38.620 36.2 35.2 36.2 37.930 36.1 35.3 35.8 37.840 36.0 35.3 35.8 37.750 35.9 35.1 35.9 37.560 36.0 35.0 36.1 37.570 35.9 34.8 36.1 37.580 36.0 34.7 36.0 37.490 35.9 34.5 35.9 37.3no act.
34.6 35.7random BL 28.5Table 2: Activation of T or P individually on thefull LexSub dataset (GAP evaluation)sion (GAP), which interpolates the precision valuesof top-n prediction lists for increasing n. Let G =?q1, .
.
.
, qm?
be the list of gold paraphrases withgold weights ?y1, .
.
.
, ym?.
Let P = ?p1, .
.
.
, pn?be the list of model predictions as ranked by themodel, and let ?x1, .
.
.
, xn?
be the gold weightsassociated with them (assume xi = 0 if pi 6?
G),where G ?
P .
Let I(xi) = 1 if pi ?
G, and zerootherwise.
We write xi = 1i?ik=1 xk for the av-erage gold weight of the first i model predictions,and analogously yi.
ThenGAP (P,G) =1?mj=1 I(yj)yjn?i=1I(xi)xiSince the model may rank multiple paraphrases thesame, we average over 10 random permutations ofequally ranked paraphrases.
We report mean GAPover all items in the dataset.Results and Discussion.
We first computed twomodels that activate either the paraphrase or thetarget, but not both.
Model 1, actT , activates onlythe target, using the complete P as paraphrase, andranking paraphrases by sim(P, act(T, s)).
Model2, actP, activates only the paraphrase, using s asthe target word, ranking by sim(act(P, s), s).The results for these models are shown in Ta-ble 2, with both kNN and percentage activation:kNN activation with a parameter of 10 means thatthe 10 closest neighbors were activated, while per-centage with a parameter of 10 means that the clos-est 10% of the exemplars were used.
Note firstthat we computed a random baseline (last row)with a GAP of 28.5.
The second-to-last row (?noactivation?)
shows two more informed baselines.94The actT ?no act?
result (34.6) corresponds to aprototype-based model that ranks paraphrase can-didates by the distance between their type vectorsand the target?s type vector.
Virtually all exem-plar models outperform this prototype model.
Notealso that both actT and actP show the best resultsfor small values of the activation parameter.
Thisindicates paraphrases can be judged on the basisof a rather small number of exemplars.
Neverthe-less, actT and actP differ with regard to the detailsof their optimal activation.
For actT , a small ab-solute number of activated exemplars (here, 20)works best , while actP yields the best results fora small percentage of paraphrase exemplars.
Thiscan be explained by the different functions playedby actT and actP (cf.
Section 3): Activation of theparaphrase must allow a guess about whether thereis reasonable interpretation of P in the context s.This appears to require a reasonably-sized samplefrom P .
In contrast, target activation merely has tocounteract the sparsity of s, and activation of toomany exemplars from T leads to oversmoothing.We obtained significances by computing 95%and 99% confidence intervals with bootstrap re-sampling.
As a rule of thumb, we find that 0.4%difference in GAP corresponds to a significant dif-ference at the 95% level, and 0.7% difference inGAP to significance at the 99% level.
The fouractivation methods (i.e., columns in Table 2) aresignificantly different from each other, with the ex-ception of the pair actT/kNN and actP/kNN (n.s.
),so that we get the following order:actP/perc > actP/kNN ?
actT/kNN > actT/percwhere > means ?significantly outperforms?.
In par-ticular, the best method (actT/kNN) outperformsall other methods at p<0.01.
Here, the best param-eter setting (10% activation) is also significantlybetter than the next-one one (20% activation).
Withthe exception of actT/perc, all activation methodssignificantly outperform the best baseline (actP, noactivation).Based on these observations, we computed athird model, actTP, that activates both T (by kNN)and P (by percentage), ranking paraphrases bysim(act(P, s), act(T, s)).
Table 3 shows the re-sults.
We find the overall best model at a similarlocation in parameter space as for actT and actP(cf.
Table 2), namely by setting the activation pa-rameters to small values.
The sensitivity of theparameters changes considerably, though.
WhenP activation (%) ?
10 20 30T activation (kNN) ?5 38.2 38.1 38.110 37.6 37.8 37.720 37.3 37.4 37.340 37.2 37.2 36.1Table 3: Joint activation of P and T on the fullLexSub dataset (GAP evaluation)we fix the actP activation level, we find compara-tively large performance differences between theT activation settings k=5 and k=10 (highly signif-icant for 10% actP, and significant for 20% and30% actP).
On the other hand, when we fix theactT activation level, changes in actP activationgenerally have an insignificant impact.Somewhat disappointingly, we are not able tosurpass the best result for actP alone.
This indicatesthat ?
at least in the current vector space ?
thesparsity of s is less of a problem than the ?dilution?of s that we face when we representing the targetword by exemplars of T close to s. Note, however,that the numerically worse performance of the bestactTP model is still not significantly different fromthe best actP model.Influence of POS and frequency.
An analysisof the results by target part-of-speech showed thatthe globally optimal parameters also yield the bestresults for individual POS, even though there aresubstantial differences among POS.
For actT , thebest results emerge for all POS with kNN activationwith k between 10 and 30.
For k=20, we obtain aGAP of 35.3 (verbs), 38.2 (nouns), and 35.1 (adjec-tives).
For actP, the best parameter for all POS wasactivation of 10%, with GAPs of 36.9 (verbs), 41.4(nouns), and 37.5 (adjectives).
Interestingly, theresults for actTP (verbs: 38.4, nouns: 40.6, adjec-tives: 36.9) are better than actP for verbs, but worsefor nouns and adjectives, which indicates that thesparsity problem might be more prominent than forthe other POS.
In all three models, we found a cleareffect of target and paraphrase frequency, with de-teriorating performance for the highest-frequencytargets as well as for the lemmas with the highestaverage paraphrase frequency.Comparison to other models.
Many of theother models are syntax-based and are thereforeonly applicable to a subset of the LexSub data.We have re-evaluated our exemplar models on thesubsets we used in Erk and Pado?
(2008, EP08, 36795ModelsEP08 EP09 TDP09EP08 dataset 27.4 NA NAEP09 dataset NA 32.2 36.5actT actP actTPEP08 dataset 36.5 38.0 39.9EP09 dataset 39.1 39.9 39.6Table 4: Comparison to other models on two sub-sets of LexSub (GAP evaluation)datapoints) and Erk and Pado?
(2009, EP09, 100 dat-apoints).
The second set was also used by Thater etal.
(2009, TDP09).
The results in Table 4 comparethese models against our best previous exemplarmodels and show that our models outperform thesemodels across the board.
3 Due to the small sizesof these datasets, statistical significance is moredifficult to attain.
On EP09, the differences amongour models are not significant, but the differencebetween them and the original EP09 model is.4 OnEP08, all differences are significant except for actPvs.
actTP.We note that both the EP08 and the EP09datasets appear to be simpler to model than thecomplete Lexical Substitution dataset, at least byour exemplar-based models.
This underscores anold insight: namely, that direct syntactic neighbors,such as arguments and modifiers, provide strongclues as to word sense.5 Conclusions and OutlookThis paper reports on work in progress on an ex-emplar activation model as an alternative to one-vector-per-word approaches to word meaning incontext.
Exemplar activation is very effective inhandling polysemy, even with a very simple (andsparse) bag-of-words vector representation.
Onboth the EP08 and EP09 datasets, our models sur-pass more complex prototype-based approaches(Tab.
4).
It is also noteworthy that the exemplaractivation models work best when few exemplarsare used, which bodes well for their efficiency.We found that the best target representations re-3Since our models had the advantage of being tuned onthe dataset, we also report the range of results across theparameters we tested.
On the EP08 dataset, we obtained 33.1?36.5 for actT; 33.3?38.0 for actP; 37.7-39.9 for actTP.
On theEP09 dataset, the numbers were 35.8?39.1 for actT; 38.1?39.9for actP; 37.2?39.8 for actTP.4We did not have access to the TDP09 predictions to dosignificance testing.sult from activating a low absolute number of exem-plars.
Paraphrase representations are best activatedwith a percentage-based threshold.
Overall, wefound that paraphrase activation had a much largerimpact on performance than target activation, andthat drawing on target exemplars other than s torepresent the target meaning in context improvedover using s itself only for verbs (Tab.
3).
This sug-gests the possibility of considering T ?s activatedparaphrase candidates as the representation of T inthe context s, rather than some vector of T itself,in the spirit of Kintsch (2001).While it is encouraging that the best parametersettings involved the activation of only few exem-plars, computation with exemplar models still re-quires the management of large numbers of vectors.The computational overhead can be reduced by us-ing data structures that cut down on the numberof vector comparisons, or by decreasing vector di-mensionality (Gorman and Curran, 2006).
We willexperiment with those methods to determine thetradeoff of runtime and accuracy for this task.Another area of future work is to move beyondbag-of-words context: It is known from WSDthat syntactic and bag-of-words contexts providecomplementary information (Florian et al, 2002;Szpektor et al, 2008), and we hope that they can beintegrated in a more sophisticated exemplar model.Finally, we will to explore task-based evalua-tions.
Relation extraction and textual entailmentin particular are tasks where similar models havebeen used before (Szpektor et al, 2008).Acknowledgements.
This work was supportedin part by National Science Foundation grant IIS-0845925, and by a Morris Memorial Grant fromthe New York Community Trust.ReferencesR.
Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.2007.
Semantic inference at the lexical-syntacticlevel.
In Proceedings of AAAI, pages 871?876, Van-couver, BC.M.
Baroni and A. Lenci.
2009.
One distributionalmemory, many semantic spaces.
In Proceedings ofthe EACL Workshop on Geometrical Models of Nat-ural Language Semantics, Athens, Greece.W.
Daelemans, A. van den Bosch, and J. Zavrel.
1999.Forgetting exceptions is harmful in language learn-ing.
Machine Learning, 34(1/3):11?43.
Special Is-sue on Natural Language Learning.K.
Erk and S. Pado?.
2008.
A structured vector space96model for word meaning in context.
In Proceedingsof EMNLP, pages 897?906, Honolulu, HI.K.
Erk and S. Pado?.
2009.
Paraphrase assessment instructured vector space: Exploring parameters anddatasets.
In Proceedings of the EACL Workshop onGeometrical Models of Natural Language Seman-tics, Athens, Greece.R.
Florian, S. Cucerzan, C. Schafer, and D. Yarowsky.2002.
Combining classifiers for word sense disam-biguation.
Journal of Natural Language Engineer-ing, 8(4):327?341.J.
Gorman and J. R. Curran.
2006.
Scaling distribu-tional similarity to large corpora.
In Proceedings ofACL, pages 361?368, Sydney.T.
Griffiths, K. Canini, A. Sanborn, and D. J. Navarro.2007.
Unifying rational models of categorizationvia the hierarchical Dirichlet process.
In Proceed-ings of CogSci, pages 323?328, Nashville, TN.A.
Kilgarriff.
1997.
I don?t believe in word senses.Computers and the Humanities, 31(2):91?113.W.
Kintsch.
2001.
Predication.
Cognitive Science,25:173?202.T.
Landauer and S. Dumais.
1997.
A solution to Platosproblem: the latent semantic analysis theory of ac-quisition, induction, and representation of knowl-edge.
Psychological Review, 104(2):211?240.D.
McCarthy and J. Carroll.
2003.
Disambiguatingnouns, verbs, and adjectives using automatically ac-quired selectional preferences.
Computational Lin-guistics, 29(4):639?654.D.
McCarthy and R. Navigli.
2009.
The English lexi-cal substitution task.
Language Resources and Eval-uation, 43(2):139?159.
Special Issue on Compu-tational Semantic Analysis of Language: SemEval-2007 and Beyond.S.
McDonald and M. Ramscar.
2001.
Testing the dis-tributional hypothesis: The influence of context onjudgements of semantic similarity.
In Proceedingsof CogSci, pages 611?616.J.
Mitchell and M. Lapata.
2008.
Vector-based modelsof semantic composition.
In Proceedings of ACL,pages 236?244, Columbus, OH.G.
L. Murphy.
2002.
The Big Book of Concepts.
MITPress.G Salton, A Wang, and C Yang.
1975.
A vector-space model for information retrieval.
Journal of theAmerican Society for Information Science, 18:613?620.H.
Schu?tze.
1998.
Automatic word sense discrimina-tion.
Computational Linguistics, 24(1):97?124.I.
Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.2008.
Contextual preferences.
In Proceedings ofACL, pages 683?691, Columbus, OH.S.
Thater, G. Dinu, and M. Pinkal.
2009.
Rankingparaphrases in context.
In Proceedings of the ACLWorkshop on Applied Textual Inference, pages 44?47, Singapore.W.
Voorspoels, W. Vanpaemel, and G. Storms.
2009.The role of extensional information in conceptualcombination.
In Proceedings of CogSci.97
