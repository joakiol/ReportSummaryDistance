Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 137?140,New York, June 2006. c?2006 Association for Computational LinguisticsWeblog Classification for Fast Splog Filtering:A URL Language Model Segmentation ApproachFranco Salvetti?!
Nicolas Nicolov!franco.salvetti@colorado.edu nicolas@umbrialistens.com?Dept.
of Computer Science, Univ.
of Colorado at Boulder, 430 UCB, Boulder, CO 80309-0430!Umbria, Inc., 1655 Walnut Str, Boulder, CO 80302AbstractThis paper shows that in the context ofstatistical weblog classification for splogfiltering based on n-grams of tokens inthe URL, further segmenting the URLsbeyond the standard punctuation is help-ful.
Many splog URLs contain phrasesin which the words are glued together inorder to avoid splog filtering techniquesbased on punctuation segmentation andunigrams.
A technique which segmentslong tokens into the words forming thephrase is proposed and evaluated.
The re-sulting tokens are used as features for aweblog classifier whose accuracy is sim-ilar to that of humans (78% vs. 76%) andreaches 93.3% of precision in identifyingsplogs with recall of 50.9%.1 IntroductionThe blogosphere, which is a subset of the web andis comprised of personal electronic journals (we-blogs) currently encompasses 27.2 million pagesand doubles in size every 5.5 months (Technorati,2006).
The information contained in the blogo-sphere has been proven valuable for applicationssuch as marketing intelligence, trend discovery, andopinion tracking (Hurst, 2005).
Unfortunately in thelast year the blogosphere has been heavily pollutedwith spam weblogs (called splogs) which are we-blogs used for different purposes, including promot-ing affiliated websites (Wikipedia, 2006).
Splogscan skew the results of applications meant to quan-titatively analyze the blogosphere.
Sophisticatedcontent-based methods or methods based on linkanalysis (Gyo?ngyi et al, 2004), while providing ef-fective splog filtering, require extra web crawlingand can be slow.
While a combination of approachesis necessary to provide adequate splog filtering, sim-ilar to (Kan & Thi, 2005), we propose, as a pre-liminary step in the overall splog filtering, a fast,lightweight and accurate method merely based onthe analysis of the URL of the weblog without con-sidering its content.For quantitative and qualitative analysis of thecontent of the blogosphere, it is acceptable to elim-inate a small fraction of good data from analysisas long as the remainder of the data is splog-free.This elimination should be kept to a minimum topreserve counts needed for reliable analysis.
Whenusing an ensemble of methods for comprehensivesplog filtering it is acceptable for pre-filtering ap-proaches to lower recall in order to improve preci-sion allowing more expensive techniques to be ap-plied on a smaller set of weblogs.
The proposedmethod reaches 93.3% of precision in classifying aweblog in terms of spam or good if 49.1% of thedata are left aside (labeled as unknown).
If all dataneeds to be classified our method achieves 78% ac-curacy which is comparable to the average accuracyof humans (76%) on the same classification task.Sploggers, in creating splogs, aim to increase thetraffic to specific websites.
To do so, they frequentlycommunicate a concept (e.g., a service or a prod-uct) through a short, sometimes non-grammaticalphrase embedded in the URL of the weblog (e.g.,http://adult-video-mpegs.blogspot.com ) .
Wewant to build a statistical classifier which leveragesthe language used in these descriptive URLs in orderto classify weblogs as spam or good.
We built aninitial language model-based classifier on the tokensof the URLs after tokenizing on punctuation (., -,137, /, ?, =, etc.).
We ran the system and got an ac-curacy of 72.2% which is close to the accuracy ofhumans?76% (the baseline is 50% as the trainingdata is balanced).
When we did error analysis on themisclassified examples we observed that many of themistakes were on URLs that contain words glued to-gether as one token (e.g., dailyfreeipod).
Had thewords in these tokens been segmented the initial sys-tem would have classified the URL correctly.
We,thus, turned our attention to additional segmentingof the URLs beyond just punctuation and using thisintra-token segmentation in the classification.Training a segmenter on standard available textcollections (e.g., PTB or BNC) did not seem the wayto procede because the lexical items used and the se-quence in which they appear differ from the usagein the URLs.
Given that we are interested in unsu-pervised lightweight approaches for URL segmenta-tion, one possibility is to use the URLs themselvesafter segmenting on punctuation and to try to learnthe segmenting (the majority of URLs are naturallysegmented using punctuation as we shall see later).We trained a segmenter on the tokens in the URLs,unfortunately this method did not provide sufficientimprovement over the system which uses tokeniza-tion on punctuation.
We hypothesized that the con-tent of the splog pages corresponding to the splogURLs could be used as a corpus to learn the seg-mentation.
We crawled 20K weblogs correspond-ing to the 20K URLs labeled as spam and goodin the training set, converted them to text, tokenizedand used the token sequences as training data for thesegmenter.
This led to a statistically significant im-provement of 5.8% of the accuracy of the splog filter.2 Engineering of splogsFrequently sploggers indicate the semantic con-tent of the weblogs using descriptive phrases?often noun groups (non-recursive noun phrases) likeadult-video-mpegs.
There are different varietiesof splogs: commercial products (especially electron-ics), vacations, mortgages, and adult-related.Users don?t want to see splogs in their resultsand marketing intelligence applications are affectedwhen data contains splogs.
Existing approachesto splog filtering employ statistical classifiers (e.g.,SVMs) trained on the tokens in a URL after to-kenization on punctuation (Kolari et al, 2006).To avoid being identified as a splog by such sys-tems one of the creative techniques that splog-gers use is to glue words together into longer to-kens for which there will not be statistical informa-tion (e.g., businessopportunitymoneyworkathomeis unlikely to appear in the training data whilebusiness, opportunity, money, work, at and homeare likely to have been seen in training).
Another ap-proach to dealing with splogs is having a list of splogwebsites (SURBL, 2006).
Such an approach basedon blacklists is now less effective because bloghostsprovide tools which can be used for the automaticcreation of a large quantity of splogs.3 Splog filteringThe weblog classifier uses a segmenter which splitsthe URL in tokens and then the token sequence isused for supervised learning and classification.3.1 URL segmentationThe segmenter first tokenizes the URLs on punctua-tion symbols.
Then the current URL tokens are ex-amined for further possible segmentation.
The seg-menter uses a sliding window of n (e.g., 6) charac-ters.
Going from left to right in a greedy fashion thesegmenter decides whether to split after the currentthird character.
Figure 1 illustrates the processing ofwww.dietthatworks.com when considering the to-ken dietthatworks.
The character ???
indicates thatthe left and right tri-grams are kept together while???
indicates a point where the segmenter decides abreak should occur.
The segmentation decisions ared i e ?
t t hatworksd i e t ?
t h aatworksFigure 1: Workings of the segmenterbased on counts collected during training.
For ex-ample, during the segmentation of dietthatworksin the case of i e t ?
t h a we essentially con-sider how many times we have seen in the trainingdata the 6-gram ?iettha?
vs. ?iet tha?.
Certaincharacters (e.g., digits) are generalized both duringtraining and segmentation.1383.2 ClassificationFor the weblog classification a simple Na?
?ve Bayesclassifier is used.
Given a token sequence T =?t1, .
.
.
, tn?, representing the segmented URL, theclass c?
?
C = {spam,good} is decided as:c?
= arg maxc?CP (c|T ) = arg maxc?CP (c) ?
P (T |c)P (T )= arg maxc?CP (c) ?
P (T |c)= arg maxc?CP (c) ?n?i=1P (ti|c)In the last step we made the conditional indepen-dence assumption.
For calculating P (ti|c) we useLaplace (add one) smoothing (Jurafsky & Martin,2000).
We have also explored classification via sim-ple voting techniques such as:a = sgnn?i=1sgn (P (ti|spam) ?
P (ti|good))c?
={ spam, if a = 1good, otherwiseBecause we are interested in having control over theprecision/recall of the classifier we introduce a scoremeant to be used for deciding whether to label aURL as unknown.score(T ) =???
?P (spam|T ) ?
P (good|T )P (spam|T ) + P (good|T )???
?If score(T ) exceeds a certain threshold ?
we labelT as spam or good using the greater probability ofP (spam|T ) or P (good|T ).
To control the presi-cion of the classifier we can tune ?
.
For instance,when we set ?
= 0.75 we achieve 93.3% of preci-sion which implied a recall of 50.9%.
An alternatecommonly used technique to compute a score is tolook at the log likelihood ratio.4 Experiments and resultsFirst we discuss the segmenter.
10,000 spam and10,000 good weblog URLs and their correspondingHTML pages were used for the experiments.
The20,000 weblog HTML pages are used to induce thesegmenter.
The first experiment was aimed at find-ing how common extra segmentation beyond punc-tuation is as a phenomenon.
The segmenter was runon the actual training URLs.
The number of URLsthat are additionally segmented besides the segmen-tation on punctuation are reported in Table 1.# of # spam # goodsplits URLs URLs1 2,235 2,2742 868 4593 223 464 77 75 2 16 4 18 3 ?Total 3,412 2,788Table 1: Number of extra segmentations in a URLThe multiple segmentations need not all occur on thesame token in the URL after initial segmentation onpunctuations.The segmenter was then evaluated on a separatetest set of 1,000 URLs for which the ground truthfor the segmentation was marked.
The results arein Table 2.
The evaluation is only on segmentationevents and does not include tokenization decisionsaround punctuation.Precision Recall F-measure84.31 48.84 61.85Table 2: Performance of the segmenterFigure 2 shows long tokens which are correctly split.The weblog classifier was then run on the test set.The results are shown in Table 3.cash ?
for ?
your ?
houseunlimitted ?
pet ?
suplliesjim ?
and ?
body ?
fatweight ?
loss ?
product ?
infokick ?
the ?
boy ?
and ?
runbringing ?
back ?
the ?
pastfood ?
for ?
your ?
speakersFigure 2: Correct segmentations139accuracy 78%prec.
spam 82%rec.
spam 71%f-meas spam 76%prec.
good 74%rec.
good 84%f-meas good 79%Table 3: Classification resultsThe performance of humans on this task was alsoevaluated.
Eight individuals performed the splogidentification just looking at the unsegmented URLs.The results for the human annotators are given in Ta-ble 4.
The average accuracy of the humans (76%) issimilar to that of the system (78%).Mean ?accuracy 76% 6.71prec.
spam 83% 7.57rec.
spam 65% 6.35f-meas spam 73% 7.57prec.
good 71% 6.35rec.
good 87% 6.39f-meas good 78% 6.08Table 4: Results for the human annotatorsFrom an information retrieval perspective if only50.9% of the URLs are retrieved (labelled as ei-ther spam or good and the rest are labelledas unknown) then of the spam/good decisions93.3% are correct.
This is relevant for cases wherea URL splog filter is in cascade followed by, for ex-ample, a content-based one.5 DiscussionThe system performs better with the intra-token seg-mentation because the system is forced to guess un-seen events on fewer occasions.
For instance giventhe input URL www.ipodipodipod.com in the sys-tem which segments solely on punctuation both thespam and the good model will have to guess theprobability of ipodipodipod and the results dependmerely on the smoothing technique.Even if we reached the average accuracy of hu-mans we expect to be able to improve the systemfurther as the maximum accuracy among the humanannotators is 90%.
Among the errors of the seg-menter the most common are related to plural nouns(?girl?s?
vs.
?girls?)
and past tense of verbs(?dedicate?d?
vs.
?dedicated?)
.The proposed approach has ramifications for splogfiltering systems that want to consider the outwardlinks from a weblog.6 ConclusionsWe have presented a technique for determiningwhether a weblog is splog based merely on alalyz-ing its URL.
We proposed an approach where weinitially segment the URL in words and then do theclassification.
The technique is simple, yet veryeffective?our system reaches an accuracy of 78%(while humans perform at 76%) and 93.3% of preci-sion in classifying a weblog with recall of 50.9%.Acknowledgements.
We wish to thank Ted Kre-mer, Howard Kaushansky, Ash Beits, Allen Bennett,Susanne Costello, Hillary Gustave, Glenn Meuth,Micahel Sevilla and Ron Woodward for help withthe experiments and comments on an earlier draft.ReferencesGyo?ngyi, Zoltan, Hector Garcia-Molina & Jan Pedersen.
2004.?Combating Web Spam with TrustRank?.
Proceedings of the30th International Conference on Very Large Data Bases(VLDB).Matthew Hurst.
2005.
?Deriving Marketing Intelligence fromOnline Discussion?.
11th ACM SIGKDD Int.
Conf.
onKnowledge Discovery in Data Mining (KDD05), 419-428.Chicago, Illinois, USA.Jurafsky, D. & J.H.
Martin.
2000.
Speech and Language Pro-cessing.
Upper Saddle River, NJ: Prentice Hall.Min-Yen Kan & Hoang Oanh Nguyen Thi.
2005.
?Fast Web-page Classification Using URL Features?.
14th ACM in-ternational conference on Information and Knowledge Man-agement, 325-326.Kolari, Pranam, Tim Finin & Anupam Joshi.
2006.
?SVMs forthe Blogosphere: Blog Identification and Splog Detection?.AAAI Symposium on Computational Approaches to Analyz-ing Weblogs, 92-99.
Stanford.SURBL.
2006.
SURBL ?
Spam URI Realtime Blocklists,http://www.surbl.orgTechnorati.
2006.
State of the Blogosphere, Febru-ary 2006 Part 1: On Blogosphere Growth,technorati.com/weblog/2006/02/81.htmlWikipedia.
2006.
Splog (Spam blog),http://en.wikipedia.org/wiki/Splog140
