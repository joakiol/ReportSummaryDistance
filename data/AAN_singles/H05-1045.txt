Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 355?362, Vancouver, October 2005. c?2005 Association for Computational LinguisticsIdentifying Sources of Opinions with Conditional Random Fields andExtraction PatternsYejin Choi and Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853{ychoi,cardie}@cs.cornell.eduEllen Riloff and Siddharth PatwardhanSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{riloff,sidd}@cs.utah.eduAbstractRecent systems have been developed forsentiment classification, opinion recogni-tion, and opinion analysis (e.g., detect-ing polarity and strength).
We pursue an-other aspect of opinion analysis: identi-fying the sources of opinions, emotions,and sentiments.
We view this problem asan information extraction task and adopta hybrid approach that combines Con-ditional Random Fields (Lafferty et al,2001) and a variation of AutoSlog (Riloff,1996a).
While CRFs model source iden-tification as a sequence tagging task, Au-toSlog learns extraction patterns.
Our re-sults show that the combination of thesetwo methods performs better than eitherone alone.
The resulting system identifiesopinion sources with 79.3% precision and59.5% recall using a head noun matchingmeasure, and 81.2% precision and 60.6%recall using an overlap measure.1 IntroductionIn recent years, there has been a great deal of in-terest in methods for automatically identifying opin-ions, emotions, and sentiments in text.
Much ofthis research explores sentiment classification, a textcategorization task in which the goal is to classifya document as having positive or negative polar-ity (e.g., Das and Chen (2001), Pang et al (2002),Turney (2002), Dave et al (2003), Pang and Lee(2004)).
Other research efforts analyze opinion ex-pressions at the sentence level or below to recog-nize opinions, their polarity, and their strength (e.g.,Dave et al (2003), Pang and Lee (2004), Wilson etal.
(2004), Yu and Hatzivassiloglou (2003), Wiebeand Riloff (2005)).
Many applications could ben-efit from these opinion analyzers, including prod-uct reputation tracking (e.g., Morinaga et al (2002),Yi et al (2003)), opinion-oriented summarization(e.g., Cardie et al (2004)), and question answering(e.g., Bethard et al (2004), Yu and Hatzivassiloglou(2003)).We focus here on another aspect of opinionanalysis: automatically identifying the sources ofthe opinions.
Identifying opinion sources willbe especially critical for opinion-oriented question-answering systems (e.g., systems that answer ques-tions of the form ?How does [X] feel about [Y]??
)and opinion-oriented summarization systems, bothof which need to distinguish the opinions of onesource from those of another.1The goal of our research is to identify direct andindirect sources of opinions, emotions, sentiments,and other private states that are expressed in text.To illustrate the nature of this problem, consider theexamples below:S1: Taiwan-born voters favoring independence...1In related work, we investigate methods to identify theopinion expressions (e.g., Riloff and Wiebe (2003), Wiebe andRiloff (2005), Wilson et al (2005)) and the nesting structureof sources (e.g., Breck and Cardie (2004)).
The target of eachopinion, i.e., what the opinion is directed towards, is currentlybeing annotated manually for our corpus.355S2: According to the report, the human rightsrecord in China is horrendous.S3: International officers believe that the EU willprevail.S4: International officers said US officials want theEU to prevail.In S1, the phrase ?Taiwan-born voters?
is the di-rect (i.e., first-hand) source of the ?favoring?
sen-timent.
In S2, ?the report?
is the direct source ofthe opinion about China?s human rights record.
InS3, ?International officers?
are the direct source ofan opinion regarding the EU.
The same phrase inS4, however, denotes an indirect (i.e., second-hand,third-hand, etc.)
source of an opinion whose directsource is ?US officials?.In this paper, we view source identification as aninformation extraction task and tackle the problemusing sequence tagging and pattern matching tech-niques simultaneously.
Using syntactic, semantic,and orthographic lexical features, dependency parsefeatures, and opinion recognition features, we train alinear-chain Conditional Random Field (CRF) (Laf-ferty et al, 2001) to identify opinion sources.
In ad-dition, we employ features based on automaticallylearned extraction patterns and perform feature in-duction on the CRF model.We evaluate our hybrid approach using the NRRCcorpus (Wiebe et al, 2005), which is manuallyannotated with direct and indirect opinion sourceinformation.
Experimental results show that theCRF model performs well, and that both the extrac-tion patterns and feature induction produce perfor-mance gains.
The resulting system identifies opinionsources with 79.3% precision and 59.5% recall us-ing a head noun matching measure, and 81.2% pre-cision and 60.6% recall using an overlap measure.2 The Big PictureThe goal of information extraction (IE) systems isto extract information about events, including theparticipants of the events.
This task goes beyondNamed Entity recognition (e.g., Bikel et al (1997))because it requires the recognition of role relation-ships.
For example, an IE system that extracts in-formation about corporate acquisitions must distin-guish between the company that is doing the acquir-ing and the company that is being acquired.
Sim-ilarly, an IE system that extracts information aboutterrorism must distinguish between the person whois the perpetrator and the person who is the victim.We hypothesized that IE techniques would be well-suited for source identification because an opinionstatement can be viewed as a kind of speech eventwith the source as the agent.We investigate two very different learning-basedmethods from information extraction for the prob-lem of opinion source identification: graphical mod-els and extraction pattern learning.
In particular, weconsider Conditional Random Fields (Lafferty et al,2001) and a variation of AutoSlog (Riloff, 1996a).CRFs have been used successfully for Named En-tity recognition (e.g., McCallum and Li (2003),Sarawagi and Cohen (2004)), and AutoSlog has per-formed well on information extraction tasks in sev-eral domains (Riloff, 1996a).
While CRFs treatsource identification as a sequence tagging task, Au-toSlog views the problem as a pattern-matching task,acquiring symbolic patterns that rely on both thesyntax and lexical semantics of a sentence.
We hy-pothesized that a combination of the two techniqueswould perform better than either one alone.Section 3 describes the CRF approach to identify-ing opinion sources and the features that the systemuses.
Section 4 then presents a new variation of Au-toSlog, AutoSlog-SE, which generates IE patterns toextract sources.
Section 5 describes the hybrid sys-tem: we encode the IE patterns as additional featuresin the CRF model.
Finally, Section 6 presents ourexperimental results and error analysis.3 Semantic Tagging via ConditionalRandom FieldsWe defined the problem of opinion source identifi-cation as a sequence tagging task via CRFs as fol-lows.
Given a sequence of tokens, x = x1x2...xn,we need to generate a sequence of tags, or labels,y = y1y2...yn.
We define the set of possible labelvalues as ?S?, ?T?, ?-?, where ?S?
is the first to-ken (or Start) of a source, ?T?
is a non-initial token(i.e., a conTinuation) of a source, and ?-?
is a tokenthat is not part of any source.2A detailed description of CRFs can be found in2This is equivalent to the IOB tagging scheme used in syn-tactic chunkers (Ramshaw and Marcus, 1995).356Lafferty et al (2001).
For our sequence taggingproblem, we create a linear-chain CRF based onan undirected graph G = (V,E), where V is theset of random variables Y = {Yi|1 ?
i ?
n},one for each of n tokens in an input sentence;and E = {(Yi?1, Yi)|1 < i ?
n} is the setof n ?
1 edges forming a linear chain.
For eachsentence x, we define a non-negative clique poten-tial exp(?Kk=1 ?kfk(yi?1, yi, x)) for each edge, andexp(?K?k=1 ?
?kf ?k(yi, x)) for each node, where fk(...)is a binary feature indicator function, ?k is a weightassigned for each feature function, and K and K ?are the number of features defined for edges andnodes respectively.
Following Lafferty et al (2001),the conditional probability of a sequence of labels ygiven a sequence of tokens x is:P (y|x) = 1Zxexp?Xi,k?k fk(yi?1, yi, x)+Xi,k?
?k f ?k(yi, x)?
(1)Zx =Xyexp?Xi,k?k fk(yi?1, yi, x) +Xi,k?
?k f ?k(yi, x)?
(2)where Zx is a normalization constant for eachx.
Given the training data D, a set of sen-tences paired with their correct ?ST-?
source la-bel sequences, the parameters of the model aretrained to maximize the conditional log-likelihood?
(x,y)?D P (y|x).
For inference, given a sentence xin the test data, the tagging sequence y is given byargmaxy?P (y?|x).3.1 FeaturesTo develop features, we considered three propertiesof opinion sources.
First, the sources of opinions aremostly noun phrases.
Second, the source phrasesshould be semantic entities that can bear or expressopinions.
Third, the source phrases should be di-rectly related to an opinion expression.
When con-sidering only the first and second criteria, this taskreduces to named entity recognition.
Because of thethird condition, however, the task requires the recog-nition of opinion expressions and a more sophisti-cated encoding of sentence structure to capture re-lationships between source phrases and opinion ex-pressions.With these properties in mind, we define the fol-lowing features for each token/word xi in an inputsentence.
For pedagogical reasons, we will describesome of the features as being multi-valued or cate-gorical features.
In practice, however, all featuresare binarized for the CRF model.Capitalization features We use two boolean fea-tures to represent the capitalization of a word:all-capital, initial-capital.Part-of-speech features Based on the lexical cat-egories produced by GATE (Cunningham et al,2002), each token xi is classified into one of a setof coarse part-of-speech tags: noun, verb, adverb,wh-word, determiner, punctuation, etc.
We do thesame for neighboring words in a [?2,+2] windowin order to assist noun phrase segmentation.Opinion lexicon features For each token xi, we in-clude a binary feature that indicates whether or notthe word is in our opinion lexicon ?
a set of wordsthat indicate the presence of an opinion.
We do thesame for neighboring words in a [?1,+1] window.Additionally, we include for xi a feature that in-dicates the opinion subclass associated with xi, ifavailable from the lexicon.
(e.g., ?bless?
is clas-sified as ?moderately subjective?
according to thelexicon, while ?accuse?
and ?berate?
are classifiedmore specifically as ?judgments?.)
The lexicon isinitially populated with approximately 500 opinionwords 3 from (Wiebe et al, 2002), and then aug-mented with opinion words identified in the trainingdata.
The training data contains manually producedphrase-level annotations for all expressions of opin-ions, emotions, etc.
(Wiebe et al, 2005).
We col-lected all content words that occurred in the trainingset such that at least 50% of their occurrences werein opinion annotations.Dependency tree features For each token xi, wecreate features based on the parse tree produced bythe Collins (1999) dependency parser.
The purposeof the features is to (1) encode structural informa-tion, and (2) indicate whether xi is involved in anygrammatical relations with an opinion word.
Twopre-processing steps are required before features canbe constructed:3Some words are drawn from Levin (1993); others are fromFramenet lemmas (Baker et al 1998) associated with commu-nication verbs.3571.
Syntactic chunking.
We traverse the depen-dency tree using breadth-first search to identifyand group syntactically related nodes, produc-ing a flatter, more concise tree.
Each syntac-tic ?chunk?
is also assigned a grammatical role(e.g., subject, object, verb modifier, time,location, of-pp, by-pp) based on its con-stituents.
Possessives (e.g., ?Clinton?s idea?
)and the phrase ?according to X?
are handled asspecial cases in the chunking process.2.
Opinion word propagation.
Although theopinion lexicon contains only content wordsand no multi-word phrases, actual opinions of-ten comprise an entire phrase, e.g., ?is reallywilling?
or ?in my opinion?.
As a result, wemark as an opinion the entire chunk that con-tains an opinion word.
This allows each tokenin the chunk to act as an opinion word for fea-ture encoding.After syntactic chunking and opinion word propa-gation, we create the following dependency tree fea-tures for each token xi:?
the grammatical role of its chunk?
the grammatical role of xi?1?s chunk?
whether the parent chunk includes an opinionword?
whether xi?s chunk is in an argument positionwith respect to the parent chunk?
whether xi represents a constituent boundarySemantic class features We use 7 binary fea-tures to encode the semantic class of each wordxi: authority, government, human, media,organization or company, proper name,and other.
The other class captures 13 seman-tic classes that cannot be sources, such as vehicleand time.Semantic class information is derived from namedentity and semantic class labels assigned to xi by theSundance shallow parser (Riloff, 2004).
Sundanceuses named entity recognition rules to label nounphrases as belonging to named entity classes, andassigns semantic tags to individual words based ona semantic dictionary.
Table 1 shows the hierarchythat Sundance uses for semantic classes associatedwith opinion sources.
Sundance is also used to rec-ognize and instantiate the source extraction patternsPROPER NAMEAUTHORITY LOCATIONCITYCOUNTRYPLANETPROVINCEPERSON NAMEPERSON DESCNATIONALITYTITLECOMPANYGOVERNMENTMEDIAORGANIZATIONHUMANSOURCEFigure 1: The semantic hierarchy for opinionsourcesthat are learned by AutoSlog-SE, which is describedin the next section.4 Semantic Tagging via ExtractionPatternsWe also learn patterns to extract opinion sources us-ing a statistical adaptation of the AutoSlog IE learn-ing algorithm.
AutoSlog (Riloff, 1996a) is a super-vised extraction pattern learner that takes a train-ing corpus of texts and their associated answer keysas input.
A set of heuristics looks at the contextsurrounding each answer and proposes a lexico-syntactic pattern to extract that answer from the text.The heuristics are not perfect, however, so the result-ing set of patterns needs to be manually reviewed bya person.In order to build a fully automatic system thatdoes not depend on manual review, we combinedAutoSlog?s heuristics with statistics from the an-notated training data to create a fully automaticsupervised learner.
We will refer to this learneras AutoSlog-SE (Statistically Enhanced variationof AutoSlog).
AutoSlog-SE?s learning process hasthree steps:Step 1: AutoSlog?s heuristics are applied to everynoun phrase (NP) in the training corpus.
Thisgenerates a set of extraction patterns that, col-lectively, can extract every NP in the trainingcorpus.Step 2: The learned patterns are augmented withselectional restrictions that semantically con-strain the types of noun phrases that are legiti-mate extractions for opinion sources.
We used358the semantic classes shown in Figure 1 as se-lectional restrictions.Step 3: The patterns are applied to the training cor-pus and statistics are gathered about their ex-tractions.
We count the number of extrac-tions that match annotations in the corpus (cor-rect extractions) and the number of extractionsthat do not match annotations (incorrect extrac-tions).
These counts are then used to estimatethe probability that the pattern will extract anopinion source in new texts:P (source | patterni) =correct sourcescorrect sources + incorrect sourcesThis learning process generates a set of extractionpatterns coupled with probabilities.
In the next sec-tion, we explain how these extraction patterns arerepresented as features in the CRF model.5 Extraction Pattern Features for the CRFThe extraction patterns provide two kinds of infor-mation.
SourcePatt indicates whether a wordactivates any source extraction pattern.
For exam-ple, the word ?complained?
activates the pattern?<subj> complained?
because it anchors the ex-pression.
SourceExtr indicates whether a word isextracted by any source pattern.
For example, in thesentence ?President Jacques Chirac frequently com-plained about France?s economy?, the words ?Pres-ident?, ?Jacques?, and ?Chirac?
would all be ex-tracted by the ?<subj> complained?
pattern.Each extraction pattern has frequency and prob-ability values produced by AutoSlog-SE, hence wecreate four IE pattern-based features for each tokenxi: SourcePatt-Freq, SourceExtr-Freq,SourcePatt-Prob, and SourceExtr-Prob,where the frequency values are divided into threeranges: {0, 1, 2+} and the probability values are di-vided into five ranges of equal size.6 ExperimentsWe used the Multi-Perspective Question Answering(MPQA) corpus4 for our experiments.
This corpus4The MPQA corpus can be freely obtained athttp://nrrc.mitre.org/NRRC/publications.htm.consists of 535 documents that have been manu-ally annotated with opinion-related information in-cluding direct and indirect sources.
We used 135documents as a tuning set for model developmentand feature engineering, and used the remaining 400documents for evaluation, performing 10-fold crossvalidation.
These texts are English language ver-sions of articles that come from many countries andcover many topics.5We evaluate performance using 3 measures: over-lap match (OL), head match (HM), and exact match(EM).
OL is a lenient measure that considers an ex-traction to be correct if it overlaps with any of the an-notated words.
HM is a more conservative measurethat considers an extraction to be correct if its headmatches the head of the annotated source.
We reportthese somewhat loose measures because the annota-tors vary in where they place the exact boundariesof a source.
EM is the strictest measure that requiresan exact match between the extracted words and theannotated words.
We use three evaluation metrics:recall, precision, and F-measure with recall and pre-cision equally weighted.6.1 BaselinesWe developed three baseline systems to assess thedifficulty of our task.
Baseline-1 labels as sourcesall phrases that belong to the semantic categoriesauthority, government, human, media,organization or company, proper name.Table 1 shows that the precision is poor, suggest-ing that the third condition described in Section 3.1(opinion recognition) does play an important role insource identification.
The recall is much higher butstill limited due to sources that fall outside of the se-mantic categories or are not recognized as belong-ing to these categories.
Baseline-2 labels a nounphrase as a source if any of the following are true:(1) the NP is the subject of a verb phrase containingan opinion word, (2) the NP follows ?according to?,(3) the NP contains a possessive and is preceded byan opinion word, or (4) the NP follows ?by?
and at-taches to an opinion word.
Baseline-2?s heuristicsare designed to address the first and the third condi-tions in Section 3.1.
Table 1 shows that Baseline-2is substantially better than Baseline-1.
Baseline-35This data was obtained from the Foreign Broadcast Infor-mation Service (FBIS), a U.S. government agency.359Recall Prec F1OL 77.3 28.8 42.0Baseline-1 HM 71.4 28.6 40.8EM 65.4 20.9 31.7OL 62.4 60.5 61.4Baseline-2 HM 59.7 58.2 58.9EM 50.8 48.9 49.8OL 49.9 72.6 59.2Baseline-3 HM 47.4 72.5 57.3EM 44.3 58.2 50.3OL 48.5 81.3 60.8Extraction Patterns HM 46.9 78.5 58.7EM 41.9 70.2 52.5CRF: OL 56.1 81.0 66.3basic features HM 55.1 79.2 65.0EM 50.0 72.4 59.2CRF: OL 59.1 82.4 68.9basic + IE pattern HM 58.1 80.5 67.5features EM 52.5 73.3 61.2CRF-FI: OL 57.7 80.7 67.3basic features HM 56.8 78.8 66.0EM 51.7 72.4 60.3CRF-FI: OL 60.6 81.2 69.4basic + IE pattern HM 59.5 79.3 68.0features EM 54.1 72.7 62.0Table 1: Source identification performance tablelabels a noun phrase as a source if it satisfies bothBaseline-1 and Baseline-2?s conditions (this shouldsatisfy all three conditions described in Section 3.1).As shown in Table 1, the precision of this approachis the best of the three baselines, but the recall is thelowest.6.2 Extraction Pattern ExperimentWe evaluated the performance of the learned extrac-tion patterns on the source identification task.
Thelearned patterns were applied to the test data andthe extracted sources were scored against the manualannotations.6 Table 1 shows that the extraction pat-terns produced lower recall than the baselines, butwith considerably higher precision.
These resultsshow that the extraction patterns alone can identify6These results were obtained using the patterns that had aprobability > .50 and frequency > 1.nearly half of the opinion sources with good accu-racy.6.3 CRF ExperimentsWe developed our CRF model using the MALLETcode from McCallum (2002).
For training, we useda Gaussian prior of 0.25, selected based on the tun-ing data.
We evaluate the CRF using the basic fea-tures from Section 3, both with and without the IEpattern features from Section 5.
Table 1 shows thatthe CRF with basic features outperforms all of thebaselines as well as the extraction patterns, achiev-ing an F-measure of 66.3 using the OL measure,65.0 using the HM measure, and 59.2 using theEM measure.
Adding the IE pattern features fur-ther increases performance, boosting recall by about3 points for all of the measures and slightly increas-ing precision as well.CRF with feature induction.
One limitation oflog-linear function models like CRFs is that theycannot form a decision boundary from conjunctionsof existing features, unless conjunctions are explic-itly given as part of the feature vector.
For thetask of identifying opinion sources, we observedthat the model could benefit from conjunctive fea-tures.
For instance, instead of using two separatefeatures, HUMAN and PARENT-CHUNK-INCLUDES-OPINION-EXPRESSION, the conjunction of the twois more informative.For this reason, we applied the CRF feature in-duction approach introduced by McCallum (2003).As shown in Table 1, where CRF-FI stands for theCRF model with feature induction, we see consis-tent improvements by automatically generating con-junctive features.
The final system, which com-bines the basic features, the IE pattern features,and feature induction achieves an F-measure of 69.4(recall=60.6%, precision=81.2%) for the OL mea-sure, an F-measure of 68.0 (recall=59.5%, preci-sion=79.3%) for the HM measure, and an F-measureof 62.0 (recall=54.1%, precision=72.7%) for the EMmeasure.6.4 Error AnalysisAn analysis of the errors indicated some commonmistakes:?
Some errors resulted from error propagation in360our subsystems.
Errors from the sentence bound-ary detector in GATE (Cunningham et al, 2002)were especially problematic because they causedthe Collins parser to fail, resulting in no depen-dency tree information.?
Some errors were due to complex and unusualsentence structure, which our rather simple fea-ture encoding for CRF could not capture well.?
Some errors were due to the limited coverage ofthe opinion lexicon.
We failed to recognize somecases when idiomatic or vague expressions wereused to express opinions.Below are some examples of errors that we foundinteresting.
Doubly underlined phrases indicate in-correctly extracted sources (either false positivesor false negatives).
Opinion words are singlyunderlined.False positives:(1) Actually, these three countries do have one commondenominator, i.e., that their values and policies do notagree with those of the United States and none of themare on good terms with the United States.
(2) Perhaps this is why Fidel Castro has not spoken outagainst what might go on in Guantanamo.In (1), ?their values and policies?
seems like a rea-sonable phrase to extract, but the annotation does notmark this as a source, perhaps because it is some-what abstract.
In (2), ?spoken out?
is negated, whichmeans that the verb phrase does not bear an opinion,but our system failed to recognize the negation.False negatives:(3) And for this reason, too, they have a moral duty tospeak out, as Swedish Foreign Minister Anna Lindh,among others, did yesterday.
(4) In particular, Iran and Iraq are at loggerheads witheach other to this day.Example (3) involves a complex sentence structurethat our system could not deal with.
(4) involves anuncommon opinion expression that our system didnot recognize.7 Related WorkTo our knowledge, our research is the first to auto-matically identify opinion sources using the MPQAopinion annotation scheme.
The most closely re-lated work on opinion analysis is Bethard et al(2004), who use machine learning techniques toidentify propositional opinions and their holders(sources).
However, their work is more limitedin scope than ours in several ways.
Their workonly addresses propositional opinions, which are?localized in the propositional argument?
of cer-tain verbs such as ?believe?
or ?realize?.
In con-trast, our work aims to find sources for all opinions,emotions, and sentiments, including those that arenot related to a verb at all.
Furthermore, Berthardet al?s task definition only requires the identifica-tion of direct sources, while our task requires theidentification of both direct and indirect sources.Bethard et al evaluate their system on manuallyannotated FrameNet (Baker et al, 1998) and Prop-Bank (Palmer et al, 2005) sentences and achieve48% recall with 57% precision.Our IE pattern learner can be viewed as a crossbetween AutoSlog (Riloff, 1996a) and AutoSlog-TS (Riloff, 1996b).
AutoSlog is a supervised learnerthat requires annotated training data but does notcompute statistics.
AutoSlog-TS is a weakly super-vised learner that does not require annotated databut generates coarse statistics that measure each pat-tern?s correlation with relevant and irrelevant docu-ments.
Consequently, the patterns learned by bothAutoSlog and AutoSlog-TS need to be manually re-viewed by a person to achieve good accuracy.
Incontrast, our IE learner, AutoSlog-SE, computesstatistics directly from the annotated training data,creating a fully automatic variation of AutoSlog.8 ConclusionWe have described a hybrid approach to the problemof extracting sources of opinions in text.
We castthis problem as an information extraction task, usingboth CRFs and extraction patterns.
Our research isthe first to identify both direct and indirect sourcesfor all types of opinions, emotions, and sentiments.Directions for future work include trying to in-crease recall by identifying relationships betweenopinions and sources that cross sentence boundaries,and relationships between multiple opinion expres-sions by the same source.
For example, the fact thata coreferring noun phrase was marked as a sourcein one sentence could be a useful clue for extractingthe source from another sentence.
The probability orthe strength of an opinion expression may also playa useful role in encouraging or suppressing sourceextraction.3619 AcknowledgmentsWe thank the reviewers for their many helpful com-ments, and the Cornell NLP group for their adviceand suggestions for improvement.
This work wassupported by the Advanced Research and Develop-ment Activity (ARDA), by NSF Grants IIS-0208028and IIS-0208985, and by the Xerox Foundation.ReferencesC.
Baker, C. Fillmore & J. Lowe.
1998.
The BerkeleyFrameNet Project.
In Proceedings of the COLING-ACL.S.
Bethard, H. Yu, A. Thornton, V. Hativassiloglou & D. Juraf-sky.
2004.
Automatic extraction of opinion propositions andtheir holders.
In Proceedings of AAAI Spring Symposium onExploring Attitude and Affect in Text.D.
Bikel, S. Miller, R. Schwartz & R. Weischedel.
1997.Nymble: A High-Performance Learning Name-Finder.
InProceedings of the Fifth Conference on Applied Natural Lan-guage Processing.E.
Breck & C. Cardie.
2004.
Playing the Telephone Game:Determining the Hierarchical Structure of Perspective andSpeech Expressions.
In Proceedings of 20th InternationalConference on Computational Linguistics.C.
Cardie, J. Wiebe, T. Wilson & D. Litman.
2004.
Low-level annotations and summary representations of opinionsfor multiperspective QA.
In New Directions in Question An-swering.
AAAI Press/MIT Press.M.
Collins.
1999.
Head-driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.H.
Cunningham, D. Maynard, K. Bontcheva & V. Tablan.
2002.GATE: A Framework and Graphical Development Environ-ment for Robust NLP Tools and Applications.
In Proceed-ings of the 40th Anniversary Meeting of the Association forComputational Linguistics.S.
Das & M. Chen.
2001.
Yahoo for amazon: Extracting marketsentiment from stock message boards.
In Proceedings of the8th Asia Pacific Finance Association Annual Conference.K.
Dave, S. Lawrence & D. Pennock.
2003.
Mining the peanutgallery: Opinion extraction and semantic classification ofproduct reviews.
In International World Wide Web Confer-ence.J.
Lafferty, A. K. McCallum & F. Pereira.
2001.
ConditionalRandom Fields: Probabilistic Models for Segmenting andLabeling Sequence Data.
In Proceedings of 18th Interna-tional Conference on Machine Learning.B.
Levin.
1993.
English Verb Classes and Alternations: APreliminary Investigation.
University of Chicago Press.A.
K. McCallum.
2002.
MALLET: A Machine Learning forLanguage Toolkit.
http://mallet.cs.umass.edu.A.
K. McCallum.
2003.
Efficiently Inducing Features of Con-ditional Random Fields.
In Conference on Uncertainty inArtificial Intelligence.A.
K. McCallum & W. Li.
2003.
Early Results for NamedEntity Recognition with Conditional Random Fields, FeatureInduction and Web-Enhanced Lexicons.
In Conference onNatural Language Learning.S.
Morinaga, K. Yamanishi, K. Tateishi & T. Fukushima 2002.Mining Product Reputations on the Web.
In Proceedings ofthe 8th Internatinal Conference on Knowledge Discover andData Mining.M.
Palmer, D. Gildea & P. Kingsbury.
2005.
The PropositionBank: An Annotated Corpus of Semantic Roles.
In Compu-tational Linguistics 31.B.
Pang, L. Lee & S. Vaithyanathan.
2002.
Thumbs up?
sen-timent classification using machine learning techniques.
InProceedings of the 2002 Conference on Empirical Methodsin Natural Language Processing.B.
Pang & L. Lee.
2004.
A sentimental education: Sentimentanalysis using subjectivity summarization based on mini-mum cuts.
In Proceedings of the 42nd Annual Meeting ofthe Association for Computational Linguistics.L.
A. Ramshaw & M. P. Marcus.
1995.
Nymble: A High-Performance Learning Name-Finder.
In Proceedings of the3rd Workshop on Very Large Corpora.E.
Riloff.
1996a.
An Empirical Study of Automated DictionaryConstruction for Information Extraction in Three Domains.In Artificial Intelligence, Vol.
85.E.
Riloff.
1996b.
Automatically Generating Extraction Patternsfrom Untagged Text.
In Proceedings of the 13th NationalConference on Artificial Intelligence.E.
Riloff & J. Wiebe.
2003.
Learning extraction patterns forsubjective expressions.
In Proceesings of 2003 Conferenceon Empirical Methods in Natural Language Processing.E.
Riloff & W. Phillips.
2004.
An Introduction to the Sun-dance and AutoSlog Systems Technical Report UUCS-04-015, School of Computing, University of Utah.S.
Sarawagi & W. W. Cohen.
2004.
Semi-Markov Condi-tional Random Fields for Information Extraction 18th An-nual Conference on Neural Information Processing Systems.P.
Turney.
2002.
Thumbs up or thumbs down?
semantic orien-tation applied to unsupervised classification of reviews.
InProceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics.T.
Wilson, J. Wiebe & R. Hwa.
2004.
Just how mad are you?finding strong and weak opinion clauses.
In Proceedings ofthe 9th National Conference on Artificial Intelligence.T.
Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J. Wiebe,Y.
Choi, C. Cardie, E. Riloff & S. Patwardhan.
2005.
Opin-ionFinder: A system for subjectivity analysis.
Demonstra-tion Description in Conference on Empirical Methods inNatural Language Processing.J.
Yi, T. Nasukawa, R. Bunescu & W. Niblack.
2003.
SentimentAnalyzer: Extracting Sentiments about a Given Topic usingNatural Language Processing Techniques.
In Proceedings ofthe 3rd IEEE International Conference on Data Mining.H.
Yu & V. Hatzivassiloglou.
2003.
Towards answering opin-ion questions: Separating facts from opinions and identify-ing the polarity of opinion sentences.
In Proceedings of theConference on Empirical Methods in Natural Language Pro-cessing.J.
Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis, B. Fraser,D.
Litman, D. Pierce, E. Riloff & T. Wilson.
2002.
NRRCSummer Workshop on Multiple-Perspective Question An-swering: Final Report.J.
Wiebe & E. Riloff.
2005.
Creating subjective and objectivesentence classifiers from unannotated texts.
Sixth Interna-tional Conference on Intelligent Text Processing and Com-putational Linguistics.J.
Wiebe, T. Wilson & C. Cardie.
2005.
Annotating expressionsof opinions and emotions in language.
Language Resourcesand Evaluation, 1(2).362
