IIIIIIIIIIIIIndexing with WordNet  synsets can improve text retrievalJulio Gonzalo and Felisa Verdejo and Ir ina Chugur and Juan Cigarr~inUNEDCiudad Universitaria, s.n.28040 Madrid - Spain{julio, felisa, irina, juanci}@ieec, uned.
esAbst rac tThe classical, vector space model for text retrievalis shown to give better esults (up to 29% better inour experiments) if WordNet synsets are chosen asthe indexing space, instead of word forms.
This re-sult is obtained for a manually disambiguated testcollection (of queries and documents) derived fromthe SEMCOR semantic oncordance.
The sensitiv-ity of retrieval performance to (automatic) disam-biguation errors when indexing documents i alsomeasured.
Finally, it is observed that if queries arenot disambiguated, indexing by synsets performs (atbest) only as good as standard word indexing.1 In t roduct ionText retrieval deals with the problem of finding allthe relevant documents in a text collection for agiven user's query.
A large-scale semantic databasesuch as WordNet (Miller, 1990) seems to have a greatpotential for this task.
There are, at least, two ob-vious reasons:?
It offers the possibility to discriminate wordsenses in documents and queries.
This wouldprevent matching spring in its "metal device"sense with documents mentioning spring in thesense of springtime.
And then retrieval accu-racy could be improved.?
WordNet provides the chance of matching se-mantically related words.
For instance, spring,fountain, outflow, outpouring, in the appropri-ate senses, can be identified as occurrences ofthe same concept, 'natural flow of ground wa-ter'.
And beyond synonymy, WordNet can beused to measure semantic distance between oc-curring terms to get more sophisticated ways ofcomparing documents and queries.However, the general feeling within the informa-tion retrieval community is that dealing explicitlywith semantic information does not improve signif-icantly the performance of text retrieval systems.This impression is founded on the results of someexperiments measuring the role of Word Sense Dis-ambiguation (WSD) for text retrieval, on one hand,and some attempts to exploit the features of Word-Net and other lexical databases, on the other hand.In (Sanderson, 1994), word sense ambiguity isshown to produce only minor effects on retrieval ac-curacy, apparently confirming that query/documentmatching strategies already perform an implicit dis-ambiguation.
Sanderson also estimates that if ex-plicit WSD is performed with less than 90% accu-racy, the results are worse than non disambiguatingat all.
In his experimental setup, ambiguity is in-troduced artificially in the documents, ubstitutingrandomly chosen pairs of words (for instance, ba-nana and kalashnikov) with artificially ambiguousterms (banana/kalashnikov).
While his results arevery interesting, it remains unclear, in our opinion,whether they would be corroborated with real oc-currences of ambiguous words.
There is also otherminor weakness in Sanderson's experiments.
Whenhe ~disambiguates" a term such as spring/bank toget, for instance, bank, he has done only a partialdisambiguation, as bank can be used in more thanone sense in the text collection.Besides disambiguation, many attempts have beendone to exploit WordNet for text retrieval purposes.Mainly two aspects have been addressed: the enrich-ment of queries with semantically-related t rms, onone hand, and the comparison of queries and doc-uments via conceptual distance measures, on theother.Query expansion with WordNet has shown to bepotentially relevant o enhance recall, as it permitsmatching relevant documents hat could not containany of the query terms (Smeaton et al, 1995).
How-ever, it has produced few successful experiments.For instance, (Voorhees, 1994) manually expanded50 queries over a TREC-1 collection (Harman, 1993)using synonymy and other semantic relations fromWordNet 1.3.
Voorhees found that the expansionwas useful with short, incomplete queries, and ratheruseless for complete topic statements -where otherexpansion techniques worked better-.
For shortqueries, it remained the problem of selecting the ex-pansions automatically; doing it badly could degraderetrieval performance rather than enhancing it.
In38IIIIIIIIIIIIIIIIIII(Richardson and Smeaton, 1995), a combination ofrather sophisticated techniques based on WordNet,including automatic disambiguation a d measures ofsemantic relatedness between query/document con-cepts resulted in a drop of effectiveness.
Unfortu-nately, the effects of WSD errors could not be dis-cerned from the accuracy of the retrieval strategy.However, in (Smeaton and Quigley, 1996), retrievalon a small collection of image captions - that is, onvery short documents - is reasonably improved us-ing measures of conceptual distance between wordsbased on WordNet 1.4.
Previously, captions andqueries had been manually disambiguated againstWordNet.
The reason for such success is that withvery short documents (e.g.
boys playing in the sand)the chance of finding the original terms of the query(e.g.
of children running on a beach) are much lowerthan for average-size documents (that typically in-elude many phrasings for the same concepts).
Theseresults are in agreement with (Voorhees, 1994), butit remains the question of whether the conceptualdistance matching would scale up to longer docu-ments and queries.
In addition, the experiments in.
(Smeaton and Quigley, 1996) only consider nouns,while WordNet offers the chance to use all open-classwords (nouns, verbs, adjectives and adverbs).Our essential retrieval strategy in the experimentsreported here is to adapt a classical vector modelbased system, using WordNet synsets as indexingspace instead of word forms.
This approach com-bines two benefits for retrieval: one, that terms axefully disambiguated (this should improve precision);and two, that equivalent terms can be identified (thisshould improve recall).
Note that query expansiondoes not satisfy the first condition, as the terms usedto expand are words and, therefore, are in turn am-biguous.
On the other hand, plain word sense dis-ambiguation does not satisfy the second condition.as equivalent senses of two different words are notmatched.
Thus, indexing by synsets gets maximummatching and minimum spurious matching, seeminga good starting point to study text retrieval withWordNet.Given this approach, our goal is to test twomain issues which are not clearly answered -to ourknowledge- by the experiments mentioned above:?
Abstracting from the problem of sense disam-biguation, what potential does WordNet offerfor text retrieval?
In particular, we would liketo extend experiments with manually disam-biguated queries and documents o average-sizetexts.?
Once the potential of WordNet is known for amanually disambiguated collection, we want totest the sensitivity of retrieval performance todisambiguation errors introduced by automaticWSD.This paper reports on our first results answeringthese questions.
The next section describes the testcollection that we have produced.
The experimentsare described in Section 3, and the last Section dis-cusses the results obtained.2 The  tes t  co l lec t ionThe best-known publicly available corpus hand-tagged with WordNet senses is SEMCOR (Miller etal., 1993), a subset of the Brown Corpus of about100 documents that occupies about 11 Mb.
(in-cluding tags) The collection is rather heterogeneous,covering politics, sports, music, cinema, philosophy,excerpts from fiction novels, scientific texts...
Anew, bigger version has been made available recently(Landes et al, 1998), but we have not still adaptedit for our collection.We have adapted SEMCOR in order to build a testcollection -that we call IR-SEMCOR-  in four manualsteps:?
We have split the documents to get coherentchunks of text for retrieval.
We have obtained171 fragments that constitute our text collec-tion, with an averagv length of 1331 words perfragment.?
We have extended the original TOPIC tags ofthe Brown Corpus with a hierarchy of subtags,assigning a set of tags to each text in our col-lection.
This is not used in the experimentsreported here.?
We have written a summary for each of the frag-ments, with lengths varying between 4 and 50words and an average of 22 words per summary.Each summary is a human explanation of thetext contents, not a mere bag of related key-words.
These summaries erve as queries onthe text collection, and then there is exactlyone relevant document per query.?
Finally, we have hand-tagged each of thesummaries with WordNet 1.5 senses.
Whena word or term was not present in thedatabase, it was left unchanged.
In general,such terms correspond to groups (vg.
Ful-ton_County_Grand-Jury), persons (Cervantes)or locations (Fulton).We also generated a list Of "stop-senses" and a listof "stop-synsets', automatically translating a stan-dard list of stop words for English.Such a test collection offers the chance to measurethe adequacy of WordNet-based approaches toIR in-dependently from the disambiguator being used, butalso offers the chance to measure the role of auto-matic disambiguation by introducing different rates39!
!Experiment07o correct documentretrieved in first place62.0 Indexing by synsetsIndexing by word senses 53.2Indexing by words (basic SMART) 48.0Indexing by synsets with a 5% errors ratio 62.0Id.
with 10% errors ratio 60.8Id.
with 20% errors ratio 56.1Id.
with 30% errors ratio 54.4Indexing with all possible synsets (no disambiguation) 52.6Id.
with 60% errors ratio 49.1Synset indexing with non-disambiguated queries 48.5Word-Sense indexing with non-disambiguated queries 40.9Table 1: Percentage of correct documents retrieved in first placeof "disambignation errors" in the collection.
Theonly disadvantage is the small size of the collection,which does not allow fine-grained distinctions in theresults.
However, it has proved large enough to givemeaningful statistics for the experiments reportedhere.Although designed for our concrete text retrievaltesting purposes, the resulting database could alsobe useful for many other tasks.
For instance, it couldbe used to evaluate automatic summarization sys-tems (measuring the semantic relation between themanually written and hand-tagged summaries of IR-SEMCOR and the output of text summarization sys-tems) and other related tasks.3 The  exper imentsWe have performed a number of experiments u ing astandard vector-model based text retrieval system,SMART (Salton, 1971), and three different indexingspaces: the original terms in the documents (forstandard SMART runs), the word-senses correspond-ing to the document terms (in other words, a man-ually disambiguated version of the documents) andthe WordNet synsets corresponding to the documentterms (roughly equivalent to concepts occurring inthe documents).These are all the experiments considered here:1.
The original texts as documents and the sum-maries as queries.
This is a classic SMART run,with the peculiarity that there is only one rele-vant document per query.2.
Both documents (texts) and queries (sum-maries) are indexed in terms of word-senses.That means that we disambiguate manually allterms.
For instance "debate" might be substi-tuted with "debate~l:10:01:?'.
The three num-bers denote the part of speech, the WordNetlexicographer's file and the sense number withinthe file.
In this case, it is a noun belonging tothe noun.communication file.With this collection we can see if plain disam-biguation is helpful for retrieval, because wordsenses are distinguished but synonymous wordsenses are not identified.3.
In the previous collection, we substitute achword sense for a unique identifier of its associ-ated synset.
For instance, "debate~l:lO:01:.
"is substituted with "n04616654", which is anidentifier for"{argument, debate1}" (a discussion in whichreasons are advanced for and against someproposition or proposal; "the argument overforeign aid goes on and on')This collection represents conceptual indexing,as equivalent word senses are represented witha unique identifier.4.
We produced ifferent versions of the synsetindexed collection, introducing fixed percent-ages of erroneous synsets.
Thus we simulateda word-sense disambiguation process with 5%,10%, 20%, 30% and 60% error rates.
The er-rors were introduced randomly in the ambigu-ous words of each document.
With this set ofexperiments we can measure the sensitivity ofthe retrieval process to disambiguation errors.5.
To complement the previous experiment, wealso prepared collections indexed with all pos-sible meanings (in their word sense and synsetversions) for each term.
This represents a lowerbound for automatic disambiguation: we shouldnot disambiguate if performance is worse thanconsidering all possible senses for every wordform.6.
We produced also a non-disambiguated versionof the queries (again, both in its word sense and40Figure 1: Different indexing approachesc0u .=o.0.80.60.40.20 10.3 0.41.
Indexing by synsets o2.
Indexing by word senses -+---3.
Indexing by words (SMART) -o--12 ~~- .
.
.
.I I I I !0.5 0.6 0.7 0.8 0.9Recallsynset variants).
This set of queries was runagainst he manually disambiguated collection.In all cases, we compared arc and ann standardweighting schemes, and they produced very similarresults.
Thus we only report here on the results fornnn weighting scheme.4 Discussion of results4.1 Indexing approachIn Figure 1 we compare different indexing ap-proaches: indexing by synsets, indexing by words(basic SMART) and indexing by word senses (ex-periments 1, 2 and 3).
The leftmost point in eachcurve represents he percentage of documents thatwere successfully ranked as the most relevant for itssummary/query.
The next point represents he doc-uments retrieved as the first or the second most rel-evant o its summary/query, and so on.
Note that,as there is only one relevant document per query,the leftmost point is the most representative of eachcurve.
Therefore, we have included this results ep-arately in Table 1.The results are encouraging:?
Indexing by WordNet synsets produces aremarkable improvement on our test collection.A 62% of the documents are retrieved in firstplace by its summary, against 48% of the ba-sic SMART run.
This represents 14% moredocuments, a 29% improvement with respectto SMART.
This is an excellent result, al-though we should keep in mind that is obtainedwith manually disambiguated queries and doc-uments.
Nevertheless, it shows that WordNetcan greatly enhance text retrieval: the problemresides in achieving accurate automatic WordSense Disambiguation.?
Indexing by word senses improves perfor-mance when considering up to four documentsretrieved for each query/summary, although itis worse than indexing by synsets.
This con-firms our intuition that synset indexing has ad-vantages over plain word sense disambiguation,because it permits matching semantically simi-lar terms.Taking only the first document retrieved foreach summary, the disambiguated collectiongives a 53.2% success against a 48% of theplain SMART query, which represents a 11% im-provement.
For recall levels higher than 0.85,however, the disambiguated collection performsslightly worse.
This may seem surprising, asword sense disambiguation should only increaseour knowledge about queries and documents.But we should bear in mind that WordNet 1.5 isnot the perfect database for text retrieval, andindexing by word senses prevents ome match?ings that can be useful for retrieval.
For in-41IIIIIIIIIIIIIIIIIIIt-Ootl.0.80.60.40.20 I0.3 0.4Figure 2: sensitivity to disambiguation errors!
!
I !1.
Manual disambiguation x2.
5% error -~---3.
10% error -E3--4.
20% error .-~ .....5.
30% error --~---6.
All possible synsets per word (without disambigua~on) -~.-7.
60% error -<,---8.
SMART -~'---21 3~.... ::.-....-...:.. " - .
.
.
.
- .
.>.?
.--..::.......
..... ~.
.~?e.
~--!
I I I I0.5 0.6 0.7 0.8 0.9Recallstance, design is used as a noun repeatedly inone of the documents, while its summary usesdesign as a verb.
WordNet 1.5 does not includecross-part-of-speech semantic relations, so thisrelation cannot be used with word senses, whileterm indexing simply (and successfully!)
doesnot distinguish them.
Other problems of Word-Net for text retrieval include too much fine-grained sense-distinctions and lack of domaininformation; see (Gonzalo et al, In press) fora more detailed discussion on the adequacy ofWordNet structure for text retrieval.4.2 Sensit ivity to d isambiguat ion  errorsFigure 2 shows the sensitivity of the synset indexingsystem to degradation of disambiguation accuracy(corresponding to the experiments 4 and 5 describedabove).
Prom the plot, it can be seen that:?
Less than 10% disambiguating errors doesnot substantially affect performance.
This isroughly in agreement with (Sanderson, 1994).?
For error ratios over 10%, the performance de-grades quickly.
This is also in agreement with(Sanderson, 1994).?
However, indexing by synsets remains betterthan the basic SMART run up to 30% disam-biguation errors.
From 30% to 60%, the datadoes not show significant differences with stan-dard SMART word indexing.
This predictiondiffers from (Sanderson, 1994) result (namely,that it is better not to disambiguate below a90% accuracy).
The main difference is thatwe are using concepts rather than word senses.But, in addition, it must be noted that Sander-son's setup used artificially created ambiguouspseudo words (such as 'bank/spring ~which arenot guaranteed to behave as real ambiguouswords.
Moreover, what he understands as dis-ambiguating is selecting -in the example- bankor spring which remain to be ambiguous wordsthemselves.?
If we do not disambiguate, the performance isslightly worse than disambiguating with 30% er-rors, but remains better than term indexing, al-though the results are not definitive.
An inter-esting conclusion is that, if we can disambiguatereliably the queries, WordNet synset indexingcould improve performance ven without dis-ambiguating the documents.
This could be con-firmed on much larger collections, as it does notinvolve manual disambiguation.It is too soon to say if state-of-the-art WSD tech-niques can perform with less than 30% errors, be-cause each technique is evaluated in fairly differentsettings.
Some of the best results on a compara-ble setting (namely, disambiguating against Word-Net, evaluating on a subset of the Brown Corpus,and treating the 191 most frequently occurring and42IIIIIIIIIIIIIIIIIIIc -ogJa.0.80.60.40.200.3Figure 3: Performance with non-disambiguated queriesi !
!
!Indexing by words (SMART) oSynset indexing with non-disambiguated queries -+---Word-sense indexing with non-disambiguated queries -D--12- o .
.
.
o ~ " ' lb .""-gl..
?I I I l I I0.4 0.5 0.6 0.7 0.8 0.9Recallambiguous words of English) are reported reportedin (Ng, 1997).
They reach a 58.7% accuracy on aBrown Corpus subset and a 75.2% on a subset of theWall Street Journal Corpus.
A more careful evalua-tion of the role of WSD is needed to know if this isgood enough for our purposes.Anyway, we have only emulated a WSD algorithmthat just picks up one sense and discards the rest.
Amore reasonable approach ere could be giving dif-ferent probabilities for each sense of a word, and usethem to weight synsets in the vectorial representa-tion of documents and queries.4.3 Performance for non-disambiguatedqueriesIn Figure 3 we have plot the results of runs witha non-disambiguated v rsion of the queries, both forword sense indexing and synset indexing, against themanually disambiguated collection (experiment 6).The synset run performs approximately asthe basicSMART run.
It seems therefore useless to apply con-ceptual inde.,dng if no disambiguation f the query isfeasible.
This is not a major problem in an interac-tive system that may help the user to disambiguatehis query, but it must be taken into account if theprocess is not interactive and the query is too shortto do reliable disambiguation.5 Conc lus ionsWe have experimented with a retrieval approachbased on indexing in terms of WordNet synsets in-stead of word forms, trying to address two questions:1) what potential does WordNet offer for text re-trieval, abstracting from the problem of sense disam-biguation, and 2) what is the sensitivity of retrievalperformance to disambiguation errors.
The answerto the first question is that indexing by synsetscan be very helpful for text retrieval, our experi-ments give up to a 29% improvement over a standardSMART run indexing with words.
We believe thatthese results have to be further contrasted, but theystrongly suggest hat WordNet can be more usefulto Text Retrieval than it was previously thought.The second question needs further, more fine-grained, experiences to be clearly answered.
How-ever, for our test collection, we find that error ratesbelow 30% still produce better results than stan-dard word indexing, and that from 30% to 60% er-ror rates, it does not behave worse than the standardSMART run.
We also find that the queries have tobe disambiguated to take advantage ofthe approach;otherwise, the best possible results with synset in-dexing does not improve the performance of stan-dard word indexing.Our first goal now is to improve our retrievalsystem in many ways, studying how to enrich thequery with semantically related synsets, how to corn-43IIIIIIIIIIIIl!iIIIIpare documents and queries using semantic informa-tion beyond the cosine measure, and how to obtainweights for synsets according to their position in theWordNet hierarchy, among other issues.A second goal is to apply synset indexing in aCross-Language environment, using the Euro Word-Net multilingual database (Gonzalo et al, In press).Indexing by synsets offers a neat way of performinglanguage-independent r trieval, by mapping synsetsinto the EuroWordNet InterLingual Index that finksmonolingual wordnets for all the languages coveredby EuroWordNet.AcknowledgmentsThis research is being supported by the EuropeanCommunity, project LE #4003 and also partially bythe Spanish government, project TIC-96-1243-CO3-O1.We are indebted to Ren~e Pohlmann for giving us goodpointers at an early stage of this work, and to AnselmoPefias and David FernAndez for their help finishing upthe test collection.Re ferencesJ.
Gonzalo, M. F. Verdejo, C. Peters, and N. Cal-zolari.
In press.
Applying EuroWordnet to multi-lingual text retrieval.
Journal of Computers andthe Humanities, Special Issue on Euro WordNet.D.
K. Harman.
1993.
The first text retrieval con-ference (TREC-1).
Information Processing andManagement, 29(4):411--414.S.
Landes, C. Leacock, and R. Tengi.
1998.
Build-ing semantic oncordances.
In WordNet: An Elec-tronic Lexical Database.
MIT Press.G.
A. Miller, C. Leacock, R. Tengi, and R. T.Bunker.
1993.
A semantic oncordance.
In Pro-ceedings of the ARPA Workshop on Human Lan-guage Technology.
Morgan Kanffman.G.
Miller.
1990.
Special issue, Wordnet: An on-linelexical database.
International Journal of Lexi-cography, 3(4).H.
T. Ng.
1997.
Exemplar-based word sense dis-ambiguation: Some recent improvements.
In Pro-ceedings of the Second Conference on EmpiricalMethods in NLP.R.
Richardson and A.F.
Smeaton.
1995.
UsingWordnet in a knowledge-based approach to infor-mation retrieval.
In Proceedings of the BCS-IRSGColloquium, Crewe.G.
Salton, editor.
1971.
The SMART Retrieval Sys-tem: Experiments in Automatic Document Pro-cessing.
Prentice-Hall.M.
Sanderson.
1994.
Word sense disambiguationand information retrieval.
In Proceedings of 17thInternational Con\[erence onResearch and Devel-opment in Information Retrieval.A.F.
Smeaton and A. Quigley.
1996.
Experimentson using semantic distances between words in im-age caption retrieval.
In Proceedings of the 19 ta44International Conference on Research and Devel-opment in IR.A.
Smeaton, F. Kelledy, and R. O'Donnell.
1995.TREC-4 experiments at dublin city university:Thresolding posting lists, query expansion withWordnet and POS tagging of spanish.
In Proceed-ings of TREC-4.Ellen M. Voorhees.
1994.
Query expansion-usinglexieal-semantie r lations.
In Proceedings of the17th Annual International ACM-SIGIR Confer-ence on Research and Development in InformationRetrieval.
