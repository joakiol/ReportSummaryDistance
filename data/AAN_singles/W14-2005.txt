Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 37?45,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsLearning Verb Classes in an Incremental ModelLibby Barak, Afsaneh Fazly, and Suzanne StevensonDepartment of Computer ScienceUniversity of TorontoToronto, Canada{libbyb,afsaneh,suzanne}@cs.toronto.eduAbstractThe ability of children to generalize overthe linguistic input they receive is key toacquiring productive knowledge of verbs.Such generalizations help children extendtheir learned knowledge of constructionsto a novel verb, and use it appropriately insyntactic patterns previously unobservedfor that verb?a key factor in languageproductivity.
Computational models canhelp shed light on the gradual developmentof more abstract knowledge during verbacquisition.
We present an incrementalBayesian model that simultaneously andincrementally learns argument structureconstructions and verb classes given nat-uralistic language input.
We show how thedistributional properties in the input lan-guage influence the formation of general-izations over the constructions and classes.1 IntroductionUsage-based accounts of language learning notethat young children rely on verb-specific knowl-edge to produce their early utterances (e.g.,Tomasello, 2003).
However, evidence suggeststhat even young children can generalize theirverb knowledge to novel verbs and syntacticframes (e.g., Fisher, 2002), and that the abstractknowledge gradually strengthens over time (e.g.,Tomasello and Abbot-Smith, 2002).
One area ofverb usage where more sophisticated abstractionappears necessary for fully adult productivity inlanguage is the knowledge of verb alternations.A verb alternation is a pairing of constructionsshared by a number of verbs, in which the twoconstructions express related argument structures(Levin, 1993): e.g., the dative alternation involvesthe related forms of the prepositional dative (PD;X gave Y to Z) and the double-object dative (DO; Xgave Z Y).
Such alternations enable language usersto readily adapt new and low frequency verbs toappropriate constructions of the language by gen-eralizing the observed use of one such form to theother.1For example, Conwell and Demuth (2007) showthat 3-year-old children understand that a novelverb observed only in the DO dative (John gor-ped Heather the book) can also be used in the PDform (John gorped the book to Heather), thoughthe children can only generalize such knowledgeunder certain experimental conditions.
Wonnacottet al.
(2008) demonstrate the proficiency of adultsin making such generalizations within an artificiallanguage learning scenario, which enables the re-searchers to explore the distributional propertiesof the linguistic input that facilitate the acquisitionof such generalizations.
The results suggest thatthe overall frequency of the syntactic patterns aswell as the distribution of verbs across the patternsplay a facilitatory role in the formation of abstractverb knowledge (in the form of verb alternations)in adult language learners.In this work, we propose a computationalmodel that extends an existing Bayesian model ofverb argument structure acquisition (Alishahi andStevenson, 2008)[AS08] to support the learning ofverb classes over the acquired constructions.
Ourmodel is novel in its approach to verb class forma-tion, because it clusters tokens of a verb that reflectthe distribution of the verb over the learned con-structions each time the verb is used in an input.That is, the model forms verb classes by cluster-ing verb tokens that reflect the evolving usages ofthe verbs in various constructions.We use this new model to analyze the role ofthe classes and the distributional properties of theinput in learning abstract verb knowledge, given1The generalization of an alternation refers to a speakerusing one variant of an alternation for a verb (e.g., PD) havingonly observed the verb in the other variant (e.g., DO).37naturalistic input that contains many verbs andmany constructions.
The model can form higher-level generalizations such as learning verb alterna-tions, which is not possible with the AS08 model(cf.
the findings of Parisien and Stevenson, 2010).Moreover, because our model gradually forms itsrepresentations of constructions and classes overtime (in contrast to other Bayesian models, suchas Parisien and Stevenson, 2010; Perfors et al.,2010), it is possible to analyze the monotonically-growing representations and show their compati-bility with the developmental patterns seen in chil-dren (Conwell and Demuth, 2007).
We also repli-cate some of the observations of Wonnacott et al.
(2008) on the role of distributional properties ofthe language in influencing the degree of general-ization over an alternation.2 Related WorkTo explore the properties of learning mechanismsthat are capable of mimicking child and adult psy-cholinguistic observations, a number of cognitivemodeling studies have focused on learning ab-stract verb knowledge from individual verb usages(e.g., Alishahi and Stevenson, 2008; Perfors et al.,2010; Parisien and Stevenson, 2010).
Here we fo-cus on such computational models that enable thesort of higher-level generalization that people doacross verb alternations, unlike the AS08 model.The hierarchical Bayesian models of Perforset al.
(2010) and Parisien and Stevenson (2010)focus on learning this kind of higher-level general-ization.
The model of Perfors et al.
(2010) learnsverb alternations, i.e., pairs of syntactic patternsshared by certain groups of verbs.
By incorpo-rating this sort of abstract knowledge into theirmodel, Perfors et al.
are able to simulate the abil-ity of adults to generalize across verb alternations(as in Wonnacott et al., 2008).
That is, Perforset al.
predict the ability of a novel verb to occurin a syntactic structure after exposure to it in thealternative pattern of that alternation.
However,this model is trained on data that contains only alimited number of verbs and syntactic patterns un-like naturalistic Child-directed Speech (CDS) andmoreover incorporates built-in information aboutverb constructions.The hierarchical Dirichlet model of Parisienand Stevenson (2010) addresses these limitationsby working with natural child-directed speech(CDS) data.
Moreover, the model of Parisien andStevenson simultaneously learns constructions asin AS08 and verb classes based on verb alterna-tion behaviour, showing that the latter level of ab-straction is necessary to support effective learn-ing of verb alternations.
Still, the models of bothParisien and Stevenson and Perfors et al.
can onlybe utilized as a batch process and hence are lim-ited in the analysis of developmental trajectories.Although it is possible to simulate development bytraining such models on increasing portions of in-put, such an approach does not ensure that the rep-resentations given n + i inputs can be developedfrom the representation given n inputs.In this paper, we propose a significant extensionto the model of AS08, by adding an extra level ofabstraction that incrementally learns verb classesby drawing on the distribution of verbs over thelearned constructions.
The new model combinesthe advantages of having a monotonic clusteringmodel that enables the analysis of developing clus-ters, with the simultaneous learning of construc-tions and verb classes.3 The Computational ModelAs mentioned above, our model is an extensionof the model of AS08 in which we add a level oflearned abstract knowledge about verbs.
Specif-ically, our model uses a Bayesian clustering pro-cess to learn clusters of verb usages that occur insimilar argument structure constructions, as in theoriginal model of AS08.
To this, we add anotherlevel of abstraction that learns clusters of verbsthat exhibit similar distributional patterns of oc-currence across the learned constructions?that is,classes of verbs that occur in similar sets of con-structions, and in similar proportions.
To distin-guish between the clusters of the two levels of ab-straction in our new model, we refer to the clustersof verb usages as constructions, and to the group-ings of verbs given their distribution over thoseconstructions as verb classes.3.1 Overview of the ModelThe model learns from a sequence of frames,where each frame is a collection of features rep-resenting what the learner might extract from anutterance s/he has heard.
Similarly to previouscomputational studies (e.g., Parisien and Steven-son, 2010), here we focus on syntactic featuressince our goal is to understand the acquisition ofacceptable syntactic structures of verbs indepen-38Figure 1: A visual representation of the two levels of ab-straction in the model, with sample verb usages input (andextracted input frames), constructions, and classes.dently of their meaning, as in some relevant psy-cholinguistic (Wonnacott et al., 2008) and com-putational studies (Parisien and Stevenson, 2010).We focus particularly on properties such as syn-tactic slots and argument count.
(These features,as in Parisien and Stevenson (2010), provide amore flexible and generalizable representation of asyntactic structure than the syntactic pattern stringused by AS08.)
See the bottom rows of boxes inFigure 1 for sample input verb usages with theirextracted frames.The model incrementally clusters the extractedinput frames into constructions that reflect prob-abilistic associations of the features across simi-lar verb usages; see the middle level of Figure 1.Each learned cluster is a probabilistic (and possi-bly noisy) representation of an argument structureconstruction: e.g., a cluster containing frames cor-responding to usages such as I eat apples, She tookthe ball, and He got a book, etc., represents a Tran-sitive Action construction.2Such constructions al-low for some degree of generalization over the ob-served input; e.g., when seeing a novel verb in aTransitive utterance, the model predicts the simi-larity of this verb to other Action verbs appearingin that pattern (Alishahi and Stevenson, 2008).Grouping of verb usages into constructions maynot be sufficient for making higher-level general-izations across verb alternations.
Knowledge of al-ternations is only captured indirectly in construc-tions (because usages of the same verb can oc-cur in multiple clusters).
Following Parisien andStevenson (2010), we hypothesize that true gen-eralization behaviour requires explicit knowledgethat verbs have commonalities in their patterns ofoccurrence across constructions; this is the basis2Because the associations are probabilistic, a linguisticconstruction may be represented by more than one cluster.for verb classes (Levin, 1993; Merlo and Steven-son, 2000; Schulte im Walde and Brew, 2002).To capture this, our model learns groupings ofverbs that have similar distributions across thelearned constructions.
These groupings form verbclasses that provide a higher-level of abstractionover the input; see the top level in Figure 1.
Con-sider the dative alternation: the classes capture thefact that some verbs may occur only in preposi-tional dative (PD) forms, such as sing, while oth-ers occur only in double object (DO) forms (call),while still others alternate ?
i.e., they occur in both(bring).Our model simultaneously learns both of thesetypes of knowledge: constructions are clusters ofverb usages, and classes are clusters of verb dis-tributions over those constructions.
Importantly, itdoes so incrementally, which allows us to exam-ine the developmental trajectory of acquiring al-ternations such as the dative as the learned clus-ters grow over time.
Moreover, both types of clus-tering are monotonic, i.e., we do not re-structurethe groupings that our model learns.
However, themodel in both levels is clustering verb tokens ?
i.e.,the features corresponding to the verb at that timein the input, its usage or its current distribution ?so that the same verb type may be added to variousclusters at different stages in the training.3.2 Learning Constructions of Verb UsagesThe model of AS08 groups input frames into clus-ters on the basis of the overall similarity in thevalues of their features.
Importantly, the modellearns these clusters incrementally in response tothe input; the number and type of clusters is notpredetermined.
The model considers the creationof a new cluster for a given frame if the frame isnot sufficiently similar to any of the existing clus-ters.
Formally, the model finds the best cluster fora given input frame F as in:BestCluster(F ) = argmaxk?ClustersP (k|F ) (1)where k ranges over all existing clusters and a newone.
Using Bayes rule:P (k|F ) =P (k)P (F |k)P (F )?
P (k)P (F |k) (2)The prior probability of a cluster P (k) is estimatedas the proportion of frames that are in k out ofall observed input frames, thus assigning a higher39prior more frequent constructions.
The likelihoodP (F |k) is estimated based on the match of fea-ture values in F and in the frames of k (assumingindependence of the features):P (F |k) =?i?FeaturesPi(j|k) (3)where j is the value of the ithfeature of F , andPi(j|k) is calculated using a smoothed version of:Pi(j|k) =counti(j, k)nk(4)where counti(j, k) is the number of times feature ihas the value j in cluster k, and nkis the number offrames in k. We compare the slot features as sets tocapture similarities in overlapping syntactic slotsrather than enforcing an exact match.
The modeluses the Jaccard similarity score to measure thedegree of overlap between two feature sets, insteadof the direct count of occurrence in Eqn.
(4):sim score(S1, S2) =|S1?
S2||S1?
S2|(5)where S1and S2in our experiments here are thesets of syntactic slot features.3.3 Learning Verb ClassesOur new model extends the construction-formation model of AS08 by grouping verbs intoclasses on the basis of their distribution acrossthe learned constructions.
That is, verbs that havestatistically-similar patterns of occurrence acrossthe learned constructions will be considered asforming a verb class.
For example, in Figure 1 wesee that bring and read may be put into the sameclass because they both occur in a similar relativefrequency across the DO and PD constructions(the leftmost and rightmost constructions in thefigure).We use the same incremental Bayesian cluster-ing algorithm for learning the verb classes as forlearning constructions.
At the class level, the fea-ture used for determining similarity of items inclustering is the distribution of each verb acrossthe learned constructions.
As for constructions,the model learns the verb classes incrementally;the number and type is not predetermined.
More-over, just as constructions are gradually formedfrom successively processing a particular verb us-age at each input step, the model forms verbclasses from a sequence of snapshots of the inputverb?s distribution over the constructions at eachinput step.
This means that our model is formingclasses of verb tokens rather than types; if a verb?sbehaviour changes over the duration of the input,subsequent tokens (the distributions over construc-tions at later points in time) may be clustered intoa different class (or classes) than earlier tokens,even though prior decisions cannot be undone.Formally, after clustering the input frame attime t into a construction, as explained above, themodel extracts the current distribution dvtof itshead verb v over the learned constructions; this isestimated as a smoothed version of v?s relative fre-quency in each construction:P (k|v) =count(v, k)nv(6)where count(v, k) is the number of times that in-puts with verb v have been clustered into construc-tion k, and nvis the number of times v has oc-curred in the input thus far.To cluster this snapshot of the verb?s distribu-tion, dvt, it is compared to the distributions en-coded by the model?s classes.
The distribution dcof an existing class c is the weighted average ofthe distributions of its member verb tokens:dc=1|c|?v?ccount(v, c)?
dv(7)where |c| is the size of class c, count(v, c) is thenumber of occurrences of v that have been as-signed to c, and dvis the distribution of the verb vgiven by the tokens of v (the ?snapshots?
of distri-butions of v assigned to class c).
That is, dvin c isan average of the distributions of all dvtfor verb vthat have been clustered into c.The model finds the best class for a given verbdistribution dvtbased on its similarity to the dis-tributions of all existing classes and a new one:BestClass(dvt) = argmaxc?Classes(1?DJS(dc?dvt))(8)where c ranges over all existing classes as well asa new class that is represented as a uniform dis-tribution over the existing constructions.
Jensen?Shannon divergence, DJS, is a popular method formeasuring the distance between two distributions:It is based on the KL?divergence, but it is symmet-ric and has a finite value between 0 and 1:DJS(p?q) =12DKL(p?12(p+ q)) +12DKL(q?12(p+ q)) (9)40non-ALT ALTDO-only PD-only DO PDNumber of verbs 12 5 6Relative frequency 14% 2% 2% 1%Table 1: Number of non-alternating (non-ALT) and alter-nating (ALT) verbs in our lexicon, as well as the relative fre-quency of each construction in our generated input corpora.4 Experimental Setup4.1 Generation of the Input CorporaWe follow the input generation method of AS08to create naturalistic corpora that are based on thedistributional properties of verbs over various con-structions, as observed in child-directed speech(CDS).
Our input-generation lexicon contains 71verbs drawn from AS08 (11 action verbs) andBarak et al.
(2013) (31 verbs of varying syntac-tic patterns), plus an additional 40 of the most fre-quent verbs in CDS, in order to have a range ofverbs that occur with the PD and DO construc-tions.
Table 4.1 shows the number of verbs thatappear in the DO or PD construction only (non-alternating), as well as those that alternate acrossthe two.
(The table also gives the relative fre-quency of each dative construction in our gener-ated input corpora.)
Each verb lexical entry in-cludes its overall frequency, and its relative fre-quency with each of a number of observed syn-tactic constructions.
The frequencies are extractedfrom a manual annotation of a sample of 100child-directed utterances per verb from a collec-tion of eight corpora from CHILDES (MacWhin-ney, 2000).3An input corpus is generated by it-eratively selecting a random verb and a syntacticconstruction based on their frequencies accordingto the lexicon, so that all input corpora used in oursimulations have the distributional properties ob-served in CDS, but show some variation in precisemake-up and ordering of verb usages.
The gener-ated input consists of frames (a set of features) thatcorrespond to verb usages in CDS.4.2 SimulationsBecause the generation of the input data is prob-abilistic, we conduct 100 simulations for eachexperiment (each using a different input cor-pus) to avoid any dependency on specific id-iosyncratic properties of a single generated cor-pus.
For each simulation, we train our model3Brown (1973); Suppes (1974); Kuczaj (1977); Bloomet al.
(1974); Sachs (1983); Lieven et al.
(2009).on an automatically-generated corpus of 15, 000frames, from which the model learns construc-tions and verb classes.
At specified points inthe input, we present the model with usages ofa novel verb in a DO and/or PD frame, andthen test the model?s generalization ability bypredicting DO and PD frames given that verb.Since we are interested in the relative likeli-hoods of the two frames, we report the differ-ence between the log-likelihood of the DO frameand the log-likelihood of the PD frame, i.e.,log-likelihood(DO)?
log-likelihood(PD).Specifically, we form a partial frame Ftest(con-taining all usage features except for the verb) thatreflects either the PD or the DO syntax, and assessthe probability P (Ftest|v) for each of these, as in:P (Ftest|v) =?k?ConstructionsP (Ftest|k)P (k|v)(10)where P (Ftest|k) is calculated as in Eqn.
(3).We can calculate P (k|v) in two different ways:using only the knowledge in the constructions ofthe model, and using the knowledge that takes intoaccount the verb classes over the constructions.For model predictions based on the constructionlevel only, we calculate P (k|v) as in Eqn.
(6),which is the smoothed relative frequency of theverb v over construction k.Predictions using knowledge of the verb classeswill instead determine P (k|v) drawing on the fitof verb v to the various classes (specifically, thesimilarity of v?s distribution over constructions tothe distribution encoded in each class), and thelikelihood of each construction k for each class c(specifically, the likelihood of k given the distribu-tion over constructions encoded in c), as in:P (k|v) ?
?c?ClassesP (k|c)P (c|v) (11)where P (k|c) is the probability of constructionk given class c?s distribution over constructions(dc); and P (c|v) is the probability of c given verbv?s distribution dvover the constructions (usingJensen-Shannon divergence as in Eqn.
(9)).Due to the different number of clusters in eachof the construction and class layers of the model,the likelihoods computed for each will differ inthe range of values.
For this reason, specific val-ues cannot be directly compared across the layersof the model, rather we must analyze the generaltrends of the construction-only and class-based re-sults.415 EvaluationIn this section we examine whether and how ourmodel generalizes across the two variants of thedative alternation, the double-object dative (DO)and the prepositional dative (PD).
To do so, wemeasure the tendency of the model to produce anovel verb observed in one dative frame in thatsame frame, or in the other dative frame (unob-served for that verb).
Our goal is to understand theimpact of the learned constructions and classes onthis generalization behaviour.
Following Parisienand Stevenson (2010), we examine three inputconditions in which the novel verb occurs: (i)twice with the DO syntax (non-alternating); (ii)twice with the PD syntax (non-alternating); or (iii)once each with DO and PD syntax (alternating).4We then ask the model to predict the likelihood ofproducing each dative frame with that verb.
Ourfocus here is on comparing the generalization abil-ities of the two levels of abstract knowledge in ourmodel: the constructions versus the verb classes.As a reminder, we use the dative alternation asone example for considering this kind of higher-level generalization behaviour observed in adultsand to a lesser extent in children.
Moreover, weperform the analysis in the context of naturalisticinput that contains many verbs (those that appearin the dative and those that do not), and a variety ofconstructions , to provide a realistic setting for thetask.
Our settings differ from the psycholinguis-tic studies in the variability of constructions com-pared with the artificial language used by Won-nacott et al., and in focusing only on the syntac-tic properties unlike Conwell and Demuth.
How-ever, we follow the settings of these studies in an-alyzing the syntactic properties of a generated ut-terance given minimal exposure to a novel verb.Therefore, we aim to replicate their general ob-servations by showing that (i) children are limitedin their ability to generalize across verb alterna-tions compared with adults, and (ii) the frequencyof a construction has a positive correlation with thegeneralization rate of the construction.5.1 Generalization of Learned KnowledgeWe examine the generalization patterns of ourmodel when presented with a novel verb in DO/PDforms after being trained on 15, 000 inputs, whichwe compare to the performance of adults in such4For the alternating condition, half the simulations haveDO first, and half have PD first.Figure 2: The difference between the log-likelihood valuesof the DO and PD frames, given each of the three input con-ditions: DO only, PD only, and Alternating.
Values abovezero denote a higher likelihood for the DO frame, and valuesbelow zero denote a higher likelihood for the PD frame.language tasks.
We first consider the case wherethe model predictions are based solely on theknowledge of constructions.
Here we expect thepredictions to correspond to the syntactic proper-ties of the two inputs observed for the novel verb,with limited generalization.
That is, we expect anon-alternating verb to be much more likely in theobserved dative frame, and an alternating verb tobe equally likely in both frames.
The left handside of Figure 2 presents the differences in log-likelihoods of the predicted DO and PD frames forthe novel verb using the construction-based prob-abilities.
The results confirm our expectation thatthe knowledge of constructions can support onlylimited generalization across the variants of an al-ternation.
For the non-alternating conditions, theobserved frame is highly favoured, and for theAlternating test scenario, the DO and PD frameshave nearly equal likelihoods.We next turn to using the knowledge of verbclasses, which we expect to enable generaliza-tions that correspond to verb alternation behaviour?
that is, we expect the model predictions hereto reflect the knowledge that verbs that occur inone form of the alternation also often occur inthe other form of the alternation.
This is possiblebecause the classes in the model encode the dis-tributional patterns of verbs across constructions.In the absence of other factors, we would expectthe Alternating condition to again show near equallikelihoods for the two frames, and the two non-alternating conditions to show a slight preferencefor the observed frame (rather than the strong pref-erence seen in the construction-based predictions),because the unobserved frame is also likely due tothe knowledge here of the alternation.The right hand side of Figure 2 presents the42difference in the log-likelihoods of the DO andPD frames when using the knowledge encodedin the verb classes.
The results are not directlyin line with the simple prediction above: Thenon-alternating (DO-only and PD-only) condi-tions show a weak preference (as expected) for oneframe over another, but both favour the DO frame,as does the Alternating condition.
That is, the PD-only and Alternating conditions show a preferencefor the DO frame that does not follow simply fromthe knowledge of alternations.The DO preference in the PD-only and Alter-nating conditions arises due to distributional fac-tors in the input, related to the frequencies of theconstructions reported in Table 1.
First, the DOframe is overall much more likely than the PDframe, causing generalization in the PD-only andAlternating conditions to lean more to that frame.Second, fully 1/3 of the uses of the PD frame inthe corpus are with verbs that alternate (i.e., 1%of the corpus are PD frames of alternating PD-DO verbs, out of a total of 3% of the corpus be-ing PD frames), while only 1/8 of the uses of theDO frame are with alternating rather than non-alternating verbs.
Recall that our classes encodethe distribution (roughly relative frequency) of theverbs in the class occurring across the differentconstructions.
This means that in our class-basedpredictions, greater weight will be given to con-structions with DO when observing a PD framethan to constructions with PD when observing aDO frame.
These results underline the importanceof using naturalistic input and considering the im-pact of various distributional factors on general-ization of verb knowledge.In contrast to the construction-based results, ourclass-based results conform with the experimentalfindings of Wonnacott et al.
(2008), who show thatadult (artificial) language learners robustly gener-alize a newly-learned verb observed in a singlesyntactic form by producing it in the alternatingsyntactic form under certain language conditions.Moreover, we show similar distributional effectsto theirs ?
the overall frequency of the syntacticpatterns, as well as the distribution of verbs acrossthose patterns ?
in the level of preference for oneform over another, within the context of our nat-uralistic data with multiple verbs, constructions,and alternations.
These results show that the verbclasses in the model are able to capture useful ab-stract knowledge that is key to understanding thehuman ability to make high-level generalizationsacross verb alternations.5.2 Development of GeneralizationsNext, we present the results of our model evalu-ated throughout the course of training in order tounderstand the developmental pattern of general-ization.
We perform the same construction-basedor class-based prediction tasks (the likelihoods ofa DO and PD frame), following the same inputconditions (a novel verb with two DO frames, twoPD frames, or one of each) at given points duringthe 15, 000 inputs.
As above, we present the dif-ference in the log-likelihood values of the DO andthe PD frames in order to focus on the relative like-lihoods of the two frames within each condition ofconstruction-based or class-based predictions.Figure 3(a) presents the results for the DO-only test scenario.
As in Section 5.1, forboth construction-based and class-based predic-tions there is a higher likelihood for the DO framethroughout the course of training.
In contrast, theincremental results for the PD-only test scenario,in Figure 3(b), display a developing level of gen-eralization throughout the training stage for theclass-based predictions.
While the construction-based predictions reflect a much higher likelihoodfor the PD frame, the results from the verb classesare in favor of the PD frame only initially; aftertraining on 5000 input frames, the likelihood ofthe DO frame becomes higher for this test sce-nario.
These results indicate that using construc-tion knowledge alone does not enable generaliza-tion from the PD frame to the DO frame; in con-trast, the verb class knowledge enables the grad-ual acquisition of generalization ability over thecourse of training.Finally, Figure 3(c) presents the results for theAlternating test scenario for the two types of pre-dictions.
As in Section 5.1, both construction-based and class-based predictions have a smallpreference for the DO frame.
In the construction-based predictions, this preference lessens overtime to where the likelihoods for DO and PD arealmost equal, while the class-based predictionsstay relatively constant in their preference for theDO frame.
In some ways the construction-basedpredictions are more expected in response to anapparently alternating verb; however, the class-based predictions show a higher degree of general-ization, responding to the higher frequency of the43(a) DO only (b) PD only (c) AlternatingFigure 3: Difference of log-likelihood values of the DO and PD frames over the course of training for the constructions andthe verb classes for each of the 3 test scenarios.
Values above zero denote a higher likelihood for the DO frame, and valuesbelow zero denote a higher likelihood for the PD frame.DO frame and the higher association of PD frameswith DO alternates.
These results again empha-size the importance of further exploring the roleof distributional factors on generalization of verbknowledge in children.The developmental results presented here are inline with the suggestions of Tomasello (2003) thatthe productions of younger children follow ob-served patterns in the input, and only later reflectrobust generalizations of their knowledge acrossverbs.
Conwell and Demuth (2007) for example,found evidence of generalization across verb al-ternations in 3-year-old children, but their produc-tion of unobserved forms for a novel verb wasvery sensitive to the precise context of the ex-periment and the distributional patterns across thenovel verbs.
In accord with these observations, thedevelopmental trajectories in our model show thatour class-based predictions increase in their degreeof generalization over time, and are sensitive tovarious distributional factors in the input, such asthe overall expectation for a frame and the expec-tation that a verb will alternate.6 DiscussionWe present a novel computational model thatprobabilistically learns two levels of abstractionsover individual verb usages: constructions thatare clusters of similar verb usages, and classes ofverbs with similar distributional behaviour acrossthe constructions.
Specifically, we extend themodel of AS08 by incrementally learning token-based verb classes that generalize over the con-struction knowledge level.
In contrast to the mod-els of Parisien and Stevenson and Perfors et al.,our model is incremental, and hence enables theanalysis of the monotonically developing classesto show the relation to the development of gener-alization ability in human learners.We analyze how generalization is supported byeach level of learning in our model: constructionsand verb classes.
Our results confirm (cf.
Parisienand Stevenson, 2010) that a higher-level knowl-edge of the verb classes is required to replicate theobserved patterns of generalization, such as pro-ducing a novel verb gorp in the in the prepositionaldative pattern after hearing it in the double objectdative pattern.
In addition, our analysis of the in-crementally developing verb classes shows that thegeneralization knowledge gradually emerges overtime, similar to what is observed in children.The flexibility of input representation of ourmodel enables us to further explore the propertiesof the input in learning abstract knowledge, fol-lowing psycholinguistic studies.
Our results repli-cate the findings of Wonnacott et al.
on the roleof the distributional properties over the alternat-ing syntactic forms, but in naturalistic settings ofmany constructions.
In future, we plan to extendthis analysis by manipulating the distributions ofour input data to replicate the exact settings of theartificial language used by Wonnacott et al.. More-over, in this study, we followed the settings of pre-vious computational and psycholinguistic studiesthat focused on the syntactic properties of the in-put (Perfors et al., 2010; Parisien and Stevenson,2010; Wonnacott et al., 2008; Conwell and De-muth, 2007).
However, we can further our anal-ysis by incorporating semantic features in the in-put to study syntactic bootstrapping effects (Scottand Fisher, 2009) as well as the role of seman-tic properties in constraining the generalizationsacross the alternating forms.AcknowledgmentsThe authors would like to thank Afra Alishahi forproviding us with the code and data for her model,and to Chris Parisien for sharing his data with us.44ReferencesAfra Alishahi and Suzanne Stevenson.
2008.
Acomputational model of early argument struc-ture acquisition.
Cognitive Science, 32(5):789?834.Libby Barak, Afsaneh Fazly, and Suzanne Steven-son.
2013.
Acquisition of desires before beliefs:A computational investigation.
In Proceedingsof CoNLL-2013.Lois Bloom, Lois Hood, and Patsy Lightbown.1974.
Imitation in language development:If, when, and why.
Cognitive Psychology,6(3):380?420.Roger Brown.
1973.
A first language: The earlystages.
Harvard Univ.
Press.Erin Conwell and Katherine Demuth.
2007.
Earlysyntactic productivity: Evidence from dativeshift.
Cognition, 103(2):163?179.Cynthia Fisher.
2002.
The role of abstract syntac-tic knowledge in language acquisition: A replyto Tomasello.
Cognition, 82(3):259?278.A.
Kuczaj, Stan.
1977.
The acquisition of regularand irregular past tense forms.
Journal of VerbalLearning and Verbal Behavior, 16(5):589?600.B.
Levin.
1993.
English verb classes and alterna-tions: A preliminary investigation, volume 348.University of Chicago press Chicago, IL.Elena Lieven, Doroth?e Salomo, and MichaelTomasello.
2009.
Two-year-old children?s pro-duction of multiword utterances: A usage-basedanalysis.
Cognitive Linguistics, 20(3):481?507.B.
MacWhinney.
2000.
The CHILDES project:Tools for analyzing talk, volume 2.
PsychologyPress.P.
Merlo and S. Stevenson.
2000.
Automatic verbclassification based on statistical distributionsof argument structure.
Computational Linguis-tics, 27(3):373?408.Christopher Parisien and Suzanne Stevenson.2010.
Learning verb alternations in a usage-based bayesian model.
In Proceedings of the32nd annual meeting of the Cognitive ScienceSociety.Amy Perfors, Joshua B. Tenenbaum, and Eliz-abeth Wonnacott.
2010.
Variability, negativeevidence, and the acquisition of verb argu-ment constructions.
Journal of Child Language,37(03):607?642.Jacqueline Sachs.
1983.
Talking about the Thereand Then: The emergence of displaced refer-ence in parent?child discourse.
Children?s lan-guage, 4.Sabine Schulte im Walde and Chris Brew.
2002.Inducing German semantic verb classes frompurely syntactic subcategorisation information.In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics,pages 223?230, Philadelphia, PA.Rose M Scott and Cynthia Fisher.
2009.
Two-year-olds use distributional cues to interprettransitivity-alternating verbs.
Language andcognitive processes, 24(6):777?803.Patrick Suppes.
1974.
The semantics of children?slanguage.
American psychologist, 29(2):103.Michael Tomasello.
2003.
Constructing a lan-guage: A usage-based theory of language ac-quisition.Michael Tomasello and Kirsten Abbot-Smith.2002.
A tale of two theories: Response toFisher.
Cognition, 83(2):207?214.Elizabeth Wonnacott, Elissa L Newport, andMichael K Tanenhaus.
2008.
Acquiring andprocessing verb argument structure: Distribu-tional learning in a miniature language.
Cog-nitive psychology, 56(3):165?209.45
