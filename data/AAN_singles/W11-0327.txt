Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 229?237,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsAdapting Text instead of the Model: An Open Domain ApproachGourab Kundu, Dan RothUniversity of Illinois at Urbana ChampaignUrbana, IL 61801{kundu2,danr}@illinois.eduAbstractNatural language systems trained on labeleddata from one domain do not perform wellon other domains.
Most adaptation algorithmsproposed in the literature train a new model forthe new domain using unlabeled data.
How-ever, it is time consuming to retrain big mod-els or pipeline systems.
Moreover, the domainof a new target sentence may not be known,and one may not have significant amount ofunlabeled data for every new domain.To pursue the goal of an Open Domain NLP(train once, test anywhere), we propose ADUT(ADaptation Using label-preserving Transfor-mation), an approach that avoids the need forretraining and does not require knowledge ofthe new domain, or any data from it.
Our ap-proach applies simple label-preserving trans-formations to the target text so that the trans-formed text is more similar to the training do-main; it then applies the existing model onthe transformed sentences and combines thepredictions to produce the desired predictionon the target text.
We instantiate ADUT forthe case of Semantic Role Labeling (SRL)and show that it compares favorably with ap-proaches that retrain their model on the targetdomain.
Specifically, this ?on the fly?
adapta-tion approach yields 13% error reduction fora single parse system when adapting from thenews wire text to fiction.1 IntroductionIn several NLP tasks, systems trained on annotateddata from one domain perform well when testedon the same domain but adapt poorly to other do-mains.
For example, all systems of CoNLL 2005shared task (Carreras and M?rquez, 2005) on Se-mantic Role Labeling showed a performance degra-dation of almost 10% or more when tested on a dif-ferent domain.Most works in domain adaptation have focusedon learning a common representation across train-ing and test domains (Blitzer et al, 2006; Daum?III,2007; Huang and Yates, 2009).
Using this represen-tation, they retrain the model for every new domain.But these are not Open Domain Systems since themodel needs to be retrained for every new domain.This is very difficult for pipeline systems like SRLwhere syntactic parser, shallow parser, POS taggerand then SRL need to be retrained.
Moreover, thesemethods need to have a lot of unlabeled data that istaken from the same domain, in order to learn mean-ingful feature correspondences across training andtest domain.
These approaches cannot work whenthey do not have a lot of unlabeled data from the testdomain or when the test domain in itself is very di-verse, e.g., the web.The contribution of this paper is a new frame-work for adaptation.
We propose ADUT (ADap-tation Using label-preserving Transformation) as aframework in which a previously learned model canbe used on an out-of-domain example without re-training and without looking at any labeled or unla-beled data for the domain of the new example.
Theframework transforms the test sentence to generatesentences that have, in principle, identical labelingbut that are more like instances from the training do-main.
Consequently, it is expected that the exist-229ing model will make better predictions on them.
Allthese predictions are then combined to choose themost probable and consistent prediction for the testsentence.ADUT is a general technique which can be ap-plied to any natural language task.
In this paper, wedemonstrate its usefulness on the task of semanticrole labeling (Carreras and M?rquez, 2005).
Start-ing with a system that was trained on the news textand does not perform well on fiction, we show thatADUT provides significant improvement on fiction,and is competitive with the performance of algo-rithms that were re-trained on the test domain.The paper is organized as follows.
Section 2 dis-cusses two motivating examples.
Section 3 gives aformal definition of our adaptation framework.
Sec-tion 4 describes the transformation operators that weapplied for this task.
Section 5 presents our joint in-ference approach.
Section 6 describes our semanticrole labeling system and our experimental results arein Section 7.
Section 8 describes the related worksfor domain adaptation.
Finally in Section 9 we con-clude the paper with a discussion.2 Motivating ExamplesOne of the key reasons for performance degradationof an NLP tool is unseen features such as words inthe new domain that were not seen in the trainingdomain.
But if an unknown word is replaced by aknown word without changing the labeling of thesentence, tools perform better.
For example, in thetask of syntactic parsing, the unknown word checkupcauses the Charniak parser to make a wrong co-ordination decision on the sentenceHe was discharged from the hospital af-ter a two-day checkup and he and his par-ents had what Mr. Mckinley described asa ?celebration lunch?
at the cafeteria onthe campus.If we replace the word checkup with its hyper-nym examination which appears in training data, theparse gets corrected.
Figure 1 shows both originaland corrected parse trees.For the task of semantic role labeling, systems donot perform well on the predicates that are infre-quent in training domain.
But if an infrequent predi-cate is replaced with a frequent predicate from train-ing domain such that both predicates have similarsemantic argument structure, the system performsbetter.
Consider the following sentenceScotty gazed out at ugly gray slums.The semantic role for the phrase at ugly gray slumswith respect to predicate gaze is A1.
But the pred-icate gaze appears only once in training data andour model predicts at ugly gray slums as AM-LOCinstead of A1.
But if gaze is replaced with lookwhich occurs 328 times in training data and has sim-ilar argument structure (in the same VerbNet class asgaze), the system makes the correct prediction.3 Problem FormulationLet the in-domain distribution be Di and out-of-domain distribution be Do.
We have a model ftrained over a set of labeled examples drawn fromDi.
If Di and Do are very dissimilar, f will not per-form well on examples drawn from Do.
The prob-lem is to get good performance from f on Do with-out retraining f .We define a Transformation g to be a function thatmaps an example e into a set of examples E. So g :X ?
2X where X is the entire space of examples.In this paper, we only consider the Label-preservingTransformations which satisfy the property that alltransformed examples in E have the same label asinput example e, i.e., ?x x ?
Sk ?
g(x) ?
Skwhere Sk is the set of examples with label k .
LetG be a set of label-preserving transformation func-tions.
G = {g1, g2, .
.
., gp}.At evaluation time, for test example d, we willapply G to get a set of examples T1.
Let T2 = {d?
?T1 : Di(d?)
> Di(d)}.
So all examples in T2 havesame label as d but have a higher probability thand to be drawn from the in-domain distribution.
Sof should perform better on examples in T2 than ond.
For each d?
?
T2, f will produce scores for theoutput labels.
The scores will be combined subjectto constraints to produce the final output.4 Transformation FunctionsAfter applying a transformation function to get anew sentence from an input sentence, we remem-ber the mapping of segments across the original230a.
S1SNPHeVPwas VPdischarged PPfrom the hospitalPPafter NPNPa two-daycheckupSBARNPand he and his parentsVPhad .
.
.
campus.b.
S1SSNPHeVPwas VPdischarged PPfrom the hospitalPPafter NPa two-dayexaminationand SNPhe and his parentsVPhad .
.
.
campus.Figure 1: a.
Original Parse tree b.
Corrected Parse tree after replacement of unknown word checkup by examinationand transformed sentence.
Thus, after annotatingthe transformed sentence with SRL, we can transferthe roles to the original sentence through this map-ping.
Transformation functions can be divided intotwo categories.
The first category is Transforma-tions From List which uses external resources likeWordNet, VerbNet and Word Clusters.
The secondis Learned Transformations that uses transformationrules that have been learned from training data.4.1 Transformation From ListI.
Replacement of Predicate:As noted in (Huang and Yates, 2010), 6.1% of thepredicates in the Brown test set do not appear in WSJtraining set and 11.8% appear at most twice.
Sincethe semantic roles of a sentence depend on the pred-icate, these infrequent predicates hurt SRL perfor-mance on new domains.
Note that since all predi-cates in PropBank are verbs, we will use the wordspredicate and verb interchangeably.We count the frequency of each predicate and itsaccuracy in terms of F1 score over the training data.If the frequency or the F1 score of the predicate inthe test sentence is below a threshold, we perturbthat predicate.
We take all the verbs in the same classof VerbNet1 as the original verb (in case the verb ispresent in multiple classes, we take all the classes).In case the verb is not present in VerbNet, we takeits synonyms from WordNet.
If there is no synonymin WordNet, we take the hypernyms.From this collection of new verbs, we select verbsthat have a high accuracy and a high frequency in1http://verbs.colorado.edu/ mpalmer/projects/verbnet.htmltraining.
We replace the original verb with each ofthese new verbs and generate one new sentence foreach new verb; the sentence is retained if the parsescore for the new sentence is higher than the parsescore for the original sentence.2 VerbNet has de-fined a set of verb-independent thematic roles andgrouped the verbs according to their usage in frameswith identical thematic roles.
But PropBank anno-tation was with respect to each verb.
So the samethematic role is labeled as different roles for dif-ferent verbs in PropBank.
For example, both warnand advise belong to the same VerbNet class (37.9)and take thematic roles of Recipient (person beingwarned or advised) and Topic (topic of warning oradvising).
But Recipient was marked as A2 for warnand A1 for advise and Topic was marked as A1 forwarn and A2 for advise in PropBank annotation.Semlink3 provides a mapping from the thematic roleto PropBank role for each verb.
After the SRL anno-tates the new sentence with PropBank roles for thenew verb, we map the PropBank roles of the newverb to their corresponding thematic roles and thenmap the thematic roles to the corresponding Prop-Bank roles for the original verb.II.
Replacement and Removal of Quoted Strings:Quoted sentences can vary a lot from one domainto another.
For example, in WSJ, quoted sentencesare like formal statements but in Brown, these arelike informal conversations.
We generate the trans-formations in the following ways:1) We use the content of the quoted string as one2Parse score is the parse probability returned by Charniak or Stanford parser.3http://verbs.colorado.edu/semlink/231sentence.
2) We replace each quoted string in turnwith a simple sentence (This is good) to generate anew sentence.
3) If a sentence has a quoted string inthe beginning, we move that quoted string after thefirst NP and VP that immediately follow the quotedstring.
For example, from the input sentence, ?Wejust sit quiet?, he said.
we generate the sentences 1)We just sit quiet 2) ?This is good?, he said.
3) Hesaid, ?We just sit quiet?.III.
Replacement of Unseen Words:A major difficulty for domain adaptation is thatsome words in the new domain do not appear in thetraining domain.
In the Brown test set, 5% of totalwords were never seen in the WSJ training set.Given an unseen word which is not a verb, wereplace it with WordNet synonyms and hypernymsthat were seen in the training data.
We used theclusters obtained in (Liang, 2005) from running theBrown algorithm (Brown et al, 1992) on Reuters1996 dataset.
But since this cluster was generatedautomatically, it is noisy.
So we chose replacementsfrom the Brown clusters selectively.
We only replacethose words for which the POS tagger and the syn-tactic parser predicted different tags.
For each suchword, we find its cluster and select the set of wordsfrom the cluster.
We delete from this set al wordsthat do not take at least one part-of-speech tag thatthe original word can take (from WordNet).
For eachcandidate synonym or hypernym or cluster member,we get a new sentence.
Finally we only keep thosesentences that have higher parse scores than the orig-inal sentence.IV.
Sentence Split based on Stop Symbols:We split each sentence based on stop symbols like; and .
.
Each of the splitted sentences becomes onetransformation of the original sentence.V.
Sentence Simplification:We have a set of heuristics for simplifying theconstituents of the parse tree; for example, replac-ing an NP with its first and last word, removal ofPRN phrases etc.
We apply these heuristics and gen-erate simpler sentences until no more simplificationis possible.
Examples of our heuristics are given inTable 1.Note that we can use composition of multipletransformation functions as one function.
A compo-sition p1  p2(s) = ?a?p1(s)p2(a).
We apply III,IIII, IVI and VI.Node Input Example Simplified Example OperationNP He and she ran.
He ran.
replaceNP The big man ran.
The man ran.
replaceADVP He ran fast.
He ran.
deletePP He ran in the field.
He ran.
deletePRN He ?
though sick ?
ran.
He ran.
deleteVP He walked and ran.
He ran.
deleteTO I want him to run.
I want that he can ran.
rewriteTable 1: Examples of Simplifications (Predicate is run)4.2 Learned TransformationsThe learned model is inaccurate over verbs and rolesthat are infrequent in the training data.
The purposeof the learned transformation is to transfer such aphrase in the test sentence in place of a phrase of asimpler sentence; this is done such that there existsa mapping from the role of the phrase in the newsentence to the role of the phrase in the original sen-tence.Phrase Representation: A phrase tuple is a 3-tuple (t, i, h) where, t is the phrase type, i is the in-dex, and h is the headword of the phrase.
We denoteby PR the Phrase Representation of a sentence ?
anordered list of phrase tuples.
A phrase tuple corre-sponds to a node in the tree.
We only consider phrasetuples that correspond to nodes that are (1) a siblingof the predicate node or (2) a sibling of an ancestorof the predicate node.
Phrase tuples inPR are sortedbased on their position in the sentence.
The index iof the phrase tuple containing the predicate is takento be zero with the indices of the phrase tuples onthe left (right) sequentially decreasing (increasing).Transformation Rule: We denote by Label(n, s)the semantic role of nth phrase in the PR of thesentence s. Let Replace(ns, nt, ss, st) be a newsentence that results from inserting the phrase ns insentence ss instead of phrase nt in sentence st. Wewill refer to st as target sentence and to nt as thetarget phrase.
Let sp be a sequence of phrase tuplesnamed as source pattern.
If Label(ns, ss) = r1 andLabel(nt, Replace(ns, nt, ss, st)) = r2, then denotef(r2) = r1.
In this case we call the 6-tuple (st, nt,p, sp, ns, f ) a transformation rule.
We call f the232label correspondence function.Example: Consider the sentence st = ?But it didnot sing."
and the rule ?
: (st, nt, p, sp, ns, f).
Let:nt = ?3, p = entitle,sp = [?2, NP, ?
][?1, AUX, ?
][0, V, entitle][1, ?, to]ns = ?2, f = {<A0, A2>} ?
{<Ai,Ai>|i 6= 0}.The PR of ?.st is {[?4, CC, But] [?3, NP, it][?2, AUX, did] [?1, RB, not] [0, VB, sing] [1, ., .
]}.Consider the input sentence ss: Mr. X was entitledto a discount .
with PR of {[?2, NP, X] [?1, AUX,was] [0, V, entitle] [1, PP, to][2, ., .]}.
Since ?.sp isa subsequence of the PR of ss, ?
will apply to thepredicate entitle of ss.
The transformed sentence is:str = Replace(?.ns, ?.nt, ss, ?.st) = But Mr. Xdid not sing.
with PR of {[?4, CC, But] [?3, NP,X] [?2, AUX, did] [?1, RB, not] [0, VB, sing] [1,., .]}.
If the SRL system assigns the semantic roleof A0 to the phrase Mr. X of str, the semantic roleof Mr. X in ss can be recovered through ?.f since?.f(A0) = A2 = Label(?2, ss).While checking if ?.sp is a subsequence of thePR of the input sentence, ?
in each tuple of ?.sphas to be considered a trivial match.
So ?
willmatch the sentence He is entitled to a reward.
withPR = {[?2, NP, He] [?1, AUX, is] [0, V, entitle][1, PP, to][2, ., .]}
but will not match the sentenceThe conference was entitled a big success.
withPR = {[?2, NP, conference] [?1, AUX, was] [0,V, entitle] [1, S, success][2, ., .]}
(mismatch positionis bolded).
The index of a phrase tuple cannot be ?,only the head word or type can be ?
and the ruleswith more ?
strings in the source pattern are moregeneral since they can match more sentences.Algorithm 1 GenerateRules1: Input: predicate v, semantic role r, Training sentences D, SRLModel M2: Output: set of rules R3: R?
GetInitialRules(v, r,D,M)4: repeat5: J ?
ExpandRules(R)6: K ?
R ?
J7: sort K based on accuracy, support, size of source pattern8: select some rules R ?
K based on database coverage9: until all rules in R have been expanded before10: return RThe algorithm for finding rules for a semantic roler of a predicate v is given in Algorithm 1.
It is aspecific to general beam search procedure that startswith a set of initial rules (Line 3, detail in Algorithm2) and finds new rules from these rules (Line 5, de-tail in Algorithm 3).
In Line 7, the rules are sortedby decreasing order of accuracy, support and numberof ?
strings in the source pattern.
In Line 8, a set ofrules are selected to cover all occurrences of the se-mantic role r with the predicate v a specific numberof times.
This process continues until no new rulesare found.
Note that these rules need to be learnedonly once and can be used for every new domain.Algorithm 2 GetInitialRules1: Input: predicate v, semantic role r, Training sentences D, SRL-Model M2: Output: Set of initial rules I3: I ?
?4: T ?
{s ?
D : length(s) <= e}5: S ?
{s ?
D : s has role r for predicate v}6: M ?
Set of all semantic roles7: for each phrase p1 in s1 ?
S with gold label r for predicate v do8: for each phrase p2 in s2 ?
T labeled as a core argument do9: if s1 6= s2 and p1 and p2 have same phrase types then10: ?
?
empty rule11: ?.st ?
s2, ?.p?
v12: ?.nt ?
index of p2 in PR of s213: ?.ns ?
index of p1 in PR of s114: ?.sp ?
phrase tuples for phrases from p1 to v and twophrases after v in PR of s115: L?
?16: for each sentence s3 ?D with predicate v do17: if ?.sp is a subsequence of PR of s3 then18: x?
replace(?.ns, ?.nt, s3, ?.st)19: annotate x with SRL using M20: r1 ?
the gold standard semantic role of thephrase with index ?.ns in PR of s321: r2 ?
Label(?.nt, x)22: if r2 /?
L then23: insert(r2, r1) in ?.f24: L = L ?
{r2}25: end if26: end if27: end for28: for each role j ?M ?
L do29: insert(j, j) in ?.f30: end for31: I ?
I?
{?
}32: end if33: end for34: end for35: return IThe algorithm for generating initial rules for thesemantic role r of predicate v is given in Algorithm2.
Shorter sentences are preferred to be target sen-tences(Line 4).
A rule ?
is created for every (p1,p2)pair where p1, p2 are phrases, p1 has the semanticrole r in some sentence s1, p2 is labeled as a coreargument(A0 ?
A5) in some sentence in T and thephrase types of p1 and p2 in their respective parsetrees are same(Lines 7 ?
9).
Every sentence s3 in233training corpus with predicate ?.p is a potential can-didate for applying ?
(Line 16) if ?.sp is a subse-quence ofPR of s3(Line 17).
After applying ?
to s3,a transformed sentence x is created(Line 18).
Lines20 ?
26 find the semantic role r2 of the transferredphrase from SRL annotation of x using model Mand create a mapping from r2 to the gold standardrole r1 of the phrase in s3.
L maintains the set of se-mantic roles for which mappings have been created.In lines 28 ?
30, all unmapped roles are mapped tothemselves.The algorithm for creating new rules from a setof existing rules is given in Algorithm 3.
Lines 4 ?13 generate all immediate more general neighbors ofthe current rule by nullifying the headword or phrasetype element in any of the phrase tuples in its sourcepattern.Algorithm 3 ExpandRules1: Input: a set of rules R2: Output: a set of expanded rules E3: E ?
?4: for each phrase tuple c in the source pattern of r ?
R do5: if c is not the tuple for predicate then6: create a new rule r?
with all components of r7: mark the head word of c in the source pattern of r?
to ?8: add r?
to E9: create a new rule r??
with all components of r10: mark the phrase type of c in the source pattern of r??
to ?11: add r??
to E12: end if13: end for14: return E5 Combination by Joint InferenceThe transformation functions transform an inputsentence into a set of sentences T .
From each trans-formed sentence ti, we get a set of argument can-didates Si.
Let S =?|T |i=1 Si be the set of all ar-guments.
Argument classifier assigns scores foreach argument over the output labels(roles) in Sthat is then converted into a probability distribu-tion over the possible labels using the softmax func-tion (Bishop, 1995).
Note that multiple argumentswith the same span can be generated from multipletransformed sentences.First, we take all arguments from S with distinctspan and put them in S?.
For each argument arg inS?, we calculate scores over possible labels as thesum over the probability distribution (over output la-bels) of all arguments in S that have the same spanas arg divided by the number of sentences in T thatcontained arg.
This results in a set of arguments withdistinct spans and for each argument, a set of scoresover possible labels.
Following the joint inferenceprocedure in (Punyakanok et al, 2008), we want toselect a label for each argument such that the totalscore is maximized subject to some constraints.
Letus index the set S?
as S?1:M where M = |S?|.
Alsoassume that each argument can take a label from aset P .
The set of arguments in S?1:M can take a setof labels c1:M ?
P 1:M .
Given some constraints, theresulting solution space is limited to a feasible set F;the inference task is: c1:M = arg maxc1:M?F (P 1:M )?Mi=1 score(S?i = ci).The constraints used are: 1) No overlapping orembedding argument.
2) No duplicate argument forcore arguments A0-A5 and AA.
3) For C-arg, therehas to be an arg argument.6 Experimental SetupIn this section, we discuss our experimental setupfor the semantic role labeling system.
Similar to theCoNLL 2005 shared tasks, we train our system usingsections 02-21 of the Wall Street Journal portion ofPenn TreeBank labeled with PropBank.
We test oursystem on an annotated Brown corpus consisting ofthree sections (ck01 - ck03).Since we need to annotate new sentences withsyntactic parse, POS tags and shallow parses, we donot use annotations in the CoNLL distribution; in-stead, we re-annotate the data using publicly avail-able part of speech tagger and shallow parser1, Char-niak 2005 parser (Charniak and Johnson, 2005) andStanford parser (Klein and Manning, 2003).Our baseline SRL model is an implementation of(Punyakanok et al, 2008) which was the top per-forming system in CoNLL 2005 shared task.
Due tospace constraints, we omit the details of the systemand refer readers to (Punyakanok et al, 2008).7 ResultsResults for ADUT using only the top parse of Char-niak and Stanford are shown in Table 2.
The Base-line model using top Charniak parse (BaseLine-Charniak) and top Stanford parse (BaseLine-Stanford) score respectively 76.4 and 73.3 on the1http://cogcomp.cs.illinois.edu/page/software234WSJ test set.
Since we are interested in adaptation,we report and compare results for Brown test setonly.
On this set, both ADUT-Charniak and ADUT-Stanford significantly outperform their respectivebaselines.
We compare with the state-of-the-art sys-tem of (Surdeanu et al, 2007).
In (Surdeanu etal., 2007), the authors use three models: Model1 and 2 do sequential tagging of chunks obtainedfrom shallow parse and full parse.
Model 3 assumeseach predicate argument maps to one syntactic con-stituent and classifies it individually.
So Model 3matches our baseline model.
ADUT-Charniak out-performs the best individual model (Model 2) of(Surdeanu et al, 2007) by 1.6% and Model 3 by3.9%.
We also tested another system that used clus-ter features and word embedding features computedfollowing (Collobert and Weston, 2008).
But wedid not see any performance improvement on Brownover baseline.System P R F1BaseLine-Charniak 69.6 61.8 65.5ADUT-Charniak 72.75 66.1 69.3BaseLine-Stanford 70.8 56.5 62.9ADUT-Stanford 72.5 60.0 65.7(Surdeanu et al, 2007)(Model 2) 71.8 64.0 67.7(Surdeanu et al, 2007)(Model 3) 72.4 59.7 65.4Table 2: Comparing single parse system on Brown.All state-of-the-art systems for SRL are a com-bination of multiple systems.
So we combinedADUT-Stanford, ADUT-Charniak and another sys-tem ADUT-Charniak-2 based on 2nd best Charniakparse using joint inference.
In Table 3, We com-pare with (Punyakanok et al, 2008) which was thetop performing system in CoNLL 2005 shared task.We also compare with the multi parse system of(Toutanova et al, 2008) which uses a global jointmodel using multiple parse trees.
In (Surdeanu et al,2007), the authors experimented with several com-bination strategies.
Their first combination strategywas similar to ours where they directly combined theoutputs of different systems using constraints (de-noted as Cons in Table 3).
But their best result onBrown set was obtained by treating the combina-tion of multiple systems as a meta-learning problem.They trained a new model to score candidate argu-ments produced by individual systems before com-bining them through constraints (denoted as LBI inTable 3).
We also compare with (Huang and Yates,2010) where the authors retrained a SRL model us-ing HMM features learned over unlabeled data ofWSJ and Brown.System P R F1 Retrain(Punyakanok et al, 2008) 73.4 62.9 67.8 ?
(Toutanova et al, 2008) NR NR 68.8 ?
(Surdeanu et al, 2007) (Cons) 78.2 62.1 69.2 ?
(Surdeanu et al, 2007) (LBI) 81.8 61.3 70.1 ?ADUT-combined 74.3 67.0 70.5 ?
(Huang and Yates, 2010) 77.0 70.9 73.8 XTable 3: Comparison of the multi parse system on Brown.Table 3 shows that ADUT-Combined performsbetter than (Surdeanu et al, 2007) (Cons) when in-dividual systems have been combined similarly.
Webelieve that the techniques in (Surdeanu et al, 2007)of using multiple models of different kinds (twobased on sequential tagging of chunks to capture ar-guments whose boundaries do not match a syntac-tic constituent) and training an additional model tocombine the outputs of individual systems are or-thogonal to the performance improvement that wehave and applying these methods will further in-crease the performance of our final system which isa research direction we want to pursue in future.We did an ablation study to determine whichtransformations help and by how much.
Table 4presents results when only one transformation is ac-tive at a time.
We see that each transformation im-proves over the baseline.The effect of the transformation of Replacementof Predicate on infrequent verbs is shown in Table5.
This transformation improves F1 as much as 6%on infrequent verbs.The running time for ADUT-Charniak on Brownset is 8 hours compared to SRL training time of 20hours.
Average number of transformed sentencesgenerated by ADUT-Charniak for every sentencefrom Brown is 36.
The times are calculated basedon a machine with 2x 6-Core Xeon X5650 Proces-sor with 48G memory.235Transformation P R F1Baseline 69.6 61.8 65.5Replacement of Unknown Words 70.6 62.1 66.1Replacement of Predicate 71.2 62.8 66.8Replacement of Quotes 71.0 63.4 67.0Simplification 70.3 62.9 66.4RuleTransformation 70.9 62.2 66.2Sentence Split 70.8 62.1 66.2Together 72.75 66.1 69.3Table 4: Ablation Study for ADUT-CharniakFrequency Baseline Replacement of Predicate0 64.2 67.8less than 3 59.7 65.1less than 7 58.9 64.8all predicates 65.5 66.78Table 5: Performance on Infrequent Verbs for the Trans-formation of Replacement of Predicate8 Related WorkTraditional adaptation techniques like (Daum?III,2007; Chelba and Acero, 2004; Finkel and Man-ning, 2009; Jiang and Zhai, 2007; Blitzer et al,2006; Huang and Yates, 2009; Ando and Zhang,2005; Ming-wei Chang and Roth, 2010) need to re-train the model for every new domain.
In (Umansky-Pesin et al, 2010), there was no retraining; instead,a POS tag was predicted for every unknown wordin the new domain by considering contexts of thatword collected by web search queries.
We differfrom them in that our transformations are label-preserving; moreover, our transformations aim atmaking the target text resemble the training text.We also present an algorithm to learn transformationrules from training data.
Our application domain,SRL, is also more complex and structured than POStagging.In (McClosky et al, 2010), the task of multiplesource parser adaptation was introduced.
The au-thors trained parsing models on corpora from dif-ferent domains and given a new text, used a linearcombination of trained models.
Their approach re-quires annotated data from multiple domains as wellas unlabeled data for the new domain, which is notneeded in our framework.
In (Huang and Yates,2010), the authors trained a HMM over the Browntest set and the WSJ unlabeled data.
They derivedfeatures from Viterbi optimal states of single wordsand spans of words and retrained their models us-ing these features.
In (Vickrey and Koller, 2008),a large number of hand-written rules were used tosimplify the parse trees and reduce syntactic vari-ation to overcome feature sparsity.
We have sev-eral types of transformations, and use less than 10simplification heuristics, based on replacing largerphrases with smaller phrases and deleting unneces-sary parse tree nodes.
There are also some methodsfor unsupervised semantic role labeling (Swier andStevenson, 2004), (Abend et al, 2009) that easilyadapt across domains but their performances are notcomparable to supervised systems.9 ConclusionWe presented a framework for adaptating naturallanguage text so that models can be used across do-mains without modification.
Our framework sup-ports adapting to new domains without any data orknowledge of the target domain.
We showed that ourapproach significantly improves SRL performanceover the state-of-the-art single parse based systemon Brown set.
In the future, we would like to extendthis approach to other NLP problems and study howcombining multiple systems can further improve itsperformance and robustness.Acknowledgements This research is sponsoredby the Army Research Laboratory (ARL) underagreement W911NF-09-2-0053 and by the DefenseAdvanced Research Projects Agency (DARPA) Ma-chine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-0181.
Any opinions, findings, conclusions or rec-ommendations are those of the authors and do notnecessarily reflect the view of the ARL, the DARPA,AFRL, or the US government.ReferencesOmri Abend, Roi Reichart, and Ari Rappoport.
2009.Unsupervised argument identification for semanticrole labeling .
In Proceedings of the ACL.Rie Kubota Ando and Tong Zhang.
2005.
A frameworkfor learning predictive structures from multiple labeled236and unlabeled data .
Journal of Machine Learning Re-search.Christopher Bishop.
1995.
Neural Networks for Patternrecognition, chapter 6.4: Modelling conditional distri-butions.
Oxford University Press.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP).Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. D. Pietra, and Jenifer C. Lai.
1992.
Class-basedn-gram models of natural language.
ComputationalLinguistics, 18(4):467?479.Xavier Carreras and Llu?s M?rquez.
2005.
Introductionto the conll-2005 shared task: Semantic role labeling .In Proceedings of CoNLL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of ACL.Ciprian Chelba and Alex Acero.
2004.
Little datacan help a lot.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP).Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof ICML.Hal Daum?III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of the the Annual Meeting of theAssociation of Computational Linguistics (ACL).Jenny R. Finkel and Christopher D. Manning.
2009.
Hi-erarchical bayesian domain adaptation .
In Proceed-ings of NAACL.Fei Huang and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervisedsequence-labeling .
In Proceedings of ACL.Fei Huang and Alexander Yates.
2010.
Open-domainsemantic role labeling by modeling word spans.
InProceedings of ACL.Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in nlp.
In Proceedings ofACL.Dan Klein and Christopher D. Manning.
2003.
Fast exactinference with a factored model for natural languageparsing.
In Proceedings of NIPS.Percy Liang.
2005.
Semi-supervised learning for naturallanguage.
Masters thesis, Massachusetts Institute ofTechnology.David McClosky, Eugene Charniak, and Mark Johnson.2010.
Automatic domain adaptation for parsing.
InProceedings of NAACL.Michael Connor Ming-wei Chang and Dan Roth.
2010.The necessity of combining adaptation methods.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), Mas-sachusetts, USA.Vasin Punyakanok, Dan Roth, and Wen tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2).Mihai Surdeanu, Llu?s M?rquez, Xavier Carreras, andPere R. Comas.
2007.
Combination strategies for se-mantic role labeling.
Journal of Artificial IntelligenceResearch, 29:105?151.Robert S. Swier and Suzanne Stevenson.
2004.
Unsuper-vised semantic role labelling.
In Proceedings of Em-pirical Methods in Natural Language Processing.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34:161?191.Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-poport.
2010.
A multi-domain web-based algorithmfor pos tagging of unknown words .
In Proceedings ofColing.David Vickrey and Daphne Koller.
2008.
Sentence sim-plification for semantic role labeling.
In Proceedingsof the ACL-HLT.237
