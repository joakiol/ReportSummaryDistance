Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 99?108,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsImproving Peer Feedback Prediction: The Sentence Level is RightHuy V. NguyenDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260hvn3@pitt.eduDiane J. LitmanDepartment of Computer Science & LRDCUniversity of PittsburghPittsburgh, PA 15260litman@cs.pitt.eduAbstractRecent research aims to automatically pre-dict whether peer feedback is of high qual-ity, e.g.
suggests solutions to identifiedproblems.
While prior studies have fo-cused on peer review of papers, simi-lar issues arise when reviewing diagramsand other artifacts.
In addition, previousstudies have not carefully examined howthe level of prediction granularity impactsboth accuracy and educational utility.
Inthis paper we develop models for predict-ing the quality of peer feedback regard-ing argument diagrams.
We propose toperform prediction at the sentence level,even though the educational task is to la-bel feedback at a multi-sentential com-ment level.
We first introduce a corpusannotated at a sentence level granularity,then build comment prediction models us-ing this corpus.
Our results show that ag-gregating sentence prediction outputs tolabel comments not only outperforms ap-proaches that directly train on commentannotations, but also provides useful infor-mation for enhancing peer review systemswith new functionality.1 IntroductionPeer review systems are increasingly being usedto facilitate the teaching and assessment of studentwriting.
Peer feedback can complement and evenbe as useful as teacher feedback; students can alsobenefit by producing peer feedback.
Past researchhas shown that feedback implementation is sig-nificantly correlated to the presence of desirablefeedback features such as the description of so-lutions to problems (Nelson and Schunn, 2009).Since it would be very time-consuming for in-structors to identify feedback of low quality post-hoc, recent research has used natural languageprocessing (NLP) to automatically predict whetherpeer feedback contains useful content for guidingstudent revision (Cho, 2008; Ramachandran andGehringer, 2011; Xiong et al., 2012).
Such real-time predictions have in turn been used to enhanceexisting online peer-review systems, e.g.
by trig-gering tutoring that is designed to improve feed-back quality (Nguyen et al., June 2014).While most prior research of peer review qual-ity has focused on feedback regarding papers, sim-ilar issues arise when reviewing other types of ar-tifacts such as program code, graphical diagrams,etc.
(Nguyen and Litman, July 2013).
In addi-tion, previous studies have not carefully examinedhow the level of prediction granularity (e.g.
multi-sentential review comments versus sentences) im-pacts both the accuracy and the educational utilityof the predictive models.
For example, while thetutoring intervention of (Nguyen et al., June 2014)highlighted low versus high quality feedback com-ments, such a prediction granularity could not sup-port the highlighting of specific text spans that alsomight have been instructionally useful.In this paper, we first address the problem ofpredicting feedback type (i.e.
problem, solution,non-criticism) in peer reviews of student argumentdiagrams.
In problem feedback, the reviewer de-scribes what is wrong or needs to be improved inthe diagram.
In solution feedback, the reviewerprovides a way to fix a problem or to improve thediagram quality.
Feedback is non-criticism whenit is neither a problem nor a solution (e.g.
when itprovides only positive feedback or summarizes).Examples are shown in Figure 1.1The second goal of our research is to design ourprediction framework so that it can support real-time tutoring about feedback quality.
We hypoth-1Our peer review corpus comes from a system that usesan end-comment feedback approach as shown in Figure 1.While it is possible to instead directly annotate a reviewedartifact, this has been shown to encourage feedback on low-level issues, and is not good for more global feedback.99#3.
Are the relevance, validity, and reason fields in the supportive arcs complete andconvincing?
If not, indicate where the argument for relevance or validity is missing orunclear.
Suggest ways to make the validity or relevance argument more convincing orsensible.Not all of these field are filled out, which makes it hard to get a clear idea of how legitthese studies are.
Also, some are unclear.
An example is 24-supports where the reasonis a question.
I think there should be a substantial reason there instead of a questionto convince me why it is relevant.#5.
Is at least one credible opposing Finding, Study, or Theory connected to eachHypothesis?
If there is no opposition, suggest a spot for a potential counterargument.If there is opposition, is it credible?
If the opposition is not credible, explain why.There is a good piece of credible opposition, though it is hard to tell from the diagramwhat the study exactly did.#1.
Are any parts of the diagram hard to understand because they are unclear?
If so,describe any particularly confusing parts of the diagram and suggest ways to increaseclarity.The argument diagram was easy to follow.
I was able to effortlessly go through thediagram and connect each part.Your comments need to suggest solution.For every comment below, if you point out a problem make sure thatyou provide a solution to fix that problem.Your comments which are highlighted in GREEN already have solutionsprovided, while the RED comments mention only problem.
Examplesof problem and solution text are formatted in ITALIC and BOLD.I?ve revised mycomments.Please checkagain.Could you showme someexamples ofproblem andsolutioncomments?My commentsdon?t have theissue that youdescribe.
Pleasesubmitcomments.Figure 1: A mock-up interface of a peer review system where the prediction of feedback type triggersa system tutoring intervention.
Left: three sample feedback comments including a non-criticism (top),a solution (middle), and a problem (bottom).
Right-top: a system tutoring intervention to teach thestudent reviewer to provide a solution whenever a problem is mentioned.
Right-bottom: possible studentresponses to the system?s tutoring.esize that using a student?s own high-quality re-views during tutoring, and identifying the explicittext that makes the review high quality, will helpstudents learn how to improve their lower qualityreviews.
To facilitate this goal, we develop pre-diction models that work at the sentence level ofgranularity.Figure 1 presents a mock-up of our envisionedpeer review interface.
To tutor the student aboutsolutions (figure right), the system uses live ex-amples taken from the student?s current review(figure left).
Color is used to display the feed-back type predictions: here a non-criticism is dis-played in black, while the criticisms that are pos-itive and negative examples of solution are dis-played in green and red, respectively.
In addition,to help the student focus on the important aspect ofthe (green) positive example, the sentence that ac-tually specifies the solution is highlighted in bold.This paper presents our first results towards re-alizing this vision.
The contributions of our workare two-fold.
First, we develop a sentence-levelmodel for predicting feedback type in a diagramreview corpus.
While our peer review systemworks at the level of feedback comments (text ofeach box in Figure 1), we find it is more accu-rate to annotate and predict at finer-grained gran-ularity levels, then use these predictions to inferthe comment?s feedback type.
By introducing asmall overhead to annotate peer feedback, we cre-ated a phrase level-annotated corpus of argumentdiagram reviews.
Our experimental results showthat our learned prediction models using labeledsentences outperform models trained and testedat comment level.
In addition, our models out-perform models previously developed for paperrather than diagram feedback, and also show po-tential generality by avoiding the use of domain-specific features.
Second, we demonstrate thatour sentence-level prediction can be used to sup-port visualizations useful for tutoring.
Particularsentences that are predicted to express the com-ment?s feedback type are highlighted for instruc-tional purposes (e.g.
the bold highlighting in Fig-ure 1).2 Related workIn instructional science, research has been con-ducted to understand what makes peer feedbackhelpful.
At the secondary school level, Gielen etal.
(2010) found that the presence of justificationin feedback significantly improved students?
writ-ing performance.
At the university level, Nelsonand Schunn (2009) found that feedback on paperswas more likely to be implemented when the feed-back contained solutions or pinpointed problemlocations.
Lippman et al.
(2012) found that similarfeedback properties led to greater implementation100of feedback on diagrams as well.Building on such findings, researchers have be-gun to develop automated methods to identifyhelpful feedback.
Cho (2008) was the first totake a machine learning approach.
Peer feedback,i.e.
comments, were manually segmented intoidea units2and human-coded for various featuresincluding problem detection, solution suggestion,praise, criticism, and summary.
Feedback wasthen labeled as helpful or not-helpful based on thepresence of such features.
The study showed thatfeedback could be classified regarding helpfulnesswith up to 67% accuracy using simple NLP tech-niques including ngrams and part-of-speech.
Ourwork is different from (Cho, 2008) in that we focuson predicting particular feedback types (i.e.
solu-tion and problem) rather than helpfulness in gen-eral.
Also, as the raw feedback to peer-review sys-tems is typically at the comment-level, and beingaware that idea-units are difficult to automaticallysegment, we instead predict at the sentence-levelto make model deployment more practical.Our work is more similar to (Xiong and Litman,2010; Xiong et al., June 2010; Xiong et al., 2012),in which NLP and machine learning were used toautomatically predict whether peer reviews of stu-dent papers contained specific desirable feedbackfeatures.
Xiong and Litman used NLP-based fea-tures including paper ngrams, predefined keywordlists, and dependency parses to predict feedbacktype.
For feedback of type criticism, they alsodeveloped models to further predict problem lo-calization and solution.
Following (Cho, 2008),Xiong and Litman evaluated their models on peerreview data that had been manually segmentedinto idea units.
As noted above, the difficulty ofautomatically segmenting raw comments into ideaunits makes deployment of such models less prac-tical than our sentence-level approach.
Also likeCho (2008), while their models predicted a labelfor each idea unit, the relevant text that led to theprediction was not identified.
We will address thislimitation by introducing a more fine-grained an-notated corpus.Regarding peer reviews of student argumentdiagrams rather than papers, Nguyen and Lit-man (July 2013) developed a rule-based algorithmfor predicting feedback that contained localizationtext (e.g.
?Hypothesis 4?).
Their approach was to2Cf.
(Cho, 2008) ?a self-contained message on a singlepiece of strength or weakness found in peer writing.
?first identify common words between a peer com-ment and its diagram, then classify phrases con-taining these words into different localization pat-terns.
Although we similarly focus on diagramrather than paper feedback, our work addresses adifferent prediction task (namely, predicting feed-back type rather than localization).
We also usestatistical machine learning rather than a rule-based approach, in conjunction with more generallinguistic features, to allow us to ultimately useour models for papers as well as diagrams withminimal modification or training.Outside of peer review, research has been per-formed recently to mine wishes and suggestionsin product reviews and political discussion.
Gold-berg et al.
(2009) analyzed the WISH3corpus andbuilt wish detectors based on simple word cuesand templates.
Focusing on product reviews only,Ramanand et al.
(2010) created two corpora ofsuggestion wishes (wishes for a change in an exist-ing product or service) and purchasing wishes (ex-plicit expressions of a desire to purchase a prod-uct), and developed rules for identifying wish sen-tences from non-wish ones.
Both (Goldberg et al.,2009; Ramanand et al., 2010) created rules manu-ally by examining the data.
Although we hypoth-esize that wishes are related to solutions in peerreview, our educational data makes direct applica-tion of product-motivated rules difficult.
We thuscurrently use statistical machine learning for ourinitial research, but plan to explore incorporatingexpression rules to enhance our model.Sub-sentence annotation has gained much inter-est in sentiment analysis and opinion mining.
Onenotable work is (Wilson et al., 2005) in which theauthor addressed the problem that the contextualpolarity (i.e.
positive, negative, or neutral) of thephrase in which a word appears may be differentfrom the word?s prior polarity.
We will also use aphrase-level annotation, as described below.3 Argument diagram review corpusDiagramming software tools such as LASAD(Scheuer et al., 2010) are increasingly beingused to teach student argumentation skills throughgraphical representations.
Graphical argument en-vironments typically allow students to create di-agrams in which boxes represent statements andlinks represent argumentative or rhetorical rela-tions.
This helps students focus on abstract argu-3http://www.timessquarenyc.org/nye/nye interactive.html101Figure 2: Part of a student argument diagram.ment schemes before learning how to write argu-mentative essays.
To further help students creategood argument diagrams, it has recently been sug-gested that receiving and providing feedback onargument diagrams might yield useful pedagogi-cal benefits (Falakmassir et al., July 2013), anal-ogously to improving writing via peer review ofpapers.Our corpus consists of a subset of commentsfrom diagram reviews collected from nine separatesections of an undergraduate psychology course.Student argument diagrams were created using aninstructor-defined diagram ontology.
The diagramontology defines five different types of nodes:Current study, Hypothesis, Theory, Finding, andStudy (for reference).
The ontology also definesfour different types of arcs that connect nodes:Supports, Opposes, Part-of, and Undecided.
Fig-ure 2 shows part of a student argument diagramthat includes two studies, each of which supportsa finding which in turn supports or opposes a hy-pothesis.
In the course that generated our corpus,students first created graphical argument diagramsusing LASAD to justify given hypotheses.
Studentargument diagrams were then distributed, usingthe SWoRD (Cho and Schunn, 2007) web-basedpeer-review system, to other students in the classfor reviewing.
Student authors potentially revisedtheir argument diagrams based on peer feedback,then used the diagrams to write the introductionof associated papers.
Diagram reviews consist ofmultiple written feedback comments in response<IU> <Pr>Not all of these field are filled out,which makes it hard to get a clear idea of howlegit these studies are.</Pr> </IU> <IU><Pr>Also, some are unclear.
An example is 24-supports where the reason is a question.</Pr><Sl>I think there should be a substantialreason there instead of a question to convinceme why it is relevant.</Sl> </IU>Table 1: Example of an annotated comment.Markers<IU>: idea unit,<Sl>: solution,<Pr>:problem.
Problem text is italic and solution text isbold for illustration purpose.to rubric prompts, i.e.
review dimensions.
Studentreviewers were required to provide at least one butno more than three comments for each of five re-view dimensions.
Figure 1 shows three samplepeer comments for three review dimensions (i.e.dimensions 1, 3 and 5).Following prior work on peer review analy-sis (Lippman et al., 2012; Nguyen and Litman,July 2013), the first author composed a codingmanual for peer reviews of argument diagrams.An annotator first segments each comment intoidea units (defined as contiguous feedback re-ferring to a single topic).
Note that idea-unitsegmentation is necessary to make coding reli-able.
We however do not exploit idea unit in-formation for our current prediction tasks.
Thenthe annotator codes each idea unit for differentfeatures among which solution and problem are102Label Number of commentsSolution 178Problem 194Combined 135Non-criticism 524Total 1031Table 2: Comment label distribution.the two labels used in this study.
These la-bels are then used to assign a feedback type(i.e.
solution, problem, combined, andnon-criticism) to the comment as a whole.The comment is labeled Solution if at leastone of its idea units presents a solution but noproblem unit is explicitly present.
If no solutionidea is found, the comment is labeled Problemif at least one of its idea units presents a prob-lem.
The comment is labeled Combined ifit has both solution and problem idea units, orNon-criticism if it does not have solution orproblem.
Non-criticism units can be praise,summary or text that does not express any idea,e.g.
?Yes, it is.?
Table 1 shows an example anno-tated feedback comment that consists of two ideaunits.
The first idea unit is about empty fields, andthe second is about reason is a question.
Based onthe annotations shown, the comment as a wholehas the label Combined.We had one undergraduate psychology majorannotate the 1031 comments in our corpus, yield-ing the label distribution shown in Table 2.
Thefirst author also annotated 244 randomly selectedcomments, solely to evaluate inter-coder agree-ment.
The obtained agreement of comment labelswas high, with accuracy 0.81 and kappa 0.74.In addition to comment labeling, the annotatoralso highlighted4text spans that explain the labels.The marked text span must either express solutionor problem information but cannot express both.Therefore we require the annotator to highlight atthe phrase (i.e.
sub-sentence) level, and that eachmarked text must be completely within an ideaunit.
Generally speaking this requirement doesnot increase cognitive workload because annota-tors already have to read the comment and noticeany solution or problem mentioned before label-ing.4Highlighting was made possible using macros in Mi-crosoft Word.
Annotators select the text of interest, then clicka button corresponding to the relevant label, e.g.
problem.Label SentenceProblem Not all of these field are filled out,which makes it hard to get a clearidea of how legit these studies are.Problem Also, some are unclear.Problem An example is 24-supports wherethe reason is a question.Solution I think there should be a substantialreason there instead of a questionto convince me why it is relevant.Table 3: Examples of labeled sentences extractedfrom the annotated comment.Category Number of sentencesSolution 389Problem 458Non-criticism 1061Total 1908Table 4: Sentence label distribution.Although we asked the annotator to mark textspans which convey problem or solution informa-tion, we did not ask the annotator to break eachtext span into sentences.
The first reason is thatthe problem or solution text might only be partof a sentence and highlighting only the informa-tive part will give us more valuable data.
Second,sentence segmentation can be performed automat-ically with high accuracy.
After the corpus wasannotated, we ran a sentence segmentation proce-dure using NLTK5to create a labeled corpus atthe sentence level as follows.
Each comment isbroken into three possible parts: solution includ-ing all solution text marked in the comment, prob-lem including all problem text, and other for non-criticism text.
Each part is then segmented intosentences and each sentence is assigned the labelof the part to which it belongs.
It may happenthat the segmented text is a phrase rather than acomplete sentence.
We consider such phrases asreduced sentential-like text, and we use the termsentence(s) to cover such sub-sentence forms, aswell.
Labeled sentences of the comment in Table 1are shown in Table 3.
After discarding empty sen-tences and those of length 1 (all of those are in thenon-criticism category), there are 1908 sentencesremaining, distributed as shown in Table 4.5www.nltk.org1034 Experimental setupSections 6 and 7 report the results of two differentexperiments involving the prediction of feedbacktypes at the comment level.
While each experi-ment differs in the exact classes to be predicted,both compare the predictive utility of the same twodifferent model-building approaches:?
Trained using comments (CTRAIN): ourbaseline6approach learns comment predic-tion models using labeled feedback com-ments for training.?
Trained using sentences (STRAIN): our pro-posed approach learns sentence predictionmodels using labeled sentences, then aggre-gates sentence prediction outputs to createcomment labels.
For example, the aggrega-tion used for the experiment in Section 6 is asfollows: if at least one sentence is predictedas Solution/Problem then the comment is as-signed Solution/Problem.We hypothesize that the proposed approach willyield better predictive performance than the base-line because the former takes advantage of cleanerand more discriminative training data.To make the features of the two approachescomparable, we use the same set of generic lin-guistic features:?
Ngrams to capture word cues: word un-igrams, POS/word bigrams, POS/word tri-grams, word and POS pairs, punctuation,word count.?
Dependency parse to capture structure cues.We skip domain and course-specific features (e.g.review dimensions, diagram keywords like hy-pothesis) in order to make the learned model moreapplicable to different diagram review data.
In-stead, we search for diagram keywords in com-ments and replace them with the string ?KEY-WORD?.
The keyword list can be extracted au-tomatically from LASAD?s diagram ontology.Adding metadata features such as comment andsentence ordering did not seem to improve perfor-mance so we do not include such features in theexperiments below.6The use of comment-level annotations for training andtesting is similar to (Nguyen and Litman, July 2013).Following (Xiong et al., 2012), we learn predic-tion models using logistic regression.
However, inour work both feature extraction and model learn-ing are performed using the LightSide7toolkit.
Asour data is collected from nine separate sectionsof the same course, to better evaluate the models,we perform cross-section evaluation in which foreach fold we train the model using data from 8sections and test on the remaining section.
Re-ported results are averaged over 9-fold cross vali-dations.
Four metrics are used to evaluate predic-tion performance.
Accuracy (Acc.)
and Kappa (?
)are used as standard performance measurements.Since our annotated corpus has imbalanced datawhich makes the learned models bias to the ma-jority classes, we also report the Precision (Prec.
)and Recall (Recl.)
of predicting the minor classes.5 Sentence prediction performanceWe first evaluate models for predicting binary ver-sions of the sentence labels from Table 4 (e.g.
so-lution or not), as this output will be aggregated inour proposed STRAIN approach.
The results ofusing sentence training (STr) and sentence testing(STe) are shown in the STR/STE row of Table 5.For comparison, the first row of the table showsthe performance of a majority baseline approach(MAJOR), which assigns all sentences the label ofthe relevant major class in each prediction task.To confirm that a sentence-level annotated corpusis necessary to train sentence prediction models,a third approach that uses labeled comment datafor training (CTr) but sentences for testing (STe)is included in the CTR/STE row.
As we can see,STR/STE models outperform those of CTR/STEand MAJOR for all 4 metrics8.
The comment ver-sus sentence training yields significant differencesfor predicting Problem and Criticism sentences.6 Three feedback type prediction tasksIn this experiment we evaluate our hypothesis thatSTRAIN outperforms CTRAIN by comparing per-formance on three feedback type prediction tasksat the comment level (derived from Table 2):?
Problem v. Non-problem.
The Problem classincludes problem and combined comments.7http://ankara.lti.cs.cmu.edu/side/download.html8Note that ?
in general, and precision and recall of minorclasses, are not applicable when evaluating MAJOR.104Solution Problem CriticismModel Acc.
?
Prec.
Recl.
Acc.
?
Prec.
Recl.
Acc.
?
Prec.
Recl.MAJOR 0.80 - - - 0.76 - - - 0.56 - - -CTR/STE 0.87 0.57 0.70 0.62 0.75 0.22 0.48 0.29 0.75 0.48 0.76 0.63STR/STE 0.88 0.61 0.76 0.63 0.81 0.44 0.62 0.51 0.80 0.59 0.79 0.74Table 5: Prediction performance of three tasks at the sentence level.
Comparing STR/STE to CTR/STE:Italic means higher with p < 0.05, Bold means higher with p < 0.01.?
Solution v. Non-solution.
The Solution classincludes solution and combined comments.?
Criticism v. Non-criticism.
The Criticismclass includes problem, solution and com-bined comments.The two approaches are also compared to majoritybaselines (MAJOR) and a hybrid approach (HY-BRD) that trains models using labeled sentencedata but tests on labeled comments.As shown in Table 6, both MAJOR and HYBRDperform much worse than CTRAIN and STRAIN.We note that while HYBRD gives comparably highprecision, its kappa and recall do not match thoseof CTRAIN and STRAIN.
Comparing CTRAINand STRAIN, the results confirm our hypothesisthat STRAIN outperforms CTRAIN.
The major ad-vantage of STRAIN is that it only needs one cor-rectly predicted sentence to yield the correct com-ment label.
This is particularly beneficial for pre-dicting problem comments, where the improve-ment is significant for 3 of 4 metrics.As our evaluation is cross-section, folds do nothave identical label distributions.
Therefore welook at prediction performance for each of the nineindividual sections.
We find that the sentence levelapproach yields higher performance on all fourmetrics in six sections when predicting both So-lution and Problem task, but only two sections forCriticism.
For the Criticism task ?
where it is notnecessary to exclusively differentiate between So-lution and Problem, training prediction models us-ing labeled sentences does not yield higher perfor-mance than the traditional approach.Roughly comparing predicting at the sentencelevel (Table 5) versus the comment level (Table 6),we note that the sentence level tasks are more dif-ficult (e.g.
lower absolute kappas) despite an in-tuition that the labeled sentence corpus is cleanerand more discriminative compared to the labeledcomment corpus.
The observed performance dis-parity shows the necessity of developing bettersentence prediction models, which we leave to fu-ture work.7 A case study experimentTo the best of our knowledge, (Xiong et al.,June 2010; Xiong et al., 2012) contain the onlypublished models developed for predicting feed-back types.
A comment-level solution predictionmodel has since been deployed in their peer re-view software to evaluate student reviewer com-ments in classroom settings, using the following 3-way classification algorithm9.
Each student com-ment is classified as either a criticism (i.e.
presentsproblem/solution information) or a non-criticism.The non-criticism comment is labeled NULL.
Thecriticism comment is labeled SOLUTION if it con-tains solution information, and labeled PROBLEMotherwise.To evaluate our proposed STRAIN approach intheir practically-motivated setting, we follow thedescription above to relabel peer feedback com-ments in our corpus to new labels: NULL, PROB-LEM, and SOLUTION.
We also asked the authorsof (Xiong et al., 2012) for access to their currentmodel and we were able to run their model on ourfeedback comment data.
While it is not appro-priate to directly compare model performance asXiong et al.
were working with paper (not dia-gram) review data, we report their model output,named PAPER, to provide a reference baseline.
Weexpect the PAPER model to work on our diagramreview data to some extent, particularly due to itspredefined seed words for solution and problemcues.
Our CTRAIN baseline, in contrast, trainsmodels regarding the new label set using relabeleddiagram comment data, with the same features andlearning algorithm from the prior sections.
Themajority baseline, MAJOR, assigns all commentsthe major class label (which is now NULL).Regarding our STRAIN sentence level ap-9Personal communication.105Solution Problem CriticismModel Acc.
?
Prec.
Recl.
Acc.
?
Prec.
Recl.
Acc.
?
Prec.
Recl.MAJOR 0.70 - - - 0.68 - - - 0.51 - - -HYBRD 0.82 0.52 0.87 0.48 0.75 0.36 0.68 0.41 0.78 0.56 0.84 0.68CTRAIN 0.87 0.67 0.84 0.71 0.76 0.43 0.65 0.55 0.83 0.66 0.85 0.80STRAIN 0.88 0.71 0.86 0.74 0.81 0.55 0.71 0.66 0.85 0.70 0.84 0.85Table 6: Prediction performance of three tasks at comment level.
Comparing STRAIN to CTRAIN: Italicmeans higher with p < 0.1, Bold means higher with p < 0.05.1.
For each sentence, label it SOLUTION if it ispredicted as Solution by the Solution model.2.
For a predicted Non-solution sentence, labelit NULL if it is predicted as Non-criticism by theCriticism model.3.
For a predicted Criticism sentence, label itPROBLEM if it is predicted as Problem by theProblem model.4.
For a predicted Non-problem sentence, labelit SOLUTIONTable 7: Relabel procedure.proach, we propose two aggregation proceduresto infer comment labels given sentence predictionoutput.
In the first procedure, RELABELFIRST, weinfer new sentence labels regarding NULL, PROB-LEM, and SOLUTION using a series of condi-tional statements.
The order of statements is cho-sen heuristically given the performance of indi-vidual models (see Table 5) and is described inTable 7.
Given the sentences?
inferred labels,the comment is labeled SOLUTION if it has atleast one SOLUTION sentence.
Else, it is labeledPROBLEM if at least one of its sentences is PROB-LEM, and labeled NULL otherwise.
Our secondaggregation procedure, called INFERFIRST, fol-lows an opposite direction in which we infer com-ment labels regarding Solution, Problem, and Crit-icism before re-labeling the comment regardingSOLUTION, PROBLEM, and NULL following theorder of conditional statements in the relabel pro-cedure.As shown in Table 8, the MAJOR and PAPERmodels perform much worse than the other threemodels.
While the PAPER model has accuracyclose to that of the other models, its kappa is farlower.
Regarding the three models trained on di-agram review data, the two sentence level modelsoutperform the CTRAIN model.
Particularly, kap-Model Acc.
?MAJOR 0.51 -PAPER 0.71 0.49CTRAIN 0.76 0.60RELABELFIRST 0.79 0.66INFERFIRST 0.79 0.66Table 8: Prediction performance of different ap-proaches in a case study.pas of the two sentence level models are either sig-nificantly higher (for INFERFIRST) or marginallyhigher (for RELABELFIRST) compared to kappaof CTRAIN.
To further investigate performancedisparity between models, we report in Table 9precision and recall of different models for eachclass.
The PAPER model achieves high preci-sion but low recall for SOLUTION and PROBLEMclasses.
We reason that the model?s seed wordshelp its precision, but its ngram features, whichwere trained using paper review data, cannot ad-equately cover positive instances in our corpus.The two sentence level models perform better forthe PROBLEM class than the other two models,which is consistent with what is reported in Ta-ble 6.
Comparing the two sentence level models,INFERFIRST better balances precision and recallthan RELABELFIRST.8 The sentence level is rightThe experimental results in the previous two sec-tions have demonstrated that sentence predictionoutput helps improve prediction performance atthe comment level.
This supports our hypothesisthat sentence prediction is the right level for en-hancing peer review systems to detect and respondto multi-sentence review comments of low qual-ity.
In our labeled sentence corpus, each instanceeither expresses a solution, a problem, or is a non-criticism, so the data is cleaner and more discrim-106SOLUTION PROBLEM NULLModel Prec.
Recl.
Prec.
Recl.
Prec.
Recl.MAJOR - - - - 0.51 1.00PAPER 0.81 0.62 0.58 0.29 0.70 0.92CTRAIN 0.84 0.75 0.55 0.41 0.78 0.90RELABELFIRST 0.72 0.90 0.66 0.48 0.88 0.84INFERFIRST 0.75 0.86 0.61 0.55 0.88 0.84Table 9: Precision and recall of different models in a case study.inative than the labeled comment corpus.
This isa nice property that helps reduce feature colloca-tion across exclusive classes, Problem vs. Solutionfor example, which is a danger of training on feed-back comments due to Combined instances.
More-over, our annotated comment corpus has solutionand problem text marked at the sub-sentence level,which is a valuable resource for learning solutionand problem patterns and linguistic cues.Improving peer feedback prediction accuracy isnot the only reason we advocate for the sentencelevel.
We envision that the sentence level is thenecessary lower bound that a peer review systemneeds to handle new advanced functionalities suchas envisioned in Figure 1.
Being able to highlightfeatured text in a peer comment is a useful visu-alization function that should help peer reviewerslearn from live examples, and may also help stu-dent authors quickly notice the important point ofthe comment.Sentence and phrase level annotation is madeeasy with the availability of many text annota-tion toolkits; BRAT10(Stenetorp et al., 2012) isan example.
From our work, marking text spansby selecting and clicking requires a minimal ad-ditional effort from annotators and does not causemore cognitive workload.
Moreover, we hypoth-esize that through highlighting the text, an anno-tator has to reason about why she would choose alabel, which in turn makes the annotation processmore reliable.
We plan to test whether annotationperformance does indeed improve in future work.9 Conclusions and future workIn this paper we present a sentence-level anno-tated corpus of argument diagram peer reviewdata, which we use to develop comment-level pre-dictions of peer feedback types.
Our work is thefirst of its kind in building an automated feed-10http://brat.nlplab.org/back type assessment component for reviews ofargument diagrams rather than papers.
We havedemonstrated that using sentence prediction out-puts to label the corresponding comments outper-forms the traditional approach that learns mod-els using labeled comments.
The improvementof using sentence prediction outputs is more sig-nificant for more difficult tasks, i.e.
Problem vs.Non-problem, in which textual expression variesgreatly from explicit to implicit.
In a case studymimicking a real application setting to experimentwith the proposed models, we achieved a simi-lar verification of the utility of sentence models.Given our imbalanced training data labels and ouravoidance of using domain-specific features, thesefirst results of our two experiments are promising.In these first studies, our models were trainedusing generic prediction procedures, e.g., usingbasic linguistic features without feature selectionor tuning.
Thus our next step is to analyze pre-diction features for their predictiveness.
We alsoplan to incorporate human-engineered rules for so-lution and problem text.
We aim to improve per-formance while keeping feature generality.
An in-teresting experiment we may conduct is to test ourlearned models on paper review data to evaluateperformance and generality in an extreme setting.AcknowledgmentsThis work is supported by NFS Grant No.1122504.
We are grateful to our colleagues forsharing the data.
We thank Kevin Ashley, WencanLuo, Fan Zhang, other members of the Argument-Peer and ITSPOKE groups as well as the anony-mous reviewers for their valuable feedback.ReferencesKwangsu Cho and Christian D. Schunn.
2007.
Scaf-folded writing and rewriting in the discipline: Aweb-based reciprocal peer review system.
Comput-ers and Education, 48(3):409?426.107Kwangsu Cho.
2008.
Machine classification of peercomments in physics.
In Proceedings 1st inter-national conference on Educational Data Mining(EDM), pages 192?196.Mohammad Falakmassir, Kevin Ashley, and ChristianSchunn.
July 2013.
Using argument diagrammingto improve peer grading of writing assignments.
InProceedings of the 1st Workshop on Massive OpenOnline Courses at 16th International Conference onArtificial Intelligence in Education (AIED), Mem-phis, TN, pages 41?48.Sarah Gielen, Elien Peeters, Filip Dochy, PatrickOnghena, and Katrien Struyven.
2010.
Improv-ing the effectiveness of peer feedback for learning.Learning and Instruction, 20(4):304?315.Andrew B. Goldberg, Nathanael Fillmore, David An-drzejewski, Zhiting Xu, Bryan Gibson, and Xiao-jin Zhu.
2009.
May all your wishes come true: Astudy of wishes and how to recognize them.
In Pro-ceedings Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,NAACL-HTL, pages 263?271.Jordan Lippman, Mike Elfenbein, Matthew Diabes,Cori Luchau, Collin Lynch, Kevin Ashley, and ChrisSchunn.
2012.
To revise or not to revise: Whatinfluences undergrad authors to implement peer cri-tiques of their argument diagrams?
In InternationalSociety for the Psychology of Science and Technol-ogy 2012 Conference.
Poster.Melissa M. Nelson and Christian D. Schunn.
2009.The nature of feedback: how different types of peerfeedback affect writing performance.
InstructionalScience, 37(4):375?401.Huy V. Nguyen and Diane J. Litman.
July 2013.
Iden-tifying localization in peer reviews of argument di-agrams.
In Proceedings 16th International Confer-ence on Artificial Intelligence in Education (AIED),Memphis, TN, pages 91?100.Huy Nguyen, Wenting Xiong, and Diane Litman.
June2014.
Classroom evaluation of a scaffolding inter-vention for improving peer review localization.
InProceedings 12th International Conference on Intel-ligent Tutoring Systems (ITS), Honolulu, HI, pages272?282.Lakshmi Ramachandran and Edward F. Gehringer.2011.
Automated assessment of review quality us-ing latent semantic analysis.
In Proceedings 11thIEEE International Conference on Advanced Learn-ing Technologies (ICALT), pages 136?138.J.
Ramanand, Krishna Bhavsar, and NiranjanPedanekar.
2010.
Wishful thinking: findingsuggestions and ?buy?
wishes from product reviews.In Proceedings the NAACL-HLT 2010 Workshopon Computational Approaches to Analysis andGeneration of Emotion in Text, pages 54?61.Oliver Scheuer, Frank Loll, Niels Pinkwart, andBruce M. McLaren.
2010.
Computer-supported ar-gumentation: A review of the state of the art.
In-ternational Journal of Computer-Supported Collab-orative Learning, 5(1):43?102.Pontus Stenetorp, Sampo Pyysalo, Goran Topic,Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2012.
Brat: a web-based tool for nlp-assistedtext annotation.
In Proceedings the Demonstra-tions at the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 102?107.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Joint Human LanguageTechnology Conference and the Conference on Em-pirical Methods in Natural Language Processing(HLT-EMNLP), pages 347?354.Wenting Xiong and Diane Litman.
2010.
Identify-ing problem localization in peer-review feedback.
InProceedings 10th International Conference on Intel-ligent Tutoring System (ITS), Pittsburgh, PA. Poster.Wenting Xiong, Diane Litman, and Christian Schunn.2012.
Natural language processing techniques forresearching and improving peer feedback.
Journalof Writing Research, 4(2):155?176.Wenting Xiong, Diane Litman, and Christian Schunn.June 2010.
Assessing reviewer?s performance basedon mining problem localization in peer-review data.In Proceedings 3rd International Conference onEducational Data Mining (EDM), Pittsburgh, PA,pages 211?220.108
