Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 363?371,Honolulu, October 2008. c?2008 Association for Computational LinguisticsStudying the History of Ideas Using Topic ModelsDavid HallSymbolic SystemsStanford UniversityStanford, CA 94305, USAdlwh@stanford.eduDaniel JurafskyLinguisticsStanford UniversityStanford, CA 94305, USAjurafsky@stanford.eduChristopher D. ManningComputer ScienceStanford UniversityStanford, CA 94305, USAmanning@stanford.eduAbstractHow can the development of ideas in a sci-entific field be studied over time?
We ap-ply unsupervised topic modeling to the ACLAnthology to analyze historical trends in thefield of Computational Linguistics from 1978to 2006.
We induce topic clusters using LatentDirichlet Allocation, and examine the strengthof each topic over time.
Our methods findtrends in the field including the rise of prob-abilistic methods starting in 1988, a steady in-crease in applications, and a sharp decline ofresearch in semantics and understanding be-tween 1978 and 2001, possibly rising againafter 2001.
We also introduce a model of thediversity of ideas, topic entropy, using it toshow that COLING is a more diverse confer-ence than ACL, but that both conferences aswell as EMNLP are becoming broader overtime.
Finally, we apply Jensen-Shannon di-vergence of topic distributions to show that allthree conferences are converging in the topicsthey cover.1 IntroductionHow can we identify and study the exploration ofideas in a scientific field over time, noting periods ofgradual development, major ruptures, and the wax-ing and waning of both topic areas and connectionswith applied topics and nearby fields?
One im-portant method is to make use of citation graphs(Garfield, 1955).
This enables the use of graph-based algorithms like PageRank for determining re-searcher or paper centrality, and examining whethertheir influence grows or diminishes over time.However, because we are particularly interestedin the change of ideas in a field over time, we havechosen a different method, following Kuhn (1962).In Kuhn?s model of scientific change, science pro-ceeds by shifting from one paradigm to another.Because researchers?
ideas and vocabulary are con-strained by their paradigm, successive incommensu-rate paradigms will naturally have different vocabu-lary and framing.Kuhn?s model is intended to apply only to verylarge shifts in scientific thought rather than at themicro level of trends in research foci.
Nonetheless,we propose to apply Kuhn?s insight that vocabularyand vocabulary shift is a crucial indicator of ideasand shifts in ideas.
Our operationalization of this in-sight is based on the unsupervised topic model La-tent Dirichlet Allocation (LDA; Blei et al (2003)).For many fields, doing this kind of historical studywould be very difficult.
Computational linguisticshas an advantage, however: the ACL Anthology, apublic repository of all papers in the ComputationalLinguistics journal and the conferences and work-shops associated with the ACL, COLING, EMNLP,and so on.
The ACL Anthology (Bird, 2008), andcomprises over 14,000 documents from conferencesand the journal, beginning as early as 1965 through2008, indexed by conference and year.
This re-source has already been the basis of citation anal-ysis work, for example, in the ACL Anthology Net-work of Joseph and Radev (2007).
We apply LDAto the text of the papers in the ACL Anthology toinduce topics, and use the trends in these topics overtime and over conference venues to address ques-tions about the development of the field.363Venue # Papers Years FrequencyJournal 1291 1974?Present QuarterlyACL 2037 1979-Present YearlyEACL 596 1983?Present ?2 YearsNAACL 293 2000?Present ?YearlyApplied NLP 346 1983?2000 ?3 YearsCOLING 2092 1965-Present 2 YearsHLT 957 1986?Present ?2 YearsWorkshops 2756 1990-Present YearlyTINLAP 128 1975?1987 RarelyMUC 160 1991?1998 ?2 YearsIJCNLP 143 2005 ?
?Other 120 ??
?
?Table 1: Data in the ACL AnthologyDespite the relative youth of our field, computa-tional linguistics has witnessed a number of researchtrends and shifts in focus.
While some trends areobvious (such as the rise in machine learning meth-ods), others may be more subtle.
Has the field got-ten more theoretical over the years or has there beenan increase in applications?
What topics have de-clined over the years, and which ones have remainedroughly constant?
How have fields like Dialogue orMachine Translation changed over the years?
Arethere differences among the conferences, for exam-ple between COLING and ACL, in their interestsand breadth of focus?
As our field matures, it is im-portant to go beyond anecdotal description to givegrounded answers to these questions.
Such answerscould also help give formal metrics to model the dif-ferences between the many conferences and venuesin our field, which could influence how we thinkabout reviewing, about choosing conference topics,and about long range planning in our field.2 Methodology2.1 DataThe analyses in this paper are based on a text-only version of the Anthology that comprises some12,500 papers.
The distribution of the Anthologydata is shown in Table 1.2.2 Topic ModelingOur experiments employ Latent Dirichlet Allocation(LDA; Blei et al (2003)), a generative latent variablemodel that treats documents as bags of words gener-ated by one or more topics.
Each document is char-acterized by a multinomial distribution over topics,and each topic is in turn characterized by a multino-mial distribution over words.
We perform parame-ter estimation using collapsed Gibbs sampling (Grif-fiths and Steyvers, 2004).Possible extensions to this model would be to in-tegrate topic modelling with citations (e.g., Dietz etal.
(2007), Mann et al (2006), and Jo et al (2007)).Another option is the use of more fine-grained or hi-erarchical model (e.g., Blei et al (2004), and Li andMcCallum (2006)).All our studies measure change in various as-pects of the ACL Anthology over time.
LDA, how-ever, does not explicitly model temporal relation-ships.
One way to model temporal relationships isto employ an extension to LDA.
The Dynamic TopicModel (Blei and Lafferty, 2006), for example, rep-resents each year?s documents as generated from anormal distribution centroid over topics, with thefollowing year?s centroid generated from the pre-ceding year?s.
The Topics over Time Model (Wangand McCallum, 2006) assumes that each documentchooses its own time stamp based on a topic-specificbeta distribution.Both of these models, however, impose con-straints on the time periods.
The Dynamic TopicModel penalizes large changes from year to yearwhile the beta distributions in Topics over Time arerelatively inflexible.
We chose instead to performpost hoc calculations based on the observed proba-bility of each topic given the current year.
We definep?
(z|y) as the empirical probability that an arbitrarypaper d written in year y was about topic z:p?
(z|y) =?d:td=yp?(z|d)p?(d|y)=1C?d:td=yp?
(z|d)=1C?d:td=y?z?i?dI(z?i = z)(1)where I is the indicator function, td is the date docu-ment d was written, p?
(d|y) is set to a constant 1/C.3 Summary of TopicsWe first ran LDA with 100 topics, and took 36 thatwe found to be relevant.
We then hand-selected seed36400.05 0.10.15 0.2198019851990199520002005ClassificationProbabilisticModelsStat.
Parsing Stat.
MTLex.
SemFigure 1: Topics in the ACL Anthology that show astrong recent increase in strength.words for 10 more topics to improve coverage of thefield.
These 46 topics were then used as priors to anew 100-topic run.
The top ten most frequent wordsfor 43 of the topics along with hand-assigned labelsare listed in Table 2.
Topics deriving from manualseeds are marked with an asterisk.4 Historical Trends in ComputationalLinguisticsGiven the space of possible topics defined in the pre-vious section, we now examine the history of thesein the entire ACL Anthology from 1978 until 2006.To visualize some trends, we show the probabilitymass associated with various topics over time, plot-ted as (a smoothed version of) p?
(z|y).4.1 Topics Becoming More ProminentFigure 1 shows topics that have becomemore promi-nent more recently.Of these new topics, the rise in probabilistic mod-els and classification/tagging is unsurprising.
In or-der to distinguish these two topics, we show 20 ofthe strongly weighted words:Probabilistic Models: model word probability set datanumber algorithm language corpus method figure proba-bilities table test statistical distribution function al valuesperformanceClassification/Tagging: features data corpus set featuretable word tag al test accuracy pos classification perfor-mance tags tagging text task information classSome of the papers with the highest weights forthe probabilistic models class include:N04-1039 Goodman, Joshua.
Exponential Priors For MaximumEntropy Models (HLT-NAACL, 2004)W97-0309 Saul, Lawrence, Pereira, Fernando C. N. Aggregate AndMixed-Order Markov Models For Statistical LanguageProcessing (EMNLP, 1997)P96-1041 Chen, Stanley F., Goodman, Joshua.
An EmpiricalStudy Of Smoothing Techniques For Language Model-ing (ACL, 1996)H89-2013 Church, Kenneth Ward, Gale, William A. EnhancedGood-Turing And CatCal: Two New Methods For Esti-mating Probabilities Of English Bigrams (Workshop OnSpeech And Natural Language, 1989)P02-1023 Gao, Jianfeng, Zhang, Min Improving Language ModelSize Reduction Using Better Pruning Criteria (ACL,2002)P94-1038 Dagan, Ido, Pereira, Fernando C. N. Similarity-BasedEstimation Of Word Cooccurrence Probabilities (ACL,1994)Some of the papers with the highest weights forthe classification/tagging class include:W00-0713 Van Den Bosch, Antal Using Induced Rules As Com-plex Features In Memory-Based Language Learning(CoNLL, 2000)W01-0709 Estabrooks, Andrew, Japkowicz, Nathalie AMixture-Of-Experts Framework For Text Classification (WorkshopOn Computational Natural Language Learning CoNLL,2001)A00-2035 Mikheev, Andrei.
Tagging Sentence Boundaries (ANLP-NAACL, 2000)H92-1022 Brill, Eric.
A Simple Rule-Based Part Of Speech Tagger(Workshop On Speech And Natural Language, 1992)As Figure 1 shows, probabilistic models seem tohave arrived significantly before classifiers.
Theprobabilistic model topic increases around 1988,which seems to have been an important year forprobabilistic models, including high-impact paperslike A88-1019 and C88-1016 below.
The ten papersfrom 1988 with the highest weights for the proba-bilistic model and classifier topics were the follow-ing:C88-1071 Kuhn, Roland.
Speech Recognition and the Frequencyof Recently Used Words (COLING)J88-1003 DeRose, Steven.
Grammatical Category Disambiguationby Statistical Optimization.
(CL Journal)C88-2133 Su, Keh-Yi, and Chang, Jing-Shin.
Semantic and Syn-tactic Aspects of Score Function.
(COLING)A88-1019 Church, Kenneth Ward.
A Stochastic Parts Program andNoun Phrase Parser for Unrestricted Text.
(ANLP)C88-2134 Sukhotin, B.V. Optimization Algorithms of Decipheringas the Elements of a Linguistic Theory.
(COLING)P88-1013 Haigh, Robin, Sampson, Geoffrey, and Atwell, Eric.Project APRIL: a progress report.
(ACL)A88-1005 Boggess, Lois.
Two Simple Prediction Algorithms to Fa-cilitate Text Production.
(ANLP)C88-1016 Peter F. Brown, et al A Statistical Approach to MachineTranslation.
(COLING)A88-1028 Oshika, Beatrice, et al.
Computational Techniques forImproved Name Search.
(ANLP)C88-1020 Campbell, W.N.
Speech-rate Variation and the Predictionof Duration.
(COLING)What do these early papers tell us about how365Anaphora Resolution resolution anaphora pronoun discourse antecedent pronouns coreference reference definite algorithmAutomata string state set finite context rule algorithm strings language symbolBiomedical medical protein gene biomedical wkh abstracts medline patient clinical biologicalCall Routing call caller routing calls destination vietnamese routed router destinations gorinCategorial Grammar proof formula graph logic calculus axioms axiom theorem proofs lambekCentering* centering cb discourse cf utterance center utterances theory coherence entities localClassical MT japanese method case sentence analysis english dictionary figure japan wordClassification/Tagging features data corpus set feature table word tag al testComp.
Phonology vowel phonological syllable phoneme stress phonetic phonology pronunciation vowels phonemesComp.
Semantics* semantic logical semantics john sentence interpretation scope logic form setDialogue Systems user dialogue system speech information task spoken human utterance languageDiscourse Relations discourse text structure relations rhetorical relation units coherence texts rstDiscourse Segment.
segment segmentation segments chain chains boundaries boundary seg cohesion lexicalEvents/Temporal event temporal time events tense state aspect reference relations relationFrench Function de le des les en une est du par pourGeneration generation text system language information knowledge natural figure domain inputGenre Detection genre stylistic style genres fiction humor register biber authorship registersInfo.
Extraction system text information muc extraction template names patterns pattern domainInformation Retrieval document documents query retrieval question information answer term text webLexical Semantics semantic relations domain noun corpus relation nouns lexical ontology patternsMUC Terrorism slot incident tgt target id hum phys type fills perpMetaphor metaphor literal metonymy metaphors metaphorical essay metonymic essays qualia analogyMorphology word morphological lexicon form dictionary analysis morphology lexical stem arabicNamed Entities* entity named entities ne names ner recognition ace nes mentions mentionParaphrase/RTE paraphrases paraphrase entailment paraphrasing textual para rte pascal entailed daganParsing parsing grammar parser parse rule sentence input left grammars npPlan-Based Dialogue plan discourse speaker action model goal act utterance user informationProbabilistic Models model word probability set data number algorithm language corpus methodProsody prosodic speech pitch boundary prosody phrase boundaries accent repairs intonationSemantic Roles* semantic verb frame argument verbs role roles predicate argumentsYale School Semantics knowledge system semantic language concept representation information network concepts baseSentiment subjective opinion sentiment negative polarity positive wiebe reviews sentence opinionsSpeech Recognition speech recognition word system language data speaker error test spokenSpell Correction errors error correction spelling ocr correct corrections checker basque corrected detectionStatistical MT english word alignment language source target sentence machine bilingual mtStatistical Parsing dependency parsing treebank parser tree parse head model al npSummarization sentence text evaluation document topic summary summarization human summaries scoreSyntactic Structure verb noun syntactic sentence phrase np subject structure case clauseTAG Grammars* tree node trees nodes derivation tag root figure adjoining grammarUnification feature structure grammar lexical constraints unification constraint type structures ruleWSD* word senses wordnet disambiguation lexical semantic context similarity dictionaryWord Segmentation chinese word character segmentation corpus dictionary korean language table systemWordNet* synset wordnet synsets hypernym ili wordnets hypernyms eurowordnet hyponym ewn wnTable 2: Top 10 words for 43 of the topics.
Starred topics are hand-seeded.36600.05 0.10.15 0.2198019851990199520002005Computational SemanticsConceptualSemanticsPlan-Based Dialogueand DiscourseFigure 2: Topics in the ACL Anthology that show astrong decline from 1978 to 2006.probabilistic models and classifiers entered thefield?
First, not surprisingly, we note that the vastmajority (9 of 10) of the papers appeared in con-ference proceedings rather than the journal, con-firming that in general new ideas appear in confer-ences.
Second, of the 9 conference papers, mostof them appeared in the COLING conference (5) orthe ANLP workshop (3) compared to only 1 in theACL conference.
This suggests that COLING mayhave been more receptive than ACL to new ideasat the time, a point we return to in Section 6.
Fi-nally, we examined the background of the authors ofthese papers.
Six of the 10 papers either focus onspeech (C88-1010, A88-1028, C88-1071) or werewritten by authors who had previously published onspeech recognition topics, including the influentialIBM (Brown et al) and AT&T (Church) labs (C88-1016, A88-1005, A88-1019).
Speech recognitionis historically an electrical engineering field whichmade quite early use of probabilistic and statisticalmethodologies.
This suggests that researchers work-ing on spoken language processing were an impor-tant conduit for the borrowing of statistical method-ologies into computational linguistics.4.2 Topics That Have DeclinedFigure 2 shows several topics that were more promi-nent at the beginning of the ACL but which haveshown the most precipitous decline.
Papers stronglyassociated with the plan-based dialogue topic in-clude:J99-1001 Carberry, Sandra, Lambert, Lynn.
A Process Model ForRecognizing Communicative Acts And Modeling Nego-tiation Subdialogues (CL, 1999)J95-4001 McRoy, Susan W., Hirst, Graeme.
The Repair Of SpeechAct Misunderstandings By Abductive Inference (CL,1995)P93-1039 Chu, Jennifer.
Responding To User Queries In A Collab-orative Environment (ACL, 1993)P86-1032 Pollack, Martha E. A Model Of Plan Inference ThatDistinguishes Between The Beliefs Of Actors And Ob-servers (ACL, 1986)T78-1017 Perrault, Raymond C., Allen, James F. Speech Acts AsA Basis For Understanding Dialogue Coherence (Theo-retical Issues In Natural Language Processing, 1978)P84-1063 Litman, Diane J., Allen, James F. A Plan RecognitionModel For Clarification Subdialogues (COLING-ACL,1984)Papers strongly associated with the computationalsemantics topic include:J90-4002 Haas, Andrew R. Sentential Semantics For PropositionalAttitudes (CL, 1990)P83-1009 Hobbs, Jerry R. An Improper Treatment Of Quantifica-tion In Ordinary English (ACL, 1983)J87-1005 Hobbs, Jerry R., Shieber, Stuart M. An Algorithm ForGenerating Quantifier Scopings (CL, 1987)C90-1003 Johnson, Mark, Kay, Martin.
Semantic Abstraction AndAnaphora (COLING, 1990)P89-1004 Alshawi, Hiyan, Van Eijck, Jan.
Logical Forms In TheCore Language Engine (ACL, 1989)Papers strongly associated with the conceptual se-mantics/story understanding topic include:C80-1022 Ogawa, Hitoshi, Nishi, Junichiro, Tanaka, Kokichi.
TheKnowledge Representation For A Story UnderstandingAnd Simulation System (COLING, 1980)A83-1012 Pazzani, Michael J., Engelman, Carl.
Knowledge BasedQuestion Answering (ANLP, 1983)P82-1029 McCoy, Kathleen F. Augmenting A Database Knowl-edge Representation For Natural Language Generation(ACL, 1982)H86-1010 Ksiezyk, Tomasz, Grishman, Ralph An EquipmentModel And Its Role In The Interpretation Of NominalCompounds (Workshop On Strategic Computing - Natu-ral Language, 1986)P80-1030 Wilensky, Robert, Arens, Yigal.
PHRAN - AKnowledge-Based Natural Language Understander(ACL, 1980)A83-1013 Boguraev, Branimir K., Sparck Jones, Karen.
How ToDrive A Database Front End Using General Semantic In-formation (ANLP, 1983)P79-1003 Small, Steven L. Word Expert Parsing (ACL, 1979)The declines in both computational semantics andconceptual semantics/story understanding suggeststhat it is possible that the entire field of natural lan-guage understanding and computational semanticsbroadly construed has fallen out of favor.
To seeif this was in fact the case we created a metatopiccalled semantics in which we combined various se-mantics topics (not including pragmatic topics likeanaphora resolution or discourse coherence) includ-ing: lexical semantics, conceptual semantics/story36700.05 0.10.15 0.20.25198019851990199520002005SemanticsFigure 3: Semantics over timeunderstanding, computational semantics, WordNet,word sense disambiguation, semantic role labeling,RTE and paraphrase, MUC information extraction,and events/temporal.
We then plotted p?
(z ?
S|y),the sum of the proportions per year for these top-ics, as shown in Figure 3.
The steep decrease in se-mantics is readily apparent.
The last few years hasshown a levelling off of the decline, and possibly arevival of this topic; this possibility will need to beconfirmed as we add data from 2007 and 2008.We next chose two fields, Dialogue and MachineTranslation, in which it seemed to us that the topicsdiscovered by LDA suggested a shift in paradigms inthese fields.
Figure 4 shows the shift in translation,while Figure 5 shows the change in dialogue.The shift toward statistical machine translation iswell known, at least anecdotally.
The shift in di-alogue seems to be a move toward more applied,speech-oriented, or commercial dialogue systemsand away from more theoretical models.Finally, Figure 6 shows the history of several top-ics that peaked at intermediate points throughout thehistory of the field.
We can see the peak of unifica-tion around 1990, of syntactic structure around 1985of automata in 1985 and again in 1997, and of wordsense disambiguation around 1998.5 Is Computational Linguistics BecomingMore Applied?We don?t know whether our field is becoming moreapplied, or whether perhaps there is a trend to-wards new but unapplied theories.
We therefore00.05 0.10.15 0.2198019851990199520002005Statistical MTClassical MTFigure 4: Translation over time00.05 0.10.15 0.2198019851990199520002005Dialogue SystemsPlan-Based Dialogueand DiscourseFigure 5: Dialogue over time00.05 0.10.15 0.2198019851990199520002005TAGGeneration AutomataUnificationSyntactic Structure Events WSDFigure 6: Peaked topics36800.05 0.10.15 0.20.25198019851990199520002005ApplicationsFigure 7: Applications over time00.05 0.10.15 0.2198019851990199520002005Statistical MTDialogue SystemsSpelling CorrectionCall RoutingSpeech RecognitionBiomedicalFigure 8: Six applied topics over timelooked at trends over time for the following appli-cations: Machine Translation, Spelling Correction,Dialogue Systems, Information Retrieval, Call Rout-ing, Speech Recognition, and Biomedical applica-tions.Figure 7 shows a clear trend toward an increasein applications over time.
The figure also shows aninteresting bump near 1990.
Why was there sucha sharp temporary increase in applications at thattime?
Figure 8 shows details for each application,making it clear that the bump is caused by a tempo-rary spike in the Speech Recognition topic.In order to understand why we see this temporaryspike, Figure 9 shows the unsmoothed values of theSpeech Recognition topic prominence over time.Figure 9 clearly shows a huge spike for the years1989?1994.
These years correspond exactly to theDARPA Speech and Natural Language Workshop,00.05 0.10.15 0.2198019851990199520002005Figure 9: Speech recognition over timeheld at different locations from 1989?1994.
Thatworkshop contained a significant amount of speechuntil its last year (1994), and then it was revivedin 2001 as the Human Language Technology work-shop with a much smaller emphasis on speech pro-cessing.
It is clear from Figure 9 that there is stillsome speech research appearing in the Anthologyafter 1995, certainly more than the period before1989, but it?s equally clear that speech recognitionis not an application that the ACL community hasbeen successful at attracting.6 Differences and Similarities AmongCOLING, ACL, and EMNLPThe computational linguistics community has twodistinct conferences, COLING and ACL, with dif-ferent histories, organizing bodies, and philoso-phies.
Traditionally, COLING was larger, with par-allel sessions and presumably a wide variety of top-ics, while ACL had single sessions and a more nar-row scope.
In recent years, however, ACL hasmoved to parallel sessions, and the conferences areof similar size.
Has the distinction in breadth of top-ics also been blurred?
What are the differences andsimilarities in topics and trends between these twoconferences?More recently, the EMNLP conference grew outof the Workshop on Very Large Corpora, sponsoredby the Special Interest Group on Linguistic Dataand corpus-based approaches to NLP (SIGDAT).EMNLP started as a much smaller and narrower369conference but more recently, while still smallerthan both COLING and ACL, it has grown largeenough to be considered with them.
How does thebreadth of its topics compare with the others?Our hypothesis, based on our intuitions as con-ference attendees, is that ACL is still more narrowin scope than COLING, but has broadened consid-erably.
Similarly, our hypothesis is that EMNLP hasbegun to broaden considerably as well, although notto the extent of the other two.In addition, we?re interested in whether the topicsof these conferences are converging or not.
Are theprobabilistic and machine learning trends that aredominant in ACL becoming dominant in COLINGas well?
Is EMNLP adopting some of the topics thatare popular at COLING?To investigate both of these questions, we need amodel of the topic distribution for each conference.We define the empirical distribution of a topic z at aconference c, denoted by p?
(z|c) by:p?
(z|c) =?d:cd=cp?(z|d)p?(d|c)=1C?d:cd=cp?
(z|d)=1C?d:cd=c?z?i?dI(z?i = z)(2)We also condition on the year for each conference,giving us p?
(z|y, c).We propose to measure the breadth of a confer-ence by using what we call topic entropy: the condi-tional entropy of this conference topic distribution.Entropy measures the average amount of informa-tion expressed by each assignment to a random vari-able.
If a conference has higher topic entropy, then itmore evenly divides its probability mass across thegenerated topics.
If it has lower, it has a far morenarrow focus on just a couple of topics.
We there-fore measured topic entropy:H(z|c, y) = ?K?i=1p?
(zi|c, y) log p?
(zi|c, y) (3)Figure 10 shows the conditional topic entropyof each conference over time.
We removed fromthe ACL and COLING lines the years when ACL3.63.8 44.24.44.64.8 55.25.45.6  198019851990199520002005ACL Conference COLINGEMNLPJointCOLING/ACLFigure 10: Entropy of the three major conferences peryearand COLING are colocated (1984, 1998, 2006),and marked those colocated years as points separatefrom either plot.
As expected, COLING has beenhistorically the broadest of the three conferences,though perhaps slightly less so in recent years.
ACLstarted with a fairly narrow focus, but became nearlyas broad as COLING during the 1990?s.
However, inthe past 8 years it has become more narrow again,with a steeper decline in breadth than COLING.EMNLP, true to its status as a ?Special Interest?
con-ference, began as a very narrowly focused confer-ence, but now it seems to be catching up to at leastACL in terms of the breadth of its focus.Since the three major conferences seem to be con-verging in terms of breadth, we investigated whetheror not the topic distributions of the conferences werealso converging.
To do this, we plotted the Jensen-Shannon (JS) divergence between each pair of con-ferences.
The Jensen-Shannon divergence is a sym-metric measure of the similarity of two pairs of dis-tributions.
The measure is 0 only for identical dis-tributions and approaches infinity as the two differmore and more.
Formally, it is defined as the aver-age of the KL divergence of each distribution to theaverage of the two distributions:DJS(P ||Q) =12DKL(P ||R) +12DKL(Q||R)R =12(P + Q)(4)Figure 11 shows the JS divergence between eachpair of conferences over time.
Note that EMNLP37000.05 0.10.15 0.20.25 0.30.35 0.40.45 0.5198019851990199520002005ACL/COLINGACL/EMNLPEMNLP/COLINGFigure 11: JS Divergence between the three major con-ferencesand COLING have historically met very infre-quently in the same year, so those similarity scoresare plotted as points and not smoothed.
The trendacross all three conferences is clear: each confer-ence is not only increasing in breadth, but also insimilarity.
In particular, EMNLP and ACL?s differ-ences, once significant, are nearly erased.7 ConclusionOur method discovers a number of trends in thefield, such as the general increase in applications,the steady decline in semantics, and its possible re-versal.
We also showed a convergence over time intopic coverage of ACL, COLING, and EMNLP aswell an expansion of topic diversity.
This growthand convergence of the three conferences, perhapsinfluenced by the need to increase recall (Church,2005) seems to be leading toward a tripartite real-ization of a single new ?latent?
conference.AcknowledgmentsMany thanks to Bryan Gibson and Dragomir Radevfor providing us with the data behind the ACL An-thology Network.
Also to Sharon Goldwater and theother members of the Stanford NLP Group as wellas project Mimir for helpful advice.
Finally, manythanks to the Office of the President, Stanford Uni-versity, for partial funding.ReferencesSteven Bird.
2008.
Association of Computational Lin-guists Anthology.
http://www.aclweb.org/anthology-index/.David Blei and John D. Lafferty.
2006.
Dynamic topicmodels.
ICML.David Blei, Andrew Ng, , and Michael Jordan.
2003.
La-tent Dirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022.D.
Blei, T. Gri, M. Jordan, and J. Tenenbaum.
2004.
Hi-erarchical topic models and the nested Chinese restau-rant process.Kenneth Church.
2005.
Reviewing the reviewers.
Com-put.
Linguist., 31(4):575?578.Laura Dietz, Steffen Bickel, and Tobias Scheffer.
2007.Unsupervised prediction of citation influences.
InICML, pages 233?240.
ACM.Eugene Garfield.
1955.
Citation indexes to science: Anew dimension in documentation through associationof ideas.
Science, 122:108?111.Tom L. Griffiths and Mark Steyvers.
2004.
Finding sci-entific topics.
PNAS, 101 Suppl 1:5228?5235, April.Yookyung Jo, Carl Lagoze, and C. Lee Giles.
2007.Detecting research topics via the correlation betweengraphs and texts.
In KDD, pages 370?379, New York,NY, USA.
ACM.Mark T. Joseph and Dragomir R. Radev.
2007.
Citationanalysis, centrality, and the ACL anthology.
Techni-cal Report CSE-TR-535-07, University of Michigan.Department of Electrical Engineering and ComputerScience.Thomas S. Kuhn.
1962.
The Structure of Scientific Rev-olutions.
University Of Chicago Press.Wei Li and Andrew McCallum.
2006.
Pachinko alloca-tion: DAG-structured mixture models of topic correla-tions.
In ICML, pages 577?584, New York, NY, USA.ACM.Gideon S. Mann, David Mimno, and Andrew McCal-lum.
2006.
Bibliometric impact measures leveragingtopic analysis.
In JCDL ?06: Proceedings of the 6thACM/IEEE-CS joint conference on Digital libraries,pages 65?74, New York, NY, USA.
ACM.Xuerui Wang and Andrew McCallum.
2006.
Topics overtime: a non-Markov continuous-time model of topicaltrends.
In KDD, pages 424?433, New York, NY, USA.ACM.371
