Statistical Machine Translation withWord- and Sentence-Aligned Parallel CorporaChris Callison-Burch David Talbot Miles OsborneSchool on InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWcallison-burch@ed.ac.ukAbstractThe parameters of statistical translation models aretypically estimated from sentence-aligned parallelcorpora.
We show that significant improvements inthe alignment and translation quality of such mod-els can be achieved by additionally including word-aligned data during training.
Incorporating word-level alignments into the parameter estimation ofthe IBM models reduces alignment error rate andincreases the Bleu score when compared to trainingthe same models only on sentence-aligned data.
Onthe Verbmobil data set, we attain a 38% reductionin the alignment error rate and a higher Bleu scorewith half as many training examples.
We discusshow varying the ratio of word-aligned to sentence-aligned data affects the expected performance gain.1 IntroductionMachine translation systems based on probabilistictranslation models (Brown et al, 1993) are gener-ally trained using sentence-aligned parallel corpora.For many language pairs these exist in abundantquantities.
However for new domains or uncommonlanguage pairs extensive parallel corpora are oftenhard to come by.Two factors could increase the performance ofstatistical machine translation for new languagepairs and domains: a reduction in the cost of cre-ating new training data, and the development ofmore efficient methods for exploiting existing train-ing data.
Approaches such as harvesting parallelcorpora from the web (Resnik and Smith, 2003)address the creation of data.
We take the second,complementary approach.
We address the prob-lem of efficiently exploiting existing parallel cor-pora by adding explicit word-level alignments be-tween a number of the sentence pairs in the train-ing corpus.
We modify the standard parameter esti-mation procedure for IBM Models and HMM vari-ants so that they can exploit these additional word-level alignments.
Our approach uses both word- andsentence-level alignments for training material.In this paper we:1.
Describe how the parameter estimation frame-work of Brown et al (1993) can be adapted toincorporate word-level alignments;2.
Report significant improvements in alignmenterror rate and translation quality when trainingon data with word-level alignments;3.
Demonstrate that the inclusion of word-levelalignments is more effective than using a bilin-gual dictionary;4.
Show the importance of amplifying the contri-bution of word-aligned data during parameterestimation.This paper shows that word-level alignments im-prove the parameter estimates for translation mod-els, which in turn results in improved statisticaltranslation for languages that do not have largesentence-aligned parallel corpora.2 Parameter Estimation UsingSentence-Aligned CorporaThe task of statistical machine translation is tochoose the source sentence, e, that is the most prob-able translation of a given sentence, f , in a for-eign language.
Rather than choosing e?
that di-rectly maximizes p(e|f), Brown et al (1993) applyBayes?
rule and select the source sentence:e?
= argmaxep(e)p(f |e).
(1)In this equation p(e) is a language model probabil-ity and is p(f |e) a translation model probability.
Aseries of increasingly sophisticated translation mod-els, referred to as the IBM Models, was defined inBrown et al (1993).The translation model, p(f |e) defined as amarginal probability obtained by summing overword-level alignments, a, between the source andtarget sentences:p(f |e) =?ap(f ,a|e).
(2)While word-level alignments are a crucial com-ponent of the IBM models, the model parame-ters are generally estimated from sentence-alignedparallel corpora without explicit word-level align-ment information.
The reason for this is thatword-aligned parallel corpora do not generally ex-ist.
Consequently, word level alignments are treatedas hidden variables.
To estimate the values ofthese hidden variables, the expectation maximiza-tion (EM) framework for maximum likelihood esti-mation from incomplete data is used (Dempster etal., 1977).The previous section describes how the trans-lation probability of a given sentence pair is ob-tained by summing over all alignments p(f |e) =?a p(f ,a|e).
EM seeks to maximize the marginallog likelihood, log p(f |e), indirectly by iterativelymaximizing a bound on this term known as the ex-pected complete log likelihood, ?log p(f ,a|e)?q(a),1log p(f |e) = log?ap(f ,a|e) (3)= log?aq(a)p(f ,a|e)q(a)(4)?
?aq(a) logp(f ,a|e)q(a)(5)= ?log p(f ,a|e)?q(a) + H(q(a))where the bound in (5) is given by Jensen?s inequal-ity.
By choosing q(a) = p(a|f , e) this bound be-comes an equality.This maximization consists of two steps:?
E-step: calculate the posterior probabilityunder the current model of every permissi-ble alignment for each sentence pair in thesentence-aligned training corpus;?
M-step: maximize the expected log like-lihood under this posterior distribution,?log p(f ,a|e)?q(a), with respect to the model?sparameters.While in standard maximum likelihood estima-tion events are counted directly to estimate param-eter settings, in EM we effectively collect frac-tional counts of events (here permissible alignmentsweighted by their posterior probability), and usethese to iteratively update the parameters.1Here ?
??q(?)
denotes an expectation with respect to q(?
).Since only some of the permissible alignmentsmake sense linguistically, we would like EM to usethe posterior alignment probabilities calculated inthe E-step to weight plausible alignments higherthan the large number of bogus alignments whichare included in the expected complete log likeli-hood.
This in turn should encourage the parame-ter adjustments made in the M-step to converge tolinguistically plausible values.Since the number of permissible alignments fora sentence grows exponentially in the length of thesentences for the later IBM Models, a large num-ber of informative example sentence pairs are re-quired to distinguish between plausible and implau-sible alignments.
Given sufficient data the distinc-tion occurs because words which are mutual trans-lations appear together more frequently in alignedsentences in the corpus.Given the high number of model parameters andpermissible alignments, however, huge amounts ofdata will be required to estimate reasonable transla-tion models from sentence-aligned data alone.3 Parameter Estimation Using Word- andSentence-Aligned CorporaAs an alternative to collecting a huge amount ofsentence-aligned training data, by annotating someof our sentence pairs with word-level alignmentswe can explicitly provide information to highlightplausible alignments and thereby help parametersconverge upon reasonable settings with less trainingdata.Since word-alignments are inherent in the IBMtranslation models it is straightforward to incorpo-rate this information into the parameter estimationprocedure.
For sentence pairs with explicit word-level alignments marked, fractional counts over allpermissible alignments need not be collected.
In-stead, whole counts are collected for the single handannotated alignment for each sentence pair whichhas been word-aligned.
By doing this the expectedcomplete log likelihood collapses to a single term,the complete log likelihood (p(f ,a|e)), and the E-step is circumvented.The parameter estimation procedure now in-volves maximizing the likelihood of data alignedonly at the sentence level and also of data alignedat the word level.
The mixed likelihood function,M, combines the expected information containedin the sentence-aligned data with the complete in-formation contained in the word-aligned data.M =Ns?s=1(1?
?
)?log p(fs,as|es)?q(as)+Nw?w=1?
log p(fw,aw|ew) (6)Here s and w index the Ns sentence-aligned sen-tences and Nw word-aligned sentences in our cor-pora respectively.
Thus M combines the expectedcomplete log likelihood and the complete log likeli-hood.
In order to control the relative contributionsof the sentence-aligned and word-aligned data inthe parameter estimation procedure, we introduce amixing weight ?
that can take values between 0 and1.3.1 The impact of word-level alignmentsThe impact of word-level alignments on parameterestimation is closely tied to the structure of the IBMModels.
Since translation and word alignment pa-rameters are shared between all sentences, the pos-terior alignment probability of a source-target wordpair in the sentence-aligned section of the corpusthat were aligned in the word-aligned section willtend to be relatively high.In this way, the alignments from the word-aligneddata effectively percolate through to the sentence-aligned data indirectly constraining the E-step ofEM.3.2 Weighting the contribution ofword-aligned dataBy incorporating ?, Equation 6 becomes an interpo-lation of the expected complete log likelihood pro-vided by the sentence-aligned data and the completelog likelihood provided by word-aligned data.The use of a weight to balance the contributionsof unlabeled and labeled data in maximum like-lihood estimation was proposed by Nigam et al(2000).
?
quantifies our relative confidence in theexpected statistics and observed statistics estimatedfrom the sentence- and word-aligned data respec-tively.Standard maximum likelihood estimation (MLE)which weighs all training samples equally, corre-sponds to an implicit value of lambda equal to theproportion of word-aligned data in the whole ofthe training set: ?
= NwNw+Ns .
However, havingthe total amount of sentence-aligned data be muchlarger than the amount of word-aligned data impliesa value of ?
close to zero.
This means that M can bemaximized while essentially ignoring the likelihoodof the word-aligned data.
Since we believe that theexplicit word-alignment information will be highlyeffective in distinguishing plausible alignments inthe corpus as a whole, we expect to see benefits bysetting ?
to amplify the contribution of the word-aligned data set particularly when this is a relativelysmall portion of the corpus.4 Experimental DesignTo perform our experiments with word-level aligne-ments we modified GIZA++, an existing and freelyavailable implementation of the IBM models andHMM variants (Och and Ney, 2003).
Our modifi-cations involved circumventing the E-step for sen-tences which had word-level alignments and incor-porating these observed alignment statistics in theM-step.
The observed and expected statistics wereweighted accordingly by ?
and (1?
?)
respectivelyas were their contributions to the mixed log likeli-hood.In order to measure the accuracy of the predic-tions that the statistical translation models make un-der our various experimental settings, we choosethe alignment error rate (AER) metric, which is de-fined in Och and Ney (2003).
We also investigatedwhether improved AER leads to improved transla-tion quality.
We used the alignments created duringour AER experiments as the input to a phrase-baseddecoder.
We translated a test set of 350 sentences,and used the Bleu metric (Papineni et al, 2001) toautomatically evaluate machine translation quality.We used the Verbmobil German-English parallelcorpus as a source of training data because it hasbeen used extensively in evaluating statistical trans-lation and alignment accuracy.
This data set comeswith a manually word-aligned set of 350 sentenceswhich we used as our test set.Our experiments additionally required a verylarge set of word-aligned sentence pairs to be in-corporated in the training set.
Since previous workhas shown that when training on the complete setof 34,000 sentence pairs an alignment error rate aslow as 6% can be achieved for the Verbmobil data,we automatically generated a set of alignments forthe entire training data set using the unmodified ver-sion of GIZA++.
We wanted to use automatic align-ments in lieu of actual hand alignments so that wewould be able to perform experiments using largedata sets.
We ran a pilot experiment to test whetherour automatic would produce similar results to man-ual alignments.We divided our manual word alignments intotraining and test sets and compared the performanceof models trained on human aligned data againstmodels trained on automatically aligned data.
ASize of training corpusModel .5k 2k 8k 16kModel 1 29.64 24.66 22.64 21.68HMM 18.74 15.63 12.39 12.04Model 3 26.07 18.64 14.39 13.87Model 4 20.59 16.05 12.63 12.17Table 1: Alignment error rates for the various IBMModels trained with sentence-aligned data100-fold cross validation showed that manual andautomatic alignments produced AER results thatwere similar to each other to within 0.1%.2Having satisfied ourselves that automatic align-ment were a sufficient stand-in for manual align-ments, we performed our main experiments whichfell into the following categories:1.
Verifying that the use of word-aligned data hasan impact on the quality of alignments pre-dicted by the IBM Models, and comparing thequality increase to that gained by using a bilin-gual dictionary in the estimation stage.2.
Evaluating whether improved parameter esti-mates of alignment quality lead to improvedtranslation quality.3.
Experimenting with how increasing the ratio ofword-aligned to sentence-aligned data affectedthe performance.4.
Experimenting with our ?
parameter which al-lows us to weight the relative contributionsof the word-aligned and sentence-aligned data,and relating it to the ratio experiments.5.
Showing that improvements to AER and trans-lation quality held for another corpus.5 Results5.1 Improved alignment qualityAs a staring point for comparison we trainedGIZA++ using four different sized portions of theVerbmobil corpus.
For each of those portions weoutput the most probable alignments of the testingdata for Model 1, the HMM, Model 3, and Model2Note that we stripped out probable alignments from ourmanually produced alignments.
Probable alignments are largeblocks of words which the annotator was uncertain of how toalign.
The many possible word-to-word translations implied bythe manual alignments led to lower results than with the auto-matic alignments, which contained fewer word-to-word trans-lation possibilities.Size of training corpusModel .5k 2k 8k 16kModel 1 21.43 18.04 16.49 16.20HMM 14.42 10.47 9.09 8.80Model 3 20.56 13.25 10.82 10.51Model 4 14.19 10.13 7.87 7.52Table 2: Alignment error rates for the various IBMModels trained with word-aligned data4,3 and evaluated their AERs.
Table 1 gives align-ment error rates when training on 500, 2000, 8000,and 16000 sentence pairs from Verbmobil corpuswithout using any word-aligned training data.We obtained much better results when incorpo-rating word-alignments with our mixed likelihoodfunction.
Table 2 shows the results for the differ-ent corpus sizes, when all of the sentence pairs havebeen word-aligned.
The best performing model inthe unmodified GIZA++ code was the HMM trainedon 16,000 sentence pairs, which had an alignmenterror rate of 12.04%.
In our modified code thebest performing model was Model 4 trained on16,000 sentence pairs (where all the sentence pairsare word-aligned) with an alignment error rate of7.52%.
The difference in the best performing mod-els represents a 38% relative reduction in AER.
In-terestingly, we achieve a lower AER than the bestperforming unmodified models using a corpus thatis one-eight the size of the sentence-aligned data.Figure 1 show an example of the improvedalignments that are achieved when using the wordaligned data.
The example alignments were heldout sentence pairs that were aligned after training on500 sentence pairs.
The alignments produced whenthe training on word-aligned data are dramaticallybetter than when training on sentence-aligned data.We contrasted these improvements with the im-provements that are to be had from incorporating abilingual dictionary into the estimation process.
Forthis experiment we allowed a bilingual dictionaryto constrain which words can act as translations ofeach other during the initial estimates of translationprobabilities (as described in Och and Ney (2003)).As can be seen in Table 3, using a dictionary reducesthe AER when compared to using GIZA++ withouta dictionary, but not as dramatically as integratingthe word-alignments.
We further tried combining adictionary with our word-alignments but found thatthe dictionary results in only very minimal improve-ments over using word-alignments alone.3We used the default training schemes for GIZA++, and leftmodel smoothing parameters at their default settings.Thenassume.DannreserviereichzweiEinzelzimmerIwillreservetwosingle,nehmerooms, Iichmalan.
(a) Sentence-alignedThenassume.DannreserviereichzweiEinzelzimmerIwillreservetwosingle,nehmerooms, Iichmalan.
(b) Word-alignedThenassume.DannreserviereichzweiEinzelzimmerIwillreservetwosingle,nehmerooms, Iichmalan.
(c) ReferenceFigure 1: Example alignments using sentence-aligned training data (a), using word-aligned data (b), and areference manual alignment (c)Size of training corpusModel .5k 2k 8k 16kModel 1 23.56 20.75 18.69 18.37HMM 15.71 12.15 9.91 10.13Model 3 22.11 16.93 13.78 12.33Model 4 17.07 13.60 11.49 10.77Table 3: The improved alignment error rates whenusing a dictionary instead of word-aligned data toconstrain word translationsSentence-aligned Word-alignedSize AER Bleu AER Bleu500 20.59 0.211 14.19 0.2332000 16.05 0.247 10.13 0.2608000 12.63 0.265 7.87 0.27816000 12.17 0.270 7.52 0.282Table 4: Improved AER leads to improved transla-tion quality5.2 Improved translation qualityThe fact that using word-aligned data in estimat-ing the parameters for machine translation leads tobetter alignments is predictable.
A more signifi-cant result is whether it leads to improved transla-tion quality.
In order to test that our improved pa-rameter estimates lead to better translation quality,we used a state-of-the-art phrase-based decoder totranslate a held out set of German sentences intoEnglish.
The phrase-based decoder extracts phrasesfrom the word alignments produced by GIZA++,and computes translation probabilities based on thefrequency of one phrase being aligned with another(Koehn et al, 2003).
We trained a language modelAER when whenRatio ?
= Standard MLE ?
= .90.1 11.73 9.400.2 10.89 8.660.3 10.23 8.130.5 8.65 8.190.7 8.29 8.030.9 7.78 7.78Table 5: The effect of weighting word-aligned datamore heavily that its proportion in the training data(corpus size 16000 sentence pairs)using the 34,000 English sentences from the train-ing set.Table 4 shows that using word-aligned data leadsto better translation quality than using sentence-aligned data.
Particularly, significantly less data isneeded to achieve a high Bleu score when usingword alignments.
Training on a corpus of 8,000 sen-tence pairs with word alignments results in a higherBleu score than when training on a corpus of 16,000sentence pairs without word alignments.5.3 Weighting the word-aligned dataWe have seen that using training data consistingof entirely word-aligned sentence pairs leads tobetter alignment accuracy and translation quality.However, because manually word-aligning sentencepairs costs more than just using sentence-aligneddata, it is unlikely that we will ever want to labelan entire corpus.
Instead we will likely have a rel-atively small portion of the corpus word aligned.We want to be sure that this small amount of datalabeled with word alignments does not get over-whelmed by a larger amount of unlabeled data.0.070.075 0.080.085 0.090.095 0.10.105 0.110.115 0.120.10.20.30.40.50.60.70.80.91Alignment Error RateLambda20% word-aligned50%word-aligned70%word-aligned100%word-alignedFigure 2: The effect on AER of varying ?
for a train-ing corpus of 16K sentence pairs with various pro-portions of word-alignmentsThus we introduced the ?
weight into our mixedlikelihood function.Table 5 compares the natural setting of ?
(whereit is proportional to the amount of labeled data in thecorpus) to a value that amplifies the contribution ofthe word-aligned data.
Figure 2 shows a variety ofvalues for ?.
It shows as ?
increases AER decreases.Placing nearly all the weight onto the word-aligneddata seems to be most effective.4 Note this did notvary the training data size ?
only the relative contri-butions between sentence- and word-aligned train-ing material.5.4 Ratio of word- to sentence-aligned dataWe also varied the ratio of word-aligned tosentence-aligned data, and evaluated the AER andBleu scores, and assigned high value to ?
(= 0.9).Figure 3 shows how AER improves as moreword-aligned data is added.
Each curve on the graphrepresents a corpus size and shows its reduction inerror rate as more word-aligned data is added.
Forexample, the bottom curve shows the performanceof a corpus of 16,000 sentence pairs which startswith an AER of just over 12% with no word-alignedtraining data and decreases to an AER of 7.5% whenall 16,000 sentence pairs are word-aligned.
Thiscurve essentially levels off after 30% of the data isword-aligned.
This shows that a small amount ofword-aligned data is very useful, and if we wantedto achieve a low AER, we would only have to label4,800 examples with their word alignments ratherthan the entire corpus.Figure 4 shows how the Bleu score improves asmore word-aligned data is added.
This graph also4At ?
= 1 (not shown in Figure 2) the data that is onlysentence-aligned is ignored, and the AER is therefore higher.0.060.08 0.10.120.140.160.18 0.20.2200.20.40.60.81Alignment error rateRatioof word-aligned tosentence-aligneddata500 sentence pairs2000sentence pairs8000sentence pairs16000sentence pairsFigure 3: The effect on AER of varying the ratio ofword-aligned to sentence-aligned data0.20.210.220.230.240.250.260.270.280.2900.20.40.60.81Bleu ScoreRatioof word-aligned tosentence-aligneddata500 sentence pairs2000sentence pairs8000sentence pairs16000sentence pairsFigure 4: The effect on Bleu of varying the ratio ofword-aligned to sentence-aligned datareinforces the fact that a small amount of word-aligned data is useful.
A corpus of 8,000 sentencepairs with only 800 of them labeled with word align-ments achieves a higher Bleu score than a corpus of16,000 sentence pairs with no word alignments.5.5 Evaluation using a larger training corpusWe additionally tested whether incorporating word-level alignments into the estimation improved re-sults for a larger corpus.
We repeated our experi-ments using the Canadian Hansards French-Englishparallel corpus.
Figure 6 gives a summary of the im-provements in AER and Bleu score for that corpus,when testing on a held out set of 484 hand alignedsentences.On the whole, alignment error rates are higherand Bleu scores are considerably lower for theHansards corpus.
This is probably due to the dif-ferences in the corpora.
Whereas the Verbmobilcorpus has a small vocabulary (<10,000 per lan-Sentence-aligned Word-alignedSize AER Bleu AER Bleu500 33.65 0.054 25.73 0.0642000 25.97 0.087 18.57 0.1008000 19.00 0.115 14.57 0.12016000 16.59 0.126 13.55 0.128Table 6: Summary results for AER and translationquality experiments on Hansards dataguage), the Hansards has ten times that many vocab-ulary items and has a much longer average sentencelength.
This made it more difficult for us to create asimulated set of hand alignments; we measured theAER of our simulated alignments at 11.3% (whichcompares to 6.5% for our simulated alignments forthe Verbmobil corpus).Nevertheless, the trend of decreased AER and in-creased Bleu score still holds.
For each size of train-ing corpus we tested we found better results usingthe word-aligned data.6 Related WorkOch and Ney (2003) is the most extensive analy-sis to date of how many different factors contributetowards improved alignments error rates, but the in-clusion of word-alignments is not considered.
Ochand Ney do not give any direct analysis of howimproved word alignments accuracy contributes to-ward better translation quality as we do here.Mihalcea and Pedersen (2003) described a sharedtask where the goal was to achieve the best AER.
Anumber of different methods were tried, but noneof them used word-level alignments.
Since the bestperforming system used an unmodified version ofGiza++, we would expected that our modifed ver-sion would show enhanced performance.
Naturallythis would need to be tested in future work.Melamed (1998) describes the process of manu-ally creating a large set of word-level alignments ofsentences in a parallel text.Nigam et al (2000) described the use of weightto balance the respective contributions of labeledand unlabeled data to a mixed likelihood function.Corduneanu (2002) provides a detailed discussionof the instability of maximum likelhood solutionsestimated from a mixture of labeled and unlabeleddata.7 Discussion and Future WorkIn this paper we show with the appropriate modifi-cation of EM significant improvement gains can behad through labeling word alignments in a bilingualcorpus.
Because of this significantly less data is re-quired to achieve a low alignment error rate or highBleu score.
This holds even when using noisy wordalignments such as our automatically created set.One should take our research into account whentrying to efficiently create a statistical machinetranslation system for a language pair for which aparallel corpus is not available.
Germann (2001)describes the cost of building a Tamil-English paral-lel corpus from scratch, and finds that using profes-sional translations is prohibitively high.
In our ex-perience it is quicker to manually word-align trans-lated sentence pairs than to translate a sentence, andword-level alignment can be done by someone whomight not be fluent enough to produce translations.It might therefore be possible to achieve a higherperformance at a fraction of the cost by hiring a non-professional produce word-alignments after a lim-ited set of sentences have been translated.We plan to investigate whether it is feasible touse active learning to select which examples willbe most useful when aligned at the word-level.
Sec-tion 5.4 shows that word-aligning a fraction of sen-tence pairs in a training corpus, rather than the entiretraining corpus can still yield most of the benefitsdescribed in this paper.
One would hope that by se-lectively sampling which sentences are to be manu-ally word-aligned we would achieve nearly the sameperformance as word-aligning the entire corpus.AcknowledgementsThe authors would like to thank Franz Och, Her-mann Ney, and Richard Zens for providing theVerbmobil data, and Linear B for providing itsphrase-based decoder.ReferencesPeter Brown, Stephen Della Pietra, Vincent Della Pietra,and Robert Mercer.
1993.
The mathematics of ma-chine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311, June.Adrian Corduneanu.
2002.
Stable mixing of completeand incomplete information.
Master?s thesis, Mas-sachusetts Institute of Technology, February.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical Soci-ety, 39(1):1?38, Nov.Ulrich Germann.
2001.
Building a statistical machinetranslation system from scratch: How much bang forthe buck can we expect?
In ACL 2001 Workshop onData-Driven Machine Translation, Toulouse, France,July 7.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the HLT/NAACL.I.
Dan Melamed.
1998.
Manual annotation of trans-lational equivalence: The blinker project.
CognitiveScience Technical Report 98/07, University of Penn-sylvania.Rada Mihalcea and Ted Pedersen.
2003.
An evaluationexercise for word alignment.
In Rada Mihalcea andTed Pedersen, editors, HLT-NAACL 2003 Workshop:Building and Using Parallel Texts.Kamal Nigam, Andrew K. McCallum, Sebastian Thrun,and Tom M. Mitchell.
2000.
Text classification fromlabeled and unlabeled documents using EM.
MachineLearning, 39(2/3):103?134.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51, March.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic eval-uation of machine translation.
IBM Research ReportRC22176(W0109-022), IBM.Philip Resnik and Noah Smith.
2003.
The web as a par-allel corpus.
Computational Linguistics, 29(3):349?380, September.
