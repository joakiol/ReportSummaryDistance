Proceedings of the Third Workshop on Statistical Machine Translation, pages 143?146,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsPhrase-Based and Deep SyntacticEnglish-to-Czech Statistical Machine Translation ?Ondr?ej Bojar and Jan Hajic?Institute of Formal and Applied Linguistics?UFAL MFF UK, Malostranske?
na?me?st??
25CZ-11800 Praha, Czech Republic{bojar,hajic}@ufal.mff.cuni.czAbstractThis paper describes our two contributions toWMT08 shared task: factored phrase-basedmodel using Moses and a probabilistic tree-transfer model at a deep syntactic layer.1 IntroductionCzech is a Slavic language with very rich morphol-ogy and relatively free word order.
The Czechmorphological system (Hajic?, 2004) defines 4,000tags in theory and 2,000 were actually seen in abig tagged corpus while the English Penn Treebanktagset contains just about 50 tags.
In our parallelcorpus (see below), the English vocabulary size is148k distinct word forms but more than twice as bigin Czech, 343k distinct word forms.When translating to Czech from an analytic lan-guage such as English, target word forms have tobe chosen correctly to produce a grammatical sen-tence and preserve the expressed relations betweenelements in the sentence, e.g.
verbs and their modi-fiers.This year, we have taken two radically differentapproaches to English-to-Czech MT.
Section 2 de-scribes our setup of the phrase-based system Moses(Koehn et al, 2007) and Section 3 focuses on a sys-tem with probabilistic tree transfer employed at adeep syntactic layer and the new challenges this ap-proach brings.
?The work on this project was supported by the grants FP6-IST-5-034291-STP (EuroMatrix), MSM0021620838, M?SMT?CR LC536, and GA405/06/0589.2 Factored Phrase-Based MT to CzechBojar (2007) describes various experiments withfactored translation to Czech aimed at improvingtarget-side morphology.
We use essentially the samesetup with some cleanup and significantly largertarget-side training data:Parallel data from CzEng 0.7 (Bojar et al, 2008),with original sentence-level alignment and tokeniza-tion.
The parallel corpus was taken as a monolithictext source disregarding differences between CzEngdata sources.
We use only 1-1 aligned sentences.Word alignment using GIZA++ toolkit (Och andNey, 2000), the default configuration as available intraining scripts for Moses.
We based the word align-ment on Czech and English lemmas (base formsof words) as provided by the combination of tag-gers and lemmatizers by Hajic?
(2004) for Czech andBrants (2000) followed by Minnen et al (2001) forEnglish.
We symmetrized the two GIZA++ runs us-ing grow-diag-final heuristic.Truecasing.
We attempted to preserve meaning-bearing case distinctions.
The Czech lemmatizerproduces case-sensitive lemmas and thus makes iteasy to cast the capitalization of the lemma back onthe word form.1 For English we approximate thesame effect by a two-step procedure.21We change the capitalization of the form to match thelemma in cases where the lemma is lowercase, capitalized (uc-first) or all-caps.
For mixed-case lemmas, we keep the formintact.2We first collect a lexicon of the most typical ?shapes?
foreach word form (ignoring title-like sentences with most wordscapitalized and the first word in a sentence).
Capitalized andall-caps words in title-like sentences are then changed to their143Decoding steps.
We use a simple two-step sce-nario similar to class-based models (Brown and oth-ers, 1992): (1) the source English word forms aretranslated to Czech word forms and (2) full Czechmorphological tags are generated from the Czechforms.Language models.
We use the following 6 inde-pendently weighted language models for the target(Czech) side:?
3-grams of word forms based on all CzEng 0.7data, 15M tokens,?
3-grams of word forms in Project Syndicatesection of CzEng (in-domain for WMT07 andWMT08 NC-test set), 1.8M tokens,?
4-grams of word forms based on Czech Na-tional Corpus (Kocek et al, 2000), versionSYN2006, 365M tokens,?
three models of 7-grams of morphological tagsfrom the same sources.Lexicalized reordering using the mono-tone/swap/discontinuous bidirectional model basedon both source and target word forms.MERT.
We use the minimum-error rate trainingprocedure by Och (2003) as implemented in theMoses toolkit to set the weights of the various trans-lation and language models, optimizing for BLEU.Final detokenization is a simple rule-based pro-cedure based on Czech typographical conventions.Finally, we capitalize the beginnings of sentences.See BLEU scores in Table 2 below.3 MT with a Deep Syntactic Transfer3.1 Theoretical BackgroundCzech has a well-established theory of linguisticanalysis called Functional Generative Description(Sgall et al, 1986) supported by a big treebankingenterprise (Hajic?
and others, 2006) and on-goingadaptations for other languages including English(Cinkova?
and others, 2004).
There are two layerstypical shape.
In other sentences we change the case only if atypically lowercase word is capitalized (e.g.
at the beginningof the sentence) or if a typically capitalized word is all-caps.Unknown words in title-like sentences are lowercased and leftintact in other sentences.PredSb uvedla , z?e Pred=VPNP said VPFigure 1: Sample treelet pair, a-layer.of syntactic analysis, both formally captured as la-belled ordered dependency trees: the ANALYTICAL(a-, surface syntax) representation bears a 1-1 corre-spondence between tokens in the sentence and nodesin the tree; the TECTOGRAMMATICAL (t-, deep syn-tax) representation contains nodes only for autose-mantic words and adds nodes for elements not ex-pressed on the surface but required by the grammar(e.g.
dropped pronouns).We use the following tools to automatically anno-tate plaintext up to the t-layer: (1) TextSeg ( ?Ces?ka,2006) for tokenization, (2) tagging and lemmatiza-tion see above, (3) parsing to a-layer: Collins (1996)followed by head-selection rules for English, Mc-Donald and others (2005) for Czech, (4) parsing to t-layer: ?Zabokrtsky?
(2008) for English, Klimes?
(2006)for Czech.3.2 Probabilistic Tree TransferThe transfer step is based on Synchronous Tree Sub-stitution Grammars (STSG), see Bojar and ?Cmejrek(2007) for a detailed explanation.
The essence is alog-linear model to search for the most likely syn-chronous derivation ??
of the source T1 and target T2dependency trees:??
= argmax?
s.t.
source is T1exp(M?m=1?mhm(?
))(1)The key feature function hm in STSG representsthe probability of attaching pairs of dependencytreelets ti1:2 such as in Figure 1 into aligned pairs offrontiers ( ) in another treelet pair tj1:2 given fron-tier state labels (e.g.
Pred- VP in Figure 1):hSTSG(?)
= logk?i=0p(ti1:2 | frontier states) (2)Other features include e.g.
number of internalnodes (drawn as in Figure 1) produced, numberof treelets produced, and more importantly the tra-ditional n-gram language model if the target (a-)tree144is linearized right away or a binode model promot-ing likely combinations of the governor g(e) and thechild c(e) of an edge e ?
T2:hbinode(?)
= log?e?T2p(c(e) | g(e)) (3)The probabilistic dictionary of aligned treeletpairs is extracted from node-aligned (GIZA++ onlinearized trees) parallel automatic treebank as inMoses?
training: all treelet pairs compatible with thenode alignment.3.2.1 Factored Treelet TranslationLabels of nodes at the t-layer are not atomic butconsist of more than 20 attributes representing var-ious linguistic features.3 We can consider the at-tributes as individual factors (Koehn and Hoang,2007).
This allows us to condition the translationchoice on a subset of source factors only.
In order togenerate a value for each target-side factor, we usea sequence of mapping steps similar to Koehn andHoang (2007).
For technical reasons, our currentimplementation allows to generate factored target-side only when translating a single node to a singlenode, i.e.
preserving the tree structure.In our experiments we used 8 source (English) t-node attributes and 14 target (Czech) attributes.3.3 Recent Experimental ResultsTable 1 shows BLEU scores for various configura-tions of our decoder.
The abbreviations indicate be-tween which layers the tree transfer was employed(e.g.
?eact?
means English a-layer to Czech t-layer).The ?p?
layer is an approximation of phrase-basedMT: the surface ?syntactic?
analysis is just a left-to-right linear tree.4 For setups ending in t-layer, weuse a deterministic generation the of Czech sentenceby Pta?c?ek and ?Zabokrtsky?
(2006).For WMT08 shared task, Table 2, we used a vari-ant of the ?etct factored?
setup with the annotationpipeline as incorporated in TectoMT ( ?Zabokrtsky?,2008) environment and using TectoMT internal3Treated as atomic, t-node labels have higher entropy(11.54) than lowercase plaintext (10.74).
The t-layer by itselfdoes not bring any reduction in vocabulary.
The idea is that theattributes should be more or less independent and should mapeasier across languages.4Unlike Moses, ?epcp?
does not permit phrase reordering.Tree-based Transfer LM Type BLEUepcp n-gram 10.9?0.6eaca n-gram 8.8?0.6epcp none 8.7?0.6eaca none 6.6?0.5etca n-gram 6.3?0.6etct factored, preserving structure binode 5.6?0.5etct factored, preserving structure none 5.3?0.5eact, target side atomic binode 3.0?0.3etct, atomic, all attributes binode 2.6?0.3etct, atomic, all attributes none 1.6?0.3etct, atomic, just t-lemmas none 0.7?0.2Phrase-based (Moses) as reported by Bojar (2007)Vanilla n-gram 12.9?0.6Factored to improve target morphology n-gram 14.2?0.7Table 1: English-to-Czech BLEU scores for syntax-basedMT on WMT07 DevTest.WMT07 WMT08DevTest NC Test News TestMoses 14.9?0.9 16.4?0.6 12.3?0.6Moses, CzEng data only 13.9?0.9 15.2?0.6 10.0?0.5etct, TectoMT annotation 4.7?0.5 4.9?0.3 3.3?0.3Table 2: WMT08 shared task BLEU scores.rules for t-layer parsing and generation instead ofKlimes?
(2006) and (Pta?c?ek and ?Zabokrtsky?, 2006).3.3.1 DiscussionOur syntax-based approach does not reach scoresof phrase-based MT due to the following reasons:Cumulation of errors at every step of analysis.Data loss due to incompatible parses and nodealignment.
Unlike e.g.
Quirk et al (2005) or Huanget al (2006) who parse only one side and project thestructure, we parse both languages independently.Natural divergence and random errors in either ofthe parses and/or the alignment prevent us from ex-tracting many treelet pairs.Combinatorial explosion in target node at-tributes.
Currently, treelet options are fully built inadvance.
Uncertainty in the many t-node attributesleads to too many insignificant variations while e.g.different lexical choices are pushed off the stack.While vital for final sentence generation (see Ta-ble 1), fine-grained t-node attributes should be pro-duced only once all key structural, lexical and formdecisions have been made.
The same sort of explo-sion makes complicated factored setups not yet fea-sible in Moses, either.145Lack of n-gram LM in the (deterministic) gen-eration procedures from a t-tree.
While we supportfinal LM-based rescoring, there is too little variancein n-best lists due to the explosion mentioned above.Too many model parameters given our stacklimit.
We use identical MERT implementation tooptimize ?ms but in the large space of hypotheses,MERT does not converge.3.3.2 Related ResearchOur approach should not be confused with theTectoMT submission by Zdene?k ?Zabokrtsky?
with adeterministic transfer: heuristics fully exploiting thesimilarity of English and Czech t-layers.Ding and Palmer (2005) improve over word-basedMT baseline with a formalism very similar to STSG.Though not explicitly stated, they seem not to en-code frontiers in the treelets and allow for adjunction(adding siblings), like Quirk et al (2005), which sig-nificantly reduces data sparseness.Riezler and III (2006) report an improvement inMT grammaticality on a very restricted test set:short sentences parsable by an LFG grammar with-out back-off rules.4 ConclusionWe have presented our best-performing factoredphrase-based English-to-Czech translation and ahighly experimental complex system with tree-based transfer at a deep syntactic layer.
We havediscussed some of the reasons why the phrase-basedMT currently performs much better.ReferencesOndr?ej Bojar and Martin ?Cmejrek.
2007.
MathematicalModel of Tree Transformations.
Project EuroMatrix -Deliverable 3.2, ?UFAL, Charles University, Prague.Ondr?ej Bojar, Zdene?k ?Zabokrtsky?, Pavel ?Ces?ka, PeterBen?a, and Miroslav Jan??c?ek.
2008.
CzEng 0.7: Paral-lel Corpus with Community-Supplied Translations.
InProc.
of LREC 2008.
ELRA.Ondr?ej Bojar.
2007.
English-to-Czech Factored MachineTranslation.
In Proc.
of ACL Workshop on StatisticalMachine Translation, pages 232?239, Prague.Thorsten Brants.
2000.
TnT - A Statistical Part-of-Speech Tagger .
In Proc.
of ANLP-NAACL.Peter F. Brown et al 1992.
Class-based n-gram mod-els of natural language.
Computational Linguistics,18(4):467?479.Pavel ?Ces?ka.
2006.
Segmentace textu.
Bachelor?s The-sis, MFF, Charles University in Prague.Silvie Cinkova?
et al 2004.
Annotation of English on thetectogrammatical level.
Technical Report TR-2006-35, ?UFAL/CKL, Prague, Czech Republic.Michael Collins.
1996.
A New Statistical Parser Basedon Bigram Lexical Dependencies.
In Proc.
of ACL.Yuan Ding and Martha Palmer.
2005.
Machine Transla-tion Using Probabilistic Synchronous Dependency In-sertion Grammars.
In Proc.
of ACL.Jan Hajic?.
2004.
Disambiguation of Rich Inflection(Computational Morphology of Czech).
Nakladatel-stv??
Karolinum, Prague.Jan Hajic?
et al 2006.
Prague Dependency Treebank 2.0.LDC2006T01, ISBN: 1-58563-370-4.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical Syntax-Directed Translation with ExtendedDomain of Locality.
In Proc.
of AMTA, Boston, MA.Va?clav Klimes?.
2006.
Analytical and TectogrammaticalAnalysis of a Natural Language.
Ph.D. thesis, ?UFAL,MFF UK, Prague, Czech Republic.Jan Kocek, Marie Kopr?ivova?, and Karel Kuc?era, edi-tors.
2000.
?Cesky?
na?rodn??
korpus - u?vod a pr???ruc?kauz?ivatele.
FF UK - ?U ?CNK, Praha.Philipp Koehn and Hieu Hoang.
2007.
Factored Transla-tion Models.
In Proc.
of EMNLP.Philipp Koehn, Hieu Hoang, et al 2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.
InProc.
of ACL Demo and Poster Sessions.Ryan McDonald et al 2005.
Non-Projective Depen-dency Parsing using Spanning Tree Algorithms.
InProc.
of HLT/EMNLP 2005.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Natu-ral Language Engineering, 7(3):207?223.Franz Josef Och and Hermann Ney.
2000.
A Comparisonof Alignment Models for Statistical Machine Transla-tion.
In Proc.
of COLING, pages 1086?1090.Franz Josef Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proc.
of ACL.Jan Pta?c?ek and Zdene?k ?Zabokrtsky?.
2006.
Synthesisof Czech Sentences from Tectogrammatical Trees.
InProc.
of TSD, pages 221?228.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency Treelet Translation: Syntactically InformedPhrasal SMT.
In Proc.
of ACL, pages 271?279.Stefan Riezler and John T. Maxwell III.
2006.
Grammat-ical Machine Translation.
In Proc.
of HLT/NAACL.Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.The Meaning of the Sentence and Its Semantic andPragmatic Aspects.
Academia, Prague.Zdene?k ?Zabokrtsky?.
2008.
Tecto MT.
Technical report,?UFAL/CKL, Prague, Czech Republic.
In prep.146
