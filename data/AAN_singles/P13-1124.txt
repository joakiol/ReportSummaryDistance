Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1264?1274,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTwo-Neighbor Orientation Model with Cross-Boundary Global ContextsHendra Setiawan, Bowen Zhou, Bing Xiang and Libin ShenIBM T.J.Watson Research Center1101 Kitchawan RoadYorktown Heights, NY 10598, USA{hendras,zhou,bxiang,lshen}@us.ibm.comAbstractLong distance reordering remains one ofthe greatest challenges in statistical ma-chine translation research as the key con-textual information may well be beyondthe confine of translation units.
In thispaper, we propose Two-Neighbor Orien-tation (TNO) model that jointly modelsthe orientation decisions between anchorsand two neighboring multi-unit chunkswhich may cross phrase or rule bound-aries.
We explicitly model the longestspan of such chunks, referred to as Max-imal Orientation Span, to serve as aglobal parameter that constrains under-lying local decisions.
We integrate ourproposed model into a state-of-the-artstring-to-dependency translation systemand demonstrate the efficacy of our pro-posal in a large-scale Chinese-to-Englishtranslation task.
On NIST MT08 set, ourmost advanced model brings around +2.0BLEU and -1.0 TER improvement.1 IntroductionLong distance reordering remains one of the great-est challenges in Statistical Machine Translation(SMT) research.
The challenge stems from thefact that an accurate reordering hinges upon themodel?s ability to make many local and globalreordering decisions accurately.
Often, suchreordering decisions require contexts that spanacross multiple translation units.1 Unfortunately,previous approaches fall short in capturing suchcross-unit contextual information that could be1We define translation units as phrases in phrase-basedSMT, and as translation rules in syntax-based SMT.critical in reordering.
Specifically, the popular dis-tortion or lexicalized reordering models in phrase-based SMT focus only on making good local pre-diction (i.e.
predicting the orientation of imme-diate neighboring translation units), while transla-tion rules in syntax-based SMT come with a strongcontext-free assumption, which model only the re-ordering within the confine of the rules.
In thispaper, we argue that reordering modeling wouldgreatly benefit from richer cross-boundary contex-tual informationWe introduce a reordering model that incorpo-rates such contextual information, named the Two-Neighbor Orientation (TNO) model.
We first iden-tify anchors as regions in the source sentencesaround which ambiguous reordering patterns fre-quently occur and chunks as regions that are con-sistent with word alignment which may span mul-tiple translation units at decoding time.
Most no-tably, anchors and chunks in our model may notnecessarily respect the boundaries of translationunits.
Then, we jointly model the orientations ofchunks that immediately precede and follow theanchors (hence, the name ?two-neighbor?)
alongwith the maximal span of these chunks, to whichwe refer as Maximal Orientation Span (MOS).As we will elaborate further in next sections,our models provide a stronger mechanism to makemore accurate global reordering decisions for thefollowing reasons.
First of all, we consider theorientation decisions on both sides of the anchorssimultaneously, in contrast to existing works thatonly consider one-sided decisions.
In this way, wehope to upgrade the unigram formulation of exist-ing reordering models to a higher order formula-tion.
Second of all, we capture the reordering ofchunks that may cross translation units and maybe composed of multiple units, in contrast to ex-1264isting works that focus on the reordering betweenindividual translation units.
In effect, MOS acts asa global reordering parameter that guides or con-strains the underlying local reordering decisions.To show the effectiveness of our model, weintegrate our TNO model into a state-of-the-art syntax-based SMT system, which uses syn-chronous context-free grammar (SCFG) rules tojointly model reordering and lexical translation.The introduction of nonterminals in the SCFGrules provides some degree of generalization.However as mentioned earlier, the context-freeassumption ingrained in the syntax-based for-malism often limits the model?s ability to in-fluence global reordering decision that involvescross-boundary contexts.
In integrating TNO, wehope to strengthen syntax-based system?s abilityto make more accurate global reordering deci-sions.Our other contribution in this paper is a prac-tical method for integrating the TNO model intosyntax-based translations.
The integration is non-trivial since the decoding of syntax-based SMTproceeds in a bottom-up fashion, while our modelis more natural for top-down parsing, thus themodel?s full context sometimes is often availableonly at the latest stage of decoding.
We implementan efficient shift-reduce algorithm that facilitatesthe accumulation of partial context in a bottom-upfashion, allowing our model to influence the trans-lation process even in the absence of full context.We show the efficacy of our proposal in a large-scale Chinese-to-English translation task wherethe introduction of our TNO model provides asignificant gain over a state-of-the-art string-to-dependency SMT system (Shen et al, 2008) thatwe enhance with additional state-of-the-art fea-tures.
Even though the experimental results car-ried out in this paper employ SCFG-based SMTsystems, we would like to point out that our mod-els is applicable to other systems including phrase-based SMT systems.The rest of the paper is organized as follows.In Section 2, we introduce the formulation of ourTNO model.
In Section 3, we introduce and moti-vate the concept of Maximal Orientation Span.
InSection 4, we introduce four variants of the TNOmodel with different model complexities.
In Sec-tion 5, we describe the training procedure to esti-mate the parameters of our models.
In Section 6,we describe our shift-reduce algorithm which inte-grates our proposed TNO model into syntax-basedSMT.
In Section 7, we describe our experimentsand present our results.
We wrap up with relatedwork in Section 8 and conclusion in Section 9.2 Two-Neighbor Orientation ModelGiven an aligned sentence pair ?
= (F,E,?
), let?(?)
be all possible chunks that can be extractedfrom ?
according to: 2{(f j2j1/ei2i1) :?j1?
j?
j2,?i : (j, i)?
?, ii?
i?
i2 ??i1?
i?
i2,?j : (j, i)?
?, ji?
j?j2}Our Two-Neighbor Orientation model (TNO)designatesA ?
?(?)
as anchors and jointly mod-els the orientation of chunks that appear immedi-ately to the left and to the right of the anchors aswell as the identities of these chunks.
We defineanchors as chunks, around which ambiguous re-ordering patterns frequently occur.
Anchors canbe learnt automatically from the training data oridentified from the linguistic analysis of the sourcesentence.
In our experiments, we use a simpleheuristics based on part-of-speech tags which willbe described in Section 7.More concretely, given A ?
?(?
), let a =(f j2j1/ei2i1) ?
A be a particular anchor.
Then, letCL(a) ?
?(?)
be a?s left neighbors and letCR(a) ?
?(?)
be a?s right neighbors, iff:?CL = (f j4j3/ei4i3) ?
CL(a) : j4 + 1 = j1 (1)?CR = (f j6j5/ei6i5) ?
CR(a) : j2 + 1 = j5 (2)Given CL(a) and CR(a), let CL = (f j4j3/ei4i3) andCR = (f j6j5/ei6i5) be a particular pair of left and rightneighbors of a = (f j2j1/ei2i1).
Then, the orientationof CL and CR are OL(CL, a) and OR(CR, a) re-spectively and each may take one of the followingfour orientation values (similar to (Nagata et al,2006)):?
Monotone Adjacent (MA), if (i4 + 1) = i1for OL and if (i2 + 1) = i5 for OR?
Reverse Adjacent (RA), if (i2 + 1) = i3 forOL and if (i6 + 1) = i1 for OR?
Monotone Gap (MG), if (i4 + 1) < i1 forOL and if (i2 + 1) < i5 for OR2We represent a chunk as a source and target phrase pair(f j2j1/ei2i1 ) where the subscript and the superscript indicate thestarting and the ending indices as such f j2j1 denotes a sourcephrase that spans from j1 to j2.1265Figure 1: An aligned Chinese-English sentence pair.
Circlesrepresent alignment points.
Black circle represents the an-chor; boxes represent the anchor?s neighbors.?
Reverse Gap (RG), if (i2 + 1) < i3 for OLand if (i6 + 1) < i1 for OR.
(1)The first clause (monotone, reverse) indicateswhether the target order of the chunks follows thesource order; the second (adjacent, gap) indicateswhether the chunks are adjacent or separated by anintervening phrase when projected.To be more concrete, let us consider an alignedsentence pair in Fig.
1, which is adapted from(Chiang, 2005).
Suppose there is only one anchor,i.e.
a = (f77 /e77) which corresponds to the wordde(that).
By applying Eqs.
1 and 2, we can inferthat a has three left neighbors and four right neigh-bors, i.e.
CL(a) = (f66 /e99), (f65 /e98), (f63 /e118 ) andCR(a) = (f88 /e55), (f98 /e65), (f108 /e64), (f118 /e63)respectively.
Then, by applying Eq.1, we can compute the orientation val-ues of each of these neighbors, whichare OL(CL(a), a) = RG,RA,RA andOR(CR(a), a) = RG,RA,RA,RA.
As shown,most of the neighbors have Reverse Adjacent(RA) orientation except for the smallest left andright neighbors (i.e.
(f66 /e99) and (f88 /e55)) whichhave Reverse Gap (RG) orientation.Given the anchors together with its neighboringchunks and their orientations, the Two-NeighborOrientation model takes the following form:?a?A?CL?CL(a),CR?CR(a)PTNO(CL, OL, CR, OR|a; ?)
(2)For conciseness, references that are clear fromcontext, such the reference to CL and a inOL(CL, a), are dropped.3 Maximal Orientation SpanAs shown in Eq.
2, the TNO model has to enu-merate all possible pairing of CL ?
CL(a) andCR ?
CR(a).
To make the TNO model moretractable, we simplify the TNO model to consideronly the largest left and right neighbors, referredto as the Maximal Orientation Span/MOS (M ).More formally, given a = (f j2j1/ei2i1), the left andthe right MOS of a are:ML(a) = arg max(fj4j3 /ei4i3 )?CL(a)(j4 ?
j3)MR(a) = arg max(fj6j5 /ei6i5 )?CR(a)(j6 ?
j5)Coming back to our example, the left and rightMOS of the anchor are ML(a) = (f63 /e118 ) andMR(a) = (f118 /e63).
In Fig.
1, they are denoted asthe largest boxes delineated by solid lines.As such, we reformulate Eq.
2 into:?a?A?CL?CL(a),CR?CR(a)PTNO(ML, OL,MR, OR|a; ?).
?CL==ML?CR==MR(3)where ?
returns 1 if (CL == ML ?CR == MR),otherwise 0.Beyond simplifying the computation, the keybenefit of modeling MOS is that it serves as aglobal parameter that can guide or constrain un-derlying local reorderings.
As a case in point, letus consider a cheating exercise where we have totranslate the Chinese sentence in Fig.
1 with thefollowing set of hierarchical phrases3:Xa?
?Aozhou1shi2X1,Australia1 is2X1?Xb?
?yu3 Beihan4X1, X1with3 North4 Korea?Xc?
?you5bangjiao6, have5dipl.6 rels.?Xd?
?X1de7shaoshu8 guojia9 zhi10 yi11,one11of10the few8 countries9 that7X1?This set of hierarchical phrases represents a trans-lation model that has resolved all local ambiguities(i.e.
local reordering and lexical mappings) exceptfor the spans of the hierarchical phrases.
With thisexample, we want to show that accurate local de-cisions (rather obviously) don?t always lead to ac-curate global reordering and to demonstrate thatexplicit MOS modeling can play a crucial role toaddress this issue.
To do so, we will again focuson the same anchor de (that).3We use hierarchical phrase-based translation system as acase in point, but the merit is generalizable to other systems.1266d?
?X1de7shaoshu8 guojia9 zhi10 yi11?, ?one11of10the few8 countries9 that7X1?a?
?
?Aozhou1shi2X1?de7shaoshu8 guojia9 zhi10 yi11?,?one11of10the few8 countries9 that7?Australia1 is2X1??b?
?
?Aozhou1shi2 ?yu3 Beihan4X1?
?de7shaoshu8 guojia9 zhi10 yi11?,?one11of10the few8 countries9 that7?Australia1 is2?X1with3 North4 Korea???c?
?d ?aAozhou1shi2 ?byu3 Beihan4 ?cyou5bangjiao6?c?b?ade7shaoshu8 guojia9 zhi10 yi11 ?d ,?one11of10the few8 countries9 that7?Australia1 is2?
?have5dipl.6 rels.
?with3 North4 Korea??
?Table 1: Derivation of Xd ?Xa ?Xb ?Xc that leads to an incorrect translation.a?
?Aozhou1shi2X1?, ?Australia1 is2X1?b?
?Aozhou1shi2?yu3Beihan4X1?
?, ?Australia1 is2?X1with3 North4 Korea??d?
?Aozhou1shi2?yu3Beihan4?X1de7shaoshu8 guojia9 zhi10 yi11??
?,?Australia1 is2?
?one11of10the few8 countries9 that7X1?with3 North4 Korea??c?
?aAozhou1shi2?byu3Beihan4 ?d ?cyou5bangjiao6?cde7shaoshu8 guojia9 zhi10 yi11 ?d ?b?a,Australia1 is2?
?one11of10the few8 countries9 that7?have5dipl.6?
?with3 North4 Korea?
?Table 2: Derivation of Xa ?Xb ?Xd ?Xc that leads to the correct translation.As the rule?s identifier, we attach an alphabetletter to each rule?s left hand side, as such the an-chor de (that) appears in rule Xd.
We also attachthe word indices as the superscript of the sourcewords and project the indices to the target wordsaligned, as such ?have5?
suggests that the word?have?
is aligned to the 5-th source word, i.e.
you.Note that to facilitate the projection, the rules mustcome with internal word alignment in practice.Now the indices on the target words in the rulesare different from those in Fig.
1.
We will alsoextensively use indices in this sense in the sub-sequent section about decoding.
In such a sense,ML(a) = (f63 /e63) and MR(a) = (f118 /e118 ).Given the rule set, there are three possiblederivations, i.e.
Xd ?Xa ?Xb ?Xc,Xa ?Xb ?Xd ?Xc, and Xa ?Xd ?Xb ?Xc, where ?
in-dicates that the first operand dominates the secondoperand in the derivation tree.
The application ofthe rules would show that the first derivation willproduce an incorrect reordering while the last twowill produce the correct ones.
Here, we would liketo point out that even in this simple example whereall local decisions are made accurate, this ambigu-ity occurs and it would occur even more so in thereal translation task where local decisions may behighly inaccurate.Next, we will show that the MOS-related in-formation can help to resolve this ambiguity, byfocusing more closely on the first and the secondderivations, which are detailed in Tables 1 and 2.Particularly, we want to show that the MOS gen-erated by the incorrect derivation does not matchthe MOS learnt from Fig.
1.
As shown, at theend of the derivation, we have all the informa-tion needed to compute the MOS (i.e.
?)
which isequivalent to that available at training time, i.e.
thesource sentence, the complete translation and theword alignment.
Running the same MOS extrac-tion procedure on both derivations would producethe right MOS that agrees with the right MOS pre-viously learnt from Fig.
1, i.e.
(f118 /e118 ).
How-ever, that?s not the case for left MOS, which weunderline in Tables 1 and 2.
As shown, the incor-rect derivation produces a left MOS that spans sixwords, i.e.
(f61 /e61), while the correct derivationproduces a left MOS that spans four words, i.e.
(f63 /e63).
Clearly, the MOS of the incorrect deriva-tion doesn?t agree with the MOS we learnt fromFig.
1, unlike the MOS of the correct translation.This suggests that explicit MOS modeling wouldprovide a mechanism for resolving crucial globalreordering ambiguities that are beyond the abilityof local models.Additionally, this illustration also shows a casewhere MOS acts as a cross-boundary contextwhich effectively relaxes the context-free assump-tion of hierarchical phrase-based formalism.
InTables 1 and 2?s full derivations, we indicate ruleboundaries explicitly by indexing the angle brack-ets, e.g.
?a indicates the beginning of rule Xa inthe derivation.
As the anchor appears in Xd, we1267highlight its boundaries in box frames.
de (that)?sMOS respects rule boundaries if and only if allthe words come entirely from Xd?s antecedent or?d and ?d appears outside of MOS; otherwise itcrosses the rule boundaries.
As clearly shown inTable 2, the left MOS of the correct derivation (un-derlined) crosses the rule boundary (of Xd) since?d appears within the MOS.Going back to the formulation, focusing onmodeling MOS would simplify the formulation ofTNO model from Eq.
2 into:?a?APTNO(ML, OL,MR, OR|a; ?)
(4)which doesn?t require enumerating of all possiblepairs of CL and CR.4 Model Decomposition and VariantsTo make the model more tractable, we decomposePTNO in Eq.
4 into the following four factors:P (MR|a)?
P (OR|MR, a)?
P (ML|OR,MR, a)?
P (OL|ML, OR,MR, a).
Subsequently, we willrefer to them as PMR , POR , PML and POL respec-tively.
Each of these factors will act as an addi-tional feature in the log-linear framework of ourSMT system.
The above decomposition followsa generative story that starts from generating theright neighbor first.
There are other equally credi-ble alternatives, but based on empirical results, wesettle with the above.Next, we present four different variants of themodel (not to be confused with the four factorsabove).
Each variant has a different probabilisticconditioning of the factors.
We start by makingstrong independence assumptions in Model 1 andthen relax them as we progress to Model 4.
Thedescription of the models is as follow:?
Model 1.
We assume PML and PMR to beequal to 1 and POR ?
P (OR|a; ?)
to be in-dependent of MR and POL ?
P (OL|a; ?)
tobe in independent of ML,MR and OR.?
Model 2.
On top of Model 1, wemake POL dependent on POR , thusPOL?P (OL|OR, a; ?).?
Model 3.
On top of Model 2, we make PORdependent on MR and POL on MR and ML,thus POR ?
P (OR|MR, a; ?)
and POL ?P (OL|ML, OR,MR; a,?)
.?
Model 4.
On top of Model 3, we model PMRand PML as multinomial distributions esti-mated from training data.Model 1 represents a model that focuses onmaking accurate one-sided decisions, independentof the decision on the other side.
Model 2 isdesigned to address the deficiency of Model 1since Model 1 may assign non-zero probability toimprobable assignment of orientation values, e.g.Monotone Adjacent for the left neighbor and Re-verse Adjacent for the right neighbor.
Model 2does so by conditioning POL on OR.
In Model 3,we start incorporating MOS-related information inpredicting OL and OR.
In Model 4, we explicitlymodel the MOS of each anchor.5 TrainingThe TNO model training consists of two differ-ent training regimes: 1) discriminative for train-ing POL ,POR ; and 2) generative for training PML ,PMR .
Before describing the specifics, we start bydescribing the procedure to extract anchors andtheir corresponding MOS from training data, fromwhich we collect statistics and extract features totrain the model.For each aligned sentence pair (F,E,?)
in thetraining data, the training starts with the iden-tification of the regions in the source sentencesas anchors (A).
For our Chinese-English experi-ments, we use a simple heuristic that equates asanchors, single-word chunks whose correspondingword class belongs to closed-word classes, bear-ing a close resemblance to (Setiawan et al, 2007).In total, we consider 21 part-of-speech tags; someof which are as follow: VC (copula), DEG, DEG,DER, DEV (de-related), PU (punctuation), AD(adjectives) and P (prepositions).Next we generate all possible chunks ?(?
)as previously described in Sec.
3.
We then de-fine a functionMinC(?, j1, j2) which returns theshortest chunk that can span from j1 to j2.
If(f j2j1 /ei2i1) ?
?, then MinC returns (f j2j1 /ei2i1).The algorithm to extract MOS takes ?
and ananchor a = (f j2j1 /ei2i1) as input; and outputs thechunk that qualifies as MOS or none.
Alg.
1provides the algorithm to extract the right MOS;the algorithm to extract the left MOS is identicalto Alg.
1, except that it scans for chunks to theleft of the anchor.
In Alg.
1, there are two in-termediate parameters si and ei which representthe active search range and should initially be setto j2 + 1 and |F | respectively.
Once we obtaina,ML(a) andMR(a), we computeOL(ML(a), a)and OR(MR(a), a) and are ready for training.1268To estimate POL and POR , we train discrimi-native classifiers that predict the orientation val-ues and use the normalized posteriors at decodingtime as additional feature scores in SMT?s log lin-ear framework.
We train the classifiers on a richset of binary features ranging from lexical to part-of-speech (POS) and to syntactic features.Algorithm 1: Function MRExinput : a = (f j2j1 /ei2i1), si, ei: int; ?
: chunksoutput: (f j4j3 /ei4i3) : chunk or ?
(f j4j3 /ei4i3) = MinC(?, j2 + 1, ei)if (j3 == j2 + 1 ?
j4 == ei) then?
f j4j3 /ei4i3elseif (j2 + 1 == ei) then?
?elseif (ei-2 ?
si) then?MREx(a, si, ei?
1,?
)elsem = d(si+ei)/2e(f j4j3 /ei4i4) = MinC(?, j2 + 1,m)if (j3 == j2 + 1) thenc = MREx(a,m, ei?
1,?
)if (c == ?)
then?
f j4j3 /ei4i3else?
cendelse?MREx(a, si,m?
1,?
)endendendendSuppose a = (f j2j1 /ei2i1), ML(a) = (f j4j3 /ei4i3)and ML(a) = (f j6j5 /ei6i5), then based on the con-text?s location, the elementary features employedin our classifiers can be categorized into:1. anchor-related: slex (the actual word off j2j1 ), spos (part-of-speech (POS) tag ofslex), sparent (spos?s parent in the parsetree), tlex (ei2i1?s actual target word)..2. surrounding: lslex (the previous word /f j1?1j1?1 ), rslex (the next word / f j2+1j2+1 ), lspos(lslex?s POS tag), rspos (rslex?s POStag), lsparent (lslex?s parent), rsparent(rslex?s parent).3. non-local: lanchorslex (the previousanchor?s word) , ranchorslex (the next an-chor?s word), lanchorspos (lanchorslex?sPOS tag), ranchorspos (ranchorslex?sPOS tag).4.
MOS-related: mosl int slex (the actualword of f j3j3 ), mosl ext slex (the actual wordof f j3j3 ), mosl int spos (mosl int slex?sPOS tag), mosl ext spos (mosl ext spos?sPOS tag), mosr int slex (the actual word off j3j3 ), mosr ext slex (the actual word of f j3j3 ),mosr int spos (mosr int slex?s POS tag),mosr ext spos (mosr ext spos?s POS tag).For Model 1, we train one classifier each forPOR and POL .
For Model 2-4, we train four clas-sifiers for POL for each value of OR.
We use onlythe MOS features for Model 3 and 4.
Addition-ally, we augment the feature set with compoundfeatures, e.g.
conjunction of the lexical of the an-chor and the lexical of the left and the right an-chors.
Although they increase the number of fea-tures significantly, we found that these compoundfeatures are empirically beneficial.We come up with > 50 types of features, whichconsist of a combination of elementary and com-pound features.
In total, we generate hundreds ofmillions of such features from the training data.To keep the number features to a manageable size,we employ the L1-regularization in training to en-force sparse solutions, using the off-the-shelf LIB-LINEAR toolkit (Fan et al, 2008).
After training,the number of features in our classifiers decreasesto below 5 million features for each classifier.We train PML and PMR via the relative fre-quency principle.
To avoid the sparsity issue, werepresent ML as (mosl int spos,mosl ext spos)and MR as (mosr int spos,mosr ext spos).
Wecondition PML and PMR only on spos and the ori-entation, estimating them as follow:P (ML|spos, OL) =N(ML, spos, OL)N(spos, OL)P (MR|spos, OR) =N(MR, spos, OR)N(spos, OR)where N returns the count of the events in thetraining data.1269Target string (w/ source index) Symbol(s) read Op.
Stack(s)(1) Xc have5 dipl.6 rels.
[5][6] S,S,R Xc:[5-6](2) Xd one11 of10 few8 countries9 [11][10] S,S,R [10-11]that7 Xc(3) [8][9] S,S,R,R [8-11](4) [7] S [8-11][7](5) Xc:[5,6] S Xd:[8-11][7][5,6](6) Xb Xd with3 North4 Korea Xd:[8-11][7][5,6] S [8-11][7][5,6](7) [3][4] S,S,R,R Xb:[8-11][7][3-6](8) Xa Australia1 is2 Xb [1][2] S,S,R [1-2](9) Xb:[8-11][7][3,6] S,A Xa:[1-2][8-11][7][3,6]Table 3: The application of the shift-reduce parsing algorithm, which corresponds to Table 2?s derivation.6 DecodingIntegrating the TNO Model into syntax-basedSMT systems is non-trivial, especially with theMOS modeling.
The method described in Sec.
3assumes ?
= (F,E,?
), thus it is only applicableat training or at the last stage of decoding.
Sincemany reordering decisions may have been madeat the earlier stages, the late application of TNOmodel would limit the utility of the model.
In thissection, we describe an algorithm that facilitatesthe incremental construction of MOS and the com-putation of TNO model on partial derivations.The algorithm bears a close resemblance to theshift-reduce algorithm where a stack is used to ac-cumulate (partial) information about a, ML andMR for each a ?
A in the derivation.
This al-gorithm takes an input stream and applies eitherthe shift or the reduce operations starting from thebeginning until the end of the stream.
The shift op-eration advances the input stream by one symboland push the symbol into the stack; while the re-duce operation applies some reduction rule to thetopmost elements of the stack.
The algorithm ter-minates at the end of the input stream where theresulting stack will be propagated to the parent forthe later stage of decoding.
In our case, the in-put stream is the target string of the rule and thesymbol is the corresponding source index of theelements of the target string.
The reduction rulelooks at two indices and merge them if they areadjacent (i.e.
has no intervening phrase).
We for-bid the application of the reduction rule to anchors.Table 3 shows the execution trace of the algorithmfor the derivation described in Table 2.As shown, the algorithm starts with an emptystack.
It then projects the source index to the cor-responding target word and then enumerates thetarget string in a left to right fashion.
If it findsa target word with a source index, it applies theshift operation, pushing the index to the stack.
Un-less the symbol corresponds to an anchor, it triesto apply the reduce operation.
Line (4) indicatesthe special treatment to the anchor.
If the symbolread is a nonterminal, then we push the entire stackthat corresponds to that nonterminal.
For example,when the algorithm reads Xd at line (6), it pushesthe entire stack from line (5).This algorithm facilitates the incremental con-struction of MOS which may cross rule bound-aries.
For example, at the end of the application ofXd at line (5), the current left MOS is [5-6].
How-ever, the algorithm grows it to [3-6] after the appli-cation of ruleXb at line (7).
Furthermore, it allowsus to compute the models from partial hypothesis.For example, at line (5), we can compute POL byconsidering [5,6] as ML to be updated with [3,6]in line (7).
This way, we expect our TNO modelwould play a bigger role at decoding time.Specific to SCFG-based translation, the valuesof OL and OR are identical in the partial or inthe full derivations.
For example, the orientationvalues of de (that)?s left neighbor is always RA.This statement holds, even though at the end ofSection 2, we stated that de (that)?s left neigh-bor may have other orientation values, i.e.
RGfor CL(a) = (f66 /e99).
The formal proof is omit-ted, but the intuition comes from the fact that thederivations for SCFG-based translation are sub-set of ?(?)
and that (f66 /e99) will never becomeML forMinC(CL(a), a) respectively (chunk thatspans a and CL).
Consequently, for Model 1 andModel 2, we can obtain the model score earlier inthe decoding process.12707 ExperimentsOur baseline systems is a state-of-the-art string-to-dependency system (Shen et al, 2008).
Thesystem is trained on 10 million parallel sentencesthat are available to the Phase 1 of the DARPABOLT Chinese-English MT task.
The training cor-pora include a mixed genre of newswire, weblog,broadcast news, broadcast conversation, discus-sion forums and comes from various sources suchas LDC, HK Law, HK Hansard and UN data.In total, our baseline model employs about40 features, including four from our proposedTwo-Neighbor Orientation model.
In addition tothe standard features including the rule transla-tion probabilities, we incorporate features that arefound useful for developing a state-of-the-art base-line, such as the provenance features (Chiang etal., 2011).
We use a large 6-gram language model,which was trained on 10 billion English wordsfrom multiple corpora, including the English sideof our parallel corpus plus other corpora such asGigaword (LDC2011T07) and Google News.
Wealso train a class-based language model (Chen,2009) on two million English sentences selectedfrom the parallel corpus.
As the backbone ofour string-to-dependency system, we train 3-grammodels for left and right dependencies and un-igram for head using the target side of the bi-lingual training data.
To train our Two-NeighborOrientation model, we select a subset of 5 millionaligned sentence pairs.For the tuning and development sets, we setaside 1275 and 1239 sentences selected fromLDC2010E30 corpus.
We tune the decodingweights with PRO (Hopkins and May, 2011) tomaximize BLEU-TER.
As for the blind test set,we report the performance on the NIST MT08evaluation set, which consists of 691 sentencesfrom newswire and 666 sentences from weblog.We pick the weights that produce the highest de-velopment set scores to decode the test set.Table 4 summarizes the experimental results onNIST MT08 newswire and weblog.
In column 2,we report the classification accuracy on a subset oftraining data.
Note that these numbers are for ref-erence only and not directly comparable with eachother since the features used in these classifiersinclude several gold standard information, suchas the anchors?
target words, the anchors?
MOS-related features (Model 3 & 4) and the orientationof the right MOS (Model 2-4); all of which haveAcc MT08 nw MT08 wbBLEU TER BLEU TERS2D - 36.77 53.28 26.34 57.41M1 72.5 37.60 52.70 27.59 56.33M2 77.4 37.86 52.68 27.74 56.11M3 84.5 38.02 52.42 28.22 55.82M4 84.5 38.55 52.41 28.44 56.45Table 4: The NIST MT08 results on newswire (nw) and we-blog (wb) genres.
S2D is the baseline string-to-dependencysystem (line 1), on top of which Two-Neighbor OrientationModel 1 to 4 are employed (line 2-5).
The best TER andBLEU results on each genre are in bold.
For BLEU, higherscores are better, while for TER, lower scores are better.to be predicted at decoding time.In columns 2 and 4, we report the BLEU scores,while in columns 3 and 5, we report the TERscores.
The performance of our baseline string-to-dependency syntax-based SMT is shown in thefirst line, followed by the performance of our Two-Neighbor Orientation model starting from Model1 to Model 4.
As shown, the empirical resultsconfirm our intuition that SMT can greatly benefitfrom reordering model that incorporate cross-unitcontextual information.Model 1 provides most of the gain across thetwo genres of around +0.9 to +1.2 BLEU and -0.5to -1.1 TER.
Model 2 which conditions POL onOR provides an additional +0.2 BLEU improve-ment on BLEU score consistently across the twogenres.
As shown in line 4, we see a strongerimprovement in the inclusion of MOS-related in-formation as features in Model 3.
In newswire,Model 3 gives an additional +0.4 BLEU and -0.2TER, while in weblog, it gives a stronger improve-ment of an additional +0.5 BLEU and -0.3 TER.The inclusion of explicit MOS modeling in Model4 gives a significant BLEU score improvement of+0.5 but no TER improvement in newswire.
Inweblog, Model 4 gives a mixed results of +0.2BLEU score improvement and a hit of +0.6 TER.We conjecture that the weblog text has a more am-biguous orientation span that are more challengingto learn.
In total, our TNO model gives an encour-aging result.
Our most advanced model gives sig-nificant improvement of +1.8 BLEU/-0.8 TER innewswire domain and +2.1 BLEU/-1.0 TER overa strong string-to-dependency syntax-based SMTenhanced with additional state-of-the-art features.12718 Related WorkOur work intersects with existing work in manydifferent respects.
In this section, we mainly focuson work related to the probabilistic conditioningof our TNO model and the MOS modeling.Our TNO model is closely related to the Uni-gram Orientation Model (UOM) (Tillman, 2004),which is the de facto reordering model of phrase-based SMT (Koehn et al, 2007).
UOM viewsreordering as a process of generating (b, o) in aleft-to-right fashion, where b is the current phrasepair and o is the orientation of b with the pre-viously generated phrase pair b?.
UOM makesstrong independence assumptions and formulatesthe model as P (o|b).
Tillmann and Zhang (2007)proposed a Bigram Orientation Model (BOM) toinclude both phrase pairs (b and b?)
into the model.Their original intent is to model P (o, b|o?, b?
), butperhaps due to sparsity concerns, they settle withP (o|b, b?
), dropping the conditioning on the pre-vious orientation o?.
Subsequent improvementsuse the P (o|b, b?)
formula, for example, for in-corporating various linguistics feature like part-of-speech (Zens and Ney, 2006), syntactic (Chang etal., 2009), dependency information (Bach et al,2009) and predicate-argument structure (Xiong etal., 2012).
Our TNO model is more faithful to theBOM?s original formulation.Our MOS concept is also closely related to hi-erarchical reordering model (Galley and Manning,2008) in phrase-based decoding, which computeso of b with respect to a multi-block unit that maygo beyond b?.
They mainly use it to avoid overes-timating ?discontiguous?
orientation but fall shortin modeling the multi-block unit, perhaps due todata sparsity issue.
Our MOS is also closely re-lated to the efforts of modeling the span of hi-erarchical phrases in formally syntax-based SMT.Early works reward/penalize spans that respect thesyntactic parse constituents of an input sentence(Chiang, 2005), and (Marton and Resnik, 2008).
(Xiong et al, 2009) learn the boundaries fromparsed and aligned training data, while (Xiong etal., 2010) learn the boundaries from aligned train-ing data alone.
Recent work couples span mod-eling tightly with reordering decisions, either byadding an additional feature for each hierarchicalphrase (Chiang et al, 2008; Shen et al, 2009) orby refining the nonterminal label (Venugopal etal., 2009; Huang et al, 2010; Zollmann and Vo-gel, 2011).
Common to this work is that the spansmodeled may not correspond to MOS, which maybe suboptimal as discussed in Sec.
3.In equating anchors with the function wordclass, our work, particularly Model 1, is closelyrelated to the function word-centered model of Se-tiawan et al (2007) and Setiawan et al (2009).However, we provide a discriminative treatmentto the model to include a richer set of features in-cluding the MOS modeling.
Our work in incorpo-rating global context also intersects with existingwork in Preordering Model (PM), e.g.
(Niehuesand Kolss, 2009; Costa-jussa` and Fonollosa, 2006;Genzel, 2010; Visweswariah et al, 2011; Trombleand Eisner, 2009).
The goal of PM is to reorder theinput sentence F into F ?
whose order is closer tothe target language order, whereas the goal of ourmodel is to directly reorder F into the target lan-guage order.
The crucial difference is that we haveto integrate our model into SMT decoder, which ishighly non-trivial.9 ConclusionWe presented a novel approach to address a kindof long-distance reordering that requires globalcross-boundary contextual information.
Our ap-proach, which we formulate as a Two-NeighborOrientation model, includes the joint modeling oftwo orientation decisions and the modeling of themaximal span of the reordered chunks through theconcept of Maximal Orientation Span.
We de-scribe four versions of the model and implementan algorithm to integrate our proposed model intoa syntax-based SMT system.
Empirical resultsconfirm our intuition that incorporating cross-boundaries contextual information improves trans-lation quality.
In a large scale Chinese-to-Englishtranslation task, we achieve a significant improve-ment over a strong baseline.
In the future, we hopeto continue this line of research, perhaps by learn-ing to identify anchors automatically from trainingdata, incorporating a richer set of linguistics fea-tures such as dependency structure and strength-ening the modeling of Maximal Orientation Span.AcknowledgementsWe would like to acknowledge the support of DARPA un-der Grant HR0011-12-C-0015 for funding part of this work.The views, opinions, and/or findings contained in this arti-cle/presentation are those of the author/ presenter and shouldnot be interpreted as representing the official views or poli-cies, either expressed or implied, of the DARPA.1272ReferencesNguyen Bach, Qin Gao, and Stephan Vogel.
2009.Source-side dependency tree reordering models withsubtree movements and constraints.
In Proceed-ings of the Twelfth Machine Translation Summit(MTSummit-XII), Ottawa, Canada, August.
Interna-tional Association for Machine Translation.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D. Manning.
2009.
Discriminativereordering with Chinese grammatical relations fea-tures.
In Proceedings of the Third Workshop on Syn-tax and Structure in Statistical Translation (SSST-3)at NAACL HLT 2009, pages 51?59, Boulder, Col-orado, June.
Association for Computational Linguis-tics.Stanley Chen.
2009.
Shrinking exponential languagemodels.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 468?476, Boulder, Col-orado, June.
Association for Computational Linguis-tics.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 224?233, Honolulu,Hawaii, October.David Chiang, Steve DeNeefe, and Michael Pust.2011.
Two easy improvements to lexical weighting.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 455?460, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL?05), pages263?270, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.Marta R. Costa-jussa` and Jose?
A. R. Fonollosa.
2006.Statistical machine reordering.
In Proceedings ofthe 2006 Conference on Empirical Methods in Nat-ural Language Processing, pages 70?76, Sydney,Australia, July.
Association for Computational Lin-guistics.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 848?856, Honolulu, Hawaii, October.
As-sociation for Computational Linguistics.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine trans-lation.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (Coling2010), pages 376?384, Beijing, China, August.
Col-ing 2010 Organizing Committee.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1352?1362, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Zhongqiang Huang, Martin Cmejrek, and BowenZhou.
2010.
Soft syntactic constraints for hierar-chical phrase-based translation using latent syntac-tic distributions.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 138?147, Cambridge, MA, Octo-ber.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion, June.Yuval Marton and Philip Resnik.
2008.
Soft syntac-tic constraints for hierarchical phrased-based trans-lation.
In Proceedings of The 46th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 1003?1011, Columbus, Ohio, June.Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,and Kazuteru Ohashi.
2006.
A clustered globalphrase reordering model for statistical machinetranslation.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 713?720, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Jan Niehues and Muntsin Kolss.
2009.
A POS-basedmodel for long-range reorderings in SMT.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, pages 206?214, Athens, Greece,March.
Association for Computational Linguistics.Hendra Setiawan, Min-Yen Kan, and Haizhou Li.2007.
Ordering phrases with function words.
InProceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 712?719, Prague, Czech Republic, June.
Association forComputational Linguistics.Hendra Setiawan, Min Yen Kan, Haizhou Li, and PhilipResnik.
2009.
Topological ordering of functionwords in hierarchical phrase-based translation.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing1273of the AFNLP, pages 324?332, Suntec, Singapore,August.
Association for Computational Linguistics.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.A new string-to-dependency machine translation al-gorithm with a target dependency language model.In Proceedings of ACL-08: HLT, pages 577?585,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective use of lin-guistic and contextual information for statistical ma-chine translation.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 72?80, Singapore, August.
Asso-ciation for Computational Linguistics.Christoph Tillman.
2004.
A unigram orienta-tion model for statistical machine translation.
InHLT-NAACL 2004: Short Papers, pages 101?104,Boston, Massachusetts, USA, May 2 - May 7.
Asso-ciation for Computational Linguistics.Christoph Tillmann and Tong Zhang.
2007.
Ablock bigram prediction model for statistical ma-chine translation.
ACM Transactions on Speech andLanguage Processing (TSLP), 4(3).Roy Tromble and Jason Eisner.
2009.
Learning linearordering problems for better translation.
In Proceed-ings of the 2009 Conference on Empirical Methodsin Natural Language Processing, pages 1007?1016,Singapore, August.
Association for ComputationalLinguistics.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference grammars:Softening syntactic constraints to improve statisti-cal machine translation.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 236?244,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Karthik Visweswariah, Rajakrishnan Rajkumar, AnkurGandhe, Ananthakrishnan Ramanathan, and JiriNavratil.
2011.
A word reordering model for im-proved machine translation.
In Proceedings of the2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 486?496, Edinburgh,Scotland, UK., July.
Association for ComputationalLinguistics.Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.2009.
A syntax-driven bracketing model for phrase-based translation.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on NaturalLanguage Processing of the AFNLP, pages 315?323, Suntec, Singapore, August.
Association forComputational Linguistics.Deyi Xiong, Min Zhang, and Haizhou Li.
2010.Learning translation boundaries for phrase-baseddecoding.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 136?144, Los Angeles, California,June.
Association for Computational Linguistics.Deyi Xiong, Min Zhang, and Haizhou Li.
2012.
Mod-eling the translation of predicate-argument structurefor smt.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 902?911, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Richard Zens and Hermann Ney.
2006.
Discrimina-tive reordering models for statistical machine trans-lation.
In Human Language Technology Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics (HLT-NAACL):Proceedings of the Workshop on Statistical MachineTranslation, pages 55?63, New York City, NY, June.Association for Computational Linguistics.Andreas Zollmann and Stephan Vogel.
2011.
A word-class approach to labeling pscfg rules for machinetranslation.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 1?11,Portland, Oregon, USA, June.
Association for Com-putational Linguistics.1274
