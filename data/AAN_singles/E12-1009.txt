Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 77?87,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsThe Best of Both Worlds ?
A Graph-based Completion Model forTransition-based ParsersBernd Bohnet and Jonas KuhnUniversity of StuttgartInstitute for Natural Language Processing{bohnet,jonas}@ims.uni-stuttgart.deAbstractTransition-based dependency parsers areoften forced to make attachment deci-sions at a point when only partial infor-mation about the relevant graph configu-ration is available.
In this paper, we de-scribe a model that takes into account com-plete structures as they become availableto rescore the elements of a beam, com-bining the advantages of transition-basedand graph-based approaches.
We also pro-pose an efficient implementation that al-lows for the use of sophisticated featuresand show that the completion model leadsto a substantial increase in accuracy.
Weapply the new transition-based parser on ty-pologically different languages such as En-glish, Chinese, Czech, and German and re-port competitive labeled and unlabeled at-tachment scores.1 IntroductionBackground.
A considerable amount of recentresearch has gone into data-driven dependencyparsing, and interestingly throughout the continu-ous process of improvements, two classes of pars-ing algorithms have stayed at the centre of at-tention, the transition-based (Nivre, 2003) vs. thegraph-based approach (Eisner, 1996; McDonaldet al 2005).1 The two approaches apply funda-mentally different strategies to solve the task offinding the optimal labeled dependency tree overthe words of an input sentence (where supervisedmachine learning is used to estimate the scoringparameters on a treebank).The transition-based approach is based on theconceptually (and cognitively) compelling idea1More references will be provided in sec.
2.that machine learning, i.e., a model of linguis-tic experience, is used in exactly those situationswhen there is an attachment choice in an other-wise deterministic incremental left-to-right pars-ing process.
As a new word is processed, theparser has to decide on one out of a small num-ber of possible transitions (adding a dependencyarc pointing to the left or right and/or pushing orpopping a word on/from a stack representation).Obviously, the learning can be based on the fea-ture information available at a particular snapshotin incremental processing, i.e., only surface in-formation for the unparsed material to the right,but full structural information for the parts of thestring already processed.
For the completely pro-cessed parts, there are no principled limitations asregards the types of structural configurations thatcan be checked in feature functions.The graph-based approach in contrast empha-sizes the objective of exhaustive search over allpossible trees spanning the input words.
Com-monly, dynamic programming techniques areused to decide on the optimal tree for each par-ticular word span, considering all candidate splitsinto subspans, successively building longer spansin a bottom-up fashion (similar to chart-basedconstituent parsing).
Machine learning drivesthe process of deciding among alternative can-didate splits, i.e., feature information can drawon full structural information for the entire ma-terial in the span under consideration.
However,due to the dynamic programming approach, thefeatures cannot use arbitrarily complex structuralconfigurations: otherwise the dynamic program-ming chart would have to be split into exponen-tially many special states.
The typical featuremodels are based on combinations of edges (so-77called second-order factors) that closely followthe bottom-up combination of subspans in theparsing algorithm, i.e., the feature functions de-pend on the presence of two specific dependencyedges.
Configurations not directly supported bythe bottom-up building of larger spans are morecumbersome to integrate into the model (since thecombination algorithm has to be adjusted), in par-ticular for third-order factors or higher.Empirically, i.e., when applied in supervisedmachine learning experiments based on existingtreebanks for various languages, both strategies(and further refinements of them not mentionedhere) turn out roughly equal in their capabilityof picking up most of the relevant patterns well;some subtle strengths and weaknesses are com-plementary, such that stacking of two parsers rep-resenting both strategies yields the best results(Nivre and McDonald, 2008): in training and ap-plication, one of the parsers is run on each sen-tence prior to the other, providing additional fea-ture information for the other parser.
Another suc-cessful technique to combine parsers is voting ascarried out by Sagae and Lavie (2006).The present paper addresses the question ifand how a more integrated combination of thestrengths of the two strategies can be achievedand implemented efficiently to warrant competi-tive results.The main issue and solution strategy.
In or-der to preserve the conceptual (and complexity)advantages of the transition-based strategy, theintegrated algorithm we are looking for has tobe transition-based at the top level.
The advan-tages of the graph-based approach ?
a more glob-ally informed basis for the decision among dif-ferent attachment options ?
have to be includedas part of the scoring procedure.
As a prerequi-site, our algorithm will require a memory for stor-ing alternative analyses among which to choose.This has been previously introduced in transition-based approaches in the form of a beam (Johans-son and Nugues, 2006): rather than representingonly the best-scoring history of transitions, the kbest-scoring alternative histories are kept around.As we will indicate in the following, the mereaddition of beam search does not help overcomea representational key issue of transition-basedparsing: in many situations, a transition-basedparser is forced to make an attachment decisionfor a given input word at a point where no or onlypartial information about the word?s own depen-dents (and further decendents) is available.
Fig-ure 1 illustrates such a case.Figure 1: The left set of brackets indicates materialthat has been processed or is under consideration; onthe right is the input, still to be processed.
Access to in-formation that is yet unavailable would help the parserto decide on the correct transition.Here, the parser has to decide whether to create anedge between house and with or between boughtand with (which is technically achieved by firstpopping house from the stack and then adding theedge).
At this time, no information about the ob-ject of with is available; with fails to provide whatwe call a complete factor for the calculation of thescores of the alternative transitions under consid-eration.
In other words, the model cannot makeuse of any evidence to distinguish between thetwo examples in Figure 1, and it is bound to getone of the two cases wrong.Figure 2 illustrates the same case from the per-spective of a graph-based parser.Figure 2: A second order model as used in graph-basedparsers has access to the crucial information to buildthe correct tree.
In this case, the parser condsiders theword friend (as opposed to garden, for instance) as itintroduces the bold-face edge.Here, the combination of subspans is performedat a point when their internal structure has beenfinalized, i.e., the attachment of with (to boughtor house) is not decided until it is clear that friendis the object of with; hence, the semantically im-portant lexicalization of with?s object informs thehigher-level attachment decision through a so-called second order factor in the feature model.78Given a suitable amount of training data, themodel can thus learn to make the correct deci-sion.
The dynamic-programming based graph-based parser is designed in such a way that anyscore calculation is based on complete factors forthe subspans that are combined at this point.Note that the problem for the transition-basedparser cannot be remedied by beam search alone.If we were to keep the two options for attach-ing with around in a beam (say, with a slightlyhigher score for attachment to house, but withbought following narrowly behind), there wouldbe no point in the further processing of the sen-tence at which the choice could be corrected: thetransition-based parser still needs to make the de-cision that friend is attached to with, but this willnot lead the parser to reconsider the decision madeearlier on.The strategy we describe in this paper appliesin this very type of situation: whenever infor-mation is added in the transition-based parsingprocess, the scores of all the histories stored inthe beam are recalculated based on a scoringmodel inspired by the graph-based parsing ap-proach, i.e., taking complete factors into accountas they become incrementally available.
As a con-sequence the beam is reordered, and hence, theincorrect preference of an attachment of with tohouse (based on incomplete factors) can later becorrected as friend is processed and the completesecond-order factor becomes available.2The integrated transition-based parsing strategyhas a number of advantages:(1) We can integrate and investigate a number ofthird order factors, without the need to implementa more complex parsing model each time anew toexplore the properties of such distinct model.
(2) The parser with completion model main-tains the favorable complexity of transition-basedparsers.
(3) The completion model compensates for thelower accuracy of cases when only incomplete in-formation is available.
(4) The parser combines the two leading pars-ing paradigms in a single efficient parser with-out stacking the two approaches.
Therefore the2Since search is not exhaustive, there is of course a slightdanger that the correct history drops out of the beam beforecomplete information becomes available.
But as our experi-ments show, this does not seem to be a serious issue empiri-cally.parser requires only one training phase (withoutjackknifing) and it uses only a single transition-based decoder.The structure of this paper is as follows.
In Sec-tion 2, we discuss related work.
In Section 3, weintroduce our transition-based parser and in Sec-tion 4 the completion model as well as the im-plementation of third order models.
In Section 5,we describe experiments and provide evaluationresults on selected data sets.2 Related WorkKudo and Matsumoto (2002) and Yamada andMatsumoto (2003) carried over the idea for de-terministic parsing by chunks from Abney (1991)to dependency parsing.
Nivre (2003) describesin a more strict sense the first incremental parserthat tries to find the most appropriate dependencytree by a sequence of local transitions.
In orderto optimize the results towards a more globallyoptimal solution, Johansson and Nugues (2006)first applied beam search, which leads to a sub-stantial improvment of the results (cf.
also (Titovand Henderson, 2007)).
Zhang and Clark (2008)augment the beam-search algorithm, adapting theearly update strategy of Collins and Roark (2004)to dependency parsing.
In this approach, theparser stops and updates the model when the or-acle transition sequence drops out of the beam.In contrast to most other approaches, the trainingprocedure of Zhang and Clark (2008) takes thecomplete transition sequence into account as it iscalculating the update.
Zhang and Clark compareaspects of transition-based and graph-based pars-ing, and end up using a transition-based parserwith a combined transition-based/second-ordergraph-based scoring model (Zhang and Clark,2008, 567), which is similar to the approach wedescribe in this paper.
However, their approachdoes not involve beam rescoring as the partialstructures built by the transition-based parser aresubsequently augmented; hence, there are cases inwhich our approach is able to differentiate basedon higher-order factors that go unnoticed by thecombined model of (Zhang and Clark, 2008, 567).One step beyond the use of a beam is a dynamicprogramming approach to carry out a full searchin the state space, cf.
(Huang and Sagae, 2010;Kuhlmann et al 2011).
However, in this caseone has to restrict the employed features to a setwhich fits to the elements composed by the dy-79namic programming approach.
This is a trade-offbetween an exhaustive search and a unrestricted(rich) feature set and the question which providesa higher accuracy is still an open research ques-tion, cf.
(Kuhlmann et al 2011).Parsing of non-projective dependency trees isan important feature for many languages.
Atfirst most algorithms were restricted to projec-tive dependency trees and used pseudo-projectiveparsing (Kahane et al 1998; Nivre and Nilsson,2005).
Later, additional transitions were intro-duced to handle non-projectivity (Attardi, 2006;Nivre, 2009).
The most common strategy usesthe swap transition (Nivre, 2009; Nivre et al2009), an alternative solution uses two planesand a switch transition to switch between the twoplanes (Go?mez-Rodr?
?guez and Nivre, 2010).Since we use the scoring model of a graph-based parser, we briefly review releated workon graph-based parsing.
The most well knowngraph-based parser is the MST (maximum span-ning tree) parser, cf.
(McDonald et al 2005; Mc-Donald and Pereira, 2006).
The idea of the MSTparser is to find the highest scoring tree in a graphthat contains all possible edges.
Eisner (1996)introduced a dynamic programming algorithm tosolve this problem efficiently.
Carreras (2007) in-troduced the left-most and right-most grandchildas factors.
We use the factor model of Carreras(2007) as starting point for our experiments, cf.Section 4.
We extend Carreras (2007) graph-based model with factors involving three edgessimilar to that of Koo and Collins (2010).3 Transition-based Parser with a BeamThis section specifies the transition-based beam-search parser underlying the combined approachmore formally.
Sec.
4 will discuss the graph-based scoring model that we are adding.The input to the parser is a word string x,the goal is to find the optimal set y of labelededges xi?l xj forming a dependency tree over x?{root}.
We characterize the state of a transition-based parser as pii=?
?i, ?i, yi, hi?, pii ?
?, the setof possible states.
?i is a stack of words from xthat are still under consideration; ?i is the inputbuffer, the suffix of x yet to be processed; yi theset of labeled edges already assigned (a partial la-beled dependency tree); hi is a sequence record-ing the history of transitions (from the set of op-erations ?
= {shift, left-arcl, right-arcl, reduce,swap}) taken up to this point.
(1) The initial state pi0 has an empty stack, theinput buffer is the full input string x, and the edgeset is empty.
(2) The (partial) transition function?
(pii, t) : ?
x ?
?
?
maps a state and an opera-tion t to a new state pii+1.
(3) Final states pif arecharacterized by an empty input buffer and stack;no further transitions can be taken.The transition function is informally defined asfollows: The shift transition removes the first ele-ment of the input buffer and pushes it to the stack.The left-arcl transition adds an edge with label lfrom the first word in the buffer to the word ontop of the stack, removes the top element fromthe stack and pushes the first element of the inputbuffer to the stack.The right-arcl transition adds an edge from wordon top of the stack to the first word in the inputbuffer and removes the top element of the inputbuffer and pushes that element onto the stack.The reduce transition pops the top word from thestack.The swap changes the order of the two top el-ements on the stack (possibly generating non-projkective trees).When more than one operation is applicable, ascoring function assigns a numerical value (basedon a feature vector and a weight vector trainedby supervised machine learning) to each possi-ble continuation.
When using a beam search ap-proach with beam size k, the highest-scoring k al-ternative states with the same length n of transi-tion history h are kept in a set ?beamn?.In the beam-based parsing algorithm (cf.
thepseudo code in Algorithm 1), all candidate statesfor the next set ?beamn+1?
are determined usingthe transition function ?
, but based on the scor-ing function, only the best k are preserved.
(Fi-nal) states to which no more transitions apply arecopied to the next state set.
This means that onceall transition paths have reached a final state, theoverall best-scoring states can be read off the fi-nal ?beamn?.
The y of the top-scoring state is thepredicted parse.Under the plain transition-based scoringregime scoreT , the score for a state pi is the sumof the ?local?
scores for the transitions ti in thestate?s history sequence:scoreT (pi) =?|h|i=0 w ?
f(pii, ti)80Algorithm 1: Transition-based parser// x is the input sentence, k is the beam size?0 = ?, ?0 = x, y0 = ?, h = ?pi0 ?
?
?0, ?0, y0, h0?
// initial parts of a statebeam0?
{pi0} // create initial staten?
0 // iterationrepeatn?
n+ 1for all pij ?
beamn?1 dotransitions?
possible-applicable-transition (pij)// if no transition is applicable keep state pij :if transitions = ?
then beamn ?
beamn ?
{pij}else for all ti ?
transitions do// apply the transition i to state jpi ?
?
(pij , ti)beamn ?
beamn ?
{pi}// end for// end forsort beamn due to the score(pij)beamn ?
sublist (beamn, 0, k)until beamn?1 = beamn // beam changed?w is the weight vector.
Note that the featuresf(pii, ti) can take into account all structural andlabeling information available prior to taking tran-sition ti, i.e., the graph built so far, the words (andtheir part of speech etc.)
on the stack and in theinput buffer, etc.
But if a larger graph configu-ration involving the next word evolves only later,as in Figure 1, this information is not taken intoaccount in scoring.
For instance, if the featureextraction uses the subcategorization frame of aword under consideration to compute a score, it isquite possible that some dependents are still miss-ing and will only be attached in a future transition.4 Completion ModelWe define an augmented scoring function whichcan be used in the same beam-search algorithm inorder to ensure that in the scoring of alternativetransition paths, larger configurations can be ex-ploited as they are completed in the incrementalprocess.
The feature configurations can be largelytaken from graph-based approaches.
Here, spansfrom the string are assembled in a bottom-up fash-ion, and the scoring for an edge can be based onstructurally completed subspans (?factors?
).Our completion model for scoring a state pinincorporates factors for all configurations (match-ing the extraction scheme that is applied) that arepresent in the partial dependency graph yn builtup to this point, which is continuously augmented.This means if at a given point n in the transitionpath, complete information for a particular config-uration (e.g., a third-order factor involving a head,its dependent and its grand-child dependent) isunavailable, scoring will ignore this factor at timen, but the configuration will inform the scoringlater on, maybe at point n+ 4, when the completeinformation for this factor has entered the partialgraph yn+4.We present results for a number of differentsecond-order and third-order feature models.Second Order Factors.
We start with themodel introduced by Carreras (2007).
Figure 3illustrates the factors used.Figure 3: Model 2a.
Second order factors of Carreras(2007).
We omit the right-headed cases, which aremirror images.
The model comprises a factoring intoone first order part and three second order factors (2-4): 1) The head (h) and the dependent (c); 2) the head,the dependent and the left-most (or right-most) grand-child in between (cmi); 3) the head, the dependent andthe right-most (or left-most) grandchild away from thehead (cmo).
4) the head, the dependent and betweenthose words the right-most (or left-most) sibling (ci).Figure 4: 2b.
The left-most dependent of the head orthe right-most dependent in the right-headed case.Figure 4 illustrates a new type of factor we use,which includes the left-most dependent in the left-headed case and symmetricaly the right-most sib-ling in the right-head case.Third Order Factors.
In addition to the secondorder factors, we investigate combinations of thirdorder factors.
Figure 5 and 6 illustrate the thirdorder factors, which are similar to the factors ofKoo and Collins (2010).
They restrict the factorto the innermost sibling pair for the tri-siblings81and the outermost pair for the grand-siblings.
Weuse the first two siblings of the dependent fromthe left side of the head for the tri-siblings andthe first two dependents of the child for the grand-siblings.
With these factors, we aim to capturenon-projective edges and subcategorization infor-mation.
Figure 7 illustrates a factor of a sequenceof four nodes.
All the right headed variants aresymmetrically and left out for brevity.Figure 5: 3a.
The first two children of the head, whichdo not include the edge between the head and the de-pendent.Figure 6: 3b.
The first two children of the dependent.Figure 7: 3c.
The right-most dependent of the right-most dependent.Integrated approach.
To obtain an integratedsystem for the various feature models, the scoringfunction of the transition-based parser from Sec-tion 3 is augmented by a family of scoring func-tions scoreGm for the completion model, wheremis from 2a, 2b, 3a etc., x is the input string, and yis the (partial) dependency tree built so far:scoreTm(pi) = scoreT (pi) + scoreGm(x, y)The scoring function of the completion modeldepends on the selected factor model Gm.
Themodel G2a comprises the edge factoring of Fig-ure 3.
With this model, we obtain the followingscoring function.scoreG2a(x, y) =?
(h,c)?y w ?
ffirst(x,h,c)+?
(h,c,ci)?y w ?
fsib(x,h,c,ci)+?
(h,c,cmo)?y w ?
fgra(x,h,c,cmo)+?
(h,c,cmi)?y w ?
fgra(x,h,c,cmi)The function f maps the input sentence x, anda subtree y defined by the indexes to a feature-vector.
Again, w is the corresponding weight vec-tor.
In order to add the factor of Figure 4 to ourmodel, we have to add the scoring function (2a)the sum:(2b) scoreG2b(x, y) = scoreG2a(x, y)+?
(h,c,cmi)?y w ?
fgra(x,h,c,cmi)In order to build a scoring function for combi-nation of the factors shown in Figure 5 to 7, wehave to add to the equation 2b one or more of thefollowing sums:(3a)?
(h,c,ch1,ch2)?y w ?
fgra(x,h,c,ch1,ch2)(3b)?
(h,c,cm1,cm2)?y w ?
fgra(x,h,c,cm1,cm2)(3c)?
(h,c,cmo,tmo)?y w ?
fgra(x,h,c,cmo,tmo)Feature Set.
The feature set of the transitionmodel is similar to that of Zhang and Nivre(2011).
In addition, we use the cross product ofmorphologic features between the head and thedependent since we apply also the parser on mor-phologic rich languages.The feature sets of the completion model de-scribed above are mostly based on previous work(McDonald et al 2005; McDonald and Pereira,2006; Carreras, 2007; Koo and Collins, 2010).The models denoted with + use all combinationsof words before and after the head, dependent,sibling, grandchilrden, etc.
These are respectivelythree-, and four-grams for the first order and sec-ond order.
The algorithm includes these featuresonly the words left and right do not overlap withthe factor (e.g.
the head, dependent, etc.).
We usefeature extraction procedure for second order, andthird order factors.
Each feature extracted in thisprocedure includes information about the positionof the nodes relative to the other nodes of the partand a factor identifier.Training.
For the training of our parser, we usea variant of the perceptron algorithm that uses thePassive-Aggressive update function, cf.
(Freundand Schapire, 1998; Collins, 2002; Crammer etal., 2006).
The Passive-Aggressive perceptronuses an aggressive update strategy by modifyingthe weight vector by as much as needed to clas-sify correctly the current example, cf.
(Crammeret al 2006).
We apply a random function (hashfunction) to retrieve the weights from the weightvector instead of a table.
Bohnet (2010) showedthat the Hash Kernel improves parsing speed andaccuracy since the parser uses additionaly nega-tive features.
Ganchev and Dredze (2008) used82this technique for structured prediction in NLP toreduce the needed space, cf.
(Shi et al 2009).We use as weight vector size 800 million.
Afterthe training, we counted 65 millions non zeroweights for English (penn2malt), 83 for Czechand 87 millions for German.
The feature vectorsare the union of features originating from thetransition sequence of a sentence and the featuresof the factors over all edges of a dependency tree(e.g.
G2a, etc.).
To prevent over-fitting, we useaveraging to cope with this problem, cf.
(Freundand Schapire, 1998; Collins, 2002).
We calculatethe error e as the sum of all attachment errors andlabel errors both weighted by 0.5.
We use thefollowing equations to compute the update.loss: lt = e-(scoreT (xgt , ygt )-scoreT (xt, yt))PA-update: ?t =lt||fg?fp||2We train the model to select the transitions andthe completion model together and therefore, weuse one parameter space.
In order to compute theweight vector, we employ standard online learn-ing with 25 training iterations, and carry out earlyupdates, cf.
Collins and Roark (2004; Zhang andClark (2008).Efficient Implementation.
Keeping the scoringwith the completion model tractable with millionsof feature weights and for second- and third-orderfactors requires careful bookkeeping and a num-ber of specialized techniques from recent work ondependency parsing.We use two variables to store the scores (a)for complete factors and (b) for incomplete fac-tors.
The complete factors (first-order factors andhigher-order factors for which further augmenta-tion is structurally excluded) need to be calculatedonly once and can then be stored with the tree fac-tors.
The incomplete factors (higher-order factorswhose node elements may still receive additionaldescendants) need to be dynamically recomputedwhile the tree is built.The parsing algorithm only has to compute thescores of the factored model when the transition-based parser selects a left-arc or right-arc transi-tion and the beam has to be sorted.
The parsersorts the beam when it exceeds the maximal beamsize, in order to discard superfluous parses orwhen the parsing algorithm terminates in order toselect the best parse tree.
The complexity of thetransition-based parser is quadratic due to swapoperation in the worse case, which is rare, andO(n) in the best case, cf.
(Nivre, 2009).
Thebeam size B is constant.
Hence, the complexityis in the worst case O(n2).The parsing time is to a large degree deter-mined by the feature extraction, the score calcu-lation and the implementation, cf.
also (Goldbergand Elhadad, 2010).
The transition-based parseris able to parse 30 sentences per second.
Theparser with completion model processes about 5sentences per second with a beam size of 80.Note, we use a rich feature set, a completionmodel with third order factors, negative features,and a large beam.
3We implemented the following optimizations:(1) We use a parallel feature extraction for thebeam elements.
Each process extracts the fea-tures, scores the possible transitions and computesthe score of the completion model.
After the ex-tension step, the beam is sorted and the best ele-ments are selected according to the beam size.
(2) The calculation of each score is optimized (be-yond the distinction of a static and a dynamiccomponent): We calculate for each location de-termined by the last element sl ?
?i and the firstelement of b0 ?
?i a numeric feature representa-tion.
This is kept fix and we add only the numericvalue for each of the edge labels plus a value forthe transition left-arc or right-arc.
In this way, wecreate the features incrementally.
This has somesimilarity to Goldberg and Elhadad (2010).
(3) We apply edge filtering as it is used in graph-based dependency parsing, cf.
(Johansson andNugues, 2008), i.e., we calculate the edge weightsonly for the labels that were found for the part-of-speech combination of the head and dependent inthe training data.5 Parsing Experiments and DiscussionThe results of different parsing systems are of-ten hard to compare due to differences in phrasestructure to dependency conversions, corpus ver-sion, and experimental settings.
For better com-parison, we provide results on English for twocommonly used data sets, based on two differ-ent conversions of the Penn Treebank.
The firstuses the Penn2Malt conversion based on the head-36 core, 3.33 Ghz Intel Nehalem83Section Sentences PoS Acc.Training 2-21 39.832 97.08Dev 24 1.394 97.18Test 23 2.416 97.30Table 1: Overview of the training, development andtest data split converted to dependency graphs withhead-finding rules of (Yamada and Matsumoto, 2003).The last column shows the accuracy of Part-of-Speechtags.finding rules of Yamada and Matsumoto (2003).Table 1 gives an overview of the properties of thecorpus.
The annotation of the corpus does notcontain non-projective links.
The training datawas 10-fold jackknifed with our own tagger.4.
Ta-ble 1 shows the tagging accuracy.Table 2 lists the accuracy of our transition-based parser with completion model together withresults from related work.
All results use pre-dicted PoS tags.
As a baseline, we present in ad-dition results without the completion model anda graph-based parser with second order features(G2a).
For the Graph-based parser, we used 10training iterations.
The following rows denotedwith Ta, T2a, T2ab, T2ab3a, T2ab3b, T2ab3bc, andT2a3abc present the result for the parser with com-pletion model.
The subscript letters denote theused factors of the completion model as shownin Figure 3 to 7.
The parsers with subscribed plus(e.g.
G2a+) in addition use feature templates thatcontain one word left or right of the head, depen-dent, siblings, and grandchildren.
We left thosefeature in our previous models out as they may in-terfere with the second and third order factors.
Asin previous work, we exclude punctuation marksfor the English data converted with Penn2Malt inthe evaluation, cf.
(McDonald et al 2005; Kooand Collins, 2010; Zhang and Nivre, 2011).5 Weoptimized the feature model of our parser on sec-tion 24 and used section 23 for evaluation.
We usea beam size of 80 for our transition-based parserand 25 training iterations.The second English data set was obtained byusing the LTH conversion schema as used in theCoNLL Shared Task 2009, cf.
(Hajic?
et al 2009).This corpus preserves the non-projectivity of thephrase structure annotation, it has a rich edgelabel set, and provides automatic assigned PoS4http://code.google.com/p/mate-tools/5We follow Koo and Collins (2010) and ignore any tokenwhose POS tag is one of the following tokens ??
??
:,.Parser UAS LAS(McDonald et al 2005) 90.9(McDonald and Pereira, 2006) 91.5(Huang and Sagae, 2010) 92.1(Zhang and Nivre, 2011) 92.9(Koo and Collins, 2010) 93.04(Martins et al 2010) 93.26T (baseline) 92.7G2a (baseline) 92.89T2a 92.94 91.87T2ab 93.16 92.08T2ab3a 93.20 92.10T2ab3b 93.23 92.15T2ab3c 93.17 92.10T2ab3abc+ 93.39 92.38G2a+ 93.1(Koo et al 2008) ?
93.16(Carreras et al 2008) ?
93.5(Suzuki et al 2009) ?
93.79Table 2: English Attachment Scores for thePenn2Malt conversion of the Penn Treebank for thetest set.
Punctuation is excluded from the evaluation.The results marked with ?
are not directly comparableto our work as they depend on additional sources ofinformation (Brown Clusters).tags.
From the same data set, we selected thecorpora for Czech and German.
In all cases, weused the provided training, development, and testdata split, cf.
(Hajic?
et al 2009).
In contrastto the evaluation of the Penn2Malt conversion,we include punctuation marks for these corporaand follow in that the evaluation schema of theCoNLL Shared Task 2009.
Table 3 presents theresults as obtained for these data set.The transition-based parser obtains higher ac-curacy scores for Czech but still lower scores forEnglish and German.
For Czech, the result of Tis 1.59 percentage points higher than the top la-beled score in the CoNLL shared task 2009.
Thereason is that T includes already third order fea-tures that are needed to determine some edge la-bels.
The transition-based parser with completionmodel T2a has even 2.62 percentage points higheraccuracy and it could improve the results of theparser T by additional 1.03 percentage points.The results of the parser T are lower for Englishand German compared to the results of the graph-based parser G2a.
The completion model T2a canreach a similar accuracy level for these two lan-guages.
The third order features let the transition-based parser reach higher scores than the graph-based parser.
The third order features contributefor each language a relatively small improvement84Parser Eng.
Czech German(Gesmundoet al 2009)?
88.79/- 80.38 87.29(Bohnet, 2009) 89.88/- 80.11 87.48T (Baseline) 89.52/92.10 81.97/87.26 87.53/89.86G2a (Baseline) 90.14/92.36 81.13/87.65 87.79/90.12T2a 90.20/92.55 83.01/88.12 88.22/90.36T2ab 90.26/92.56 83.22/88.34 88.31/90.24T2ab3a 90.20/90.51 83.21.88.30 88.14/90.23T2ab3b 90.26/92.57 83.22/88.35 88.50/90.59T2ab3abc 90.31/92.58 83.31/88.30 88.33/90.45G2a+ 90.39/92.8 81.43/88.0 88.26/90.50T2ab3ab+ 90.36/92.66 83.48/88.47 88.51/90.62Table 3: Labeled Attachment Scores of parsers thatuse the data sets of the CoNLL shared task 2009.
Inline with previous work, punctuation is included.
Theparsers marked with ?
used a joint model for syntacticparsing and semantic role labelling.
We provide moreparsing results for the languages of CoNLL-X SharedTask at http://code.google.com/p/mate-tools/.Parser UAS LAS(Zhang and Clark, 2008) 84.3(Huang and Sagae, 2010) 85.2(Zhang and Nivre, 2011) 86.0 84.4T2ab3abc+ 87.5 85.9Table 4: Chinese Attachment Scores for the conver-sion of CTB 5 with head rules of Zhang and Clark(2008).
We take the standard split of CTB 5 and usein line with previous work gold segmentation, POS-tags and exclude punctuation marks for the evaluation.of the score.
Small and statistically significant im-provements provides the additional second orderfactor (2b).6 We tried to determine the best thirdorder factors or set of factors but we cannot denotesuch a factor which is the best for all languages.For German, we obtained a significant improve-ment with the factor (3b).
We believe that this isdue to the flat annotation of PPs in the Germancorpus.
If we combine all third order factors weobtain for the Penn2Malt conversion a small im-provement of 0.2 percentage points over the re-sults of (2ab).
We think that a more deep featureselection for third order factors may help to im-prove the actuary further.In Table 4, we present results on the ChineseTreebank.
To our knowledge, we obtain the bestpublished results so far.6The results of the baseline T compared to T2ab3abc arestatistically significant (p < 0.01).6 Conclusion and Future WorkThe parser introduced in this paper combinesadvantageous properties from the two majorparadigms in data-driven dependency parsing,in particular worst case quadratic complexity oftransition-based parsing with a swap operationand the consideration of complete second andthird order factors in the scoring of alternatives.While previous work using third order factors, cf.Koo and Collins (2010), was restricted to unla-beled and projective trees, our parser can producelabeled and non-projective dependency trees.In contrast to parser stacking, which involvesrunning two parsers in training and application,we use only the feature model of a graph-basedparser but not the graph-based parsing algorithm.This is not only conceptually superior, but makestraining much simpler, since no jackknifing hasto be carried out.
Zhang and Clark (2008) pro-posed a similar combination, without the rescor-ing procedure.
Our implementation allows for theuse of rich feature sets in the combined scoringfunctions, and our experimental results show thatthe ?graph-based?
completion model leads to anincrease of between 0.4 (for English) and about1 percentage points (for Czech).
The scores gobeyond the current state of the art results for ty-pologically different languages such as Chinese,Czech, English, and German.
For Czech, English(Penn2Malt) and German, these are to our knowl-ege the highest reported scores of a dependencyparser that does not use additional sources of in-formation (such as extra unlabeled training datafor clustering).
Note that the efficient techniquesand implementation such as the Hash Kernel, theincremental calculation of the scores of the com-pletion model, and the parallel feature extractionas well as the parallelized transition-based pars-ing strategy play an important role in carrying outthis idea in practice.ReferencesS.
Abney.
1991.
Parsing by chunks.
In Principle-Based Parsing, pages 257?278.
Kluwer AcademicPublishers.G.
Attardi.
2006.
Experiments with a Multilan-guage Non-Projective Dependency Parser.
In TenthConference on Computational Natural LanguageLearning (CoNLL-X).B.
Bohnet.
2009.
Efficient Parsing of Syntactic and85Semantic Dependency Structures.
In Proceedingsof the 13th Conference on Computational NaturalLanguage Learning (CoNLL-2009).B.
Bohnet.
2010.
Top accuracy and fast dependencyparsing is not a contradiction.
In Proceedings of the23rd International Conference on ComputationalLinguistics (Coling 2010), pages 89?97, Beijing,China, August.
Coling 2010 Organizing Commit-tee.X.
Carreras, M. Collins, and T. Koo.
2008.
Tag,dynamic programming, and the perceptron for ef-ficient, feature-rich parsing.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning, CoNLL ?08, pages 9?16, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.X.
Carreras.
2007.
Experiments with a Higher-orderProjective Dependency Parser.
In EMNLP/CoNLL.M.
Collins and B. Roark.
2004.
Incremental parsingwith the perceptron algorithm.
In ACL, pages 111?118.M.
Collins.
2002.
Discriminative Training Methodsfor Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms.
In EMNLP.K.
Crammer, O. Dekel, S. Shalev-Shwartz, andY.
Singer.
2006.
Online Passive-Aggressive Al-gorithms.
Journal of Machine Learning Research,7:551?585.J.
Eisner.
1996.
Three New Probabilistic Models forDependency Parsing: An Exploration.
In Proceed-ings of the 16th International Conference on Com-putational Linguistics (COLING-96), pages 340?345, Copenhaen.Y.
Freund and R. E. Schapire.
1998.
Large marginclassification using the perceptron algorithm.
In11th Annual Conference on Computational Learn-ing Theory, pages 209?217, New York, NY.
ACMPress.K.
Ganchev and M. Dredze.
2008.
Small statisti-cal models by random feature mixing.
In Proceed-ings of the ACL-2008 Workshop on Mobile Lan-guage Processing.
Association for ComputationalLinguistics.A.
Gesmundo, J. Henderson, P. Merlo, and I. Titov.2009.
A Latent Variable Model of Syn-chronous Syntactic-Semantic Parsing for MultipleLanguages.
In Proceedings of the 13th Confer-ence on Computational Natural Language Learning(CoNLL-2009), Boulder, Colorado, USA., June 4-5.Y.
Goldberg and M. Elhadad.
2010.
An efficient al-gorithm for easy-first non-directional dependencyparsing.
In HLT-NAACL, pages 742?750.C.
Go?mez-Rodr?
?guez and J. Nivre.
2010.
ATransition-Based Parser for 2-Planar DependencyStructures.
In ACL, pages 1492?1501.J.
Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,M.
Anto`nia Mart?
?, L. Ma`rquez, A. Meyers, J. Nivre,S.
Pado?, J.
S?te?pa?nek, P. Stran?a?k, M. Surdeanu,N.
Xue, and Y. Zhang.
2009.
The CoNLL-2009shared task: Syntactic and semantic dependenciesin multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning (CoNLL 2009): Shared Task, pages1?18, Boulder, United States, June.L.
Huang and K. Sagae.
2010.
Dynamic programmingfor linear-time incremental parsing.
In Proceedingsof the 48th Annual Meeting of the Association forComputational Linguistics, pages 1077?1086, Up-psala, Sweden, July.
Association for ComputationalLinguistics.R.
Johansson and P. Nugues.
2006.
Investigatingmultilingual dependency parsing.
In Proceedingsof the Shared Task Session of the Tenth Confer-ence on Computational Natural Language Learning(CoNLL-X), pages 206?210, New York City, UnitedStates, June 8-9.R.
Johansson and P. Nugues.
2008.
Dependency-based Syntactic?Semantic Analysis with PropBankand NomBank.
In Proceedings of the Shared TaskSession of CoNLL-2008, Manchester, UK.S.
Kahane, A. Nasr, and O. Rambow.
1998.Pseudo-projectivity: A polynomially parsable non-projective dependency grammar.
In COLING-ACL,pages 646?652.T.
Koo and M. Collins.
2010.
Efficient third-orderdependency parsers.
In Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics, pages 1?11, Uppsala, Sweden,July.
Association for Computational Linguistics.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.pages 595?603.T.
Kudo and Y. Matsumoto.
2002.
Japanese de-pendency analysis using cascaded chunking.
Inproceedings of the 6th conference on Natural lan-guage learning - Volume 20, COLING-02, pages 1?7, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.M.
Kuhlmann, C.
Go?mez-Rodr?
?guez, and G. Satta.2011.
Dynamic programming algorithms fortransition-based dependency parsers.
In ACL, pages673?682.Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,and Mario Figueiredo.
2010.
Turbo parsers: De-pendency parsing by approximate variational infer-ence.
pages 34?44.R.
McDonald and F. Pereira.
2006.
Online Learningof Approximate Dependency Parsing Algorithms.In In Proc.
of EACL, pages 81?88.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line Large-margin Training of Dependency Parsers.In Proc.
ACL, pages 91?98.J.
Nivre and R. McDonald.
2008.
Integrating Graph-Based and Transition-Based Dependency Parsers.In ACL-08, pages 950?958, Columbus, Ohio.86J.
Nivre and J. Nilsson.
2005.
Pseudo-projective de-pendency parsing.
In ACL.J.
Nivre, M. Kuhlmann, and J.
Hall.
2009.
An im-proved oracle for dependency parsing with onlinereordering.
In Proceedings of the 11th Interna-tional Conference on Parsing Technologies, IWPT?09, pages 73?76, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.J.
Nivre.
2003.
An Efficient Algorithm for Pro-jective Dependency Parsing.
In 8th InternationalWorkshop on Parsing Technologies, pages 149?160,Nancy, France.J.
Nivre.
2009.
Non-Projective Dependency Parsingin Expected Linear Time.
In Proceedings of the47th Annual Meeting of the ACL and the 4th IJC-NLP of the AFNLP, pages 351?359, Suntec, Singa-pore.K.
Sagae and A. Lavie.
2006.
Parser combina-tion by reparsing.
In NAACL ?06: Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers on XX,pages 129?132, Morristown, NJ, USA.
Associationfor Computational Linguistics.Q.
Shi, J. Petterson, G. Dror, J. Langford, A. Smola,and S.V.N.
Vishwanathan.
2009.
Hash Kernels forStructured Data.
In Journal of Machine Learning.J.
Suzuki, H. Isozaki, X. Carreras, and M Collins.2009.
An empirical study of semi-supervised struc-tured conditional models for dependency parsing.In EMNLP, pages 551?560.I.
Titov and J. Henderson.
2007.
A Latent VariableModel for Generative Dependency Parsing.
In Pro-ceedings of IWPT, pages 144?155.H.
Yamada and Y. Matsumoto.
2003.
Statistical De-pendency Analysis with Support Vector Machines.In Proceedings of IWPT, pages 195?206.Y.
Zhang and S. Clark.
2008.
A tale of twoparsers: investigating and combining graph-basedand transition-based dependency parsing usingbeam-search.
In Proceedings of EMNLP, Hawaii,USA.Y.
Zhang and J. Nivre.
2011.
Transition-based de-pendency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: HumanLanguage Technologies, pages 188?193, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.87
