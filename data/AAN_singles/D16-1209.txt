Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1992?1997,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsGated Word-Character Recurrent Language ModelYasumasa MiyamotoCenter for Data ScienceNew York Universityyasumasa.miyamoto@nyu.eduKyunghyun ChoCourant Institute ofMathematical Sciences& Centre for Data ScienceNew York Universitykyunghyun.cho@nyu.eduAbstractWe introduce a recurrent neural network lan-guage model (RNN-LM) with long short-term memory (LSTM) units that utilizes bothcharacter-level and word-level inputs.
Ourmodel has a gate that adaptively finds the op-timal mixture of the character-level and word-level inputs.
The gate creates the final vec-tor representation of a word by combiningtwo distinct representations of the word.
Thecharacter-level inputs are converted into vec-tor representations of words using a bidirec-tional LSTM.
The word-level inputs are pro-jected into another high-dimensional space bya word lookup table.
The final vector rep-resentations of words are used in the LSTMlanguage model which predicts the next wordgiven all the preceding words.
Our modelwith the gating mechanism effectively utilizesthe character-level inputs for rare and out-of-vocabulary words and outperforms word-levellanguage models on several English corpora.1 IntroductionRecurrent neural networks (RNNs) achieve state-of-the-art performance on fundamental tasks of naturallanguage processing (NLP) such as language model-ing (RNN-LM) (J?zefowicz et al, 2016; Zoph et al,2016).
RNN-LMs are usually based on the word-level information or subword-level information suchas characters (Mikolov et al, 2012), and predictionsare made at either word level or subword level re-spectively.In word-level LMs, the probability distributionover the vocabulary conditioned on preceding wordsis computed at the output layer using a softmax func-tion.
1 Word-level LMs require a predefined vocab-ulary size since the computational complexity of asoftmax function grows with respect to the vocab-ulary size.
This closed vocabulary approach tendsto ignore rare words and typos, as the words do notappear in the vocabulary are replaced with an out-of-vocabulary (OOV) token.
The words appearingin vocabulary are indexed and associated with high-dimensional vectors.
This process is done through aword lookup table.Although this approach brings a high degree offreedom in learning expressions of words, infor-mation about morphemes such as prefix, root, andsuffix is lost when the word is converted into anindex.
Also, word-level language models requiresome heuristics to differentiate between the OOVwords, otherwise it assigns the exactly same vectorto all the OOV words.
These are the major limita-tions of word-level LMs.In order to alleviate these issues, we introducean RNN-LM that utilizes both character-level andword-level inputs.
In particular, our model has a gatethat adaptively choose between two distinct ways torepresent each word: a word vector derived from thecharacter-level information and a word vector storedin the word lookup table.
This gate is trained tomake this decision based on the input word.According to the experiments, our model with thegate outperforms other models on the Penn Treebank(PTB), BBC, and IMDB Movie Review datasets.Also, the trained gating values show that the gatingmechanism effectively utilizes the character-level1softmax function is defined as f(xi) = exp xi?k exp xk.1992information when it encounters rare words.RelatedWork Character-level language models thatmake word-level prediction have recently been pro-posed.
Ling et al (2015a) introduce the compo-sitional character-to-word (C2W) model that takesas input character-level representation of a wordand generates vector representation of the word us-ing a bidirectional LSTM (Graves and Schmidhu-ber, 2005).
Kim et al (2015) propose a convolu-tional neural network (CNN) based character-levellanguage model and achieve the state-of-the-art per-plexity on the PTB dataset with a significantly fewerparameters.Moreover, word?character hybrid models havebeen studied on different NLP tasks.
Kang etal.
(2011) apply a word?character hybrid languagemodel on Chinese using a neural network languagemodel (Bengio et al, 2003).
Santos and Zadrozny(2014) produce high performance part-of-speechtaggers using a deep neural network that learnscharacter-level representation of words and asso-ciates them with usual word representations.
Bo-janowski et al (2015) investigate RNN models thatpredict characters based on the character and wordlevel inputs.
Luong and Manning (2016) presentword?character hybrid neural machine translationsystems that consult the character-level informationfor rare words.2 Model DescriptionThe model architecture of the proposed word?character hybrid language model is shown in Fig.
1.Word Embedding At each time step t, both theword lookup table and a bidirectional LSTM takethe same word wt as an input.
The word-level inputis projected into a high-dimensional space by a wordlookup table E ?
R|V |?d, where |V | is the vocabu-lary size and d is the dimension of a word vector:xwordwt = E>wwt , (1)where wwt ?
R|V | is a one-hot vector whose i-th el-ement is 1, and other elements are 0.
The character?level input is converted into a word vector by us-ing a bidirectional LSTM.
The last hidden states offorward and reverse recurrent networks are linearlyFigure 1: The model architecture of the gated word-characterrecurrent language model.
wt is an input word at t. xwordwt isa word vector stored in the word lookup table.
xcharwt is a wordvector derived from the character-level input.
gwt is a gatingvalue of a word wt.
w?t+1 is a prediction made at t.combined:xcharwt = Wfhfwt +Wrhrwt + b, (2)where hfwt ,hrwt ?
Rd are the last states ofthe forward and the reverse LSTM respectively.Wf ,Wr ?
Rd?d and b ?
Rd are trainable param-eters, and xcharwt ?
Rd is the vector representation ofthe word wt using a character input.
The generatedvectors xwordwt and xcharwt are mixed by a gate gwt asgwt = ?
(v>g xwordwt + bg)xwt = (1?
gwt)xwordwt + gwtxcharwt ,(3)where vg ?
Rd is a weight vector, bg ?
R isa bias scalar, ?(?)
is a sigmoid function.
This gatevalue is independent of a time step.
Even if a wordappears in different contexts, the same gate valueis applied.
Hashimoto and Tsuruoka (2016) applya very similar approach to compositional and non-compositional phrase embeddings and achieve state-of-the-art results on compositionality detection andverb disambiguation tasks.Language Modeling The output vector xwt is usedas an input to a LSTM language model.
Since theword embedding part is independent from the lan-guage modeling part, our model retains the flexibil-ity to change the architecture of the language model-ing part.
We use the architecture similar to the non-regularized LSTM model by Zaremba et al (2014).1993PTB BBC IMDBModel Validation Test Validation Test Validation TestGated Word & Char, adaptive 117.49 113.87 78.56 87.16 71.99 72.29Gated Word & Char, adaptive (Pre-train) 117.03 112.90 80.37 87.51 71.16 71.49Gated Word & Char, g = 0.25 119.45 115.55 79.67 88.04 71.81 72.14Gated Word & Char, g = 0.25 (Pre-train) 117.01 113.52 80.07 87.99 70.60 70.87Gated Word & Char, g = 0.5 126.01 121.99 89.27 94.91 106.78 107.33Gated Word & Char, g = 0.5 (Pre-train) 117.54 113.03 82.09 88.61 109.69 110.28Gated Word & Char, g = 0.75 135.58 135.00 105.54 111.47 115.58 116.02Gated Word & Char, g = 0.75 (Pre-train) 179.69 172.85 132.96 136.01 106.31 106.86Word Only 118.03 115.65 84.47 90.90 72.42 72.75Character Only 132.45 126.80 88.03 97.71 98.10 98.59Word & Character 125.05 121.09 88.77 95.44 77.94 78.29Word & Character (Pre-train) 122.31 118.85 84.27 91.24 80.60 81.01Non-regularized LSTM (Zaremba, 2014) 120.7 114.5 - - - -Table 1: Validation and test perplexities on Penn Treebank (PTB), BBC, IMDB Movie Reviews datasets.One step of LSTM computation corresponds toft = ?
(Wfxwt +Ufht?1 + bf )it = ?
(Wixwt +Uiht?1 + bi)c?t = tanh (Wc?xwt +Uc?ht?1 + bc?
)ot = ?
(Woxwt +Uoht?1 + bo)ct = ft  ct?1 + it  c?tht = ot  tanh (ct) ,(4)where Ws,Us ?
Rd?d and bs ?
Rd for s ?
{f, i, c?, o} are parameters of LSTM cells.
?(?)
isan element-wise sigmoid function, tanh(?)
is anelement-wise hyperbolic tangent function, and  isan element-wise multiplication.The hidden state ht is affine-transformed fol-lowed by a softmax function:Pr (wt+1 = k|w<t+1) = exp(v>k ht + bk)?k?
exp(v>k?ht + bk?)
,(5)where vk is the k-th column of a parameter matrixV ?
Rd?|V | and bk is the k-th element of a biasvector b ?
Rd.
In the training phase, we minimizesthe negative log-likelihood with stochastic gradientdescent.3 Experimental SettingsWe test five different model architectures on thethree English corpora.
Each model has a uniqueword embedding method, but all models share thesame LSTM language modeling architecture, thathas 2 LSTM layers with 200 hidden units, d = 200.Except for the character only model, weights in thelanguage modeling part are initialized with uniformrandom variables between -0.1 and 0.1.
Weights ofa bidirectional LSTM in the word embedding partare initialized with Xavier initialization (Glorot andBengio, 2010).
All biases are initialized to zero.Stochastic gradient decent (SGD) with mini-batchsize of 32 is used to train the models.
In the first kepochs, the learning rate is 1.
After the k-th epoch,the learning rate is divided by l each epoch.
k man-ages learning rate decay schedule, and l controlsspeed of decay.
k and l are tuned for each modelbased on the validation dataset.As the standard metric for language modeling,perplexity (PPL) is used to evaluate the model per-formance.
Perplexity over the test set is computedas PPL = exp(?
1N?Ni=1 log p(wi|w<i)), where Nis the number of words in the test set, and p(wi|w<i)is the conditional probability of a word wi given allthe preceding words in a sentence.
We use Theano(2016) to implement all the models.
The code forthe models is available from https://github.com/nyu-dl/gated_word_char_rlm.3.1 Model VariationsWord Only (baseline) This is a traditional word-level language model and is a baseline model for ourexperiments.Character Only This is a language model whereeach input word is represented as a character se-1994Train Validation testPTB # Sentences 42k 3k 4k# Word 888k 70k 79kBBC # Sentences 37k 2k 2k# Word 890k 49k 53kIMDB # Sentences 930k 153k 152k# Word 21M 3M 3MTable 2: The size of each dataset.quence similar to the C2W model in (Ling et al,2015a).
The bidirectional LSTMs have 200 hiddenunits, and their weights are initialized with Xavierinitialization.
In addition, the weights of the forget,input, and output gates are scaled by a factor of 4.The weights in the LSTM language model are alsoinitialized with Xavier initialization.
All biases areinitialized to zero.
A learning rate is fixed at 0.2.Word & Character This model simply concate-nates the vector representations of a word con-structed from the character input xcharwt and the wordinput xwordwt to get the final representation of a wordxwt , i.e.,xwt =[xcharwt ;xwordwt].
(6)Before being concatenated, the dimensions of xcharwtand xwordwt are reduced by half to keep the size of xwtcomparably to other models.Gated Word & Character, Fixed Value Thismodel uses a globally constant gating value to com-bine vector representations of a word constructedfrom the character input xcharwt and the word inputxwordwt asxwt = (1?
g)xwordwt + gxcharwt , (7)where g is some number between 0 and 1.
Wechoose g = {0.25, 0.5, 0.75}.Gated Word & Character, Adaptive This modeluses adaptive gating values to combine vector repre-sentations of a word constructed from the characterinput xcharwt and the word input xwordwt as the Eq (3).3.2 DatasetsPenn Treebank We use the Penn Treebank Corpus(Marcus et al, 1993) preprocessed by Mikolov etal.
(2010).
We use 10k most frequent words and51 characters.
In the training phase, we use onlysentences with less than 50 words.BBC We use the BBC corpus prepared by Greene& Cunningham (2006).
We use 10k most frequentwords and 62 characters.
In the training phase, weuse sentences with less than 50 words.IMDB Movie Reviews We use the IMDB MoveReview Corpus prepared by Maas et al (2011).
Weuse 30k most frequent words and 74 characters.
Inthe training phase, we use sentences with less than50 words.
In the validation and test phases, we usesentences with less than 500 characters.3.3 Pre-trainingFor the word?character hybrid models, we applieda pre-training procedure to encourage the modelto use both representations.
The entire model istrained only using the word-level input for the firstm epochs and only using the character-level input inthe next m epochs.
In the first m epochs, a learn-ing rate is fixed at 1, and a smaller learning rate0.1 is used in the next m epochs.
After the 2m-thepoch, both the character-level and the word-levelinputs are used.
We use m = 2 for PTB and BBC,m = 1 for IMDB.Lample et al (2016) report that a pre-trainedword lookup table improves performance of theirword & character hybrid model on named entityrecognition (NER).
In their method, word embed-dings are first trained using skip-n-gram (Ling etal., 2015b), and then the word embeddings are fine-tuned in the main training phase.4 Results and Discussion4.1 PerplexityTable 1 compares the models on each dataset.
Onthe PTB and IMDB Movie Review dataset, the gatedword & character model with a fixed gating value,gconst = 0.25, and pre-training achieves the lowestperplexity .
On the BBC datasets, the gated word& character model without pre-training achieves thelowest perplexity.Even though the model with fixed gating valueperforms well, choosing the gating value is not clearand might depend on characteristics of datasets suchas size.
The model with adaptive gating values doesnot require tuning it and achieves similar perplexity.1995(a) Gated word & character.
(b) Gated word & character with pre-taining.Figure 2: A log-log plot of frequency ranks and gating values trained in the gated word & character models with/without pre-training.4.2 Values of Word?Character GateThe BBC and IMDB datasets retain out-of-vocabulary (OOV) words while the OOV wordshave been replaced by <unk> in the Penn Treebankdataset.
On the BBC and IMDB datasets, our modelassigns a significantly high gating value on the un-known word token UNK compared to the other words.We observe that pre-training results the differentdistributions of gating values.
As can be seen inFig.
2 (a), the gating value trained in the gated word& character model without pre-training is in generalhigher for less frequent words, implying that the re-current language model has learned to exploit thespelling of a word when its word vector could nothave been estimated properly.
Fig.
2 (b) shows thatthe gating value trained in the gated word & charac-ter model with pre-training is less correlated with thefrequency ranks than the one without pre-training.The pre-training step initializes a word lookup tableusing the training corpus and includes its informa-tion into the initial values.
We hypothesize that therecurrent language model tends to be word?input?oriented if the informativeness of word inputs andcharacter inputs are not balanced especially in theearly stage of training.Although the recurrent language model with orwithout pre-training derives different gating values,the results are still similar.
We conjecture that theflexibility of modulating between word-level andcharacter-level representations resulted in a betterlanguage model in multiple ways.Overall, the gating values are small.
However,this does not mean the model does not utilize thecharacter-level inputs.
We observed that the wordvectors constructed from the character-level inputsusually have a larger L2 norm than the word vec-tors constructed from the word-level inputs do.
Forinstance, the mean values of L2 norm of the 1000most frequent words in the IMDB training set are52.77 and 6.27 respectively.
The small gate valuescompensate for this difference.5 ConclusionWe introduced a recurrent neural network languagemodel with LSTM units and a word?character gate.Our model was empirically found to utilize thecharacter-level input especially when the model en-counters rare words.
The experimental results sug-gest the gate can be efficiently trained so that themodel can find a good balance between the word-level and character-level inputs.AcknowledgmentsThis work is done as a part of the course DS-GA 1010-001 Independent Study in Data Scienceat the Center for Data Science, New York Univer-sity.
KC thanks the support by Facebook, Google(Google Faculty Award 2016) and NVidia (GPUCenter of Excellence 2015-2016).
YM thanks Ken-taro Hanaki, Israel Malkin, and Tian Wang for theirhelpful feedback.
KC and YM thanks the anony-mous reviewers for their insightful comments andsuggestions.1996References[Bengio et al2003] Yoshua Bengio, R?jean Ducharme,Pascal Vincent, and Christian Janvin.
2003.
A neu-ral probabilistic language model.
Journal of MachineLearning Research, 3:1137?1155.
[Bojanowski et al2015] Piotr Bojanowski, ArmandJoulin, and Tomas Mikolov.
2015.
Alternative struc-tures for character-level rnns.
CoRR, abs/1511.06303.
[dos Santos and Zadrozny2014] C?cero Nogueira dosSantos and Bianca Zadrozny.
2014.
Learningcharacter-level representations for part-of-speechtagging.
In Proceedings of the 31th InternationalConference on Machine Learning, ICML 2014,Beijing, China, 21-26 June 2014, pages 1818?1826.
[Glorot and Bengio2010] Xavier Glorot and Yoshua Ben-gio.
2010.
Understanding the difficulty of trainingdeep feedforward neural networks.
In Proceedings ofthe Thirteenth International Conference on ArtificialIntelligence and Statistics, AISTATS 2010, Chia La-guna Resort, Sardinia, Italy, May 13-15, 2010, pages249?256.
[Graves and Schmidhuber2005] Alex Graves and J?rgenSchmidhuber.
2005.
Framewise phoneme classifica-tion with bidirectional LSTM and other neural networkarchitectures.
Neural Networks, 18(5-6):602?610.
[Greene and Cunningham2006] Derek Greene andPadraig Cunningham.
2006.
Practical solutions to theproblem of diagonal dominance in kernel documentclustering.
In Machine Learning, Proceedings of theTwenty-Third International Conference (ICML 2006),Pittsburgh, Pennsylvania, USA, June 25-29, 2006,pages 377?384.
[Hashimoto and Tsuruoka2016] Kazuma Hashimoto andYoshimasa Tsuruoka.
2016.
Adaptive joint learningof compositional and non-compositional phrase em-beddings.
CoRR, abs/1603.06067.
[J?zefowicz et al2016] Rafal J?zefowicz, Oriol Vinyals,Mike Schuster, Noam Shazeer, and Yonghui Wu.2016.
Exploring the limits of language modeling.CoRR, abs/1602.02410.
[Kang et al2011] Moonyoung Kang, Tim Ng, and LongNguyen.
2011.
Mandarin word-character hybrid-input neural network language model.
In INTER-SPEECH 2011, 12th Annual Conference of the In-ternational Speech Communication Association, Flo-rence, Italy, August 27-31, 2011, pages 625?628.
[Kim et al2015] Yoon Kim, Yacine Jernite, David Son-tag, and Alexander M. Rush.
2015.
Character-awareneural language models.
CoRR, abs/1508.06615.
[Lample et al2016] Guillaume Lample, Miguel Balles-teros, Sandeep Subramanian, Kazuya Kawakami, andChris Dyer.
2016.
Neural architectures for named en-tity recognition.
CoRR, abs/1603.01360.
[Ling et al2015a] Wang Ling, Tiago Lu?s, Lu?s Marujo,R?mon Fernandez Astudillo, Silvio Amir, Chris Dyer,Alan W Black, and Isabel Trancoso.
2015a.
Findingfunction in form: Compositional character models foropen vocabulary word representation.
EMNLP.
[Ling et al2015b] Wang Ling, Yulia Tsvetkov, SilvioAmir, Ramon Fermandez, Chris Dyer, Alan W. Black,Isabel Trancoso, and Chu-Cheng Lin.
2015b.
Notall contexts are created equal: Better word represen-tations with variable attention.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2015, Lisbon, Portu-gal, September 17-21, 2015, pages 1367?1372.
[Luong and Manning2016] Minh-Thang Luong andChristopher D. Manning.
2016.
Achieving openvocabulary neural machine translation with hybridword-character models.
CoRR, abs/1604.00788.
[Maas et al2011] Andrew L. Maas, Raymond E. Daly,Peter T. Pham, Dan Huang, Andrew Y. Ng, andChristopher Potts.
2011.
Learning word vectors forsentiment analysis.
In The 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, Proceedings of the Con-ference, 19-24 June, 2011, Portland, Oregon, USA,pages 142?150.
[Marcus et al1993] Mitchell P. Marcus, Beatrice San-torini, and Mary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of english: The penn tree-bank.
Computational Linguistics, 19(2):313?330.
[Mikolov et al2010] Tomas Mikolov, Martin Karafi?t,Luk?s Burget, Jan Cernock?, and Sanjeev Khudan-pur.
2010.
Recurrent neural network based languagemodel.
In INTERSPEECH 2010, 11th Annual Confer-ence of the International Speech Communication As-sociation, Makuhari, Chiba, Japan, September 26-30,2010, pages 1045?1048.
[Mikolov et al2012] Tomas Mikolov, Ilya Sutskever,Anoop Deoras, Hai-Son Le, and Stefan Kombrink.2012.
Subword language modeling with neural net-works.
[Theano Development Team2016] Theano DevelopmentTeam.
2016.
Theano: A Python framework for fastcomputation of mathematical expressions.
arXiv e-prints, abs/1605.02688, May.
[Zaremba et al2014] Wojciech Zaremba, Ilya Sutskever,and Oriol Vinyals.
2014.
Recurrent neural networkregularization.
CoRR, abs/1409.2329.
[Zoph et al2016] Barret Zoph, Ashish Vaswani, JonathanMay, and Kevin Knight.
2016.
Simple, fastnoise-contrastive estimation for large rnn vocabularies.NAACL.1997
