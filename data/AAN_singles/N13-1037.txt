Proceedings of NAACL-HLT 2013, pages 359?369,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsWhat to do about bad language on the internetJacob Eisensteinjacobe@gatech.eduSchool of Interactive ComputingGeorgia Institute of TechnologyAbstractThe rise of social media has brought compu-tational linguistics in ever-closer contact withbad language: text that defies our expecta-tions about vocabulary, spelling, and syntax.This paper surveys the landscape of bad lan-guage, and offers a critical review of the NLPcommunity?s response, which has largely fol-lowed two paths: normalization and domainadaptation.
Each approach is evaluated in thecontext of theoretical and empirical work oncomputer-mediated communication.
In addi-tion, the paper presents a quantitative analy-sis of the lexical diversity of social media text,and its relationship to other corpora.1 IntroductionAs social media becomes an increasingly importantapplication domain for natural language processing,we encounter language that is substantially differentfrom many benchmark corpora.
The following ex-amples are all from the social media service Twitter:?
Work on farm Fri.
Burning piles of brushWindyFire got out of control.
Thank God forgood naber He help get undr control Pants-BurnLegWound.
(Senator Charles Grassley)?
Boom!
Ya ur website suxx bro(Sarah Silverman)?
...dats why pluto is pluto it can neva b a star(Shaquille O?Neil)?
michelle obama great.
job.
and.
whit all my.respect she.
look.
great.
congrats.
to.
her.
(Ozzie Guillen)These examples are selected from celebrities (forprivacy reasons), but they contain linguistic chal-lenges that are endemic to the medium, includingnon-standard punctuation, capitalization, spelling,vocabulary, and syntax.
The consequences for lan-guage technology are dire: a series of papers hasdetailed how state-of-the-art natural language pro-cessing (NLP) systems perform significantly worseon social media text.
In part-of-speech tagging, theaccuracy of the Stanford tagger (Toutanova et al2003) falls from 97% on Wall Street Journal text to85% accuracy on Twitter (Gimpel et al 2011).
Innamed entity recognition, the CoNLL-trained Stan-ford recognizer achieves 44% F-measure (Ritter etal., 2011), down from 86% on the CoNLL testset (Finkel et al 2005).
In parsing, Foster et al(2011) report double-digit decreases in accuracy forfour different state-of-the-art parsers when appliedto social media text.The application of language technology to so-cial media is potentially transformative, leveragingthe knowledge and perspectives of millions of peo-ple.
But to deliver on this potential, the problemsat the core of the NLP pipeline must be addressed.A growing thread of research takes up this chal-lenge, including a shared task and workshop on?parsing the web,?
with new corpora which appearto sit somewhere between the Wall Street Journaland Twitter on the spectrum of bad language (Petrovand McDonald, 2012).
But perhaps surprisingly,very little of this research has considered why socialmedia language is so different.
This review paperattempts to shed some light on this question, sur-veying a strong tradition of empirical and theoreti-359cal research on computer-mediated communication(CMC).
I argue that the two main computational ap-proaches to dealing with bad language ?
normaliza-tion and domain adaptation ?
are based on theoriesof social media language that are not descriptivelyaccurate.
I have worked and continue to work inboth of these areas, so I make this argument not asa criticism of others, but in a spirit of self-reflection.It is hoped that a greater engagement with sociolin-guistic and CMC research will lead to new, nuancedapproaches to the challenge of bad language.Why so much Twitter?
Most of the examplesin this paper will focus on Twitter, a microblog-ging service.
Munro and Manning (2012) arguethat Twitter has unfairly dominated recent research,at the expense of email and SMS text messages,which they found to be both linguistically distinctfrom Twitter and significantly more prevalent (in2010).
This matches earlier research arguing thatemail contained relatively little ?neography,?
com-pared with text messages and chat (Anis, 2007).A crucial advantage for Twitter is that it is publicby default, while SMS and email are private.
Thismakes Twitter data less problematic from a privacystandpoint,1 far easier to obtain, and more amenableto target applications such as large-scale mining ofevents (Sakaki et al 2010; Benson et al 2011) andopinions (Sauper et al 2011).
Similar argumentcould be made on behalf of other public social me-dia, such as blog comments (Ali-Hasan and Adamic,2007), forums, and chatrooms (Paolillo, 2001).
Themain advantage of Twitter over these media is con-venience in gathering large datasets through a sin-gle streaming interface.
More comparative evalu-ation is needed to determine linguistic similaritiesand differences between Twitter and these other me-dia; Section 4 presents an evaluation of the lexicalsimilarity between Twitter and political blogs.2 A tour of bad languageWhile many NLP researchers and engineers havewrestled with the difficulties imposed by bad lan-guage, there has been relatively little considera-tion of why language in social media is so differ-ent from our other corpora.
A survey of laypeo-1boyd and Crawford (2012) note that ?public by default?data still raises important ethical considerations.ple found that more than half of the respondentsagreed with the following partial explanations fornon-standard spelling on the internet: ?people areunsure of the correct spellings,?
?it?s faster,?
?it?s be-come the norm,?
and ?people want to represent theirown dialects and/or accents?
(Jones, 2010).
Let usnow consider the evidence for these and other poten-tial explanations.2.1 IlliteracySome commentators have fixated on the proposalthat the authors of non-standard language in socialmedia are simply unaware or incapable of usingmore standard language (Thurlow, 2006).
But em-pirical research suggests that many users of bad lan-guage are capable of using more traditional forms.Drouin and Davis (2009) find no significant differ-ences in the literacy scores of individuals who door do not use non-standard vocabulary in text mes-sages.
Tagliamonte and Denis (2008) review tracesof instant messaging conversations among students,arguing that they ?pick and choose ... from the en-tire stylistic repertoire of the language?
in a way thatwould be impossible without skilled command ofboth formal and informal registers.
While news textis usually more carefully composed and edited thanmuch of the language in social media, there is littleevidence that bad language results from an inabilityto speak anything else.2.2 Length limitsIn the case of Twitter, the limit of 140 characters foreach message is frequently cited as an explanationfor bad language (Finin et al 2010).
Does Twitter?scharacter limit cause users to prefer shorter words,such as u instead of you?
If so, one might expectshortening to be used most frequently in messagesthat are near the 140-character limit.
Using a datasetof one million English-language tweets (Bamman etal., 2012), I have computed the average length ofmessages containing both standard words and theirnon-standard alternatives, focusing on the top fivenon-standard shortenings identified by the automaticmethod of Gouws et al(2011a).
The shortening urcan substitute for both your and you?re.
While witand bout are also spellings for standard words, man-ual examination of one hundred randomly selectedexamples for each surface form revealed only one360standard length alternative lengthyour 85.1?
0.4ur 81.9?
0.6you?re 90.0?
0.1with 87.9?
0.3 wit 78.8?
0.7going 82.7?
0.5 goin 72.2?
1.0know 86.1?
0.4 kno 78.4?
1.0about 88.9?
0.4 bout 74.5?
0.7Table 1: Average length of messages containing standardforms and their shorteningscase in which the standard meaning was intended forwit, and none for bout.The average message lengths are shown in Ta-ble 1.
In all five cases, the non-standard form tendsto be used in shorter messages ?
not in long mes-sages near the 140 character limit.
Moreover, thisdifference is substantially greater than the saving ofone or two characters offered by shortened form.This is not consistent with the explanation that Twit-ter?s character limit is the primary factor driving theuse of shortened forms.
It is still possible that Twit-ter?s length limitations might indirectly cause wordshortenings: for example, by legitimizing shortenedforms or causing authors to develop a habit of pre-ferring them.
But factors other than the length limitmust be recruited to explain why such conventionsor habits apply only to some messages and not oth-ers.2.3 Text input affordancesText input affordances ?
whether standard key-boards or predictive entry on mobile devices ?
playa role in computer-mediated communication that isperhaps under-appreciated.
Gouws et al(2011b) in-vestigate orthographic variation on Twitter, and finddifferences across devices: for example, that mes-sages from iPhones include more contractions thanmessages from Blackberries, and that tweets sentfrom the web browser are more likely to drop vow-els.
While each affordance facilitates some writ-ing styles and inhibits others, the affordances them-selves are unevenly distributed across users.
For ex-ample, older people may prefer standard keyboards,and wealthier people may be more likely to owniPhones.
Affordances are a moving target: new de-vices and software are constantly becoming avail-able, the software itself may adapt to the user?s in-put, and the user may adapt to the software and de-vice.2.4 PragmaticsEmoticons are frequently thought of as introduc-ing an expressive, non-verbal component into writ-ten language, mirroring the role played by facial ex-pressions in speech (Walther and D?Addario, 2001),but they can also be seen as playing a pragmaticfunction: marking an utterance as facetious, ordemonstrating a non-confrontational, less investedstance (Dresner and Herring, 2010).
In many cases,phrasal abbreviations like lol (laugh out loud),lmao (laughing my ass off ), smh (shake my head),and ikr (i know, right?)
play a similar role: yea shednt like me lol; lmao I?m playin son.
A key differ-ence from emoticons is that abbreviations can actas constituents, as in smh at your ignorance.
An-other form of non-standard language is expressivelengthening (e.g., coooolllllll), found by Brody andDiakopoulos (2011) to indicate subjectivity and sen-timent.
In running dialogues ?
such as in onlinemultiplayer games ?
the symbols * and ?
can playan explicit pragmatic function (Collister, 2011; Col-lister, 2012).2.5 Social variablesA series of papers has documented the interac-tions between social media text and social vari-ables such as age (Burger and Henderson, 2006;Argamon et al 2007; Rosenthal and McKeown,2011), gender (Burger et al 2011; Rao et al 2010),race (Eisenstein et al 2011), and location (Eisen-stein et al 2010; Wing and Baldridge, 2011).
Fromthis literature, it is clear that many of the featuresthat characterize bad language have strong associa-tions with specific social variables.
In some cases,these associations mirror linguistic variables knownfrom speech ?
such as geographically-associatedlexical items like hella, or transcriptions of phono-logical variables like ?g-dropping?
(Eisenstein et al2010).
But in other cases, apparently new lexicalitems, such as the abbreviations ctfu, lls, and af,acquire surprisingly strong associations with geo-graphical areas and demographic groups (Eisensteinet al 2011).A robust finding from the sociolinguistics litera-ture is that non-standard forms that mark social vari-361ables, such as regional dialects, are often inhibited informal registers (Labov, 1972).
For example, whilethe Pittsburgh spoken dialect sometimes features theaddress term yinz (Johnstone et al 2006), one wouldnot expect to find many examples in financial re-ports.
Other investigators have found that much ofthe content in Twitter concerns social events and selfpresentation (Ramage et al 2010), which may en-courage the use of less formal registers in whichsocially-marketed language is uninhibited.The use of non-standard language is often seenas a form of identity work, signaling authentic-ity, solidarity, or resistance to norms imposed fromabove (Bucholtz and Hall, 2005).
In spoken lan-guage, many of the linguistic variables that performidentity work are phonological ?
for example, Eck-ert (2000) showed how the northern cities vowelshift was used by a subset of suburban teenagers toindex affiliation with Detroit.
The emergence of newlinguistic variables in social media suggests that thisidentity work is as necessary in social media as itis in spoken language.
Some of these new variablesare transcriptions of existing spoken language vari-ables: like finna, which transcribes fixing to.
Oth-ers ?
abbreviations like ctfu and emoticons ?
seemto be linguistic inventions created to meet the needsof social communication in a new medium.
In anearly study of variation in social media, Paolillo(1999) notes that code-switching between Englishand Hindi also performs this type of identity work.Finally, it is an uncomfortable fact that the textin many of our most frequently-used corpora waswritten and edited predominantly by working-agewhite men.
The Penn Treebank is composed ofprofessionally-written news text from 1989, whenminorities comprised 7.5% of the print journalismworkforce; the proportion of women in the journal-ism workforce was first recorded in 1999, when itwas 37% (American Society of Newspaper Editors,1999).
In contrast, Twitter users in the USA con-tain an equal proportion of men and women, anda higher proportion of young adults and minoritiesthan in the population as a whole (Smith and Brewer,2012).
Such demographic differences are very likelyto lead to differences in language (Green, 2002;Labov, 2001; Eckert and McConnell-Ginet, 2003).Overall, the reasons for language diversity in so-cial media are manifold, though some of the mostfrequently cited explanations (illiteracy and lengthrestrictions) do not hold up to scrutiny.
The in-creasing prevalence of emoticons, phrasal abbrevi-ations (lol, ctfu), and expressive lengthening mayreflect the increasing use of written language forephemeral social interaction, with the concomitantneed for multiple channels through which to expressmultiple types of meaning.
The fact many such neol-ogisms are closely circumscribed in geography anddemographics may reflect diffusion through socialnetworks that are assortative on exactly these dimen-sions (Backstrom et al 2010; Thelwall, 2009).
Butan additional consideration is that non-standard lan-guage is deliberately deployed in the performance ofidentity work and stancetaking.
This seems a partic-ularly salient explanation for the use of lexical vari-ables that originate in spoken language (jawn, hella),and for the orthographic transcription of phonolog-ical variation (Eisenstein, 2013).
Determining therole and relative importance of social network diffu-sion and identity work as factors in the diversifica-tion of social media language is an exciting directionfor future research.3 What can we do about it?Having surveyed the landscape of bad language andits possible causes, let us now turn to the responsesoffered by the language technology research com-munity.3.1 NormalizationOne approach to dealing with bad language is toturn it good: ?normalizing?
social media or SMSmessages to better conform to the sort of languagethat our technology expects.
Approaches to normal-ization include the noisy-channel model (Cook andStevenson, 2009), string and distributional similar-ity (Han and Baldwin, 2011; Han et al 2012), se-quence labeling (Choudhury et al 2007; Liu et al2011a), and machine translation (Aw et al 2006).As this task has been the focus of substantial atten-tion in recent years, labeled datasets have becomeavailable and accuracies have climbed.That said, it is surprisingly difficult to find aprecise definition of the normalization task.
Writ-ing before social media was a significant focus forNLP, Sproat et al(2001) proposed to replace non-362standard words with ?the contextually appropriateword or sequence of words.?
In some cases, thisseems clear enough: we can rewrite dats why plutois pluto with that?s why...
But it is not difficult to findcases that are less clear, putting would-be normaliz-ers in a difficult position.
The labeled dataset of Hanand Baldwin (2011) addresses a more tractable sub-set of the normalization problem, annotating onlytoken-to-token normalizations.
Thus, imma ?
atranscription of I?m gonna, which in turn transcribesI?m going to ?
is not normalized in this dataset.
Ab-breviations like LOL and WTF are also not normal-ized, even when they are used to abbreviate syntac-tic constituents, as in wtf is the matter with you?
Norare words like hella and jawn normalized, since theyhave no obvious one-word transcription in standardEnglish.
These decisions no doubt help to solidifythe reliability of the annotations, but they provide anoverly optimistic impression of the ability of stringedit distance and related similarity-based techniquesto normalize bad language.
The resulting gold stan-dard annotations seem little more amenable to au-tomated parsing and information extraction than theoriginal text.But if we critique normalization for not goingfar enough, we must also ask whether it goes toofar.
The logic of normalization presupposes that the?norm?
can be identified unambiguously, and thatthere is a direct mapping from non-standard wordsto the elements in this normal set.
On closer exami-nation, the norm reveals itself to be slippery.
Whosenorm are we targeting?
Should we normalize flvr toflavor or flavour?
Where does the normal end andthe abnormal begin?
For example, Han and Baldwinnormalize ain to ain?t, but not all the way to isn?t.While ain?t is certainly well-known to speakers ofStandard American English, it does not appear in thePenn Treebank and probably could not be used in theWall Street Journal, except in quotation.Normalization is often impossible without chang-ing the meaning of the text.
Should we normalize thefinal word of ya ur website suxx bro to brother?
Atthe very least, this adds semantic ambiguity wherethere was none before (is she talking to her biolog-ical brother?
or possibly to a monk?).
Languagevariation does not arise from passing standard textthrough a noisy channel; it often serves a pragmaticand/or stancetaking (Du Bois, 2007) function.
Elim-inating variation would strip those additional lay-ers of meaning from whatever propositional contentmight survive the normalization process.
Sarah Sil-verman?s ya ur website suxx bro can only be under-stood as a critique from a caricatured persona ?
thetype of person who ends sentences with bro.
Sim-ilarly, we can assume that Shaquille O?Neil is ca-pable of writing that?s why Pluto is Pluto, but thatto do so would convey an undesirably didactic andauthoritative stance towards the audience and topic.This is not to deny that there is great poten-tial value in research aimed at understanding or-thographic variation through a combination of lo-cal context, string similarity, and related finite-statemachinery.
Given the productivity of orthographicsubstitutions in social media text, it is clear that lan-guage technology must be made more robust.
Nor-malization may point the way towards such robust-ness, even if we do not build an explicit normaliza-tion component directly into the language process-ing pipeline.
Another potential benefit of this re-search is to better understand the underlying ortho-graphic processes that lead to the diversity of lan-guage in social media, how these processes diffuseover social networks, and how they impact compre-hensibility for both the target and non-target audi-ences.3.2 Domain adaptationRather than adapting text to fit our tools, we mayinstead adapt our tools to fit the text.
A series ofpapers has followed the mold of ?NLP for Twit-ter,?
including part-of-speech tagging (Gimpel et al2011; Owoputi et al 2013), named entity recogni-tion (Finin et al 2010; Ritter et al 2011; Liu et al2011b), parsing (Foster et al 2011), dialogue mod-eling (Ritter et al 2010) and summarization (Sharifiet al 2010).
These papers adapt various parts of thenatural language processing pipeline for social me-dia text, and make use of a range of techniques:?
preprocessing to normalize expressive length-ening, and eliminate or group all hashtags,usernames, and URLs (Gimpel et al 2011;Foster et al 2011)?
new labeled data, enabling the application ofsemi-supervised learning (Finin et al 2010;Gimpel et al 2011; Ritter et al 2011)363?
new annotation schemes specifically cus-tomized for social media text (Gimpel et al2011)?
self-training on unlabeled social mediatext (Foster et al 2011)?
distributional features to address the sparsityof bag-of-words features (Gimpel et al 2011;Owoputi et al 2013; Ritter et al 2011)?
joint normalization, incorporated directly intodownstream application (Liu et al 2012)?
distant supervision, using named entity on-tologies and topic models (Ritter et al 2011)Only a few of these techniques (normalization andnew annotation systems) are specific to social me-dia; the rest can found in other domain adaptationsettings.
Is domain adaptation appropriate for socialmedia?
Darling et al(2012) argue that social me-dia is not a coherent domain at all, and that a POStagger for Twitter will not necessarily generalize toother social media.
One can go further: Twitter it-self is not a unified genre, it is composed of manydifferent styles and registers, with widely varyingexpectations for the degree of standardness and di-mensions of variation (Androutsopoulos, 2011).
Iam the co-author on a paper entitled ?Part-of-speechtagging for Twitter,?
but if we take this title literally,it is impossible on a trivial level: Twitter containstext in dozens or hundreds of languages, includingmany for which no POS tagger exists.
Even withina single language ?
setting aside issues of code-switching (Paolillo, 1996) ?
Twitter and other so-cial media can contain registers ranging from hash-tag wordplay (Naaman et al 2011) to the officialpronouncements of the British Monarchy.
And evenif all good language is alike, bad language can bebad in many different ways ?
as Androutsopoulos(2011) notes when contrasting the types of variationencountered when ?visiting a gamer forum?
versus?joining the Twitter profile of a rap star.
?4 The lexical coherence of social mediaThe internal coherence of social media ?
and itsrelationship to other types of text ?
can be quan-tified in terms of the similarity of distributions overbigrams.
While there are many techniques for com-paring word distributions, I apply the relatively sim-ple method of counting out-of-vocabulary (OOV) bi-grams.
The relationship between OOV rate and do-main adaptation has been explored by McClosky etal.
(2010), who use it as a feature to predict how wella parser will perform when applied across domains.2Specifically, the datasets A and B are comparedby counting the number of bigram tokens in A thatare unseen in B.
The following corpora are com-pared:?
Twitter-month: randomly selected tweetsfrom each month between January 2010 to Oc-tober 2012 (Eisenstein et al 2012).?
Twitter-hour: randomly selected tweets fromeach hour of the day, randomly sampled duringthe period from January 2010 to October 2012.?
Twitter-#: tweets in which the first token is ahashtag.
The hashtag itself is not included inthe bigram counts; see below for more detailson which bigrams are included.?
Twitter-@: tweets in which the first token is ausername.
The username itself is not includedin the bigram counts.?
Penn Treebank: sections 2-21?
Infinite Jest: the text of the 1996 novel byDavid Foster Wallace (Wallace, 2012).
Con-sists of only 482,558 tokens.?
Blog articles: A randomly-sampled subsetof the American political blog posts gatheredby Yano et al(2009).?
Blog comments: A randomly-selected subsetof comments associated with the blog posts de-scribed above.In all corpora, only fully alphabetic tokens arecounted; thus, all hashtags and usernames are dis-carded.
The Twitter text is tokenized using Tweet-2A very recent study compares Twitter with other corpora,using a number of alternative metrics, such as the use of highand low frequency words, pronouns, and intensifiers (Hu et al2013).
This is complementary to the present study, which fo-cuses on the degree of difference in the lexical distributions ofcorpora gathered from various media.3645 10 15 20month gap1.0001.0051.0101.0151.0201.0251.0301.035relativeproportionofOOV bigramswith NEswithout NEsFigure 1: Lexical mismatch increases over time, as socialmedia language evolves.motif;3 the Penn Treebank data uses the gold stan-dard tokenization; Infinite Jest and the blog data aretokenized using NLTK (Bird et al 2009).
All to-kens are downcased, and sequences of three or moreconsecutive identical characters are reduced to threecharacters (e.g., coooool?
coool).
All Twitter cor-pora are subject to the following filters: messagesmust be from the United States and should be writtenin English,4 they may not include hyperlinks (elim-inating most marketing messages), they may not beretweets, and the author must not have more than1,000 followers or follow more than 1,000 people.These criteria serve to eliminate text from celebri-ties, businesses, or automated bots.Twitter over time Figure 1 shows how the pro-portion of out-of-vocabulary bigrams increases overtime.
It is possible that the core features of languageare constant but the set of named entities that arementioned changes over time.
To control for this,the CMU Twitter Part-of-Speech tagger (Owoputi etal., 2013) was used to identify named entity men-tions, and they were replaced with a special token.3https://github.com/brendano/tweetmotif4Approximate language detection was performed as follows.We first identify the 1000 most common words, then sort all au-thors by the proportion of these types that they used, and elim-inate the bottom 10%.
This filtering mechanism eliminates in-dividuals who never write in English, but a small amount offoreign language still enters the dataset via code-switching au-thors.
The effect of more advanced language detection meth-ods (Bergsma et al 2012) on these results may be consideredin future work.2 4 6 8 10 12hour gap1.0001.0051.0101.0151.0201.0251.0301.0351.040relativeproportionofOOV bigramswith NEswithout NEsFigure 2: Different times of day have unique lexical sig-natures, reflecting differing topics and authors.The OOV rate is standardized with respect to a one-month time gap, where it is 24.4% when named en-tities are included, and 21.3% when they are not.These rates reach maxima at 25.2% and 22.0% re-spectively, with dips at 12 and 24 months indicat-ing cyclic yearly effects.
While the proportion ofOOV tokens is smaller when named entities are notincluded, the rate of growth is similar in each case.The steadily increasingly rate of OOV bigrams sug-gests that we cannot annotate our way out of the badlanguage problem.
An NLP system trained fromdata gathered in January 2010 will be increasinglyoutdated as time passes and social media languagecontinues to evolve.One need not wait months to see language changeon Twitter: marked changes can be observed overthe course of a single day (Golder and Macy, 2011).A quantitative comparison is shown in Figure 2.Here the OOV rate is standardized with respect toa one-hour gap, where it is 24.2% when named en-tities are included, and 21.1% when they are not.These rates rise monotonically as the time gap in-creases, peaking at 25.1% and 21.9% respectively.Such diurnal changes may reflect the diverse lan-guage of the different types of authors who postthroughout the day.Types of usage The Twitter-# and Twitter-@ cor-pora are designed to capture the diversity of waysin which social media is used to communicate.Twitter-# contains tweets that begin with hashtags,and are thus more likely to be part of running jokes365or trending topics (Naaman et al 2011).
Twitter-@ contains tweets that begin with usernames ?
anaddressing mechanism that is used to maintain dia-logue threads on the site.
These datasets are com-pared with a set of randomly selected tweets fromJune 2011, and with several other corpora: PennTreebank, the novel Infinite Jest, and text and com-ments from political blogs.
There was no attempt toremove named entities from any of these corpora, assuch a comparison would merely reflect the differentaccuracy levels of NER in each corpus.The results are shown in Table 2.
A few observa-tions stand out.
First, the Penn Treebank is the clearoutlier: a PTB dictionary has by far the most OOVtokens for all three Twitter domains and Infinite Jest,although it is a better match for the blog corporathan Infinite Jest is.
Second, the social media arefairly internally coherent: the Twitter datasets bet-ter match each other than any other corpus, with amaximum OOV rate of 33.4 for Twitter-# againstTwitter-@, though this is significantly higher thanthe OOV rate of 27.8 between two separate genericTwitter samples drawn from the same month.
Fi-nally, the OOV rate increase between Twitter andblogs ?
also social media ?
is substantial.
Con-trary to expectations, the Blog-body corpus was nocloser to the PTB standard than Blog-comment.These results suggest that the Penn Treebank cor-pus is so distant from social media that there are in-deed substantial gains to be reaped by adapting fromnews text towards generic Twitter or Blog target do-mains.
The internal differences within these socialmedia ?
at least as measured by the distinctionsdrawn in Table 2 ?
are much smaller than the dif-ferences between these corpora and the PTB stan-dard.
However, in the long run, the effectivenessof this approach will be limited, as it is clear fromFigure 1 that social media is a moving target.
Anystatic system that we build today, whether by man-ual annotation or automated adaptation, will see itsperformance decay over time.5 What to do nextLanguage is shaped by a constant negotiation be-tween processes that encourage change and linguis-tic diversity, and countervailing processes that en-force existing norms.
The decision of the NLP com-munity to focus so much effort on news text is em-inently justified on practical grounds, but has unin-tended consequences not just for technology but forlanguage itself.
By developing software that worksbest for standard linguistic forms, we throw theweight of language technology behind those forms,and against variants that are preferred by disempow-ered groups.
By adopting a model of ?normaliza-tion,?
we declare one version of language to be thenorm, and all others to be outside that norm.
Byadopting a model of ?domain adaptation,?
we con-fuse a medium with a coherent domain.
Adaptinglanguage technology towards the median Tweet canimprove accuracy on average, but it is certain toleave many forms of language out.Much of the current research on the relationshipbetween social media language and metadata has thegoal of using language to predict the metadata ?revealing who is a woman or a man, who is fromOklahoma or New Jersey, and so on.
This perspec-tive on social variables and personal identity ignoresthe local categories that are often more linguisti-cally salient (Eckert, 2008); worse, it strips individ-uals of any agency in using language as a resourceto create and shape their identity (Coupland, 2007),and conceals the role that language plays in creatingand perpetuating categories like gender (Bucholtzand Hall, 2005).
An alternative possibility is to re-verse the relationship between language and meta-data, using metadata to achieve a more flexible andheterogeneous domain adaptation that is sensitive tothe social factors that shape variation.
Such a re-versal would help language technology to move be-yond false dichotomies between normal and abnor-mal text, source and target domains, and good andbad language.AcknowledgmentsThis paper benefitted from discussions with DavidBamman, Natalia Cecire, Micha Elsner, SharonGoldwater, Scott Kiesling, Brendan O?Connor,Tyler Schnoebelen, and Yi Yang.
Many thanks toBrendan O?Connor and David Bamman for provid-ing Twitter datasets, Tae Yano for the blog com-ment dataset, and Byron Wallace for the Infinite Jestdataset.
Thanks also to the anonymous reviewers fortheir helpful feedback.366Tw-June Tw-@ Tw-# Blog-body Blog-comment Infinite-Jest PTBTw-June 28.7 29.3 47.1 48.6 54.0 63.9Tw-@ 25.9 29.7 47.8 49.9 56.3 66.4Tw-# 29.8 33.4 49.6 51.0 54.7 66.2Blog-body 41.9 44.1 43.8 27.2 49.1 48.0Blog-comment 47.4 49.6 49.2 30.2 53.0 48.4Infinite-Jest 49.4 51.1 49.9 48.3 47.4 55.5PTB 72.2 73.1 72.7 64.5 61.9 71.9Table 2: Percent OOV bigram tokens across corpora.
Rows are the dataset providing the tokens, columns are thedataset providing the dictionary.ReferencesNoor Ali-Hasan and Lada Adamic.
2007.
Expressing so-cial relationships on the blog through links and com-ments.
In Proceedings of ICWSM.American Society of Newspaper Editors.
1999.
1999Newsroom Census: Minority Employment Inches up inDaily Newspapers.
American Society of NewspaperEditors, Reston, VA.Jannis Androutsopoulos.
2011.
Language change anddigital media: a review of conceptions and evidence.In Nikolas Coupland and Tore Kristiansen, editors,Standard Languages and Language Standards in aChanging Europe.
Novus, Oslo.Jacques Anis.
2007.
Neography: Unconventionalspelling in French SMS text messages.
In BrendaDanet and Susan C. Herring, editors, The multilingualinternet: Language, culture, and communication on-line, pages 87 ?
115.
Oxford University Press.Shlomo Argamon, Moshe Koppel, James W. Pennebaker,and Jonathan Schler.
2007.
Mining the blogosphere:age, gender, and the varieties of self-expression.
FirstMonday, 12(9).AiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for SMS text normaliza-tion.
In Proceedings of ACL, pages 33?40.Lars Backstrom, Eric Sun, and Cameron Marlow.
2010.Find me if you can: improving geographical predictionwith social and spatial proximity.
In Proceedings ofWWW, pages 61?70.David Bamman, Jacob Eisenstein, and Tyler Schnoebe-len.
2012.
Gender in twitter: Styles, stances, andsocial networks.
Technical Report 1210.4567, arXiv,October.Edward Benson, Aria Haghighi, and Regina Barzilay.2011.
Event discovery in social media feeds.
In Pro-ceedings of ACL.Shane Bergsma, Paul McNamee, Mossaab Bagdouri,Clayton Fink, and Theresa Wilson.
2012.
Languageidentification for creating language-specific twittercollections.
In Proceedings of the Second Workshopon Language in Social Media, pages 65?74, June.Steven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural language processing with Python.
O?Reilly Me-dia, Incorporated.danah boyd and Kate Crawford.
2012.
Critical questionsfor big data.
Information, Communication & Society,15(5):662?679, May.Samuel Brody and Nicholas Diakopoulos.
2011.Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!
: usingword lengthening to detect sentiment in microblogs.In Proceedings of EMNLP.Mary Bucholtz and Kira Hall.
2005.
Identity and inter-action: A sociocultural linguistic approach.
Discoursestudies, 7(4-5):585?614.John D. Burger and John C. Henderson.
2006.
An explo-ration of observable features related to blogger age.
InAAAI Spring Symposium: Computational Approachesto Analyzing Weblogs.John D. Burger, John C. Henderson, George Kim, andGuido Zarrella.
2011.
Discriminating gender on twit-ter.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and modeling of the structure oftexting language.
International Journal on DocumentAnalysis and Recognition, 10(3):157?174.Lauren B. Collister.
2011.
*-repair in online discourse.Journal of Pragmatics, 43(3):918?921, February.Lauren B. Collister.
2012.
The discourse deictics ?
and<-- in a world of warcraft community.
Discourse,Context & Media, 1(1):9?19, March.Paul Cook and Suzanne Stevenson.
2009.
An unsuper-vised model for text message normalization.
In Pro-ceedings of the NAACL-HLT Workshop on Computa-tional Approaches to Linguistic Creativity, pages 71?78.Nikolas Coupland.
2007.
Style (Key Topics in Sociolin-guistics).
Cambridge University Press, July.367William M. Darling, Michael J. Paul, and Fei Song.2012.
Unsupervised part-of-speech tagging innoisy and esoteric domains with a syntactic-semanticbayesian hmm.
In Proceedings of EACL Workshop onSemantic Analysis in Social Media.Eli Dresner and Susan C. Herring.
2010.
Functions ofthe non-verbal in cmc: Emoticons and illocutionaryforce.
Communication Theory, 20(3):249?268.Michelle Drouin and Claire Davis.
2009.
R u txting?
isthe use of text speak hurting your literacy?
Journal ofLiteracy Research, 41(1):46?67.John W. Du Bois.
2007.
The stance triangle.
In RobertEngelbretson, editor, Stancetaking in discourse, pages139?182.
John Benjamins Publishing Company, Ams-terdam/Philadelphia.Penelope Eckert and Sally McConnell-Ginet.
2003.
Lan-guage and Gender.
Cambridge University Press, NewYork.Penelope Eckert.
2000.
Linguistic variation as socialpractice.
Blackwell.Penelope Eckert.
2008.
Variation and the indexical field.Journal of Sociolinguistics, 12(4):453?476.Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2010.
A latent variable model for ge-ographic lexical variation.
In Proceedings of EMNLP.Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.2011.
Discovering sociolinguistic associations withstructured sparsity.
In Proceedings of ACL.Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2012.
Mapping the geographicaldiffusion of new words.
Technical Report 1210.5268,arXiv.Jacob Eisenstein.
2013.
Phonological factors in socialmedia writing.
In Proceedings of the NAACL Work-shop on Language Analysis in Social Media.Tim Finin, Will Murnane, Anand Karandikar, NicholasKeller, Justin Martineau, and Mark Dredze.
2010.Annotating named entities in twitter data with crowd-sourcing.
In Proceedings of the NAACL HLT Work-shop on Creating Speech and Language Data withAmazon?s Mechanical Turk.Jenny R. Finkel, Trond Grenager, and Christopher Man-ning.
2005.
Incorporating non-local information intoinformation extraction systems by gibbs sampling.
InProceedings of ACL, pages 363?370.Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,Joseph Le Roux, Joakim Nivre, Deirdre Hogan, andJosef van Genabith.
2011.
From news to comment:Resources and benchmarks for parsing the languageof web 2.0.
In Proceedings of IJCNLP.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging fortwitter: annotation, features, and experiments.
In Pro-ceedings of ACL.Scott A. Golder and Michael W. Macy.
2011.
Di-urnal and seasonal mood vary with work, sleep,and daylength across diverse cultures.
Science,333(6051):1878?1881, September.Stephan Gouws, Dirk Hovy, and Donald Metzler.
2011a.Unsupervised mining of lexical variants from noisytext.
In Proceedings of the First workshop on Unsu-pervised Learning in NLP, pages 82?90, July.Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-uard Hovy.
2011b.
Contextual bearing on linguisticvariation in social media.
In Proceedings of the ACLWorkshop on Language in Social Media.Lisa Green.
2002.
African American English.
Cam-bridge University Press.Bo Han and Timothy Baldwin.
2011.
Lexical normalisa-tion of short text messages: Makn sens a# twitter.
InProceedings of ACL, volume 1.Bo Han, Paul Cook, and Timothy Baldwin.
2012.
Auto-matically constructing a normalisation dictionary formicroblogs.
In Proceedings of EMNLP.Yuheng Hu, Kartik Talamadupula, and Subbarao Kamb-hampati.
2013.
Dude, srsly?
: The surprisingly for-mal nature of twitter?s language.
In Proceedings ofICWSM.Barbara Johnstone, Jennifer Andrus, and Andrew EDanielson.
2006.
Mobility, indexicality, and the en-registerment of pittsburghese.
Journal of English Lin-guistics, 34(2):77?104.Lucy Jones.
2010.
The changing face of spelling on theinternet.
Technical report, The English Spelling Soci-ety.William Labov.
1972.
Sociolinguistic patterns.Philadelphia: University of Pennsylvania Press.William Labov.
2001.
Principles of linguistic change.Vol.2 : Social factors.
Blackwell Publishers, Oxford.Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.2011a.
Insertion, deletion, or substitution?
normal-izing text messages without pre-categorization nor su-pervision.
In Proceedings of ACL, pages 71?76.Xiaohua Liu, Shaodian Zhang, Furu Wei, and MingZhou.
2011b.
Recognizing named entities in tweets.In Proceedings of ACL.Xiaohua Liu, Ming Zhou, Xiangyang Zhou, ZhongyangFu, and Furu Wei.
2012.
Joint inference of named en-tity recognition and normalization for tweets.
In Pro-ceedings of ACL.David McClosky, Eugene Charniak, and Mark Johnson.2010.
Automatic domain adaptation for parsing.
InProceedings of NAACL, pages 28?36, June.368Robert Munro and Christopher D. Manning.
2012.Short message communications: users, topics, and in-language processing.
In Proceedings of the 2nd ACMSymposium on Computing for Development.Mor Naaman, Hila Becker, and Luis Gravano.
2011.
Hipand trendy: Characterizing emerging trends on twit-ter.
Journal of the American Society for InformationScience and Technology, 62(5):902?918.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A. Smith.
2013.Improved part-of-speech tagging for online conver-sational text with word clusters.
In Proceedings ofNAACL.John C. Paolillo.
1996.
Language choice onsoc.culture.punjab.
Electronic Journal of Communica-tion/La Revue Electronique de Communication, 6(3).John C. Paolillo.
1999.
The virtual speech community:Social network and language variation on irc.
Journalof Computer-Mediated Communication, 4(4):0.John C. Paolillo.
2001.
Language variation on internetrelay chat: A social network approach.
Journal of So-ciolinguistics, 5(2):180?213.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
In Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Daniel Ramage, Sue Dumais, and D. Liebling.
2010.Characterizing microblogs with topic models.
In Pro-ceedings of ICWSM.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in twitter.
In Proceedings of Workshop onSearch and mining user-generated contents.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Un-supervised modeling of twitter conversations.
In Pro-ceedings of NAACL.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: an experi-mental study.
In Proceedings of EMNLP.Sara Rosenthal and Kathleen McKeown.
2011.
Age pre-diction in blogs: A study of style, content, and onlinebehavior in pre- and Post-Social media generations.
InProceedings of ACL.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes twitter users: real-time eventdetection by social sensors.
In Proceedings of WWW,pages 851?860.Christina Sauper, Aria Haghighi, and Regina Barzilay.2011.
Content models with attitude.
In Proceedingsof ACL.Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.2010.
Summarizing microblogs automatically.
InProceedings of NAACL.Aaron Smith and Joanna Brewer.
2012.
Twitter use2012.
Technical report, Pew Research Center, May.Richard Sproat, Alan W Black, Stanley Chen, ShankarKumar, Mari Ostendorf, and Christopher Richards.2001.
Normalization of non-standard words.
Com-puter Speech & Language, 15(3):287?333.Sali A. Tagliamonte and Derek Denis.
2008.
Linguisticruin?
lol!
instant messaging and teen language.
Amer-ican Speech, 83(1):3?34, March.Mike Thelwall.
2009.
Homophily in MySpace.
J. Am.Soc.
Inf.
Sci., 60(2):219?231.Crispin Thurlow.
2006.
From statistical panic to moralpanic: The metadiscursive construction and popularexaggeration of new media language in the print me-dia.
J. Computer-Mediated Communication, pages667?701.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of NAACL.Byron Wallace.
2012.
Multiple narrative disentangle-ment: Unraveling infinite jest.
In Proceedings ofNAACL.Joseph B. Walther and Kyle P. D?Addario.
2001.
Theimpacts of emoticons on message interpretation incomputer-mediated communication.
Social ScienceComputer Review, 19(3):324?347.Benjamin Wing and Jason Baldridge.
2011.
Simple su-pervised document geolocation with geodesic grids.In Proceedings of ACL.Tae Yano, William W. Cohen, and Noah A. Smith.
2009.Predicting response to political blog posts with topicmodels.
In Proceedings of NAACL.369
