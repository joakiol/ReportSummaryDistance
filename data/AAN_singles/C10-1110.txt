Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 976?983,Beijing, August 2010Weakly Supervised Morphology Learning for Agglutinating LanguagesUsing Small Training SetsKsenia ShalonovaComputer Science,University of Bristolksenia@cs.bris.ac.ukBruno Gole?niaComputer Science,University of Bristolcsbsgg@bristol.ac.ukAbstractThe paper describes a weakly supervisedapproach for decomposing words into allmorphemes: stems, prefixes and suffixes,using wordforms with marked stems astraining data.
As we concentrate onunder-resourced languages, the amountof training data is limited and we needsome amount of supervision in the formof a small number of wordforms withmarked stems.
In the first stage we in-troduce a new Supervised Stem Extrac-tion algorithm (SSE).
Once stems havebeen extracted, an improved unsupervisedsegmentation algorithm GBUMS (Graph-Based Unsupervised Morpheme Segmen-tation) is used to segment suffix or prefixsequences into individual suffixes and pre-fixes.
The approach, experimentally val-idated on Turkish and isiZulu languages,gives high performance on test data and iscomparable to a fully supervised method.1 IntroductionThe major function of morphological analysis isdecomposition of words into their constituents -stems and prefixes/suffixes.
In recent years Ma-chine Learning approaches were used for word de-composition.
There is a number of both unsuper-vised morphology learning systems that use ?raw?wordforms as training data (Creutz and Lagus,2002; Goldsmith, 2001; Kazakov and Manand-har, 2001) and supervised morphology learningsystems using segmented wordforms into stemsand affixes as training data (Oflazer et al, 2001).The supervised morphology learning systems areusually based on two-level morphology (Kosken-niemmi, 1983).
There is also a weakly supervisedapproach that uses, for example, wordpairs as in-put, and this was applied mainly to fusional lan-guages for stem extraction (Erjavec and Dzeroski,2004).
Our project concerns developing speechtechnology for under-resourced languages.
Forthis type of applications we need a relatively fast,cheap (i.e.
does not require large training sets),almost knowledge-free approach that gives highperformance.
We have chosen to use wordformswith marked stems as training data in order to ful-fill the criteria mentioned above.Morphological analysis is used in many prac-tical Natural Language Processing applicationssuch as Machine Translation, Text Mining, spell-checkers etc.
Our near-term goal is the integrationof the morphology learning algorithms into thelanguage-independent Text-to-Speech (TTS) sys-tem for improvement of grapheme-to-phonemerules, stress prediction and tone assignment.
Inparticular, the morphology learning algorithmsdescribed in this paper will be incorporated intothe available isiZulu TTS system for automaticprediction of lexical tones.
In the isiZulu languagelexical tone assignment depends on the morphemeboundary.
The current isiZulu TTS system istone-deaf due to the lack of morphological de-composition.
A number of perception tests willbe carried out in order to evaluate which perfor-mance of morphology decomposition is accept-able for TTS and will improve the quality of thesynthesised speech.
It seems that the unsuper-vised morphology learning systems can be rela-tively easy to implement from scratch, but theirperformance probably cannot be regarded as highenough to improve the performance of the synthe-sized speech.
In order to overcome this problemwe present a novel synthesis of supervised and un-supervised induction techniques for morphologylearning.Our approach consists of two parts: the new su-pervised stem extraction algorithm for agglutinat-976ing languages and the improved version of theunsupervised algorithm for segmenting affix se-quences.
In (Shalonova et al, 2009) the authorspresented the function learning approach calledTASR (Tree of Aligned Suffix Rules) for extract-ing stems in fusional languages given wordpairs(word in grammatical form - word in basic form).While this algorithm gives good performance forRussian and English, it gives quite poor perfor-mance for agglutinating languages as shown inSection 4.
A new approach for stem extraction inagglutinating languages is required for two mainreasons.
Firstly, suffix (or prefix) sequences inagglutinating languages can be much longer thanin fusional languages and TASR does not seemto be efficient on long affix sequences as it doesnot generalise data in the efficient way and gen-erates too many specific rules.
This leads to poorperformance on unseen data.
Secondly, in someagglutinating languages it could be easier for na-tive speakers to provide a stem (i.e.
to provide alist of wordforms with annotated stems), whereasin highly inflective fusional languages the stem isoften strongly bound with suffix sequences, andproviding a proper stem requires high linguisticexpertise.
TASR approach is more appropriate forword-and-paradigm or realizational morphologythat focuses on the whole word form rather thanon word segmentation.
For example, in Russianthe infinitive verb govorit?
(?to speak?)
generatesa set of grammatical forms or a paradigm - gov-orivshij, govor?aschij, govorim etc.The second part of our approach is the im-proved version of GBUAS algorithm (Shalonovaet al, 2009) that provides affix segmentation givenunannotated affix sequences.
Given stem bound-aries in the training set, our method splits theinput word into all morphemes: stems and pre-fixes/suffixes.
Our two-stage approach is testedon the under-resourced language isiZulu contain-ing both prefixes and suffixes, as well as on Turk-ish containing only suffixes.
Turkish is the mostcommonly spoken of the Turkic languages (over77 million people).
isiZulu is the Bantu lan-guage with about 10 million speakers and it isthe most widely spoken home language in SouthAfrica.
Both Turkish and isiZulu use agglutina-tion to form new words from noun and verb stems.In comparison to Turkish, isiZulu is a tonal lan-guage.
In contrast to East Asian languages, inisiZulu there are three steps for tone assignment:lexical, morphemic and terraced.
For TTS the lex-ical and morphemic tones will need to be recov-ered from the lexicon and the grammar as the or-thography has no tone marking.
The terraced tonerelation can in general be recovered and markedautomatically from the tone sequence with a finitestate model.2 Stem Extraction AlgorithmThe Stem Extraction Algorithm (SSE) is the su-pervised algorithm for stem extraction.
The train-ing data for the SSE represent wordforms withthe marked stem boundaries.
During the train-ing stage we collect a set of all possible stem ex-traction rules from training data and assign pre-cision measures to each rule.
Each rule is of theform L R where ?
?
is the stem boundary, L andR are the left and right graphemic contexts of astem boundary of different lengths.
We differen-tiate prefix Lpre f Rstem and suffix Lstem Rsu f fstem extraction rules that correspond to the rulescontaining the left-hand stem boundary and theright-hand stem boundary respectively.
For ex-ample, the Turkish word yer (?earth?)
with themarked word boundary #ye r# generates the fol-lowing Lstem Rsu f f rules: #ye r#, #ye r, ye r#,#ye , ye r, e r#, r#, ye , e r, r, and e , wherethe symbol ?#?
signifies the word initial and fi-nal positions.
We are implementing similar fea-ture vectors used for automatic pronunciation pre-diction based on the focal grapheme (in our caseit is a stem boundary) and left/right graphemiccontexts of different length (Davel and Barnard,2008).
The idea of implementing expanding con-text in NLP tasks is usually applied for two-leveldata like grapheme-to-phoneme mapping rules(Torkkola, 1993), whereas in our case we use itfor one-level data.The precision measure for each rule is calcu-lated by the formula p/(p+n+?)
where p and n arethe number of positive and negative examples, and?
is used to cover the cases where there are no neg-ative examples.
A high precision is desirable andthis occurs when there are high values of p andlow values of n (i.e.
many positive examples and977few negative examples).
Using negative examplesin contrast to using only rule frequencies (or pos-itive examples) improves the performance of thealgorithm.Definition 1.
The number of positive examples forthe rule Lstem Rsuff (or rule Lpref Rstem) is thenumber of training instances of Stem Su f f ixes(or Pre f ixes Stem) containing the substring L R.Definition 2.
The number of negative exam-ples for rule Lstem Rsuff (or Lpref Rstem) is thenumber of training instances Stem Su f f ixes (orPre f ixes Stem) such that Stem + Su f f ixes (orPre f ixes + Stem) contains substring L+R andStem Su f f ixes (or Pre f ixes Stem) does not con-tain substring L R where ?+?
denotes string con-catenation.In the above definitions ?
?
is a stem boundary.Example 1.
Suppose we have only three isiZuluverbs: zi bek e, zi nak eke and a hlul eke.
Forthe Lstem Rsu f f rule ?ek e?, the word zi bek egenerates one positive example and the two otherwords zi nak eke and a hlul eke generate onenegative example each.The approach given in Algorithm 1 aims to findthe unique longest rule-pair ?Lpre f Rstem andLstem Rsu f f ?
with the highest precision that isapplied to the input wordform for stem extraction.In case the language does not have prefixes likeTurkish, the longest rule Lstem Rsu f f with thehighest precision is applied.
The decision of us-ing either a rule-pair or just a single suffix ruleis influenced by prior knowledge that a particu-lar language has got either both prefixes and suf-fixes like isiZulu or only suffixes like Turkish.From now on we will use the term ?rulepair?
inapplication both to the rulepair ?Lpre f Rstem andLstem Rsu f f ?
in case of isiZulu and to the rule-pair ?
and Lstem Rsu f f ?
with an empty first ele-ment in case of Turkish.Algorithm 1 Choosing rule pair for stem extrac-tion.input W = raw wordform; P and S are sets ofunique Lpre f Rstem and Lstem Rsu f f rulesoutput result rule pairresult rule pair ?
/0iMaxlength ?
?repeat(p1,s1) ?
getrulepair (P ?
S, W,iMaxlength)(p2,s2) ?
getrulepair (P ?
S \ (p1,s1), W,iMaxlength)iMaxlength ?
length(p1,s1)until (p1,s1) = /0 or precision (p1,s1)6= precision(p2,s2) or length (p1,s1) 6=length(p2,s2)result rule pair ?
(p1,s1)function getrulepair(PS, W, iMaxlength)ilength ?
0r ?
/0for all (p,s) ?
PS doif (p,s) matches W thenif length(p,s) < iMaxlength andlength(p,s) > ilength thenilength ?
length(p,s)r ?
(p,s)elseif length(p,s) = ilength andprecision(p,s) > precision(r) thenr ?
(p,s)end ifend ifend ifend forreturn rend functionThe search is carried out on the set of rulepairs matching an input raw wordform.
The setis sorted by length first, and then by precisionmeasure within each length category.For example, if rulepairs have the followinglength-precision values:?4-0.5,?4-0.5?,?4-0.2?978?3-0.4?,?3-0.3?
?2-0.3?rulepair with the value 3-0.4 is selected.The rulepair matches the input word ifLpre f Rstem and Lstem Rsu f f rules can be ap-plied without contradicting each other.
For exam-ple, the rule pair ?#a hl?
and ?l eke?
matches theword a hlul eke, whereas the rule pair ?#a hlulek?and ?le ke?
does not match this word.
For eachinput wordform the set of its own rulepair candi-dates is generated.
The search in the algorithmamong these rulepairs starts from the longest rule-pairs, and this allows more specific rules and ex-ceptions to be applied first, whereas the more gen-eral rules are applied if no specific rules cover theinput wordform.3 Graph-Based UnsupervisedMorpheme SegmentationIn this section we extend GBUMS (Graph-Based Unsupervised Morpheme Segmentation)that segments sequences of prefixes and suffixes(Gole?nia et al, 2009).
We propose an exten-sion of GBUMS which uses the graph structure ofGBUMS through a brute-force method.
Our ex-periments showed the improved results on train-ing set and allowed GBUMS to be run on the testsets for two languages: Turkish and isiZulu.The algorithm GBUMS was originally devel-oped in (Shalonova et al, 2009) under the nameGBUSS (Graph-Based Unsupervised Suffix Seg-mentation) to extract suffix sequences efficientlyand it was applied to Russian and Turkish lan-guages on training sets.
We refer to prefixes andsuffixes generally as morphemes.
GBUMS usesa morpheme graph in a bottom-up way.
Similarto Harris (Harris, 1955), the algorithm is basedon letter frequencies.
However, when Harris usessuccessor and predecessor frequencies, they useposition-independent n-gram statistics to mergesingle letters into morphemes until a stopping cri-terion is fulfilled.In the morpheme graph, each node represents amorpheme and each directed edge the concatena-tion of two morphemes labelled with the frequen-cies in a M-corpus (see Figure 1).
M-corpus is alist of morpheme sequencesDefinition 3.
Let M = {mi|1 ?
i ?
|M|} be a setof morphemes, let fi be the frequency with whichmorpheme mi occurs in a M-corpus of morphemesequences, let vi = (mi, fi) for 1 ?
i ?
n, and letfi, j denote the number of morpheme sequences inthe corpus in which morpheme mi is followed bymorpheme mj.
The morpheme graph G = (V,E)is a directed graph with vertices or nodes V ={vi|1 ?
i ?
|V |} and edges E = {(vi,v j)| fi, j > 0}.We treat fi, j as the label of the edge from vi to v j.In G, each node is initialised with a letter ac-cording to a M-corpus, then one by one, nodesare merged to create the real morphemes.
Tomerge nodes, an evaluation function is required.In (Gole?nia et al, 2009), Golenia et al employedthe Morph Lift evaluation function based on its re-lation to the lift of a rule for association rules indata mining (Brin et al, 1997).Definition 4.
Morph Li f t is defined for a pair ofmorphemes m1 and m2 as follows:Morph Li f t(m1,m2) = f1,2f1 + f2 (1)From now on, we know how to merge nodes.Now, we need to figure out the most importantpart of GBUMS, which is the stopping crite-rion.
The stopping criterion is to prevent over-generalisation.
In other words, the algorithmneeds to be stopped before getting the initial M-corpus (since no merging is possible).
This cri-terion is based on the Bayesian Information Cri-terion (BIC) and Jensen-Shannon divergence (Li,2001).BIC is used for selecting a model (set of mor-phemes) which fits a data set (M-Corpus) withoutbeing too complex.
We want to point out that BICis related to MDL.
BIC is a trade-off between themaximum likelihood, the parameters of the model(probability and length of each morpheme) andthe number of elements in the data set (frequencyof each morpheme).
A smaller value of BIC cor-responds to a better model fit.
The maximum ofthe Jensen-Shannon divergence is used in order toanalyse the increase of log-likelihood among allpossible models.
The Jensen-Shannon divergenceis defined as follows (Dagan et al, 1997):Definition 5.
The Jensen-Shannon divergence isdefined for two morphemes m1 and m2 as the de-979crease in entropy between the concatenated andthe individual morphemes:DJS(m1,m2) = H(m1 ?m2)?
Lm1H(m1)+Lm2H(m2)N (2)where H(m) =?P(m) log2 P(m) N = ?m Freq(m)and Lm is the string length of m.Stopping criterion requires that ?BIC < 0which translates to:maxm1,m2 DJS(m1,m2) ?
2log2 N (3)Algorithm 2 The GBUMS morpheme segmenta-tion algorithminput M-Corpus = List of Stringsoutput M-CorpusSeg = List of StringsM-CorpusSeg ?
SegmentInLetters(M-Corpus);Graph ?
InitialiseGraph(M-CorpusSeg);repeatMax ?
0;for all (p,q) ?
Graph doML Max ?
Morph Lift(p, q);if ML Max > Max thenMax ?
ML Max;pMax ?
p;qMax ?
q;end ifend forGraph ?
MergeNodes(Graph, pMax,qMax);M-CorpusSeg ?
DeleteBoundaries(M-CorpusSeg, Label(pMax), Label(qMax));Graph ?
AdjustGraph(M-corpusSeg,Graph);until StoppingCriterion(pMax, qMax, Max)After several merging iterations, the output ofthe algorithm is the graph shown in Figure 1.
TheGBUMS is presented in Algorithm 2.Note that the M-Corpus is completely segmentedat the beginning of the algorithm.
Then, theboundaries in the segmented M-Corpus are re-moved step by step according to a pair found in thegraph with the maximum value for Morph Li f t.When the stopping criterion is fulfilled, the seg-mented M-Corpus represents the morpheme se-quences.At this point we present our extension ofGBUMS based on a brute-force heuristic whichscores every possible segmentation of an inputmorpheme sequence using graph values.
Weconsider the morpheme graph as a model whereeach morpheme sequence can be extracted by theMGraph function (eq.
4).Definition 6.
We define MGraph of a morphemesequence without boundaries x as follows:MGraph(x) = argmaxt?x1Nt ?Ct ?m?t Lmlog( fm +1)(4)where?
t is a morpheme sequence with boundaries ofx,?
m is a morpheme of t,?
fm is the frequency of the morpheme m,?
Nt is the number of morphemes existing inthe graph,?
Ct is the number of morphemes existing andcontiguous in the graph.Firstly, as a post-processing procedure theMGraph function improves the performance ontraining data.
Secondly, it permits the identifica-tion of unseen morphemes.
That is why the modelgenerated by GBUMS can be run on test data sets.Example 2.
Let our final morpheme graph be asshown in Figure 1 where nodes represent suffixesand their frequencies.Let x=?ekwe?
be our input suffix sequence that wewant to segment into individual suffixes.
We splitthis input sequence into all possible substringsfrom individual characters up to size of the inputstring length: e-k-w-e, e-k-we, e-kw-e, ek-w-e, .
.
.
,ekwe.Using equation 4, we evaluate each substring andselect the one with the highest score as the correctsegmentation.
Here, we have 7 potential segmen-tations with a score higher than 0 (MGraph > 0),e.g: e-k-w-e = (log(3)+ log(3))/2 = 1.0986, ek-w-e =(2log(4)+ log(3))/2 = 1.9356 and ek-we =9802log(4) = 2.7726.Consequently, ek-we is chosen as the correct seg-mentation for the substring ?ekwe?.We would like to highlight that our new methodcan identify unseen cases with M-Graph, for in-stance, in the previous example suffix ?we?
wasnot present in the training graph, but was correctlyextracted.	Figure 1: Example of a suffix subgraph in thetraining phase for isiZulu.4 ResultsOur experiments were based on Turkish data con-taining 1457 verbs and 2267 nouns, and isiZuludata containing 846 nouns and 931 verbs, withone single unambiguous segmentation per word.1Both isiZulu and Turkish data were uniquely sam-pled from the most frequent word lists.Our first experiments compared TASR and thenew SSE algorithm for stem extraction (10-foldcross validation assumes the following trainingand test set sizes: training sets containing 1311wordforms for verbs and 2040 wordforms fornouns; test sets containing 146 wordforms forverbs and 227 wordforms for nouns).
As can beseen from the Table 1, the performance of the SSEalgorithm on Turkish data is much higher than thatof TASR on the same dataset.
As we mentioned inSection 1, TASR is not suitable for agglutinatinglanguages with long suffix sequences.
AlthoughTASR algorithm gives an excellent performanceon Russian, for most Turkish words it fails to ex-tract proper stems.1In agglutinating languages some wordforms even withinone POS category can have several possible segmentations.Test FMeaTASR Nouns 20.7?6.8Verbs 12.6?5.9SSE Nouns 84.3?3.2Verbs 82.1?3.7Table 1: Comparison of TASR and SSE for Turk-ish using 10-fold cross validation.Our next experiments evaluated the perfor-mance of GBUMS on its own given unsegmentedsuffix sequences from Turkish nouns and verbs astraining data.
The performance on these trainingdata increased by approximately 3-4 % in com-parison to the results presented in (Shalonova etal., 2009).
We would like to point out that theresults in (Shalonova et al, 2009) are based ontraining data rather than on test data, whereas inthe current paper we run our algorithms on test(or unseen) data.
Our final experiments examinedperformance on the test sets and were run bothon Turkish and isiZulu data.
We compared ourapproach with Morfessor run both in supervisedand in unsupervised mode.
Although Morfessor isknown as one of the best unsupervised morphol-ogy learning systems, it is possible to run it in thesupervised mode as well (Spiegler et al, 2008).The training data for SSE+ GBUMS containedwordforms with marked stems.
During trainingstage the SSE algorithm was collecting informa-tion about stem boundaries and the GBUMS al-gorithm was run on unlabelled suffix and pre-fix sequences from the same training set.
Thetest stage for the SSE+GBUMS approach was runon ?raw?
wordforms by applying the SSE algo-rithm first for stem extraction and then runningGBUMS algorithm for segmenting prefix or suf-fix sequences after the SSE has extracted stems.Training data for supervised Morfessor used thesame wordforms as for the SSE+GBUMS train-ing set and contained wordforms segmented intostems and affixes (i.e.
words segmentated intoall morphemes were given as training data).
Thetest data for supervised Morfesor were the sameas those used for SSE+GBUMS.
Morfessor in un-supervised mode was run on ?raw?
wordforms astraining data.
To evaluate our current work we ap-981Test FMeaSupervised Morfessor Nouns 74.6?2.3Verbs 84.5?2.2SSE+ GBUMS Nouns 78.8?2.4Verbs 76.9?0.7Unsupervised Morfessor Nouns 26.6?2.6Verbs 28.4?2.8Table 2: Comparison of Morfessor andSSE+GBUMS for Turkish using 10-fold crossvalidation.plied the SSE+GBUMS approach for the under-resourced agglutinating language isiZulu contain-ing both prefixes and suffixes and for Turkish con-taining only suffixes.
The results show (Table 2and Table 3) that our weakly supervised approachis comparable with the supervised Morfessor anddecisively outperforms the unsupervised Morfes-sor.
We think that it is useful to point out that un-supervised morphology learning systems in gen-eral require much larger training sets for betterperformance.
F-measure is the harmonic meanof precision and recall, whereas precision is theproportion of true morpheme boundaries amongthe boundaries found, recall is the proportionof boundaries found among the true boundaries.In our experiments the GBUMS algorithm hadno restrictions on affix length (Shalonova et al,2009), but if there were restrictions, performancecould be better.
For isiZulu nouns our approachsignificantly outperformed supervised Morfessor,whereas for Turkish verbs SSE+GBUMS per-formed much worse.
The best overall resultsobtained by GBUMS were based on the isiZulunouns where about 53% of all affixes were sin-gle letter affixes, whereas the worst results our ap-proach gave for Turkish verbs where only about12% of affixes are composed of one letter.
It isimportant to notice that the GBUMS algorithm,which is completely unsupervised, gives better re-sults for extracting one letter affixes compared toMorfessor.5 ConclusionsIn the paper we described a weakly supervisedapproach for learning morphology in agglutinat-Test FMeaSupervised Morfessor Nouns 76.7?1.6Verbs 88.5?2.4SSE+ GBUMS Nouns 87.9?1.9Verbs 84.5?2.5Unsupervised Morfessor Nouns 27.4?5.1Verbs 26.9?5.0Table 3: Comparison of Morfessor andSSE+GBUMS for isiZulu using 10-fold crossvalidation.ing languages.
We were successful in our ulti-mate goal of synthesis of supervised and unsuper-vised induction techniques by achieving high per-formance on small amount of training data.
Ourweakly supervised approach is comparable withthe supervised morphology learning system.
Aswe are working with the languages for which lin-guistic resources are very limited (in particularwords with morpheme boundaries), the developedmethod fulfills our goals of providing key compo-nents for speech and language products for suchunder-resourced languages.
We speculate that thecurrent performance might be improved by addinga small amount of completely ?raw?
data to thetraining set.The integration of our algorithms into workingTTS systems is of key importance.
As our near-term goal is the integration of morphology learn-ing component into the currently working isiZuluTTS system, we will have to analyse the neces-sity of a Part of Speech Tagger (POS) and mor-phological disambiguation.
In agglutinating lan-guages some wordforms can be segmented in dif-ferent ways (i.e.
have different surface forms)and Machine Learning approaches normally se-lect the most probable segmentation, and there-fore our morphology disambiguation can be im-portant.
Morphological disambiguation for TTScan be considered a less complex problem thanfull morphological disambiguation as it can belinked, for example, to lexical tone disambigua-tion that may not require the full POS tag set.We intend to carry out user perception tests in or-der to evaluate the possible improvement in theisiZulu TTS quality after morphology informationis added.9826 AcknowledgmentWe would like to thank Kemal Oflazer from Sa-banci University in Istanbul for his help and theTurkish data.
We also thank Viktor Zimu and Eti-enne Barnard from CSIR in Pretoria for provid-ing us isiZulu data.
We also thank Roger Tuckerfor his support in the project.
The work wassponsored by the EPSRC Grant EP/E010857/1?Learning the morphology of complex syntheticlanguages?.ReferencesBrin, S., R. Motwani, J. Ullman, and S. Tsur.
1997.Dynamic itemset counting and implication rules formarket basket data.
In ACM SIGMOD internationalconference on Management of data, pages 255?264.ACM.Creutz, M. and K. Lagus.
2002.
Unsupervised discov-ery of morphemes.
Proceedings of the Workshop onMorphological and Phonological Learning of ACL-02, pages 21?30.Dagan, I., L. Lee, and F. Pereira.
1997.
Similarity-Based Methods for Word Sense Disambiguation.Thirty-Fifth Annual Meeting of the ACL and EighthConference of the EACL, pages 56?63.Davel, M. and E. Barnard.
2008.
Pronunciation pre-diction with default refine.
Computer Speech andLanguage, 22:374?393.Erjavec, T. and S. Dzeroski.
2004.
Machine learn-ing of morphosyntactic structure: Lemmatising un-known Slovene words.
Applied Artificial Intelli-gence, 18(1):17?40.Goldsmith, J.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 27:153?198.Gole?nia, B., S. Spiegler, and P. Flach.
2009.
UN-GRADE: UNsupervised GRAph DEcomposition.In Working Notes for the CLEF 2009 Workshop,CLEF 2009, Corfu, Greece.Harris, Z.
1955.
From Phoneme to Morpheme.
Lan-guage, 31(2):190?222.Kazakov, D. and S. Manandhar.
2001.
Unsupervisedlearning of word segmentation rules with geneticalgorithms and inductive logic programming.
Ma-chine Learning, 43:121?162.Koskenniemmi, K. 1983.
Two-level Morphology:A General Computational Model for Word FormRecognition and Production.
Ph.D. thesis, Univer-sity of Helsinki.Li, W. 2001.
New stopping criteria for segment-ing DNA sequences.
Physical Review Letters,86(25):5815?5818.Oflazer, K., M. McShane, and S. Nirenburg.
2001.Bootstrapping morphological analyzers by combin-ing human elicitation and machine learning.
Com-putational Linguistics, 27(1):59?85.Shalonova, K., B. Golenia, and P. Flach.
2009.Towards learning morphology for under-resourcedlanguages.
IEEE Transactions on Audio, Speechand Language Procesing, 17(5):956?965.Spiegler, S., B. Golenia, K. Shalonova, P. Flach, andR.
Tucker.
2008.
Learning the morphology of Zuluwith different degrees of supervision.
IEEE SpokenLanguage Technology Workshop, pages 9?12.Torkkola, K. 1993.
An efficient way to learn Englishgrapheme-to-phoneme rules automatically.
Pro-ceedings of the International Conference on Acous-tics, Speech, and Signal Processing, pages 199?202.983
