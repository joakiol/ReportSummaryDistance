An Effective Hybrid Machine Learning Approach for CoreferenceResolutionFeiliang RenNatural Language Processing LabCollege of Information Science and En-gineeringNortheastern University, P.R.Chinarenfeiliang@ise.neu.edu.cnJingbo ZhuNatural Language Processing LabCollege of Information Science and En-gineeringNortheastern University, P.R.Chinazhujingbo@ise.neu.edu.cnAbstractWe present a hybrid machine learning ap-proach for coreference resolution.
In ourmethod, we use CRFs as basic trainingmodel, use active learning method to gen-erate combined features so as to make ex-isted features used more effectively; at last,we proposed a novel clustering algorithmwhich used both the linguistics knowledgeand the statistical knowledge.
We built acoreference resolution system based on theproposed method and evaluate its perform-ance from three aspects: the contributionsof active learning; the effects of differentclustering algorithms; and the resolutionperformance of different kinds of NPs.
Ex-perimental results show that additional per-formance gain can be obtained by using ac-tive learning method; clustering algorithmhas a great effect on coreference resolu-tion?s performance and our clustering algo-rithm is very effective; and the key ofcoreference resolution is to improve theperformance of the normal noun?s resolu-tion, especially the pronoun?s resolution.1 IntroductionCoreference resolution is the process of determin-ing whether two noun phrases (NPs) refer to thesame entity in a document.
It is an important taskin natural language processing and can be classi-fied into pronoun phrase (denoted as PRO) resolu-tion, normal noun phrase (denoted as NOM) reso-lution, and named noun phrase (denoted as NAM)resolution.
Machine learning approaches recast thisproblem as a classification task based on con-straints that are learned from an annotated corpus.Then a separate clustering mechanism is used toconstruct a partition on the set of NPs.Previous machine learning approaches forcoreference resolution (Soon et al 2001; Ng et al2002; Florian et al 2004, etc) usually selected amachine learning approach to train a classificationmodel, used as many as possible features for thetraining of this classification model, and finallyused a clustering algorithm to construct a partitionon the set of NPs based on the statistical data ob-tained from trained classification model.
Their ex-perimental results showed that different kinds offeatures had different contributions for system?sperformance, and usually the more features used,the better performance obtained.
But they rarelyfocused on how to make existed features usedmore effectively; besides, they proposed their ownclustering algorithm respectively mainly used thestatistical data obtained from trained classificationmodel, they rarely used the linguistics knowledgewhen clustering different kinds of NPs.
Also, therewere fewer experiments conducted to find out theeffect of a clustering algorithm on final system?sperformance.In this paper, we propose a new hybrid machinelearning method for coreference resolution.
We useNP pairs to create training examples; use CRFs asa basic classification model, and use active learn-ing method to generate some combined features soas to make existed features used more effectively;at last, cluster NPs into entities by a novel cascadeclustering algorithm.The rest of the paper is organized as follows.Section 2 presents our coreference resolution sys-24Sixth SIGHAN Workshop on Chinese Language Processingtem in detail.
Section 3 is our experiments and dis-cussions.
And at last, we conclude our work in sec-tion 4.2 Coreference ResolutionThere are three basic components for a coreferenceresolution system that uses machine learning ap-proach: the training set creation, the feature selec-tion, and the coreference clustering algorithm.
Wewill introduce our methods for these componentsrespectively as follows.2.1 Training Set CreationPrevious researchers (Soon et al, 2001, VincentNg et al, 2002, etc) took different creation strate-gies for positive examples and negative examples.Because there were no experimental resultsshowed that these kinds of example creation meth-ods were helpful for system?s performance, wecreate both positive examples and negative exam-ples in a unified NP pair wise manner.Given an input NP chain of an annotated docu-ment, select a NP in this NP chain from left to rightone by one, and take every of its right side?s NP,we generate a positive example if they refer to thesame entity or a negative example if they don?trefer to the same entity.
For example, there is a NPchain n1-n2-n3-n4 found in document, we willgenerate following training examples: (n1-n2, 1?
),(n1-n3,  ), (n1-n4,  ), (n2-n3,1?
1?
1?
), (n2-n4,  ), and (n3-n4,  ).
Where denotes thatthis is a positive example, and denotes that thisis a negative example.1?
1?
1+1?2.2 Feature SetsIn our system, two kinds of features are used.
Oneis atomic feature, the other is combined feature.We define the features that have only one genera-tion condition as atomic features, and define theunion of some atomic features as combined fea-tures.2.2.1 Atomic FeaturesAll of the atomic features used in our system arelisted as follows.String Match Feature (denoted as Sm): Its possi-ble values are exact, left, right, included, part,alias, and other.
If two NPs are exactly stringmatched, return exact; if one NP is the left sub-string of the other, return left; if one NP is the rightsubstring of the other, return right; if all the char-acters in one NP are appeared in the other but notbelong to set {left, right}, return included; if some(not all) characters in one NP are appeared in theother, return part; if one NP is the alias of the other,return alias; if two NPs don?t have any commoncharacters, return other.Lexical Similarity Features (denoted as Ls):compute two NP?s similarity and their head words?similarity using following formula 1.1 21 21 22 (( , )( ) ( )SameChar n nSim n nLen n Len n, )?= +   (1)Here means the commoncharacters?
number in  and ;  is thetotal characters?
number in .
( , )1 2SameChar n n1n 2n ( )Len niniEdit Distance Features (denoted as Ed): computetwo NP?s edit distance and their head words?
editdistance (Wagner and Fischer, 1974), and the pos-sible values are true and false.
If the edit distanceof two NPs (or the head words of these two NPs)are less than or equal to 1, return true, else returnfalse.Distance Features (denoted as Dis): distance be-tween two NPs in words, NPs, sentences, para-graphs, and characters.Length Ratio Features (denoted as Lr): the lengthratio of two NPs, and their head words.
Their pos-sible values belong to the range (0 .
,1]NP?s Semantic Features (denoted as Sem): thePOSs of two NPs?
head words; the types of the twoNPs (NAM, NOM or PRO); besides, if one of theNP is PRO, the semantic features will also includethis NP?s gender information and plurality infor-mation.Other Features (denoted as Oth): whether twoNPs are completely made up of capital Englishcharacters; whether two NPs are completely madeup of lowercase English characters; whether twoNPs are completely made up of digits.2.2.2 Combined Features Generated by Ac-tive LearningDuring the process of model training for corefer-ence resolution, we found that we had very feweravailable resources compared with previous re-searchers.
In their works, they usually had someextra knowledge-based features such as alias table,abbreviation table, wordnet and so on; or they  had25Sixth SIGHAN Workshop on Chinese Language Processingsome extra in-house analysis tools such as propername parser, chunk parser, rule-based shallowcoreference resolution parser, and so on (HalDaume III, etc, 2005; R.Florian, etc, 2004; VincentNg, etc, 2002; etc).
Although we also collectedsome aliases and abbreviations, the amounts arevery small compared with previous researchers?.We hope we can make up for this by making ex-isted features used more effectively by activelearning method.Formally, active learning studies the closed-loopphenomenon of a learner selecting actions or mak-ing queries that influence what data are added to itstraining set.
When actions or queries are selectedproperly, the data requirements for some problemsdecrease drastically (Angluin, 1988; Baum & Lang,1991).
In our system, we used a pool-based activelearning framework that is similar as Manabu Sas-sano (2002) used, this is shown in figure 1.Figure 1: Our Active Learning FrameworkIn this active learning framework, an initial clas-sifier is trained by CRFs [1] that uses only atomicfeatures, and then two human teachers are asked tocorrect some selected wrong classified examplesindependently.
During the process of correction,without any other available information, systemonly shows the examples that are made up of fea-tures to the human teachers; then these two humanteachers have to use the information of someatomic features?
combinations to decide whethertwo NPs refer to the same entity.
We record allthese atomic features?
combinations that used byboth of these human teachers, and take them ascombined features.For example, if both of these human teacherscorrect a wrong classified example based on theknowledge that ?if two NPs are left substring1 http://www.chasen.org/~taku/software/CRF++/matched, lexical similarity feature is greater than0.5, I think they will refer to the same entity?, thecorresponding combined feature would be de-scribed as: ?Sm(NPs)-Ls(NPs)?, which denotes thehuman teachers made their decisions based on thecombination information of ?String Match Fea-tures?
and ?Lexical Similarity Features?.Figure 2: Selection Algorithm1.
Select all the wrong classified examples whoseCRFs?
probability  belongs to range [0.4, 0.6]2.
Sort these examples in decreasing order.3.
Select the top m examplesIn figure 1, ?information?
means the valuabledata that can improve the system?s performanceafter correcting their classification.
The selectionalgorithm for ?informative?
is the most importantcomponent in an active learning framework.
Wedesigned it from the degree of correcting difficulty.We know 0.5 is a critical value for an example?sclassification.
For a wrong classified example, thecloser its probability value to 0.5, the easier for usto correct its classification.
Following this, our se-lection algorithm for ?informative?
is designed asshown in figure 2.1.
Build an initial classifier2.
While teacher can correct examples based onfeature combinationsa) Apply the current classifier to training ex-amplesb) Find m most informative training examplesc) Have two teachers correct these examplesbased on feature combinationsd) Add the feature combinations that are usedby both of these two teachers to featuresets in CRFs and train a new classifier.When add new combined features won?t lead toa performance improvement, we end active learn-ing process.
Totally we obtained 21 combined fea-tures from active learning.
Some of them are listedin table 1.Table 1: Some Combined FeaturesSm(NPs)-Sm(HWs)-Ls(NPs)-Ls(HWs)Sm(NPs)-Sm(HWs)-Ls(NPs)Sm(NPs)-Sm(HWs)-Ls(HWs)Sm(NPs)-Sm(HWs)-Lr(NPs)-Lr(HWs)Sm(NPs)-Sm(HWs)-Lr(NPs)Sm(NPs)-Sm(HWs)-Sem(HW1)-Sem(HW2)Sm(NPs)-Sm(HWs)-Sem(NP1)-Sem(NP2)Sm(NPs)-Sm(HWs)-Lr(HWs)?
?Here ?Sm(NPs)?
means the string match fea-ture?s value of two NPs, ?Sm(HWs)?
means thestring match feature?s value of two NPs?
headwords.
?HWs?
means the head words of two NPs.Combined feature ?Sm(NPs)-Sm(HWs)-Ls(NPs)?means when correcting a wrong classified example,both these human teachers made their decisionsbased on the combination information of Sm(NPs),Sm(HWs), and Ls(NPs) .
Other combined featureshave the similar explanation.26Sixth SIGHAN Workshop on Chinese Language ProcessingAnd at last, we take all the atomic features andthe combined features as final features to train thefinal CRFs classifier.2.3 Clustering AlgorithmFormally, let { : be NPs in a docu-ment.
Let us define the set ofNPs whose types are all NAMs; definethe set of NPs whose types areall NOMs; define the set ofNPs whose types are all PROs.
Let bethe map from NP index i to entity index1 }im i n?
?
n1{ ,..., }a a afS N N=1{ ,..., }o o oS N N= gk1{ ,..., }p p pS N N=:g i jaj .
For aNP index , let us definethe set of indices of thepartially-established entities before clustering ,and , the set of the partially-established entities.
Let  be the(1 )k k n?
??
{ (1),..., ( 1)}kJ g g k=km{ : }k t kE e t J= ?ije j th?
NP inentity.
Let i th?
( , )i jprob m m be the probabilitythat  and refer to the same entity, and im jm( , )i jprob m m can be trained from CRFs.Given that has been formed before cluster-ing , can take two possible actions:if , then the active NP is said to linkwith the entitykEkm km( ) kg k J?
km( )g ke ; otherwise it starts a new en-tity ( )g ke .In this work, P L is used tocompute the link probability, where t , is 1iff links with ; the random variable A is theindex of the partial entity to which m is linking.
( 1| , ,k k )E m A t= =J?
k Lkm tekOur clustering algorithm is shown in figure 3.The basic idea of our clustering algorithm is thatNAMs, NOMs and PROs have different abilitiesstarting an entity.
For NAMs, they are inherentantecedents in entities, so we start entities based onthem first.For NOMs, they have a higher ability of actingas antecedents in entities than PROs, but lowerthan NAMs.
We cluster them secondly, and add aNOM in an existed entity as long as their linkprobability is higher than a threshold.
And duringthe process of the link probabilities computations,we select a NP in an existed entity carefully, andtake these two NPs?
link probability as the linkprobability between this NOM and current entity.The selection strategy is to try to make these linkprobabilities have the greatest distinction.And for PROs, they have the lowest ability ofacting as antecedents in entities, most of the time,they won?t be antecedents in entities; so we clusterthem into an existed entity as long as there is anon-zero link probability.3 Experiments and DiscussionsOur experiments are conducted on Chinese EDR(Entity Detection and Recognize) &EMD (EntityMention Detection) corpora from LDC.
These cor-pora are the training data for ACE (AutomaticContent Extraction) evaluation 2004 and ACEevaluation 2005.
These corpora are annotated andcan be used to train and test the coreference resolu-tion task directly.Figure 3: Our Clustering AlgorithmInput: M = { :1 }im i n?
?Output: a partition E of the set MInitialize: 0 { {{ : }}}i i i aH e m m S?
= ?if x c y dm e m e?
?
?
?
, c d?
, and xm is alias ofym , then  ' \{ } { }d c dH H e e e?
?
?foreach k om S?
that hasn?t been clusteredif 0ke is NAM and d?
makes ( , ) 0tde NOM?
?P= arg maxte| | min{ ( , ) ( , )}tdk tdd kprob m e e NOM??
=?esleif 0ke is NAM and , ( , ) 0tdd e NOM??
==P= arg maxte| | min{ ( , ) ( , )}tdk tdd kprob m e e NAM??
=?esleif 0ke is NOMP= arg maxte0( , )k tprob m eif P ??
, ' \{ } { { }}t t kH H e e m?
?
?else ' { }kH H m?
?foreach k pm S?
that hasn?t been clusteredP= arg max ( , )tkm eprob m m?if 0P > , ' \{ } { { }}t t kH H e e m?
?
?else ' { }kH H m?
?return H27Sixth SIGHAN Workshop on Chinese Language ProcessingIn ACE 2004 corpus, there are two types ofdocuments: paper news (denoted as newswire) andbroadcast news (denoted as broadca); for ACE2005 corpus, a new type added: web log docu-ments (denoted as weblogs).
Totally there are 438documents in ACE 2004 corpus and 636 docu-ments in ACE 2005 corpus.
We randomly dividethese two corpora into two parts respectively, 75%of them for training CRFs model, and 25% of themfor test.
By this way, we get 354 documents fortraining and 84 documents for test in ACE 2004corpus; and 513 documents for training and 123documents for test in ACE 2005 corpus.Some statistics of ACE2005 corpus andACE2004 corpus are shown in table 2.Our experiments were classified into threegroups.
Group 1 (denoted as ExperimentA) is de-signed to evaluate the contributions of active learn-ing for the system?s performance.
We developedtwo systems for ExperimentA, one is a system thatused only the atomic features for CRFs trainingand we took it as a baseline system, the other is asystem that used both the atomic features and thecombined features for CRFs training and we took itas our final system.
The experimental results areshown in table 3 and table 4 for different corpusrespectively.
Bold font is the results of our finalsystem, and normal font is the results of baselinesystem.
Here we used the clustering algorithm asdescribed in figure 3.Group 2 (denoted as ExperimentB) is designedto investigate the effects of different clustering al-gorithm for coreference resolution.
We imple-mented another two clustering algorithms: algo-rithm1 that is proposed by Ng et al (2002) andalgorithm2 that is proposed by Florian et al (2004).We compared the performance of them with ourclustering algorithm and experimental results areshown in table 5.Group 3 (denoted as ExperimentC) is designedto evaluate the resolution performances of differentkinds of NPs.
We think this is very helpful for usto find out the difficulties and bottlenecks ofcoreference resolution; and also is helpful for ourfuture work.
Experimental results are shown intable 6.In ExperimentB and ExperimentC, we usedboth atomic features and combined features forCRFs classification model training.
And in table5,table6 and table7, the data before ?/?
are experi-mental results for ACE2005 corpus and the dataafter ?/?
are experimental results for ACE2004corpus.In all of our experiments, we use recall, preci-sion, and F-measure as evaluation metrics, and de-noted as R, P, and F for short respectively.Table 2: Statistics of ACE2005/2004 CorporaTraining Test# of all documents 513/354 123/84# of broadca 204/204 52/47# of newswire 229/150 54/47#of weblogs 80/0 17/0# of characters 248972/164443 55263/35255# of NPs 28173/18995 6257/3966# of entities 12664/8723 2783/1828# of neg examples 722919/488762 142949/89894# of  pos examples 72000/44682 15808/8935Table3: ExperimentA for ACE2005 CorporaR P Fbroadca 79.0/76.2 75.4/72.9 77.2/74.5newswire 73.2/72.9 68.7/67.8 70.9/70.3weblogs 72.3/68.5 65.5/63.3 68.8/65.8total 75.4/73.7 70.9/69.3 73.1/71.4Table4: ExperimentA for ACE2004 CorporaR P Fbroadca 74.7/71.0 72.4/68.9 73.5/69.9newswire 77.7/73.1 73.0/68.6 75.2/70.7Total 76.2/72.0 72.7/68.7 74.4/70.4Table5: ExperimentB for ACE2005/2004 CorporaR P Falgorithm1 61.0/63.5 59.5/62.8 60.2/63.2algorithm2 61.0/62.4 60.7/62.8 60.9/62.6Ours 75.4/76.2 70.9/72.7 73.1/74.4Table6: ExperimentC for ACE2005/2004 CorporaR P FNAM 80.5/81.4 77.9/79.2 79.2/80.1NOM 62.6/62.5 54.4/56.8 58.2/59.5PRO 28.4/29.8 22.7/24.0 25.2/26.6From table 3 and table 4 we can see that the fi-nal system?s performance made a notable im-provement compared with the baseline system inboth corpora.
We know the only difference ofthese two systems is whether used active learningmethod.
This indicates that by using active learn-ing method, we make the existed features usedmore effectively and obtain additional performancegain accordingly.
One may say that even withoutactive learning method, he still can add some com-bined features during CRFs model training.
Butthis can?t guarantee it would make a performance28Sixth SIGHAN Workshop on Chinese Language Processingimprovement at anytime.
Active learning methodprovides us a way that makes this combined fea-tures?
selection process goes in a proper manner.Generally, a system can obtain an obvious per-formance improvement after several active learn-ing iterations.
We still noticed that the contribu-tions of active learning for different kinds ofdocuments are different.
In ACE04 corpus, bothkinds of documents?
performance obtained almostequal improvements; in ACE05 corpus, there isalmost no performance improvement for newswiredocuments, but broadcast documents?
performanceand web log documents?
performance obtainedgreater improvements.
We think this is because fordifferent kinds of documents, they have differentkinds of correcting rules (these rules refer to thecombination methods of atomic features) for thewrong classified examples, some of these rulesmay be consistent, but some of them may be con-flicting.
Active learning mechanism will balancethese conflicts and select a most appropriate globaloptimization for these rules.
This can also explainwhy ACE04 corpus obtains more performance im-provement than ACE05 corpus, because there aremore kinds of documents in ACE05 corpus, andthus it is more likely to lead to rule conflicts duringactive learning process.Experimental results in table 5 show that if otherexperimental conditions are the same, there areobvious differences among the performances withdifferent clustering algorithms.
This surprised usvery much because both algorithm1 and algo-rithm2 worked very well in their own learningframeworks.
We know R.Florian et al (2004) firstproposed algorithm2 using maximum entropymodel.
Is this the reason for the poor performanceof algorithm2 and algorithm1?
To make sure this,we conducted other experiments that changed theCRFs model to maximum entropy model [2] with-out changing any other conditions and the experi-mental results are shown in table 7.The experimental results are the same: our clus-tering algorithm achieved better performance.
Wethink this is mainly because the following reason,that in our clustering algorithm, we notice the factthat different kinds of NPs have different abilitiesof acting as antecedents in an entity, and take dif-ferent clustering strategy for them respectively,2 http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.htmlthis is obvious better than the methods that onlyuse statistical data.Table7: ExperimentB for ACE2005/2004 Corporawith ME ModelR P Falgorithm1 48.9/48.3 44.2/50.3 46.4/49.3algorithm2 57.4/59.5 52.3/61.4 54.7/60.4Ours 68.1/69.8 65.7/72.6 66.9/71.2We also noticed that the experimental resultswith maximum entropy model are poorer than withCRFs model.
We think this maybe because that thecombined features are obtained under CRFs model,thus they will be more suitable for CRFs modelthan for maximum entropy model, that is to saythese obtained combined features don?t play thesame role in maximum entropy model as they do inCRFs model.Experimental results in table 6 surprised usgreatly.
PRO resolution gets so poor a performancethat it is only about 1/3 of the NAM resolution?sperformance.
And NOM resolution?s performanceis also pessimistic, which reaches about 80% of theNAM resolution?s performance.
After analyses wefound this is because there is too much confusinginformation for NOM?s resolution and PRO?s reso-lution and system can hardly distinguish them cor-rectly with current features description for an ex-ample.
For example, in a Chinese document, aNOM ????
(means president) may refer to aperson A at sometime, but refer to person B at an-other time, and there is no enough information forsystem to distinguish A and B.
It is worse for PROresolution because a PRO can refer to any NAM orNOM from a very long distance, there is little in-formation for the system to distinguish which oneit really refers to.
For example, two PROs that bothof whom are ???
(means he) , one refers to personA, the other refers to person B, even our human canhardly distinguish them, not to say the system.Fortunately, generally there are more NAMs andNOMs in a document, but less PROs.
If they havesimilar amounts in a document, you can imagehow poor the performance of the coreference reso-lution system would be.4 ConclusionsIn this paper, we present a hybrid machine learningapproach for coreference resolution task.
It usesCRFs as a basic classification model and uses ac-tive learning method to generate some combined29Sixth SIGHAN Workshop on Chinese Language Processingfeatures to make existed features used more effec-tively; and we also proposed an effective clusteringalgorithm that used both the linguistics knowledgeand the statistical knowledge.
Experimental resultsshow that additional performance gain can be ob-tained by using active learning method, clusteringalgorithm has a great effect on coreference resolu-tion?s performance and our clustering algorithm isvery effective.
Our experimental results also indi-cate the key of coreference resolution is to improvethe performance of the NOM resolution, especiallythe PRO resolution; both of them remain chal-lenges for a coreference resolution system.AcknowledgmentsWe used the method proposed in this paper forChinese EDR (Entity Detection and Recognition)task of ACE07 (Automatic Content Extraction2007) and achieved very encouraging result.And this work was supported in part by the Na-tional Natural Science Foundation of China underGrant No.60473140; the National 863 High-techProject No.2006AA01Z154; the Program for NewCentury Excellent Talents in University No.NCET-05-0287; and the National 985 Project No.985-2-DB-C03.ReferenceAndrew Kachites McCallum and Kamal Nigam, 1998.Employing EM and pool-based active learning fortext classification.
In Proceedings of the Fifteenth In-ternational Conference on Machine Learning, pp359-367Cohn, D., Grahramani, Z., & Jordan, M.1996.
Activelearning with statistical models.
Journal of ArtificialIntelligence Research, 4. pp 129-145Cynthia A.Thompson, Mary Leaine Califf, and Ray-mond J.Mooney.
1999.
Active learning for naturallanguage parsing and information extraction.
In Pro-ceedings of the Seventeenth International Conferenceon Machine Learning, pp 406-414Hal Daume III and Daniel Marcu, 2005, A large-scaleexploration of effective global features for a joint en-tity detection and tracking model.
Proceedings ofHLT/EMNLP, 2005http://www.nist.gov/speech/tests/ace/ace07/doc, TheACE 2007 (ACE07) Evaluation Plan, Evaluation ofthe Detection and Recognition of ACE Entities, Val-ues, Temporal Expressions, Relations and EventsJohn Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In International Conference on Machine Lear-ingin(ICML01)Lafferty, J., McCallum, A., & Pereira, F. 2001.
Condi-tional random fields: Probabilistic models for seg-menting and labeling sequence data.
Proc.
ICMLManabu Sassano.
2002.
An Empirical Study of ActiveLearning with Support Vector Machines for JapaneseWord Segmentation.
Proceeding of the 40th AnnualMeeting of the Association for Computational Lin-guistics (ACL), 2002, pp 505-512Min Tang, Xiaoqiang Luo, Salim Roukos.2002.
ActiveLearning for Statistical Natural Language Parsing.Proceedings of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), 2002,pp.120-127.Pinto, D., McCallum, A., Lee, X., & Croft, W.B.
2003.combining classifiers in text categorization.
SIGIR?03: Proceedings of the Twenty-sixth Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval.Radu Florian, Hongyan Jing, Nanda Kambhatla andImed Zitouni, ?Factoring Complex Models: A CaseStudy in Mention Detection?, in Procedings of the21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the ACL, pages473-480, Sydney, July 2006.R Florian, H Hassan et al 2004.
A statistical model formultilingual entity detection and tracking.
In Proc.
OfHLT/NAACL-04, pp1-8Sha, F., & Pereira, F. 2003.
Shallow parsing with condi-tional random fields.
Proceedings of Human Lan-guage Technology, NAACL.Simon Tong, Daphne Koller.
2001.
Support Vector Ma-chine Active Learning with Applications to TextClassification.
Journal of Machine Learning Re-search,(2001) pp45-66.V.Ng and C.Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In Proceedingsof the ACL?02, pp.104-111.W.M.Soon, H.T.Ng, et al2001.
A Machine LearningApproach to Coreference Resolution of NounPhrases.
Computational Linguistics, 27(4):521-54430Sixth SIGHAN Workshop on Chinese Language Processing
