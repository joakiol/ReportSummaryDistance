Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
640?648, Prague, June 2007. c?2007 Association for Computational LinguisticsSemi-Markov Models for Sequence SegmentationQinfeng ShiNICTA, Statistical Machine LearningAustralian National UniversityCanberra, 2601 ACTqinfeng.shi@rsise.anu.edu.auYasemin AltunToyota Technological Institute1427 E 60th StChicago, IL 60637altun@tti-c.orgAlex SmolaNICTA, Statistical Machine LearningAustralian National UniversityCanberra, 2601 ACTAlex.Smola@nicta.com.auS.
V. N. VishwanathanNICTA, Statistical Machine LearningAustralian National UniversityCanberra, 2601 ACTSVN.Vishwanathan@nicta.com.auAbstractIn this paper, we study the problem of auto-matically segmenting written text into para-graphs.
This is inherently a sequence label-ing problem, however, previous approachesignore this dependency.
We propose a novelapproach for automatic paragraph segmen-tation, namely training Semi-Markov mod-els discriminatively using a Max-Marginmethod.
This method allows us to modelthe sequential nature of the problem and toincorporate features of a whole paragraph,such as paragraph coherence which cannotbe used in previous models.
Experimentalevaluation on four text corpora shows im-provement over the previous state-of-the artmethod on this task.1 IntroductionIn this paper, we study automatic paragraph segmen-tation (APS).
This task is closely related to somewell known problems such as text segmentation, dis-course parsing, topic shift detection and is relevantfor various important applications in speech-to-textand text-to-text tasks.In speech-to-text applications, the output of aspeech recognition system, such as the output of sys-tems creating memos and documents for the Parlia-ment House, is usually raw text without any punc-tuation or paragraph breaks.
Clearly, such textrequires paragraph segmentations.
In text-to-textprocessing, such as summarization, the output textdoes not necessarily retain the correct paragraphstructure and may require post-processing.
Thereis psycholinguistic evidence as cited by Sporleder& Lapata (2004) showing that insertion of para-graph breaks could improve the readability.
More-over, it has been shown that different languages mayhave cross-linguistic variations in paragraph bound-ary placement (Zhu, 1999), which indicates that ma-chine translation can also benefit from APS.
APScan also recover the paragraph breaks that are oftenlost in the OCR applications.There has been growing interest within the NLPcommunity for APS in recent years.
Previous meth-ods such as Sporleder & Lapata (2004); Genzel(2005); Filippova & Strube (2006) treat the problemas a binary classification task, where each sentenceis labeled as the beginning of a paragraph or not.They focus on the use of features, such as surfacefeatures, language modeling features and syntacticfeatures.
The effectiveness of features is investi-gated across languages and/or domains.
However,these approaches ignore the inherent sequential na-ture of APS.
Clearly, consecutive sentences withinthe same paragraph depend on each other.
More-over, paragraphs should exhibit certain propertiessuch as coherence, which should be explored withinan APS system.
One cannot incorporate such prop-erties/features when APS is treated as a binary clas-sification problem.
To overcome this limitation, wecast APS as a sequence prediction problem, wherethe performance can be significantly improved byoptimizing the choice of labeling over whole se-quences of sentences, rather than individual sen-tences.Sequence prediction is one of the most promi-640Figure 1: Top: sequence (horizontal line) with seg-ment boundaries (vertical lines).
This correspondsto a model where we estimate each segment bound-ary independently of all other boundaries.
Middle:simple semi-Markov structure.
The position of thesegment boundaries only depends on the position ofits neighbors, as denoted by the (red) dash arcs.
Bot-tom: a more sophisticated semi-Markov structure,where each boundary depends on the position of twoof its neighbors.
This may occur, e.g., when the de-cision of where to place a boundary depends on thecontent of two adjacent segments.
The longer rangeinteraction is represented by the additional (blue)arcs.nent examples of structured prediction.
This prob-lem is generally formalized such that there existsone variable for each observation in the sequenceand the variables form aMarkov chain (HMM).
Seg-mentation of a sequence has been studied as a classof sequence prediction problems with common ap-plications such as protein secondary structure pre-diction, Named Entity Recognition and segmenta-tion of FAQ?s.
The exceptions to this approachare Sarawagi & Cohen (2004); Raetsch & Sonnen-burg (2006), which show that Semi-Markov mod-els (SMMs) (Janssen & Limnois, 1999), which area variation of Markov models, are a natural formu-lation for sequence segmentation.
The advantage ofthese models, depicted in Figure 1, is their abilityto encode features that capture properties of a seg-ment as a whole, which is not possible in an HMMmodel.
In particular, these features can encode simi-larities between two sequence segments of arbitrarylengths, which can be very useful in tasks such asAPS.In this paper, we present a Semi-Markov modelfor APS and propose a max-margin training on thesemethods.
This training method is a generalization ofthe Max-Margin methods for HMMs (Altun et al,2003b) to SMMs.
It follows the recent literatureon discriminative learning of structured prediction(Lafferty et al, 2001; Collins, 2002; Altun et al,2003a; Taskar et al, 2003).
Our method inherits theadvantages of discriminative techniques, namely theability to encode arbitrary (overlapping) features andnot making implausible conditional independenceassumptions.
It also has advantages of SMM mod-els, namely the ability to encode features at seg-ment level.
We present a linear time inference al-gorithm for SMMs and outline the learning method.Experimental evaluation on datasets used previouslyon this task (Sporleder & Lapata, 2004) shows im-provement over the state-of-the art methods on APS.2 Modeling Sequence SegmentationIn sequence segmentation, our goal is to solve the es-timation problem of finding a segmentation y ?
Y ,given an observation sequence x ?
X .
For exam-ple, in APS x can be a book which is a sequenceof sentences.
In a Semi-Markov model, there ex-ists one variable for each subsequence of observa-tions (i. e. multiple observations) and these variablesform a Markov chain.
This is opposed to an HMMwhere there exists one variable for each observation.More formally, in SMMs, y ?
Y is a sequence ofsegment labelings si = (bi, li) where bi is a non-negative integer denoting the beginning of the ithsegment which ends at position bi+1 ?
1 and whoselabel is given by li (Sarawagi & Cohen, 2004).
Sincein APS the label of the segments is irrelevant, werepresent each segment simply by the beginning po-sition y := {bi}L?1i=0 with the convention that b0 = 0and bL = N where N is the number of observationsin x.
Here, L denotes the number of segments in y.So the first segment is [0, b1), and the last segmentis [bL?1, N), where [a, b) denotes all the sentencesfrom a to b inclusive a but exclusive b.We cast this estimation problem as finding a dis-criminant function F (x, y) such that for an obser-vation sequence x we assign the segmentation thatreceives the best score with respect to F ,y?
(x) := argmaxy?YF (x, y).
(1)641As in many learning methods, we consider functionsthat are linear in some feature representation ?,F (x, y;w) = ?w,?
(x, y)?.
(2)Here, ?
(x, y) is a feature map defined over the jointinput/output space as detailed in Section 2.3.2.1 Max-Margin TrainingWe now present a maximum margin training forpredicting structured output variables, of which se-quence segmentation is an instance.
One of the ad-vantages of this method is its ability to incorporatethe cost function that the classifier is evaluated with.Let ?
(y, y?)
be the cost of predicting y?
instead of y.For instance, ?
is usually the 0-1 loss for binary andmulticlass classification.
However, in segmentation,this may be a more sophisticated function such asthe symmetric difference of y and y?
as discussed inSection 2.2.
Then, one can argue that optimizing aloss function that incorporates this cost can lead tobetter generalization properties.
One can find a the-oretical analysis of this approach in Tsochantaridiset al (2004).We follow the general framework of Tsochan-taridis et al (2004) and look for a hyperplane thatseparates the correct labeling yi of each observa-tion sequence xi in our training set from all the in-correct labelings Y ?yi with some margin that de-pends on ?
additively 1.
In order to allow someoutliers, we use slack variables ?i and maximize theminimum margin, F (xi, yi)?maxy?Y ?yi F (xi, y),across training instances i. Equivalently,minw,?12?w?2 + Cm?i=1?i (3a)?i, y ?w,?
(xi, yi)?
?
(xi, y)?
?
?
(yi, y)?
?i.
(3b)To solve this optimization problem efficiently, one1There is an alternative formulation that is multiplicative in?.
We prefer (3) due to computational efficiency reasons.can investigate its dual given bymin?12?i,j,y,y??iy?jy???
(xi, y),?
(xj , y?)?(4)??i,y?
(yi, y)?iy?i, y?y?iy ?
C, ?iy ?
0.Here, there exists one parameter ?iy for each train-ing instance xi and its possible labeling y ?Y .
Solving this optimization problem presents aformidable challenge since Y generally scales expo-nentially with the number of variables within eachvariable y.
This essentially makes it impossibleto find an optimal solution via enumeration.
In-stead, one may use a column generation algorithm(Tsochantaridis et al, 2005) to find an approximatesolution in polynomial time.
The key idea is to findthe most violated constraints (3b) for the current setof parameters and satisfy them up to some precision.In order to do this, one needs to findargmaxy?Y?
(yi, y) + ?w,?
(xi, y)?
, (5)which can usually be done via dynamic program-ming.
As we shall see, this is an extension of theViterbi algorithm for Semi Markov models.Note that one can express the optimizationand estimation problem in terms of kernelsk((x, y), (x?, y?))
:= ??
(x, y),?
(x?, y?)?.
We referthe reader to Tsochantaridis et al (2005) for details.To adapt the above framework to the segmenta-tion setting, we need to address three issues: a) weneed to specify a loss function ?
for segmentation,b) we need a suitable feature map ?
as defined inSection 2.3, and c) we need to find an algorithmto solve (5) efficiently.
The max-margin training ofSMMs was also presented in Raetsch & Sonnenburg(2006)2.2 Cost FunctionTo measure the discrepancy between y and some al-ternative sequence segmentation y?, we simply countthe number of segment boundaries that have a) beenmissed and b) been wrongly added.
Note that thisdefinition allows for errors exceeding 100% - for642Algorithm 1 Max-Margin Training AlgorithmInput: data xi, labels yi, sample sizem, toleranceInitialize Si = ?
for all i, and w = 0.repeatfor i = 1 to m dow =?i?y?Si?iy?
(xi, y)y?
= argmaxy?Y ?w,?
(xi, y)?+ ?
(yi, y)?
= max(0,maxy?Si ?w,?
(xi, y)?
+?
(yi, y))if ?w,?
(xi, y?
)?+ ?
(yi, y) > ?
+  thenIncrease constraint set Si ?
Si ?
y?Optimize (4) wrt ?iy,?y ?
Si.end ifend foruntil S has not changed in this iterationinstance, if we were to place considerably moreboundaries than can actually be found in a sequence.The number of errors is given by the symmetricdifference between y and y?, when segmentationsare viewed as sets.
This can be written as?
(y, y?)
= |y|+ |y?| ?
2|y ?
y?|= |y|+l??i=1[1?
2{b?i ?
y}].
(6)Here | ?
| denotes the cardinality of the set.
Eq.
(6)plays a vital role in solving (5), since it allows us todecompose the loss in y?
into a constant and func-tions depending on the segment boundaries b?i only.Note that in the case where we want to segment andlabel, we simply would need to check that the posi-tions are accurate and that the labels of the segmentsmatch.2.3 Feature RepresentationSMMs can extract three kinds of features from theinput/output pairs: a) node features, i. e. features thatencode interactions between attributes of the obser-vation sequence and the (label of a) segment (ratherthan the label of each observation as in HMM), b)features that encode interactions between neighbor-ing labels along the sequence and c) edge features,i.
e. features that encode properties of segments.
Thefirst two types of features are commonly used inother sequence models, such as HMMs and Con-ditional Random Fields (CRFs).
The third featuretype is specific to Semi-Markov models.
In particu-lar, these features can encode properties of a wholesegment or similarities between two sequence seg-ments of arbitrary lengths.
The cost of this express-ibility is simply a constant factor of the complexityof Markov models, if the maximum length of a seg-ment is bounded.
This type of features are particu-larly useful in the face of sparse data.As in HMMs, we assume stationarity in our modeland sum over the features of each segment to get?
(x, y).
Then, ?
corresponding to models of themiddle structure given in Figure 1 is given by?
(x, y?)
:= (?0,l?
?1?i=1?1(n?i, x),l?
?i=1?2(b?i?1, b?i, x)).We let ?0 = l?
?
1, the number of segments.
Thenode features ?1 capture the dependency of the cur-rent segment boundary to the observations, whereasthe edge features?2 represent the dependency of thecurrent segment to the observations.
To model thebottom structure in Figure 1, one can design featuresthat represent the dependency of the current segmentto its adjacent segments as well as the observations,?3(x, bi?2, bi?1, bi).
The specific choices of the fea-ture map ?
are presented in Section 3.2.4 Column Generation on SMMsTractability of Algorithm 1 depends on the existenceof an efficient algorithm that finds the most violatedconstraint (3b) via (5).
Both the cost function of Sec-tion 2.2 and the feature representation of Section 2.3are defined over a short sequence of segment bound-aries.
Therefore, using the Markovian property, onecan perform the above maximization step efficientlyvia a dynamic programming algorithm.
This is asimple extension of the Viterbi algorithm.
The infer-ence given by (1) can be performed using the samealgorithm, setting ?
to a constant function.We first state the dynamic programming recursionfor F + ?
in its generality.
We then give the pseu-docode for ?3 = ?.Denote by T (t?, t+;x) the largest value of?
(y, p)+F (x, p) for any partial segmentation p thatstarts at position 0 and which ends with the segment[t?, t+).
Moreover, let M be a upper bound on the643Algorithm 2 Column GenerationInput: sequence x, segmentation y, max-lengthof a segment MOutput: score s, segment boundaries y?Initialize vectors T ?
Rm and R ?
Ym to 0for i = 1 to l doRi = argmaxmax(0,i?M)?j<iTj + g(j, i)Ti = TRi + g(Ri, i)end fors = Tm + |y|y?
= {m}repeati = y?firsty?
?
{Ri, y?
}until i = 0length of a segment.
The recursive step of the dy-namic program is given byT (t?, t+;x) = maxmax(0,t?
?M)?k<t?T (k, t?
;x)+ g(k, t?, t+)where we defined the increment g(k, t?, t+) as?
?0(x),?1(x, t+),?2(x, t?, t+),?3(x, k, t?, t+), w?+ 1?
2 {(t?, t+) ?
y}where by convention T (i, i?)
= ??
if i < 0 forall labels.
Since T needs to be computed for all val-ues of t+ ?
M ?
t?
< t+, we need to computeO(|x|M) many values, each of which requires anoptimization over M possible values.
That is, stor-age requirements are O(|x|M), whereas the com-putation scales with O(|x|M2).
If we have a goodbound on the maximal sequence length, this can bedealt with efficiently.
Finally, the recursion is set upby T (0, 0, x) = |y|.See Algorithm 2 for pseudocode, when ?3 = ?.The segmentation corresponding to (5) is found byconstructing the path traversed by the argument ofthe max operation generating T .3 FeaturesWe now specify the features described in Section 2.3for APS.
Note that the second type of features donot exist for APS since we ignore the labelings ofsegments.3.1 Node Features ?1Node features?1(bj , x) represent the information ofthe current segment boundary and some attributes ofthe observations around it (which we define as thecurrent, preceding and successive sentences).
Theseare sentence level features, which we adapt fromGenzel (2005) and Sporleder & Lapata (2004) 2.
Forthe bj th sentence, x(bj), we use the following fea-tures?
Length of x(bj).?
Relative Position of x(bj).?
Final punctuation of x(bj).?
Number of capitalized words in x(bj).?
Word Overlap of x(bj) with the next oneWover(x(bj), x(bj + 1)) =2 | x(bj) ?
x(bj + 1) || x(bj) | + | x(bj + 1) |.?
First word of x(bj).?
Bag Of Words (BOW) features: Let the bag ofwords of a set of sentences S beB(S) = (c0, c1, ..., ci, ..., cN?1),where N is the size of the dictionary and ci isthe frequency of word i in S.?
BOW of x(bj), B({x(bj)})?
BOW of x(bj) and the previous sentenceB({x(bj ?
1), x(bj)})?
BOW of x(bj) and the succeeding sen-tence B({x(bj), x(bj + 1)})?
The inner product of the two items above?
Cosine Similarity of x(bj) and the previoussentenceCS(x(bj ?
1), x(bj))=?B(x(bj ?
1)), B(x(bj))?| B(x(bj ?
1)) | ?
| B(x(bj)) |2Due to space limitations, we omit the motivations for thesefeatures and refer the reader to the literature cited above.644?
Shannon?s Entropy of x(bj) computed by us-ing a language model as described in Genzel &Charniak (2003).?
Quotes(Qp, Qc, Qi).
Qp andQc are the numberof pairs of quotes in the previous(Nump) andcurrent sentence (Numc), Qp = 0.5 ?
Numpand Qc = 0.5?Numc.3.1.1 Edge Features ?2Below is the set of features?2(bj , bj+1, x) encod-ing information about the current segment.
Thesefeatures represent the power of the Semi-Markovmodels.
Note that ?3 features also belong to edgefeatures category.
In this paper, we did not use ?3feature due to computational issues.?
Length of The Paragraph: This feature ex-presses the assumption that one would want tohave a balance across the lengths of the para-graphs assigned to a text.
Very long and veryshort paragraphs should be uncommon.?
Cosine Similarity of the current paragraph andneighboring sentences: Ideally, one would liketo measure the similarity of two consecutiveparagraphs and search for a segmentation thatassigns low similarity scores (in order to fa-cilitate changes in the content).
This can beencoded using ?3(x, bj?1, bj , bj+1) features.When such features are computationally expen-sive, one can measure the similarity of the cur-rent paragraph with the preceding sentence asCS(P, x(bj ?
1))=?BOW (P ), BOW (x(bj ?
1))?| BOW (P ) | ?
| BOW (x(bj ?
1)) |where P is the set of sentences in the currentparagraph, [bj , bj+1).
A similar feature is usedfor CS(P, x(bj+1)).?
Shannon?s Entropy of the Paragraph: The mo-tivation for including features encoding the en-tropy of the sentences is the observation that theentropy of paragraph initial sentences is lowerthan the others (Genzel & Charniak, 2003).The motivation for including features encod-ing the entropy of the paragraphs, on the otherhand, is that the entropy rate should remainmore or less constant across paragraphs, es-pecially for long texts like books.
We ignorethe sentence boundaries and use the same tech-nique that we use to compute the entropy of asentence.3.2 Feature RescalingMost of the features described above are binary.There are also some features such as the entropywhose value could be very large.
We rescale all thenon-binary valued features so that they do not over-ride the effect of the binary features.
The scaling isperformed as follows:unew =u?min(u)max(u)?min(u)where unew is the new feature and u is the old fea-ture.
min(u) is the minimum of u, and max(u)is the maximum of u.
An exception to this is therescaling of BOW features which is given byB(x(bj))new = B(x(bj))/?B(x(bj)), B(x(bj))?
?., .?
denotes the inner product.4 ExperimentsWe collected four sets of data for our experiments.The first corpus, which we call SB, consists of man-ually annotated text from the book The Adventuresof Bruce-Partington Plans by Arthur Conan-Doyle.The second corpus, which we call SA, again con-sists of manually annotated text but from 10 differ-ent books by Conan-Doyle.
Our third corpus con-sists of German (GER) and English (ENG) texts.The German data consisting of 12 German novelswas used by Sporleder & Lapata (2006).
This datauses automatically assigned paragraph boundaries,with the labeling error expected to be around 10%.The English data contains 12 well known Englishbooks from Project Gutenberg (http://www.gutenberg.org/wiki/Main Page).
For thisdataset the paragraph boundaries were marked man-ually.All corpora were approximately split into train-ing (72%), development (21%), and test set (7%)(see Table 1).
The table also reports the accuracy ofthe baseline classifier, denoted as BASE, which ei-ther labels all sentences as paragraph boundaries or645Table 1: Number of sentences and % accuracy of thebaseline classifier (BASE) on various datasets usedin our experiments.TOTAL TRAIN DEV TEST BASESB 59,870 43,678 12,174 3,839 53.70SA 69,369 50,680 14,204 4,485 58.62ENG 123,261 88,808 25,864 8,589 63.41GER 370,990 340,416 98,610 31,964 62.10non-boundaries, choosing whichever scheme yieldsa better accuracy.We evaluate our system using accuracy, precision,recall, and the F1-score given by (2?
Precision?Recall)/(Precision+Recall) and compare our re-sults to Sporleder & Lapata (2006) who used Boos-Texter (Schapire & Singer, 2000) as a learning al-gorithm.
To the best of our knowledge, BoosTexter(henceforth called BT) is the leading method pub-lished for this task so far.
In order to evaluate the im-portance of the edge features and the resultant large-margin constraint, we also compare against a stan-dard binary Support Vector Machine (SVM) whichuses node features alone to predict whether eachsentence is the beginning of a paragraph or not.
Fora fair comparison, all classifiers used the linear ker-nel and the same set of node features.We perform model selection for all three algo-rithms by choosing the parameter values that achievethe best F1-score on the development set.
Forboth the SVM as well as our algorithm, SMM, wetune the parameter C (see (3a)) which measures thetrade-off between training error and margin.
For BT,we tune the number of Boosting iterations, denotedby N .4.1 ResultsIn our first experiment, we compare the perfor-mance of our algorithm, SMM, on the English andGerman corpus to a standard SVM and BoosTex-ter.
We report these result in Table 2.
Our algo-rithm achieves the best F1-score on the ENG cor-pus.
SMM performs very competitively on the GERcorpus, achieving accuracies close to those of BT.We observed a large discrepancy between the per-formance of our algorithm on the development andTable 2: Test results on ENG and GER data aftermodel selection.DATASET ALGO.
ACC.
REC.
PREC.
F1ENG SMM 75.61 46.67 77.78 58.33SVM 58.54 26.67 40.00 32.00BT 65.85 33.33 55.56 41.67GER SMM 70.56 46.81 65.67 54.66SVM 39.92 100.00 38.68 55.79BT 72.58 54.26 67.11 60.00the test datasets.
The situation is similar for bothSVM and BT.
For instance, BT when trained onthe ENG corpora, achieves an optimal F1-score of18.67% after N = 100 iterations.
For the same Nvalue, the test performance is 41.67%.
We conjec-ture that this discrepancy is because the books thatwe use for training and test are written by differ-ent authors.
While there is some generic informa-tion about when to insert a paragraph break, it isoften subjective and part of the authors style.
Totest this hypothesis, we performed experiments onthe SA and SB corpus, and present results in Table3.
Indeed, the F1-scores obtained on the develop-ment and test corpus closely match for text drawnfrom the same book (whilst exhibiting better over-all performance), differs slightly for text drawn fromdifferent books by the same author, and has a largedeviation for the GER and ENG corpus.Table 3: Comparison on various ENG datasets.DATASET ACC.
REC.
PREC.
F1-SCORESB (DEV) 92.81 86.44 92.73 89.47SB (TEST) 96.30 96.00 96.00 96.00SA (DEV) 82.24 61.11 82.38 70.17SA (TEST) 81.03 79.17 76.00 77.55ENG (DEV) 69.84 18.46 78.63 29.90ENG (TEST) 75.61 46.67 77.78 58.33There is one extra degree of freedom that we canoptimize in our model, namely the offset, i. e. theweight assigned to the constant feature ?0.
Afterfixing all the parameters as described above, we varythe value of the offset parameter and pick the valuethat gives the F1-score on the development data.
Wechoose to use F1-score, since it is the error measurethat we care about.
Although this extra optimization646leads to better F1-score in German (69.35% as op-posed to 54.66% where there is no extra tuning ofthe offset), it results in a decrease of the F1-score inEnglish (52.28% as opposed to 58.33%).
These re-sults are reported in Table 4.
We found that the dif-ference of the F1-score of tuning and not tuning thethreshold on the development set was not a good in-dicator on the usefulness of this extra parameter.
Weare now investigating other properties, such as vari-ance on the development data, to see if the tuning ofthe threshold can be used for better APS systems.Figure 2: Precision-recall curvesFigure 2 plots the precision-recall curve obtainedon various datasets.
As can be seen the performanceof our algorithm on the SB dataset is close to opti-mum, whilst it degrades slightly on the SA dataset,and substantially on the ENG and GER datasets.This further confirms our hypothesis that our algo-rithm excels in capturing stylistic elements from asingle author, but suffers slightly when trained toidentify generic stylistic elements.
We note that thisis not a weakness of our approach alone.
In fact, allthe other learning algorithms also suffer from thisshortcoming.Table 4: Performance on ENG test set tuning theoffset for best F1-score on ENG development set.DATASET ACC.
REC.
PREC.
F1-SCOREENG 75.61 46.67 77.78 58.33ENG +?0 39.02 93.33 36.84 52.28GER 70.56 46.81 65.67 54.66GER + ?0 75.40 73.40 65.71 69.355 ConclusionWe presented a competitive algorithm for paragraphsegmentation which uses the ideas from large mar-gin classifiers and graphical models to extend thesemi-Markov formalism to the large margin case.We obtain an efficient dynamic programming for-mulation for segmentation which works in lineartime in the length of the sequence.
Experimentalevaluation shows that our algorithm is competitivewhen compared to the state-of-the-art methods.As future work, we plan on implementing ?3 fea-tures in order to perform an accuracy/time analy-sis.
By defining appropriate features, we can useour method immediately for text and discourse seg-mentation.
It would be interesting to compare thismethod to Latent Semantic Analysis approaches fortext segmentation as studied for example in Bestgen(2006) and the references thereof.ReferencesAltun, Y., Hofmann, T., & Johnson, M. (2003a).Discriminative Learning for Label Sequences viaBoosting.
In In Proceedings of NIPS 2003.Altun, Y., Tsochantaridis, I., & Hofmann, T.(2003b).
Hidden markov support vector ma-chines.
In International Conference on MachineLearning.Bestgen, Y.
(2006).
Improving text segmentation us-ing latent semantic analysis: A reanalysis of choi,wiemer-hastings, and moore (2001).
Computa-tional Linguistics, 32, 5?12.Collins, M. (2002).
Discriminative training meth-ods for hidden markov models.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing.Filippova, K., & Strube, M. (2006).
Using linguisti-cally motivated features for paragraph segmenta-tion.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing.Genzel, D. (2005).
A paragraph boundary detec-tion system.
In Proceedings of the Conferenceon Computational Linguistics and Intelligent TextProcessing.Genzel, D., & Charniak, E. (2003).
Variation of en-tropy and parse tree of sentences as a function of647the sentence number.
In Proceedings of the Con-ference on Empirical Methods in Natural Lan-guage Processing.Janssen, J., & Limnois, N. (1999).
Semi-markovmodels and applications.
Kluwer Academic.Lafferty, J. D., McCallum, A., & Pereira, F. (2001).Conditional random fields: Probabilistic model-ing for segmenting and labeling sequence data.
In18th International Conference onMachine Learn-ing ICML.Raetsch, G., & Sonnenburg, S. (2006).
Large scalehidden Semi-Markov SVMs for gene structureprediction.
In In Proceedings of NIPS 2006.Sarawagi, S., & Cohen, W. (2004).
Semi-MarkovConditional Random Fields for Information Ex-traction.
In Advances in Neural Information Pro-cessing Systems (NIPS).Schapire, R. E., & Singer, Y.
(2000).
Boostexter:A boosting-based system for text categorization.Machine Learning, 39(2/3), 135?168.Sporleder, C., & Lapata, M. (2004).
Automatic para-graph identification: A study across languagesand domains.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing.Sporleder, C., & Lapata, M. (2006).
Broad coverageparagraph segmentation across languages and do-mains.
ACM Trans.
Speech Lang.
Process., 3(2),1?35.Taskar, B., Guestrin, C., & Koller, D. (2003).
Max-margin markov networks.
In S. Thrun, L. Saul, &B. Scho?lkopf, eds., Advances in Neural Informa-tion Processing Systems 16.Tsochantaridis, I., Hofmann, T., Joachims, T., & Al-tun, Y.
(2004).
Support vector machine learningfor interdependent and structured output spaces.In ICML ?04: Twenty-first international confer-ence on Machine learning.
New York, NY, USA:ACM Press.
ISBN 1-58113-828-5.Tsochantaridis, I., Joachims, T., Hofmann, T., & Al-tun, Y.
(2005).
Large margin methods for struc-tured and interdependent output variables.
Jour-nal of Machine Learning Research.Zhu, C. (1999).
Ut once more: The sentence as thekey functional unit of translation.
Meta, 44(3),429?447.648
