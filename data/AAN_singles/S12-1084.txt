First Joint Conference on Lexical and Computational Semantics (*SEM), pages 575?578,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsTiantianzhu7:System Description of Semantic Textual Similarity (STS) inthe SemEval-2012 (Task 6)Tiantian ZhuDepartment of Computer Science andTechnologyEast China Normal University51111201046@student.ecnu.edu.cnMan LanDepartment of Computer Science andTechnologyEast China Normal Universitymlan@cs.ecnu.edu.cnAbstractThis paper briefly reports our submissions tothe Semantic Textual Similarity (STS) taskin the SemEval 2012 (Task 6).
We first useknowledge-based methods to compute wordsemantic similarity as well as Word Sense Dis-ambiguation (WSD).
We also consider wordorder similarity from the structure of the sen-tence.
Finally we sum up several aspects ofsimilarity with different coefficients and getthe sentence similarity score.1 IntroductionThe task of semantic textual similarity (STS) is tomeasure the degree of semantic equivalence betweentwo sentences.
It plays an increasingly importantrole in several text-related research and applications,such as text mining, Web page retrieval, automaticquestion-answering, text summarization, and ma-chine translation.
The goal of the Semeval 2012 STStask (task 6) is to build a unified framework for theevaluation of semantic textual similarity modules fordifferent systems and to characterize their impact onNLP applications.Generally, there are two ways to measure sim-ilarity of two sentences, i.e, corpus-based meth-ods and knowledge-based methods.
The corpus-based method typically computes sentence similar-ity based on the frequency of word occurrence or theco-occurrence between collocated words.
For ex-ample, in (Islam and Inkpen, 2008) they proposed acorpus-based sentence similarity measure as a func-tion of string similarity, word similarity and com-mon word order similarity (CWO).
The knowledge-based method computes sentence similarity basedon the semantic information collected from knowl-edge bases.
With the aid of a number of success-ful computational linguistic projects, many seman-tic knowledge bases are readily available, for ex-ample, WordNet, Spatial Date Transfer Standard,Gene Ontology, etc.
Among them, the most widelyused one is WordNet, which is organized by mean-ings and developed at Princeton University.
Sev-eral methods computed word similarity by usingWordNet, such as the Lesk method in (Banerjee andPedersen, 2003), the lch method in (Leacock andChodorow, 1998)and the wup method in (Wu andPalmer, 1994).
Generally, although the knowledge-based methods heavily depend on the knowledgebases, they performed much better than the corpus-based methods in most cases.
Therefore, in our STSsystem, we use a knowledge-based method to com-pute word similarity.The rest of this paper is organized as follows.
Sec-tion 2 describes our system.
Section 3 presents theresults of our system.2 System DescriptionUsually, a sentence is composed of some nouns,verbs, adjectives, adverbs and/or some stop words.We found that these words carry a lot of informa-tion, especially the nouns and verbs.
Although theadjectives and adverbs also make contribution to thesemantic meaning of the sentence, they are muchweaker than the nouns and verbs.
So we considerto measure the sentence semantic similarities fromthree aspects.
We define the following three types ofsimilarity from two compared sentences to measure575the semantic similarity: (1) Noun Similarity to mea-sure the similarity between the nouns from the twocompared sentences, (2) Verb Similarity to measurethe similarity between Verbs, (3) ADJ-ADV Simi-larity to measure the similarity between the adjec-tives and adverbs from each sentence.
Besides thesemantic information similarity, we also found thatthe structure of the sentences carry some informa-tion which cannot be ignored.
Therefore, we definethe last aspect of the sentence similarity as Word Or-der Similarity.
In the following we will introduce thedifferent components of our system.2.1 POSAs a basic natural language processing technique,part of speech tagging is to identify the part ofspeech of individual words in the sentence.
In or-der to compute the three above semantic similari-ties, we first identify the nouns, verbs, adjectives,and adverbs in the sentence.
Then we can calculatethe Noun Similarity, Verb Similarity and ADJ-ADVSimilarity from two sentences.2.2 Semantic similarity between wordsThe word similarity measurement have importantimpact on the performance of sentence similarity.Currently, many lexical resources based approachesperform comparatively well to compute semanticword similarities.
However, the exact resources theyare based are quite different.
For example, some arebased on dictionary and/or thesaurus, and others arebased on WordNet.WordNet is a machine-readable lexical database.The words in Wordnet are classified into four cat-egories, i.e., nouns, verbs, adjectives and adverbs.WordNet groups these words into sets of syn-onyms called synsets, provides short definitions, andrecords the various semantic relations between thesesynsets.
The synsets are interlinked by means ofconceptual-semantic and lexical relations.
Word-Net alo provides the most common relationshipsinclude Hyponym/Hypernym (i.e., is-a relationships)and Meronym/Holonym (i.e., part-of relationships).Nouns and verbs are organized into hierarchiesbased on the hyponymy/hypernym relation betweensynsets while adjectives and adverbs are not.In this paper, we adopt the wup method in (Wuand Palmer, 1994) to estimate the semantic similar-ity between two words, which estimates the seman-tic similarity between two words based on the depthof the two words in WordNet and the depth of theirleast common subsumer (LCS), where LCS is de-fined as the common ancestor deepest in the taxon-omy.For example, given two words, w1 and w2, thesemantic similarity s(w1,w2) is the function of theirdepth in the taxonomy and the depth of their leastcommon subsumer.
If d1 and d2 are the depth ofw1 and w2 in WordNet, and h is the depth of theirleast common subsumer in WordNet, the semanticsimilarity can be written as:s(w1, w2) =2.0 ?
hd1 + d2(1)2.3 Word Sense DisambiguationWord Sense Disambiguation (WSD) is to identifythe actual meaning of a word according to the con-text.
In our word similarity method, we take thenearest meaning of two words into considerationrather than their actual meaning.
More impor-tantly, the nearest meaning does not always repre-sent the actual meaning.
In our system, we useda WSD algorithm proposed by (Ted Pedersen etal.,2005), which computes semantic relatedness ofword senses using extended gloss overlaps of theirdictionary definitions.
We utilize this WSD algo-rithm for each sentence to get the actual meaning ofeach word before computing the word semantic sim-ilarity.2.4 Semantic SimilarityWe adopt a similar way to compute the three types ofsemantic similarities.
Here we take Noun Similarityas an example.Suppose sentence s1 and s2 are the two sentencesto be compared, s1 has a nouns while s2 has b nouns.Then we get a ?
b noun pairs and use the word sim-ilarity method mentioned in section 2.2 to computethe Noun Similarity of each noun pair.
After that,for each noun, we choose its highest score in nounpairs as its similarity score.
Then we use the formulabelow to compute the Noun Similarity.SimNoun =(?ci=1 ni) ?
(a + b)2ab(2)576where c represents the number of noun words insequence a and sequence b, c = min(a, b); ni rep-resents the highest matching similarity score of i-thword in the shorter sequence with respect to one ofthe words in the longer sequence; and?ci=1 ni rep-resents the sum of the highest matching similarityscore between the words in sequence a and sequenceb.
Similarly, we can get SimV erb.
Since there is noHyponym/Hypernym relation for adjectives and ad-verbs in WordNet, we just compute ADJ-ADV Sim-ilarity based on the frequency of overlap of simplewords.2.5 Word Order SimilarityWe believe that word order information also makecontributions to sentence similarity.
In most cases,the longer common sequence (LCS) the two sen-tences have, the higher similarity score the sentencesget.
For example the pair of sentences s1 and s2, weremove all the punctuation from the sentences:?
s1: But other sources close to the sale saidVivendi was keeping the door open to furtherbids and hoped to see bidders interested in in-dividual assets team up?
s2: But other sources close to the sale saidVivendi was keeping the door open for furtherbids in the next day or twoSince the length of the longest common sequenceis 14, we use the following formula to compute theword order similarity.SimWordOrder =lengthofLCSshorterlength(3)where the shorter length means the length of theshorter sentence.2.6 Overall SimilarityAfter we have the Noun Similarity, Verb Similar-ity, ADJ-ADV Similarity and Word Order Similar-ity, we calculate the Overall Similarity of two com-pared sentences based on these four scores of simi-larity.
We combine them in the following way:Simsent = aSimNoun + bSimV erb+cSimADJ?ADV + dSimWordOrder(4)Where a, b, c and d are the coefficients whichdenote the contribution of each aspect to the over-all sentence similarity, For different data collections,we empirically set different coefficients, for exam-ple, for the MSR Paraphrase data, the four coeffi-cients are set as 0.5, 0.3, 0.1, 0.1, because it is hardto get the highest score 5 even when the two sen-tences are almost the same meaning, We empiricallyset a threshold, if the score exceeds the threshold weset the score 5.3 Experiment and Results on STSFirstly, Stanford parser1 is used to parse eachsentence and to tag each word with a part ofspeech(POS).
Secondly, WordNet SenseRelate All-Words2, a WSD tool from CPAN is used to disam-biguate and to assign a sense for each word based onthe assigned POS.We submitted three runs: run 1 with WSD, run 2without WSD, run 3 removing stop words and with-out WSD.
The stoplist is available online3.
Table 1lists the performance of these three systems as wellas the baseline and the rank 1 results on STS task inSemEval 2012.We can see that run1 gets the best result, whichmeans WSD has improved the accuracy of sentencesimilarity.
Run3 gets better result than run2, whichproves that stop words do disturb the computation ofsentence similarity, removing them is a better choicein our system.4 ConclusionIn our work, we adopt a knowledge-based word sim-ilarity method with WSD to measure the seman-tic similarity between two sentences from four as-pects: Noun Similarity, Verb Similarity, ADJ-ADVSimilarity and Word Order Similarity.
The resultsshow that WSD improves the pearson coefficient atsome degree.
However, our system did not get agood rank.
It indicates there still exists many prob-lems such as wrong POS tag and wrong WSD whichmight lead to wrong meaning of one word in a sen-tence.1http://nlp.stanford.edu/software/lex-parser.shtml2http://search.cpan.org/Tedpederse/WordNet-SenseRelate-AllWords-0.193http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop577Table 1: STS system configuration and results on STS task.Run ALL ALLnrm Mean MSRpar MSRvid SMTeur OnWN SMTnewsrank 1 .7790 .8579 .6773 .6830 .8739 .5280 .6641 .4937baseline .3110 .6732 .4356 .4334 .2996 .4542 .5864 .39081 .4533 .7134 .4192 .4184 .5630 .2083 .4822 .27452 .4157 .7099 .3960 .4260 .5628 .1546 .4552 .19233 .4446 .7097 .3740 .3411 .5946 .1868 .4029 .1823AcknowledgmentsThe authors would like to thank the organizers fortheir invaluable support making STS a first-rank andinteresting international event.ReferencesChukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-dul Kadir, Shyamala C. Doraisamy.
2010.
Word SenseDisambiguation-based Sentence Similarity.
In Proc.COLING-ACL, Beijing.Jin Feng, Yiming Zhou, Trevor Martin.
2008.
Sen-tence Similarity based on Relevance.
Proceedings ofIPMU?08, Torremolinos.Yuhua Li, David McLean, Zuhair A. Bandar, James D.O?Shea, and Keeley Crockett.
2009.
Sentence Simi-larity Based on Semantic Nets and Corpus Statistics.LIN LI, XIA HU, BI-YUN HU, JUN WANG, YI-MINGZHOU.
2009.
MEASURING SENTENCE SIMILAR-ITY FROM DIFFERENT ASPECTS.Islam Aminul and Diana Inkpen.
2008.
Semantic TextSimilarity Using Corpus-Based Word Similarity andString Similarity.
ACM Transactions on KnowledgeDiscovery from Data.Banerjee and Pedersen.
2003.
Extended gloss overlapsas a measure of semantic relatedness.
In Proceed-ings of the Eighteenth International Joint Conferenceon Artificial Intelligence (IJCAI-03), pages805C810,Acapulco, Mexico.Leacock and Chodorow.
1998.
Combining local con-text and WordNet similarity for word sense identifica-tion.
In Christiane Fellbaum, editor, WordNet: AnElectronic Lexical Database.
The MIT Press, Cam-bridge,MA.Z.Wu and M.Palmer.
1994.
Verbs semantics andlexical selection.
In Proceedings of the 32nd an-nual meeting on Association for Computional Linguis-tics,Morristown, NJ, USA.Ted Pedersen, Satanjeev Banerjee, Siddharth Patward-han.
2005.
Maximizing Semantic Relatedness to Per-form Word Sense Disambiguation.578
