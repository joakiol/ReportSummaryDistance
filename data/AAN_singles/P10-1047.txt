Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 454?464,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsSyntax-to-Morphology Mapping in Factored Phrase-Based StatisticalMachine Translation from English to TurkishReyyan YeniterziLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USAreyyan@cs.cmu.eduKemal OflazerComputer ScienceCarnegie Mellon University-QatarPO Box 24866, Doha, Qatarko@cs.cmu.eduAbstractWe present a novel scheme to apply fac-tored phrase-based SMT to a language pairwith very disparate morphological struc-tures.
Our approach relies on syntac-tic analysis on the source side (English)and then encodes a wide variety of localand non-local syntactic structures as com-plex structural tags which appear as ad-ditional factors in the training data.
Onthe target side (Turkish), we only per-form morphological analysis and disam-biguation but treat the complete complexmorphological tag as a factor, instead ofseparating morphemes.
We incrementallyexplore capturing various syntactic sub-structures as complex tags on the En-glish side, and evaluate how our transla-tions improve in BLEU scores.
Our max-imal set of source and target side trans-formations, coupled with some additionaltechniques, provide an 39% relative im-provement from a baseline 17.08 to 23.78BLEU, all averaged over 10 training andtest sets.
Now that the syntactic analy-sis on the English side is available, wealso experiment with more long distanceconstituent reordering to bring the Englishconstituent order close to Turkish, but findthat these transformations do not provideany additional consistent tangible gainswhen averaged over the 10 sets.1 IntroductionStatistical machine translation into a morphologi-cally complex language such as Turkish, Finnishor Arabic, involves the generation of target wordswith the proper morphology, in addition to prop-erly ordering the target words.
Earlier work ontranslation from English to Turkish (Oflazer andDurgar-El-Kahlout, 2007; Oflazer, 2008; Durgar-El-Kahlout and Oflazer, 2010) has used an ap-proach which relied on identifying the contextu-ally correct parts-of-speech, roots and any mor-phemes on the English side, and the complete se-quence of roots and overt derivational and inflec-tional morphemes for each word on the Turkishside.
Once these were identified as separate to-kens, they were then used as ?words?
in a stan-dard phrase-based framework (Koehn et al, 2003).They have reported that, given the typical com-plexity of Turkish words, there was a substantialpercentage of words whose morphological struc-ture was incorrect: either the morphemes werenot applicable for the part-of-speech category ofthe root word selected, or the morphemes werein the wrong order.
The main reason given forthese problems was that the same statistical trans-lation, reordering and language modeling mecha-nisms were being employed to both determine themorphological structure of the words and, at thesame time, get the global order of the words cor-rect.
Even though a significant improvement of astandard word-based baseline was achieved, fur-ther analysis hinted at a direction where morphol-ogy and syntax on the Turkish side had to be dealtwith using separate mechanisms.Motivated by the observation that many lo-cal and some nonlocal syntactic structures in En-glish essentially map to morphologically complexwords in Turkish, we present a radically differentapproach which does not segment Turkish wordsinto morphemes, but uses a representation equiv-alent to the full word form.
On the English side,we rely on a full syntactic analysis using a depen-dency parser.
This analysis then lets us abstractand encode many local and some nonlocal syn-tactic structures as complex tags (dynamically, asopposed to the static complex tags as proposed byBirch et al (2007) and Hassan et al (2007)).
Thus454we can bring the representation of English syntaxcloser to the Turkish morphosyntax.Such an approach enables the following: (i)Driven by the pattern of morphological structuresof full word forms on the Turkish side representedas root words and complex tags, we can iden-tify and reorganize phrases on the English side,to ?align?
English syntax to Turkish morphologywherever possible.
(ii) Continuous and discontin-uous variants of certain (syntactic) phrases can beconflated during the SMT phrase extraction pro-cess.
(iii) The length of the English sentences canbe dramatically reduced, as most function wordsencoding syntax are now abstracted into complextags.
(iv) The representation of both the source andthe target sides of the parallel corpus can now bemostly normalized.
This facilitates the use of fac-tored phrase-based translation that was not pre-viously applicable due to the morphological com-plexity on the target side and mismatch betweensource and target morphologies.We find that with the full set of syntax-to-morphology transformations and some additionaltechniques we can get about 39% relative im-provement in BLEU scores over a word-basedbaseline and about 28% improvement of a factoredbaseline, all experiments being done over 10 train-ing and test sets.
We also find that further con-stituent reordering taking advantage of the syntac-tic analysis of the source side, does not providetangible improvements when averaged over the 10data sets.This paper is organized as follows: Sec-tion 2 presents the basic idea behind syntax-to-morphology alignment.
Section 3 describesour experimental set-up and presents results froma sequence of incremental syntax-to-morphologytransformations, and additional techniques.
Sec-tion 4 summarizes our constituent reordering ex-periments and their results.
Section 5 presents areview of related work and situates our approach.We assume that the reader is familiar with thebasics of phrase-based statistical machine transla-tion (Koehn et al, 2003) and factored statisticalmachine translation (Koehn and Hoang, 2007).2 Syntax-to-Morphology MappingIn this section, we describe how we map betweencertain source language syntactic structures andtarget words with complex morphological struc-tures.
At the top of Figure 1, we see a pair of(syntactic) phrases, where we have (positionally)aligned the words that should be translated to eachother.
We can note that the function words on andFigure 1: Transformation of an English preposi-tional phrasetheir are not really aligned to any of the Turkishwords as they really correspond to two of the mor-phemes of the last Turkish word.When we tag and syntactically analyze the En-glish side into dependency relations, and morpho-logically analyze and disambiguate the Turkishphrase, we get the representation in the middle ofFigure 1, where we have co-indexed componentsthat should map to each other, and some of thesyntactic relations that the function words are in-volved in are marked with dependency links.1The basic idea in our approach is to take variousfunction words on the English side, whose syntac-tic relationships are identified by the parser, andthen package them as complex tags on the relatedcontent words.
So, in this example, if we movethe first two function words from the English sideand attach as syntactic tags to the word they are independency relation with, we get the aligned rep-resentation at the bottom of Figure 1.2,3 Here wecan note that all root words and tags that corre-spond to each other are nicely structured and arein the same relative order.
In fact, we can treateach token as being composed of two factors: theroots and the accompanying tags.
The tags on theTurkish side encode morphosyntactic informationencoded in the morphology of the words, while the1The meanings of various tags are as follows: Depen-dency Labels: PMOD - Preposition Modifier; POS - Pos-sessive.
Part-of-Speech Tags for the English words: +IN -Preposition; +PRP$ - Possessive Pronoun; +JJ - Adjective;+NN - Noun; +NNS - Plural Noun.
Morphological FeatureTags in the Turkish Sentence: +A3pl - 3rd person plural;+P3sg - 3rd person singular possessive; +Loc - Locative case.Note that we mark an English plural noun as +NN NNS to in-dicate that the root is a noun and there is a plural morphemeon it.
Note also that economic is also related to relations butwe are not interested in such content words and their rela-tions.2We use to prefix such syntactic tags on the English side.3The order is important in that we would like to attach thesame sequence of function words in the same order so thatthe resulting tags on the English side are the same.455(complex) tags on the English side encode local(and sometimes, non-local) syntactic information.Furthermore, we can see that before the transfor-mations, the English side has 4 words, while af-terwards it has only 2 words.
We find (and elab-orate later) that this reduction in the English sideof the training corpus, in general, is about 30%,and is correlated with improved BLEU scores.
Webelieve the removal of many function words andtheir folding into complex tags (which do not getinvolved in GIZA++ alignment ?
we only align theroot words) seems to improve alignment as thereare less number of ?words?
to worry about duringthat process.4Another interesting side effect of this represen-tation is the following.
As the complex syntac-tic tags on the English side are based on syntacticrelations and not necessarily positional proximity,the tag for relations in a phrase like in their cul-tural, historical and economic relations would beexactly the same as above.
Thus phrase extrac-tion algorithms can conflate all constructs like intheir .
.
.
economic relations as one phrase, regard-less of the intervening modifiers, assuming thatparser does its job properly.Not all cases can be captured as cleanly as theexample above, but most transformations capturelocal and nonlocal syntax involving many functionwords and then encode syntax with complex tagsresembling full morphological tags on the Turk-ish side.
These transformations, however, are notmeant to perform sentence level constituent re-ordering on the English side.
We explore theselater.We developed set of about 20 linguistically-motivated syntax-to-morphology transformationswhich had variants parameterized depending onwhat, for instance, the preposition or the adverbialwas, and how they map to morphological struc-ture on the Turkish side.
For instance, one generalrule handles cases like while .
.
.
verb and if .
.
.
verbetc., mapping these to appropriate complex tags.It is also possible that multiple transformationscan apply to generate a single English complextag: a portion of the tag can come from a verbcomplex transformation, and another from an ad-verbial phrase transformation involving a markedsuch as while.
Our transformations handle the fol-lowing cases:?
Prepositions attach to the head-word of their4Fraser (2009) uses the first four letters of German wordsafter morphological stripping and compound decompositionto help with alignment in German to English and reversetranslation.complement noun phrase as a component inits complex tag.?
Possessive pronouns attach to the head-wordthey specify.?
The possessive markers following a noun(separated by the tokenizer) attached to thenoun.?
Auxiliary verbs and negation markers attachto the lexical verb that they form a verb com-plex with.?
Modals attach to the lexical verb they modify.?
Forms of be used as predicates with adjecti-val or nominal dependents attach to the de-pendent.?
Forms of be or have used to form passivevoice with past participle verbs, and forms ofbe used with -ing verbs to form present con-tinuous verbs, attach to the verb.?
Various adverbial clauses formed with if,while, when, etc., are reorganized so thatthese markers attach to the head verb of theclause.As stated earlier, these rules are linguistically mo-tivated and are based on the morphological struc-ture of the target language words.
Hence for dif-ferent target languages these rules will be differ-ent.
The rules recognize various local and nonlo-cal syntactic structures in the source side parse treethat correspond to complex morphological of tar-get words and then remove source function wordsfolding them into complex tags.
For instance, thetransformations in Figure 1 are handled by scriptsthat process Malt Parser?s dependency structureoutput and that essentially implement the follow-ing sequence of rules expressed as pseudo code:1) if (<Y>+PRP$ POS <Z>+NN<TAG>)then {APPEND <Y>+PRP$ TO <Z>+NN<TAG>REMOVE <Y>+PRP$}2) if (<X>+IN PMOD <Z>+NN<TAG>)then {APPEND <X>+IN TO <Z>+NN<TAG>REMOVE <X>+IN}Here <X>, <Y> and <Z> can be considered as Pro-log like-variables that bind to patterns (mostly rootwords), and the conditions check for specified de-pendency relations (e.g., PMOD) between the leftand the right sides.
When the condition is satis-fied, then the part matching the function word isremoved and its syntactic information is appendedto form the complex tag on the noun (<TAG> wouldeither match null string or any previously ap-pended function word markers.
)55We outline two additional rules later when we see a morecomplex example in Figure 2.456There are several other rules that handle moremundane cases of date and time constructions (forwhich, the part of the date construct which theparser attaches a preposition, is usually differentthan the part on the Turkish side that gets inflectedwith case markers, and these have to be reconciledby overriding the parser output.
)The next section presents an example of a sen-tence with multiple transformations applied, afterdiscussing the preprocessing steps.3 Experimental Setup and Results3.1 Data PreparationWe worked on an English-Turkish parallel corpuswhich consists of approximately 50K sentenceswith an average of 23 words in English sentencesand 18 words in Turkish sentences.
This is thesame parallel data that has been used in earlierSMT work on Turkish (Durgar-El-Kahlout andOflazer, 2010).
Let?s assume we have the follow-ing pair of parallel sentences:E: if a request is made orally the authority mustmake a record of itT: istek so?zlu?
olarak yap?lm?s?sa yetkili makam bunukaydetmelidirOn the English side of the data, we use the Stan-ford Log-Linear Tagger (Toutanova et al, 2003),to tag the text with Penn Treebank Tagset.
Onthe Turkish side, we perform a full morphologicalanalysis, (Oflazer, 1994), and morphological dis-ambiguation (Yuret and Tu?re, 2006) to select thecontextually salient interpretation of words.
Wethen remove any morphological features that arenot explicitly marked by an overt morpheme.6 Sofor both sides we get,E: if+IN a+DT request+NN is+VBZ made+VBN orally+RBthe+DT authority+NN must+MD make+VB a+DT record+NNof+IN it+PRPT: istek+Noun so?zlu?+Adj olarak+Verb+ByDoingSoyap+Verb+Pass+Narr+Cond yetkili+Adj makam+Nounbu+Pron+Acc kaydet+Verb+Neces+CopFinally we parse the English sentences usingMaltParser (Nivre et al, 2007), which gives uslabeled dependency parses.
On the output of theparser, we make one more transformation.
We re-place each word with its root, and possibly add anadditional tag for any inflectional information con-veyed by overt morphemes or exceptional forms.This is done by running the TreeTagger (Schmid,1994) on the English side which provides the rootsin addition to the tags, and then carrying over thisinformation to the parser output.
For example,is is tagged as be+VB VBZ, made is tagged asmake+VB VBN, and a word like books is tagged6For example, the morphological analyzer outputs +A3sgto mark a singular noun, if there is no explicit plural mor-pheme.
Such markers are removed.as book+NN NNS (and not as books+NNS).
Onthe Turkish side, each marker with a preceding+ is a morphological feature.
The first markeris the part-of-speech tag of the root and the re-mainder are the overt inflectional and derivationalmarkers of the word.
For example, the analy-sis kitap+Noun+P2pl+A3pl+Gen for a wordlike kitap-lar-?n?z-?n7 (of your books)represents the root kitap (book), a Noun, withthird person plural agreement A3pl, second per-son plural possessive agreement, P2pl and geni-tive case Gen.The sentence representations in the middle partof Figure 2 show these sentences with some of thedependency relations (relevant to our transforma-tions) extracted by the parser, explicitly marked aslabeled links.
The representation at the bottom ofthis figure (except for the co-indexation markers)corresponds to the final transformed form of theparallel training and test data.
The co-indexationis meant to show which root words on one sidemap to which on the other side.
Ultimately wewould want the alignment process to uncover theroot word alignments indicated here.
We can alsonote that the initial form of the English sentencehas 14 words and the final form after transforma-tions, has 7 words (with complex tags).83.2 ExperimentsWe evaluated the impact of the transformationsin factored phrase-based SMT with an English-Turkish data set which consists of 52712 parallelsentences.
In order to have more confidence in theimpact of our transformations, we randomly gen-erated 10 training, test and tune set combinations.For each combination, the latter two were 1000sentences each and the remaining 50712 sentenceswere used as training sets.9,10We performed our experiments with the Mosestoolkit (Koehn et al, 2007).
In order to encouragelong distance reordering in the decoder, we useda distortion limit of -1 and a distortion weight of7- shows surface morpheme boundaries.8We could give two more examples of rules to processthe if-clause in the example in Figure 2.
These rules wouldbe applied sequentially: The first rule recognizes the pas-sive construction mediated by be+VB<AGR> forming a verbcomplex (VC) with <Y>+VB_VBN and appends the formerto the complex tag on the latter and then deletes the formertoken.
The second rule then recognizes <X>+IN relating to<Y>+VB<TAGS>with VMOD and appends the former to thecomplex tag on the latter and then deletes the former token.9The tune set was not used in this work but reserved forfuture work so that meaningful comparisons could be made.10It is possible that the 10 test sets are not mutually exclu-sive.457Figure 2: An English-Turkish sentence pair with multiple transformations applied0.1.11 We did not use MERT to further optimizeour model.12For evaluation, we used the BLEU metric (Pap-ineni et al, 2001).
Each experiment was repeatedover the 10 data sets.
Wherever meaningful, wereport the average BLEU scores over 10 data setsalong with the maximum and minimum values andthe standard deviation.11These allow and do not penalize unlimited distortions.12The experience with MERT for this language pair hasnot been very positive.
Earlier work on Turkish indicates thatstarting with default Moses parameters and applying MERTto the resulting model does not even come close to the per-formance of the model with those two specific parameters setas such (distortion limit -1 and distortion weight 0.1), mostlikely because the default parameters do not encourage therange of distortions that are needed to deal with the con-stituent order differences.
Earlier work on Turkish also showsthat even when the weight-d parameter is initialized with thisspecific value, the space explored for distortion weight andother parameters do not produce any improvements on thetest set, even though MERT claims there are improvementson the tune set.The other practical reasons for not using MERT werethe following: at the time we performed this work, thediscussion thread at http://www.mail-archive.com/moses-support@mit.edu/msg01012.htmlindicated that MERT was not tested on multiple factors.The discussion thread at http://www.mail-archive.com/moses-support@mit.edu/msg00262.htmlclaimed that MERT does not help very much with factoredmodels.
With these observations, we opted not to experimentwith MERT with the multiple factor approach we employed,given that it would be risky and time consuming to runMERT needed for 10 different models and then not neces-sarily see any (consistent) improvements.
MERT howeveris orthogonal to the improvements we achieve here and canalways be applied on top of the best model we get.3.2.1 The Baseline SystemsAs a baseline system, we built a standard phrase-based system, using the surface forms of the wordswithout any transformations, and with a 3-gramLM in the decoder.
We also built a second baselinesystem with a factored model.
Instead of using justthe surface form of the word, we included the root,part-of-speech and morphological tag informationinto the corpus as additional factors alongside thesurface form.13 Thus, a token is represented withthree factors as Surface|Root|Tags whereTags are complex tags on the English side, andmorphological tags on the Turkish side.14Moses lets word alignment to align over any ofthe factors.
We aligned our training sets using onlythe root factor to conflate statistics from differentforms of the same root.
The rest of the factors arethen automatically assumed to be aligned, basedon the root alignment.
Furthermore, in factoredmodels, we can employ different language modelsfor different factors.
For the initial set of experi-ments we used 3-gram LMs for all the factors.For factored decoding, we employed a modelwhereby we let the decoder translate a surfaceform directly, but if/when that fails, the decodercan back-off with a generation model that buildsa target word from independent translations of theroot and tags.13In Moses, factors are separated by a ?|?
symbol.14Concatenating Root and Tags gives the Surfaceform, in that the surface is unique given this concatenation.458The results of our baseline models are given intop two rows of Table 1.
As expected, the word-based baseline performs worse than the factoredbaseline.
We believe that the use of multiple lan-guage models (some much less sparse than the sur-face LM) in the factored baseline is the main rea-son for the improvement.3.2.2 Applying Syntax-to-MorphologyMapping TransformationsTo gauge the effects of transformations separately,we first performed them in batches on the En-glish side.
These batches were (i) transforma-tions involving nouns and adjectives (Noun+Adj),(ii) transformations involving verbs (Verb), (iii)transformations involving adverbs (Adv), and(iv) transformations involving verbs and adverbs(Verb+Adv).We also performed one set of transformationson the Turkish side.
In general, English preposi-tions translate as case markers on Turkish nouns.However, there are quite a number of lexical post-positions in Turkish which also correspond to En-glish prepositions.
To normalize these with thehandling of case-markers, we treated these postpo-sitions as if they were case-markers and attachedthem to the immediately preceding noun, and thenaligned the resulting training data (PostP).15The results of these experiments are presentedin Table 1.
We can observe that the com-bined syntax-to-morphology transformations onthe source side provide a substantial improvementby themselves and a simple target side transfor-mation on top of those provides a further boostto 21.96 BLEU which represents a 28.57% rel-ative improvement over the word-based baselineand a 18.00% relative improvement over the fac-tored baseline.Experiment Ave. STD Max.
Min.Baseline 17.08 0.60 17.99 15.97Factored Baseline 18.61 0.76 19.41 16.80Noun+Adj 21.33 0.62 22.27 20.05Verb 19.41 0.62 20.19 17.99Adv 18.62 0.58 19.24 17.30Verb+Adv 19.42 0.59 20.17 18.13Noun+Adj 21.67 0.72 22.66 20.38+Verb+AdvNoun+Adj+Verb 21.96 0.72 22.91 20.67+Adv+PostPTable 1: BLEU scores for a variety of transforma-tion combinationsWe can see that every transformation improves15Note than in this case, the translations would be gener-ated in the same format, but we then split such postpositionsfrom the words they are attached to, during decoding, andthen evaluate the BLEU score.the baseline system and the highest performance isattained when all transformations are performed.However when we take a closer look at the indi-vidual transformations performed on English side,we observe that not all of them have the same ef-fect.
While Noun+Adj transformations give us anincrease of 2.73 BLEU points, Verbs improve theresult by only 0.8 points and improvement withAdverbs is even lower.
To understand why weget such a difference, we investigated the corre-lation of the decrease in the number of tokens onboth sides of the parallel data, with the change inBLEU scores.
The graph in Figure 3 plots theBLEU scores and the number of tokens in the twosides of the training data as the data is modifiedwith transformations.
We can see that as the num-ber of tokens in English decrease, the BLEU scoreincreases.
In order to measure the relationshipbetween these two variables statistically, we per-formed a correlation analysis and found that thereis a strong negative correlation of -0.99 betweenthe BLEU score and the number of English tokens.We can also note that the largest reduction in thenumber of tokens comes with the application ofthe Noun+Adj transformations, which correlateswith the largest increase in BLEU score.It is also interesting to look at the n-gram pre-cision components of the BLEU scores (again av-eraged).
In Table 2, we list these for words (ac-tual BLEU), roots (BLEU-R) to see how effectivewe are in getting the root words right, and mor-phological tags, (BLEU-M), to see how effectivewe are in getting just the morphosyntax right.
It1-gr.
2-gr.
3-gr.
4-gr.BLEU 21.96 55.73 27.86 16.61 10.68BLEU-R 27.63 68.60 35.49 21.08 13.47BLEU-M 27.93 67.41 37.27 21.40 13.41Table 2: Details of Word, Root and MorphologyBLEU Scoresseems we are getting almost 69% of the root wordsand 68% of the morphological tags correct, butnot necessarily getting the combination equally asgood, since only about 56% of the full word formsare correct.
One possible way to address is to uselonger distance constraints on the morphologicaltag factors, to see if we can select them better.3.2.3 Experiments with higher-orderlanguage modelsFactored phrase-based SMT allows the use of mul-tiple language models for the target side, for dif-ferent factors during decoding.
Since the numberof possible distinct morphological tags (the mor-phological tag vocabulary size) in our training data459Figure 3: BLEU scores vs number of tokens in the training sets(about 3700) is small compared to distinct num-ber of surface forms (about 52K) and distinct roots(about 15K including numbers), it makes sense toinvestigate the contribution of higher order n-gramlanguage models for the morphological tag factoron the target side, to see if we can address the ob-servation in the previous section.Using the data transformed with Noun+Adj-+Verb+Adv+PostP transformations which previ-ously gave us the best results overall, we experi-mented with using higher order models (4-gramsto 9-grams) during decoding, for the morphologi-cal tag factor models, keeping the surface and rootmodels at 3-gram.
We observed that for all the 10data sets, the improvements were consistent for upto 8-gram.
The BLEU with the 8-gram for onlythe morphological tag factor averaged over the 10data sets was 22.61 (max: 23.66, min: 21.37, std:0.72) compared to the 21.96 in Table 1.
Using a 4-gram root LM, considerably less sparse than wordforms but more sparse that tags, we get a BLEUscore of 22.80 (max: 24.07, min: 21.57, std: 0.85).The details of the various BLEU scores are shownin the two halves of Table 3.
It seems that largern-gram LMs contribute to the larger n-gram preci-sions contributing to the BLEU but not to the uni-gram precision.3-gram root LM 1-gr.
2-gr.
3-gr.
4-gr.BLEU 22.61 55.85 28.21 17.16 11.36BLEU-R 28.21 68.67 35.80 21.55 14.07BLEU-M 28.68 67.50 37.59 22.02 14.224-gram root LM 1-gr.
2-gr.
3-gr.
4-gr.BLEU 22.80 55.85 28.39 17.34 11.54BLEU-R 28.48 68.68 35.97 21.79 14.35BLEU-M 28.82 67.49 37.63 22.17 14.40Table 3: Details of Word, Root and MorphologyBLEU Scores, with 8-gram tag LM and 3/4-gramroot LMs3.2.4 Augmenting the Training DataIn order to alleviate the lack of large scale parallelcorpora for the English?Turkish language pair, weexperimented with augmenting the training datawith reliable phrase pairs obtained from a previousalignment.
Phrase table entries for the surface fac-tors produced by Moses after it does an alignmenton the roots, contain the English (e) and Turkish (t)parts of a pair of aligned phrases, and the proba-bilities, p(e|t), the conditional probability that theEnglish phrase is e given that the Turkish phraseis t, and p(t|e), the conditional probability thatthe Turkish phrase is t given the English phrase ise.
Among these phrase table entries, those withp(e|t) ?
p(t|e) and p(t|e) + p(e|t) larger thansome threshold, can be considered as reliable mu-tual translations, in that they mostly translate toeach other and not much to others.
We extracted460from the phrase table those phrases with 0.9 ?p(e|t)/p(t|e) ?
1.1 and p(t|e) + p(e|t) ?
1.5and added them to the training data to further biasthe alignment process.
The resulting BLEU scorewas 23.78 averaged over 10 data sets (max: 24.52,min: 22.25, std: 0.71).164 Experiments with ConstituentReorderingThe transformations in the previous section donot perform any constituent level reordering, butrather eliminate certain English function words astokens in the text and fold them into complex syn-tactic tags.
That is, no transformations reorderthe English SVO order to Turkish SOV,17 for in-stance, or move postnominal prepositional phrasemodifiers in English, to prenominal phrasal mod-ifiers in Turkish.
Now that we have the parsesof the English side, we have also investigated amore comprehensive set of reordering transforma-tions which perform the following constituent re-orderings to bring English constituent order morein line with the Turkish constitent order at the topand embedded phrase levels:?
Object reordering (ObjR), in which the ob-jects and their dependents are moved in frontof the verb.?
Adverbial phrase reordering (AdvR), whichinvolve moving post-verbal adverbial phrasesin front of the verb.?
Passive sentence agent reordering (PassAgR),in which any post-verbal agents marked byby, are moved in front of the verb.?
Subordinate clause reordering (SubCR)which involve moving postnominal relativeclauses or prepositional phrase modifers infront of any modifiers of the head noun.Similarly any prepositional phrases attachedto verbs are moved to in front of the verb.We performed these reorderingson top of the data obtained with theNoun+Adj+Verb+Adv+PostP transformationsearlier in Section 3.2.2 and used the same decoderparameters.
Table 4 shows the performanceobtained after various combination of reorderingoperations over the 10 data sets.
Although therewere some improvements for certain cases, none16These experiments were done on top of the model in3.2.3 with a 3-gram word and root LMs and 8-gram tag LM.17Although Turkish is a free-constituent order language,SOV is the dominant order in text.of reordering gave consistent improvements forall the data sets.
A cursory examinations ofthe alignments produced after these reorderingtransformations indicated that the resulting rootalignments were not necessarily that close tobeing monotonic as we would have expected.Experiment Ave. STD Max.
Min.Baseline 21.96 0.72 22.91 20.67ObjR 21.94 0.71 23.12 20.56ObjR+AdvR 21.73 0.50 22.44 20.69ObjR+PassAgR 21.88 0.73 23.03 20.51ObjR+SubCR 21.88 0.61 22.77 20.92Table 4: BLEU scores of after reordering transfor-mations5 Related WorkStatistical Machine Translation into a morpholog-ically rich language is a challenging problem inthat, on the target side, the decoder needs to gen-erate both the right sequence of constituents andthe right sequence of morphemes for each word.Furthermore, since for such languages one cangenerate tens of hundreds of inflected variants,standard word-based alignment approaches suf-fer from sparseness issues.
Koehn (2005) appliedstandard phrase-based SMT to Finnish using theEuroparl corpus and reported that translation toFinnish had the worst BLEU scores.Using morphology in statistical machine trans-lation has been addressed by many researchers fortranslation from or into morphologically rich(er)languages.
Niessen and Ney (2004) used mor-phological decomposition to get better alignments.Yang and Kirchhoff (2006) have used phrase-based backoff models to translate unknown wordsby morphologically decomposing the unknownsource words.
Lee (2004) and Zolmann et al(2006) have exploited morphology in Arabic-English SMT.
Popovic and Ney (2004) investi-gated improving translation quality from inflectedlanguages by using stems, suffixes and part-of-speech tags.
Goldwater and McClosky (2005)use morphological analysis on the Czech side toget improvements in Czech-to-English statisticalmachine translation.
Minkov et al (2007) haveused morphological postprocessing on the targetside, to improve translation quality.
Avramidis andKoehn (2008) have annotated English with addi-tional morphological information extracted from asyntactic tree, and have used this in translation toGreek and Czech.
Recently, Bisazza and Federico(2009) have applied morphological segmentationin Turkish-to-English statistical machine transla-tion and found that it provides nontrivial BLEU461score improvements.In the context of translation from English toTurkish, Durgar-El Kahlout and Oflazer (2010)have explored different representational units ofthe lexical morphemes and found that selectivelysplitting morphemes on the target side providednontrivial improvement in the BLEU score.
Theirapproach was based on splitting the target Turk-ish side, into constituent morphemes while our ap-proach in this paper is the polar opposite: we donot segment morphemes on the Turkish side butrather join function words on the English side tothe related content words.
Our approach is some-what similar to recent approaches that use com-plex syntactically-motivated complex tags.
Birchet al (2007) have integrated more syntax in afactored translation approach by using CCG su-pertags as a separate factor and have reporteda 0.46 BLEU point improvement in Dutch-to-English translations.
Although they used su-pertags, these were obtained not via syntactic anal-ysis but by supertagging, while we determine, onthe fly, the appropriate syntactic tags based on syn-tactic structure.
A similar approach based on su-pertagging was proposed by Hassan et al (2007).They used both CCG supertags and LTAG su-pertags in Arabic-to-English phrase-based transla-tion and have reported about 6% relative improve-ment in BLEU scores.
In the context of reorder-ing, one recent work (Xu et al, 2009), was ableto get an improvement of 0.6 BLEU points by us-ing source syntactic analysis and a constituent re-ordering scheme like ours for English-to-Turkishtranslation, but without using any morphology.6 ConclusionsWe have presented a novel way to incorporatesource syntactic structure in English-to-Turkishphrase-based machine translation by parsing thesource sentences and then encoding many localand nonlocal source syntactic structures as addi-tional complex tag factors.
Our goal was to ob-tain representations of source syntactic structuresthat parallel target morphological structures, andenable us to extend factored translation, in appli-cability, to languages with very disparate morpho-logical structures.In our experiments over a limited amount train-ing data, but repeated with 10 different trainingand test sets, we found that syntax-to-morphologymapping transformations on the source side sen-tences, along with a very small set of transforma-tions on the target side, coupled with some ad-ditional techniques provided about 39% relativeimprovement in BLEU scores over a word-basedbaseline and about 28% improvement of a factoredbaseline.
We also experimented with numerousadditional syntactic reordering transformation onthe source to further bring the constituent order inline with the target order but found that these didnot provide any tangible improvements when av-eraged over the 10 different data sets.It is possible that the techniques presented inthis paper may be less effective if the availabledata is much larger, but we have reasons to be-lieve that they will still be effective then also.
Thereduction in size of the source language side ofthe training corpus seems to be definitely effectiveand there no reason why such a reduction (if notmore) will not be observed in larger data.
Also,the preprocessing of English prepositional phrasesand many adverbial phrases usually involve ratherlong distance relations in the source side syntacticstructure18 and when such structures are coded ascomplex tags on the nominal or verbal heads, suchlong distance syntax is effectively ?localized?
andthus can be better captured with the limited win-dow size used for phrase extraction.One limitation of the approach presented hereis that it is not directly applicable in the reversedirection.
The data encoding and set-up can di-rectly be employed to generate English ?transla-tion?
expressed as a sequence of root and complextag combinations, but then some of the complextags could encode various syntactic constructs.
Tofinalize the translation after the decoding step, thefunction words/tags in the complex tag would thenhave to be unattached and their proper positionsin the sentence would have to be located.
Theproblem is essentially one of generating multiplecandidate sentences with the unattached functionwords ambiguously positioned (say in a lattice)and then use a second language model to rerankthese sentences to select the target sentence.
Thisis an avenue of research that we intend to look atin the very near future.AcknowledgementsWe thank Joakim Nivre for providing us with theparser.
This publication was made possible by thegenerous support of the Qatar Foundation throughCarnegie Mellon University?s Seed Research pro-gram.
The statements made herein are solely theresponsibility of the authors.18For instance, consider the example in Figure 2 involvingif with some additional modifiers added to the interveningnoun phrase.462ReferencesEleftherios Avramidis and Philipp Koehn.
2008.
En-riching morphologically poor languages for statis-tical machine translation.
In Proceedings of ACL-08/HLT, pages 763?770, Columbus, Ohio, June.Alexandra Birch, Miles Osborne, and Philipp Koehn.2007.
CCG supertags in factored translation models.In Proceedings of SMT Workshop at the 45th ACL.Arianna Bisazza and Marcello Federico.
2009.
Mor-phological pre-processing for Turkish to English sta-tistical machine translation.
In Proceedings of theInternational Workshop on Spoken Language Trans-lation, Tokyo, Japan, December.I?lknur Durgar-El-Kahlout and Kemal Oflazer.
2010.Exploiting morphology and local word reordering inEnglish to Turkish phrase-based statistical machinetranslation.
IEEE Transactions on Audio, Speech,and Language Processing.
To Appear.Alexander Fraser.
2009.
Experiments in morphosyn-tactic processing for translating to and from German.In Proceedings of the Fourth Workshop on Statis-tical Machine Translation, pages 115?119, Athens,Greece, March.
Association for Computational Lin-guistics.Sharon Goldwater and David McClosky.
2005.
Im-proving statistical MT through morphological anal-ysis.
In Proceedings of HLT/EMNLP-2005, pages676?683, Vancouver, British Columbia, Canada,October.Hany Hassan, Khalil Sima?an, and Andy Way.
2007.Supertagged phrase-based statistical machine trans-lation.
In Proceedings of the 45th ACL, pages 288?295, Prague, Czech Republic, June.
Association forComputational Linguistics.Philipp Koehn and Hieu Hoang.
2007.
Factored trans-lation models.
In Proceedings of EMNLP.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT/NAACL-2003.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th ACL?demonstrationsession, pages 177?180.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit X.Young-Suk Lee.
2004.
Morphological analysis forstatistical machine translation.
In Proceedings ofHLT/NAACL-2004 ?
Companion Volume, pages 57?60.Einat Minkov, Kristina Toutanova, and Hisami Suzuki.2007.
Generating complex morphology for machinetranslation.
In Proceedings of the 45th ACL, pages128?135, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Sonja Niessen and Hermann Ney.
2004.
Statisti-cal machine translation with scarce resources usingmorpho-syntatic information.
Computational Lin-guistics, 30(2):181?204.Joakim Nivre, Hall Johan, Nilsson Jens, ChanevAtanas, Gu?ls?en Eryig?it, Sandra Ku?bler, MarinovStetoslav, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language EngineeringJournal, 13(2):99?135.Kemal Oflazer and I?lknur Durgar-El-Kahlout.
2007.Exploring different representational units inEnglish-to-Turkish statistical machine translation.In Proceedings of Statistical Machine TranslationWorkshop at the 45th Annual Meeting of theAssociation for Computational Linguistics, pages25?32.Kemal Oflazer.
1994.
Two-level description of Turk-ish morphology.
Literary and Linguistic Comput-ing, 9(2):137?148.Kemal Oflazer.
2008.
Statistical machine translationinto a morphologically complex language.
In Pro-ceedings of the Conference on Intelligent Text Pro-cessing and Computational Linguistics (CICLing),pages 376?387.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th ACL, pages 311?318.Maja Popovic and Hermann Ney.
2004.
Towards theuse of word stems and suffixes for statistical ma-chine translation.
In Proceedings of the 4th LREC,pages 1585?1588, May.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings ofInternational Conference on New Methods in Lan-guage Processing.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of HLT/NAACL-2003, pages 252?259.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improveSMT for subject-object-verb languages.
In Proceed-ings HLT/NAACL-2009, pages 245?253, June.Mei Yang and Katrin Kirchhoff.
2006.
Phrase-basedbackoff models for machine translation of highly in-flected languages.
In Proceedings of EACL-2006,pages 41?48.463Deniz Yuret and Ferhan Tu?re.
2006.
Learning mor-phological disambiguation rules for Turkish.
InProceedings of HLT/NAACL-2006, pages 328?334,New York City, USA, June.Andreas Zollmann, Ashish Venugopal, and StephanVogel.
2006.
Bridging the inflection morphol-ogy gap for Arabic statistical machine translation.In Proceedings of HLT/NAACL-2006 ?
CompanionVolume, pages 201?204, New York City, USA, June.464
