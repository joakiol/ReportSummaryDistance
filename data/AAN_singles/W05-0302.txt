Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 5?12,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsMerging PropBank, NomBank, TimeBank, Penn Discourse Treebank and CoreferenceJames Pustejovsky, Adam Meyers, Martha Palmer, Massimo PoesioAbstractMany recent annotation efforts for Englishhave focused on pieces of the larger problemof semantic annotation, rather than initiallyproducing a single unified representation.This paper discusses the issues involved inmerging four of these efforts into a unifiedlinguistic structure: PropBank, NomBank, theDiscourse Treebank and CoreferenceAnnotation undertaken at the University ofEssex.
We discuss resolving overlapping andconflicting annotation as well as how thevarious annotation schemes can reinforceeach other to produce a representation that isgreater than the sum of its parts.1.
IntroductionThe creation of the Penn Treebank (Marcus et al1993) and the word sense-annotated SEMCOR(Fellbaum, 1997) have shown how even limitedamounts of annotated data can result in majorimprovements in complex natural languageunderstanding systems.
These annotated corporahave led to high-level improvements for parsingand word sense disambiguation (WSD), on thesame scale as previously occurred for Part ofSpeech tagging by the annotation of the Browncorpus and, more recently, the British NationalCorpus (BNC) (Burnard, 2000).
However, thecreation of semantically annotated corpora haslagged dramatically behind the creation of otherlinguistic resources: in part due to the perceivedcost, in part due to an assumed lack of theoreticalagreement on basic semantic judgments, in part,finally, due to the understandable unwillingnessof  research groups to get involved in such anundertaking.
As a result, the need for suchresources has become urgent.Many recent annotation efforts for English havefocused on pieces of the larger problem ofsemantic annotation, rather than producing asingle unified representation like Head-drivenPhrase Structure Grammar (Pollard and Sag1994) or the Prague Dependency Tecto-gramatical Representation (Hajicova & Kucer-ova, 2002).
PropBank (Palmer et al 2005)annotates predicate argument structure anchoredby verbs.
NomBank (Meyers, et.
al., 2004a)annotates predicate argument structure anchoredby nouns.
TimeBank (Pustejovsky et al 2003)annotates the temporal features of propositionsand the temporal relations between propositions.The Penn Discourse Treebank (Miltsakaki et al2004a/b) treats discourse connectives aspredicates and the sentences being joined asarguments.
Researchers at Essex wereresponsible for the coreference markup schemedeveloped in MATE (Poesio et al 1999; Poesio,2004a) and have annotated corpora using thisscheme including a subset of the Penn Treebank(Poesio and Vieira, 1998), and the GNOMEcorpus (Poesio, 2004a).
This paper discusses theissues involved in creating a Unified LinguisticAnnotation (ULA) by merging annotation ofexamples using the schemata from these efforts.Crucially, all individual annotations can be keptseparate in order to make it easy to producealternative annotations of a specific type ofsemantic information without need to modify theannotation at the other levels.
Embarking onseparate annotation efforts has the advantage ofallowing researchers to focus on the difficultissues in each area of semantic annotation andthe disadvantage of inducing a certain amount oftunnel vision or task-centricity ?
annotatorsworking on a narrow task tend to see allphenomena in light of the task they are workingon, ignoring other factors.
However, mergingthese annotation efforts allows these biases to bedealt with.
The result, we believe, could be amore detailed semantic account than possible ifthe ULA had been the initial annotation effortrather than the result of merging.There is a growing community consensus thatgeneral annotation, relying on linguistic cues,and in particular lexical cues, will produce anenduring resource that is useful, replicable andportable.
We provide the beginnings of one suchlevel derived from several distinct annotationefforts.
This level could provide the foundationfor a major advance in our ability toautomatically extract salient relationships fromtext.
This will in turn facilitate breakthroughs inmessage understanding, machine translation, factretrieval, and information retrieval.2.
The Component Annotation SchemataWe describe below existing independentannotation efforts, each one of which is focusedon a specific aspect of the semanticrepresentation task: semantic role labeling,5coreference, discourse relations, temporalrelations, etc.
They have reached a level ofmaturity that warrants a concerted attempt tomerge them into a single, unified representation,ULA.
There are several technical and theoreticalissues that will need to be resolved in order tobring these different layers together seamlessly.Most of these approaches have annotated thesame type of data, Wall Street Journal text, so itis also important to demonstrate that theannotation can be extended to other genres suchas spoken language.
The demonstration ofsuccess for the extensions would be the trainingof accurate statistical semantic taggers.PropBank: The Penn Proposition Bank focuseson the argument structure of verbs, and providesa corpus annotated with semantic roles,including participants traditionally viewed asarguments and adjuncts.
An important goal is toprovide consistent semantic role labels acrossdifferent syntactic realizations of the same verb,as in the window in [ARG0 John] broke [ARG1the window] and [ARG1 The window] broke.Arg0 and Arg1 are used rather than the moretraditional Agent and Patient to keep theannotation as theory-neutral as possible, and tofacilitate mapping to richer representations.
The1M word Penn Treebank II Wall Street Journalcorpus has been successfully annotated withsemantic argument structures for verbs and isnow available via the Penn Linguistic DataConsortium as PropBank I (Palmer, et.
al., 2005).Coarse-grained sense tags, based on groupings ofWordNet senses, are being added, as well aslinks from the argument labels in the FramesFiles to FrameNet frame elements.
There areclose parallels to other semantic role labelingprojects, such as FrameNet (Baker, et.
al., 1998;Fillmore & Atkins, 1998; Fillmore & Baker,2001), Salsa (Ellsworth, et.al, 2004), PragueTectogrammatics (Hajicova & Kucerova, 2002)and IAMTC, (Helmreich, et.
al., 2004)NomBank: The NYU NomBank project can beconsidered part of the larger PropBank effort andis designed to provide argument structure forinstances of about 5000 common nouns in thePenn Treebank II corpus (Meyers, et.
al., 2004a).PropBank argument types and related verbFrames Files are used to provide a commonalityof annotation.
This enables the development ofsystems that can recognize regularizations oflexically and syntactically related sentencestructures, whether they occur as verb phrases ornoun phrases.
For example, given an IE systemtuned to a hiring scenario (MUC-6, 1995),NomBank and PropBank annotation facilitategeneralization over patterns.
PropBank andNomBank would both support a single IE patternstating that the object (ARG1) of appoint is Johnand the subject (ARG0) is IBM, allowing asystem to detect that IBM hired John from eachof the following strings: IBM appointed John,John was appointed by IBM, IBM's appointmentof John, the appointment of John by IBM andJohn is the current IBM appointee.Coreference: Coreference involves the detectionof subsequent mentions of invoked entities, as inGeorge Bush,?
he?.
Researchers at Essex (UK)were responsible for the coreference markupscheme developed in MATE (Poesio et al 1999;Poesio, 2004a), partially implemented in theannotation tool MMAX and now proposed as anISO standard; and have been responsible for thecreation of two small, but commonly usedanaphorically annotated corpora ?
the Vieira /Poesio subset of the Penn Treebank (Poesio andVieira, 1998), and the GNOME corpus (Poesio,2004a).
Parallel coreference annotation effortsfunded by ACE have resulted in similarguidelines, exemplified by BBN?s recentannotation of Named Entities, common nounsand pronouns.
These two approaches provide asuitable springboard for an attempt at achieving acommunity consensus on coreference.Discourse Treebank:  The Penn DiscourseTreebank (PDTB) (Miltsakaki et al2004a/b) isbased on the idea that discourse connectives arepredicates with associated argument structure(for details see (Miltsakaki et al2004a,Miltsakaki et al2004b).
The long-range goal isto develop a large scale and reliably annotatedcorpus that will encode coherence relationsassociated with discourse connectives, includingtheir argument structure and anaphoric links,thus exposing a clearly defined level of discoursestructure and supporting the extraction of a rangeof inferences associated with discourseconnectives.
This annotation references the PennTreebank annotations as well as PropBank, andcurrently only considers Wall Street Journal text.TimeBank: The Brandeis TimeBank corpus,funded by ARDA, focuses on the annotation ofall major aspects in natural language textassociated with temporal and event information(Day, et al 2003, Pustejovsky, et al 2004).Specifically, this involves three areas of theannotation: temporal expressions, event-denoting6expressions, and the links that express either ananchoring of an event to a time or an ordering ofone event relative to another.
Identifying eventsand their temporal anchorings is a critical aspectof reasoning, and without a robust ability toidentify and extract events and their temporalanchoring from a text, the real aboutness of thearticle can be missed.
The core of TimeBank is aset of 200 news reports documents, consisting ofWSJ, DUC, and ACE articles, each annotated toTimeML 1.2 specification.
It is currently beingextended to AQUAINT articles.
The corpus isavailable from the timeml.org website.3.
Unifying Linguistic AnnotationsSince September, 2004, researchers representingseveral different sites and annotation projectshave begun collaborating to produce a detailedsemantic annotation of two difficult sentences.These researchers aim to produce a single unifiedrepresentation with some consensus from theNLP community.
This effort has given rise toboth a listserv email list and this workshop:http://nlp.cs.nyu.edu/meyers/pie-in-the-sky.html,http://nlp.cs.nyu.edu/meyers/frontiers/2005.htmlThe merging operations discussed here wouldseem crucial to the furthering of this effort.3.1 The Initial Pie in the Sky ExampleThe following two consecutive sentences havebeen annotated for Pie in the Sky.Two Sentences From ACE Corpus FileNBC20001019.1830.0181?
but Yemen's president says the FBI has toldhim the explosive material could only havecome from the U.S., Israel or two Arabcountries.?
and to a former federal bomb investigator,that description suggests a powerfulmilitary-style plastic explosive c-4 that canbe cut or molded into different shapes.Although the full Pie-in-the-Sky analysisincludes information from many differentannotation projects, the Dependency Structure inFigure 1 includes only those components thatrelate to PropBank, NomBank, Discourseannotation, coreference and TimeBank.
Severalparts of this representation require furtherexplanation.
Most of these are signified by thespecial arcs, arc labels, and nodes.
Dashed linesrepresent transparent arcs, such as the transparentdependency between the argument (ARG1) ofmodal can and the or.
Or is transparent in that itallows this dependency to pass through it to cutand mold.
There are two small arc loops --investigator is its own ARG0 and description isits own ARG1.
Investigator is a relational nounin NomBank.
There is assumed to be anunderlying relation between the Investigator(ARG0), the beneficiary or employer (the ARG2)and the item investigated (ARG1).
Similarly,description acts as its own ARG1 (the thingdescribed).
There are four special coreference arclabels: ARG0-CF, ARG-ANAPH, EVENT-ANAPH and ARG1-SBJ-CF.
At the target ofthese arcs are pointers referring to phrases fromthe previous sentence or previous discourse.
Thefirst three of these labels are on arcs with thenoun description as their source.
The ARG0-CFlabel indicates that the phrase Yemen's president(**1**) is the ARG0, the one who is doing thedescribing.
The EVENT-ANAPH label points toa previous mention of the describing event,namely the clause: The FBI told him theexplosive material?
(**3**).
However, as notedabove, the NP headed by description representsthe thing described in addition to the action.
TheARG-ANAPH label points to the thing that theFBI told him the explosive material can onlycome from ?
(**2**).
The ARG1-SBJ-CF labellinks the NP from the discourse what the bombwas made from as the subject with the NPheaded by explosive as its predicate, much thesame as it would in a copular construction suchas: What the bomb was made from is theexplosive C-4.
Similarly, the arc ARG1-APPmarks C-4 as an apposite, also predicated to theNP headed by explosive.
Finally, the thick arcslabeled SLINK-MOD represent TimeML SLINKrelations between eventuality variables, i.e.,  thecut and molded events are modally subordinateto the suggests proposition.
The mergedrepresentation aims to be compatible with theprojects from which it derives, each of whichanalyzes a different aspect of linguistic analysis.Indeed most of the dependency labels are basedon the annotation schemes of those projects.We have also provided the individual PropBank,NomBank and TimeBank annotations below intextual form, in order to highlight potentialpoints of interaction.PropBank:  and [Arg2 to a former federal bombinvestigator], [Arg0 that description][Rel_suggest.01 suggests]  [Arg1 [Arg1 a powerfulmilitary-style plastic explosive c-4] that7[ArgM-MOD can] be [Rel_cut.01 cut] or  [Rel_mold.01molded] [ArgM-RESULT into different shapes]].NomBank: and to a former [Arg2 federal] [Arg1bomb] [Rel investigator], that descriptionsuggests a powerful [Arg2 military] - [Rel style]plastic [Arg1 explosive] c-4 that can be cutor molded into different shapes.TimeML: and to a former federal bombinvestigator, that description [Event = ei1suggests]  a powerful military-style plasticexplosive c-4 that  can be [Event = ei2 modal=?can?
cut]or  [Event = ei3 modal=?can?
molded]  into differentshapes.
<SLINK eventInstanceID = ei1subordinatedEventID = ei2 relType = ?Modal?/><SLINK eventInstanceID = ei1subordinatedEventID = ei3 relType = ?Modal?/>Figure 1.
Dependency Analysis of Sentence 2Note that the subordinating Events indicated bythe TimeML SLINKS refer to the predicateargument structures labeled by PropBank, andthat the ArgM-MODal also labeled by PropBankcontains modality information also crucial to theSLINKS.
While the grammatical modal on cutand mold is captured as an attribute value on theevent tag, the governing event predicate suggestintroduces a modal subordination to its internalargument, along with its relative clause.
Whilethis markup is possible in TimeML, it is difficultto standardize (or automate, algorithmically)since arguments are not marked up unless theyare event denoting.3.2 A More Complex ExampleTo better illustrate the interaction betweenannotation levels, and the importance of merginginformation resident in one level but notnecessarily in another, consider the sentencebelow which has more complex temporalproperties than the Pie-in-the-Sky sentences andits dependency analysis (Figure 2).According to reports, sea trials for a patrol boatdeveloped by Kazakhstan are being conductedand the formal launch is planned for thebeginning of April this year.Figure 2.
Dependency Analysis of a Sentencewith Interesting Temporal PropertiesThe graph above incorporates these distinctannotations into a merged representation, muchlike the previous analysis.
This sentence hasmore TimeML annotation than the previoussentence.
Note the loops of arcs which show thatAccording to plays two roles in the sentence: (1)it heads a constituent that is the ARGM-ADV ofthe verbs conducted and planned; (2) it indicatesthat the information in this entire sentence isattributed to the reports.
This loop is problematicin some sense because the adverbial appears tomodify a constituent that includes itself.
Inactuality, however, one would expect that theARGM-ADV role modifies the sentence minusthe adverbial, the constituent that you would getif you ignore the transparent arc from ARGM-8ADV to the rest of the sentence.
Alternatively, amerging decision may elect to delete the ARGM-ADV arcs, once the more specific predicateargument structure of the sentence adverbialannotation is available.The PropBank annotation for this sentencewould label arguments for develop, conduct andplan, as given below.
[ArgM-ADV According to reports], [Arg1sea trials for[Arg1 a patrol boat] [Rel_develop.02 developed] [Arg0by Kazakhstan]] are being[Rel_conduct.01 conducted]  and [Arg1 the formallaunch] is [Rel_plan.01 planned][ArgM-TMP for the beginning of April this year].NomBank would add arguments for report, trial,launch and beginning as follows:According to [Rel_report.01 reports], [Arg1 [ArgM-LOCsea [Rel_trial.01 trials] [Arg1 for [Arg1-CF_launch.01 apatrol boat] developed by Kazakhstan] are beingconducted and the [ArgM-MNR formal] [Rel_launch.01launch] is planned for the [[REL_beginning.01beginning] [ARG1 of April this year]].TimeML, however, focuses on the anchoring ofevents to explicit temporal expressions (ordocument creation dates) through TLINKs, aswell as subordinating relations, such as thoseintroduced by modals, intensional predicates,and other event-selecting predicates, throughSLINKs.
For discussion, only part of thecomplete annotation is shown below.According to [Event = ei1  reports], sea [Event = ei3trials] for a boat [Event = ei4  developed]  byKazakhstan are being [Event = ei5  conducted] andthe formal [Event = ei6  launch]is  [Event = ei7  planned] for the [Timex3= t1  beginningof April] [Timex3= t2 this year].<SLINK eventID=?ei1?
subordinatedEvent=?ei5,ei7?
relType=EVIDENTIAL/><TLINK eventID=?ei4?
relatedToEvent =?ei3?relType=BEFORE/><TLINK eventID=?ei6?
relatedToTime=?t1?relType=IS_INCLUDED /><SLINK eventID=?ei7?subordinatedEvent=?ei6?
relType=?MODAL?/><TLINK eventID=?ei5?
relatedToEvent=?ei3?relType=IDENTITY/>Predicates such as plan and nominals such asreport are lexically encoded to introduceSLINKs with a specific semantic relation, in thiscase, a ?MODAL?
relType,.
This effectivelyintroduces an intensional context over thesubordinated events.These examples illustrate the type of semanticrepresentation we are trying to achieve.
It isclear that our various layers already capturemany of the intended relationships, but they donot do so in a unified, coherent fashion.
Ourgoal is to develop both a framework and aprocess for annotation that allows the individualpieces to be automatically assembled into acoherent whole.4.0 Merging Annotations4.1 First Order Merging of AnnotationWe begin by discussing issues that arise indefining a single format for a mergedrepresentation of PropBank, NomBank andCoreference, the core predicate argumentstructures and  referents for the arguments.
Onepossible representation format would be toconvert each annotation into features and valuesto be added to a larger feature structure.
1 Theresulting feature structure would combine standalone and offset annotation ?
it would includeactual words and features from the text as well asspecial features that point to the actual text(character offsets) and, perhaps, syntactic trees(offsets along the lines of PropBank/NomBank).Alternative global annotation schemes includeannotation graphs (Cieri & Bird, 2001), andMATE (Carletta, et.
al., 1999).
There are manyareas in which the boundaries between theseannotations have not been clearly defined, suchas the treatment of support constructions andlight verbs, as discussed below.
Determining themost suitable format for the mergedrepresentation should be a top priority.4.2 Resolving Annotation OverlapThere are many possible interactions betweendifferent types of annotation: aspectual verbshave argument labels in PropBank, but are alsoimportant roles for temporal relations.
Support1 The Feature Structure has many advantages as a targetrepresentation including: (1) it is easy to add lots of detailedfeatures; and (2) the mathematical properties of FeatureStructures are well understood, i.e., there are well-definedrule-writing languages, subsumption and unificationrelations, etc.
defined for Feature Structures (Carpenter,1992) The downside is that a very informative FeatureStructure is difficult for a human to read.9constructions also have argument labels, and thequestion arises as to whether these should beassociated with the support verb or thepredicative nominal.
Given the sentence Theygave the chefs a standing ovation, a PropBankcomponent will assign role labels to argumentsof give; a NomBank component will assignargument structure to ovation that labels thesame participants.
If the representations areequivalent, the question arises as to which ofthem (or both) should be included in the mergedrepresentation.
The following graph  (Figure 3)is a combined PropBank and NomBank analysisof this sentence.
"They" is the ARG0 of both"give" and "ovation"; "the chefs" is the ARG2 of"give", but the "ARG1" of ovation; "ovation" isthe ARG1 of "give" and "give" is a support verbfor "ovation".
For this case, a reasonable choicemight be to preserve the argument structure fromboth NomBank and PropBank, and to do thesame for other predicative nominals that havegive (or receive, obtain, request?)
as a supportverb, e.g., (give a kiss/hug/squeeze, give alecture/speech, give a promotion, etc.).
Forother support constructions, such as take a walk,have a headache and make a mistake, the noun isreally the main predicate and it is questionablewhether the verbal argument structure carriesgavechefstheTheya ovationstandingNPNPSARG0RELARG2ARG1NPARG1 RELARG0SUPPORTFigure 3.
Merged PropBank/NomBank representationof They gave the chefs a standing ovation.much information, e.g., there are no selectionrestrictions between light verbs and their subject(ARG0) -- these are inherited from the noun.Thus make a mistake selects a different type ofsubject than make a gain, e.g., people andorganizations make mistakes, but stock pricesmake gains.
For these constructions, the mergedrepresentation might not need to include the(ARG0) relation between the subject of thesentence and make, and future propbankingefforts might do well to ignore the sharedarguments of such instances and leave them forNomBank.
However, the merged representationwould inherit PropBank?s annotation of someother light verb features including: negation, e.g.,They did not take a walk; modality, e.g., Theymight take a walk; and sentence adverbials, e.g.,They probably will take a walk.4.3 Resolving Annotation ConflictsInteractions between linguistic phenomena canaid in quality control, and conflicts found duringthe deliberate merging of different annotationsprovides an opportunity to correct and fine-tunethe original layers.
For example, predicateargument structure (PropBank and NomBank)annotation sometimes assumes differentconstituent structure than the Penn Treebank.
Wehave noticed some tendencies that help resolvethese conflicts, e.g., prenominal nounconstituents as in Indianapolis 500, which formsa single argument in NomBank, is correctlypredicted to be a constituent, even though thePenn Treebank II assumes a flatter structure.Similarly, idioms and multiword expressionsoften cause problems for both PropBank andNomBank.
PropBank annotators tend to viewargument structure in terms of verbs andNomBank annotators tend to view argumentstructure in terms of nouns.
Thus many examplesthat, perhaps, should be viewed as idioms areviewed as special senses of either verbs or nouns.Having idioms detected and marked beforepropbanking and nombanking could greatlyimprove efficiency.Annotation accuracy is often evaluated in termsof inter-annotation consistency.
Task definitionsmay need to err on the side of being moreinclusive in order to simplify the annotators task.For example, the NomBank project assumes thefollowing definition of a support verb (Meyers,et.al., 2004b):  ??
a verb which takes at leasttwo arguments NP1 and XP2 such that XP2 is anargument of the head of NP1.
For example, inJohn took a walk, a support verb (took) sharesone of its arguments (John) with the head of itsother argument (walk).?
The easiest way toapply this definition is without exception, so itwill include idiomatic expressions such as keeptabs on, take place, pull strings.
Indeed, thedividing line between support constructions andidioms is difficult to draw (Meyers 2004b).PropBank annotators are also quite comfortablewith associating general meanings to the mainverbs of idiomatic expressions and labeling their10argument roles, as in cases like bring home thebacon and mince words with.
Since idioms oftenhave interpretations that are metaphoricalextensions of their literal meaning, this is notnecessarily incorrect.
It may be helpful to havethe literal dependencies and the idiomaticreading both represented.
The fact that bothtypes of meaning are available is evidenced byjokes, irony, and puns.With respect to idioms and light verbs, TimeMLcan be viewed as a mediator between PropBankand NomBank.
In TimeML, light verbs and thenominalizations accompanying them are markedwith two separate EVENT tags.
This guaranteesan annotation independent of textual linearityand therefore ensures a parallel treatment fordifferent textual configurations.
In (a) the lightverb construction "make an allusion" isconstituted of a verb and an NP headed by anevent-denoting noun, whereas in (b) the nominalprecedes a VP, which in addition contains asecond N:(a) Max [made an allusion] to the crime.
(b) Several anti-war [demonstrations have takenplace] around the globe.Both verbal and nominal heads are taggedbecause they both contribute relevantinformation to characterizing the nature of theevent.
The nominal element plays a role in themore semantically based task of eventclassification.
On the other hand, the informationin the verbal component is important at twodifferent levels: it provides the grammaticalfeatures typically associated with verbalmorphology, such as tense and aspect, and at thesame time it may help in disambiguating caseslike take/give a class, make/take a phone call.The two tagged events are marked as identical bya TLINK introduced for that purpose.
TheTimeML annotation for the example in (a) isprovided below.Max [Event = ei1  made] an [Event = ei2  allusion] tothe crime.<TLINK eventID="ei1"relatedToEvent="ei2"relType=IDENTITY>Some cases of support in NomBank could alsobe annotated as "bridging" anaphora.
Considerthe sentence: The pieces make up the whole.It is unclear whether make up is a support verblinking whole as the ARG1 of pieces or if piecesis linked to whole by bridging anaphora.There are also clearer cases.
In Nastase, a rivalplayer defeated Jimmy Connors in the thirdround, the word rival and Jimmy Connors areclearly linked by bridging.
However, a waywardNomBank annotator might construct a supportchain (player + defeated) to link rival with itsARG1 Jimmy Connors.
In such a case, amerging of annotation could reveal annotationerrors.
In contrast, a NomBank annotator wouldbe correct in linking John as an argument of walkin John took a series of walks (the support chaintook + series consists of a support verb and atransparent noun), but this may not be obvious tothe non-NomBanker.
Thus the merging ofannotation may result in the more consistentspecifications for all.In our view, this process of annotating all layersof information and then merging them in asupervised manner, taking note of the conflicts,is a necessary prerequisite to defining moreclearly the boundaries between the differenttypes of annotation and determining how theyshould fit together.
Other areas of annotationinteraction include: (1) NomBank  andCoreference, e.g.
deriving that John teachesMary from John is Mary's teacher involves: (a)recognizing that teacher is an argumentnominalization such that the teacher is the ARG0of teach (the one who teaches); and (b) markingJohn and teacher as being linked by predication(in this case, an instance of type coreference);and (2) Time and Modality -  when a fact used tobe true, there are two time components: one inwhich the fact is true and one in which it is false.Clearly more areas of interaction will emerge asmore annotation becomes available and as themerging of annotation proceeds.5.
SummaryWe proposed a way of taking advantage of thecurrent practice of separating aspects of semanticanalysis of text into small manageable pieces.We propose merging these pieces, initially in acareful, supervised way, and hypothesize that theresult could be a more detailed semantic analysisthan was previously available.
This paperdiscusses some of the reasons that the mergingprocess should be supervised.
We primarily gaveexamples involving the interaction of PropBank,NomBank and TimeML.
However, as themerging process continues, we anticipate otherconflicts that will require resolution.ReferencesC.
F. Baker, F. Collin, C. J. Fillmore, and J. B.Lowe (1998), The Berkeley FrameNetproject.
In Proc.
of COLING/ACL-98,  86--9011O.
Babko-Malaya, M. Palmer, X. Nianwen, S.Kulick, A. Joshi (2004), Propbank II,Delving Deeper, In Proc.
of HLT-NAACLWorkshop: Frontiers in Corpus Annotation.R.
Carpenter (1992), The Logic of TypedFeature Structures.
Cambridge Univ.
Press.J.
Carletta and A. Isard (1999), The MATEAnnotation Workbench: User Requirements.In Proc.
of the ACL Workshop: TowardsStandards and Tools for Discourse Tagging.Univ.
of Maryland, 11-17C.
Cieri and S. Bird (2001), Annotation Graphsand Servers and Multi-Modal Resources:Infrastructure for  Interdisciplinary Education,Research and Development Proc.
of the ACLWorkshop on Sharing Tools and Resourcesfor Research  and Education, 23-30D.
Day,  L. Ferro, R. Gaizauskas, P. Hanks, M.Lazo, J. Pustejovsky, R.
Saur?, A.
See, A.Setzer, and B. Sundheim (2003), TheTIMEBANK Corpus.
Corpus Linguistics.M.
Ellsworth, K. Erk, P. Kingsbury and S. Pado(2004), PropBank, SALSA, and FrameNet:How Design Determines Product, in Proc.
ofLREC 2004 Workshop: Building LexicalResources from Semantically AnnotatedCorpora.C.
Fellbaum (1997), WordNet: An ElectronicLexical Database, MIT Press..C. J. Fillmore and B. T. S. Atkins (1998),FrameNet and lexicographic relevance.
In theProc.
of the First International Conferenceon Language Resources and Evaluation.C.
J. Fillmore and C. F. Baker (2001), Framesemantics for text understanding.
In Proc.
ofNAACL WordNet and Other LexicalResources Workshop.E.
Hajivcova and I. Kuvcerov'a (2002).Argument/Valency Structure in PropBank,LCS Database and Prague DependencyTreebank: A Comparative Pilot Study.
In theProc.
of the Third International Conferenceon Language Resources and Evaluation(LREC 2002),  846--851.S.
Helmreich, D. Farwell, B. Dorr, N. Habash, L.Levin, T. Mitamura, F. Reeder, K. Miller, E.Hovy, O. Rambow and A. Siddharthan,(2004),Interlingual Annotation of Multilingual TextCorpora, Proc.
of the HLT-EACL Workshopon Frontiers in Corpus Annotation.A, Meyers, R. Reeves, C. Macleod, R, Szekely,V.
Zielinska, B.
Young, and R. Grishman(2004a), The NomBank Project: An InterimReport, Proc.
of HLT-EACL Workshop:Frontiers in Corpus Annotation.A.
Meyers, R. Reeves, and C. Macleod (2004b),NP-External Arguments: A Study ofArgument Sharing in English.
In The ACL2004 Workshop on Multiword Expressions:Integrating Processing.E.
Miltsakaki, R. Prasad, A. Joshi and B.
Webber.
(2004a), The Penn Discourse Treebank.
InProc.
4th International Conference onLanguage Resources and Evaluation (LREC2004).E.
Miltsakaki, R. Prasad, A. Joshi and B. Webber(2004b), Annotation of DiscourseConnectives and their Arguments, in Proc.
ofHLT-NAACL Workshop: Frontiers in CorpusAnnotationM.
Marcus, B. Santorini, and M. Marcinkiewicz(1993), Building a large annotated corpus ofenglish: The penn treebank.
ComputationalLinguistics, 19:313--330.M.
Palmer, D. Gildea, P. Kingsbury (2005), TheProposition Bank: A Corpus Annotated withSemantic Roles, Computational LinguisticsJournal, 31:1.M.
Poesio (2004a), The MATE/GNOMEScheme for Anaphoric Annotation, Revisited,Proc.
of SIGDIALM.
Poesio (2004b), Discourse Annotation andSemantic Annotation in the GNOME Corpus,Proc.
of ACL Workshop on DiscourseAnnotation.M.
Poesio and M. Alexandrov-Kabadjov (2004),A general-purpose, off-the-shelf system foranaphora resol.. Proc.
of LREC.M.
Poesio, F. Bruneseaux, and L. Romary(1999), The MATE meta-scheme forcoreference in dialogues in multiple language,Proc.
of the ACL Workshop on Standards forDiscourse Tagging.M.
Poesio and R. Vieira (1998), A corpus-basedinvestigation of definite description use.Computational Linguistics, 24(2).C.
Pollard and I.
A.
Sag (1994), Head-drivenphrase structure grammar.
Univ.
of ChicagoPress.J.
Pustejovsky, R.
Saur?, J. Casta?o, D. R.Radev, R. Gaizauskas, A. Setzer, B.Sundheim and G. Katz (2004), RepresentingTemporal and Event Knowledge for QASystems.
In Mark T. Maybury (ed.
), NewDirections in Question Answering, MIT Press.J.
Pustejovsky,  B. Ingria, R.
Saur?, J. Casta?o, J.Littman, R. Gaizauskas, A. Setzer, G. Katz,and I. Mani (2003), The SpecificationLanguage TimeML.
In I. Mani, J.Pustejovsky, and R. Gaizauskas, editors, TheLanguage of Time: A Reader.
Oxford Univ.Press.12
