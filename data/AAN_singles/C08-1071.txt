Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 561?568Manchester, August 2008When is Self-Training Effective for Parsing?David McClosky, Eugene Charniak, and Mark JohnsonBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{dmcc|ec|mj}@cs.brown.eduAbstractSelf-training has been shown capable ofimproving on state-of-the-art parser per-formance (McClosky et al, 2006) despitethe conventional wisdom on the matter andseveral studies to the contrary (Charniak,1997; Steedman et al, 2003).
However, ithas remained unclear when and why self-training is helpful.
In this paper, we testfour hypotheses (namely, presence of aphase transition, impact of search errors,value of non-generative reranker features,and effects of unknown words).
Fromthese experiments, we gain a better un-derstanding of why self-training works forparsing.
Since improvements from self-training are correlated with unknown bi-grams and biheads but not unknown words,the benefit of self-training appears most in-fluenced by seeing known words in newcombinations.1 IntroductionSupervised statistical parsers attempt to capturepatterns of syntactic structure from a labeled set ofexamples for the purpose of annotating new sen-tences with their structure (Bod, 2003; Charniakand Johnson, 2005; Collins and Koo, 2005; Petrovet al, 2006; Titov and Henderson, 2007).
Theseannotations can be used by various higher-level ap-plications such as semantic role labeling (Pradhanet al, 2007) and machine translation (Yamada andKnight, 2001).c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.However, labeled training data is expensive toannotate.
Given the large amount of unlabeled textavailable for many domains and languages, tech-niques which allow us to use both labeled andunlabeled text to train our models are desirable.These methods are called semi-supervised.
Self-training is a specific type of semi-supervised learn-ing.
In self-training, first we train a model on thelabeled data and use that model to label the unla-beled data.
From the combination of our originallabeled data and the newly labeled data, we train asecond model ?
our self-trained model.
The pro-cess can be iterated, where the self-trained modelis used to label new data in the next iteration.
Onecan think of self-training as a simple case of co-training (Blum and Mitchell, 1998) using a singlelearner instead of several.
Alternatively, one canthink of it as one step of the Viterbi EM algorithm.Studies prior to McClosky et al (2006) failed toshow a benefit to parsing from self-training (Char-niak, 1997; Steedman et al, 2003).
While the re-cent success of self-training has demonstrated itsmerit, it remains unclear why self-training helps insome cases but not others.
Our goal is to better un-derstand when and why self-training is beneficial.In Section 2, we discuss the previous applica-tions of self-training to parsing.
Section 3 de-scribes our experimental setup.
We present andtest four hypotheses of why self-training helps inSection 4 and conclude with discussion and futurework in Section 5.2 Previous WorkTo our knowledge, the first reported uses of self-training for parsing are by Charniak (1997).
Heused his parser trained on the Wall Street Journal(WSJ, Mitch Marcus et al (1993)) to parse 30 mil-lion words of unparsed WSJ text.
He then trained561a self-trained model from the combination of thenewly parsed text with WSJ training data.
How-ever, the self-trained model did not improve on theoriginal model.Self-training and co-training were subsequentlyinvestigated in the 2002 CLSP Summer Work-shop at Johns Hopkins University (Steedman etal., 2003).
They considered several different pa-rameter settings, but in all cases, the number ofsentences parsed per iteration of self-training wasrelatively small (30 sentences).
They performedmany iterations of self-training.
The largest seedsize (amount of labeled training data) they usedwas 10,000 sentences from WSJ, though many ex-periments used only 500 or 1,000 sentences.
Theyfound that under these parameters, self-training didnot yield a significant gain.Reichart and Rappoport (2007) showed that onecan self-train with only a generative parser if theseed size is small.
The conditions are similar toSteedman et al (2003), but only one iteration ofself-training is performed (i.e.
all unlabeled data islabeled at once).1 In this scenario, unknown words(words seen in the unlabeled data but not in train-ing) were a useful predictor of when self-trainingimproves performance.McClosky et al (2006) showed that self-trainingimproves parsing accuracy when the two-stageCharniak and Johnson (2005) reranking parser isused.
Using both stages (a generative parser anddiscriminative reranker) to label the unlabeled dataset is necessary to improve performance.
Only re-training the first stage had a positive effect.
How-ever, after retraining the first stage, both stages pro-duced better parses.
Unlike Steedman et al (2003),the training seed size is large and only one itera-tion of self-training is performed.
Error analysisrevealed that most improvement comes from sen-tences with lengths between 20 and 40 words.
Sur-prisingly, improvements were also correlated withthe number of conjunctions but not with the num-ber of unknown words in the sentence.To summarize, several factors have been iden-tified as good predictors of when self-training im-proves performance, but a full explanation of whyself-training works is lacking.
Previous work es-tablishes that parsing all unlabeled sentences atonce (rather than over many iterations) is impor-tant for successful self-training.
The full effect of1Performing multiple iterations presumably fails becausethe parsing models become increasingly biased.
However,this remains untested in the large seed case.seed size and the reranker on self-training is notwell understood.3 Experimental SetupWe use the Charniak and Johnson reranking parser(outlined below), though we expect many of theseresults to generalize to other generative parsersand discriminative rerankers.
Our corpora consistof WSJ for labeled data and NANC (North Amer-ican News Text Corpus, Graff (1995)) for unla-beled data.
We use the standard WSJ division forparsing: sections 2-21 for training (39,382 sen-tences) and section 24 for development (1,346 sen-tences).
Given self-training?s varied performancein the past, many of our experiments use the con-catenation of sections 1, 22, and 24 (5,039 sen-tences) rather than the standard development setfor more robust testing.A full description of the reranking parser can befound in Charniak and Johnson (2005).
Brieflyput, the reranking parser consists of two stages:A generative lexicalized PCFG parser which pro-poses a list of the n most probable parses (n-bestlist) followed by a discriminative reranker whichreorders the n-best list.
The reranker uses about1.3 million features to help score the trees, themost important of which is the first stage parser?sprobability.
In Section 4.3, we mention two classesof reranker features in more depth.
Since some ofexperiments rely on details of the first stage parser,we present a summary of the parsing model.3.1 The Parsing ModelThe parsing model assigns a probability to a parse?
by a top-down process of considering each con-stituent c in ?
and, for each c, first guessing thepreterminal of c, t(c) then the lexical head of c,h(c), and then the expansion of c into further con-stituents e(c).
Thus the probability of a parse isgiven by the equationp(?)
=?c?
?p(t(c) | l(c),R(c))?p(h(c) | t(c), l(c),R(c))?p(e(c) | l(c), t(c), h(c),R(c))where l(c) is the label of c (e.g., whether it is anoun phrase (np), verb phrase, etc.)
and R(c) isthe relevant history of c ?- information outside cthat the probability model deems important in de-termining the probability in question.562For each expansion e(c) we distinguish one ofthe children as the ?middle?
child M(c).
M(c) isthe constituent from which the head lexical itemh is obtained according to deterministic rules thatpick the head of a constituent from among theheads of its children.
To the left of M is a sequenceof one or more left labels Li(c) including the spe-cial termination symbol ?
and similarly for the la-bels to the right, Ri(c).
Thus an expansion e(c)looks like:l ?
?Lm...L1MR1...Rn?.
(1)The expansion is generated by guessing first M ,then in order L1through Lm+1(= ?
), and simi-larly for R1through Rn+1.So the parser assigns a probability to the parsebased upon five probability distributions, T (thepart of speech of the head), H (the head), M (thechild constituent which includes the head), L (chil-dren to the left of M ), and R (children to the rightof M ).4 Testing the Four HypothesesThe question of why self-training helps in somecases (McClosky et al, 2006; Reichart and Rap-poport, 2007) but not others (Charniak, 1997;Steedman et al, 2003) has inspired various the-ories.
We investigate four of these to better un-derstand when and why self-training helps.
Ata high level, the hypotheses are (1) self-traininghelps after a phase transition, (2) self-training re-duces search errors, (3) specific classes of rerankerfeatures are needed for self-training, and (4) self-training improves because we see new combina-tions of words.4.1 Phase TransitionThe phase transition hypothesis is that once aparser has achieved a certain threshold of perfor-mance, it can label data sufficiently accurately.Once this happens, the labels will be ?goodenough?
for self-training.To test the phase transition hypothesis, we usethe same parser as McClosky et al (2006) but trainon only a fraction of WSJ to see if self-training isstill helpful.
This is similar to some of the ex-periments by Reichart and Rappoport (2007) butwith the use of a reranker and slightly larger seedsizes.
The self-training protocol is the same asin (Charniak, 1997; McClosky et al, 2006; Re-ichart and Rappoport, 2007): we parse the entireunlabeled corpus in one iteration.
We start by tak-ing a random subset of the WSJ training sections(2-21), accepting each sentence with 10% proba-bility.
With the sampled training section and thestandard development data, we train a parser and areranker.
In Table 1, we show the performance ofthe parser with and without the reranker.
For ref-erence, we show the performance when using thecomplete training division as well.
Unsurprisingly,both metrics drop as we decrease the amount oftraining data.
These scores represent our baselinesfor this experiment.Using this parser model, we parse one millionsentences from NANC, both with and without thereranker.
We combine these one million sentenceswith the sampled subsets of WSJ training and trainnew parser models from them.2Finally, we evaluate these self-trained models(Table 2).
The numbers in parentheses indicate thechange from the corresponding non-self-trainedmodel.
As in Reichart and Rappoport (2007), wesee large improvements when self-training on asmall seed size (10%) without using the reranker.However, using the reranker to parse the self-training and/or evaluation sentences further im-proves results.
From McClosky et al (2006), weknow that when 100% of the training data is used,self-training does not improve performance with-out a reranker.From this we conclude that there is no suchthreshold phase transition in this case.
High per-formance is not a requirement to successfully useself-training for parsing, since there are lower per-forming parsers which can self-train and higherperforming parsers which cannot.
The higher per-forming Charniak and Johnson (2005) parser with-out reranker achieves an f -score of 89.0 on section24 when trained on all of WSJ.
This parser doesnot benefit from self-training unless paired with areranker.
Contrast this with the same parser trainedon only 10% of WSJ, where it gets an f -score of85.8 (Table 2) or the small seed models of Reichartand Rappoport (2007).
Both of these lower per-forming parsers can successfully self-train.
Ad-ditionally, we now know that while a reranker isnot required for self-training when the seed size issmall, it still helps performance considerably (f -score improves from 87.7 to 89.0 in the 10% case).2We do not weight the original WSJ data, though our ex-pectation is that performance would improve if WSJ weregiven a higher relative weight.
This is left as future work.563% WSJ # sentences Parser f -score Reranking parser f -score10 3,995 85.8 87.0100 39,832 89.9 91.5Table 1: Parser and reranking parser performance on sentences ?
100 words in sections 1, 22, and 24when trained on different amounts of training data.
% WSJ is the percentage of WSJ training data trainedon (sampled randomly).
Note that the full amount of development data is still used as held out data.Parsed NANC with reranker?
Parser f -score Reranking parser f -scoreNo 87.7 (+1.9) 88.7 (+1.7)Yes 88.4 (+2.6) 89.0 (+2.0)Table 2: Effect of self-training using only 10% of WSJ as labeled data.
The parser model is trained fromone million parsed sentences from NANC + WSJ training.
The first column indicates whether the millionNANC sentences were parsed by the parser or reranking parser.
The second and third columns differ inwhether the reranker is used to parse the test sentences (WSJ sections 1, 22, and 24, sentences 100 wordsand shorter).
Numbers in parentheses are the improvements over the corresponding non-self-trainedparser.4.2 Search ErrorsAnother possible explanation of self-training?s im-provements is that seeing newly labeled data re-sults in fewer search errors (Daniel Marcu, per-sonal communication).
A search error would in-dicate that the parsing model could have producedbetter (more probable) parses if not for heuristicsin the search procedure.
The additional parse treesmay help produce sharper distributions and reducedata sparsity, making the search process easier.
Totest this, first we present some statistics on the n-best lists (n = 50) from the baseline WSJ trainedparser and self-trained model3 from McClosky etal.
(2006).
We use each model to parse sentencesfrom held-out data (sections 1, 22, and 24) and ex-amine the n-best lists.We compute statistics of the WSJ and self-trained n-best lists with the goal of understand-ing how much they intersect and whether there aresearch errors.
On average, the n-best lists over-lap by 66.0%.
Put another way, this means thatabout a third of the parses from each model areunique, so the parsers do find a fair number of dif-ferent parses in their search.
The next questionis where the differences in the n-best lists lie ?if all the differences were near the bottom, thiswould be less meaningful.
Let W and S repre-sent the n-best lists from the baseline WSJ and self-trained parsers, respectively.
The topm(?)
func-tion returns the highest scoring parse in the n-bestlist ?
according to the reranker and parser model3http://bllip.cs.brown.edu/selftraining/m.4 Almost 40% of the time, the top parse inthe self-trained model is not in the WSJ model?sn-best list, (tops(S) /?
W ) though the two mod-els agree on the top parse roughly 42.4% of thetime (tops(S) = topw(W )).
Search errors canbe formulated as tops(S) /?
W ?
tops(S) =topw(W ?
S).
This captures sentences where theparse that the reranker chose in the self-trainedmodel is not present in the WSJ model?s n-best list,but if the parse were added to the WSJ model?s list,the parse?s probability in the WSJ model and otherreranker features would have caused it to be cho-sen.
These search errors occur in only 2.5% ofthe n-best lists.
At first glance, one might thinkthat this could be enough to account for the differ-ences, since the self-trained model is only severaltenths better in f -score.
However, we know fromMcClosky et al (2006) that on average, parses donot change between the WSJ and self-trained mod-els and when they do, they only improve slightlymore than half the time.
For this reason, we run asecond test more focused on performance.For our second test we help the WSJ trainedmodel find the parses that the self-trained modelfound.
For each sentence, we start with the n-bestlist (n = 500 here) from the WSJ trained parser,W .
We then consider parses in the self-trainedparser?s n-best list, S, that are not present in W(S ?
W ).
For each of these parses, we deter-mine its probability under the WSJ trained parsing4Recall that the parser?s probability is a reranker featureso the parsing model influences the ranking.564Model f -scoreWSJ 91.5WSJ & search help 91.7Self-trained 92.0Table 3: Test of whether ?search help?
from theself-trained model impacts the WSJ trained model.WSJ + search help is made by adding self-trainedparses not proposed by the WSJ trained parser butto which the parser assigns a positive probability.The WSJ reranker is used in all cases to select thebest parse for sections 1, 22, and 24.model.
If the probability is non-zero, we add theparse to the n-best list W , otherwise we ignore theparse.
In other words, we find parses that the WSJtrained model could have produced but didn?t dueto search heuristics.
In Table 3, we show the per-formance of the WSJ trained model, the model with?search help?
as described above, and the self-trained model on WSJ sections 1, 22, and 24.
TheWSJ reranker is used to pick the best parse fromeach n-best list.
WSJ with search help performsslightly better than WSJ alone but does not reachthe level of the self-trained model.
From these ex-periments, we conclude that reduced search errorscan only explain a small amount of self-training?simprovements.4.3 Non-generative reranker featuresWe examine the role of specific reranker featuresby training rerankers using only subsets of the fea-tures.
Our goal is to determine whether someclasses of reranker features benefit self-trainingmore than others.
We hypothesize that featureswhich are not easily captured by the generativefirst-stage parser are the most beneficial for self-training.
If we treat the parser and reranking parseras different (but clearly dependent) views, this is abit like co-training.
If the reranker uses featureswhich are captured by the first-stage, the viewsmay be too similar for there to be an improvement.We consider two classes of features (GEN andEDGE) and their complements (NON-GEN andNON-EDGE).5 GEN consists of features thatare roughly captured by the first-stage generativeparser: rule rewrites, head-child dependencies, etc.EDGE features describe items across constituentboundaries.
This includes the words and parts of5A small number of features overlap hence these sizes donot add up.Feature set # features f -scoreGEN 448,349 90.4NON-GEN 885,492 91.1EDGE 601,578 91.0NON-EDGE 732,263 91.1ALL 1,333,519 91.3Table 4: Sizes and performance of reranker featuresubsets.
Reranking parser f -scores are on all sen-tences in section 24.speech of the tokens on the edges between con-stituents and the labels of these constituents.
Thisrepresents a specific class of features not capturedby the first-stage.
These subsets and their sizes areshown in Table 4.
For comparison, we also includethe results of experiments using the full feature set,as in McClosky et al (2006), labeled ALL.
TheGEN features are roughly one third the size of thefull feature set.We evaluate the effect of these new rerankermodels on self-training (Table 4).
For each fea-ture set, we do the following: We parse one millionNANC sentences with the reranking parser.
Com-bining the parses with WSJ training data, we traina new first-stage model.
Using this new first-stagemodel and the reranker subset, we evaluate on sec-tion 24 of WSJ.
GEN?s performance is weakerwhile the other three subsets achieve almost thesame score as the full feature set.
This confirmsour hypothesis that when the reranker helps in self-training it is due to features which are not capturedby the generative first-stage model.4.4 Unknown WordsGiven the large size of the parsed self-training cor-pus, it contains an immense number of parsingevents which never occur in the training corpus.The most obvious of these events is words ?
thevocabulary grows from 39,548 to 265,926 wordsas we transition from the WSJ trained model tothe self-trained model.
Slightly less obvious is bi-grams.
There are roughly 330,000 bigrams in WSJtraining data and approximately 4.8 million newbigrams in the self-training corpus.One hypothesis (Mitch Marcus, personal com-munication) is that the parser is able to learn a lotof new bilexical head-to-head dependencies (bi-heads) from self-training.
The reasoning is as fol-lows: Assume the self-training corpus is parsed ina mostly correct manner.
If there are not too many565new pairs of words in a sentence, there is a de-cent chance that we can tag these words correctlyand bracket them in a reasonable fashion from con-text.
Thus, using these parses as part of the train-ing data improves parsing because should we seethese pairs of words together in the future, we willbe more likely to connect them together properly.We test this hypothesis in two ways.
First, weperform an extension of the factor analysis simi-lar to that in McClosky et al (2006).
This is donevia a generalized linear regression model intendedto determine which features of parse trees can pre-dict when the self-training model will perform bet-ter.
We consider many of the same features (e.g.bucketed sentence length, number of conjunctions,and number of unknown words) but also considertwo new features: unknown bigrams and unknownbiheads.
Unknown items (words, bigrams, bi-heads) are calculated by counting the number ofitems which have never been seen in WSJ train-ing but have been seen in the parsed NANC data.Given these features, we take the f -scores for eachsentence when parsed by the WSJ and self-trainedmodels and look at the differences.
Our goal is tofind out which features, if any, can predict these f -score differences.
Specifically, we ask the questionof whether seeing more unknown items indicateswhether we are more likely to see improvementswhen self-training.The effect of unknown items on self-training?srelative performance is summarized in Figure 1.For each item, we show the total number of incor-rect parse nodes in sentences that contain the item.We also show the change in the number of correctparse nodes in these sentences between the WSJand self-trained models.
A positive change meansthat performance improved under self-training.
Inother words, looking at Figure 1a, the greatest per-formance improvement occurs, perhaps surpris-ingly, when we have seen no unknown words.As we see more unknown words, the improve-ment from self-training decreases.
This is a prettyclear indication that unknown words are not a goodpredictor of when self-training improves perfor-mance.A possible objection that one might raise is thatusing unknown biheads as a regression feature willbias our results if they are counted from gold treesinstead of parsed trees.
Seeing a bihead in train-ing will cause the otherwise sparse biheads dis-tribution to be extremely peaked around that bi-f -score Model89.8 ?
WSJ (baseline)89.8 ?
WSJ+NANC M89.9 ?
WSJ+NANC T89.9 ?
WSJ+NANC L90.0 ?
WSJ+NANC R90.0 WSJ+NANC MT90.1 WSJ+NANC H90.2 WSJ+NANC LR90.3 WSJ+NANC LRT90.4 WSJ+NANC LMRT90.4 WSJ+NANC LMR90.5 WSJ+NANC LRH90.7 ?
WSJ+NANC LMRH90.8 ?
WSJ+NANC (fully self-trained)Table 5: Performance of the first-stage parseron various combinations of distributions WSJ andWSJ+NANC (self-trained) models on sections 1,22, and 24.
Distributions are L (left expansion), R(right expansion), H (head word), M (head phrasalcategory), and T (head POS tag).
?
and ?
indicatethe model is not significantly different from base-line and self-trained model, respectively.head.
If we see the same pair of words in testing,we are likely to connect them in the same fash-ion.
Thus, if we count unknown biheads from goldtrees, this feature may explain away other improve-ments: When gold trees contain a bihead found inour self-training data, we will almost always see animprovement.
However, given the similar trends inFigures 1b and 1c, we propose that unknown bi-grams can be thought of as a rough approximationof unknown biheads.The regression analysis reveals that unknown bi-grams and unknown biheads are good predictors off -score improvements.
The significant predictorsfrom McClosky et al (2006) such as the numberof conjunctions or sentence length continue to behelpful whereas unknown words are a weak pre-dictor at best.
These results are apparent in Figure1: as stated before, seeing more unknown wordsdoes not correlate with improvements.
However,seeing more unknown bigrams and biheads doespredict these changes fairly well.
When we haveseen zero or one new bigrams or biheads, self-training negatively impacts performance.
Afterseeing two or more, we see positive effects untilabout six to ten after which improvements taperoff.566To see the effect of biheads on performancemore directly, we also experiment by interpolat-ing between the WSJ and self-trained models on adistribution level.
To do this, we take specific dis-tributions (see Section 3.1) from the self-trainedmodel and have them override the correspondingdistributions in a compatible WSJ trained model.From this we hope to show which distributionsself-training boosts.
According to the biheads hy-pothesis, the H distribution (which captures infor-mation about head-to-head dependencies) shouldaccount for most of the improvement.The results of moving these distributions isshown in Table 5.
For each new model, we showwhether the model?s performance is not signifi-cantly different than the baseline model (indicatedby ?)
or not significantly different than the self-trained model (?).
H (biheads) is the strongest sin-gle feature and the only one to be significantly bet-ter than the baseline.
Nevertheless, it is only 0.3%higher, accounting for 30% of the full self-trainingimprovement.
In general, the performance im-provements from distributions are additive (+/?0.1%).
Self-training improves all distributions, sobiheads are not the full picture.
Nevertheless, theyremain the strongest single feature.5 DiscussionThe experiments in this paper have clarified manydetails about the nature of self-training for parsing.We have ruled out the phase transition hypothe-sis entirely.
Reduced search errors are responsiblefor some, but not all, of the improvements in self-training.
We have confirmed that non-generativereranker features are more beneficial than genera-tive reranker features since they make the rerank-ing parser more different from the base parser.
Fi-nally, we have found that while unknown bigramsand biheads are a significant source of improve-ment, they are not the sole source of it.
Sinceunknown words do not correlate well with self-training improvements, there must be somethingabout the unknown bigrams and biheads which areaid the parser.
Our belief is that new combinationsof words we have already seen guides the parser inthe right direction.
Additionally, these new combi-nations result in more peaked distributions whichdecreases the number of search errors.However, while these experiments and othersget us closer to understanding self-training, we stilllack a complete explanation.
Naturally, the hy-0 1 2 3 4 5 6 7 10 11 12Total number ofincorrectnodes060000 1 2 3 4 5 6 7 10 11 12Number of unknown words in treeReductioninincorrectnodes0300(a) Effect of unknown words on performance0 2 4 6 8 10 12 14 16 18 20Total number ofincorrectnodes010000 2 4 6 8 10 12 14 16 18 20Number of unknown bigrams in treeReductioninincorrectnodes?100100(b) Effect of unknown bigrams on performance0 2 4 6 8 10 12 14 16 18 20 25Total number ofincorrectnodes010000 2 4 6 8 10 12 14 16 18 20 25Number of unknown biheads in treeReductioninincorrectnodes?50100(c) Effect of unknown biheads on performanceFigure 1: Change in the number of incorrect parsetree nodes between WSJ and self-trained modelsas a function of number of unknown items.
See-ing any number of unknown words results in fewererrors on average whereas seeing zero or one un-known bigrams or biheads is likely to hurt perfor-mance.567potheses tested are by no means exhaustive.
Addi-tionally, we have only considered generative con-stituency parsers here and a good direction for fu-ture research would be to see if self-training gener-alizes to a broader class of parsers.
We suspect thatusing a generative parser/discriminative rerankerparadigm should allow self-training to extend toother parsing formalisms.Recall that in Reichart and Rappoport (2007)where only a small amount of labeled data wasused, the number of unknown words in a sen-tence was a strong predictor of self-training ben-efits.
When a large amount of labeled data is avail-able, unknown words are no longer correlated withthese gains, but unknown bigrams and biheads are.When using a small amount of training data, un-known words are useful since we have not seenvery many words yet.
As the amount of train-ing data increases, we see fewer new words butthe number of new bigrams and biheads remainshigh.
We postulate that this difference may helpexplain the shift from unknown words to unknownbigrams and biheads.
We hope to further inves-tigate the role of these unknown items by seeinghow our analyses change under different amountsof labeled data relative to unknown item rates.AcknowledgmentsThis work was supported by DARPA GALE contractHR0011-06-2-0001.
We would like to thank Matt Lease, therest of the BLLIP team, and our anonymous reviewers fortheir comments.
Any opinions, findings, and conclusions orrecommendations expressed in this paper are those of the au-thors and do not necessarily reflect the views of DARPA.ReferencesBlum, Avrim and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Pro-ceedings of the 11th Annual Conference on Compu-tational Learning Theory (COLT-98).Bod, Rens.
2003.
An efficient implementation of anew DOP model.
In 10th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, Budapest, Hungary.Charniak, Eugene and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proc.
of the 2005 Meeting of the Assoc.for Computational Linguistics (ACL), pages 173?180.Charniak, Eugene.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proc.AAAI, pages 598?603.Collins, Michael and Terry Koo.
2005.
DiscriminativeReranking for Natural Language Parsing.
Computa-tional Linguistics, 31(1):25?69.Graff, David.
1995.
North American News Text Cor-pus.
Linguistic Data Consortium.
LDC95T21.McClosky, David, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the Human Language TechnologyConference of the NAACL, Main Conference, pages152?159.Mitch Marcus et al 1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Comp.
Lin-guistics, 19(2):313?330.Petrov, Slav, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 433?440, Sydney,Australia, July.
Association for Computational Lin-guistics.Pradhan, Sameer, Wayne Ward, and James Martin.2007.
Towards robust semantic role labeling.
In Hu-man Language Technologies 2007: The Conferenceof the North American Chapter of the Association forComputational Linguistics; Proceedings of the MainConference, pages 556?563, Rochester, New York,April.
Association for Computational Linguistics.Reichart, Roi and Ari Rappoport.
2007.
Self-trainingfor enhancement and domain adaptation of statisticalparsers trained on small datasets.
Proceedings of the45th Annual Meeting of the Association of Computa-tional Linguistics, pages 616?623.Steedman, Mark, Steven Baker, Jeremiah Crim,Stephen Clark, Julia Hockenmaier, Rebecca Hwa,Miles Osborne, Paul Ruhlen, and Anoop Sarkar.2003.
CLSP WS-02 Final Report: Semi-SupervisedTraining for Statistical Parsing.
Technical report,Johns Hopkins University.Titov, Ivan and James Henderson.
2007.
Constituentparsing with incremental sigmoid belief networks.In Proceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 632?639, Prague, Czech Republic, June.
Association forComputational Linguistics.Yamada, Kenji and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedingsof the 39th Annual Meeting of the Association forComputational Linguistics, pages 523?529.568
