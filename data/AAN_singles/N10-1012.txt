Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 100?108,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAutomatic Evaluation of Topic CoherenceDavid Newman,??
Jey Han Lau,?
Karl Grieser?, and Timothy Baldwin,???
NICTA Victoria Research Laboratory, Australia?
Dept of Computer Science, University of California, Irvine?
Dept of Computer Science and Software Engineering, University of Melbourne, Australia?
Dept of Information Systems, University of Melbourne, Australianewman@uci.edu, depthchargex@gmail.com,kgrieser@csse.unimelb.edu.au, tb@ldwin.netAbstractThis paper introduces the novel task of topiccoherence evaluation, whereby a set of words,as generated by a topic model, is rated forcoherence or interpretability.
We apply arange of topic scoring models to the evaluationtask, drawing on WordNet, Wikipedia and theGoogle search engine, and existing researchon lexical similarity/relatedness.
In compar-ison with human scores for a set of learnedtopics over two distinct datasets, we show asimple co-occurrence measure based on point-wise mutual information over Wikipedia datais able to achieve results for the task at ornearing the level of inter-annotator correla-tion, and that other Wikipedia-based lexicalrelatedness methods also achieve strong re-sults.
Google produces strong, if less consis-tent, results, while our results over WordNetare patchy at best.1 IntroductionThere has traditionally been strong interest withincomputational linguistics in techniques for learningsets of words (aka topics) which capture the latentsemantics of a document or document collection, inthe form of methods such as latent semantic analysis(Deerwester et al, 1990), probabilistic latent seman-tic analysis (Hofmann, 2001), random projection(Widdows and Ferraro, 2008), and more recently, la-tent Dirichlet alocation (Blei et al, 2003; Griffithsand Steyvers, 2004).
Such methods have been suc-cessfully applied to a myriad of tasks including wordsense discrimination (Brody and Lapata, 2009), doc-ument summarisation (Haghighi and Vanderwende,2009), areal linguistic analysis (Daume III, 2009)and text segmentation (Sun et al, 2008).
In eachcase, extrinsic evaluation has been used to demon-strate the effectiveness of the learned topics in theapplication domain, but standardly, no attempt hasbeen made to perform intrinsic evaluation of the top-ics themselves, either qualitatively or quantitatively.In machine learning, on the other hand, researchershave modified and extended topic models in a vari-ety of ways, and evaluated intrinsically in terms ofmodel perplexity (Wallach et al, 2009), but there hasbeen less effort on qualitative understanding of thesemantic nature of the learned topics.This research seeks to fill the gap between topicevaluation in computational linguistics and machinelearning, in developing techniques to perform intrin-sic qualitative evaluation of learned topics.
Thatis, we develop methods for evaluating the qual-ity of a given topic, in terms of its coherence toa human.
After learning topics from a collectionof news articles and a collection of books, we askhumans to decide whether individual learned top-ics are coherent, in terms of their interpretabilityand association with a single over-arching seman-tic concept.
We then propose models to predicttopic coherence, based on resources such as Word-Net, Wikipedia and the Google search engine, andmethods ranging from ontological similarity to linkoverlap and term co-occurrence.
Over topics learnedfrom two distinct datasets, we demonstrate that thereis remarkable inter-annotator agreement on what isa coherent topic, and additionally that our methodsbased on Wikipedia are able to achieve nearly perfectagreement with humans over the evaluation of topiccoherence.This research forms part of a larger researchagenda on the utility of topic modelling in gist-ing and visualising document collections, and ulti-mately enhancing search/discovery interfaces over100document collections (Newman et al, to appeara).Evaluating topic coherence is a component of thelarger question of what are good topics, what char-acteristics of a document collection make it moreamenable to topic modelling, and how can the po-tential of topic modelling be harnessed for humanconsumption (Newman et al, to appearb).2 Related WorkMost earlier work on intrinsically evaluating learnedtopics has been on the basis of perplexity results,where a model is learned on a collection of train-ing documents, then the log probability of the un-seen test documents is computed using that learnedmodel.
Usually perplexity is reported, which is theinverse of the geometric mean per-word likelihood.Perplexity is useful for model selection and adjust-ing parameters (e.g.
number of topics T ), and isthe standard way of demonstrating the advantage ofone model over another.
Wallach et al (2009) pre-sented efficient and unbiased methods for computingperplexity and evaluating almost any type of topicmodel.While statistical evaluation of topic models isreasonably well understood, there has been muchless work on evaluating the intrinsic semantic qual-ity of topics learned by topic models, which couldhave a far greater impact on the overall value oftopic modeling for end-user applications.
Some re-searchers have started to address this problem, in-cluding Mei et al (2007) who presented approachesfor automatic labeling of topics (which is core to thequestion of coherence and semantic interpretabil-ity), and Griffiths and Steyvers (2006) who appliedtopic models to word sense discrimination tasks.Misra et al (2008) used topic modelling to identifysemantically incoherent documents within a docu-ment collection (vs. coherent topics, as targeted inthis research).
Chang et al (2009) presented thefirst human-evaluation of topic models by creatinga task where humans were asked to identify whichword in a list of five topic words had been ran-domly switched with a word from another topic.This work showed some possibly counter-intuitiveresults, where in some cases humans preferred mod-els with higher perplexity.
This type of result showsthe need for further exploring measures other thanperplexity for evaluating topic models.
In earlierwork, we carried out preliminary experimentationusing pointwise mutual information and Google re-sults to evaluate topic coherence over the same setof topics as used in this research (Newman et al,2009).Part of this research takes inspiration from thework on automatic evaluation in machine translation(Papineni et al, 2002) and automatic summarisation(Lin, 2004).
Here, the development of automatedmethods with high correlation with human subjectshas opened the door to large-scale automated evalua-tion of system outputs, revolutionising the respectivefields.
While our aspirations are more modest, thebasic aim is the same: to develop a fully-automatedmethod for evaluating a well-grounded task, whichachieves near-human correlation.3 Topic ModellingIn order to evaluate topic modelling, we require atopic model and set of topics for a given documentcollection.
While the evaluation methodology wedescribe generalises to any method which gener-ates sets of words, all of our experiments are basedon Latent Dirichlet Allocation (LDA, aka DiscretePrincipal Component Analysis), on the grounds thatit is a state-of-the-art method for generating topics.LDA is a Bayesian graphical model for text docu-ment collections represented by bags-of-words (seeBlei et al (2003), Griffiths and Steyvers (2004),Buntine and Jakulin (2004)).
In a topic model, eachdocument in the collection of D documents is mod-elled as a multinomial distribution over T topics,where each topic is a multinomial distribution overW words.
Typically, only a small number of wordsare important (have high likelihood) in each topic,and only a small number of topics are present in eachdocument.The collapsed Gibbs sampled topic model simul-taneously learns the topics and the mixture of topicsin documents by iteratively sampling the topic as-signment z to every word in every document, usingthe Gibbs sampling update:p(zid = t|xid = w, z?id) ?N?idwt + ?
?w N?idwt + W?N?idtd + ?
?t N?idtd + T?101where zid = t is the assignment of the ith word indocument d to topic t, xid = w indicates that thecurrent observed word is w, and z?id is the vector ofall topic assignments not including the current word.Nwt represents integer count arrays (with the sub-scripts denoting what is counted), and ?
and ?
areDirichlet priors.The maximum a posterior (MAP) estimates of thetopics p(w|t), t = 1 .
.
.
T are given by:p(w|t) = Nwt + ?
?w Nwt + W?We will follow the convention of representing atopic via its top-n words, ordered by p(w|t).
Here,we use the top-ten words, as they usually providesufficient detail to convey the subject of a topic,and distinguish one topic from another.
For theremainder of this paper, we will refer to individ-ual topics by its list of top-ten words, denoted byw = (w1, .
.
.
, w10).4 Topic Evaluation MethodsWe experiment with scoring methods based onWordNet (Section 4.1), Wikipedia (Section 4.2) andthe Google search engine (Section 4.3).
In the caseof Google, we query for the entire topic, but withWordNet and Wikipedia, this takes the form of scor-ing each word-pair in a given topic w based on thecomponent words (w1, .
.
.
, w10).
Given some (sym-metric) word-similarity measure D(wi, wj), twostraightforward ways of producing a combined scorefrom the 45 (i.e.
(102)) word-pair scores are: (1) thearithmetic mean, and (2) the median, as follows:Mean-D-Score(w) =mean{D(wi, wj), ij ?
1 .
.
.
10, i < j}Median-D-Score(w) =median{D(wi, wj), ij ?
1 .
.
.
10, i < j}Intuitively, the median seems the more natural rep-resentation, as it is less affected by outlier scores,but we experiment with both, and fall back to empir-ical verification of which is the better combinationmethod.4.1 WordNet similarityWordNet (Fellbaum, 1998) is a lexical ontologythat represents word sense via ?synsets?, whichare structured in a hypernym/hyponym hierarchy(nouns) or hypernym/troponym hierarchy (verbs).WordNet additionally links both synsets and wordsvia lexical relations including antonymy, morpho-logical derivation and holonymy/meronym.In parallel with the development of WordNet, anumber of computational methods for calculatingthe semantic relatedness/similarity between synsetpairs (i.e.
sense-specified word pairs) have been de-veloped, as we outline below.
These methods ap-ply to synset rather than word pairs, so to generate asingle score for a given word pair, we look up eachword in WordNet and exhaustively generate scoresfor each sense pairing defined by them, and calcu-late their arithmetic mean.1The majority of the methods (all methods otherthan HSO, VECTOR and LESK) are restricted to op-erating strictly over hierarchical links within a sin-gle hierarchy.
As the verb and noun hierarchies arenot connected (other than via derivational links), thismeans that it is generally not possible to calculatethe similarity between noun and verb senses, for ex-ample.
In such cases, we simply drop the synsetpairing in question from our calculation of the mean.The least common subsumer (LCS) is a commonfeature to a number of the measures, and is definedas the deepest node in the hierarchy that subsumesboth of the synsets under question.For all our experiments over WordNet, we use theWordNet::Similarity package.Path distance (PATH)The simplest of the WordNet-based measures isto count the number of nodes visited while goingfrom one word to another via the hypernym hierar-chy.
The path distance between two nodes is de-fined as the number of nodes that lie on the short-est path between two words in the hierarchy.
This1We also experimented with the median, and trialled filter-ing the set of senses in a variety of ways, e.g.
using only thefirst sense (the sense with the highest prior) for a given word,or using only the word senses associated with the POS with thehighest prior.
In all cases, the overall trend was for the correla-tion with the human scores to drop relative to the mean, so weonly present the numbers for the mean in this paper.102count of nodes includes the beginning and endingword nodes.Leacock-Chodorow (LCH)The measure of semantic similarity devised byLeacock et al (1998) finds the shortest path betweentwo WordNet synsets (sp(c1, c2)) using hypernymand synonym relationships.
This path length is thenscaled by the maximum depth of WordNet (D), andthe log likelihood taken:simlch(c1, c2) = ?
logsp(c1, c2)2 ?DWu-Palmer (WUP)Wu and Palmer (1994) proposed to scale the depthof the two synset nodes (depthc1 and depthc2) bythe depth of their LCS (depth(lcsc1,c2)):simwup(c1, c2) =2 ?
depth(lcsc1,c2)depthc1 + depthc2 + 2 ?
depth(lcsc1,c2)The scaling means that specific terms (deeper in thehierarchy) that are close together are more semanti-cally similar than more general terms, which have ashort path distance between them.
Only hypernymrelationships are used in this measure, as the LCSis defined by the common member in the concepts?hypernym path.Hirst-St Onge (HSO)Hirst and St-Onge (1998) define a measure of se-mantic similarity based on length and tortuosity ofthe path between nodes.
Hirst and St-Onge attributedirections (up, down and horizontal) to the larger setof WordNet relationships, and identify the path fromone word to another utilising all of these relation-ships.
The relatedness score is then computed bythe weighted sum of the path length between the twowords (len(c1, c2)) and the number of turns the pathmakes (turns(c1, c2)) to take this route:relhso(c1, c2) =C ?
len(c1, c2)?
k ?
turns(c1, c2)where C and k are constants.
Additionally, a set ofrestrictions is placed on the path so that it may notbe more than a certain length, may not contain morethan a set number of turns, and may only take turnsin certain directions.Resnik Information Content (RES)Resnik (1995) presents a method for weightingedges in WordNet (avoiding the assumption that alledges between nodes have equal importance), byweighting edges between nodes by their frequencyof use in textual corpora.Resnik found that the most effective measure ofcomparison using this methodology was to measurethe Information Content (IC(c) = ?
log p(c)) ofthe subsumer with the greatest Information Contentfrom the set of all concepts that subsumed the twoinitial concepts (S(c1, c2)) being compared:simres(c1, c2) = maxc?S(c1,c2)[?
log p(c)]Lin (LIN)Lin (1998) expanded on the Information Theo-retic approach presented by Resnik by scaling theInformation Content of each node by the informa-tion content of their LCS:simlin(c1, c2) =2?
log p(lcsc1,c2)log p(c1) + log p(c2)This measure contrasts the joint content of the twoconcepts with the difference between them.Jiang-Conrath (JCN)Jiang and Conrath (1997) define a measure thatutilises the components of the information contentof the LCS in a different manner:simjcn(c1, c2) =1IC(a) + IC(b)?
2?
IC(lcsa,b)Instead of defining commonality and difference aswith Lin?s measure, the key determinant is the speci-ficity of the two nodes compared with their LCS.Lesk (LESK)Lesk (1986) proposed a significantly different ap-proach to lexical similarity to that proposed in themethods presented above, using the lexical over-lap in dictionary definitions (or glosses) to disam-biguate word sense.
The sense definitions that con-tain the most words in common indicate the mostlikely sense of the word given its co-occurrence withsimilar word senses.
Banerjee and Pedersen (2002)103adapted this method to utilise WordNet sense glossesrather than dictionary definitions, and expand thedictionary definitions via ontological links, and it isthis method we experiment with in this paper.Vector (VECTOR)Schu?tze (1998) uses the words surrounding a termin a piece of text to form a context vector that de-scribes the context in which the word sense appears.For a set of words associated with a target sense, acontext vector is computed as the centroid vector ofthese words.
The centroid context vectors each rep-resent a word sense.
To compare word senses, thecosine similarity of the context vectors is used.4.2 WikipediaIn the last few years, there has been a surge of in-terest in using Wikipedia to calculate semantic sim-ilarity, using the Wikipedia article content, in-articlelinks and document categories (Stru?be and Ponzetto,2006; Gabrilovich and Markovitch, 2007; Milne andWitten, 2008).
We present a selection of such meth-ods below.
There are a number of Wikipedia-basedscoring methods which we do not present resultsfor here (notably Stru?be and Ponzetto (2006) andGabrilovich and Markovitch (2007)), due to theircomputational complexity and uncertainty about thefull implementation details of the methods.As with WordNet, a given word will often havemultiple entries in Wikipedia, grouped in a disam-biguation page.
For MIW, RACO and DOCSIM,we apply the same strategy as we did with Word-Net, in exhaustively calculating the pairwise scoresbetween the sets of documents associated with eachterm, and averaging across them.Milne-Witten (MIW)Milne and Witten (2008) adapted the Resnik(1995) methodology to utilise the count of linkspointing to an article.
As Wikipedia is self-referential (articles link to related articles), no ex-ternal data is needed to find the ?referred-to-edness?of a concept.
Milne and Witten use an adapted In-formation Content measure that weights the numberof links from one article to another (c1 ?
c2) by thetotal number of links to the second article:w(c1 ?
c2) = |c1 ?
c2| ?
log?x?W|W ||c1, x)|where x is an article in W , Wikipedia.
This mea-sure provides the similarity of one article to another,however this is asymmetrical.
The above metric isused to find the weights of all outlinks from the twoarticles being compared:~c1 = (w(c1 ?
l1), w(c1 ?
l2), ?
?
?
, w(c1 ?
ln))~c2 = (w(c2 ?
l1), w(c2 ?
l2), ?
?
?
, w(c2 ?
ln))for the set of links l that is the union of the sets ofoutlinks from both articles.
The overall similarityof the two articles is then calculated by taking thecosine similarity of the two vectors.Related Article Concept Overlap (RACO)We also determine the category overlap of twoarticles by examining the outlinks of both articles,in the form of the Related Article Concept Overlap(RACO) measure.
The concept overlap of the setsof respective outlinks is given by the union of thetwo sets of categories from the outlinks from eacharticle:overlap(c1, c1) =??(?l?ol(c1)cat(l))?(?l?ol(c2)cat(l))?
?where ol(c1) is the set of outlinks from article c1,and cat(l) is the set of categories of which the arti-cle at outlink l is a member.
To account for articlesize (and differing number of outlinks), the Jaccardcoefficient is used:relraco(c1, c2) =??
(?l?ol(c1) cat(l))?
(?l?ol(c2) cat(l))????
?l?ol(c1) cat(l)??+??
?l?ol(c2) cat(l)?
?Document Similarity (DOCSIM)In addition to these two measures of semantic re-latedness, we experiment with simple cosine simi-larity of the text of Wikipedia articles as a measureof semantic relatedness.Term Co-occurrence (PMI)Another variant is to treat Wikipedia as a singlemeta-document and score word pairs using term co-occurrence.
Here, we calculate the pointwise mu-tual information (PMI) of each word pair, estimated104Selected high-scoring topics (unanimous score=3):[NEWS] space earth moon science scientist light nasa mission planet mars ...[NEWS] health disease aids virus vaccine infection hiv cases infected asthma ...[BOOKS] steam engine valve cylinder pressure piston boiler air pump pipe ...[BOOKS] furniture chair table cabinet wood leg mahogany piece oak louis ...Selected low-scoring topics (unanimous score=1):[NEWS] king bond berry bill ray rate james treas byrd key ...[NEWS] dog moment hand face love self eye turn young character ...[BOOKS] soon short longer carried rest turned raised filled turn allowed ...[BOOKS] act sense adv person ppr plant sax genus applied dis ...Table 1: A selection of high-scoring and low-scoring topicsfrom the entire corpus of over two million EnglishWikipedia articles (?1 billion words).
PMI has beenstudied variously in the context of collocation ex-traction (Pecina, 2008), and is one measure of thestatistical independence of observing two words inclose proximity.
Using a sliding window of 10-words to identify co-occurrence, we computed thePMI of all a given word pair (wi, wj) as, followingNewman et al (2009):PMI(wi, wj) = logp(wi, wj)p(wi)p(wj)4.3 Search engine-based similarityFinally, we present two search engine-based scor-ing methods, based on Newman et al (2009).
Inthis case the external data source is the entire WorldWide Web, via the Google search engine.
Unlikethe methods presented above, here we query for thetopic in its entirety,2 meaning that we return a topic-level score rather than scores for individual word orword sense pairs.
In each case, we mark each searchterm with the advanced search option + to searchfor the terms exactly as is and prevent Google fromusing synonyms or lexical variants of the term.
Anexample query is: +space +earth +moon +science+scientist +light +nasa +mission +planet +mars.Google title matches (TITLES)Firstly, we score topics by the relative occurrenceof their component words in the titles of documentsreturned by Google:Google-titles-match(w) = 1 [wi = vj ]2All queries were run on 15/09/2009.where i = 1, .
.
.
, 10 and j = 1, .
.
.
, |V |, vj areall the unique terms mentioned in the titles from thetop-100 search results, and 1 is the indicator functionto count matches.
For example, in the top-100 re-sults for our query above, there are 194 matches withthe ten topic words, so Google-titles-match(w) =194.Google log hits matches (LOGHITS)Second, we issue queries as above, but return thelog number of hits for our query:Google-log-hits(w) =log10(# results from search for w)where w is the search string +w1 +w2 +w3 .
.
.+w10.
For example, our query above returns171,000 results, so Google-log-hits(w) = 5.2. andthe URL titles from the top-100 results include a to-tal of 194 matches with the ten topic words, so forthis topic Google-titles-match(w)=194.5 Experimental SetupWe learned topics for two document collections: acollection of news articles, and a collection of books.These collections were chosen to produce sets oftopics that have more variable quality than one typi-cally observes when topic modeling highly uniformcontent.
The collection of D = 55, 000 news arti-cles was selected from English Gigaword, and thecollection of D = 12, 000 books was downloadedfrom the Internet Archive.
We refer to these collec-tions as NEWS and BOOKS, respectively.Standard procedures were used to tokenize eachcollection and create the bags-of-words.
We learned105Resource Method Median MeanWordNetHSO ?0.29 0.34JCN 0.08 0.22LCH ?0.18 ?0.07LESK 0.38 0.37LIN 0.18 0.25PATH 0.19 0.11RES ?0.10 0.13VECTOR 0.07 0.20WUP 0.03 0.10WikipediaRACO 0.61 0.63MIW 0.69 0.60DOCSIM 0.45 0.50PMI 0.78 0.77Google TITLES 0.80LOGHITS 0.46Gold-standard IAA 0.79 0.73Table 2: Spearman rank correlation ?
values for thedifferent scoring methods over the NEWS dataset (best-performing method for each resource underlined; best-performing method overall in boldface)topic models of NEWS and BOOKS using T = 200and T = 400 topics respectively.
We randomlyselected a total of 237 topics from the two collec-tions for user scoring.
We asked N = 9 users toscore each of the 237 topics on a 3-point scale where3=?useful?
(coherent) and 1=?useless?
(less coher-ent).We provided annotators with a rubric and guide-lines on how to judge whether a topic was usefulor useless.
In addition to showing several examplesof useful and useless topics, we instructed users todecide whether the topic was to some extent coher-ent, meaningful, interpretable, subject-heading-like,and something-you-could-easily-label.
For our pur-poses, the usefulness of a topic can be thought ofas whether one could imagine using the topic in asearch interface to retrieve documents about a par-ticular subject.
One indicator of usefulness is theease by which one could think of a short label to de-scribe a topic.Table 1 shows a selection of high- and low-scoring topics, as scored by the N = 9 users.
Thefirst topic illustrates the notion of labelling coher-ence, as space exploration, e.g., would be an obvi-ous label for the topic.
The low-scoring topics dis-play little coherence, and one would not expect themResource Method Median MeanWordNetHSO 0.15 0.59JCN ?0.20 0.19LCH ?0.31 ?0.15LESK 0.53 0.53LIN 0.09 0.28PATH 0.29 0.12RES 0.57 0.66VECTOR ?0.08 0.27WUP 0.41 0.26WikipediaRACO 0.62 0.69MIW 0.68 0.70DOCSIM 0.59 0.60PMI 0.74 0.77Google TITLES 0.51LOGHITS ?0.19Gold-standard IAA 0.82 0.78Table 3: Spearman rank correlation ?
values for the dif-ferent scoring methods over the BOOKS dataset (best-performing method for each resource underlined; best-performing method overall in boldface)to be useful as categories or facets in a search inter-face.
Note that the useless topics from both collec-tions are not chance artifacts produced by the mod-els, but are in fact stable and robust statistical fea-tures in the data sets.6 ResultsThe results for the different topic scoring methodsover the NEWS and BOOKS collections are pre-sented in Tables 2 and 3, respectively.
In each ta-ble, we separate out the scoring methods into thosebased on WordNet (from Section 4.1), those basedon Wikipedia (from Section 4.2), and those based onGoogle (from Section 4.3).As stated in Section 4, we experiment with twomethods for combining the word-pair scores (for allmethods other than the two Google methods, whichoperate natively over a word set), namely the arith-metic mean and median.
We present the numbersfor these two methods in each table.
In each case,we evaluate via Spearman rank correlation, revers-ing the sign of the calculated ?
value for PATH (as itis the only instance of a distance metric, where thegold-standard is made up of similarity values).We include the inter-annotator agreement (IAA)in the final row of each table, which we consider106to be the upper bound for the task.
This is calcu-lated as the average Spearman rank correlation be-tween each annotator and the mean/median of theremaining annotators for that topic.
Encouragingly,there is relatively little difference in the IAA be-tween the two datasets; the median-based calcula-tion produces slightly higher ?
values and is empiri-cally the method of choice.3Of all the topic scoring methods tested, PMI(term co-occurrence via simple pointwise mutual in-formation) is the most consistent performer, achiev-ing the best or near-best results over both datasets,and approaching or surpassing the inter-annotatoragreement.
This indicates both that the task oftopic evaluation as defined in this paper is com-putationally tractable, and that word-pair based co-occurrence is highly successful at modelling topiccoherence.Comparing the different resources, Wikipedia isfar and away the most consistent performing, withPMI producing the best results, followed by MIWand RACO, and finally DOCSIM.
There is rela-tively little difference in results between NEWS andBOOKS for the Wikipedia methods.
Google achievesthe best results over NEWS, for TITLES (actuallyslightly above the IAA), but the results fall awaysharply over BOOKS.
The reason for this can beseen in the sample topics in Table 1: the topics forBOOKS tend to be more varied in word class thanfor NEWS, and contain less proper names; also, thegenre of BOOKS is less well represented on the web.We hypothesise that Wikipedia?s encyclopedic na-ture means that it has good coverage over both do-mains, and thus more robust.Turning to WordNet, the overall results aremarkedly better over BOOKS, again largely becauseof the relative sparsity of proper names in the re-source.
The results for individual methods are some-what surprising.
Whereas JCN and LCH have beenshown to be two of the best-performing methodsover lexical similarity tasks (Budanitsky and Hirst,2005; Agirre et al, 2009), they perform abysmallyat the topic scoring task.
Indeed, the spread of re-sults across the WordNet similarity methods (no-3Note that the choice of mean or median for IAA is in-dependent of that for the scoring methods, as they are com-bining different things: annotator scores in the one hand, andword/concept pair scores on the other.tably HSO, JCN, LCH, LIN, RES and WUP) ismuch greater than we had expected.
The single mostconsistent method is LESK, which is based on lexi-cal overlap in definition sentences and makes rela-tively modest use of the WordNet hierarchy.
Supple-mentary evaluation where we filtered out all propernouns from the topics (based on simple POS priorsfor each word learned from an automatically-taggedversion of the British National Corpus) led to a slightincrease in results for the WordNet methods; the fullresults are omitted for reasons of space.
In futurework, we intend to carry out error analysis to deter-mine why some of the methods performed so badly,or inconsistently across the two datasets.There is no clear answer to the question ofwhether the mean or median is the best method forcombining the pair-wise scores.7 ConclusionsWe have proposed the novel task of topic coher-ence evaluation as a form of intrinsic topic evalu-ation with relevance in document search/discoveryand visualisation applications.
We constructeda gold-standard dataset of topic coherence scoresover the output of a topic model for two distinctdatasets, and evaluated a wide range of topic scor-ing methods over this dataset, drawing on WordNet,Wikipedia and the Google search engine.
The sin-gle best-performing method was term co-occurrencewithin Wikipedia based on pointwise mutual infor-mation, which achieve results very close to the inter-annotator agreement for the task.
Google was alsofound to perform well over one of the two datasets,while the results for the WordNet-based methodswere overall surprisingly low.AcknowledgementsNICTA is funded by the Australian government as rep-resented by Department of Broadband, Communicationand Digital Economy, and the Australian Research Coun-cil through the ICT centre of Excellence programme.
DNhas also been supported by a grant from the Institute ofMuseum and Library Services, and a Google ResearchAward.ReferencesE Agirre, E Alfonseca, K Hall, J Kravalova, M Pas?ca,and A Soroa.
2009.
A study on similarity and re-107latedness using distributional and WordNet-based ap-proaches.
In Proc.
of HLT: NAACL 2009, pages 19?27, Boulder, Colorado.S Banerjee and T Pedersen.
2002.
An adapted Lesk algo-rithm for word sense disambiguation using WordNet.Proc.
of CICLing?02, pages 136?145.DM Blei, AY Ng, and MI Jordan.
2003.
Latent Dirich-let alocation.
Journal of Machine Learning Research,3:993?1022.S Brody and M Lapata.
2009.
Bayesian word senseinduction.
In Proc.
of EACL 2009, pages 103?111,Athens, Greece.A Budanitsky and G Hirst.
2005.
Evaluating WordNet-based Measures of Lexical Sematic Relatedness.Computational Linguistics, 32(1):13?47.WL Buntine and A Jakulin.
2004.
Applying discretePCA in data analysis.
In Proc.
of UAI 2004, pages59?66.J Chang, J Boyd-Graber, S Gerris, C Wang, and D Blei.2009.
Reading tea leaves: How humans interpret topicmodels.
In Proc.
of NIPS 2009.H Daume III.
2009.
Non-parametric bayesian areal lin-guistics.
In Proc.
of HLT: NAACL 2009, pages 593?601, Boulder, USA.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society of Information Science, 41(6).C Fellbaum, editor.
1998.
WordNet: An Electronic Lexi-cal Database.
MIT Press, Cambridge, USA.E Gabrilovich and S Markovitch.
2007.
Computing se-mantic relatedness using Wikipedia-based explicit se-mantic analysis.
In Proc.
of IJCAI?07, pages 1606?1611, Hyderabad, India.T Griffiths and M Steyvers.
2004.
Finding scientific top-ics.
In Proc.
of the National Academy of Sciences, vol-ume 101, pages 5228?5235.T Griffiths and M Steyvers.
2006.
Probabilistic topicmodels.
In Latent Semantic Analysis: A Road toMeaning.A Haghighi and L Vanderwende.
2009.
Exploring con-tent models for multi-document summarization.
InProc.
of HLT: NAACL 2009, pages 362?370, Boulder,USA.G Hirst and D St-Onge.
1998.
Lexical chains as repre-sentations of context for the detection and correctionof malapropism.
In Fellbaum (Fellbaum, 1998), pages305?332.T Hofmann.
2001.
Unsupervised learning by proba-bilistic latent semantic analysis.
Machine Learning,42(1):177?196.JJ Jiang and DW Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProc.
of COLING?97, pages 19?33, Taipei, Taiwan.C Leacock, G A Miller, and M Chodorow.
1998.
Usingcorpus statistics and WordNet relations for sense iden-tification.
Computational Linguistics, 24(1):147?65.M Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
In Proc.
of SIGDOC?86,pages 24?26, Toronto, Canada.D Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In Proc.
of COLING/ACL?98, pages 768?774, Montreal, Canada.C-Y Lin.
2004.
ROUGE: a package for automaticevaluation of summaries.
In Proc.
of the ACL 2004Workshop on Text Summarization Branches Out (WAS2004), pages 74?81, Barcelona, Spain.Q Mei, X Shen, and CX Zhai.
2007.
Automatic labelingof multinomial topic models.
In Proc.
of KDD 2007,pages 490?499.D Milne and IH Witten.
2008.
An effective, low-cost measure of semantic relatedness obtained fromWikipedia links.
In Proc.
of AAAI Workshop onWikipedia and Artificial Intelligence, pages 25?30,Chicago, USA.H Misra, O Cappe, and F Yvon.
2008.
Using LDA todetect semantically incoherent documents.
In Proc.
ofCoNLL 2008, pages 41?48, Manchester, England.D Newman, S Karimi, and L Cavedon.
2009.
Externalevaluation of topic models.
In Proc.
of ADCS 2009,pages 11?18, Sydney, Australia.D Newman, T Baldwin, L Cavedon, S Karimi, D Mar-tinez, and J Zobel.
to appeara.
Visualizing docu-ment collections and search results using topic map-ping.
Journal of Web Semantics.D Newman, Y Noh, E Talley, S Karimi, and T Bald-win.
to appearb.
Evaluating topic models for digitallibraries.
In Proc.
of JCDL/ICADL 2010, Gold Coast,Australia.K Papineni, S Roukos, T Ward, and W-J Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL 2002, pages 311?318,Philadelphia, USA.P Pecina.
2008.
Lexical Association Measures: Colloca-tion Extraction.
Ph.D. thesis, Charles University.P Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxonomy.
In Proc.
of IJ-CAI?95, pages 448?453, Montreal, Canada.H Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?123.M Stru?be and SP Ponzetto.
2006.
WikiRelate!
comput-ing semantic relateness using Wikipedia.
In Proc.
ofAAAI?06, pages 1419?1424, Boston, USA.Q Sun, R Li, D Luo, and X Wu.
2008.
Text segmentationwith LDA-based Fisher kernel.
In Proc.
of ACL-08:HLT, pages 269?272.HM Wallach, I Murray, R Salakhutdinov, andDM Mimno.
2009.
Evaluation methods fortopic models.
In Proc.
of ICML 2009, page 139.D Widdows and K Ferraro.
2008.
Semantic Vectors:A scalable open source package and online technol-ogy management application.
In Proc.
of LREC 2008,Marrakech, Morocco.Z Wu and M Palmer.
1994.
Verb selection and lexicalselection.
In Proc.
of ACL?94, pages 133?138, LasCruces, USA.108
