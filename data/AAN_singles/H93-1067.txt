PROSODY/PARSE SCORINGAND ITS APPL ICATION iN  ATISN.
M. Veilleuz M. OstendorfElectrical, Computer and Systems EngineeringBoston University, Boston, MA 02215ABSTRACTProsodic patterns provide important cues for resolving syn-tactic ambiguity, and might he used to improve the accu-racy of automatic speech understanding.
With this goal, wepropose a method of scoring syntactic parses in terms of ob-served prosodic ues, which can be used in ranking sentencehypotheses and associated parses.
Specifically, the score isthe probability of acoustic features of a hypothesized wordsequence given an associated syntactic parse, based on acous-tic and "language" (prosody/syntax) models that representprobabilities in terms of abstract prosodic labeis.
This workreports initial efforts aimed at extending the algorithm tospontaneous peech, specifically the ATIS task, where theprosody/parse core is shown to improve the average rank ofthe correct sentence hypothesis.1.
INTRODUCTIONHuman listeners bring several sources of informationto bear in interpreting an utterance, including syn-tax, semantics, discourse, pragmatics and prosodic ues.Prosody, in particular, provides information about syn-tactic structure (via prosodic onstituent s ructure) andinformation focus (via phrasal prominence), and is en-coded in the acoustic signal in terms of timing, energyand intonation patterns.
Since computer knowledge r p-resentations are not as sophisticated as human knowl-edge, utterances that are straightforward fora human tointerpret may be "ambiguous" to an automatic speechunderstanding system.
For this reason, it is useful toinclude as many knowledge sources as possible in auto-matic speech understanding, and prosody is currently anuntapped resource.
In fact, some syntactic ambiguitiescan be resolved by listeners from prosody alone \[1\].One way to incorporate prosody in speech understand-ing is to score the expected prosodic structure for eachcandidate sentence hypothesis and syntactic parse in re-lation to the observed prosodic structure.
In a speechunderstanding system where multiple sentence hypothe-ses are passed from recognition to natural language pro-cessing, the prosody/parse core could be used to rankhypotheses and associated parses, directly or in combina-tion with other scores.
The parse scoring approach wasproposed in previous work \[2\], where automatically de-tected prosodic phrase breaks were scored either in termsof their correlation with prosodic structure predictedfrom parse information or in terms of their likelihoodaccording to a probabilistic prosody/syntax model.
Re-cently, the parse scoring approach was reformulated \[3\]to avoid explicit recognition of prosodic patterns, whichis a sub-optimal intermediate d cision.
Specifically, thenew score is the probability of a hypothesized word se-quence and associated syntactic parse given acoustic fea-tures, where both an acoustic model and a "language"(prosody/syntax) model are used to represent the proba-bility of utterance, analogous to speech recognition tech-niques.
The parse scoring formalism was also extendedto incorporate phrasal prominence information, in ad-dition to phrase breaks.
In previous work, we demon-strated the feasibility of using parse scoring to find thecorrect interpretation in a corpus of professionally readambiguous sentences.
In this work, we use the parse scor-ing approach to rerank a speech understanding system'sN-best output, specifically in the ATIS task domain, inorder to improve sentence understanding accuracy.In the following section, we describe the parsescoring system and the probabilistic acoustic andprosody/syntax models.
Next, we discuss issues thatarose in extending the parse scoring algorithm tothe ATIS task, including several modifications neededto handle new problems associated with spontaneousspeech and the new parser and recognizer.
We thenpresent experimental results for the task of rerankingthe top N recognizer hypotheses and associated parsesusing prosody/parse cores.
Finally, we discuss the im-plications of the results for future work.2.
PARSE SCORING2.1.
Genera l  Formal i smThe goal of this work is to reorder the set of N-best recog-nizer hypotheses by ranking each hypothesis and associ-ated parse in terms of a prosody score.
More specifically,the prosody-parse core is the probability of a sequenceof acoustic observations x = {z l , .
.
.
,  zn} given the hy-pothesized parse, p(x\[parse), where x is a sequence of335duration and fO measurements associated with the rec-ognizer output.
We compute this probability using anintermediate phonological representation f a sequenceof abstract prosodic labels a = {al .
.
.
.
, an}:p(xlparse) = ~ p(x\[a)p(alparse).
(1)aThis representation implies the development of two prob-abillstic models: an acoustic model of prosodic patterns,p(x\[a), and a model of the relationship between prosodyand syntax p(alpaxse), analogous to a language model inspeech recognition.The general formalism can accommodate many typesof abstract labels in the prosodic pattern sequence a.Here, the prosodic labeling scheme is an extension ofthat proposed in \[1\] and includes integer break indices,one for each word to indicate prosodic onstituent s ruc-ture, and a binary indicator of presence vs. absence ofprominence on every syllable.
Thus, the prosodic la-bel sequence is given by a = (b, p), where b representsthe break sequence and p represents he prominence se-quence.
To simplify the current implementation, weas-sume b and p are independent.
This assumption impliesthe use of two acoustic models, p(xlb) and p(xlp), andtwo prosody/syntax models, p(blparse ) and p(plparse).
(Relaxation of the independence assumption is discussedin Section 5.
)Both the acoustic and prosody/syntax models make useof (different) binary decision trees.
A binary decisiontree \[4\] is an ordered sequence of binary questions thatsuccessively split the data, ultimately into sets associ-ated with the tree's terminal nodes or leaves.
Decisiontrees are particularly useful for prosody applications be-cause they can easily model feature sets with both cat-egorical and continuous variables without requiring in-dependence assumptions.
During training, the sequenceof questions i selected from a specified set to minimizesome impurity criterion on the sample distribution ofclasses in the training data.
For typical classificationproblems, a leaf would then be associated with a classlabel.
In this work, however, leaves are associated withthe posterior distribution of the classes given the leafnode, and the tree can be thought of as "quantizing"the feature vectors.
Here, the classes are either the dif-ferent levels of breaks, one after each word, or the binaryprominence labels, one for each syllable.2.2.
Acoustic ModelThe acoustic models, one for breaks and one for promi-nences, are based on decision trees originally developedfor automatic prosodic labeling \[5, 6\].
The form of thetwo models is essentially the same.
The break model, forexample, represents the probability distribution of thedifferent breaks at a word boundary p(blTAb(Z)), whereTAb(z) is the terminal node of the acoustic break treecorresponding to observation z.
Assuming the observa-tions are conditionally independent given the breaks, theprobability of the observation sequence is given byp(x\[b) = ~Ip(z ,  lb,) = p(b, lTAb(Z,))p(zl)i=1 i=I p(bi)using the decision tree acoustic model.
The probabilityp(x\]p) is computed using a similar formula with a sepa-rate acoustic tree TAp(x) trained to model prominence.The key differences between the two acoustic models arein the labels represented and the acoustic features used.The break model represents several different levels ofbreaks, while the prominence model represents =k promi-nence.
Breaks are associated with words and prominencemarkers are associated with syllables, so the observa-tion sequences for the two models are at the word leveland syllable level, respectively.
Both models rely on fea-tures computed from speech annotated with phone andword boundary markers found during speech recognition.Phonetic segmentations facilitate the use of timing cues,that in this work are based on segment duration ormal-ized according to phone-dependent means and variancesadapted for estimated speaking rate.
The observationvectors used in the break model TAb \[5\] include featuresassociated with normalized phone duration and pauseduration.
The observation vectors used to model promi-nence TAp \[6\] include similar features, as well as F0 andenergy measurements.2.3.
P rosody /Syntax  ModelThe break and prominence prosody/syntax models arealso based on decision trees, in this case originally de-signed for synthesis applications.
Hirschberg and col-leagues have proposed the use of decision trees to predictpresence vs. absence of prosodic breaks \[7\] and of pitchaccents \[8\], with very good results.
Our use of treesfor prosody/syntax models differs from this work, in thenumber of prosodic labels represented, in the use of treesto provide probability distributions rather than classifi-cation labels, and in the use of trees for parse scoringrather than prediction.
Again, the break and promi-nence models share the same basic form.
The leavesof the prosody/syntax break tree Tab, for example, areassociated with a probability distribution of the breaksgiven the syntactic feature vector zi, p(blTsb(Zi)).
Theseprobabilities are used directly in computing p(blparse ),assuming the breaks are conditionally independent given336the quantized features Tsb(zi):np(b\[parse) = H p(bi\[Tsb(Zi)).i=1Again, the probability p(plpar'~) can be computed usingthe same approach but with a separate prosody/syntaxprominence tree Tsp.For all prosody/syntax models, the feature vectors usedin the tree are based on part-of-speech tags and syn-tactic bracketing associated with the hypothesized wordsequence.
For the break model Tsb, the feature vec-tors (one for each word) include content/function wordlabels, syntactic onstituent labels at different levels ofbracketing, measures of distance in branches from thetop and the bottom of the syntactic tree, and locationin the sentence in terms of numbers of words.
For theprominence model Tsp \[9\], the feature vectors (one foreach syllable) include part-of-speech labels, lexical stressassignment and syllable position within the word.2.4.
Joint Probability ScoreUsing the acoustic and prosody/syntax models and theindependence assumptions described above, the proba-bility of the acoustic observations x = (x(b), x(p)) givenan hypothesized parse is:p(xlparse) = p(x (b) \[parse)p(x(P)Iparse)where the break models contribute to the termt'lwP(x(b)\[parse) = H p(zi) E p(blTAb(zi))p(blTsb(zi))i=1 b p(b)and the prominence models contribute a similar term.
Ifthe problem is to rank different hypothesized parses forthe same word sequence, i.e., the same observation se-quence x, then the term 1-Ii p(zi) can be neglected.
How-ever, if different observation sequences are being com-pared, as is the case for different recognition hypothe-ses, then an explicit model of the observations is needed.Since the acoustic model readily available to this effortdoes not provide the p(zi) information, we simply nor-malize for differences in the length of the word sequence(nu,) and of the syllable sequence (n,):n~Sj = ~ E logE  p(blTAb(Z,))P(blTsb(zi))nw i=l b p(b)1 n .
+ - -  ~ log ~ P(pITAp(zi))P(PITsp(zi)) (2)n, i=1 p P(P) "The score given by Equation 2 differs from the proba-bilistic score reported in previous work \[2\] primarily inthat it uses the probability of breaks at each word bound-ary rather than a single detected break, but also in thatit incorporates information about phrasal prominence.3.
APPL ICAT ION TO AT ISThe speech corpus is spontaneous speech from the ATIS(Air Travel Information Service) domain, collected byseveral different sites whose efforts were coordinated bythe MADCOW group \[10\].
The ATIS corpus includesspeech from human subjects who were given a set ofair travel planning "scenarios" to solve via spoken lan-guage communication with a computer.
Queries madeby the subjects are classified differently according towhether they are evaluable in isolation (class A), requirecontextual information (class D) or having no canonicaldatabase answer (class X), but these distinctions are ig-nored in our work.
In the ATIS task domain, speechunderstanding performance is measured in terms of re-sponse accuracy with a penalty for incorrect responses,as described in \[11\].
Our experiments will not assessunderstanding accuracy, which is a function of the com-plete speech understanding system, but rather the rankof the correct answer after prosody/parse coring.A subset of the ATIS corpus was hand-labeled withprosodic breaks and prominences for training the acous-tic and prosody/syntax models.
Since the spoken lan-guage systems at the various data collection sites differin their degree of automation, mode of communication,and display, the training subset was selected to representa balanced sample from each of four sites (BBN, CMU,MIT and SRI) and from males and females.
The Octo-ber 1991 test set is used in the experiments reported inSection 4.The prosody/parse coring mechanism was evaluated inthe context of the MIT ATIS system \[12\], which com-municates the top N recognition hypotheses to the nat-ural language component for further processing.
Thespeech recognition component, he SUMMIT system,was used to provide phone alignments for the acousticmodel.
The SUMMIT system uses segment-based acous-tic phone models, a bigram stochastic language modeland a probabilistic left-right parser to provide furtherlinguistic constraints \[12\].
TINA, MIT's natural an-guage component \[13\], interleaves syntactic and task-specific semantic onstraints to parse an utterance.
Asa result, the parse structure captures both syntactic andsemantic onstituents.
For example, parse tree nodesmay be labeled as CITY-NAME or FLIGHT-EVENTrather than with general syntactic labels.
In addition,TINA falls back on a robust parsing mechanism whena complete parse is not found, using a combination ofthe basic parser and discourse processing mechanism ap-337plied within the utterance \[14\].
The robust parser en-ables TINA to handle many more queries, which may bedifficult to parse because they contain complex and/orincomplete syntactic structures, disfluencies, or simplyrecognition errors.
The robust parser assigns constituentstructure to as much of the utterance as possible andleaves the unassigned terminals in the word string, andtherefore generates bracketings with a flatter syntacticstructure than that for a complete parse.In order to port our models and scoring algorithm to theATIS task, the first change needed was a revision to theprosodic labeling system to handle spontaneous speechphenomena.
The changes included the addition of twomarkers introduced in the TOBI prosodic labeling sys-tem \[15\].
First, the diacritic "p" was added to breakindices where needed to indicate that an exceptionallylong pause or lengthening occurs due to hesitation \[15\].As in our previous work, we used a seven level breakindex system to represent levels in a constituent hierar-chy, a superset of the TOBI breaks.
(The binary accentlabels represent a simplification or core subset of theTOBI system.)
The "p" diacritic is used fairly often: on5% of the total breaks, on 14% of the breaks at levels2 and 3, and somewhat more often in utterances thatrequired a robust parse.
In addition, a new intonationalmarker, %r, was added to indicate the beginning of anintonational phrase when the previous phrase did nothave a well-formed terminus, e.g.
in the case of repairsand restarts.
The %r marker was rarely used and there-fore not incorporated in the models.
Two other prosodic"break" labels were added to handle problems that arosein the ATIS corpus: "L" for linking was added for mark-ing the boundaries within a lexical item (e.g.
San LFrancisco) and "X" for cases where the labelers did notwant to mark a word boundary between items (e.g.
af-ter an interrupted word such as fli.).
The different breakmarkers were grouped in the following classes for robustprobability estimates in acoustic modeling: (0,1,L), 2, 3,4-5, 6, (2p,3p), and (4p,5p).
In these experiments, therelatively few sentences with an "X" break were simplyleft out of the training set.Another new problem introduced by the ATIS task wasthe definition of a "word", an important issue becauseprosodic break indices are labeled at each word bound-ary.
The human labelers, the SUMMIT recognition sys-tem and the TINA natural anguage processing systemall used different lexicons, differing on the definition ofa "compound word" (e.g.
air-fare, what-is-the).
Thesedifferences were handled in training by: defining wordboundaries according to the smallest unit marked in anyof the three systems, using the MIT lexicons to associatethe parse and recognition word boundaries, and assign-ing any hand-labeled "L" breaks to "1" where the rec-ognizer or parser indicated a word boundary.
In testing,only the mapping between the recognition and naturallanguage components is needed, and again the smallestword units are chosen.The main changes to the acoustic model in moving tothe ATIS task were associated with the particular phoneinventory used by the SUMMIT system.
The differencesin the phone inventory resulted in some minor changesto the syllabification algorithm (syllable boundaries areneeded for acoustic feature xtraction).
In addition, thephone label set was grouped into classes for estimatingrobust duration means and variances.
We also revisedthe pause duration feature to measure the total durationof all interword symbols.The changes to the prosody/syntax model simply in-volved defining new questions for the decision tree de-sign.
The first change involved introducing new cate-gories of parse tree bracketing labels, in part to handlethe different naming conventions used in TINA and inpart to take advantage of the semantic information pro-vided by TINA.
In addition, new types of questions wereadded to handle cases that included non-branching on-terminals, specifically, questions about the full level ofbracketing and the bracketing defined only by binarybranching non-terminals (i.e., using two definitions ofthe "bottom" of the syntactic tree) and questions aboutthe non-terminal labels at multiple levels.
Because of thedifferences in syntactic structure for word strings associ-ated with a robust parse as opposed to a complete parse,we chose to model the prosody of breaks given a robustparse separately, which is equivalent to forcing the firstbranch of the tree to test for the use of the robust parser.In summary, many changes were necessary in porting thealgorithm to ATIS, some of which were required by thetask of understanding spontaneous speech while otherswere specific to the particular recognizer and parser usedhere.4.
EXPERIMENTSIn the experimental evaluation of theprosody/parse coring algorithm on ATIS, the acousticand prosody/syntax models were trained on the subsetof ATIS utterances that were hand-labeled with prosodicmarkers.
The acoustic model was trained from phoneticalignments provided by the MIT recognizer, where therecognizer output was constrained to match the tran-scribed word sequence.
The prosody/syntax model wastrained from TINA parses of the transcribed word se-quence.For the parse scoring experiments, MIT provided the N338best recognition hypotheses and one parse per hypothe-sis for each utterance in the October 1991 test set.
Thesentence accuracy rate of the top recognition hypothe-sis, before any prosodic or natural anguage processing,was 32%.
We restored the top 10 hypotheses, choos-ing the same number used by the current version of theMIT ATIS system.
185 of 383 utterances (48%) includedthe correct word string in the top 10.
Excluding a fewother sentences because of processing difficulties, a totalof 179 utterances were used in evaluating improvementsin rank due to prosody.
For each sentence hypothesis,we extracted a sequence of acoustic features from thephone alignments and F0 contours and a sequence ofsyntactic features from the associated parse.
Thus, everyutterance yielded ten sequences of acoustic observationvectors and ten associated sequences of parse features,one pair for each of the ten-best hypothesized word se-quences.
Each observation sequence was then scored ac-cording to the syntactic structure of the correspondingparse, yielding p(xilparsei), i = 1 .
.
.
.
,10 for each ut-terance.The prosody/parse score was used as one componentin a linear combination of scores, also including theMIT SUMMIT acoustic score and language model score,which was used to rerank the sentence hypotheses.
Weinvestigated the use of a combined prosody score andseparate break and prominence scores, and separatingthe scores gave slightly better performance.
The weightsin the linear combination are estimated on the October1991 data, using the method reported in \[16\].
(Althoughthis is not a fair test in the sense that we are train-ing the three weights on the test set, our experimentsin recognition i dicate that performance improvementsobtained typically translate to improvements on inde-pendent test sets.)
The acoustic scores were normalizedby utterance length in frames, and the other scores byutterance l ngth in words.
We compared the rankingsof the correct word string for the score combination us-ing only the MIT acoustic and language scores with therankings according to the score combination that alsoused the prosody/parse probability.
The average rankof the correct utterance, for those in the top 10 to be-gin with, moved from 1.87 without he prosody score to1.67 with the prosody score, a gain of about 23% giventhat the best rank is 1.0.
A paired difference test in-dicates that the difference in performance is significant(t~ = 2.47, ~/2 < .005).
In addition, we noticed thatincorporation of the prosody score rarely dropped therank of the correct sentence by more than one, whereasit often improved the rank by more than one.5.
D ISCUSSIONIn summary, we have described a prosody/parse cor-ing criterion based on the probability of acoustic obser-vations given a candidate parse.
The model is generalenough to handle a variety of prosodic labels, though wehave focused here on prosodic breaks and prominences.Motivated by the good results in previous experimentswith this algorithm on professionally read speech, thegoal of this work was to extend the model to spontaneousspeech and evaluate its usefulness in the context of anactual speech understanding system, i.e.
the MIT ATISsystem.
Experimental results indicate that prosody canbe used to improve the ranking of the correct sentenceamong the top N. We expect he improved ranking willtranslate to improved understanding accuracy, thoughclearly this needs to be confirmed in experiments with aspoken language system.There are several alternatives for improving both theacoustic and prosody/syntax models.
In particular, thecurrent score uses a heuristic to account for differencesin observation sequences, which could be better handledby explicitly representing p(xla) rather than the pos-terior probability p(alx ) in the acoustic model.
Otherpossible extensions include relaxation of independenceassumptions, in particular the independence of breaksand prominences, ince other work \[9\] has shown thatbreaks are useful for predicting prominence.
Of course,this would require increased amounts of training dataand somewhat more complex algorithms for computingthe parse score.
Finally, these experiments representinitial efforts in working with the MIT recognizer andparser, and new acoustic and syntactic features mighttake better advantage of the MIT system.The parse scoring algorithm istrained automatically andis in principal easily extensible to other tasks and otherspeech understanding systems.
However, our effort toevaluate the algorithm in the ATIS domain raised someissues associated with portability.
New prosodic labelswere added to accommodate h sitation and disfluencyphenomena observed in spontaneous speech, a problemthat we expect will diminish as prosodic labeling conven-tions converge.
Problems arose due to the differences inthe definition of a "word" among component modules inthe system, which might be addressed by standardizationof lexical representation a d/or by additional changes toprosodic labeling conventions.
Finally, the specific hoiceof questions used in the decision trees was determined inpart by hand to accommodate the output "vocabulary"of the particular ecognizer and parser used.
Thoughthis aspect could be completely automated by creatingstandards for parse trees and recognizer "phone" labels,the use of some hand-tuning ofquestions allows us to op-339timize performance by taking advantage of the featuresof different systems and knowledge of the task domain.Clearly, performance in different spoken language sys-tents will be affected by several factors, including thereliability and level of detail of the parser, the accu-racy of the recognizer, the types of ambiguities in thetask domain and the sophistication of other knowledgesources (e.g.
semantic, discourse) in the system.
We planto explore these issues further by assessing performanceof the algorithm in the SRI ATIS system.
(Of course,it may be that the constrained semantics of the ATIStask make it difficult to assess the potential benefits ofprosodic information.)
Implementation a d evaluationof prosody/parse coring in the two systems hould haveimplications for spoken language system design, and ourinitial work already raises some issues.
In particular,there are cases where prosody could benefit speech un-derstanding, but is not useful unless the natural lan-guage component provides more than one parse for ahypothesized word string, e.g.
for lists of numbers andfor utterances with possible disfluencies.
In addition, itmight be useful to have explicit filled pause models usedin recognition (a capability available in some versions ofthe MIT system that was not used in this experiment),to help distinguish esitations (marked by the "p" dia-critic) from well-formed prosodic boundaries.In conclusion, we emphasize that these experiments rep-resent initial efforts at integrating prosody in speech un-derstanding and there is clearly much more work to bedone in this area.
In addition to improving the basiccomponents of the model and evaluating more parse hy-potheses, there are many other possible architecturesthat might be investigated for integrating prosody inspeech understanding.ACKNOWLEDGMENTSThe authors gratefully acknowledge: C. Wightman forthe use of his acoustic models; K. Ross for his promi-nence prediction model; E. Shriberg, K. Hunicke-Smith,C.
Fong and M. Hendrix for help with prosodic labeling;and L. Hirschman, M. Phillips and S. Senefffor providingthe MIT recognizer and parser outputs as well as manyhelpful discussions about the features/format of he MITSUMMIT and TINA systems.
This research was jointlyfunded by NSF and DARPA under NSF grant no.
IRI-8905249.References1.
P. Price, M. Ostendorf, S. Shattuck-Hufnagel, & C.Fong, "The Use of Prosody in Syntactic Disambigua-tion" J. of the Acoust.
Society of America 90, 6,pp.
2956-2970, 1991.2.
M. Ostendorf, C. Wightman, and N. Veilleux, "ParseScoring with Prosodic Information: An Analy-sis/Synthesis Approach," Computer Speech and Lan-guage, to appear 1993.3.
N. VeiUeux and M. Ostendorf, "Probabilistic ParseScoring with Prosodic Information," Proc.
of the In-ter.
Conf.
on Acoustics, Speech and Signal Processing,pp.
II51-54, 1993.4.
L. Breiman, J. Friedman, R. Olshen, and C. Stone,Classification and Regression Trees, Wadsworth andBrooks/Cole Advanced Books and Software, Monterey,CA, 1984.5.
C. Wightman and M. Ostendorf, "Automatic Recogni-tion of Prosodic Phrases," Proc.
of the Inter.
Conf.
onAcoustics, Speech and Signal Processing, pp.
321-324,1991.6.
C. Wightman and M. Ostendorf, "Automatic Recogni-tion of Intonation Features," Proe.
of the Inter.
Con\].on Acoustics, Speech and Signal Processing, pp.
221-224,1992.7.
M. Wang and J. Hirschberg, "Automatic classification ofintonational phrase boundaries," Computer Speech andLanguage, 6-2, pp.
175-196, 1992.8.
J. Hirschberg, "Pitch Accent in Context: PredictingProminence from Text," Artificial Intelligence, to ap-pear.9.
K. Ross, M. Ostendorf and S. Shattuck-Hufnagel, "Fac-tors Affecting Pitch Accent Placement," Proc.
of theInter.
Conf.
on Spoken Language Processing, pp.
365-368, 1992.10.
L. Hirschman et aL, "Multi-Site Data Collection for aSpoken Language Corpus," Proc.
of the DARPA Work-shop on Speech and Natural Language, pp.
7-14, 1992.11.
D. Pallett et aL, "DARPA February 1992 ATIS Bench-mark Test Results," Proc.
of the DARPA Workshop onSpeech and Natural Language, pp.
15-27, 1992.12.
V. Zue et al, "The MIT ATIS System: February 1992Progress Report," Proc.
of the DARPA Workshop onSpeech and Natural Language, pp.
84-88, 1992.13.
S. Seneff, "TINA: A Natural Language System for Spo-ken Language Applications," J.
Association for Compu-tational Linguistics, pp.
61-86, March 1992.14.
S. Seneff, "A Relaxation Method for UnderstandingSpontaneous Speech Utterances," Proe.
of the DARPAWorkshop on Speech and Natural Language, pp.
299-304, February 1992.15.
K. Silverman, M. Beckman, J. Pitrelli, M. Oaten-doff, C. Wightman, P. Price, J. Pierrehumbert, and J.Hirschberg, "TOBI: A Standard Scheme for LabelingProsody," Proc.
of the Inter.
Conf.
on Spoken LanguageProcessing, pp.
867-870, Banff, October 1992.16.
M. Ostendorf, A. Kannan, S. Austin, O. Kimball, R.Schwartz and J. R. Rohlicek, "Integration of DiverseRecognition Methodologies Through Reevaluation of N-Best Sentence Hypotheses," Proc.
of the DARPA Work-shop on Speech and Natural Language, February 1991,pp.
83-87.340
