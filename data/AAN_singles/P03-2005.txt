An Adaptive Approach to Collecting Multimodal InputAnurag GuptaUniversity of New South WalesSchool of Computer Science and EngineeringSydney, NSW 2052 Australiaakgu380@cse.unsw.edu.auAbstractMultimodal dialogue systems allow usersto input information in multiple modali-ties.
These systems can handle simultane-ous or sequential composite multimodalinput.
Different coordination schemes re-quire such systems to capture, collect andintegrate user input in different modali-ties, and then respond to a joint interpreta-tion.
We performed a study to understandthe variability of input in multimodal dia-logue systems and to evaluate methods toperform the collection of input informa-tion.
An enhancement in the form of in-corporation of a dynamic time window toa multimodal input fusion module wasproposed in the study.
We found that theenhanced module provides superior tem-poral characteristics and robustness whencompared to previous methods.1 IntroductionA number of multimodal dialogue systems are be-ing developed in the research community.
A com-mon component in these systems is a multimodalinput fusion (MMIF) module which performs thefunctions of collecting the user input supplied indifferent modalities, determining when the user hasfinished providing input, fusing the collected in-formation to create a joint interpretation and send-ing the joint interpretation to a dialogue managerfor reasoning and further processing (Oviatt et.
al.,2000).
A general requirement of the MMIF moduleis to allow flexibility in the user input and to relaxany restrictions on the use of available modalitiesexcept those imposed by the application itself.
Theflexibility and the multiple ways to coordinatemultimodal inputs pose a problem in determining,within a short time period after the last input, that auser has completed his or her turn.
A method, Dy-namic Time Windows, is proposed to address thisissue.
Dynamic Time Windows allows the use ofany modality, in any order and time, with very lit-tle delay in determining the end of a user turn.2 MotivationWhen providing composite multimodal input, i.e.input that needs to be interpreted or combined to-gether for proper understanding, the user has flexi-bility in the timing of those multimodal inputs.Considering two inputs at a time, the user can inputthem either sequentially or simultaneously.
A mul-timodal input may consist of more than two inputs,leading to a large number of composite schemes.MMIF needs to deal with these complex schemesand determine a suitable time when it is mostunlikely to receive any further input and indicatethe end of a user turn.The determination of the end of a user turn be-comes a problem because of the following twoconflicting requirements:1.
For naturalness, the user should not beconstrained by pre-defined interaction re-quirements, e.g.
to speak within a specifiedtime after touching the display.
To allowthis flexibility in the sequential interactionmetaphor, the user can provide coordinatedmultimodal input anytime after providinginput in some modality.
Also each modal-ity has a unique processing time require-ment due to differing resource needs andcapture times e.g.
spoken input takeslonger compared with touch.
The MMIFneeds to consider such delays before send-ing information to a dialogue manager(DM).
These requirements tend to increasethe time to wait for further informationfrom input modalities.2.
Users would expect the system to respondas soon as they complete their input.
Thus,the fusion module should take as little timeas possible before sending the integratedinformation to the dialogue manager.3 The MMIF moduleWe developed a multimodal input fusion moduleto perform a user study.
The MMIF module isbased on the model proposed by Gupta (2003).
TheMMIF receives semantic information in the formof typed feature structures (Carpenter, 1992) fromthe individual modalities.
It combines typed fea-ture structures received from different modalitiesduring a complete turn using an extended unifica-tion algorithm (Gupta et.
al., 2002).
The output is ajoint interpretation of the multimodal input that issent to a DM that can perform reasoning and pro-vide with suitable system replies.3.1 End of turn predictionBased on current approaches, the following meth-ods were chosen to perform an analysis to deter-mine a suitable method for predicting the end of auser turn:1.
Windowing - In this method, after receiv-ing an input, the MMIF waits for a speci-fied time for further input.
After 3 seconds,the collected input is integrated and sent tothe DM.
This is similar to Johnston et.
al.
(2002) who uses a 1 second wait period.2.
Two Inputs - In this method, multimodalinput is assumed to consist of two inputsfrom two modalities.
After inputs fromtwo modalities have been received, the in-tegration process is performed and the re-sult sent to the DM.
A window of 3seconds is used after receiving the first in-put.
(Oviatt et.
al.
1997)3.
Information evaluation - In this method in-tegration is performed after receiving eachinput, and the result is evaluated to deter-mine if the information can be transformedto a command that the system can under-stand.
If transformation is possible, thework of MMIF is deemed complete andthe information is sent to the DM.
In thecase of an incomplete transformation, awindowing technique is used.
This ap-proach is similar to that of Vo and Waibel(1997).4 Use case studyWe used a multimodal in-car navigation system(Gupta et.
al., 2002), developed using the MMIFmodule and a dialogue manager (Thompson andBliss, 2000) to perform this study.
Users can inter-act with a map-based display to get information onvarious locations and driving instructions.
The in-teraction is performed using speech, handwriting,touch and gesture, either simultaneously or sequen-tially.
The system was set-up on a 650MHz com-puter with 256MB of RAM and a touch screen.Figure 1: Multimodal navigation system4.1 Subjects and TaskThe subjects for the study were both male and fe-male in the age group of 25-35.
All the subjectswere working in technical fields and had daily in-teraction with computer-based systems at work.Before using the system, each of the subjects wasbriefed about the tasks they needed to perform andgiven a demonstration of using the system.The tasks performed by the subjects were:?
Dialogue with the system to specify a fewdifferent destinations, e.g.
a gas station, ahotel, an address, etc.
and?
Issue commands to control the map displaye.g.
zoom to a certain area on the map.Some of the tasks could be completed both un-imodally or multimodally, while others requiredmultiple inputs from the same modality, e.g.
pro-viding multiple destinations using touch.
We askedthe users to perform certain tasks in both unimodaland multimodal manner.
The users were free tochoose their preferred mode of interaction for aparticular task.
We observed users?
behavior dur-ing the interaction.
The subjects answered a fewquestions after every interaction on acceptability ofthe system response.
If it was not acceptable, weasked for their preference.4.2 ObservationsThe following observations were made during andafter analysis of the user study based on aggregateresults from using all the three methods of collect-ing multimodal input.MultimodalityThese observations were of critical importance tounderstand the nature of multimodal input.?
Multimodal commands and dialogue usuallyconsisted of two or three segments ofinformation from the modalities.?
Users tried to maintain synchronization be-tween their inputs in multiple modalities byclosely following cross-modal referenceswith the referred object.
Each user preferredeither to speak first and then touch or viceversa almost consistently, implying a pre-ferred interaction style.?
Sometimes it took a long time for some mo-dalities to produce a semantic representationafter capturing information (e.g.
when therewas a long spoken input or when used onlower end machines).
The MMIF moduledid not collect all the inputs in that turn be-cause it received some input after a longtime interval from the previous input(s).User preference?
Users became impatient when the systemdid not respond within a certain time periodand so they tried to re-enter the input whenthe system state was not being displayed tothem.?
During certain stages of interaction, the usercould only interact with the system unimo-dally.
In those cases they preferred that thesystem does not wait.Performance of various schemesThe performance of the various methods to predictthe completion of the user turn depended on thekind of activity the user was performing.
A multi-modal command is defined as multimodal inputthat can be translated to a system action withoutthe need for dialogue, for example, zooming in acertain area of a map.
On the other hand, multimo-dal dialogue involved multi-turn interaction inwhich the user guided the system (or was guidedby the system) to provide information or to per-form some action.?
When a multimodal command was issued,the user preferred the ?information evalua-tion?
and ?two input?
methods.
This was be-cause most of the multimodal commandswere issued using two modalities.
The?Windowing?
method suffered from adelayed response from the system.
The usergot the impression that the system did notcapture their input.?
During multimodal dialogue the perform-ance of the ?two input?
method was poor assometimes a multimodal turn has more thantwo inputs.
Multimodal dialogue usually didnot result in the evaluation of a completecommand so the performance of the ?infor-mation evaluation?
technique was similar tothat of ?Windowing?.Efficiency?
If users acted unimodally, then it took themlonger than the average time required toprovide the same information in multimodalmanner.4.3 MeasurementsSeveral statistical measures were extracted fromthe data collected during the user study.MultimodalityThe total number of user turns was 112.
83% ofthem had multimodal input.
This shows an over-whelming preference for multimodal interaction.This is compared to 86% recorded in (Oviatt et.
al.1997).
95% of the time users used only two mo-dalities in a turn.
Usually there were multiple in-puts in the same modality.
Of the multimodalturns, 75% had only two inputs, and the rest hadmore than 2 inputs.
To provide multimodal input,speech and touch/gesture were used 80% of thetime, handwriting and gesture were used 15% ofthe time and speech and handwriting were used 5%of the time.Temporal analysisDuring multimodal interaction, 45% of inputsoverlapped each other in time, while the remaining55% followed the previous after some delay.
Thisreinforces earlier recordings of 42% simultaneousmultimodal inputs (Oviatt et.
al.
1997).
The aver-age time between the start of simultaneous inputsin two different modalities was 1.5 seconds.
Thisalso matches earlier observations of 1.4 secondslag between the end of pen and start of speech(Oviatt et.
al.
1997).
The average duration of amultimodal turn was 2.5 seconds without includingthe time delay to determine the end of turn.
Theaverage delay to determine the end of user turnduring multimodal interaction was 2.3 secs.EfficiencyWe observed that unimodal commands required18% longer time to issue than multimodal com-mands, implying multimodal input is faster.
Forexample, it is easier to point to a location on a mapusing touch than using speech to describe it.
Along sentence also decreases the probability of rec-ognition.
This compares favorably with observa-tions made in (Oviatt et.
al., 1997) which recordeda 10% faster task performance for multimodal in-teraction.RobustnessWe labeled as errors the cases where the MMIFdid not produce the expected result or when all theinputs were not collected.
In 8% of the observedturns, users tried to repeat their input because ofslow observed response from the system.
In an-other 6% of observed turns, all the input from thatturn was not collected properly.
4% was due to aninput modality taking a long time to process userinput (possibility due to resource shortfall) and theremaining 2% were due to the user taking a longtime between multimodal inputs.5 AnalysisFollowing an analysis of the above observationsand measurements, we came to the followingconclusions:?
Multimodal input is segmented with the usermaking a conscious effort to provide syn-chronization between inputs in multiple mo-dalities.
The synchronization techniqueapplied is unique to every user.
Multimodalinput is likely to have a limited number ofsegments provided in different modalities.?
Processing time can be a key element forMMIF when deploying multimodal interac-tive systems on devices with limited re-sources.?
Knowledge of the availability of currentmodalities and the task at hand can improvethe performance of MMIF.
Based on thecurrent task for which the user has providedinput, different techniques should be appliedto determine the end of user turn.?
Users need to be made aware of the status ofthe MMIF and the modes available to them.A uniform interface design methodologyshould be used, allowing the availability ofall the modalities during all times.?
Timing between inputs in different modali-ties is critical to determine the exact rela-tionship between the referent and thereferred.5.1 Temporal relationshipBased on the observations, a fine-grained classifi-cation of the temporal relationship between userinputs is proposed.
Temporal relationship is de-fined to be the way in which the modalities areused during interaction.
Figure 2 shows the varioustemporal relationships between feature structuresthat are received from the modalities.
A, B, C, D,E, and F are all feature structures and their extentdenotes the capture period.
These relationships willallow for a better prediction of when and whichmodality is likely to be used next by the user.?
Temporally subsumes ?
A feature structureX temporally subsumes another featurestructure Y if all time points of Y are con-tained in X.
In the figure D temporally sub-sumes E.?
Temporally Intersects ?
A feature structureX temporally intersects another featurestructure Y if there is at least one time pointthat is contained in both of them.
However,the end point of X is not contained in Y andthe start point of Y is not contained in X. Inthe figure B and C temporally intersect eachother.?
Temporally Disjoint ?
A feature structureX is temporally disjoint from another featurestructure Y if there are no time points incommon between X and Y.
In the figure, Band F are temporally disjoint.?
Contiguous ?
A feature structure X is con-tiguous with another feature structure Y if Xstarts immediately after Y ends.
The twoevents have no time points in common, butthere is no time point between them.
For ex-ample, in the figure A is contiguous after B.TimeABCDEFFigure 2: Feature structure temporal relationships6 Enhancement to MMIFIt was proposed to augment the MMIF componentwith a wait mechanism that collects informationfrom input modalities and adaptively determinesthe time when no further input is expected.
Thefollowing factors were used during the design ofthe adaptive wait mechanism:1.
If the modality is specialized (i.e.
it is usu-ally used unimodally) then the likelihoodof getting information in another modalityis greatly reduced.2.
If the modality usually occurs in combina-tion with other modalities then the likeli-hood of receiving information in anothermodality is increased.3.
If the number of segments of informationwithin a turn is more than two or threethen the likelihood of receiving further in-formation from other modalities is re-duced.4.
If the duration of information in a certainmodality is greater than usual, it is likelythat the user has provided most of the in-formation in that modality in a unimodalmanner.6.1 Dynamic Time WindowsThe enhanced method is the same as the informa-tion evaluation method except, that instead of thestatic time window, a dynamic time window basedon current input and previous learning is used.Time Window predictionA statistical linear predictor was incorporated intothe MMIF.
This linear predictor provided a dy-namic time window estimate of the time to wait forfurther information.
The linear prediction (see fig-ure 2) was based on statistical averages of the timerequired by a modality i to process information(AvgDuri), the time between modalities i and j be-coming active (AvgTimeDiffi j), etc.
The forwardprediction coefficients (ci and cij) were based onthe predicted modalities to be used or active, thecurrent modality used, and the temporal relation-ship between the predicted and current modality.??
?=+=njiijijniii fAvgTimeDifcAvgDurcTTW1Figure 3: Linear prediction equationBayesian LearningMachine learning techniques were employed tolearn the preferred interaction style of each user.The preferred user interaction style included themost probable modality(s) to be used next and theirtemporal relationship.
Since there is a lot of uncer-tainty in the knowledge of the preferred interactionstyle, a Bayesian network approach to learning wasused.
The nodes in the Bayesian network were thefollowing:a) Modality currently being usedb) Type of current input (i.e.
type of semanticstructure)c) Number of inputs within the current turnd) Time spent since beginning of current turn(this was made discrete in 4 segments)e) Modality to be used nextf) Temporal relationship with the next mo-dalityg) Time in current modality greater than av-erage (true or false)Learning was applied on the network using datacollected during previous user testing.
Learningwas also applied online using data from previoususer turns thus adapting to the current user.7 ResultsThe enhanced module was tested using the datacollected in previous tests and further online tests.The average delay in determining the end of turnreduced to 1.3 secs.
This represents a 40% im-provement on the earlier results.
Also based ononline experiments, with the same users and tasks,the number of times users repeated their input wasreduced to 2% and collection errors reduced to 3%(compared to 8% and 6% respectively).
The im-provement was partly due to the reduced delay inthe determination of the end of the user?s turn andalso due to prediction of the preferred interactionstyle.
It was also observed that the performanceincreased by a further 5% by using online learning.The results demonstrate the effectiveness of theproposed approach to the robustness and temporalperformance of MMIF.8 ConclusionAn MMIF module with Dynamic Time Widowsapplied to an adaptive wait mechanism that canlearn from user?s interaction style improved theinteractivity in a multimodal system.
By predictingthe end of a user turn, the proposed method in-creased the usability of the system by reducingerrors and improving response time.
Future workwill focus on user adaptation and on the user inter-face to make best use of MMIF.ReferencesAnurag Gupta, Raymond Lee and Eric Choi.
2002.Multi-modal Dialogues As Natural User InterfaceFor Automobile Environment.
In Proceedings of Aus-tralian Speech Science and Technology Conference,Melbourne, Australia.Anurag Gupta.
2003.
A Reference Model for Multimo-dal Input Interpretation.
In Proceedings of Confer-ence on Human Factors in Computing Systems(CHI2003), Ft. Lauderale, FL.Michael Johnston, Srinivas Bangalore, Gunaranjan Va-sireddy, Amanda Stent, Patrick Ehlen, MarilynWalker, Steve Whittaker, and Preetam Maloor.
2002.MATCH: An Architecture for Multimodal DialogueSystems.
In proceedings of 40th annual meeting ofAssociation of Computational Linguistics (ACL-02),Philadelphia, pp.
376-383Minh T. Vo and Alex Waibel.
1997.
Modelling andInterpreting Multimodal Inputs: A Semantic Integra-tion Approach.
Carnegie Mellon University Techni-cal Report CMU-CS-97-192.
Pittsburgh, PA.Robert Carpenter.
1992.
The logic of typed featurestructures.
Cambridge University Press, Cambridge.Sharon L. Oviatt, A. DeAngeli, and K. Kuhn.
1997.Integration and synchronization of input modes dur-ing multimodal human-computer interaction.
In Pro-ceedings of Conference on Human Factors inComputing Systems, CHI, ACM Press, NY, pp.
415?422.Sharon L. Oviatt, Phil.
R. Choen, Li Z. Wu, J. Vergo, L.Duncan, Bernard Shum, J. Bers, T. Holzman, TerryWinograd, J. Landay, J. Larson, D. Ferro.
2000.
De-signing the user interface for multimodal speech andpen-based gesture applications: State of the art sys-tems and future research directions.
Human Com-puter Interaction, 15(4), pp.
263-322.Will Thompson and Harry Bliss.
2000.
A DeclarativeFramework for building Compositional Dialog Mod-ules.
In Proceedings of International Conference ofSpeech and Language Processing, Beijing, China.
pp.640 ?
643.
