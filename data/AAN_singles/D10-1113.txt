Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162?1172,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsMeasuring Distributional Similarity in ContextGeorgiana DinuDepartment of Computational LinguisticsSaarland UniversitySaarbru?cken, Germanydinu@coli.uni-sb.deMirella LapataSchool of InformaticsUniversity of EdinburghEdinburgh, UKmlap@inf.ed.ac.ukAbstractThe computation of meaning similarity asoperationalized by vector-based models hasfound widespread use in many tasks rangingfrom the acquisition of synonyms and para-phrases to word sense disambiguation and tex-tual entailment.
Vector-based models are typ-ically directed at representing words in isola-tion and thus best suited for measuring simi-larity out of context.
In his paper we proposea probabilistic framework for measuring sim-ilarity in context.
Central to our approach isthe intuition that word meaning is representedas a probability distribution over a set of la-tent senses and is modulated by context.
Ex-perimental results on lexical substitution andword similarity show that our algorithm out-performs previously proposed models.1 IntroductionThe computation of meaning similarity as op-erationalized by vector-based models has foundwidespread use in many tasks within natural lan-guage processing (NLP).
These range from the ac-quisition of synonyms (Grefenstette, 1994; Lin,1998) and paraphrases (Lin and Pantel, 2001) toword sense disambiguation (Schuetze, 1998), tex-tual entailment (Clarke, 2009), and notably informa-tion retrieval (Salton et al, 1975).The popularity of vector-based models lies intheir unsupervised nature and ease of computation.In their simplest incarnation, these models repre-sent the meaning of each word as a point in ahigh-dimensional space, where each component cor-responds to some co-occurring contextual element(Landauer and Dumais, 1997; McDonald, 2000;Lund and Burgess, 1996).
The advantage of takingsuch a geometric approach is that the similarity ofword meanings can be easily quantified by measur-ing their distance in the vector space, or the cosineof the angle between them.Vector-based models do not explicitly identify thedifferent senses of words and consequently repre-sent their meaning invariably (i.e., irrespective of co-occurring context).
Consider for example the adjec-tive heavy which we may associate with the gen-eral meaning of ?dense?
or ?massive?.
However,when attested in context, heavy may refer to an over-weight person (e.g., She is short and heavy but shehas a heart of gold.)
or an excessive cannabis user(e.g., Some heavy users develop a psychological de-pendence on cannabis.
).Recent work addresses this issue indirectly withthe development of specialized models that repre-sent word meaning in context (Mitchell and Lap-ata, 2008; Erk and Pado?, 2008; Thater et al, 2009).These methods first extract typical co-occurrencevectors representing a mixture of senses and then usevector operations to either obtain contextualized rep-resentations of a target word (Erk and Pado?, 2008)or a representation for a set of words (Mitchell andLapata, 2009).In this paper we propose a probabilistic frame-work for representing word meaning and measuringsimilarity in context.
We model the meaning of iso-lated words as a probability distribution over a set oflatent senses.
This distribution reflects the a priori,out-of-context likelihood of each sense.
Becausesense ambiguity is taken into account directly in the1162vector construction process, contextualized meaningcan be modeled naturally as a change in the origi-nal sense distribution.
We evaluate our approach onword similarity (Finkelstein et al, 2002) and lexicalsubstitution (McCarthy and Navigli, 2007) and showimprovements over competitive baselines.In the remainder of this paper we give a briefoverview of related work, emphasizing vector-basedapproaches that compute word meaning in context(Section 2).
Next, we present our probabilisticframework and different instantiations thereof (Sec-tions 3 and 4).
Finally, we discuss our experimentalresults (Sections 5 and 6) and conclude the paperwith future work.2 Related workVector composition methods construct representa-tions that go beyond individual words (e.g., forphrases or sentences) and thus by default obtainword meanings in context.
Mitchell and Lapata(2008) investigate several vector composition op-erations for representing short sentences (consist-ing of intransitive verbs and their subjects).
Theyshow that models performing point-wise multiplica-tion of component vectors outperform earlier pro-posals based on vector addition (Landauer and Du-mais, 1997; Kintsch, 2001).
They argue that multi-plication approximates the intersection of the mean-ing of two vectors, whereas addition their union.Mitchell and Lapata (2009) further show that theirmodels yield improvements in language modeling.Erk and Pado?
(2008) employ selectional prefer-ences to contextualize occurrences of target words.For example, the meaning of a verb in the presenceof its object is modeled as the multiplication of theverb?s vector with the vector capturing the inverseselectional preferences of the object; the latter arecomputed as the centroid of the verbs that occurwith this object.
Thater et al (2009) improve on thismodel by representing verbs in a second order space,while the representation for objects remains first or-der.
The meaning of a verb boils down to restrictingits vector to the features active in the argument noun(i.e., dimensions with value larger than zero).More recently, Reisinger and Mooney (2010)present a method that uses clustering to pro-duce multiple sense-specific vectors for each word.Specifically, a word?s contexts are clustered to pro-duce groups of similar context vectors.
An aver-age prototype vector is then computed separatelyfor each cluster, producing a set of vectors for eachword.
These cluster vectors can be used to determinethe semantic similarity of both isolated words andwords in context.
In the second case, the distancebetween prototypes is weighted by the probabilitythat the context belongs to the prototype?s cluster.Erk and Pado?
(2010) propose an exemplar-basedmodel for capturing word meaning in context.
Incontrast to the prototype-based approach, no cluster-ing takes place, it is assumed that there are as manysenses as there are instances.
The meaning of a wordin context is the set of exemplars most similar to it.Unlike Reisinger and Mooney (2010) and Erk andPado?
(2010) our model is probabilistic (we repre-sent word meaning as a distribution over a set of la-tent senses), which makes it easy to integrate andcombine with other systems via mixture or productmodels.
More importantly, our approach is concep-tually simpler as we use a single vector representa-tion for isolated words as well as for words in con-text.
A word?s different meanings are simply mod-eled as changes in its sense distribution.
We shouldalso point out that our approach is not tied to a spe-cific sense induction method and can be used withdifferent variants of vector-space models.3 Meaning Representation in ContextIn this section we first describe how we representthe meaning of individual words and then move onto discuss our model of inducing meaning represen-tations in context.Observed Representations Most vector spacemodels in the literature perform computations ona co-occurrence matrix where each row repre-sents a target word, each column a document oranother neighboring word, and each entry theirco-occurrence frequency.
The raw counts are typ-ically mapped into the components of a vector insome space using for example conditional probabil-ity, the log-likelihood ratio or tf-idf weighting.
Un-der this representation, the similarity of word mean-ings can be easily quantified by measuring their dis-tance in the vector space, the cosine of the angle be-tween them, or their scalar product.1163Our model assumes the same type of input data,namely a co-occurrence matrix, where rows corre-spond to target words and columns to context fea-tures (e.g., co-occurring neighbors).
Throughoutthis paper we will use the notation ti with i : 1..Ito refer to a target word and cj with j : 1..J to referto context features.
A cell (i, j) in the matrix rep-resents the frequency of occurrence of target ti withcontext feature cj over a corpus.Meaning Representation over Latent SensesWe further assume that the target words ti i : 1...Ifound in a corpus share a global set of meaningsor senses Z = {zk|k : 1...K}.
And therefore themeaning of individual target words can be describedas a distribution over this set of senses.
More for-mally, a target ti is represented by the following vec-tor:v(ti) = (P(z1|ti), ...,P(zK|ti)) (1)where component P (z1|ti) is the probability ofsense z1 given target word ti, component P (z2|ti)the probability of sense z2 given ti and so on.The intuition behind such a representation is thata target word can be described by a set of core mean-ings and by the frequency with which these are at-tested.
Note that the representation in (1) is notfixed but parametrized with respect to an input cor-pus (i.e., it only reflects word usage as attested inthat corpus).
The senses z1 .
.
.
zK are latent and canbe seen as a means of reducing the dimensionalityof the original co-occurrence matrix.Analogously, we can represent the meaning of atarget word given a context feature as:v(ti, cj) = (P(z1|ti, cj), ...,P(zK|ti, cj)) (2)Here, target ti is again represented as a distributionover senses, but is now modulated by a specific con-text cj which reflects actual word usage.
This distri-bution is more ?focused?
compared to (1); the con-text helps disambiguate the meaning of the targetword, and as a result fewer senses will share mostof the probability mass.In order to create the context-aware representa-tions defined in (2) we must estimate the proba-bilities P (zk|ti, cj) which can be factorized as theproduct of P (ti, zk), the joint probability of target tiand latent sense zk, and P (cj |zk, ti), the conditionalprobability of context cj given target ti and sense zk:P (zk|ti, cj) =P (ti, zk)P (cj |zk, ti)?k P (ti, zk)P (cj |zk, ti)(3)Problematically, the term P (cj |zk, ti) is difficult toestimate since it implies learning a total number ofK ?
I J-dimensional distributions.
We will there-fore make the simplifying assumption that targetwords ti and context features cj are conditionally in-dependent given sense zk:P (zk|ti, cj) ?P (zk|ti)P (cj |zk)?k P (zk|ti)P (cj |zk)(4)Although not true in general, the assumption is rela-tively weak.
We do not assume that words and con-text features occur independently of each other, butonly that they are generated independently given anassigned meaning.
A variety of latent variable mod-els can be used to obtain senses z1 .
.
.
zK and es-timate the distributions P (zk|ti) and P (cj |zk); wegive specific examples in Section 4.Note that we abuse terminology here, as thesenses our models obtain are not lexicographicmeaning distinctions.
Rather, they denote coarse-grained senses or more generally topics attested inthe document collections our model is trained on.Furthermore, the senses are not word-specific butglobal (i.e., shared across all words) and modulatedeither within or out of context probabilistically viaestimating P (zk|ti, cj) and P (zk|ti), respectively.4 ParametrizationsThe general framework outlined above can beparametrized with respect to the input co-occurrencematrix and the algorithm employed for inducing thelatent structure.
Considerable latitude is availablewhen creating the co-occurrence matrix, especiallywhen defining its columns, i.e., the linguistic con-texts a target word is attested with.
These con-texts can be a small number of words surroundingthe target word (Lund and Burgess, 1996; Loweand McDonald, 2000), entire paragraphs, documents(Salton et al, 1975; Landauer and Dumais, 1997)or even syntactic dependencies (Grefenstette, 1994;Lin, 1998; Pado?
and Lapata, 2007).1164Analogously, a number of probabilistic modelscan be employed to induce the latent senses.
Ex-amples include Probabilistic Latent Semantic Anal-ysis (PLSA, Hofmann (2001)), Probabilistic Prin-cipal Components Analysis (Tipping and Bishop,1999), non-negative matrix factorization (NMF, Leeand Seung (2000)), and latent Dirichlet alocation(LDA, Blei et al (2003)).
We give a more detaileddescription of the latter two models as we employthem in our experiments.Non-negative Matrix Factorization Non-negative matrix factorization algorithms approx-imate a non-negative input matrix V by twonon-negative factors W and H , under a givenloss function.
W and H are reduced-dimensionalmatrices and their product can be regarded as acompressed form of the data in V :VI,J ?WI,KHK,J (5)where W is a basis vector matrix and H is an en-coded matrix of the basis vectors in equation (5).Several loss functions are possible, such as meansquared error and Kullback-Leibler (KL) diver-gence.
In keeping with the formulation in Sec-tion 3 we opt for a probabilistic interpretation ofNMF (Gaussier and Goutte, 2005; Ding et al, 2008)and thus minimize the KL divergence between WHand V .min?i,j(Vi,j logVi,jWHi,j?
Vi,j +WHi,j) (6)Specifically, we interpret matrix V asVij = P (ti, cj), and matrices W and H as P (ti, zk)and P (cj |zk), respectively.
We can also ob-tain the following more detailed factorization:P (ti, cj) =?k P (ti)P (zk|ti)P (cj |zk).Le WH denote the factors in a NMF decom-position of an input matrix V and B be a diag-onal matrix with Bkk =?j Hkj .
B?1H gives arow-normalized version of H .
Similarly, givenmatrix WB, we can define a diagonal matrix A,with Aii =?k(WB)ik.
A?1WB row-normalizesmatrix WB.
The factorization WH can now be re-written as:WH=AA?1WBB?1H=A(A?1WB)(B?1H)which allows us to interpret A as P (ti), A?1WBas P (zk|ti) and B?1H as P (cj |zk).
These interpre-tations are valid since the rows of A?1WB and ofB?1H sum to 1, matrix A is diagonal with trace 1because elements in WH sum to 1, and all entriesare non-negative.Latent Dirichlet Allocation LDA (Blei et al,2003) is a probabilistic model of text generation.Each document d is modeled as a distributionover K topics, which are themselves characterizedby distributions over words.
The individual wordsin a document are generated by repeatedly samplinga topic according to the topic distribution and thensampling a single word from the chosen topic.More formally, we first draw the mixing propor-tion over topics ?d from a Dirichlet prior with pa-rameters ?.
Next, for each of the Nd words wdn indocument d, a topic zdn is first drawn from a multi-nomial distribution with parameters ?dn.
The prob-ability of a word token w taking on value i giventhat topic z = j is parametrized using a matrix ?with bij = P (w = i|z = j).
Integrating out ?d?sand zdn?s, gives P (D|?, ?
), the probability of a cor-pus (or document collection):M?d=1?P (?d|?)?
?Nd?n=1?zdnP (zdn|?d)P (wdn|zdn, ?)?
?d?dThe central computational problem in topicmodeling is to obtain the posterior distri-bution P (?, z|w, ?, ?)
of the hidden vari-ables z = (z1, z2, .
.
.
, zN ).
given a docu-ment w = (w1, w2, .
.
.
, wN ).
Although thisdistribution is intractable in general, a varietyof approximate inference algorithms have beenproposed in the literature.
We adopt the Gibbssampling procedure discussed in Griffiths andSteyvers (2004).
In this model, P (w = i|z = j) isalso a Dirichlet mixture (denoted ?)
with symmetricpriors (denoted ?
).We use LDA to induce senses of target wordsbased on context words, and therefore each row tiin the input matrix transforms into a document.
Thefrequency of ti occurring with context feature cj isthe number of times word cj is encountered in the?document?
associated with ti.
We train the LDAmodel on this data to obtain the ?
and ?
distribu-1165tions.
?
gives the sense distributions of each tar-get ti: ?ik = P (zk|ti) and ?
the context-word dis-tribution for each sense zk: ?kj = P (cj |zk).5 Experimental Set-upIn this section we discuss the experiments we per-formed in order to evaluate our model.
We describethe tasks on which it was applied, the corpora usedfor model training and our evaluation methodology.Tasks The probabilistic model presented in Sec-tion 3 represents words via a set of induced senses.We experimented with two types of semantic spacebased on NMF and LDA and optimized parametersfor these models on a word similarity task.
Thelatter involves judging the similarity sim(ti, t?i) =sim(v(ti), v(t?i)) of words ti and t?i out of context,where v(ti) and v(t?i) are obtained from the output ofNMF or LDA, respectively.
In our experiments weused the data set of Finkelstein et al (2002).
It con-tains 353 pairs of words and their similarity scoresas perceived by human subjects.The contextualized representations were nextevaluated on lexical substitution (McCarthy andNavigli, 2007).
The task requires systems to findappropriate substitutes for target words occurring incontext.
Typically, systems are given a set of substi-tutes, and must produce a ranking such that appro-priate substitutes are assigned a higher rank com-pared to non-appropriate ones.
We made use of theSemEval 2007 Lexical Substitution Task benchmarkdata set.
It contains 200 target words, namely nouns,verbs, adjectives and adverbs, each of which occursin 10 distinct sentential contexts.
The total set con-tains 2,000 sentences.
Five human annotators wereasked to provide substitutes for these target words.Table 1 gives an example of the adjective still andits substitutes.Following Erk and Pado?
(2008), we pool togetherthe total set of substitutes for each target word.Then, for each instance the model has to produce aranking for the total substitute set.
We rank the can-didate substitutes based on the similarity of the con-textualized target and the out-of-context substitute,sim(v(ti, cj), v(t?i)), where ti is the target word, cj acontext word and t?i a substitute.
Contextualizingjust one of the words brings higher discriminativepower to the model rather than performing compar-Sentences SubstitutesIt is important to apply theherbicide on a still day, be-cause spray drift can killnon-target plants.calm (5) not-windy (1)windless (1)A movie is a visual docu-ment comprised of a seriesof still images.motionless (3) unmov-ing (2) fixed (1) sta-tionary (1) static (1)Table 1: Lexical substitution data example for the adjec-tive still ; numbers in parentheses indicate the frequencyof the substitute.isons with the target and its substitute embedded inan identical context (see also Thater et al (2010) fora similar observation).Model Training All the models we experimentedwith use identical input data, i.e., a bag-of-wordsmatrix extracted from the GigaWord collection ofnews text.
Rows in this matrix are target words andcolumns are their co-occurring neighbors, within asymmetric window of size 5.
As context words, weused a vocabulary of the 3,000 most frequent wordsin the corpus.1We implemented the classical NMF factorizationalgorithm described in Lee and Seung (2000).
Theinput matrix was normalized so that all elementssummed to 1.
We experimented with four dimen-sions K: [600 ?
1000] with step size 200.
We ranthe algorithm for 150 iterations to obtain factors Wand H which we further processes as described inSection 4 to obtain the desired probability distribu-tions.
Since the only parameter of the NMF modelis the factorization dimension K, we performed twoindependent runs with each K value and averagedtheir predictions.The parameters for the LDA model are the num-ber of topicsK and Dirichlet priors ?
and ?.
We ex-perimented with topics K: [600?
1400], again withstep size 200.
We fixed ?
to 0.01 and tested two val-ues for ?
: 2K (Porteous et al, 2008) and50K (Griffithsand Steyvers, 2004).
We used Gibbs sampling onthe ?document collection?
obtained from the inputmatrix and estimated the sense distributions as de-scribed in Section 4.
We ran the chains for 1000 iter-1The GigaWord corpus contains 1.7B words; we scale downall the counts by a factor of 70 to speed up the computation ofthe LDA models.
All models use this reduced size input data.1166ations and averaged over five iterations [600?1000]at lag 100 (we observed no topic drift).We measured similarity using the scalar prod-uct, cosine, and inverse Jensen-Shannon (IJS) diver-gence (see (7), (8), and (9), respectively):sp(v, w) =< v,w >=?iviwi (7)cos(v, w) =?v, w?||v|| ||w||(8)IJS(v, w) =1JS(v,w)(9)JS(v,w) =12KL(v|m) +12KL(w|m) (10)where m is a shorthand for 12(v + w) andKL the Kullback-Leibler divergence, KL(v|w) =?i vilog(viwi).Among the above similarity measures, the scalarproduct has the most straightforward interpretationas the probability of two targets sharing a commonmeaning (i.e., the sum over all possible meanings).The scalar product assigns 1 to a pair of identi-cal vectors if and only if P (zi) = 1 for some iand P (zj) = 0,?j 6= i.
Thus, only fully disam-biguated words receive a score of 1.
Beyond similar-ity, the measure also reflects how ?focused?
the dis-tributions in question are, as very ambiguous wordsare unlikely to receive high scalar product values.Given a set of context words, we contextualize thetarget using one context word at a time and computethe overall similarity score by multiplying the indi-vidual scores.Baselines Our baseline models for measuring sim-ilarity out of context are Latent Semantic Analysis(Landauer and Dumais, 1997) and a simple seman-tic space without any dimensionality reduction.For LSA, we computed the U?V SVD decompo-sition of the original matrix to rank k = 1000.
Anydecomposition of lower rank can be obtained fromthis by setting rows and columns to 0.
We evaluateddecompositions to ranks K: [200 ?
1000], at each100 step.
Similarity computations were performedin the lower rank approximation matrix U?V , asoriginally proposed in Deerwester et al (1990), andin matrix U which maps the words into the conceptspace.
It is common to compute SVD decomposi-tions on matrices to which prior weighting schemeshave been applied.
We experimented with tf-idfweighting and line normalization.Our second baseline, the simple semantic space,was based on the original input matrix on whichwe applied several weighting schemes such as point-wise mutual information, tf-idf, and line normaliza-tion.
Again, we measured similarity using cosine,scalar product and inverse JS divergence.
In addi-tion, we also experimented with Lin?s (1998) simi-larity measure:lin(v, w) =?i?I(v)?I(w)(vi + wi)?i?I(v) vi +?l?I(w)wi(11)where the values in v and w are point-wise mutualinformation, and I(?)
gives the indices of positivevalues in a vector.Our baselines for contextualized similarity werevector addition and vector multiplication whichwe performed using the simple semantic space(Mitchell and Lapata, 2008) and dimensionalityreduced representations obtained from NMF andLDA.
To create a ranking of the candidate substi-tutes we compose the vector of the target with itscontext and compare it with each substitute vector.Given a set of context words, we contextualize thetarget using each context word at a time and multi-ply the individual scores.Evaluation Method For the word similarity taskwe used correlation analysis to examine the rela-tionship between the human ratings and their cor-responding vector-based similarity values.
We re-port Spearman?s ?
correlations between the simi-larity values provided by the models and the meanparticipant similarity ratings in the Finkelstein et al(2002) data set.
For the lexical substitution task, wecompare the system ranking with the gold standardranking using Kendall?s ?b rank correlation (which isadjusted for tied ranks).
For all contextualized mod-els we defined the context of a target word as thewords occurring within a symmetric context windowof size 5.
We assess differences between models us-ing stratified shuffling (Yeh, 2000).22Given two system outputs, the null hypothesis (i.e., thatthe two predictions are indistinguishable) is tested by randomlymixing the individual instances (in our case sentences) of thetwo outputs.
We ran a standard number of 10000 iterations.1167Model Spearman ?SVS 38.35LSA 49.43NMF 52.99LDA 53.39LSAMIX 49.76NMFMIX 51.62LDAMIX 51.97Table 2: Results on out of context word similarity usinga simple co-occurrence based vector space model (SVS),latent semantic analysis, non-negative matrix factoriza-tion and latent Dirichlet alocation as individual modelswith the best parameter setting (LSA, NMF, LDA) and asmixtures (LSAMIX, NMFMIX, LDAMIX).6 ResultsWord Similarity Our results on word similar-ity are summarized in Table 2.
The simple co-occurrence based vector space (SVS) performed bestwith tf-idf weighting and the cosine similarity mea-sure.
With regard to LSA, we obtained best re-sults with initial line normalization of the matrix,K = 600 dimensions, and the scalar product sim-ilarity measure while performing computations inmatrix U .
Both NMF and LDA models are generallybetter with a larger number of senses.
NMF yieldsbest performance with K = 1000 dimensions andthe scalar product similarity measure.
The best LDAmodel also uses the scalar product, has K = 1200topics, and ?
set to 50K .Following Reisinger and Mooney (2010), we alsoevaluated mixture models that combine the outputof models with varying parameter settings.
For bothNMF and LDA we averaged the similarity scores re-turned by all runs.
For comparison, we also presentan LSA mixture model over the (best) middle in-terval K values.
As can be seen, the LSA modelimproves slightly, whereas NMF and LDA performworse than their best individual models.3 Overall,we observe that NMF and LDA yield significantly(p < 0.01) better correlations than LSA and the sim-3It is difficult to relate our results to Reisinger and Mooney(2010), due to differences in the training data and the vector rep-resentations it gives rise to.
As a comparison, a baseline config-uration with tf-idf weighting and the cosine similarity measureyields a correlation of 0.38 with our data and 0.49 in Reisingerand Mooney (2010).Model Kendall?s ?bSVS 11.05Add-SVS 12.74Add-NMF 12.85Add-LDA 12.33Mult-SVS 14.41Mult-NMF 13.20Mult-LDA 12.90Cont-NMF 14.95Cont-LDA 13.71Cont-NMFMIX 16.01Cont-LDAMIX 15.53Table 3: Results on lexical substitution using a simplesemantic space model (SVS), additive and multiplicativecompositional models with vector representations basedon co-occurrences (Add-SVS, Mult-SVS), NMF (Add-NMF, Mult-NMF), and LDA (Add-LDA, Mult-LDA) andcontextualized models based on NMF and LDA with thebest parameter setting (Cont-NMF, Cont-LDA) and asmixtures (Cont-NMFMIX, Cont-LDAMIX).ple semantic space, both as individual models and asmixtures.Lexical Substitution Our results on lexical sub-stitution are shown in Table 3.
As a baseline wealso report the performance of the simple semanticspace that does not use any contextual information.This model returns the same ranking of the substi-tute candidates for each instance, based solely ontheir similarity with the target word.
This is a rel-atively competitive baseline as observed by Erk andPado?
(2008) and Thater et al (2009).We report results with contextualized NMF andLDA as individual models (the best word similar-ity settings) and as mixtures (as described above).These are in turn compared against additive andmultiplicative compositional models.
We imple-mented an additive model with pmi weighting andLin?s similarity measure which is defined in an ad-ditive fashion.
The multiplicative model uses tf-idf weighting and cosine similarity, which involvesmultiplication of vector components.
Other combi-nations of weighting schemes and similarity mea-sures delivered significantly lower results.
We alsoreport results for these models when using the NMFand LDA reduced representations.1168Model Adv Adj Noun VerbSVS 22.47 14.38 09.52 7.98Add-SVS 22.79 14.56 11.59 10.00Mult-SVS 22.85 16.37 13.59 11.60Cont-NMFMIX 26.13 17.10 15.16 14.18Cont-LDAMIX 21.21 16.00 16.31 13.67Table 4: Results on lexical substitution for different partsof speech with a simple semantic space model (SVS), twocompositional models (Add-SVS, Mult-SVS), and con-textualized mixture models with NMF and LDA (Cont-NMFMIX, Cont-LDAMIX), using Kendall?s ?b correlationcoefficient.All models significantly (p < 0.01) outperformthe context agnostic simple semantic space (seeSVS in Table 3).
Mixture NMF and LDA mod-els are significantly better than all variants of com-positional models (p < 0.01); the individual mod-els are numerically better, however the differenceis not statistically significant.
We also find that themultiplicative model using a simple semantic space(Mult-SVS) is the best performing compositionalmodel, thus corroborating the results of Mitchell andLapata (2009).
Interestingly, dimensionality compo-sitional models.
This indicates that the better resultswe obtain are due to the probabilistic formulation ofour contextualized model as a whole rather than theuse of NMF or LDA.
Finally, we observe that theCont-NMF model is slightly better than Cont-LDA,however the difference is not statistically significant.To allow comparison with previous results re-ported on this data set, we also used the General-ized Average Precision (GAP, Kishida (2005)) as anevaluation measure.
GAP takes into account the or-der of candidates ranked correctly by a hypotheticalsystem, whereas average precision is only sensitiveto their relative position.
The best performing mod-els are Cont-NMFMIX and Cont-LDAMIX obtaininga GAP of 42.7% and 42.9%, respectively.
Erk andPado?
(2010) report a GAP of 38.6% on this data setwith their best model.Table 4 shows how the models perform across dif-ferent parts of speech.
While verbs and nouns seemto be most difficult, we observe higher gains fromthe use of contextualized models.
Cont-LDAMIXobtains approximately 7% absolute gain for nounsand Cont-NMFMIX approximately 6% for verbs.
AllSenses Word DistributionsTRAFFIC (0.18) road, traffic, highway, route, bridgeMUSIC (0.04) music, song, rock, band, dance, playFAN (0.04) crowd, fan, people, wave, cheer, streetVEHICLE (0.04) car, truck, bus, train, driver, vehicleTable 5: Induced senses of jam and five most likely wordsgiven these senses using an LDA model; sense probabili-ties are shown in parentheses.contextualized models obtain smaller improvementsfor adjectives.
For adverbs most models do not im-prove over the no-context setting, with the exceptionCont-NMFMIX.Finally, we also qualitatively examined how thecontext words influence the sense distributions oftarget words using examples from the lexical sub-stitution dataset and the output of an individualCont-LDA model.
In many cases, a target wordstarts with a distribution spread over a larger numberof senses, while a context word shifts this distribu-tion to one majority sense.
Consider, for instance,the target noun jam in the following sentence:(1) With their transcendent, improvisational jamsand Mayan-inspired sense of a higher, meta-physical purpose, the band?s music delivers aspiritual sustenance that has earned them a verydevoted core following.Table 5 shows the out-of-context senses activatedfor jam together with the five most likely words as-sociated with them.4 Sense probabilities are alsoshown in parentheses.
As can be seen, initially twotraffic-related and two music-related senses are acti-vated, however with low probabilities.
In the pres-ence of the context word band, we obtain a muchmore ?focused?
distribution, in which the MUSICsense has 0.88 probability.
The system ranks riffand gig as the most likely two substitutes for jam.The gold annotation also lists session as a possiblesubstitute.In a large number of cases, the target is only par-tially disambiguated by a context word and this isalso reflected in the resulting distribution.
An ex-4Sense names are provided by the authors in an attempt tobest describe the clusters (i.e., topics for LDA) to which wordsare assigned.1169ample is the word bug which initially has a distribu-tion triggering the SOFTWARE (0.09, computer, soft-ware, microsoft, windows) and DISEASE (0.06, dis-ease, aids, virus, cause) senses.
In the context ofclient, bug remains ambiguous between the sensesSECRET-AGENCY (0.34, agent, secret, intelligence,FBI) ) and SOFTWARE (0.29):(2) We wanted to give our client more than just alist of bugs and an invoice ?
we wanted toprovide an audit trail of our work along withmeaningful productivity metrics.There are also cases where the contextualized dis-tributions are not correct, especially when senses aredomain specific.
An example is the word functionoccurring in its mathematical sense with the contextword distribution.
However, the senses that are trig-gered by this pair all relate to the ?service?
sense offunction.
This is a consequence of the newspapercorpus we use, in which the mathematical sense offunction is rare.
We also see several cases wherethe target word and one of the context words are as-signed senses that are locally correct, but invalid inthe larger context.
In the following example:(3) Check the shoulders so it hangs well, stops athips or below, and make sure the pants are longenough.The pair (check, shoulder) triggers senses IN-JURY (0.81, injury, left, knee, shoulder) andBALL-SPORTS (0.10, ball, shot, hit, throw).
How-ever, the sentential context ascribes a meaning thatis neither related to injury nor sports.
This suggeststhat our models could benefit from more principledcontext feature aggregation.Generally, verbs are not as good context wordsas nouns.
To give an example, we often encounterthe pair (let, know), used in the common ?inform?meaning.
The senses we obtain for this pair, are,however, rather uninformative general verb classes:{see, know, think, do} (0.57) and {go, say, do,can} (0.20).
This type of error can be eliminated ina space where context features are designed to bestreflect the properties of the target words.7 ConclusionsIn this paper we have presented a general frame-work for computing similarity in context.
Key in thisframework is the representation of word meaning asa distribution over a set of global senses where con-textualized meaning is modeled as a change in thisdistribution.
The approach is conceptually simple,the same vector representation is used for isolatedwords and words in context without being tied to aspecific sense induction method or type of semanticspace.We have illustrated two instantiations of thisframework using non-negative matrix factorizationand latent Dirichlet alocation for inducing the la-tent structure, and shown experimentally that theyoutperform previously proposed methods for mea-suring similarity in context.
Furthermore, both ofthem benefit from mixing model predictions over aset of different parameter choices, thus making pa-rameter tuning redundant.The directions for future work are many and var-ied.
Conceptually, we have defined our model in anasymmetric fashion, i.e., by stipulating a differencebetween target words and contextual features.
How-ever, in practice, we used vector representations thatdo not distinguish the two: target words and con-textual features are both words.
This choice wasmade to facilitate comparisons with the popular bag-of-words vector space models.
However, differen-tiating target from context representations may bebeneficial particularly when the similarity compu-tations are embedded within specific tasks such asthe acquisition of paraphrases, the recognition of en-tailment relations, and thesaurus construction.
Alsonote that our model currently contextualizes targetwords with respect to individual contexts.
Ideally,we would like to compute the collective influence ofseveral context words on the target.
We plan to fur-ther investigate how to select or to better aggregatethe entire set of features extracted from a context.Acknowledgments The authors acknowledge thesupport of the DFG (Dinu; International Re-search Training Group ?Language Technology andCognitive Systems?)
and EPSRC (Lapata; grantGR/T04540/01).1170ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Daoud Clarke.
2009.
Context-theoretic semantics fornatural language: an overview.
In Proceedings ofthe Workshop on Geometrical Models of Natural Lan-guage Semantics, pages 112?119, Athens, Greece.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas, and Richard Harshman.
1990.
Indexing bylatent semantic analysis.
Journal of the American So-ciety for Information Science, 41:391?407.Chris Ding, Tao Li, and Wei Peng.
2008.
On the equiv-alence between non-negative matrix factorization andprobabilistic latent semantic indexing.
ComputationalStatistics & Data Analysis, 52(8):3913?3927.Katrin Erk and Sabastian Pado?.
2008.
A structured vec-tor space model for word meaning in context.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 897?906,Honolulu, Hawaii.Katrin Erk and Sebastian Pado?.
2010.
Exemplar-basedmodels for word meaning in context.
In Proceedingsof the ACL 2010 Conference Short Papers, pages 92?97, Uppsala, Sweden.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing search in context: the conceptrevisited.
ACM Transactions on Information Systems,20(1):116?131.Eric Gaussier and Cyril Goutte.
2005.
Relation betweenPLSA and NMF and implications.
In Proceedings ofthe 28th Annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 601?602, New York, NY.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publishers.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101(Suppl.
1):5228?5235.Thomas Hofmann.
2001.
Unsupervised learning byprobabilistic latent semantic analysis.
Machine Learn-ing, 41(2):177?196.Walter Kintsch.
2001.
Predication.
Cognitive Science,25:173?202.Kazuaki Kishida.
2005.
Property of average precisionand its generalization: An examination of evaluationindicator for information retrieval experiments.
NIITechnical Report.Thomas K. Landauer and Susan T. Dumais.
1997.
A so-lution to Plato?s problem: The latent semantic analysistheory of acquisition, induction and representation ofknowledge.
Psychological Review, 104(2):211?240.Daniel D. Lee and H. Sebastian Seung.
2000.
Algo-rithms for non-negative matrix factorization.
In NIPS,pages 556?562.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural LanguageEngineering, 7(4):342?360.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the joint AnnualMeeting of the Association for Computational Linguis-tics and International Conference on ComputationalLinguistics, pages 768?774, Montre?al, Canada.Will Lowe and Scott McDonald.
2000.
The direct route:Mediated priming in semantic space.
In Proceedingsof the 22nd Annual Conference of the Cognitive Sci-ence Society, pages 675?680, Philadelphia, PA.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instruments,and Computers, 28:203?208.Diana McCarthy and Roberto Navigli.
2007.
SemEval-2007 Task 10: English Lexical Substitution Task.
InProceedings of SemEval, pages 48?53, Prague, CzechRepublic.Scott McDonald.
2000.
Environmental Determinants ofLexical Processing Effort.
Ph.D. thesis, University ofEdinburgh.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, pages 236?244, Columbus, Ohio.Jeff Mitchell and Mirella Lapata.
2009.
Language mod-els based on semantic composition.
In Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 430?439, Suntec, Singa-pore.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Ian Porteous, David Newman, Alexander Ihler, ArthurAsuncion, Padhraic Smyth, and Max Welling.
2008.Fast collapsed gibbs sampling for latent Dirichlet alo-cation.
In Proceeding of the 14th ACM SIGKDD inter-national conference on Knowledge discovery and datamining, pages 569?577, New York, NY.Joseph Reisinger and Raymond J. Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 109?117, Los Angeles, California.G Salton, A Wang, and C Yang.
1975.
A vector-spacemodel for information retrieval.
Journal of the Ameri-can Society for Information Science, 18:613?620.1171Hinrich Schuetze.
1998.
Automatic word sense discrim-ination.
Journal of Computational Linguistics, 24:97?123.Stefan Thater, Georgiana Dinu, and Manfred Pinkal.2009.
Ranking paraphrases in context.
In Proceed-ings of the 2009 Workshop on Applied Textual Infer-ence, pages 44?47, Suntec, Singapore.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 948?957, Uppsala,Sweden.Michael E. Tipping and Chris M. Bishop.
1999.
Prob-abilistic principal component analysis.
Journal of theRoyal Statistical Society, Series B, 61:611?622.Alexander Yeh.
2000.
More accurate tests for the statis-tical significance of result differences.
In Proceedingsof the 18th Conference on Computational Linguistics,pages 947?953, Saarbru?cken, Germany.1172
