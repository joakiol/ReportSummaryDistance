Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 12?20,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsFine-Grained Emotion Recognition in Olympic TweetsBased on Human ComputationValentina Sintsovaa, b Claudiu MusataaArtificial Intelligence Laboratory bHuman Computer Interaction GroupSchool of Computer and Communication SciencesSwiss Federal Institute of Technology (EPFL)CH-1015, Lausanne, Switzerland{valentina.sintsova, claudiu-cristian.musat, pearl.pu}@epfl.chPearl PubAbstractIn this paper, we detail a method for do-main specific, multi-category emotion recog-nition, based on human computation.
We cre-ate an Amazon Mechanical Turk1 task thatelicits emotion labels and phrase-emotion as-sociations from the participants.
Using theproposed method, we create an emotion lex-icon, compatible with the 20 emotion cate-gories of the Geneva Emotion Wheel.
GEWis the first computational resource that can beused to assign emotion labels with such a highlevel of granularity.
Our emotion annotationmethod also produced a corpus of emotion la-beled sports tweets.
We compared the cross-validated version of the lexicon with existingresources for both the positive/negative andmulti-emotion classification problems.
Weshow that the presented domain-targeted lex-icon outperforms the existing general purposeones in both settings.
The performance gainsare most pronounced for the fine-grained emo-tion classification, where we achieve an accu-racy twice higher than the benchmark.21 IntroductionSocial media platforms such as Twitter.com have be-come a common way for people to share opinionsand emotions.
Sports events are traditionally ac-companied by strong emotions and the 2012 summerOlympic Games in London were not an exception.In this paper we describe methods to analyze anddata mine the emotional content of tweets about this1www.mturk.com2The corpus and the lexicon are available upon email requestevent using human computation.
Our goal is to cre-ate an emotion recognition method, capable of clas-sifying domain specific emotions with a high emo-tion granularity.
In the stated case, domain speci-ficity refers not only to the sport event, but also tothe Twitter environment.We focus on the categorical representation ofemotions because it allows a more fine-grained anal-ysis and it is more natural for humans.
In daily lifewe use emotion names to describe specific feelingsrather than give numerical evaluations or specify po-larity.
So far, the multi-item emotion classificationproblem has received much less attention.One reason is that high quality training corporaare difficult to construct largely due to the cost of hu-man annotators.
Further, if emotion representationis not carefully designed, the annotator agreementcan be very low.
The higher the number of consid-ered emotions is, the more difficult it is for humansto agree on a label for a given text.
Low qualitylabeling leads to difficulties in extracting powerfulclassification features.
This problem is further com-pounded in parsimonious environments, like Twit-ter, where the short text leads to a lack of emotionalcues.
All this presents challenges in developing ahigh-quality emotion recognition system operatingwith a fine-grained emotion category set within achosen domain.In this paper, we show how to tackle the abovechallenges through human computation, using anonline labor market such as the Amazon Mechani-cal Turk or AMT (Snow et al 2008).
To overcomethe possible difficulties in annotation we employ awell-designed emotion assessment tool, the Geneva12Emotion Wheel (GEW) (Scherer, 2005).
Having 20separate emotion categories, it provides a desirablehigh level of emotion granularity.
In a given task, weshow the annotators the tweets, related to the afore-mentioned sports event, and ask them to classify thetweets?
emotional content into one of the providedemotion categories.
The action sequence requiresthem to both label the tweets and to specify the tex-tual constructs that support their decision.
We viewthe selected textual constructs as probable classifi-cation features.
The proposed method thus simul-taneously produces an emotion annotated corpus oftweets and creates an emotion lexicon.
The resultingweighted emotion lexicon is a list of phrases indica-tive of emotion presence.
It consists solely of onesselected by respondents, while their weights werelearnt based on their occurrence in the constructedSports-Related Emotion Corpus (SREC).We show that the human-based lexicon is wellsuited for the particularities of the chosen environ-ment, and also for an emotion model with a highnumber of categories.
Firstly, we show that domainspecificity matters, and that non-specialists, usingtheir common sense, can extract features that areuseful in classification.
We use the resulting lexi-con, OlympLex, in a binary polarity classificationproblem on the domain data and show that it outper-forms several traditional lexicons.In multi-emotion classification, we show that itis highly accurate in classifying tweets into 20emotion categories of the Geneva Emotion Wheel(GEW) (Scherer, 2005).
As a baseline for compar-ison we use the Geneva wheel compatible lexicon,the Geneva Affect Label Coder (GALC) (Scherer,2005).
The experiments show that OlympLex sig-nificantly outperforms this baseline.Such a detailed emotion representation allows usto create an accurate description of the sentiment thechosen event evokes in its viewers.
For instance, wefind that Pride is the dominant emotion, and that itis 2.3 times more prevalent than Anger.2 Related WorkGEW Emotion Representation Model In ourwork we used the emotion categories from theGeneva Emotion Wheel (GEW, version 2.0).
GEWwas developed as a tool for obtaining self-reports ofemotional experience with a goal to structure the ex-haustive list of possible emotion names used in free-format self-reports with minimal loss in expressibil-ity.
It presents 20 (10 positive/10 negative) emo-tion categories frequently answered in free-formatself-reports as main options.
Each emotion categoryis represented by two common emotion names toemphasize its family nature (e.g.
Happiness/Joy3).These categories are arranged on the circle follow-ing the underlying 2-dimensional space of valence(positive-negative) and control (high-low).
Severallevels of intensity for each emotion category are pre-sented as answer options.
Also, 2 other answers arepossible: No emotion and Other emotion with free-format input in the latter case.Compared to raw dimensional models whereemotion states are described as points in space (e.g.Pleasure-Arousal-Dominance model, PAD (Mehra-bian, 1996)) GEW has an advantage of categoricalrepresentation where emotion state is described interms of discrete set of emotion names.
It allowshumans to measure their emotions in terms of emo-tion names they accustomed to instead of unnat-ural numerical measurements.
Among commonlyused emotion categories sets GEW categories are themost fine-grained, compared, for instance, to Ek-man?s (1992) or Plutchik?s (1980) basic emotions.While these models have been popular in emotionrecognition research, their main shortcoming is theirlimited items.
In sports events, fans and spectatorsnot only feel strong emotions, but also likely want toexpress them in multitudes of expressions.
Pride/E-lation, Envy/Jealousy are just two examples that aremissed in those models with basic emotions.Lexical Resources Emotion recognition is closelyrelated to the positive/negative sentiment classifica-tion.
In a traditional approach the units defining thepolarity of the text are polarity-bearing terms.
A listof such terms with corresponding polarity label orscore forms a polarity lexicon.
Commonly used ex-amples of polarity lexicons include GI (Stone et al1968), Bing Liu?s lexicon (Hu and Liu, 2004), andOpinionFinder (Wilson et al 2009).Similarly, emotion lexicons can be defined aslists of terms bearing emotions with their corre-3In the paper text we often use one name per category forbrevity reasons13sponding emotion information.
Depending on theconstruction methods, they can be separated intothose that constructed manually (GALC (Scherer,2005)), semi-automatically (WordNet-Affect (Strap-parava and Valitutti, 2004)) or via human computa-tion (ANEW (Bradley and Lang, 1999), NRC (Mo-hammad and Turney, 2010; Mohammad and Turney,2012)).
Our work is most closely related to the NRClexicon which was also extracted via human compu-tation on AMT.
The authors developed a task where,for a given term, the annotators rated to what extentthe term is associated to each emotion of Plutchik?sset.
In contrast, in our work, we harvest emotionallabels and features in context.
The terms are associ-ated with emotions in the context of the tweet theyappear in.
We use the approach suggested by (Amanand Szpakowicz, 2007) where humans are asked toselect an excerpt of the text expressing emotion.Moreover, we ask the annotators for additional inter-changeable, emotional expressions for the same sit-uation.
Lexicons obtained from unsupervised learn-ing methods using automatically annotated Twitterdata (Mohammad, 2012) have also been proposed,but their performance has been shown to be inferiorto benchmarks such as NRC.The underlying emotion representation model dif-fers from one emotion lexicon to another.
For in-stance, ANEW uses the PAD dimensions, Plutchik?sbasic categories are used by NRC and Ekman?s cat-egories in WordNet-Affect.
However, such repre-sentations do not provide a sufficient emotion gran-ularity level.
There is only one lexicon which incor-porates GEW emotion model: the GALC (Scherer,2005) lexicon.
It contains 279 unigram stems (e.g.happ*) explicitly expressing one of 36 emotion cate-gories (covering all GEW categories).
We use there-fore this lexicon for benchmarking.The main differences of our lexicon compared toits predecessors lie in the usage of new fine-grainedemotion set, new methods of human computationemployed in its construction and specificity to thecontext of Twitter posts and sport-related emotions.3 Emotional Labeling and EmotionFeature ElicitationWe created a Human Computation method, usingthe online labor market (Amazon Mechanical Turkor AMT) to simultaneously accomplish two goals.The first is to have a reliable, human annotation ofthe emotions within a text corpus.
The second is toenable the respondents to provide us with the fea-tures needed to construct an emotion lexicon.
In thissection we describe the processes of data selection,annotation, and refinement, as well as provide thestatistical description of the obtained data.3.1 Data CollectionOur goal is to analyze the emotions of the spectatorsof Olympic games.
We consider the tweets about theOlympics posted during the 2012 Olympic games asa data source for this analysis.
We assume that thesame emotions are expressed in the same way for allthe sports.
We thus narrow the scope of our analysisto a single sport ?
gymnastics.Traditionally, the gymnastics teams from the USAhave strong bid for victory.
Thus, we assume that alarge group of English-speaking nation may be in-terested in it.
Then, gymnastics is a dynamic type ofsport where each moment of performance can playa crucial role in final results, enhancing the emo-tional experience in audience.
Also, it is less com-mon than, for instance, running or swimming, thusthe occurrence of this term in tweets, at the time ofthe Olympics, will more likely signal a reference tothe Olympic gymnasts.We used the hashtag #gymnastics (hashtags rep-resent topics in tweets) to obtain the tweets relatedto the gymnastic competitions during the Olympicstime resulting in 199, 730 such tweets.
An emo-tional example is ?Well done #gymnastics we havea SILVER yeayyyyyyyyy!!!!
Wohoooo?.3.2 Annotation ProcessWe developed a Human-Intelligence Task (HIT) onthe AMT for annotation of a subset of the collectedtweets with emotion-related information.3.2.1 Task descriptionOne HIT consisted of the annotation of one pre-sented tweet.
A worker was asked to read a tweettext and to fulfill the following subtasks:Subtask 1 Decide on the dominant emotion theauthor of the tweet felt in the moment of its writ-ing (emotion label) and how strong it was (emotionstrength).
Even though an emotion mixture could14Iteration 1 2 (Ben) 2 (Ball) 3 4 5 4+5Polarity agreement 78.5 68 33.3 66.7 73.9 75.9 75.7Emotion agreement 38.5 24.7 13.34 29.3 25.84 29.7 29.3Average number of emotion tweet 1.6 1.26 0.64 1.28 1.2 1.72 1.67indicators per answera additional - 0.25 0.36 1.41 1.3 2.05 1.99Table 1: Basic statistics on the data collected over the annotation iterations.aonly among answers where non-neutral emotion label is assignedbe felt, a worker had to choose one emotion that pre-vailed all others.
This kept him focused on one mainemotion in the subtasks 2 and 3.
To elicit this in-formation we employed the Geneva Emotion Wheel(GEW) described in the Related Work with minorchanges: we used 3 strength labels (low, mediumand high) instead of 5 in initial version.
The set ofemotion categories remained unchanged: 20 GEWemotion categories plus 2 additional answer options:No emotion and Other emotion.
We required work-ers to type the emotion name in latter case.Subtask 2 In case an emotion was present,a worker was then asked to choose the excerpts ofthe tweet indicating its presence, the (tweet emotionindicators).
She was asked to find all the expres-sions of the chosen emotion present in the tweet text.It could be one word, emoticon, or subsequence ofthe tweet words.
We asked her to also include thewords modifying the strength of emotion (e.g.
tochoose so excited instead of excited).Subtask 3 Input additional emotion indicators ofchosen emotion.
Similarly to the previous subtask,a worker was asked to input the textual expressionsof the chosen emotion.
However, in this case theexpressions had to be not from the tweet text, butgenerated based on personal experience.
E.g.
shecould state that she uses poor thing to express Pity.3.2.2 HIT IterationsThe design of annotation schema and correspond-ing instructions as well as search for the optimal HITparameters took several iterations.
Table 1 containsthe statistics on inter-annotator agreements and onthe number of provided emotion indicators for eachiteration.
Beside emotion agreement, we also con-sider polarity agreement.
The polarity label of ananswer is defined as the polarity of its emotion label.No emotion implies a Neutral polarity.
For answerswith Other emotion we manually detected their po-larity based on provided emotion name if applicable,or set Neutral polarity otherwise.Iteration 1 Firstly, we annotated 200 tweets (setS1), using respondents within our laboratory, intoa set of 12 emotion categories (SportEm) whichwe considered first to be representative for the emo-tions incited by sport events: Love, Pride, Excite-ment, Positive Surprise, Joy, Like, Other Positive,Anger/Hate, Shame, Anxiety, Shock, Sadness, Dis-like, Other Negative.
For each tweet an annota-tor gave the emotion label and chose correspondingtweet emotion indicators.
The tweets of S1 includedboth tweets with predefined emotional words andwithout.
The details of selection process are omit-ted due to space limitations.Iteration 2 We launched two batches of HITs onAMT: Ball and Ben.
A HIT batch is defined by aset of tweets to label, with some parameters specificfor AMT, such as the number of different workersfor each tweet (we used 4 in all our experiments),the payment for one HIT, or specific worker require-ments, (e.g.
for Ben we also required that workersshould be from the U.S.).
We grouped 25 tweetsfrom S1 with HIT payment of $0.05 in Ben, whereasfor Ball we included only 10 tweets with paymentof $0.03.
The annotation schema used the emotionsof SportEm.
For each tweet an annotator gave theemotion label and provided tweet emotion indica-tors.
The field for additional emotion indicators in-put was presented as optional.We discovered that the answers in Ball had an un-acceptable quality, with a low agreement and manyimpossible labels.
This can be explained either bylower understanding of English or less reliability ofworkers from all around the world compared to theU.S.
workers.
Consequently, all our next iterationshad the requirement on workers to be from the U.S.Iteration 3 We launched a new HIT batch to an-15notate the full S1 with emotions from SportEm.Starting with this iteration, the payment was set to$0.04.
The additional emotion indicators field wasshown as compulsory.
The experiment showed thatAMT workers generally followed the instructionsachieving emotion agreement only slightly worsethan ours.Iteration 4 We decided to use the more fine-grained and well researched GEW emotion cate-gories.
Thus, we launched another HIT batch toannotate S1 again, in terms of GEW emotion cat-egories (with a schema given in Task Description).Even though a new task contained more answer op-tions emotion agreement stayed in the same rangebetween 0.25 and 0.3.Iteration 5 We launched a final batch with thedescribed GEW schema to annotate more tweets.We selected Olympics related tweets that had a highlikelihood of being emotional.
We first selectedtweets using the emotion indicators obtained duringthe previous iterations and found more than 5 timesin the collected corpus (418 terms).
For each key-word in this list we extracted up to 3 tweets contain-ing this term (1244 tweets).
In addition, we addedthe tweets without keywords from the list, but postedby the users who used these emotional keywordsin their other tweets, supposing that these users aremore likely to express their emotions.
Overall, 1800tweets were selected, but 13 were excluded becausethey were not written in English.The resulting corpus contains the data gatheredduring the iterations 4 and 5.
It consists of 1987tweets annotated each by 4 workers with emotionlabel, emotion strength, and related emotion indica-tors.
The Fleiss Kappa (Fleiss, 1971) for emotion la-bels is 0.24 which is considered to be fair by Landisand Koch (1977), but quite low compared to usualkappa values in other tasks (e.g.
polarity annotationusually has Kappa in a range of 0.7?0.8).
We con-clude that the annotation in terms of multi-categoryemotions is highly subjective and ambiguous task,confirming our assumptions on existence of emotionmixtures.3.3 Quality ControlThe results of crowdsourcing usually require addi-tional refinement.
The workers who give maliciousanswers intentionally or due to lack of understand-ing worsen the data quality.
We detect such workersautomatically using the following 2 criteria:Average Polarity Conformity A worker?s answerhas a polarity conformity of 1 if at least one workerindicated the same polarity for the same tweet (0otherwise).
A worker?s average polarity conformityis computed from all his answers.
This criterionaims to detect the workers who repeatedly disagreewith other workers.Dominant Emotion Frequency The dominantemotion of a worker is the one which appears mostfrequently in his answers.
The dominant emotionfrequency, among the worker?s answers, is the cri-terion value.
This criterion aims to detect workersbiased towards specific emotion.A worker who has the average polarity conformitybelow a predefined threshold or the dominant emo-tion frequency above a threshold is considered tohave an insufficient quality and all his answers areexcluded from the corpus.
The threshold for eachcriterion is computed as a percentile of an approxi-mated normal distribution of workers criterion val-ues for probability limit of 0.01.To increase the confidence in the computed cri-teria values, we establish a minimum number oftweets Tmin any worker should annotate to be sub-jected to the criteria.
To establish this number foreach criterion, we use the following algorithm:Let Xn(w) be the criterion value computed usingonly first n answers of worker w in order of theirsubmission.
For each worker we detect Nmin(w) ?the minimum number of answers after which the cri-terion value stops varying greatly:|Xn(w)?Xn?1(w)| ?
0.05, ?n ?
Nmin(w) (1)We then compute Tmin as the ceiling of the averagevalue of of Nmin(w) among workers who annotatedat least 20 tweets.The described procedure on detection of badworkers allowed the analysis of 83% of the answers.Using it, we excluded 8 workers, with their corre-sponding 260 answers.In addition to removing these workers, we alsoexcluded malicious answers: 736 answers that had apolarity conformity of 0.
This additional filter wasapplied to all the remaining answers from the previ-ous method.
We also excluded the 121 answers with16Other emotion and the answers for 12 tweets, thatwere left with only 1 answer by this stage.As a result of quality control, there were excluded14.2% of initial answers.
Overall, 1957 tweets withcorresponding 6819 annotations remained (3.48 an-swers per tweet in average).
These answers composethe final Sport-Related Emotion Corpus (SREC).3.4 Emotion distribution in SRECTo provide a glimpse of the data we present the dis-tribution of emotion categories among all answers inthe figure 1.
The most frequently answered emotioncategory was Pride, followed by Involvement.
Theseemotions are natural in the context of sport events,however course-grained emotion models could notdistinguish them.
It highlights the advantage of fine-grained GEW emotion set to express the subtletiesof the domain.020040060080010001200InvolvementAmusementPrideHappinessPleasureLoveAweReliefSurpriseNostalgiaPitySadnessWorryShameGuiltRegretEnvyDisgustContemptAngerNoEmotionFigure 1: Distribution of emotion labels in worker?s an-swers (after application of quality control)4 Emotion Recognition ModelThe output of our emotion recognition method is thedistribution of emotions within a text, in terms ofGEW emotion categories.
It is represented as a tuplein the probability spaceP ={p?
= (p1, .
.
.
, p21),21?i=1pi = 1}(2)where pi represents the percentage of ith emotion infelt emotion mixture.
The emotion set contains 20GEW categories and No Emotion as 21st category.We use a lexicon of emotion indicators, whichare words or word sequences indicative of emotionpresence.
Each indicator termt has attached emo-tion distribution tuple p?t ?
P. To compute the re-sult tuple p?
for a text d we sum up all the tuplesof emotion indicators found within this text with thenumber of times they were found:p?
(d) =?termt?dnt(d) p?t (3)If no indicators are present in the text, a full weightis given to No emotion category (p21 = 1).
We alsoneglect all negated indicators occurrences detectedby the negation words (no, not, *n?t, never) placedahead of an indicator.Lexicon Construction We construct the lexiconby selecting the emotion indicators and computingtheir emotion distributions.
We use a training corpusthat has a format described in the previous section.The training process consists of the following steps:Among all tweet and additional emotion indica-tors provided by workers, we select those that weresuggested more than once.For each tweet we have several emotion labelsfrom the data.
We determine the emotion distribu-tion of the tweet by computing the frequency of eachemotion label over all the answers corresponding tothat tweet.For each answer we construct a link between eachterm suggested in the additional emotion indicatorsfield and the answer?s emotion label.
This link isrepresented as a tuple p?
?
P with weight 1 forlinked emotion category.
Then, for each detectedemotion indicator we compute its emotion distribu-tion by averaging all the emotion distributions it ap-peared in.
This includes the emotion distributions ofthe tweets where this indicator occurred without anegation and the emotion distributions of the corre-sponding indicator-emotion links.We define an indicator to be ambiguous if its dom-inant polarity (polarity having the highest sum ofthe weights for corresponding emotions) has sum-mary weight smaller than 0.75.
All such terms areremoved from the result lexicon.Result Lexicon Description Following the speci-fied process over the full SREC data, we computedan emotion lexicon, OlympLex, that contains 3193terms.
The ratio of positive terms to negative onesis 7:3 (term polarity is defined as dominant polarityof term emotion distribution).
Unigrams compose37.5% of the lexicon, bigrams ?
30.5%, all otherterms are ngrams of a higher order (up to 5).175 Experimental EvaluationWe evaluated our lexicon on the SREC corpus asa classifier, using ten-fold cross-validation to avoidpossible overfitting.
The precompiled universal lex-icons were used for benchmarking.
As no training isrequired, we tested them over the full data.5.1 Polarity ClassificationWe considered the basic polarity classification taskwith 3 classes (Positive, Negative and Neutral).
Weused only 1826 tweets that have one dominant polar-ity based on workers?
answers.
This dominant polar-ity was taken as a true polarity label of a tweet.The output polarity label of our classifier is dom-inant polarity of found emotion distribution: a po-larity having the highest sum of the weights for cor-responding emotions.
The output of prior sentimentlexicons is computed analogously: we sum up thenumber of found lexicon terms in the tweet textfor each emotion or polarity category (depending onwhich categorization is provided by the lexicon) andoutput the polarity having the highest sum value.
Iftwo polarities have the same sum weight, the outputpolarity is Neutral.We used standard classification evaluation mea-sures: accuracy, precision, recall and F1-score.
Weconsidered only non-neutral classes (Positive andNegative) for precision and recall.
Table 2 shows theresults of our classifier, compared with other knownsentiment lexicons.
The proposed lexicon outper-forms every other one, both in terms of accuracy andF1-score.
As it was the only lexicon fitted to theOlympic gymnastics data, its superiority reveals theadvantage of domain-targeted lexicon construction.Lexicon P R F1 AOlympLex* 81.7 73.2 77.2 72.5BingLiu 80.4 52.9 63.8 53.6OpinionFinder 66.0 46.6 54.6 46.6GeneralInquirer 69.8 44.4 54.3 44.5NRC* 60.6 39.7 48.0 40.4WnAffect* 78.6 28.1 41.4 30.1GALC* 81.6 25.6 39.0 27.9Table 2: The results of polarity classification evaluation.P=precision, R=recall, F1 = F1-score, A=accuracy*A lexicon employing several emotion categories5.2 Emotion ClassificationWe evaluated emotion recognition results in the set-ting of a multi-label classification problem.
The out-put is a set of labels instead of a standard singlelabel answer.
In this case, the output of the clas-sifier (OC) was defined as a set of dominant emo-tions in the found emotion distribution p?.
This setcontained the emotions having the highest weightspi.
The set of emotion labels given for this tweetby workers formed a true output ?
a set of true la-bels (OT ) of emotion classification.
As a baselinefor multi-category emotion classification we consid-ered the GALC lexicon (Scherer, 2005).Multi-label Evaluation We used the standardevaluation metrics adapted for multi-label output(Tsoumakas and Katakis, 2007).
For each tweet, wefirst computed the precision P = |OC?OT ||OC | , whichshows how many of emotions outputted by the clas-sifier were correct.
Then the recall R = |OC?OT ||OT | ,which shows how many of true labels were foundby classifier, and the accuracy A = |OC?OT ||OC?OT | , whichshows how close the sets of classifier and true la-bels were.
These values were averaged among allapplicable tweets.
For precision and recall we usedonly the tweets with non-neutral answers in OC andOT correspondingly (meaning that No emotion labelwas not present in a set).Table 3 shows the comparative results of our andGALC lexicons.
Compared to the GALC baseline,our classifier has both higher precision and recall.Higher recall is explained by the fact that our lexi-con is larger and contains also ngram terms.
In ad-dition, it includes not only explicit emotion expres-sions (e.g.
sad or proud), but also implicit ones (e.g.yes or mistakes).Per-Category Evaluation Another way to evalu-ate the output of multi-label classifier is to evaluate itfor each emotion category separately.
For each cat-egory we computed precision, recall and F1-score.Lexicon P R F1 AGALC 49.0 10.2 16.8 12.5OlympLex 53.5 24.9 34.0 25.4Table 3: Results of multi-label evaluation.
P=precision,R=recall, F1 = F1-score, A=accuracy18GALC OlympLex GALC OlympLexNegative P R F1 P R F1 Positive P R F1 P R F1Anger 48.4 10.8 17.7 53.3 26 35 Involvement 52.4 2.4 4.6 49.4 17.6 26Contempt - 0 - 42.1 4.7 8.5 Amusement 51 11.6 18.9 55 24.6 34Disgust 50 1.4 2.8 39.4 9.4 15.2 Pride 89.6 6.7 12.5 60.8 59.4 60.1Envy 100 11.1 20 55.6 13.9 22.2 Happiness 46.3 8.8 14.8 45.1 9.8 16.1Regret 53.3 3.4 6.4 36.3 12.4 18.5 Pleasure 44.8 5.9 10.4 48.8 17.9 26.2Guilt 25 5.6 9.1 0 0 - Love 38.1 27.4 31.9 48.0 8.2 14Shame 18.5 9.8 12.8 25 3.9 6.8 Awe 42.9 6.7 11.5 54.2 23.7 33Worry 54.8 21.5 30.9 43.2 15 22.2 Relief 100 17.1 29.2 50 4.9 8.9Sadness 52.5 19.6 28.6 41.7 9.3 15.3 Surprise 38.3 9 14.6 33.3 6 10.2Pity 75 2.5 4.9 57.8 31.4 40.7 Nostalgia 20.5 14.5 17 28.6 3.2 5.8Table 4: Evaluation results at per-category level.
P=precision, R=recall, F1 = F1-scoreThe results of this evaluation in comparison withbenchmark GALC lexicon are presented in the ta-ble 4.
Overall, our lexicon performs better on mostof the categories (12 out of 20) in terms of F1-score.The highest F1-score is achieved for such Olympicrelated emotion as Pride.5.3 DiscussionThe fact that the terms from the GALC lexicon arefound in 31% of tweets indicates that people do ex-press their emotions explicitly with emotional terms.However, a list of currently available explicit emo-tional terms is not extensive.
For instance, it doesnot cover slang terms.
Moreover, people do notlimit themselves to only explicit emotional terms.Our lexicon constructed based on the answers pro-vided by non-expert humans achieves a significantlyhigher recall.
This highlights the importance of em-ploying the human common knowledge in the pro-cess of extraction of emotion bearing features.6 ConclusionWe presented a context-aware human computationmethod for emotion labeling and feature extrac-tion.
We showed that inexpert annotators, usingtheir common sense, can successfully attach emo-tion labels to tweets, and also extract relevant emo-tional features.
Using their answers, we carefullyconstructed a linguistic resource for emotion clas-sification.
The suggested method can be reused toconstruct additional lexicons for different domains.An important aspect that differentiates our workis the emotion granularity.
To the best of our knowl-edge, this was the first attempt to create lexicalresources for emotion classification based on theGeneva Emotion Wheel (GEW), which has as manyas 20 emotion categories.
This level of granularityenabled us to capture the subtleties of the emotionalresponses in the target domain, tweets regarding the2012 summer Olympics in London.
In this dataset,we found that the prevalent emotion is Pride, a detailwhich is unattainable using previous methods.Another differentiator is that, unlike most previ-ous approaches, we relied on human computationfor both labeling and feature extraction tasks.
Weshowed that human generated features can be suc-cessfully used in emotional classification, outper-forming various existing methods.
A further differ-ence from prior lexicons is the fact that ours wasbuilt with a context-sensitive method.
This led toa higher accuracy on the target domain, compared tothe general purpose lexicon.We benchmarked the cross-validated versionof created OlympLex lexicon with the existinguniversal-domain lexicons for both polarity andmulti-emotion problems.
In suggested settings weshowed that it can outperform general purpose lex-icons in the binary classification due to its domainspecificity.
We also obtained significant improve-ments over the baseline GALC lexicon, which wasthe only preexisting one compatible with the GEW.However, high domain specificity of the createdlexicon and restricted variety of data used in its con-struction implies possible limitations of its usage forother types of data.
Its porting and generalization toother domains is one of the future directions.19ReferencesCecilia Ovesdotter Alm, Dan Roth, and Richard Sproat.2005.
Emotions from text: machine learning for text-based emotion prediction.
In Proceedings of the con-ference on Human Language Technology and Empir-ical Methods in Natural Language Processing, pages579?586.
Association for Computational Linguistics.Saima Aman and Stan Szpakowicz.
2007.
Identifyingexpressions of emotion in text.
In Text, Speech andDialogue, pages 196?205.
Springer.Margaret M Bradley and Peter J Lang.
1999.
Affectivenorms for english words (anew): Instruction manualand affective ratings.
Technical report, Technical Re-port C-1, The Center for Research in Psychophysiol-ogy, University of Florida.Taner Danisman and Adil Alpkocak.
2008.
Feeler: Emo-tion classification of text using vector space model.In AISB 2008 Convention Communication, Interactionand Social Intelligence, volume 1, page 53.Paul Ekman.
1992.
An argument for basic emotions.Cognition & Emotion, 6(3-4):169?200.Joseph L Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
Psychological bulletin,76(5):378.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177.ACM.Sunghwan Mac Kim, Alessandro Valitutti, and Rafael ACalvo.
2010.
Evaluation of unsupervised emotionmodels to textual affect recognition.
In Proceedingsof the NAACL HLT 2010 Workshop on ComputationalApproaches to Analysis and Generation of Emotion inText, pages 62?70.
Association for Computational Lin-guistics.J Richard Landis and Gary G Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, pages 159?174.Hugo Liu, Henry Lieberman, and Ted Selker.
2003.A model of textual affect sensing using real-worldknowledge.
In Proceedings of the 8th internationalconference on Intelligent user interfaces, pages 125?132.
ACM.Albert Mehrabian.
1996.
Pleasure-arousal-dominance:A general framework for describing and measuring in-dividual differences in temperament.
Current Psychol-ogy, 14(4):261?292.Saif M Mohammad and Peter D Turney.
2010.
Emo-tions evoked by common words and phrases: Usingmechanical turk to create an emotion lexicon.
In Pro-ceedings of the NAACL HLT 2010 Workshop on Com-putational Approaches to Analysis and Generation ofEmotion in Text, pages 26?34.Saif M Mohammad and Peter D Turney.
2012.
Crowd-sourcing a word?emotion association lexicon.
Com-putational Intelligence.Saif M Mohammad.
2012.
# emotional tweets.
In Pro-ceedings of the First Joint Conference on Lexical andComputational Semantics-Volume 1: Proceedings ofthe main conference and the shared task, and Volume2: Proceedings of the Sixth International Workshop onSemantic Evaluation, pages 246?255.
Association forComputational Linguistics.Alena Neviarouskaya, Helmut Prendinger, and MitsuruIshizuka.
2011.
Affect analysis model: Novel rule-based approach to affect sensing from text.
NaturalLanguage Engineering, 17(1):95.Robert Plutchik.
1980.
A general psychoevolutionarytheory of emotion.
Emotion: Theory, research, andexperience, 1(3):3?33.Klaus R Scherer.
2005.
What are emotions?
and howcan they be measured?
Social science information,44(4):695?729.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages 254?263.
Association for Computational Linguistics.Phil J Stone, Dexter C Dunphy, Marshall S Smith, andDM Ogilvie.
1968.
The general inquirer: A computerapproach to content analysis.
Journal of Regional Sci-ence, 8(1).Carlo Strapparava and Alessandro Valitutti.
2004.Wordnet-affect: an affective extension of wordnet.
InProceedings of LREC, volume 4, pages 1083?1086.Grigorios Tsoumakas and Ioannis Katakis.
2007.
Multi-label classification: An overview.
InternationalJournal of Data Warehousing and Mining (IJDWM),3(3):1?13.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analysis.Computational linguistics, 35(3):399?433.20
