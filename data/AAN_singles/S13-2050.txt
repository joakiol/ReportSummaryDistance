Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 300?306, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsAI-KU: Using Substitute Vectors and Co-Occurrence Modeling for WordSense Induction and DisambiguationOsman Bas?kaya Enis Sert Volkan Cirik Deniz YuretArtificial Intelligence LaboratoryKoc?
University, I?stanbul, Turkey{obaskaya,esert,vcirik,dyuret}@ku.edu.trAbstractWord sense induction aims to discover differ-ent senses of a word from a corpus by us-ing unsupervised learning approaches.
Once asense inventory is obtained for an ambiguousword, word sense discrimination approacheschoose the best-fitting single sense for a givencontext from the induced sense inventory.However, there may not be a clear distinctionbetween one sense and another, although fora context, more than one induced sense canbe suitable.
Graded word sense method al-lows for labeling a word in more than onesense.
In contrast to the most common ap-proach which is to apply clustering or graphpartitioning on a representation of first or sec-ond order co-occurrences of a word, we pro-pose a system that creates a substitute vec-tor for each target word from the most likelysubstitutes suggested by a statistical languagemodel.
Word samples are then taken accord-ing to probabilities of these substitutes and theresults of the co-occurrence model are clus-tered.
This approach outperforms the othersystems on graded word sense induction taskin SemEval-2013.1 IntroductionThere exists several drawbacks of representing theword senses with a fixed-list of definitions of a man-ually constructed lexical database.
There is no guar-antee that they reflect the exact meaning of a tar-get word in a given context since they usually con-tain definitions that are too general (Ve?ronis, 2004).More so, lexical databases often include many raresenses while missing corpus/domain-specific senses(Pantel and Lin, 2004).
The goal of Word Sense In-duction (WSI) is to solve these problems by auto-matically discovering the meanings of a target wordfrom a text, not pre-defined sense inventories.
WordSense Discrimination (WSD) approaches determinebest-fitting sense among the meanings that are dis-covered for an ambiguous word.
However, (Erket al 2009) suggested that annotators often gavehigh ratings to more than one WordNet sense for thesame occurrence.
They introduced a novel annota-tion paradigm allowing that words have more thanone sense with a degree of applicability.Unlike previous SemEval tasks in which systemslabeled a target word?s meaning with only one sense,word sense induction task in SemEval-2013 relaxesthis by allowing a target word to have more than onesense if applicable.Word sense induction approaches can be catego-rized into graph based models, bayesian, and vector-space ones.
In graph-based approaches, every con-text word is represented as a vertex and if two con-text words co-occur in one or more instances of atarget word, then two vertices are connected withan edge.
When the graph is obtained, one of thegraph clustering algorithm is employed.
As a result,different partitions indicate the different senses of atarget word (Ve?ronis, 2004).
Agirre et al(2006) ex-plored the use of two graph algorithms for unsuper-vised induction and tagging of nominal word sensesbased on corpora.
Recently, Korkontzelos and Man-andhar (2010) proposed a graph-based model whichachieved good results on word sense induction anddiscrimination task in SemEval-2010.300Brody and Lapata (2009) proposed a Bayesianapproach modeling the contexts of the ambiguousword as samples from a multinomial distributionover senses which are in turn characterized as dis-tributions over words.Vector-space models, on the other hand, typicallycreate context vector by using first or second or-der co-occurrences.
Once context vector has beenconstructed, different clustering algorithms may beapplied.
However, representing the context withfirst or second order co-occurrences can be difficultsince there are plenty of parameters to be consid-ered such as the order of occurrence, context win-dow size, statistical significance of words in the con-text window and so on.
Instead of dealing withthese, we suggest representing the context with themost likely substitutes determined by a statisticallanguage model.
Statistical language models basedon large corpora has been examined in (Yuret, 2007;Hawker, 2007; Yuret and Yatbaz, 2010) for unsuper-vised word sense disambiguation and lexical substi-tution.
Moreover, the best results in unsupervisedpart-of-speech induction achieved by using substi-tute vectors (Yatbaz et al 2012).In this paper, we propose a system that representsthe context of each target word by using high prob-ability substitutes according to a statistical languagemodel.
These substitute words and their probabili-ties are used to create word pairs (instance id - sub-stitute word) to feed our co-occurrence model.
Theoutput of the co-occurrence model is clustered by k-means algorithm.
Our systems perform well amongother submitted systems in SemEval-2013.Rest of the paper is organized as follows.
Sec-tion 2 describes the provided datasets and evalu-ation measures of the task.
Section 3 gives de-tails of our algorithm and is divided into five con-tiguous subsections that correspond to each step ofour system.
In Section 4 we present the differ-ences between our three systems and their perfor-mances.
Finally, Section 5 summarizes our work inthis task.
The code to replicate this work is availableat http://goo.gl/jPTZQ.2 Data and Evaluation MethodologyThe test data for the graded word sense inductiontask in SemEval-2013 includes 50 terms containing20 verbs, 20 nouns and 10 adjectives.
There are atotal of 4664 test instances provided.
All evalua-tion was performed on test instances only.
In ad-dition, the organizers provided sense labeled trialdata which can be used for tuning.
This trial datais a redistribution of the Graded Sense and Usagedata set provided by Katrin Erk, Diana McCarthy,and Nicholas Gaylord (Erk et al 2009).
It consistsof 8 terms; 3 verbs, 3 nouns, and 2 adjectives allwith moderate polysemy (4-7 senses).
Each termin trial data has 50 contexts, in total 400 instancesprovided.
Lastly, participants can use ukWaC1, a 2-billion word web-gathered corpus, for sense induc-tion.
Furthermore, unlike in previous WSI tasks, or-ganizers allow participants to use additional contextsnot found in the ukWaC under the condition that theysubmit systems for both using only the ukWaC andwith their augmented corpora.The gold-standard of test data was prepared usingWordNet 3.1 by 10 annotators.
Since WSI systemsreport their annotations in a different sense inven-tory than WordNet 3.1, a mapping procedure shouldbe used first.
The organizers use the sense mappingprocedure explained in (Jurgens, 2012).
This proce-dure has adopted the supervised evaluation settingof past SemEval WSI Tasks, but the main differ-ence is that the former takes into account applica-bility weights for each sense which is a necessaryfor graded word sense.Evaluation can be divided into two categories: (1)a traditional WSD task for Unsupervised WSD andWSI systems, (2) a clustering comparison settingthat evaluates the similarity of the sense inventoriesfor WSI systems.
WSD evaluation is made accord-ing to three objectives:?
Their ability to detect which senses are appli-cable (Jaccard Index is used)?
Their ability to rank the applicable senses ac-cording to the level of applicability (WeightedKendall?s ?
is used)?
Their ability to quantify the level of applicabil-ity for each sense (Weighted Normalized Dis-counted Cumulative Gain is used)Clustering comparison is made by using:1Available here: http://wacky.sslmit.unibo.it301?
Fuzzy Normalized Mutual Information: It cap-tures the alignment of the two clusterings inde-pendent of the cluster sizes and therefore servesas an effective measure of the ability of an ap-proach to accurately model rare senses.?
Fuzzy B-Cubed: It provides an item-basedevaluation that is sensitive to the cluster sizeskew and effectively captures the expected per-formance of the system on a dataset where thecluster (i.e., sense) distribution would be equiv-alent.More details can be found on the task website.23 AlgorithmIn this section, we explain our algorithm.
First, wedescribe data enrichment procedure then we will an-swer how each instance?s substitute vector was con-structed.
In contrast to common practice which isclustering the context directly, we first performedword sampling on the substitute vectors and cre-ated instance id - substitute word pairs as explainedin Subsection 3.3.
These pairs were used in theco-occurrence modeling step described in Subsec-tion 3.4.
Finally, we clustered these co-occurrencemodeling output with the k-means clustering algo-rithm.
It is worth noting that this pipeline is per-formed on each target word separately.SRILM (Stolcke, 2002) is employed on entireukWaC corpus for the 4-gram language model toconduct all experiments.3.1 Data EnrichmentData enrichment aims to increase the number of in-stances of target words.
Our preliminary experi-ments on the trial data showed that additional con-texts increase the performance of our systems.Assuming that our target word is book in nounform.
We randomly fetch 20,000 additional contextsfrom ukWaC where our target word occurs with thesame part-of-speech tag.
This implies that we skipthose sentences in which the word book functions asa verb.
These additional contexts are labeled withunique numbers so that we can distinguish actual in-stances in the test data.
We follow this procedure for2www.cs.york.ac.uk/semeval-2013/task13/Substitute Probabilitysolve 0.305complete 0.236meet 0.096overcome 0.026counter 0.022tackle 0.014address 0.012... ...... ...Table 1: The most likely substitutes for meetevery target word in the test data.
In total, 1 mil-lion additional instances were fetched from ukWac.Hereafter we refer to this new dataset with as an ex-panded dataset.3.2 Substitute VectorsUnlike other WSI methods which rely on the first orthe second order co-occurrences (Pedersen, 2010),we represent the context of each target word instanceby finding the most likely substitutes suggested bythe 4-gram language model we built from ukWaCcorpus.
The high probability substitutes reflect bothsemantic and syntactic properties of the context asseen in Table 1 for the following example:And we need Your help to meet the chal-lenge!For every instance in our expanded dataset, weuse three tokens each on the left and the right side ofa target word as a context when estimating the prob-abilities for potential lexical substitutes.
This tightwindow size might seem limited, however, tight con-text windows give better scores for semantic simi-larity, while larger context windows or second-ordercontext words are better for modeling general top-ical relatedness (Sahlgren, 2006; Peirsman et al2008).Fastsubs (Yuret, 2012) was used for this processand the top 100 most likely substitutes were used forrepresenting each instance since the rest of the sub-stitutes had negligible probabilities.
These top 100probabilities were normalized to add up to 1.0 giv-ing us a final substitute vector for a particular targetword?s instance.
Note that the substitute vector is a302Instance ID Substitute Wordmeet1 completemeet1 solvemeet1 solvemeet1 overcome... ...... ...meet1 meetmeet1 completemeet1 solvemeet1 solveTable 2: Substitute word sampling for instance meet1Figure 1: Co-Occcurrence Embedding Sphere for meetfunction of the context only and is indifferent to thetarget word.At the end of this step, we had 1,004,466 sub-stitute vectors.
The next common step might be tocluster these vectors either locally, which means ev-ery target word will be clustered separately; or glob-ally, which indicates all instances (approximately 1million) will be clustered together.
Both approachesled us to lower scores than the presented method.Therefore, instead of clustering substitute vectors di-rectly, we relied on co-occurrence modeling.3.3 Substitute Word SamplingBefore running S-CODE (Maron et al 2010) tomodel co-occurrence statistics, we needed to per-form the substitute word sampling.
For each targetword?s instance, we sample 100 substitutes from itssubstitute vector.
Assuming that our target word ismeet and its substitute vector is the one shown inInstance ID Substitute Wordmeet1 completemeet1 solve... ...meet2 holdmeet2 visit... ...meet20100 assemble... ...meet20100 gatherTable 3: Substitute sampling for a target word meet.Instance ID - Substitute word pairsTable 1.
We choose 100 substitutes from this in-stance?s substitute vector by using individual proba-bilities of substitutes.
As seen in Table 2, those sub-stitutes which have high probabilities dominate theright column.
Recall that Table 2 illustrates only oneinstance (subscript denotes the instance number) forthe target word meet which has 20,000 and 100 in-stances from the context enrichment procedure andthe test, respectively.
We followed the same proce-dure for every instance of each target word.
Table 3depicts instance id - substitute word pairs for thetarget word meet rather than for only one instanceshown in Table 2.3.4 Co-Occurrence ModelingAfter sampling, we had approximately 20,000 in-stance id - substitute word pairs.
These pairs wereused to feed S-CODE.
The premise is that wordswith similar meanings will occur in similar contexts(Harris, 1954), and at the end this procedure enablesus to put together words with similar meanings aswell as making the clustering procedure more accu-rate.
If two different instances have similar substi-tute word pairs (i.e, similar contexts) then these twoword pairs attract each other and they will be locatedclosely on the unit sphere, otherwise they will repeland eventually be far away from each other (see Fig-ure 1).3.5 ClusteringWe used k-means clustering on S-CODE sphere.Note that the procedures explained in the fore-gone subsections were repeated for each target303System JI WKT WNDCGAllInstancesai-ku 0.759 0.804 0.432ai-ku(a1000) 0.759 0.794 0.612ai-ku(r5-a1000) 0.760 0.800 0.541MFS 0.381 0.655 0.337All-Senses 0.757 0.745 0.660All-Senses-freq-ranked 0.757 0.789 0.671All-Senses-avg-ranked 0.757 0.806 0.706Random-3 0.776 0.784 0.306Random-n 0.795 0.747 0.301Table 4: Supervised results on the trial set using mediangold-standard (JI: Jaccard Index FScore, WKT: WeightedKendall?s Tau FScore, WNDCG: Weighted NormalizedDiscounted Cumulative Gain FScore)word.
More precisely, the substitute sampling, co-occurrence modeling and clustering were performedone by one for each target word.We picked 22 as k value since the test set con-tained words with 3 to 22 senses.
After all wordpairs were labeled, we counted all class labels foreach instance in the test set.
For example, if meet1?s50 word pairs are labeled with c1 and 30 word pairsare labeled with c2 and finally 20 word pairs are la-beled with c3, then this particular instance wouldhave 50% sense1, 30% sense2 and 20% sense3.4 Evaluation ResultsIn this section, we will discuss evaluation scores andthe characteristics of the test and the trial data.All three AI-KU systems followed the same pro-cedures described in Section 3.
After clustering,some basic post-processing operations were per-formed for ai-ku(a1000) and ai-ku(r5-a1000).
Forai-ku(a1000), we added 1000 to all sense labelswhich were obtained from the clustering procedure;for ai-ku(r5-a1000), those sense labels occurred lessthan 5 times in clustering were removed since weconsidered them to be unreliable labels, afterwardswe added 1000 for all remaining sense labels.Supervised Metrics: Table 5 shows the perfor-mance of our systems on the test data using allinstances (verbs, nouns, adjectives) for all super-vised measures and in comparison with the sys-tems that performed best and worst, most frequentsense (MFS), all senses equally weighted, all sensesaverage weighted, random-3, and random-n base-System JI WKT WNDCGAllInstancesai-ku 0.197 0.620 0.387ai-ku(a1000) 0.197 0.606 0.215ai-ku(r5-a1000) 0.244 0.642 0.332Submitted-Best 0.244 0.642 0.387All-Best 0.552 0.787 0.499All-Worst 0.149 0.465 0.215MFS 0.552 0.560 0.412All-Senses-eq-weighted 0.149 0.787 0.436All-Senses-avg-ranked 0.187 0.613 0.499Random-3 0.244 0.633 0.287Random-n 0.290 0.638 0.286Table 5: Supervised results on the test set.
(Submitted-Best indicates the best scores among all submitted sys-tem.
All-Best indicates the best scores among all sub-mitted systems and baselines.
JI: Jaccard Index FS-core, WKT: Weighted Kendall?s Tau FScore, WNDCG:Weighted Normalized Discounted Cumulative Gain FS-core)Trial Data Test DataNumber of Sense 4.97 1.19Sense Perplexity 5.79 3.78Table 6: Average number of senses and average senseperplexity for trial and test datalines.
Bold numbers indicate that ai-ku achievedbest scores among all submitted systems.
Our sys-tems performed generally well for all three super-vised measures and slightly better for all submit-ted systems.
On the other hand, baselines achievedbetter scores than all participants.
More precisely,on sense detection objective, MFS baseline obtained0.552 which is the top score, while the best submit-ted system could reach only 0.244.
Why is it the casethat MFS had one of the worst sense detection scoreon trial data (see Table 4), but best on test data?
Un-like the trial data, test data largely consists of onlyone sense instances, MFS usually gives correct an-swer.
Table 6 illustrates the characteristics of thetest and trial data.
Instances annotated with multiplesense had a very small fraction in the test data.
Infact, 517 instances in the test set were annotated withtwo senses (11%) and only 25 were annotated withthree senses (0.5%).
However, trial data providedby the organizers had almost 5 senses per instanceon the average.
A similar results can be observedin All-Senses baselines.
On sense ranking objec-304System FScore FNMI FB-CubedAllSingle-senseInstancesai-ku 0.641 0.045 0.351ai-ku(a1000) 0.601 0.023 0.288ai-ku(r5-a1000) 0.628 0.026 0.421Submitted-Best 0.641 0.045 0.441All-Best 0.641 0.048 0.570All-Worst 0.477 0.006 0.180MFS 0.578 - -SemCor-MFS 0.477 - -One Sense 0.569 0.0 0.570Random-3 0.555 0.010 0.359Random-n 0.533 0.006 0.223Table 7: Supervised and unsupervised results on the testset using instances which have only one sense.
Bold num-bers indicate that ai-ku achieved the best submitted sys-tem scores.
(FScore: Supervised FScore, FNMI: FuzzyNormalized Mutual Information, FB-Cubed: Fuzzy B-Cubed FScore)tives, All-Sense-eq-weighted outperformed all othersystems.
The reason is the same as the above.
Thisbaseline ranks all senses equally and since most in-stances had been annotated only one sense, the otherwrong senses were tied and placed at the second po-sition in ranking.
As a result, this baseline achievedthe highest score.
Finally, for quantifying the levelof applicability for each sense, Weighted NDCG wasemployed.
ai-ku outperformed other submitted sys-tems, but top score was achieved by all-sense-avg-weighted baseline.
Addition to these results, orga-nizers provided scores for instances which have onlyone sense.
This setting contains 89% of the test data.Table 7 shows supervised and unsupervised scoresfor all single-sense instances.
Our base system, ai-ku, outperformed all other system and all baselinesfor FScore.
Moreover, it also achieved the secondbest score (0.045) for Fuzzy NMI.
Only one base-line (one sense per instance) obtained slightly betterscore (0.048) for this metric.
For Fuzzy B-Cubed,ai-ku(r5-a1000) obtained 0.421 which is the thirdbest score.Clustering Comparison: This evaluation settingaims to measure the similarity of the induced senseinventories for WSI systems.
Unlike supervisedmetrics, it avoids potential loss of sense informationsince this setting does not require any sense map-ping procedure to convert induced senses to a Word-System Fuzzy NMI Fuzzy B-CubedAllInstancesai-ku 0.065 0.390ai-ku(a1000) 0.035 0.320ai-ku(r5-a1000) 0.039 0.451Submitted-Best 0.065 0.483All-Best 0.065 0.623All-Worst 0.016 0.201Random-2 0.028 0.474Random-3 0.018 0.382Random-n 0.016 0.245Table 8: Scores on clustering measures (Fuzzy NMI:Fuzzy Normalized Mutual Information, Fuzzy B-Cubed:Fuzzy B-Cubed FScore)All instancesai-ku 7.72ai-ku(a1000) 7.72ai-ku(r5-a1000) 3.11Table 9: Average number of senses for each ai-ku systemson test dataNet sense.
ai-ku performed best for Fuzzy NMIamong other systems included baselines.
For FuzzyB-Cubed, ai-ku(r5a1000) outperformed random-3and random-n baselines.
Table 8 depicts the per-formance of our systems, best and worst systems aswell as the random baselines.The best scores for the graded word sense in-duction task in SemEval-2013 are mostly achievedby baselines in supervised setting.
Major problemis that there is huge sense differences between testand trial data regarding to number of sense distribu-tion.
Participants that used trial data as for param-eter tuning and picking the best algorithm achievedlower scores than baselines since test data does notshow properties of trial data.
Consequently, ai-kusystems produce significantly more senses than thegold-standard (see Table 9), and this mainly deterio-rates our performance.5 ConclusionIn this paper, we presented substitute vector repre-sentation and co-occurrence modeling on WSI task.Clustering substitute vectors directly gives lowerscores.
Thus, taking samples from each target?s sub-stitute vector, we obtained instance id - substituteword pairs.
These pairs were used by S-CODE.
Fi-305nally we run k-means on the S-CODE.
Although oursystems were highly ranked among the other submit-ted systems, no system showed better performancethan the top baselines for all metrics.
One explana-tion is that trial data does not reflect the characteris-tics of test data according to their number of sensedistributions.
Systems used trial data biased to re-turn more than one sense for each instance since av-erage number of sense is almost five in trial data.
Inaddition, baselines (except random ones) know truesense distribution in the test data beforehand whichmake them harder to beat.ReferencesEneko Agirre, David Mart?
?nez, Oier Lo?pez de Lacalleand Aitor Soroa.
2006.
Two graph-based algorithmsfor state-of-the-art WSD.
In Proceedings of the 2006Conference on Empirical Methods in Natural Lan-guage Processing, pages 585-593.Samuel Brody and Mirella Lapata.
2009.
Bayesian WordSense Induction.
In Proceedings of the 12th Con-ference of the European Chapter of the ACL (EACL2009), pages 103-111, Athens, Greece.Katrin Erk, Diana McCarthy, Nicholas Gaylord.
2009.Investigations on Word Senses and Word Usages, InProceedings of ACL-09 Singapore.Zellig S. Harris.
2012.
Distributional structure.
Word,Vol.
10, pages 146-162.Tobias Hawker.
2007.
USYD: WSD and lexical substi-tution using the Web 1T corpus In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 207214, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Ioannis Korkontzelos and Suresh Manandhar.
2010.UoY: Graphs of Unambiguous Vertices for WordSense Induction and Disambiguation.
In Proceedingsof the 5th International Workshop on Semantic Evalu-ation.
Uppsala, Sweden.David Jurgens.
2012.
An Evaluation of Graded SenseDisambiguation using Word Sense Induction.
In Se-mEval ?12 Proceedings of the First Joint Conferenceon Lexical and Computational Semantics.
pages 189-198.Yariv Maron, Michael Lamar, and Elie Bienenstock.2012.
Sphere embedding: An application to part-of-speech induction.
In J. Lafferty, C. K. I. Williams, J.Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors, InAdvances in Neural Information Processing Systems23, pages 1567-1575.Patrick Pantel and Dekang Lin.
2002.
Discovering WordSenses from Text.
In Proceedings of the 8th ACMSIGKDD Conference, pages 613-619, New York, NY,USA.
ACM.Ted Pedersen.
2010.
Duluth-WSI: SenseClusters Ap-plied to the Sense Induction Task of SemEval-2.
InProceedings of the 5th International Workshop on Se-mantic Evaluation.
pages 363-366, Uppsala, Sweden.Yves Peirsman, Kris Heylen and Dirk Geeraerts.
2008.Size Matters.
Tight and Loose Context Definitions inEnglish Word Space Models.
In Proceedings of theESSLLI Workshop on Distributional Lexical Seman-tics, Hamburg, Germany.Magnus Sahlgren.
2002.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. dissertation, De-partment of Linguistics, Stockholm University.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
Proceedings InternationalConference on Spoken Language Processing, pages257286.Jean Ve?ronis.
2004.
HyperLex: Lexical Cartography forInformation Retrieval.
Computer Speech & Language,18(3):223-252.Mehmet Ali Yatbaz, Enis Sert and Deniz Yuret.
2012.Learning Syntactic Categories Using ParadigmaticRepresentations of Word Context.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL 2012,July 12-14, 2012, Jeju Island, Korea.Deniz Yuret.
2012.
FASTSUBS: An Efficient Admis-sible Algorithm for Finding the Most Likely LexicalSubstitutes Using a Statistical Language Model.
Com-puting Research Repository (CoRR).Deniz Yuret.
2007.
KU: Word sense disambiguation bysubstitution.
In Proceedings of the Fourth Interna-tional Workshop on Semantic Evaluations (SemEval-2007), pages 207214, Prague, Czech Republic, June.Association for Computational Linguistics.Deniz Yuret and Mehmet Ali Yatbaz.
2010.
The noisychannel model for unsupervised word sense disam-biguation.
Computational Linguistics, Volume 36 Is-sue 1, March 2010, pages 111-127.306
