Proceedings of NAACL-HLT 2013, pages 569?578,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsOn Quality Ratings for Spoken Dialogue Systems ?
Experts vs. UsersStefan Ultes, Alexander Schmitt, and Wolfgang MinkerUlm UniversityAlbert-Einstein-Allee 4389073 Ulm, Germany{stefan.ultes,alexander.schmitt,wolfgang.minker}@uni-ulm.deAbstractIn the field of Intelligent User Interfaces, Spo-ken Dialogue Systems (SDSs) play a key roleas speech represents a true intuitive meansof human communication.
Deriving informa-tion about its quality can help rendering SDSsmore user-adaptive.
Work on automatic esti-mation of subjective quality usually relies onstatistical models.
To create those, manualdata annotation is required, which may be per-formed by actual users or by experts.
Here,both variants have their advantages and draw-backs.
In this paper, we analyze the relation-ship between user and expert ratings by in-vestigating models which combine the advan-tages of both types of ratings.
We explore twonovel approaches using statistical classifica-tion methods and evaluate those with a pre-existing corpus providing user and expert rat-ings.
After analyzing the results, we eventu-ally recommend to use expert ratings insteadof user ratings in general.1 Introduction and MotivationIn human-machine interaction it is important thatuser interfaces can adapt to the specific requirementsof its users.
Handicapped persons or angry users, forexample, have specific needs and should be treateddifferently than regular users.Speech is a major component of modern user in-terfaces as it is the natural means of human com-munication.
Therefore, it seems logical to use Spo-ken Dialogue Systems (SDS) as part of IntelligentUser Interfaces enabling speech communication ofdifferent complexity reaching from simple spokencommands up to complex dialogues.
Besides thespoken words, the speech signal also may be usedto acquire information about the user state, e.g.,about their emotional state (cf., e.g., (Polzehl etal., 2011))).
By additional analysis of the human-computer-dialogues, even more abstract informa-tion may be derived, e.g., the quality of the system(cf., e.g., (Engelbrecht and Mo?ller, 2010)).
Systemquality information may be used to adapt the sys-tem?s behavior online during the ongoing dialogue(cf.
(Ultes et al 2012)).For determining the quality of Spoken DialogueSystems, several aspects are of interest.
Mo?ller etal.
(2009) presented a taxonomy of quality criteria.They describe quality as a bipartite issue consistingof Quality of Service (QoS) and Quality of Experi-ence (QoE).
Quality of Service describes objectivecriteria like dialogue duration or number of turns.While these are well-defined items that can be de-termined easily, Quality of Experience, which de-scribes the user experience with subjective criteria,is more vague and without a sound definition, e.g.,User Satisfaction (US).Subjective aspects like US are either determinedby using questionnaires like SASSI (Hone and Gra-ham, 2000) or the ITU-standard augmented frame-work for questionnaires (Mo?ller, 2003), or by us-ing single-valued ratings, i.e., a rater only appliesone single score.
In general, two major categoriesof work on determining single-valued User Satisfac-tion exist.
The satisfaction ratings are applied either?
by users during or right after the dialogue or?
by experts by listening to recorded dialogues.569In this work, users or user raters are people whoactually perform a dialogue with the system and ap-ply ratings while doing so.
There is no constraintabout their expertise in the field of Human Com-puter Interaction or Spoken Dialogue Systems: Theymay be novices or have a high expertise.
With ex-perts or expert raters, we refer to people who arenot participating in the dialogue thus constitutinga completely different set of people.
Expert raterslisten to recorded dialogues after the interactionsand rate them by assuming the point of view of theactual person performing the dialogue.
These ex-perts are supposed to have some experience with di-alogue systems.
In this work, expert raters were ?ad-vanced students of computer science and engineer-ing?
(Schmitt et al 2011a).For User Satisfaction, ratings applied by the usersseem to be clearly the better choice over ratings ap-plied by third persons.
However, determining trueUser Satisfaction is only possible by asking realusers interacting with the system.
Ideally, the ratingsare applied by users talking to a system employed inthe field, e.g., commercial systems, as these usershave real concerns.For such Spoken Dialogue Systems, though, itis not easy to get users to apply quality ratingsto the dialogue ?
especially for each system-user-exchange.
The users would have to rate either bypressing a button on the phone or by speech, whichwould significantly influence the performance of thedialogue.
Longer dialogues imply longer call dura-tions which cost money.
Further, most callers onlywant to quickly get some information from the sys-tem.
Therefore, it may be assumed that most usersdo not want to engage in dialogues which are ar-tificially made longer.
This also inhabits the riskthat users who participated in long dialogues donot want to call again.
Therefore, collecting rat-ings applied by users are considered to be expensive.One possible way of overcoming the problem of rat-ing input would be to use some special installationwhich enables the users to provide ratings more eas-ily (cf.
(Schmitt et al 2011b)).
However, this is alsoexpensive and the system?s usability would be veryrestricted.
Further, this setup could most likely onlybe used in a lab situation.Expert raters, on the other hand, are able to simplylisten to the recorded dialogues and to apply ratings,e.g., by using a specialized rating software.
Thisprocess is much easier and does not require the sameamount of effort needed for acquiring user ratings.Further, as already pointed out, we refer to expertsas people who have some basic understanding of di-alogue systems but are not required to be high-levelexperts in the field.
That is why we believe that thesepeople can be found easily.As both categories of ratings have their advan-tages and disadvantages, this contribution aims atlearning about the differences and similarities ofuser and expert ratings with the ultimate goal ofeither being able to predict user ratings more effi-ciently or of advocating for replacing the use of userratings by using only expert ratings in general.Therefore, this work analyzes the relation be-tween quality ratings applied by user and expertraters by analyzing approaches which take advan-tage of both categories: Using the less expensiverating process with expert raters and still predict-ing real User Satisfaction ratings.
Moreover, thisworks?
goal is to shed light on the question whetherinformation about one rating (in this case the lessexpensive expert ratings) may be used to predict theother rating (the more expensive user ratings).
Forthis, we present two approaches applying two differ-ent statistical classification methods for a showcasecorpus.
Results of both methods are compared to agiven baseline.The remainder of this paper is organized as fol-lows.
First, we give a brief overview of work donein both categories (user ratings vs. expert ratings) inSection 2 and present our choice of data the analy-sis in this paper is based on in Section 3.
Further,evaluation metrics are illustrated in Section 4 andapproaches on facilitating prediction of user raterscores by expert rater information are presented inSection 5 followed by an evaluation and discussionof the results in Section 6.2 Significant Related WorkPredicting User Satisfaction for SDSs has been inthe focus of research for many years, most famouslythe PARADISE framework by Walker et al(1997).The authors assume a linear dependency betweenquantitative parameters derived from the dialogueand US, modeling this dependency using linear re-570gression.
Unfortunately, for generating the regres-sion model, weighting factors have to be computedfor each system anew.
This generates high costsas dialogues have to be performed with real userswhere each user further has to complete a question-naire after completing the dialogue.
Moreover, inthe PARADISE framework, only quality measure-ment for the whole dialogue (or system) is allowed.However, this is not suitable for using quality infor-mation for online adaption of the dialogue (cf.
(Ulteset al 2012)).
Furthermore, PARADISE relies onquestionnaires while we focus on work using single-valued ratings.Numerous work on predicting User Satisfactionas a single-valued rating task for each system-user-exchange has been performed in both categories.This work is briefly presented in the following.2.1 Expert RatingsHigashinaka et al(2010a) proposed a model to pre-dict turn-wise ratings for human-human dialogues(transcribed conversation) and human-machine di-alogues (text from chat system).
Ratings rangingfrom 1-7 were applied by two expert raters label-ing ?Smoothness?, ?Closeness?, and ?Willingness?not achieving a Match Rate per Rating (MR/R)1 ofmore than 0.2-0.24.
This results are only slightlyabove the random baseline of 0.14.
Further workby Higashinaka et al(2010b) uses ratings for over-all dialogues to predict ratings for each system-user-exchange.
Again, evaluating in three usersatisfaction categories ?Smoothness?, ?Closeness?,and ?Willingness?
with ratings ranging from 1-7achieved best performance of 0.19 MR/R.Interaction Quality (IQ) has been introduced bySchmitt et al(2011a) as an alternative performancemeasure to User Satisfaction.
In their terminology,US ratings are only applied by users.
As their pre-sented measure uses ratings applied by expert raters,a different term is used.
Each system-user exchangewas annotated by three different raters using strictguidelines.
The ratings ranging from 1-5 are usedas target variable for statistical classifiers using a setof automatically derivable interaction parameters asinput.
They achieve a MR/R of 0.58.1MR/R is equal to Unweighted Average Recall (UAR)which is explained in Section 4.2.2 User RatingsAn approach presented by Engelbrecht et al(2009)uses Hidden Markov Models (HMMs) to model theSDS as a process evolving over time.
User Satisfac-tion was predicted at any point within the dialogueon a 5 point scale.
Evaluation was performed basedon labels the users applied themselves during the di-alogue.Hara et al(2010) derived turn level ratings froman overall score applied by the users after the dia-logue.
Using n-gram models reflecting the dialoguehistory, the achieved results for recognizing UserSatisfaction on a 5 point scale showed to be hardlyabove chance.Work by Schmitt et al(2011b) deals with deter-mining User Satisfaction from ratings applied by theusers themselves during the dialogues.
A statisticalclassification model was trained using automaticallyderived interaction parameter to predict User Satis-faction for each system-user-exchange on a 5-pointscale achieving an MR/R of 0.49.3 CorpusThe corpus used by Schmitt et al(2011b) not onlycontains user ratings but also expert ratings whichmakes it a perfect candidate for our research pre-sented in this paper.
Adopting the terminology bySchmitt et al user ratings are described as User Sat-isfaction (US) whereas expert ratings are referred towith the term Interaction Quality (IQ) (cf.
(Schmittet al 2011a)).
The data used for all experimentsof this work was collected by Schmitt et al(2011b)during a lab user study with 38 users in the domainof the ?Let?s Go Bus Information?
system (Raux etal., 2006) of the Carnegie Mellon University in Pitts-burgh.
128 calls were collected consisting of a totalof 2,897 system-user exchanges.
Both ratings, IQand US, are at a scale from 1 to 5 where 1 stands for?extremely unsatisfied?
and 5 for ?satisfied?.
Eachdialogue starts with a rating of 5 as the user is ex-pected to be satisfied in the beginning because noth-ing unsatisfying has happened yet.Further, the corpus also provides interaction pa-rameters which may be used as input variablesfor the IQ and US recognition models.
Theseparameters have been derived automatically fromthree dialogue modules: Automatic Speech Recog-571s 1u 1s 2u 2s 3u 3s nu n?e 1e 2e 3e ne n?e n-1e n-2e 1e 2e 3e n+1?exchangelevel parameterswindowlevel parameters:{#}, {Mean}, etc.dialoguelevel parameters:#,Mean,etc.Figure 1: The three different modeling levels representing the interaction at exchange en: The most detailed exchangelevel, comprising parameters of the current exchange; the window level, capturing important parameters from theprevious n dialog steps (here n = 3); the dialog level, measuring overall performance values from the entire previousinteraction.nition, Spoken Language Understanding, and Dia-logue Management.
Furthermore, the parametersare modeled on three different levels (see Figure 1):?
Exchange level parameters can be derived di-rectly from the respective dialogue modules,e.g., ASRConfidence.?
Dialogue level parameters consist of counts (#),means (Mean), etc.
of the exchange level pa-rameters calculated from all exchanges of thewhole dialogue up to the current exchange, e.g.,MeanASRConfidence.?
Window level parameters consist of counts({#}), means ({Mean}), etc.
of the exchangelevel parameters calculated from the last threeexchanges, e.g., {Mean}ASRConfidence.4 Evaluation metricsFor measuring the performance of the classificationalgorithms, we rely on Unweighted Average Recall(UAR), Cohen?s Kappa and Spearman?s Rho.
Thelatter two also represent a measure for similarity ofpaired data.
All measures will be briefly describedin the following:Unweighted Average Recall The Unweighted Av-erage Recall (UAR) is defined as the sum of allclass-wise recalls rc divided by the number ofclasses |C|:UAR =1|C|?c?Crc .
(1)Recall rc for class c is defined asrc =1|Rc||Rc|?i=1?hiri , (2)where ?
is the Kronecker-delta, hi and ri rep-resent the corresponding hypothesis-reference-pair of rating i, and |Rc| the total number ofall ratings of class c. In other words, UARfor multi-class classification problems is the ac-curacy corrected by the effects of unbalanceddata.Cohen?s Kappa To measure the relative agreementbetween two corresponding sets of ratings, thenumber of label agreements corrected by thechance level of agreement divided by the max-imum proportion of times the labelers couldagree is computed.
?
is defined as?
=p0 ?
pc1?
pc, (3)where p0 is the rate of agreement and pc is thechance agreement (Cohen, 1960).
As US andIQ are on an ordinal scale, a weighting factor wis introduced reducing the discount of disagree-ments the smaller the difference is between tworatings (Cohen, 1968):w =|r1 ?
r2||rmax ?
rmin|.
(4)Here, r1 and r2 denote the rating pair and rmaxand rmin the maximal and minimal rating.
Thisresults inw = 0 for agreement andw = 1 if theratings have maximal difference.Spearman?s Rho The correlation of two variablesdescribes the degree by that one variable can beexpressed by the other.
Spearman?s Rank Cor-relation Coefficient is a non-parametric methodassuming a monotonic function between the572two variables (Spearman, 1904).
It is definedby?
=?i(xi ?
x?
)(yi ?
y?)?
?i(xi ?
x?
)2?i(yi ?
y?
)2, (5)where xi and yi are corresponding ranked rat-ings and x?
and y?
the mean ranks.
Thus, twosets of ratings can have total correlation even ifthey never agree.
This would happen if all rat-ings are shifted by the same value, for example.5 Recognition of US Using IQ InformationAs discussed in Section 1, automatic recognition ofratings applied by users as performed by Schmitt etal.
(2011b) for User Satisfaction is time-consumingand expensive.
Therefore, approaches are presentedwhich facilitate expert ratings, i.e., Interaction Qual-ity, with the hope of making US recognition morefeasible.
IQ an US are strongly related as both met-rics represent the same quantity applied by differ-ent rater groups.
Results of the Mann-Whitney Utest, which is used to test for significant differencebetween Interaction Quality and User Satisfaction,show their difference (p < 0.05) but values for Co-hen?s Kappa (Cohen, 1960) and Spearman?s RankCorrelation Coefficient (Spearman, 1904) empha-size the that IQ and US are quite similar.
Achieving?
= 0.5 can be considered as a moderate agreementaccording to Landis and Koch?s Kappa BenchmarkScale (Landis and Koch, 1977).
Furthermore, a cor-relation of ?
= 0.66 (p < 0.01) indicates a strongrelationship between IQ and US (Cohen, 1988).While it has been shown that user and expert rat-ings are similar, it is desirable nonetheless to beingable to predict real user ratings.
These ratings are thedesired kind of ratings when it comes to subjectivedialogue system assessment.
Only users can give arating about their satisfaction level, i.e., how theylike the system and the interaction with the system.However, user ratings are expensive as elaborated inSection 1.
Therefore, we investigate approaches torecognize US which rely on means of IQ recogni-tion.5.1 Belief-Based Sequential RecognitionMethods used for IQ and US recognition by Schmittet al(2011b; 2011a) suffer from the fact that thesequential character of the data is modeled inade-quately as they assume statistical independence be-tween the single exchanges (recognition of IQ andUS does not depend on the respective value of theprevious exchange).
Hence, we present a Marko-vian approach overcoming these issues.
A probabil-ity distribution over all US states, called belief state,is updated after each system-user-exchange takingalso into account the belief state of the previous ex-change.
This belief update2 is equivalent to the For-ward Algorithm known from Hidden Markov Mod-els (cf.
(Rabiner, 1989)).
In doing so, the new USprobabilities also depend on the US values of theprevious exchange.
Moreover, a latent variable isintroduced in order to decouple the target variableUS with the variable the observation probability de-pends on IQ.
This results in an indirect approachfor recognizing User Satisfaction that is based on themore affordable recognition of Interaction Qualityassuming that a universal mapping between IQ andUS exists.Thus, to determine the probability b(US) of hav-ing the true User Satisfaction label US after the cur-rent system-user-exchange, we rely on InteractionQuality recognition, whose observation probabilityis depicted as P (o|IQ).
Furthermore, for couplingboth quantities, we introduce a coherence probabil-ity P (IQ|US).
Belief update for estimating the newvalues for b?(US?)
is as follows:b?(US?)
= ?
?
?IQ?P (o?|IQ?)
?
P (IQ?|US?)?
?USP (US?|US)b(US) (6)The observation probability P (o?|IQ?)
is modeledusing confidence scores of classifiers applied for IQrecognition.
Further, we compute the sum over allprevious US beliefs b(US) weighted by the transi-tion probability P (US?|US).
Both, transition andcoherence probability have been computed by tak-ing the frequency of their occurrences in the trainingdata.
The ?
factor is used for normalization only.Since we are aiming at generating an estimate U?S2Terminology is taken from Partially Observable MarkovDecision Processes, cf.
(Kaelbling et al 1998)573at each exchange, it is calculated byU?S = arg maxUS?b?(US?)
(7)generating a sequence of estimates for each dia-logue.As the action of the system a can be expected toinfluence the satisfaction level of the user, action-dependency is added to Equation 6 resulting inb?(US?)
= ?
?
?IQ?P (o?|IQ?)
?
P (IQ?|US?, a)?
?USP (US?|US, a)b(US).
(8)Hence, each system action a influences coherenceand transition probabilities.
It should be noted thataction-dependency can only be introduced as in aSDS each turn a system action is selected and ex-ecuted by the dialogue manager.5.2 Model ExchangeWhile in Belief-Based Sequential Recognition, prob-ability models are used for coupling expert and userratings explicitly, a simpler approach has also beenexamined.
A statistical classifier trained on the tar-get variable IQ is used to evaluate classification ofthe target variable US.
This seems to be reasonableas the set of scores and meaning of the scores of bothmetrics are equivalent.
Furthermore, necessary pre-requisites are fulfilled: the sample corpus containsboth labels, the labels for US and IQ correspond, andboth recognition approaches are based on the samefeature set.6 Experiments and ResultsFor evaluating Belief-Based Sequential Recognition,not only the absolute performance is of interest butalso how this performance is influenced by the char-acteristics of the observation probability, i.e., theperformance of the applied statistical classificationapproach and the variance of their confidence scores.In order to obtain different confidence characteris-tics, multiple classification algorithms, or algorithmvariants respectively, are needed.
Hence, five statis-tical classifiers have been chosen arbitrarily to pro-duce the observation probabilities for Belief-BasedSequential Recognition:?
SVM3 with cubic kernel?
SVM with RBF-kernel?
Naive Bayes?
Naive Bayes with kernel?
Rule InductionIn contrast to Schmitt et al(2011b; 2011a), a re-duced feature set was used consisting of 43 parame-ters as some textual parameters were removed whichare very specific and take many different values, e.g.,UTTERANCE (the system utterance) or INTERPRE-TATION (the interpretation of the speech input).The resulting feature set consists of the followingparameters (parameter names are in accordance withthe parameter names of the LEGO corpus (Schmittet al 2012)):Exchange Level ACTIVITY, ACTIVITYTYPE,UTD, BARGED-IN?, ASRCONFIDENCE,MEANASRCONFIDENCE, TURNNUMBER,MODALITY, LOOPNAME, ASRRECOGNI-TIONSTATUS, ROLEINDEX, ROLENAME,NOISE?, HELPREQUEST?, REPROMPT?,WPST, WPUTDialogue Level #BARGEINS #ASRSUCCESS,#HELPREQUESTS, #TIMEOUTS, #TIME-OUTS ASRREJECTIONS, #ASRREJEC-TIONS, #REPROMPTS, #SYSTEMQUES-TIONS, #SYSTEMTURNS, #USERTURNS,%BARGEINS, %ASRSUCCESS, %HEL-PREQUESTS, %TIMEOUTS, %TIME-OUTS ASRREJECTIONS, %ASRREJEC-TIONS, %REPROMPTSWindow Level {#}TIMEOUTS ASRREJCTIONS,{#}HELPREQUESTS, {#}ASRREJECTIONS,{MEAN}ASRCONFIDENCE, {#}TIMEOUTS,{#}REPROMPTS, {#}SYSTEMQUESTIONS,{#}ASRSUCCESS, {#}BARGEINSAll results are evaluated with respect to the ref-erence experiment of direct US recognition (USrecognition using models trained on US).
This isperformed in accordance to Schmitt et al(2011b)using the statistical classification algorithms stated3Support Vector Machine, cf.
(Vapnik, 1995)574Table 1: Results (UAR, Cohen?s Kappa, and Spearman?sRho) of 10-fold cross-validation for US recognition of USrecognition using models trained on USClassifier UAR ?
?SVM (cubic Kernel) 0.39 0.33 0.48SVM (RBF-Kernel) 0.39 0.42 0.55Naive Bayes 0.36 0.40 0.55Naive Bayes (Kernel) 0.42 0.44 0.59Rule Induction 0.50 0.51 0.61Table 2: Results (UAR, Cohen?s Kappa, and Spearman?sRho) of 10-fold cross-validation for US recognition of theModel Exchange approach (trained on IQ, evaluated onUS)Classifier UAR ?
?SVM (cubic Kernel) 0.34 0.42 0.55SVM (RBF-Kernel) 0.34 0.42 0.58Naive Bayes 0.35 0.40 0.57Naive Bayes (Kernel) 0.34 0.37 0.60Rule Induction 0.34 0.42 0.59above.
The performance of the reference experimentis shown in Table 1.Using the same feature set, these classification al-gorithms are also applied for the evaluation of theModel Exchange approach using 10-fold cross val-idation.
Note that the parameters of the classifiersalso remained the same.
The data was partitionedrandomly on exchange level, i.e., without regardingtheir belonging to a specific dialogue.
The measuredresults of the Model Exchange approach for the fiveclassification methods can be seen in Table 2.While the results are significantly above chance4,comparing them to the reference experiment revealsthat in terms of UAR the reference experiment out-performs Model Exchange for all five classifiers.The achieved ?
and ?
values show similar scoresfor both the reference experiment and the Model Ex-change approach.
However, in the data used for theexperiments, the amount of occurrences of the rat-ings was not balanced (equal for all classes) whichhas been identified as the most likely reason for thiseffect.Experiments for Belief-Based Sequential Recog-nition have also been performed using 10-fold crossvalidation.
As complete dialogues and the order4UAR of 0.2 for five classesTable 3: Results (UAR, Cohen?s Kappa, and Spearman?sRho) of 10-fold cross-validation for US recognition ofaction-independent Belief-Based Sequential RecognitionClassifier UAR ?
?SVM (cubic Kernel) 0.28 0.36 0.48SVM (RBF-Kernel) 0.30 0.40 0.54Naive Bayes 0.32 0.39 0.54Naive Bayes (Kernel) 0.33 0.45 0.61Rule Induction 0.33 0.47 0.63Table 4: Results (UAR, Cohen?s Kappa, and Spearman?sRho) of 10-fold cross-validation for US recognition ofaction-dependent Belief-Based Sequential RecognitionClassifier UAR ?
?SVM (cubic Kernel) 0.28 0.35 0.48SVM (RBF-Kernel) 0.29 0.40 0.54Naive Bayes 0.32 0.40 0.55Naive Bayes (Kernel) 0.34 0.44 0.60Rule Induction 0.35 0.47 0.62of exchanges within the dialogues are important forthis approach, the data was partitioned randomly onthe dialogue level.
As previously explained, for theprobability distributions of the observation proba-bility model, classification results of IQ recognitionwith 10-fold cross validation has been used in orderto get good estimates for the whole data set.
Re-sults for the action-independent version can be seenin Table 3.For the action-dependent version, four differentbasic actions ANNOUNCEMENT, CONFIRMATION,QUESTION, and WAIT have been used, generat-ing results presented in Table 4.
The results il-lustrate that neither action-independent nor action-dependent Belief-Based Sequential Recognition canoutperform the reference experiment (cf.
Table 1).Still, both variants achieve results clearly abovechance.
Again, the unbalanced data causes ?
and?
to be similar to the reference experiment.A comparison of the action-independent with theaction-dependent approach shows almost no differ-ences in their performances.
Only a slight tendencytowards better UARs for action-dependency can bespotted.Figure 2 displays the performances of both vari-ants of Belief-Based Sequential Recognition alongwith performance of IQ recognition and the vari-ance ?2 of the corresponding confidence distribu-5750.200.300.400.500.60 0.000.10SVM?(cubic)SVM?(RBF)BayesBayes?(K)RuleIQ??
?IQUS?BeliefUS?Belief?ActionFigure 2: UAR of IQ recognition and Belief-Based Se-quential Recognition along with ?2 of confidence distri-butions of IQ recognitionTable 5: Recognition performance and variance of confi-dence distributions for IQ recognitionClassifier ?2 UAR ?
?SVM (cubic Kernel) 0.03 0.38 0.54 0.69SVM (RBF-Kernel) 0.05 0.48 0.65 0.77Naive Bayes 0.13 0.49 0.57 0.71Naive Bayes (Kernel) 0.12 0.52 0.59 0.73Rule Induction 0.13 0.55 0.68 0.79tion (cf.
Table 5).
It can easily be seen that withrising UAR for IQ recognition, ?2 also rises.
Thisdirectly transfers to the performance of the Belief-Based Sequential Recognition.
The more accu-rate the observation performance, the more accuratethe belief prediction.
Furthermore, when compar-ing the action-dependent to the action-independentvariant of Belief-Based Sequential Recognition, bet-ter IQ performance and therefore a higher variancealso causes slightly better results for the action-dependent variant.
These differences, however, areonly marginally.
Therefore, they do not allow fordrawing a conclusion.7 ConclusionsFor estimating User Satisfaction-like ratings, twocategories exist: work relying on user ratings andwork relying on expert ratings.
To learn somethingabout their differences and similarities, we exploredthe possibility of using the information encoded inthe expert ratings to predict user ratings with thehope to get acceptable user rating prediction results.Therefore, we investigated if it is possible to de-termine the preferred true User Satisfaction valuebased on less expensive expert ratings.
For this, acorpus containing both kinds of ratings was chosen,i.e., User Satisfaction (US) and Interaction Qual-ity (IQ) ratings.
Furthermore, interaction parame-ters were used to create statistical recognition mod-els for predicting IQ and US, respectively.
Two ap-proaches have been investigated: Belief-Based Se-quential Recognition, which is based on an HMM-like structure with IQ as an additional latent variable,and Model Exchange, which uses statistical modelstrained on IQ to recognize US.
Unfortunately, nei-ther Belief-Based Sequential Recognition nor ModelExchange achieved results with an acceptable UAR.The high correlation between expert and user rat-ings, depicted by high values for Cohen?s ?
andSpearman?s ?, already allow the conclusion that ex-pert ratings can be used as a good replacement foruser ratings.
Moreover, the presented recognition re-sults of the Model Exchange approach being clearlyabove chance underpin the strong similarity of IQand US.
Furthermore, IQ recognition is much morereliable and accurate than US recognition (shown byhigher UAR, ?
and ?
values).While the experiments disproved the hope of get-ting acceptable user rating prediction results, the ob-tained results confirmed the similarity between bothkinds of ratings.
And as it is not necessary to useuser ratings for most applications, e.g., for using thequality information to automatically improve the in-teraction (cf.
(Ultes et al 2012)), we believe that itsuffices to use expert ratings as those can be acquiredeasier and less expensively and are similar enoughto user ratings.
Prompting the user to apply qualityratings in everyday situations with real-life systemswill always be annoying to the user while recordingof such interactions are always much easier to rate.By providing a study for determining quality rat-ings of dialogues, we hope to encourage other re-searchers to look into this research for other param-eters, e.g., emotion recognition.ReferencesJacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
In Educational and Psychological Mea-surement, volume 20, pages 37?46, April.Jacob Cohen.
1968.
Weighted kappa: Nominal scale576agreement provision for scaled disagreement or partialcredit.
Psychological bulletin, 70(4):213.Jacob Cohen.
1988.
Statistical power analysis for thebehavioral sciences.
New Jersey: Lawrence ErlbaumAssociates, July.Klaus-Peter Engelbrecht and Sebastian Mo?ller.
2010.
AUser Model to Predict User Satisfaction with SpokenDialog Systems.
In Gary Geunbae Lee, Joseph Mari-ani, Wolfgang Minker, and Satoshi Nakamura, editors,Spoken Dialogue Systems for Ambient Environments.2nd Int.
Workshop on Spoken Dialogue Systems Tech-nology, Lecture Notes in Artificial Intelligence, pages150?155.
Springer, October.Klaus-Peter Engelbrecht, Florian Go?dde, Felix Hartard,Hamed Ketabdar, and Sebastian Mo?ller.
2009.
Mod-eling user satisfaction with hidden markov model.
InSIGDIAL ?09: Proceedings of the SIGDIAL 2009 Con-ference, pages 170?177, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Sunao Hara, Norihide Kitaoka, and Kazuya Takeda.2010.
Estimation method of user satisfaction usingn-gram-based dialog history model for spoken dialogsystem.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC?10), Valletta, Malta, May.
European LanguageResources Association (ELRA).Ryuichiro Higashinaka, Yasuhiro Minami, KohjiDohsaka, and Toyomi Meguro.
2010a.
Issues inpredicting user satisfaction transitions in dialogues:Individual differences, evaluation criteria, and predic-tion models.
In Gary Lee, Joseph Mariani, WolfgangMinker, and Satoshi Nakamura, editors, SpokenDialogue Systems for Ambient Environments, volume6392 of Lecture Notes in Computer Science, pages48?60.
Springer Berlin / Heidelberg.Ryuichiro Higashinaka, Yasuhiro Minami, KohjiDohsaka, and Toyomi Meguro.
2010b.
Modelinguser satisfaction transitions in dialogues from over-all ratings.
In Proceedings of the SIGDIAL 2010Conference, pages 18?27, Tokyo, Japan, September.Association for Computational Linguistics.Kate S. Hone and Robert Graham.
2000.
Towards a toolfor the subjective assessment of speech system inter-faces (sassi).
Nat.
Lang.
Eng., 6(3-4):287?303.L.
P. Kaelbling, M. L. Littman, and A. R. Cassandra.1998.
Planning and acting in partially observablestochastic domains.
Artificial Intelligence, 101(1-2):99?134.J.
R. Landis and G. G. Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1):159?174, March.Sebastian Mo?ller, Klaus-Peter Engelbrecht, C. Ku?hnel,I.
Wechsung, and B. Weiss.
2009.
A taxonomy ofquality of service and quality of experience of multi-modal human-machine interaction.
In Quality of Mul-timedia Experience, 2009.
QoMEx 2009.
InternationalWorkshop on, pages 7?12, July.Sebastian Mo?ller.
2003.
Subjective Quality Evalua-tion of Telephone Services Based on Spoken Dia-logue Systems.
ITU-T Recommendation P.851, Inter-national Telecommunication Union, Geneva, Switzer-land, November.
Based on ITU-T Contr.
COM 12-59(2003).Tim Polzehl, Alexander Schmitt, and Florian Metze.2011.
Salient features for anger recognition in germanand english ivr portals.
In Wolfgang Minker, Gary Ge-unbae Lee, Satoshi Nakamura, and Joseph Mariani,editors, Spoken Dialogue Systems Technology and De-sign, pages 83?105.
Springer New York.
10.1007/978-1-4419-7934-6 4.Lawrence R. Rabiner.
1989.
A tutorial on hidden Markovmodels and selected applications in speech recogni-tion.
Morgan Kaufmann Publishers Inc., San Fran-cisco, CA, USA.Antoine Raux, Dan Bohus, Brian Langner, Alan W.Black, and Maxine Eskenazi.
2006.
Doing researchon a deployed spoken dialogue system: One year oflets go!
experience.
In Proc.
of the International Con-ference on Speech and Language Processing (ICSLP),September.Alexander Schmitt, Benjamin Schatz, and WolfgangMinker.
2011a.
Modeling and predicting quality inspoken human-computer interaction.
In Proceedingsof the SIGDIAL 2011 Conference, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.Alexander Schmitt, Benjamin Schatz, and WolfgangMinker.
2011b.
A statistical approach for estimat-ing user satisfaction in spoken human-machine inter-action.
In Proceedings of the IEEE Jordan Confer-ence on Applied Electrical Engineering and Comput-ing Technologies (AEECT), Amman, Jordan, Decem-ber.
IEEE.Alexander Schmitt, Stefan Ultes, and Wolfgang Minker.2012.
A parameterized and annotated corpus of thecmu let?s go bus information system.
In InternationalConference on Language Resources and Evaluation(LREC).C.
Spearman.
1904.
The proof and measurement of as-sociation between two things.
American Journal ofPsychology, 15:88?103.Stefan Ultes, Alexander Schmitt, and Wolfgang Minker.2012.
Towards quality-adaptive spoken dialogue man-agement.
In NAACL-HLT Workshop on Future di-rections and needs in the Spoken Dialog Commu-577nity: Tools and Data (SDCTD 2012), pages 49?52,Montre?al, Canada, June.
Association for Computa-tional Linguistics.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc., NewYork, NY, USA.Marilyn Walker, Diane Litman, Candace A. Kamm, andAlicia Abella.
1997.
Paradise: a framework for eval-uating spoken dialogue agents.
In Proceedings of theeighth conference on European chapter of the Associ-ation for Computational Linguistics, pages 271?280,Morristown, NJ, USA.
Association for ComputationalLinguistics.578
