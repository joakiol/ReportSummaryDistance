Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 308?316,Sydney, July 2006. c?2006 Association for Computational LinguisticsPriming Effects in Combinatory Categorial GrammarDavid ReitterSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UKdreitter@inf.ed.ac.ukJulia HockenmaierInst.
for Res.
in Cognitive ScienceUniversity of Pennsylvania3401 Walnut StreetPhiladelphia PA 19104, USAjuliahr@cis.upenn.eduFrank KellerSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UKkeller@inf.ed.ac.ukAbstractThis paper presents a corpus-based ac-count of structural priming in human sen-tence processing, focusing on the role thatsyntactic representations play in such anaccount.
We estimate the strength of struc-tural priming effects from a corpus ofspontaneous spoken dialogue, annotatedsyntactically with Combinatory Catego-rial Grammar (CCG) derivations.
Thismethodology allows us to test a range ofpredictions that CCG makes about prim-ing.
In particular, we present evidencefor priming between lexical and syntacticcategories encoding partially satisfied sub-categorization frames, and we show thatpriming effects exist both for incrementaland normal-form CCG derivations.1 IntroductionIn psycholinguistics, priming refers to the fact thatspeakers prefer to reuse recently encountered lin-guistic material.
Priming effects typically man-ifest themselves in shorter processing times orhigher usage frequencies for reused material com-pared to non-reused material.
These effects are at-tested both in language comprehension and in lan-guage production.
Structural priming occurs whena speaker repeats a syntactic decision, and hasbeen demonstrated in numerous experiments overthe past two decades (e.g., Bock, 1986; Braniganet al, 2000).
These experimental findings showthat subjects are more likely to choose, e.g., apassive voice construction if they have previouslycomprehended or produced such a construction.Recent studies have used syntactically anno-tated corpora to investigate structural priming.The results have demonstrated the existence ofpriming effects in corpus data: they occur for spe-cific syntactic constructions (Gries, 2005; Szm-recsanyi, 2005), consistent with the experimen-tal literature, but also generalize to syntactic rulesacross the board, which repeated more often thanexpected by chance (Reitter et al, 2006b; Dubeyet al, 2006).
In the present paper, we build onthis corpus-based approach to priming, but focuson the role of the underlying syntactic represen-tations.
In particular, we use priming to evaluateclaims resulting from a particular syntactic theory,which is a way of testing the representational as-sumptions it makes.Using priming effects to inform syntactic the-ory is a novel idea; previous corpus-based primingstudies have simply worked with uncontroversialclasses of constructions (e.g., passive/active).
Thecontribution of this paper is to overcome this limi-tation by defining a computational model of prim-ing with a clear interface to a particular syntac-tic framework.
The general assumption we makeis that priming is a phenomenon relating to gram-matical constituents ?
these constituents determinethe syntactic choices whose repetition can lead topriming.
Crucially, grammatical frameworks dif-fer in the grammatical constituents they assume,and therefore predict different sets of priming ef-fects.We require the following ingredients to pursuethis approach: a syntactic theory that identifiesa set of constituents, a corpus of linguistic dataannotated according to that syntactic theory, anda statistical model that estimates the strength ofpriming based on a set of external factors.
We canthen derive predictions for the influence of thesefactors from the syntactic theory, and test themusing the statistical model.
In this paper, we useregression models to quantify structural primingeffects and to verify predictions made by Com-binatory Categorial Grammar (CCG, Steedman(2000)), a syntactic framework that has the theo-retical potential to elegantly explain some of thephenomena discovered in priming experiments.308CCG is distinguished from most other gram-matical theories by the fact that its rules aretype-dependent, rather than structure-dependentlike classical transformations.
Such rules adherestrictly to the constituent condition on rules, i.e.,they apply to and yield constituents.
Moreover,the syntactic types that determine the applicabilityof rules in derivations are transparent to (i.e., aredetermined, though not necessarily uniquely, by)the semantic types that they are associated with.As a consequence, syntactic types are more ex-pressive and more numerous than standard parts ofspeech: there are around 500 highly frequent CCGtypes, against the standard 50 or so Penn TreebankPOS tags.
As we will see below, these propertiesallow CCG to discard a number of traditional as-sumptions concerning surface constituency.
Theyalso allow us to make a number of testable pre-dictions concerning priming effects, most impor-tantly (a) that priming effects are type-driven andindependent of derivation, and, as a corollary;(b) that lexical and derived constituents of thesame type can prime each other.
These effects arenot expected under more traditional views of prim-ing as structure-dependent.This paper is organized as follows: Section 2explains the relationship between structural prim-ing and CCG, which leads to a set of specific pre-dictions, detailed in Section 3.
Sections 4 and 5present the methodology employed to test thesepredictions, describing the corpus data and the sta-tistical analysis used.
Section 6 then presents theresults of three experiments that deal with primingof lexical vs. phrasal categories, priming in incre-mental vs. normal form derivations, and frequencyeffects in priming.
Section 7 provides a discussionof the implications of these findings.2 Background2.1 Structural PrimingPrevious studies of structural priming (Bock,1986; Branigan et al, 2000) have made few the-oretical assumptions about syntax, regardless ofwhether the studies were based on planned exper-iments or corpora.
They leverage the fact that al-ternations such as He gave Raquel the car keys vs.He gave the car keys to Raquel are nearly equiva-lent in semantics, but differ in their syntactic struc-ture (double object vs. prepositional object).
Insuch experiments, subjects are first exposed to aprime, i.e., they have to comprehend or produceeither the double object or the prepositional ob-ject structure.
In the subsequent trial, the target,they are the free to produce or comprehend eitherof the two structures, but they tend to prefer theone that has been primed.
In corpus studies, thefrequencies of the alternative constructions can becompared in a similar fashion (Gries, 2005; Szm-recsanyi, 2005).Reitter et al (2006b) present a different methodto examine priming effects in the general case.Rather than selecting specific syntactic alterna-tions, general syntactic units are identified.
Thismethod detects syntactic repetition in corpora andcorrelates its probability with the distance betweenprime and target, where at great distance, any rep-etition can be attributed to chance.
The size ofthe priming effect is then estimated as the differ-ence between the repetition probability close tothe prime and far away from the prime.
This isa way of factoring out chance repetition (whichis required if we do not deal with syntactic alter-nations).
By relying on syntactic units, the prim-ing model includes implicit assumptions about theparticular syntactic framework used to annotatethe corpus under investigation.2.2 Priming and Lexicalized GrammarPrevious work has demonstrated that priming ef-fects on different linguistic levels are not indepen-dent (Pickering and Branigan, 1998).
Lexical rep-etition makes repetition on the syntactic level morelikely.
For instance, suppose we have two verbalphrases (prime, target) produced only a few sec-onds apart.
Priming means that the target is morelikely to assume the same syntactic form (e.g., apassive) as the prime.
Furthermore, if the headverbs in prime and target are identical, experi-ments have demonstrated a stronger priming ef-fect.
This effect seems to indicate that lexical andsyntactic representations in the grammar share thesame information (e.g., subcategorization infor-mation), and therefore these representations canprime each other.Consequently, we treat subcategorization ascoterminous with syntactic type, rather than as afeature exclusively associated with lexemes.
Suchtypes determine the context of a lexeme or phrase,and are determined by derivation.
Such an anal-ysis is exactly what categorial grammars suggest.The rich set of syntactic types that categories af-ford may be just sufficient to describe all and only309the units that can show priming effects duringsyntactic processing.
That is to say that syntac-tic priming is categorial type-priming, rather thanstructural priming.Consistent with this view, Pickering and Brani-gan (1998) assume that morphosyntactic featuressuch as tense, aspect or number are represented in-dependently from combinatorial properties whichspecify the contextual requirements of a lexicalitem.
Property groups are represented centrallyand shared between lexicon entries, so that theymay ?
separately ?
prime each other.
For ex-ample, the pre-nominal adjective red in the redbook primes other pre-nominal adjectives, but nota post-nominal relative clause (the book that?s red)(Cleland and Pickering, 2003; Scheepers, 2003).However, if a lexical item can prime a phrasalconstituent of the same type, and vice versa, thena type-driven grammar formalism like CCG canprovide a simple account of the effect, becauselexical and derived syntactic types have the samecombinatory potential, which is completely spec-ified by the type, whereas in structure-driven the-ories, this information is only implicitly given inthe derivational process.2.3 Combinatory Categorial GrammarCCG (Steedman, 2000) is a mildly context-sensitive, lexicalized grammar formalism with atransparent syntax-semantics interface and a flex-ible constituent structure that is of particular in-terest to psycholinguistics, since it allows the con-struction of incremental derivations.
CCG has alsoenjoyed the interest of the NLP community, withhigh-accuracy wide-coverage parsers(Clark andCurran, 2004; Hockenmaier and Steedman, 2002)and generators1 available (White and Baldridge,2003).Words are associated with lexical categorieswhich specify their subcategorization behaviour,eg.
((S[dcl]\NP)/NP)/NP is the lexical categoryfor (tensed) ditransitive verbs in English such asgives or send, which expect two NP objects totheir right, and one NP subject to their left.
Com-plex categories X/Y or X\Y are functors whichyield a constituent with category X, if they are ap-plied to a constituent with category Y to their right(/Y) or to their left (\Y).Constituents are combined via a small set ofcombinatory rule schemata:Forward Application: X/Y Y ?> X1http://opennlp.sourceforge.net/Backward Application: Y X\Y ?> XForward Composition: X/Y Y/Z ?B X/ZBackward Composition: Y\Z X\Y ?B X\ZBackw.
Crossed Composition: Y/Z X\Y ?B X/ZForward Type-raising: X ?T T/(T\X)Coordination: X conj X ??
XFunction application is the most basic operation(and used by all variants of categorial grammar):I saw the manNP (S\NP)/NP NP>S\NP<SComposition (B) and type-raising (T) are neces-sary for the analysis of long-range dependenciesand for incremental derivations.
CCG uses thesame lexical categories for long-range dependen-cies that arise eg.
in wh-movement or coordina-tion as for local dependencies, and does not re-quire traces:the man that I sawNP (NP\NP)/(S/NP) NP (S\NP)/NP>TS/(S\NP)>BS/NP>NP\NPI saw and you heard the manNP (S\NP)/NP conj NP (S\NP)/NP>T >TS/(S\NP) S/(S\NP)>B >BS/NP S/NP<?>S/NP>SThe combinatory rules of CCG allow multiple,semantically equivalent, syntactic derivations ofthe same sentence.
This spurious ambiguity isthe result of CCG?s flexible constituent structure,which can account for long-range dependenciesand coordination (as in the above example), andalso for interaction with information structure.CCG parsers often limit the use of the combi-natory rules (in particular: type-raising) to obtaina single right-branching normal form derivation(Eisner, 1996) for each possible semantic inter-pretation.
Such normal form derivations only usecomposition and type-raising where syntacticallynecessary (eg.
in relative clauses).3 Predictions3.1 Priming EffectsWe expect priming effects to apply to CCG cat-egories, which describe the type of a constituentincluding the arguments it expects.
Under our as-sumption that priming manifests itself as a ten-dency for repetition, repetition probability shouldbe higher for short distances from a prime (seeSection 5.2 for details).3103.2 Terminal and Non-terminal CategoriesIn categorial grammar, lexical categories specifythe subcategorization behavior of their heads, cap-turing local and non-local arguments, and a smallset of rule schemata defines how constituents canbe combined.Phrasal constituents may have the same cate-gories as lexical items.
For example, the verb sawmight have the (lexical) category (S\NP)/NP,which allows it to combine with an NP to the right.The resulting constituent for saw Johanna wouldbe of category S\NP ?
a constituent which expectsan NP (the subject) to its left, and also the lexi-cal category of an intransitive verb.
Similarly, theconstituent consisting of a ditransitive verb and itsobject, gives the money, has the same category assaw.
Under the assumption that priming occurs forthese categories, we proceed to test a hypothesisthat follows from the fact that categories merelyencode unsatisfied subcategorized arguments.Given that a transitive verb has the same cat-egory as the constituent formed by a ditransitiveverb and its direct object, we would expect thatboth categories can prime each other, if they arecognitive units.
More generally, we would expectthat lexical (terminal) and phrasal (non-terminal)categories of the same syntactic type may primeeach other.
The interaction of such conditions withthe priming effect can be quantified in the statisti-cal model.3.3 Incrementality of AnalysesType-raising and composition allow derivationsthat are mostly left-branching, or incremental.Adopting a left-to-right processing order for a sen-tence is important, if the syntactic theory is tomake psycholinguistically viable predictions (Niv,1994; Steedman, 2000).Pickering et al (2002) present priming experi-ments that suggest that, in production, structuraldominance and linearization do not take place indifferent stages.
Their argument involves verbalphrases with a shifted prepositional object suchas showed to the mechanic a torn overall.
At adominance-only level, such phrases are equivalentto non-shifted prepositional constructions (showeda torn overall to the mechanic), but the two vari-ants may be differentiated at a linearization stage.Shifted primes do not prime prepositional objectsin their canonical position, thus priming must oc-cur at a linearized level, and a separate dominancelevel seems unlikely (unless priming is selective).CCG is compatible with one-stage formulations ofsyntax, as no transformation is assumed and cate-gories encode linearization together with subcate-gorization.CCG assumes that the processor may producesyntactically different, but semantically equivalentderivations.2 So, while neither the incrementalanalysis we generate, nor the normal-form, rep-resent one single correct derivation, they are twoextremes of a ?spectrum?
of derivations.
We hy-pothesize that priming effects predicted on the ba-sis of incremental CCG analyses will be as strongthan those predicted on the basis of their normal-form equivalents.4 Corpus Data4.1 The Switchboard CorpusThe Switchboard (Marcus et al, 1994) corpus con-tains transcriptions of spoken, spontaneous con-versation annotated with phrase-structure trees.Dialogues were recorded over the telephoneamong randomly paired North American speak-ers, who were just given a general topic to talkabout.
80,000 utterances of the corpus have beenannotated with syntactic structure.
This portion,included in the Penn Treebank, has been time-aligned (per word) in the Paraphrase project (Car-letta et al, 2004).Using the same regression technique as em-ployed here, Reitter et al (2006b) found a markedstructural priming effect for Penn-Treebank stylephrase structure rules in Switchboard.4.2 DisfluenciesSpeech is often disfluent, and speech repairs areknown to repeat large portions of the precedingcontext (Johnson and Charniak, 2004).
The orig-inal Switchboard transcripts contains these disflu-encies (marked up as EDITED):( (S >>>(EDITED(RM (-DFL- \bs [) )(EDITED(RM (-DFL- \bs [) )(CC And)(, ,)(IP (-DFL- \bs +) ))(CC and)(, ,)(RS (-DFL- \bs ]) )(IP (-DFL- \bs +) ))<<<2Selectional criteria such as information structure and in-tonation allow to distinguish between semantically differentanalyses.311(CC and)>>>(RS (-DFL- \bs ]) )<<<(NP-SBJ (PRP I) )(VP (VBP guess)(SBAR (-NONE- 0)(S (NP-SBJ (DT that) )(VP (BES ?s)(SBAR-NOM-PRD(WHNP-1 (WP what) )(S (NP-SBJ (PRP I) )(ADVP (RB really) )(VP (VBP like)(NP (-NONE- *T*-1) ))))))))(.
.)
(-DFL- E_S) ))It is unclear to what extent these repetitionsare due to priming rather than simple correc-tion.
In disfluent utterances, we therefore elimi-nate reparanda and only keep repairs (the portionsmarked with >...< are removed).
Hesitations (uh,etc.
), and utterances with unfinished constituentsare also ignored.4.3 Translating Switchboard to CCGSince the Switchboard annotation is almost iden-tical to the one of the Penn Treebank, we use asimilar translation algorithm to Hockenmaier andSteedman (2005).
We identify heads, argumentsand adjuncts, binarize the trees, and assign cat-egories in a recursive top-down fashion.
Nonlo-cal dependencies that arise through wh-movementand right node raising (*T* and *RNR* traces) arecaptured in the resulting derivation.
Figure 1 (left)shows the rightmost normal form CCG derivationwe obtain for the above tree.
We then transformthis normal form derivation into the most incre-mental (i.e., left-branching) derivation possible, asshown in Figure 1 (right).This transformation is done by a top-down re-cursive procedure, which changes each tree ofdepth two into an equivalent left-branching anal-ysis if the combinatory rules allow it.
This pro-cedure is run until no further transformation canbe executed.
The lexical categories of both deriva-tions are identical.5 Statistical Analysis5.1 Priming of CategoriesCCG assumes a minimal set of combinatory ruleschemata.
Much more than in those rules, syntac-tic decisions are evident from the categories thatoccur in the derivation.Given the categories for each utterance, we canidentify their repeated use.
A certain amountof repetition will obviously be coincidental.
Butstructural priming predicts that a target categorywill occur more frequently closer to a potentialprime of the same category.
Therefore, we cancorrelate the probability of repetition with the dis-tance between prime and target.
Generalized Lin-ear Mixed Effects Models (GLMMs, see next sec-tion) allow us to evaluate and quantify this corre-lation.Every syntactic category is counted as a poten-tial prime and (almost always) as a target for prim-ing.
Because interlocutors tend to stick to a topicduring a conversation for some time, we excludecases of syntactic repetition that are a results ofthe repetition of a whole phrase.Previous work points out that priming is sensi-tive to frequency (Scheepers (2003) for high/lowrelative clause attachments, (Reitter et al, 2006a)for phrase structure rules).
Highly frequent itemsdo not receive (as much) priming.
We includethe logarithm of the raw frequency of the syntac-tic category in Switchboard (LNFREQ) to approx-imate the effect that frequency has on accessibilityof the category.5.2 Generalized Linear Mixed EffectsRegressionWe use generalized linear mixed effects regressionmodels (GLMM, Venables and Ripley (2002)) topredict a response for a number of given categorial(?factor?)
or continuous (?predictor?)
explanatoryvariables (features).
Our data is made up of in-stances of repetition examples and non-repetitionexamples from the corpus.
For each target in-stance of a syntactic category c occurring in aderivation and spanning a constituent that beginsat time t, we look back for possible instances ofconstituents with the same category (the prime)in a time frame of [t ?
d ?
0.5; t ?
d + 0.5] sec-onds.
If such instances can be found, we have apositive example of repetition.
Otherwise, c is in-cluded as a data point with a negative outcome.We do so for a range of different distances d, com-monly 1 ?
d ?
15 seconds.3 For each data point,we include the logarithm of the distance d betweenpriming period and target as an explanatory vari-able LNDIST.
(See Reitter et al (2006b) for aworked example.
)In order to eliminate cases of lexical repeti-tion of a phrase, e.g., names or lexicalized noun3This approach uses a number of data points per target,looking backwards for primes.
The opposite way ?
lookingforwards for targets ?
would make similar predictions.312Normal form derivation Incremental derivationS[dcl]S/SandS[dcl]S/(S\NP)NPIS[dcl]\NP(S[dcl]\NP)/S[dcl]guessS[dcl]S/(S\NP)NPthatS[dcl]\NP(S[dcl]\NP)/NP?sNPNP/(S[dcl]/NP)whatS[dcl]/NPS/(S\NP)NPI(S[dcl]\NP)/NP(S\NP)/(S\NP)really(S[dcl]\NP)/NPlikeS[dcl]S[dcl]/(S[dcl]/NP)S[dcl]/NPS[dcl]/(S\NP)S[dcl]/S[dcl]S/(S\NP)S/SandS/(S\NP)NPI(S[dcl]\NP)/S[dcl]guessS/(S\NP)NPthat(S[dcl]\NP)/NP?sNP/(S[dcl]/NP)whatS[dcl]/NPS/(S\NP)S/(S\NP)NPI(S\NP)/(S\NP)really(S[dcl]\NP)/NPlikeFigure 1: Two derivations (normal form: left), incremental: right) for the sentence fragment and I guessthat?s what I really like from Switchboard.phrases, which we consider topic-dependent or in-stances of lexical priming, we only collect syntac-tic repetitions with at least one differing word.Without syntactic priming, we would assumethat there is no correlation between the probabil-ity that a data point is positive (repetition occurs)and distance d. With priming, we would expectthat the probability is inversely proportional to d.Our model uses lnd as predictor LNDIST, sincememory effects usually decay exponentially.The regression model fitted is then simply achoice of coefficients ?i, among them one for eachexplanatory variable i.
?i expresses the contribu-tion of i to the probability of the outcome event,that is, in our case, successful priming.
The coeffi-cient of interest is the one for the time correlation,i.e.
?lnDist .
It specifies the strength of decay ofrepetition probability over time.
If no other vari-ables are present, a model estimates the repetitionprobability for a data point i asp?i = ?0 +?lnDist ln DISTiPriming is present if the estimated parameter isnegative, i.e.
the repetition probability decreaseswith increasing distance between prime and target.Other explanatory variables, such as ROLE,which indicates whether priming occurs within aspeaker (production-production priming, PP) orin between speakers (comprehension-productionpriming, CP), receive an interaction coefficientthat adds linearly to ?lnDist .
Additional interac-tion variables are included depending on the ex-perimental question.44Lastly, we identify the target utterance in a random fac-tor in our model, grouping the several measurements (15 forthe different distances from each target) as repeated measure-ments, since they depend on the same target category occur-rence and are partially inter-dependent.From the data produced, we include all casesof reptition and a an equal number of randomlysampled non-repetition cases.56 Experiments6.1 Experiment 1: Priming in Incrementaland Normal-form DerivationsHypothesis CCG assumes a multiplicity of se-mantically equivalent derivations with differentsyntactic constituent structures.
Here, we in-vestigate whether two of these, the normal-formand the most incremental derivation, differ in thestrength with which syntactic priming occurs.Method A joint model was built containing rep-etition data from both types of derivations.
Sincewe are only interested in cases where the twoderivations differ, we excluded all constituentswhere a string of words was analyzed as a con-stituent in both derivations.
This produced a dataset where the two derivations could be contrasted.A factor DERIVATION in the model indicateswhether the repetition occurred in a normal-form(NF) or an incremental derivation (INC).Results Significant and substantial priming ispresent in both types of derivations, for both PPand CP priming.
There is no significant differencein priming strength between normal-form andincremental derivations (?lnDist:NF = 0.008, p =0.95).
The logarithm of the raw category fre-quency is negatively correlated with the primingstrength (?lnDist:lnFreq = 0.151, p < 0.0001.
Notethat a negative coefficient for LNDIST indicates5We trained our models using Penalized Quasi-Likelihood (Venables and Ripley, 2002).
This techniqueworks best if data is balanced, i.e.
we avoid having very rarepositive examples in the data.
Experiment 2 was conductedon a subset of the data.313CP:NormalFormPP:NormalFormCP:IncrementalPP:Incremental1.0 1.2 1.4 1.6- - - -Figure 2: Decay effect sizes in Experiment 1for combinations of comprehension-production orproduction-production priming and in incrementalor normal-form derivations.
Error bars show (non-simultaneous) 95% confidence intervals.decay.
The lower this coefficient, the more decay,hence priming).If there was no priming of categories for incre-mentally formed constituents, we would expect tosee a large effect of DERIVATION.
In the contrary,we see no effect at a high p, where the that theregression method used is demonstrably powerfulenough to detect even small changes in the prim-ing effect.
We conclude that there is no detectabledifference in priming between the two derivationtypes.
In Fig.
2, we give the estimated primingeffect sizes for the four conditions.6The result is compatible with CCG?s separationof derivation structure and the type of the resultof derivation.
It is not the derivation structure thatprimes, but rather the type of the result.
It is alsocompatible with the possibility of a non-traditionalconstituent structure (such as the incremental anal-ysis), even though it is clear that neither incremen-tal nor normal-form derivations necessarily repre-sent the ideal analysis.The category sets occurring in both derivationvariants was largely disjunct, making testing foractual overlap between different derivations im-possible.6.2 Experiment 2: Priming between Lexicaland Phrasal CategoriesHypothesis Since CCG categories simply en-code unsatisfied subcategorization constraints,constituents which are very different from a tradi-tional linguistic perspective can receive the samecategory.
This is, perhaps, most evident in phrasal6Note that Figures 2 and 3 stem from nested models thatestimate the effect of LNDIST within the four/eight condi-tions.
Confidence intervals will be larger, as fewer data-points are available than when the overall effect of a singlefactor is compared.CP:lex?lexPP:lex?lexCP:lex?phrPP:lex?phrCP:phr?lexPP:phr?lexCP:phr?phrPP:phr?phr?1.0 ?1.2 ?1.4 ?1.6 ?1.8 ?2.0Figure 3: Decay effect sizes in Experiment 2,for combinations of comprehension-productionor production-production priming and lexical orphrasal primes and targets, e.g.
the third bardenotes the decay in repetition probability of aphrasal category as prime and a lexical one astarget, where prime and target occurred in utter-ances by the same speaker.
Error bars show (non-simultaneous) 95% confidence intervals.and lexical categories (where, e.g., an intransitiveverb is indistinguishable from a verb phrase).Bock and Loebell (1990)?s experiments suggestthat priming effects are independent of the subcat-egorization frame.
There, an active voice sentenceprimed a passive voice one with the same phrasestructure, but a different subcategorization.
If wefind priming from lexical to phrasal categories,then our model demonstrates priming of subcat-egorization frames.From a processing point of view, phrasal cat-egories are distinct from lexical ones.
Lexicalcategories are bound to the lemma and therebylinked to the lexicon, while phrasal categoriesare the result of a structural composition or de-composition process.
The latter ones representtemporary states, encoding the syntactic process.Here, we test whether lexical and phrasal cate-gories can prime each other, and if so, contrast thestrength of these priming effects.Method We built a model which allowed lex-ical and phrasal categories to prime each other.A factor, STRUCTURAL LEVEL was introduced314to distinguish the four cases: priming in betweenphrasal categories and in between lexical ones,from lexical ones to phrasal ones and from phrasalones to lexical ones.Recall that each data point encodes a possibilityto repeat a CCG category, referring to a particularinstance of a target category at time t and a timespan of duration of one second [t?d?0.5, t?d +0.5] in which a priming instance of the same cate-gory could occur.
If it occurred at least once, thedata point was counted as a possible example ofpriming (response variable: true), otherwise it wasincluded as a counter-example (response variable:false).
For the target category, its type (lexical orphrasal) was clear.
For the category of the prime,we included two data points, one for each type,with a response indicating whether a prime of thecategory of such a type occurred in the time win-dow.
We built separate models for incremental andnormal form derivations.
Models were fitted toa balanced subset, including all repetitions and arandomly sampled subset of non-repetitions.Results Both the normal-form and the incre-mental model show qualitatively the same re-sults.
STRUCTURALLEVEL has a significantinfluence on priming strength (LN DIST) forthe cases where a lexical item serves as prime(e.g., normal-form PP: ?lnDist:lex?lex = 0.261,p < 0.0001; ?lnDist:lex?phr = 0.166, p < 0.0001;?lnDist:phr?lex = 0.056, p < 0.05; as compared tothe baseline phr?
phr.
N.B.
higher values denoteless decay & priming).
Phrasal categories primeother phrasal and lexical categories, but there is alower priming effect to be seen from lexical cate-gories.
Figure 3 presents the resulting effect sizes.Albeit significant, we assume the effect of primetype is attributable to processing differences ratherthan the strong difference that would indicate thatthere is no priming of, e.g., lexical subcategoriza-tion frames.
As the analysis of effect sizes shows,we can see priming from and in between both lex-ical and phrasal categories.Additionally, there is no evidence suggestingthat, once frequency is taken into account, syntac-tic processes happening high up in derivation treesshow more priming (see Scheepers 2003).7 DiscussionWe can confirm the syntactic priming effect forCCG categories.
Priming occurs in incrementalas well as in normal-form CCG derivations, and atdifferent syntactic levels in those derivations: wedemonstrated that priming effects persists acrosssyntactic stages, from the lowest one (lexical cate-gories) up to higher ones (phrasal categories).
Thisis what CCG predicts if priming of categories isassumed.Linguistic data is inherently noisy.
Annotationscontain errors, and conversions such as the one toCCG may add further error.
However, since noiseis distributed across the corpus, it is unlikely to af-fect priming effect strength or its interaction withthe factors we used: priming, in this study, is de-fined as decay of repetition probability.
We seethe lack of control in the collection of a corpus likeSwitchboard not only as a challenge, but also as anadvantage: it means that realistic data is present inthe corpus, allowing us to conduct a controlled ex-periments to validate a claim about a specific the-ory of competence grammar.The fact that CCG categories prime could beexplained in a model that includes a basic formof subcategorization.
All categories, if lexical orphrasal, contain a subcategorization frame, withonly those categories present that have yet to besatisfied.
Our CCG based models make predic-tions for experimental studies, e.g., that specificheads with open subcategorization slots (such astransitive verbs) will be primed by phrases that re-quire the same kinds of arguments (such as verbalphrases with a ditransitive verb and an argument).The models presented take the frequency of thesyntactic category into account, reducing noise,especially in the conditions with lower numbersof (positive) reptition examples (e.g., CP and in-cremental derivations in Experiment 1).
Whetherthere are significant qualitative and quantitativedifferences of PP and CP priming with respect tochoice of derivation type ?
which would point outprocessing differences in comprehension vs. pro-duction priming ?
will be a matter of future work.At this point, we do not explicitly discriminatedifferent syntactic frameworks.
Comparing prim-ing effects in a corpus annotated in parallel accord-ing to different theories will be a matter of futurework.8 ConclusionsWe have discussed an empirical, corpus-based ap-proach to use priming effects in the validation ofgeneral syntactic models.
The analysis we pre-sented is compatible with the reality of a lexical-315ized, categorial grammar such as CCG as a com-ponent of the human sentence processor.
CCG isunusual in allowing us to compare different typesof derivational analyses within the same grammarframework.
Focusing on CCG allowed us to con-trast priming under different conditions, while stillmaking a statistical and general statement aboutthe priming effects for all syntactic phenomenacovered by the grammar.AcknowledgementsWe would like to thank Mark Steedman, Roger Levy, Jo-hanna Moore and three anonymous reviewers for their com-ments.
The authors are grateful for being supported by thefollowing grants: DR by The Edinburgh Stanford Link, JHby NSF ITR grant 0205456, FK by The Leverhulme Trust(grant F/00 159/AL ?
Syntactic Parallelism).ReferencesJ.
Kathryn Bock.
1986.
Syntactic persistence in language pro-duction.
Cognitive Psychology, 18:355?387.J.
Kathryn Bock and Helga Loebell.
1990.
Framing sen-tences.
Cognition, 35:1?39.Holly P. Branigan, Martin J. Pickering, and Alexandra A. Cle-land.
2000.
Syntactic co-ordination in dialogue.
Cogni-tion, 75:B13?25.Jean Carletta, S. Dingare, Malvina Nissim, and T. Nikitina.2004.
Using the NITE XML toolkit on the Switchboardcorpus to study syntactic choice: a case study.
In Proc.
4thLanguage Resources and Evaluation Conference.
Lisbon,Portugal.Stephen Clark and James R. Curran.
2004.
Parsing the WSJusing CCG and log-linear models.
In Proc.
of the 42ndAnnual Meeting of the Association for Computational Lin-guistics.
Barcelona, Spain.A.
A. Cleland and M. J. Pickering.
2003.
The use of lexi-cal and syntactic information in language production: Ev-idence from the priming of noun-phrase structure.
Journalof Memory and Language, 49:214?230.Amit Dubey, Frank Keller, and Patrick Sturt.
2006.
Inte-grating syntactic priming into an incremental probabilisticparser, with an application to psycholinguistic modeling.In Proc.
of the 21st International Conference on Computa-tional Linguistics and 44th Annual Mtg of the Associationfor Computational Linguistics.
Sydney, Australia.Jason Eisner.
1996.
Efficient normal-form parsing for com-binatory categorial grammar.
In Proceedings of the 34thAnnual Meeting of the Association for Computational Lin-guistics, pages 79?86.
Santa Cruz,CA.Stefan Th.
Gries.
2005.
Syntactic priming: A corpus-based approach.
Journal of Psycholinguistic Research,34(4):365?399.Julia Hockenmaier and Mark Steedman.
2002.
Generativemodels for statistical parsing with Combinatory Catego-rial Grammar.
In Proc.
40th Annual Meeting of the Asso-ciation for Computational Linguistics.
Philadelphia, PA.Julia Hockenmaier and Mark Steedman.
2005.
CCGbank:Users?
manual.
Technical Report MS-CIS-05-09, Com-puter and Information Science, University of Pennsylva-nia.Mark Johnson and Eugene Charniak.
2004.
A tag-based noisychannel model of speech repairs.
In Proc.
42nd AnnualMeeting of the Association for Computational Linguistics,pages 33?39.
Barcelona, Spain.M.
Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,A.
Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994.The Penn treebank: Annotating predicate argument struc-ture.
In Proc.
ARPA Human Language Technology Work-shop.
Plainsboro, NJ.Michael Niv.
1994.
A psycholinguistically motivated parserfor CCG.
In Mtg.
of the Association for ComputationalLinguistics, pages 125?132.Martin J. Pickering and Holly P. Branigan.
1998.
The rep-resentation of verbs: Evidence from syntactic priming inlanguage production.
Journal of Memory and Language,39:633?651.Martin J. Pickering, Holly P. Branigan, and Janet F. McLean.2002.
Constituent structure is formulated in one stage.Journal of Memory and Language, 46:586?605.David Reitter, Frank Keller, and Johanna D. Moore.
2006a.Computational modelling of structural priming in dia-logue.
In Proc.
Human Language Technology conference- North American chapter of the Association for Compu-tational Linguistics annual mtg.
New York City.David Reitter, Johanna D. Moore, and Frank Keller.
2006b.Priming of syntactic rules in task-oriented dialogue andspontaneous conversation.
In Proc.
28th Annual Confer-ence of the Cognitive Science Society.Christoph Scheepers.
2003.
Syntactic priming of relativeclause attachments: Persistence of structural configurationin sentence production.
Cognition, 89:179?205.Mark Steedman.
2000.
The Syntactic Process.
MIT Press.Benedikt Szmrecsanyi.
2005.
Creatures of habit: A corpus-linguistic analysis of persistence in spoken english.
Cor-pus Linguistics and Linguistic Theory, 1(1):113?149.William N. Venables and Brian D. Ripley.
2002.
ModernApplied Statistics with S. Fourth Edition.
Springer.Mike White and Jason Baldridge.
2003.
Adapting chart re-alization to CCG.
In Proc.
9th European Workshop onNatural Language Generation.
Budapest, Hungary.316
