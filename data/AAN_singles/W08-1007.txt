Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 47?54,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Dependency-Driven Parser for GermanDependency and Constituency RepresentationsJohan HallVa?xjo?
UniversitySwedenjohan.hall@vxu.seJoakim NivreVa?xjo?
University andUppsala UniversitySwedenjoakim.nivre@vxu.seAbstractWe present a dependency-driven parser thatparses both dependency structures and con-stituent structures.
Constituency representa-tions are automatically transformed into de-pendency representations with complex arc la-bels, which makes it possible to recover theconstituent structure with both constituent la-bels and grammatical functions.
We report alabeled attachment score close to 90% for de-pendency versions of the TIGER and Tu?Ba-D/Z treebanks.
Moreover, the parser is able torecover both constituent labels and grammat-ical functions with an F-Score over 75% forTu?Ba-D/Z and over 65% for TIGER.1 IntroductionIs it really that difficult to parse German?
Ku?bler etal.
(2006) point out three grammatical features thatcould make parsing of German more difficult: finiteverb placement, flexible phrase ordering and discon-tinuous constituents.
Earlier studies by Dubey andKeller (2003) and Dubey (2005) using the Negratreebank (Skut et al, 1997) reports that lexicaliza-tion of PCFGs decrease the parsing accuracy whenparsing Negra?s flat constituent structures.
However,Ku?bler et al (2006) present a comparative studythat suggests that it is not harder to parse Germanthan for example English.
By contrast, Rehbein andvan Genabith (2007) study different parser evalua-tion metrics by simulating parser errors on two Ger-man treebanks (with different treebank annotationschemes) and they claim that the question whetherGerman is harder to parse than English is still unde-cided.This paper does not try to answer the questionabove, but presents a new way of parsing constituentstructures that can output the whole structure withall grammatical functions.
The shared task on pars-ing German was to parse both the constituency ver-sion and the dependency version of the two Ger-man treebanks: TIGER (Brants et al, 2002) andTu?Ba-D/Z (Telljohann et al, 2005).
We present adependency-driven parser that parses both depen-dency structures and constituent structures using anextended version of MaltParser 1.0.1 The focus ofthis paper is how MaltParser parses the constituentstructures with a dependency-based algorithm.This paper is structured as follows.
Section 2briefly describes the MaltParser system, while sec-tion 3 continues with presenting the dependencyparsing.
Section 4 explains how a transition-baseddependency-driven parser can be turned into a con-stituency parser.
Section 5 presents the experimen-tal evaluation and discusses the results.
Finally sec-tion 6 concludes.2 MaltParserMaltParser is a transition-based parsing systemwhich was one of the top performing systems onmultilingual dependency parsing in the CoNLL2006 shared task (Buchholz and Marsi, 2006; Nivreet al, 2006) and the CoNLL shared task 2007 (Nivreet al, 2007; Hall et al, 2007).
The basic idea ofMaltParser is to derive dependency graphs using agreedy parsing algorithm that approximates a glob-1MaltParser is distributed with an open-source licenseand can be downloaded free of charge from following page:http://www.vxu.se/msi/users/jha/maltparser/47ally optimal solution by making a sequence of lo-cally optimal choices.
The system is equipped withseveral parsing algorithms, but we have chosen toonly optimize Nivre?s parsing algorithm for boththe dependency track and the constituency track.Nivre?s algorithm is a deterministic algorithm forbuilding labeled projective dependency structures inlinear time (Nivre, 2006).
There are two essentialparameters that can be varied for this algorithm.
Thefirst is the arc order and we selected the arc-eager or-der that attaches the right dependents to their head assoon as possible.
The second is the stack initializa-tion and we chose to use an empty stack initializa-tion that attaches root dependents with a default rootlabel after completing the left-to-right pass over theinput.The algorithm uses two data structures: a stackto store partially processed tokens and a queue ofremaining input tokens.
The arc-eager transition-system has four parser actions:1.
LEFT-ARC(r): Adds an arc labeled r from thenext input token to the top token of the stack,the top token is popped from the stack becauseit must be complete with respect to left andright dependents at this point.2.
RIGHT-ARC(r): Adds an arc labeled r fromthe top token of the stack to the next input tokenand pushes the next input token onto the stack(because it may have dependents further to theright).3.
REDUCE: Pops the top token of the stack.
Thistransition can be performed only if the top to-ken has been assigned a head and is needed forpopping a node that was pushed in a RIGHT-ARC(r) transition and which has since foundall its right dependents.4.
SHIFT: Pushes the next input token onto thestack.
This is correct when the next input tokenhas its head to the right or should be attachedto the root.MaltParser uses history-based feature models forpredicting the next parser action at nondeterminis-tic choice points.
Previously, MaltParser combinedthe prediction of the transition with the prediction ofthe arc label r into one complex prediction with onefeature model.
The experiments presented in this pa-per use another prediction strategy, which divide theprediction of the parser action into several predic-tions.
First the transition is predicted; if the transi-tion is SHIFT or REDUCE the nondeterminism is re-solved, but if the predicted transition is RIGHT-ARCor LEFT-ARC the parser continues to predict the arclabel r. This prediction strategy enables the systemto have three different feature models: one for pre-dicting the transition and two for predicting the arclabel r (RIGHT-ARC and LEFT-ARC).
We will seein section 4 that this change makes it more feasi-ble to encode the inverse mapping into complex arclabels for an arbitrary constituent structure withoutlosing any information.All symbolic features were converted to nu-merical features and we use the quadratic kernelK(xi, xj) = (?xTi xj + r)2 of the LIBSVM pack-age (Chang and Lin, 2001) for mapping histories toparser actions and arc labels.
All results are basedon the following settings of LIBSVM: ?
= 0.2 andr = 0 for the kernel parameters, C = 0.5 for thepenalty parameter, and  = 1.0 for the terminationcriterion.
We also split the training instances intosmaller sets according to the fine-grained part-of-speech of the next input token to train separate one-versus-one multi-class LIBSVM-classifiers.3 Dependency ParsingParsing sentences with dependency structures likethe one in Figure 1 is straightforward using Malt-Parser.
During training, the parser reconstructs thecorrect transition sequence needed to derive the goldstandard dependency graph of a sentence.
This in-volves choosing a label r for each arc, which ina pure dependency structure is an atomic symbol.For example, in Figure 1, the arc from hat to Beck-meyer is labeled SUBJ.
This is handled by train-ing a separate labeling model for RIGHT-ARC andLEFT-ARC.
During parsing, the sentence is pro-cessed in the same way as during training except thatthe parser requests the next transition from the tran-sition classifier.
If the predicted transition is an arctransition (RIGHT-ARC or LEFT-ARC), it then asksthe corresponding classifier for the arc label r.One complication when parsing the dependencyversion of the two German treebanks is that they48Figure 1: The sentence ?For this statement has Beckmeyer until now not presented any evidence.?
is taken fromdependency version of Tu?Ba-D/Z treebank.contain non-projective structures, such as the depen-dency graph illustrated in Figure 1.
Nivre?s pars-ing algorithm only produces projective dependencystructures, and therefore we used pseudo-projectiveparsing for recovering non-projective structures.The training data are projectivized and informationabout these transformations is encoded into the arclabels to enable deprojectivizition of the parser out-put (Nivre and Nilsson, 2005).4 Constituency ParsingThis section explains how a transition-based depen-dency parser can be used for parsing constituentstructures.
The basic idea is to use the commonpractice of transforming a constituent structure intoa dependency graph and encode the inverse mappingwith complex arc labels.
Note that the goal is not tocreate the best dependency representation of a con-stituent structure.
Instead the main objective is tofind a general method to transform constituency todependency so that is easy to do the inverse trans-formation without losing any information.
More-over, another goal is to transform the constituentstructures so that it is feasible for a transition-baseddependency parser to induce a parser model basedon the resulting dependency graphs and during pars-ing use this parser model to derive constituent struc-tures with the highest accuracy possible.
Hence, thetransformation described below is not designed withthe purpose of deriving a linguistically sound depen-dency graph from a constituent structure.Our strategy for turning a dependency parser intoa constituency parser can be summarized with thefollowing steps:1.
Identify the lexical head of every constituent inthe constituent structure.2.
Identify the head of every token in the depen-dency structure.3.
Build a labeled dependency graph that encodesthe inverse mapping in the arc labels.4.
Induce a parser model based on the labeled de-pendency graphs.5.
Use the induced parser model to parse new sen-tences into dependency graphs.6.
Derive the constituent structure by performingthe inverse mapping encoded in the dependencygraph produced in step 5.4.1 Identify the HeadsThe first steps are basically the steps that are usedto convert a constituent structure to a dependencystructure.
One way of doing this is to traverse theconstituent structure from the root node and iden-tify the head-child and the lexical head of all con-stituent nodes in a recursive depth-first search.
Usu-ally this process is governed by pre-defined head-finding rules that define the direction of the searchfor each distinct constituent label.
Moreover, itis quite common that the head-finding rules definesome kind of priority lists over which part of speechor grammatical function is the more preferable head-child.For our experiment on German we have kept thissearch of the head-child and lexical head very sim-ple.
For the TIGER treebank we perform a left-to-right search to find the leftmost lexical child.
Ifno lexical child can be found, the head-child of the49constituent will be the leftmost constituent child andthe lexical head will be the lexical child of the headchild recursively.
For the Tu?Ba-D/Z treebank we gothigher accuracy if we varied the direction of searchaccording to the label of the target constituent.2 Wealso tried more complex and linguistically motivatedhead rules, but unfortunately no improvement in ac-curacy could be found.
We want to stress that theuse of more complex head rules was done late in theparser optimization process and it would not be asurprise if more careful experiments resulted in theopposite conclusion.Given that all constituents have been assigned alexical head it is a straightforward process to iden-tify the head and the dependents of all input tokens.The algorithm investigates, for each input token, thecontaining constituent?s lexical head, and if the to-ken is not the lexical head of the constituent it takesthe lexical head as its head in the dependency graph;otherwise the head will be assigned the lexical headof a higher constituent in the structure.
The root ofthe dependency graph will be the lexical head of theroot of the constituent structure.4.2 Build a Labeled Dependency GraphThe next step builds a labeled dependency represen-tation that encodes the inverse mapping in the arclabels of the dependency graph.
Each arc label is aquadruple consisting of four sublabels (dependencyrelation, head relations, constituent labels, attach-ment).
The meaning of each sublabel is following:?
The dependency relation is the grammaticalfunction of the highest constituent of which thedependent is the lexical head.?
The head relations encode the path of functionlabels from the dependent to the highest con-stituent of which is the lexical head (with pathelements separated by |).?
The constituent labels encode the path of con-stituent labels from the dependent to the highestconstituent of which is the lexical head (withpath elements separated by |).2It was beneficial to make a right-to-left search for the fol-lowing labels: ADJX, ADVX, DM, DP, NX, PX?
The attachment is a non-negative integer i thatencodes the attachment level of the highest con-stituent of which it is the lexical head.4.3 Encoding ExampleFigure 2 illustrates the procedure of encoding theconstituency representation as a dependency graphwith complex arc labels for a German sentence.The constituent structure is shown above the sen-tence and below we can see the resulting depen-dency graph after the transformation.
We want tostress that the resulting dependency graph is not lin-guistically sound, and the main purpose is to demon-strate how a constituent structure can be encoded ina dependency graph that have all information needfor the inverse transformation.For example, the constituent MF has no lexicalchild and therefore the head-child is the leftmostconstituent NX.
The lexical head of MF is the tokenBeckmeyer because it is the lexical head of NX.
Forthe same reason the lexical head of the constituentSIMPX is the token Fu?r and this token will be thehead of the token Beckmeyer, because SIMPX dom-inates MF.
In the dependency graph this is illustratedwith an arc from the head Fu?r to its dependent Beck-meyer.The arc Fu?r to Beckmeyer is labeled with a com-plex label (?
?, HD|ON, NX|MF, 2), which consistsof four sublabels.
The first sublabel is the grammat-ical function above MF and because this is missinga dummy label ??
is used instead.
The sublabelHD|ON encodes a sequence of head relations fromthe lexical head Beckmeyer to MF.
The constituentlabels are encoded in the same way in the third sub-label NX|MF.
Finally, the fourth sublabel indicatesthe attachment level of the constituent MF.
In thiscase, MF should be attached to the constituent twolevels up in the structure with respect to the headFu?r.3The two arcs diese to Behauptung and keinen toNachweis both have the complex arc label (HD, *, *,0), because the tokens Behauptung and Nachweis areattached to a constituent without being a lexical headof any dominating constituent.
Consequently, thereare no sequences of head relations and constituent3If the fourth sublabel had an attachment level of 1, then theconstituent MF would be attached to the constituent VF insteadof the constituent SIMPX.50Figure 2: The sentence ?For this statement has Beckmeyer until now not presented any evidence.?
is taken fromTu?Ba-D/Z treebank and show the encoding of a constituent structure as a dependency graph.labels to encode, and these are therefore marked *.The encoding of the virtual root VROOT is treatedin a special way and the label VROOT is regarded asa dependency relation instead of a constituent label.If we compare the dependency graphs in Figure 1and Figure 2, we can see large differences.
The morelinguistically motivated dependency graph (LDG) inFigure 1 has a completely difference structure anddifferent arc labels compared to the automaticallygenerated dependency graph (ADG) in Figure 2.There are several reasons, some of which are listedhere:?
Different conversions strategies: LDG is basedon a conversion that sometimes leads to non-projective structures for non-local dependen-cies.
For example, in Figure 2, the extractedPP Fu?r diese Behauptung has the grammati-cal function OAMOD, which indicates that itis a modifier (MOD) of a direct object (OA)elsewhere in the structure (in this case keinenNachweis).
In LDG, this is converted to a non-projective dependency from Nachweis to Fu?r(with the label PP).
No such transformtion isattempted in ADC, which simply attaches Fu?rto the lexical head of the containing constituent.?
Different head-finding rules: ADG are derivedwithout almost no rules at all.
Most likely, theconversion of LDG makes use of several lin-guistically sound head-finding rules.
A strikingdifference is the root of the dependency graph,where LDG has its root at the linguistically mo-tivated token hat.
Whereas ADG has its root atthe end of the sentence, because the leftmostlexical child of the virtual root VROOT is thepunctuation.?
Different arc labels: ADG encodes the con-stituent structure in the complex arc labels tobe able to recover the constituent structure,whereas LDG have linguistically motivated de-pendency relations that are not present in theconstituent structure.We believe that our simplistic approach can be fur-ther improved by using ideas from the conversionprocess of LDG.514.4 Inverse MappingThe last step of our presented strategy is to make theinverse transformation from a dependency graph toa constituent structure.
This is done by a bottom-up and top-down process of the dependency graph.First we iterate over all tokens in the dependencygraph and restore the sequence of constituent nodeswith constituent labels and grammatical functionsfor each individual token using the information ofthe sublabels head relations and constituent labels.After this bottom-up process we have the lineage ofconstituents for each token where the token is thelexical head.
The top-down process then traversethe dependency graph recursively from the root withpre-order depth-first search.
For each token, thehighest constituent of the lineage of the token is at-tached to its head lineage at an attachment level ac-cording to the sublabel attachment.
Finally, the edgebetween the dominating constituent and the highestconstituent of the lineage is labeled with a grammat-ical function according to the sublabel dependencyrelation.4.5 ParsingFor the constituency versions of both TIGER andTu?Ba-D/Z we can recover the constituent structurewithout any loss of information, if we transformfrom constituency to dependency and back again toconstituency.
During parsing we predict the sub-labels separately with separate feature models forRIGHT-ARC and LEFT-ARC.
Moreover, the parsedconstituent structure can contain discontinuous con-stituency because of wrong attachment levels of con-stituents.
To overcome this problem, the structureis post-processed and the discontinuous constituentsare forced down in the structure so that the parseroutput can be represented in a nested bracketing for-mat.5 ExperimentsThe shared task on parsing German consisted ofparsing either the dependency version or the con-stituency version of two German treebanks, al-though we chose to parse both versions.
This sectionfirst presents the data sets used.
We continue with abrief overview of how we optimized the four differ-ent parser models.
Finally, the results are discussed.5.1 Data SetsThe prepared training and development data dis-tributed by the organizers were based on the GermanTIGER (Brants et al, 2002) and Tu?Ba-D/Z (Telljo-hann et al, 2005) treebanks, one dependency andone constituency version for each treebank.
Bothtreebanks contain German newspaper text and theprepared data sets were of the same size.
The devel-opment set contained 2611 sentences and the train-ing set contained 20894 sentences.
The dependencyand constituency versions contained the same set ofsentences.The dependency data were formated accordingto the CoNLL dependency data format.4 TheLEMMA, FEATS, PHEAD and PDEPREL columnsof the CoNLL format were not used at all.The constituency data have been converted into abracketing format similar to the Penn Treebank for-mat.
All trees are dominated by a VROOT nodeand all constituents are continuous.
The test dataconsisted of sentences with gold-standard part-of-speech tags and also the gold-standard grammaticalfunctions attached to the part-of-speech tags.
Unfor-tunately, we were not aware of that the grammaticalfunctions attached to the part-of-speech tags shouldbe regarded as input to the parser and therefore ourpresented results are based on not using the gram-matical functions attached to the part-of-speech tagsas input to the parser.We divided the development data into two sets,one set used for parser optimization (80%) and theother 20% we saved for final preparation before therelease of the test data.
For the final test run wetrained parser models on all the data, both the train-ing data and the development data.5.2 Parser optimizationWe ran several experiments to optimize the four dif-ferent parser models.
The optimization of the de-pendency versions was conducted in a way simi-lar to the parser optimization of MaltParser in theCoNLL shared tasks (Nivre et al, 2006; Hall et al,2007).
A new parameter for the extended version4More information about the CoNLL dependency data for-mat can be found at: http://nextens.uvt.nl/ conll/#dataformat.Yannick Versley has done work of converting both treebanks toa dependency annotation that is similar to the Hamburg depen-dency format.52of MaltParser 1.0 is the prediction strategy, wherewe could choose between combining the predictionof the transition with the prediction of the arc labelinto one complex prediction or dividing the predic-tion of the parser action into two predictions (onemodel for predicting the transition and two modelsfor predicting the arc label depending on the out-come of the transition-model).
It was beneficial touse the divided predication strategy for all four datasets.
In the next step we performed a feature opti-mization with both forward and backward selection,starting from a model extrapolated from many pre-vious experiments on different languages.
Becausewe chose to use the divided predication strategy thisstep was more complicated compared to using thecombined strategy, because we needed to optimizethree feature models (one transition-model and twoarc-label models, one for RIGHT-ARC and one forLEFT-ARC).The optimization of the constituency versions waseven more complex because each parser model con-tained nine feature models (one transition-model,two models for each sublabel).
Another problemfor the parser optimization was the fact that we triedout new ideas and for example changed the encod-ing a couple of times.
Due to the time constraintsof the shared task it was not possible to start parseroptimization all over again for every change.
Wealso performed some late experiments with differenthead-finding rules to make the intermediate depen-dency graphs more linguistically sound, but unfor-tunately these experiments did not improve the pars-ing accuracy.
We want to emphasize that the timefor developing the extended version of MaltParserto handle constituency was severely limited, espe-cially the implementation of head-finding rules, soit is very likely that head-finding rules can improveparsing accuracy after more careful testing and ex-periments.5.3 Results and DiscussionThe results based on the prepared test data for the de-pendency and constituency tracks are shown in table1.
The label attachment score (LAS) was used by theorganizer for evaluating the dependency versions,that is, the proportion of tokens that are assigned thecorrect head and the correct arc label (punctuationincluded).
We can see that the dependency resultsDependency ConstituencyTreebank LAS LP LR LFTIGER 90.80 67.06 63.40 65.18Tu?Ba-D/Z 88.64 76.44 74.79 75.60Table 1: The results for the extended version of Malt-Parser 1.0 in the shared task on parsing German depen-dency and constituency representations.are close to 90% for both the treebanks, 90.80 forTIGER and 88.64 for Tu?ba-D/Z, which were the un-challenged best scores in the shared task.
The high-est score on parsing German in the CoNLL-X sharedtask was obtained by the system of McDonald et al(2006) with a LAS of 87.34 based on the TIGERtreebank, but we want to stress that these resultsare not comparable due to different data sets (anda different policy regarding the inclusion of punctu-ation).The constituency versions were evaluated accord-ing to the labeled recall (LR), labeled precision(LP) and labeled F-score (LF).
Labeled in this con-text means that both the constituent label and thegrammatical function should agree with the gold-standard, but grammatical functions labeling theedge between a constituent and a token were not in-cluded in the evaluation.
The labeled F-scores are75.60 for Tu?ba-D/Z and 65.18 for TIGER and theseresults are the second best results in the shared taskout of three systems.
We want to emphasize that theresults may not be strictly comparable because ofdifferent use of the grammatical functions attachedto the parts of speech in the bracketing format.
Wedid not use these grammatical functions as input,instead these were assigned by the parser.
Our re-sults are competitive if we compare with Ku?bler etal.
(2006), who report 51.41 labeled F-score on theNegra treebank and 75.33 on the Tu?Ba-D/Z treebankusing the unlexicalized, markovized PCFG versionof the Stanford parser.We believe that our results for the constituencyrepresentations can be improved upon by investi-gating different methods for encoding the inversemapping in the complex arc labels and performinga more careful evaluation of head-finding rules toderive a more linguistically sound dependency rep-resentation.
Another interesting line of future workis to try to parse discontinuous constituents by using53a non-projective parsing algorithm like the Coving-ton algorithm (Covington, 2001) or using pseudo-projective parsing for discontinuous constituencyparsing (Nivre and Nilsson, 2005).6 ConclusionWe have shown that a transition-based dependency-driven parser can be used for parsing German withboth dependency and constituent representations.We can report state-of-the-art results for parsing thedependency versions of two German treebanks, andwe have demonstrated, with promising results, howa dependency parser can parse full constituent struc-tures by encoding the inverse mapping in complexarc labels of the dependency graph.
We believe thatthis method can be improved by using, for example,head-finding rules.AcknowledgmentsWe want to thank the treebank providers for makingthe data available for the shared task and the orga-nizers for their efforts in organizing it.
Thanks alsoto two reviewers for useful comments.ReferencesSabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER Tree-bank.
In Proceedings of the Workshop on Treebanksand Linguistic Theories Sozopol, pages 1?18.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X Shared Task on Multilingual Dependency Parsing.In Proceedings of the Tenth Conference on Computa-tional Natural Language Learning (CoNLL-X), pages149?164.Chih-Chung Chang and Chih-Jen Lin.
2001.
LIBSVM:A Library for Support Vector Machines.Michael A. Covington.
2001.
A Fundamental Algorithmfor Dependency Parsing.
In Proceedings of the 39thAnnual ACM Southeast Conference, pages 95?102.Amit Dubey and Frank Keller.
2003.
Probabilistic Pars-ing for German using Sister-Head Dependencies.
InProceedings of the 41st Annual Meeting of the Associ-ation for Computational Linguistics (ACL), pages 96?103.Amit Dubey.
2005.
What to do when Lexicaliza-tion fails: Parsing German with Suffix Analysis andSmoothing.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 314?321.Johan Hall, Jens Nilsson, Joakim Nivre, Gu?ls?en Eryig?it,Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.2007.
Single Malt or Blended?
A Study in Mul-tilingual Parser Optimization.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL 2007,pages 933?939.Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier.2006.
Is it Really that Difficult to Parse German.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2006), pages 111?119.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual Dependency Analysis with aTwo-Stage Discriminative Parser.
In Proceedings ofthe Tenth Conference on Computational Natural Lan-guage Learning (CoNLL-X), pages 216?220.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-ProjectiveDependency Parsing.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 99?106.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,and Svetoslav Marinov.
2006.
Labeled Pseudo-Projective Dependency Parsing with Support VectorMachines.
In Proceedings of the Tenth Conference onComputational Natural Language Learning (CoNLL-X), pages 221?225.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 Shared Task on DependencyParsing.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 915?932.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer.Ines Rehbein and Josef van Genabith.
2007.
TreebankAnnotation Schemes and Parser Evaluation for Ger-man.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning (EMNLP-CoNLL 2007), pages 630?639.Wojciech Skut, Brigitte Krenn, Thorsten Brants, andHans Uszkoreit.
1997.
An Annotation Scheme forFree Word Order Languages.
In Proceedings of theFifth Conference on Applied Natural Language Pro-cessing (ANLP), pages 314?321.Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,and Heike Zinsmeister.
2005.
Stylebook forthe Tu?bingen Treebank of Written German (Tu?Ba-D/Z).
Seminar fu?r Sprachwissenschaft, Universita?tTu?bingen, Germany.54
