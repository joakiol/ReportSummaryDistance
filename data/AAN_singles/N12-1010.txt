2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91?102,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsIntrinsic and Extrinsic Evaluation of an Automatic User DisengagementDetector for an Uncertainty-Adaptive Spoken Dialogue SystemKate Forbes-Riley and Diane Litman and Heather Friedberg and Joanna Drummond?University of PittsburghPittsburgh, PA 15260, USAforbesk@pitt.edu, litman@pitt.edu, haf13@pitt.eduAbstractWe present a model for detecting user dis-engagement during spoken dialogue interac-tions.
Intrinsic evaluation of our model (i.e.,with respect to a gold standard) yields resultson par with prior work.
However, since ourgoal is immediate implementation in a sys-tem that already detects and adapts to user un-certainty, we go further than prior work andpresent an extrinsic evaluation of our model(i.e., with respect to the real-world task).
Cor-relation analyses show crucially that our au-tomatic disengagement labels correlate withsystem performance in the same way as thegold standard (manual) labels, while regres-sion analyses show that detecting user disen-gagement adds value over and above detectingonly user uncertainty when modeling perfor-mance.
Our results suggest that automaticallydetecting and adapting to user disengagementhas the potential to significantly improve per-formance even in the presence of noise, whencompared with only adapting to one affectivestate or ignoring affect entirely.1 IntroductionSpoken dialogue systems that can detect and adaptto user affect1 are fast becoming reality (Schulleret al, 2009b; Batliner et al, 2008; Prendinger andIshizuka, 2005; Vidrascu and Devillers, 2005; Lee?Now at Univ.
Toronto: jdrummond@cs.toronto.edu1We use affect for emotions and attitudes that affect howusers communicate.
Other speech researchers also combineconcepts of emotion, arousal, and attitudes where emotion isnot full-blown (Cowie and Cornelius, 2003).and Narayanan, 2005; Shafran et al, 2003).
Thebenefits are clear: affect-adaptive systems have beenshown to increase task success (Forbes-Riley andLitman, 2011a; D?Mello et al, 2010; Wang et al,2008) or improve other system performance met-rics such as user satisfaction (Liu and Picard, 2005;Klein et al, 2002).
However, to date most affec-tive systems researchers have focused either only onaffect detection, or only on detecting and adaptingto a single affective state.
The next step is thus todevelop and evaluate spoken dialogue systems thatdetect and respond to multiple affective states.We previously showed that detecting and re-sponding to user uncertainty during spoken dialoguecomputer tutoring significantly improves task suc-cess (Forbes-Riley and Litman, 2011a).
We arenow taking the next step: incorporating automaticdetection and adaptation to user disengagement aswell, with the goal of further improving task suc-cess.
We targeted user uncertainty and disengage-ment because manual annotation showed them to bethe two most common user affective states in oursystem and both are negatively correlated with tasksuccess (Litman and Forbes-Riley, 2009; Forbes-Riley and Litman, 2011b).
Thus, we hypothesizethat providing appropriate responses to these stateswould reduce their frequency, consequently improv-ing task success.
Although we address these userstates in the tutoring domain, spoken dialogue re-searchers across domains and applications have in-vestigated the automatic detection of both user un-certainty (e.g.
(Drummond and Litman, 2011; Pon-Barry and Shieber, 2011; Paek and Ju, 2008; Alwanet al, 2007)) and user disengagement (e.g., (Schuller91et al, 2010; Wang and Hirschberg, 2011; Schulleret al, 2009a)), to improve system performance.The detection of user disengagement in particularhas received substantial attention in recent years,due to growing awareness of its potential for neg-atively impacting commercial applications (Wangand Hirschberg, 2011; Schuller et al, 2009a).In this paper we present a model for automati-cally detecting user disengagement during spokendialogue interactions.
Intrinsic evaluation of ourmodel yields results on par with those of prior work.However, we argue that while intrinsic evaluationsare necessary, they aren?t sufficient when immedi-ate implementation is the goal, because there is no apriori way to know when the model?s performance isacceptable to use in a working system.
This problemis particularly relevant to affect detection because itis such a difficult task, where no one achieves near-perfect results.
We argue that for such tasks someextrinsic evaluation is also necessary, to show thatthe automatic labels are useful and/or are a reason-able substitute for a gold standard before undertak-ing a labor-intensive and time-consuming evaluationwith real users.
Here we use correlational analy-ses to show that our automatic disengagement la-bels are related to system performance in the sameway as the gold standard (manual) labels.
We fur-ther show through regression analyses that detectinguser disengagement adds value over and above de-tecting only user uncertainty when modeling perfor-mance.
These results provide strong evidence thatenhancing a spoken dialogue system to detect andadapt to multiple affective states (specifically, userdisengagement and uncertainty) has the potential tosignificantly improve performance even in the pres-ence of noise due to automatic detection, when com-pared with only adapting to one affective state or ig-noring affect entirely.2 Related WorkOur focus in this paper is on first using machinelearning to develop a detector of user disengagementfor spoken dialogue systems, and then evaluating itsusefulness as fully as possible prior to its implemen-tation and deployment with real users.Disengaged users are highly undesirable inhuman-computer interaction because they increasethe potential for user dissatisfaction and task fail-ure; thus over the past decade there has already beensubstantial prior work focused on detecting user dis-engagement and the closely related states of bore-dom, motivation and lack of interest (e.g., (Schulleret al, 2010; Wang and Hirschberg, 2011; Jeon etal., 2010; Schuller et al, 2009a; Bohus and Horvitz,2009; Martalo et al, 2008; Porayska-Pomsta et al,2008; Kapoor and Picard, 2005; Sidner and Lee,2003; Forbes-Riley and Litman, 2011b)).Within this work, specific affect definitions varyslightly with the intention of being coherent withinthe application and domain and being relevant to thespecific adaptation goal (Martalo et al, 2008).
How-ever, affective systems researchers generally agreethat disengaged users show little involvement in theinteraction, and often display facial, gestural and lin-guistic signals such as gaze avoidance, finger tap-ping, humming, sarcasm, et cetera.The features used to detect disengagement alsovary depending on system domain and applica-tion.
For example, Sidner & Lee (2003) are in-terested in modeling more natural and collabora-tive human-robot interactions during basic conver-sations.
They define an algorithm for the engage-ment process that involves appropriate eye gaze andturn-taking.
Martalo et al (2008) study how userengagement influences dialogue patterns during in-teractions with an embodied agent that gives ad-vice about healthy dieting.
They model engage-ment using manually coded dialogue acts based onthe SWBDL-DAMSL scheme (Stolcke et al, 2000).Bohus and Horvitz (2009) study systems that attractand engage users for dynamic, multi-party dialoguesin open-world settings.
They model user intentionsto engage the system with cues from facial sensorsand the dialogue.
Within recent spoken dialogueresearch, acoustic-prosodic, lexical and contextualfeatures have been found to be effective detectorsof disengagement (Schuller et al, 2010; Wang andHirschberg, 2011; Jeon et al, 2010); we will brieflycompare our own results with these in Section 5.While all of the above-mentioned research haspresented intrinsic evaluations of their disengage-ment modeling efforts that indicate a reasonable de-gree of accuracy as compared to a gold standard(e.g., manual coding), only a few have yet demon-strated that the model?s detected values are useful92in practice and/or are a reasonable substitute forthe gold standard with respect to some practicalobjective (e.g., a relationship to performance).
Inparticular, two studies (Bohus and Horvitz, 2009;Schuller et al, 2009a) have gone directly from in-trinsic evaluation of (dis)engagement models to per-forming user studies with the implemented model,thereby bypassing other less expensive and lesslabor-intensive means of extrinsic evaluation toquantify their model?s usefulness?and potentially in-dicate its need to be further improved?before de-ployment with real users.
Neither study reports sta-tistically significant improvements in system perfor-mance as a result of detecting user (dis)engagement.Finally, while substantial spoken dialogue and af-fective systems research has shown that users dis-play a range of affective states while interacting witha system (e.g.
(Schuller et al, 2009b; Conati andMaclaren, 2009; Batliner et al, 2008; Devillers andVidrascu, 2006; Lee and Narayanan, 2005; Shafranet al, 2003; Ang et al, 2002)), to date only a few af-fective systems have been built that detect and adaptto multiple user affective states (e.g., (D?Mello et al,2010; Aist et al, 2002; Tsukahara andWard, 2001)),and most of these have been deployed with cru-cial natural language processing components ?wiz-arded?
by a hidden human agent (e.g., who performsspeech recognition or affect annotation on the userturns); moreover, none have yet shown significantimprovements in system performance as a result ofadapting to multiple user affective states.3 ITSPOKE: Spoken Dialogue TutorWe develop and evaluate our disengagement detec-tor using a corpus of spoken dialogues from a 2008controlled experiment evaluating our uncertainty-adaptive spoken dialogue tutoring system, IT-SPOKE (Intelligent Tutoring SPOKEn dialog sys-tem) (Forbes-Riley and Litman, 2011a).2ITSPOKE tutors 5 Newtonian physics problems(one per dialogue), using a Tutor Question - Stu-dent Answer - Tutor Response format.
Aftereach tutor question, the student speech is digi-tized from head-mounted microphone input and sent2ITSPOKE is a speech-enhanced and otherwise modifiedversion of the Why2-Atlas text-based qualitative physics tu-tor (VanLehn et al, 2002).to the Sphinx2 recognizer, which yields an auto-matic transcript (Huang et al, 1993).
This an-swer?s (in)correctness is then automatically classi-fied based on this transcript, using the TuTalk se-mantic analyzer (Jordan et al, 2007), and the an-swer?s (un)certainty is automatically classified byinputting features of the speech signal, the automatictranscript, and the dialogue context into a logisticregression model.
We will discuss these featuresfurther in Section 5.
All natural language process-ing components were trained using prior ITSPOKEcorpora.
The appropriate tutor response is deter-mined based on the answer?s automatically labeled(in)correctness and (un)certainty and then sent to theCepstral text-to-speech system3, whose audio outputis played through the student headphones and is alsodisplayed on a web-based interface.The experimental procedure was as follows: col-lege students with no college-level physics (1) reada short physics text, (2) took a pretest, (3) worked5 ?training?
problems with ITSPOKE, where eachuser received a varying level of uncertainty adapta-tion based on condition, (4) took a user satisfactionsurvey, (5) took a posttest isomorphic to the pretest,and (6) worked a ?test?
problem with ITSPOKE thatwas isomorphic to the 5th training problem, whereno user received any uncertainty adaptation.The resulting corpus contains 432 dialogues (6per student) and 7216 turns from 72 students, 47female and 25 male.
All turns are used in the dis-engagement detection experiments described next.However, only the training problem dialogues (360,5 per student, 6044 student turns) are used for theperformance analyses in Sections 6-7, because thefinal test problem was given after the instrumentsmeasuring performance (survey and posttest).Our survey and tests are the same as those used inmultiple prior ITSPOKE experiments (c.f., (Forbes-Riley and Litman, 2011a)).
The pretest and posttesteach contain 26 multiple choice questions queryingknowledge of the topics covered in the dialogues.Average pretest and posttest scores in the corpuswere 51.0% and 73.1% (out of 100%) with stan-dard deviations of 14.5% and 13.8%, respectively.The user satisfaction survey contains 16 statementsrated on a 5-point Likert scale.
Average total sur-3an outgrowth of Festival (Black and Taylor, 1997).93vey score was 60.9 (out of 80), with a standard de-viation of 8.5.
While the statements themselves arelisted elsewhere (Forbes-Riley and Litman, 2009),9 statements concern the tutoring domain (e.g., Thetutor was effective/precise/useful), 7 of which weretaken from (Baylor et al, 2003) and 2 of whichwere created for our system.
3 statements concernuser uncertainty levels and were created for our sys-tem.
4 statements concern the spoken dialogue in-teraction (e.g., It was easy to understand the tutor?sspeech) and were taken from (Walker et al, 2002).Our survey has also been incorporated into other re-cent work exploring user satisfaction in spoken dia-logue computer tutors (Dzikovska et al, 2011).
InSection 6 we discuss how user scores on these in-struments are used to measure system performance.See (Forbes-Riley and Litman, 2011a) for furtherdetails of ITSPOKE and the 2008 experiment.Following the experiment, the entire corpuswas manually labeled for (in)correctness (cor-rect, incorrect), (un)certainty (CER, UNC) and(dis)engagement (ENG, DISE) by one trained an-notator.
Table 1 shows the distribution of the la-beled turns in the 2008 ITSPOKE corpus.
In priorITSPOKE corpora, our annotator displayed interan-notator agreement of 0.85 and 0.62 Kappa on cor-rectness and uncertainty, respectively (Forbes-Rileyand Litman, 2011a).
For the disengagement label,a reliability analysis was performed over several an-notation rounds on subsets of the 2008 ITSPOKEcorpus by this and a second trained annotator, yield-ing 0.55 Kappa (this analysis is described in detailelsewhere (Forbes-Riley et al, 2011)).
Our Kap-pas indicate that user uncertainty and disengage-ment can both be annotated with moderate reliabil-ity in our dataset, on par with prior emotion anno-tation work (c.f., (Pon-Barry and Shieber, 2011)).Note however that the best way to label users?
in-ternal affective state(s) is still an open question.Many system researchers (including ourselves) relyon trained labelers (e.g., (Pon-Barry et al, 2006;Porayska-Pomsta et al, 2008)) while others use self-reports (e.g., (Conati and Maclaren, 2009; Gratch etal., 2009; McQuiggan et al, 2008)).
Both meth-ods are problematic; for example both can be ren-dered inaccurate when users mask their true feel-ings.
Two studies that have compared self-reports,peer labelers, trained labelers, and combinations oflabelers (Afzal and Robinson, 2011; D?Mello et al,2008) both illustrate the common finding that hu-man annotators display low to moderate interannota-tor reliability for affect annotation, and both studiesshow that trained labelers yield the highest reliabil-ity on this task.
Despite the lack of high interan-notator reliability, responding to affect detected bytrained human labels has still been shown to improvesystem performance (see Section 1).Table 1: 2008 ITSPOKE Corpus Description (N=7216)Turn Label Total PercentDisengaged 1170 16.21%Correct 5330 73.86%Uncertain 1483 20.55%Uncertain+Disengaged 373 5.17%4 Automatically Detecting UserDisengagement (DISE) in ITSPOKEAs noted in Section 1, we have developed a user dis-engagement detector to incorporate into our existinguncertainty-adaptive spoken dialogue system.
Theresult will be a state of the art system that adapts tomultiple affective states during the dialogue.4.1 Binary DISE LabelOur disengagement annotation scheme (Forbes-Riley et al, 2011) was derived from empirical ob-servations in our data but draws on prior work,including work mentioned in Section 2, appraisaltheory-based emotion models (e.g., Conati and Ma-claren (2009))4, and prior approaches to annotatingdisengagement or related states in tutoring (Lehmanet al, 2008; Porayska-Pomsta et al, 2008).Briefly, our overall Disengagement label (DISE)is used for turns expressing moderate to strong dis-engagement towards the interaction, i.e., responsesgiven without much effort or without caring aboutappropriateness.
Responses might also be accompa-nied by signs of inattention, boredom, or irritation.Clear examples include answers spoken quickly inleaden monotone, with sarcastic or playful tones,or with off-task sounds such as rhythmic tapping or4Appraisal theorists distinguish emotional behaviors fromtheir underlying causes, arguing that emotions result from anevaluation of a context.94electronics usage.5 Note that our DISE label is de-fined independently of the tutoring domain and thusshould generalize across spoken dialogue systems.Figure 1 illustrates the DISE, (in)correctness, and(un)certainty labels across 3 tutor/student turn pairs.U1 is labeled DISE and UNC because the studentgave up immediately and with irritation when toomuch prior knowledge was required.
U2 is labeledDISE and UNC because the student avoided giv-ing a specific numerical value, offering instead avague (and obviously incorrect) answer.
U3 is la-beled DISE and CER because the student sang thecorrect answer, indicating a lack of interest in thelarger purpose of the material being discussed.6T1: What is the definition of Newton?s Second Law?U1: I have no idea <sigh>.
(DISE, incorrect, UNC).
.
.T2: What?s the numerical value of the man?s accelera-tion?
Please specify the units too.U2: The speed of the elevator.
Meters per second.
(DISE,incorrect, UNC).
.
.T3: What are the forces acting on the keys after the manreleases them?U3: graaa-vi-tyyyyy <sings the answer> (DISE, cor-rect, CER)Figure 1: Corpus Example Illustrating the User Turn La-bels ((Dis)Engagement, (In)Correctness, (Un)Certainty)4.2 DISE Detection MethodMachine learning classification was done at the turnlevel using WEKA software7 and 10-fold cross val-idation.
A J48 decision tree was chosen because ofits easily read output and the fact that previous ex-periments with our data showed little variance be-5Affective systems research has found total disengagementrare in laboratory settings (Lehman et al, 2008; Martalo et al,2008).
As in that research, we equate the DISE label with noor low engagement.
Since total disengagement is common inreal-world unobserved human-computer interactions (deletingunsatisfactory software being an extreme example) it remainsan open question as to how well laboratory findings generalize.6Our original scheme distinguished six DISE subtypesthat trained annotators distinguished with a reliability of .43Kappa (Forbes-Riley et al, 2011).
However, pilot experimentsindicated that our models cannot accurately distinguish them,thus our DISE detector focuses on the DISE label.7http://www.cs.waikato.ac.nz/ml/weka/tween different machine learning algorithms (Drum-mond and Litman, 2011).
We also use a cost matrix,which heavily penalizes classifying a true DISE in-stance as false, because our class distributions arehighly skewed (16.21% DISE turns) and the costmatrix successfully mitigated the skew?s effect inour prior work, where the uncertainty distribution isalso skewed (20.55% UNC turns) (Drummond andLitman, 2011).To train our DISE model, we first extracted the setof speech and dialogue features shown in Figure 2from the user turns in our corpus.
As shown, theacoustic-prosodic features represent duration, paus-ing, pitch, and energy, and were normalized by thefirst user turn, as well as totaled and averaged overeach dialogue.
The lexical and dialogue featuresconsist of the current dialogue name (i.e., one of thesix physics problems) and turn number, the currentITSPOKE question?s name (e.g.,T3 in Figure 1 hasa unique identifier) and depth in the discourse struc-ture (e.g., an ITSPOKE remediation question afteran incorrect user answer would be at one greaterdepth than the prior question), a word occurrencevector for the automatically recognized text of theuser turn, an automatic (in)correctness label, andlastly, the number of user turns since the last cor-rect turn (?incorrect runs?).
We also included twouser-based features, gender and pretest score.?
Acoustic-Prosodic Featurestemporal features: turn duration, prior pause dura-tion, turn-internal silencefundamental frequency (f0) and energy (RMS) fea-tures: maximum, minimum, mean, std.
deviationrunning totals and averages for all features?
Lexical and Dialogue Featuresdialogue name and turn numberquestion name and question depthITSPOKE-recognized lexical items in turnITSPOKE-labeled turn (in)correctnessincorrect runs?
User Identifier Features:gender and pretest scoreFigure 2: Features Used to Detect Disengagement (DISE)for each User Turn95Table 2: Results of 10-fold Cross-Validation Experiment with J48 Decision Tree Algorithm Detecting the Binary DISELabel in the 2008 ITSPOKE Corpus (N=7216 user turns)Algorithm Accuracy UA Precision UA Recall UA Fmeasure CC MLEDecision Tree 83.1% 68.9% 68.7% 68.8% 0.52 0.25Majority Label 83.8% 41.9% 50.0% 45.6% ?
0.27Note that although our feature set was drawn pri-marily from our prior uncertainty detection exper-iments (Forbes-Riley and Litman, 2011a; Drum-mond and Litman, 2011), we have also experi-mented with other features, including state-of-the-art acoustic-prosodic features used in the last Inter-speech Challenges (Schuller et al, 2010; Schuller etal., 2009b) and made freely available in the openS-MILE Toolkit (Florian et al, 2010).
To date, how-ever, these features have only decreased the cross-validation performance of our models.8 While someof our features are tutoring-specific, these have sim-ilar counterparts in other applications (i.e., answer(in)correctness corresponds to a more general no-tion of ?response appropriateness?
in other domains,while pretest score corresponds to the general no-tion of domain expertise).
Moreover, all of our fea-tures are fully automatic and available in real-time,so that the model can be directly implemented anddeployed.
To that end, we now describe the resultsof our intrinsic and extrinsic evaluations of our DISEmodel, aimed at determining whether it is ready tobe evaluated with real users.5 Intrinsic Evaluation: Cross-ValidationTable 2 shows the averaged results of the cross-validation with the J48 decision tree algorithm.
Inaddition to accuracy, we use Unweighted Aver-age (UA) Precision9, Recall, and F-measure be-cause they are the standard measures used to eval-uate current affect recognition technology, particu-larly for unbalanced two-class problems (Schulleret al, 2009b).
In addition, we use the cross corre-lation (CC) measure and mean linear error (MLE)because these metrics were used in recent work forevaluating disengagement (level of interest) detec-tors for the Interspeech 2010 challenge (Schuller et8We also tried using our automatic UNC label as a feature inour DISE model, but our results weren?t significantly improved.9simply ((Precision(DISE) + Precision(ENG))/2)al., 2010; Wang and Hirschberg, 2011; Jeon et al,2010)).10 Note however that the Interspeech 2010task differs from ours not only in the corpus and fea-tures, but also in the learning task: they used regres-sion to detect a continuous level of interest rangingfrom 0 to 1, while we detect a binary class.
Thuscomparison between our results and those are onlysuggestive rather than conclusive.As shown in Table 2, we also compare our resultswith those of majority class (ENG) labeling of thesame turns.
Since (7216-1170)/7216 user turns inthe corpus are engaged (recall Table 1), always se-lecting the majority class (ENG) label for these turnsthus yields 83.8% accuracy (with 0% precision andrecall for DISE, and 83.8% precision and 100% re-call for ENG).
While our DISE model does not out-perform majority class labeling with respect to ac-curacy, this is not surprising given the steep skewin class distribution, and our learned model signif-icantly outperforms the baseline with respect to allthe other measures (p<.001).11Our CC and MLE results are on par with the bestresults from the state-of-the-art systems competingin the 2010 Interspeech Challenge, where the taskwas to detect level of interest.
In particular, the win-ner obtained a CC of 0.428 (higher numbers are bet-ter) and an MLE of 0.146 (lower numbers are bet-ter) (Jeon et al, 2010), while a subsequent studyyielded a CC of 0.480 and an MLE of 0.131 onthe same corpus (Wang and Hirschberg, 2011).
Ourresults are also on par with the best results of theother prior research on detecting disengagement dis-cussed in Section 2 that detects a small number ofdisengagement classes and reports accuracy and/orrecall and precision.
For example, (Martalo et al,2008) report average precision of 75% and recall10Pearson product-moment correlation coefficient (CC) is ameasure of the linear dependence that is widely used in regres-sion settings.
MLE is a regression performance measure for themean absolute error between an estimator and the true value.11CC is undefined for majority class labeling.96of 74% (detecting three levels of disengagement),while (Kapoor and Picard, 2005) report an accuracyof 86% for detecting binary (dis)interest.Our final DISE model was produced by runningthe J48 algorithm over our entire corpus.
The re-sulting decision tree contains 141 nodes and 75leaves.
Inspection of the tree reveals that all of thefeature types in Figure 2 (acoustic-prosodic, lexi-cal/dialogue, user identifier) are used as decisionnodes in the tree, although not all variations on thesetypes were used.
The upper-level nodes of the treeare usually considered to be more informative fea-tures as compared to lower-level nodes, since theyare queried for more leaves.
The upper level ofthe DISE model consists entirely of temporal, lex-ical, pitch and energy features as well as questionname and depth and incorrect runs, while featuressuch as gender, turn number, and dialogue nameappear only near the leaves, and pretest score andturn (in)correctness don?t appear at all.
The amountof pausing prior to the start of the user turn is themost important feature for determining disengage-ment, with pauses shorter than a quarter second be-ing labeled DISE, suggesting that fast answers are astrong signal of disengagement in our system.
Userswho answer quickly may do so without taking thetime to think it through; the more engaged user, incontrast, takes more time to prepare an answer.Three lexical items from the student turns, ?fric-tion?, ?light?, and ?greater?, are the next most im-portant features in the tree, suggesting that particularconcepts and question types can be typically associ-ated with user disengagement in a system.
For ex-ample, open-ended system questions may lead usersto disengage due to frustration from not knowingwhen their answer is complete.
One common casein ITSPOKE involves asking users to name all theforces on an object; some users don?t know howmany to list, so they start listing random forces, suchas ?friction.?
On the other hand, multiple choicequestions can also lead users to disengage; they be-gin with a reasonable chance of being correct andthus don?t take the time to think through their an-swer.
One common case in ITSPOKE involves ask-ing users to determine which of two objects has thegreater or lesser force, acceleration, and velocity.While our feature set is highly generalizable toother domains, it is an empirical question as towhether the feature values we found maximally ef-fective for predicting disengagement also general-ize to other domains.
Intuition is often unreliable,and it has been widely shown in affect predictionthat the answer can depend on domain, dataset, andlearning algorithm employed.
Moreover, there aremany types of spoken dialogue systems with dif-ferent styles and no single type can represent theentire field.
That said, it is also important to notethat there are lessons to be learned from the featuresselected for one particular domain, in terms of thetake-home message for other domains.
For example,the fact that ?prior pause?
is selected as a strong sig-nal of disengagement in ITSPOKE dialogues mayindicate that the feature itself (regardless of its se-lected value) could be transferred to different do-mains, alone or in the demonstrated combinationswith the other selected features.6 Extrinsic Evaluation: CorrelationNext we use extrinsic evaluation to confirm that ourfinal DISE model is both useful and a reasonablesubstitute for our gold standard manual DISE la-bels.
With respect to showing the utility of detectingDISE, we use a correlational analysis to show thatthe gold standard (manual) DISE values are signif-icantly predictive of two different measures of sys-tem performance.12 With respect to showing the ad-equacy of our current level of detection performancefor the learned DISE model, we demonstrate that af-ter replacing the manual DISE labels with the au-tomatic DISE labels when running our correlations,the automatic labels are related to performance inthe same way as the gold standard labels.Thus for both our automatically detected DISE la-bels (auto) and our gold standard DISE labels (man-ual), we first computed the total number of occur-rences for each student, and then computed a bivari-ate Pearson?s correlation between this total and twodifferent metrics of performance: learning gain (LG)and user satisfaction (US).
In the tutoring domain,learning is the primary performance metric and as iscommon in this domain we compute it as normal-ized learning gain ((posttest score-pretest score)/(1-12Spoken dialogue research has shown that redesigning a sys-tem in light of such correlational analysis can indeed yield per-formance improvements (Rotaru and Litman, 2009).97Table 3: Correlations between Disengagement and both Satisfaction and Learning in ITSPOKE Corpus (N=72 users)Measure Mean (SD) User Satisfaction Learning GainR p R pTotal Manual DISE 12.3 (7.3) -0.25 0.031 -0.35 0.002Total Automatic DISE 12.6 (7.4) -0.26 0.029 -0.31 0.009pretest score)).
In spoken dialogue systems, user sat-isfaction is the primary performance metric and asis common in this domain we compute it by totalingover the user satisfaction survey scores.13Table 3 shows first the mean and standard devia-tion for the DISE label over all students, the Pear-son?s Correlation coefficient (R) and its significance(p).
As shown, both our manual and automatic DISElabels are significantly related to performance, re-gardless of whether we measure it as user satisfac-tion or learning gain.14 Moreover, in both cases thecorrelations are nearly identical between the man-ual and automatic labels.
These results indicate thatthe detected DISE values are a useful substitute forthe gold standard, and suggest that redesigning IT-SPOKE to recognize and respond to DISE can sig-nificantly improve system performance.7 Extrinsic Evaluation: Affective StateMultiple RegressionBecause we are adding our disengagement detectorto a spoken dialogue system that already detects andadapts to user uncertainty, we argue that it is alsonecessary to evaluate whether greater performancebenefits are likely to be obtained by adapting to asecond state.
In other words, given how difficult it isto effectively detect and adapt to one user affectivestate, is performance likely to improve by detectingand adapting to multiple affective states?To answer this question, we performed a multi-ple linear regression analysis aimed at quantifyingthe relative usefulness of the automatically detected13Identical results were obtained by using an average insteadof a total, and only slightly weaker results were obtained whennormalizing the DISE totals as the percentages of total turns.14We previously found a related correlation between differentDISE and learning measures, during the analysis of our DISEannotation scheme (Forbes-Riley and Litman, 2011b).
In par-ticular, we showed a significant partial correlation between thepercentage of manual DISE labels and posttest controlled forpretest score.disengagement and uncertainty labels when predict-ing our system performance metrics.
We ran fourstepwise linear regressions.
The first regression pre-dicted learning gain, and gave the model two possi-ble inputs: the total number of automatic DISE la-bels and UNC labels per user.
We then ran the sameregression again, this time predicting user satisfac-tion.
For comparison, we ran the same two regres-sions using the manual DISE and UNC labels.As the trained regression models in Figure 3 show,when predicting learning gain, selecting both auto-matically detected affective state metrics as inputssignificantly increases the model?s predictive poweras compared to only selecting one.15 The (stan-dardized) feature weights indicate relative predic-tive power in accounting for the variance in learn-ing gain.
As shown, both automatic affect metricshave the same weight in the final model.
This re-sult suggests that adapting to our automatically de-tected disengagement and uncertainty labels can fur-ther improve learning over and above adapting to un-certainty alone.
Although the final model?s predic-tive power is low (R2=0.15), our interest here is onlyin investigating whether the two affective states aremore useful in combination than in isolation for pre-dicting performance.
In similar types of stepwise re-gressions on prior ITSPOKE corpora, we?ve shownthat more complete models of system performanceincorporating many predictors of learning (i.e.
af-fective states in conjunction with other dialogue fea-tures) can yield R2 values of over .5 (Forbes-Rileyet al, 2008).1615Using the stepwise method, Automatic DISE was the firstfeature selected, and Automatic UNC the second.
However,note that a model consisting of only the Automatic UNC metricalso yields significantly worse predictive power than selectingboth affective state metrics.
Further note that almost identicalmodels were produced using percentages rather than totals.16R2 is the standard reported metric for linear regressions.However, for consistency with Table 3, note that the two modelsin Figure 3 yield R values of -.31 and -.38, respectively.98Learning Gain = -.31 * Total Automatic DISE (R2=.09, p=.009)Learning Gain = -.24 * Total Automatic DISE - .24 * Total Automatic UNC (R2=.15, p=.004)Figure 3: Performance Model?s Predictive Power Increases Significantly with Multiple Affective FeaturesInterestingly, for the regression models of learn-ing gain that used manual affect metrics, only theDISE metric was selected as an input.
This indi-cates that the automatic affective state labels are use-ful in combination for predicting performance in away that is not reflected in their gold standard coun-terparts.
Detecting multiple affective states mightthus be one way to compensate for the noise that isintroduced in a fully-automated affective spoken di-alogue system.Similarly, only the DISE metric was selectedfor inclusion in the regression model of user sat-isfaction, regardless of whether manual or auto-matic labels were used.
A separate correlationanalysis showed that user uncertainty is not sig-nificantly correlated with user satisfaction in oursystem, though we previously found that multipleuncertainty-related metrics do significantly correlatewith learning (Litman and Forbes-Riley, 2009).8 Summary and Current DirectionsIn this paper we used extrinsic evaluations to pro-vide evidence for the utility of a new system de-sign involving the complex task of user affect de-tection, prior to undertaking an expensive and time-consuming evaluation of an affect-adaptive systemwith real users.
In particular, we first presented anovel model for automatically detecting user disen-gagement in spoken dialogue systems.
We showedthrough intrinsic evaluations (i.e., cross-validationexperiments using gold-standard labels) that themodel yields results on par with prior work.
Wethen showed crucially through novel extrinsic eval-uation that the resulting automatically detected dis-engagement labels correlate with two primary per-formance metrics (user satisfaction and learning) inthe same way as gold standard (manual) labels.
Thissuggests that adapting to the automatic disengage-ment labels has the potential to significantly improveperformance even in the presence of noise from theautomatic labeling.
Finally, further extrinsic anal-yses using multiple regression suggest that adapt-ing to our automatic disengagement labels can im-prove learning (though not user satisfaction) overand above the improvement achieved by only adapt-ing to automatically detected user uncertainty.We have already developed and implemented anadaptation for user disengagement in ITSPOKE.The disengagement adaptation draws on empiri-cal analyses of our data and effective responsesto user disengagement presented in prior work(c.f., (Forbes-Riley and Litman, 2011b)), We arecurrently evaluating our disengagement adaptationin the ?ideal?
environment of a Wizard of Oz exper-iment, where user disengagement, uncertainty, andcorrectness are labeled by a hidden human duringuser interactions with ITSPOKE.Based on the evaluations here, we believe our dis-engagement model is ready for implementation inITSPOKE.
We will then evaluate the resulting spo-ken dialogue system for detecting and adapting tomultiple affective states in an upcoming controlledexperiment with real users.AcknowledgmentsThis work is funded by NSF award 0914615.
Wethank Scott Silliman for systems support.ReferencesS.
Afzal and P. Robinson.
2011.
Natural affect data:Collection and annotation.
In Sidney D?Mello andRafael Calvo, editors, Affect and Learning Technolo-gies.
Springer.G.
Aist, B. Kort, R. Reilly, J. Mostow, and R. Pi-card.
2002.
Experimentally augmenting an intelli-gent tutoring system with human-supplied capabili-ties: Adding human-provided emotional scaffolding toan automated reading tutor that listens.
In Proc.
In-telligent Tutoring Systems Conference (ITS) Workshopon Empirical Methods for Tutorial Dialogue Systems,pages 16?28, San Sebastian, Spain.A.
Alwan, Y. Bai, M. Black, L. Caseyz, M. Gerosa,M.
Heritagez, M. Iseliy, M. Jonesz, A. Kazemzadeh,S.
Lee, S. Narayanan, P. Pricex, J. Tepperman, and99S.
Wangy.
2007.
A system for technology based as-sessment of language and literacy in young children:the role of multiple information sources.
In Proceed-ings of the 9th IEEE International Workshop on Multi-media Signal Processing (MMSP), pages 26?30, Cha-nia, Greece, October.J.
Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-cke.
2002.
Prosody-based automatic detection of an-noyance and frustration in human-computer dialog.
InJ.
H. L. Hansen and B. Pellom, editors, Proceedingsof the International Conference on Spoken LanguageProcessing (ICSLP), pages 2037?2039, Denver, USA.A.
Batliner, S. Steidl, C. Hacker, and E. Noth.
2008.Private emotions vs. social interaction - a data-drivenapproach towards analysing emotion in speech.
UserModeling and User-Adapted Interaction: The Journalof Personalization Research, 18:175?206.A.
L. Baylor, J. Ryu, and E. Shen.
2003.
The effectof pedagogical agent voice and animation on learning,motivation, and perceived persona.
In Proceedings ofthe ED-MEDIA Conference, Honolulu, Hawaii, June.A.
Black and P. Taylor.
1997.
Festival speech synthe-sis system: system documentation (1.1.1).
The Centrefor Speech Technology Research, University of Edin-burgh, http://www.cstr.ed.ac.uk/projects/festival/.D.
Bohus and E. Horvitz.
2009.
Models for multipartyengagement in open-world dialog.
In Proceedings ofSIGdial, London, UK.C.
Conati and H. Maclaren.
2009.
Empirically build-ing and evaluating a probabilistic model of user af-fect.
User Modeling and User-Adapted Interaction,19(3):267?303.R.
Cowie and R. R. Cornelius.
2003.
Describing theemotional states that are expressed in speech.
SpeechCommunication, 40(1-2):5?32.L.
Devillers and L. Vidrascu.
2006.
Real-life emo-tions detection with lexical and paralinguistic cues onhuman-human call center dialogs.
In Ninth Inter-national Conference on Spoken Language Processing(ICSLP, pages 801?804, Pittsburgh, PA, September.S.
D?Mello, S. Craig, A. Witherspoon, B. McDaniel, andA.
Graesser.
2008.
Automatic detection of learner?saffect from conversational cues.
User Modeling andUser-Adapted Interaction: The Journal of Personal-ization Research, 18:45?80.S.
D?Mello, B. Lehman, J. Sullins, R. Daigle, R. Combs,K.
Vogt, L. Perkins, and A. Graesser.
2010.
A timefor emoting: When affect-sensitivity is and isn?t effec-tive at promoting deep learning.
In Intelligent TutoringSystems Conference, pages 245?254, Pittsburgh, PA,USA, June.J.
Drummond and D. Litman.
2011.
Examining the im-pacts of dialogue content and system automation onaffect models in a spoken tutorial dialogue system.In Proc.
12th Annual Meeting of the Special InterestGroup on Discourse and Dialogue (SIGDIAL), pages312?318, Portland, Oregon, June.M.
Dzikovska, J. Moore, N. Steinhauser, and G. Camp-bell.
2011.
Exploring user satisfaction in a tutorialdialogue system.
In Proc.
12th Annual Meeting ofthe Special Interest Group on Discourse and Dialogue(SIGDIAL), pages 162?172, Portland, Oregon, June.E.
Florian, M. Wollmer, and B. Schuller.
2010.
The Mu-nich versatile and fast open-source audio feature ex-tractor.
In Proc.
ACM Multimedia (MM), pages 1459?1462, Florence, Italy.K.
Forbes-Riley and D. Litman.
2009.
A user modeling-based performance analysis of a wizarded uncertainty-adaptive dialogue system corpus.
In Proc.
Inter-speech, Brighton, UK, September.K.
Forbes-Riley and D. Litman.
2011a.
Benefits andchallenges of real-time uncertainty detection and adap-tation in a spoken dialogue computer tutor.
SpeechCommunication, 53(9?10):1115?1136.K.
Forbes-Riley and D. Litman.
2011b.
When doesdisengagement correlate with learning in spoken dia-log computer tutoring?
In Proceedings 15th Interna-tional Conference on Artificial Intelligence in Educa-tion (AIED), Auckland, NZ, June.K.
Forbes-Riley, M. Rotaru, and D. Litman.
2008.
Therelative impact of student affect on performance mod-els in a spoken dialogue tutoring system.
User Model-ing and User-Adapted Interaction, 18(1-2):11?43.K.
Forbes-Riley, D. Litman, and H. Friedberg.
2011.
An-notating disengagement for spoken dialogue computertutoring.
In Sidney D?Mello and Rafael Calvo, editors,Affect and Learning Technologies.
Springer.Jonathan Gratch, Stacy Marsella, Ning Wang, andBrooke Stankovic.
2009.
Assessing the validity ofappraisal-based models of emotion.
In Proceedings ofACII, Amsterdam, Netherlands.X.
D. Huang, F. Alleva, H. W. Hon, M. Y. Hwang, K. F.Lee, and R. Rosenfeld.
1993.
The SphinxII speechrecognition system: An Overview.
Computer, Speechand Language.J.
H. Jeon, R. Xia, and Y. Liu.
2010.
Level of interestsensing in spoken dialog using multi-level fusion ofacoustic and lexical evidence.
In INTERSPEECH?10,pages 2802?2805.P.
Jordan, B.
Hall, M. Ringenberg, Y. Cui, and C.P.
Rose.2007.
Tools for authoring a dialogue agent that par-ticipates in learning studies.
In Proc.
Artificial Intelli-gence in Education (AIED), pages 43?50.A.
Kapoor and R. W. Picard.
2005.
Multimodal affectrecognition in learning environments.
In 13th AnnualACM International Conference on Multimedia, pages677?682, Singapore.100J.
Klein, Y.
Moon, and R. Picard.
2002.
This computerresponds to user frustration: Theory, design, and re-sults.
Interacting with Computers, 14:119?140.C.
M. Lee and S. Narayanan.
2005.
Towards detect-ing emotions in spoken dialogs.
IEEE Transactionson Speech and Audio Processing, 13(2), March.B.
Lehman, M. Matthews, S. D?Mello, and N. Per-son.
2008.
What are you feeling?
Investigatingstudent affective states during expert human tutoringsessions.
In Intelligent Tutoring Systems Conference(ITS), pages 50?59, Montreal, Canada, June.D.
Litman and K. Forbes-Riley.
2009.
Spoken tutorialdialogue and the feeling of another?s knowing.
In Pro-ceedings 10th Annual Meeting of the Special InterestGroup on Discourse and Dialogue (SIGDIAL), Lon-don, UK, September.K.
Liu and R. W. Picard.
2005.
Embedded empathyin continuous, interactive health assessment.
In CHIWorkshop on HCI Challenges in Health Assessment.A.
Martalo, N. Novielli, and F. de Rosis.
2008.
Attitudedisplay in dialogue patterns.
In Proc.
AISB 2008 Sym-posium on Affective Language in Human andMachine,pages 1?8, Aberdeen, Scotland, April.S.
McQuiggan, B. Mott, and J. Lester.
2008.
Model-ing self-efficacy in intelligent tutoring systems: An in-ductive approach.
User Modeling and User-AdaptedInteraction (UMUAI), 18(1-2):81?123, February.T.
Paek and Y.-C. Ju.
2008.
Accommodating explicituser expressions of uncertainty in voice search orsomething like that.
In Proceedings of the 9th AnnualConference of the International Speech Communica-tion Association (INTERSPEECH 08), pages 1165?1168, Brisbane, Australia, September.H.
Pon-Barry and S. Shieber.
2011.
Recognizing uncer-tainty in speech.
EURASIP Journal on Advances inSignal Processing.H.
Pon-Barry, K. Schultz, E. Owen Bratt, B. Clark, andS.
Peters.
2006.
Responding to student uncertainty inspoken tutorial dialogue systems.
International Jour-nal of Artificial Intelligence in Education, 16:171?194.K.
Porayska-Pomsta, M. Mavrikis, and H. Pain.
2008.Diagnosing and acting on student affect: the tutor?sperspective.
User Modeling and User-Adapted In-teraction: The Journal of Personalization Research,18:125?173.H.
Prendinger and M. Ishizuka.
2005.
The Empa-thetic Companion: A character-based interface that ad-dresses users?
affective states.
International Journal ofApplied Artificial Intelligence, 19(3):267?285.M.
Rotaru and D. Litman.
2009.
Discourse structure andperformance analysis: Beyond the correlation.
In Pro-ceedings 10th Annual Meeting of the Special InterestGroup on Discourse and Dialogue (SIGDIAL), Lon-don, UK.B.
Schuller, R. Muller, F. Eyben, J. Gast, B. Hrnler,M.
Wollmer, G. Rigoll, A. Hthker, and H. Konosu.2009a.
Being bored?
recognising natural interest byextensive audiovisual integration for real-life applica-tion.
Image and Vision Computing Journal, SpecialIssue on Visual and Multimodal Analysis of HumanSpontaneous Behavior, 27:1760?1774.B.
Schuller, S. Steidl, and A. Batliner.
2009b.
TheInterspeech 2009 Emotion Challenge.
In Proceed-ings of the 10th Annual Conference of the Inter-national Speech Communication Association (Inter-speech), ISCA, Brighton, UK, September.B.
Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Dev-illers, C. Muller, and S. Narayanan.
2010.
TheInterspeech 2010 Paralinguistic Challenge.
In Pro-ceedings of the 11th Annual Conference of the In-ternational Speech Communication Assocation (Inter-speech), pages 2794?2797, Chiba, Japan, September.I.
Shafran, M. Riley, and M. Mohri.
2003.
Voice signa-tures.
In Proceedings of the IEEE Automatic SpeechRecognition and Understanding Workshop (ASRU),pages 31?36, St. Thomas, US Virgin Islands.C.
Sidner and C. Lee.
2003.
An architecture for engage-ment in collaborative conversations between a robotand a human.
Technical Report TR2003-12, MERL.A.
Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-tin, and M. Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26(3).W.
Tsukahara and N. Ward.
2001.
Responding to subtle,fleeting changes in the user?s internal state.
In Pro-ceedings of the SIG-CHI on Human factors in comput-ing systems, pages 77?84, Seattle, WA.
ACM.K.
VanLehn, P. W. Jordan, C.
Rose?, D. Bhembe,M.
Bo?ttner, A. Gaydos, M. Makatchev, U. Pap-puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-vastava, and R. Wilson.
2002.
The architecture ofWhy2-Atlas: A coach for qualitative physics essaywriting.
In Proc.
Intl.
Conf.
on Intelligent TutoringSystems.L.
Vidrascu and L. Devillers.
2005.
Detection of real-life emotions in dialogs recorded in a call center.
InProceedings of INTERSPEECH, Lisbon, Portugal.M.Walker, A. Rudnicky, R. Prasad, J. Aberdeen, E. Bratt,J.
Garofolo, H. Hastie, A.
Le, B. Pellom, A. Potami-anos, R. Passonneau, S. Roukos, G. Sanders, S. Sen-eff, and D. Stallard.
2002.
DARPA communicator:Cross-system results for the 2001 evaluation.
In Proc.ICSLP.W.
Wang and J. Hirschberg.
2011.
Detecting levels ofinterest from spoken dialog with multistream predic-tion feedback and similarity based hierarchical fusion101learning.
In Proc.
12th Annual Meeting of the Spe-cial Interest Group on Discourse and Dialogue (SIG-DIAL), pages 152?161, Portland, Oregon, June.N.
Wang, W.L.
Johnson, R. E. Mayer, P. Rizzo, E. Shaw,and H. Collins.
2008.
The politeness effect: Peda-gogical agents and learning outcomes.
InternationalJournal of Human-Computer Studies, 66(2):98?112.102
