Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368?378,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLexical Normalisation of Short Text Messages: Makn Sens a #twitterBo Han and Timothy BaldwinNICTA Victoria Research LaboratoryDepartment of Computer Science and Software EngineeringThe University of Melbournehanb@student.unimelb.edu.au tb@ldwin.netAbstractTwitter provides access to large volumes ofdata in real time, but is notoriously noisy,hampering its utility for NLP.
In this paper, wetarget out-of-vocabulary words in short textmessages and propose a method for identify-ing and normalising ill-formed words.
Ourmethod uses a classifier to detect ill-formedwords, and generates correction candidatesbased on morphophonemic similarity.
Bothword similarity and context are then exploitedto select the most probable correction can-didate for the word.
The proposed methoddoesn?t require any annotations, and achievesstate-of-the-art performance over an SMS cor-pus and a novel dataset based on Twitter.1 IntroductionTwitter and other micro-blogging services are highlyattractive for information extraction and text miningpurposes, as they offer large volumes of real-timedata, with around 65 millions tweets posted on Twit-ter per day in June 2010 (Twitter, 2010).
The qualityof messages varies significantly, however, rangingfrom high quality newswire-like text to meaninglessstrings.
Typos, ad hoc abbreviations, phonetic sub-stitutions, ungrammatical structures and emoticonsabound in short text messages, causing grief for textprocessing tools (Sproat et al, 2001; Ritter et al,2010).
For instance, presented with the input u mustbe talkin bout the paper but I was thinkin movies(?You must be talking about the paper but I wasthinking movies?
),1 the Stanford parser (Klein and1Throughout the paper, we will provide a normalised versionof examples as a gloss in double quotes.Manning, 2003; de Marneffe et al, 2006) analysesbout the paper and thinkin movies as a clause andnoun phrase, respectively, rather than a prepositionalphrase and verb phrase.
If there were some way ofpreprocessing the message to produce a more canon-ical lexical rendering, we would expect the qualityof the parser to improve appreciably.
Our aim in thispaper is this task of lexical normalisation of noisyEnglish text, with a particular focus on Twitter andSMS messages.
In this paper, we will collectivelyrefer to individual instances of typos, ad hoc abbre-viations, unconventional spellings, phonetic substi-tutions and other causes of lexical deviation as ?ill-formed words?.The message normalisation task is challenging.It has similarities with spell checking (Peterson,1980), but differs in that ill-formedness in text mes-sages is often intentional, whether due to the desireto save characters/keystrokes, for social identity, ordue to convention in this text sub-genre.
We proposeto go beyond spell checkers, in performing deabbre-viation when appropriate, and recovering the canon-ical word form of commonplace shorthands like b4?before?, which tend to be considered beyond theremit of spell checking (Aw et al, 2006).
The freewriting style of text messages makes the task evenmore complex, e.g.
with word lengthening such asgoooood being commonplace for emphasis.
In ad-dition, the detection of ill-formed words is difficultdue to noisy context.Our objective is to restore ill-formed words totheir canonical lexical forms in standard English.Through a pilot study, we compared OOV words inTwitter and SMS data with other domain corpora,368revealing their characteristics in OOV word distri-bution.
We found Twitter data to have an unsur-prisingly long tail of OOV words, suggesting thatconventional supervised learning will not performwell due to data sparsity.
Additionally, many ill-formed words are ambiguous, and require contextto disambiguate.
For example, Gooood may refer toGood or God depending on context.
This providesthe motivation to develop a method which does notrequire annotated training data, but is able to lever-age context for lexical normalisation.
Our approachfirst generates a list of candidate canonical lexicalforms, based on morphological and phonetic vari-ation.
Then, all candidates are ranked accordingto a list of features generated from noisy contextand similarity between ill-formed words and can-didates.
Our proposed cascaded method is shownto achieve state-of-the-art results on both SMS andTwitter data.Our contributions in this paper are as follows: (1)we conduct a pilot study on the OOV word distri-bution of Twitter and other text genres, and anal-yse different sources of non-standard orthography inTwitter; (2) we generate a text normalisation datasetbased on Twitter data; (3) we propose a novel nor-malisation approach that exploits dictionary lookup,word similarity and word context, without requir-ing annotated data; and (4) we demonstrate that ourmethod achieves state-of-the-art accuracy over bothSMS and Twitter data.2 Related workThe noisy channel model (Shannon, 1948) has tradi-tionally been the primary approach to tackling textnormalisation.
Suppose the ill-formed text is Tand its corresponding standard form is S, the ap-proach aims to find argmaxP (S|T ) by comput-ing argmaxP (T |S)P (S), in which P (S) is usu-ally a language model and P (T |S) is an error model.Brill and Moore (2000) characterise the error modelby computing the product of operation probabilitieson slice-by-slice string edits.
Toutanova and Moore(2002) improve the model by incorporating pronun-ciation information.
Choudhury et al (2007) modelthe word-level text generation process for SMS mes-sages, by considering graphemic/phonetic abbrevi-ations and unintentional typos as hidden Markovmodel (HMM) state transitions and emissions, re-spectively (Rabiner, 1989).
Cook and Stevenson(2009) expand the error model by introducing infer-ence from different erroneous formation processes,according to the sampled error distribution.
Whilethe noisy channel model is appropriate for text nor-malisation, P (T |S), which encodes the underlyingerror production process, is hard to approximateaccurately.
Additionally, these methods make thestrong assumption that a token ti ?
T only dependson si ?
S, ignoring the context around the token,which could be utilised to help in resolving ambigu-ity.Statistical machine translation (SMT) has beenproposed as a means of context-sensitive text nor-malisation, by treating the ill-formed text as thesource language, and the standard form as the targetlanguage.
For example, Aw et al (2006) propose aphrase-level SMT SMS normalisation method withbootstrapped phrase alignments.
SMT approachestend to suffer from a critical lack of training data,however.
It is labor intensive to construct an anno-tated corpus to sufficiently cover ill-formed wordsand context-appropriate corrections.
Furthermore,it is hard to harness SMT for the lexical normali-sation problem, as even if phrase-level re-orderingis suppressed by constraints on phrase segmenta-tion, word-level re-orderings within a phrase are stillprevalent.Some researchers have also formulated text nor-malisation as a speech recognition problem.
For ex-ample, Kobus et al (2008) firstly convert input texttokens into phonetic tokens and then restore them towords by phonetic dictionary lookup.
Beaufort et al(2010) use finite state methods to perform FrenchSMS normalisation, combining the advantages ofSMT and the noisy channel model.
Kaufmann andKalita (2010) exploit a machine translation approachwith a preprocessor for syntactic (rather than lexical)normalisation.Predominantly, however, these methods requirelarge-scale annotated training data, limiting theiradaptability to new domains or languages.
In con-trast, our proposed method doesn?t require annotateddata.
It builds on the work on SMS text normalisa-tion, and adapts it to Twitter data, exploiting multi-ple data sources for normalisation.369Figure 1: Out-of-vocabulary word distribution in English Gigaword (NYT), Twitter and SMS data3 Scoping Text Normalisation3.1 Task Definition of Lexical NormalisationWe define the task of text normalisation to be a map-ping from ?ill-formed?
OOV lexical items to theirstandard lexical forms, focusing exclusively on En-glish for the purposes of this paper.
We define thetask as follows:?
only OOV words are considered for normalisa-tion;?
normalisation must be to a single-token word,meaning that we would normalise smokin tosmoking, but not imo to in my opinion; a side-effect of this is to permit lower-register contrac-tions such as gonna as the canonical form ofgunna (given that going to is out of scope as anormalisation candidate, on the grounds of be-ing multi-token).Given this definition, our first step is to identifycandidate tokens for lexical normalisation, wherewe examine all tokens that consist of alphanumericcharacters, and categorise them into in-vocabulary(IV) and out-of-vocabulary (OOV) words, relative toa dictionary.
The OOV word definition is somewhatrough, because it includes neologisms and propernouns like hopeable or WikiLeaks which have notmade their way into the dictionary.
However, itgreatly simplifies the candidate identification task,at the cost of pushing complexity downstream tothe word detection task, in that we need to explic-itly distinguish between correct OOV words and ill-formed OOV words such as typos (e.g.
earthquak?earthquake?
), register-specific single-word abbre-viations (e.g.
lv ?love?
), and phonetic substitutions(e.g.
2morrow ?tomorrow?
).An immediate implication of our task definition isthat ill-formed words which happen to coincide withan IV word (e.g.
the misspelling of can?t as cant) areoutside the scope of this research.
We also considerthat deabbreviation largely falls outside the scope oftext normalisation, as abbreviations can be formedfreely in standard English.
Note that single-wordabbreviations such as govt ?government?
are verymuch within the scope of lexical normalisation, asthey are OOV and match to a single token in theirstandard lexical form.Throughout this paper, we use the GNU aspelldictionary (v0.60.6)2 to determine whether a tokenis OOV.
In tokenising the text, hyphenanted tokensand tokens containing apostrophes (e.g.
take-off andwon?t, resp.)
are treated as a single token.
Twit-ter mentions (e.g.
@twitter), hashtags (e.g.
#twitter)and urls (e.g.
twitter.com) are excluded from consid-eration for normalisation, but left in situ for contextmodelling purposes.
Dictionary lookup of Internetslang is performed relative to a dictionary of 5021items collected from the Internet.33.2 OOV Word Distribution and TypesTo get a sense of the relative need for lexical nor-malisation, we perform analysis of the distributionof OOV words in different text types.
In particular,we calculate the proportion of OOV tokens per mes-sage (or sentence, in the case of edited text), bin themessages according to the OOV token proportion,and plot the probability mass contained in each binfor a given text type.
The three corpora we compare2We remove all one character tokens, except a and I, andtreat RT as an IV word.3http://www.noslang.com370are the New York Times (NYT),4 SMS,5 and Twit-ter.6 The results are presented in Figure 1.Both SMS and Twitter have a relatively flat distri-bution, with Twitter having a particularly large tail:around 15% of tweets have 50% or more OOV to-kens.
This has implications for any context mod-elling, as we cannot rely on having only isolated oc-currences of OOV words.
In contrast, NYT shows amore Zipfian distribution, despite the large numberof proper names it contains.While this analysis confirms that Twitter and SMSare similar in being heavily laden with OOV tokens,it does not shed any light on the relative similarity inthe makeup of OOV tokens in each case.
To furtheranalyse the two data sources, we extracted the setof OOV terms found exclusively in SMS and Twit-ter, and analysed each.
Manual analysis of the twosets revealed that most OOV words found only inSMS were personal names.
The Twitter-specific set,on the other hand, contained a heterogeneous col-lection of ill-formed words and proper nouns.
Thissuggests that Twitter is a richer/noisier data source,and that text normalisation for Twitter needs to bemore nuanced than for SMS.To further analyse the ill-formed words in Twit-ter, we randomly selected 449 tweets and manu-ally analysed the sources of lexical variation, todetermine the phenomena that lexical normalisa-tion needs to deal with.
We identified 254 to-ken instances of lexical normalisation, and brokethem down into categories, as listed in Table 1.?Letter?
refers to instances where letters are miss-ing or there are extraneous letters, but the lexi-cal correspondence to the target word form is triv-ially accessible (e.g.
shuld ?should?).
?NumberSubstitution?
refers to instances of letter?numbersubstitution, where numbers have been substitutedfor phonetically-similar sequences of letters (e.g.
4?for?).
?Letter&Number?
refers to instances whichhave both extra/missing letters and number substitu-tion (e.g.
b4 ?before?).
?Slang?
refers to instances4Based on 44 million sentences from English Gigaword.5Based on 12.6 thousand SMS messages from How and Kan(2005) and Choudhury et al (2007).6Based on 1.37 million tweets collected from the Twitterstreaming API from Aug to Oct 2010, and filtered for mono-lingual English messages; see Section 5.1 for details of the lan-guage filtering methodology.Category RatioLetter&Number 2.36%Letter 72.44%Number Substitution 2.76%Slang 12.20%Other 10.24%Table 1: Ill-formed word distributionof Internet slang (e.g.
lol ?laugh out loud?
), as foundin a slang dictionary (see Section 3.1).
?Other?
isthe remainder of the instances, which is predomi-nantly made up of occurrences of spaces having be-ing deleted between words (e.g.
sucha ?such a?).
Ifa given instance belongs to multiple error categories(e.g.
?Letter&Number?
and it is also found in a slangdictionary), we classify it into the higher-occurringcategory in Table 1.From Table 1, it is clear that ?Letter?
accountsfor the majority of ill-formed words in Twitter, andthat most ill-formed words are based on morpho-phonemic variations.
This empirical finding assistsin shaping our strategy for lexical normalisation.4 Lexical normalisationOur proposed lexical normalisation strategy in-volves three general steps: (1) confusion set gen-eration, where we identify normalisation candidatesfor a given word; (2) ill-formed word identification,where we classify a word as being ill-formed or not,relative to its confusion set; and (3) candidate selec-tion, where we select the standard form for tokenswhich have been classified as being ill formed.
Inconfusion set generation, we generate a set of IVnormalisation candidates for each OOV word typebased on morphophonemic variation.
We call thisset the confusion set of that OOV word, and aim toinclude all feasible normalisation candidates for theword type in the confusion set.
The confusion can-didates are then filtered for each token occurrence ofa given OOV word, based on their local context fitwith a language model.4.1 Confusion Set GenerationRevisiting our manual analysis from Section 3.2,most ill-formed tokens in Twitter are morphophone-mically derived.
First, inspired by Kaufmann andKalita (2010), any repititions of more than 3 let-ters are reduced back to 3 letters (e.g.
cooool is re-371Criterion Recall AverageCandidatesTc ?
1 40.4% 24Tc ?
2 76.6% 240Tp = 0 55.4% 65Tp ?
1 83.4% 1248Tp ?
2 91.0% 9694Tc ?
2 ?
Tp ?
1 88.8% 1269Tc ?
2 ?
Tp ?
2 92.7% 9515Table 2: Recall and average number of candidates for dif-ferent confusion set generation strategiesduced to coool).
Second, IV words within a thresh-old Tc character edit distance of the given OOVword are calculated, as is widely used in spell check-ers.
Third, the double metaphone algorithm (Philips,2000) is used to decode the pronunciation of all IVwords, and IV words within a threshold Tp edit dis-tance of the given OOV word under phonemic tran-scription, are included in the confusion set; this al-lows us to capture OOV words such as earthquick?earthquake?.
In Table 2, we list the recall and av-erage size of the confusion set generated by the fi-nal two strategies with different threshold settings,based on our evaluation dataset (see Section 5.1).The recall for lexical edit distance with Tc ?
2 ismoderately high, but it is unable to detect the correctcandidate for about one quarter of words.
The com-bination of the lexical and phonemic strategies withTc ?
2?Tp ?
2 is more impressive, but the numberof candidates has also soared.
Note that increasingthe edit distance further in both cases leads to an ex-plosion in the average number of candidates, withserious computational implications for downstreamprocessing.
Thankfully, Tc ?
2?Tp ?
1 leads to anextra increment in recall to 88.8%, with only a slightincrease in the average number of candidates.
Basedon these results, we use Tc ?
2?Tp ?
1 as the basisfor confusion set generation.Examples of ill-formed words where we are un-able to generate the standard lexical form are clip-pings such as fav ?favourite?
and convo ?conversa-tion?.In addition to generating the confusion set, werank the candidates based on a trigram languagemodel trained over 1.5GB of clean Twitter data, i.e.tweets which consist of all IV words: despite theprevalence of OOV words in Twitter, the sheer vol-ume of the data means that it is relatively easy to col-lect large amounts of all-IV messages.
To train thelanguage model, we used SRILM (Stolcke, 2002)with the -<unk> option.
If we truncate the rankingto the top 10% of candidates, the recall drops backto 84% with a 90% reduction in candidates.4.2 Ill-formed Word DetectionThe next step is to detect whether a given OOVwordin context is actually an ill-formed word or not, rel-ative to its confusion set.
To the best of our knowl-edge, we are the first to target the task of ill-formedword detection in the context of short text messages,although related work exists for text with lower rel-ative occurrences of OOV words (Izumi et al, 2003;Sun et al, 2007).
Due to the noisiness of the data, itis impractical to use full-blown syntactic or seman-tic features.
The most direct source of evidence isIV words around an OOV word.
Inspired by workon labelled sequential pattern extraction (Sun et al,2007), we exploit large-scale edited corpus data toconstruct dependency-based features.First, we use the Stanford parser (Klein and Man-ning, 2003; de Marneffe et al, 2006) to extract de-pendencies from the NYT corpus (see Section 3.2).For example, from a sentence such as One obviousdifference is the way they look, we would extractdependencies such as rcmod(way-6,look-8)and nsubj(look-8,they-7).
We then trans-form the dependencies into relational features foreach OOV word.
Assuming that way were an OOVword, e.g., we would extract dependencies of theform (look,way,+2), indicating that look oc-curs 2 words after way.
We choose dependencies torepresent context because they are an effective wayof capturing key relationships between words, andsimilar features can easily be extracted from tweets.Note that we don?t record the dependency type here,because we have no intention of dependency parsingtext messages, due to their noisiness and the volumeof the data.
The counts of dependency forms arecombined together to derive a confidence score, andthe scored dependencies are stored in a dependencybank.Given the dependency-based features, a linearkernel SVM classifier (Fan et al, 2008) is trainedon clean Twitter data, i.e.
the subset of Twitter mes-sages without OOV words.
Each word is repre-372sented by its IV words within a context windowof three words to either side of the target word,together with their relative positions in the formof (word1,word2,position) tuples, and theirscore in the dependency bank.
These form the pos-itive training exemplars.
Negative exemplars areautomatically constructed by replacing target wordswith highly-ranked candidates from their confusionset.
Note that the classifier does not require any handannotation, as all training exemplars are constructedautomatically.To predict whether a given OOV word isill-formed, we form an exemplar for eachof its confusion candidates, and extract(word1,word2,position) features.
Ifall its candidates are predicted to be negative by themodel, we mark it as correct; otherwise, we treatit as ill-formed, and pass all candidates (not justpositively-classified candidates) on to the candidateselection step.
For example, given the messageway yu lookin shuld be a sin and the OOV wordlookin, we would generate context features for eachcandidate word such as (way,looking,-2),and classify each such candidate.In training, it is possible for the exact same fea-ture vector to occur as both positive and negative ex-emplars.
To prevent positive exemplars being con-taminated from the automatic generation, we re-move all negative instances in such cases.
The(word1,word2,position) features are sparseand sometimes lead to conservative results in ill-formed word detection.
That is, without valid fea-tures, the SVM classifier tends to label uncertaincases as correct rather than ill-formed words.
Thisis arguably the right approach to normalisation, inchoosing to under- rather than over-normalise incases of uncertainty.As the context for a target word often containsOOV words which don?t occur in the dependencybank, we expand the dependency features to includecontext tokens up to a phonemic edit distance of 1from context tokens in the dependency bank.
Inthis way, we generate dependency-based featuresfor context words such as seee ?see?
in (seee,flm, +2) (based on the target word flm in thecontext of flm to seee).
However, expanded depen-dency features may introduce noise, and we there-fore introduce expanded dependency weights wd ?
{0.0, 0.5, 1.0} to ameliorate the effects of noise: aweight of wd = 0.0 means no expansion, while 1.0means expanded dependencies are indistinguishablefrom non-expanded (strict match) dependencies.We separately introduce a threshold td ?
{1, 2, ..., 10} on the number of positive predictionsreturned by the detection classifier over the set ofnormalisation candidates for a given OOV token: thetoken is considered to be ill-formed iff td or morecandidates are positively classified, i.e.
predicted tobe correct candidates.4.3 Candidate SelectionFor OOV words which are predicted to be ill-formed, we select the most likely candidate from theconfusion set as the basis of normalisation.
The finalselection is based on the following features, in linewith previous work (Wong et al, 2006; Cook andStevenson, 2009).Lexical edit distance, phonemic edit distance,prefix substring, suffix substring, and the longestcommon subsequence (LCS) are exploited to cap-ture morphophonemic similarity.
Both lexical andphonemic edit distance (ED) are normalised by thereciprocal of exp(ED).
The prefix and suffix fea-tures are intended to capture the fact that leadingand trailing characters are frequently dropped fromwords, e.g.
in cases such as ish and talkin.
We cal-culate the ratio of the LCS over the maximum stringlength between ill-formed word and the candidate,since the ill-formed word can be either longer orshorter than (or the same size as) the standard form.For example, mve can be restored to either me ormove, depending on context.
We normalise these ra-tios following Cook and Stevenson (2009).For context inference, we employ both languagemodel- and dependency-based frequency features.Ranking by language model score is intuitively ap-pealing for candidate selection, but our trigrammodel is trained only on clean Twitter data and ill-formed words often don?t have sufficient context forthe language model to operate effectively, as in bt?but?
in say 2 sum1 bt nt gonna say ?say to some-one but not going to say?.
To consolidate the con-text modelling, we obtain dependencies from the de-pendency bank used in ill-formed word detection.Although text messages are of a different genre toedited newswire text, we assume they form similar373dependencies based on the common goal of gettingacross the message effectively.
The dependency fea-tures can be used in noisy contexts and are robustto the effects of other ill-formed words, as they donot rely on contiguity.
For example, uz ?use?
in idid #tt uz me and yu, dependencies can capture rela-tionships like aux(use-4, do-2), which is be-yond the capabilities of the language model due tothe hashtag being treated as a correct OOV word.5 Experiments5.1 Dataset and baselinesThe aim of our experiments is to compare the effec-tiveness of different methodologies over text mes-sages, based on two datasets: (1) an SMS corpus(Choudhury et al, 2007); and (2) a novel Twitterdataset developed as part of this research, based ona random sampling of 549 English tweets.
The En-glish tweets were annotated by three independentannotators.
All OOV words were pre-identified,and the annotators were requested to determine: (a)whether each OOV word was ill-formed or not; and(b) what the standard form was for ill-formed words,subject to the task definition outlined in Section 3.1.The total number of ill-formed words contained inthe SMS and Twitter datasets were 3849 and 1184,respectively.7The language filtering of Twitter to automaticallyidentify English tweets was based on the languageidentification method of Baldwin and Lui (2010),using the EuroGOV dataset as training data, a mixedunigram/bigram/trigram byte feature representation,and a skew divergence nearest prototype classifier.We reimplemented the state-of-art noisy channelmodel of Cook and Stevenson (2009) and SMT ap-proach of Aw et al (2006) as benchmark meth-ods.
We implement the SMT approach in Moses(Koehn et al, 2007), with synthetic training andtuning data of 90,000 and 1000 sentence pairs, re-spectively.
This data is randomly sampled from the1.5GB of clean Twitter data, and errors are gener-ated according to distribution of SMS corpus.
The10-fold cross-validated BLEU score (Papineni et al,2002) over this data is 0.81.7The Twitter dataset is available at http://www.csse.unimelb.edu.au/research/lt/resources/lexnorm/.In addition to comparing our method with com-petitor methods, we also study the contribution ofdifferent feature groups.
We separately compare dic-tionary lookup over our Internet slang dictionary,the contextual feature model, and the word similar-ity feature model, as well as combinations of thesethree.5.2 Evaluation metricsThe evaluation of lexical normalisation consists oftwo stages (Hirst and Budanitsky, 2005): (1) ill-formed word detection, and (2) candidate selection.In terms of detection, we want to make sense of howwell the system can identify ill-formed words andleave correct OOV words untouched.
This step iscrucial to further normalisation, because if correctOOV words are identified as ill-formed, the candi-date selection step can never be correct.
Conversely,if an ill-formed word is predicted to be correct, thecandidate selection will have no chance to normaliseit.We evaluate detection performance by token-levelprecision, recall and F-score (?
= 1).
Previous workover the SMS corpus has assumed perfect ill-formedword detection and focused only on the candidateselection step, so we evaluate ill-formed word de-tection for the Twitter data only.For candidate selection, we once again evalu-ate using token-level precision, recall and F-score.Additionally, we evaluate using the BLEU scoreover the normalised form of each message, as theSMT method can lead to perturbations of the tokenstream, vexing standard precision, recall and F-scoreevaluation.5.3 Results and AnalysisFirst, we test the impact of the wd and td valueson ill-formed word detection effectiveness, based ondependencies from either the Spinn3r blog corpus(Blog: Burton et al (2009)) or NYT.
The results forprecision, recall and F-score are presented in Fig-ure 2.Some conclusions can be drawn from the graphs.First, higher detection threshold values (td) give bet-ter precision but lower recall.
Generally, as td israised from 1 to 10, the precision improves slightlybut recall drops dramatically, with the net effect thatthe F-score decreases monotonically.
Thus, we use a374Figure 2: Ill-formed word detection precision, recall andF-scoresmaller threshold, i.e.
td = 1.
Second, there are dif-ferences between the two corpora, with dependen-cies from the Blog corpus producing slightly lowerprecision but higher recall, compared with the NYTcorpus.
The lower precision for the Blog corpus ap-pears to be due to the text not being as clean as NYT,introducing parser errors.
Nevertheless, the differ-ence in F-score between the two corpora is insignif-icant.
Third, we obtain the best results, especiallyin terms of precision, for wd = 0.5, i.e.
with ex-panded dependencies, but penalised relative to non-expanded dependencies.Overall, the best F-score is 71.2%, with a preci-sion of 61.1% and recall of 85.3%, obtained overthe Blog corpus with td = 1 and wd = 0.5.
Clearlythere is significant room for immprovements in theseresults.
We leave the improvement of ill-formedword detection for future work, and perform eval-uation of candidate selection for Twitter assumingperfect ill-formed word detection, as for the SMSdata.From Table 3, we see that the general perfor-mance of our proposed method on Twitter is betterthan that on SMS.
To better understand this trend,we examined the annotations in the SMS corpus, andfound them to be looser than ours, because they havedifferent task specifications than our lexical normal-isation.
In our annotation, the annotators only nor-malised ill-formed word if they had high confidenceof how to normalise, as with talkin ?talking?.
Forill-formed words where they couldn?t be certain ofthe standard form, the tokens were left untouched.However, in the SMS corpus, annotations such assammis ?same?
are also included.
This leads to aperformance drop for our method over the SMS cor-pus.The noisy channel method of Cook and Stevenson(2009) shares similar features with word similarity(?WS?
), However, when word similarity and con-text support are combined (?WS+CS?
), our methodoutperforms the noisy channel method by about 7%and 12% in F-score over SMS and Twitter corpora,respectively.
This can be explained as follows.
First,the Cook and Stevenson (2009) method is type-based, so all token instances of a given ill-formedword will be normalised identically.
In the Twit-ter data, however, the same word can be normaliseddifferently depending on context, e.g.
hw ?how?
inso hw many time remaining so I can calculate it?vs.
hw ?homework?
in I need to finish my hw first.Second, the noisy channel method was developedspecifically for SMS normalisation, in which clip-ping is the most prevalent form of lexical variation,while in the Twitter data, we commonly have in-stances of word lengthening for emphasis, such asmoviiie ?movie?.
Having said this, our method issuperior to the noisy channel method over both theSMS and Twitter data.The SMT approach is relatively stable on the twodatasets, but well below the performance of ourmethod.
This is due to the limitations of the trainingdata: we obtain the ill-formed words and their stan-dard forms from the SMS corpus, but the ill-formedwords in the SMS corpus are not sufficient to coverthose in the Twitter data (and we don?t have suffi-cient Twitter data to train the SMT method directly).Thus, novel ill-formed words are missed in normal-isation.
This shows the shortcoming of superviseddata-driven approaches that require annotated datato cover all possibilities of ill-formed words in Twit-ter.The dictionary lookup method (?DL?)
unsurpris-ingly achieves the best precision, but the recallon Twitter is not competitive.
Consequently, theTwitter normalisation cannot be tackled with dictio-nary lookup alone, although it is an effective pre-processing strategy when combined with more ro-375Dataset Evaluation NC MT DL WS CS WS+CS DL+WS+CSSMSPrecision 0.465 ?
0.927 0.521 0.116 0.532 0.756Recall 0.464 ?
0.597 0.520 0.116 0.531 0.754F-score 0.464 ?
0.726 0.520 0.116 0.531 0.755BLEU 0.746 0.700 0.801 0.764 0.612 0.772 0.876TwitterPrecision 0.452 ?
0.961 0.551 0.194 0.571 0.753Recall 0.452 ?
0.460 0.551 0.194 0.571 0.753F-score 0.452 ?
0.622 0.551 0.194 0.571 0.753BLEU 0.857 0.728 0.861 0.878 0.797 0.884 0.934Table 3: Candidate selection effectiveness on different datasets (NC = noisy channel model (Cook and Stevenson,2009); MT = SMT (Aw et al, 2006); DL = dictionary lookup; WS = word similarity; CS = context support)bust techniques such as our proposed method, andeffective at capturing common abbreviations such asgf ?girlfriend?.Of the component methods proposed in this re-search, word similarity (?WS?)
achieves higher pre-cision and recall than context support (?CS?
), sig-nifying that many of the ill-formed words emanatefrom morphophonemic variations.
However, whencombined with word similarity features, contextsupport improves over the basic method at a level ofstatistical significance (based on randomised estima-tion, p < 0.05: Yeh (2000)), indicating the comple-mentarity of the two methods, especially on Twitterdata.
The best F-score is achieved when combin-ing dictionary lookup, word similarity and contextsupport (?DL+WS+CS?
), in which ill-formed wordsare first looked up in the slang dictionary, and onlyif no match is found do we apply our normalisationmethod.We found several limitations in our proposed ap-proach by analysing the output of our method.
First,not all ill-formed words offer useful context.
Somehighly noisy tweets contain almost all misspellingsand unique symbols, and thus no context featurescan be extracted.
This also explains why ?CS?
fea-tures often fail.
For such cases, the method falls backto context-independent normalisation.
We foundthat only 32.6% ill-formed words have all IV wordsin their context windows.
Moreover, the IV wordsmay not occur in the dependency bank, further de-creasing the effectiveness of context support fea-tures.
Second, the different features are linearlycombined, where a weighted combination is likelyto give better results, although it also requires a cer-tain amount of well-sampled annotations for tuning.6 Conclusion and Future WorkIn this paper, we have proposed the task of lexi-cal normalisation for short text messages, as foundin Twitter and SMS data.
We found that most ill-formed words are based on morphophonemic varia-tion and proposed a cascaded method to detect andnormalise ill-formed words.
Our ill-formed worddetector requires no explicit annotations, and thedependency-based features were shown to be some-what effective, however, there was still a lot ofroom for improvement at ill-formed word detection.In normalisation, we compared our method withtwo benchmark methods from the literature, andachieved that highest F-score and BLEU score byintegrating dictionary lookup, word similarity andcontext support modelling.In future work, we propose to pursue a number ofdirections.
First, we plan to improve our ill-formedword detection classifier by introducing an OOVword whitelist.
Furthermore, we intend to allevi-ate noisy contexts with a bootstrapping approach, inwhich ill-formed words with high confidence and noambiguity will be replaced by their standard forms,and fed into the normalisation model as new trainingdata.AcknowledgementsNICTA is funded by the Australian government as rep-resented by Department of Broadband, Communicationand Digital Economy, and the Australian Research Coun-cil through the ICT centre of Excellence programme.ReferencesAiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for SMS text normal-ization.
In Proceedings of the 21st International Con-376ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 33?40, Sydney, Australia.Timothy Baldwin and Marco Lui.
2010.
Language iden-tification: The long and the short of the matter.
InHLT ?10: Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages229?237, Los Angeles, USA.Richard Beaufort, Sophie Roekhaut, Louise-Ame?lieCougnon, and Ce?drick Fairon.
2010.
A hybridrule/model-based finite-state framework for normaliz-ing SMS messages.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 770?779, Uppsala, Sweden.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.
InACL ?00: Proceedings of the 38th Annual Meetingon Association for Computational Linguistics, pages286?293, Hong Kong.Kevin Burton, Akshay Java, and Ian Soboroff.
2009.
TheICWSM 2009 Spinn3r Dataset.
In Proceedings of theThird Annual Conference on Weblogs and Social Me-dia, San Jose, USA.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and modeling of the structure oftexting language.
International Journal on DocumentAnalysis and Recognition, 10:157?174.Paul Cook and Suzanne Stevenson.
2009.
An unsu-pervised model for text message normalization.
InCALC ?09: Proceedings of the Workshop on Computa-tional Approaches to Linguistic Creativity, pages 71?78, Boulder, USA.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC 2006),Genoa, Italy.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Graeme Hirst and Alexander Budanitsky.
2005.
Cor-recting real-word spelling errors by restoring lexicalcohesion.
Natural Language Engineering, 11:87?111.Yijue How and Min-Yen Kan. 2005.
Optimizing pre-dictive text entry for short message service on mobilephones.
In Human Computer Interfaces International(HCII 05), Las Vegas, USA.Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, ThepchaiSupnithi, and Hitoshi Isahara.
2003.
Automatic er-ror detection in the Japanese learners?
English spokendata.
In Proceedings of the 41st Annual Meeting onAssociation for Computational Linguistics - Volume 2,pages 145?148, Sapporo, Japan.Joseph Kaufmann and Jugal Kalita.
2010.
Syntactic nor-malization of Twitter messages.
In International Con-ference on Natural Language Processing, Kharagpur,India.Dan Klein and Christopher D.Manning.
2003.
Fast exactinference with a factored model for natural languageparsing.
In Advances in Neural Information Process-ing Systems 15 (NIPS 2002), pages 3?10, Whistler,Canada.Catherine Kobus, Franois Yvon, and Graldine Damnati.2008.
Transcrire les SMS comme on reconnat la pa-role.
In Actes de la Confrence sur le Traitement Au-tomatique des Langues (TALN?08), pages 128?138.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, pages 177?180, Prague, Czech Republic.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, pages 311?318, Philadelphia, USA.James L. Peterson.
1980.
Computer programs for de-tecting and correcting spelling errors.
Commun.
ACM,23:676?687, December.Lawrence Philips.
2000.
The double metaphone searchalgorithm.
C/C++ Users Journal, 18:38?43.Lawrence R. Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?286.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of Twitter conversations.
In HLT?10: Human Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages172?180, Los Angeles, USA.Claude Elwood Shannon.
1948.
A mathematical the-ory of communication.
Bell System Technical Journal,27:379?423, 623?656.Richard Sproat, Alan W. Black, Stanley Chen, ShankarKumar, Mari Ostendorf, and Christopher Richards.2001.
Normalization of non-standard words.
Com-puter Speech and Language, 15(3):287 ?
333.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In International Conference on Spo-377ken Language Processing, pages 901?904, Denver,USA.Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, andMing Zhou.
2007.
Mining sequential patterns and treepatterns to detect erroneous sentences.
In Proceedingsof the 45th Annual Meeting of the Association of Com-putational Linguistics, pages 81?88, Prague, CzechRepublic.Kristina Toutanova and Robert C. Moore.
2002.
Pro-nunciation modeling for improved spelling correction.In Proceedings of the 40th Annual Meeting on Associ-ation for Computational Linguistics, ACL ?02, pages144?151, Philadelphia, USA.Twitter.
2010.
Big goals, big game, big records.http://blog.twitter.com/2010/06/big-goals-big-game-big-records.html.Retrieved 4 August 2010.Wilson Wong, Wei Liu, and Mohammed Bennamoun.2006.
Integrated scoring for spelling error correction,abbreviation expansion and case restoration in dirtytext.
In Proceedings of the Fifth Australasian Con-ference on Data Mining and Analytics, pages 83?89,Sydney, Australia.Alexander Yeh.
2000.
More accurate tests for the statis-tical significance of result differences.
In Proceedingsof the 18th Conference on Computational Linguistics -Volume 2, COLING ?00, pages 947?953, Saarbru?cken,Germany.378
