Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 36?37,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Constraint Programming Approach to Probabilistic Syntactic ProcessingIrene Langkilde-GearyIndependent ConsultantSouth Jordan, UT USAi.l.geary@gmail.com1 IntroductionInteger linear programming (ILP) is a frameworkfor solving combinatorial problems with linear con-straints of the form y = c1x1 + c2x2 + ... + cnxnwhere the variables (ie., y and xis) take on only in-teger values.
ILP is a special case of a larger fam-ily of contraint-based solving techniques in whichvariables may take on additional types of values (eg.discrete, symbolic, real, set, and structured) or in-volve additional kinds of constraints (eg.
logicaland non-linear, such as x ?
y ?
z and y = cxn).Constraint-based problem solving approaches offera more natural way of modeling many kinds of real-world problems.
Furthermore, the declarative natureof constraint-based approaches makes them versatilesince the order in which the variables are solved isnot predetermined.
The same program can thus bereused for solving different subsets of the problem?svariables.
Additionally, in some cases, constraint-based approaches can solve problems more effi-ciently or accurately than alternative approaches.Constraint Programming (CP) is a field of re-search that develops algorithms and tools forconstraint-based problem solving.
This abstract de-scribes work-in-progress on a project to developa CP-based general-purpose broad-coverage prob-abilistic syntactic language processing system forEnglish.
Because of its declarative nature, the sys-tem can be used for both parsing and realization aswell as their subtasks (such as tagging, chunk pars-ing, lexical choice, or word ordering) or hybridiza-tions (like text-to-text generation).
We expect thistool to be useful for a wide range of applicationsfrom information extraction to machine translationto human-computer dialog.
An ambitious projectsuch as this poses a number of questions and difficultchallenges, including: a) how to declaratively repre-sent the syntactic structure of sentences, b) how tointegrate the processing of hard constraints with soft(probabilistic) ones, c) how to overcome problemsof intractibility associated with large problems andrich representations in learning, inference, as wellas search.2 Related WorkDeclarative and constraint-based representationsand computation mechanisms have been the subjectof much research in the fields of both Linguisticsand Computer Science over the last 30-40 years,at times motivating each other but also sometimesdeveloping independently.
Although there is quitea large literature on constraint-based processing inNLP, the notion of a constraint and the methods forprocessing them vary significantly from that in CP.See (Duchier et al, 1998; Piwek and van Deemter,2006; Blache, 2000).
The CP approach has beendesigned for a broader ranger of applications andrests on a stronger, more general theoretical foun-dation.
It coherently integrates a variety of solvingtechniques whereas theoretical linguistic formalismshave traditionally used only a single kind of con-straint solver, namely unification.
In comparison,the 2009 ILPNLP workshop focuses on NLP pro-cessing using solely integer linear constraints.363 MethodologyThree key elements of our approach are its syntacticrepresentation, confidence-based beam search, and anovel on-demand learning and inference algorithm.The last is used to calculate probability-based fea-ture costs and the confidences used to heuristicallyguide the search for the best solution.
A descriptionof the flat featurized dependency-style syntactic rep-resentation we use is available in (Langkilde-Gearyand Betteridge, 2006), which describes how the en-tire Penn Treebank (Marcus et al, 1993) was con-verted to this representation.
The representation hasbeen designed to offer finer-grained declarativenessthan other existing representations.Our confidence-based search heuristic evaluatesthe conditional likelihood of undetermined outputvariables (ie., word features) at each step of searchand heuristically selects the case of the mostly likelyvariable/value pair as the next (or only one) to ex-plore.
The likelihood is contextualized by the in-put variables and any output variables which havealready been explored and tentatively solved.
Al-though one theoretical advantage of CP (and ILP)is the ability to calculate an overall optimal solu-tion through search, we unexpectedly found thatour confidence-based heuristic led to the first inter-mediate solution typically being the optimal.
Thisallowed us to simplify the search methodology toa one-best or threshold-based beam search withoutany significant loss in accuracy.
The result is dra-matically improved scalability.We use the concurrent CP language Mozart/Ozto implement our approach.
We previously im-plemented an exploratory prototype that used rawfrequencies instead of smoothed probabilities forthe feature costs and search heuristic confidences.
(Langkilde-Geary, 2005; Langkilde-Geary, 2007).The lack of smoothing severely limited the applica-bility of the prototype.
We are currently finishingdevelopment of the before-mentioned on-demandlearning algorithm which will overcome that chal-lenge and allow us to evaluate our approach?s ac-curacy and efficiency on a variety of NLP tasks oncommon test sets.
Informal preliminary results onthe much-studied subtask of part-of-speech taggingindicate that our method outperforms a Naive Bayes-based baseline in terms of accuracy and within 2%of state-of-the-art single-classifier methods, whilerunning in linear time with respect to the number ofoutput variables or word tokens.
We are not awareof any other approach that achieves this level of ac-curacy in comparable algorithmic time.4 ConclusionThe versatility and potential scalability of our ap-proach are its most noteworthy aspects.
We ex-pect it to be able to handle not only a wider vari-ety of NLP tasks than existing approaches but alsoto tackle harder tasks that have been intractible be-fore now.
Although ILP has the same theoreticalpower as CP for efficiently solving problems, ourapproach takes advantage of several capabilities thatCP offers that ILP doesn?t, including modeling withnot only linear constraints but also logical, set-basedand other kinds of constraints; customized searchmethodology with dynamically computed costs, andconditionally applied constraints, among others.ReferencesP.
Blache.
2000.
Constraints, linguistic theories and nat-ural language processing.
Natural Language Process-ing, 1835.D.
Duchier, C. Gardent, and J. Niehren.
1998.
Concur-rent constraint programming in oz for natural languageprocessing.
Technical report, Universitt des Saarlan-des.I.
Langkilde-Geary and J. Betteridge.
2006.
A factoredfunctional dependency transformation of the englishpenn treebank for probabilistic surface generation.
InProc.
LREC.I.
Langkilde-Geary.
2005.
An exploratory applica-tion of constraint optimization in mozart to probabilis-tic natural language processing.
In H. Christiansen,P.
Skadhauge, and J. Villadsen, editors, Proceedings ofthe International Workshop on Constraint Solving andLanguage Processing (CSLP), volume 3438.
Springer-Verlag LNAI.I.
Langkilde-Geary.
2007.
Declarative syntactic process-ing of natural language using concurrent constraintprogramming and probabilistic dependency modeling.In Proc.
UCNLG.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of english: the Penntreebank.
Computational Linguistics, 19(2).P.
Piwek and K. van Deemter.
2006.
Constraint-basednatural language generation: A survey.
Technical re-port, The Open University.37
