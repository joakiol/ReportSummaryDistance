Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 110?119,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsStability and Accuracy in Incremental Speech RecognitionEthan O.
Selfridge?, Iker Arizmendi?, Peter A.
Heeman?, and Jason D.
Williams??
Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR?AT&T Labs ?
Research, Shannon Laboratory, Florham Park, NJ{selfridg,heemanp}@ohsu.edu {iker,jdw}@research.att.comAbstractConventional speech recognition ap-proaches usually wait until the userhas finished talking before returning arecognition hypothesis.
This results inspoken dialogue systems that are unableto react while the user is still speaking.Incremental Speech Recognition (ISR),where partial phrase results are returnedduring user speech, has been used tocreate more reactive systems.
However,ISR output is unstable and so prone torevision as more speech is decoded.
Thispaper tackles the problem of stabilityin ISR.
We first present a method thatincreases the stability and accuracy ofISR output, without adding delay.
Giventhat some revisions are unavoidable,we next present a pair of methods forpredicting the stability and accuracy ofISR results.
Taken together, we believethese approaches give ISR more utility forreal spoken dialogue systems.1 IntroductionIncremental Speech Recognition (ISR) enables aspoken dialogue system (SDS) to react quickerthan when using conventional speech recogni-tion approaches.
Where conventional methodsonly return a result after some indication of usercompletion (for example, a short period of si-lence), ISR returns partial phrase results whilethe user is still speaking.
Having access to a real-time stream of user speech enables more naturalbehavior by a SDS, and is a foundation for cre-ating systems which take a more active role inconversations.Research by Fink et al(1998) and Skantze& Schlangen (2009), among others, has demon-strated the efficacy of ISR but has also drawnattention to a significant obstacle to widespreaduse: partial phrase results are generally unsta-ble and so, as more speech is decoded, are proneto revision.
For example, the ISR component ina bus information SDS may return the partial?leaving from Hills?, where ?Hills?
is a neigh-borhood name.
It may then return the revi-sion ?leaving from Pittsburgh?, which the sys-tem must handle gracefully.
Given this propen-sity to revise, a Stability Measure (SM) ?
like-lihood of a partial result remaining unchangedcompared to the final result ?
is necessary foroptimal incremental system behavior.
Further-more, since a stable partial may still be inaccu-rate, a Confidence Measure (CM) ?
likelihoodof partial correctness ?
is also necessary.Effective ISR enables systems to participate inmore dynamic turn-taking.
For instance, thesetwo measures would enable an SDS to identifyinaccurate recognition results while the user isstill speaking.
The SDS could then interruptand prompt the user to start again.
On theother hand, ISR allows systems to handle pausesgracefully.
If the SDS recognizes that an utter-ance is incomplete (though stable and accurate),it could give the user more time to speak beforereacting.We present two contributions specific to theuse of ISR.
First, we characterize three ap-proaches to ISR which make different trade-offsbetween stability and the number of partialsgenerated.
We then present a novel hybrid ap-proach that combines their strengths to increase110stability without adding latency.
However, evenwith this method, some partial results are stilllater revised.
The second contribution of thepaper is to present a pair of methods which pre-dict the stability and accuracy of each partialresult.
These two measures are designed for usein concert by dialogue systems, which must de-cide whether to act on each partial result in realtime.2 Background and Related WorkWe now describe modern speech recognitionmethodology, the production of partial phraseresults, and the advantages and deficiencies ofISR.
In this we seek only to provide a topicalfoundation, and not a comprehensive review.Most modern speech recognition engines useHidden-Markov Models and the Viterbi algo-rithm to decode words from audio.
Decod-ing employs three models: an acoustic model,which assigns probabilities to speech audio givena phone; a lexicon, which specifies phone se-quences for a word; and a language model, whichspecifies the probability of a word sequence.
Theaim of the decoding process is to find the N mostprobable word sequences given the audio spokenand these three models.Two useful but different forms of languagemodels are commonly used in spoken dialoguesystems.
A Rule-based Language Model (RLM)specifies a list of valid sentences which may berecognized, usually via expansion rules.
By con-trast, a Statistical Language Model (SLM) spec-ifies a vocabulary of words, allowing arbitrarysentences to be formed.
Both models specifyprobabilities over their respective sets ?
RLMsvia whole-sentence probabilities, and SLMs viaprobabilities of short word sequences called N-grams.
In an SLM, special word symbols areused to represent the beginning and end of thephrase, so the probability of beginning or endingphrases with words can be modeled.As speech frames are received, the recognizerbuilds up a lattice which compactly describes theprobable sequences of words decoded from theaudio.
In conventional turn-based speech recog-nition, decoding continues until the user finishesspeaking.
Once the user has finished, the enginesearches the lattice for the most probable wordsequence and returns this to the dialogue man-ager.
By contrast, in ISR the engine inspectsthe lattice as it is being built, and returns partialresults to the dialogue manager as they becomeavailable.
A key issue for ISR is that partialresults may later be revised, because as morespeech is received and the lattice is extended, adifferent path may become the most probable.In other words, partial results are unstable inthe sense that they may later be revised.
Notethat stability is not the same as accuracy: a par-tial result may be accurate (correct so far) butunstable, because it is later revised.
Similarly, astable result may not be accurate.In the literature, ISR has been proposed fordialogue systems to enable them to engage inmore natural, human-like interactions.
Stud-ies have shown that incremental systems reactfaster than non-incremental ones, and are well-liked by users because of their naturalness (Aistet al, 2007; Skantze and Schlangen, 2009).
Aistet al (2007) found that incremental speechrecognition yielded 20% faster task completion.Moreover, adding ISR improved users?
satisfac-tion with the interaction; the authors attributedthis improvement to ?naturalness?
: ?incremen-tal systems are more like human-human con-versation than their non-incremental counter-parts.?
Skantze & Schlangen (2009) observed asimilar trend, finding that an incremental sys-tem was ?clearly preferred?
since it ?was ex-perienced as more pleasant and human-like?,though it did not actually outperform the non-incremental system in a number dictation task.Some recent work has focused on incremen-tal natural language understanding (NLU).
De-Vault et al (2009) showed that when using arelatively small number of semantic possibili-ties the correct interpretation could be predictedby early incremental results.
Schlangen et al(2009) demonstrated that an incremental refer-ence resolver could identify the correct referenceout of 12 more than 50% of the time.
Thistype of NLU can use context and other infor-mation to be somewhat resilient to errors, andword recognition inaccuracies may not yield a111change in understanding.
In this paper we focuson improving accuracy and stability at the wordlevel; we belief that improvements at the wordlevel are likely to improve performance at theunderstanding level, although we do not evalu-ate this here.A number of researchers have described meth-ods for evaluating and improving the stability ofISR results (Baumann et al, 2009; Fink et al,1998).
Baumann, Atterer, & Schlangen spokedirectly to stability by comparing partial phraseresults against the ?final hypothesis producedby the ASR?.
They show that increasing theamount of ?right context?
?
the amount ofspeech after the end of the putative partial result?
increases the stability of the partials.
Fink etal.
(1998) also used a right context delay to de-crease the word error rate of ISR results.A key limitation of these past efforts to im-prove stability is that adding right context nec-essarily incurs delay, which degrades responsive-ness and erodes the overall benefits of ISR.
Fur-thermore, past work has not addressed the prob-lem of identifying which partials are likely to berevised.
In this paper, we tackle both of theseproblems.
We first present a method for im-proving stability by considering features of thelattice itself, without incurring the delay asso-ciated with adding right context.
Additionally,since some partials will still be revised, we thenpropose a method of scoring the stability of par-tial speech recognition results.3 Three approaches to ISRWe now describe three approaches to ISR: Ba-sic, Terminal, and Immortal.
Basic ISR simplyreturns the most likely word sequence observedafter some number of speech frames has been de-coded (in our case every 3 frames or 30ms).
Thisis the least restrictive approach, and we believeis the method used by recent ISR research.Terminal ISR, a more restrictive approach,finds a partial result if the most likely paththrough the (partially-decoded) lattice ends ata terminal node in the language model.
The in-tuition is that if a partial result finishes a com-plete phrase expected by the language model,it is more likely to be stable.
The meaning ofterminal is slightly different for rule-based lan-guage models (RLMs) and statistical languagemodels (SLMs).
For a rule-based grammar,the terminal node is simply one that ends avalid phrase (?Pittsburgh?
in ?leaving from Pitts-burgh?).
For an SLM, a terminal node indicatesthat the most likely successor state is the spe-cial end-of-sentence symbol.
In other words, inan SLM Terminal partial result, the languagemodel assigns the highest probability to endingthe phrase.A third method, Immortal ISR, is the mostrestrictive method (Spohrer et al, 1980).
If allpaths of the lattice come together into a node?
called an immortal node ?
then the latticestructure before that node will be unchanged byany subsequent decoding.
This structure guar-antees that the best word sequence prior to animmortal node is stable.
Immortal ISR operatesidentically for both RLMs and SLMs.1To compare these approaches we evaluatetheir performance.
Utterances were extractedfrom real calls to the Carnegie Mellon ?LetsGo!?
bus information system for Pittsburgh,USA (Raux et al, 2005; Parent and Eskenazi,2009).
We chose this domain because this cor-pus is publicly available, and this domain hasrecently been used as a test bed for dialoguesystems (Black et al , 2010).
The AT&T WAT-SON speech recognition engine was used, modi-fied to output partials as described above (Goffinet al, 2005).
We tested these three approachesto ISR on three different recognition tasks.
Thefirst two tasks used rule-based language models(RLM), and the third used a statistical languagemodel (SLM).The two rule-based language models were de-veloped for AT&T ?Let?s Go?
dialogue sys-tem, prior to its deployment (Williams et al, 2010).
The first RLM (RLM1) consisted1The choice of search beam size affects both accuracyand the number of immortal nodes produced: a smallerbeams yields a sparser lattice with more immortal nodesand lower accuracy; a larger beam yields a richer latticewith fewer immortal nodes and higher accuracy.
In thiswork we used our recognizer?s default beam size, whichallows recognition to run in less than real time and yieldsnear-asymptotic accuracy for all experiments.112of street and neighborhood names, built fromthe bus timetable database.
The second RLM(RLM2) consisted of just neighborhood names.Utterances to test RLM1 and RLM2 were se-lected from the corpus provided by CarnegieMellon to match the expected distribution ofspeech at the dialogue states where RLM1 andRLM2 would be used.
RLM1 was evaluated ona set of 7722 utterances, and RLM2 on 5411 ut-terances.
To simulate realistic use, both RLMtest sets were built so that 80% of utterancesare in-grammar, and 20% are out-of-grammar.The SLM was a 3-gram trained on a set of 140Kutterances, and is tested on a set of 42620 ut-terances.In past work, Raux et al (2005) report worderror rates (WERs) of 60-68% on data from thesame dialogue system, though on a different setof utterances.
By comparison, our SLM yieldsa WER of 35%, which gives us some confidencethat our overall recognition accuracy is compet-itive, and that our results are relevant.Table 1 provides a few statistics of the LMsand test sets, including whole-utterance accu-racy, computed using an exact string match.Results are analyzed in two groups: All, whereall of the utterances are analyzed, and Multi-Word (MW), where only utterances whose tran-scribed speech (what was actually said) hasmore than one word.
Intuitively, these utter-ances are where ISR would be most effective.That said, ISR is beneficial for both short andlong utterances ?
for example, ISR systemscan react faster to users regardless of utterancelength.ISR was run using each of the three ap-proaches (Basic, Terminal, Immortal) in each ofthe three configurations (RLM1, RLM2, SLM).The mean number of partials per utterance isshown in Table 2.
For all ISR methods, the moreflexible SLM produces more partials than theRLMs.
Also as expected, multi-word utterancesproduce substantially more partials per utter-ance than when looking at the entire utteranceset.
The Basic approach produces nearly dou-ble the number of partials than Terminal ISRdoes, and Immortal ISR production highlightsits primary weakness: in many utterances, noTable 1: Statistics for Recognition Tasks.
In all ta-bles, All refers to all utterances in a test set, andMW refers to the subset of multi-word utterances ina test set.RLM1 RLM2 SLMNum.
Utts All 7722 5411 42620Num.
Utts MW 3213 1748 20396Words/Utt All 1.7 1.5 2.3Words/Utt MW 2.8 2.6 3.8Utt.
Acc.
All.
50 % 60 % 62 %Utt.
Acc.
MW 53 % 56 % 44 %immortal nodes are found.
Given this however,immortal node occurrence is directly related tothe number of words, as indicted by the greaternumber of immortal partials in multi-word ut-terances.Stability is assessed by comparing the partialto the final recognition result.
For simplicity, werestrict our analysis to 1-Best hypotheses.
If thepartial 1-Best hypothesis is a prefix (or full ex-act match) of the final 1-Best hypothesis then itis considered stable.
For instance, if the partial1-Best hypothesis is ?leaving from Forbes?
thenit would be stable if the final 1-Best is ?leavingfrom Forbes?
or ?leaving from Forbes and Mur-ray?
but not if it is ?from Forbes and Murray?
or?leaving?.
Accuracy is assessed similarly exceptthat the transcribed reference is used instead ofthe final recognition result.We report stability and accuracy in Table 3.Immortal partials are excluded from stabilitysince they are guaranteed to be stable.
The firstfour rows report stability, and the second sixreport accuracy.
The results show that Termi-nal Partials are relatively unstable, with 23%-Table 2: Average Number of Partials per utteranceISR Group RLM1 RLM2 SLMBasic All 12.0 9.9 11.6MW 14.6 12.3 29.7Terminal All 5.4 3.3 6.2MW 6.4 4.1 8.8Immortal All 0.22 0.32 0.55MW 0.42 0.67 0.63113Table 3: Stability and Accuracy PercentagesISR Group RLM1 RLM2 SLMStabilityBasic All 10 % 11 % 7 %MW 14 % 15 % 9 %Terminal All 23 % 31 % 37 %MW 20 % 28 % 36 %AccuracyBasic All 9 % 1 % 5 %MW 11 % 13 % 6 %Terminal All 13 % 21 % 24 %MW 12 % 17 % 21 %Immortal All 91 % 93 % 55 %MW 90 % 90 % 56 %37% of partials being stable, and that their sta-bility drops off when looking at multi-word ut-terances.
SLM stability seems to be somewhathigher than that of the RLM.
Basic partialsare even more unstable (about 10% of partialsare stable), with extremely low stability for theSLM.
Unlike Terminal ISR, their stability growswhen only multi-word utterances are analyzed,though the maximum is still quite low.The results also show that partials are alwaysless accurate than they are stable, indicatingthat not all stable partials are accurate.
Immor-tal partials are rare, but when they are found,they are much more accurate than Terminal orBasic partials.
The RLM accuracy is very high,and we suspect that immortal nodes are corre-lated with utterances which are easier to recog-nize.
Terminal ISR is far more accurate thanBasic ISR for all of the utterances, but its im-provement declines for multi-word RLMs.We have shown three types of ISR: Basic, Ter-minal and Immortal ISR.
While Basic and Ter-minal ISR are both highly productive, TerminalISR is far more stable and accurate than Basic.Furthermore, there are far more Basic partialsthan Terminal partials, implying that the dia-logue manager would have to handle more un-stable and inaccurate partials more often.
Giventhis, Terminal ISR is a far better ?productiveISR?
than the Basic method.
Taking produc-tion and stability together, there is a double dis-Table 4: Lattice-Aware ISR (LAISR) Example1-best Partial Typeyew Terminalsarah Terminalbaum Terminaldallas Terminaldowntown Terminaldowntown Immortaldowntown pittsburgh Terminaldowntown pittsburgh Immortalsociation between Terminal and Immortal ISR.Terminal partials are over produced and rela-tively unstable.
Furthermore, they are even lessstable when the transcribed reference is greaterthan one word.
On the other hand, Immortalpartials are stable and quite accurate, but toorare for use alone.
By integrating the ImmortalPartials with the Terminal ones, we may be ableto increase the stability and accuracy overall.4 Lattice-Aware ISR (LAISR)We introduce Lattice-Aware ISR (LAISR ?pronounced ?laser?
), that integrates Terminaland Immortal ISR by allowing both types of par-tials to be found.
The selection procedure worksby first checking for an Immortal partial.
If oneis not found then it looks for a Terminal.
Re-dundant partials are returned when the partialtype changes.
An example recognition is shownin Table 4.
Notice how the first four partialsare completely unstable.
This is very common,and suppressing this noise is one of the primarybenefits of using more right context.
Basic ISRhas even more of this type of noise.LAISR was evaluated on the three recogni-tion tasks described above (see Table 5).
Thefirst two rows show the average number of par-tials per utterance for each task and utterancegroup.
Unsurprisingly, these numbers are quitesimilar to Terminal ISR.
The stability percent-age of LAISR is shown in the second two rows.For all the utterances, there appears to be a veryslight improvement when compared to Termi-nal ISR in Table 3.
The improvement increasesfor MW utterances, with LAISR improving over114Table 5: Lattice-Aware ISR StatsPartials per UtteranceRLM1 RLM2 SLMAll 5.6 3.5 6.7MW 6.7 4.5 9.6Stability PercentageAll 24 % 33 % 40 %MW 24 % 35 % 41 %Accuracy PercentageAll 15 % 23 % 26 %MW 16 % 22 % 24 %Terminal ISR by 4?7 percentage points.
Thisis primarily because there is a higher occur-rence of Immortal partials as the utterance getslonger.
Accuracy is reported in the final tworows.
Like the previous ISR methods described,the accuracy percentage is lower than the sta-bility percentage.
When compared to TerminalISR, LAISR accuracy is slightly higher, whichconfirms the benefit of incorporating immortalpartials with their relatively high accuracy.
Tobe useful in practice, it is important to exam-ine when in the utterance ISR results are be-ing produced.
For example, if most of the par-tials are returned towards the end of utterances,than ISR is of little value over standard turn-based recognition.
Figure 1 shows the percentof partials returned from the start of speech tothe final partial for MW utterances using theSLM.
This figure shows that partials are re-turned rather evenly over the duration of ut-terances.
For example, in the first 10% of dura-tion of each utterance, about 10% of all partialresults are returned.
Figure 1 also reports thestability and accuracy of the partials returned.These numbers grow as decoding progresses, butshows that mid-utterance results do yield rea-sonable accuracy: partials returned in the mid-dle of utterances (50%-60% duration) have anaccuracy of near 30%, compared to final partials47% percent.For use in a real-time dialogue system, it isalso important to assess latency.
Here we definelatency as the difference in (real-world) time be-tween (1) when the recognizer receives the lastFigure 1: Percent of LAISR partials returned fromthe start of detected speech to the final partial usingthe SLM.
The percentage of partials returned thatare stable/accurate are also shown.frame of audio for a segment of speech, and (2)when the partial that covers that segment ofspeech is returned from the recognizer.
Mea-suring latencies of LAISR on each task, we findthat RLM1 has a median of 0.26 seconds and amean of 0.41s; RLM2 has a median of 0.60s anda mean of 1.48s; and SLM has a median of 1.04sand a mean of 2.10s.
Since reducing latencywas not the focus on this work, no speed opti-mizations have been made, and we believe thatstraightforward optimization can reduce theselatencies.
For example, on the SLM, simplyturning off N-Best processing reduces the me-dian latency to 0.55s and the mean to 0.79s.Human reaction time to speech is roughly 0.20seconds (Fry, 1975), so even without optimiza-tion the RLM latencies are not far off humanperformance.In sum, LAISR produces a steady streamof partials with relatively low latency over thecourse of recognition.
LAISR has higher stabil-ity and accuracy than Terminal ISR, but its par-tials are still quite unstable and inaccurate.
Thismeans that in practice, dialogue systems willneed to make important decisions about whichpartials to use, and which to discard.
This needmotivated us to devise techniques for predictingwhen a partial is stable, and when it is accurate,which we address next.115Table 6: Equal Error Rates: Significant improvements in bold.
Basic at p < 0.016, Terminal at p < 0.002,and LAISR at p < 0.00001All Multi-WordStability Measure (SM) Equal Error RateRLM 1 RLM 2 SLM RLM 1 RLM 2 SLMBasic WATSON Score 13.3 13.3 12.8 15.6 16.4 15.2Regression 10.7 11.3 12.3 13.2 15.2 15.1Terminal WATSON Score 24.3 29.1 34.4 26.6 26.0 34.1Regression 19.7 26.5 26.5 23.0 24.3 24.7LAISR WATSON Score 24.7 29.3 35.0 24.0 27.0 35.3Regression 19.2 25.6 25.0 18.4 23.3 22.7Confidence Measure (CM) Equal Error RateBasic WATSON Score 11.3 11.7 9.9 14.1 14.0 11.6Regression 9.8 9.8 9.7 12.3 12.9 11.0Terminal WATSON Score 15.1 21.1 30.6 15.7 17.4 29.3Regression 11.7 16.8 20.8 12.1 14.5 18.4LAISR WATSON Score 15.8 21.8 32.3 18.4 19.5 31.8Regression 11.6 16.6 21.0 11.6 14.2 18.75 Stability and Confidence MeasuresAs seen in the previous section, partial speechrecognition results are often revised and inaccu-rate.
In order for a dialogue system to makeuse of partial results, measures of both stabilityand confidence are crucial.
A Stability Measure(SM) predicts whether the current partial is aprefix or complete match of the final recogni-tion result (regardless of whether the final resultis accurate).
A Confidence Measure (CM) pre-dicts whether the current partial is a prefix orcomplete match of what the user actually said.Both are useful in real systems: for example, ifa partial is likely stable but unlikely correct, thesystem might interrupt the user and ask themto start again.We use logistic regression to learn separateclassifiers for SM and CM.
Logistic regression isappealing because it is well-calibrated, and hasshown good performance for whole-utteranceconfidence measures (Williams and Balakrish-nan, 2009).
For this, we use the BXR pack-age with default settings (Genkin et al, 2011).For Terminal and Basic ISR we use 11 features:the raw WATSON confidence score, the individ-ual features which affect the confidence score,the normalized cost, the normalized speech like-lihood, the likelihoods of competing models,the best path score of word confusion network(WCN), the length of WCN, the worst probabil-ity in the WCN, and the length of N-best list.For LAISR, four additional features are used:three binary indicators of whether the partial isTerminal, Immortal or a Terminal following anImmortal, and one which gives the percentageof words in the hypothesis that are immortal.We built stability and confidence measures forBasic ISR, Terminal ISR, and LAISR.
Each ofthe three corpora (RLM1, RLM2, SLM) was di-vided in half to form a train set and test set.Regression models were trained on all utter-ances in the train set.
The resulting models werethen evaluated on both All and MW utterances.As a baseline for both measures, we compareto AT&T WATSON?s existing confidence score.This score is used in numerous deployed com-mercial applications, so we believe it is a fairbaseline.
Although the existing confidence scoreis designed to predict accuracy (not stability),there is no other existing mechanism for pre-dicting stability.We first report ?equal error rate?
for the mea-sures (Table 6).
Equal error rate (EER) is thesum of false accepts and false rejects at the rejec-116Figure 2: True accept percentages for stability measure (a) and confidence measure (b), using a fixed falseaccept rate of 5%.
LAISR yields highest true accept rates, with p < 0.0001 in all cases.
(a) Stability measure (b) Confidence measuretion threshold for which false accepts and falserejects are equal.
Equal error rate is a widelyused metric to evaluate the quality of scoringmodels used for accept/reject decisions.
A per-fect scoring model would yield an EER of 0.
Forstatistical significance we use ?2 contingency ta-bles with 1 degree of freedom.
It is inappropri-ate to compare EER across ISR methods, sincethe total percentage of stable or accurate par-tials significantly effects the EER.
For example,Basic ISR has relatively low EER, but this isbecause it also has a relatively low number ofstable or accurate partials.The top six rows of Table 6 show EER for theStability Measure (SM).
The left three columnsshow results on the entire test set (all utterances,of any length).
On the whole, the SM outper-forms the WATSON confidence scores, and thegreatest improvement is a 10.0 point reductionin EER for LAISR on the SLM task.
The rightthree columns show results on only multi-word(MW) utterances.
Performance is similar to theentire test set, with a maximum EER reductionof 12.6 percent.
The SLM MW performance isinteresting, suggesting that it is easier to pre-dict stability after at least one word has beendecoded, possibly due to higher probability ofimmortal nodes occurring.
This suggests therewould be benefit in combining our method withpast work that adds right-context, perhaps us-ing more context early in the utterance.
Thisidea is left for future work.The bottom six rows show results for the Con-fidence Measure (CM).
We see that that evenwhen comparing our CM against the WATSONconfidence scores, there is significant improve-ment, with a maximum of 13.1 for LAISR in theMW SLM task.The consistent improvement shows that logis-tic regression is an effective technique for learn-ing confidence and stability measures.
It is mostpowerful when combined with LAISR, and onlyslightly less so with Terminal.
Furthermore,though the gains are slight, it is also useful withBasic ISR, which speaks to the generality of theapproach.While equal error rate is useful for evaluatingdiscriminative ability, when building an actualsystem a designer would be interested to knowhow often the correct partial is accepted.
Toevaluate this, we assumed a fixed false-acceptrate of 5%, and report the resulting percentageof partials which are correctly accepted (true-accepts).
Results are shown in Figure 1.
LAISRaccepts substantially more correct partials thanother methods, indicating that LAISR would bemore useful in practice.
This result also showsa synergy between LAISR and our regression-based stability and confidence measures: notonly does LAISR improve the fraction of stable117and correct partials, but the regression is ableto identify them better than for Terminal ISR.We believe this shows the usefulness of the ad-ditional lattice features used by the regressionmodel built on LAISR results.6 Discussion and ConclusionThe adoption of ISR is hindered by the num-ber of revisions that most partials undergo.
Anumber of researchers have proposed the use ofright context to increase the stability of par-tials.
While this does increase stability, it mit-igates the primary gain of ISR: getting a rela-tively real-time stream of the user?s utterance.We offer two methods to improve ISR function-ality: the integration of low-occurring Immortalpartials with higher occurring Terminal partials(LAISR), and the use of logistic regression tolearn stability and confidence measures.We find that the integrative approach,LAISR, outperforms Terminal ISR on threerecognition tasks for a bus timetable spoken dia-logue system.
When looking at utterances withmore than one word this difference becomes evengreater, and this performance increase is due tothe addition of immortal partials, which havea higher occurrence in longer utterances.
Thissuggests that as dialogue systems are used toprocess multi-phrasal utterances and have moredynamic turn-taking interactions, immortal par-tials will play an even larger roll in ISR and par-tial stability will further improve.The Stability and Confidence measures bothhave lower Equal Error Rates than raw recog-nition scores when classifying partials.
The im-provement is greatest for LAISR, which benefitsfrom additional features describing lattice struc-ture.
It also suggests that other incremental fea-tures such as the length of right context could beuseful for predicting stability.
The higher num-ber of True Accept partials by LAISR indicatesthat this method is more useful to a dialoguemanager than Basic or Terminal ISR.
Even so,for all ISR methods there are still more use-ful stable partials than there are accurate ones.This suggests that both of these measures areimportant to the downstream dialogue manager.For example, if the partial is predicted to be sta-ble but not correct, than the agent could possi-bly interrupt the user and ask them to beginagain.There are a number of avenues for futurework.
First, this paper has examined the wordlevel; however dialogue systems generally oper-ate at the intention level.
Not all changes atthe word level yield a change in the resultingintention, so it would be interesting to applythe confidence measure and stability measuresdeveloped here to the (partial) intention level.These measures could also be applied to laterstages of the pipeline ?
for example, trackingstability and confidence in the dialogue state re-sulting from the current partial intention.
Fea-tures from the intention level and dialogue statecould be useful for these measures ?
for instance,indicating whether the current partial intentionis incompatible with the current dialogue state.Another avenue for future work would be toapply these techniques to non-dialogue real-timeASR tasks, such as transcription of broadcastnews.
Confidence and stability measures couldbe used to determine whether/when/how to dis-play recognized text to a viewer, or to informdown-stream processes such as named entity ex-traction or machine translation.Of course, an important objective is to eval-uate our Stability and Confidence Measureswith LAISR in an actual spoken dialogue sys-tem.
ISR completely restructures the conven-tional turn-based dialogue manager, giving theagent the opportunity to speak at any mo-ment.
The use of reinforcement learning to makethese turn-taking decisions has been shown in asmall simulated domain by Selfridge and Hee-man (2010), and we believe this paper buildsa foundation for pursuing these ideas in a realsystem.AcknowledgmentsThanks to Vincent Goffin for help with thiswork, and the anonymous reviewers for theirthoughtful suggestions and critique.
We ac-knowledge funding from the NSF under grantIIS-0713698.118ReferencesG.
Aist, J. Allen, E. Campana, C. Gallo, S. Stoness,Mary Swift, and Michael K. Tanenhaus.
2007.
In-cremental understanding in human-computer di-alogue and experimental evidence for advantagesover nonincremental methods.
In Proc.
DECA-LOG, pages 149?154.T.
Baumann, M. Atterer, and D. Schlangen.
2009.Assessing and improving the performance ofspeech recognition for incremental systems.
InProc.
NAACL: HLT, pages 380?388.A.
Black, S. Burger, B. Langner, G. Parent, andM.
Eskenazi, 2010.
Spoken dialog challenge 2010,In Proc.
Workshop on Spoken Language Technolo-gies (SLT), Spoken Dialog Challenge 2010 SpecialSession.David DeVault, Kenji Sagae, and David Traum.2009.
Can i finish?
learning when to respond toincremental interpretation results in interactive di-alogue.
In Proc.
SIGdial 2009 Conference, pages11?20,G.A.
Fink, C. Schillo, F. Kummert, and G. Sagerer.1998.
Incremental speech recognition for multi-modal interfaces.
In Industrial Electronics Soci-ety, 1998.
IECON?98 volume 4, pages 2012?2017.D.B.
Fry.
1975.
Simple reaction-times to speech andnon-speech stimuli.. Cortex volume 11, number 4,page 355.A.
Genkin, L. Shenzhi, D. Madigan, and DD.Lewis.
2011.
Bayesian logistic regression.http://www.bayesianregression.org.V.
Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur, A. Ljolje, S. Parthasarathy, M. Rahim,G.
Riccardi, and M. Saraclar.
2005.
The AT&TWATSON speech recognizer.
In Proc.
of ICASSP,pages 1033?1036.G.
Parent and M. Eskenazi.
2009.
Toward BetterCrowdsourced Transcription: Transcription of ayear of the Let?s Go Bus Information System Data.Proc.
of Interspeech 2005, Lisbon, Portugal.A.
Raux, B. Langner, D. Bohus, A.W.
Black, andM.
Eskenazi.
2005.
Lets go public!
taking a spo-ken dialog system to the real world.
In Proc.
ofInterspeech 2005.D.
Schlangen, T. Baumann, and M. Atterer.
2009.Incremental reference resolution: The task, met-rics for evaluation, and a Bayesian filtering modelthat is sensitive to disfluencies.
In Proc.
SIGdial,pages 30?37.E.O.
Selfridge and P.A.
Heeman.
2010.
Importance-Driven Turn-Bidding for spoken dialogue systems.In Proc.
of ACL 2010, pages 177?185.G.
Skantze and D. Schlangen.
2009.
Incrementaldialogue processing in a micro-domain.
In Proc.EACL 2009, pages 745?753J.C.
Spohrer, PF Brown, PH Hochschild, andJK Baker.
1980.
Partial traceback in continuousspeech recognition.
In Proc.
of the IEEE Interna-tional Conference on Cybernetics and Society.J.D.
Williams, I. Arizmendi and A. Conkie.2010.
Demonstration of AT&T ?Let?s Go?
: Aproduction-grade statistical spoken dialog system.In Proc Demonstration Session at IEEE Workshopon Spoken Language TechnologyJ.D.
Williams and S. Balakrishnan.
2009.
Estimat-ing probability of correctness for ASR N-Best lists.In Proc.
of SIGdial 2009, pages 132?135.119
