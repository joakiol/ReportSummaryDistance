Subcategorization Acquisition and Evaluation for Chinese VerbsXiwu Han, Tiejun Zhao, Haoliang Qi, Hao YuDepartment of Computer Science,Harbin Institute of Technology, 150001 Harbin, China{hxw, tjzhao, qhl, yh}@mtlab.hit.edu.cnAbstractThis paper describes the technology and an ex-periment of subcategorization acquisition forChinese verbs.
The SCF hypotheses are gener-ated by means of linguistic heuristic informationand filtered via statistical methods.
Evaluationon the acquisition of 20 multi-pattern verbsshows that our experiment achieved the similarprecision and recall with former researches.
Be-sides, simple application of the acquired lexiconto a PCFG parser indicates great potentialities ofsubcategorization information in the fields ofNLP.CreditsThis research is sponsored by National NaturalScience Foundation (Grant No.
60373101 and603750 19), and High-Tech Research and Devel-opment Program (Grant No.
2002AA117010-09).IntroductionSince (Brent 1991) there have been a consider-able amount of researches focusing on verb lexi-cons with respective subcategorization informa-tion specified both in the field of traditional lin-guistics and that of computational linguistics.
Asfor the former, subcategory theories illustratingthe syntactic behaviors of verbal predicates arenow much more systemically improved, e.g.
(Korhonen 2001).
And for auto-acquisition andrelevant application, researchers have made greatachievements not only in English, e.g.
(Briscoeand Carroll 1997), (Korhonen 2003), but also inmany other languages, such as Germany (Schulteim Walde 2002), Czech (Sarkar and Zeman2000), and Portuguese (Gamallo et.
al 2002).However, relevant theoretical researches onChinese verbs are generally limited to case gram-mar, valency, some semantic computation theo-ries, and a few papers on manual acquisition orprescriptive designment of syntactic patterns.Due to irrelevant initial motivations, syntacticand semantic generalizabilities of the consequentoutputs are not in such a harmony that satisfiesthe description granularity for SCF (Han andZhao 2004).
The only auto-acquisition work forChinese SCF made by (Han and Zhao 2004) de-scribes the predefinition of 152 general framesfor all verbs in Chinese, but that experiment isnot based on real corpus.
After observing andanalyzing quantity of subcategory phenomena inreal Chinese corpus in the People?s Daily(Jan.~June, 1998), we removed from Han &Zhao?s predefinition 15 SCFs that are actuallysimilar derivants of others, and then with thisfoundation and linguistic rules from (Zhao 2002)as heuristic information we generated SCF hy-potheses from the corpus of People?s Daily(Jan.~June, 1998), and statistically filtered thehypotheses into a Chinese verb SCF lexicon.
Asfar as we know, this is the first attempt of Chi-nese SCF auto-acquisition based on real corpus.In the rest of this paper, the second section de-scribes a comprehensive system that builds verbSCF lexicons from large real corpus, the respec-tive operating principles, and the knowledgecoded in our SCF.
The third section analyzed theacquired lexicon with two experiments: oneevaluated the acquisition results of 20 verbs withmulti syntactic patterns against manual goldstandard; the other checked the performance ofthe lexicon when applied in a PCFG parser.
Theforth section compares and contrasts this researchwith related works done by others.
And at last,Section 5 concludes our present achievements,disadvantages and possible future focuses.1   SCF Acquisition1.1 The Acquisition MethodThere are generally 4 steps in the process of ourauto-acquisition experiment.
First, the corpus isprocessed with a cascaded HMM parser; second,every possible local patterns for verbs are ab-stracted; and then, the verb patterns are classifiedinto SCF hypotheses according to the predefinedset; at last, hypotheses are filtered statisticallyand the respective frequencies are also recorded.The actual application program consists of 6parts as shown in the following paragraphs.a.
Segmenting and tagging: The raw cor-pus is segmented into words and taggedwith POS by the comprehensive seg-menting and tagging processor devel-oped by MTLAB of ComputerDepartment in Harbin Institute of Tech-nology.
The advantage of the POS defi-nition is that it describes some subsets ofnouns and verbs in Chinese.b.
Parsing: The tagged sentences are parsedwith a cascaded HMM parser1, devel-oped by MTLAB of HIT, but only theintermediate parsing results are used.The training set of the parser is 20,000sentences in the Chinese Tree Bank2 of(Zhao 2002).c.
Error-driven correction: Some key errorsoccurring in the former two parts arecorrected according to manually ob-tained error-driven rules, which are gen-erally about words or POS in the corpus.d.
Pattern abstraction: Verbs with largestgoverning ranges are regarded as predi-cates, then local patterns, previousphrases and respective syntactic tags areabstracted, and isolated parts are com-bined, generalized or omitted accordingto basic phrase rules in (Zhao 2002).e.
Hypothesis generation: Based on lin-guistic restraining rules, e.g.
no morethan two NP?s occurring in a series andno more than three in one pattern, andno PP TP MP occurring with NP beforeany predicates (Han and Zhao 2004), thepatterns are coordinated and classifiedinto the predefined SCF groups.
In thispart, about 5% unclassifiable patternsare removed.1 When evaluated on auto-tagged open corpus, the parser?sphrase precision if 62.3%, and phrase recall is 60.9% (Meng,2003).2 A sample of the tree bank or relevant introduction could befound at http://mtlab.hit.edu.cn.f.
Hypothesis filtering: According to thestatistical reliability of each type of theSCF hypotheses and the linguistic prin-ciple that arguments occur more fre-quently with predicates than adjuncts do,the hypotheses are filtered by means ofstatistical methods, in this paper whichare binomial hypotheses testing (BHT)and maximum likelihood estimation(MLE).Table 1: An Example of Auto-acquisitionNo Actions Resultsa) Input ???????????????????
?b) Tag andparseBNP[BMP[?/m ?/q ]?/ng ]?
/p NDE[ ?
?
?
/r ?/usde ]BVP[ ?
?
/vg ?/vq ]BVP[ ?
?
/vg ?/ut ]NP[?
?/nc ?/usde ?
?/ng ]?/wjc) CorrecterrorsBNP[BMP[?/m ?/q ]?/ng ]?/p NDE[??
?/r ?/usde ?
?/vg ?/vq ]BVP[?
?/vg ?/LE ]NP[?
?/nc ?/usde ?
?/ng ]?/wjd) AbstractpatternsBNP PP BVP[vg LE ] NPe) GeneratehypothesisNP v NP ?01000?f) Filter hy-pothesesNP v NP {01111}3In Table 1, for example, when acquiring SCFinformation for ????
(prove) and a related sen-tence in the corpus is a), our tagger and parserwill return b), and error-driven correction willreturn c) with errors of NDE and the 1st BVP cor-rected4.
Since the governing range of ????
islarger than that of ????
(ask), the other verb inthis sentence, the program abstracts its local pat-tern BVP[vg LE] and previous phrase BNP, gen-3  {01000} projects to the Chinese syntactic mor-phemes {?????????
}, 1 means the SCFmay occur with the respective morpheme, while 0may not (Han & Zhao, 2004).4 Note that not all errors in this example have been corrected,but this doesn?t affect further procession.
Also, for defini-tions of NDE and BVP see (Zhao, 2002).eralizes BNP and NDE as NP, combines the sec-ond NP with isolated part ??/p?
into PP, andreturns d).
Then the hypothesis generator returnse) as the possible SCF in which the verb mayoccurs.
Actually in the corpus there are 621 hy-pothesis tokens generated, and among them 92ones are of same arguments with e), and thus e)can pass the hypothesis testing (See also Section1.2), so we obtain one SCF for ????
as f).1.2 Filtering MethodsIn researches of subcategorization acquisition,statistical methods for hypothesis filtering mainlyinclude the BHT, the Log Likelihood Ratio(LLR), the T-test and the MLE, and the mostpopular one is the BHT.
Since (Brent 1993) be-gan to use the method, most researchers haveagreed that the BHT results in better precisionand recall with SCF hypotheses of high, mediumand low frequencies.
Only (Korhonen 2001) re-ports 11.9% total performance of the MLE betterthan the BHT.
Therefore, we applied the two sta-tistical methods in our present experiment.
Thissubsection chiefly illustrates the expressions ofour methods and definitions of parameters inthem, while performance comparison of the twowill be introduced in Section 3.When applying the BHT method, it is nec-essary to determine the probability of the primi-tive event.
As for SCF acquisition, the co-occurrence of one predefined SCF scfi with oneverb v is the relevant primitive event, and theconcerned probability is p(v|scfi) here.
However,the aim of filtering is to rule out those unreliablehypotheses, so it is the probability that one primi-tive event doesn't occur that is often used forSCF hypothesis testing, i.e.
the error probability:pe(v|scfi) = 1 p(v|scfi).
(Brent 1993) estimated peaccording to the acquisition system?s perform-ance, while (Briscoe and Carroll 1997) calculatedpe from the distribution of SCF types in ANLTand SCF tokens in Susanne as shown in the fol-lowing equation.Brent?s method mainly depends on the relatedcorpus and processing program, which maycause intolerable errors.
Briscoe and Carroll?smethod draws on both linguistic and statisticalinformation thus leading to comparatively stableestimation, and therefore has been used by manylatter researches, e.g.
(Korhonen 2001).
But thereis no MRD proper for Chinese SCF descriptionso we estimated pe from the 1,775 common verbsand SCF tokens in the related corpus of 43,000sentences used by (Han and Zhao 2004).
Weformed the equation as follows:Then the number of all hypotheses about verbvj is recorded as n, and the number of those forscfi as m. According to Bernoulli theory, theprobability P that an event with probability p ex-actly happens m times out of n such trials is:And the probability that the event happens m ormore times is:In turn, P(m+, n, pe) is the probability that scfiwrongly occurs m or more times with a verb thatdoesn't match it.
Therefore, a threshold of 0.05on this probability will yield a 95% confidencethat a high enough proportion of hypotheses forscfi have been observed for the verb legitimatelyto be assigned scfi (Korhonen 2001).The MLE method is closely related to the generalperformance of the concerned SCF acquisitionsystem.
First, we randomly draw from the ap-plied corpus a training set, which is large enoughso as to ensure similar SCF frequency distribu-tion.
Then, the frequency of scfi occurring with averb vj is recorded and used to estimate the actualprobability p(scfi| vj).
Thirdly, an empiricalthreshold is determined, such that it ensuresmaximum value of F measure on the training set.Finally, the threshold is used to filter out thoseSCF hypotheses with low frequencies from thetotal set.2    Experimental Evaluation2.1   Acquisition PerformanceUsing the previously described theory and tech-nology we have acquired an SCF lexicon for3,558 common Chinese verbs from the corpus ofPeople?s Daily (Jan.~June, 1998).
In the lexiconthe minimum number of SCF tokens for a verb is30, and the maximum is 20,000.
In order to checkthe acquisition performance of the used system,we evaluated a part of the lexicon against a man-ual gold standard.
The testing set includes 20verbs of multi syntactic patterns, and for eachverb there are 503~2,000 SCF tokens with thetotal number of 18,316 (See Table 2).
Table 3gives the evaluation results for different filteringmethods, including non-filtering 5 , BHT, andMLE with thresholds of 0.001, 0.005, 0.008 and0.01.
We calculated the type precision and recallby the following expressions as (Korhonen 2001)did:In here, true positives are correct SCF typesproposed by the system, false positives are incor-rect SCF types proposed by system, and falsenegatives are correct SCF types not proposed bythe system.Table 2: Verbs in the Testing Set6Verbs English Tokens Verbs English Tokens?
Read 503 ??
Hope 620??
Find 529 ?
See 645??
Reckon 543 ??
Invest 679?
Pull 544 ??
Know 722??
Report 612 ?
Send 800??
Develop 1,006 ??
Set up 1,186??
Behave 1,007 ??
Insist 1,200??
Decide 1,038 ?
Think 1,200??
End 1,140 ??
Require 1,200??
Begin 1142 ?
Write 2,000According to Table 3, all other filtering meth-ods outperform non-filtering, and MLE is betterthan BHT.
Among the four MLE thresholds,0.008 achieves the best comprehensive perform-ance but its F-measure is only 0.74 larger thanthat of 0.01 while its precision drops by 2.4 per-cent.
Hence, we chose 0.01 as the threshold forthe whole experiment with purpose to meet thepractical requirement of high precision and toavoid possible over-fit phenomena.
Finally, witha confidence of 95% we can estimate the generalperformance of the acquisition system with preci-sion of  60.6% +/- 2.39%, and recall of 51.3%+/-2.45%.5 Non-filtering means filtering with a zero threshold or notfiltering at all.
This method is used as baseline here.6 The English meanings given here are not intended to coverthe whole semantic range of the respective verbs, on thecontrary they are just for readers?
reference.Table 3: System Performance for DifferentFiltering MethodsMeasuresMethods Precision Recall F-measureNon-filtering 37.43% 85.9% 52.14BHT 50% 57.2% 53.360.001 39.2% 85.9% 53.830.005 40.3% 83.33% 54.330.008 58.2% 54.5% 56.3 MLE0.01 60.6% 51.3% 55.562.2    Task-oriented EvaluationIn order to further analyze the practicability ofthe previously described technology, we per-formed a simple task-oriented evaluation, apply-ing the acquired SCF lexicon in a PCFG parserhelping to choose from the n-best parsing results.The concerned parser was trained from 10,000manually parsed Chinese sentences7.
In this ex-periment there are 664 verbs and their SCF in-formation involved.
The open testing set consistsof 1,500 sentences, for each of which the PCFGparser outputs 5-best parsing results.
Then SCFhypotheses are generated for each result bymeans of the formerly mentioned technology.Finally, the maximum likelihood between hy-potheses and those SCF types for the related verbin the lexicon is calculated in the following way:where i ?
5, hi is one of the hypotheses generatedfor the parsing results, and scfj is the jth SCF typefor the concerned verb.
This calculation keeps thelikelihood between 0 and 1.
The parsing result7 These sentences and the testing corpus mentioned latter areall taken from the Chinese Tree Bank developed by MTLABof HIT, and a sample may be downloaded athttp://mtlab.hit.edu.cn.with maximum likelyhood is then regarded as thefinal choice.
When two or more hypotheses holdthe same likelihood, the one with larger or largestPCFG probability will be chosen.Table 4 shows the phrase-based and sentence-based evaluation results for the parser withoutand with SCF heuristic information.
There arethree cased included: a) The output is one-best; b)The output is 5-best and the best evaluation resultis recorded; c) The 5-best output is checked againfor the best syntactic tree by means of SCF in-formation.
The phrased-based evaluation followsthe popular method for evaluating a parser, whilethe sentence-based depends on the intersection ofthe parsed trees and those in the gold standard.Since the PCFG parser output at least one syntac-tic tree for every sentence in our testing corpus,the sentence-based precision and recall are equalto each other.Table 4: Parsing EvaluationPhrase-based Sentence-basedParsingMethodsPrecision Recall Precision= RecallOne-best 57.5% 55% 13.64%5-best 65.28% 64.59% 26.2%With SCF 62.86% 62.1% 21.66%Table 4 shows that SCF information remarka-bly improved the performance of the PCFGparser: the phrase-based precision increased by5.36% and recall by 7.1%, while the sentence-based precision and recall both increased by8.04%.
However, this doesn?t reach the upperlimit of the 5-best.
The possible reasons are: a)the our present SCF lexicon remains to be im-proved; b) our method of applying SCF informa-tion to the parser is too simple, e.g.
probabilitiesof PCFG parsing results haven?t been exploitedthoroughly.3 Related WorksAs far as we know, this is the first attempt toautomatically acquire SCF information from realChinese corpus and the first trial to apply SCFlexicon to a Chinese parser.
Our research draws alot on related works from international researches,and for the purpose of crosslingual processing,our research is kept in consistency with SCFconventions as much as possible.Due to linguistic differences, nevertheless, notall theories, methods or experiences could adaptto Chinese.
Generally, there are four aspects thatour research differs from those of other lan-guages.
First, the SCF formalization of mostformer researches follows the Levin style, inwhich most SCFs omit NP before predicates,while Chinese SCFs need to depict argumentsoccurring before verbs.
Second, except (Sarkarand Zeman 2000), most former researches arebased on manual SCF predefinition, while ourpredefined SCF set is statistically acquired (SeeHan and Zhao 2004).
Third, involved parsers offormer researches are mostly better than Chineseparsers to some degree.
Forth, our SCF informa-tion also includes 5 syntactic morphemes (Seealso Section 1.1).Meanwhile, the basic purpose for Chinese SCFacquisition is also to determine the subcategoryfeatures for a verb via its argument distributionsand then apply the lexicon to NLP tasks.
There-fore, under similar cases the respective evalua-tions are comparable.
And Table 5 gives thecomparison between our research and the bestEnglish results without semantic backoff 8  in(Korhonen 2001).Table 5: Performance Comparison BetweenChinese and English ResearchesFilteringMeasures    Non BHT MLEOurs 37.43% 50% 58.2%Precision Korhonen 24.3% 50.3% 74.8%Ours 85.9% 57.2% 54.5%Recall Korhonen 83.5% 56.6% 57.8%Ours 52.14 53.36 56.3F-measure Korhonen 37.6 53.3 65.2The comparison shows that our nonfiltering re-sult is better than Korhonen?s, both BHT resultsare similar, while our MLE result is much worse8 Semantic backoff is a method of generating SCF hypothe-ses according to the semantic classification of the concernedverb.
Note that this paper doesn?t involve verb meanings forgenerating hypotheses.
Besides, though the evaluation forEnglish SCF acquisition is the best, it?s not the newest.
Forthe newest, please refer to (Korhonen 2003), in which theprecision is 71.8% and recall is 34.5%.than Korhonen?s.
That means our hypothesisgenerator performs well but our filtering methodremains to be improved.
According to the analy-sis of relevant corpus, we found the main causemight be that low frequency SCF types accountfor 32% in our corpus while those in (Korhonen2001) sum to nearly 21%.Further more, (Briscoe and Carroll 1997) ap-plied their acquired English SCF lexicon to anintermediate parser, and reported a 7% improve-ment of both phrase-based precision and recall.Our application of SCF lexicon to a PCFG parserleads to 5.36% improvement for phrase-basedprecision, 7.1% for recall, and 8.04% for sen-tence-based precision and recall.4    ConclusionThis paper for the first time describes a largescaleexperiment of automatically acquiring SCF lexi-con from real Chinese corpus.
Perfor mance eva-luation shows that our technology and acquiringprogram have achieved similar performancecompared with former researches of other lan-guages.
And the application of the acquired lexi-con to a PCFG parser indicates great potentiali-ties of SCF information in the field of NLP.However, there is still a large gap betweenChinese subcategorization works and those of o-ther languages.
Our future work will focus on theoptimization of linguistic heuristic informationand filtering methods, the application of semanticbackoff, and the exploitation of SCF lexicon forother NLP tasks.ReferencesBrent, M. R. 1991.
Automatic acquisition of subcate-gorization frames from untagged text.
In Proceed-ings of the 29th Annual Meeting of the Associationfor Computational Linguistics, Berkeley, CA.
209-214.Brent, M. 1993.
From Grammar to Lexicon: un-supervised learning of lexical syntax.
Compu-tational Linguistics 19.3.
243-262.Briscoe, Ted and John Carroll, 1997.
Automatic ex-traction of subcategorization from corpora.
In Pro-ceedings of the 5th ACL Conference on AppliedNatural Language Processing, Washington, DC.Dorr, B. J. Gina-Anne Levow, Dekang Lin, and ScottThomas, 2000.
Chinese-English Semantic ResourceConstruction, 2nd International Conference onLanguage Resources and Evaluation (LREC2000),Athens, Greece, pp.
757--760.Gamallo, P., Agustini, A. and Lopes Gabriel P., 2002.Using Co-Composition for Acquiring Syntacticand Semantic Subcategorisation, ACL-02.Han, Xiwu, Tiejun Zhao, 2004.
FML-Based SCF Pre-definition Learning for Chinese Verbs.
Interna-tional Joint Conference of NLP 2004.Jin, Guangjin, 2001.
Semantic Computations for Mod-ern Chinese Verbs.
Beijing University Press, Bei-jing.
(in Chinese)Korhonen, Anna, 2001.
Subcategorization Acquistion,Dissertation for Ph.D, Trinity Hall University ofCambridge.
29-77.Korhonen, Anna, 2003.
Clustering Polysemic Sub-categorization Frame Distributions Semantically.Proceedings of the 41st Annual Meeting of the As-sociation for Computational Linguistics, pp.
64-71.Meng, Yao, 2003.
Research on Global Chinese Pars-ing Model and Algorithm Based on Maximum En-tropy.
Dissertation for Ph.D. Computer Department,HIT.
33-34.Sabine Shulte im Walde, 2002.
Inducing German Se-mantic Verb Classes from Purely Syntactic Sub-categorization Information.
Proceedings of the 40stACL, pp.
223-230.Sarkar, A. and Zeman, D. 2000.
Automatic Ex-traction of Subcategorization Frames forCzech.
In Proceedings of the 19th Interna-tional Conference on Computational Linguis-tics, aarbrucken, Germany.Zhan Weidong, 2000.
Valence Based Chinese Seman-tic Dictionary, Language and Character Applica-tions, Volume 1.
(in Chinese)Zhao Tiejun, 2002.
Knowledge Engineering Reportfor MTS2000.
