Statistically-Enhanced New Word Identificationin a Rule-Based Chinese SystemAndi WuMicrosoft ResearchOne Microsoft WayRedmond, WA 98052Andiwu @microsoft.comZixin JiangMicrosoft ResearchOne Microsoft WayRedmond, WA 98052jiangz@ microsoft.tomAbstractThis paper presents a mechanism of newword identification i  Chinese text whereprobabilities are used to filter candidatecharacter strings and to assign POS to theselected strings in a ruled-based system.
Thismechanism avoids the sparse data problem ofpure statistical approaches and theover-generation problem of rule-basedapproaches.
It improves parser coverage andprovides a tool for the lexical acquisition ofnew words.1 IntroductionIn this paper, new words refer to newly coinedwords, occasional words and other rarely usedwords that are neither found in the dictionary of anatural language processing system norrecognized by the derivational rules or propername identification rules of the system.
Typicalexamples of such words are shown in thefollowing sentences, with the new wordsunderlined inbold.~ ~ , ~ ~ " ~ " ,~ ~ ~ .~- -~E~f f~, ,~R~"*\[\]~.2/..~W~m~@~o~ ~ .
~ ~ oThe automatic dentification fsuch words by amachine is a trivial task in languages wherewords are separated by spaces in written texts.
Inlanguages like Chinese, where no word boundaryexists in written texts, this is by no means an easyjob.
In many cases the machine will not evenrealize that there is an unfound word in thesentence since most single Chinese characterscan be words by themselves.Purely statistical methods of wordsegmentation (e.g.
de Marcken 1996, Sproat et al1996, Tung and Lee 1994, Lin et al(1993),Chiang et al(1992), Lua, Huang et al etc.)
oftenfail to identify those words because of the sparsedata problem, as the likelihood for those words toappear in the training texts is extremely ow.There are also hybrid approaches such as (Niedt al 1995) where statistical approaches andheuristic rules are combined to identify newwords.
They generally perform better thanpurely statistical segmenters, but the new wordsthey are able to recognize are usually propernames and other elatively frequent words.
Theyrequire a reasonably big training corpus and theperformance is often domain-specific dependingon the training corpus used.Many word segmenters ignore low-frequencynew words and treat heir component charactersas independent words, since they are often of46little significance in applications where thestructure of sentences is not taken intoconsideration.
For in-depth natural languageunderstanding where full parsing is required,however, the identification of those words iscritical, because a single unidentified word cancause a whole sentence to fail.The new word identification mechanism to bepresented here is used in a wide coverageChinese parser that does full sentence analysis.
Itassumes the word segmentation processdescribed in Wu and Jiang (1998).
In this model,word segmentation, including unfound wordidentification, is not a stand-alone process, but anintegral part of sentence analysis.
Thesegmentation component provides a word latticeof the sentence that contains all the possiblewords, and the final disambiguation is achievedin the parsing process.In what follows, we will discuss twohypotheses and their implementation.
The firstone concerns the selection of candidate stringsand the second one concerns the assignment ofparts of speech (POS) to those strings.2 Selection of candidate strings2.1 HypothesisChinese used to be a monosyllabic language,with one-to-one correspondences betweensyllables, characters and words, but most wordsin modem Chinese, especially new words,consist of two or more characters.
Of the 85,135words in our system's dictionary, 9217 of themare monosyllabic, 47778 are disyllabic, 17094are m-syllabic, and the rest has four or morecharacters.
Since hardly any new character isbeing added to the language, the unfound wordswe are trying to identify are almost alwaysmultiple character words.
Therefore, if we find asequence of single characters (not subsumed byany words) after the completion of basic wordsegmentation, derivational morphology andproper name identification, this sequence is verylikely to be a new word.
This basic intuition hasbeen discussed in many papers, such as Tung andLee (1994).
Consider the following sentence.
(1) ~.~rj~ IIA~,~t~l~.J~)-~l~-~-.~t:a--.This sentence contains two new words (notincluding the name "~t~l~ which is recognizedby the proper name identification mechanism)that are unknown to our system:~f~:~rj (probably the abbreviated name of ajunior high school)~:~j (a word used in sports only but not in ourdictionary)Initial lexical processing based on dictionarylookup and proper name identification producesthe following segmentation:where ~-~rJ and ~a~.~\]- are segmented into singlecharacters.
In this case, both singlecharacter-strings are the new words we want tofind.However, not every character sequence is aword in Chinese.
Many such sequences aresimply sequences of.single-character words.Here is an example:After dictionary look up, we getwhich is a sequence of 10 single characters.However, every character here is an independentword and there is no new word in the sentence.From this we see that, while most new wordsshow up as a sequence of single characters, notevery sequence of single characters forms a newword.
The existence of a single-character st ingis the necessary but not sufficient condition for anew word.
Only those sequences of singlecharacters where the characters are unlikely tobe a sequence of independent words are goodcandidates for new words.2.2 ImplementationThe hypothesis n the previous ection can beimplemented with the use of the IndependentWord Probability (IWP), which can be a propertyof a single character or a string of characters.472.1.1 Def'ming IWPMost Chinese characters can be used either asindependent words or component parts ofmultiple character words.
The IWP of a singlecharacter is the likelihood for this character toappear as an independent word in texts:N(Word(c)) IWP(c) =N(c)where N(Word(c)) is the number of occurrencesof a character as an independent word in thesentences of a given text corpus and N(c) is thetotal number of occurrence of this character in thesame corpus.
In our implementation, wecomputed the probability from a parsed corpuswhere we went through all the leaves of the trees,counting the occurrences of each character andthe occurrences of each character as anindependent word.The parsed corpus we used contains about5,000 sentences and was of course not big enoughto contain every character in the Chineselanguage.
This did not turn out to be a majorproblem, though.
We find that, as long as all thefrequently used single-character words are in thecorpus, we can get good results, for what reallymatters is the IWP of this small set of frequentcharacters/words.
These characters/words arebound to appear in any reasonably largecollection of texts.Once we have the IWP of individual characters(IWP(c)), we can compute the IWP of a characterstring (IWP(s)).
IWP(s) is the probability of asequence of two or more characters being asequence of independent words.
This is simplythe joint probability of the IWP(c) of thecomponent characters.2.1.2 Using lWPWith IWP(c) and IWP(s) defined , we thendefine a threshold T for IWP.
A sequence S oftwo or more characters i considered a candidatefor a new word only if its IWP(s) < T.  WhenIWP(s) reaches T, the likelihood for thecharacters to be a sequence of independent wordsis too high and the string will notbe considered tobe a possible new word.
In our implementation,the value of Tis empirically determined.
A lowerT results in higher precision and lower recallwhile a higher T improves recall at the expense ofprecision.
We tried different values and weighedrecall against precision until we got the bestperformance.
~-~)J and ~ '~ in Sentence (1) areidentified as candidate dates because1WP(s ) (~)  = 8% and lWP(s)(~'~\]~) = 10%while the threshold is 15%.
In our system,precision is not a big concern at this stagebecause the final filtering is done in the parsingprocess.
We put recall first to ensure that theparser will have every word it needs.
We alsotried to increase precision, but not at the expenseof recall.3 POS AssignmentOnce a character string is identified to be acandidate for new word, we must decide whatsyntactic category or POS to assign to this?
possible new word.
This is required for sentenceanalysis where every word in the sentence musthave at least one POS.3.1.
Hypothes isMost multiple character words in Chinese haveword-internal syntactic structures, which isroughly the POS sequence of the componentcharacters (assuming each character has a POS orpotential POS).
A two-character verb, forexample, can have a V-V, V-N, V-N or A(dv)-Vinternal structure.
For a two-character string tobe assigned the POS of verb, the POS/potentialPOS of its component characters must match oneof those patterns.
However, this matching aloneis not the sufficient condition for POS assignment.Considering the fact that a single character canhave more than one POS and a single POSsequence can correspond to the internal wordstructures of different parts of speech (V-N canbe verb or a noun, for instance), simply assigningPOS on the basis of word internal structurewillresult in massive over-generation a d introducetoo much noise into the parsing process.
Toprune away the unwanted guesses, we need morehelp from statistics.When we examine the word formation processin Chinese, we find that new words are oftenmodeled on existing words.
Take the newlycoined verb ~?~J" as an example.
Scanning ourdictionary, we find that ~" appears many times asthe first character of a two-character verb, such asF~'5~, ~,  ~ '~,  ~ '~,  ~\ [ , ,  ~'~'~J~, e tc .Meanwhile, ~J" appears many times as the second48character of a two-character verb, such as ~\ ]~,~,.~\]~j-, z\]z~, ~\]~\]., ~l-~J, ~\]r~, etc.
This leads usto the following hypothesis:A candidate character string for a new word islikely to have a given POS if the componentcharacters of this string have appeared in thecorresponding positions of many existing wordswith this POS.3.2.
ImplementationTo represent the likelihood for a character toappear in a given position of a word with a givenPOS and a given length, we assign probabilitiesof the following form to each character:P( Cat, Pos, Len )where Cat is the category/POS of a word, Pos isthe position of the character in the word, and Lenis the length (number of characters) of the word.The probability of a character appearing as thesecond character in a four-character verb, forinstance, is represented asP(Verb,2,4).3.1.1.
Computing P(Cat, Pos, Len)There are many instantiations ofP(Cat, Pos, Len), depending on the values of thethree variables.
In our implementation, welimited the values of Cat to Noun, Verb andAdjective, since they are the main open classcategories and therefore the POSes of most newwords.
We also assume that most new words willhave between 2 to 4 characters, thereby limitingthe values of Pos to 1--4 and the values of Len to2--4.
Consequently each character will have 27different kinds of probability values associatedwith it.
We assign to each of them a 4-charactername where the first character is always "P", thesecond the value of Cat, the third the value of Pos,and the fourth the value of Len.
Here are someexamples:Pnl2 (the probability of appearing as the firstcharacter of a two-character noun)Pv22 (the probability of appearing as thesecond character of a two-character verb)Pa34 (the probability of appearing as the thirdcharacter of a four-character adjective)The values of those 27 kinds of probabilities areobtained by processing the 85,135 headwords inour dictionary.
For each character inChinese, wecount he number of occurrences of this characterin a given position of words with a given lengthand given category and then divide it by the totalnumber of occurrences of this character in theheadwords of the dictionary.
For example,N(vl2(c))Pv12( c ) =N(c)where N(v12(c)) is the number of occurrences ofa character in the first position of a two-characterverb while N(c) is the total number ofoccurrences of this character in the dictionaryheadwords.
Here are some of the values we getfor the character~:Pnl2(~b~) = 7%Pv12(~) = 3%Pv23(~\]) = 39%en22(~)  = 0%Pv22(~) =24%ea22(~)  =1%It is clear from those numbers that the charactertend to occur in the second position oftwo-character and three-character v rbs.3.1.2.
Using P(Cat, Pos, Len)Once a character string is identified as a newword candidate, we will calculate the POSprobabilities for the string.
For each string, wewill get P(noun), P(verb) and P(adj) which arerespectively the probabilities of this string beinga noun, a verb or an adjective.
They are the jointprobabilities of the P(Cat, Pos, Len)o f  thecomponent characters of this string.
We thenmeasure the outcome against a threshold.
For anew word string to be assigned the syntacticcategory Cat, its P(Cat) must reach the threshold.The threshold for each P(Cat ) is independentlydetermined so that we do not favor a certain POS(e.g.
Noun) simply because there are more nounsin the dictionary.If a character string reaches the threshold ofmore than one P(Cat), it will be assigned morethan one syntactic ategory.
A string that hasboth P(noun) and P(verb) reaching the threshold,for example, will have both a noun and a verbadded to the word lattice.
The ambiguity is thenresolved in the parsing process.
If a string passesthe IWP test but falls the P(Cat) test, it will49receive noun as its syntactic ategory.
In otherwords, the default POS for a new word candidateis noun.
This is what happened to ~f~ in theSentence (l).
~-~D passed tlhe IWP test, butfailed each of the P(Cat) tests.
As a result, it ismade a noun by default.
As we can see, thisassignment is the correct one (at least in thisparticular sentence).4.
Results and Discussion4.1.
Increase in Parser CoverageThe new word identification mechanismdiscussed above has been part of our system forabout 10 months.
To find out how muchcontribution it makes to our parser coverage, wetook 176,863 sentences that had been parsedsuccessfully with the new word mechanismturned on and parsed them again with the newword mechanism turned off.
When we did thistest at the beginning of these 10 months, 37640 ofthose sentences failed to get a parse when themechanism was turned off.
In other words,21.3% of the sentences were "saved" by thismechanism.
At the end of the 10 months,however, only 7749 of those sentences failedbecause of the removal of the mechanism.
Atfirst sight, this seems to indicate that the newword mechanism is doing a much lesssatisfactory job than before.
What actuallyhappened is that many of the words that wereidentified by the mechanism 10 months ago,especially those that occur frequently, have beenadded to our dictionary.
In the past 10 months,we have been using this mechanism both as acomponent of robust parsing and as a method oflexical acquisition whereby new enwies arediscovered from text corpora.
This discoveryprocedure has helped us find many words that arefound in none of the existing word lists we haveaccess to.4.2.
Precision of IdentificationApart from its contribution to parser coverage,we can also evaluate the new word identificationmechanism by looking at its precision.
In ourevaluation, we measured precision in twodifferent ways.In the first measurement, we compared thenumber of new words that are proposed by theguessing mechanism and the number of wordsthat end up in successful parses.
If  we use NWAto stand for the number of new words that areadded to the word lattice and NWU for thenumber of new words that appear in a parse tree,the precision rate will be NWU / NWA.
Actualtesting shows that this rate is about 56%.
Thismeans that the word guessing mechanism hasover-guessed and added about twice as manywords as we need.
This is not a real problem inour system, however, because the final decisionis made in the parsing process.
The lexicalcomponent is only responsible for providing aword lattice of which one of the paths is correct.In the second measurement, we had a nativespeaker of Chinese go over all the new words thatend up in successful parses and see how many ofthem sound like real words to her.
This is a fairlysubjective test but nonetheless meaningful one.It turns out that about 85% of the new words that"survived" the parsing process are real words.We would also like to run a large-scale recalltest on the mechanism, but found it to beimpossible.
To run such a test, we have to knowhow many unlisted new words actually exist in acorpus of texts.
Since there is no automatic wayof knowing it, we would have to let a humanmanually check the texts.
This is too expensiveto be feasible.4.3.
Contributions of Other ComponentsWhile the results shown above do give us someidea about how much contribution the new wordidentification mechanism akes to our system, itis actually very difficult to say precisely howmuch credit goes to this mechanism and howmuch to other components of the system.
As wecan see, the performance of this mechanism alsodepends on the following two factors:(1) The word segmentation processes prior tothe application of this mechanism.
Theyinclude dictionary lookup, derivationalmorphology, proper name identificationand the assembly of other items such astime, dates, monetary units, address, phonenumbers, etc.
These processes also groupcharacters into words.
Any improvement inthose components will also improve theperformance of the new word mechanism.If every word that "should" be found by50those processes has already been identified,the single-character sequences that remainafter those processes will have a betterchance of being real words.
(2) The parsing process that follows.
Asmentioned earlier, the lexical component ofour system does not make a final decisionon "wordhood".
It provides a word latticefrom which the syntactic parser is supposedto pick the correct path.
In the case of newword identification, the word lattice willcontain both the new words that areidentified and the all the words/charactersthat are subsumed by the new words.
Anew word proposed in the word lattice willreceive its official wordhood only when itbecomes part of a successful parse.
Torecognize a new word correctly, the parserhas to be smart enough to accept he goodguesses and reject the bad guesses.
Thisability of the parser will imporve as theparser improves in general and a betterparser will yield better final results in newword identification.Generally speaking, the mechanisms using IWPand P(Cat, Pos, Len) provide the internal criteriafor wordhood while word segmentation andparsing provide the external criteria.
The internalcriteria are statistically based whereas theexternal criteria are rule-based.
Neither can do agood job on its own without the other.
Theapproach we take here is not to be consideredstaff stical natural language processing, but it doesshow that a rule-based system can be enhancedby some statistics.
The statistics we need can beextracted from a very small corpus and adictionary and they are not domain dependent.We have benefited from the mechanism in theanalysis of many different kinds of texts.ReferencesChang, Jyun-Sheng, Shun-Der Chen, Sue-Jin Ker,Ying Chen and John S. Liu (1994) Amultiple-corpus approach to recognition of propernames in Chinese texts, Computer Processing ofChinese and Oriental Languages, Vol.
8, No.
1 pp.75-85.Chen, Keh-Jiann and Shing-Huan Liu (1992).
Wordidentification for Mandarin Chinese sentences,Proceedings of COLING-92, pp.
23-28.Chiang, T. H., Y. C. Lin and K.Y.
Su (1992).Statisitical models for word segmentation andunknown word resolution, Proceedings of the 1992R.
O. C. Computational Linguistics Conference,121-146, Taiwan.De Marcken, Carl (1996).
Unsupervised LanguageAcquisition, Ph.D dissertation, MIT.Lin, M. Y. , T. H. Chiang and K. Y. Su (1993) Aprelimnary study on unknown word problem inChinese word segmentation, Proceedings of the1993 R. O. C. Computational LinguisticsConference, 119-137, Taiwan.Lua, K T. Experiments on the use of bigram mutualinformation i Chinese natural language processing.Nie, Jian Yun, et al (1995) Unknown Word Detectionand Segmentation f Chinese using Statistical andHeuristic Knowledge, Communications of COUPS,vol 5, No.
1 &2, pp.47, Singapore.Sproat, Richard, Chilin Shih, William Gale and NancyChang (1996).
A stochastic finite-stateword-segmentation algorithm for Chinese.Computational Linguistics, Volume 22, Number 3.Tung, Cheng-Huang and Lee His-Jian (1994).Identification of unknown words from a corpus.Computer Processing of Chinese and OrientalLanguages, Vol.
8 Supplement, pp.
131-145.Wu, Andi and Zixin Jiang (1998) Word segmentationin sentence analysis, Proceedings of the 1998International Conference on Chinese InformationProcessing, pp.
169-180.Yeh, Ching-Long and His-Jian Lee (1991).Rule-based word identification for MandarinChinese sentences - a unification approach,Computer Processing of Chinese and OrientalLanguages, Vol 5, No 2, Page 97-118.51
