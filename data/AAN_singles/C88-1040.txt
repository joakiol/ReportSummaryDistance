Robust  pars ing  o f  severe ly  cor rupted  spoken ut terancesEgidio P. Giachin Claudio RullentCSELT - Centro Studi e Laborator i  TelecomunicazioniVia Reiss Romoli  274, Torino, Italy - Ph.
439-11-21691AbstractThis paper describes a technique for enabling a speechunderstanding system to deal with sentences for whichsome monosyllabic words are not recognized.
Such wordsare supposed to act as mere syntactic markers within thesystem linguistic domain.
This result is achieved by com-bining a modified caseframe approach to linguistic knowl-edge representation with a parsing strategy able to inte-grate expectations from the language model and predic-tions from words.
Experimental results show that theproposed technique permits to greatly increase the quotaof corrupted sentences correctly understandable withoutsensibly decreasing parsing efficiency.1 IntroductionThe problem addressed by this paper is how to makea speech understanding system deal wlth sentences forwhich some types of words are not recognized.The continuous peech understanding system underdevelopment a CSELT laboratories \[Fissore 88\] is part ofa question-answerlng system allowing to extract informa-tion from a data base using voice messages with high syn-tactic freedom.
The system is composed of a recognitionstage \[Laface 87\] followed in cascade by an understand-ing stage.
The recognition stage analyzes peech usingacoustic-phonetic knowledge.
Since utterances are spo-ken without pauses between words, it is not possible touniquely locate words without using syntactic and seman-tic constraints.
Thus the actual output of the recognitionstage is a set:of word hypotheses, usually called lattice inthe literature.
A word hypothesis is characterized by itsbegin and end times, corresponding to the portion of theutterance in which it has been located, by a score repre-senting its belief degree, and by the lexeme itself.
Theunderstanding stage has the task of analyzing the wordlattice using linguistic knowledge and producing a rep-resentation of the meaning of the most likely consistentword sequence.A two-stage approach to speech understanding of-fers several advantages and is the most widely followedin the current research.
A serious difficulty, however, llesin the fact that often some short words that were actu-ally uttered are not detected by the recognltion level andhence they are missing from the lattice.
To cope with thisproblem the understandlng stage must adopt a languagerepresentation a d a parsing strategy which 1) wheneverpossible, do not rely on such words to understand a sen-tence, and 2) keep parsing efficiency comparable with thecase in which no word is missing.
This paper describesa technique for obtaining such results.
The following isdivided into four sections.
The next one focuses on thevarious implications of word undetectlon on the linguis-tic processing.
Then the linguistic knowledge bases ofthe understanding system and the parsing strategy areoutlined (assuming that all words are present in the lat-tice).
Next the technique for coping with missing wordsis introduced.
Finally, experimental results are discussed,showing that the proposed technique permits to greatlyincrease the quota of corrupted sentences correctly under-standable without sensibly decreasing parsing efficiency.A discussion is also provided relating our results to otherworks addressing slmilar problems.2 A closer examination of theproblem2.1 F rom the  acoust i ca l  v iewpo intThe phenomenology of word undetectlon at the recogni-tion level is somewhat complex but mainly depends onword length.
The dependency on length penalizes hortwords over long ones; i it is partly intrinsic to the signal-processing techniques used for recognition, and also hear?ily enhanced by coarticulatlon events.
The consequence isthat short words are frequently undetected or are givenunreliable scores; then a standard parsing either wouldnot work or would encounter heavy inefficiencies.There is also an additional problem for continuous1By 'short' word  we mean a word described by one or twophonetic units.
Phonetic units can be viewed approximatelyas phonemes  \[Laface 87\].This work hoJ been partially supported by the "EEC wlbhin the Espritproject ~6.196speech.
Often short words are erroneously detected andassigned a good score.
That happens frequently whentheir phonetic representation is also part of a longer wordthat was actually uttered.
For this reason the efficiency ofa traditional parser would be reduced ue to the necessityof taking into consideration such nonexistent words.2.2 F rom the understanding view-pointShort words span the widest range of lexical categoriesand have various degrees of 'significance' (take this terminformally).
Some cannot be eluded and, if they are miss-ingj it is necessary to understand the rest of the sen-fence and to initiate an additional interaction wlth therecognition level trying to figure out the most plausiblewords among a very limited set glven by the parser; ifno accept~ble word is found, a dialogue with the usermay be t~tarted, aimed at eliciting the essential infor-mation.
Both are time-consuming operations; the lat-terp moreover, requires careful ergonomic onsiderations\[Kaplan 87,\].
However, there are words for which the sit-uation is 1Lot so drastic.
This is the case of determiners,prepositions, and auxiliary verbs.The ~;reatment of words of these categories followtwo main guidelines in the literature.
In the former~such words act mainly as syntactic markers for multi-word semantic onstituents, without providing an intrin-sic semantic ontribution.
This philosophy includes casebased \[Fillmore 68\] and conceptual-dependency based ap-proaches to natural anguage understanding \[Schank 75\].In the latter guideline, such words play an independentrole as semantic units and contribute compositionallywlth other words to the global meaning, with equal dig-nity \[Hinrlchs 86,Lesmo 85\].
Clearly, given the specificproblem we are addressing, it is mandatory to follow theformer guideline.
Happily, this commitment is coherentwith the preference granted to caseframe based parsingcoming from different and independent reasons inherentin speech understanding (see \[Hayes 86\] for an excellentdiscussion).
The peculiar caseframe based approach sum-marlzed in the next section provides in most cases theability of understanding a sentence without relying onsuch word~.3 The  s tandard  pars ing  st rat -egy3.1 Linguistic knowledge representa-tionLinguistic knowledge representation is based on the no-tion of cas~frame \[Fillmore 68\] and is described in detailin \[Poeslo 871.
Caseframes offer a number of advantagesh* speech parsing, hence their popularity in many recentspeech understanding systems \[Hayes 86,Brietzmann 86\],but cause two main difficulties.First, the analysis cannot be driven by casemarkers,as is the case with written language, since often casemark-ere are just t!lose kinds of short words that are unreliablyrecognized or undetected at all.
The standard approachis to assign to case headers the leading role, that is toinstantlate caseframes using word hypotheses to fill theirheader slot and subsequently to try to expand the caseslots.
This strategy induces parsing to proceed in a top-down fashion, and works satisfactorily when headers areamong the best-scored lexical hypotheses.
However, itcan be shown \[Gemello 87\] to cause severe problems ifthere is a bad-scored but correct header word, becausethe corresponding caseframe inetantiation will not be re-sumed until all of the caseffames having better-scored butfalse header words have been processed.
The situation ofheaders with bad scores happens quite frequently, espe-cially when the uttered sentences suffer from strong localcorruption due to coartlculatlon phenomena or environ-mental noise.
Moreover, the standard strategy does notexploit the fact, dual to the one previously outlined, thatsome word hypotheses, though not being headers, have agood and reliable score.
An integrated top-down/bottom-up strategy, able to exploit the predictive power of non-header words, is mandatory in such situations.A second difficulty is given by the integration ofcaseframes and syntax.
This is due to two conflictingrequirements.
From one side, syntax should be definedand developed as a declaratlve knowledge base indepen-dently from caseframes, ince this permits to exploit syn-tactic formalisms ~t the best and ins, ires ease of mainte-nance when the linguistic domain has to be expanded orchanged.
On the other hand, syntactic onstraints shouldbe used together with semantic ones during parsing, be-cause this reduces the size of the inferential activity.To overcome these problems~ caseframes and syntac-tic rules are pro-compiled into structures called Knowl-edge Sourcee (KSs).
Each KS owns the syntactic and se-mantic competence necessary to perform a well-formedinterpretation of a fragment of the input.
Fig.
1 showsa simple caseframe, represented via Conceptual Graphs\[Sown 84\], and a simplified view of the resulting KS ob-tained by combining it with two rules of a DependencyGrammar \]Hays 64\].
The dependency rules are aug-mented wlth information about the functional role of theimmediate constituents; this information id used by theoffline compiler as a mapping between syntax and seman-tics necessary to automatically generate the KS.
The KSaccounts for sentences like "Da quale monte haste i1 Te-vere?"
("From which mount does the Tevere originate?
").The Composition part represents a way of grouping aphrase having a MOUNT type header satisfying the Ac-tivation Condition and a phrase having a RIVER typeheader.
The Constraints part contains checks to be per-formed whenever the KS is operating.
The Meaning partallows to generate the meaning representation starting1970F-24\ [TO-HAVE-SOURCE\ ]--* (AGNT:Oompulsory) --+ RIVER--~ (LOC:Compulsory) --~ \ [MOUNT\ ]DR-12.1VERB(prop) = NOUN(interr-indlr-loe) <GOVERNOR> NOUN(subJ);; Features and Agreements<GOVERNOR> (MOOD ind) (TENSE pres) (NUMBER _x) ....NOUN- I  ....NOUN-2 (NUMBER _x) ....DR-12.2VERB(prop) = NOUN(interr-indir-loc) <GOVERNOR> PROP-NOUN(~ubJ)DeIKS KS-24.12;;CompositionTO-HAVE-SOURCE = MOUNT <HEADER> R IVER;;Constraints<HEADER>-MOUNT ((H-cat VERB)  (S-eat NOUN)  (H-feat MOOD ind TENSE pre .... ) ...)<HEADER>-R IVER ...............;;Header Activation ConditionACTION (TO-HAVE-SOURCE);;Meaning(TO-HAVE-SOURCE I * agnt i los 0)Figure 1: A caseframe (expressed in CG notation), two dependency rules and a corresponding KS.rfype: TO-HAVE-SOURCEHeader.
"NASCK"Left: MOUNT Right: RIVER /Type: MOUNTHeader: "MONTE"?
JOE / Right: none#(to be solved)Type: RIVERHeader: "TEV E FIE"Left: JOLLY Right: none-IL- \[missing\]Figure 2: An example of DI.from the meaning of the component phrases.3.2 ParsingEach of the phrase hypotheses generated by KSs dur-ing parsing relates to an utterance fragment and is calledDeduction/natance (DI).
Dis are an extension of the is-land concept in the HWIM system \[Woods 82\].
A DI issupported by word hypotheses and has a tree structurereflecting the compositional constraints of the KSs thatbuilt it.
It has a score computed by combining the scoreof the word hypotheses supporting it.
A simplified viewof a DI is shown in Fig.
2.
That DI refers to the sentence"Da quale monte nasce il 'revere?"
("From which mountdoes the Tevere originate?
"); its root has been built bythe KS of Fig.
1, and two more KSs were required tobuild the rest of it.
The tree structure of the DI reflectsthe compositional structure of the KSs.
The bottom-leftpart of the picture shows that there are two types (SPECand JOLLY) that correspond to phrases that have still tobe detected.
Such 'empty' nodes are called goa/a.
SPEGwill account for the phrase "Quale" ("Which"); JOLLYrepresents he need of a preposition that might be missingfrom the lattice (this aspect is discussed later).Parsing is accomplished by selecting the best-scoredDI or word hypothesis in the lattice and letting it to beaccreted by all of the KSs that can do the job.
Suchopportunistic score-guided search results in top-down,'expectation-based' actions that are dynamically mixedwith bottom-up, 'predictive' actions.
The actions of KSson Dis are described by operators.Top-down act ions consist in starting from a DI havinga goal, and:1. if it is a header slot, solve it with a word hypothesis(VERIFY operator);2. if it is a case-filler slot,?
solve it with already existing complete Dis(MERGE), or?
decompose it according to a KS knowledgecontents (SUBGOALING).Bot tom-up  act ions  consist in creating a new DI start'ing either1.
from a word hypothesis, which will occupy theheader slot of the new DI (ACTIVATION), or2.
from a complete DI, which will occupy a case-filler(PREDICTION).~198 ,Such a strategy is opportunistic, since the element onwhich the KSs will work is selected according to its score,and the actions to be performed on it are determinedsolely by its characteristics.The activity of the operators is mainly concernedwith the p:eopagation of constraints to the goal nodes ofeach newly-created DL Constraints are propagated froma father to a son or vice-versa ccording to the currentparsing direction.
They consist in:Time intervals, in the form of start and end ranges;Morphological information, used to check agree-men~s inside the DI;Fun(:tional information, used to verify the correct-ness of the grammatical relations that are beingestablished within the DI;Semantic type information.
This information isused when, unlike the case of Fig.
1, more thanone caseframe are represented by a single KS (theoffih~e compiler may decide to do this if the case-frames are similar and the consequent estimated re-duction of redundancy appears ufficiently great).In such a situation compliance with the single case-flames may have to be checked, hence the reasonfor this type of information.4 Deal ing with missing shortwordsAs was pointed out, there are many different kinds ofwords thai.
are short.
In general, their semantic relevancedepends on the linguistic representation a d on the cho-sen domain.
If the words are determiners, prepositionsor auxiliary verbs, however, the integration of syntax andsemantics outlined above makes them irrelevant in mostcases, as very often it allows to infer them from the otherwords of the sentence.
Such an inference may result notpossible (mainly when prepositions are concerned), or theword may belong to other categories, uch as connectives("and", "or") or proper nouns, which are short but whosesemantic relevance is out of question; in these cases thesystem must react exactly as to the lack of a 'normal ~word.Let us call 'jollies' the types of word for which onlya functlonal role is acknowledged.
Jollies are consideredmerely as ~yntactlc markers for constituents to which theydo not offer a meaning contribution per se.
The pursuedgoal is twofold:1.
Par~3ing must be enabled to proceed without themin most cases;2.
However~ whenever possible and useful, one wishto exploit their contribution i terms of time con-straint and score (remember that there are also~long' jollies, much more reliable than short ones).The general philosophy is 'ignore a jolly unless thereis substantial reasons to consider it'.
The proposed solu-tion is as follows:1.
Jollies are represented asterminal slots in the com-positional part of a KS, like headers.
There can besyntactic and even semantic onstraints on them,but they do not enter into the rule describing themeaning representation.2.
Since we assume that jollies have no semantic pre-dictive power, all of the operators are inhibited tooperate on them.3.
Another top-down operator, JVERIFY, is added tosolve jolly slots, acting only when a DI has enoughsupport from other 'significant' word hypotheses.Fig.
3 shows a KS deriving from the same caseffame ofFig.
1 but from a different dependency rule.
Such a KStreats sentences like "Da quale monte si orlgina il Tenvere?"
("From which mount does the Tevere originate?
"),in which the word "si" is a marker for verb reflexivity.The way JVERIFY operates depends on the re-sult of a predicate, JOLLY-TYPE, applied on the jollyslot.
JOLLY-TYPE has three possible values: SttORT-OR-UNESSENTIAL, LONG-OR-ESSENTIAL, and UN-KNOWN that depend on various factors, including tllelexical category assigned to the jolly slot, the temporal,morphologlc and semantic onstraints imposed on thatslot by other word hypotheses, and the availability of suchdata.
if the returned value is LONG-OR-ESSENTIAL,then the jolly must be found in the lattice, and it~ losscauses parsing to react in a way exactly similar as to theloss of any other 'normal' word.
Conversely, if the valueis SHORT-OR-UNESSENTIAL, the jolly is ignored byplacing a suitable temporal ~hole ~in the slot pf the DI.The hole has relaxed temporal boundaries so as not toimpose too strict a constraint o11 the position of wordsthat can fill adjacent slots; thresholds are used for thispurpose.
Finally, if the value is UNKNOWN, an actionlike the previous one is done, followed by a limited searchin the lattice, looking for words exceeding the maximumwidth of the 'hole'.
Such a search is necessary because itinsures that parsing does not fail when tim correct wordis a jolly larger than the 'hole'.
JVERIFY is submitted tothe standard scheduling just as the other operators are.5 Exper imental  resultsThe above ideas have been implemented in a parser calledSYNAPSIS (from SYNtax-Aided Parser for Semantic In-terpretation of Speech).
SYNAPSIS is an evolution ofthe parser included in the SUSY system for understand-ing speech and described in \[Poesio 87\].
SYNAPSIS hasbeen implemented in Common Lisp and relies on about150 KSs, able to handle a 1011-word lexicon on a re-stricted semantic domain.
An idea of the lingulstic ov-erage is given by the equivalent branching factor, which is199D1%-13.1VEP~B(prop) = NOUN(interr-indir-loc) REFLEX <GOVERNOR,> NOUN(subj);; Features and Agreements<GOVERNOR> (MOOD ind) (TENSE pres) (NUMBER .x) ....NOUN-1 ....I%EFLEX nilNOUN-2  (NUMBER ..x) ....DefKS KS-24.13;;CompositionTO-HAVE-SOURCE= MOUNT <JOLLY> <HEADER> RIVER; ;Meaning(TO-HAVE-SOURCE !
* agnt 1 ioc O)Figure 3: A KS with a jol ly field.I I I"  ,00, 1111 sentences  present ,1,3 .
1 \[ 18 \[ 0 \[ O \]i i 0 b,  lattices pre*snt 15 23 66 65 'l" l \] 11 \[Table 1: Jol ly word detection.miss|'ngjolly wordsper sentencen.
ofsentencessuccessfullyparsedaverage n. ofgenerated Dis1 2 340 18 435 15 3318 440 563about 35.
The system has been tested with 150 word lat-tices generated by processing as many sentences utteredin continuous peech with natural intonation in a normaloffice environment.
The overall performance results inabout 80% correct sentence understanding \[Fissore 88\].The thresholds for JVERIFY have been experimen-tally determined to minimize the computational load,represented by the average number of Dis generated ur-ing each parsing.
Tab.
1 shows the number of jolly wordsthat have been skipped by the parser vs. the numberof jollies actually missing in the corresponding lattices.The former figures are higher than the latter, indicatingthat many words, albeit present, have been discarded byJVERIFY because of their bad acoustical scores or theirscarce contribution to contraint propagation.The most apparent advantage of the above techniqueis the increase in the number of sentences that can be an-alyzed without querying the user for lacking information.Tab.
2 displays the number of lattices, corresponding tothe sentences containing at least one word of jolly type,in which some of such words are missing.
It is seen thatabout 75~ of them have been successfully understood.This  figure does not change substantially as the numberof missing jollies per sentence increases, and hence indi-cates robustness.
The computational load, given by thenumber of generated Dis, is somewhat affected by thenumber of missing jollies.
However, this is mainly due tothe fact that sentences with many jollies are also longerTable 2: Successful parsingand syntactically complex.
The actual efficiency can bebetter estimated from Fig.
4, where the average numberof generated Dis is plot as a function of the threshold onthe width of the jolly temporal 'hole'.
The figure displaysalso the amount of parsing failures related to jolly prob-lems (failures due to other reasons have been ignored forsimplicity).
The curve indicates that raising the thresh-old does not change much the number generated Dis (therelative oscillations of the values are small).
This meansthat the relaxation of constraints during the applicationof JVERIFY is not a source of inefficiency.
Moreover,there is a large range of values for which the parsing fail-ure remains low.The curve also shows that relaxing constraints may evenspeed up the parsing.
This can be easily explained.
Whenthe threshold is low, no jolly is skipped, and failure oc-curs when jollies are missing from the lattice.
When thethreshold is raised, skipping begins to work: good-scoredfalse jollies are no more a source of disturbance, and cor-rect but bad-scored jollies are skipped thus avoiding todelay the parsing; as a consequence the overall numberof Dis decreases.
Further enlarging the threshold revertsthis tendency, since the too-much-relaxed constraints al-low the aggregation of words that would have been dis-carded with stricter constraints; failures occur when oneof such aggregations makes up a complete parse scoring200Average ni l  ill|)el"of generated Dis(relative ut?its)k10.95 -~0.90.85 --Percentage ofillc.Orl'C ct\[)Is mldcrs tand ing% N. I Y)~-_ .
(  ' .
~{ %25%y : k  -- 0 .... .
.
0 f - I -  0%_Lk .
.
.
.
.
.
t - .
- - t - - - -  k .
.
.
.
.
.
.
.
v .
+- - -h  -~~1 10 15 20 211 3{i 35 Thresho ld  onhole w idth{time l i 'ames)F igure  4: Per fo rmance  vs. width threshold.better th~a the correc~ one.6 ( Jonc lnsmns  and  l inks w i thcur rent  researchExperimer, ts show that the presence of jolly slots solvableas described above, beside permitt ing to successfully an-aly~e a much greater quota of word lattices, also speedsup parsing preventing it from being misled by false jollies.This well ::ompensatee for the growth of the inferential~tctivity dlte to the relaxed temporM constraints in theDis contMning ~holes'.
As a consequence it is possible~o use KS having chains of two or even three adjacentjolly slots without compromising excessively the globalperformai,,:es.
This is a novel improvement over systemsthat, to our knowledge, only admit one single skippableword and use a more rigid linguistic knowledge repre-sentation \[Tomita 87\] or recognize any configuration ofmissing words but do not distinguish cases in which theinformation content of an absent word cart be ignored\[Goerz 83\].An attract ing feature of the present parsing tech-nique is th;~t the KS activities are modularized into a setof operators.
Consequently, it remains open to 'local' im-provement~, on single operators as well as to overall heuris-tic adjustments on the score-guided control strategy.
Asaa exampi~, the response of the predicate JOLLY-TYPEof the oper~tor JVER3FY may be rendered more 'intelli-gent' by exploiting further information, such as estimatesof the expected word length~ that  has not been kept intoconsideration in the present implementation.A diff,!rent philosophy arising in very recent speechunderstanding research developments entrusts the prob-lem of solving troublesome portions of the utterance(including those were jol l ies were not found) to adeeper ,%eoustlcal analysis guided by linguistic expecta-tion \[Niedermair 87\].
Our approach is not in conflict,but rather, complementary to it.
We believe that corn-bining the two approaches would lead to a research areathat should turn very fruitful in producing robust speechparsing.The authors wi~h to ezpress their gratitude to their colleague, thelate Dr. SuBs, for Id8 contribution to the develotnnent ofthe system.References\[Briet~.mann 86\] A.Brietzmann, U.Ehrlich, "The role of se-mantic processing ill an automatic speech understandingsystem", Pros COL1NG 86, Bonn.\[Fillmore 68\] C.J.Fillmore, "The case for case", in Bach~ Har-ris (eds.
), Universals in Linguistic Theoryl Itolt, Rine-hart, and Winston, New York, 1968.\[Fissore 88\] L. Fissore, E. Giachin, P. Laface, G. Micca, R.Pieraccini, C. Rullent, "Experimental results on large-vocabulary speech recognition and understanding", Proc.ICASS"P 88, New York.\[Gemello87\] R. Gemello, E. Giachln~ C. Rullent, "Aknowledge-based framework for effective probabilisticcontrol strategies in signal understanding", Prvc.
GWAI87, Springer Verlag ed.\[Goerv.
83\] G.Goerz, C.Beckstein, "How to parse gaps in spo-ken utterances", Proc.
1~t Conf.
Europ.
CnapL ACL.\[Hayes 86\] P.J.
tIayes, A.G. Hauptmann, J.G.
Carbonell, M.Tomita~ "Parsing spoken language: a semantic aseframeapproach", Prec.
COL1NG 86, Bonn.\[Itays 64\] D.G.Hays, "Dependency theory: a formalism andsome observations", Memorandum RM4087 P.R., TheRand Corporation.\[Hinrichs 86\] E.W.ttinrichs, "A compositional semantics fordirectional modifiers", Proc.
COLING 86, Bonn.\[Laface 87\] P.Laface, G.Micca, R.Pieraceini, "Experimentalresults on a large lexicon access task", Proc.
ICASSP 87,Dallas.\[Lesmo 85\] L.Lesmo, P.Torasso, "Weighted interaction of syn-tax and semantics in natural language analysis", Pro?.I JCAI85, Los Angeles.\[Kaplan 82\] S.J.Kaplan, "Cooperative responses from aportable natural language query system", Artificial Intel-ligence 19, 1982.\[Niedermair 87\] G.T.Niedermair, "Merging acoustics and lin-guistics in speech understanding", NATO ASI-Conference~Bad Windsheim.\[Poesio 87\] M.Poesio, C.Rullent, "Modified caseframe parsingfor speech understanding systems", Proc.
I JCAI 87, Mi-lano.\[Schank 75\] R.Schank, Conceptual Information Processing,North-ttolIand, New York, 1975.\[Sows 84\] J.F.Sowa, Conceptual Structures, Addison Wesley,Reading (MA), 1984.\[Tomita 87\] M.Tomita, "An efficient augmented-context-freeparsing algorithm", ComputationalLinguistic$, Vol.
13, n.1-2, Jan-June 1987.\[Woods82\] W.A.Woods, "Optimal search strategies forspeech understanding control", Artificial Intelligence 18,1982.201
