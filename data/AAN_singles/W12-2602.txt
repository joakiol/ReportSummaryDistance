Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 10?18,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUsing the Omega Index for Evaluating Abstractive Community DetectionGabriel MurrayComputer Information SystemsUniversity of the Fraser Valleygabriel.murray@ufv.caGiuseppe CareniniComputer ScienceUniversity of British Columbiacarenini@cs.ubc.caRaymond NgComputer ScienceUniversity of British Columbiarng@cs.ubc.caAbstractNumerous NLP tasks rely on clustering orcommunity detection algorithms.
For manyof these tasks, the solutions are disjoint, andthe relevant evaluation metrics assume non-overlapping clusters.
In contrast, the relativelyrecent task of abstractive community detection(ACD) results in overlapping clusters of sen-tences.
ACD is a sub-task of an abstractivesummarization system and represents a two-step process.
In the first step, we classify sen-tence pairs according to whether the sentencesshould be realized by a common abstractivesentence.
This results in an undirected graphwith sentences as nodes and predicted abstrac-tive links as edges.
The second step is toidentify communities within the graph, whereeach community corresponds to an abstrac-tive sentence to be generated.
In this paper,we describe how the Omega Index, a met-ric for comparing non-disjoint clustering so-lutions, can be used as a summarization eval-uation metric for this task.
We use the OmegaIndex to compare and contrast several commu-nity detection algorithms.1 IntroductionAutomatic summarization has long been proposedas a helpful tool for managing the massive amountsof language data in our modern lives (Luhn, 1958;Edmundson, 1969; Teufel and Moens, 1997; Car-bonell and Goldstein, 1998; Radev et al, 2001).Most summarization systems are extractive, mean-ing that a subset of sentences from an input docu-ment forms a summary of the whole.
Particular sig-nificance may be attached to the chosen sentences,e.g.
that they are relevant to a provided query, gen-erally important for understanding the overall doc-ument, or represent a particular phenomenon suchas action items from a meeting.
In any case, ex-traction consists of binary classification of candidatesentences, plus post-processing steps such as sen-tence ranking and compression.
In contrast, recentwork attempts to replicate the abstractive nature ofhuman-authored summaries, wherein new sentencesare generated that describe the input document froma higher-level perspective.
While some abstractivesummary sentences are very similar to individualsentences from the document, others are createdby synthesizing multiple document sentences intoa novel abstract sentence.
In this paper, we ad-dress a component of this latter task, namely iden-tifying which sentences from the source documentsshould be combined in generated abstract sentences.We call this task abstractive community detection(ACD), and apply the task to a publicly availablemeeting dataset.Herein we focus on describing how the OmegaIndex (Collins and Dent, 1988), a metric for com-paring non-disjoint clustering solutions, can be usedas a summarization evaluation metric for the ACDtask.
Metrics such as the Rand Index (Rand, 1971)are insufficient since they are intended only for dis-joint clusters.ACD itself is carried out in two steps.
First, weclassify sentence pairs according to whether theyshould be realized by a common abstractive sen-tence.
For this step, we use supervised machinelearning that exploits human-annotated links be-tween abstracts and extracts for a given document.This results in an undirected graph with nodes repre-senting sentences and edges representing predictedabstractive links.
Second, we identify communi-ties within the graph, where each community cor-responds to an abstractive sentence to be generated.We experiment with several divisive community de-10tection algorithms, and highlight the importance ofselecting an algorithm that allows overlapping com-munities, owing to the fact that a document sentencecan be expressed by, and linked to, more than oneabstract summary sentence in the gold-standard.The structure of the paper is as follow.
In Sec-tion 2, we compare and contrast ACD with otherrelevant tasks such as extractive summarization andtopic clustering.
In Sections 3-4, we describe thetwo ACD steps before we can fully discuss evalua-tion methods.
Section 5 describes the experimentalsetup and corpora used, including a description ofthe abstractive and extractive summary annotationsand the links between them.
In Section 6, we give adetailed description of the Omega Index and explainhow it differs from the more common Rand Index.In Sections 7-8 we present results and draw conclu-sions.2 Related WorkThe ACD task differs from more common extrac-tive summarization (Mani, 2001a; Jurafsky and Mar-tin, 2008).
Whereas extraction involves simply clas-sifying sentences as important or not, ACD is asub-task of abstractive summarization wherein doc-ument sentences are grouped according to whetherthey can be jointly realized by a common abstrac-tive sentence.
The first step of ACD, where we pre-dict links between sentence pairs, can be seen to en-compass extraction since the link is via an as-yet-ungenerated abstract sentence, i.e.
each linked sen-tence is considered summary-worthy.
However, thesecond step moves away from extraction by cluster-ing the linked sentences from the document in orderto generate abstract summary sentences.ACD also differs from topic clustering (Malioutovand Barzilay, 2006; Joty et al, 2010), though thereare superficial similarities.
A first observation is thattopic links and abstract links are genuinely differ-ent phenomena, though sometimes related.
A sin-gle abstract sentence can reference more than onetopic, e.g.
They talked about the interface designand the budget report, and a single topic can bereferenced in numerous abstract sentences.
From apractical standpoint, in our work on ACD we can-not use many of the methods and evaluation metricsdesigned for topic clustering, due to the fact that adocument sentence can belong to more than one ab-stract sentence.
This leads to overlapping commu-nities, whereas most work on topic clustering hasfocused primarily on disjoint communities where asentence belongs to a single topic.
In Section 4, wediscuss community detection algorithms and evalu-ation metrics that allow overlapping communities.Work on detecting adjacency pairs (Shriberg etal., 2004; Galley et al, 2004) also involves classify-ing sentence pairs as being somehow related.
For ex-ample, if sentence B directly follows sentence A, wemight determine that they have a relationship suchas question-answer or request-accept.
In contrast,with ACD there is no requirement that sentence pairsbe adjacent or even in proximity to one another, normust they be in a rhetorical relation.Work on sentence fusion (Barzilay and McKe-own, 2005) identifies sentences containing similaror repeated information and combines them intonew sentences.
In contrast, in our task sentencesneed not contain repeated information in order to belinked.
For example, two sentences could be linkedto a common abstract sentence due to a more com-plex rhetorical relationship such as proposal-rejector question-answer.ACD is a more general problem that may incor-porate elements of topic clustering, adjacency pairdetection and other sentence clustering or pairingtasks.
Here we try to directly learn the abstrac-tive sentence links using lower-level features such asshared n-grams and cosine similarity, as described inSection 3, but in future work we will model higher-level features of topics and rhetorical structure.3 Step 1: Building a Sentence Pair GraphIn order to describe the use of the Omega Index forthe ACD task, we must first introduce the ACD taskin some detail.
The first step in ACD is to determinewhich sentence pairs are linked.
If two sentences arelinked, it means they can be at least partly realizedin the abstract summary by a common sentence.
Adocument sentence may ?belong?
to more than oneabstract sentence.
We take a supervised classifica-tion approach to this problem, training on a datasetcontaining explicit links between extract sentencesand abstract sentences.
The corpus and relevant an-notation are described in detail in Section 5.
For11Figure 1: Linked Sentencesour gold-standard data, a sentence pair is consideredlinked if both sentences are linked to a common ab-stract sentence and not-linked otherwise.Figure 1 shows an example snippet of linked sen-tences from our corpus.
The first and second sen-tences are linked via one abstract sentence while thefirst and third sentences are linked via a different ab-stract sentence.
While it is not shown in this exam-ple, note that two sentences can also be linked viamore than one abstract sentence.We take a supervised machine learning approachtoward predicting whether a sentence pair is linked.For each pair, we extract features that can be classedas follows:?
Structural: The intervening number of sen-tences, the document position as indicated bythe midpoint of the two sentences, the com-bined length and the difference in length be-tween the two sentences, and whether the twosentences share the same speaker.?
Linguistic: The number of shared bigrams,shared part-of-speech tags, the sum and aver-age of tf.idf weights, and the cosine similarityof the sentence vectors.We run the trained classifier over sentence pairs,predicting abstractive links between sentences in thedocument.
This results in an unweighted, undirectedgraph where nodes represent sentences and edgesFigure 2: Graph with Sentence Nodesrepresent an abstractive link.
Continuing with theconversation snippet from Figure 1, we would endup with a graph like Figure 2.
This very simpleexample of a graph shows that there are abstractivelinks predicted between sentences s1 and s2 and be-tween sentences s1 and s3.
There is no direct linkpredicted between sentences s2 and s3.
However,it is possible for two sentences with no predictedlink between them to wind up in the same abstractivecommunity after running a community detection al-gorithm on the graph.
We discuss this communitydetection step in the following section.4 Step 2: Discovering AbstractiveSentence CommunitiesIn the first step of ACD, we predicted whether pairsof sentences can be at least partly realized by a com-mon abstractive sentence.
We then want to identifycommunities or clusters within the graph.
Each ofthese communities will correspond to an abstractive12Figure 3: Overlapping Communities in Graphsentence that we will generate.
Continuing with oursimple example, Figure 3 shows two communitiesthat have been identified in the graph.
Note thatthe communities are overlapping, as each containssentence s1; we would generate one abstractive sen-tence describing sentences s1 and s2 and another de-scribing sentences s1 and s3.
We will return to thiscritical issue of overlapping communities shortly.The task of identifying communities in networksor graphs has received considerable attention (Porteret al, 2009).
The Girvan-Newman algorithm (Gir-van and Newman, 2002) is a popular community de-tection method based on a measure of betweenness.The betweenness score for an edge is the number ofshortest paths between pairs of nodes in the graphthat run along that edge.
An edge with a high be-tweenness score is likely to be between two commu-nities and is therefore a good candidate for removal,as the goal is to break the initial graph into distinctcommunities.
The Girvan-Newman algorithm pro-ceeds as follows:1.
Calculate the betweenness of each edge in thegraph.2.
Remove the edge with the highest betweenness.3.
For any edge affected by Step 2, recalculate be-tweenness.4.
Repeat steps 2 and 3 until no edges remainIn this way we proceed from the full graph with alledges intact to the point where no edges remain andeach node is in its own community.
The intermediatesteps can be visualized by the resulting dendrogram,such as seen in Figure 4 1.The top row, the ?leaves?
of the dendrogram, rep-resents the individual nodes in the graph.
The rest1Image Source: Wikimedia Commons (Mhbrugman)Figure 4: Community Dendrogramof the dendrogram shows how these nodes are sit-uated in nested communities, e.g.
b and c form acommunity bc that combines with def to form bcdef.In our case, where nodes are sentences, the dendro-gram shows us how sentences combine into nestedcommunities.
This can be useful for generating ab-stracts of different granularities, e.g.
we could de-scribe bcdef in one sentence or generate two sen-tences to separately describe bc and def.The drawback of Girvan-Newman for our pur-poses is that it does not allow overlapping commu-nities, and we know that our human-annotated datacontain overlaps.
Note from Figure 4 that all com-munities decompose into disjoint nested communi-ties, such as bcdef being comprised of bc and def,not bc and bdef or some other overlapping case.We therefore hypothesize that Girvan-Newman in itstraditional form is not sufficient for our current re-search.
For the same reason, recent graph-based ap-proaches to topic clustering (Malioutov and Barzi-lay, 2006; Joty et al, 2010) are not directly applica-ble here.It is only in recent years that much attention hasbeen paid to the problem of overlapping (or non-disjoint) communities.
Here we consider two recentmodifications to the Girvan-Newman algorithm thatallow for overlaps.
The CONGA algorithm (Gre-gory, 2007) extends Girvan-Newman so that insteadof removing an edge on each iteration, we eitherremove an edge or copy a node.
When a node iscopied, an overlap is created.
Nodes are associatedwith a betweenness score (called the split between-ness) derived from the edge betweenness scores, andat each step we either remove the edge with the high-est betweenness score or copy the node with the13Figure 5: CONGA algorithmhighest split betweenness, if it is greater.
The edgeand node betweenness scores are then recalculated.In such a manner we can detect overlapping com-munities.
Figure 5 shows the CONGA copying andsplitting operations applied to our simple example,so that sentence s1 now exists in two communities.The CONGO algorithm (Gregory, 2008) is an ap-proximation of CONGA that is more efficient forlarge graphs.
Girvan-Newman (and hence CONGA)are not feasible algorithms for very large graphs, dueto the number of repeated betweenness calculations.CONGO addresses this problem by using local be-tweenness scores.
Instead of calculating between-ness using the shortest paths of every pair of nodesin the graph, only nodes within a given horizon h ofan edge are considered.
When h =?
then CONGOand CONGA are identical.
Gregory (Gregory, 2008)found good results using h = 2 or h = 3 on a va-riety of datasets including blog networks; here weexperiment h = 2.For the community detection step of our system,we run both CONGA and CONGO on our graphsand compare our results with the Girvan-Newmanalgorithm.
For all community detection methods,as well as human annotations, any sentences thatare not linked to at least one other sentence in Step1 are assigned to their own singleton communities.Also, the algorithms we are evaluating are hierarchi-cal (see Figure 4), and we evaluate at n = 18 clus-ters, since that is the average number of sentencesper abstractive meeting summary in the training set.5 Experimental SetupIn this section we describe the dataset used, includ-ing relevant annotations, as well as the statisticalclassifiers used for Step 1.5.1 AMI CorpusFor these experiments we use the AMI meeting cor-pus (Carletta, 2006), specifically, the subset of sce-nario meetings where participants play roles withina fictional company.
For each meeting, an annotatorfirst authors an abstractive summary.
Multiple an-notators then create extractive summaries by linkingsentences from the meeting transcript to sentenceswithin the abstract.
This generates a many-to-manymapping between transcript sentences and abstractsentences, so that a given transcript sentence canrelate to more than one abstract sentence and vice-verse.
A sample of this extractive-abstractive linkingwas shown in Figure 1.It is known that inter-annotator agreement can bequite low for the summarization task (Mani et al,1999; Mani, 2001b), and this is the case with theAMI extractive summarization codings.
The aver-age ?
score is 0.45.In these experiments, we use only human-authored transcripts and plan to use speech recog-nition transcripts in the future.
Note that our overallapproach is not specific to conversations or to speechdata.
Step 2 is completely general, while Step 1 usesa single same-speaker feature that is specific to con-versations.
That feature can be dropped to make ourapproach completely general (or, equivalently, thatbinary feature can be thought of as always 1 whenapplied to monologic text).5.2 ClassifiersFor Step 1, predicting abstractive links between sen-tences, we train a logistic regression classifier us-ing the liblinear toolkit2.
The training set consistsof 98 meetings and there are nearly one million sen-tence pair instances since we consider every pairingof sentences within a meeting.
The test set consistsof 20 meetings on which we perform our evaluation.6 Evaluation MetricsIn this section, we present our evaluation metrics forthe two steps of the task.6.1 Step 1 Evaluation: PRF and AUROCFor evaluating Step 1, predicting abstractive sen-tence links, we present both precision/recall/f-score2http://www.csie.ntu.edu.tw/ cjlin/liblinear/14as well as the area under the receiver operator char-acteristic curve (AUROC).
While the former scoresevaluate the classifier at a particular posterior proba-bility threshold, the AUROC evaluates the classifiermore generally by comparing the true-positive andfalse-positive rates at varying probability thresholds.6.2 Step 2 Evaluation: The Omega IndexFor evaluating Step 2, ACD, we employ a metriccalled the Omega Index which is designed for com-paring disjoint clustering solutions.
To describe andmotivate our use of this metric, it is necessary to de-scribe previous metrics upon which the Omega In-dex improves.
The Rand Index (Rand, 1971) is away of comparing disjoint clustering solutions thatis based on pairs of the objects being clustered.
Twosolutions are said to agree on a pair of objects if theyeach put both objects into the same cluster or eachinto different clusters.
The Rand Index can then beformalized as(a+ d)/Nwhere N is the number of pairs of objects, a isthe number of times the solutions agree on puttinga pair in the same cluster and d is the number oftimes the solutions agree on putting a pair in differ-ent clusters.
That is, the Rand Index is the number ofpairs that are agreed on by the two solutions dividedby the total number of pairs.
The Rand Index is in-sufficient for overlapping solutions because pairs ofobjects can exist together in more than one commu-nity.
In those cases, two solutions might agree onthe occurrence of a pair of objects in one commu-nity but disagree on the occurrence of that pair inanother community.
The Rand Index cannot capturethat distinction.An improvement to the Rand Index is the Ad-justed Rand Index (Hubert and Arabie, 1985) whichadjusts the level of agreement according to the ex-pected amount of agreement based on chance.
How-ever, the Adjusted Rand Index also cannot accountfor disjoint solutions.The Omega Index (Collins and Dent, 1988) buildson both the Rand Index and Adjusted Rand Indexby accounting for disjoint solutions and correctingfor chance agreement.
The Omega Index considersthe number of clusters in which a pair of objects istogether.
The observed agreement between solutionsis calculated byObs(s1, s2) =min(J,K)?j=0Aj/Nwhere J and K represent the maximum number ofclusters in which any pair of objects appears togetherin solutions 1 and 2, respectively, Aj is the numberof the pairs agreed by both solutions to be assignedto number of clusters j, and N is again the numberof pairs of objects.
That is, the observed agreementis the proportion of pairs classified the same way bythe two solutions.
The expected agreement is givenby:Exp(s1, s2) =min(J,K)?j=0Nj1Nj2/N2where Nj1 is the total number of pairs assignedto number of clusters j in solution 1, and Nj2 is thetotal number of pairs assigned to number of clustersj in solution 2.
The Omega Index is then calculatedasOmega(s1, s2) =Obs(s1, s2)?
Exp(s1, s2)1?
Exp(s1, s2)The numerator is the observed agreement adjustedby expected agreement, while the denominator ismaximum possible agreement adjusted by expectedagreement.
The highest possible score of 1 indicatesthat two solutions perfectly agree on how each pairof objects is clustered.
With the Omega Index, wecan now evaluate the overlapping solutions discov-ered by our community detection algorithms.37 ResultsIn this section we present the results for both stepsof ACD.
Because the Omega Index is not used forevaluating Step 1, we keep that discussion brief.7.1 Step 1 Results: Predicting AbstractiveSentence LinksFor the task of predicting abstractive links withinsentence pairs, the resulting graphs have an aver-age of 133 nodes and 1730 edges, though this varies3Software for calculating the Omega Index will be releasedupon publication of this paper.15System Prec.
Rec.
F-Score AUROCLower-Bound 0.18 1 0.30 0.50Message Links 0.30 0.03 0.05 -Abstractive Links 0.62 0.54 0.54 0.89Table 1: P/R/F and AUROCs for Link Predictionwidely depending on meeting length (from 37 nodesand 61 edges for one short meeting to 224 edges and5946 edges for a very long meeting).
In compar-ison, the gold-standard graphs have an average of113 nodes and 1360 edges.
The gold-standards sim-ilarly show huge variation in graph size dependingon meeting length.Table 1 reports both the precision/recall/f-scoresas well as the AUROC metrics.
We compare oursupervised classifier (labeled ?Abstractive Links?
)with a lower-bound where all instances are predictedas positive, leading to perfect recall and low preci-sion.
Our system scores moderately well on bothprecision and recall, with an average f-score of 0.54.The AUROC for the abstractive link classifier is0.89.It is difficult to compare with previous work since,to our knowledge, nobody has previously modeledthese extractive-abstractive mappings between doc-ument sentences and associated abstracts.
We cancompare with the results of Murray et al (2010),however, who linked sentences by aggregating theminto messages.
In that work, each message is com-prised of sentences that share a dialogue act type(e.g.
an action item) and mention at least one com-mon entity (e.g.
remote control).
Similar to ourwork, sentences can belong to more than one mes-sage.
We assess how well their message-based ap-proach captures these abstractive links, reportingtheir precision/recall/f-scores for this task in Table 1,with their system labeled ?Message Links?.
Whiletheir precision is above the lower-bound, the recalland f-score are extremely low.
This demonstratesthat their notion of message links does not capturethe phenomenon of abstractive sentence linking.7.2 Step 2 Results: Discovering AbstractiveCommunitiesFor the task of discovering abstractive communi-ties in our sentence graphs, Table 2 reports theOmega Index for the CONGA, CONGO and Girvan-Newman algorithms.
We also report the averageOmega Index for the human annotators themselves,derived by comparing each pair of annotator solu-tions for each meeting.It is not surprising that the Omega Index is low forthe inter-annotator comparison; we reported previ-ously that the ?
score for the extractive summaries ofthis corpus is 0.45.
That ?
score indicates that thereis high disagreement about which sentences are mostimportant in a meeting.
We should not be surprisedthen that there is further disagreement about how thesentences are linked to one another.
What is surpris-ing is that the automatic community detection al-gorithms achieve higher Omega Index scores thando the annotators.
Note that the higher scores ofthe community detection algorithms relative to hu-man agreement is not simply an artefact of identify-ing clustering solutions that have more overlap thanhuman solutions, since even the disjoint Girvan-Newman solutions are higher than inter-annotatorlevels.
One possible explanation is that the annota-tors are engaged in a fairly local task when they cre-ate extractive summaries; for each abstractive sen-tence, they are looking for a set of sentences fromthe document that relate to that abstract sentence,and because of high redundancy in the document thedifferent annotators choose subsets of sentences thathave little overlap but are still similar (Supportingthis, we have found that we can train on annotatorA?s extractive codings and test on annotator B?s andget good classification results even if A and B have alow ?
score.).
In contrast, the community detectionalgorithms are taking a more comprehensive, globalapproach by considering all predicted links betweensentences (Step 1) and identifying the overlappingcommunities among them (Step 2).When looking for differences between automaticand human community detection, we observed thatthe algorithms assigned more overlap to sentences16System OmegaGirvan-Newman 0.254CONGA 0.263CONGO 0.241Human 0.209Table 2: Omega Index for Community Detectionthan did the human annotators.
For example, theCONGA algorithm assigned each sentence to an av-erage of 1.1 communities while the human annota-tors assigned each to an average of 1.04 communi-ties.
Note that every sentence belongs to at least onecommunity since unlinked sentences belong to theirown singleton communities, and most sentences areunlinked, explaining why both scores are close to 1.Comparing the algorithms themselves, we findthat CONGA is better than both Girvan-Newman(marginally significant, p = 0.07) and CONGO(p = 0.015) according to paired t-test.
We be-lieve that the superiority of CONGA over Girvan-Newman points to the importance of allowing over-lapping communities.
And while CONGO is an ef-ficient approximation of CONGA that can be usefulfor very large graphs where CONGA and Girvan-Newman cannot be applied, in these experiments thelocal betweenness used by CONGO leads to loweroverall scores.
Furthermore, our networks are smallenough that both CONGA and Girvan-Newman areable to finish quickly and there is therefore no needto rely on CONGO.Our Step 2 results are dependent on the qual-ity of the Step 1 results.
We therefore test howgood our community detection results would be ifwe had gold-standard graphs rather than the imper-fect output from Step 1.
We report two sets of re-sults.
In the first case, we take an annotator?s gold-standard sentence graph showing links between sen-tences and proceed to run our algorithms over thatgraph, comparing our community detection resultswith the communities detected by all annotators.
Inthe second case, we again take an annotator?s gold-standard graph and apply our algorithms, but thenonly compare our community detection results withthe communities detected by the annotator who sup-plied the gold-standard graph.
Table 3 shows bothsets of results.
We can see that the latter set containsSystem Omega OmegaAll Annots.
1 Annot.Girvan-Newman 0.445 0.878CONGA 0.454 0.896CONGO 0.453 0.894Table 3: Omega Index, Gold-Standard Graphsmuch higher scores, again reflecting that annotatorsdisagree with each other on this task.Given gold-standard sentence graphs, CONGAand CONGO perform very similarly; the differencesare negligible.
Both are substantially better than theGirvan-Newman algorithm (all p < 0.01).
This tellsus that it is necessary to employ community detec-tion algorithms that allow overlapping communities.These results also tell us that the CONGO algorithmis more sensitive to errors in the Step 1 output sinceit performed well using the gold-standard but worsethan Girvan-Newman when using the automaticallyderived graphs.8 ConclusionAfter giving an overview of the ACD task and ourapproach to it, we described how the Omega Indexcan be used as a summarization evaluation metric forthis task, and explained why other community de-tection metrics are insufficient.
The Omega Index issuitable because it can account for overlapping clus-tering solutions, and corrects for chance agreement.The main surprising result was that all of the com-munity detection algorithms have higher Omega In-dex scores than the human-human Omega scoresrepresenting annotator agreement.
We have offeredone possibe explanation; namely, that while the hu-man annotators have numerous similar candidatesentences from the document that each could belinked to a given abstract sentence, they may be sat-isfied to only link (and thus extract) a small repre-sentative handful, whereas the community detectionalgorithms work to find all extractive-abstractivelinks.
We plan to further research this issue, and po-tentially derive other evaluation metrics that betteraccount for this phenomenon.17ReferencesR.
Barzilay and K. McKeown.
2005.
Sentence fusion formultidocument news summarization.
ComputationalLinguistics, 31(3):297?328.J.
Carbonell and J. Goldstein.
1998.
The use of MMR,diversity-based reranking for reordering documentsand producing summaries.
In Proc.
of ACM SIGIRConference on Research and Development in Informa-tion Retrieval 1998, Melbourne, Australia, pages 335?336.J.
Carletta.
2006.
Unleashing the killer corpus: expe-riences in creating the multi-everything ami meetingcorpus.
In Proc.
of LREC 2006, Genoa, Italy, pages181?190.L.
Collins and C. Dent.
1988.
Omega: A general formu-lation of the rand index of cluster recovery suitable fornon-disjoint solutions.
Multivariate Behavioral Re-search, 23:231?242.H.
P. Edmundson.
1969.
New methods in automatic ex-tracting.
J. ACM, 16(2):264?285.M.
Galley, K. McKeown, J. Hirschberg, and E. Shriberg.2004.
Identifying agreement and disagreement in con-versational speech: Use of bayesian networks to modelpragmatic dependencies.
In Proc.
of ACL 2004.M.
Girvan and M.E.J.
Newman.
2002.
Communitystructure in social and biological networks.
Proc.
ofthe National Academy of Sciences, 99:7821?7826.S.
Gregory.
2007.
An algorithm to find overlap-ping community structure in networks.
In Proc.
ofECML/PKDD 2007, Warsaw, Poland.S.
Gregory.
2008.
A fast algorithm to find overlappingcommunities in networks.
In Proc.
of ECML/PKDD2008, Antwerp, Belgium.L.
Hubert and P. Arabie.
1985.
Comparing partitions.Journal of Classification, 2:193?218.S.
Joty, G. Carenini, G. Murray, and R. Ng.
2010.
Ex-ploiting conversation structure in unsupervised topicsegmentation for emails.
In Proc.
of EMNLP 2010,Cambridge, MA, USA.D.
Jurafsky and J. H. Martin, 2008.
Speech and Lan-guage Processing.
Prentice Hall.H.
P. Luhn.
1958.
The automatic creation of litera-ture abstracts.
IBM Journal of Research Development,2(2):159?165.I.
Malioutov and R. Barzilay.
2006.
Minimum cut modelfor spoken lecture segmentation.
In Proc.
of ACL2006, Sydney, Australia.I.
Mani, D. House, G. Klein, L. Hirschman, T. Firmin,and B. Sundheim.
1999.
The TIPSTER SUMMACtext summarization evaluation.
In Proc.
of EACL1999, Bergen, Norway, pages 77?85.I.
Mani.
2001a.
Automatic Summarization.
John Ben-jamin, Amsterdam, NL.I.
Mani.
2001b.
Summarization evaluation: Anoverview.
In Proc.
of the NTCIR Workshop 2 Meetingon Evaluation of Chinese and Japanese Text Retrievaland Text Summarization, Tokyo, Japan, pages 77?85.G.
Murray, G. Carenini, and R. Ng.
2010.
Generatingand validating abstracts of meeting conversations: auser study.
In Proc.
of INLG 2010, Dublin, Ireland.M.
Porter, J-P. Onnela, and P. Mucha.
2009.
Communi-ties in networks.
Notices of the American Mathemati-cal Society, 56:1082?1097.D.
Radev, S. Blair-Goldensohn, and Z. Zhang.
2001.
Ex-periments in single and multi-document summariza-tion using MEAD.
In Proc.
of DUC 2001, New Or-leans, LA, USA.W.M.
Rand.
1971.
Objective criteria for the evaluationof clustering methods.
Journal of the American Statis-tical Association, 66:846?850.E.
Shriberg, R. Dhillon, S. Bhagat, J. Ang, , and H. Car-vey.
2004.
The ICSI meeting recorder dialog act(MRDA) corpus.
In Proceedings of SIGdial Workshopon Discourse and Dialogue, Cambridge, MA, USA,pages 97?100.S.
Teufel and M. Moens.
1997.
Sentence extraction as aclassification task.
In Proc.
of ACL 1997, Workshop onIntelligent and Scalable Text Summarization, Madrid,Spain, pages 58?65.18
