R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
530 ?
541, 2005.?
Springer-Verlag Berlin Heidelberg 2005A Chunking Strategy Towards Unknown Word Detectionin Chinese Word SegmentationZhou GuoDongInstitute for Infocomm Research, 21 Heng Mui Keng Terrace, Singapore 119613zhougd@i2r.a-star.edu.sgAbstract.
This paper proposes a chunking strategy to detect unknown words inChinese word segmentation.
First, a raw sentence is pre-segmented into asequence of word atoms 1  using a maximum matching algorithm.
Then achunking model is applied to detect unknown words by chunking one or moreword atoms together according to the word formation patterns of the wordatoms.
In this paper, a discriminative Markov model, named MutualInformation Independence Model (MIIM), is adopted in chunking.
Besides, amaximum entropy model is applied to integrate various types of contexts andresolve the data sparseness problem in MIIM.
Moreover, an error-drivenlearning approach is proposed to learn useful contexts in the maximum entropymodel.
In this way, the number of contexts in the maximum entropy model canbe significantly reduced without performance decrease.
This makes it possiblefor further improving the performance by considering more various types ofcontexts.
Evaluation on the PK and CTB corpora in the First SIGHAN Chineseword segmentation bakeoff shows that our chunking approach successfullydetects about 80% of unknown words on both of the corpora and outperformsthe best-reported systems by 8.1% and 7.1% in unknown word detection onthem respectively.1   IntroductionPrior to any linguistic analysis of Chinese text, Chinese word segmentation is thenecessary first step and one of major bottlenecks in Chinese information processingsince a Chinese sentence is written in a continuous string of characters withoutobvious separators (such as blanks)  between  the words.
During the past two decades,this research has been a hot topic in Chinese information processing [1-10].There exist two major problems in Chinese word segmentation: ambiguityresolution and unknown word detection.
While n-gram modeling and/or word co-occurrence has been successfully applied to deal with the ambiguity problems [3, 5,10, 12, 13], unknown word detection has become the major bottleneck in Chinese1In this paper, word atoms refer to basic building units in words.
For example, the word ????
?
(computer) consists of two word atoms: ???
?
(computing) and ??
?
(machine).Generally, word atoms can either occur independently, e.g.
????
(computing), or onlybecome a part of a word, e.g.
???
(machine) in the word ?????
(computer).A Chunking Strategy Towards Unknown Word Detection 531word segmentation.
Currently, almost all Chinese word segmentation systems rely ona word dictionary.
The problem is that when the words stored in the dictionary areinsufficient, the system's performance will be greatly deteriorated by the presence ofwords that are unknown to the system.
Moreover, manual maintenance of a dictionaryis very tedious and time consuming.
It is therefore important for a Chinese wordsegmentation system to identify unknown words from the text automatically.In literature, two categories of competing approaches are widely used to detectunknown words 2 : statistical approaches [5, 11, 12, 13, 14, 15] and rule-basedapproaches [5, 11, 14, 15].
Although rule-based approaches have the advantage ofbeing simple, the complexity and domain dependency of how the unknown words areproduced greatly reduce the efficiency of these approaches.
On the other hand,statistical approaches have the advantage of being domain-independent [16].
It isinteresting to note that many systems apply a hybrid approach [5, 11, 14, 15].Regardless of the choice of different approaches, finding a way to automaticallydetect unknown words has become a crucial issue in Chinese word segmentation andChinese information processing in general.Input raw sentence:     ?
?
?
?
?
?
?
?
?.MMA pre-segmentation:  ?
?
??
?
??
??
.Unknown word detection: ??
??
?
????
.Zhang Jie      graduate    from       JiaoTong  University.Fig.
1.
MMA and unknown word detection by chunking: an exampleThis paper proposes a chunking strategy to cope with unknown words in Chineseword segmentation.
First, a raw sentence is pre-segmented into a sequence of wordatoms (i.e.
single-character words and multi-character words) using a maximummatching algorithm (MMA)3.
Then a chunking model is applied to detect unknownwords by chunking one or more word atoms together according to the word formationpatterns of the word atoms.
Figure 1 gives an example.
Here, the problem of unknownword detection is re-cast as chunking one or more word atoms together to form a newword and a discriminative Markov model, named Mutual Information IndependenceModel (MIIM), is adopted in chunking.
Besides, a maximum entropy model is appliedto integrate various types of contexts and resolve the data sparseness problem inMIIM.
Moreover, an error-driven learning approach is proposed to learn useful2Some systems [13,14] focus on proper names due to their importance in Chinese informationprocessing.3A typical MMA identifies all character sequences which are found in the word dictionary andmarks them as words.
Those character sequences, which can be segmented in more than oneway, are marked as ambiguous and a word unigram model is applied to choose the mostlikely segmentation sequence.
The remaining sequences, i.e.
those not found in thedictionary, are called fragments and segmented into single characters.
In this way, eachChinese sentence is pre-segmented into a sequence of single-character words and multi-character words.
For convenience, we call these single-character words and multi-characterwords in the output of the MMA algorithm as word atoms.532 G. Zhoucontexts in the maximum entropy model.
In this way, the number of contexts in themaximum entropy model can be significantly reduced without performance decrease.This makes it possible for further improving the performance by considering morevarious types of contexts in the future.
Evaluation on the PK and CTB corpora in theFirst SIGHAN Chinese word segmentation bakeoff shows that our chunking strategyperforms best in unknown word detection on both of the corpora.The rest of the paper is as follows: In Section 2, we will discuss in details about ourchunking strategy in unknown word detection.
Experimental results are given inSection 3.
Finally, some remarks and conclusions are made in Section 4.2   Unknown Word Detection by ChunkingIn this section, we will first describe the chunking strategy in unknown worddetection of Chinese word segmentation using a discriminative Markov model, calledMutual Information Independence Model (MIIM).
Then a maximum entropy model isapplied to integrate various types of contexts and resolve the data sparseness problemin MIIM.
Finally, an error-driven learning approach is proposed to select usefulcontexts and reduce the context feature vector dimension.2.1   Mutual Information Independence Model and Unknown Word DetectionMutual Information Independence ModelIn this paper, we use a discriminative Markov model, called Mutual InformationIndependence Model (MIIM) proposed by Zhou et al[17] 4 , in unknown worddetection by chunking.
MIIM is derived from a conditional probability model.
Givenan observation sequence nn oooO L211 = , the goal of a conditional probability modelis to find a stochastic optimal state(tag) sequence nn sssS L211 =  that maximizes:)()(),(log)(log)|(log1111111 nnnnnnnOPSPOSPSPOSP?+=        (1)The second term in Equation (1) is the pair-wise mutual information (PMI)between nS1  andnO1 .
In order to simplify the computation of this term, we assume apair-wise mutual information independence (2):?==nininn OsPMIOSPMI1111 ),(),(      or?=?=?ninininnnnOPsPOsPOPSPOSP1 111111)()(),(log)()(),(log           (2)4We have renamed the discriminative Markov model in [17] as the Mutual InformationIndependence Model according to the novel pair-wise mutual information independenceassumption in the model.
Another reason is to distinguish it from the traditional HiddenMarkov Model [18] and avoid misleading.A Chunking Strategy Towards Unknown Word Detection 533That is, an individual state is only dependent on the observation sequence nO1  andindependent on other states in the state sequence nS1 .
This assumption is reasonablebecause the dependence among the states in the state sequence nS1  has already beencaptured by the first term in Equation (1).
Applying Equation (2) to Equation (1), wehave Equation (3)5:??==?
+=nininiiinn OsPSsPMIOSP1121111 )|(log),()|(log             (3)We call the above model as shown in Equation (3) the Mutual InformationIndependence Model due to its pair-wise mutual information assumption as shown inEquation (2).
The above model consists of two sub-models: the state transition model?=?niii SsPMI211 ),(  as the first term in Equation (3) and the output model?=nini OsP11 )|(log  as the second term in Equation (3).
Here, a variant of the Viterbialgorithm [19] in decoding the standard Hidden Markov Model (HMM) [18] isimplemented to find the most likely state sequence by replacing the state transitionmodel and the output model of the standard HMM with the state transition model andthe output model of the MIIM, respectively.Unknown Word DetectionFor unknown word detection by chunking, a word (known word or unknown word) isregarded as a chunk of one or more word atoms and we have:?
>=< iii wpo , ; iw is the thi ?
word atom in the sequence of wordatoms nn wwwW L211 = ; ip  is the word formation pattern of the word atom iw .Here ip  measures the word formation power of the word atom iw  and consists of:o The percentage of iw  occurring as a whole word (round to 10%)o The percentage of iw  occurring at the beginning of other words (round to10%)o The percentage of iw  occurring at the end of other words (round to 10%)o The length of iwo The occurring frequency feature of iw , which is mapped tomax(log(Frequency), 9 ).?
is : the states are used to bracket and differentiate various types of words.
In thisway, Chinese unknown word detection can be regarded as a bracketing processwhile differentiation of different word types can help the bracketing process.
is  isstructural and consists of three parts:5Details about the derivation are omitted due to space limitation.
Please see [17] for more.534 G. Zhouo Boundary Category (B): it includes four values: {O, B, M, E}, where Omeans that current word atom is a whOle word and B/M/E means that currentword atom is at the Beginning/in the Middle/at the End of a word.o Word Category (W): It is used to denote the class of the word.
In our system,words are classified into two types: pure Chinese word type and mixed wordtype (i.e.
including English characters and Chinese digits/numbers/symbols).o Word Atom Formation Pattern (P): Because of the limited number ofboundary and word categories, the word atom formation pattern describedabove is added into the structural state to represent a more accurate statetransition model in MIIM while keeping its output model.Problem with Unknown Word Detection Using MIIMFrom Equation (3), we can see that the state transition model of MIIM can becomputed by using ngram modeling [20, 21, 22], where each tag is assumed to bedependent on the N-1 previous tags (e.g.
2).
The problem with the above MIIM lies inthe data sparseness problem raised by its output model: ?=nini OsP11 )|(log .
Ideally, wewould have sufficient training data for every event whose conditional probability wewish to calculate.
Unfortunately, there is rarely enough training data to computeaccurate probabilities when decoding on new data.
Generally, two smoothingapproaches [21, 22, 23] are applied to resolve this problem: linear interpolation andback-off.
However, these two approaches only work well when the number ofdifferent information sources is very limited.
When a few features and/or a longcontext are considered, the number of different information sources is exponential.This makes smoothing approaches inappropriate in our system.
In this paper, themaximum entropy model [24] is proposed to integrate various context informationsources and resolve the data sparseness problem in our system.
The reason that wechoose the maximum entropy model for this purpose is that it represents the state-of?the-art in  the machine learning research community and there are goodimplementations of the algorithm available.
Here, we use the open NLP maximumentropy package6 in our system.2.2   Maximum EntropyThe maximum entropy model is a probability distribution estimation technique widelyused in recent years for natural language processing tasks.
The principle of themaximum entropy model in estimating probabilities is to include as much informationas is known from the data while making no additional assumptions.
The maximumentropy model returns the probability distribution that satisfies the above propertywith the highest entropy.
Formally, the decision function of the maximum entropymodel can be represented as:?==kjohfjjhZhoP1),()(1),( ?
(4)6http://maxent.sourceforge.netA Chunking Strategy Towards Unknown Word Detection 535where o is the outcome, h is the history (context feature vector in this paper), Z(h) is anormalization function, {f1, f2, ..., fk} are feature functions and {?1, ?2, ?, ?k} are themodel parameters.
Each model parameter corresponds to exactly one feature and canbe viewed as a "weight" for that feature.
All features used in the maximum entropymodel are binary, e.g.???
===.,0);(,,1),(otherwisewe?
?dAtomCurrentWortWordIndependenoifohf j     (5)In order to reliably estimate )|( 1ni OsP  in the output model of MIIM using themaximum entropy model, various context information sources are included in thecontext feature vector:?
ip : current word atom formation pattern?
ii pp 1?
: previous word atom formation pattern and current word atom formationpattern?
1+ii pp : current word atom formation pattern and next word atom formationpattern?
ii wp : current word atom formation pattern and current word atom?
iii pwp 11 ??
: previous word atom formation pattern, previous word atom andcurrent word atom formation pattern?
11 ++ iii wpp : current word atom formation pattern, next word atom formationpattern and next word atom?
iii wpp 1?
: previous word atom formation pattern, current word atom formationpattern and current word atom?
1+iii pwp : current word atom formation pattern, current word atom and next wordatom formation pattern?
iiii wpwp 11 ??
: previous word atom formation pattern, previous word atom, currentword atom formation pattern and current word atom?
11 ++ iiii wpwp : current word atom formation pattern, current word atom, next wordatom formation pattern and next word atomHowever, there exists a problem when we include above various contextinformation in the maximum entropy model: the context feature vector dimensioneasily becomes too large for the model to handle.
One easy solution to this problem isto only keep those frequently occurring contexts in the model.
Although thisfrequency filtering approach is simple, many useful contexts may not occur frequentlyand be filtered out while those kept may not be useful.
To resolve this problem, wepropose an alternative error-driven learning approach to only keep useful contexts inthe model.2.3   Context Feature Selection Using Error-Driven LearningHere, we propose an error-driven learning approach to examine the effectiveness ofvarious contexts and select useful contexts to reduce the size of the context feature536 G. Zhouvector used in the maximum entropy model for estimating )|( 1ni OsP  in the outputmodel of MIIM.
This makes it possible to further improve the performance byincorporating more various types of contexts in the future.Assume ?
is the container for useful contexts.
Given a set of existing usefulcontexts ?
and a set of new contexts ??
, the effectiveness of a new contextiC ???
, ),( iCE ?
, is measured by the iC -related reduction in errors which resultsfrom adding the new context set ??
to the useful context set ?
:),(#),(#),( iii CErrorCErrorCE ??+???=?
(6)Here, ),(# iCError ?
is the number of iC -related chunking errors before ??
isadded to ?
and ),(# iCError ??+?
is the number of iC -related chunking errorsafter ??
is added to ?
.
That is, ),( iCE ?
is the number of the chunking errorcorrections made on the context iC ???
when ??
is added to ?
.
If 0),( >?
iCE ,we declare that the new context iC  is a useful context and should be added to ?
.Otherwise, the new context iC  is considered useless and discarded.Given the above error-driven learning approach, we initialize }{ ip=?
(i.e.
weassume all the current word atom formation patterns are useful contexts) and chooseone of the other context types as the new context set ??
, e.g.
}{ ii wp=?
.
Then, wecan train two MIIMs with different output models using ?
and ??+?respectively.
Moreover, useful contexts are learnt on the training data in a two-foldway.
For each fold, two MIIMs are trained on 50% of the training data and for eachnew context iC  in ??
, evaluate its effectiveness ),( iCE ?
on the remaining 50% ofthe training data according to the context effectiveness measure as shown in Equation(6).
If 0),( >?
iCE ,  iC  is marked as a useful context and added to ?
.
In this way,all the useful contexts in ??
are incorporated into the useful context set ?
.
Similarly,we can include useful contexts of other context types into the useful context set ?
oneby one.
In this paper, various types of contexts are learnt one by one in the exact sameorder as shown in Section 2.2.
Finally, since different types of contexts may havecross-effects, the above process is iterated with the renewed useful context set ?until very few useful contexts can be found at each loop.
Our experiments show thatiteration converges within four loops.3   Experimental ResultsAll of our experiments are evaluated on the PK and CTB benchmark corpora used inthe First SIGHAN Chinese word segmentation bakeoff7 with the closed configuration.That is, only the training data from the particular corpus is used during training.
Forunknown word detection, the chunking training data is derived by using the sameMaximum Matching Algorithm (MMA) to segment each word in the original trainingdata as a chunk of word atoms.
This is done in a two-fold way.
For each fold, the7http://www.sighan.org/bakeoff2003/A Chunking Strategy Towards Unknown Word Detection 537MMA is trained on 50% of the original training data and then used to segment theremaining 50% of the original training data.
Then the MIIM is used to train achunking model for unknown word detection on the chunking training data.
Table 1shows the details of the two corpora.
Here, OOV is defined as the percentage ofwords in the test corpus not occurring in the training corpus and indicates the out-of-vocabulary rate in the test corpus.Table 1.
Statistics of the corpora used in our evaluationCorpus Abbreviation OOV Training Data Test DataBeijing University PK 6.9% 1100K words 17K wordsUPENN Chinese Treebank CTB 18.1% 250K words 40K wordsTable 2 shows the detailed performance of our system in unknown word detectionand Chinese word segmentation as a whole using the standard scoring script8 on thetest data.
In this and subsequent tables, various evaluation measures are provided:precision (P), recall (R), F-measure, recall on out-of-vocabulary words ( OOVR ) andrecall on in-vocabulary words ( IVR ).
It shows that our system achievesprecision/recall/F-measure of 93.5%/96.1%/94.8 and 90.5%/90.1%/90.3 on the PKand CTB corpora respectively.
Especially, our chunking approach can successfullydetect 80.5% and 77.6% of unknown words on the PK and CTB corpora respectively.Table 2.
Detailed performance of our system on the 1st SIGHAN Chinese word segmentationbenchmark dataCorpus P R F OOVR  IVRPK 93.5 96.1 94.8 80.5 97.3CTB 90.5 90.1 90.3 77.6 92.9Table 3 and Table 4 compare our system with other best-reported systems on thePK and CTB corpora respectively.
Table 3 shows that our chunking approach inunknown word detection outperforms others by more than 8% on the PK corpus.
Italso shows that our system performs comparably with the best reported systems onthe PK corpus when the out-of-vocabulary rate is moderate(6.9%).
Our performancein Chinese word segmentation as a whole is somewhat pulled down by the lowerperformance in recalling in-vocabulary words.
This may be due to the preference ofour chunking strategy in detecting unknown words by wrongly combining some of in-vocabulary words into unknown words.
Such preference may cause negative effect inChinese word segmentation as a whole when the gain in unknown word detectionfails to compensate the loss in wrongly combining some of in-vocabulary words intounknown words.
This happens when the out-of-vocabulary rate is not high, e.g.
on the8http://www.sighan.org/bakeoff2003/score538 G. ZhouPK corpus.
Table 4 shows that our chunking approach in unknown word detectionoutperforms others by more than 7% on the CTB corpus.
It also shows that oursystem outperforms the other best-reported systems by more than 2% in Chinese wordsegmentation as a whole on the CTB corpus.
This is largely due to the huge gain inunknown word detection when the out-of-vocabulary rate is high (e.g.
18.1% in theCTB corpus), even though our system performs worse on recalling in-vocabularywords than others.
Evaluation on both the PK and CTB corpora shows that ourchunking approach can successfully detect about 80% of unknown words on corporawith a large range of the out-of-vocabulary rates.
This suggests the powerfulness ofusing various word formation patterns of word atoms in detecting unknown words.This also demonstrates the effectiveness and robustness of our chunking approach inunknown word detection of Chinese word segmentation and its portability to differentgenres.Table 3.
Comparison of our system with other best-reported systems on the PK corpusCorpus P R F OOVR  IVROurs 93.5 96.1 94.8 80.5 97.3Zhang et al[25] 94.0 96.2 95.1 72.4 97.9Wu [26] 93.8 95.5 94.7 68.0 97.6Chen [27] 93.8 95.5 94.6 64.7 97.7Table 4.
Comparison of our system with other best-reported systems on the CTB corpusCorpus P R F OOVR  IVROurs 90.5 90.1 90.3 77.6 92.9Zhang et al[25] 87.5 88.6 88.1 70.5 92.7Duan et al[28] 85.6 89.2 87.4 64.4 94.7Finally, Table 5 and Table 6 compare our error-driven learning approach with thefrequency filtering approach in learning useful contexts for the output model of MIIMon the PK and CTB corpora respectively.
Due to memory limitation, at most 400Kuseful contexts are considered in the frequency filtering approach.
First, they showthat the error-driven learning approach is much more effective than the simplefrequency filtering approach.
With the same number of useful contexts, the error-driven learning approach outperforms the frequency filtering approach by 7.8%/0.6%and 5.5%/0.8% in OOVR (unknown word detection)/F-measure(Chinese wordsegmentation as a whole) on the PK and CTB corpora respectively.
Moreover, theerror-driven learning approach slightly outperforms the frequency filtering approachwith the best configuration of 2.5 and 3.5 times of useful contexts.
Second, they showthat increasing the number of frequently occurring contexts using the frequencyfiltering approach may not increase the performance.
This may be due to that some offrequently occurring contexts are noisy or useless and including them may haveA Chunking Strategy Towards Unknown Word Detection 539negative effect.
Third, they show that the error-driven learning approach is effectivein learning useful contexts by reducing 96-98% of possible contexts.
Finally, thefigures inside parentheses show the number of useful patterns shared between theerror-driven learning approach and the frequency filtering approach.
They show thatabout 40-50% of useful contexts selected using the error-driven learning approach donot occur frequently in the useful contexts selected using the frequency filteringapproach.Table 5.
Comparison of the error-driven learning approach with the frequency filteringapproach in learning useful contexts for the output model of MIIM on the PK corpus (Totalnumber of possible contexts: 4836K)Approach #useful contexts F OOVR  IVRError-Driven Learning 98K 94.8 80.5 97.3Frequency Filtering 98K (63K) 94.2 72.7 97.4Frequency Filtering (best performance) 250K (90K) 94.7 80.2 97.3Frequency Filtering 400K (94K) 94.6 79.1 97.1Table 6.
Comparison of the error-driven learning approach with the frequency filteringapproach in learning useful contexts for the output model of MIIM on the CTB corpus (Totalnumber of possible contexts: 1038K)Approach #useful contexts F OOVR  IVRError-Driven Learning 43K 90.3 77.6 92.9Frequency Filtering 43K (21K) 89.5 72.1 92.8Frequency Filtering (best performance) 150K 90.1 76.1 93.0Frequency Filtering 400K (40K) 89.9 75.8 92.94   ConclusionIn this paper, a chunking strategy is presented to detect unknown words in Chineseword segmentation by chunking one or more word atoms together according to thevarious word formation patterns of the word atoms.
Besides, a maximum entropymodel is applied to integrate various types of contexts and resolve the data sparsenessproblem in our strategy.
Finally, an error-driven learning approach is proposed tolearn useful contexts in the maximum entropy model.
In this way, the number ofcontexts in the maximum entropy model can be significantly reduced withoutperformance decrease.
This makes it possible for further improving the performanceby considering more various types of contexts.
Evaluation on the PK and CTBcorpora in the First SIGHAN Chinese word segmentation bakeoff shows that ourchunking strategy can detect about 80% of unknown words on both of the corpora andoutperforms the best-reported systems by 8.1% and 7.1% in unknown word detection540 G. Zhouon them respectively.
While our Chinese word segmentation system with chunking-based unknown word detection performs comparably with the best systems on the PKcorpus when the out-of-vocabulary rate is moderate(6.9%), our system significantlyoutperforms others by more than 2% when the out-of-vocabulary rate is high(18.1%).This demonstrates the effectiveness and robustness of our chunking strategy inunknown word detection of Chinese word segmentation and its portability to differentgenres.References1.
Jie CY, Liu Y and Liang NY.
(1989).
On methods of Chinese automatic segmentation,Journal of Chinese Information Processing, 3(1):1-9.2.
Li KC, Liu KY and Zhang YK.
(1988).
Segmenting Chinese word and processing differentmeanings structure, Journal of Chinese Information Processing, 2(3):27-33.3.
Liang NY, (1990).
The knowledge of Chinese word segmentation, Journal of ChineseInformation Processing, 4(2):29-33.4.
Lua KT, (1990).
From character to word - An application of information theory, ComputerProcessing of Chinese & Oriental Languages, 4(4):304-313.5.
Lua KT and Gan GW.
(1994).
An application of information theory in Chinese wordsegmentation.
Computer Processing of Chinese & Oriental Languages, 8(1):115-124.6.
Wang YC, SU HJ and Mo Y.
(1990).
Automatic processing of Chinese words.
Journal ofChinese Information Processing.
4(4):1-11.7.
Wu JM and Tseng G. (1993).
Chinese text segmentation for text retrieval: achievementsand problems.
Journal of the American Society for Information Science.
44(9):532-542.8.
Xu H, He KK and Sun B.
(1991) The implementation of a written Chinese automaticsegmentation expert system, Journal of Chinese Information Processing, 5(3):38-47.9.
Yao TS, Zhang GP and Wu YM.
(1990).
A rule-based Chinese automatic segmentationsystem, Journal of Chinese Information Processing, 4(1):37-43.10.
Yeh CL and Lee HJ.
(1995).
Rule-based word identification for Mandarin Chinesesentences - A unification approach, Computer Processing of Chinese & OrientalLanguages, 9(2):97-118.11.
Nie JY, Jin WY and Marie-Louise Hannan.
(1997).
A  hybrid approach to unknown worddetection and segmentation of Chinese, Chinese Processing of Chinese and OrientalLanguages, 11(4): pp326-335.12.
Tung CH and Lee HJ.
(1994).
Identification of unknown word from a corpus, computerProcessing of Chinese & Oriental Languages, 8(Supplement):131-146.13.
Chang JS et al (1994).
A multi-corpus approach to recognition of proper names inChinese Text, Computer Processing of Chinese & Oriental Languages, 8(1):75-8614.
Sun MS, Huang CN, Gao HY and Fang J.
(1994).
Identifying Chinese Names InUnrestricted Texts, Communications of Chinese and Oriental Languages InformationProcessing Society, 4(2):113-122.15.
Zhou GD and Lua KT, (1997).
Detection of Unknown Chinese Words Using a HybridApproach, Computer Processing of Chinese & Oriental Language, 11(1):63-75.16.
Eugene Charniak, Statistical language learning, The MIT Press,   ISBN 0-262-03216-317.
Zhou GDong and Su J.
(2002).
Named Entity Recognition Using a HMM-based ChunkTagger, Proceedings of the Conference on Annual Meeting for Computational Linguistics(ACL?2002).
473-480, Philadelphia.A Chunking Strategy Towards Unknown Word Detection 54118.
Rabiner L. 1989.
A Tutorial on Hidden Markov Models and Selected Applications inSpeech Recognition.
IEEE 77(2), pages257-285.19.
Viterbi A.J.
1967.
Error Bounds for Convolutional Codes and an Asymptotically OptimumDecoding Algorithm.
IEEE Transactions on Information Theory, IT 13(2), 260-269.20.
Gale W.A.
and Sampson G. 1995.
Good-Turing frequency estimation without tears.Journal of Quantitative Linguistics.
2:217-237.21.
Jelinek F. (1989).
Self-Organized Language Modeling for Speech Recognition.
In AlexWaibel and Kai-Fu Lee(Editors).
Readings in Speech Recognitiopn.
Morgan Kaufmann.450-506.22.
Katz S.M.
(1987).
Estimation of Probabilities from Sparse Data for the Language ModelComponent of a Speech Recognizer.
IEEE Transactions on Acoustics.
Speech and SignalProcessing.
35: 400-401.23.
Chen and Goodman.
(1996).
An Empirical Study of Smoothing Technniques for LanguageModeling.
In Proceedings of the 34th Annual Meeting of the Association of ComputationalLinguistics (ACL?1996).
pp310-318.
Santa Cruz, California, USA.24.
Ratnaparkhi A.
(1996).
A Maximum Entropy Model for Part-of-Speech Tagging.Proceedings of the Conference on Empirical Methods in Natural Language Processing.,133-142.25.
Zhang HP, Yu HK, Xiong DY and Liu Q.
(2003).
HHMM-based Chinese LexicalAnalyzer ICTCLAS.
Proceedings of 2nd SIGHAN Workshop on Chinese LanguageProcessing.
184-187.
Sapporo, Japan.26.
Wu AD.
(2003).
Chinese Word Segmentation in MSR-NLP.
Proceedings of 2nd SIGHANWorkshop on Chinese Language Processing.
172-175.
Sapporo, Japan.27.
Chen AT.
(2003).
Chinese Word Segmentation Using Minimal Linguistic Knowledge.Proceedings of 2nd SIGHAN Workshop on Chinese Language Processing.
148-151.Sapporo, Japan.28.
Duan HM, Bai XJ, Chang BB and Yu SW. (2003).
Chinese Word Segmentation at PekingUniversity.
Proceedings of 2nd SIGHAN Workshop on Chinese Language Processing.
152-155.
Sapporo, Japan.
