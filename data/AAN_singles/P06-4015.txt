Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 57?60,Sydney, July 2006. c?2006 Association for Computational LinguisticsThe SAMMIE System: Multimodal In-Car DialogueTilman Becker, Peter Poller,Jan SchehlDFKIFirst.Last@dfki.deNate Blaylock, Ciprian Gerstenberger,Ivana Kruijff-Korbayova?Saarland Universitytalk-mit@coli.uni-sb.deAbstractThe SAMMIE1 system is an in-car multi-modal dialogue system for an MP3 ap-plication.
It is used as a testing environ-ment for our research in natural, intuitivemixed-initiative interaction, with particu-lar emphasis on multimodal output plan-ning and realization aimed to produce out-put adapted to the context, including thedriver?s attention state w.r.t.
the primarydriving task.1 IntroductionThe SAMMIE system, developed in the TALKproject in cooperation between several academicand industrial partners, employs the InformationState Update paradigm, extended to model collab-orative problem solving, multimodal context andthe driver?s attention state.
We performed exten-sive user studies in a WOZ setup to guide the sys-tem design.
A formal usability evaluation of thesystem?s baseline version in a laboratory environ-ment has been carried out with overall positive re-sults.
An enhanced version of the system will beintegrated and evaluated in a research car.In the following sections, we describe the func-tionality and architecture of the system, point outits special features in comparison to existing work,and give more details on the modules that are inthe focus of our research interests.
Finally, wesummarize our experiments and evaluation results.2 FunctionalityThe SAMMIE system provides a multi-modal inter-face to an in-car MP3 player (see Fig.
1) throughspeech and haptic input with a BMW iDrive inputdevice, a button which can be turned, pushed downand sideways in four directions (see Fig.
2 left).System output is provided by speech and a graphi-cal display integrated into the car?s dashboard.
Anexample of the system display is shown in Fig.
2.1SAMMIE stands for Saarbru?cken Multimodal MP3 PlayerInteraction Experiment.Figure 1: User environment in laboratory setup.The MP3 player application offers a wide rangeof functions: The user can control the currentlyplaying song, search and browse an MP3 databaseby looking for any of the fields (song, artist, al-bum, year, etc.
), search and select playlists andeven construct and edit playlists.The user of SAMMIE has complete freedom ininteracting with the system.
Input can be throughany modality and is not restricted to answers tosystem queries.
On the contrary, the user can givenew tasks as well as any information relevant tothe current task at any time.
This is achieved bymodeling the interaction as a collaborative prob-lem solving process, and multi-modal interpreta-tion that fits user input into the context of thecurrent task.
The user is also free in their useof multimodality: SAMMIE handles deictic refer-ences (e.g., Play this title while pushing the iDrivebutton) and also cross-modal references, e.g., Playthe third song (on the list).
Table 1 shows a typ-ical interaction with the SAMMIE system; the dis-played song list is in Fig.
2.
SAMMIE supports in-teraction in German and English.3 ArchitectureOur system architecture follows the classical ap-proach (Bunt et al, 2005) of a pipelined architec-ture with multimodal interpretation (fusion) and57U: Show me the Beatles albums.S: I have these four Beatles albums.
[shows a list of album names]U: Which songs are on this one?
[selects the Red Album]S: The Red Album contains these songs[shows a list of the songs]U: Play the third one.S: [music plays]Table 1: A typical interaction with SAMMIE.fission modules encapsulating the dialogue man-ager.
Fig.
2 shows the modules and their inter-action: Modality-specific recognizers and analyz-ers provide semantically interpreted input to themultimodal fusion module that interprets them inthe context of the other modalities and the cur-rent dialogue context.
The dialogue manager de-cides on the next system move, based on its modelof the tasks as collaborative problem solving, thecurrent context and also the results from calls tothe MP3 database.
The turn planning module thendetermines an appropriate message to the user byplanning the content, distributing it over the avail-able output modalities and finally co-ordinatingand synchronizing the output.
Modality-specificoutput modules generate spoken output and graph-ical display update.
All modules interact with theextended information state which stores all contextinformation.Figure 2: SAMMIE system architecture.Many tasks in the SAMMIE system are mod-eled by a plan-based approach.
Discourse mod-eling, interpretation management, dialogue man-agement and linguistic planning, and turn plan-ning are all based on the production rule systemPATE2 (Pfleger, 2004).
It is based on some con-cepts of the ACT-R 4.0 system, in particular thegoal-oriented application of production rules, the2Short for (P)roduction rule system based on (A)ctivationand (T)yped feature structure (E)lements.activation of working memory elements, and theweighting of production rules.
In processing typedfeature structures, PATE provides two operationsthat both integrate data and also are suitable forcondition matching in production rule systems,namely a slightly extended version of the generalunification, but also the discourse-oriented opera-tion overlay (Alexandersson and Becker, 2001).4 Related Work and Novel AspectsMany dialogue systems deployed today follow astate-based approach that explicitly models thefull (finite) set of dialogue states and all possibletransitions between them.
The VoiceXML3 stan-dard is a prominent example of this approach.
Thishas two drawbacks: on the one hand, this approachis not very flexible and typically allows only so-called system controlled dialogues where the useris restricted to choosing their input from providedmenu-like lists and answering specific questions.The user never is in control of the dialogue.
Forrestricted tasks with a clear structure, such an ap-proach is often sufficient and has been applied suc-cessfully.
On the other hand, building such appli-cations requires a fully specified model of all pos-sible states and transitions, making larger applica-tions expensive to build and difficult to test.In SAMMIE we adopt an approach that mod-els the interaction on an abstract level as collab-orative problem solving and adds application spe-cific knowledge on the possible tasks, available re-sources and known recipes for achieving the goals.In addition, all relevant context information isadministered in an Extended Information State.This is an extension of the Information State Up-date approach (Traum and Larsson, 2003) to themulti-modal setting.Novel aspects in turn planning and realizationinclude the comprehensive modeling in a sin-gle, OWL-based ontology and an extended rangeof context-sensitive variation, including systemalignment to the user on multiple levels.5 Flexible Multi-modal Interaction5.1 Extended Information StateThe information state of a multimodal systemneeds to contain a representation of contextual in-formation about discourse, but also a represen-tation of modality-specific information and user-specific information which can be used to plansystem output suited to a given context.
The over-3http://www.w3.org/TR/voicexml2058all information state (IS) of the SAMMIE system isshown in Fig.
3.The contextual information partition of the ISrepresents the multimodal discourse context.
Itcontains a record of the latest user utterance andpreceding discourse history representing in a uni-form way the salient discourse entities introducedin the different modalities.
We adopt the three-tiered multimodal context representation used inthe SmartKom system (Pfleger et al, 2003).
Thecontents of the task partition are explained in thenext section.5.2 Collaborative Problem SolvingOur dialogue manager is based on anagent-based model which views dialogueas collaborative problem-solving (CPS)(Blaylock and Allen, 2005).
The basic buildingblocks of the formal CPS model are problem-solving (PS) objects, which we represent astyped feature structures.
PS object types form asingle-inheritance hierarchy.
In our CPS model,we define types for the upper level of an ontologyof PS objects, which we term abstract PS objects.There are six abstract PS objects in our modelfrom which all other domain-specific PS objectsinherit: objective, recipe, constraint, evaluation,situation, and resource.
These are used to modelproblem-solving at a domain-independent leveland are taken as arguments by all update opera-tors of the dialogue manager which implementconversation acts (Blaylock and Allen, 2005).The model is then specialized to a domain byinheriting and instantiating domain-specific typesand instances of the PS objects.5.3 Adaptive Turn PlanningThe fission component comprises detailed con-tent planning, media allocation and coordinationand synchronization.
Turn planning takes a setof CPS-specific conversational acts generated bythe dialogue manager and maps them to modality-specific communicative acts.Information on how content should be dis-tributed over the available modalities (speech orgraphics) is obtained from Pastis, a module whichstores discourse-specific information.
Pastis pro-vides information about (i) the modality on whichthe user is currently focused, derived by the cur-rent discourse context; (ii) the user?s current cog-nitive load when system interaction becomes asecondary task (e.g., system interaction whiledriving); (iii) the user?s expertise, which is rep-resented as a state variable.
Pastis also containsinformation about factors that influence the prepa-ration of output rendering for a modality, like thecurrently used language (German or English) orthe display capabilities (e.g., maximum number ofdisplayable objects within a table).
Together withthe dialogue manager?s embedded part of the in-formation state, the information stored by Pastisforms the Extended Information State of the SAM-MIE system (Fig.
3).Planning is then executed through a set of pro-duction rules that determine which kind of infor-mation should be presented through which of theavailable modalities.
The rule set is divided in twosubsets, domain-specific and domain-independentrules which together form the system?s multi-modal plan library.contextual-info:????????????????????
?last-user-utterance::[interp : set(grounding-acts)modality-requested : modalitymodalities-used : set(msInput)]discourse-history:: list(discourse-objects)modality-info::[speech : speechInfographic : graphicInfo]user-info::[cognitive-load : cogLoadInfouser-expertise : expertiseInfo]????????????????????
?task-info:[cps-state : c-situation (see below for details)pending-sys-utt : list(grounding-acts)]Figure 3: SAMMIE Information State structure.5.4 Spoken Natural Language OutputGenerationOur goal is to produce output that varies in the sur-face realization form and is adapted to the con-text.
A template-based module has been devel-oped and is sufficient for classes of system outputthat do not need fine-tuned context-driven varia-tion.
Our template-based generator can also de-liver alternative realizations, e.g., alternative syn-tactic constructions, referring expressions, or lexi-cal items.
It is implemented by a set of straightfor-ward sentence planning rules in the PATE systemto build the templates, and a set of XSLT trans-formations to yield the output strings.
Output inGerman and English is produced by accessing dif-ferent dictionaries in a uniform way.In order to facilitate incremental developmentof the whole system, our template-based mod-ule has a full coverage wrt.
the classes of sys-59tem output that are needed.
In parallel, we areexperimenting with a linguistically more power-ful grammar-based generator using OpenCCG4,an open-source natural language processing en-vironment (Baldridge and Kruijff, 2003).
This al-lows for more fine-grained and controlled choicesbetween linguistic expressions in order to achievecontextually appropriate output.5.5 Modeling with an OntologyWe use a full model in OWL as the knowledge rep-resentation format in the dialogue manager, turnplanner and sentence planner.
This model in-cludes the entities, properties and relations of theMP3 domain?including the player, data base andplaylists.
Also, all possible tasks that the user mayperform are modeled explicitly.
This task modelis user centered and not simply a model of theapplication?s API.The OWL-based model is trans-formed automatically to the internal format usedin the PATE rule-interpreter.We use multiple inheritance to model differentviews of concepts and the corresponding presen-tation possibilities; e.g., a song is a browsable-object as well as a media-object and thus allowsfor very different presentations, depending on con-text.
Thereby PATE provides an efficient and ele-gant way to create more generic presentation plan-ning rules.6 Experiments and EvaluationSo far we conducted two WOZ data collectionexperiments and one evaluation experiment witha baseline version of the SAMMIE system.
TheSAMMIE-1 WOZ experiment involved only spo-ken interaction, SAMMIE-2 was multimodal, withspeech and haptic input, and the subjects hadto perform a primary driving task using a LaneChange simulator (Mattes, 2003) in a half of theirexperiment session.
The wizard was simulatingan MP3 player application with access to a largedatabase of information (but not actual music) ofmore than 150,000 music albums (almost 1 mil-lion songs).
In order to collect data with a varietyof interaction strategies, we used multiple wizardsand gave them freedom to decide about their re-sponse and its realization.
In the multimodal setupin SAMMIE-2, the wizards could also freely de-cide between mono-modal and multimodal output.
(See (Kruijff-Korbayova?
et al, 2005) for details.
)We have just completed a user evaluation toexplore the user-acceptance, usability, and per-formance of the baseline implementation of the4http://openccg.sourceforge.netSAMMIE multimodal dialogue system.
The userswere asked to perform tasks which tested the sys-tem functionality.
The evaluation analyzed theuser?s interaction with the baseline system andcombined objective measurements like task com-pletion (89%) and subjective ratings from the testsubjects (80% positive).Acknowledgments This work has been carriedout in the TALK project, funded by the EU 6thFramework Program, project No.
IST-507802.References[Alexandersson and Becker2001] J. Alexandersson andT.
Becker.
2001.
Overlay as the basic operation fordiscourse processing in a multimodal dialogue system.
InProceedings of the 2nd IJCAI Workshop on Knowledgeand Reasoning in Practical Dialogue Systems, Seattle,Washington, August.
[Baldridge and Kruijff2003] J.M.
Baldridge and G.J.M.
Krui-jff.
2003.
Multi-Modal Combinatory Categorial Gram-mar.
In Proceedings of the 10th Annual Meeting of theEuropean Chapter of the Association for ComputationalLinguistics (EACL?03), Budapest, Hungary, April.
[Blaylock and Allen2005] N. Blaylock and J. Allen.
2005.
Acollaborative problem-solving model of dialogue.
In LailaDybkj?r and Wolfgang Minker, editors, Proceedings ofthe 6th SIGdial Workshop on Discourse and Dialogue,pages 200?211, Lisbon, September 2?3.
[Bunt et al2005] H. Bunt, M. Kipp, M. Maybury, andW.
Wahlster.
2005.
Fusion and coordination for multi-modal interactive information presentation: Roadmap, ar-chitecture, tools, semantics.
In O.
Stock and M. Zanca-naro, editors, Multimodal Intelligent Information Presen-tation, volume 27 of Text, Speech and Language Technol-ogy, pages 325?340.
Kluwer Academic.[Kruijff-Korbayova?
et al2005] I. Kruijff-Korbayova?,T.
Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,P.
Poller, J. Schehl, and V. Rieser.
2005.
An experimentsetup for collecting data for adaptive output planning ina multimodal dialogue system.
In Proc.
of ENLG, pages191?196.
[Mattes2003] S. Mattes.
2003.
The lane-change-task as a toolfor driver distraction evaluation.
In Proc.
of IGfA.
[Pfleger et al2003] N. Pfleger, J. Alexandersson, andT.
Becker.
2003.
A robust and generic discourse modelfor multimodal dialogue.
In Proceedings of the 3rdWorkshop on Knowledge and Reasoning in PracticalDialogue Systems, Acapulco.
[Pfleger2004] N. Pfleger.
2004.
Context based multimodalfusion.
In ICMI ?04: Proceedings of the 6th interna-tional conference on Multimodal interfaces, pages 265?272, New York, NY, USA.
ACM Press.
[Traum and Larsson2003] David R. Traum and Staffan Lars-son.
2003.
The information state approach to dialog man-agement.
In Current and New Directions in Discourse andDialog.
Kluwer.60
