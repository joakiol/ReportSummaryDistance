Proceedings of the 14th European Workshop on Natural Language Generation, pages 72?81,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsGraphs and Spatial Relationsin the Generation of Referring ExpressionsJette Viethenh.a.e.viethen@uvt.nlTiCCUniversity of TilburgTilburg, The NetherlandsMargaret Mitchellm.mitchell@jhu.eduHLT Centre of ExcellenceJohns Hopkins UniversityBaltimore, USAEmiel Krahmere.j.krahmer@uvt.nlTiCCUniversity of TilburgTilburg, The NetherlandsAbstractWhen they introduced the Graph-BasedAlgorithm (GBA) for referring expressiongeneration, Krahmer et al(2003) flauntedthe natural way in which it deals with re-lations between objects; but this featurehas never been tested empirically.
We fillthis gap in this paper, exploring referringexpression generation from the perspec-tive of the GBA and focusing in particu-lar on generating human-like expressionsin visual scenes with spatial relations.
Wecompare the original GBA against a variantthat we introduce to better reflect humanreference, and find that although the orig-inal GBA performs reasonably well, ournew algorithm offers an even better matchto human data (77.91% Dice).
Further, itcan be extended to capture speaker vari-ation, reaching an 82.83% Dice overlapwith human-produced expressions.1 IntroductionTen years ago, Krahmer et al(2003) published theGraph-Based Algorithm (GBA) for referring ex-pression generation (REG).
REG has since becomeone of the most researched areas within NaturalLanguage Generation, due in a large part to thecentral role it plays in communication: referringallows humans and language generation systemsalike to invoke the entities that the discourse isabout in the mind of a listener or reader.Like most REG algorithms, the GBA is focussedon the task of selecting the semantic content for areferring expression, uniquely identifying a targetreferent among all objects in its visual or linguisticcontext.
The framework used by the GBA is par-ticularly attractive because it provides fine-grainedcontrol for finding the ?best?
referring expression,encompassing several previous approaches.
Thiscontrol is made possible by defining a desiredcost function over object properties to guide theconstruction of the output expression and using asearch mechanism that does not stop at the firstsolution found.One characteristic of the GBA particularly em-phasized by Krahmer et al(2003), advancingfrom research on algorithms such as the Incre-mental Algorithm (Dale and Reiter, 1995) and theGreedy Algorithm (Dale, 1989), was the treatmentof relations between entities.
Relations such as ontop of or to the left of fall out naturally from thegraph-based representation of the domain, a facetmissing in earlier algorithms.
We believe that thismakes the GBA particularly well-suited for gener-ating language in spatial visual domains.In the years since the inception of the GBA,the REG community has become increasingly in-terested in evaluating algorithms against human-produced data in visual domains, aiming to mimichuman references to objects.
This interest hasmanifested most prominently in the 2007-2009REG Challenges (Belz and Gatt, 2007; Gatt et al2008; Gatt et al 2009) based on the TUNA Cor-pus (van Deemter et al 2012).
The GBA per-formed among the best algorithms in all three ofthese challenges.
However, in particular its abil-ity to analyze relational information could not beassessed, because the TUNA Corpus does not con-tain annotated relational descriptions.We rectify this omission in the current work bytesting the GBA on the GRE3D3 Corpus, whichwas designed to study the use of spatial rela-tions in referring expressions (Viethen and Dale,2008).
We compare against a variant of the GBAthat we introduce to build longer referring expres-72sions, following the observation that humans tendto overspecify (i.e., not be maximally brief) intheir referring expressions (Sonnenschein, 1985;Pechmann, 1989; Engelhardt et al 2006; Arts etal., 2011).
For both algorithms, we experimentwith cost functions defined at different granular-ities to produce the best match to human data.
Wefind that we can match human data better thanthe original GBA with the variant that encouragesoverspecification.With this model, we aim to further ad-vance towards human-like reference by develop-ing a method to capture speaker-specific varia-tion.
Speaker variation cannot easily be modeledby the classic input variables of REG algorithms,but a number of authors have shown that systemoutput can be improved by using speaker identityas an additional feature; this has often been ac-companied by the observation that commonalitiescan be found in the reference behaviour of differ-ent speakers (Bohnet, 2008; Di Fabbrizio et al2008a; Mitchell et al 2011b), particularly for spa-tial relations (Viethen and Dale, 2009).
In the sec-ond experiment reported in this paper, we combinethese insights by automatically clustering groupsof speakers with similar behaviour and then defin-ing separate cost functions for each group to betterguide the algorithms.Before we assess the ability of the GBA and ourvariant to produce human-like referring expres-sions containing relations (Sections 5 and 6), wewill give an overview of the relevant backgroundto the treatment of relations in REG, a short historyof the GBA, and the relevance of individual vari-ation (Section 2).
We introduce our new variantgraph-based algorithm, LongestFirst, in Section 3.2 Relations, Graphs and IndividualVariation2.1 Relations in REGIn the knowledge representation underlying mostwork in REG, each object in a scene is modeled asa set of attribute-value pairs describing the object?sproperties, such as hsize, largei.
Such a represen-tation is used in the two of the classic algorithms,the Greedy Algorithm (Dale, 1989) and the Incre-mental Algorithm (IA) (Dale and Reiter, 1995).Neither of these was originally intended to processrelations between objects.Several attempts have been made to adapt thetraditional REG algorithms to include relations be-tween objects in their output, but all of them suf-fer from problems with the knowledge representa-tion not being suited to relations.
Dale and Had-dock (1991) use a constraint network and a recur-sive loop to extend the Greedy Algorithm, whichuses the discriminatory power of an attribute asthe main selection criterion.
They treat relationsthe same as other attributes; but in most cases acertain spatial relation to a particular other ob-ject is fully distinguishing, which easily leads tostrange chains of relations in the output omittingmost other attributes (Viethen and Dale, 2006).Krahmer and Theune (2002) suggest a simi-lar adjustment for the IA by introducing a re-cursive loop if a relation to another object is in-troduced to the referring expression under con-struction.
They treat relations as fundamentallydifferent from other attributes in order to recog-nize when to enter the recursive loop, however,they fail to address the problem of infinite regress,whereby the objects in a domain might be de-scribed in a circular manner by the relations hold-ing between them.
Another relational extension tothe IA has been proposed by Kelleher and Kruijff(2006), treating relations as a completely differentclass from other attributes.
Both extensions of theIA make the simplifying assumption that relationsshould only be considered if it is not possible tofully distinguish the target referent from the sur-rounding objects in any other way, with the ideathat it takes less effort to consider and describeonly one object (Krahmer and Theune, 2002; Vie-then and Dale, 2008).2.2 A Short History of the GBAA new approach to REG was proposed by Krah-mer et al(2003).
In this approach, a scene isrepresented as a labeled directed graph (see Fig-ure 1(b)), and content selection is a subgraph con-struction problem.
Assuming a scene graph G =hVG, EGi, where vertices VG represent objects andedges EG represent the properties and relations ofthese objects with associated costs, their algorithmreturns the cheapest distinguishing subgraph thatuniquely refers to the target object v 2 VG.
Re-lations between objects (i.e., edges between dif-ferent vertices) are a natural part of this repre-sentation, without requiring special computationalmechanisms.
In addition to cost functions, theGBA requires a preference ordering (PO) over theedges to arbitrate between equally cheap descrip-tions (Viethen et al 2008).73(a) Scene 7 from the GRE3D3 Corpus.belowaboveleft-ofright-ofyellowsmallballlargesmallballredcubeyellowright-handleft-ofright-ofright-handright-hand(b) A graph representing the scene to the left.Figure 1: An example scene from the GRE3D3 Corpus and the corresponding domain graph.As the cost functions and preference orders arespecified over edges (i.e., properties), they allowmuch more fine-grained control over which prop-erties to generate for a target referent than theattribute-based preference orders employed by theIA and its descendants.
The cost functions can beused to give preference to a commonly used sizevalue, such as large, over a rarely used color value,such as mauve, although in general color is de-scribed more often than size.
This process is aidedby a branch-and-bound search that guarantees tofind the cheapest (i.e., ?best?)
referring expression.Since its inception, the GBA has been shown tobe useful for several referential phenomena.
Krah-mer and van der Sluis (2003) combined verbaldescriptions with pointing gestures by modellingeach such gesture as additional looping edges onall objects that it might be aimed at.
While the au-thors confirmed the ideas implemented in the al-gorithm in psycholinguistic studies (van der Sluis,2005), they never assessed its output in an actualdomain.van Deemter and Krahmer (2007) demonstratedhow the GBA could be used to generate referenceto sets as well as to negated and gradable prop-erties by representing implicit information as ex-plicit edges in domain graphs.
They also presenteda simple way to account for discourse saliencebased on restricting the distractor set.
Its abilityto cover such a breadth of referential phenomenamakes the GBA a reasonably robust algorithm forfurther exploring the generation of human-like ref-erence.The GBA was systematically tested againsthuman-produced referring expressions for the firsttime in the ASGRE Challenge 2007 (Belz andGatt, 2007).
This entry is described in detail in(Viethen et al 2008) and was very successful aswell in the following 2008 and 2009 REG Chal-lenges (Gatt et al 2008; Gatt et al 2009) witha free-na?
?ve cost function.
This cost function as-signs 0 cost to the most common attributes, 2 tothe rarest, and 1 to all others.
By making the mostcommon attributes free, it became possible to in-clude these attributes redundantly in a referringexpression, even if they were not strictly neces-sary for identifying the target.
The cost functionsused in the challenges were attribute-based, anddid therefore not make use of the refined controlcapabilities of the GBA.Theune et al(2011) used k-means clusteringon the property frequencies in order to providea more systematic method to transfer the FREE-NAI?VE cost function to new domains.
They foundthat using only two clusters (a high frequency anda low frequency group with associated costs of 0and 1) achieves the best results, with no significantdifferences to the FREE-NAI?VE cost function onthe TUNACorpus.
Subsequently they showed thaton this corpus, a training set of only 20 descrip-tions suffices to determine a 2-means cost functionthat performs as well as one based on 165 descrip-tions.
In (Koolen et al 2012), the same authorsextended these experiments to a Dutch version ofthe TUNA Corpus (Koolen and Krahmer, 2010)and came to a similar conclusion.
Neither of thecorpora used in these experiments included rela-tions between objects.2.3 Individual Variation in REGA number of authors have argued that to be able toproduce human-like referring expressions, an al-gorithm must account for speaker variation: Dif-ferent speakers will refer to the same object indifferent ways, and modeling this variation canbring us closer to generating the rich variety of ex-74pressions that people produce.
Several approacheshave been made in this direction.Although this was not explicitly discussed in(Jordan and Walker, 2005), the machine-learnedmodels presented there performed significantlybetter at replicating human-produced referringexpressions when a feature set was used thatincluded information about the identity of thespeaker.
In (Viethen and Dale, 2010), the impactof speaker identity as a machine-learning featureis more systematically tested.
They show that ex-act knowledge about which speaker produced areferring expression boosts performance, but alsofind many commonalities between different speak-ers?
strategies for content selection.
Mitchell etal.
(2011b) used participant identity in a machinelearner to successfully predict the kind of sizemodifier to be used in a referring expression.
Ad-ditionally, various submissions to the REG chal-lenges, particularly by Bohnet and Fabbrizio et al(Bohnet, 2008; Bohnet, 2009; Di Fabbrizio et al2008a; Di Fabbrizio et al 2008b) used speaker-specific POs to increase performance in their adap-tations of the IA.All of these systems used the exact speakeridentity as input, although many of the authorsnoted that groups of speakers behave similarly(Viethen and Dale, 2010; Mitchell et al 2011b).We build off of this idea by clustering similarspeakers together before learning parameters, andthen generate for speaker-specific clusters.
Thismethod results in a significant improvement in per-formance.3 LongestFirst: a New Search StrategyThe GBA guarantees to return the cheapest pos-sible subgraph that fully distinguishes the target.However, many distinguishing subgraphs can havethe same cost, for example, if a target can be iden-tified either by its color or by its size, and colorand size have the same cost.
Viethen et al(2008)discuss some examples in more detail.In the case that more than one cheapest sub-graph exists, the original GBA will generate thefirst it encountered.
Due to its branch-and-boundsearch strategy, this is also the smallest subgraph,corresponding to the shortest possible descriptionthat can be found at the cheapest cost.
Becauseits pruning mechanism does not allow further ex-pansion of a graph once it is distinguishing, thenumber of attributes that the algorithm can includeredundantly is limited, in particular if relationsare involved.
Attributes of visually salient nearbylandmark objects that are introduced to the refer-ring subgraph by a relation are only considered af-ter all other attributes of the target object.
This isthe case even if these attributes are free and featureearly in the preference order.The GBA is therefore not able to replicate manyoverspecified descriptions that human speakersmay use: if a subgraph containing a relation isalready distinguishing before the attributes of alandmark object are considered, the algorithm willnot include any information about the landmark.Not only is it unlikely that a landmark objectshould be included in a description without anyfurther information about it, it also seems intu-itive that speakers with a preference for certainattributes (such as color) would include these at-tributes not only for the target referent, but for alandmark object as well.We solve this problem by amending the searchalgorithm in a way that finds the longest of allthe cheapest subgraphs, and call the resulting al-gorithm LongestFirst.
This search strategy resultsin a much larger number of subgraphs to check, inparticular, when used with cost functions that in-volve a lot of free edges.
In order to keep our sys-tems tractable, we therefore limit the number ofattributes the LongestFirst algorithm can includeto four, based on the finding from (Mitchell et al2011a) that people rarely include more than fourmodifiers in a noun phrase.
In Experiment 2 weadditionally test a setting in which the maximumnumber of attributes is determined on the basis ofthe average description length in the training data.4 Implementation NoteThe original implementation of the GBA did notprovide a method to specify the order in whichedges were tried, although the edge order deter-mines the order in which distinguishing subgraphsare found by the algorithm (Krahmer et al 2003).This was fixed in (Viethen et al 2008) by addinga PO as parameter to the GBA to arbitrate betweenequally cheap solutions.A further issue arose in this implementationwhen tested on the GRE3D3 domain, becausethere was no simple way to specify which objecteach property belonged to; for the TUNA domainwhere the GBA has traditionally been evaluated, itis safe to always assume a property belongs to the75target referent.
We have therefore provided addi-tional functionality to the GBA that requires thatnot only hattribute, valuei pairs are specified, buthentity1, attribute, value, entity2i tuples, whichcan be translated directly into graph edges.
For ex-ample the tuple htg:relation:above:lmi representsthe edge labelled above between the yellow balland the red cube in Figure 1.
For direct attributes,such as size or color, entity1 and entity2 in thesetuples are identical, resulting in loop edges.
ThisJava implementation of the GBA and the Pythonimplementation of the LongestFirst algorithm areavailable at www.m-mitchell.com/code.5 Experiment 1: Relational DescriptionsIn our first experiment, we evaluate how well theGBA produces human-like reference in a corpusthat uses spatial relations.
We compare against theLongestFirst variant that encourages overspecifi-cation.5.1 MaterialTo evaluate the different systems, we use theGRE3D3 Corpus.
It consists of 630 distinguish-ing descriptions for objects in simple 3D scenes.Each of the 20 scenes contains three objects indifferent spatial relations relative to one another(see Figure 1).
The target referent, marked by anarrow, was always in a direct adjacency relation(on   top   of or in   front   of) to one of theother two objects, while the third object was al-ways placed at a small distance to the left or right.The objects are either spheres or cubes and differin size and color.
In addition to these attributes, the63 human participants who contributed to the cor-pus used the objects?
location as well as the spatialrelation between the target referent and the closestlandmark object.
Each participant described oneof two sets of 10 scenes.
The scenes in the two setsare not identical, but equivalent, so the sets can beconflated for most analyses.
Spatial relations wereused in 36.6% (232) of the descriptions, althoughthey were never necessary to distinguish the targetobject.
Further details about the corpus may befound in (Viethen and Dale, 2008).5.2 Approaches to Parameter SettingsAs discussed above, the GBA behaves differentlydepending on the PO and the cost functions overits edges.
To find the best match with humandata, we explore several different approaches tosetting these two parameters.
An important dis-tinction between the approaches we try hingeson the difference between attributes and proper-ties.
Attributes correspond to, e.g., color, size, orlocation, while properties are attribute-value pairs,e.g., hcolor, redi, hsize, largei, hlocation,middlei.Previous evaluations of the GBA typically usedparameter settings based on either attribute fre-quency (Viethen et al 2008) or property fre-quency (Koolen et al 2012).
We compare bothmethods for setting the parameters.
Because thescenes on which the corpus is based were not bal-anced for the different attribute-values, the fre-quency of a property is calculated as the pro-portion of descriptions in which it was used forthose scenes where the target actually possessedthis property.
For our evaluation, the trainablecosts and the POs are determined using cross-validation (see Section 5.3).
We use the followingapproaches:0-COST-PROP: All edges have 0 cost, and thePO is based on property frequency.
Each propertyis included (regardless of how distinguishing it is)until a distinguishing subgraph is found.0-COST-ATT: As 0-COST-PROP, but the PO isbased on attribute frequency.FREE-NAI?VE-PROP: Properties that occur inmore than 75% of descriptions where they couldbe used cost 0, properties with a frequency below20% cost 2, and all others cost 1 (Viethen et al2008).
The PO is based on property frequency.FREE-NAI?VE-ATT: As FREE-NAI?VE-PROP:, butcosts and PO are based on attribute frequency.K-PROP: Costs are assigned using k-means clus-tering over property frequencies with k=2 (Theuneet al 2011).
The PO is based on property fre-quency.K-ATT: As K-PROP, but the k-means clusteringand the PO are based on attribute frequency.5.3 Evaluation SetupWe evaluate the version of the GBA used by Vie-then et al(2008), with additional handling forrelations between entities (see Section 4).
Wecompare against our LongestFirst algorithm fromSection 3 on all approaches described in Sec-tion 5.2.
As baselines, we compare against theIncremental Algorithm (Dale and Reiter, 1995)and a simple informed approach that includes at-tributes/properties seen in more than 50% of the76training descriptions.
We do not use the IA?s re-lational extensions (Krahmer and Theune, 2002;Kelleher and Kruijff, 2006), because these woulddeliver the same relation-free output as the basicIA (relations are never necessary for identifyingthe target in GRE3D3).
These two baselines aretried with an attribute-based PO and a property-based one.
We do not expect a difference betweenthe attribute- and the property-based PO on the IA,as this difference would only come to the fore in asituation where a choice has to be made betweentwo values of the same attribute.
In the IA?s anal-ysis of the GRE3D3 domain, this can only happenwith relations, which it will not use in this domain.We use Accuracy and Dice, the two most com-mon metrics for human-likeness in REG (Gatt andBelz, 2008; Gatt et al 2009), to assess our sys-tems.
Accuracy reports the relative frequency withwhich the generated attribute set and the human-produced attribute set match exactly.
Dice mea-sures the overlap between the two attribute sets.For details, see, for example, Krahmer and vanDeemter?s (2012) survey paper.
We train and testour systems using 10-fold cross-validation.5.4 ResultsThe original version of the Graph-Based Algo-rithm shows identical performance for all ap-proaches (See Table 1).
All use a preference orderstarting with type, followed by color and size, anda cost function that favors the same attributes.
Asthese attributes always suffice to distinguish the in-tended referent, the algorithm stops before spatialrelations are considered.
For the scene in Figure 1it includes the minimal content htg:type:balli, butfor a number of scenes it overspecifies the descrip-tion.The LongestFirst/0-COST systems and theLongestFirst/K-PROP system are the only sys-tems that include relations in their output.The LongestFirst/0-COST systems both in-clude a relation in every description; however,not always the one that was included in thehuman-produced reference, resulting in 521false-positives for the attribute-based versionand 398 for the property-based one.
For thescene in Figure 1 they include htg:color:yellow,tg:size:small, tg:type:ball, tg:right of:obj3i andhtg:color:yellow, tg:size:small, tg:type:ball,tg:on top of:lmi, respectively.
The firstone of these two attribute sets (produced byOriginal LongestGBA First0-COST- Acc 39.21 0.16PROP Dice 73.40 68.750-COST- Acc 39.21 0.00ATT Dice 73.40 64.34FREE-NAI?VE Acc 39.21 46.51-ATT Dice 73.40 77.91FREE-NAI?VE Acc 39.21 38.10-PROP Dice 73.40 74.99K-PROP Acc 39.21 35.08Dice 73.40 74.66K-ATT Acc 39.21 35.08Dice 73.40 74.5650%-Base IAprop- Acc 27.30 37.14based PO Dice 72.17 72.21att- Acc 24.92 37.14based PO Dice 71.16 72.21Table 1: Experiment 1: System performance in %.We used  2 on Accuracy and paired t-tests on Diceto check for statistical significance.
The best per-formance is highlighted in boldface.
It is statisti-cally significantly different from all other systems(Acc: p < 0.02, Dice: p < 0.0001).LongestFirst/0-COST-ATT) includes the rela-tion between the target and the third objectto the right, which was almost never includedin the human-produced references, leading tomany false-positives.
The LongestFirst/K-PROPsystem results in only 45 true-positives and81 false-positives.
It includes the attribute sethtg:color:yellow, tg:type:balli for Figure 1.One of its relational descriptions (for Scene 5)contains the set htg:size:small, tg:color:blue,tg:on top of:lmi.The 50%-baseline system outperforms theLongestFirst/0-COST systems, which illustratesthe utility of cost functions in combination witha PO.
It includes the attribute set htg:color:yellow,tg:type:balli for the scene in Figure 1.
The bestperforming system is the LongestFirst algorithmwith the attribute-based FREE-NAI?VE approach,although this system produces no spatial relations.6 Experiment 2: Individual VariationWe now extend our methods to take into accountindividual variation in the content selection forreferring expressions, and evaluate whether wehave better success at reproducing participants?
re-lational descriptions.
Rather than using speakeridentity as an input parameter to the system (Sec-tion 2.3), we automatically find groups of people77who behave similarly to each other, but signifi-cantly different to speakers in the other groups.6.1 Evaluation SetupWe use k-means clustering to group the speak-ers in the GRE3D3 Corpus based on the numberof times they used each attribute and the averagelength of their descriptions.
We tried values be-tween 2 and 5 for k, but found that any valueabove 2 resulted in two very large clusters accom-panied by a number of extremely small clusters.As these small clusters would not be suitable forx-fold cross-validation, we proceed with two clus-ters, one consisting of speakers preferring rela-tively long descriptions that often contain spatialrelations (Cluster CL0, 16 speakers, 160 descrip-tions), and one consisting of speakers preferringshort, non-relational descriptions (Cluster CL1, 47speakers, 470 descriptions).We train cost functions and POs separately forthe two clusters in order to capture the differentbehaviour patterns they are based on.
We use theFREE-NAI?VE cost functions for this experiment,which outperformed all others in Experiment 1.We again use 10-fold cross-validation for the eval-uation.
In this experiment, we vary the maximumlength setting for the LongestFirst algorithm.
InExperiment 1, the maximum length for a referringexpression was set to 4 based on previous empiri-cal findings.
Here we additionally test setting it tothe rounded average length for each training fold.On Cluster CL0 this average length is 6 in all folds,on Cluster CL1 it is 3.6.2 ResultsAs shown in Table 2, the LongestFirst algorithmperforms best at generating human-like spatial re-lations (Cluster CL0), with property-based param-eters and a maximum description length deter-mined by the training set.
It produces the attributeset hlm:type:cube, tg:on top of:lm, tg:type:ball,tgcolouryellow, lm:colour:redi for Figure 1.
Thedifference to the other systems is statistically sig-nificant for both Accuracy ( 2>15, p<0.0001)and Dice (t>13, p<0.0001).
The attribute-basedparameters and the original GBA perform verybadly on this cluster.
For participants who donot tend to use spatial relations (Cluster CL1),the maximum length setting has no influence,but attribute-based parameters perform better thanproperty-based ones.
The attribute-based Longest-First systems also outperform the original GBACL0 CL1 avgFN Acc 19.38 48.94 41.43LongestFirst -PROP Dice 75.61 80.27 79.08-max-av FN Acc 0.00 60.00 44.76-ATT Dice 55.74 85.28 77.78FN Acc 0.63 48.94 36.67LongestFirst -PROP Dice 72.15 80.21 78.17-max4 FN Acc 0.00 60.00 44.76-ATT Dice 59.01 85.28 78.61FN Acc 5.00 48.30 37.30Original -PROP Dice 49.36 80.77 72.79GBA FN Acc 5.00 50.85 39.21-ATT Dice 49.36 81.58 73.40Table 2: Experiment 2: Performance in % of theLongestFirst and OriginalGraph algorithms on thetwo speaker clusters and overall using the FREE-NAI?VE (FN) approaches.
We used  2 on Accu-racy and paired t-tests on Dice to check for statis-tical significance.
The best performance in eachcolumn and those that are statistically not signifi-cantly different are highlighted in boldface.on CL1, but interestingly none of the differencesare as large as on CL0.
For the scene in Fig-ure 1 they produce the attribute set htg:type:ball,tg:colour:yellowi.The average results over both clusters (shownin the last column Table 2) are not conclusiveas to which setting should be used overall, al-though it is clear that the LongestFirst version ispreferable when evaluated by Dice.
The differ-ent result patterns on the two clusters suggest thatthe different referential behaviour of the partici-pants in the two clusters are ideally modeled us-ing different parameters.
In particular, it appearsthat property-based costs are useful for replicat-ing descriptions containing relations to other ob-jects, while attribute-based costs are useful forreplicating shorter descriptions.
The best over-all performance, achieved by combining the bestperforming systems on each cluster (LongestFirst-max-av/FN-PROP on CL0 and LongestFirst/FN-ATT with either maximum length setting on CL1),lies at 49.68% Accuracy and 82.83% Dice.
TheDice score in this combined model is significantlyhigher than the best achieved by LongestFirst-max-av/FN-PROP and from the best Dice scoreachieved on the unclustered data in Experiment 1(t=8.2, p<0.0001).
The difference in Accuracy isnot significant ( 2=1.2, p> 0.2).To get an idea of how successful the newLongestFirst approach is at replicating the use ofrelations on the clustered data, we take a closerlook at the output of the best-performing systems78on the two clusters.
On CL0, the cluster of partic-ipants who produce longer descriptions contain-ing more spatial relations, the best match to thehuman data comes from LongestFirst-max-av/FN-PROP.
147 of the 160 descriptions in this clustercontain a relation, and the system includes the cor-rect relation for all 147.
It falsely also includes arelation for the remaining 13 descriptions.
Thisshows that with the appropriate parameter settingsthe LongestFirst algorithm is able to replicate hu-man relational reference behaviour, but personalspeaker preferences are the main driving factor forthe human use of relations.CL1, the cluster with shorter descriptions,contains only 85 (18%) relational descriptions.The best performing system on this cluster(LongestFirst/FN-ATT) does not produce any rela-tions.
This is not surprising as the cost functionsand POs for this cluster are necessarily dominatedby the non-relational attributes used more regu-larly.
The cases in which relations are used stemfrom participants who do not show a clear prefer-ence for or against relations and would thereforebe hard to model in any system.
With more data itmight be possible to group these participants intoa third cluster and find suitable parameter settingsfor them.
This would only be possible if their useof relations is influenced by other factors availableto the algorithm, such as the spatial configurationof the scene.
Viethen and Dale?s (2008) analysis ofthe GRE3D3 Corpus suggests that this is the caseat least to some extent.7 Conclusions and Future WorkWe have evaluated the Graph-Based Algorithm forREG (Krahmer et al 2003) as well as a novelsearch algorithm, LongestFirst, that functions onthe same graph-based representation, to assesstheir ability to generate referring expressions thatcontain spatial relations.
We coupled the searchalgorithms with a number of different approachesto setting the cost functions and preference ordersthat guide the search.In Experiment 1, we found that ignoring the costfunction (our 0-cost approaches) is not helpful; butthe LongestFirst algorithm, which produces longerdescriptions, leads to more human-like output forthe visuospatial domain we evaluate on than theoriginal Graph-Based Algorithm or the Incremen-tal Algorithm (Dale and Reiter, 1995).
However,in order for spatial relations to be included in ahuman-like way, it was necessary to take into ac-count speaker preferences.
We modeled these inExperiment 2 by clustering the participants whohad contributed to the evaluation corpus based ontheir referential behaviour.
By training separatecost functions and preference orders for the dif-ferent clusters, we enabled the LongestFirst al-gorithm to correctly reproduce 100% of relationsused by people who regularly mentioned relations.Our findings suggest that the graph-based rep-resentation proposed by Krahmer et al(2003)can be used to successfully generate relational de-scriptions, however their original search algorithmneeds to be amended to allow more overspecifica-tion.
Furthermore, we have shown that variationin the referential behaviour of individual speak-ers has to be taken into account in order to suc-cessfully model the use of relations in referringexpressions.
We have proposed a clustering ap-proach to advance this goal based directly on thereferring behaviour of speakers rather than speakeridentity.
We have found that the best models usefine-grained property-based parameters for speak-ers who tend to use spatial relations, and coarserattribute-based parameters for speakers who tendto use shorter descriptions.In future work, we hope to expand to morecomplex domains, beyond the simple propertiesavailable in the GRE3D3 Corpus.
We also aimto explore further graph-based representations andsearch strategies, modeling non-spatial propertiesas separate vertices, similar to the approach byCroitoru and van Deemter (2007).8 AcknowledgementsViethen and Krahmer received financial supportfrom The Netherlands Organization for ScientificResearch, (NWO, Vici grant 277-70-007), andMitchell received financial support from the Scot-tish Informatics and Computer Science Alliance(SICSA), which is gratefully acknowledged.ReferencesAnja Arts, AlfonsMaes, Leonard Noordman, and CarelJansen.
2011.
Overspecification in written instruc-tion.
Linguistics, 49(3):555?574.Anja Belz and Albert Gatt.
2007.
The Attribute Se-lection for GRE Challenge: Overview and evalu-ation results.
In Proceedings of the Workshop onUsing Corpora for NLG: Language Generation and79Machine Translation (UCNLG+MT), pages 75?83,Copenhagen, Denmark.Bernd Bohnet.
2008.
The fingerprint of human refer-ring expressions and their surface realization withgraph transducers.
In Proceedings of the 5th Inter-national Conference on Natural Language Genera-tion, pages 207?210, Salt Fork OH, USA.Bernd Bohnet.
2009.
Generation of referring expres-sion with an individual imprint.
In Proceedings ofthe 12th European Workshop on Natural LanguageGeneration, pages 185?186, Athens, Greece.Madalina Croitoru and Kees van Deemter.
2007.
Aconceptual graph approach to the generation of re-ferring expressions.
In Proceedings of the 20thInternational Joint Conference on Artificial Intelli-gence, pages 2456?2461, Hyderabad, India.Robert Dale and Nicholas Haddock.
1991.
Gener-ating referring expressions involving relations.
InProceedings of the 5th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 161?166, Berlin, Germany.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the Gricean maxims in the gener-ation of referring expressions.
Cognitive Science,19(2):233?263.Robert Dale.
1989.
Cooking up referring expressions.In Proceedings of the 27th Annual Meeting of the As-sociation for Computational Linguistics, pages 68?75, Vancouver BC, Canada.Giuseppe Di Fabbrizio, Amanda Stent, and SrinivasBangalore.
2008a.
Referring expression generationusing speaker-based attribute selection and trainablerealization (ATTR).
In Proceedings of the 5th Inter-national Conference on Natural Language Genera-tion, pages 211?214, Salt Fork OH, USA.Giuseppe Di Fabbrizio, Amanda J. Stent, and SrinivasBangalore.
2008b.
Referring expression generationusing speaker-based attribute selection and train-able realization.
In Twelfth Conference on Com-putational Natural Language Learning, Manchester,UK.Paul E. Engelhardt, Karl D. Bailey, and Fernanda Fer-reira.
2006.
Do speakers and listeners observe thegricean maxim of quantity?
Journal of Memory andLanguage, 54:554?573.Albert Gatt and Anja Belz.
2008.
Attribute selectionfor referring expression generation: New algorithmsand evaluation methods.
In Proceedings of the5th International Conference on Natural LanguageGeneration, pages 50?58, Salt Fork OH, USA.Albert Gatt, Anja Belz, and Eric Kow.
2008.
TheTUNA Challenge 2008: Overview and evaluationresults.
In Proceedings of the 5th InternationalConference on Natural Language Generation, pages198?206, Salt Fork OH, USA.Albert Gatt, Anja Belz, and Eric Kow.
2009.
TheTUNA-REG Challenge 2009: Overview and eval-uation results.
In Proceedings of the 12th EuropeanWorkshop on Natural Language Generation, pages174?182, Athens, Greece.Pamela W. Jordan and Marilyn Walker.
2005.
Learn-ing content selection rules for generating object de-scriptions in dialogue.
Journal of Artificial Intelli-gence Research, 24:157?194.John Kelleher and Geert-Jan Kruijff.
2006.
Incre-mental generation of spatial referring expressions insituated dialog.
In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics, pages 1041?1048, Syd-ney, Australia.Ruud Koolen and Emiel Krahmer.
2010.
The D-TUNA Corpus: A dutch dataset for the evaluationof referring expression generation algorithms.
InProceedings of the 7th International Conference onLanguage Resources and Evaluation, Valetta, Malta.Ruud Koolen, Emiel Krahmer, and Marie?t Theune.2012.
Learning preferences for referring expressiongeneration: Effects of domain, language and algo-rithm.
In Proceedings of the 7th International Nat-ural Language Generation Conference, pages 3?11,Starved Rock, IL, USA.Emiel Krahmer and Marie?t Theune.
2002.
Effi-cient context-sensitive generation of referring ex-pressions.
In Kees van Deemter and Rodger Kibble,editors, Information Sharing: Reference and Pre-supposition in Language Generation and Interpre-tation, pages 223?264.
CSLI Publications, StanfordCA, USA.Emiel Krahmer and Kees van Deemter.
2012.
Compu-tational generation of referring expressions: A sur-vey.
Computational Linguistics, 38(1):173?218.Emiel Krahmer and Ielka van der Sluis.
2003.
A newmodel for generating multimodal referring expres-sions.
In Proceedings of the 9th European Workshopon Natural Language Generation, pages 47?57, Bu-dapest, Hungary.Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg.2003.
Graph-based generation of referring expres-sions.
Computational Linguistics, 29(1):53?72.Margaret Mitchell, Aaron Dunlop, and Brian Roark.2011a.
Semi-supervised modeling for prenominalmodifier ordering.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages236?241, Portland OR, USA.Margaret Mitchell, Kees van Deemter, and Ehud Re-iter.
2011b.
Applying machine learning to thechoice of size modifiers.
In Proceedings of the 2ndWorkshop on the Production of Referring Expres-sions, Boston MA, USA.80Thomas Pechmann.
1989.
Incremental speech produc-tion and referential overspecification.
Linguistics,27:89?110.Susan Sonnenschein.
1985.
The development of ref-erential communication skills: Some situations inwhich speakers give redundant messages.
Journalof Psycholinguistic Research, 14(5):489?508.Marie?t Theune, Ruud Koolen, Emiel Krahmer, andSander Wubben.
2011.
Does size matter - howmuch data is required to train a REG algorithm?
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 660?664, Portland OR,USA.Kees van Deemter and Emiel Krahmer.
2007.
Graphsand Booleans: On the generation of referring ex-pressions.
In Harry C. Bunt and Reinhard Muskens,editors, Computing Meaning, volume 3, pages 397?422.
Kluwer, Dordrecht, The Netherlands.Kees van Deemter, Albert Gatt, Ielka van der Sluis,and Richard Power.
2012.
Generation of referringexpressions: Assessing the incremental algorithm.Cognitive Science, 36(5):799?836.Ielka van der Sluis.
2005.
Multimodal Reference, Stud-ies in Automatic Generation of Multimodal Refer-ring Expressions.
Ph.D. thesis, Tilburg University,The Netherlands.Jette Viethen and Robert Dale.
2006.
Algorithms forgenerating referring expressions: Do they do whatpeople do?
In Proceedings of the 4th InternationalConference on Natural Language Generation, pages63?70, Sydney, Australia.Jette Viethen and Robert Dale.
2008.
The use of spatialrelations in referring expression generation.
In Pro-ceedings of the 5th International Conference on Nat-ural Language Generation, pages 59?67, Salt ForkOH, USA.Jette Viethen and Robert Dale.
2009.
Referring ex-pression generation: What can we learn from humandata?
In Proceedings of the 2009 Workshop on Pro-duction of Referring Expressions: Bridging the GapBetween Computational and Empirical Approachesto Reference, Amsterdam, The Netherlands.Jette Viethen and Robert Dale.
2010.
Speaker-dependent variation in content selection for refer-ring expression generation.
In Proceedings of the8th Australasian Language Technology Workshop,pages 81?89, Melbourne, Australia.Jette Viethen, Robert Dale, Emiel Krahmer, Marie?tTheune, and Pascal Touset.
2008.
Controlling re-dundancy in referring expressions.
In Proceedingsof the 6th International Conference on LanguageResources and Evaluation, Marrakech, Morocco.81
