Phonological  Parsing for Bi-directionalLet ter - to-Sound/Sound-to-Letter  Generat ion 1Helen M. Meng, Stephanie Seneff and Victor W. ZueSpoken Language Systems Group, Laboratory for Computer ScienceMassachusetts Institute of Technology, Cambridge, Massachusetts 02139ABSTRACTIn this paper, we describe a reversible tter-to-sound/sound-to-letter generation system based on an approach which com-bines a rule-based formalism with data-driven techniques.
Weadopt a probabilistic parsing strategy to provide ahierarchicallexical analysis of a word, including information such as mor-phology, stress, syllabification, phonemics and graphemics.Long-distance constraints are propagated by enforcing localconstraints throughout the hierarchy.
Our training and test-ing corpora are derived from the high-frequency portion ofthe Brown Corpus (10,000 words), augmented with markersindicating stress and word morphology.
We evaluated ourperformance based on an unseen test set.
The percentageof nonparsable words for letter-to-sound and sound-to-lettergeneration were 6% and 5% respectively.
Of the remainingwords our system achieved a word accuracy of 71.8~0 anda phoneme accuracy of 92.5% for letter-to-sound generation,and a word accuracy of 55.8% and letter accuracy of 89.4%for sound-to-letter generation.
We also compared our hierar-chical approach with an alternative, single-layer approach todemonstrate how the hierarchy provides a parsimonious de-scription for English orthographic-phonological regularities,while simultaneously attaining competitive generation accu-racy.INTRODUCTIONThis paper describes a trainable probabilistic systemfor reversible letter-to-sound/sound-to-letter generation.Sound-to-letter generation is a crucial aspect in the prob-lem of automatic detection/incorporation of ew words,which is in turn critical for the development of large vo-cabulary speech understanding systems.
Moreover, letter-to-sound generation will continue to be important forspeech output, especially in applications uch as read-ing machines.
To successfully achieve our goal, severalimportant issues must be addressed.
First, what shouldbe the inventory of linguistic or lexical units for describ-ing English orthographic-phonological regularities?
Sec-ond, how should these units be incorporated into therepresentation of English orthography and phonology?Third, what algorithms can be used to synthesize and an-alyze the spelling and pronunciation of an English word1This research was supported by ARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research, and agrant from Apple Computer Inc.in terms of these lexical units?
These three issues willbe addressed in detail in the following when we describeour approach and report on our system's performancefor both letter-to-sound \[1\] and sound-to-letter genera-tion \[2\].
The novel features of our approach include thereversibility of the combined parsing and generative pro-cesses, the ability to provide multiple output hypotheses,the capability of handling uncertainty in the input, aswell as our treatment of non-parsab!e words.PREVIOUS WORKLet ter - to -Sound Generat ionOne of the first approaches adopted for letter-to-soundgeneration is typified by MITalk \[8\].
It follows the theo-ries of generative grammar and the transformational cy-cle as proposed by Chomsky and Halle \[3\].
A large set ofordered cyclical rules are applied in turn to the word inquestion until a final pronunciation emerges.
While theprocess of establishing the appropriate rule set was te-dious and time-consuming, the resulting system achieveda degree of accuracy that, to our knowledge, has not yetbeen matched by other more automatic techniques.Because the generation of cyclical rules is a difficultand complicated task, several research groups have at-tempted to acquire letter-to-sound generation systemsthrough automatic or semi-automatic data-driven tech-niques, based on neural nets or on an information theo-retic approach.
Typically, the goal is to provide as littlea priori  information as possible, ideally, only a set ofpairings of letter sequences with corresponding (alignedor unaligned) phone sequences.
Iterative training algo-rithms then produce a probability model that is appliedto predict the most likely pronunciation.
Probably thebest known of these systems is NETtalk \[4\], which learnsa pronunciation of the current letter by considering thesix surrounding letters as input to the neural network.Lucassen and Mercer \[5\] acquired a set of rules automat-ically from a large lexicon of phonetically labelled databy growing decision trees using a criterion based on mu-tual information.
Although direct comparisons of per-formance of different systems is difficult due to the lackof standardized phone sets, data sets, or scoring algo-289rithms, these systems have reported phone accuracies inthe low 90's in terms of the percent of letters correctlypronounced.Sound- to -Let te r  Generat ionTo our knowledge, there has been very little previouswork reported in the literature addressing the problemof sound-to-letter generation.
We are aware of only twoprior research efforts in this area.Lucas and Damper \[6\] developed a system for bi-directional text-phonetics translation using two neuralnetworks to perform statistical string translation.
Thissystem does not require pre-aligned text-phonetic pairsfor training, but instead tries to infer appropriate seg-mentations and alignments.
In a phonetics-to-text trans-lation task using two disjoint 2,000-word corpora for train-ing mad testing, they reported a 71.3% letter and a 22.8%word accuracy.Another related effort was conducted by Alleva andLee \[7\], who used HMMs to model the acoustics of train-ing sentences based on the orthographic transcriptions.Context-dependent quad-letter acoustic models were train-ed with 15,000 sentences, and used in conjunction with a5-gram letter language model.
Testing on a disjoint cor-pus of 30 embedded and end-point detected words (placeand ship names) gave a 39.3% letter error rate and 21.1%word accuracy.
However, this result is not directly com-parable to our work because the phonemic/phonetic rep-resentation is bypassed.A H IERARCHICAL  LEX ICALREPRESENTATIONIt has long been realized from research in speech syn-thesis that a variety of linguistic knowledge sources playan important role in determining English letter/soundcorrespondences \[8\].
For example, part-of-speech causesthe noun and verb forms of "record" to be pronounceddifferently.
A morphological boundary causes the lettersequence "sch" in "discharge" to be realized differentlyfrom that in "school" or "scheme".
Stress changes theidentity of vowels in a word, e.g.
"define" vs. "defini-tion".
Also, syllabic constraints are expressible in termsof the sequential ordering of distinctive features - sonor-ity sequencing in manner features, and phonotactic con-straints in place and voicing features.
Furthermore, thereare graphemic onstraints for letter to letter transitions.A novel feature Of our system is that multiple layers ofrepresentation are incorporated to capture short and longdistance constraints.
These include word class, morphs,syllables, manner classes, phonemes and graphemes.We created a framework which describes the spellingand pronunciation of English words using only a smallinventory of labels associated with the aforementionedWORD 1.
Top-levelPRE ROOT SUF ISUF 2.
MorphologyI i J ISSYL1 RSYL RSYL 3.
Syl.
Stresso.
o.oc  r oloc co r, .oc  coo,STOP VOW STOP VOW STOP VOW STOP VOW STOP 5.
Broadclessesi I I I I I i \] id d 0 k d 6.
Phonemesi l l  I i i i ld ?
d I ?
a te * d 7.
GraphemesFigure 1: Parse tree for "dedicated" with different linguisticlayers indicated numerically.morphological and phonological units.
These units areorganized as a hierarchical tree structure, where the var-ious levels of linguistic knowledge are collectively used todescribe orthographic-phonological regularities.
Figure 1illustrates the description of the word "dedicated".The higher levels encode longer distance constraints,while the lower levels carry more local constraints.
Byallowing the terminal nodes to be dual in nature (i.e.,representing either phones or letters), we can create di-rect symmetry between the letter-to-sound and sound-to-letter generation tasks simply by swapping the in-put/output specification.One should note in Figure 1 that \[*\] is a graphemic"place-holder" introduced to maintain consistency be-tween the representations of the words "dedicate" and"dedicated", where an inflexional suffix \[ISUF\] has beenattached to the latter word.
Another noteworthy detailis the special \[M-ONSET\] category, which signifies thatthe letter 'c' should belong to the root "-dic-",2 but has.become a moved onset of the next syllable due to syllab-ification principles uch as the Maximal Onset Principleand the Stress Resyllabification Principle.
aTHE PARSING ALGORITHMWe are adopting a technique that represents a crossbetween explicit rule-driven strategies and strictly data-driven approaches.
About 100 generalized context-freerules, such as those illustrated in Table 1 are writtenby hand, and training words are parsed using TINA \[9\],according to their marked linguistic specifications.
Theparse trees of format as show in Figure 1 are then used2According to Webster's New World Dictionary, the root of"dedicated" is "-dic-", which is derived from the Latin word"dicare'.3The Maximal Onset Principle states that the number of conso-nants in the onset position should be maximized when phonotacticand morphological constraints permit, and Stress Resyllabificationrefers to maximizing the number of consonants in stressed syllables.290wordrootstressed-syllablenucleusnasa l--~ \[prefix\] root \[SUffLX\]--~ stressed-syllable \[reduced-syllable\]--* \[onset\] nucleus \[coda\]vowel( /m/  In /  /rj/)/m/  --~ ("m ... .
me ....  mn .... mb .... mm .... mp")Table 1: Example rules at each of the different layers.to train the probabilities in a set of "layered bigrams"\[10\].
We have chosen a probabilistic parsing paradigmfor four reasons: First, the probabilities serve to augmentthe known structural regularities that can be encoded insimple rules with other structural regularities which maybe automatically discovered from a large body of trainingdata.
Secondly, since the more probable parse theoriesare distinguished from the less probable ones, search ef-forts can selectively concentrate on the high probabilitytheories, which is an effective mechanism for perplex-ity reduction.
Thirdly, probabilities are less rigid thanrules, and adopting a probabilistic framework allows usto easily generate multiple parse theories.
Fourthly, theflexibility of a probabilistic framework also enables us toautomatically relax constraints to attain better coverageof the data.T ra in ing  ProcedureThe layered bigrams formalism attaches probabili-ties to sibling-sibling transitions in context-free grammarrules.
It has been shown to achieve a low perplexityat the linguistic level within the ATIS domain \[10\].
Forour current sub-word application, we have modified thelayered-bigrams in two ways: (1) parse trees are gener-ated in a bottom-up fashion instead of top-down, and(2) the contextual information used in bottom-up pre-diction includes the complete history in the immediateleft column.Our experimental corpus consists of the 10,000 mostfrequent words appearing in the Brown Corpus \[11\], whereeach word entry contains a spelling and a single unalignedphoneme string.
We used about 8,000 words for training,and a disjoint set of about 800 words for testing.The set of training probabilities are estimated by tab-ulating counts using the training parse trees.
4 It includesbottom-up rediction probabilities for each category inthe parse tree, and column advancement probabilities forextending a column to the next terminal.
The same set ofprobabilities are used for both letter-to-sound and sound-to-letter generation.Tes t ing  ProcedureIn letter-to-sound generation, the system takes in aspelling as an input, generates a parse tree in a bottom-4See \[1\] for a more detai led description of this process.up left-to-right fashion, and derives a phonemic pronunci-ation from the complete parse: In sound-to-letter gener-ation, the system accepts a string of phonemes as input,and generates letters.
An inadmissible stack decodingsearch algorithm is adopted for its simplicity.
If multiplehypotheses are desired, the algorithm can terminate af-ter multiple complete hypotheses have been popped offthe stack.
These hypotheses are subsequently re-rankedaccording to their actual parse score.
Though our searchis inadmissible, we are able to obtain multiple hypothesesinexpensively with satisfactory performance.EXPERIMENTAL  RESULTSExperiments on both letter-to-sound and sound-to-letter generation were conducted using 26 letters, onegraphemic place-holder and 52 phonemes (including sev-eral unstressed vowels and pseudo diphthongs such as /or/).
Each entry in the test corpus contains a spellingcorresponding to a single pronunciation.
The genera-tion procedures use evaluation criteria that directly mir-ror one another.
Word accuracy is the percentage ofparsable words for which the top-ranking theory gener-ates a spelling/pronunciation that matches the lexicalentry exactly.
Non-parsable words are those for whichno sPelling/pronunciation output is produced.
"Top N"word accuracy refers to the percentage of parsable wordsfor which the correctly generated spelling/pronunciationappears in the top N complete theories.
Letter/Phonemeaccuracies include insertion, substitution and deletion er-ror rates, and are obtained using the program providedby NIST for evaluating speech recognition systems.Resu l ts  on  Let ter - to -Sound Generat ionIn letter-to-sound generation, about 6% of the test setwas nonparsable.
This set consists of compound words,proper names, and words that failed due to sparse dataproblems.
Results for the parsable portion of the test setare shown in Table 2.
The 69.3% word accuracy corre-sponds to a phoneme accuracy of 91.7%, where an inser-tion rate of 1.2% has been taken into account.Thus far there are no standardized evaluation meth-ods for text-to-speech systems, and therefore comparisonamong different systems remains difficult.
Errors in thegenerated stress pattern and/or phoneme insertion er-rors are often neglected.
Evaluation criteria that havebeen used include word accuracy, accuracy per phonemeand accuracy per letter (in measuring the accuracy perletter, silent letters are regarded as mapping to a \[NULL\]phone).
We believe that accuracy per letter would gener-ally be higher than accuracy per phoneme, because thereare generally more letters than phonemes per word, andthe letters mapping to the generic ategory \[NULL\] wouldusually be correct.
To verify our claim, we computed thetwo measurements based on our training set, using thealignment provided by the training parse trees.
Our re-291Accuracy top choicetrain wordphonemetest wordphonemetop 5 top 10correct correct correct77.3% 93.7% 95.7%94.2% - -69.3% 86.2% 87.9%91.7% - -Table 2: Letter-to-Sound-Generation Experiments: Wordand Phoneme Accuracy for Training and Testing dataAccuracytrain!
wordlettertest wordI lettertop choice top 5 top 10correct correct correct58.8% 85.0% 89.3%90.6% - -51.9% 77.0% 81.1%88.6% - -Table 3: Sound-to-Letter Generation Experiments:and Letter Accuracy for Training and Testing dataWord100'701 mm?15O0 10 20 50Rank of Correct PronunciationFigure 2: Letter-to-Sound: Percent correct whole-word the-ories as a function of N-best depth for the test set100'& |50,7o,=eo.50f - - -Rank of Correct SpellingFigure 3: Sound-to-Letter: Percent correct whole-word the-ories as a function of N-best depth for the test setsult shows that a per letter measurement would lead toa .10% reduction in error rate.Figure 2 is a plot of cumulative percent correct ofwhole word theories as a function of the N-best depthfor the test set.
Although 30 complete theories were gen-erated for each word, no correct theories occur beyondN ----18 after resorting, with an asymptotic value of justover 89%.Resu l ts  on  Sound- to -Let ter  Generat ionIn sound-to-letter generation, about 4% of the testset was nonparsable.
Results for the parsable words areshown in Table 3; top-choice word accuracy for sound-to-letter is about 52%.
This corresponds to a letter accu-racy of 88.6%, with an insertion error rate of 2.5% takeninto account.
This performance compares favorably withthose reported in previous work.Figure 3 is a plot of the cumulative percent correct(in sound-to-letter generation) of whole word theories asa function of N-best depth of the test set.
The asymp-tote of the graph shows that the first 30 complete the-ories generated by the parser contain a correct theoryfor about 83% of the test words.
Within this pool, re-sorting using the actual parse score has put the correcttheory within the top 10 choices for about 81% of thecases, while the remaining 2% have their correct theo-ries ranked between N = 10 and N = 30.
Resortingseems to be less effective in the sound-to-letter case, pre-sumably because many more "promising" theories canbe generated than for letter-to-sound.
A possible rea-son for this is the ambiguity in phoneme-to-letter map-ping, and another eason is that geminant letters are of-ten mapped to the same (consonantal) phoneme.
Forexample, the generated spellings from the pronunciationof "connector" i.e., the phoneme string (k t n e k t a~),include: "conecter", "conector", "connecter", "connec-tor", "conectar", "conectyr", "conectur", "connectyr","eonnectur", "conectter', "connectter" and "cannecter'.Many of these hypotheses can be rejected with the avail-'ability of a large lexicon of legitimate English spellings.E r ror  Ana lysesBoth of the cumulative plots shown above reach anasymptotic value well below 100%.
The words that be-long to the portion of the test set lying above the asymp-tote appear intractable - a correct pronunciation/spellingdid not emerge as one of the 30 complete theories.
De-tailed analysis of these words shows that they fall into ap-proximately 4 categories.
(1) Generated pronunciationsthat have subtle deviations from the reference strings.
(2)Unusual pronunciations due to influences from foreignlanguages.
(3) Generated pronunciations which agreewith the regularity of English letter-phoneme mappings,but were nevertheless incorrect.
(4) Errors attributableto sparse data problems.
Some examples are shown inTable 4.
It is interesting to note that there is much over-lap between the set of problematic words in letter-to-sound and sound-to-letter generation.
This implies that292Category correct generated generated correctspelling spelling pronunciation pronunciation(1) Subtle acquiring equiring IkwoYrzl\] i kwo/~l l jbalance balence correct ba~hnslaunch lawnch correct lon5pronounced pronounst pnnoWnst proWnaWnst(2) Umzsual champagne shampain ~a~mplgniY ~a:mpeYndebris dibree diYbns dlbriY(3) Regular basis correct ba~sls beYstselite aleat doYt diYtviolence viallence correct voYdmsviscosity viscossity v,skoWs,ti y vIskos~ti y(4) Sparse braque brack bra~kwiY bra~kTab le  4: Some examples of generation errorsimprovements made in one generative direction shouldcarry over to the opposite direction as well.EVALUATING THEH IERARCHYWe believe that the higher level linguistic knowlegeincorporated in the hierarchy is important for our gener-ation tasks.
Consequently, we would like to empiricallyassess: (1) the relative contribution of the different lin-guistic layers towards generation accuracy, and (2) therelative merits of the overall design of the hierarchical lex-ical representation.
Our studies \[13\] are based on letter-to-sound generation only, although we expect hat theimplications of our study should carry over to sound-to-letter generation.Invest igat ions  on  the  H ierarchyThe implementation f our parser is flexible, in that itcan train and test on a variable number of layers in thehierarchy.
This enables us to explore the relative con-tribution of each linguistic level in the generation task.We conducted a series of experiments whereby an in-creasing amount of linguistic knowledge (in terms of thenumber of layers in the hierarchy) is omitted from thetraining parse trees.
For each reduced configuration, thesystem is re-trained and re-tested on the same trainingand testing corpora as described earlier.
For each ex-periment we compute the top-choice word accuracy andperplexity, which reflect the amount of constraint pro-vided by the hierarchical representation.
We also mea-sure the coverage to show the extent o which the parsercan generalize to account for previously unseen struc-tures, and count the number off system parameters inorder to observe the computational load, as well as theparsimony of the hierarchical framework in capturing En-glish orthographic-phonological regularities.
We foundthat for every layer omitted from the representation, lin-guistic constraints are lost, manifested as a lower gener-ation accuracy, higher perplexity and greater coverage.Fewer layers also require fewer training parameters.The significant exception was the case of omitting thelayer of broad classes (layer 5), which seems to introduceadditional constraints, thus giving the highest generationperformance.
The word accuracy based on the parsableportion of the test set was 71.8%, 5 which corresponds toa phoneme accuracy of 92.5%.
This improvement 6 canbe understood by realizing that broad classes can be pre-dicted from phonemes with certainty, and the inclusion ofthe broad class layer probably led to excessive smoothingacross the individual phonemes within each broad class7Again, about 6% of the test set was nonparsable.
When arobust parsing scheme is used to recover the nonparsablewords, 100% coverage was achieved, but performance de-grades to 69.2% word and 91.3% phoneme accuracy.Compar i son  w i th  a S ing le -Layer  ApproachWe also compared our current hierarchical frameworkwith an alternative approach which uses a single-layerrepresentation.
Here, a word is represented mainly byits spelling and an aligned phonemic transcription, us-ing the \[NULL\] phoneme for silent letters.
The alignmentis based on the training parse trees from the hierarchicalapproach.
For example, "bright" is transcribed as /b  raYNULL NULL \[/ .
The word is then fragmented exhaustivelyto obtain letter sequences (word fragments) shorter thana set maximum length.
During training, bigram proba-bilities and phonemic transcription probabilities are com-puted for each letter sequence.
Therefore this approach5When normalized on the entire test set, the word accuracybecomes 67.5%.6We also found improvement on sound-to-letter generation -55.8% word accuracy on the parsable test words, corresponding to89.4% letter accuracy, and 5% of the words were nonparsable.7However, broad classes may still serve a role as a "fast match"layer in recognition experiments, where their predictions could nolonger be certain, due to recognition errors.293captures some graphemic onstraints within the wordfragment, but higher level linguistic knowledge is not ex-plicitly incorporated.
Letter-to-sound generation is ac-complished by finding the "best" concatenation of lettersequences which constitutes the spelling of the test word.TO facilitate comparison with the hierarchical pproach,we use the same training and test sets to run letter-to-sound generation experiments with the single-layer ap-proach.
Several different value settings were used forthe maximum word fragment length.
We expect genera-tion accuracy to improve as the maximum word fragmentlength increases, because longer letter sequences can cap-ture more context.
However, this should be accompaniedby an increase in the number of system parameters due tothe combinatorics of the letter sequences.
Furthermore,there are no nonparsable test words in the single-layerapproach, because it can always "backolT' to mapping asingle letter to its most probable phoneme.The hierarchical approach (without the broad classlayer) achieved the same performance as the highest per-forming single-layer approach, which allowed a maximumfragrnent length of 6.
8 The mean fragment length of thesegmentations u ed in the test set by the single-layer ap-proach was 3.7, while the mean grapheme length usedby the hierarchical approach was only 1:2.
The hierar-chical approach is capable of reversible generation us-ing about 32,000 parameters, while the single-layer ap-proach requires 693,300 parameters (a 20-fold increase)for uni-directional letter-to-sound generation.
In orderto achieve reversibility, the number of parameters wouldhave to be doubled.DISCUSSIONOur current work demonstrates the utility of a hi-erarchical framework, which is relatively rich in linguis-tic knowledge, for bi-directional letter-to-sound/sound-to-letter generation.
The use of layered bigrams in ourhierarchy is extendable to encompass natural languageconstraints \[10\], prosody, discourse and perhaps even di-alogue modeling constraints on top, as well as phonet-ics and acoustics at the bottom.
As such this paradigmshould be particularly useful for applications in speechsynthesis, recognition and understanding.The versatility of our framework can lead to a varietyof applications.
These range from a lexical representationfor large-vocabulary recognition, which provides eman-tic information, syntactic information, and a clusteringmechanism for fast match \[12\], to a low-perplexity lan-gnage model for character ecogition tasks, where oursystem gives a test set perplexity of 8.0 as constrastedSwe did not investigate the cases where maximum word frag-ment lengths are set beyond 6, due to computational limitations,and the vast number of training parameters required.with 11.3 from a standard letter bigram.
In the near fu-ture, we plan to report on our robust parsing mechanismfor extending coverage, and to experiment with alterna-tive search strategies in the layered bigrams framework.REFERENCES\[1\] S. Hunnicutt, H. Meng, S. Seneff and V. Zue, "ReversibleLetter-to-Sound Sound-to-Letter Generation Based onParsing Word Morphology," pp.
763-766, Proceedings,European Conference on Speech Communication andTechnology, September, 1993.\[2\] H. Meng, S. Seneff and V. Zue, "Phonological Parsingfor Reversible Letter-to-Sound/Sound-to-Letter Genera-tion," Proc.
ICASSP-9$, April, 1994.\[3\] N. Chomsky and M. Halle, The Sound Pattern of En-glish, Harper & Row, 1968.\[4\] T. J. Sejnowski and C. R. Rosenberg, "NETtalk: Par-allel Networks that Learn to Pronounce English Text,"Complex Systems, 1, 1987.\[5\] J. M. Lucassen and R. L. Mercer, "An InformationTheoretic Approach to the Automatic Determinationof Phonemic Baseforms," pp.
42.5.1-42.5.4, Proceedings,IEEE International Conference on Acoustics, Speech andSignal Processing, 1984.\[6\] S. M. Lucas and R. I. Damper, "Syntactic Neural Net-works for Bi-directional Text-Phonetics Translation,"pp.
127-141, in Talking Machines, theories, models anddesigns, edited by G. Bailly and C. Benoit, North-Holland publishers.\[7\] F. Alleva and K. F. Lee, "Automatic New Word Acquisi-tion: Spelling from Acoustics," pp.
266-270, Proceedingsof the Darpa Speech and Natural Language Workshop,October, 1989.\[8\] J. Allen, S. Hunnicutt and D. Klatt, From Text to Speech:The MITalk System, Cambridge University Press, 1987.\[9\] S. Seneff, "TINA: A Natural Language System for Spo-ken Language Applications", Computational Linguistics,Vol.
18, No.
1, pp.
61-86, March 1992.\[10\] S. Seneff, H. Meng, and V. Zue, "Language Modellingfor Recognition and Understanding Using Layered Bi-"grams," pp.
317-320, Proceedings, Second InternationalConference on Spoken Language Processing, October,1992.\[11\] H. Kucera and W. N. Francis, Computational Analysis ofPresent-Day American English, Brown University Press,1967.\[12\] D. W. Shipman and V. W. Zue, "Properties of largelexicons: Implications for advanced isolated word recog-nition systems, Proceedings, ICASSP-8P.\[13\] H. Meng, S. Seneff and V. Zue, "The Use of Higher LevelLinguistic Knowledge for Letter-to-Sound Generation",to appear in Proceedings, International Symposium onSpeech, Image Processing and Neural Networks, April,1994.294
