Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1148?1158,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsWord Association Profiles and their Use for Automated Scoring of EssaysBeata Beigman Klebanov and Michael FlorEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541{bbeigmanklebanov,mflor}@ets.orgAbstractWe describe a new representation of thecontent vocabulary of a text we call wordassociation profile that captures the pro-portions of highly associated, mildly asso-ciated, unassociated, and dis-associatedpairs of words that co-exist in the giventext.
We illustrate the shape of the dis-tirbution and observe variation with genreand target audience.
We present a studyof the relationship between quality of writ-ing and word association profiles.
For aset of essays written by college graduateson a number of general topics, we showthat the higher scoring essays tend to havehigher percentages of both highly asso-ciated and dis-associated pairs, and lowerpercentages of mildly associated pairs ofwords.
Finally, we use word associationprofiles to improve a system for automatedscoring of essays.1 IntroductionThe vast majority of contemporary research thatinvestigates statistical properties of language dealswith characterizing words by extracting infor-mation about their behavior from large corpora.Thus, co-occurrence of words in n-word windows,syntactic structures, sentences, paragraphs, andeven whole documents is captured in vector-spacemodels built from text corpora (Turney and Pan-tel, 2010; Basili and Pennacchiotti, 2010; Erk andPado?, 2008; Mitchell and Lapata, 2008; Bullinariaand Levy, 2007; Jones and Mewhort, 2007; Padoand Lapata, 2007; Lin, 1998; Landauer and Du-mais, 1997; Lund and Burgess, 1996; Salton et al,1975).
However, little is known about typical pro-files of texts in terms of co-occurrence behaviorof their words.
Some information can be inferredfrom the success of statistical techniques in pre-dicting certain structures in text.
For example, thefact that a text segmentation algorithm that usesinformation about patterns of word co-occurrencescan detect sub-topic shifts in a text (Riedl and Bie-mann, 2012; Misra et al, 2009; Eisenstein andBarzilay, 2008) tells us that texts contain someproportion of more highly associated word pairs(those in subsequent sentences within the sametopical unit) and of less highly associated pairs(those in sentences from different topical units).1Yet, does each text have a different distributionof highly associated, mildly associated, unassoci-ated, and dis-associated pairs of words, or do textstend to strike a similar balance of these?
Whatare the proportions of the different levels of asso-ciation, how much variation there exists, and arethere systematic differences between various kindsof texts?
We present research that makes a firststep in addressing these questions.From the applied perspective, our interest is inquantifying differences between well-written andpoorly written essays, for the purposes of auto-mated scoring of essays.
We therefore concentrateon essay data for the main experiments reported inthis paper, although some additional corpora willbe used for illustration purposes.The paper is organized as follows.
Section 2presents our methodology for building word as-sociation profiles for texts.
Section 3 illustratesthe profiles for three corpora from different gen-res.
Section 4.2 presents our study of the relation-ship between writing quality and patterns of wordassociations, with section 4.5 showing the resultsof adding a feature based on word association pro-file to a state-of-art essay scoring system.
Relatedwork is reviewed is section 5.1Note that the classical approach to topical segmentationof texts, TextTiling (Hearst, 1997), uses only word repeti-tions.
The cited approaches use topic models that are in turnestimated using word co-occurrence.11482 MethodologyIn order to describe the word association profileof a text, three decisions need to be made.
Thefirst decision is how to quantify the extent of co-occurrence between two words; we will use point-wise mutual information (PMI) estimated from alarge and diverse corpus of texts.
The second iswhich pairs of words in a text to consider whenbuilding a profile for the text; we opted for all pairsof content word types occurring in a text, irrespec-tive of the distance between them.
We considerword types, not tokens; no lemmatization is per-formed.
The third decision is how to represent theco-occurrence profiles; we use a histogram whereeach bin represents the proportion of word pairs inthe given interval of PMI values.
The rest of thesection gives more detail about these decisions.To obtain comprehensive information abouttypical co-occurrence behavior of words ofEnglish, we build a first-order co-occurrenceword-space model (Turney and Pantel, 2010; Ba-roni and Lenci, 2010).
The model was generatedfrom a corpus of texts of about 2.5 billion words,counting co-occurrence in a paragraph,2 using nodistance coefficients (Bullinaria and Levy, 2007).About 2 billion words come from the Gigaword2003 corpus (Graff and Cieri, 2003).
Additional500 million words come from an in-house corpuscontaining popular science and fiction texts.
Oc-currence counts of 2.1 million word types and of1,279 million word type pairs are efficiently com-pressed using the TrendStream technology (Flor,2013), resulting in a database file of 4.7GB.
Trend-Stream is a trie-based architecture for storage, re-trieval, and updating of very large word n-gramdatasets.
We store pairwise word associations asbigrams; since associations are unordered, onlyone of the orders in actually stored in the database.There is an extensive literature on the use ofword-association measures for NLP, especially fordetection of collocations (Pecina, 2010; Evert,2008; Futagi et al, 2008).
The use of point-wise mutual information with word-space modelsis noted in (Zhang et al, 2012; Baroni and Lenci,2010; Mitchell and Lapata, 2008; Turney, 2001).Point-wise mutual information is defined as fol-lows (Church and Hanks, 1990):2In all texts, we use human-marked paragraphs, indicatedeither by a new line or by an xml markup.PMI(x, y) = log2P (x, y)P (x)P (y) (1)Differently from Church and Hanks (1990), wedisregard word order when computing P (x, y).All probabilities are estimated using frequencies.We define WAPT ?
a word association pro-file of a text T ?
as the distribution of PMI(x, y)for all pairs of content3 word types (x, y) ?T.All pairs of word types for which the associationsdatabase returned a null value (the pair has neverbeen observed in the same paragraph) are ex-cluded from the calculation.
For our main dataset(described later as setA, section 4.1), the averagepercentage of non-null values per text is 92%.To represent the WAP of a text, we use a 60-binhistogram spanning all PMI values.
The lowestbin (shown in Figures 1 and 2 as PMI = ?5) con-tains pairs with PMI?
?5; the topmost bin (shownin Figures 1 and 2 as PMI = 4.83) contains pairswith PMI> 4.67, while the rest of the bins containword pairs (x, y) with ?5 <PMI(x, y) ?
4.67.Each bin in the histogram (apart from the top andthe bottom ones) corresponds to a PMI intervalof 0.167.
We chose a relatively fine-grained bin-ning and performed no optimization for grid selec-tion; for more sophisticated gridding approachesto study non-linear relationships in the data, seeReshef et al (2011).We will say that a text A is tighter than textB if the WAP of A is shifted towards the higherend of PMI values relative to text B.
The intuitionbehind the terminology is that texts with higherproportions of highly associated pairs are likelierto be more focused, dealing with a small num-ber of topics at greater length, as opposed to textsthat bring various different themes into the text tovarious extents.
Thus, the text ?The dog barkedand wagged its tail?
is much tighter than the text?Green ideas sleep furiously?, with all the six con-tent word pairs scoring above PMI=5.5 in the firstand below PMI=2.2 in the second.43 Illustration: The shape of thedistributionFor a first illustration, we use a corpus of 5,904essays written as part of a standardized graduate3We part-of-speech tag a text using OpenNLP tagger(http://opennlp.apache.org) and only take into account com-mon and proper nouns, verbs, adjectives, and adverbs.4We omitted colorless from the second example, as color-less is actually highly associated with green (PMI=4.36).1149school admission test (a full descrption of thesedata is given in section 4.1, under setA p1-p6).
Foreach essay, we compute the WAP and represent itusing the 60-bin histogram.
For each bin in thehistogram, we compute its average value over the5,904 essays; additionally, we compute the 15thand 85th percentiles for each bin, so that the bandbetween them contains values observed for 70%of the texts.
The series with the solid thick (blue)line in Figure 1 shows the distribution of the ave-rage percentage of word type pairs per bin (essays-av); the dotted lines above and below show theband capturing the middle 70% of the distribution(essays-15 and essays-85).We observe that the shape of the WAP is verystable across essays, and the variation around theaverage is quite limited.Next, consider the thin solid (green) line withasterisk-shaped markers in Figure 1 that plots asimilarly-binned histogram for the normal distri-bution with ?=0.90 and ?=0.66.
We note thatfor values below PMI=2.17, the normal curve iswithin or almost within the 70% band for the essaydata.
The divergence occurs at the right tail withPMI>2.17, that covers, on average, about 8% ofthe pairs (5.6% and 10.4% for the 15th and 85thpercentiles, respectively).To get an idea about possible variation in thedistribution, we consider two additional corporafrom different genres.
We use a corpus of WallStreet Journal 1987 articles from the TIPSTERcollection.5 We picked articles of 250 to 700words in length, in order to keep the length of textscomparable to the essay data, while varying thegenre; 770 such articles were found.
The dashed(orange) line in Figure 1 shows the distribution ofaverage values for the WSJ collection (wsj-av).We observe that the shape of the distribution issimilar to that of essay data, although WSJ articlestend to be less tight, on average, since the distribu-tion in PMI<2.17 area in the WSJ data is shiftedto the left relative to essays.
Yet, the picture at theright tail is remarkably similar to that of the es-says, with 9% of word pairs, on average, havingPMI>2.17.The second additional corpus contains 140 lite-rary texts written or adapted for readers in grades3 and 4 in US schools (Sheehan et al, 2008).In terms of length, these texts fall into the samerange as the other corpora, averaging 507 words.5LDC93T3A in LDC catalogueThe average WAP for these texts is shown witha thin solid (purple) line with circular markersin Figure 1 (Grades 3-4).
These texts are muchtighter than texts in the other two collections, asthe distribution is shifted to the right.
The righttail, with PMI>2.17, holds 19% of all word pairsin these texts ?
more than twice the proportionin essays written by college graduates or in textsfrom the WSJ.It is instructive to check whether the over-useof highly associated pairs is felt during reading.These texts strike an adult reader as overly ex-plicit, taking the space to state things that an adultreader would readily infer or assume.
For exam-ple, consider the following opening paragraph:?Grandma Rose gave Daniel a recorder.A recorder is a musical instrument.Daniel learned to play by blowing on therecorder.
It didn?t take lots of air.
Itdidn?t take big hands to hold since it waspocket-sized.
His fingers covered thetoneholes just fine.
Soon Daniel playedentire songs.
His mother loved to lis-ten.
Sometimes she hummed along withDaniel?s recorder.
?The second and the third sentences state thingsthat for an adult reader would be too obviousto need mention.
In fact, these sentences al-most seem like training sentences ?
the kind ofsentences from which the associations betweenrecorder and musical instrument, play, blowingcan be learned.
According to Hoey?s theory oflexical priming (Hoey, 2005), one of the mainfunctions of schooling is to imbue children withthe societally sanctioned word associations.To conclude the illustration, we observe thatthere are some broad similarities between the dif-ferent copora in terms of the distribution of pairsof word types.
Thus, texts seem to be mainly madeof pairs of weakly associated words ?
about halfthe pairs of word types lie between PMI of 0.5and 1.5, in all the examined collections (52% foressays, 44% for each of WSJ and young readercorpora).
The percentages of pairs at the low andthe high ends of PMI differ with genre ?
writingfor children favors the higher end, while typicalWall Street Journal writing favors the low end,relatively to a corpus of essays on general topicswritten by college graduates.These observations are necessarily very tenta-tive, as only a few corpora were examined.
Still,1150681012e?of?pairs?of?word?typesessays?avessays?15essays?85wsj?avN(0.90,0.66)Grades?3?4024?5?4?3?2?1012345PercentagPMIFigure 1: WAP histograms for three corpora, shown with smooth lines instead of bars for readability.Average for essays (a thick solid blue line), average for WSJ articles (a dashed orange line); average forGrades 3-4 corpus (a thin solid purple line with round markers).
Normal distribution is shown with a thinsolid green line with asterisk markers.
Middle 70% of essays fall between the dotted lines.we believe the illustration is suggestive, in thatthere is both constancy in writing for a similar pur-pose (observe the limited variation around the ave-rage that captures 70% of the essays) and variationwith genre and target audience.
In what follows,we will explore more thoroughly the informationprovided by word association profiles regardingthe quality of writing.4 Application to Essay ScoringTexts written for a test and scored by relevant pro-fessionals is a setting where variation in text qua-lity is expected.
In this section, we report our ex-periments with using WAPs to explore the varia-tion in quality as quantified by essay scores.
Wefirst describe the data (section 4.1), then show thepatterns of relationships between essay scores andword association profiles (section 4.2).
Finally,we report on an experiment where we significantlyimprove the performance of a very competitive,state-of-art system for automated scoring of es-says, using a feature derived from WAP.4.1 DataWe consider two collections of essays written asresponses in an analytical writing section of ahigh-stakes standardized test for graduate schooladmission; the time limit for essay compositionwas 45 minutes.
Essays were written in responseto a prompt (essay question).
A prompt is usually ageneral statement, and the test-taker is asked to de-velop an argument supporting or refuting the state-ment.
Example prompts are: ?High-speed elec-tronic communications media, such as electronicmail and television, tend to prevent meaningfuland thoughtful communication?
and ?In the age oftelevision, reading books is not as important as itonce was.
People can learn as much by watchingtelevision as they can by reading books.
?The first collection (henceforth, setA) contains8,899 essays written in response to nine differentprompts, about 1,000 per prompt;6 the per-promptsubsets will be termed setA-p1 through setA-p9.Each essay in setA was scored by 1 to 4 humanraters on a scale of 1 to 6; the majority of essays re-ceived 2 human scores.
We use the average of theavailable human scores as the gold-standard scorefor the essay.
Most essays thereby receive an inte-ger score,7 so the ranking of the essays is coarse.From this set, p1-p6 were used for feature selec-tion, data visualization, and estimation of the re-gression models (training), while sets p7-p9 werereserved for a blind test.The second collection (henceforth, setB) con-6While we sampled exactly 1,000 essays per prompt, weremoved empty responses, resulting in 975 to 1,000 essaysper sample.7as the two raters agree most of the time1151tains 400 essays, with 200 essays written on eachof two prompts given as examples above (setB-p1and setB-p2).
In an experimental study by Attaliet al (2013), each essay was scored by 16 profes-sional raters on a scale of 1 to 6, allowing plus andminus scores as well, quantified as 0.33 ?
thus, ascore of 4- is rendered as 3.67.
This fine-grainedscale resulted in higher mean pairwise inter-ratercorrelations than the traditional integer-only scale(r=0.79 vs around r=0.70 for the operational sco-ring).
We use the average of 16 raters as the finalgrade for each essay.
This dataset provides a veryfine-grained ranking of the essays, with almost notwo essays getting exactly the same score.Rounded setA p1-p9 setBScore av min max p1 p21 .01 .00 .01 ?
?2 .05 .04 .06 .03 .033 .25 .20 .29 .30 .284 .44 .42 .47 .54 .555 .21 .16 .24 .13 .146 .04 .02 .07 .01 .02Table 1: Score distribution in the essay data.
Forthe sake of presentation in this table, all scoreswere rounded to integer scores, so a score of 3.33was counted as 3, and a score of 3.5 was countedas 4.
A cell with the value of .13 (row titled 5and column titled SetB p1) means that 13% ofthe essays in setB-p1 received scores that roundto 5.
For setA, average, minimum, and maximumvalues across the nine prompts are shown.Table 1 shows the distribution of rounded scoresin both collections.
Average essay scores are be-tween 3.74 to 3.98 across the different promptsfrom both collections.
The use of 16 raters seemsto have moved the rounded scores towards themiddle; however, the relative ranking of the essaysis much more delicate in setB than in setA.4.2 Essay Score vs WAPWe calculated correlations between essay scoreand the proportion of word pairs in each of the 60bins of the WAP histogram, separately for each ofthe prompts p1-p6 in setA.
For a sample of 1,000instances, a correlation of r=0.065 is significant atp = 0.05.
Figure 2 plots the correlations.First, we observe that, perhaps contrary to ex-pectation, the proportion of the highest values ofPMI (the area to the right of PMI=4 in Figure 2)does not yield a consistent correlation with essayscores.
Thus, inasmuch as highest PMI valuestend to capture multi-word expressions (South andAfrica; Merill and Lynch), morphological vari-ants (bids and bidding), or synonyms (mergersand takeovers), their proportion in word type pairsdoes not seem to give a clear signal regarding thequality of writing.8In contrast, the area of moderately high PMIvalues (from PMI=2.5 to PMI=3.67 in Figure 2)produces a very consistent picture, with only twopoints out of 48 in that interval9 lacking signif-icant positive correlation with essay score (p2 atPMI=3.17 and p5 at PMI=3).Next, observe the consistent negative correla-tions between essay score and the proportion ofword pairs in bins PMI=0.833 through PMI=1.5.Here again, out of the 30 data points correspond-ing to these values, only 3 failed to reach statisticalsignificance, although the trend there is still nega-tive.Finally, there is a trend towards a positive cor-relation between essay scores and the proportionof mildly negative PMI values (-2<PMI<0), thatis, better essays tend to use more pairs of dis-associated words, although this trend is not asclear-cut as the one on the right-hand side of thedistribution.Assuming that a higher proportion of high PMIpairs corresponds to more topic development andthat a higher proportion of negative PMIs corre-ponds to more creative use of language (in thatpairs are chosen that do not generally tend to ap-pear together), it seems that the better essays areboth more topical and more creative than the lowerscoring ones.
In what follows, we check whetherthe information about essay quality provided byWAP can be used to improve essay scoring.8It is also possible that some of the instances with veryhigh PMI are pairs that contain low frequency words forwhich the database predicts a spuriously high PMI based on asingle (and a-typical) co-occurrence that happens to repeat inan essay ?
similar to the Schwartz eschews example in (Man-ning and Schu?tze, 1999, Table 5.16, p. 181).
On the onehand, we do not expect such pairs to occur in any systematicpattern, so they could obscure an otherwise more systematicpattern in the high PMI bins.
On the other hand, we do notexpect to see many such pairs, simply because a repetitionof an a-typical event is likely to be very rare.
We thank ananonymous reviewer for suggesting this direction, and leavea more detailed examination of the pairs in the highest-PMIbins to future work.9There are 8 bins of width of 0.167 in the given interval,with 6 datapoints per bin.1152?0.100.10.20.3?5?4?3?2?1012345relation?with?Essay?Scorep1 p2 p3 p4 p5 p6?0.4?0.3?0.2Pearson?CorPMIFigure 2: Correlations with essay score for various bins of the WAP histogram.
P1 to P6 correspond tothe first 6 prompts in SetA.4.3 BaselineAs a baseline, we use e-rater (Attali and Burstein,2006), a state-of-art essay scoring system deve-loped at Educational Testing Service.10 E-ratercomputes more than 100 micro-features, which areaggregated into macro-features aligned with spe-cific aspects of the writing construct.
The systemincorporates macro-features measuring grammar,usage, mechanics, style, organization and develop-ment, lexical complexity, and vocabulary usage.Table 2 gives examples of micro-features coveredby the different macro-features.E-rater models are built using linear regressionon large samples of test-taker essays.
We use ageneric e-rater model built at Educational TestingService using essays across a variety of writingprompts, with no connection to the current projectand its authors.
This model obtains Pearson corre-lations of r=0.8324-0.8721 with the human scoreson setA, and the staggering r=0.9191 and r=0.9146with the human scores on setB-p1 and setB-p2,respectively.
This is a very competitive baseline,as e-rater features explain more than 70% of thevariation in essay scores on a relatively coarsescale (setA) and more than 80% of the variationin scores on a fine-grained scale (setB).10http://www.ets.org/erater/about/Macro- Example Micro-FeaturesFeatureGrammar, agreement errorsUsage, and verb formation errorsMechanics missing punctuationStyle passivevery long or short sentencesexcessive repetitionOrganization use of discourse elements:and thesis, support, conclusionDevelopmentLexical average word frequencyComplexity average word lengthVocabulary similarity to vocabulary inhigh- vs low-scoring essaysTable 2: Features used in e-rater (Attali andBurstein, 2006).4.4 Adding WAPWe define HAT ?
high associative tight-ness ?
as the percentage of word type pairswith 2.33<PMI?3.67 (bins PMI=2.5 throughPMI=3.67).
This range correponds to the longestsequence of adjacent bins in the PMI>0 area thathad a positive correlation with essay score in thesetA-p1 set.
The HAT feature attains significant1153(at p = 0.05) correlations with essay scores,r=0.11 to r=0.27 for the prompts in setA, andr=0.22 and r=0.21 for the two prompts in setB.
Wenote that the HAT feature is not correlated with es-say length.
Essay length is not used as a feature ine-rater models, but it typically correlates stronglywith the human essay score (at about r=0.70 in ourdata), as well as with the score provided by e-rater(at about r=0.80).We also explored a feature that captured thearea with the negative correlations identified insection 4.2.
This feature did not succeed in im-proving the performance over the baseline on setAp1-p6; we tentatively conclude that informationcontained in that feature, i.e.
the proprotion ofmildly associated vocabulary in an essay, is indi-rectly captured by another feature or group of fea-tures already present in e-rater.
Likewise, a featurethat calculates the average PMI for all pairs of con-tent word types in the text failed to produce an im-provement over the baseline for setA p1-p6.
Thereason for this can be observed in Figure 2: Thehigher-scoring essays having more of both the lowand the high PMI pairs leads to about the sameaverage PMI as for the lower-scoring essays thathave a higher concentration of values closer to theaverage PMI.4.5 EvaluationTo evaluate the usefulness of WAP in improvingautomated scoring of essays, we estimate a lin-ear regression model using the human score as adependent variable (label) and e-rater score andthe HAT as the two independent variables (fea-tures).
The correlations between the two inde-pendent variables (e-rater and HAT) are betweenr=0.11 and r=0.24 on the prompts in setA andsetB.We estimate a regression model on each ofsetA-pi, i ?
{1, .., 6}, and evaluate them on eachof setA-pj, j ?
{7, .., 9}, and compare the perfor-mance with that of e-rater alone on setA-pj.
Notethat e-rater itself is not trained on any of the datain setA and setB; we use the same e-rater modelfor all evaluations, a generic model that was pre-trained on a large number of essays across diffe-rent prompts.
For setB, we estimate the regressionmodel on setB-p1 and test on setB-p2, and viceversa.Table 3 shows the evaluation results.
The HATfeature leads to a statistically significant improve-Train Test E-rater E-rater+HAT ton Test on TestsetAp1 p7 0.84043 0.84021 -0.371p2 p7 0.84043 0.84045 0.408p3 p7 0.84043 0.83999 -0.597p4 p7 0.84043 0.84044 0.411p5 p7 0.84043 0.84028 -0.280p6 p7 0.84043 0.83926 -1.080p1 p8 0.83244 0.83316 1.688p2 p8 0.83244 0.83250 2.234p3 p8 0.83244 0.83327 1.530p4 p8 0.83244 0.83250 2.237p5 p8 0.83244 0.83311 1.752p6 p8 0.83244 0.83339 1.191p1 p9 0.86370 0.86612 4.282p2 p9 0.86370 0.86389 5.205p3 p9 0.86370 0.86659 4.016p4 p9 0.86370 0.86388 5.209p5 p9 0.86370 0.86591 4.390p6 p9 0.86370 0.86730 3.448setBp1 p2 0.9146 0.9178 0.983p2 p1 0.9191 0.9242 2.690Table 3: Performance of baseline model (e-rater)and models where e-rater was augmented withHAT, a feature based on the word associationprofile.
Performance is measured using Pearsoncorrelation with essay score.
We use WilcoxonSigned-Ranked test for matched pairs, and reportthe sum of signed ranks (W), the number of ranks(n), and the p value.
E-rater+HAT is significantlybetter than e-rater alone, W=138, n=20, p<0.05.We also measure significance of the improvementfor each row individually, using McNemar?s testfor significance of difference in same-sample cor-relations (McNemar, 1955, p.148); we report thet value for each test.
For values of t > 1.645,we can reject the hypothesis that e-rater+HAT isnot better than e-rater alone with 95% confidence.Significant improvements are underlined.1154ment in the performance of automated scoring.An improvement is observed for 14 out of the 18evaluations for setA, as well as for both evalua-tions for setB.11 Moreover, the largest relative im-provement of 0.55%, from 0.9191 to 0.9242, wasobserved for the setting with the highest baselineperformance, suggesting that the HAT feature isstill effective even after the delicate ranking ofthe essays revealed an exceptionally strong perfor-mance of e-rater.5 Related WorkMost of the attention in the computational linguis-tics research that deals with analysis of the lexisof texts has so far been paid to what in our termswould be the very high end of the word associa-tion profile.
Thus, following Halliday and Hasan(1976), Hoey (1991), and Morris and Hirst (1991),the notion of lexical cohesion has been used tocapture repetitions of words and occurrence ofwords with related meanings in a text.
Lexicallycohesive words are traced through the text, for-ming lexical chains or graphs, and these repre-sentations are used in a variety of applications,such as segmentation, keyword extraction, sum-marization, sentiment analysis, temporal indexing,hypelink generation, error correction (Guinaudeauet al, 2012; Marathe and Hirst, 2010; Ercan andCicekli, 2007; Devitt and Ahmad, 2007; Hirstand Budanitsky, 2005; Inkpen and De?silets, 2005;Gurevych and Strube, 2004; Stokes et al, 2004;Silber and McCoy, 2002; Green, 1998; Al-Halimiand Kazman, 1998; Barzilay and Elhadad, 1997).To our knowledge, lexical cohesion has not so farbeen used for automated scoring of essays.
Ourresults suggest that this direction is promising, asmerely the proportion of highly associated wordpairs is already contributing a clear signal regar-ding essay quality; it is possible that additionalinformation can be derived from richer represen-tations common in the lexical cohesion literature.Aspects related to the distribution of words inessays have been studied in relation to essay sco-ring.
One line of work focuses on assessing co-herence of essays.
Foltz et al (1998) use Latent11We also performed a cross-validation test on setA p1-p6, where we estimated a regression model on setA-pi andevaluate it on setA-pj, for all i, j ?
{1, .., 6}, i 6= j, andcompared the performance with that of e-rater alone on setA-pj, yielding 30 different train-test combinations.
The resultswere similar to those of the blind test presented here, with e-rater+HAT significantly improving upon e-rater alone, usingWilcoxon test, W=374, n=29, p<0.05.Semantic Analysis to model the smoothness oftransitions between adjacent segments of an essay.Higgins et al (2004) compare sentences from cer-tain discourse segments in an essay to determinetheir semantic similarity, such as comparing the-sis statements to conclusions or thesis statementsto essay prompts.
Additional approaches includeevaluation of coherence based on repeated refe-rence to entities (Burstein et al, 2010; Barzilayand Lapata, 2008; Miltsakaki and Kukich, 2004).Our approach is different in that it does not mea-sure the flow of the text, that is, the sequencingand repetition of the words, but rather assesses thechoice of vocabulary as a whole.Topic models have been proposed as a tech-nique for capturing clusters of related words thattend to occur in the same documents in a givencollection.
A text is modeled as being composedof a small number of topics, and words in the textare generated conditioned on the selected topics(Gruber et al, 2007; Blei et al, 2003).
Since(a) topics encapsulate clusters of highly associatedwords, and (b) topics for a given text are modeledas being chosen independently from each other,we expect a negative correlation between the num-ber of topics in a document and the tightness of theword association profile of the text.An alternative representation of word associ-ation profile would be a weighted graph, wherethe weights correspond to pairwise associationsbetween words.
Thus, for longer texts, graphanalysis techniques would be applicable.
Steyversand Tenenbaum (2005) analyze the graphs in-duced from large repositories like WordNet ordatabases of free associations, and find them to bescale-free and small-world; it is an open questionwhether word association graphs induced frombook-length texts would exhibit similar properties.In the theoretical tradition, our work is closest inspirit to Michael Hoey?s theory of lexical priming(Hoey, 2005), positing that users of language inter-nalize patterns of occurrence and non-occurrenceof words not only with other words, but also in cer-tain positions in a text, in certain syntactic environ-ments, and in certain evaluative contexts, and usethese when creating their own texts.
We believethat word association profiles reflect the artworkthat goes into using those internalized associationsbetween words when creating a text, achieving theright mix of strong and weak, positive and nega-tive associations.11556 ConclusionIn this paper, we described a new representationof the content vocabulary of a text we call wordassociation profile that captures the proportionsof highly associated, mildly associated, unassoci-ated, and dis-associated pairs of words selected toco-exist in the given text by its author.
We ob-served that the shape of the distribution is quitestable across various texts, with about half thepairs having a mild association; the allocation ofpairs to the higher and the lower levels of associa-tion does vary across genres and target audiences.We further presented a study of the relationshipbetween quality of writing and word associationprofiles.
For a dataset of essays written by collegegraduates on a number of general topics in a stan-dardized test for graduate school admission andscored by professional raters, we showed that thehigher scoring essays tend to have higher percen-tages of both highly associated and dis-associatedpairs, and lower percentagese of mildly associatedpairs of words.
We hypothesize that this patternis consistent with the better essays demonstratingboth a better topic development (hence the higherpercentage of highly related pairs) and a more cre-ative use of language resources, as manifested in ahigher percentage of word pairs that generally donot tend to appear together.Finally, we demonstrated that the informationprovided by word association profiles leads to asignificant improvement in a highly competitive,state-of-art essay scoring system that already mea-sures various aspects of writing quality.In future work, we intend to investigate in moredetail the contribution of various kinds of words toword association profiles, as well as pursue appli-cation to evaluation of text complexity.ReferencesReem Al-Halimi and Rick Kazman.
1998.
Temporalindexing through lexical chaining.
In C. Fellbaum,editor, WordNet: An Electronic Lexical Database,pages 333?351.
Cambridge, MA: MIT Press.Yigal Attali and Jill Burstein.
2006.
Automated EssayScoring With e-rater R?V.2.
Journal of Technology,Learning, and Assessment, 4(3).Yigal Attali, Will Lewis, and Michael Steier.
2013.Scoring with the computer: Alternative proceduresfor improving reliability of holistic essay scoring.Language Testing, 30(1):125?141.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):673?721.Regina Barzilay and Michael Elhadad.
1997.
Usinglexical chains for text summarization.
In Proceed-ings of ACL Intelligent Scalable Text SummarizationWorkshop.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Roberto Basili and Marco Pennacchiotti.
2010.
Dis-tributional lexical semantics: Toward uniform rep-resentation paradigms for advanced acquisition andprocessing tasks.
Natural Language Engineering,16(4):347?358.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.John Bullinaria and Joseph Levy.
2007.
Extractingsemantic representations from word co-occurrencestatistics: A computational study.
Behavior Re-search Methods, 39:510?526.Jill Burstein, Joel Tetreault, and Slava Andreyev.
2010.Using entity-based features to model coherence instudent essays.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 681?684, Los Angeles, California,June.
Association for Computational Linguistics.Kenneth Church and Patrick Hanks.
1990.
Word asso-ciation norms, mutual information and lexicography.Computational Linguistics, 16(1):22?29.Ann Devitt and Khurshid Ahmad.
2007.
Sentimentpolarity identification in financial news: A cohesion-based approach.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics, pages 984?991, Prague, Czech Republic,June.
Association for Computational Linguistics.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, EMNLP ?08, pages 334?343, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Gonenc Ercan and Ilyas Cicekli.
2007.
Using lexicalchains for keyword extraction.
Information Process-ing & Management, 43(6):1705?1714.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages897?906, Honolulu, Hawaii, October.
Associationfor Computational Linguistics.1156Stefan Evert.
2008.
Corpora and collocations.
InA.
Lu?deling and M.
Kyto?, editors, Corpus Linguis-tics: An International Handbook.
Berlin: Mouton deGruyter.Michael Flor.
2013.
A fast and flexible architecture forvery large word n-gram datasets.
Natural LanguageEngineering, 19(1):61?93.Peter Foltz, Walter Kintsch, and Thomas Landauer.1998.
The measurement of textual coherence withlatent semantic analysis.
Discourse Processes,25(2):285?307.Yoko Futagi, Paul Deane, Martin Chodorow, and JoelTetreault.
2008.
A computational approach to de-tecting collocation errors in the writing of non-nativespeakers of English.
Computer Assisted LanguageLearning, 21(4):353?367.David Graff and Christopher Cieri.
2003.
English Gi-gaword LDC2003T05.
Linguistic Data Consortium,Philadelphia.Stephen Green.
1998.
Automated link generation: Canwe do better than term repetition?
Computer Net-works, 30:75?84.Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.2007.
Hidden topic markov models.
Journal ofMachine Learning Research - Proceedings Track,2:163?170.Camille Guinaudeau, Guillaume Gravier, and PascaleSe?billot.
2012.
Enhancing lexical cohesion measurewith confidence measures, semantic relations andlanguage model interpolation for multimedia spokencontent topic segmentation.
Computer Speech andLanguage, 26(2):90?104.Iryna Gurevych and Michael Strube.
2004.
Seman-tic similarity applied to spoken dialogue summariza-tion.
In Proceedings of Coling 2004, pages 764?770, Geneva, Switzerland, August.
COLING.Michael A.K.
Halliday and Ruqaiya Hasan.
1976.
Co-hesion in English.
Longman, London.Marti Hearst.
1997.
Texttiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64.Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-dia Gentile.
2004.
Evaluating multiple aspects ofcoherence in student essays.
In Daniel Marcu Su-san Dumais and Salim Roukos, editors, HLT-NAACL2004: Main Proceedings, pages 185?192, Boston,Massachusetts, USA, May.
Association for Compu-tational Linguistics.Graeme Hirst and Alexander Budanitsky.
2005.
Cor-recting real-word spelling errors by restoring lexi-cal cohesion.
Natural Language Engineering,11(1):87?111.Michael Hoey.
1991.
Patterns of Lexis in Text.
OxfordUniversity Press.Michael Hoey.
2005.
Lexical Priming.
Routledge.Diana Inkpen and Alain De?silets.
2005.
Semanticsimilarity for detecting recognition errors in auto-matic speech transcripts.
In Proceedings of Empir-ical Methods in Natural Language Processing Con-ference, pages 49?56, Vancouver, British Columbia,Canada, October.
Association for ComputationalLinguistics.Michael Jones and Douglas Mewhort.
2007.
Repre-senting word meaning and order information in acomposite holographic lexicon.
Psychological Re-view, 114(1):1?37.Thomas K. Landauer and Susan T. Dumais.
1997.A solution to Plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological Review,104(2):211?240.Dekang Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In Proceedings of ACL, pages768?774, Montreal, Canada.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments & Computers, 28:203?208.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of statistical natural language process-ing.
MIT Press, Cambridge, MA, USA.Meghana Marathe and Graeme Hirst.
2010.
LexicalChains Using Distributional Measures of ConceptDistance.
In Proceedings of 11th International Con-ference on Intelligent Text Processing and Computa-tional Linguistics (CICLING), pages 291?302, Iasi,Romania, March.Quinn McNemar.
1955.
Psychological Statistics.
NewYork: J. Wiley and Sons, 2nd edition.Eleni Miltsakaki and Karen Kukich.
2004.
Evaluationof text coherence for electronic essay scoring sys-tems.
Natural Language Engineering, 10(1):25?55.Hemant Misra, Franc?ois Yvon, Joemon M. Jose, andOlivier Cappe.
2009.
Text segmentation via topicmodeling: an analytical study.
In Proceedings ofthe 18th ACM conference on Information and know-ledge management, CIKM ?09, pages 1553?1556,New York, NY, USA.
ACM.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofthe 46th Annual Meeting of the Association for Com-putational Linguistics, pages 236?244, Columbus,Ohio, June.
Association for Computational Linguis-tics.Jane Morris and Graeme Hirst.
1991.
Lexical cohe-sion, the thesaurus, and the structure of text.
Com-putational linguistics, 17(1):21?48.1157Sebastian Pado and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Pavel Pecina.
2010.
Lexical association measuresand collocation extraction.
Language Resources andEvaluation, 44:137?158.David Reshef, Yakir Reshef, Hilary Finucane, SharonGrossman, Gilean McVean, Peter Turnbaugh, EricLander, Michael Mitzenmacher, and Pardis Sabeti.2011.
Detecting novel associations in large datasets.
Science, 334(6062):1518?1524.Martin Riedl and Chris Biemann.
2012.
How text seg-mentation algorithms gain from topic models.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 553?557, Montre?al, Canada, June.
Associa-tion for Computational Linguistics.Gerard Salton, Andrew Wong, and Chung-Shu Yang.1975.
A vector space model for automatic indexing.Communications of the ACM, 18(11):613?620.Kathy Sheehan, Irene Kostin, and Yoko Futagi.
2008.When do standard approaches for measuring vo-cabulary difficulty, syntactic complexity and refer-ential cohesion yield biased estimates of text diffi-culty?
In Proceedings of the Cognitive Science So-ciety, pages 1978?1983, Washington, DC, July.Gregory Silber and Kathleen McCoy.
2002.
Efficientlycomputed lexical chains as an intermediate represen-tation for automatic text summarization.
Computa-tional Linguistics, 28(4):487?496.Mark Steyvers and Joshua B. Tenenbaum.
2005.
TheLarge-Scale Structure of Semantic Networks: Sta-tistical Analyses and a Model of Semantic Growth.Cognitive Science, 29:41?78.Nicola Stokes, Joe Carthy, and Alan F. Smeaton.
2004.Select: A lexical cohesion based news story seg-mentation system.
Journal of AI Communications,17(1):3?12.Peter Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Articial Intelligence Research,37:141?188.Peter D. Turney.
2001.
Mining the Web for Syno-nyms: PMI-IR versus LSA on TOEFL.
In EuropeanConference on Machine Learning, pages 491?502,Freiburg, Germany, September.Ziqi Zhang, Anna Gentile, and Fabio Ciravegna.
2012.Recent advances in methods of lexical semantic re-latedness ?
a survey.
Natural Language Engineer-ing, FirstView:1?69.1158
