Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1049?1054,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsEffective Feature Integration for Automated Short Answer Scoring?Keisuke SakaguchiCLSP, Johns Hopkins UniversityBaltimore, MDkeisuke@cs.jhu.eduMichael Heilman Nitin MadnaniEducational Testing ServicePrinceton, NJ{mheilman,nmadnani}@ets.orgAbstractA major opportunity for NLP to have a real-world impact is in helping educators score stu-dent writing, particularly content-based writ-ing (i.e., the task of automated short answerscoring).
A major challenge in this enterpriseis that scored responses to a particular ques-tion (i.e., labeled data) are valuable for mod-eling but limited in quantity.
Additional in-formation from the scoring guidelines for hu-mans, such as exemplars for each score leveland descriptions of key concepts, can alsobe used.
Here, we explore methods for in-tegrating scoring guidelines and labeled re-sponses, and we find that stacked generaliza-tion (Wolpert, 1992) improves performance,especially for small training sets.1 IntroductionEducational applications of NLP have considerablepotential for real-world impact, particularly in help-ing to score responses to assessments, which couldallow educators to focus more on instruction.We focus on the task of analyzing short, content-focused responses from an assessment of read-ing comprehension, following previous work onshort answer scoring (Leacock and Chodorow, 2003;Mohler et al, 2011; Dzikovska et al, 2013).
Thistask is typically defined as a text regression or clas-sification problem: we label student responses thatconsist of one or more sentences with scores on an?Work done when Keisuke Sakaguchi was an intern at ETS.Michael Heilman is now a data scientist at Civis Analytics.ordinal scale (e.g.
correct, partially correct, or in-correct; 1?5 score range, etc.).
Importantly, in ad-dition to the student response itself, we may alsohave available other information such as referenceanswers or descriptions of key concepts from thescoring guidelines for human scorers.
Such informa-tion can be cheap to acquire since it is often gener-ated as part of the assessment development process.Generally speaking, most work on short answerscoring takes one of the following approaches:?
A response-based approach uses detailed fea-tures extracted from the student response itself(e.g., word n-grams, etc.)
and learns a scoringfunction from human-scored responses.?
A reference-based approach compares the stu-dent response to reference texts, such as ex-emplars for each score level, or specificationsof required content from the assessment?s scor-ing guidelines.
Various text similarity methods(Agirre et al, 2013) can be used.These two approaches can, of course, be com-bined.
However, to our knowledge, the issues ofhow to combine the approaches and when that islikely to be useful have not been thoroughly studied.A challenge in combining the approaches is thatthe response-based approach produces a large set ofsparse features (e.g., word n-gram indicators), whilethe reference-based approach produces a small set ofcontinuous features (e.g., similarity scores betweenthe response and exemplars for different score lev-els).
A simple combination method is to train amodel on the union of the feature sets (?3.3).
How-ever, the dense reference features may be lost amongthe many sparse response features.1049Therefore, we apply stacked generalization (i.e.stacking) (Wolpert, 1992; Sakkis et al, 2001; Tor-res Martins et al, 2008) to build an ensemble of theresponse- and reference-based approaches.
To ourknowledge, there is little if any research investigat-ing the value of stacking for NLP applications suchas automated scoring.1The contributions of this paper are as follows:(1) we investigate various reference-based featuresfor short answer scoring, (2) we apply stacking(Wolpert, 1992) in order to combine the reference-and response-based methods, and (3) we demon-strate that the stacked combination outperformsother models, especially for small training sets.2 Task and DatasetWe conduct our experiments on short-answer ques-tions that are developed under the Reading for Un-derstanding (RfU) assessment framework.
Thisframework is designed to measure the reading com-prehension skills of students from grades 6 through9 by attempting to assess whether the reader hasformed a coherent mental model consistent with thetext discourse.
A more detailed description is pro-vided by Sabatini and O?Reilly (2013).We use 4 short-answer questions based on twodifferent reading passages.
The first passage is a1300-word short story.
A single question (?Q1?hereafter) asks the reader to read the story and writea 5?7 sentence synopsis in her own words that in-cludes all the main characters and action from thestory but does not include any opinions or infor-mation from outside the story.
The second passageis a 700-word article that describes the experiencesof European immigrants in the late 19th and early20th centuries.
There are 3 questions associatedwith this passage: two that ask the reader to summa-rize one section each in the article (?Q2?
and ?Q4?
)and a third that asks to summarize the entire article(?Q3?).
These 3 questions ask the reader to restricthis or her responses to 1?2 sentences each.Each question includes the following:1Some applications have used stacking but not analyzedits value.
For example, many participants used stacking inthe ASAP2 competition http://http://www.kaggle.com/c/asap-sas.
Also, Heilman and Madnani (2013) usedstacking for Task 7 of SemEval 2013.?
scored responses: short responses written bystudents, scored on a 0 to 4 scale for the firstquestion, and 0 to 3 for the other 3.?
exemplars: one or two exemplar responses foreach score level, and?
key concepts: several (?
10) sentences brieflyexpressing key concepts in a correct answer.The data for each question is split into a trainingand testing sets.
For each question, we have about2,000 scored student responses.Following previous work on automatic scoring(Shermis and Burstein, 2013), we evaluate perfor-mance using the quadratically weighted ?
(Cohen,1968) between human and machine scores (roundedand trimmed to the range of the training scores).3 Models for Short Answer ScoringNext, we describe our implementations of theresponse- and reference-based scoring methods.
Allmodels use support vector regression (SVR) (Smolaand Sch?olkopf, 2004), with the complexity parame-ter tuned by cross-validation on the training data.23.1 Response-basedOur implementation of the response-based scoringapproach (?resp?
in ?4) uses SVR to estimate amodel to predicts human scores for text responses.Various sparse binary indicators of linguistic fea-tures are used:?
binned response length (e.g.
the length-7feature fires when the character contains 128-255 characters.)?
word n-grams from n = 1 to 2?
character n-grams from n = 2 to 5, whichis more robust than word n-gram regardingspelling errors in student responses?
syntactic dependencies in the form of Parent-Label-Child (e.g.
boy-det-the for ?theboy?)?
semantic roles in the form of PropBank3style(e.g.
say.01-A0-boy for ?
(the) boy said?
)2We used the implementation of SVR in scikit-learn (Pe-dregosa et al, 2011) via SKLL (https://github.com/EducationalTestingService/skll) version 0.27.0.Other than the complexity parameter, we used the defaults.3http://verbs.colorado.edu/?mpalmer/projects/ace.html1050The syntactic and semantic features were extractedusing the ClearNLP parser.4We used the defaultmodels and options for the parser.
We treat thismodel as a strong baseline to which we will addreference-based features.3.2 Reference-basedOur implementation of the reference-based ap-proach (?ref?
in ?4) uses SVR to estimate a modelto predict human scores from various measures ofthe similarity between the response and informationfrom the scoring guidelines provided to the humanscorers.
Specifically, we use the following infor-mation from ?2: (a) sentences expressing key con-cepts that should be present in correct responses, and(b) small sets of exemplar responses for each scorelevel.
For each type of reference, we use the follow-ing similarity metrics:?
BLEU: the BLEU machine translation metric(Papineni et al, 2002), with the student responseas the translation hypothesis.
When using BLEUto compare the student response to the (muchshorter) sentences containing key concepts, weignore the brevity penalty.?
word2vec cosine: the cosine similarity betweenthe averages of the word2vec vectors (Mikolovet al, 2013) of content words in the response andreference texts (e.g., exemplar), respectively.5,6?
word2vec alignment: the alignment method be-low with word2vec word similarities.?
WordNet algnment: the alignment method be-low with the Wu and Palmer (1994) WordNet(Miller, 1995) similarity score.The WordNet and word2vec alignment metricsare computed as follows, where S is a student re-sponse, R is one of a set of reference texts, WsandWrare content words in S and R, respectively, andSim(Ws,Wr) is the word similarity function:4http://www.clearnlp.com, v2.0.25The word2vec model was trained on the English Wikipediaas of June 2012, using gensim (http://radimrehurek.com/gensim/) with 100 dimensions, a context window of 5,a minimum count of 5 for vocabulary items, and the defaultskip-gram architecture.6We define content words as ones whose POS tags beginwith ?N?
(nouns), ?V?
(verbs), ?J?
(adjectives), or ?R?
(ad-verbs).SVR #1Sparse Features(response-based)??????????
?Char n-gramWord n-gramResponse lengthDependencySemantic roleDense Features(reference-based)??????
?BLEUw2v cosinew2v alignWordNet algnSVR #2predicted scoreFigure 1: Stacking model for short answer scoring1len(S)?WsmaxWr?RSim(Ws,Wr) (1)When R is one of a set of reference texts (e.g.,one of multiple exemplars available for a given scorepoint), we use the maximum similarity over avail-able values of R. In our data, there are multiple ex-emplars per score point, but only one text (usually, asingle sentence) per key concept.
In other words, weselect the most similar exemplar response for eachscore level.3.3 Simple Model CombinationOne obvious way to combine the response- andreference-based models is to simply train a singlemodel that uses both the sparse features of the stu-dent response and the dense, real-valued similarityfeatures.
Our experiments (?4) include such a modelas a strong baseline, using SVR to estimate featureweights.3.4 Model Combination with StackingIn preliminary experiments with the training data,we observed no gains for the simple combinationmodel over the component models.
One poten-tial challenge of combining the two scoring ap-proaches is that the weights for the dense, reference-based features may be difficult to properly esti-1051mate due to regularization7and the large numberof sparse, mostly irrelevant linguistic features fromthe response-based approach.
In fact, the reference-based sparse features constitute almost 90% of theentire feature set, while the response-based densefeatures constitute the remaining 10%.This leads us to explore stacking (Wolpert, 1992),an ensemble technique where a top-layer modelmakes predictions based on predictions from lower-layer models.
Here, we train a lower-layer model toaggregate the sparse response-based features into asingle ?response-based prediction?
feature, and thentrain an upper-layer SVR model that includes thatfeature along with all of the reference-based fea-tures.
Figure 1 shows the details.8For training our stacking model, we first train theresponse-based regression model (SVR #1 in Fig-ure 1), and then train the reference-based regressionmodel (SVR #2) with an additional prediction fea-ture value from the response-based model.
Specifi-cally, the lower-layer model concentrates sparse andbinary features into a single continuous value, whichaccords with reference-based dense features in theupper-layer model.
In training the lower-layer SVRon the training data, computing the response-basedprediction feature (i.e., output of the lower-layerSVR) from the sparse features is similar to k-foldcross-validation (k = 10 here): the prediction fea-ture values are computed for each fold by response-based SVR models trained on the remaining folds.In training the upper-layer SVR on the testing data,this prediction feature is computed by a single modeltrained on the entire training set.4 ExperimentsThis section describes two experiments: an evalu-ation of reference-based similarity metrics, and anevaluation of methods for combining the reference-and response-based features by stacking.
Asmentioned in ?2, we evaluate performance using7Another possible combination approach would be to usethe combination method from ?3.3 but apply less regularizationto the reference-based features, or, equivalently, scale them bya large constant.
We only briefly explored this through trainingset cross-validation.
The stacking approach seemed to performat least as well in general.8It would also be possible to also make a lower-layer modelfor the reference-based features, though doing this did not showbenefits in preliminary experiments.Q1 Q2 Q3 Q4BLEU .72 .45 .60 .52word2vec cosine .75 .45 .61 .52word2vec alignment .76 .47 .61 .51WordNet algnment .73 .49 .59 .51All (?ref?)
.78 .52 .66 .59length .68 .42 .59 .51response-based (?resp?)
.82 .72 .75 .74Table 1: Training set cross-validation performanceof reference-based models, in quadratically weighted?, with baselines for comparison.
The response-based(?resp?)
model is a stronger baseline as described in ?3.3.Note that each reference-based model includes the lengthbin features for a fair comparison to ?resp?.quadratically weighted ?
between the human andpredicted scores.4.1 Similarity MetricsWe first evaluate the similarity metrics from ?3.2 us-ing 10-fold cross-validation on the training data.
Weevaluated SVR models for each metric individuallyas well as a model combining all features from allmetrics.
In all models, we included the responselength bin features (?3.1) as a proxy for response-based features.
We compare to the response-basedmodel (?3.1) and to a model consisting of only theresponse length bin feature.The results are shown in Table 1.
Each simi-larity metric by itself does not always improve theperformance remarkably from the baseline (i.e., theresponse length bin features).
However, when weincorporate all the similarity features, we obtainedsubstantial gain in all 4 questions.
In the subsequentmodel combination experiment, therefore, we usedall similarity features to represent the reference-based approach because it outperformed the othersimilarity models.4.2 Model CombinationNext, we tested models that use both response-and reference-based features on a held-out test set,which contains 400 responses per question.
Weevaluated the response-based (?resp?, ?3.1) andreference-based (?ref?, ?3.2) individual models aswell as the two combination methods (?ref+resp?,?3.3 and ?ref+resp stacking?, ?3.4).
We also eval-1052lllllllll lllll lllll lQ1 Q2 Q3 Q40.720.760.800.840.40.50.60.70.500.550.600.650.700.50.60.7100 200 400 800 1600 100 200 400 800 1600 100 200 400 800 1600 100 200 400 800 1600sample sizequadraticallyweightedkappal refref+respref+resp (stacking)respFigure 2: Test set results for various models trained on differently sized random samples of the training data.
Eachpoint represents an average over 20 runs, except for the rightmost points for each question, which correspond totraining on the full training set.
Note that the ?resp?
and ?ref+resp?
lines mostly overlap.uated models trained on differently sized subsets ofthe training data.
For each subset size, we averagedover 20 samples.
The results are in Figure 2.The performance of all models increased as train-ing data grew, though there were diminishing returns(note the logarithmic scale).
Also, the models withresponse-based features outperform those with justreference-based features, as observed previously byHeilman and Madnani (2013).Most importantly, while all models with response-based features perform about the same with 1,000training examples or higher, the stacked modeltended to outperform the other models for caseswhere the number of training examples was verylimited.9This indicates that stacking enables learn-ing better feature weights than a simple combinationwhen the feature set contains a mixture of sparse aswell as dense features, particularly for smaller datasizes.5 ConclusionIn this paper, we explored methods for using dif-ferent sources of information for automatically scor-ing short, content-based assessment responses.
We9We are not aware of an appropriate significance test for ex-periments where subsets of the training data are used.
How-ever, the benefits of stacking seem unlikely to be due to chance.For all 4 items, stacking outperformed the non-stacking combi-nation for 18 or more of the 20 200-response training subsets(note that under a binomial test, this would be significant withp < 0.001).
Also, for the 100-response training subsets, stack-ing was better for 16 or more of the 20 subsets (p < 0.01).combined a response-based method that uses sparsefeatures (e.g., word and character n-grams) with areference-based method that uses a small number offeatures for the similarity between the response andinformation from the scoring guidelines (exemplarsand key concepts).On four reading comprehension assessment ques-tions, we found that a combined model using stack-ing outperformed a non-stacked combination, par-ticularly for the most practically relevant caseswhere training data was limited.
We believe thatsuch an approach may be useful for dealing with di-verse feature sets in other automated scoring tasksas well as other NLP tasks.As future work, it might be interesting to explore amore sophisticated model where the regression mod-els in different layers are trained simultaneously byback-propagating the error of the upper-layer, as inneural networks.AcknowledgmentsWe would like to thank John Sabatini and KiethaBiggers for providing us with the RfU datasets.We would also like to thank Dan Blanchard, AoifeCahill, Swapna Somasundaran, Anastassia Loukina,Beata Beigman Klebanov, and the anonymous re-viewers for their help.ReferencesEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 shared1053task: Semantic textual similarity.
In Second JointConference on Lexical and Computational Semantics(*SEM), Volume 1: Proceedings of the Main Confer-ence and the Shared Task: Semantic Textual Similarity,pages 32?43, Atlanta, Georgia, USA, June.J.
Cohen.
1968.
Weighted kappa: Nominal scale agree-ment with provision for scaled disagreement or partialcredit.
Psychological Bulletin, 70(4).Myroslava Dzikovska, Rodney Nielsen, Chris Brew,Claudia Leacock, Danilo Giampiccolo, Luisa Ben-tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.2013.
SemEval-2013 task 7: The joint student re-sponse analysis and 8th recognizing textual entailmentchallenge.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop on Se-mantic Evaluation (SemEval 2013), pages 263?274,Atlanta, Georgia, USA, June.Michael Heilman and Nitin Madnani.
2013.
ETS: Do-main adaptation and stacking for short answer scor-ing.
In Second Joint Conference on Lexical and Com-putational Semantics (*SEM), Volume 2: Proceedingsof the Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 275?279, Atlanta,Georgia, USA, June.C.
Leacock and M. Chodorow.
2003. c-rater: Scoring ofshort-answer questions.
Computers and the Humani-ties, 37.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InC.J.C.
Burges, L. Bottou, M. Welling, Z. Ghahramani,and K.Q.
Weinberger, editors, Advances in Neural In-formation Processing Systems 26, pages 3111?3119.George A. Miller.
1995.
WordNet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.Michael Mohler, Razvan Bunescu, and Rada Mihalcea.2011.
Learning to grade short answer questions usingsemantic similarity measures and dependency graphalignments.
In Proceedings of ACL:HLT, pages 752?762, Portland, Oregon, USA, June.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of machine translation.
In Proceedings of ACL,pages 311?318.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in python.Journal of Machine Learning Research, 12:2825?2830.J.
Sabatini and T. O?Reilly.
2013.
Rationale for a newgeneration of reading comprehension assessments.
InB.
Miller, L. Cutting, and P. McCardle, editors, Un-raveling Reading Comprehension: Behavioral, Neu-robiological and Genetic Components.
Paul H. BrooksPublishing Co.Georgios Sakkis, Ion Androutsopoulos, GeorgiosPaliouras, Vangelis Karkaletsis, Constantine D.Spyropoulos, and Panagiotis Stamatopoulos.
2001.Stacking classifiers for anti-spam filtering of e-mail.In L. Lee and D. Harman, editors, Proceedings ofthe 6th Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2001), pages 44?50.Mark D. Shermis and Jill Burstein.
2013.
Handbook ofautomated essay evaluation: Current applications andnew directions.
Routledge.Alex J. Smola and Bernhard Sch?olkopf.
2004.
A tutorialon support vector regression.
Statistics and Comput-ing, 14(3):199?222.Andr?e Filipe Torres Martins, Dipanjan Das, Noah A.Smith, and Eric P. Xing.
2008.
Stacking dependencyparsers.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 157?166, Honolulu, Hawaii, October.David H. Wolpert.
1992.
Stacked generalization.
NeuralNetworks, 5(2):241 ?
259.Zhibiao Wu and Martha Palmer.
1994.
Verbs semanticsand lexical selection.
In Proceedings of the 32Nd An-nual Meeting on Association for Computational Lin-guistics, ACL ?94, pages 133?138, Stroudsburg, PA,USA.
Association for Computational Linguistics.1054
