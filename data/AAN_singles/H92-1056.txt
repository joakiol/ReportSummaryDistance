REDUCED CHANNEL DEPENDENCE FOR SPEECH RECOGNITIONHy Murveit, John Butzberger, and Mitch WeintraubSRI InternationalSpeech Research and Technology ProgramMenlo Park, CA, 940251.
ABSTRACTSpeech recognition systems tend to be sensitive to unimportantsteady-state variation in speech spectra (i.e.
those caused by vary-ing the microphone or channel characteristics).
There have beenmany attempts o solve this problem; however, these techniquesare often computationally burdensome, specially for real-timeimplementation.
Recently, Hermansy et el.
\[1\] and Hirsch et el.
\[2\]have suggested a simple technique that removes slow-moving lin-ear channel variation with little adverse ffect on speech recogni-tion performance.
In this paper we examine this technique, knownas RASTA filtering, and evaluate its performance when applied toSRI's DECIPHER TM speech recognition system \[3l.
We show thatRASTA filtering succeeds in reducing DECIPHERTM's depen-dence on the channel.2.
INTRODUCTIONA number of techniques have been developed to compen-sate for the effects that varying microphone and channelshave on the acoustic signal.
Erell and Weintraub \[4, 5\] haveused additive corrections in the filter-bank log energy orcepstral domains based on equalizing the long-tenn averageof the observed filter-bank log energy or cepstral vector tothat of the training data.
The techniques developed by Roseand Paul \[6\] and Acero \[7\] used an iterative technique forestimating the cepstral bias vector that will maximize thelikelihood of the input utterance.
Nadas et el.
\[8\] used anadaptive linear transformation applied to the input repre-sentation, where the adaptation uses the VQ distortion vec-tor with respect to a predefined codebook.
VanCompemolle\[10\] scaled the filter-bank log energies to a specified rangeusing running histograms, and Rohlicek \[9\] experimentedwith a number of histogram-based compensation metricsbased on equalizing different aspects of the probability dis-tribution.One important limitation of the above approaches i thatthey rely on a speech/nonspeech detector.
Each of theabove approaches computes spectral properties of the inputspeech sentence and subsequently compensates for the sta-tistical differences with certain properties of the trainingdata.
If the input acoustical signal is not segmented by sen-tence (e.g.
open microphone with no push-to-talk button)and there are long periods of silence, the above approacheswould not be able to operate without some type of reliableautomatic speech-input/sentence-detection mechanism.
Anautomatic sentence-detection mechanism would have con-siderable difficulty in reliably computing the averagespeech spectrum if there were many other nonspeechsounds m the environment.A second class of techniques developed around auditorymodels (Lyon \[11\]; Cohen \[12\]; Seneff \[13\]; Ghitza \[14\]).These techniques use various automatic gain control andother auditory-type modeling techniques to output a spec-tral vector that has been adapted based on the acoustic his-tory.
A potential limitation of this approach is that many ofthese techniques are very computationally intensive.3.
THE RASTA FILTERRASTA filtering is a high-pass filter applied to a log-spec-tral representation f speech.
It removes low-moving vari-ations from the log spectrum.
The filtering is done on thelog-spectral representation so that multiplicative distortions(such as a linear filter) become additive and may beremoved with the RASTA filter.
A simple RASTA filtermay be implemented asfollows:y(t) -~ x(t) - -x( t - -1)  .-I- (C" y ( t -  1) )where x(t), as implemented in DECIPHERa~, is a log band-pass energy which is normally used in DECIPHER TM tocompute the Mel-cepstral feature vector.
Instead, x(t) isreplaced by y(t), the high-pass version of x(t), when per-forming the cepstral transform.\]'he constant, C, in the above equation defines the timeconstant of the RASTA filter.
It is desirable that C be such280that short-term variations in the log spectra (presumablyimportant parts of the speech signal) are passed by the filter,but slower variations are blocked.
We set C = 0.97 so thatsignals that vary faster than about 1 Hz are passed and thosethat vary less than once per second tend to be blocked.
Fig-ure 1 below plots the characteristic of this filter.dB-40 .00  .. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i-60.00 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.  "
.....................................................ii-60 .00 ............................. i ......i0.00 Z0.00 40.00HzFigure 1: Characteristics of the C = 0.97 RASTA filterWhen used in conjunction with SRI's spectral estimationalgorithms \[4, 5\], the high-pass filter is applied to the filter-bank log energies after the spectral estimation operation.The estimates of clean filter-bank energies are highpass fil-tered and then transformed to obtain the cepstral vector.The cepstral vector is then differenced twice to obtain thedelta-cepstral vector and the delta-delta-cepstral vector.3.1.
Removal of an Ideal Linear FilterWe first evaluated RASTA filtering by applying abandpassfilter (Figure 2 below) to a speech recognition task--contin-uous digit recognition performance over telephone lines.The filter was applied to the test set only (no filtering wasapplied to the training data).
We compared the resultingperformance with the performance of an unfiltered test setfor both standard and RASTA filtering.
As Table 1 shows,the RASTA filtering was successful in removing the effectsof the bandpass filter, whereas the standard system suffereda significant performance degradation due to the bandpassfilter.
Compared with our standard signal processing, theRASTA filtering was able to give a slight improvement onthe female digit error rate, with no significant change in themale digit error rate.
The dramatic decrease in performancethat occurs when the telephone speech is bandpass filteredis removed by the RASTA filtering, and the results are com-parable to the original speech signal.dB_,o.oo .................... t- ':k ...... i ......................../ , , i_5o.oo..2..2.1.. ....0.00 Z.00.
.
.
.
.
.
.
.
.
- .
.
- .
.
.
.
.
.
.
.
.
.
- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- .
- .
- .
- .Hz x 1000Figure 2: The distorting bandpass filter characteristic.Original BandpassSpeech Speechmale female male femaleI Standard 3.2 3.1 13.9 11.6RASTA 3.4 2.1 3.0 1.9Table 1: Word error rates for standard signal process-ing techniques and RASTA filtering techniques using cleanand bandpass-filtered t lephone speech.4.
REDUCED MICROPHONEDEPENDENCEAfter the encouraging initial study, we tested RASTA filter-ing in a more realistic manner--measuring theperformanceimprovement, due to RASTA filtering, when dissimilarmicrophones are used in the test and training data.To do this, we recorded 50 sentences (352 words) from onetalker simultaneously using two different microphones, aSennheiser flat-response close-talking microphone that wasused to train the system, and an Electrovoice 625 handsetwith a very different frequency characteristic.
The userspoke queries for DARPA's ATIS air-travel planning task.Table 2 shows that for this speaker, the error rate was lesssensitive to the difference in microphone when RASTA ill-tering was applied than when it wasn't.
Further, there is noevidence from this and the previous tudy to indicate thatRASTA filtering degrades performance when the micro-phone remains constant.281Sennheiser Electro VoiceStandard 13 (3.7%) 31 (8.8%)RASTA 12 (3.4%) 17 (4.8%)Table 2: Number and percentage ofword errors for asingle speaker when test microphone and signal processingwere varied.5.
DESKTOP MICROPHONESRASTA filtering is most effective when differencesbetween training and testing conditions can be modeled aslinear filters.
However, many distortions do not fit thismodel.
One example is testing with a desktop microphonewith models trained with a close-talking microphone.
In thisscenario, although the microphones characteristics may beapproximately related with a linear filter, additive noisepicked up by the desktop microphone violates the linear-fil-ter assumption.To see how important these ffects are, we performed rec-ognition experiment on systems trained with sennheisermicrophones and tested with a Crown desktop microphone.These test recordings were made at Carnegie Mellon Uni-versity (CMU) and at the Massachusetts In titute of Tech-nology (MIT).
They simultaneously recorded a speakerusing both Sennheiser and Crown microphones interactingwith an ATIS (air travel planning) system.The performance of DECIPHER TM on the ATIS recordingsis shown in Tables 3 and 4.
Table 3 shows the system per-formance results on MIT's recordings, while Table 4 con-tains the system performance r sults on CMU's recordings.Speaker Sennheiser Crown Crown Crown CrownStandard Standard RASTA NRFE NRFE+RASTA4V 13.0 13.8 22.8 18.7 16.34W 1.7 5.1 1.7 4.3 3.45E 17.8 26.6 27.8 18.1 14.755 18.5 26.6 25.3 23.2 17.659 13.7 40.2 41.0 26.6 23.6Average 12.9 22.5 23.7 18.2 15.1Table 3: Word error rate for MIT recordings varying microphone and signal processingSpeaker Sennheiser Crown Crown Crown CrownStandard Standard RASTA NRFE NRFE+RASTAIF 20.7 91.8 46.9 46.9 36.7IH 20.5 93.2 75.7 71.0 35.8IK 26.2 87.1 62.3 60.3 35.8Average 22.5 90.7 61.6 59.4 36.1Table 4: Word error rate for CMU recordings varying microphone and signal processing282For the MIT recordings, note that the best performing sys-tem on the Crown microphone data was very close with theperformance on the Sennheiser recordings (12.9% vs.15.1%).
The addition of RASTA processing did not help thestandard processing on the Crown data (the error rate wentup slightly from 22.5% to 23.7%) but it did help the noise-robust estimation processing (18.2% to 15.1%).The performance on CMU's Crown recordings were muchlower.
CMU's audio recordings for were noticeably noisier;the speaker sounded as if he was much farther from themicrophone, and there were other nonstationary sounds inthe background.
Note that the error rate with the standardsignal processing isextremely high (90.7% word error).
Forthe CMU Crown microphone r cordings, the addition ofRASTA processing helped reduce the error rate for both thestandard and noise-robust estimation processing conditions.The NRFE + RASTA processing was able to reduce theerror rate by 60% over the no-processing condition on theCMU Crown microphone recordings (90.7% to 36.1%).SRI's noise-robust pectral estimation algorithms aredesigned to estimate the filter-bank log energies of the cleanspeech signal when there is additive colored noise.
The esti-mation algorithms were designed to work independentlyfrom any spectral shape introduced by the microphone andchannel variations.
Therefore, some type of additional spec-tral normalization is required to compensate for theseeffects: the combined "NRFE + RASTA" system servesthat purpose.
The RASTA system (without estimation) canhelp compensate for the linear microphone effects, but itcan help only to a limited degree with the nonlinearitiesintroduced by other sounds.6.
ROBUSTNESS OF REPRESENTATIONTO MICROPHONE VARIATIONTo understand the benefit hat we have obtained using thedifferent processing techniques, we developed a metric forthe robustness of the representation that is separate fromspeech-recognition performance.
The DARPA CSR corpus(Doddington \[15\]) was used for this evaluation since it iscontains tereo recordings.
By using stereo recordings, wecan compare the robustness inthe representation that occurswhen the microphone is changed.
In this CSR corpus, thefirst channel of these stereo recordings i always a Sen-nheiser close-talking microphone.
The second recordingchannel uses one of 15 different secondary microphones.Using this stereo database, we can compute the cepstral fea-ture vector on each microphone channel, and compare thetwo representations to determine the level of invarianceprovided by the signal-processing/representation.
The met-ric that we used for determining the robustness of the repre-sentation iscalled relative-distortion a d is computed inthefollowing equation.Relative Distortion (Ci)( Ci (Micl) -- Ci (Mic2) ) 2(N " ( rCi (Micl) Ci (Mic2)The relative distortion for cepstral coefficient C i is com-puted by comparing the cepstral value of the first micro-phone with the same cepstral value computed on thesecondary microphone.
This average squared ifference isthen normalized by the variance of this cepstral feature onthe two microphones.
This metric gives an indication ofhow much variance there is due to the microphone differ-ences relative to the overall variance of the feature due tophonetic variation.
This metric is plotted as a function of thecepstral coefficient for different signal processing algo-rithms in figure 3.Figure 3 shows that he RASTA processing helps reduce thedistortion in the lower order cepstral coefficients.
Whencombined with SRI's noise-robust pectral estimation algo-rithms, the distortion decreases even further for the lowerorder cepstral coefficients.
Neither of the algorithms helpreduce the distortion for the higher cepstral coefficients.This metric indicates that even though the robust signal pro-cessing has reduced the recognition error rate due to micro-phone differences, there is still considerable variation in thecepstral representation when the microphone ischanged.7.
SUMMARYWe have shown that high-pass filtering of the filter-bank logenergies can be an effective means of reducing the effects ofsome microphone and channel variations.
We have shownthat such filtering can be used in conjunction with our previ-ous estimation techniques to deal with both noise andmicrophone effects.
The high-pass filtering operation is asimple technique that is computationally efficient and hasbeen incorporated into our real-time demonstration system.1.2.3.REFERENCESH.
Herrnansky, N. Morgan, A. Bayya, P. Kohn, "Compen-sation for the Effects of the Communication Channel inAuditory-Like Analysis of Speech," Eurospeech, Sept.1991, pp.
1367-1370.H.
Hirseh, P. Meyer, and H.W.
Ruehl, "Improved SpeechRecognition using High-Pass Filtering of Subband Enve-lopes," Eurospeech, Sept. 1991, pp.
413-416.H.
Murveit, J. Butzberger, and M. Weintraub, "SpeechRecognition i  SRI's Resource Management a d ATIS283Re la t ive  D is to r t ion!
i i I s tandard  !
i i i1 O0 ................... ~ ..................... 4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
; .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+....... ~e  .. .
.
.
.
.
.
.
.
.
.
.
.
.
.!
i i ~s i~.
.
.
.
.
.!
!
!
!i t i , 7~~."
h f f~ .....0.90 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
{ -  .
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .. .
.
.
.
-i i i i i ..=,':~i F ,  i ,~ i / . ""
- .~ i ...... " J  i i0.00  ..................
~-.;+.
.
.................. .
, - - - - -~ .~- , - - .
-~-  ~ ~  ............ + ......i / t ' , , , \  i / I  \ i / /  ~- - - i - L .
.
- - -  i i?
'-, : A . '
i  .
.
.
.
.
.
.
.
.
.
.
!
l i030 .
.
.
.
.
~.
.............. ~ - ~ ~  1 ~ - .
~ .
~ .
.
- - +  .
.
.
./ ".- !
, .." ".
.
.
!
.
< !
!.-" !
".. ,'" ! "
."
".-i."
I iooo  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: _  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.k ',.
?
i .
."
i i, .
.
.
v .
.
-  , i !- - -  - " u .t, u....-..~-...:.L-~ .
.
.
.
.
.
.
: : .
. '
.
:  .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.i i ii0.40 .
.
.
.
.
- .
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
l ............................................... ~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ ......0.30 ................. + .................................. ~ ................0.20  ...................................................................................................... i .......................... ~ ..............................i!
ii ii i0.10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
\[ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I0.00  ~ !f~.00 4 .00  6 .00  8 .00  10 .00  12 .00Cepst ra l  IndexFigure 3: Relative distortion plot-ted as a funct ion  of the cepstralindex for different signal processinga lgor i thms (s tandard ,  NRFE,RASTA, and RASTA + NRFE).4.5.6.7.8.Systems," DARAP SLS Workshop, February 1991, pp.
94-100.A.
Erell, and M. Weintraub, "Spectral Estimation for NoiseRobust Speech Recognition," DARPA SLS WorkshopOctober 89, pp.
319-324.A.
Erell, and M. Weintraub, "Recognition ofNoisySpeech: Using Minimum-Mean Log-Spectral DistanceEstimation," DARPA SLS Workshop, June 1990, pp.
341-345.R.
Rose and D. Paul, "A Hidden Markov Model BasedKeyword Recognition System," IEEE ICASSP 1990, pp.129-132.A.
Acero, "Acoustical and Environmental Robustness inAutomatic Speech Recognition," Ph.D. Thesis Carnegie-Mellon University, September 1990A.
Nadas, D. Nahamoo, M. Picheny, "Adaptive Labeling:Normalization ofSpeech by Adaptive Transformationsbased on Vector Quantization" IEEE ICASSP 1988, pp.521-524.9.10.11.12.13.14.15.R.
Rohlicek, W. Russell, S. Roukos, H. Gish, "ContinuousHidden Markov Modeling for Speaker-Independent WordSpotting," IEEE 1CASSP 1989, pp.
627-630D.
VanCompemolle, "Increased Noise Immunity in LargeVocabulary Speech Recognition with the Aid of SpectralSubtraction,' IEEE ICASSP 1987, pp 1143-1146.R.
Lyon, "Analog Implementations f Auditory Models,"DARPA SLS Workshop, Feb. 1991 pp.
212-216.J.
Cohen, "Application of an Auditory Model to SpeechRecognition," Journ.
Acoust.
Soc.
Amen., 1989, 85(6) pp.2623-2629.S.
Seneff, "A Joint Synchrony/Mean Rate Model of Audi-tory Speech Processing," Jour.
Phonetics, January 1988O.
Ghitza, "Auditory Neural Feedback as a Basis forSpeech Processing," 1988 IEEE 1CASSP, pp.
91-94.Doddington, G., "CSR Corpus Development," DARPASLS Workshop, Feb 1992.284
