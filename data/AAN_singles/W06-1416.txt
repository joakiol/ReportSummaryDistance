Proceedings of the Fourth International Natural Language Generation Conference, pages 111?113,Sydney, July 2006. c?2006 Association for Computational LinguisticsGenerating Multiple-Choice Test Items from Medical Text:A Pilot StudyNikiforos KaramanisComputer LaboratoryUniversity of CambridgeCB3 0FD, UKnk304@cam.ac.ukLe An Ha and Ruslan MitkovComputational Linguistics Research GroupUniversity of WolverhamptonWV1 1SB, UK{L.A.Ha, R.Mitkov}@wlv.ac.ukAbstractWe report the results of a pilot study on generatingMultiple-Choice Test Items from medical text anddiscuss the main tasks involved in this process andhow our system was evaluated by domain experts.1 IntroductionAlthoughMultiple-Choice Test Items (MCTIs) areused daily for assessment, authoring them is alaborious task.
This gave rise to a relatively newresearch area within the emerging field of Text-to-Text Generation (TTG) called Multiple-ChoiceTest Item Generation (MCTIG).1Mitkov et al (2006) developed a systemwhich detects the important concepts in atext automatically and produces MCTIs testingexplicitly conveyed factual knowledge.2 Thisdiffers from most related work in MCTIG such asBrown et al (2005) and the papers in BEAUNLP-II (2005) which deploy various NLP techniques toproduce MCTIs for vocabulary assessment, oftenusing preselected words as the input (see Mitkovet al for more extensive comparisons).The approach of Mitkov et al is semi-automaticsince the MCTIs have to be reviewed by domainexperts to assess their usability.
They report thatsemi-automatic MCTIG can be more than 3 timesquicker than authoring of MCTIs without the aidof their system.1TTG, in which surface text is used as the input toalgorithms for text production, contrasts with Concept-to-Text Generation (better known as Natural LanguageGeneration) which is concerned with the automaticproduction of text from some underlying non-linguisticrepresentation of information (Reiter and Dale, 2000).2Mitkov et al used an online textbook on Linguistics astheir source text.
Clearly, their approach is not concernedwith concepts or facts derived through inferencing.
Neitherdoes it address the problem of compiling a balanced test fromthe generated MCTIs.Moreover, analysis of MCTIs produced semi-automatically and used in the classroom revealsthat their educational value is not compromised inexchange for time and labour savings.
In fact, thesemi-automatically produced MCTIs turn out tofare better than MCTIs produced without the aidof the system in certain aspects of item quality.This paper reports the results of a pilot study ongenerating MCTIs from medical text which buildson the work of Mitkov et al2 Multiple-Choice Test Item GenerationA MCTI such as the one in example (1) typicallyconsists of a question or stem, the correct answeror anchor (in our example, ?chronic hepatitis?
)and a list of distractors (options b to d):(1) Which disease or syndrome may progress to cirrhosisif it is left untreated?a) chronic hepatitisb) hepatic failurec) hepatic encephalopathyd) hypersplenismThe MCTI in (1) is based on the following clausefrom the source text (called the source clause; seesection 2.3 below):(2) Chronic hepatitis may progress to cirrhosis if it is leftuntreated.We aim to automatically generate (1) from (2)using our simple Rapid Item Generation (RIG)system that combines several componentsavailable off-the-shelf.
Based on Mitkov et al, wesaw MCTIG as consisting of at least the followingtasks: a) Parsing b) Key-Term Identification c)Source Clause Selection d) Transformation toStem e) Distractor Selection.
These are discussedin the following sections.1112.1 Sentence ParsingSentence Parsing is crucial for MCTIG since theother tasks rely greatly on this information.
RIGemploys Charniak?s (1997) parser which appearedto be quite robust in the medical domain.2.2 Key-Term IdentificationOne of our main premises is that an appropriateMCTI should have a key-term as its anchorrather than irrelevant concepts.
For instance, theconcepts ?chronic hepatitis?
and ?cirrhosis?
arequite prominent in the source text that example (2)comes from, which in turn means that MCTIscontaining these terms should be generated usingappropriate sentences from that text.RIG uses the UMLS thesaurus3 as a domainspecific resource to compute an initial set ofpotential key terms such as ?hepatitis?
from thesource text.
Similarly to Mitkov et al, the initialset is enlarged with NPs featuring potential keyterms as their heads and satisfying certain regularexpressions.
This step adds terms such as ?acutehepatitis?
(which was not included in the versionof UMLS utilised by our system) to the set.The tf.idf method (that Mitkov et al didnot find particularly effective) is used to promotethe 30 most prominent potential key terms withinthe source text for subsequent processing, rulingout generic terms such as ?patient?
or ?therapy?which are very frequent within a larger collectionof medical texts (our reference corpus).2.3 Source Clause SelectionMitkov et al treat a clause in the source textas eligible for MCTIG if it contains at least onekey term and is finite as well as of the SV(O)structure.
They acknowledge, however, that thisstrategy gives rise to a lot of inappropriate sourceclauses, which was the case in our domain too.To address this problem, we implemented amodule which filters out inappropriate structuresfor MCTIG (see Table 1 for examples).
Thisexplains why the number of key terms and MCTIsvaries among texts (Table 2).A finite main clause which contains an NPheaded by a key term and functioning as asubject or object with all the subordinate clauseswhich depend on it is a source clause eligiblefor MCTIG provided that it satisfies our filters.Example (2) is such an eligible source clause.3http://www.nlm.nih.gov/research/umls/Structure Example (key term in italics)Subordinate clause Although asthma is a lung disease, ...Negated clause Autoimmune hepatitis should notbe treated with interferon.Coordinated NP Excessive salt intake causeshypertension and hypokalemia.Initial pronoun It associates with hypertension instead.Table 1: Inappropriate structures for MCTIG.Experimentation during development showed thatour module improves source clause selection byaround 30% compared to the baseline approach ofMitkov et al2.4 Transformation to StemOnce an appropriate source clause is identified,it has to be turned to the stem of a MCTI.
Thisinvolves getting rid of discourse cues such as?however?
and substituting the NP headed by thekey term such as ?chronic hepatitis?
in (1) with awh-phrase such as ?which disease or syndrome?.The wh-phrase is headed by the semantic type ofthe key-term derived from UMLS.RIG utilises a simple transformationalcomponent which produces a stem via minimalchanges in the ordering of the source clause.
Thefiltering module discussed in the previous sectiondisregards the clauses in which the key termfunctions as a modifier or adjunct.
Additionally,most of the key terms in the eligible source clausesappear in subject position which in turn meansthat wh-fronting and inversion is performed in justa handful of cases.
The following example, againbased on the source clause in (2), is one such case:(3) To which disease or syndrome may chronic hepatitisprogress if it is left untreated?2.5 Selection of Appropriate DistractorsMCTIs aim to test the ability of the studentto identify the correct answer among severaldistractors.
An appropriate distractor is a conceptsemantically close to the anchor which, however,cannot serve as the right answer itself.RIG computes a set of potential distractorsfor a key term using the terms with the samesemantic type in UMLS (rather than WordNetcoordinates employed by Mitkov et al).
Then, weapply a simple measure of distributional similarityderived from our reference corpus to select thebest scoring distractors.
This strategy means thatMCTIs with the same answer feature very similardistractors.112# of # of Usable Usable Items w/out Replaced distractors Total Average TimeChapter Words Key-terms Items Items post-edited stems per term Time per ItemAsthma 8,843 9 66 42 (64%) 18 (27%) 2.0 140 mins 3 mins 20 secsHepatitis 10,259 17 92 49 (53%) 19 (21%) 0.9 150 mins 3 mins 04 secsHypertension 12,941 22 121 59 (49%) 15 (12%) 0.8 200 mins 3 mins 23 secsTotal 32,043 40 279 150 (54%) 52 (19%) ?
490 mins 3 mins 16 secsTable 2: Usability and efficiency of Multiple-Choice Test Item Generation from medical text.3 EvaluationRIG is a simple system which often avoidstough problems such as dealing with key-terms insyntactic positions that might puzzle the parser ormight be too difficult to question upon.
So howdoes it actually perform?Three experts in producing MCTIs for medicalassessment jointly reviewed 279 MCTIs (eachfeaturing four distractors) generated by thesystem.
Three chapters from a medical textbookserved as the source texts while a much largercollection of MEDLINE texts was used as thereference corpus.The domain experts regarded a MCTI asunusable if it could not be used in a test or requiredtoo much revision to do so.
The remaining itemswere considered to be usable and could be post-edited by the experts to improve their content andreadability or replace inappropriate distractors.As Table 2 shows, more than half of the items intotal were judged to be usable.
Additionally, aboutone fifth of the usable items did not require anyediting.
The Table also shows the total number ofkey-terms identified in each chapter as well as theaverage number of distractors replaced per term.The last column of Table 2 reports on theefficiency of MCTIG in our domain.
This variableis calculated by dividing the total time it tookthe experts to review all MCTIs by the amountof usable items which represent the actual end-product.
This is a bit longer than 3 minutesper usable item across all chapters.
Anecdotalevidence and the experts?
own estimations suggestthat it normally takes them at least 10 minutes toproduce an MCTI manually.Given the distinct domains in which our systemand the one of Mitkov et al were deployed (aswell as the differences between them), a directcomparison between them could be misleading.We note, however, that our usability scores arealways higher than their worst score (30%) andquite close to their best score (57%).
The amountof directly usable items in Mitkov et al wasbetween just 3.5% and 5%, much lower thanwhat we achieved.
They also report an almost3-fold improvement in efficiency for computer-aided MCTIG, which is very similar to ourestimate.
These results indicate what our work hascontributed to the state of the art in MCTIG.In our future work, we aim to address thefollowing issues: (a) As in Mitkov et al, theanchor of a MCTI produced by RIG alwayscorresponds to a key-term.
However, the domainexperts pointed out several cases in which it isbetter for the key-term to stay in the stem andfor another less prominent concept to serve as theanswer.
(b) Students who simply memorise theinput chapter might be able to answer the MCTI ifits surface form is too close to the source clause soanother interesting suggestion was to paraphrasethe stem during MCTIG.
(c) We also intend tointroduce greater variability in our process fordistractor selection by investigating several othermeasures of semantic similarity.AcknowledgmentsWe are grateful to Tony LaDuca, Manfred Straehle andRobert Galbraith from the National Board of MedicalExaminers (NBME) for their expert-based feedback and tothree anonymous reviewers for their comments.ReferencesBEAUNLP-II.
2005.
Papers on MCTIG by Hoshino andNakagawa, Liu et al, and Sumita et al In Proceedings ofthe 2nd Workshop on Building Educational ApplicationsUsing NLP.Jonathan Brown, Gwen Frishkoff, and Maxine Eskenazi.2005.
Automatic question generation for vocabularyassessment.
In Proceedings of HLT-EMNLP 2005, pages249?254.Eugene Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proceedings of AAAI1997, pages 598?603.Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.
2006.A computer-aided environment for generating multiple-choice test items.
Natural Language Engineering,12(2):177?194.Ehud Reiter and Robert Dale.
2000.
Building NaturalLanguage Generation Systems.
Cambridge UniversityPress.113
