Proceedings of NAACL HLT 2007, pages 348?355,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsAn Information Retrieval Approach to Sense RankingMirella Lapata and Frank KellerSchool of Informatics, University of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LW, UK{mlap,keller}@inf.ed.ac.ukAbstractIn word sense disambiguation, choosingthe most frequent sense for an ambigu-ous word is a powerful heuristic.
However,its usefulness is restricted by the availabil-ity of sense-annotated data.
In this paper,we propose an information retrieval-basedmethod for sense ranking that does not re-quire annotated data.
The method queriesan information retrieval engine to estimatethe degree of association between a wordand its sense descriptions.
Experiments onthe Senseval test materials yield state-of-the-art performance.We also show that theestimated sense frequencies correlate reli-ably with native speakers?
intuitions.1 IntroductionWord sense disambiguation (WSD), the ability toidentify the intended meanings (senses) of wordsin context, is crucial for accomplishing many NLPtasks that require semantic processing.
Examples in-clude paraphrase acquisition, discourse parsing, ormetonymy resolution.
Applications such as machinetranslation (Vickrey et al, 2005) and information re-trieval (Stokoe, 2005) have also been shown to ben-efit from WSD.Given the importance of WSD for basic NLPtasks and multilingual applications, much work hasfocused on the computational treatment of senseambiguity, primarily using data-driven methods.Most accurate WSD systems to date are super-vised and rely on the availability of training data(see Yarowsky and Florian 2002; Mihalcea and Ed-monds 2004 and the references therein).
Althoughsupervised methods typically achieve better perfor-mance than unsupervised alternatives, their appli-cability is limited to those words for which senselabeled data exists, and their accuracy is stronglycorrelated with the amount of labeled data avail-able.
Furthermore, current supervised approachesrarely outperform the simple heuristic of choosingthe most common or dominant sense in the train-ing data (henceforth ?the first sense heuristic?
), de-spite taking local context into account.
One reasonfor this is the highly skewed distribution of wordsenses (McCarthy et al, 2004a).
A large number offrequent content words is often associated with onlyone dominant sense.Obtaining the first sense via annotation is ob-viously costly and time consuming.
Sense anno-tated corpora are not readily available for differentlanguages or indeed sense inventories.
Moreover,a word?s dominant sense will vary across domainsand text genres (the word court in legal documentswill most likely mean tribunal rather than yard).It is therefore not surprising that recent work (Mc-Carthy et al, 2004a; Mohammad and Hirst, 2006;Brody et al, 2006) attempts to alleviate the anno-tation bottleneck by inferring the first sense auto-matically from raw text.
Automatically acquired firstsenses will undoubtedly be noisy when compared tohuman annotations.
Nevertheless, they can be use-fully employed in two important tasks: (a) to createpreliminary annotations, thus supporting the ?anno-tate automatically, correct manually?
methodologyused to provide high volume annotation in the PennTreebank project; and (b) in combination with super-vised WSD methods that take context into account;for instance, such methods could default to the dom-inant sense for unseen words or words with uninfor-mative contexts.This paper focuses on a knowledge-lean senseranking method that exploits a sense inventory likeWordNet and corpus data to automatically inducedominant senses.
The proposed method infers theassociations between words and sense descriptionsautomatically by querying an IR engine whose in-dex terms have been compiled from the corpusof interest.
The approach is inexpensive, language-independent, requires minimal supervision, and usesno additional knowledge other than the word sensesproper and morphological query expansions.
We348evaluate our method on two tasks.
First, we usethe acquired dominant senses to disambiguate themeanings of words in the Senseval-2 (Palmer et al,2001) and Senseval-3 (Snyder and Palmer, 2004)data sets.
Second, we simulate native speakers?
intu-itions about the salience of word meanings and ex-amine whether the estimated sense frequencies cor-relate with sense production data.
In all cases our ap-proach outperforms a naive baseline and yields per-formances comparable to state of the art.In the following section, we provide an overviewof existing work on sense ranking.
In Section 3, weintroduce our IR-based method, and describe severalsense ranking models.
In Section 4, we present ourresults.
Discussion of our results and future workconclude the paper (Section 5).2 Related WorkMcCarthy et al (2004a) were the first to pro-pose a computational model for acquiring dominantsenses from text corpora.
Key in their approach isthe observation that distributionally similar neigh-bors often provide cues about a word?s senses.
Themodel quantifies the degree of similarity betweena word?s sense descriptions and its closest neigh-bors, thus delivering a ranking over senses where themost similar sense is intuitively the dominant sense.Their method exploits two notions of similarity,distributional and semantic.
Distributionally similarwords are acquired from the British National Cor-pus using an information-theoretic similarity mea-sure (Lin, 1998) operating over dependency re-lations (e.g., verb-subject, verb-object).
The latterare obtained from the output of Briscoe and Car-roll?s (2002) parser.
The semantic similarity betweenneighbors and senses is measured using a manuallycrafted taxonomy such as WordNet (see Budanitskyand Hirst 2001 for an overview of WordNet-basedsimilarity measures).Mohammad and Hirst (2006) propose an algo-rithm for inferring dominant senses without rely-ing on distributionally similar neighbors.
Their ap-proach capitalizes on the collocational nature ofsemantically related words.
Assuming a coarse-grained sense inventory (e.g., the Macquarie The-saurus), it first creates a matrix whose columns rep-resent all categories (senses) c1 .
.
.cn in the inven-tory and rows the ambiguous target words w1 .
.
.wm;the matrix cells record the number of times a tar-get word ti co-occurs with category c j within a win-dow of size s. Using an appropriate statistical test,they estimate the relative strength of association be-tween an ambiguous word and each of its senses.The sense with the highest association is the pre-dominant sense.Our work shares with McCarthy et al (2004a) andMohammad and Hirst (2006) the objective of infer-ring dominant senses automatically.
We propose aknowledge-lean method that relies on word associa-tion and requires no syntactic annotation.
The lattermay be unavailable when working with languagesother than English for which state-of-the-art parsersor taggers have not been developed.
Mohammad andHirst (2006) estimate the co-occurrence frequencyof a word and its sense descriptors by consideringsmall window sizes of up to five words.
These esti-mates will be less reliable for moderately frequentwords or for sense inventories with many senses.Our approach is more robust to sparse data ?
wework with document-based frequencies ?
and thussuitable for both coarse and fine grained sense in-ventories.
Furthermore, it is computationally inex-pensive; in contrast to McCarthy et al (2004a) wedo not rely on the structure of the sense inventoryfor measuring the similarity between synonyms andtheir senses.
Moreover, unlike Mohammad and Hirst(2006), our algorithm only requires co-occurrencefrequencies for the target word and its senses, with-out considering all senses in the inventory and allwords in the corpus simultaneously.3 Method3.1 MotivationCentral in our approach is the assumption that con-text provides important cues regarding a word?smeaning.
The idea dates back at least to Firth (1957)(?You shall know a word by the company it keeps?
)and underlies most WSD work to date.
Another ob-servation that has found wide application in WSD isthat words tend to exhibit only one sense in a givendiscourse or document (Gale et al, 1992).
Further-more, documents are typically written with certaintopics in mind which are often indicated by worddistributional patterns (Harris, 1982).For example, documents talking about congres-sional tenure are likely to contain words such as termof office or incumbency, whereas documents talkingabout legal tenure (i.e., the right to hold property)349are likely to include the words right or land.
Now,we could estimate which sense of tenure is mostprevalent simply by comparing whether tenure co-occurs more often with term of office than with landprovided we knew that both of these terms are se-mantically related to tenure.
Fortunately, senses inWordNet (and related taxonomies) are representedby synonym terms.
So, all we need to do for esti-mating a word?s sense frequencies is to count howoften it co-occurs with its synonyms.
We adopt herea fairly broad definition of co-occurrence, two wordsco-occur if they are attested in the same document.We could obtain such counts from any documentcollection; however, to facilitate comparisons withprior work (e.g., McCarthy et al 2004a), all our ex-periments use the British National Corpus (BNC).
Inwhat follows we describe in detail how we retrieveco-occurrence counts from the BNC and how we ac-quire dominant senses.3.2 Dominant Sense AcquisitionThroughout the paper we use the term frequency as ashorthand for document frequency, i.e., the numberof documents that contain a word or a set of wordswhich may or may not be adjacent.
The methodwe propose here exploits document frequencies ofwords and their sense definitions.
We base our dis-cussion below on the WordNet sense inventory andits representation of senses in terms of synonymsets (synsets).
However, our approach is not lim-ited to this particular lexicon; any dictionary withsynonym-based sense definitions could serve ourpurposes.As an example consider the noun tenure, whichhas the following senses in WordNet:(1) Sense 1tenure, term of office, incumbency=> termSense 2tenure, land tenure=> legal rightThe senses are represented by the two synsets{tenure, term of office, incumbency} and{tenure, land tenure}.
(The hypernyms for eachsense are also listed; indicated by the arrows.)
Wecan now approximate the frequency with which aword w1 occurs with the sense s by computing itssynonym frequencies: for each word w2 ?
syns(s),the set of synonyms of s, we field a query of the formw1 AND w2.
These synonym frequencies can then beused to determine the most frequent sense of w1 in avariety of ways (to be detailed below).The synsets for the two senses in (1) give rise tothe queries in (2) and (3).
Note that two queries aregenerated for the first synset, as it contains two syn-onyms of the target word tenure.
(2) a.
"tenure" AND "term of office"b.
"tenure" AND "incumbency"(3) "tenure" AND "land tenure"For example, query (2-a) will return the number ofdocuments in which tenure and term of office co-occur.
Presumably, tenure is mainly used in its dom-inant sense in these documents.
In the same way,query (3) will return documents in which tenure isused in the sense of land tenure.
Note that this wayof approximating synonym frequencies as documentfrequencies crucially relies on the ?one sense perdiscourse?
hypothesis (Gale et al, 1992), under theassumption that a document counts as a discoursefor word sense disambiguation purposes.Apart from synonym frequencies, we also gener-ate hypernym frequencies by submitting queries ofthe form w1 AND w2, for each w2 ?
hype(s), the set ofimmediate hypernyms of the sense s. The hypernymqueries for the two senses of tenure are:(4) "tenure" AND "term"(5) "tenure" AND "legal right"Hypernym queries are particularly useful for synsetsof size one, i.e., where a word in a given sense hasno synonyms, and is only differentiated from othersenses by its hypernyms.Before submitting queries such as the ones in(2) and (3) to an IR engine, we perform queryexpansion to make sure that all relevant in-flected forms are included.
For example the queryterm "tenure" is expanded to ("tenure" OR"tenures"), i.e., both singular and plural nounforms are generated.
Similarly, all inflected verbforms are generated, e.g., "keep up" gives rise tothe query term ("keep up" OR "keeps up" OR"keeping up" OR "kept up").
John Carroll?ssuite of morphological tools (morpha and morphg)is used to generate inflected forms for verbs and350nouns.1The queries generated this way are then submittedto an IR engine to obtain document counts.
Specifi-cally, we indexed the BNC using GLIMPSE (GlobalImplicit Search) a fast and flexible indexing andquery system2 (Manber and Wu, 1994).
GLIMPSEsupports approximate and exact matching, Booleanqueries, wild cards, regular expressions, and manyother options.
The text is divided into equal sizeblocks and an inverted index is created containingthe words and the block numbers in which they oc-cur.
Given a query, GLIMPSE will retrieve the rele-vant documents using a two-level search method.
Itwill first locate the query in the inverted index andthen use sequential search to find an exact answer.Once synonym frequencies and hypernym fre-quencies are in place, we can compute a word?s pre-dominant sense in a number of ways.
First, we canvary the way the frequency of a given sense is esti-mated based on synonym frequencies:?
Sum: The frequency of a given synset is com-puted as the sum of the synonym frequen-cies.
For example, the frequency of the dom-inant sense of tenure would be computed byadding up the document frequencies returnedby queries (2-a) and (2-b).?
Average (Avg): The frequency of a synset iscomputed by taking the average of synonymfrequencies.?
Highest (High): The frequency of a synset isdetermined by the synonym with the highestfrequency.Secondly, we can vary whether or not hypernyms aretaken into account:?
No hypernyms (?Hyp): Only the synonymfrequencies are included when computing thefrequency of a synset.
For example, only thequeries of (2-a) and (2-b) are relevant for esti-mating the dominant sense of tenure.?
Hypernyms (+Hyp): Both synonym and hy-pernym frequencies are taken into account1The tools can be downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/morph.html.2The software can be downloaded from http://webglimpse.net/download.phpwhen computing sense frequency.
For example,the frequency for the senses of tenure wouldbe computed based on the document frequen-cies returned by queries (2-a), (2-b), and (4)(by summing, averaging, or taking the highestvalue, as before).The third option relates to whether the sense fre-quencies are used in raw or in normalized form:?
Non-normalized (?Norm): The raw synonymfrequencies are used as estimates of sense fre-quencies.?
Normalized (+Norm): Sense frequencies arecomputed by dividing the word-synonym fre-quency by the frequency of the synonym inisolation.
For example, the normalized fre-quency for (2-a) is computed by dividingthe document frequency for "tenure" AND"term of office" by the document fre-quency for "term of office".
Normalizingtakes into account the fact that the members ofthe synset of a sense may differ in frequency.The combination of the above parameters yields 12sense ranking models.
We explore the parameterspace exhaustively on the Senseval-2 benchmarkdata set.
The best performing model on this data setis then used in all our subsequent experiments.
Weuse Senseval-2 as a development set, but we alsodemonstrate that a far smaller manually annotatedsample is sufficient for selecting the best model.4 ExperimentsOur experiments were driven by three questions:(1) Is WSD feasible at all with a model that doesnot employ any syntactic or semantic knowledge?Recall that McCarthy et al (2004a) propose a modelthat crucially relies on a robust parser for estimat-ing dominant senses.
(2) What is the best parametersetting for our model?
(3) Do the acquired dominantsenses correlate with human judgments?
If our sensefrequencies exhibit no such correlation, it is unlikelythat they will be useful in practical applications.To address the first two questions we use the in-duced first senses to perform WSD on the Senseval-2 and Senseval-3 data sets.
For our third question wecompare native speakers?
semantic intuitions againstthe BNC sense frequencies.351?Norm +Norm+Hyp ?Hyp +Hyp ?HypP R P R P R P RSum 42.3 40.8 46.3 44.6 45.9 44.3 48.6 46.8High 51.6 49.8 51.1 49.3 57.2 55.1 59.7 57.6Avg 44.1 42.6 48.5 46.8 49.6 47.8 51.5 49.6Table 1: Results for Senseval-2 data by model in-stantiation4.1 Model SelectionThe goal of our first experiment is to establish whichmodel configuration (see Section 3.2) is best suitedfor the WSD task.
We thus varied how the overallfrequency is computed (Sum, High, Avg), whetherhyponyms are included (?Hyp), and whether thefrequencies are normalized (?Norm).
To explore theparameter space, we used the Senseval-2 all-wordstest data as our development set.
This data set con-sists of three documents from the Wall Street Jour-nal containing approximately 2,400 content words.Following McCarthy et al (2004a), we first use ourmethod to find the dominant sense for all word typesin the corpus and then use that sense to disambiguatetokens without taking contextual information intoaccount.
We used WordNet 1.7.1 (Fellbaum, 1998)senses.3We compared our results to a baseline that se-lects for each word type a random sense, assumesit is the dominant one, and uses it to disambiguateall instances of the target word (McCarthy et al,2004a).
We also report the WSD performance of amore competitive baseline that always chooses thesense with the largest synset as the dominant sense.Consider again the word tenure from Section 3.2.According to this baseline, the dominant sense fortenure is the first one since it is represented by thelargest synset (three members).Our results on Senseval-2 are summarized in Ta-ble 1.
We observe that models that do not includehypernyms yield consistently better precision andrecall than models that include them.
On the onehand, hypernyms render the estimated sense distri-butions less sparse.
On the other hand, they intro-duce considerable noise; the resulting sense frequen-cies are often similar ?
the same hypernyms can be3Senseval-2 is annotated with WordNet 1.7 senses whichwe converted to 1.7.1 using a publicly available mapping (seehttp://www.cs.unt.edu/?rada/downloads.html).BaseR BaseS ModelP R P R P R NNoun 26.8 25.4 45.8 43.4 53.1?# 50.2?# 1,063Verb 11.2 11.1 19.9 19.5 48.2?# 47.3?# 569Adj 22.1 21.4 56.5 56.0 56.7?
56.2?
451Adv 48.0 45.9 66.4 62.9 86.4?# 81.8?# 301All 26.3 25.4 42.2 40.7 59.7?# 57.6?# 2,384Table 2: Results of best model (High, +Norm,?Hyp) for Senseval-2 data by part of speech(?
: sig.
diff.
from BaseR, #: sig.
diff.
from BaseS;p < 0.01 using ?2 test)shared among several senses ?
and selecting one pre-dominant sense over the other can be due to verysmall frequency differences.
We also find that mod-els with normalized document counts outperformmodels without normalization.
This is not surpris-ing, there is ample evidence in the literature (Mo-hammad and Hirst, 2006; Turney, 2001) that associ-ation measures (e.g., conditional probability, mutualinformation) are better indicators of lexical similar-ity than raw frequency.
Finally, selecting the syn-onym with the highest frequency (and defaulting toits sense) achieves better results in comparison to av-eraging or summing over all synsets.In sum, the best performing model is High,+Norm, ?Hyp, achieving a precision of 59.7% anda recall of 57.9%.
The results for this model are bro-ken down by part of speech in Table 2.
Here, wealso include a comparison with the random base-line (BaseR) and a baseline that selects the dominantsense by synset size (BaseS).
We observe that theoptimal model significantly outperforms both base-lines on the complete data set (see row All in Ta-ble 2) and on most individual parts of speech (perfor-mances are comparable for our model and BaseS onadjectives).
BaseS is far better than BaseR and gen-erally harder to beat.
Defaulting to synset size in theabsence of any other information is a good heuristic;large synsets often describe frequent senses.
Vari-ants of our model that select a dominant sense bysumming over synset members are closest to thisbaseline.
Note that our best performing model doesnot rely on synset size; it simply selects the synonymwith the highest frequency, despite the fact that itmight belong to a large or small synset.
We con-jecture that its superior performance is due to thecollocational nature of semantic similarity (Turney,352?Norm +Norm+Hyp ?Hyp +Hyp ?HypP R P R P R P RSum 42.3 40.8 46.3 44.6 45.2 44.7 44.6 44.0High 51.6 49.8 51.1 49.3 55.0 54.3 61.3 60.5Avg 44.1 42.6 48.5 46.8 51.5 50.8 50.4 49.8Table 3: Results for 10% of Senseval-2 data bymodel instantiation2001).In order to establish that High, +Norm, ?Hyp isthe optimal model, we utilized the whole Senseval-2 data set.
Using such a large dataset is more likelyto yield a stable parameter setting, but it also raisesthe question whether parameter optimization couldtake place on a smaller dataset which is less costlyto produce.
Table 3 explores the parameter space ona sample randomly drawn from Senseval-2 that con-tains only 240 tokens (i.e., one tenth of the originaldata set).
The behavior of our models on this smallersample is comparable to that on the entire Senseval-2 data.
Importantly, both sets yield the same bestmodel, i.e., High, +Norm, ?Hyp.
In the remainderof this paper we will use this model for further ex-periments without additional parameter tuning.4.2 Application to Senseval-3 DataWe next evaluate our best model the on theSenseval-3 English all-words data set.
Senseval-3consists of two Wall Street Journal articles andone excerpt from the Brown corpus (approximately5,000 content words in total).
Similarly to the ex-periments reported in the previous section, we usedWordNet 1.7.1.
We calculate recall and precisionwith the Senseval-3 scorer.Our results are given in Table 4.
Besides thetwo baselines (BaseR and BaseS), we also com-pare our model to McCarthy et al (2004b)4 andthe best unsupervised (IRST-DDD) and supervised(GAMBLE) systems that participated in Senseval-3.IRST-DDD was developed by Strapparava et al(2004) and performs domain driven disambiguation.Specifically, the approach compares the domain ofthe context surrounding the target word with the do-mains of its senses and uses a version of WordNet4Comparison against Mohammad and Hirst (2006) was notpossible since they use a sense inventory other than WordNet(i.e., Roget?s thesaurus) and evaluate their model on artificiallygenerated sense-tagged data.P RBaseR 23.1#?$?
22.7#?$?BaseS 36.6??$?
35.9?
?$?McCarthy 49.0?#$?
43.0?#$?IR-Model 58.0?#??
57.0?#?
?IRST-DDD 58.3?#??
58.2?#?
?Semcor 62.4?#?$ 62.4?#?$GAMBLE 65.1?#?$?
65.2?#?$?Table 4: Comparison of results on Senseval-3 data(?
: sig.
diff.
from BaseR, #: sig.
diff.
from BaseS,?
: sig.
diff.
from McCarthy, $: sig.
diff.
from IR-Model, ?
: sig.
diff.
from SemCor; p < 0.01 using?2 test)BaseR BaseS ModelP R P R P R NNoun 27.8 12.2 41.1 41.0 58.1?# 58.0?# 900Verb 12.8 4.6 20.0 19.9 61.0?# 60.8?# 732Adj 29.2 5.2 56.5 56.5 50.3?
50.3?
363Adv 100.0 0.6 100.0 81.2 100.0 81.2 16All 23.1 22.7 36.6 35.9 58.0?# 57.0?# 2,011Table 5: Results of best model (High, +Norm,?Hyp) for Senseval-3 data by part of speech(?
: sig.
diff.
from BaseR, #: sig.
diff.
from BaseS;p < 0.01 using ?2 test)augmented with domain labels (e.g., economy, ge-ography).
GAMBL (Decadt et al, 2004) is a super-vised system: a classifier is trained for each ambigu-ous word using memory-based learning.
We also re-port the performance achieved by defaulting to thefirst WordNet entry for a given word and part ofspeech.
Entries in WordNet are ranked accordingto the sense frequency estimates obtained from themanually annotated SemCor corpus.
First senses ob-tained from SemCor will be naturally less noisy thanthose computed by our method which does not makeuse of manual annotation in any way.
We thereforeconsider the WSD performance achieved with Sem-Cor first senses as an upper bound for automaticallyacquired first senses.Our model significantly outperforms the twobaselines and McCarthy et al (2004b).
Its precisionand recall according to individual parts of speech isshown in Table 5.
The model performs comparablyto IRST-DDD and significantly worse than GAM-BLE.
This is not entirely surprising given that GAM-353BLE is a supervised system trained on a varietyof manually annotated resources including SemCor,data from previous Senseval workshops and the ex-ample sentences in WordNet 1.7.1.
GAMBLE is theonly system that significantly outperforms the Sem-Cor upper bound.
Finally, note that our model isconceptually simpler than McCarthy et al (2004b)and IRST-DDD.
It neither requires a parser (for ob-taining distributionally similar neighbors) nor anyknowledge other than WordNet (e.g., domain la-bels).
This makes our method portable to languagesfor which syntactic analysis tools and elaborate se-mantic resources are not available.4.3 Modeling Human DataResearch in psycholinguistics has shown that themeanings of ambiguous words are not perceived asequally salient in the absence of a biasing context(Durkin and Manning, 1989; Twilley et al, 1994).Rather, language users often ascribe dominant andsubordinate meanings to polysemous words.
Previ-ous studies have elicited intuitions with regard toword senses using a free association task.
For ex-ample, Durkin and Manning (1989) collected asso-ciation norms from native speakers for 175 ambigu-ous words.
They asked subjects to read each wordand write down the first meaning that came to mind.The words were presented out of context.
From thesubjects?
responses, they computed sense frequen-cies, which revealed that most words were attributeda particular meaning with a markedly higher fre-quency than other meanings.In this experiment, we examine whether ourmodel agrees with human intuitions regarding theprevalence of word senses.
We inferred the dominantmeanings for the polysemous words used in Durkinand Manning (1989).
These exhibit a relatively highdegree of ambiguity (the average number of sensesper word is three) and cover a wide variety of partsof speech (for the full set of words and elicitedsense frequencies see their Appendix A, pp.
501?609).
One stumbling block to using this data arethe meanings associated with the ambiguous words.These were provided by native English speakers andmay not necessarily correspond to senses describedby trained lexicographers.
Fortunately, we were ableto map most of them (except for six which we dis-carded) on WordNet synsets (version 1.6); two an-notators performed the mapping by comparing thesense descriptions provided by Durkin and Manningact Freq answer Freqpretense/performance 37 response 81to perform 30 solution 18to take action 16division 12a deed 3Table 6: Meaning frequencies for act and answer;normative data from Durkin and Manning (1989)to WordNet synsets.
The annotators agreed in theirassignments 81% of the time.
Disagreements wereresolved through mediation.Examples of Durkin and Manning?s (1989)normative data are given in Table 6.
The senseresponse for answer was mapped to the WordNetsynset {answer, reply, response} (Sense 1),the sense solution was mapped to the synset{solution, answer, result, resolution,solvent} (Sense 2), etc.
Durkin and Manning didnot take part of speech ambiguity into account, asTable 6 shows, subjects came up with meaningsrelating to the verb and noun part of speech of act.We explored the relationship between the sensefrequencies provided by human subjects and thoseestimated by our model by computing the Spearmanrank correlation coefficient ?.
We obtained sensefrequencies from the BNC using the best modelfrom Section 4.1 (High, +Norm, ?Hyp).
We foundthat the resulting sense frequencies were signifi-cantly correlated with the human sense frequencies(?
= 0.384, p < 0.01).
We performed the same ex-periment using McCarthy et al?s (2004a) model,which also achieved a significant correlation (?
=0.316, p < 0.01).
This result provides an additionalvalidation of our model as it demonstrates that thesense frequencies it generates can capture the sensepreferences of naive human subjects (rather thantrained lexicographers).5 DiscussionIn this paper we proposed an IR-based approachfor inducing dominant senses automatically.
Ourmethod estimates the degree of association betweenwords and their sense descriptions (represented bysynsets in WordNet) simply by querying an IR en-gine.
Evaluation on the Senseval data sets showedthat our model significantly outperformed a naiverandom sense baseline and a more competitive one354based on synset size.
Our method was significantlybetter than McCarthy et al (2004b) on Senseval-2and Senseval-3.
On the latter data set, its perfor-mance was comparable to that of the best unsuper-vised system (Strapparava et al, 2004).An important future direction lies in evaluatingthe disambiguation potential of our models acrossdomains and languages.
Furthermore, our experi-ments have relied on WordNet for providing theappropriate sense descriptions.
Future work mustassess whether the models presented in this pa-per can be extended to alternative sense invento-ries (e.g., dictionary definitions) that may differ ingranularity and structure.
We will also experimentwith a wider range of lexical association measuresfor quantifying the similarity of a word and itssynonyms.
Examples include odds ratio (Moham-mad and Hirst, 2006) and Turney?s (2001) IR-basedpointwise mutual information (PMI-IR).Our experiments revealed that the IR-based modelis particularly good at disambiguating certain partsof speech (e.g., verbs, see Tables 2 and 5).
A promis-ing direction is the combination of different rankingmodels (Brody et al, 2006) and the integration ofdominant sense models with supervised WSD.Acknowledgments We are grateful to Diana Mc-Carthy for her help with this work.
The au-thors acknowledge the support of EPSRC (grantEP/C538447/1).ReferencesBriscoe, Ted and John Carroll.
2002.
Robust accurate statisticalannotation of general text.
In Proceedings of the 3rd Interna-tional Conference on Language Resources and Evaluation.Las Palmas, Gran Canaria, pages 1499?1504.Brody, Samuel, Roberto Navigli, and Mirella Lapata.
2006.
En-semble methods for unsupervised WSD.
In Proceedings ofthe 21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Association forComputational Linguistics.
Sydney, Australia, pages 97?104.Budanitsky, Alexander and Graeme Hirst.
2001.
Semanticdistance in WordNet: An experimental, application-orientedevaluation of five measures.
In Proceedings of the NAACLWorkshop on WordNet and Other Lexical Resources.
Pitts-burgh, PA.Decadt, Bart, Ve?ronique Hoste, Walter Daelemans, and Antalvan den Bosch.
2004.
GAMBL, genetic algorithm optimiza-tion of memory-based WSD.
In Mihalcea and Edmonds(2004), pages 108?112.Durkin, Kevin and Jocelyn Manning.
1989.
Polysemy andthe subjective lexicon: Semantic relatedness and the salienceof intraword senses.
Journal of Psycholinguistic Research18(6):577?612.Fellbaum, Christiane, editor.
1998.
WordNet: An ElectronicDatabase.
MIT Press, Cambridge, MA.Firth, J. R. 1957.
A Synopsis of Linguistic Theory 1930-1955.Oxford: Philological Society.Gale, William A., Kenneth W. Church, and David Yarowsky.1992.
A method for disambiguating word senses in a largecorpus.
Computers and the Humanities 26(5?6):415?439.Harris, Zellig.
1982.
Discourse and sublanguage.
In R. Kit-tredge and J. Lehrberger, editors, Language in RestrictedSemantic Domains, Walter de Gruyter, Berlin; New York,pages 231?236.Lin, Dekang.
1998.
An information-theoretic definition of sim-ilarity.
In Proceedings of the 15th International Conferenceon Machine Learning.
Madison, WI, pages 296?304.Manber, Udi and Sun Wu.
1994.
GLIMPSE: a tool to searchthrough entire file systems.
In Proceedings of USENIX Win-ter 1994 Technical Conference.
San Francisco, CA, pages23?32.McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll.2004a.
Finding predominant senses in untagged text.
InProceedings of the 42nd Annual Meeting of the Associationfor Computational Linguistics.
Barcelona, pages 279?286.McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll.2004b.
Using automatically acquired predominant sensesfor word sense disambiguation.
In Mihalcea and Edmonds(2004), pages 151?154.Mihalcea, Rada and Phil Edmonds, editors.
2004.
Proceed-ings of Senseval-3: The 3rd International Workshop on theEvaluation of Systems for the Semantic Analysis of Text.Barcelona.Mohammad, Saif and Graeme Hirst.
2006.
Determining wordsense dominance using a thesaurus.
In Proceedings of the11th Conference of the European Chapter of the Associationfor Computational Linguistics.
Trento, Italy, pages 121?128.Palmer, Martha, Christiane Fellbaum, Scott Cotton, LaurenDelfs, and Hoa Trang Dang.
2001.
English tasks: All wordsand verb lexical sample.
In Proceedings of Senseval-2: The3rd International Workshop on the Evaluation of Systems forthe Semantic Analysis of Text.
Toulouse.Snyder, Benjamin and Martha Palmer.
2004.
The English all-words task.
In Mihalcea and Edmonds (2004).Stokoe, Christopher.
2005.
Differentiating homonymy and pol-ysemy in information retrieval.
In Proceedings of the HumanLanguage Technology Conference and the Conference onEmpirical Methods in Natural Language Processing.
Van-couver, pages 403?410.Strapparava, Carlo, Alfio Gliozzo, and Claudio Giuliano.
2004.Word-sense disambiguation for machine translation.
In Mi-halcea and Edmonds (2004), pages 229?234.Turney, Peter D. 2001.
Mining the web for synonyms: PMI-IRversus LSA on TOEFL.
In Proceedings of the 12th EuropeanConference on Machine Learning.
Freiburg, Germany, pages491?502.Twilley, L. C., P. Dixon, D. Taylor, and K. Clark.
1994.
Univer-sity of Alberta norms of relative meaning frequency for 566homographs.
Memory and Cognition 22(1):111?126.Vickrey, David, Luke Biewald, Marc Teyssier, and DaphneKoller.
2005.
Word-sense disambiguation for machine trans-lation.
In Proceedings of the Human Language TechnologyConference and the Conference on Empirical Methods inNatural Language Processing.
Vancouver, pages 771?778.Yarowsky, David and Radu Florian.
2002.
Evaluating sense dis-ambiguation across diverse parameter spaces.
Natural Lan-guage Engineering 9(4):293?310.355
