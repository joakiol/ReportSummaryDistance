Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 556?566,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsFactored Soft Source Syntactic Constraints for Hierarchical MachineTranslationZhongqiang HuangRaytheon BBN Technologies50 Moulton StCambridge, MA, USAzhuang@bbn.comJacob DevlinRaytheon BBN Technologies50 Moulton StCambridge, MA, USAjdevlin@bbn.comRabih ZbibRaytheon BBN Technologies50 Moulton StCambridge, MA, USArzbib@bbn.comAbstractThis paper describes a factored approach toincorporating soft source syntactic constraintsinto a hierarchical phrase-based translationsystem.
In contrast to traditional approachesthat directly introduce syntactic constraints totranslation rules by explicitly decorating themwith syntactic annotations, which often ex-acerbate the data sparsity problem and causeother problems, our approach keeps transla-tion rules intact and factorizes the use of syn-tactic constraints through two separate mod-els: 1) a syntax mismatch model that asso-ciates each nonterminal of a translation rulewith a distribution of tags that is used tomeasure the degree of syntactic compatibil-ity of the translation rule on source spans; 2)a syntax-based reordering model that predictswhether a pair of sibling constituents in theconstituent parse tree of the source sentenceshould be reordered or not when translated tothe target language.
The features producedby both models are used as soft constraintsto guide the translation process.
Experimentson Chinese-English translation show that theproposed approach significantly improves astrong string-to-dependency translation sys-tem on multiple evaluation sets.1 IntroductionHierarchical phrase-based translation models (Chi-ang, 2007) are widely used in machine translationsystems due to their ability to achieve local flu-ency through phrasal translation and handle non-local phrase reordering using synchronous context-free grammars.
A large number of previous workshave tried to introduce grammaticality to the trans-lation process by incorporating syntactic constraintsinto hierarchical translation models.
Despite somedifferences in the granularity of syntax units (e.g.,tree fragments (Galley et al 2004; Liu et al 2006),treebank tags (Shen et al 2008; Chiang, 2010), andextended tags (Zollmann and Venugopal, 2006)),most previous work incorporates syntax into hier-archical translation models by explicitly decoratingtranslation rules with syntactic annotations.
Theseapproaches inevitably exacerbate the data sparsityproblem and cause other problems such as increasedgrammar size, worsened derivational ambiguity, andunavoidable parsing errors (Hanneman and Lavie,2013).In this paper, we propose a factored approachthat incorporates soft source syntactic constraintsinto a hierarchical string-to-dependency translationmodel (Shen et al 2008).
The general ideas are ap-plicable to other hierarchical models as well.
Insteadof enriching translation rules with explicit syntacticannotations, we keep the original translation rulesintact, and factorize the use of source syntactic con-straints through two separate models.The first is a syntax mismatch model that intro-duces source syntax into the nonterminals of transla-tion rules, and measures the degree of syntactic com-patibility between a translation rule and the sourcespans it is applied to during decoding.
When a hi-erarchical translation rule is extracted from a par-allel training sentence pair, we determine a tag foreach nonterminal based on the dependency parse ofthe source sentence.
Instead of fragmenting rulestatistics by directly labeling nonterminals with tags,556we keep the original string-to-dependency transla-tion rules intact and associate each nonterminal witha distribution of tags.
That distribution is then usedto measure the syntactic compatibility between thesyntactic context from which the translation rule isextracted and the syntactic analysis of a test sen-tence.The second is a syntax-based reordering modelthat takes advantage of phrasal cohesion in transla-tion (Fox, 2002).
The reordering model takes a pairof sibling constituents in the source parse tree as in-put, and uses source syntactic clues to predict theordering distribution (straight vs. inverted) of theirtranslations on the target side.
The resulting order-ing distribution is used in the decoder at the wordpair level to guide the translation process.
This sep-arate reordering model allows us to utilize sourcesyntax to improve reordering in hierarchical trans-lation models without having to explicitly annotatetranslation rules with source syntax.Our results show that both the syntax mismatchmodel and the syntax-based reordering model areable to achieve significant gains over a strongChinese-English MT baseline.
The rest of the pa-per is organized as follows.
Section 2 discussesrelated work in the literature.
Section 3 providesan overview of our baseline string-to-dependencytranslation system.
Section 4 describes the detailsof the syntax mismatch and syntax-based reorderingmodels.
Experimental results are presented in Sec-tion 5.
The last section concludes the paper.2 Related WorkAttempts to use rich syntactic annotations do notalways result in improved performance when com-pared to purely hierarchical models that do notuse linguistic guidance.
For example, as shownin (Mi and Huang, 2008), tree-to-string translationmodels (Huang et al 2006) only start to outper-form purely hierarchical models when significant ef-forts were made to alleviate parsing errors by usingforest-based approaches in both rule extraction anddecoding.
Using only syntactic phrases is too re-strictive in phrasal translation as many useful phrasepairs are not syntactic constituents (Koehn et al2003).
The syntax-augmented translation modelof Zollmann and Venugopal (2006) annotates non-terminals in hierarchical rules with thousands of ex-tended syntactic categories in order to capture thesyntactic variations of phrase pairs.
This resultsin exacerbated data sparsity problems, partially dueto the requirement of exact matches in nonterminalsubstitutions between translation rules in the deriva-tion.
Several solutions were proposed.
Shen etal.
(2009) and Chiang (2010) used soft match fea-tures to explicitly model the substitution of nonter-minals with different labels; Venugopal et al(2009)used a preference grammar to soften the syntacticconstraints through the use of a preference distribu-tion of syntactic categories; and recently Hannemanand Lavie (2013) proposed a clustering approachto reduce the number of syntactic categories.
Ourproposed syntax mismatch model associates non-terminals with a distribution of tags.
It is simi-lar to the preference grammar in (Venugopal et al2009); however, we use treebank tags and focus onthe syntactic compatibility between translation rulesand the source sentence.
The work of Huang et al(2010) is most similar to ours, with the main differ-ence being that their syntactic categories are latentand learned automatically in a data driven fashionwhile we simply use treebank tags based on depen-dency parsing.
Marton and Resnik (2008) also ex-ploited soft source syntax constraints without mod-ifying translation rules.
However, they focused onthe quality of translation spans based on the syn-tactic analysis of the source sentence, while ourmethod explicitly models the syntactic compatibil-ity between translation rules and source spans.Most research on reordering in machine transla-tion focuses on phrase-based translation models asthey are inherently weak at non-local reordering.Previous efforts to improve reordering for phrase-based systems can be largely classified into two cat-egories.
Approaches in the first category try to re-order words in the source sentence in a preprocess-ing step to reduce reordering in both word alignmentand MT decoding.
The reordering decisions are ei-ther made using manual or automatically learnedrules (Collins et al 2005; Xia and McCord, 2004;Xia and McCord, 2004; Genzel, 2010) based on thesyntactic analysis of the source sentence, or con-structed through an optimization procedure that usesfeature-based reordering models trained on a word-aligned parallel corpus (Tromble and Eisner, 2009;557Khapra et al 2013).
Approaches in the second cate-gory try to explicitly model phrase reordering in thetranslation process.
These approaches range fromsimple distance based distortion models (Koehn etal., 2003) that globally penalizes reordering basedon the distorted distance, to lexicalized reorderingmodels (Koehn et al 2005; Al-Onaizan and Pap-ineni, 2006) that assign reordering preferences ofadjacent phrases for individual phrases, and to hi-erarchical reordering models (Galley and Manning,2008; Cherry, 2013) that handle reordering prefer-ences beyond adjacent phrases.
Although hierarchi-cal translation models are capable of handling non-local reordering, their accuracy is far from perfect.Xu et al(2009) showed that the syntax-augmentedhierarchical model (Zollmann and Venugopal, 2006)also benefits from reordering source words in a pre-processing step.
Explicitly adding syntax to trans-lation rules helps with reordering in general, but itintroduces additional complexities, and is still lim-ited by the context-free nature of hierarchical rules.Our work exploits an alternative direction that usesan external reordering model to improve word re-ordering of hierarchical models.
Gao et al(2011),Xiong et al(2012), and Li et al(2013) also studiedexternal reordering models for hierarchical models.However, they focused on specific word pairs suchas a word and its dependents or a predicate and itsarguments, while our proposed general frameworkconsiders all word pairs in a sentence.
Our syntax-based reordering model exploits phrasal cohesion intranslation (Fox, 2002) by modeling the reorderingof sibling constituents in the source parse tree, whichis similar to the recent work of Yang et al(2012).However, the latter focuses on finding the optimalreordering of sibling constituents before MT decod-ing, while our proposed model generates reorderingfeatures that are used together with other MT fea-tures to determine the optimal reordering during MTdecoding.3 String-to-Dependency TranslationOur baseline translation system is based on a string-to-dependency translation model similar to the im-plementation in (Shen et al 2008).
It is an extensionof the hierarchical translation model of Chiang et al(2006) that requires the target side of a phrase pairto have a well-formed dependency structure, definedas either of the two types:?
fixed structure: a single rooted dependencysub-tree with each child being a complete con-stituent.
In this case, the phrase has a uniquehead word inside the phrase, i.e., the root ofthe dependency sub-tree.
Each dependent ofthe head word, together with all of its descen-dants, is either completely inside the phrase orcompletely outside the phrase.
For example,the phrase give him in Figure 1 (a) has a fixeddependency structure with head word give.?
floating structure: a sequence of siblings witheach being a complete constituent.
In this case,the phrase is composed of a sequence of siblingconstituents whose common parent is outsidethe phrase.
For example, the phrase him thatbrown coat in Figure 1 is a floating structurewhose common parent give is not in the phrase.Requiring the target side to have a well-formeddependency structure is less restrictive than requir-ing it to be a syntactic constituent, allowing moretranslation rules to be extracted.
However, it stillresults in fewer rules than pure hierarchical transla-tion models and might hurt MT performance.
Thewell-formed dependency structure on the target sidemakes it possible to introduce syntax features dur-ing decoding.
Shen et al(2008) obtained signif-icant improvements from including a dependencylanguage model score in decoding, outweighing thenegative effect of the dependency constraint.
Shen etal.
(2009) proposed an approach to label each non-terminal, which can be either on the left-hand-side(LHS) or the right-hand-side (RHS) of the rule, withthe head POS tag of the underlying target phrase ifit has a fixed dependency structure1, and measurethe mismatches between nonterminal labels when aRHS nonterminal of a rule is substantiated with theLHS nonterminal of another rule during decoding.This also resulted in further improvements in MTperformance.
Figure 1 (c) shows an example string-to-dependency translation rule in our baseline sys-tem.1Nonterminals corresponding to floating structures keeptheir default label ?X?
as experiments show that it is not bene-ficial to label them differently.558X :  give  X2X1X  :  X1  X2(b) pure hierarchical ruleVV  :  give  PRP2NN1X  :  X1  X2(c) string-to-dependency rule??
??
?
??
  ?give   himthat  brown  coat(a) word alignmentsFigure 1: An example of extracting a string-to-dependency translation rule from word alignments.
Thenonterminals on the target side of the hierarchical rule(b) all correspond to fixed dependency structures and sothey are labeled by the respective head tag in the string-to-dependency rule (c).4 Factored Syntactic ConstraintsAlthough the string-to-dependency formulationhelps to improve the grammaticality of translations,it lacks the ability to incorporate source syntax intothe translation process.
We next describe a factoredapproach to address this problem by utilizing sourcesyntax through two models: one that introduces syn-tactic awareness to translation rules themselves, andanother that focuses on reordering based on the syn-tactic analysis of the source.4.1 Syntax Mismatch ModelA straightforward method to introduce awarenessof source syntax to translation rules is to applythe same well-formed dependency constraint andhead POS annotation on the target side of string-to-dependency translation rules to the source side.However, as discussed earlier, this would signifi-cantly reduce the number of rules that can be ex-tracted, exacerbate data sparsity, and cause otherproblems, especially given that the target side is al-ready constrained by the dependency requirement.A relaxed method is to bypass the dependencyconstraint and only annotate source nonterminalswhose underlying phrase is a fixed dependencystructure with the head POS tag of the phrase.
Thismethod would still extract all of the rules that canbe extracted from the baseline string-to-dependencyVV  :  give  PRP2NN1X  :  X1  X224VV : 0.7NN : 0.1X : 0.23524NN : 0.8VV : 0.1X : 0.13524PN : 0.5NN : 0.4X : 0.135VV(a) nonterminal tag distributionssource:gross:NNPNspan tag:(b) source span tags?his?
?pen?megivedependency:Figure 2: Example distribution of tags for nonterminalson the source side (a) and example tags for source spans(b)translation model, but the extra annotation on non-terminals can split a rule into multiple rules, with theonly difference being the nonterminal labels on thesource side.
Unfortunately, our experiments haveshown that even this moderate annotation resultsin significantly lower translation quality due to thefragmentation of translation rules, and the increasedderivational ambiguity.
We have also tried to includesome source tag mismatch features (with details de-scribed later) to measure the syntactic compatibilitybetween the nonterminal labels of a translation ruleand the corresponding tags of source spans.
This im-proves translation accuracy, but not enough to com-pensate for the performance drop caused by annotat-ing source nonterminals.Our proposed method introduces syntax to trans-lation rules without sacrificing performance.
Insteadof imposing dependency constraints or explicitly an-notating source nonterminals, we keep the originalstring-to-dependency translation rules intact and as-sociate each nonterminal on the source side with adistribution of tags.
The tags are determined basedon the dependency structure of training samples.
Ifthe underlying source phrase of a nonterminal is afixed dependency structure in a training sample, weuse the head POS tag of the phrase as the tag.
Oth-erwise, we use the default tag ?X?
to denote float-559Feature Condition Valuef1 ts = X P(tr = X)f2 ts = X P(tr ?
X)f3 ts ?
X P(tr = X)f4 ts ?
X P(tr = ts)f5 ts ?
X P(tr ?
X, tr ?
ts)Table 1: Source tag mismatch features.
The default valueof each feature is zero if the source span tag ts does notmatch the conditioning structures and dependency structures that are notwell formed.
As a result, we still extract the sameset of rules as in the baseline string-to-dependencytranslation model, and also obtain a distribution oftags for each nonterminal.
Figure 2 (a) illustrates theexample tag distributions of a string-to-dependencytranslation rule.
The tag distributions provide an ap-proximation of the source syntax of the training datafrom which the translation rules are extracted.
Theyare used to measure the syntactic compatibility be-tween a translation rule and the source spans it isapplied to.
At decoding time, we parse the sourcesentence and assign each span a tag in the same wayas it is done during rule extraction, as shown in theexample in Figure 2 (b).
When a translation rule isused to expand a derivation, for each nonterminal(which can be on the LHS or RHS) on the sourceside of the rule, five source tag mismatch featuresare computed based on the distribution of tags P(tr)on the rule nonterminal, and the tag ts on the cor-responding source span.
The features are defined inTable 1.
We use soft features instead of hard syn-tactic constraints, and allow the tuning process tochoose the appropriate weight for each feature.
Asshown in Section 5, these source syntax mismatchfeatures help to improve the baseline system.4.2 Syntax-based Reordering ModelMost previous research on reordering models has fo-cused on improving word reordering for statisticalphrase-based translation systems (e.g., (Collins etal., 2005; Al-Onaizan and Papineni, 2006; Trombleand Eisner, 2009)).
There has been less work on im-proving the reordering of hierarchical phrase-basedtranslation systems (see (Xu et al 2009; Gao et al2011; Xiong et al 2012) for a few exceptions), ex-cept through explicit syntactic annotation of transla-tion rules.
It is generally assumed that hierarchicalmodels are inherently capable of handling both lo-cal and non-local reorderings.
However, many hier-archical translation rules are noisy and have limitedcontext, and so may not be able to produce transla-tions in the right order.We propose a general framework that incorpo-rates external reordering information into the decod-ing process of hierarchical translation models.
Tosimplify the presentation, we make the assumptionthat every source word translates to one or moretarget words, and that the translations for a pairof source words is either straight or inverted.
Wediscuss the general case later.
Given a sentencew1,?,wn, suppose we have a separate reorderingmodel that predicts Porder(oij), the probability distri-bution of ordering oij ?
{straight, inverted} betweenthe translations of any source word pair (wi,wj).We can measure the goodness of a given hypothe-sis h with respect to the ordering predicted by thereordering model as the sum of log probabilities2for ordering each pair of source words, as definedin Equation 1:forder(h) = ?1?i<j?nlog Porder(oij = ohij) (1)where ohij is the ordering between the translations ofsource word pair (wi,wj) in hypothesis h. The re-ordering score forder(h) can be computed efficientlythrough recursion during hierarchical decoding asfollows:?
Base case: for phrasal (i.e.
non-hierarchical)rules, the ordering of translations for any wordpair covered by the source phrase can be deter-mined based on the word alignment of the rule.The value of the reordering score can be simplycomputed according to Equation 1.?
Recursive case: when a hierarchical rule is usedto expand a partial derivation, two types ofword pairs are encountered: a) word pairs thatare covered exclusively by one of the nonter-minals on the RHS of the rule, and b) other2In practice, the log probability is thresholded to avoid neg-ative infinity, which would otherwise result in a hard constraint.560(a)VV  :  give  PRP2NN1X  :  X1  X2source:gross: his penmegive(b)word pair translation order(?, ) inverted(?, ?)
inverted(?, ) inverted(?, ?)
inverted(?, ) inverted(?, ?)
inverted(, ?)
straight(?, ?)
previously considered(?, ?)
previously considered(?, ?)
previously consideredFigure 3: An example rule application (a) with the trans-lation order of new source word pairs covered by the ruleshown in (b).
The translation order of word pairs coveredby X1 is previously considered and is thus not shown.word pairs.
The reordering scores of the for-mer would be already computed in previousrule applications, and can simply be retrievedfrom the partial derivation.
Word pairs of thelatter case are new word pairs introduced by thehierarchical rule, and their ordering can be de-termined based on the alignment of the hierar-chical rule.
The value of the reordering scoreof the new derivation is the sum of the reorder-ing scores retrieved from the partial derivationsfor the nonterminals and the reordering scoresof the new word pairs.Figure 3 shows an example of determining theordering of translations when applying a string-to-dependency rule.
The alignment in the translationrule is able to fully determine the translation orderfor all new word pairs introduced by the rule.
Forexample, ??/pen?
is covered by X1 in the rule andthe translation order for X1 and ??/give?
is invertedon the target side.
Since ??/pen?
is translated to-gether with other words covered by X1 as a group,we can determine that the translation order betweenthe source word pair ??/pen?
and ??/give?
is alsoinverted on the target side.
The words ??/his?,??
?, ??
/pen?
are all covered by the same nonter-Reordering featuresThe syntactic production ruleThe syntactic labels of the nodes in the contextThe head POS tags of the nodes in the contextThe dep.
labels of the nodes in the contextThe seq.
of dep.
labels connecting the two nodesThe length of the nodes in the contextTable 2: Features in the reordering modelminal X1 and thus their pairwise reordering scoreshave already been considered in previous rule appli-cations.In practice, not all source words in a translationrule are translated to a target word; sometimes thereis no clear ordering between the translations of twosource words.
In such cases we use a binary discountfeature instead of the reordering feature.This reordering framework relies on an externalmodel to provide the ordering probability distribu-tion of source word pairs.
In this paper, we inves-tigate a simple maximum-entropy reordering modelbased on the syntactic parse tree of the source sen-tence.
This allows us to take advantage of the sourcesyntax to improve reordering without using syntacticannotations in translation rules.
The syntax-basedreordering model attempts to predict the reorderingprobability of a pair of sibling constituents in thesource parse tree, building on the fact that syntac-tic phrases tend to move in a group during transla-tion (Fox, 2002).
The reordering model is trained ona word-aligned corpus.
For each pair of sibling con-stituents in the source parse tree, we determine thetranslation order on the target side based on wordalignments.
If there is a clear ordering3, i.e., eitherstraight or inverted, on the target side, we includethe context of the constituent pair and its translationorder as a sample for training or evaluating the max-imum entropy reordering model.
Table 2 lists thefeatures of the reordering model.The ordering distributions of source word pairsare determined based on the ordering distributionsof sibling constituent pairs.
For each pair of sib-3If the translations overlap with other, the non-overlappingparts are used to determine the translation order.561ling constituents4 in the parse tree of a source sen-tence, we compute its distribution of translation or-der using the reordering model.
The distribution isshared among all word pairs covered by the respec-tive constituents, which guarantees that the order-ing distribution of any source word pair is computedexactly once.
The ordering distributions of sourceword pairs are then used through the general reorder-ing framework in the decoder to guide the decodingprocess.5 Experiments5.1 Experimental SetupOur main experiments use the Chinese-English par-allel training data and development sets released bythe LDC, and made available to the DARPA GALEand BOLT programs.
We train the translation modelon 100 million words of parallel data.
We use a 8 bil-lion words of English monolingual data to train twolanguage models: a trigram language model used inchart decoding, and a 5-gram language model usedin n-best rescoring.
The systems are tuned and eval-uated on a mixture of newswire and web forum textfrom the development sets available for the DARPAGALE and BOLT programs, with up to 4 indepen-dent references for each source sentence.
We alsoevaluate our final systems on both newswire andweb text from the NIST MT06 and MT08 evalua-tions using an experimental setup compatible withthe NIST MT12 Chinese-English constrained track.In this setup, the translation and language modelsare trained on 35 million words of parallel data and3.8 billion words of English monolingual data, re-spectively.
The systems are tuned on the MT02-05 development sets.
All systems are tuned andevaluated on IBM BLEU (Papineni et al 2002).The baseline string-to-dependency translation sys-tem uses more than 10 core features and a large num-ber of sparse binary features similar to the methoddescribed in (Chiang et al 2009).
It achieves trans-lation accuracies comparable to the top ranked sys-tems in the NIST MT12 evaluation.4Note that the constituent pairs used to train the reorderingmodel are filtered to only contain these with clear ordering onthe target side, while no such pre-filtering is applied to con-stituent pairs when applying the reordering model in translation.We leave it to future work to address this mismatch problem.GIZA++ (Och and Ney, 2003) is used for auto-matic word alignment in all of the experiments.
Weuse Charniak?s parser (Charniak and Johnson, 2005)on the English side to obtain string-to-dependencytranslation rules, and use a latent variable PCFGparser (Huang and Harper, 2009) to parse the sourceside of the parallel training data as well as thetest sentences for extracting syntax mismatch andreordering features.
For both languages, depen-dency structures are read off constituency trees us-ing manual head word percolation rules.
We usea lexicon-based longest-match-first word segmenterto tokenize source Chinese sentences.
Since thesource tokenization used in our MT system is dif-ferent from the treebank tokenization used to trainthe Chinese parser, the source sentences are first to-kenized using the treebank-trained Stanford Chinesesegmenter (Tseng et al 2005), then parsed withthe Chinese parser, and finally projected to MT tok-enization based on the character alignment betweenthe tokens.
The syntax-based reordering model istrained on a set of Chinese-English manual wordalignment corpora released by the LDC5.5.2 Syntax Mismatch ModelWe first conduct experiments on the GALE/BOLTdata sets to evaluate different strategies of incor-porating source syntax into string-to-dependencytranslation rules.
As mentioned in Section 4.1, con-straining the source side of translation rules to onlywell-formed dependency structures is too restrictivegiven that our baseline system already has depen-dency constraint on the target side.
We evaluatethe relaxed method that only annotates source non-terminals with the head POS tag of the underlyingphrase if the phrase is a fixed dependency structure.As shown in Table 3, nonterminal annotation resultsin a big drop in performance, decreasing the BLEUscore of the baseline from 27.82 to 25.54.
This sug-gests that it is undesirable to further fragment thetranslation rules.
Introducing the syntax mismatchfeatures described in Section 4.1 helps to improve5The alignment corpora are LDC2012E24, LDC2012E72,LDC2012E95, and LDC2013E02.
The reordering model canalso be trained on automatically aligned data; however, our ex-periments show that using manual alignments results in a bet-ter accuracy for the reordering model itself and more improve-ments for the MT system.562BLEUbaseline 27.82+ tag annotation only 25.54+ tag annotation, mismatch feat.
25.90+ tag distribution, mismatch feat.
28.23Table 3: Effects of tag annotation, tag distribution, andsyntax mismatch features on MT performance on theGALE/BOLT data set.BLEU from 25.54 to 25.90.
This improvement isnot large enough to compensate for the performancedrop caused by annotating the nonterminals.Our proposed approach, on the other hand, doesnot modify the translation rules in the baseline sys-tem, but only associates each nonterminal with a dis-tribution of tags.
For that reason, it does not sufferfrom the aforementioned problem.
It achieves ex-actly the same performance as the baseline systemif no source syntactic constraints are imposed dur-ing decoding.
When the source syntax mismatchfeatures are used, the proposed approach is able toachieve a gain of 0.41 in BLEU over the baselinesystem.
Table 4 lists the learned weights of the syn-tax mismatch features after MT tuning.
The nega-tive weights of f1 and f2 mean that the MT systempenalizes source spans that do not have a fixed de-pendency structure, and it assigns a higher penaltyto rules whose nonterminals have a high probabilityof being extracted from source phrases that do nothave a fixed dependency structure.
When the sourcespan has a fixed dependency structure, the MT sys-tem prefers translation rules that have a high proba-bility of matching the tag on the source span (featuref4) over the ones that do not match (features f3 andf5).
This result is consistent with our expectationsof the syntax mismatch features.Feature Description Weightf1 ts = X, tr = X ?1.543f2 ts = X, tr ?
X ?0.676f3 ts ?
X, tr = X 0.380f4 ts ?
X, tr ?
X, tr = ts 1.677f5 ts ?
X, tr ?
X, tr ?
ts 0.232Table 4: Learned syntax mismatch feature weight5.3 Syntax-based Reordering ModelBefore evaluating the syntax-based reorderingmodel, we would like to establish the upper boundimprovement that could be achieved using the gen-eral reordering framework for hierarchical transla-tion models.
Towards that goal, we conduct an ora-cle experiment on the GALE/BOLT development setthat uses the oracle translation order from the ref-erence as the external reordering model.
For eachsource sentence in the development set, we pair itwith the first reference translation (out of up to 4 in-dependent translations).
We then add the sentencepairs from the development set to the parallel train-ing data and run GIZA++ to obtain word alignments.We consider the GIZA++ word alignments for thedevelopment set to be all correct, and use it to de-termine the oracle order in the reference translation.For the ordering distribution, we set the log proba-bility of the reference translation order to 0 and thereverse order to -1 to avoid negative infinity.
Asshown in Table 5, the system tuned and evaluatedwith the oracle reordering model significantly out-performs the baseline by a large margin of 2.32 inBLEU on the GALE/BOLT test set.
This suggeststhat there is room for potential improvement by us-ing a fairly trained reordering model.BLEUbaseline 27.82+ oracle reorder 30.14+ syntax reorder 28.40Table 5: Effects of external reordering features on MTperformance on the GALE/BOLT test set.We next evaluate the syntax-based reorderingmodel.
We train the model on manually alignedChinese-English corpora.
Since the tokenizationused in the manual alignment corpora is differentfrom the tokenization used in our MT system, themanual alignment is projected to the MT tokeniza-tion based on the character alignment between thetokens.
Some extraneously tagged alignment linksin the manual alignment corpora are not useful formachine translation and are thus removed beforeprojecting the alignment.
As described in Sec-tion 4.2, the syntax-based reordering method mod-563els the translation order of sibling constituent pairsin the source parse tree.
As a result of strong phrasalcohesion (Fox, 2002), we find that 94% of con-stituent pairs have a clear ordering on the targetside.
We only retain these constituent pairs for train-ing and evaluating the reordering model.
In orderto evaluate the accuracy of the maximum entropyreordering model, we divide the manual alignmentcorpora into 2/3 for training and 1/3 for evaluation.A baseline that only chooses the majority order (i.e.straight) has an accuracy of 69%, while the syntax-based reordering model improves the accuracy to79%.The final reordering model used in MT is trainedon all of the samples extracted from the manualalignment corpora.
As shown in Table 5, the syntax-based reordering feature improves the baseline by0.58 in BLEU, which is a good improvement givenour strong baseline.
Table 6 lists the number ofshifting errors in TER measurement (Snover et al2006) of various systems on the GALE/BOLT testset.
The syntax-based reordering model achieves a6.1% reduction in the number of shifting errors inthe baseline system, and its combination with thesyntax mismatch model achieves an additional re-duction of 0.6%.
This suggests that the proposedmethod helps to improve word reordering in transla-tion.Shifting errorsbaseline 3205+ syntax mismatch 3089+ syntax reorder 3010+ syntax mismatch and reorder 2990Table 6: Number of shifting errors in TER measurementof multiple systems on the GALE/BOLT test set5.4 Final ResultsTable 7 shows the final results on the GALE/BOLTtest set, as well as the NIST MT06 and MT08 testsets.
Both the syntax mismatch and the syntax-basedreordering features improve the baseline system, re-sulting in moderate to significant gains in all of thefive test sets.
The two features are complementaryto each other and their combination results in betterimprovement in four out of the five test sets com-pared to adding them separately.
In three out of thefive test sets, the improvement from the combina-tion of the two features is statistically significant atthe 95% confidence level over the baseline, with thelargest absolute improvement of 1.43 in BLEU ob-tained on MT08 web.6 ConclusionIn this paper, We have discussed problems resultingfrom explicitly decorating translation rules with syn-tactic annotations.
We presented a factored approachto incorporate soft source syntax mismatch and re-ordering constraints to hierarchical machine transla-tion, and showed how our models avoid the pitfallsof the explicit decoration approach.
Experiments onChinese-English translation show that the proposedapproach significantly improves a strong string-to-dependency translation baseline on multiple evalu-ation sets.
There are many directions in which thiswork can be continued.
The syntax mismatch modelcan be extended to dynamically adjust the transla-tion distribution based on the syntactic compatibil-ity between a translation rule and a source sentence.It also might be beneficial to look beyond syntacticconstituent pairs when modeling reordering, giventhat phrasal cohesion does not always hold in trans-lation.
The general framework that uses an externalreordering model in hierarchical models via featurescan also be naturally extended to use multiple re-ordering models.AcknowledgmentsThis work was supported in part by DARPA/IPTOContract No.
HR0011-12-C-0014 under the BOLTProgram.
The views expressed are those of the au-thors and do not reflect the official policy or positionof the Department of Defense or the U.S. Govern-ment.
The authors would like to thank the anony-mous reviewers for their helpful comments.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Distor-tion models for statistical machine translation.
In Pro-ceedings of the Annual Meeting of the Association forComputational Linguistics.564MT06 news MT06 web MT08 news MT08 web GALE/BOLTbaseline 43.76 36.13 40.52 27.78 27.82+ syntax mismatch 43.89 36.72 40.82+ 28.54- 28.23-+ syntax reorder 44.01 36.40 41.23/ 28.95/ 28.40/+ syntax mismatch and reorder 44.28+ 36.43+ 41.14- 29.21/ 28.62/improvement over baseline +0.52 +0.30 +0.62 +1.43 +0.8Table 7: Results on Chinese-English MT.
The symbols +, -, and / indicate that the system is better than the baselineat the 85%, 95%, and 99% confidence levels, respectively, as defined in (Koehn, 2004).Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the Annual Meeting of the As-sociation for Computational Linguistics.Colin Cherry.
2013.
Improved reordering for phrase-based translation using sparse features.
In Proceed-ings of the Conference of the North American Chapterof the Association for Computational Linguistics.David Chiang, Mona Diab, Nizar Habash, Owen Ram-bow, and Safiullah Shareef.
2006.
Parsing Arabic di-alects.
In Conference of the European Chapter of theAssociation for Computational Linguistics.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In Proceedings of the Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics.Heidi Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2004.
What?s in a translation rule.
In Pro-ceedings of the Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology.Yang Gao, Philipp Koehn, and Alexandra Birch.
2011.Soft dependency constraints for reordering in hierar-chical phrase-based translation.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine transla-tion.
In Proceedings of the International Conferenceon Computational Linguistics.Greg Hanneman and Alon Lavie.
2013.
Improvingsyntax-augmented machine translation by coarseningthe label set.
In Proceedings of the Conference of theNorth American Chapter of the Association for Com-putational Linguistics.Zhongqiang Huang and Mary Harper.
2009.
Self-training PCFG grammars with latent annotationsacross languages.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.A syntax-directed translator with extended domain oflocality.
In Proceedings of the Workshop on Computa-tionally Hard Problems and Joint Inference in Speechand Language Processing.Zhongqiang Huang, Martin C?mejrek, and Bowen Zhou.2010.
Soft syntactic constraints for hierarchicalphrase-based translation using latent syntactic distri-butions.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing.Mitesh M. Khapra, Ananthakrishnan Ramanathan, andKarthik Visweswariah.
2013.
Improving reorderingperformance using higher order and structural features.In Proceedings of the Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the Conference of the North American Chap-ter of the Association for Computational Linguisticson Human Language Technology.Philipp Koehn, Amittai Axelrod, Alexandra Birch, ChrisCallison-Burch, Miles Osborne, and David Talbot.2005.
Edinburgh system description for the 2005 iwslt565speech translation evaluation.
In International Work-shop on Spoken Language Translation.Philipp Koehn.
2004.
Pharaoh: A bean search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of the Conference of Associationfor Machine Translation in the Americas.Junhui Li, Philip Resnik, and Hal Daume.
2013.
Mod-eling syntactic and semantic structures in hierarchi-cal phrase-based translation.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In Proceedings of the Annual Meeting of the Associa-tion for Computational Linguistics.Haitao Mi and Liang Huang.
2008.
Forest-based transla-tion rule extraction.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computional Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proceedings of the An-nual Meeting on Association for Computational Lin-guistics.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics.Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective use of linguis-tic and contextual information for statistical machinetranslation.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Translationin the Americas.Roy Tromble and Jason Eisner.
2009.
Learning linear or-dering problems for better translation.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter.
In Proceedingsof the SIGHAN Workshop on Chinese Language Pro-cessing.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference grammars: soft-ening syntactic constraints to improve statistical ma-chine translation.
In Proceedings of the Conferenceof the North American Chapter of the Association forComputational Linguistics.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In Proceedings of the International Confer-ence on Computational Linguistics.Deyi Xiong, Min Zhang, and Haizhou Li.
2012.
Model-ing the translation of predicate-argument structure forsmt.
In Proceedings of the Annual Meeting of the As-sociation for Computational Linguistics.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improve smtfor subject-object-verb languages.
In Proceeding ofthe Annual Conference of the North American Chap-ter of the Association for Computational Linguistics.Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.2012.
A ranking-based approach to word reorderingfor statistical machine translation.
In Proceedings ofthe Annual Meeting of the Association for Computa-tional Linguistics.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the Workshop on Statistical MachineTranslation.566
