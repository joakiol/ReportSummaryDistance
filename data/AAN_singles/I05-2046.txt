Using Maximum Entropy to Extract Biomedical Named Entitieswithout DictionariesTzong-Han Tsai, Chia-Wei Wu, and Wen-Lian HsuInstitute of Information Science, Academia SinicaNankang, Taipei, Taiwan 115{thtsai, cwwu, hsu}@iis.sinica.edu.twAbstractCurrent NER approaches include:dictionary-based, rule-based, or ma-chine learning.
Since there is noconsolidated nomenclature for mostbiomedical NEs, most NER systemsrelying on limited dictionaries or rulesdo not perform satisfactorily.
In thispaper, we apply Maximum Entropy(ME) to construct our NER framework.We represent shallow linguistic infor-mation as linguistic features in our MEmodel.
On the GENIA 3.02 corpus, oursystem achieves satisfactory F-scoresof 74.3% in protein and 70.0% overallwithout using any dictionary.
Oursystem performs significantly betterthan dictionary-based systems.
Usingpartial match criteria, our systemachieves an F-score of 81.3%.
Usingappropriate domain knowledge tomodify the boundaries, our system hasthe potential to achieve an F-score ofover 80%.1 IntroductionBiomedical literature available on the web has ex-perienced unprecedented growth in recent years.Therefore, demand for efficiently processingthese documents is increasing rapidly.
There hasbeen a surge of interest in mining biomedicalliterature.
Some possible applications for suchefforts include the reconstruction and predictionof pathways, establishing connections betweengenes and disease, finding the relationships be-tween genes, and much more.Critical tasks for biomedical literature min-ing include named entity recognition (NER), to-kenization, relation extraction, indexing and cate-gorization/clustering (Cohen and Hunter, 2005).Among these technologies, NER is most fun-damental.
It is defined as recognizing objectsof a particular class in plain text.
Dependingon required application, NER can extract objectsranging from protein/gene names to disease/virusnames.In general, biomedical NEs do not follow anynomenclature (Shatkay and Feldman, 2003) andcan comprise long compound words and short ab-breviations (Pakhomov, 2002).
Some NEs con-tain various symbols and other spelling variations.On average, any NE of interest has five synonyms.Biomedical NER is a challenging problem.
Thereare many different aspects to deal with.
For ex-ample, one can have unknown acronyms, abbre-viations, or words containing hyphens, digits, let-ters, and Greek letters; Adjectives preceding anNE may or may not be part of that NE depend-ing on the context and applications; NEs with thesame orthographical features may fall into differ-ent categories; An NE may also belong to mul-tiple categories intrinsically; An NE of one cate-gory may contain an NE of another category in-side it.To tackle these challenges, researchers usethree main approaches: dictionary-based, rule-based, and machine learning.
In biomedical do-main, there are more and more well-curated re-sources, including lexical resources such as Lo-268cusLink (Maglott, 2002) and ontologies such asMeSH (NLM, 2003).
One might think thatdictionary-based systems relying solely on theseresources could achieve satisfactory performance.However, according to (Pakhomov, 2002), theytypically perform quite poorly, with average re-call rates in the range of only 10-30%.
Rule-basedapproaches, on the other hand, are more accurate,but less portable across domains.
Therefore, wechose the machine learning approach.Various machine learning approaches such asME (Kazama et al, 2002), SVM (Kazama et al,2002; Song et al, 2004), HMM (Zhao, 2004) areapplied to NER.
In this paper, we chose ME asour framework since it is much easier to representvarious features in such a framework.
In addi-tion, ME models are flexible enough to capturemany correlated features, including overlappingand non-independent features.
We can thus usemultiple features with more ease than on an HMMsystem.
ME-based tagger, in particular, excel atsolving sequence tagging problems such as POStagging (Ratnaparkhi, 1997), general EnglishNER (Borthwick, 1999), and Chunking (Koeling,2000).In this paper, we describe how to construct aME-based framework that can exploit shallow lin-guistic information in the recognition of biomed-ical named entities.
Hopefully, our experiencein integrating these features may prove useful forthose interested in constructing machine learningbased NER system.2 Maximum Entropy Based Tagger2.1 FormulationIn the Biomedical NER problem, we regard eachword in a sentence as a token.
Each token is asso-ciated with a tag that indicates the category of theNE and the location of the token within the NE,for example, B c, I c where c is a category, andthe two tags denote respectively the beginning to-ken and the following token of an NE in categoryc.
In addition, we use the tag O to indicate that atoken is not part of an NE.
The NER problem canthen be phrased as the problem of assigning oneof 2n + 1 tags to each token, where n is the num-ber of NE categories.
For example, one way totag the phrase ?IL-2 gene expression, CD28, andNF-kappa B?
in a paper is [B-DNA, I-DNA, O, O,B-protein, O, O, B-protein, I-protein].2.2 Maximum Entropy ModelingME is a flexible statistical model which assignsan outcome for each token based on its historyand features.
ME computes the probability p(o|h)for any o from the space of all possible outcomesO, and for every h from the space of all possi-ble histories H .
A history is all the condition-ing data that enables one to assign probabilitiesto the space of outcomes.
In NER, history canbe viewed as all information derivable from thetraining corpus relative to the current token.
Thecomputation of p(o|h) in ME depends on a set ofbinary-valued features, which are helpful in mak-ing predictions about the outcome.
For instance,one of our features is: when all alphabets of thecurrent token are capitalized, it is likely to be partof a biomedical NE.
Formally, we can representthis feature as follows:f(h, o) =????
?1 : if W0-AllCaps(h)=trueand o=B-protein0 : otherwise(1)Here, W0-AllCaps(h) is a binary function thatreturns the value true if all alphabets of the cur-rent token in the history h are capitalized.
Given aset of features and a training corpus, the ME esti-mation process produces a model in which everyfeature fi has a weight ?i.
From (Berger et al,1996), we can compute the conditional probabil-ity as:p(o|h) =1Z(h)?i?fi(h,o)i (2)Z(h) =?o?i?fi(h,o)i (3)The probability is given by multiplying theweights of active features (i.e., those fi(h, o) =1).
The weight ?i is estimated by a procedurecalled Generalized Iterative Scaling (GIS) (Dar-roch and Ratcliff, 1972).
This method improvesthe estimation of weights iteratively.
The ME esti-mation technique guarantees that, for every fea-ture fi, the expected value of ?equals the empiricalexpectation of ?in the training corpus.269As noted in (Borthwick, 1999), ME allowsusers to focus on finding features that character-izes the problem while leaving feature weight as-signment to the ME estimation routine.
Whennew features, e.g., syntax features, are added toME, users do not need to reformulate the model asin the HMM model.
The ME estimation routinecan automatically calculate new weight assign-ments.
More complete discussions of ME includ-ing a description of the MEs estimation proce-dure and references to some of the many success-ful computational linguistics systems using MEcan be found in the following introduction (Rat-naparkhi, 1997).2.3 DecodingAfter having trained an ME model and assignedthe proper weights ?to each feature fi, decoding(i.e., marking up) a new piece of text becomessimple.
First, the ME module tokenizes the text.Then, for each token, we check which features areactive and combine ?i of the active features ac-cording to Equation 2.
Finally, the probability ofa tag sequence y1...yn given a sentence w1...wnis approximated as follows:p(o1...on|w1...wn) ?n?j=1p(oj |hj) (4)where hj is the context for word wj .
The tag-ger uses beam search to find the most probablesequence given the sentence.
Sequences contain-ing invalid subsequences are filtered out.
For in-stance, the sequence [B-protein, I-DNA] is in-valid because it does not contain an ending tokenand these two tokens are not in the same cate-gory.
Further details on the beam search can befound in http://www-jcsu.jesus.cam.ac.uk/?tdk22/project/beam.html.3 Linguistic Features3.1 Orthographical FeaturesTable 1 lists some orthographical features usedin our system.
In our experience, ALLCAPS,CAPSMIX, and INITCAP are more useful thanothers.Table 1: Orthographical featuresFeature name Regular ExpressionINITCAP [A-Z].
*CAPITALIZED [A-Z][a-z]+ALLCAPS [A-Z]+CAPSMIX .*[A-Z][a-z].
* |.*[a-z][A-Z].
*ALPHANUMERIC .*[A-Za-z].*[0-9].
* |.*[0-9].*[A-Za-z].
*SINGLECHAR [A-Za-z]SINGLEDIGIT [0-9]DOUBLEDIGIT [0-9][0-9]INTEGER -?
[0-9]+REAL -?
[0-9][.,]+[0-9]+ROMAN [IVX]+HASDASH .*-.
*INITDASH -.
*ENDDASH .
*-PUNCTUATION [,.;:?
!-+]QUOTE [???
]3.2 Context FeaturesWords preceding or following the target wordmay be useful for determining its category.
Takethe sentence ?The IL-2 gene localizes to bandsBC on mouse Chromosome 3?
for example.
If thetarget word is ?IL-2,?
the following word ?gene?will help ME to distinguish ?IL-2 gene?
from theprotein of the same name.
Obviously, the morecontext words analyzed the better and more pre-cise the results.
However, widening the contextwindow quickly leads to an explosion of the num-ber of possibilities to calculate.
In our experience,a suitable window size is five.3.3 Part-of-speech FeaturesPart of speech information is quite useful for iden-tifying NEs.
Verbs and prepositions usually indi-cate an NEs boundaries, whereas nouns not foundin the dictionary are usually good candidates fornamed entities.
Our experience indicates that fiveis also a suitable window size.
The MBT POStagger (Daelemans et al, 1996) is used to providePOS information.
We trained it on GENIA 3.02pand achieves 97.85% accuracy.3.4 Word Shape FeaturesNEs in the same category may look similar (e.g.,IL-2 and IL-4).
So we have come up with sim-ple way to normalize all similar words.
Accord-ing to our method, capitalized characters are allreplaced by ?A?, digits are all replaced by ?0?,270Table 2: Basic statistics for the data setData # abs # sen # wordsGENIA 3.02 2,000 18,546 472,006 (236.00/abs)non-English characters are replaced by ?
?
(un-derscore), and non-capitalized characters are re-placed by ?a?.
For example, Kappa-B will be nor-malized as ?Aaaaa A?.
To further normalize thesewords, we shorten consecutive strings of iden-tical characters to one character.
For example,?Aaaaa A?
is normalized to ?Aa A?.3.5 Prefix and Suffix FeaturesSome prefixes and suffixes can provide goodclues for classifying named entities.
For example,words which end in ?ase?
are usually proteins.
Inour experience, the acceptable length for prefixesand suffixes is 3-5 characters.4 Experiment4.1 DatasetsIn our experiment, we use the GENIA version3.02 corpus (Kim et al, 2003).
Its basic statis-tics is summarized in Table 2.
Frequencies for allNE classes in it are showed in Table 3.4.2 ResultsIn Table 4, one can see that F-scores for proteinand cell-type are comparably high.
We believethis is because protein and cell type are amongthe top three most frequent categories in the train-ing set (as shown in Table 3).
One notices, how-ever, that although DNA is the second most fre-quent category, it does not have a high F-score.We think this discrepancy is due to the fact thatDNA names are commonly used in proteins, caus-ing a substantial overlap between these two cate-gories.
RNAs performance is comparably low be-cause its training set is much smaller than thoseof other categories.
Cell lines performance is thelowest since it overlaps heavily with cell type andits training set is also very small.In Table 5, one can see that, using the par-tial matching criterion, the precision rates, recallrates, and F-scores of protein names are all over85%.
The overall F-Score is 81.3%.
The tablealso shows that 83.9% of our systems suggestionsTable 4: NER performance of each NE categoryon the GENIA 3.02 data (10-fold CV)NE category Precision Recall F-scoreprotein 74.1 74.5 74.3DNA 65.9 54.4 59.6RNA 75.3 48.0 58.6cell line 65.4 51.4 57.6cell type 72.3 69.1 70.7Overall 72.0 67.9 70.0Table 5: Partial matching performance on the GE-NIA 3.02 corpus (10-fold CV)NE category Precision Recall F-scoreprotein 85.3 85.5 85.4DNA 80.3 66.3 72.7RNA 84.0 53.0 65.0cell line 80.9 63.3 71.1cell type 83.1 79.4 81.2Overall 83.9 78.9 81.3correctly identify at least one part of an NE, andthat our system tags at least one part of 78.9%of all NEs in the test corpus.
The precision rate inall categories is over 80%, showing that , by usingappropriate post-processing methods, our systemcan achieve high precision in all NE categories.In Table 6, we compare our system with twodictionary-based systems.
One exploits hand-crafted rules based on heuristics and protein namedictionaries (Seki and Mostafa, 2003).
We de-note this system as ?rule + dictionary?.
The othersystem (Tsuruoka and Tsujii, 2004) has two con-figurations: the first one exploits patterns to de-tect protein names and their fragments, whichis denoted as ?dictionary expansion?
; the sec-ond one further applies naive Bayes filters to ex-clude erroneous detections, which is denoted as?dictionary expansion + filters?.
One can seethat our system performs better than these dic-tionary/heuristic systems by a wide margin.
Thebasic ?rule + dictionary?
system achieves only54.4% recall.
By expanding the original dic-tionary (?dictionary expansion?
), they improvethe recall rate to 68.1%.
After applying postprocessing filters (?dictionary expansion + fil-ters?
), the recall rate dropped slightly, but preci-sion increased by 25.7%.
Still, our system per-forms better than the best dictionary-based systemby 7.6%.271Table 3: Frequencies for NEs in each data setData protein DNA RNA cell type cell line AllGENIA 3.02 30,269 9,533 951 6,718 3,830 51,301Table 6: Performance comparison between sys-tems with and w/o dictionaries in extracting pro-tein names on the GENIA 3.02 dataSystem Precision Recall F-scoreour system 74.1 74.5 74.3rule + dictionary 42.6 54.4 47.8dictionary expansion 46.0 68.1 54.8dictionary expansion + filters 71.7 62.3 66.65 Analysis and discussionRecognition disagreement between our systemand GENIA is caused by the following two fac-tors: Annotation problems:1.
Preceding adjective problemSome descriptive adjectives are annotated asparts of the following NE, but some are not.2.
Nested NEsIn GENIA, we found that in some instancesonly embedded NEs are annotated while inother instances, only the outside NE is an-notated.
However, according to the GENIAtagging guidelines, the outside NE should betagged.
For example, in 59 instances of thephrase ?IL-2 gene?, ?IL-2?
is tagged as aprotein 13 times, while in the other 46 it istagged as a DNA.
This irregularity can con-fuse machine learning based systems.3.
Cell-line/cell-type confusionNEs in the cell line class are from certain celltypes.
It is difficult even for an expert to dis-tinguish them.System recognition errors:1.
MisclassificationSome protein molecules or regions are mis-classified as DNA molecules or regions.These errors may be solved by exploitingmore context information.2.
Coordinated phrasesIn GENIA, most conjunction phrases aretagged as single NEs.
However, conjunc-tion phrases are usually composed of severalNEs, punctuation, and conjunctions such as?and?, ?or?
and ?but not?.
Therefore, oursystem sometimes only tags one of these NEcomponents.
For example, in the phrase ?c-Fos and c-Jun family members?, only ?c-Jun family members?
is tagged as a proteinby our system, while in GENIA, the wholephrase is tagged as a protein.3.
False positivesSome entities appeared without accompany-ing a specific name, for example, only men-tion about ?the epitopes?
rather than whichkind of epitopes.
The GENIA corpus tendsto ignore these entities, but their contexts aresimilar to the entities with specific names,therefore, our system sometimes incorrectlyrecognizes them as an NE.6 ConclusionOur system successfully integrates linguistic fea-tures into the ME framework.
Without usingany biomedical dictionaries, our system achievesa satisfactory F-score of 74.3% in protein and70.0% overall.
Our system performs significantlybetter than dictionary-based systems.
Using par-tial match criteria, our system achieves an F-scoreof 81.3%.
That means, with appropriate bound-ary modification algorithms (with domain knowl-edge), our system has the potential to achieve anF-score of over 80%.It is still difficult to recognize long, compli-cated NEs and to distinguish between two over-lapping NE classes, such as cell-line and cell-type.
This is because biomedical texts have com-plicated syntax and involve more expert knowl-edge than general domain news articles.
An-other serious problem is annotation inconsistency,which confuses machine learning models andmakes evaluation difficult.
Certain errors, such asthose in boundary identification, are more tolera-ble if the main purpose is to discover relationships272between NEs.In the future, we will exploit more linguisticfeatures such as composite features and externalfeatures.
Finally, to reduce human annotation ef-fort and to alleviate the scarcity of available anno-tated corpora, we will develop machine learningtechniques to learn from Web corpora in differentbiomedical domains.AcknowledgementsWe are grateful for the support of National Sci-ence Council under GRANT NSC94-2752-E-001-001.ReferencesA.
Berger, S. A. Della Pietra, and V. J. Della Pietra.1996.
A maximum entropy approach to natural lan-guage processing.
Computer Linguistics, 22:39?71.A.
Borthwick.
1999.
A Maximum Entropy Approachto Named Entity Recognition.
Phd thesis, New YorkUniversity.K.
Bretonnel Cohen and Lawrence Hunter.
2005.
Nat-ural language processing and systems biology.
InW.
Dubitzky and F. Azuaje, editors, Artificial In-telligence and Systems Biology, Springer Series onComputational Biology.
Springer.Walter Daelemans, Jakub Zavrel, Peter Berck, andSteven Gillis.
1996.
Mbt: A memory-based part ofspeech tagger-generator.
In E. Ejerhed and I. Da-gan, editors, Fourth Workshop on Very Large Cor-pora, pages 14?27.J.
N. Darroch and D. Ratcliff.
1972.
Generalized iter-ative scaling for log-linear models.
Annals of Math-ematicl Statistics, 43:1470?1480.J.
Kazama, T. Makino, Y. Ohta, and J. Tsujii.
2002.Tuning support vector machines for biomedicalnamed entity recognition.
In ACL-02 Workshop onNatural Language Processing in Biomedical Appli-cations.Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, andJun?ichi Tsujii.
2003.
Genia corpus - a semanti-cally annotated corpus for bio-textmining.
Bioin-formatics, 19(suppl.
1).Rob Koeling.
2000.
Chunking with maximum en-tropy models.
In CoNLL-2000.D.
Maglott.
2002.
Locuslink: a directory of genes.
InNCBI Handbook, pages 19?1 to 19?16.NLM.
2003.
Mesh: Medical subject headings.S.
Pakhomov.
2002.
Semi-supervised maximum en-tropy based approach to acronym and abbreviationnormalization in medical text.
In the 40th AnnualMeeting of the Association for Computational Lin-guistics (ACL).A.
Ratnaparkhi.
1997.
A simple introduction to maxi-mum entropy models for natural language process-ing.
Technical Report Techical Report 97-08, Insti-tute for Research in Cognitive Science Universityof Pennsylvania.Kazuhiro Seki and Javed Mostafa.
2003.
An approachto protein name extraction using heuristics and adictionary.
In ASIST 2003.Hagit Shatkay and Ronen Feldman.
2003.
Min-ing the biomedical literature in the genomic era:an overview.
Journal of Computational Biology,10(6):821?855.Yu Song, Eunju Kim, Gary Geunbae Lee, andByoung-kee Yi.
2004.
Posbiotm-ner in the sharedtask of bionlp/nlpba 2004.
In the Joint Workshop onNatural Language Processing in Biomedicine andits Applications (JNLPBA-2004), Geneva, Switzer-land.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2004.
Im-proving the performance of dictionary-based ap-proaches in protein name recognition.
Journal ofBiomedical Informatics, 37(6):461?470.Shaojun Zhao.
2004.
Named entity recognition inbiomedical texts using an hmm model.
In COL-ING 2004 International Joint Workshop on NaturalLanguage Processing in Biomedicine and its Appli-cations (NLPBA).273
