Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 36?44,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCorpus-Driven Terminology Development: Populating SwedishSNOMED CT with Synonyms Extracted from Electronic Health RecordsAron Henriksson1, Maria Skeppstedt1, Maria Kvist1,2, Martin Duneld1, Mike Conway31Department of Computer and Systems Sciences (DSV), Stockholm University, Sweden2Department of Learning, Informatics, Management and Ethics (LIME), Karolinska Institute, Sweden3Division of Biomedical Informatics, University of California San Diego, USAAbstractThe various ways in which one can re-fer to the same clinical concept needs tobe accounted for in a semantic resourcesuch as SNOMED CT.
Developing termi-nological resources manually is, however,prohibitively expensive and likely to re-sult in low coverage, especially given thehigh variability of language use in clinicaltext.
To support this process, distributionalmethods can be employed in conjunctionwith a large corpus of electronic healthrecords to extract synonym candidates forclinical terms.
In this paper, we exem-plify the potential of our proposed methodusing the Swedish version of SNOMEDCT, which currently lacks synonyms.
Amedical expert inspects two thousand termpairs generated by two semantic spaces ?one of which models multiword terms inaddition to single words ?
for one hundredpreferred terms of the semantic types dis-order and finding.1 IntroductionIn recent years, the adoption of standardized ter-minologies for the representation of clinical con-cepts ?
and their textual instantiations ?
has en-abled meaning-based retrieval of information fromelectronic health records (EHRs).
By identify-ing and linking key facts in health records, theever-growing stores of clinical documentation nowavailable to us can more readily be processedand, ultimately, leveraged to improve the qual-ity of care.
SNOMED CT1 has emerged as thede facto international terminology for represent-ing clinical concepts in EHRs and is today usedin more than fifty countries, despite only being1http://www.ihtsdo.org/snomed-ct/available in a handful of languages2.
Translationsinto several other languages are, however, underway3.
This translation effort is essential for morewidespread integration of SNOMED CT in EHRsystems globally.Translating a comprehensive4 terminology suchas SNOMED CT to an additional language is,however, a massive and expensive undertaking.
Asubstantial part of this process involves enrich-ing the terminology with synonyms in the tar-get language.
SNOMED CT has, for instance,recently been translated into Swedish; however,the Swedish version does not as yet contain syn-onyms.
Methods and tools that can accelerate thelanguage porting process in general and the syn-onym identification task in particular are clearlyneeded, not only to lower costs but also to in-crease the coverage of SNOMED CT in clinicaltext.
Methods that can account for real-world lan-guage use in the clinical setting, then, as well as tochanges over time, are particularly valuable.This paper evaluates a semi-automatic methodfor the extraction of synonyms of SNOMED CTpreferred terms using models of distributional se-mantics to induce semantic spaces from a largecorpus of clinical text.
In contrast to most ap-proaches that exploit the notion of distributionalsimilarity for synonym extraction, this method ad-dresses the key problem of identifying synonymybetween terms of varying length: a simple solutionis proposed that effectively incorporates the notionof paraphrasing in a distributional framework.
Thesemantic spaces ?
and, by extension, the method ?are evaluated for their ability to extract synonymsof SNOMED CT terms of the semantic types dis-order and finding in Swedish.2SNOMED CT is currently available in US English, UKEnglish, Spanish, Danish and Swedish.3http://www.ihtsdo.org/snomed-ct/snomed-ct0/different-languages/4SNOMED CT contains more than 300,000 active con-cepts and over a million relations.362 BackgroundSynonymy is an aspect of semantics that con-cerns the fact that concepts can be instantiatedusing multiple linguistic expressions, or, viewedconversely, that multiple linguistic expressionscan refer to the same concept.
As synonymousexpressions do not necessarily consist of singlewords, we sometimes speak of paraphrasing ratherthan synonymy (Androutsopoulos and Malakasio-tis, 2010).
This variability of language use needsto be accounted for in order to build high-qualitynatural language processing (NLP) and text min-ing systems.
This is typically achieved by us-ing thesauri or encoding textual instantiations ofconcepts in a semantic resource, e.g.
an ontol-ogy.
Creating such resources manually is, how-ever, prohibitively expensive and likely to leadto low coverage, especially in the clinical genrewhere language use variability is exceptionallyhigh (Meystre et al 2008).2.1 Synonym ExtractionAs a result, the task of extracting synonyms ?and other semantic relations ?
has long been acentral challenge in the NLP research commu-nity, not least in the biomedical (Cohen and Hersh,2005) and clinical (Meystre et al 2008) do-mains.
A wide range of techniques has been pro-posed for relation extraction in general and syn-onym extraction in particular ?
lexico-syntacticpatterns (Hearst, 1992), distributional semantics(Dumais and Landauer, 1997) and graph-basedmodels (Blondel et al 2004) ?
from a varietyof sources, including dictionaries (Blondel et al2004), linked data such as Wikipedia (Nakayamaet al 2007), as well as both monolingual (Hindle,1990) and multilingual (van der Plas and Tiede-mann, 2006) corpora.
In recent years, ensemblemethods have been applied to obtain better perfor-mance on the synonym extraction task, combin-ing models from different families (Peirsman andGeeraerts, 2009), with different parameter settings(Henriksson et al 2012) and induced from differ-ent data sources (Wu and Zhou, 2003).In the context of biomedicine, the goal has of-ten been to extract synonyms of gene and pro-tein names from the biomedical literature (Yu andAgichtein, 2003; Cohen et al 2005; McCrae andCollier, 2008).
In the clinical domain, Conwayand Chapman (2012) used a rule-based approachto generate potential synonyms from the BioPor-tal ontology web service, verifying candidate syn-onyms against a large clinical corpus.
Zeng etal.
(2012) used three query expansion methodsfor information retrieval of clinical documents andfound that a model of distributional semantics ?LDA-based topic modeling ?
generated the bestsynonyms.
Henriksson et al(2012) combinedmodels of distributional semantics ?
random in-dexing and random permutation ?
to extract syn-onym candidates for Swedish MeSH5 terms andpossible abbreviation-definition pairs.
In the con-text of SNOMED CT, distributional methods havebeen applied to capture synonymous relations be-tween terms of varying length: 16-24% of EnglishSNOMED CT synonyms present in a large clini-cal corpus were successfully identified in a list oftwenty suggestions (Henriksson et al 2013).2.2 Distributional SemanticsModels of distributional semantics (see Cohen andWiddows (2009) for an overview of methods andtheir application in the biomedical domain) wereinitially motivated by the inability of the vectorspace model to account for synonymy, which hada negative impact on recall in information retrievalsystems (Deerwester et al 1990).
The theoreticalfoundation underpinning such models of seman-tics is the distributional hypothesis (Harris, 1954),according to which words with similar meaningstend to appear in similar contexts.
By exploitingthe availability of large corpora, the meaning ofterms can be modeled based on their distributionin different contexts.
An estimate of the semanticrelatedness between terms can then be quantified,thereby, in some sense, rendering semantics com-putable.An obvious application of distributional seman-tics is the extraction of semantic relations betweenterms, such as synonymy, hyp(o/er)nymy and co-hyponymy (Panchenko, 2013).
As synonyms areinterchangeable in some contexts ?
and thus havesimilar distributional profiles ?
synonymy is cer-tainly a semantic relation that should be captured.However, since hyp(o/er)nyms and co-hyponyms?
in fact, even antonyms ?
are also likely to havesimilar distributional profiles, such semantic rela-tions will be extracted too.Many models of distributional semantics dif-fer in how context vectors, representing term5Medical Subject Headings (http://www.nlm.nih.gov/mesh).37meaning, are constructed.
They are typically de-rived from a term-context matrix that containsthe (weighted, normalized) frequency with whichterms occur in different contexts.
Partly dueto the intractability of working with such high-dimensional data, it is projected into a lower-dimensional (semantic) space, while approxi-mately preserving the relative distances betweendata points.
Methods that rely on computa-tionally expensive dimensionality reduction tech-niques suffer from scalability issues.Random IndexingRandom indexing (RI) (Kanerva et al 2000) isa scalable and computationally efficient alterna-tive in which explicit dimensionality reduction isavoided: a lower dimensionality d is instead cho-sen a priori as a model parameter and the d-dimensional context vectors are then constructedincrementally.
Each unique term in the corpus isassigned a static index vector, consisting of ze-ros and a small number of randomly placed 1sand -1s6.
Each term is also assigned an initiallyempty context vector, which is incrementally up-dated by adding the index vectors of the surround-ing words within a sliding window, weighted bytheir distance to the target term.
The semantic re-latedness between two terms is then estimated bycalculating, for instance, the cosine similarity be-tween their context vectors.Random PermutationRandom permutation (RP) (Sahlgren et al 2008)is a modification of RI that attempts to take intoaccount term order information by simply permut-ing (i.e.
shifting) the index vectors according totheir direction and distance from the target termbefore they are added to the context vector.
RPhas been shown to outperform RI on the synonympart of the TOEFL7 test.Model ParametersThe model parameters need to be configured forthe task that the semantic space is to be usedfor.
For instance, with a document-level con-text definition, syntagmatic relations are mod-eled, i.e.
terms that belong to the same topic(<car, motor, race>), whereas, with a sliding win-dow context definition, paradigmatic relations are6By generating sparse vectors of a sufficiently high di-mensionality in this way, the context representations will benearly orthogonal.7Test Of English as a Foreign Languagemodeled (<car, automobile, vehicle>) (Sahlgren,2006).
Synonymy is an instance of a paradigmaticrelation.The dimensionality has also been shown to bepotentially very important, especially when thesize of the vocabulary and the number of contexts8are large (Henriksson and Hassel, 2013).3 Materials and MethodsThe task of semi-automatically identifying syn-onyms of SNOMED CT preferred terms is hereapproached by, first, statistically identifying mul-tiword terms in the data and treating them as com-pounds; then, performing a distributional analysisof a preprocessed clinical corpus to induce a se-mantic term space; and, finally, extracting the se-mantically most similar terms for each preferredterm of interest.The experimental setup can be broken downinto the following steps: (1) data preparation, (2)term recognition, (3) model parameter tuning and(4) evaluation.
Semantic spaces are induced withdifferent parameter configurations on two datasetvariants: one with unigram terms only and one thatalso includes multiword terms.
The model param-eters are tuned using MeSH, which contains syn-onyms for Swedish.
The best parameter settingsfor each of the two dataset variants are then em-ployed in the final evaluation, where a medical ex-pert inspects one hundred term lists extracted forSNOMED CT preferred terms belonging to the se-mantic types disorder and finding.3.1 Data PreparationThe data used to induce the semantic spaces is ex-tracted from the Stockholm EPR Corpus (Dalia-nis et al 2009), which contains Swedish healthrecords from the Karolinska University Hospitalin Stockholm9.
The subset (?33 million tokens)used in these experiments comprises all forms oftext-based records ?
i.e., clinical notes ?
froma large variety of clinical practices.
The docu-ments in the corpus are initially preprocessed bysimply lowercasing tokens and removing punctu-ation and digits.
Lemmatization is not performed,as we want to be able to capture morphological8The vocabulary size and the number of contexts areequivalent when employing a window context definition.9This research has been approved by the Regional EthicalReview Board in Stockholm (Etikpro?vningsna?mnden i Stock-holm), permission number 2012/834-31/5.38variants of terms; stop-word filtering is not per-formed, as traditional stop words ?
for instance,high-frequency function words ?
could potentiallybe constituents of multiword terms.3.2 Term RecognitionMultiword terms are extracted statistically fromthe corpus using the C-value statistic (Frantzi andAnaniadou, 1996; Frantzi et al 2000).
This tech-nique has been used successfully for term recog-nition in the biomedical domain, largely due toits ability to handle nested terms (Zhang et al2008).
Using the C-value statistic for term recog-nition first requires a list of candidate terms, forwhich the C-value can then be calculated.
Here,this is simply produced by extracting n-grams ?unigrams, bigrams and trigrams ?
from the corpuswith TEXT-NSP (Banerjee and Pedersen, 2003).The statistic is based on term frequency and termlength (number of words); if a candidate term ispart of a longer candidate term (as will be the casefor practically all unigram and bigram terms), thenumber and frequency of those longer terms arealso taken into account (Figure 1).In order to improve the quality of the extractedterms, a number of filtering rules is applied to thegenerated term list: terms that begin and/or endwith certain words, e.g.
prepositions and articles,are removed.
The term list ?
ranked according toC-value ?
is further modified by giving priority toterms of particular interest, e.g.
SNOMED CT dis-order and finding preferred terms: these are movedto the top of the list, regardless of their C-value.As a result, the statistical foundation on which thedistributional method bases its semantic represen-tation will effectively be strengthened.The term list is then used to perform exact stringmatching on the entire corpus: multiword termswith a higher C-value than their constituents areconcatenated.
We thereby treat multiword terms asseparate (term) types with distinct distributions inthe data, different from those of their constituents.3.3 Model Parameter TuningTerm spaces with different parameter configu-rations are induced from the two dataset vari-ants: one containing only unigram terms (Uni-gram Word Spaces) and one containing also mul-tiword terms (Multiword Term Spaces).
The fol-lowing model parameters are tuned:?
Distributional Model: Random indexing (RI)vs. Random permutation (RP)?
Context Window Size: 2+2, 4+4, 8+8 sur-rounding terms (left+right of the target term)?
Dimensionality: 1000, 2000, 3000As the Swedish version of SNOMED CT cur-rently does not contain synonyms, it cannot beused to perform the parameter tuning automat-ically.
This is instead done with the Swedishversion of MeSH, which is one of the very fewstandard terminologies that contains synonyms formedical terms in Swedish.
However, as the op-timal parameter configurations for capturing syn-onymy are not necessarily identical for all seman-tic types, the parameter tuning is performed byevaluating the semantic spaces for their ability toidentify synonyms of MeSH terms that belong tothe categories Disease or Syndrome and Sign orSymptom.
These particular categories are simplychosen as they, to a reasonable extent, seem tocorrespond to the SNOMED CT semantic typesstudied in this paper, namely Disorder and Find-ing.
Only synonym pairs that appear at least fiftytimes in each of the dataset variants are included(155 for Unigram Word Spaces and 123 for Mul-tiword Term Spaces), as the statistical foundationfor terms that only occur rarely in the data maynot be sufficiently solid.
In these Multiword TermSpaces, the MeSH terms ?
but not the synonyms?
are given precedence in the term list.
A termis provided as input to a semantic space and thetwenty semantically most similar terms are out-put, provided that they also appear at least fiftytimes in the data.
Recall Top 20 is calculated foreach input term: what proportion of the MeSHsynonyms are identified in a list of twenty sugges-tions?
Since each synonym pair must appear atleast fifty times in the corresponding dataset vari-ant, it should be duly noted that the optimizationsets will not be identical, which in turn means thatthe results of the Unigram Word Spaces and theMultiword Term Spaces are not directly compara-ble.
The optimal parameter configuration, then,may be different when also multiword terms aremodeled.3.4 EvaluationThe optimal parameter configuration for eachdataset variant is employed in the final evaluation.In this Multiword Term Space, the SNOMED CT39C-value(a) ={log2 |a| ?
f(a) if a is not nestedlog2 |a| ?
(f(a)?1P (Ta)?bTa f(b)) otherwisea = candidate term Ta = set of extracted candidate terms that contain ab = longer candidate terms P (Ta) = number of candidate terms in Taf(a) = term frequency of a f(b) = term frequency of longer candidate term b|a| = length of candidate term (number of words)Figure 1: C-Value Formula.
The formula for calculating C-value of candidate terms.preferred terms of interest, rather than the MeSHterms, are prioritized in the term list.
The seman-tic spaces ?
and, in effect, the method ?
are pri-marily evaluated for their ability to identify syn-onyms of SNOMED CT preferred terms, in thiscase of concepts that belong to the semantic typesdisorder and finding.
The need to identify syn-onyms for these semantic types is clear, as it hasbeen shown that the coverage of SNOMED CTfor mentions of disorders (38%) and, in particu-lar, findings (23%) in Swedish clinical text is low(Skeppstedt et al 2012).
Since the Swedish ver-sion of SNOMED CT currently lacks synonyms,the evaluation reasonably needs to be manual, asthere is no reference standard.
One option, then,could be to choose a random sample of preferredterms to use in the evaluation.
A potential draw-back of such a(n) (unguided) selection is that manyconcepts in the English version of SNOMED CTdo not have any synonymous terms, which mightlead to evaluators spending valuable time lookingfor something which does not exist.
An alterna-tive approach, which is assumed here, is to inspectconcepts that have many synonyms in the Englishversion of SNOMED CT.
The fact that some con-cepts have many textual instantiations in one lan-guage does not necessarily imply that they alsohave many textual instantiations in another lan-guage.
This, however, seems to be the case whencomparing the English and Swedish versions ofMeSH: terms10 that have the most synonyms in theEnglish version tend to have at least one synonymin the Swedish version to a larger extent than a ran-dom selection of terms (60% and 62% of the termsin the Swedish version have at least one synonymwhen looking at the top 100 and top 50 terms withthe most synonyms in the English version, com-pared to 41% overall in the Swedish version).For the two dataset variants, we thus select 25SNOMED CT preferred terms for each semantic10These calculations are based on MeSH terms that belongto the categories Disease or Syndrome and Sign or Symptom.type ?
disorder and finding ?
that (1) have the mostsynonyms in the English version and (2) occur atleast fifty times in the data.
In total, fifty termsare input to the Unigram Word Space and anotherfifty terms (potentially with some overlap) are in-put to the Multiword Term Space.
A medical ex-pert inspects the twenty semantically most simi-lar terms for each input term.
Synonymy is herethe primary semantic relation of interest, but thesemantic spaces are also evaluated for their abil-ity, or tendency, to extract other semantic term re-lations: hypernyms or hyponyms, co-hyponyms,antonyms, as well as disorder-finding relations.4 ResultsThe term recognition and concatenation of mul-tiword terms naturally affect some properties ofthe dataset variants, such as the vocabulary size(number of types) and the type-token ratio.
TheUnigram Word Space contains 381,553 types andan average of 86.54 tokens/type, while the Mul-tiword Term Space contains 2,223,953 types andan average of 9.72 tokens/type.
This, in turn, mayhave an effect on which parameter configuration is?optimal?
for the synonym extraction task.
In fact,this seems to be the case when tuning the parame-ters for the two dataset variants.
For the UnigramWord Spaces, random indexing with a sliding con-text window of 8+8 terms and a dimensionality of2000 seems to work best, whereas for the Mul-tiword Term Spaces, random permutation with asliding window context of 4+4 terms and a dimen-sionality of 3000 works better (Table 1).When these parameter configurations are ap-plied to the SNOMED CT terms, a total of 40 syn-onyms are extracted by the Unigram Word Spaceand 33 synonyms by the Multiword Term Space(Table 2).
On average, 0.80 and 0.66 synonymsare extracted per preferred term, respectively.
Thenumber of identified synonyms per input termvaries significantly: for some, none; for others, upto ten.
Other semantic relations are also extracted40Unigram Word Spaces Multiword Term SpacesRI RP RI RPSliding Window?
2+2 4+4 8+8 2+2 4+4 8+8 2+2 4+4 8+8 2+2 4+4 8+81000 dimensions 0.43 0.47 0.48 0.41 0.45 0.42 0.21 0.25 0.26 0.25 0.26 0.242000 dimensions 0.43 0.48 0.49 0.48 0.48 0.43 0.21 0.24 0.25 0.25 0.25 0.243000 dimensions 0.44 0.47 0.48 0.46 0.45 0.43 0.22 0.24 0.24 0.23 0.27 0.25Table 1: Model Parameter Tuning.
Results, reported as recall top 20, for MeSH synonyms that appearat least 50 times in each of the dataset variants (unigram vs. multiword).
Random indexing (RI) andRandom permutation (RP) term spaces were built with different context window sizes (2+2, 4+4, 8+8surrounding terms) and dimensionality (1000, 2000, 3000).by the semantic spaces: mainly co-hyponyms,but also hypernyms and hyponyms, antonyms anddisorder-finding relations.
The Unigram WordSpace extracts, on average, 0.52 hypernyms or hy-ponyms, 1.8 co-hyponyms, 0.1 antonyms and 0.34disorder-finding relations.
The Multiword TermSpace extracts, on average, 0.16 hypernyms orhyponyms, 1.1 co-hyponyms, 0.14 antonyms and0.66 disorder-finding relations.
In general, moreof the above semantic relations are extracted bythe Unigram Word Space than by the MultiwordTerm Space (178 vs. 136).
It is, however, inter-esting to note that almost twice as many disorder-finding relations are extracted by the latter com-pared to the former.
Of course, none of the re-lations extracted by the Unigram Word Space in-volve a multiword term; on the other hand, morethan half (around 57%) of the relations extractedby the Multiword Term Space involve at least onemultiword term.Both semantic spaces identify more synonymsof preferred terms that belong to the semantic typefinding than disorder (in total 56 vs. 39).
The sameholds true for hyp(er/o)nyms and co-hypnoyms;however, the converse is true for antonyms anddisorder-finding relations.5 DiscussionThe results demonstrate that it is indeed possibleto extract synonyms of medical terms by perform-ing a distributional analysis of a large corpus ofclinical text ?
unigram-unigram relations, as wellas unigram-multiword and multiword-unigram re-lations.
It is also clear, however, that other se-mantically related terms share distributional pro-files to a similar degree as synonymous terms.
Thepredominance of the other semantic relations, ex-cept for antonymy, in the term lists can reason-ably be explained by the simple fact that thereexist more hypernyms, hyponyms, co-hyponymsand disorder-finding relations than synonyms (orantonyms).It is also evident that more semantic relations,and indeed more synonyms, are extracted by theUnigram Word Space than the Multiword TermSpace.
Again, it is important to underline that theresults cannot be compared without due qualifica-tion since the evaluation sets are not identical: theUnigram Word Space does not contain any mul-tiword terms, for instance.
The ability to modelmultiword terms in a distributional framework andto handle semantic composition ?
i.e., how mean-ing is, and sometimes is not, composed by themeaning of its constituents ?
has long been an en-deavor in the NLP research community (Sag et al2002; Baroni and Zamparelli, 2010; Grefenstetteand Sadrzadeh, 2011; Mitchell, 2011).
Treatingmultiword terms as compound tokens is a simpleand rather straightforward approach, which alsomakes intuitive sense: rather than treat individ-ual words as clearly delineated bearers of mean-ing, identify semantic units ?
regardless of termlength ?
and model their distributional profiles.Unfortunately, there are problems with this ap-proach.
First, the attendant increase in vocabu-lary size entails a lower tokens-type ratio, whichin turn means that the statistical foundation forterms will weaken.
In this case, the average token-type ratio decreased from 86.54 to 9.72.
This ap-proach therefore requires access to a sufficientlylarge corpus.
Second, the inflation in vocabularysize entails a corresponding increase in the num-ber of vectors in the semantic space.
This not onlyrequires more memory; to ensure that the crucialnear-orthogonality property11 of RI-based modelsis maintained, the dimensionality has to be suffi-11Random indexing assumes that the index vectors ?
rep-resenting distinct contexts ?
are nearly orthogonal.41Unigram Word Space Multiword Term SpaceDISORDER FINDING DISORDER FINDINGSynonymssum 18 22 16 17average 0.72 0.88 0.64 0.68?
1 / preferred term 12 12 8 6involves mwe - - 10 13Hyp(er/o)nymssum 12 14 4 4average 0.48 0.56 0.16 0.16?
1 / preferred term 6 8 4 3involves mwe - - 3 3Co-hyponymssum 34 56 22 33average 1.36 2.24 0.88 1.32?
1 / preferred term 14 17 10 13involves mwe - - 19 15Antonymssum 3 2 4 3average 0.12 0.08 0.16 0.12?
1 / preferred term 3 2 3 3involves mwe - - 0 1Disorder-Findingsum 11 6 28 5average 0.44 0.24 1.12 0.2?
1 / preferred term 6 5 12 5involves mwe - - 11 2Table 2: Evaluation Results.
The types of semantic relations extracted among the twenty most se-mantically similar terms of 25 DISORDER and 25 FINDING SNOMED CT preferred terms from eachsemantic space.
Sum is the total number of identified relevant terms.
Average is the average number ofrelevant terms per preferred term.
?
1 / preferred term is the number of preferred terms for which atleast one relevant term is identified.
Involves mwe is the number of relevant relations where either thepreferred term or the relevant term is a multiword expression.ciently large in relation to the number of contexts(represented by index vectors).
In the MultiwordTerm Space the vocabulary size is over two million(compared to less than 400,000 in the UnigramWord Space).
A dimensionality of 3000 is likelyinsufficient to ensure that each term type has aninitial distinct and uncorrelated representation.
Inthe evaluation, there were several examples wheretwo groups of terms ?
semantically homogenouswithin each group, but semantically heterogenousacross groups ?
co-existed in the same term list:these ?topics?
had seemingly collapsed into thesame subspace.
Despite these problems, it shouldbe recognized that the Multiword Term Space is, infact, able to retrieve 23 synonymous relations thatinvolve at least one multiword term.
The UnigramWord Space cannot retrieve any such relations.The ability to extract high-quality terms wouldseem to be an important prerequisite for this ap-proach to modeling multiword terms in a distribu-tional framework.
However, despite employing arather simple means of extracting terms ?
withoutusing any syntactic information ?
the terms thatactually appeared in the lists of semantically re-lated terms were mostly reasonable.
This perhapsindicates that the term recognition task does notneed to be perfect: terms of interest, of course,need to be identified, but some noise in the formof bad terms might be acceptable.
A weaknessof the term recognition part is, however, that toomany terms were identified, which in turn led tothe aforementioned inflation in vocabulary size.42Limiting the number of multiword terms in the ini-tial term list ?
for instance by extracting syntacticphrases as candidate terms ?
could provide a pos-sible solution to this problem.Overall, more synonyms were identified for thesemantic type finding than for disorder.
One pos-sible explanation for this could be that there aremore ways of describing a finding than a disorder?
not all semantic types can be assumed to havethe same number of synonyms.
The same holdstrue for all other semantic relations except for dis-order-finding, where disorders generated a muchlarger number of distributionally similar findingsthan vice versa.
This could perhaps also be ex-plained by the possible higher number of syn-onyms for finding than disorder.When this method was evaluated using theEnglish version of SNOMED CT, 16-24% ofknown synonyms were identified (Henriksson etal., 2013).
In this case, however, we extractedsynonym candidates for terms that may or maynot have synonyms.
This is thus a scenario thatmore closely resembles how this method wouldactually be used in a real-life setting to populatea terminology with synonyms.
Although the com-parison with MeSH showed that terms with manysynonyms in English also tend to have at least onesynonym in Swedish, approximately 40% of themdid not have any synonyms.
It is thus not certainthat the terms used in this evaluation all have atleast one synonym, which was also noted by theevaluator in this study.6 ConclusionsIn this study, we have demonstrated a methodthat could potentially be used to expedite the lan-guage porting process of terminologies such asSNOMED CT. With access to a large corpus ofclinical text in the target language and an initialset of terms, this language-independent method isable to extract and present candidate synonyms tothe lexicographer, thereby providing valuable sup-port for semi-automatic terminology development.A means to model multiword terms in a distri-butional framework is an important feature of themethod and is crucial for the synonym extractiontask.AcknowledgmentsThis work was partly supported by the SwedishFoundation for Strategic Research through theproject High-Performance Data Mining for DrugEffect Detection (ref.
no.
IIS11-0053) at Stock-holm University, Sweden, and partly funded bythe Stockholm University Academic Initiativethrough the Interlock project.
Finally, we wouldlike to thank the reviewers for their constructivefeedback.ReferencesIon Androutsopoulos and Prodromos Malakasiotis.2010.
A Survey of Paraphrasing and Textual En-tailment Methods.
Journal of Artificial IntelligenceResearch, 38:135?187.Satanjeev Banerjee and Ted Pedersen.
2003.
The De-sign, Implementation, and Use of the Ngram Statis-tic Package.
In Proceedings of CICLing, pages 370?381.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare Vectors, Adjectives are Matrices: RepresentingAdjective-Noun Constructions in Semantic Space.In Proceedings of EMNLP, pages 1183?1193.Vincent D. Blondel, Anah??
Gajardo, Maureen Hey-mans, Pierre Senellart, and Paul Van Dooren.2004.
A Measure of Similarity between Graph Ver-tices: Applications to Synonym Extraction and WebSearching.
SIAM Review, 46(4):647?666.Aaron M. Cohen and William R. Hersh.
2005.
A Sur-vey of Current Work in Biomedical Text Mining.Briefings in Bioinformatics, 6(1):57?71.Trevor Cohen and Dominic Widdows.
2009.
EmpiricalDistributional Semantics: Methods and BiomedicalApplications.
J Biomed Inform, 42(2):390?405.AM Cohen, WR Hersh, C Dubay, and K Spackman.2005.
Using co-occurrence network structure toextract synonymous gene and protein names frommedline abstracts.
BMC Bioinformatics, 6(1):103.Mike Conway and Wendy W. Chapman.
2012.
Dis-covering Lexical Instantiations of Clinical Con-cepts using Web Services, WordNet and Corpus Re-sources.
In AMIA Fall Symposium, page 1604.Hercules Dalianis, Martin Hassel, and SumithraVelupillai.
2009.
The Stockholm EPR Corpus:Characteristics and Some Initial Findings.
In Pro-ceedings of ISHIMR, pages 243?249.Scott Deerwester, Susan T. Dumais, George W Fur-nas, Thomas K Landauer, and Richard Harshman.1990.
Indexing by Latent Semantic Analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Susan T. Dumais and Thomas K. Landauer.
1997.
ASolution to Plato?s Problem: The Latent Semantic43Analysis Theory of Acquisition, Induction and Rep-resentation of Knowledge.
Psychological Review,104(2):211?240.Katerina Frantzi and Sophia Ananiadou.
1996.
Ex-tracting Nested Collocations.
In Proceedings ofCOLING, pages 41?46.Katerina Frantzi, Sophia Ananiadou, and HidekiMima.
2000.
Automatic Recognition of Multi-Word Terms: The C-value/NC-value Method.
In-ternational Journal on Digital Libraries, 3(2):115?130.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental Support for a Categorical Composi-tional Distributional Model of Meaning.
In Pro-ceedings of EMNLP, pages 1394?1404.Zellig S. Harris.
1954.
Distributional Structure.
Word,10:146?162.Marti Hearst.
1992.
Automatic Acquisition of Hy-ponyms from Large Text Corpora.
In Proceedingsof COLING, pages 539?545.Aron Henriksson and Martin Hassel.
2013.
Optimiz-ing the Dimensionality of Clinical Term Spaces forImproved Diagnosis Coding Support.
In Proceed-ings of Louhi.Aron Henriksson, Hans Moen, Maria Skeppstedt, Ann-Marie Eklund, and Vidas Daudaravicius.
2012.Synonym Extraction of Medical Terms from Clini-cal Text Using Combinations of Word Space Mod-els.
In Proceedings of SMBM, pages 10?17.Aron Henriksson, Mike Conway, Martin Duneld, andWendy W. Chapman.
2013.
Identifying Syn-onymy between SNOMED Clinical Terms of Vary-ing Length Using Distributional Analysis of Elec-tronic Health Records.
In AMIA Annual Symposium(submitted).Donald Hindle.
1990.
Noun Classification fromPredicate-Argument Structures.
In Proceedings ofACL, pages 268?275.Pentti Kanerva, Jan Kristofersson, and Anders Holst.2000.
Random Indexing of Text Samples for LatentSemantic Analysis.
In Proceedings CogSci, page1036.John McCrae and Nigel Collier.
2008.
SynonymSet Extraction from the Biomedical Literature byLexical Pattern Discovery.
BMC Bioinformatics,9(1):159.Ste?phane M. Meystre, Guergana K. Savova, Karin C.Kipper-Schuler, John F. Hurdle, et al2008.
Ex-tracting Information from Textual Documents in theElectronic Health Record: A Review of Recent Re-search.
Yearb Med Inform, 35:128?44.Jeffrey Mitchell.
2011.
Composition in DistributionalModels of Semantics.
Ph.D. thesis, University ofEdinburgh.Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio.2007.
Wikipedia Mining for an Association WebThesaurus Construction.
In Proceedings of WISE,pages 322?334.Alexander Panchenko.
2013.
Similarity Measures forSemantic Relation Extraction.
Ph.D. thesis, PhDthesis, Universite?
catholique de Louvain & BaumanMoscow State Technical University.Yves Peirsman and Dirk Geeraerts.
2009.
PredictingStrong Associations on the Basis of Corpus Data.
InProceedings of EACL, pages 648?656.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
MultiwordExpressions: A Pain in the Neck for NLP.
In Pro-ceedings of CICLing, pages 1?15.Magnus Sahlgren, Anders Holst, and Pentti Kanerva.2008.
Permutations as a Means to Encode Orderin Word Space.
In Proceedings of CogSci, pages1300?1305.Magnus Sahlgren.
2006.
The Word-Space Model:Using Distributional Analysis to Represent Syntag-matic and Paradigmatic Relations between Words inHigh-Dimensional Vector Spaces.
Ph.D. thesis, PhDthesis, Stockholm University.Maria Skeppstedt, Maria Kvist, and Hercules Dalianis.2012.
Rule-based Entity Recognition and Coverageof SNOMED CT in Swedish Clinical Text.
In Pro-ceedings of LREC, pages 1250?1257.Lonneke van der Plas and Jo?rg Tiedemann.
2006.Finding Synonyms Using Automatic Word Align-ment and Measures of Distributional Similarity.
InProceedings of COLING/ACL, pages 866?873.Hua Wu and Ming Zhou.
2003.
Optimizing Syn-onym Extraction Using Monolingual and BilingualResources.
In Proceedings of the Second Interna-tional Workshop on Paraphrasing, pages 72?79.Hong Yu and Eugene Agichtein.
2003.
ExtractingSynonymous Gene and Protein Terms from Biolog-ical Literature.
Bioinformatics, 19(suppl 1):i340?i349.Qing T Zeng, Doug Redd, Thomas Rindflesch, andJonathan Nebeker.
2012.
Synonym, Topic Modeland Predicate-Based Query Expansion for Retriev-ing Clinical Documents.
In Proceedings AMIA An-nual Symposium, pages 1050?9.Ziqi Zhang, Jose?
Iria, Christopher Brewster, and FabioCiravegna.
2008.
A Comparative Evaluation ofTerm Recognition Algorithms.
In Proceedings ofLREC.44
