SUBJECT-BASED EVALUATION MEASURES FOR INTERACTIVESPOKEN LANGUAGE SYSTEMSPatti Price, 1 Lynette Hirschman, 2 Elizabeth Shriberg, 3 Elizabeth Wade 41SRI International,  333 Ravenswood Ave., EJ 133, Menlo  Park, CA 94306.2 M IT  Laboratory for Computer  Science, Cambridge,  MA 021393University of  Cal i fornia at Berkeley, Department  of  Psychology,  Berkeley, CA947204Stanford Umversity,  Department  of  Psychology,  Stanford, CA 94305ABSTRACTThe DARPA Spoken Language effort has profited greatly from itsemphasis on tasks and common evaluation metrics.
Common,standardized valuation procedures have helped the community ofocus research effort, to measure progress, and to encourage com-munication among participating sites.
The task and the evaluationmetrics, however, must be consistent with the goals of the SpokenLanguage program, namely interactive problem solving.
Our eval-uation methods have evolved with the technology, moving fromevaluation of read speech from a fixed corpus through evaluationof isolated canned sentences to evaluation of spontaneous speechin context in a canned corpus.
A key component missed in currentevaluations is the role of subject interaction with the system.Because of the great variability across ubjects, however, it is nec-essary to use either a large number of subjects or a within-subjectdesign.
This paper proposes a within-subject design comparingthe results of a software-sharing exercise carried out jointly byM1T and SRI.1.
INTRODUCTIONThe use of a common task and a common set of evaluationmetrics has been a comerstone ofDARPA-funded researchin speech and spoken language systems.
This approachallows researchers toevaluate and compare alternativetechniques and to learn from each other's uccesses andfailures.
The choice of metrics for evaluation is a crucialcomponent of the research program, since there will bestrong pressure to make improvements with respect to themetric used.
Therefore, we must select metrics carefully ifthey are to be relevant both to our research goals and totransition of the technology from the laboratory into appli-cations.The program goal of the Spoken Language Systems (SLS)effort is to support human-computer interactive problemsolving.
The DARPA SLS community has made significantprogress toward this goal, and the development of appropri-ate evaluation metrics has played a key role in this effort.We have moved from evaluation of closed vocabulary, readspeech (resource management) for speech recognition eval-uation to open vocabulary for spontaneous speech (ATIS).In June 1990, the first SLS dry run evaluated only tran-scribed spoken input for sentences that could be interpretedindependent of context.
At the DARPA workshop in Febru-ary 1991, researchers eported on speech recognition, spo-ken language understanding, and natural languageunderstanding results for context-independent sentencesand also for pairs of context-setting + context-dependentsentences.
At the present workshop, we witness anothermajor step: we are evaluating systems on speech, spokenlanguage and natural language for all evaluable utteranceswithin entire dialogues, requiring that systems handle achsentence in its dialogue context, with no externally sup-plied context classification i formation.2.
EVALUATION METHODOLOGY:WHERE ARE WE?The current measures have been and will continue to beimportant in measuring progress, but they do not assess theinteractive component of the system, a component that willplay a critical role in future systems deployed m real tasks.Indeed, some current metrics may penalize systems thatattempt to be co-operative (for example, use of theweighted error, see below, and the maximal answer con-straints).
We propose acomplementary evaluation para-digm that makes possible the evaluation of interactivesystems.
In this section we outline the current state of eval-uation methodology and point out some shortcomings.The current evaluation procedure is fully automated, usinga canned corpus as input and a set of canonical databasetuples as output reference answers.
The evaluation mea-sures the recognition and understanding components ofaspoken language system, based on the number of correctlyanswered, incorrectly answered, and unanswered queries.These are then incorporated into a single number to pro-duce a weighted error: percent "No-Answer" plus twice thepercent "incorrect" (this formulation is equivalent to 1minus the "Score", where the "Score" is the percent "Cor-rect" minus the percent "incorrect).
"34To date, evaluation has focused on only half of the user-to-system-to-user loop: these measures evaluate how well thesystem understands what the user says, but they ignore howfelicitously the system answers the user.
The use of acanned corpus effectively precludes any attempt by the sys-tem to direct he course of the conversation, i cluding elic-iting missing information from the subject, carrying on aclarification dialogue (for ambiguous or misunderstoodinput) or providing partial answers.
For example, supposethe user says "I want to go to Dallas next Sunday".
Thereare a number of ways for the system to respond:1.
Ask for a place of departure:SYSTEM: "Where are you leaving from?"2.
Inform the user that the information is incomplete:SYSTEM: "Please give both a place of depar-ture and a destination."3.
Show all flights arriving in Dallas from anywhere:SYSTEM: "Here are the flights arriving in Dallasfrom anywhere."4.
Refuse to answer the question, with some diagnosticmessage:SYSTEM: "I'm sorry, there are too many flightsto show; please give me more information."5.
Refuse to answer the question:SYSTEM: "Please rephrase your question.
"Obviously some of these responses seem more helpful thanothers (although it is not immediately clear that there is a"best" response).
In any case, what the user says nextdepends on which strategy the system adopts.
For example,if the first strategy ischosen, where the system asks the userto specify a departure city, the user might well respond"Boston."
This makes ense as an answer to SystemResponse 1, but is not easily interpreted if the system usesSystem Response 2: "Please give both a place of departureand a destination."
Maintaining coherence indialoguedepends on what the system says to the user during datacollection.
If we continue to use only canned ialogue inevaluation, then we can choose among ways to evaluatesystems:1.
Canonical Response: All systems must provide theidentical response to each situation;2.
Dialogue Resynchronization: Each system must beable to process the data collection system's response,and resynchronize itsstate based on what the userreceived as a response during data collection;3.
Discarding Interactive Dialogue: We throw out of thecommon evaluation all interactive (particularlymixed initiative) dialogue.Mtemative 1was rejected as stifling experimentation nbuilding interactive systems.
Alternative 2 was described in\[6\], but some sites have felt that it was too burdensome toimplement (multiparty dialogue tracking is not an immedi-ate requirement for our current spoken language systems).We are left with Alternative 3, disallowing interactive dia-logue in evaluation.
This is cause for concern, since usingspoken language systems for interactive problem solving isa shared goal \[9\].
It also leaves open the possibility that weinadvertently direct our research toward strategies that max-imize our current measures but may not translate to anymeasurable gains when used in real-world applications.The push toward a single-number, automated, evaluationscore has encouraged system builders to adopt strategiesthat maximize this score.
To arrive at a single number, it isnecessary to combine right answers, wrong answers, and"no answers".
The current formula makes a wrong answertwice as bad as saying "I don't understand."
As indicated byAppelt at the October 1991 Spoken Language Interim Meet-ing, these strategies may not always be the most helpful to auser interacting with the system to accomplish a task.
Forexample, in our current evaluation framework, the answerin the following interchange could be counted as wrong,because the information would be insufficiently restricted:USER: Show me flights from Boston to Dallas leavingtomorrow on some kind of wide-body airplaneSYSTEM: You want to see flights from Boston to Dal-las leaving tomorrow, but I didn't understand the restof the sentence; here are tomorrow's flights from Bos-ton to Dallas: (shows the flights but doesn't restrict answerto wide-bodies).Similarly, the following interchange would typically bescored correct:USER: Show me the flights to Boston arriving before 7 a.m.SYSTEM: Here are the flights from anywhere to Bos-ton arriving before 7 a.m.: (shows allflights to Bostonfrom anywhere arriving before 7a.m.
).Meanwhile the following interaction counts as "unevalu-able" because the system takes the initiative, providing a(possibly irreproducible) context necessary to interpret theuser's next input:USER: Show me the flights to Boston arriving before 7 a.m.SYSTEM: Where are you departing from?USER: DallasSYSTEM: Here are the flights to Boston from Dallasarriving before 7 a.m.:35To move this technology toward realistic applications, weneed to focus on building useful systems.
This requires thatwe explore trade-offs between guessing, partial answers,reporting lack of understanding, and having a clarificationdialogue with the user.
We also need to explore differentstyles of interaction between system and user.
In addition,most aspects of the system interface (particularly, the formof the output) are not being assessed at all using currentmetrics (e.g., display of information, presence or absence ofspoken output, mixed initiative strategies).
We need todevelop complementary evaluation techniques that allow usto make progress and measure performance on interactivesystems, rather than confining ourselves to a metric thatmay penalize cooperativeness.
Further, we need a sanitycheck on our measures to reassure ourselves that gains wemake according to the measures will translate to gains inapplication areas.
The time is right for this next step, nowthat many sites have real-time spoken language systems.3.
METHODSWe have argued that interactive systems cannot be evalu-ated solely on canned input; live subjects are required.However, live subjects can introduce uncontrolled variabil-ity across users which can make interpretation f resultsdifficult.
To address this concem, we propose awithin-sub-ject design, in which each subject solves a scenario usingeach system to be compared, and the scenario rder andsystem order are counterbalanced.
However, the within-subject design requires that each subject have access to thesystems to be compared, which means that the systemsunder test must all be running in one place at one time (orelse that subjects must be shipped to the sites where the sys-tems reside, which introduces a significant time delay).Given the goal of deployable software, we chose to ship thesoftware rather than the users, but this raises many infra-structure issues, such as software portability and modular-ity, and use of common hardware and software.Our original plan was to test across three systems: the MITsystem, the SRI system, and a hybrid SRI-speech/MIT-NLsystem.
SRI would compare the SRI and SRI-MIT hybridsystems; MIT would compare the M1T and SRI-MIThybrids.
The first stumbling block was the need to licenseeach system at the other site; this took some time, but waseventually resolved.
The next stumbling block was use ofsite-specific hardware and software.
The SRI system usedD/A hardware that was not available at MIT.
Conversely,the MIT system required aLucid Lisp license, which wasnot immediately available to the SRI group.
Further,research software typically does not have the documenta-tion, support, and portability needed for rapid and efficientexchange.
Eventually, the experiment was pared down tocomparing the SRI system and the SRI/MIT hybrid systemat SRI.
These infrastructure issues have added considerableoverhead to the experiment.The SRI SLS employs the DECIPHER tm speech recogni-tion system \[4\] serially connected to SRI's TemplateMatcher system \[7,1\].
The pnming threshold of the recog-nizer was tuned so that system response time was about 2.5times utterance duration.
This strategy had the side-effect ofpruning out more hypotheses than in the comparable bench-mark system, and a higher word error rate was observed as aconsequence.
The system accesses the relational version ofthe Official Airline Guide database (implemented in Pro-log), formats the answer and displays it on the screen.
Theuser interface for this system is described in \[16\].
This sys-tem, referred to as the SRI SLS, will be compared to thehybrid SRI/MIT SLS.
The hybrid system employs the iden-tical version of the DECIPHER recognizer, set at the samepnming threshold.
All other aspects of the system differ.
Inthe SRI/MIT hybrid system, the DECIPHER recognitionoutput is connected to MIT's TINA \[15\] natural-languageunderstanding system and then to M1T software for data-base access, response formatting, and display.
Thus, theexperiment proposed here compares SRI's natural language(NL) understanding and response generation with the samecomponents from MIT.
We made no attempt to separate thecontribution of the NL components from those of the inter-face and display, since the point of this experiment was todebug the methodology; we simply cut the MIT system atthe point of easiest separation.
Below, we describe thosefactors that were held constant in the experiment and themeasures to be used on the resulting data.3.1.
Subjects, Scenarios, InstructionsData collection will proceed as described in Shriberg et al1992 \[16\] with the following exceptions: (1) updated ver-sions of the SRI Template Matcher and recognizer will beused; (2) subjects will use a new data collection facility (theroom is smaller and has no window but is acoustically simi-lar to the room used previously); (3) the scenarios to besolved have unique solutions; (4) the debriefing question-nalre will be a merged version of the questions used ondebriefing questionnaires at SRI and at MIT in separateexperiments; and (5) each subject will solve two scenarios,one using the SRI SLS and one using the SRI/MIT hybridSLS.
Changes from our previous data collection efforts areirrelevant as all comparisons will be made within the exper-imental paradigm and conditions described here.MIT designed and tested two scenarios that were selectedfor this experiment:SCENARIO A.
Find a flight from Philadelphia to Dallasthat makes a stop in Atlanta.
The flight should serve break-fast.
Find out what type of aircraft is used on the flight toDallas.
Information requested: aircraft ype.SCENARIO B.
Find a flight from Atlanta to Baltimore.
Theflight should be on a Boeing 757 and arrive around 7:00p.m.
Identify the flight (by number) and what meal is served36on the flight.
Information requested: flight number, mealtype.We will counterbalance the two scenarios and the two sys-tems by having one quarter of the subjects participate ineach of four conditions:1.
Scenario A on SRI SLS, then Scenario B on SRI/M1T hybrid SLS2.
Scenario A on SRI/MIT hybrid SLS, then Scenario Bon SRI SLS3.
Scenario B on SRI SLS, then Scenario A on SPRI/MIT hybrid SLS and4.
Scenario B on SRI/MIT hybrid SLS, then Scenario Aon SRI SLS).A total of 12 subjects will be used, 3 in each of the aboveconditions.
After subjects complete the two scenarios, oneon each of the two systems, they will complete a debriefingquestionnaire whose answers will be used in the data analy-sis.3.2.
MeasuresIn this initial experiment, we will examine several measuresin an attempt to find those most appropriate for our goals.One measure for commercial pplications i the number ofunits sold, or the number of dollars of profit.
Most develop-ment efforts, however, cannot wait that long to measuresuccess or progress.
Further, to generalize to other condi-tions, we need to gain insight into why some systems mightbe better than others.
We therefore chose to build on experi-ments described in \[12\] and to investigate he relationsamong several measures, including:?
User satisfaction.
Subjects will be asked to assesstheir satisfaction with each system (using a scale of1-5) with respect to the scenario solution they found,the speed of the system, their ability to get the infor-mation they wanted, the ease of learning to use thesystem, comparison with looking up information i abook, etc.
There will also be some open-ended ques-tions in the debriefing questionnaire to allow sub-jects to provide feedback in areas we may not haveconsidered.?
Correctness of answer.
Was the answer etrievedfrom the database correct?
This measure involvesexamination of the response and assessment ofcor-rectness.
As with the annotation procedures \[10\],some subjective judgment is involved, but thesedecisions can be made fairly reliably (see \[12\] for adiscussion on interevaluator agreement using log fileevaluation).
A system with a higher percentage ofcorrect answers may be viewed as "better."
However,other factors may well be involved that correctnessdoes not measure.
A correlation of correctness withuser satisfaction will be a stronger indication of theusefulness of this measure.
Lack of correlation mightreveal an interaction with other important factors.?
Time to complete task, as measured from the firstpush-to-talk until the user's last system action.
Oncetask and subject are controlled, as in the currentdesign, making this measurement becomes meaning-ful.
A system which results in faster completiontimes may be preferred, although it is again impor-tam to assess the correlation of time to completionwith user satisfaction.?
User waiting time, as measured between the end ofthe first query and the appearance of the response.Faster ecognition has been shown to be more satis-fying \[16\] and may correlate with overall user satis-faction.?
User response time, as measured between the appear-ance of the previous response and the push-to-talkfor the next answer.
This time may include the timethe user needs to formulate a question suitable for thesystem to answer as well as the time it takes the userto assimilate the material displayed on the screen.
Inany case, user response time as defined here is dis-tinct from waiting time, and is a readily measurablecomponent of time to completion.?
Recognition word error rate for each scenario.
Pre-sumably higher accuracy will result in more user sat-isfaction, and these measures will also allow us tomake comparison with benchmark systems operatingat different error rates.?
Frequency and type of diagnostic error messages.Systems will typically display some kind of messagewhen it has failed to understand the subject.
Thesecan be automatically logged and tabulated.4.
SUMMARY AND DISCUSSIONAs pointed out by LTC Mettala in his remarks at this meet-ing, we need to know more than the results of our currentbenchmark evaluations.
We need to know how changes inthese benchmarks will change the suitability of a giventechnology for a given application.
We need to know howour benchmarks correlate with user satisfaction and userefficiency.
In a sense, we need to evaluate our evaluationmeasures .37At this writing, the MIT software has been transferred toSRI, and data collection is about o begin.
We find that whatbegan as an exercise in evaluation has become an exercisein software sharing.
We do not want to deny the importanceof software sharing and its role in strengthening portability.However, the difficulties involved (legal and other paper-work, acquisition of software and/or hardware, extensiveinteraction between the two sites) are costly enough that webelieve we should also consider mechanisms that achieveour goals without requiring exchange of complete systems.Two such possibilities are described below.Existing logfiles, including standard transcriptions, couldbe presented toa panel of evaluators for judgments of theappropriateness of individual answers and of the interactionas a whole.
In a sense, then, the evaluators would simulatedifferent users going through the same problem solvingexperience as the subject who generated the logfile.
Cross-site variability of subjects used for this procedure could besomewhat controlled by specifying characteristics of thesesubjects (first time users, 2 hours of experience, daily com-puter user, etc.).
This approach as several importantadvantages:?
It allows a much richer set of interactive strategiesthan our current metrics can assess, which can spurresearch in the direction of the stated program goals.?
It provides an opportunity to assess and improve thecorrelation of our current metrics with measures thatare closer to the views of consumers of the technol-ogy, which should yield greater predictive power inmatching agiven technology to a given application.?
It provides a sanity check for our current evaluationmeasures, which could otherwise lead to improvedscores but not necessarily to improved technology.?
It allows the same scenario-session to be experi-enced by more than one user, which addresses thesubject-variability ssue.?
It requires no exchange of software or hardware, andtakes advantage of existing data structures currentlyrequired of all data collection sites, which means it isrelatively inexpensive toimplement.The method however does NOT make use of a strictlywithin-subject design, i.e., the same subject does not inter-act with different systems (although the same evaluatorwould assess different systems).
As a result, the logfileevaluation may require use of more subjects, or other tech-niques for addressing the issue of subject variability.A live evaluation i  which sites would bring their respec-tive systems to a common location for assessment by apanel of evaluators could provide a means for a within-sub-ject design.
The solution of having a live test would havebenefits imilar to those outlined above for the logfile eval-uation, but m addition subjects could assess the speed ofsystem response, which the logfile proposal largely ignores.However, it would be more costly to transport the systemsand the panel of evaluators than to ship logfiles (althoughmost sites curretnly bring demonstration systems to meet-ings).The logfile proposal could be modified to overcome its lim-ited value in assessment of timing (at some additionalexpense) by the creation of a mechanism that would playback the logfiles using a standard isplay mechanism andbased on the time stamps appearing in the logfiles.
Thiswould also open the possibility of having evaluators hearthe speech of the subject, rather than just seeing transcrip-tions.The costs involved for the use of such measures i negligi-ble given the potential benefits.
We propose these methodsnot as a replacement for the current measures, but rather asa complement tothem and as a reality check on their func-tion in promoting technological progress.Acknowledgment.
We gratefully acknowledge support forthe work at SRI by DARPA through the Office of NavalResearch Contract N00014-90-C-0085 (SRI), and ResearchContract N00014-89-J-1332 (M1T).
The Government hascertain rights in this material.
Any opinions, findings, andconclusions or recommendations expressed in this materialare those of the authors and do not necessarily reflect heviews of the government funding agencies.
We also grate-fully acknowledge the efforts of David Goodine of MIT andof Steven Tepper at SRI in the software transfer and instal-lation.
This research was supported by DARPAReferences1.
Appolt, D., Jackson, E., and R. Moore, "Integration of TwoComplementary Approaches to Natural Language Under-standing," Proc.
Fifth DARPA Speech and Natural Lan-guage Workshop, M. Marcus (eel.
), Morgan Kaufmann,1992; this volume.2.
Bates, M., Boisen, S., and J. Makhoul, "Developing anEvaluation Methodology for Spoken Language Systems,"pp.
102-108 in Proc.
Third Darpa Speech and LanguageWorkshop, Morgan Kaufmann, 1990.3.
Bly, B., P. Price, S. Tepper, E. Jackson, and V. Abrash,"Designing the Human Machine Interface in the ATISDomain," pp.
136-140 in Proc.
Third Darpa Speech andLanguage Workshop, Morgan Kaufmann, 1990.4.
Butzberger, J. H. Murveit, M. Weintraub, E Price, and E.Shriberg, "Modeling Spontaneous Speech Effects in LargeVocabulary Speech Applications," Proc.
Fifth DarpaSpeech and Language Workshop, M. Marcus (ed.
), MorganKanfmann, 1992; this volume.5.
Hemphi11, C. T., J. J. Godfrey, and G. R. Doddington, "TheATIS Spoken Language System Pilot Corpus," pp.
96-10138in Prec.
Third Darpa Speech and Language Workshop,Morgan Kaufmann, 1990.6.
Hirschman, L., D. A. Dahl, D. P. McKay, L. M. Norton, L.,and M. C. Linebarger, "Beyond Class A: A Proposal forAutomatic Evaluation of Discourse," pp.
109-113 in Prec.Third Darpa Speech and Language Workshop, MorganKaufmann, 1990.7.
Jackson, E., D. Appelt, J.
Bear, R. Moore, A. Podlozny, "ATemplate Matcher for Robust NL Interpretation," pp.190-194 in Prec.
Fourth DARPA Speech and Natural LanguageWorkshop, P.Price (od.
), Morgan Kaufmann, 1991.8.
Kowtko, J.
(2. and P. J.
Price, "Data Collection and Analy-sis in the Air Travel Planning Domain," pp.
119-125 inPrec.
Second Darpa Speech and Language Workshop,Morgan Kaufmann, 1989.9.
Makhoul, J., F. Jelinek, L. Rabiner, C. Weinstein, and V.Zue, pp.
463-479 in Prec.
Second DARPA Speech andNatural Language Workshop, Morgan Kaufmann, 1989.10.
"Multi-Site Data Collection for a Spoken Language Sys-tem," MADCOW, Prec.
Fifth Darpa Speech and Lan-guage Workshop, M. Marcus (ed.
), Morgan Kaufmann,1992; this volume.11.
Polifroni, J., S. Seneff, V. W. Zue, and L. Hirschman,"ATIS Data Collection at M1T," DARPA SLS Note 8, Spo-ken Language Systems Group, MIT Laboratory for Com-puter Science, Cambridge, MR, November, 1990.12.
Polifroni, J., Hirschman, L., Seneff, S., and V. Zue,"Experiments in Evaluating Interactive Spoken LanguageSystems," Prec.
Fifth Darpa Speech and Language Work-shop, M. Marcus (ed.
), Morgan Kaufmann, 1992; this vol-ume.13.
Price P., "Evaluation of Spoken Language Systems: TheATIS Domain," pp.
91-95 in Prec.
Third Darpa Speechand Language Workshop, Morgan Kaufmann, 1990.14.
Ramshaw, L.A. and S. Boisen, "An SLS Answer Compar-ator," SLS Note 7, BBN Systems and Technologies Corpo-ration, Cambridge, MR, May 1990.15.
Seneff, S., Hirschman, L. and V. Zue, "Interactive ProblemSolving and Dialogue in the ATIS Domain," pp.
354-359in Prec.
Fourth Darpa Speech and Language Workshop, P.Price (ed.
), Morgan Kaufmann, 1991.16.
Shriberg, E., E. Wade, and P. Price, "Human-MachineProblem Solving Using Spoken Language Systems (SLS):Factors Affecting Performance and User Satisfaction,"Prec.
Fifth Darpa Speech and Language Workshop, M.Marcus (ed.
), Morgan Kaufmann, 1992; this volume.39
