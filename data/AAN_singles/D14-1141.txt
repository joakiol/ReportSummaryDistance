Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1353?1362,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsPCFG Induction for Unsupervised Parsing and Language ModellingJames SciclunaUniversit?e de Nantes,CNRS, LINA, UMR6241,F-44000, Francejames.scicluna@univ-nantes.frColin de la HigueraUniversit?e de Nantes,CNRS, LINA, UMR6241,F-44000, Francecdlh@univ-nantes.frAbstractThe task of unsupervised induction ofprobabilistic context-free grammars(PCFGs) has attracted a lot of attentionin the field of computational linguistics.Although it is a difficult task, work in thisarea is still very much in demand sinceit can contribute to the advancement oflanguage parsing and modelling.
In thiswork, we describe a new algorithm forPCFG induction based on a principledapproach and capable of inducing accurateyet compact artificial natural languagegrammars and typical context-free gram-mars.
Moreover, this algorithm can workon large grammars and datasets and inferscorrectly even from small samples.
Ouranalysis shows that the type of grammarsinduced by our algorithm are, in theory,capable of modelling natural language.One of our experiments shows that ouralgorithm can potentially outperform thestate-of-the-art in unsupervised parsing onthe WSJ10 corpus.1 IntroductionThe task of unsupervised induction of PCFGs hasattracted a lot of attention in the field of compu-tational linguistics.
This task can take the formof either parameter search or structure learning.In parameter search, a CFG is fixed and the fo-cus is on assigning probabilities to this grammarusing Bayesian methods (Johnson et al., 2007) ormaximum likelihood estimation (Lari and Young,1990).
In structure learning, the focus is on build-ing the right grammar rules from scratch.
We takethe latter approach.Unsupervised structure learning of (P)CFGs isa notoriously difficult task (de la Higuera, 2010;Clark and Lappin, 2010), with theoretical resultsshowing that in general it is either impossibleto achieve (Gold, 1967; de la Higuera, 1997)or requires impractical resources (Horning, 1969;Yang, 2012).
At the same time, it is well knownthat context-free structures are needed for betterlanguage parsing and modelling, since less expres-sive models (such as HMMs) are not good enough(Manning and Sch?utze, 2001; Jurafsky and Mar-tin, 2008).
Moreover, the trend is towards unsu-pervised (rather than supervised) learning meth-ods due to the lack in most languages of annotateddata and the applicability in wider domains (Merloet al., 2010).
Thus, despite its difficulty, unsuper-vised PCFG grammar induction (or induction ofother similarly expressive models) is still an im-portant task in computational linguistics.In this paper, we describe a new algorithm forPCFG induction based on a principled approachand capable of inducing accurate yet compactgrammars.
Moreover, this algorithm can work onlarge grammars and datasets and infers correctlyeven from small samples.
We show that our algo-rithm is capable of achieving competitive resultsin both unsupervised parsing and language mod-elling of typical context-free languages and arti-ficial natural language grammars.
We also showthat the type of grammars we propose to learn are,in theory, capable of modelling natural language.2 Preliminaries2.1 Grammars and LanguagesA context-free grammar (CFG) is a 4-tuple?N,?, P, I?, where N is the set of non-terminals,?
the set of terminals, P the set of production rulesand I a set of starting non-terminals (i.e.
multi-ple starting non-terminals are possible).
The lan-guage generated from a particular non-terminal Ais L(A) = {w|A??
w} and the language gen-erated by a grammar G is L(G) =?S?IL(S).A CFG is in Chomsky Normal Form (CNF) if ev-1353ery production rule is of the form N ?
NN orN ?
?.A probabilistic context-free grammar (PCFG)is a CFG with a probability value assigned to everyrule and every starting non-terminal.
The prob-ability of a leftmost derivation from a PCFG isthe product of the starting non-terminal probabil-ity and the production probabilities used in thederivation.
The probability of a string generatedby a PCFG is the sum of all its leftmost deriva-tions?
probabilities.
The stochastic language gen-erated from a PCFG G is (L(G), ?G), where ?Gis the distribution over ?
?defined by the probabil-ities assigned to the strings by G. For a PCFG tobe consistent, the probabilities of the strings in itsstochastic language must add up to 1 (Wetherell,1980).
Any PCFG mentioned from now onwardsis assumed to be consistent.2.2 Congruence RelationsA congruence relation?
on ?
?is any equivalencerelation on ?
?that respects the following condi-tion: if u ?
v and x ?
y then ux ?
vy.
The con-gruence classes of a congruence relation are sim-ply its equivalence classes.
The congruence classof w ?
??w.r.t.
a congruence relation ?
is de-noted by [w]?.
The set of contexts of a substringwwith respect to a language L, denoted Con(w,L),is {(l, r) ?
???
?
?| lwr ?
L}.
Two strings uand v are syntactically congruent with respect toL, written u ?Lv, if Con(u, L) = Con(v, L).This is a congruence relation on ??.
The con-text distribution of a substring w w.r.t.
a stochasticlanguage (L, ?
), denoted C(L,?
)w, is a distributionwhose support is all the possible contexts over al-phabet ?
(i.e.
???
??)
and is defined as follows:C(L,?
)w(l, r) =?(lwr)?l?,r?????(l?wr?
)Two strings u and v are stochastically congru-ent with respect to (L, ?
), written u?=(L,?
)v, ifC(L,?
)uis equal to C(L,?)v.
This is a congruencerelation on ?
?.2.3 Congruential GrammarsClark (2010a) defines Congruential CFGs (C-CFGs) as being all the CFGs G which, for anynon-terminal A, if u ?
L(A) then L(A) ?
[u]?L(G)(where [u]?L(G)is the syntactic congru-ence class of u w.r.t.
the language of G).
Thisclass of grammars was defined with learnabilityin mind.
Since these grammars have a directrelationship between congruence classes and thenon-terminals, their learnability is reduced to thatof finding the correct congruence classes (Clark,2010a).This class of grammars is closely relatedto the class of NTS-grammars (Boasson andS?enizergues, 1985).
Any C-CFG is an NTS-grammar but not vice-versa.
However, it is notknown whether languages generated by C-CFGsare all NTS-languages (Clark, 2010a).
Note thatNTS-languages are a subclass of deterministiccontext-free languages and contain the regularlanguages, the substitutable (Clark and Eyraud,2007) and k-l-substitutable context-free languages(Yoshinaka, 2008), the very simple languages andother CFLs such as the Dyck language (Boassonand S?enizergues, 1985).We define a slightly more restrictive class ofgrammars, which we shall call Strongly Congru-ential CFGs (SC-CFGs).
A CFG G is a SC-CFG if, for any non-terminal A, if u ?
L(A)then L(A) = [u]?L(G).
The probabilistic equiv-alent of this is the class of Strongly CongruentialPCFGs (SC-PCFGs), defined as all the PCFGs Gwhich, for any non-terminal A, if u ?
L(A) thenL(A) = [u]?=(L(G),?).
In other words, the non-terminals (i.e.
syntactic categories in natural lan-guage) of these grammars directly correspond toclasses of substitutable strings (i.e.
substitutablewords and phrases in NL).
One may ask whetherthis is too strict a restriction for natural languagegrammars.
We argue that it is not, for the follow-ing reasons.First of all, this restriction complies with the ap-proach taken by American structural linguists forthe identification of syntactic categories, as shownby Rauh (2010): ?
[Zellig and Fries] identifiedsyntactic categories as distribution classes, em-ploying substitution tests and excluding semanticproperties of the items analysed.
Both describesyntactic categories exclusively on the basis oftheir syntactic environments and independently ofany inherent properties of the members of thesecategories?.Secondly, we know that such grammars are ca-pable of describing languages generated by gram-mars that contain typical natural language gram-matical structures (see Section 4.1; artificial natu-ral language grammars NL1-NL7, taken from var-ious sources, generate languages which can be de-scribed by SC-PCFGs).13543 AlgorithmCOMINO (our algorithm) induces SC-PCFGsfrom a positive sample S. The steps involved are:1.
Inducing the stochastically congruent classesof all the substrings of S2.
Selecting which of the induced classes arenon-terminals and subsequently building aCFG.3.
Assigning probabilities to the induced CFG.The approach we take is very different from tra-ditional grammar induction approaches, in whichgrouping of substitutable substrings is done incre-mentally as the same groups are chosen to rep-resent non-terminals.
We separate these two taskso that learning takes place in the grouping phasewhilst selection of non-terminals is done indepen-dently by solving a combinatorial problem.For the last step, the standard EM-algorithm forPCFGs (Lari and Young, 1990) is used.
In Sec-tions 3.1 and 3.2, the first and second steps of thealgorithm are described in detail.
We analyse ouralgorithm in Section 3.3.3.1 Inducing the Congruence ClassesWe describe in Algorithm 1 how the congruenceclasses are induced.Algorithm 1: Learn Congruence ClassesInput: A multiset S; parameters: n, d, i;distance function dist on localcontexts of size kOutput: The congruence classes CC over thesubstrings of S1 Subs?
Set of all substrings of S ;2 CC ?
{{w} | w ?
Subs} ;3 while True do4 Pairs?
{(x, y) | x, y ?
CC, x 6= y,|S|x?
n , |S|y?
n} ;5 if |Pairs| = 0 then exitloop ;6 Order Pairs based on distk;7 (x, y)?
Pairs[0] ;8 init = {[w]CC| w ?
S} ;9 if distk(x, y) ?
d and |init| ?
i thenexitloop ;10 CC ?
Merge(x, y, CC) ;11 end12 return CC ;At the beginning, each substring (or phrase fornatural language) in the sample is assigned its owncongruence class (line 2).
Then, pairs of frequentcongruence classes are merged together depend-ing on the distance between their empirical con-text distribution, which is calculated on local con-texts.
The following points explain each keyword:?
The empirical context distribution of a sub-string w is simply a probability distributionover all the contexts of w, where the prob-ability for a context (l, r) is the number ofoccurrences of lwr in the sample divided bythe number of occurrences of w. This is ex-tended to congruence classes by treating eachsubstring in the class as one substring (i.e.
thesum of occurrences of lwir, for all wiin theclass, divided by the sum of occurrences ofall wi).?
Due to the problem of sparsity with contexts(in any reasonably sized corpus of naturallanguage, very few phrases will have morethan one occurrence of the same context),only local contexts are considered.
The lo-cal contexts of w are the pairs of first k sym-bols (or words for natural language) preced-ing and following w. The lower k is, the lesssparsity is a problem, but the empirical con-text distribution is less accurate.
For naturallanguage corpora, k is normally set to 1 or 2.?
A frequent congruence class is one whosesubstring occurrences in the sample add upto more than a pre-defined threshold n. In-frequent congruence classes are ignored dueto their unreliable empirical context dis-tribution.
However, as more merges aremade, more substrings are added to infre-quent classes, thus increasing their frequencyand eventually they might be considered asfrequent classes.?
A distance function dist between samplesof distributions over contexts is needed bythe algorithm to decide which is the closestpair of congruence classes, so that they aremerged together.
We used L1-Distance andPearson?s chi-squared test for experiments inSections 4.1 and 4.2 respectively.?
After each merge, other merges are logicallydeduced so as to ensure that the relation re-1355mains a congruence1.
In practice, the vastmajority of the merges undertaken are logi-cally deduced ones.
This clearly relieves thealgorithm from taking unnecessary decisions(thus reducing the chance of erroneous de-cisions).
On the downside, one bad mergecan have a disastrous ripple effect.
Thus, tominimize as much as possible the chance ofthis happening, every merge undertaken is thebest possible one at that point in time (w.r.t.the distance function used).
The same idea isused in DFA learning (Lang et al., 1998).This process is repeated until either 1) no pairsof frequent congruence classes are left to merge(line 5) or 2) the smallest distance between thecandidate pairs is bigger or equal to a pre-definedthreshold d and the number of congruence classescontaining strings from the sample is smaller orequal to a pre-defined threshold i (line 9).The first condition of point 2 ensures that con-gruence classes which are sufficiently close toeach other are merged together.
The second con-dition of point 2 ensures that the hypothesizedcongruence classes are generalized enough (i.e.
toavoid undergeneralization).
For natural languageexamples, one would expect that a considerablenumber of sentences are grouped into the sameclass because of their similar structure.
Obviously,one can make use of only one of these conditionsby assigning the other a parameter value whichmakes it trivially true from the outset (0 for d and|Subs| for i).3.2 Building the Context-Free GrammarDeciding which substrings are constituents (in ourcase, this translates into choosing which congru-ence classes correspond to non-terminals) is aproblematic issue and is considered a harder taskthan the previous step (Klein, 2004).
A path fol-lowed by a number of authors consists in using anOckham?s razor or Minimal Description Lengthprinciple approach (Stolcke, 1994; Clark, 2001;Petasis et al., 2004).
This generally leads to choos-ing as best hypothesis the one which best com-presses the data.
Applying this principle in ourcase would mean that the non-terminals should be1for example, if a congruence class contains the phrases?the big?
and ?that small?, and another class contains ?dogbarked?
and ?cat meowed?, it can be logically deduced thatthe phrases ?the big dog barked?,?the big cat meowed?, ?thatsmall dog barked?
and ?that small cat meowed?
should be inthe same class.assigned in such a way that the grammar built isthe smallest possible one (in terms of the numberof non-terminals and/or production rules) consis-tent with the congruence classes.
To our knowl-edge, only local greedy search is used by systemsin the literature which try to follow this approach.We propose a new method for tackling thisproblem.
We show that all the possible SC-CFGsin CNF consistent with the congruence classes di-rectly correspond to all the solutions of a booleanformula built upon the congruence classes, wherethe variables of this formula correspond to non-terminals (and, with some minor adjustments, pro-duction rules as well).
Thus, finding the smallestpossible grammar directly translates into finding asolution which has the smallest possible amountof true variables.
Finding a minimal solution forthis type of formula is a known NP-Hard problem(Khanna et al., 2000).
However, sophisticated lin-ear programming solvers (Berkelaar et al., 2008)can take care of this problem.
For small examples(e.g.
all the examples in Table 1), these solversare able to find an exact solution in a few sec-onds.
Moreover, these solvers are capable of find-ing good approximate solutions to larger formulascontaining a few million variables.The formula contains one variable per congru-ence class.
All variables corresponding to congru-ence classes containing strings from the sampleare assigned the value True (since there must be astarting non-terminal that generates these strings).All variables corresponding to congruence classescontaining symbols from ?
are assigned the valueTrue (since for every a ?
?, there must be a ruleA ?
a).
Finally, and most importantly, for everycongruence class [w] and for every string w in [w](|w| = n), the following conditional statement isadded to the formula:v(w) ?
(v(w1,1) ?
v(w2,n)) ?
(v(w1,2) ?v(w3,n)) ?
.
.
.
?
(v(w1,n?1) ?
v(wn,n))where v(x) is the variable corresponding to thecongruence class [x] and wi,jis the substring of wfrom the ithto the jthsymbol ofw.
This statementis representing the fact that if a congruence class[w] is chosen as a non-terminal then for each stringin w ?
[w], there must be at least one CNF ruleA ?
BC that generates w and thus there mustbe at least one division of w into w1,kwk+1,nsuchthat B corresponds to [w1,k] and C corresponds to[wk+1,n].The grammar extracted from the solution of this1356formula is made up of all the possible CNF pro-duction rules built from the chosen non-terminals.The starting non-terminals are those which corre-spond to congruence classes that contain at leastone string from the sample.The following is a run of the whole process ona simple example:Sample {ab, aabb, aaabbb}Congruence Classes1 : [a], 2 : [b], 3 : [ab, aabb, aaabbb], 4 : [aa],5 : [bb], 6 : [aab, aaabb], 7 : [abb, aabbb],8 : [aaa], 9 : [bbb], 10 : [aaab], 11 : [abbb]Boolean FormulaThere is one conditional statement per sub-string.
For example,X6?
(X1?X3)?
(X4?X2) represents the two possible ways aab incongruence class 6 can be split (a|ab , aa|b).Variables X1, X2and X3are true.X3?
(X1?X2)X3?
(X1?X7) ?
(X4?X5) ?
(X6?X2)X3?
(X1?X7)?
(X4?X11)?
(X8?X9)?
(X10?X5) ?
(X6?X2)X4?
(X1?X1)X5?
(X2?X2)X6?
(X1?X3) ?
(X4?X2)X6?
(X1?X3)?
(X4?X7)?
(X8?X5)?(X10?X2)X7?
(X1?X5) ?
(X3?X2)X7?
(X1?X11)?
(X4?X9)?
(X6?X5)?(X3?X2)X8?
(X1?X4) ?
(X4?X1)X9?
(X2?X5) ?
(X5?X2)X10?
(X1?X6)?
(X4?X3)?
(X8?X2)X11?
(X1?X9)?
(X3?X5)?
(X7?X2)SolutionRunning the solver on this formula will re-turn the following true variables that make upa minimal solution: X1, X2, X3and X7.GrammarFor every statement x?
.
.
.?
(y?
z)?
.
.
.where x,y and z are true, a production rulex ?
yz is added.
So, the following grammaris built:X3is the starting non-terminalX3?
X1X7|X1X2X7?
X3X2X1?
a X2?
b3.3 AnalysisIn the first phase of the algorithm, we are group-ing all the substrings of the sample S according tothe congruence relation?=(L,?
), where (L, ?)
is thetarget stochastic language (for natural language,this is the language model).
To do so, we are as-suming that S was i.i.d.
generated from (L, ?
).In the second phase, we are representing the spaceof all CFGs consistent with the classes obtainedin phase one as different solutions to a booleanformula.
Here we introduce our bias in favour ofsmaller grammars by finding a minimal solution tothe formula.
In the last phase, probabilities are as-signed to the grammar obtained in phase two usingthe standard MLE algorithm for PCFGs.Unlike many other systems, in our case the hy-pothesis space of grammars is well-defined.
Thisallows us to analyse our algorithm in a theoreti-cal framework and obtain theoretical learnabilityresults.
Moreover, this gives us an idea on thetypes of syntactical features our system is capableof learning.Assuming our algorithm always takes correctmerge decisions, the sample required for identifi-cation needs only to be structurally complete w.r.t.the target grammar (i.e.
every production rules isused at least once in the generation of the sample).This means that, in theory, our algorithm can workwith very small samples (polynomial size w.r.t.
thenumber of rules in the target grammar).Some approaches in the literature assume thatwhenever a particular substring is a constituentin some sentence, then it is automatically a con-stituent in all other sentences (whenever it does notoverlap with previously chosen constituents) (vanZaanen, 2001; Clark, 2001; Adriaans et al., 2000).In reality, this is clearly not the case.
A simpleexperiment on the WSJ10 corpus reveals that only16 of the most frequent 1009 POS sequences (oc-curring 10 or more times in the sample) which areat least once constituents, are in fact always con-stituents.
This assumption does not hold for am-biguous grammars in our class.The approach we take to solve the smallestgrammar problem can be extended to other classesof grammars.
A similar formula can be built forgrammars whose non-terminals have a one-to-onecorrespondence with congruence classes contain-ing features of their language (Clark, 2010b).13574 Experiments and Discussion4.1 Experiments on Artificial DataWe tested our system on 11 typical context-freelanguages and 9 artificial natural language gram-mars taken from 4 different sources (Stolcke,1994; Langley and Stromsten, 2000; Adriaanset al., 2000; Solan et al., 2005).
The 11 CFLs in-clude 7 described by unambiguous grammars:UC1: anbnUC2: anbncmdmUC3: anbmn ?
mUC4: apbq, p 6= q UC5: Palindromes over alpha-bet {a, b}with a central marker UC6:Palindromesover alphabet {a, b} without a central markerUC7: Lukasiewicz language (S ?
aSS|b)and 4 described by ambiguous grammars:AC1: |w|a= |w|bAC2: 2|w|a= |w|bAC3: Dycklanguage AC4: Regular expressions.The 9 artificial natural language grammars are:NL1: Grammar ?a?, Table 2 in (Langley andStromsten, 2000) NL2: Grammar ?b?, Table 2in (Langley and Stromsten, 2000) NL3: Lexicalcategories and constituency, pg 96 in (Stolcke,1994) NL4:Recursive embedding of constituents,pg 97 in (Stolcke, 1994) NL5: Agreement, pg98 in (Stolcke, 1994) NL6: Singular/plural NPsand number agreement, pg 99 in (Stolcke, 1994)NL7: Experiment 3.1 grammar in (Adriaans et al.,2000) NL8:Grammar in Table 10 (Adriaans et al.,2000) NL9: TA1 grammar in (Solan et al., 2005).The quality of the learned model depends onits capacity to predict the correct structure (parsetrees) on the one hand and to predict the correctsentence probabilities on the other (i.e.
assignsa probability distribution close to the target one).To evaluate parse trees, we follow suggestionsgiven by van Zaanen and Geertzen (2008) and usemicro-precision and micro-recall over all the non-trivial brackets.
We take the harmonic mean ofthese two values to obtain the Unlabelled brack-ets F1score (UF1).
The learned distribution canbe evaluated using perplexity (when the target dis-tribution is not known) or some similarity metricbetween distributions (when the target distributionis known).
In our case, the target distribution isEx.
|?| |N | |P |UC1 2 3 4UC2 4 7 9UC3 2 3 5UC4 2 5 9UC5 2 3 5UC6 2 3 8UC7 2 2 3AC1 2 4 9AC2 2 5 11AC3 2 3 5AC4 7 8 13NL1 9 8 15NL2 8 8 13NL3 12 10 18NL4 13 11 22NL5 16 12 23NL6 19 17 32NL7 12 3 9NL8 30 10 35NL9 50 45 81Table 1: Size of the alphabet, number of non-terminals and productions rules of the grammars.Relative Entropy UF1Ex.
|S| COMINO ADIOS COMINO ABLUC1 10 0.029 1.876 100 100UC2 50 0.0 1.799 100 100UC5 10 0.111 7.706 100 100UC7 10 0.014 1.257 100 27.86AC1 50 0.014 4.526 52.36 35.51AC2 50 0.098 6.139 46.95 14.25AC3 50 0.057 1.934 99.74 47.48AC4 100 0.124 1.727 83.63 14.58NL7 100 0.0 0.124 100 100NL1 100 0.202 1.646 24.08 24.38NL2 200 0.333 0.963 45.90 45.80NL3 100 0.227 1.491 36.34 75.95NL5 100 0.111 1.692 88.15 79.16NL6 400 0.227 0.138 36.28 100UC3 100 0.411 0.864 61.13 100UC4 100 0.872 2.480 42.84 100UC6 100 1.449 1.0 20.14 8.36NL4 500 1.886 2.918 65.88 52.87NL8 1000 1.496 1.531 57.77 50.04NL9 800 1.701 1.227 12.49 28.53Table 2: Relative Entropy and UF1results of oursystem COMINO vs ADIOS and ABL respec-tively.
Best results are highlighted, close results(i.e.
with a difference of at most 0.1 for relativeentropy and 1% for UF1) are both highlighted1358known.
We chose relative entropy2as a good mea-sure of distance between distributions.Our UF1results over test sets of one thousandstrings were compared to results obtained by ABL(van Zaanen, 2001), which is a system whoseprimary aim is that of finding good parse trees(rather than identifying the target language).
Al-though ABL does not obtain state-of-the-art re-sults on natural language corpora, it proved to bethe best system (for which an implementation isreadily available) for unsupervised parsing of sen-tences generated by artificial grammars.
Resultsare shown in Table 1.We calculated the relative entropy on a test setof one million strings generated from the targetgrammar.
We compared our results with ADIOS(Solan et al., 2005), a system which obtains com-petitive results on language modelling (Waterfallet al., 2010) and whose primary aim is of correctlyidentifying the target language (rather than findinggood parse trees).
Results are shown in Table 1.For the tests in the first section of Table 1 (i.e.above the first dashed line), our algorithm was ca-pable of exactly identifying the structure of the tar-get grammar.
Notwithstanding this, the bracketingresults for these tests did not always yield perfectscores.
This happened whenever the target gram-mar was ambiguous, in which case the most prob-able parse trees of the target and learned grammarcan be different, thus leading to incorrect bracket-ing.
For the tests in the second section of Table 1(i.e.
between the two dashed lines), our algorithmwas capable of exactly identifying the target lan-guage (but not the grammar).
In all of these cases,the induced grammar was slightly smaller than thetarget one.
For the remaining tests, our algorithmdid not identify the target language.
In fact, it al-ways overgeneralised.
The 3 typical CFLs UC3,UC4 and UC6 are not identified because they arenot contained in our subclass of CFLs.
Inspite ofthis, the relative entropy results obtained are stillrelatively good.
Overall, it is fair to say that theresults obtained by our system, for both languagemodelling and unsupervised parsing on artificialdata, are competitive with the results obtained byother methods.2The relative entropy (or Kullback-Leibler divergence)between a target distribution D and a hypothesized distri-bution D?is defined as?w???ln(D(w)D?(w))D(w).
Add-onesmoothing is used to solve the problem of zero probabilities.4.2 Natural Language ExperimentsWe also experimented on natural language cor-pora.
For unsupervised parsing, we tested oursystem on the WSJ10 corpus, using POS taggedsentences as input.
Due to time efficiency, wechanged the algorithm for finding congruenceclasses.
Instead of always choosing the best pos-sible merge w.r.t.
the distance function, a distancethreshold is set and all congruence classes whosedistance is smaller than the threshold are merged.Also, we changed the distance function from L1-Distance to Pearson?s ?2test.In a first experiment (vaguely similar to the onedone by Luque and L?opez (2010)), we constructedthe best possible SC-CFG consistent with themerges done in the first phase and assigned prob-abilities to this grammar using Inside-Outside.In other words, we ran the second phase ofour system in a supervised fashion by using thetreebank to decide which are the best congru-ence classes to choose as non-terminals.
TheCNF grammar we obtained from this experiment(COMINO-UBOUND) gives very good parsingresults which outperform results from state-of-the-art systems DMV+CCM (Klein, 2004), U-DOP(Bod, 2006a), UML-DOP (Bod, 2006b) and In-cremental (Seginer, 2007) as shown in Table 2.Moreover, the results obtained are very close tothe best results one can ever hope to obtain fromany CNF grammar on WSJ10 (CNF-UBOUND)(Klein, 2004).
However, the grammar we obtaindoes not generalise enough and does not describe agood language model.
In a second experiment, weran the complete COMINO system.
The grammarobtained from this experiment did not give com-petitive parsing results.The first experiment shows that the merge deci-sions taken in the first phase do not hinder the pos-sibility of finding a very good grammar for pars-ing.
This means that the merge decisions takenby our system are good in general.
Manual anal-ysis on some of the merges taken confirms this.This experiment also shows that there exists a non-trivial PCFG in our restrictive class of grammarsthat is capable of achieving very good parsing re-sults.
This is a positive sign for the question ofhow adequate SC-PCFGs are for modelling natu-ral languages.
However, the real test remains thatof finding SC-PCFGs that generate good bracket-ings and good language models.
The second ex-periment shows that the second phase of our al-1359Model UP UR UF1State-of-the-artDMV+CCM 69.3 88.0 77.6U-DOP 70.8 88.2 78.5UML-DOP - - 82.9Incremental 75.6 76.2 75.9Upper boundsCOMINO-UBOUND 75.8 96.9 85.1CNF-UBOUND 78.8 100.0 88.1Table 3: Parsing results on WSJ10.
Note that In-cremental is the only system listed as state-of-the-art which parses from plain text and can generatenon-binary treesgorithm is not giving good results.
This meansthat the smallest possible grammar might not bethe best grammar for parsing.
Therefore, other cri-teria alongside the grammar size are needed whenchoosing a grammar consistent with the merges.4.3 Discussion and Future WorkIn order to improve our system, we think that ouralgorithm has to take a less conservative mergingstrategy in the first phase.
Although the mergesbeing taken are mostly correct, our analysis showsthat not enough merging is being done.
The prob-lematic case is that of taking merge decisions on(the many) infrequent long phrases.
Althoughmany logically deduced merges involve infrequentphrases and also help in increasing the frequencyof some long phrases, this proved to be not enoughto mitigate this problem.
As for future work, wethink that clustering techniques can be used to helpsolve this problem.A problem faced by the system is that, in cer-tain cases, the statistical evidence on which mergedecisions are taken does not point to the intuitivelyexpected merges.
As an example, consider the twoPOS sequences ?DT NN?
and ?DT JJ NN?
in theWSJ corpus.
Any linguist would agree that thesesequences are substitutable (in fact, they have lotsof local contexts in common).
However, statisti-cal evidence points otherwise, since their contextdistributions are not close enough.
This happensbecause, in certain positions of a sentence, ?DTNN?
is far more likely to occur than ?DT JJ NN?(w.r.t.
the ratio of their total frequencies) and inother positions, ?DT JJ NN?
occurs more than ex-pected.
The following table shows the frequenciesof these two POS sequences over the whole WSJcorpus and their frequencies in contexts (#,VBD)and (IN,#) (the symbol # represents the end orbeginning of a sentence):Totals (#,VBD) (IN,#)?DT NN?
42,222 1,034 2,123?DT JJ NN?
15,243 152 1,119Ratios 3.16 6.80 1.90It is clear that the ratios do not match, thus lead-ing to context distributions which are not closeenough.
Thus, this shows that basic sequencessuch as ?DT NN?
and ?DT JJ NN?, which lin-guists would group into the same concept NP, arestatistically derived from different sub-concepts ofNP.
Our algorithm is finding these sub-concepts,but it is being evaluated on concepts (such as NP)found in the treebank (created by linguists).From the experiments we did on artificial nat-ural language grammars, it resulted that the tar-get grammar was always slightly bigger than thelearned grammar.
Although in these cases we stillmanaged to identify the target language or havea good relative entropy result, the bracketing re-sults were in general not good.
This and our sec-ond experiment on the WSJ10 corpus show thatthe smallest possible grammar might not be thebest grammar for bracketing.
To not rely solely onfinding the smallest grammar, a bias can be addedin favour of congruence classes which, accordingto constituency tests (like the Mutual Informationcriterion in Clark (2001)), are more likely to con-tain substrings that are constituents.
This can bedone by giving different weights to the congruenceclass variables in the formula and finding the so-lution with the smallest sum of weights of its truevariables.The use of POS tags as input can also have itsproblems.
Although we solve the lexical spar-sity problem with POS tags, at the same time welose a lot of information.
In certain cases, onePOS sequence can include raw phrases which ide-ally are not grouped into the same congruenceclass.
To mitigate this problem, we can use POStags only for rare words and subdivide or ignorePOS tags for frequent words such as determinantsand prepositions.
This will reduce the number ofraw phrases represented by POS sequences whilstkeeping lexical sparsity low.13605 ConclusionWe defined a new class of PCFGs that adequatelymodels natural language syntax.
We described alearning algorithm for this class which scales wellto large examples and is even capable of learningfrom small samples.
The grammars induced bythis algorithm are compact and perform well onunsupervised parsing and language modelling oftypical CFLs and artificial natural language gram-mars.AcknowledgementsThe authors acknowledge partial support by theR?egion des Pays de la Loire.ReferencesPieter W. Adriaans, Marten Trautwein, and MarcoVervoort.
Towards High Speed Grammar Induc-tion on Large Text Corpora.
In V?aclav Hlav?ac,Keith G. Jeffery, and Jir??
Wiedermann, edi-tors, SOFSEM, volume 1963 of Lecture Notesin Computer Science, pages 173?186.
Springer,2000.Michel Berkelaar et al.
lpSolve: Interface to Lpsolve v. 5.5 to solve linear/integer programs.
Rpackage version, 5(4), 2008.Luc Boasson and G?eraud S?enizergues.
NTS Lan-guages Are Deterministic and Congruential.
J.Comput.
Syst.
Sci., 31(3):332?342, 1985.Rens Bod.
Unsupervised Parsing with U-DOP.In Proceedings of the Tenth Conference onComputational Natural Language Learning,CoNLL-X ?06, pages 85?92, Stroudsburg, PA,USA, 2006a.
Association for ComputationalLinguistics.Rens Bod.
An All-Subtrees Approach to Unsu-pervised Parsing.
In Proceedings of the 21st In-ternational Conference on Computational Lin-guistics and the 44th annual meeting of the As-sociation for Computational Linguistics, pages865?872.
Association for Computational Lin-guistics, 2006b.Alexander Clark.
Unsupervised Language Acqui-sition: Theory and Practice.
PhD thesis, Uni-versity of Sussex, 2001.Alexander Clark.
Distributional Learning of SomeContext-Free Languages with a Minimally Ad-equate Teacher.
In Sempere and Garc?
?a (2010),pages 24?37.Alexander Clark.
Towards General Algorithmsfor Grammatical Inference.
In Marcus Hut-ter, Frank Stephan, Vladimir Vovk, and ThomasZeugmann, editors, ALT, volume 6331 of Lec-ture Notes in Computer Science, pages 11?30.Springer, 2010b.Alexander Clark and R?emi Eyraud.
Polyno-mial Identification in the Limit of SubstitutableContext-free Languages.
Journal of MachineLearning Research, 8:1725?1745, 2007.Alexander Clark and Shalom Lappin.
Unsuper-vised Learning and Grammar Induction.
InAlexander Clark, Chris Fox, and Shalom Lap-pin, editors, The Handbook of ComputationalLinguistics and Natural Language Processing,pages 197?220.
Wiley-Blackwell, 2010.Alexander Clark, Franc?ois Coste, and LaurentMiclet, editors.
Grammatical Inference: Al-gorithms and Applications, 9th InternationalColloquium, ICGI 2008, Saint-Malo, France,September 22-24, 2008, Proceedings, volume5278 of Lecture Notes in Computer Science,2008.
Springer.Colin de la Higuera.
Characteristic Sets forPolynomial Grammatical Inference.
MachineLearning, 27(2):125?138, 1997.Colin de la Higuera.
Grammatical Inference:Learning Automata and Grammars.
2010.E.
Mark Gold.
Language Identification in theLimit.
Information and Control, 10(5):447?474, 1967.James Jay Horning.
A Study of Grammatical In-ference.
PhD thesis, 1969.Mark Johnson, Thomas L. Griffiths, and SharonGoldwater.
Bayesian Inference for PCFGs viaMarkov Chain Monte Carlo.
In Candace L.Sidner, Tanja Schultz, Matthew Stone, andChengXiang Zhai, editors, HLT-NAACL, pages139?146.
The Association for ComputationalLinguistics, 2007.Daniel Jurafsky and James H. Martin.
Speech andLanguage Processing (2nd Edition) (PrenticeHall Series in Artificial Intelligence).
PrenticeHall, 2 edition, 2008.Sanjeev Khanna, Madhu Sudan, Luca Trevisan,and David P. Williamson.
The Approximabil-ity of Constraint Satisfaction Problems.
SIAMJ.
Comput., 30(6):1863?1920, 2000.1361Dan Klein.
The Unsupervised Learning of Natu-ral Language Structure.
PhD thesis, StanfordUniversity, 2004.Kevin J. Lang, Barak A. Pearlmutter, and Rod-ney A.
Price.
Results of the AbbadingoOne DFA Learning Competition and a NewEvidence-Driven State Merging Algorithm.
InVasant Honavar and Giora Slutzki, editors,ICGI, volume 1433 of Lecture Notes in Com-puter Science, pages 1?12.
Springer, 1998.Pat Langley and Sean Stromsten.
LearningContext-Free Grammars with a Simplicity Bias.In Ramon L?opez de M?antaras and Enric Plaza,editors, ECML, volume 1810 of Lecture Notesin Computer Science, pages 220?228.
Springer,2000.Karim Lari and Steve J.
Young.
The Estimationof Stochastic Context-Free Grammars using theInside-Outside Algorithm.
Computer Speech &Language, 4(1):35 ?
56, 1990.Franco M. Luque and Gabriel G. Infante L?opez.Bounding the Maximal Parsing Performance ofNon-Terminally Separated Grammars.
In Sem-pere and Garc?
?a (2010), pages 135?147.Christopher D. Manning and Hinrich Sch?utze.Foundations of Statistical Natural LanguageProcessing.
MIT Press, 2001.Paola Merlo, Harry Bunt, and Joakim Nivre.
Cur-rent Trends in Parsing Technology.
In Trendsin Parsing Technology, pages 1?17.
Springer,2010.Georgios Petasis, Georgios Paliouras, Constan-tine D. Spyropoulos, and Constantine Halat-sis.
eg-GRIDS: Context-Free Grammatical In-ference from Positive Examples Using GeneticSearch.
In Georgios Paliouras and YasubumiSakakibara, editors, ICGI, volume 3264 of Lec-ture Notes in Computer Science, pages 223?234.
Springer, 2004.Gisa Rauh.
Syntactic Categories: Their Identifi-cation and Description in Linguistic Theories.Oxford Surveys in Syntax & Morphology No.7.OUP Oxford, 2010.Yoav Seginer.
Fast Unsupervised IncrementalParsing.
In John A. Carroll, Antal van denBosch, and Annie Zaenen, editors, ACL.The Association for Computational Linguistics,2007.Jos?e M. Sempere and Pedro Garc?
?a, editors.Grammatical Inference: Theoretical Resultsand Applications, 10th International Collo-quium, ICGI 2010, Valencia, Spain, September13-16, 2010.
Proceedings, volume 6339 of Lec-ture Notes in Computer Science, 2010.
Springer.Zach Solan, David Horn, Eytan Ruppin, and Shi-mon Edelman.
Unsupervised learning of nat-ural languages.
Proceedings of the NationalAcademy of Sciences of the United States ofAmerica, 102(33):11629?11634, 2005.Andreas Stolcke.
Bayesian learning of probabilis-tic language models.
PhD thesis, University ofCalifornia, Berkeley, 1994.Menno van Zaanen.
Bootstrapping Structure intoLanguage: Alignment-Based Learning.
PhDthesis, University of Leeds, 2001.Menno van Zaanen and Jeroen Geertzen.
Prob-lems with Evaluation of Unsupervised Empiri-cal Grammatical Inference Systems.
In Clarket al.
(2008), pages 301?303.Heidi R. Waterfall, Ben Sandbank, Luca Onnis,and Shimon Edelman.
An Empirical Genera-tive Framework for Computational Modeling ofLanguage Acquisition.
Journal of Child Lan-guage, 37:671?703, 6 2010.Charles S. Wetherell.
Probabilistic Languages: AReview and Some Open Questions.
ACM Com-put.
Surv., 12(4):361?379, 1980.Charles Yang.
Computational Models of SyntacticAcquisition.
Wiley Interdisciplinary Reviews:Cognitive Science, 3(2):205?213, 2012.Ryo Yoshinaka.
Identification in the Limit of k, l-Substitutable Context-Free Languages.
In Clarket al.
(2008), pages 266?279.1362
