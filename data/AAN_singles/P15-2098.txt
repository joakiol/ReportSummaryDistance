Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 594?598,Beijing, China, July 26-31, 2015. c?2015 Association for Computational LinguisticsRadical Embedding: Delving Deeper to Chinese RadicalsXinlei Shi, Junjie Zhai, Xudong Yang, Zehua Xie, Chao LiuSogou Technology Inc., Beijing, China{shixinlei, zhaijunjie, yangxudong, xiezehua, liuchao}@sogou-inc.comAbstractLanguages using Chinese charactersare mostly processed at word level.
In-spired by recent success of deep learn-ing, we delve deeper to character andradical levels for Chinese language pro-cessing.
We propose a new deep learn-ing technique, called ?radical embed-ding?, with justifications based on Chi-nese linguistics, and validate its fea-sibility and utility through a set ofthree experiments: two in-house stan-dard experiments on short-text catego-rization (STC) and Chinese word seg-mentation (CWS), and one in-field ex-periment on search ranking.
We showthat radical embedding achieves com-parable, and sometimes even better, re-sults than competing methods.1 IntroductionChinese is one of the oldest written languagesin the world, but it does not attract much at-tention in top NLP research forums, proba-bly because of its peculiarities and drastic dif-ferences from English.
There are sentences,words, characters in Chinese, as illustrated inFigure 1.
The top row is a Chinese sentence,whose English translation is at the bottom.
Inbetween is the pronunciation of the sentencein Chinese, called PinYin, which is a form ofRomanian phonetic representation of Chinese,similar to the International Phonetic Alpha-bet (IPA) for English.
Each squared symbolis a distinct Chinese character, and there areno separators between characters calls for Chi-nese Word Segmentation (CWS) techniques togroup adjacent characters into words.In most current applications (e.g., catego-rization and recommendation etc.
), Chinese isEnglish:    It is a nice day today.Pinyin:      j?n ti?n/ ti?n q?/ zh?n/ h?o!Chinese:   !
"#"# "$"#%"#&!a word a characterFigure 1: Illustration of Chinese Languagerepresented at the word level.
Inspired by re-cent success of delving deep (Szegedy et al.,2014; Zhang and LeCun, 2015; Collobert etal., 2011), an interesting question arises then:can we delve deeper than word level represen-tation for better Chinese language processing?If the answer is yes, how deep can it be donefor fun and for profit?Intuitively, the answer should be positive.Nevertheless, each Chinese character is seman-tically meaningful, thanks to its pictographicroot from ancient Chinese as depicted in Fig-ure 2.
We could delve deeper by decomposingeach character into character radicals.The right part of Figure 2 illustrates the de-composition.
This Chinese character (mean-ing ?morning?)
is decomposed into 4 radicalsthat consists of 12 strokes in total.
In Chi-nese linguistics, each Chinese character can bedecomposed into no more than four radicalsbased on a set of preset rules1.
As depicted bythe pictograms in the right part of Figure 2,the 1st radical (and the 3rd that happens tobe the same) means ?grass?, and the 2nd andthe 4th mean the ?sun?
and the ?moon?, re-spectively.
These four radicals altogether con-vey the meaning that ?the moment when sunarises from the grass while the moon wanesaway?, which is exactly ?morning?.
On theother hand, it is hard to decipher the seman-tics of strokes, and radicals are the minimumsemantic unit for Chinese.
Building deep mod-1http://en.wikipedia.org/wiki/Wubi_method594characterpictogram! "
!
#strokerOracle Bone Script ca.
1200-1050 BCEBronze Scriptca.
800 BCESmall Seal Scriptca.
220 BCEClerical Scriptca.
50 BCERegular Scriptca.
200 CE1 radical2 3 4Figure 2: Decomposition of Chinese Characterels from radicals could lead to interesting re-sults.In sum, this paper makes the followingthree-fold contributions: (1) we propose anew deep learning technique, called ?radicalembedding?, for Chinese language processingwith proper justifications based on Chineselinguistics; (2) we validate the feasibility andutility of radical embedding through a set ofthree experiments, which include not only twoin-house standard experiments on short-textcategorization (STC) and Chinese word seg-mentation (CWS), but an in-field experimenton search ranking as well; (3) this initial suc-cess of radical embedding could shed somelight on new approaches to better languageprocessing for Chinese and other languagesalike.The rest of this paper is organized as fol-lows.
Section 2 presents the radical embed-ding technique and the accompanying deepneural network components, which are com-bined and stacked to solve three applicationproblems.
Section 3 elaborates on the threeapplications and reports on the experiment re-sults.
With related work briefly discussed inSection 4, Section 5 concludes this study.
Forclarity, we limit the study to Simplified Chi-nese in this paper.2 Deep Networks with RadicalEmbeddingsThis section presents the radical embeddingtechnique, and the accompanying deep neu-ral network components.
These componentsare combined to solve the three applicationsin Section 3.Word embedding is a popular technique inNLP (Collobert et al., 2011).
It maps words tovectors of real numbers in a relatively low di-mensional space.
It is shown that the proxim-ity in this numeric space actually embodies al-gebraic semantic relationship, such as ?Queeninput outputConvolutionf ?
Rmk ?
Rny ?
Rm+n?1yi=?i+n?1s=ifs?
ks?i0 ?
i ?
m?
n + 1Max-pooling x ?
Rd y = max(x) ?
RLookupTableM ?
Rd?|D|Ii?
R|D|?1vi= MIi?
RdTanh x ?
Rdy ?
Rdyi=exi?e?xiexi+e?xi0 ?
i ?
d?
1Linear x ?
Rd y = x ?
RdReLU x ?
Rdy ?
Rdyi= 0 if xi?
0yi= xiif xi> 00 ?
i ?
d?
1Softmax x ?
Rdy ?
Rdyi=exi?dj=1exj0 ?
i ?
d?
1Concatenatexi ?
Rd0 ?
i ?
n?
1y = (x0,x1, ...,xn?1)?
Rd?nD: radical vocabularyM : a matrix containing |D| columns, each columnis a d-dimensional vector represent radical in D.Ii: a one hot vector stands for the ith radical in vocabularyTable 1: Neural Network Components?
Woman + Man ?
King?
(Mikolov et al.,2013).
As demonstrated in previous work,this numeric representation of words has led tobig improvements in many NLP tasks such asmachine translation (Sutskever et al., 2014),question answering (Iyyer et al., 2014) anddocument ranking (Shen et al., 2014).Radical embedding is similar to word em-bedding except that the embedding is at rad-ical level.
There are two ways of embedding:CBOW and skip-gram (Mikolov et al., 2013).We here use CBOW for radical embedding be-cause the two methods exhibit few differences,and CBOW is slightly faster in experiments.Specifically, a sequence of Chinese charactersis decomposed into a sequence of radicals, towhich CBOW is applied.
We use the word2vecpackage (Mikolov et al., 2013) to train radicalvectors, and then initialize the lookup tablewith these radical vectors.We list the network components in Table 1,which are combined and stacked in Figure 3to solve different problems in Section 3.
Eachcomponent is a function, the input column ofTable 1 demonstrates input parameters andtheir dimensions of these functions, the out-put column shows the formulas and outputs.3 Applications and ExperimentsIn this section, we explain how to stack thecomponents in Table 1 to solve three prob-lems: short-text categorization, Chinese wordsegmentation and search ranking, respectively.595Convolution 1?3ReLU 256Lookup Table 30KMax-PoolingShort TextReLU 256Linear 128Softmax 3LossCalInput TextLookup Table 30KConcatenateTanh 256ReLU 256Softmax 2LossCalLabel 3Label 2Query Titlea TitlebLookup Table 30KConvolution 1?3 Convolution 1?3 Convolution 1?3Linear 100 Linear 100 Linear 100ReLU 512 ReLU 512 ReLU 512ReLU 512 ReLU 512 ReLU 512Linear 256 Linear 256 Linear 256Max-Pooling Max-Pooling Max-PoolingLossCal300(a) STC (b) CWS (c) Search RankingFigure 3: Application Models using Radical EmbeddingAccuracy(%)Competing Methods Deep Neural Networks with EmbeddingLR SVM wrd chr rdc wrd+rdc chr+rdcFinance 93.52 94.06 94.89 95.85 94.75 95.70 95.74Sports 92.40 92.83 95.10 95.01 92.24 95.87 95.91Entertainment 91.72 92.24 94.32 94.77 93.21 95.11 94.78Average 92.55 93.04 94.77 95.21 93.40 95.56 95.46Table 2: Short Text Categorization Results3.1 Short-Text CategorizationFigure 3(a) presents the network structure ofthe model for short-text categorization, wherethe width of each layer is marked out as well.From the top down, a piece of short text,e.g., the title of a URL, is fed into the net-work, which goes through radical decomposi-tion, table-lookup (i.e., locating the embed-ding vector corresponding to each radical),convolution, max pooling, two ReLU layersand one fully connected layer, all the way tothe final softmax layer, where the loss is cal-culated against the given label.
The stan-dard back-propagation algorithm is used tofine tune all the parameters.The experiment uses the top-3 categoriesof the SogouCA and SogouCS news corpus(Wang et al., 2008).
100,000 samples of eachcategory are randomly selected for trainingand 10,000 for testing.
Hyper-parametersfor SVM and LR are selected through cross-validation.
Table 2 presents the accuracy ofdifferent methods, where ?wrd?, ?chr?, and?rdc?
denote word, character, and radical em-bedding, respectively.
As can be seen, embed-ding methods outperform competing LR andSVM algorithms uniformly, and the fusion ofradicals with words and characters improvesboth.3.2 Chinese Word SegmentationFigure 3(b) presents the CWS network ar-chitecture.
It uses softmax as well becauseit essentially classifies whether each charac-ter should be a segmentation boundary.
Theinput is firstly decomposed into a radical se-quence, on which a sliding window of size3 is applied to extract features, which arepipelined to downstream levels of the network.We evaluate the performance using twostandard datasets: PKU and MSR, as pro-vided by (Emerson, 2005).
The PKU datasetcontains 1.1M training words and 104K testwords, and the MSR dataset contains 2.37Mtraining words and 107K test words.
We usethe first 90% sentences for training and therest 10% sentences for testing.
We compareradical embedding with the CRF method2,FNLM (Mansur et al., 2013) and PSA (Zhenget al., 2013), and present the results in Table3.
Note that no dictionary is used in any ofthese algorithms.We see that the radical embedding (RdE)method, as the first attempt to segment wordsat radical level, actually achieves very compet-itive results.
It outperforms both CRF andFNLM on both datasets, and is comparablewith PSA.2http://crfpp.googlecode.com/svn/trunk/doc/index.html?source=navbar596Data Approach Precision Recall F1PKUCRF 88.1 86.2 87.1FNLM 87.1 87.9 87.5PSA 92.8 92.0 92.4RdE 92.6 92.1 92.3MSRCRF 89.3 87.5 88.4FNLM 92.3 92.2 92.2PSA 92.9 93.6 93.3RdE 93.4 93.3 93.3Table 3: CWS Result Comparison3.3 Web Search RankingFinally, we report on an in-field experimentwith Web search ranking.
Web search lever-ages many kinds of ranking signals, an impor-tant one of which is the preference signals ex-tracted from click-through logs.
Given a set oftriplets {query, titlea, titleb} discovered fromclick logs, where the URL titleais preferredto titlebfor the query.
The goal of learning isto produce a matching model between queryand title that maximally agrees with the pref-erence triplets.
This learnt matching model iscombined with other signals, e.g., PageRank,BM25F, etc.
in the general ranking.
The deepnetwork model for this task is depicted in Fig-ure 3(c), where each triplet goes through sevenlayers to compute the loss using Equation (1),where qi, ai, bi are the output vectors for thequery and two titles right before computingthe loss.
The calculated loss is then back prop-agated to fine tune all the parameters.m?i=1log(1 + exp(?c ?
(qTiai|qi||ai|?qTibi|qi||bi|)))(1)The evaluation is carried out on a propri-etary data set provided by a leading Chi-nese search engine company.
It contains95,640,311 triplets, which involve 14,919,928distinct queries and 65,125,732 distinct titles.95,502,506 triplets are used for training, withthe rest 137,805 triplets as testing.
It is worthnoting that the testing triplets are hard cases,mostly involving long queries and short titletexts.Figure 4 presents the results, where we varythe amount of training data to see how the per-formance varies.
The x-axis lists the percent-age of training dataset used, and 100% meansusing the entire training dataset, and the y-axis is the accuracy of the predicted prefer-ences.
We see that word embedding is over-dataset percentage (%)1 5 10 50 100accuracy (%)5455565758596061 Radical EmbeddingWord EmbeddingFigure 4: Search Ranking Resultsall superior to radical embedding, but it isinteresting to see that word embedding sat-urates using half of the data, while rankingwith radical embedding catches up using theentire dataset, getting very close in accuracy(60.78% vs. 60.47%).
Because no more datais available beyond the 95,640,311 triplets, un-fortunately we cannot tell if radical embed-ding would eventually surpass word embed-ding with more data.4 Related WorkThis paper presents the first piece of work onembedding radicals for fun and for profit, andwe are mostly inspired by fellow researchersdelving deeper in various domains (Zheng etal., 2013; Zhang and LeCun, 2015; Collobertet al., 2011; Kim, 2014; Johnson and Zhang,2014; dos Santos and Gatti, 2014).
For exam-ple, Huang et al.
?s work (Huang et al., 2013) onDSSM uses letter trigram as the basic repre-sentation, which somehow resembles radicals.Zhang and Yann?s recent work (Zhang and Le-Cun, 2015) represents Chinese at PinYin level,thus taking Chinese as a western language.Although working at PinYin level might bea viable approach, using radicals should bemore reasonable from a linguistic point ofview.
Nevertheless, PinYin only represents thepronunciation, which is arguably further awayfrom semantics than radicals.5 ConclusionThis study presents the first piece of evidenceon the feasibility and utility of radical embed-ding for Chinese language processing.
It is in-spired by recent success of delving deep in var-ious domains, and roots on the rationale thatradicals, as the minimum semantic unit, couldbe appropriate for deep learning.
We demon-strate the utility of radical embedding through597two standard in-house and one in-field exper-iments.
While some promising results are ob-tained, there are still many problems to be ex-plored further, e.g., how to leverage the lay-out code in radical decomposition that is cur-rently neglected to improve performance.
Aneven more exciting topic could be to train rad-ical, character and word embedding in a uni-fied hierarchical model as they are naturallyhierarchical.
In sum, we hope this preliminarywork could shed some light on new approachesto Chinese language processing and other lan-guages alike.ReferencesRonan Collobert, Jason Weston, Le?on Bot-tou, Michael Karlen, Koray Kavukcuoglu, andPavel P. Kuksa.
2011.
Natural language pro-cessing (almost) from scratch.
Journal of Ma-chine Learning Research, 12:2493?2537.C?
?cero Nogueira dos Santos and Maira Gatti.
2014.Deep convolutional neural networks for senti-ment analysis of short texts.
In 25th Inter-national Conference on Computational Linguis-tics, Proceedings of the Conference: TechnicalPapers, pages 69?78.Thomas Emerson.
2005.
The second internationalchinese word segmentation bakeoff.
In Proceed-ings of the fourth SIGHAN workshop on Chineselanguage Processing, volume 133.Mohit Iyyer, Jordan Boyd-Graber, LeonardoClaudino, Richard Socher, and Hal Daume?
III.2014.
A neural network for factoid question an-swering over paragraphs.
In Proceedings of the2014 Conference on Empirical Methods in Nat-ural Language Processing, pages 633?644.Rie Johnson and Tong Zhang.
2014.
Effective useof word order for text categorization with convo-lutional neural networks.
CoRR, abs/1412.1058.Yoon Kim.
2014.
Convolutional neural networksfor sentence classification.
In Proceedings of the2014 Conference on Empirical Methods in Nat-ural Language Processing, pages 1746?1751.Mairgup Mansur, Wenzhe Pei, and Baobao Chang.2013.
Feature-based neural language model andchinese word segmentation.
In Sixth Interna-tional Joint Conference on Natural LanguageProcessing, 2013, Nagoya, Japan, October 14-18, 2013, pages 1271?1277.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gre-gory S. Corrado, and Jeffrey Dean.
2013.
Dis-tributed representations of words and phrasesand their compositionality.
In Advances in Neu-ral Information Processing Systems, pages 3111?3119.Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,and Gregoire Mesnil.
2014.
A latent seman-tic model with convolutional-pooling structurefor information retrieval.
In Proceedings of the23rd ACM International Conference on Confer-ence on Information and Knowledge Manage-ment, pages 101?110.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.2014.
Sequence to sequence learning with neuralnetworks.
In Advances in Neural InformationProcessing Systems, pages 3104?3112.Christian Szegedy, Wei Liu, Yangqing Jia, PierreSermanet, Scott Reed, Dragomir Anguelov, Du-mitru Erhan, Vincent Vanhoucke, and AndrewRabinovich.
2014.
Going deeper with convolu-tions.
CoRR, abs/1409.4842.Canhui Wang, Min Zhang, Shaoping Ma, andLiyun Ru.
2008.
Automatic online news is-sue construction in web environment.
In Pro-ceedings of the 17th International Conference onWorld Wide Web, pages 457?466.Xiang Zhang and Yann LeCun.
2015.
Text under-standing from scratch.
CoRR, abs/1502.01710.Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.2013.
Deep learning for chinese word segmen-tation and POS tagging.
In Proceedings of the2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 647?657.598
