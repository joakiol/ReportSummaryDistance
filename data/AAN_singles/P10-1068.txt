Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 659?670,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsTowards robust multi-tool tagging.
An OWL/DL-based approachChristian ChiarcosUniversity of Potsdam, Germanychiarcos@uni-potsdam.deAbstractThis paper describes a series of experi-ments to test the hypothesis that the paral-lel application of multiple NLP tools andthe integration of their results improves thecorrectness and robustness of the resultinganalysis.It is shown how annotations created byseven NLP tools are mapped onto tool-independent descriptions that are definedwith reference to an ontology of linguisticannotations, and how a majority vote andontological consistency constraints can beused to integrate multiple alternative ana-lyses of the same token in a consistentway.For morphosyntactic (parts of speech) andmorphological annotations of three Ger-man corpora, the resulting merged sets ofontological descriptions are evaluated incomparison to (ontological representationof) existing reference annotations.1 Motivation and overviewNLP systems for higher-level operations or com-plex annotations often integrate redundant modu-les that provide alternative analyses for the samelinguistic phenomenon in order to benefit fromtheir respective strengths and to compensate fortheir respective weaknesses, e.g., in parsing (Crys-mann et al, 2002), or in machine translation (Carlet al, 2000).
The current trend to parallel and dis-tributed NLP architectures (Aschenbrenner et al,2006; Gietz et al, 2006; Egner et al, 2007; Lu?
?sand de Matos, 2009) opens the possibility of ex-ploring the potential of redundant parallel annota-tions also for lower levels of linguistic analysis.This paper evaluates the potential benefits ofsuch an approach with respect to morphosyntax(parts of speech, pos) and morphology in German:In comparison to English, German shows a richand polysemous morphology, and a considerablenumber of NLP tools are available, making it apromising candidate for such an experiment.Previous research indicates that the integrationof multiple part of speech taggers leads to moreaccurate analyses.
So far, however, this line of re-search focused on tools that were trained on thesame corpus (Brill and Wu, 1998; Halteren et al,2001), or that specialize to different subsets of thesame tagset (Zavrel and Daelemans, 2000; Tufis?,2000; Borin, 2000).
An even more substantial in-crease in accuracy and detail can be expected iftools are combined that make use of different an-notation schemes.For this task, ontologies of linguistic annota-tions are employed to assess the linguistic infor-mation conveyed in a particular annotation and tointegrate the resulting ontological descriptions in aconsistent and tool-independent way.
The mergedset of ontological descriptions is then evaluatedwith reference to morphosyntactic and morpho-logical annotations of three corpora of Germannewspaper articles, the NEGRA corpus (Skut etal., 1998), the TIGER corpus (Brants et al, 2002)and the Potsdam Commentary Corpus (Stede,2004, PCC).2 Ontologies and annotationsVarious repositories of linguistic annotation termi-nology have been developed in the last decades,ranging from early texts on annotation standards(Bakker et al, 1993; Leech and Wilson, 1996)over relational data base models (Bickel andNichols, 2000; Bickel and Nichols, 2002) tomore recent formalizations in OWL/RDF (or withOWL/RDF export), e.g., the General Ontology ofLinguistic Description (Farrar and Langendoen,2003, GOLD), the ISO TC37/SC4 Data Cate-gory Registry (Ide and Romary, 2004; Kemps-659Snijders et al, 2009, DCR), the OntoTag ontology(Aguado de Cea et al, 2002), or the TypologicalDatabase System ontology (Saulwick et al, 2005,TDS).
Despite their common level of representa-tion, however, these efforts have not yet convergedinto a unified and generally accepted ontology oflinguistic annotation terminology, but rather, dif-ferent resources are maintained by different com-munities, so that a considerable amount of dis-agreement between them and their respective defi-nitions can be observed.1Such conceptual mismatches and incompatibi-lities between existing terminological repositorieshave been the motivation to develop the OLiA ar-chitecture (Chiarcos, 2008) that employs a shal-low Reference Model to mediate between (onto-logical models of) annotation schemes and severalexisting terminology repositories, incl.
GOLD, theDCR, and OntoTag.
When an annotation receivesa representation in the OLiA Reference Model,it is thus also interpretable with respect to otherlinguistic ontologies.
Therefore, the findings forthe OLiA Reference Model in the experiments de-scribed below entail similar results for an applica-tion of GOLD or the DCR to the same task.2.1 The OLiA ontologiesThe Ontologies of Linguistic Annotations ?briefly, OLiA ontologies (Chiarcos, 2008) ?
re-present an architecture of modular OWL/DL on-tologies that formalize several intermediate stepsof the mapping between concrete annotations, aReference Model and existing terminology reposi-tories (?External Reference Models?
in OLiA ter-minology) such as the DCR.2The OLiA ontologies were originally develo-ped as part of an infrastructure for the sustain-able maintenance of linguistic resources (Schmidtet al, 2006) where they were originally applied1As one example, a GOLD Numeral is a De-terminer (Numeral v Quantifier v Determiner,http://linguistics-ontology.org/gold/2008/Numeral), whereas a DCR Numeral is de-fined on the basis of its semantic function,without any references to syntactic categories(http://www.isocat.org/datcat/DC-1334).Thus, two in two of them is a DCR Numeral but not a GOLDNumeral.2The OLiA Reference Model is accessible viahttp://nachhalt.sfb632.uni-potsdam.de/owl/olia.owl.
Several annotation models, e.g., stts.owl,tiger.owl, connexor.owl, morphisto.owl can befound in the same directory together with the correspondinglinking files stts-link.rdf, tiger-link.rdf,connexor-link.rdf and morphisto-link.rdf.to the formal representation and documentation ofannotation schemes, and for concept-based anno-tation queries over to multiple, heterogeneous cor-pora annotated with different annotation schemes(Rehm et al, 2007; Chiarcos et al, 2008).
NLPapplications of the OLiA ontologies include a pro-posal to integrate them with the OntoTag ontolo-gies and to use them for interface specificationsbetween modules in NLP pipeline architectures(Buyko et al, 2008).
Further, Hellmann (2010)described the application of the OLiA ontologieswithin NLP2RDF, an OWL-based blackboard ap-proach to assess the meaning of text from gram-matical analyses and subsequent enrichment withontological knowledge sources.OLiA distinguishes three different classes ofontologies:?
The OLIA REFERENCE MODEL specifiesthe common terminology that different anno-tation schemes can refer to.
It is primarilybased on a blend of concepts of EAGLES andGOLD, and further extended in accordancewith different annotation schemes, with theTDS ontology and with the DCR (Chiarcos,2010).?
Multiple OLIA ANNOTATION MODELs for-malize annotation schemes and tag sets.
An-notation Models are based on the originaldocumentation and data samples, so that theyprovide an authentic representation of the an-notation not biased with respect to any partic-ular interpretation.?
For every Annotation Model, a LINKINGMODEL defines subClassOf (v) relation-ships between concepts/properties in the re-spective Annotation Model and the Refe-rence Model.
Linking Models are interpre-tations of Annotation Model concepts andproperties in terms of the Reference Model,and thus multiple alternative Linking Modelsfor the same Annotation Model are possi-ble.
Other Linking Models specify v re-lationships between Reference Model con-cepts/properties and concepts/properties ofan External Reference Model such as GOLDor the DCR.The OLiA Reference Model (namespace olia)specifies concepts that describe linguistic cate-gories (e.g., olia:Determiner) and grammati-cal features (e.g., olia:Accusative), as well660Figure 1: Attributive demonstrative pronouns(PDAT) in the STTS Annotation ModelFigure 2: Selected morphosyntactic categories in theOLiA Reference ModelFigure 3: Individuals for accusative and sin-gular in the TIGER Annotation ModelFigure 4: Selected morphological features in theOLiA Reference Modelas properties that define possible relations be-tween those (e.g., olia:hasCase).
More gen-eral concepts that represent organizational in-formation rather than possible annotations (e.g.,MorphosyntacticCategory and CaseFeature)are stored in a separate ontology (namespaceolia top).The Reference Model is a shallow ontology: Itdoes not specify disjointness conditions of con-cepts and cardinality or domain restrictions ofproperties.
Instead, it assumes that such con-straints are inherited by means of v relationshipsfrom an External Reference Model.
Different Ex-ternal Reference Models may take different posi-tions on the issue ?
as languages do3 ?, so thatthis aspect is left underspecified in the ReferenceModel.3Based on primary experience with Western Euro-pean languages, for example, one might assume that ahasGender property applies to nouns, adjectives, pronounsand determiners only.
Yet, this is language-specific restric-tion: Russian finite verbs, for example, show gender congru-ency in past tense.Figs.
2 and 4 show excerpts of category and fea-ture hierarchies in the Reference Model.With respect to morphosyntactic annotations(parts of speech, pos) and morphological an-notations (morph), five Annotation Models forGerman are currently available: STTS (Schilleret al, 1999, pos), TIGER (Brants and Hansen,2002, morph), Morphisto (Zielinski and Simon,2008, pos, morph), RFTagger (Schmid and Laws,2008, pos, morph), Connexor (Tapanainen andJa?rvinen, 1997, pos, morph).
Further AnnotationModels for pos and morph cover five different an-notation schemes for English (Marcus et al, 1994;Sampson, 1995; Mandel, 2006; Kim et al, 2003,Connexor), two annotation schemes for Russian(Meyer, 2003; Sharoff et al, 2008), an annotationscheme designed for typological research and cur-rently applied to approx.
30 different languages(Dipper et al, 2007), an annotation scheme forOld High German (Petrova et al, 2009), and an an-notation scheme for Tibetan (Wagner and Zeisler,2004).661Figure 5: The STTS tags PDAT and ART, their rep-resentation in the Annotation Model and linkingwith the Reference Model.Annotation Models differ from the ReferenceModel mostly in that they include not only con-cepts and properties, but also individuals: An-notation Model concepts reflect an abstract con-ceptual categorization, whereas individuals re-present concrete values used to annotate thecorresponding phenomenon.
An individual isapplicable to all annotations that match thestring value specified by this individual?s hasTag,hasTagContaining, hasTagStartingWith, orhasTagEndingWith properties.
Fig.
1 illus-trates the structure of the STTS AnnotationModel (namespace stts) for the individualstts:PDAT that represents the tag used for at-tributive demonstrative pronouns (demonstrativedeterminers).
Fig.
3 illustrates the individualstiger:accusative and tiger:singular fromthe hierarchy of morphological features in theTIGER Annotation Model (namespace tiger).Fig.
5 illustrates the linking between the STTSAnnotation Model and the OLiA Reference Modelfor the individuals stts:PDAT and stts:ART.2.2 Integrating different morphosyntacticand morphological analysesWith the OLiA ontologies as described above, an-notations from different annotation schemes cannow be interpreted in terms of the OLiA ReferenceModel (or External Reference Models like GOLDor the DCR).As an example, consider the attributive demon-strative pronoun diese in (1).(1)DiesethisnichtnotneuenewErkenntnisinsightkonntecoulddertheMarktmarketderof.theMo?glichkeitenpossibilitiesamon.theSonnabendSaturdayininTreuenbrietzenTreuenbrietzenbestensin.the.best.wayunterstreichenunderline.
?The ?Market of Possibilities?, held this Saturdayin Treuenbrietzen, provided best evidence for thiswell-known (lit.
?not new?)
insight.?
(PCC, #4794)The phrase diese nicht neue Erkenntnis poses twochallenges.
First, it has to be recognized that thedemonstrative pronoun is attributive, although it isseparated from adjective and noun by nicht ?not?.Second, the phrase is in accusative case, althoughthe morphology is ambiguous between accusativeand nominative, and nominative case would be ex-pected for a sentence-initial NP.The Connexor analysis (Tapanainen andJa?rvinen, 1997) actually fails in both aspects (2).
(2) PRON Dem FEM SG NOM (Connexor)The ontological analysis of this annotation beginsby identifying the set of individuals from the Con-nexor Annotation Model that match it accordingto their hasTag (etc.)
properties.
The RDF tripletconnexor:NOM connexor:hasTagContaining?NOM?4 indicates that the tag is an applicationof the individual connexor:NOM, an instanceof connexor:Case.
Further, the annota-tion matches connexor:PRON (an instance ofconnexor:Pronoun), etc.
The result is a set ofindividuals that express different aspects of themeaning of the annotation.For these individuals, the Annotation Modelspecifies superclasses (rdf:type) and other prop-erties, i.e., connexor:NOM connexor:hasCaseconnexor:NOM, etc.
The linguistic unit repre-sented by the actual token can now be character-ized by these properties: Every property applica-ble to a member in the individual set is assumed tobe applicable to the linguistic unit as well.
In orderto save space, we use a notation closer to predicatelogic (with the token as implicit subject).
In termsof the Annotation Model, the token diese is thusdescribed by the following descriptions:4RDF triplets are quoted in simplified form, with XMLnamespaces replacing the actual URIs.662(3) rdf:type(connexor:Pronoun)connexor:hasCase(connexor:NOM) ...The Linking Model connexor-link.rdfprovides us with the information that (i)connexor:Pronoun is a subclass of the Re-ference Model concept olia:Pronoun, (ii)connexor:NOM is an instance of the ReferenceModel concept olia:Nominative, and (iii)olia:hasCase is a subproperty of olia:hasCase.Accordingly, the predicates that describe the to-ken diese can be reformulated in terms of the Re-ference Model.
rdf:type(connexor:Pronoun)entails rdf:type(olia:Pronoun), etc.
Similarly,we know that for some i:olia:Nominative it istrue that olia:hasCase(i), abbreviated here asolia:hasCase(some olia:Nominative).In this way, the grammatical information con-veyed in the original Connexor annotation canbe represented in an annotation-independent andtagset-neutral way as shown for the Connexor a-nalysis in (4).
(4) rdf:type(olia:PronounOrDeterminer)rdf:type(olia:Pronoun)olia:hasNumber(some olia:Singular)olia:hasGender(some olia:Feminine)rdf:type(olia:DemonstrativePronoun)olia:hasCase(some olia:Nominative)Analogously, the corresponding RFTagger analy-sis (Schmid and Laws, 2008) given in (5) canbe transformed into a description in terms of theOLiA Reference Model such as in (6).
(5) PRO.Dem.Attr.-3.Acc.Sg.Fem (RFTagger)(6) rdf:type(olia:PronounOrDeterminer)olia:hasNumber(some olia:Singular)olia:hasGender(some olia:Feminine)olia:hasCase(some olia:Accusative)rdf:type(olia:DemonstrativeDeterminer)rdf:type(olia:Determiner)For every description obtained from these (andfurther) analyses, an integrated and consistent gen-eralization can be established as described in thefollowing section.3 Processing linguistic annotations3.1 Evaluation setupFig.
6 sketches the architecture of the evalua-tion environment set up for this study.5 The in-put to the system is a set of documents with5The code used for the evaluation setup is available underhttp://multiparse.sourceforge.net.Figure 6: Evaluation setupTIGER/NEGRA-style morphosyntactic or mor-phological annotation (Skut et al, 1998; Brantsand Hansen, 2002) whose annotations are used asgold standard.From the annotated document, the plain tok-enized text is extracted and analyzed by one ormore of the following NLP tools:(i) Morphisto, a morphological analyzer withoutcontextual disambiguation (Zielinski and Si-mon, 2008),(ii) two part of speech taggers: the TreeTag-ger (Schmid, 1994) and the Stanford Tagger(Toutanova et al, 2003),(iii) the RFTagger that performs part of speech andmorphological analysis (Schmid and Laws,2008),(iv) two PCFG parsers: the StanfordParser (Kleinand Manning, 2003) and the BerkeleyParser(Petrov and Klein, 2007), and(v) the Connexor dependency parser (Tapanainenand Ja?rvinen, 1997).These tools annotate parts of speech, and those in(i), (iii) and (v) also provide morphological fea-tures.
All components ran in parallel threads onthe same machine, with the exception of Mor-phisto that was addressed as a web service.
The setof matching Annotation Model individuals for ev-ery annotation and the respective set of ReferenceModel descriptions are determined by means of663OLiA description ?
Morphisto Connexor RF Tree Stanford Stanford BerkeleyTagger Tagger Tagger Parser Parserword class type(...)PronounOrDeterminer 7 1(4/4)?
1 1 1 1 1 1Determiner 5.5 0.5??
0 1 1 1 1 1DemonstrativeDeterminer 5.5 0.5??
0 1 1 1 1 1Pronoun 1.5 0.5??
1 0 0 0 0 0DemonstrativePronoun 1.5 0.5??
1 0 0 0 0 0morphology hasXY(...) n/a n/a n/a n/ahasNumber(some Singular) 2.5 0.5 (2/4) 1 1 ?
Morphisto produces four alternative candidate analyseshasGender(some Feminine) 2.5 0.5 (2/4) 1 1 for this example, so every alternative analysis receives thehasCase(some Accusative) 1.5 0.5 (2/4) 0 1 confidence score 0.25hasCase(some Nominative) 1.5 0.5 (2/4) 1 0 ??
Morphisto does not distinguish attributive and substitutivehasNumber(some Plural) 0.5 0.5 (2/4) 0 0 pronouns, it predicts type(Determiner unionsq Pronoun)Table 1: Confidence scores for diese in ex.
(1)the Pellet reasoner (Sirin et al, 2007) as describedabove.A disambiguation routine (see below) then de-termines the maximal consistent set of ontologicaldescriptions.
Finally, the outcome of this processis compared to the set of descriptions correspond-ing to the original annotation in the corpus.3.2 DisambiguationReturning to examples (4) and (6) above, wesee that the resulting set of descriptions con-veys properties that are obviously contradic-ting, e.g., hasCase(some Nominative) besideshasCase(some Accusative).Our approach to disambiguation combines on-tological consistency criteria with a confidenceranking.
As we simulate an uninformed approach,the confidence ranking follows a majority vote.For diese in (1), the consultation of all seventools results a confidence ranking as shown in Tab.1: If a tool supports a description with its analy-sis, the confidence score is increased by 1 (or by1/n if the tool proposes n alternative annotations).A maximal consistent set of descriptions is thenestablished as follows:(i) Given a confidence-ranked list of availabledescriptions S = (s1, ..., sn) and a result setT = ?.
(ii) Let s1 be the first element of S =(s1, ..., sn).
(iii) If s1 is consistent with every description t ?T , then add s1 to T : T := T ?
{s1}(iv) Remove s1 from S and iterate in (ii) until Sis empty.The consistency of ontological descriptions is de-fined here as follows:6?
Two concepts A and B are consistent iffA ?
B or A v B or B v AOtherwise, A and B are disjoint.?
Two descriptions pred1(A) and pred2(B)are consistent iffA and B are consistent orpred1 is neither a subpropertynor a superproperty of pred2This heuristic formalizes an implicit disjoint-ness assumption for all concepts in the on-tology (all concepts are disjoint unless oneis a subconcept of the other).
Further, itimposes an implicit cardinality constraint onproperties (e.g., hasCase(some Accusative) andhasCase(some Nominative) are inconsistent be-cause Accusative and Nominative are siblingconcepts and thus disjoint).For the example diese, the descriptionstype(Pronoun) and type(DemonstrativePro-noun) are inconsistent with type(Determiner),and hasNumber(some Plural) is inconsistentwith hasNumber(some Singular) (Figs.
2 and4); these descriptions are thus ruled out.
ThehasCase descriptions have identical confidencescores, so that the first hasCase description thatthe algorithm encounters is chosen for the set ofresulting descriptions, the other one is ruled outbecause of their inconsistency.6The OLiA Reference Model does not specify disjoint-ness constraints, and neither do GOLD or the DCR as Exter-nal Reference Models.
The axioms of the OntoTag ontolo-gies, however, are specific to Spanish and cannot be directlyapplied to German.664PCC TIGER NEGRAbest-performing tool (StanfordTagger).960 .956 .990?average (and std.
deviation) for tool combinations1 tool .868 (.109) .864 (.122) .870 (.113)2 tools .928 (.018) .931 (.021) .943 (.028)3 tools .947 (.014) .948 (.013) .956 (.018)4 tools .956 (.006) .955 (.009) .963 (.013)5 tools .959 (.006) .960 (.007) .964 (.009)6 tools .963 (.003) .963 (.007) .965 (.007)all tools .967 .960 .965?
The Stanford Tagger was trained on the NEGRA corpus.Table 2: Recall for rdf:type descriptions for word classesTIGER NEGRA1 tool .678 (.106) .660 (.091)Morphisto .573 .568Connexor .674 .662RFTagger .786 .7512 tools .761 (.019) .740 (.012)C+M .738 .730M+R .769 .737C+R .773 .753all tools .791 .770Table 3: Recall for morphologicalhasXY() descriptionsThe resulting, maximal consistent set of de-scriptions is then compared with the ontologicaldescriptions that correspond to the original anno-tation in the corpus.4 EvaluationSix experiments were conducted with the goal toevaluate the prediction of word classes and mor-phological features on parts of three corpora ofGerman newspaper articles: NEGRA (Skut et al,1998), TIGER (Brants et al, 2002), and the Pots-dam Commentary Corpus (Stede, 2004, PCC).From every corpus 10,000 tokens were consideredfor the analysis.TIGER and NEGRA are well-known resourcesthat also influenced the design of several of thetools considered.
For this reason, the PCC wasconsulted, a small collection of newspaper com-mentaries, 30,000 tokens in total, annotated withTIGER-style parts of speech and syntax (by mem-bers of the TIGER project).
None of the tools con-sidered here were trained on this data, so that itprovides independent test data.The ontological descriptions were evaluated forrecall:7(7) recall(T ) =?ni=1 |Dpredicted(ti)?Dtarget(ti)|?ni=1 |Dtarget(ti)|In (7), T is a text (a list of tokens) with T =(t1, ..., tn), Dpredicted(t) are descriptions retrievedfrom the NLP analyses of the token t, andDtarget(t) is the set of descriptions that corres-pond to the original annotation of t in the corpus.7Precision and accuracy may not be appropriate measure-ments in this case: Annotation schemes differ in their ex-pressiveness, so that a description predicted by an NLP toolbut not found in the reference annotation may neverthelessbe correct.
The RFTagger, for example, assigns demonstra-tive pronouns the feature ?3rd person?, that is not found inTIGER/NEGRA-style annotation because of its redundancy.4.1 Word classesTable 2 shows that the recall of rdf:type de-scriptions (for word classes) increases continu-ously with the number of NLP tools applied.
Thecombination of all seven tools actually shows abetter recall than the best-performing single NLPtool.
(The NEGRA corpus is an apparent excep-tion only; the exceptionally high recall of the Stan-ford Tagger reflects the fact that it was trained onNEGRA.
)A particularly high increase in recall occurswhen tools are combined that compensate for theirrespective deficits.
Morphisto, for example, ge-nerates alternative morphological analyses, so thatthe disambiguation algorithm performs a randomchoice between these.
Morphisto has thus theworst recall among all tools considered (PCC .69,TIGER .65, NEGRA .70 for word classes).
Ascompared to this, Connexor performs a contextualdisambiguation; its recall is, however, limited byits coarse-grained word classes (PCC .73, TIGER.72, NEGRA .73).
The combination of both toolsyields a more detailed and context-sensitive ana-lysis and thus results in a boost in recall by morethan 13% (PCC .87, TIGER .86, NEGRA .86).4.2 Morphological featuresFor morphological features, Tab.
3 shows thesame tendencies that were also observed for wordclasses: The more tools are combined, the greaterthe recall of the generated descriptions, and the re-call of combined tools often outperforms the recallof individual tools.The three tools that provide morphological an-notations (Morphisto, Connexor, RFTagger) wereevaluated against 10,000 tokens from TIGER andNEGRA respectively.
The best-performing toolwas the RFTagger, which possibly reflects the fact665that it was trained on TIGER-style annotations,whereas Morphisto and Connexor were developedon the basis of independent resources and thus dif-fer from the reference annotation in their respec-tive degree of granularity.5 Summary and DiscussionWith the ontology-based approach described inthis paper, the performance of annotation tools canbe evaluated on a conceptual basis rather than bymeans of a string comparison with target annota-tions.
A formal model of linguistic concepts is ex-tensible, finer-grained and, thus, potentially moreadequate for the integration of linguistic annota-tions than string-based representations, especiallyfor heterogeneous annotations, if the tagsets in-volved are structured according to different designprinciples (e.g., due to different terminological tra-ditions, different communities involved, etc.
).It has been shown that by abstracting fromtool-specific representations of linguistic anno-tations, annotations from different tagsets can berepresented with reference to the OLiA ontologies(and/or with other OWL/RDF-based terminologyrepositories linked as External Reference Models).In particular, it is possible to compare an existingreference annotation with annotations produced byNLP tools that use independently developed anddifferently structured annotation schemes (such asConnexor vs. RFTagger vs. Morphisto).Further, an algorithm for the integration of dif-ferent annotations has been proposed that makesuse of a majority-based confidence ranking andontological consistency conditions.
As consis-tency conditions are not formally defined in theOLiA Reference Model (which is expected to in-herit such constraints from External ReferenceModels), a heuristic, structure-based definition ofconsistency was applied.This heuristic consistency definition is overlyrigid and rules out a number of consistent alter-native analyses, as it is the case for overlappingcategories.8 Despite this rigidity, we witness anincrease of recall when multiple alternative analy-ses are integrated.
This increase of recall may re-sult from a compensation of tool-specific deficits,e.g., with respect to annotation granularity.
Also,the improved recall can be explained by a compen-sation of overfitting, or deficits that are inherent to8Preposition-determiner compounds like German am ?onthe?, for example, are both prepositions and determiners.a particular approach (e.g., differences in the co-verage of the linguistic context).It can thus be stated that the integration of mul-tiple alternative analyses has the potential to pro-duce linguistic analyses that are both more robustand more detailed than those of the original tools.The primary field of application of this ap-proach is most likely to be seen in a context whereapplications are designed that make direct use ofOWL/RDF representations as described, for ex-ample, by Hellmann (2010).
It is, however, alsopossible to use ontological representations to boot-strap novel and more detailed annotation schemes,cf.
Zavrel and Daelemans (2000).
Further, theconversion from string-based representations toontological descriptions is reversible, so that re-sults of ontology-based disambiguation and vali-dation can also be reintegrated with the originalannotation scheme.
The idea of such a reversionalgorithm was sketched by Buyko et al (2008)where the OLiA ontologies were suggested as ameans to translate between different annotationschemes.96 Extensions and Related ResearchNatural extensions of the approach described inthis paper include:(i) Experiments with formally defined consis-tency conditions (e.g., with respect to restric-tions on the domain of properties).
(ii) Context-sensitive disambiguation of mor-phological features (e.g., by combinationwith a chunker and adjustment of confidencescores for morphological features over all to-kens in the current chunk, cf.
Kermes andEvert, 2002).
(iii) Replacement of majority vote by more elab-orate strategies to merge grammatical analy-ses.9The mapping from ontological descriptions to tags of aparticular scheme is possible, but neither trivial nor neces-sarily lossless: Information of ontological descriptions thatcannot be expressed in the annotation scheme under consid-eration (e.g., the distinction between attributive and substitu-tive pronouns in the Morphisto scheme) will be missing inthe resulting string representation.
For complex annotations,where ontological descriptions correspond to different sub-strings, an additional ?tag grammar?
may be necessary to de-termine the appropriate ordering of substrings according tothe annotation scheme (e.g., in the Connexor analysis).666(iv) Application of the algorithm for the ontolog-ical processing of node labels and edge labelsin syntax annotations.
(v) Integration with other ontological knowledgesources in order to improve the recall ofmorphosyntactic and morphological analy-ses (e.g., for disambiguating grammaticalcase).Extensions (iii) and (iv) are currently pursued inan ongoing research effort described by Chiarcoset al (2010).
Like morphosyntactic and morpho-logical features, node and edge labels of syntac-tic trees are ontologically represented in severalAnnotation Models, the OLiA Reference Model,and External Reference Models, the merging al-gorithm as described above can thus be appliedfor syntax, as well.
Syntactic annotations, how-ever, involve the additional challenge to align dif-ferent structures before node and edge labels canbe addressed, an issue not further discussed herefor reasons of space limitations.Alternative strategies to merge grammatical a-nalyses may include alternative voting strategiesas discussed in literature on classifier combina-tion, e.g., weighted majority vote, pairwise voting(Halteren et al, 1998), credibility profiles (Tufis?,2000), or hand-crafted rules (Borin, 2000).
Anovel feature of our approach as compared to exis-ting applications of these methods is that confi-dence scores are not attached to plain strings, butto ontological descriptions: Tufis?, for example,assigned confidence scores not to tools (as in aweighted majority vote), but rather, assessed the?credibility?
of a tool with respect to the predictedtag.
If this approach is applied to ontological de-scriptions in place of tags, it allows us to considerthe credibility of pieces of information regardlessof the actual string representation of tags.
For ex-ample, the credibility of hasCase descriptions canbe assessed independently from the credibility ofhasGender descriptions even if the original anno-tation merged both aspects in one single tag (as theRFTagger does, for example, cf.
ex.
5).Extension (v) has been addressed in previous re-search, although mostly with the opposite perspec-tive: Already Cimiano and Reyle (2003) noted thatthe integration of grammatical and semantic ana-lyses may be used to resolve ambiguity and un-derspecifications, and this insight has also moti-vated the ontological representation of linguisticresources such as WordNet (Gangemi et al, 2003),FrameNet (Scheffczyk et al, 2006), the linking ofcorpora with such ontologies (Hovy et al, 2006),the modelling of entire corpora in OWL/DL (Bur-chardt et al, 2008), and the extension of existingontologies with ontological representations of se-lected linguistic features (Buitelaar et al, 2006;Davis et al, 2008).Aguado de Cea et al (2004) sketched an ar-chitecture for the closer ontology-based integra-tion of grammatical and semantic information u-sing OntoTag and several NLP tools for Spanish.Aguado de Cea et al (2008) evaluate the benefitsof this approach for the Spanish particle se, andconclude for this example that the combination ofmultiple tools yields more detailed and more ac-curate linguistic analyses of particularly proble-matic, polysemous function words.
A similar in-crease in accuracy has also been repeatedly re-ported for ensemble combination approaches, thatare, however, limited to tools that produce annota-tions according to the same tagset (Brill and Wu,1998; Halteren et al, 2001).These observations provide further support forour conclusion that the ontology-based integrationof morphosyntactic analyses enhances both the ro-bustness and the level of detail of morphosyntac-tic and morphological analyses.
Our approach ex-tends the philosophy of ensemble combination ap-proaches to NLP tools that do not only employ dif-ferent strategies and philosophies, but also differ-ent annotation schemes.AcknowledgementsFrom 2005 to 2008, the research on linguisticontologies described in this paper was fundedby the German Research Foundation (DFG) inthe context of the Collaborative Research Center(SFB) 441 ?Linguistic Data Structures?, ProjectC2 ?Sustainability of Linguistic Resources?
(Uni-versity of Tu?bingen), and since 2007 in the contextof the SFB 632 ?Information Structure?, ProjectD1 ?Linguistic Database?
(University of Pots-dam).
The author would also like to thank Ju-lia Ritz, Angela Lahee, Olga Chiarcos and threeanonymous reviewers for helpful hints and com-ments.667ReferencesG.
Aguado de Cea, A?.
I. de Mon-Rego, A. Pareja-Lora,and R. Plaza-Arteche.
2002.
OntoTag: A semanticweb page linguistic annotation model.
In Procee-dings of the ECAI 2002 Workshop on Semantic Au-thoring, Annotation and Knowledge Markup, Lyon,France, July.G.
Aguado de Cea, A. Gomez-Perez, I. Alvarez deMon, and A. Pareja-Lora.
2004.
OntoTag?s lin-guistic ontologies: Improving semantic web anno-tations for a better language understanding in ma-chines.
In Proceedings of the International Confe-rence on Information Technology: Coding and Com-puting (ITCC?04), Las Vegas, Nevada, USA, April.G.
Aguado de Cea, J. Puch, and J.A?.
Ramos.
2008.Tagging Spanish texts: The problem of ?se?.
In Pro-ceedings of the Sixth International Conference onLanguage Resources and Evaluation (LREC 2008),Marrakech, Morocco, May.A.
Aschenbrenner, P. Gietz, M.W.
Ku?ster, C. Ludwig,and H. Neuroth.
2006.
TextGrid.
A modular plat-form for collaborative textual editing.
In Procee-dings of the International Workshop on Digital Lib-rary Goes e-Science (DLSci06), pages 27?36, Ali-cante, Spain, September.D.
Bakker, O. Dahl, M. Haspelmath, M. Koptjevskaja-Tamm, C. Lehmann, and A. Siewierska.
1993.EUROTYP guidelines.
Technical report, EuropeanScience Foundation Programme in Language Typol-ogy.B.
Bickel and J. Nichols.
2000.
Thegoals and principles of AUTOTYP.http://www.uni-leipzig.de/?autotyp/theory.html.
version of 01/12/2007.B.
Bickel and J. Nichols.
2002.
Autotypologizingdatabases and their use in fieldwork.
In Proceedingsof the LREC 2002 Workshop on Resources and Toolsin Field Linguistics, Las Palmas, Spain, May.L.
Borin.
2000.
Something borrowed, somethingblue: Rule-based combination of POS taggers.
InProceedings of the 2nd International Conference onLanguage Resources and Evaluation (LREC 2000),Athens, Greece, May, 31st ?
June, 2nd.S.
Brants and S. Hansen.
2002.
Developments in theTIGER annotation scheme and their realization inthe corpus.
In Proceedings of the Third Interna-tional Conference on Language Resources and Eva-luation (LREC 2002), pages 1643?1649, Las Pal-mas, Spain, May.S.
Brants, S. Dipper, S. Hansen, W. Lezius, andG.
Smith.
2002.
The TIGER treebank.
In Procee-dings of the Workshop on Treebanks and LinguisticTheories, pages 24?41, Sozopol, Bulgaria, Septem-ber.E.
Brill and J. Wu.
1998.
Classifier combinationfor improved lexical disambiguation.
In Procee-dings of the 36th Annual Meeting of the Associationfor Computational Linguistics and the 17th Inter-national Conference on Computational Linguistics(COLING-ACL 1998), pages 191?195, Montre?al,Canada, August.P.
Buitelaar, T. Declerck, A. Frank, S. Racioppa,M.
Kiesel, M. Sintek, R. Engel, M. Romanelli,D.
Sonntag, B. Loos, V. Micelli, R. Porzel, andP.
Cimiano.
2006.
LingInfo: Design and applica-tions of a model for the integration of linguistic in-formation in ontologies.
In Proceedings of the 5thInternational Conference on Language Resourcesand Evaluation (LREC 2006), Genoa, Italy, May.A.
Burchardt, S.
Pado?, D. Spohr, A. Frank, andU.
Heid.
2008.
Formalising Multi-layer Corpora inOWL/DL ?
Lexicon Modelling, Querying and Con-sistency Control.
In Proceedings of the 3rd Inter-national Joint Conference on NLP (IJCNLP 2008),Hyderabad, India, January.E.
Buyko, C. Chiarcos, and A. Pareja-Lora.
2008.Ontology-based interface specifications for a NLPpipeline architecture.
In Proceedings of the Interna-tional Conference on Language Resources and Eva-luation (LREC 2008), Marrakech, Morocco, May.M.
Carl, C. Pease, L.L.
Iomdin, and O. Streiter.
2000.Towards a dynamic linkage of example-based andrule-based machine translation.
Machine Transla-tion, 15(3):223?257.C.
Chiarcos, S. Dipper, M. Go?tze, U. Leser,A.
Lu?deling, J. Ritz, and M. Stede.
2008.
A FlexibleFramework for Integrating Annotations from Differ-ent Tools and Tag Sets.
Traitement Automatique desLangues, 49(2).C.
Chiarcos, K. Eckart, and J. Ritz.
2010.
Creating andexploiting a resource of parallel parses.
In 4th Lin-guistic Annotation Workshop (LAW 2010), held inconjunction with ACL-2010, Uppsala, Sweden, July.C.
Chiarcos.
2008.
An ontology of linguistic annota-tions.
LDV Forum, 23(1):1?16.
Foundations of On-tologies in Text Technology, Part II: Applications.C.
Chiarcos.
2010.
Grounding an ontology of lin-guistic annotations in the Data Category Registry.In Workshop on Language Resource and LanguageTechnology Standards (LR&LTS 2010), held in con-junction with LREC 2010, Valetta, Malta, May.P.
Cimiano and U. Reyle.
2003.
Ontology-based se-mantic construction, underspecification and disam-biguation.
In Proceedings of the Lorraine/SaarlandWorkshop on Prospects and Recent Advances in theSyntax-Semantics Interface, pages 33?38, Nancy,France, October.B.
Crysmann, A. Frank, B. Kiefer, S. Mu?ller, G. Neu-mann, J. Piskorski, U. Scha?fer, M. Siegel, H. Uszko-reit, F. Xu, M. Becker, and H. Krieger.
2002.
An668integrated architecture for shallow and deep proces-sing.
In Proceedings of 40th Annual Meeting of theAssociation for Computational Linguistics, pages441?448, Philadelphia, Pennsylvania, USA, July.B.
Davis, S. Handschuh, A. Troussov, J.
Judge, andM.
Sogrin.
2008.
Linguistically light lexical ex-tensions for ontologies.
In Proceedings of the SixthInternational Conference on Language Resourcesand Evaluation (LREC 2008), Marrakech, Morocco,May.S.
Dipper, M. Go?tze, and S. Skopeteas, editors.
2007.Information Structure in Cross-Linguistic Corpora:Annotation Guidelines for Phonology, Morpholo-gy, Syntax, Semantics, and Information Structure.Interdisciplinary Studies on Information Structure(ISIS), Working Papers of the SFB 632; 7.
Univer-sita?tsverlag Potsdam, Potsdam, Germany.M.T.
Egner, M. Lorch, and E. Biddle.
2007.
UIMAGrid: Distributed large-scale text analysis.
In Pro-ceedings of the Seventh IEEE International Sym-posium on Cluster Computing and the Grid (CC-GRID?07), pages 317?326, Rio de Janeiro, Brazil,May.S.
Farrar and D.T.
Langendoen.
2003.
Markup andthe GOLD ontology.
In EMELD Workshop on Di-gitizing and Annotating Text and Field Recordings.Michigan State University, July.A.
Gangemi, R. Navigli, and P. Velardi.
2003.
The On-toWordNet project: Extension and axiomatization ofconceptual relations in WordNet.
In R. Meersmanand Z. Tari, editors, Proceedings of On the Moveto Meaningful Internet Systems (OTM2003), pages820?838, Catania, Italy, November.P.
Gietz, A. Aschenbrenner, S. Budenbender, F. Jan-nidis, M.W.
Ku?ster, C. Ludwig, W. Pempe, T. Vitt,W.
Wegstein, and A. Zielinski.
2006.
TextGridand eHumanities.
In Proceedings of the SecondIEEE International Conference on e-Science andGrid Computing (E-SCIENCE ?06), pages 133?141,Amsterdam, The Netherlands, December.H.
van Halteren, J. Zavrel, and W. Daelmans.
1998.Improving data driven wordclass tagging by systemcombination.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and the 17th International Conference onComputational Linguistics (COLING-ACL 1998),Montre?al, Canada, August.H.
van Halteren, J. Zavrel, and W. Daelmans.
2001.Improving accuracy in word class tagging throughthe combination of machine learning systems.
Com-putational Linguistics, 27(2):199?229.S.
Hellmann.
2010.
The semantic gap of formalizedmeaning.
In The 7th Extended Semantic Web Confe-rence (ESWC 2010), Heraklion, Greece, May 30th ?June 3rd.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: the 90% solu-tion.
In Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology (HLT-NAACL 2006),pages 57?60, New York, June.N.
Ide and L. Romary.
2004.
A registry of standarddata categories for linguistic annotation.
In Procee-dings of the Fourth Language Resources and Evalu-ation Conference (LREC 2004), pages 135?39, Lis-boa, Portugal, May.M.
Kemps-Snijders, M. Windhouwer, P. Wittenburg,and S.E.
Wright.
2009.
ISOcat: remodelling meta-data for language resources.
International Journalof Metadata, Semantics and Ontologies, 4(4):261?276.H.
Kermes and S. Evert.
2002.
YAC ?
A recur-sive chunker for unrestricted German text.
In Pro-ceedings of the Third International Conference onLanguage Resources and Evaluation (LREC 2002),pages 1805?1812, Las Palmas, Spain, May.J.D.
Kim, T. Ohta, Y. Tateisi, and J. Tsujii.
2003.
GE-NIA corpus ?
A semantically annotated corpus forbio-textmining.
Bioinformatics, 19(1):180?182.D.
Klein and C.D.
Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Lin-guistics, pages 423?430, Sapporo, Japan, July.G.
Leech and A. Wilson.
1996.
EAGLES recommen-dations for the morphosyntactic annotation of cor-pora.
Version of March 1996.T.
Lu?
?s and D.M.
de Matos.
2009.
High-performancehigh-volume layered corpora annotation.
In Procee-dings of the Third Linguistic Annotation Workshop(LAW-III) held in conjunction with ACL-IJCNLP2009, pages 99?107, Singapore, August.M.
Mandel.
2006.
Integrated annotation of biomedicaltext: Creating the PennBioIE corpus.
In Text Min-ing Ontologies and Natural Language Processing inBiomedicine, Manchester, UK, March.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1994.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational linguis-tics, 19(2):313?330.R.
Meyer.
2003.
Halbautomatische morphosyntak-tische Annotation russischer Texte.
In R. Ham-mel and L. Geist, editors, Linguistische Beitra?gezur Slavistik aus Deutschland und O?sterreich.
X.JungslavistInnen-Treffen, Berlin 2001, pages 92?105.
Sagner, Mu?nchen.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In Proceedings of the Confe-rence of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technology (HLT-NAACL 2007), pages 404?411, Rochester, NY, April.669S.
Petrova, C. Chiarcos, J. Ritz, M. Solf, and A. Zeldes.2009.
Building and using a richly annotated inter-linear diachronic corpus: The case of Old High Ger-man Tatian.
Traitement automatique des langues etlangues anciennes, 50(2):47?71.G.
Rehm, R. Eckart, and C. Chiarcos.
2007.
An OWL-and XQuery-based mechanism for the retrieval oflinguistic patterns from XML-corpora.
In Procee-dings of Recent Advances in Natural Language Pro-cessing (RANLP 2007), Borovets, Bulgaria, Septem-ber.G.
Sampson.
1995.
English for the computer: The SU-SANNE corpus and analytic scheme.
Oxford Uni-versity Press.A.
Saulwick, M. Windhouwer, A. Dimitriadis, andR.
Goedemans.
2005.
Distributed tasking in on-tology mediated integration of typological databasesfor linguistic research.
In Proceedings of the 17thConference on Advanced Information Systems Engi-neering (CAiSE?05), Porto, Portugal, June.J.
Scheffczyk, A. Pease, and M. Ellsworth.
2006.Linking FrameNet to the suggested upper mergedontology.
In Proceedings of the Fourth Interna-tional Conference on Formal Ontology in Informa-tion Systems (FOIS 2006), pages 289?300, Balti-more, Maryland, USA, November.A.
Schiller, S. Teufel, C. Thielen, and C. Sto?ckert.1999.
Guidelines fu?r das Tagging deutscherTextcorpora mit STTS.
Technical report, Universityof Stuttgart, University of Tu?bingen.H.
Schmid and F. Laws.
2008.
Estimation of condi-tional probabilities with decision trees and an ap-plication to fine-grained pos tagging.
In Procee-dings of the 22nd International Conference on Com-putational Linguistics (COLING 2008), Manchester,UK, August.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In Proceedings of InternationalConference on New Methods in Language Process-ing, pages 44?49, Manchester, UK, September.T.
Schmidt, C. Chiarcos, T. Lehmberg, G. Rehm,A.
Witt, and E. Hinrichs.
2006.
Avoiding datagraveyards: From heterogeneous data collected inmultiple research projects to sustainable linguisticresources.
In Proceedings of the E-MELD work-shop on Digital Language Documentation: Toolsand Standards: The State of the Art, East Lansing,Michigan, US, June.S.
Sharoff, M. Kopotev, T. Erjavec, A. Feldman, andD.
Divjak.
2008.
Designing and evaluating Rus-sian tagsets.
In Proceedings of the 6th InternationalConference on Language Resources and Evaluation(LREC 2008), Marrakech, Morocco, May.E.
Sirin, B. Parsia, B.C.
Grau, A. Kalyanpur, andY.
Katz.
2007.
Pellet: A practical OWL/DL rea-soner.
Web Semantics: Science, Services and Agentson the World Wide Web, 5(2):51?53.W.
Skut, T. Brants, B. Krenn, and H. Uszkoreit.
1998.A linguistically interpreted corpus of German news-paper text.
In In Proceedings of the ESSLLI Work-shop on Recent Advances in Corpus Annotation,Saarbru?cken, Germany, August.M.
Stede.
2004.
The Potsdam Commentary Corpus.In Proceedings of the 2004 ACL Workshop on Dis-course Annotation, pages 96?102, Barcelona, Spain,July.P.
Tapanainen and T. Ja?rvinen.
1997.
A nonprojec-tive dependency parser.
In Proceedings of the 5thConference on Applied Natural Language Process-ing, pages 64?71, Washington, DC, April.K.
Toutanova, D. Klein, C.D.
Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In Proceedings of the2003 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology (HLT-NAACL 2003),Edmonton, Canada, May.D.
Tufis?.
2000.
Using a large set of EAGLES-compliant morpho-syntactic descriptors as a tagsetfor probabilistic tagging.
In Proceedings of the 2ndInternational Conference on Language Resourcesand Evaluation (LREC 2000), pages 1105?1112,Athens, Greece, May, 31st ?
June, 2nd.A.
Wagner and B. Zeisler.
2004.
A syntactically an-notated corpus of Tibetan.
In Fourth InternationalConference on Language Resources and Evaluation(LREC 2004), Lisboa, Portugal, May.J.
Zavrel and W. Daelemans.
2000.
Bootstrapping atagged corpus through combination of existing het-erogeneous taggers.
In Proceedings of the 2nd In-ternational Conference on Language Resources andEvaluation (LREC 2000), Athens, Greece, May, 31st?
June, 2nd.A.
Zielinski and C. Simon.
2008.
Morphisto: Anopen-source morphological analyzer for German.
InProceedings of the Conference on Finite State Meth-ods in Natural Language Processing (FSMNLP), Is-pra, Italy, September.670
