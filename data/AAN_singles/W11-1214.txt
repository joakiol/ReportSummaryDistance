Unsupervised Alignment of Comparable Data and Text ResourcesAnja Belz Eric KowSchool of Computing, Engineering and MathematicsUniversity of BrightonBrighton BN2 4GJ, UK{A.S.Belz,E.Y.Kow}@brighton.ac.ukAbstractIn this paper we investigate automatic data-text alignment, i.e.
the task of automaticallyaligning data records with textual descrip-tions, such that data tokens are aligned withthe word strings that describe them.
Our meth-ods make use of log likelihood ratios to esti-mate the strength of association between datatokens and text tokens.
We investigate data-text alignment at the document level and atthe sentence level, reporting results for sev-eral methodological variants as well as base-lines.
We find that log likelihood ratios pro-vide a strong basis for predicting data-textalignment.1 IntroductionMuch of NLP system building currently uses alignedparallel resources that provide examples of the in-puts to a system and the outputs it is intended to pro-duce.
In Machine Translation (MT), such resourcestake the form of sentence-aligned parallel corpora ofsource-language and target-language texts; in pars-ing and surface realisation, parse-annotated corporaof naturally occurring texts are used, where in pars-ing, the inputs are the sentences in the texts and theoutputs are the parses represented by the annotationson the sentences, and in surface realisation, the rolesof inputs and outputs are reversed.In MT parallel resources exist, and in fact are pro-duced in large quantities daily, and in some cases(e.g.
multilingual parliamentary proceedings) arepublicly available.
Moreover, even if resources arecreated specifically for system building (e.g.
NIST?sOpenMT evaluations) the cost is offset by the factthat the resulting translation system can be expectedto generalise to new domains to some extent.While parse-annotated corpora are in the first in-stance created by hand, here too, parsers and surfacerealisers built on the basis of such corpora are ex-pected to generalise beyond the immediate corpusdomain.In data-to-text generation, as in parsing, parallelresources do not occur naturally and have to be cre-ated manually.
The associated cost is, however, in-curred for every new task, as systems trained on agiven parallel data-text resource cannot be expectedto generalise beyond task and domain.
Automaticdata-text alignment methods, i.e.
automatic methodsfor creating parallel data-text resources, would beextremely useful for system building in this situa-tion, but no such methods currently exist.In MT there have been recent efforts (reviewedin the following section) to automatically producealigned parallel corpora from comparable resourceswhere texts in two different languages are about sim-ilar topics, but are not translations of each other).Taking our inspiration from this work in MT, in thispaper we investigate the feasibility of automaticallycreating aligned parallel data-text resources fromcomparable data and text resources available on theweb.
This task of automatic data-text alignment,previously unexplored as far as we are aware, is thetask of automatically aligning data records with tex-tual descriptions, such that data tokens are alignedwith the word strings that describe them.
For exam-ple, the data tokens height metres=250 might bealigned with the word string with an altitude of 250102Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 102?109,49th Annual Meeting of the Association for Computational Linguistics,Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguisticsmetres above sea level.We start in Section 2 with an overview of data-to-text generation and of related work in MT.
InSection 3 we describe our comparable data and textresources and the pre-processing methods we applyto them.
In Section 4 we provide an overview ofour unsupervised learning task and of the method-ology we have developed for it.
We then describeour methods and results for sentence selection (Sec-tion 5) and sentence-level data selection (Section 6)in more detail.
We finish with a discussion of ourresults and some conclusions (Section 7).2 Background and Related ResearchWork in data-to-text generation has involved a va-riety of different domains, including generatingweather forecasts from meteorological data (Sripadaet al, 2003), nursing reports from intensive care data(Portet et al, 2009), and museum exhibit descrip-tions from database records (Isard et al, 2003; Stocket al, 2007); types of data have included dynamictime-series data (such as meteorological or medicaldata) and static database entries (as in museum ex-hibits).The following is an example of an input/outputpair from the M-PIRO project (Androutsopoulos etal., 2005), where the input is a database record for amuseum artifact, and the output is a description ofthe artifact:creation-period=archaic-period,current-location=Un-museum-Pennsylvania,painting-techinique-used=red-figure-technique,painted-by=Eucharides, creation-time=between(500 year BC)(480 year BC)Classical kylixThis exhibit is a kylix; it was created during the archaic periodand was painted with the red figure technique by Eucharides.It dates from between 500 and 480 B.C.
and currently it is inthe University Museum of Pennsylvania.While data and texts in the three example domainscited above do occur naturally, two factors meanthey cannot be used directly as target corpora ortraining data for building data-to-text generationsystems: one, most are not freely available to re-searchers (e.g.
by simply being available on theWeb), and two, more problematically, the correspon-dence between inputs and outputs is not as directas it is, say, between a source language text and itstranslation.
In general, naturally occurring resourcesof data and related texts are not parallel, but aremerely what has become known as comparable inthe MT literature, with only a subset of data havingcorresponding text fragments, and other text frag-ments having no obvious corresponding data items.Moreover, data transformations may be necessarybefore corresponding text fragments can be identi-fied.In this paper we look at the possibility of automat-ically identifying parallel data-text fragments fromcomparable corpora in the case of data-to-text gen-eration from static database records.
Such a paral-lel data-text resource could then be used to train anexisting data-to-text generation system, or even tobuild a new statistical generator from scratch, e.g.using techniques from statistical MT (Belz and Kow,2009).In statistical MT, the expense of manually creat-ing new parallel MT corpora, and the need for verylarge amounts of parallel training data, has led to asizeable research effort to develop methods for auto-matically constructing parallel resources.
This worktypically starts by identifying comparable corpora.Much of it has focused on identifying word trans-lations in comparable corpora, e.g.
Rapp?s approachwas based on the simple and elegant assumption thatif words Af and Bf have a higher than chance co-occurrence frequency in one language, then two ap-propriate translations Ae and Be in another languagewill also have a higher than chance co-occurrencefrequency (Rapp, 1995; Rapp, 1999).
At the otherend of the spectrum, Resnik and Smith (2003) searchthe Web to detect web pages that are translations ofeach other.
Other approaches aim to identify pairsof sentences (Munteanu and Marcu, 2005) or sub-sentential fragments (Munteanu and Marcu, 2006)that are parallel within comparable corpora.The latter approach is particularly relevant to ourwork.
Munteanu and Marcu start by translating eachdocument in the source language (SL) word for wordinto the target language (TL).
The result is given toan information retrieval (IR) system as a query, andthe top 20 results are retained and paired with thegiven SL document.
They then obtain all sentencepairs from each pair of SL and TL documents, anddiscard those sentence pairs that have only a small103number of words that are translations of each other.To the remaining sentences they then apply a frag-ment detection method which tries to distinguish be-tween source fragments that have a translation on thetarget side, and fragments that do not.The biggest difference between the MT situationand the data-to-text generation situation is that in theformer, sentence-aligned parallel resources exist andcan be used as a starting point.
E.g.
Munteanu andMarcu use an existing parallel Romanian-Englishcorpus to (automatically) create a lexicon which isthen used in various ways in their method.
In data-to-text generation we have no analogous resources tohelp us get started.
The approach to data-text align-ment described in this paper therefore uses no priorknowledge, and all our learning methods are unsu-pervised.3 Data and Texts about British HillsAs a source of data, we use the Database of BritishHills (BHDB) created by Chris Crocker,1 version11.3, which contains measurements and other in-formation about 5,614 British hills.
We add someinformation to the BHDB records by performing re-verse geocoding via the Google Map API2 which al-lows us to convert latitude and longitude informa-tion from the hills database into country and regionnames.
We add the latter to each database record.On the text side, we use Wikipedia articles inthe WikiProject British and Irish Hills (retrieved on2009-11-09).
At the time of retrieval there were 899pages covered by this WikiProject, 242 of whichwere of quality category B or above.33.1 Aligning database entries with documentsGiven that different hills can share the same name,and that the same hill can have several differentnames and spellings, matching up the data records inthe BHDB with articles in Wikipedia is not entirelytrivial.
The method we use is to take a given hill?sname from the BHDB record and to perform a searchof Wikipedia with the hill?s name as a search term,using the Mediawiki API.
We then pair up the BHDB1http://www.biber.fsnet.co.uk2http://code.google.com/apis/maps/3B = The article is mostly complete and without major is-sues, but requires some further work.k-name v-name-Beacon_Fellk-area v-area-Lakes:_S_Fellsk-height-metres v-height-metres-255k-height-feet v-height-feet-837k-feature v-feature-cairnk-classification v-classification-WOk-classification v-classification-Huk-locality v-locality-Skelwithk-admin-area-level1 v-admin-area-level1-Englandk-admin-area-level2 v-admin-area-level2-Cumbriak-country v-country-United_KingdomFigure 1: Result of preprocessing BHDB record for Bea-con Fell.record with the Wikipedia article returned as the topsearch result.We manually evaluated the data-text pairsmatched by this method, scoring each pairgood/unsure/bad.
We found that 759 pairs out of 899(the number of Wikipedia articles in the WikiPro-ject British and Irish Hills at the time of retrieval),or 84.4%, were categorised ?good?
(i.e.
they hadbeen matched correctly), a further 89 pairs (9.8%)were categorised ?unsure?, and the remainder was awrong match.
This gave us a corpus of 759 correctlymatched data record/text pairs to work with.We randomly selected 20 of the data record/textpairs for use as a development set to optimise mod-ules on, and another 20 pairs for use as a test set, forwhich we did not compute scores until the methodswere finalised.
We manually annotated the 40 textsin the development and test sets to mark up whichsubsets of the data and which text substrings cor-respond to each other for each sentence (indicatingparallel fragments as shown at the bottom of Fig-ure 2).3.2 Pre-processing of data records and textsDatabase records: We perform three kinds of pre-processing on the data fields of the BHDB databaserecords: (1) deletion; (2) structure flattening, and(3) data conversion including the reverse geocodingmentioned above (the result of these preprocessingsteps for the English hill Beacon Fell can be seen inFigure 1).Furthermore, for each data field key = valuewe separate out key and value, prefixing the keywith k- and the value with v-key (e.g.
v-area andk-area-Berkshire).
Each data field is thus con-104verted into two ?data tokens?.Texts: For the texts, we first strip out Wikipediamark-up to yield text-only versions.
We then per-form sentence splitting and tokenisation (with ourown simple tools).
Each text thus becomes a se-quence of strings of ?text tokens?.4 Task and Methodology OverviewOur aim is to automatically create aligned data-textresources where database records are paired withdocuments, and in each document, strings of wordtokens are aligned with subsets of data tokens fromthe corresponding database record.
The first twoitems shown in Figure 2 are the text of the Wikipediaarticle and the BHDB record about Black Chew Head(the latter cut down to the fields we actually useand supplemented by the administrative area infor-mation from reverse geocoding).
The remainder ofthe figure shows fragments of text paired with sub-sets of data fields that could be extracted from thetwo comparable inputs.How to get from a collection of texts and a sepa-rate but related collection of database records, to theparallel fragments shown at the bottom of Figure 2is in essence the task we address.
In order to do thisautomatically, we identify the following steps (thelist includes, for the sake of completeness, the datarecord/document pairing and pre-processing meth-ods from the previous section):1.
Identify comparable data and text resources andpair up individual data records and documents(Section 3).2.
Preprocess data and text, including e.g.
tokeni-sation and sentence splitting (Section 3.2).3.
Select sentences that are likely to contain wordstrings that correspond to (?realise?)
any datafields (Section 5).4.
For each sentence selected in the previous step,select the subset of data tokens that are likely tobe realised by the word strings in the sentence(Section 6).5.
Extract parallel fragments (future work).5 Sentence SelectionThe Wikipedia articles about British Hills in our cor-pus tend to have a lot of text in them for which thecorresponding entry in BHDB contains no matchingdata.
This is particularly true of longer articles aboutmore well-known hills such as Ben Nevis.
The ar-ticle about the latter, for example, contains sectionsabout the name?s etymology, the geography, geol-ogy, climate and history, and even a section about theBen Nevis Distillery and another about ships namedafter the hill, none of which the BHDB entry for BenNevis contains any data about.
The task of sentenceselection is to rule out such sections, and pick outthose sentences that are likely to contain text that canbe aligned with data.
Using the example in Figure 2,the aim would be to select the first two sentencesonly.Our sentence selection method consists of (i) esti-mating the strength of association between data andtext tokens (Section 5.1); and (ii) selecting thosesentences for further consideration that have suf-ficiently strong and/or numerous associations withdata tokens (Section 5.2).5.1 Computing positive and negativeassociations between data and textWe measure the strength of association between datatokens and text tokens using log-likelihood ratioswhich have been widely used for this sort of pur-pose (especially lexical association) since they wereintroduced to NLP (Dunning, 1993).
They were e.g.used by Munteanu & Marcu (2006) to obtain a trans-lation lexicon from word-aligned parallel texts.We start by obtaining counts for the number oftimes each text token w co-occurs with each datatoken d, the number of times w occurs without d be-ing present, the number of times d occurs without w,and finally, the number of times neither occurs.
Co-occurrence here is at the document/data record level,i.e.
a data token and a text token co-occur if they arepresent in the same document/data record pair (pairsas produced by the method described in Section 3).This allows us to compute log likelihood ratios forall data-token/text-token pairs, using one of the G2formulations from Moore (2004) which is shown inslightly different representation in Figure 3.
The re-sulting G2 scores tell us whether the frequency withwhich a data token d and a text token w co-occurdeviates from that expected by chance.If the G2 score for a given (d,w) pair is greaterthan their joint probability p(d)p(w), then the asso-105Wikipedia text:Black Chew Head is the highest point ( or county top ) of Greater Manchester , and forms part of the Peak District , in northern England .
Lyingwithin the Saddleworth parish of the Metropolitan Borough of Oldham , close to Crowden , Derbyshire , it stands at a height of 542 metres abovesea level .
Black Chew Head is an outlying part of the Black Hill and overlooks the Chew Valley , which leads to the Dovestones Reservoir .Entry from Database of British Hills:name area height m height ft feature classification top locality adm area1 adm area2 countryBlack Peak 542 1778 fence Dewey Greater Glossop England Derbyshire UKChew Head District ManchesterParallel fragments:name area top adm area1 adm area2Black Chew Head Peak District Greater Manchester England DerbyshireBlack Chew Head is the highest point ( or county top) of Greater Manchester , and forms part of the PeakDistrict , in northern England .height (m)542 it stands at a height of 542 metres above sea level .Figure 2: Black Chew Head: Wikipedia article, entry in British Hills database (the part of it we use), and parallelfragments that could be extracted.ciation is taken to be positive, i.e.
w is likely to bepart of a realisation of d, otherwise the associationis taken to be negative, i.e.
w is likely not to be partof a realisation of d.Note that we use the notation G2+ below to denotea G2 score which reflects a positive association.5.2 Selecting sentences on the basis ofassociation strengthIn this step, we consider each sentence s in turn.
Weignore those text tokens that have only negative asso-ciations with data tokens.
For each of the remainingtext tokens ws in s we obtain maxg2score(ws), itshighest G2+ score with any data token d in the set Dof data tokens in the database record:maxg2score(ws) = argmaxd?DG2+(d,ws)We then use these scores in two different ways toselect sentences for further processing:1.
Thresholding: Select all sentences that have atleast one text token w with maxg2score(w) >t, where t is a given threshold.2.
Greater-than-the-mean selection: Select allsentences whose mean maxg2score (com-puted over all text tokens with positive associ-ation in the sentence) is greater than the meanof mean maxg2scores (computed over all sen-tences in the corpus).The reason why we are not interested in negative as-sociations in sentence selection is that we want toidentify those sentences that are likely to contain atext fragment of interest (characterised by high pos-itive association scores), and such sentences maywell also contain material unlikely to be of interest(characterised by negative association scores).5.3 ResultsTable 1 shows the results for sentence selection, interms of Precision, Recall and F1 Scores.
In addi-tion to the two methods described in the precedingsection, we computed two baselines.
Baseline 1 se-lects just the first sentence, which yields a Precisionof 1 and a Recall of 0.141 for the test set (0.241 forthe development set), indicating that in the manuallyaligned data, the first sentence is always selected andthat less than a quarter of sentences selected are firstsentences.
Baseline 2 selects all sentences whichyields a Recall of 1 and a Precision of 0.318 for thetest set (0.377 for the development set), indicatingthat around one third of all sentences were selectedin the manually aligned data.Greater-than-the-mean selection roughly evensout Recall and Precision scores, with an F1 Scoreabove both baselines.
As for thresholded selection,applying thresholds t < 10 results in all sentencesbeing selected (hence the same R/P/F1 scores as forBaseline 2).4 Very high thresholds (500+) result in4This ties in with Moore?s result confirming previous anec-106G2(d,w) = 2N(p(d,w)log p(d,w)p(d)p(w) + p(d,?w)logp(d,?w)p(d)p(?w) + p(?d,w)logp(?d,w)p(?d)p(w) + p(?d,?w)logp(?d,?w)p(?d)p(?w))Figure 3: Formula for computing G2 from Moore (2004) (N is the sample size).Development Set Test SetSelection Method P R F1 P R F11st sentence only (Baseline 1) 1.000 0.241 0.388 1.000 0.141 0.247All sentences (Baseline 2) 0.377 1.000 0.548 0.318 1.000 0.483Greater-than-the-mean selection 0.516 0.590 0.551 0.474 0.634 0.542Thresholded selection t = 60 0.487 0.928 0.639 0.423 0.965 0.588Table 1: Sentence selection results in terms of Precision, Recall and F1 Score.very high Precision (> .90) with Recall droppingbelow 0.15.
In the table, we show just the thresholdthat achieved the highest F1 Score on the develop-ment set (t = 60).Selecting a threshold on the basis of highest F1Score (rather than, say, F0.5) in our case means weare favouring Recall over Precision, the intuition be-ing that at this stage it is more important not to losesentences that are likely to have useful realisations inthem (than it is to get rid of sentences that are not).6 Data SelectionFor data selection, the aim is to select, for each sen-tence remaining after sentence selection, the subsetof data tokens that are realised by (some part of) thesentence.
In terms of Figure 2, the aim would be toselect for each of sentence 1 and 2 the data tokenswhich are shown next to the fragment(s) extractedfrom it at the bottom of Figure 2.
Looked at anotherway, we want to get rid of any data tokens that arenot likely to be realised by any part of the sentencethey are paired with.We preform sentence selection separately for eachsentence s, obtaining the subset Ds of data tokenslikely to be realised by s, in one of the followingtwo ways:1.
Individual selection: Retain all and only those datatokens that have a sufficiently strong positive asso-ciation with at least one text token ws:Ds = {d??
?ws(G2+(d,ws) > t)}dotal evidence that G2 scores above 10 are a reliable indicationof significant association (Moore, 2004, p. 239).2.
Pairwise selection: Consider each pair of key andvalue data tokens dki , dvi that were originally de-rived from the same data field fi.
Retain all andonly those pairs dki , dvi where either dki or dvi has asufficiently strong association with at least one texttoken:Ds ={dki , dvi???
?wsj(G2+(dki , wsj ) > t)?
?wsm(G2+(dvi , wsm) > t)}Note that while previously each sentence in a textwas associated with the same set of data tokens (theoriginal complete set), after data selection each sen-tence is associated with its own set of data tokenswhich may be smaller than the original set.If data selection produces an empty data token setDs for a given sentence s, then s, along with its datatoken set Ds, are removed from the set of pairs ofdata token set and sentence.We evaluate data selection for the baseline of se-lecting all sentences, and the above two methods incombination with different thresholds t. As the eval-uation measure we use the Dice coefficient (a mea-sure of set similarity), computed at the documentlevel between (i) the union D of all sentence-levelsets of data tokens selected by a given method and(ii) the corresponding reference data token set DR,i.e.
the set of data tokens in the manual annotationsof the same text in the development/test data.
Diceis defined as follows:Dice(D,DR) = 2|D ?DR||D|+ |DR|Table 6 shows results for the baseline and individualand pairwise data selection, on the development set107Sentence selection methodGreater-than-the-mean Thresholded, t = 60 All-sentences 1st-sentenceDevSet All data tokens 0.666 0.666 0.666 0.666Individual selection t = 0: 0.666 t = 0: 0.666 t = 0: 0.666 t = 0: 0.666Pairwise selection t = 19: 0.706 t = 18: 0.709 t = 18: 0.717 t = 1: 0.697TestSet All data tokens 0.716 0.748 0.748 0.748Individual selection t = 0: 0.716 t = 0: 0.748 t = 0: 0.748 t = 0: 0.748Pairwise selection t = 19: 0.751 t = 18: 0.777 t = 18: 0.775 t = 1: 0.767Table 2: Data selection results in terms of Dice coefficient.
Results shown for data selection methods preceded bydifferent sentence selection methods.
(top half of the table), and on the test set (bottomhalf).
In each case we show results for the givendata selection method applied after each of the fourdifferent sentence selection methods described inSection 5: greater-than-the-mean, thresholded witht = 60, and the first-sentence-only and all-sentencesbaselines (these index the columns).Again, we optimised the two non-baseline meth-ods on the development set, finding the best thresh-old t separately for each combination of a givendata selection method with a given sentence selec-tion method.
This yielded the t values shown in thecells in the table.Looking at the results, selecting data tokens indi-vidually (second row in each half of Table 6) cannotimprove Dice scores compared to leaving the origi-nal data token set in place (first row); this is the caseacross all four sentence selection methods.
The pair-wise data selection method (third row) achieves thebest results, although it does not appear to make areal difference whether or not sentence selection isapplied prior to data selection.7 ConclusionIn this paper we have reported our work to dateon data-text alignment, a previously unexploredproblem as far as we are aware.
We looked atalignment of two comparable resources (one a col-lection of data records about British Hills, theother a collection of texts about British Hills) atthe data record/document level, where our simplesearch-based method achieved an accuracy rate of84%.
Next we looked at alignment at the datarecord/sentence level.
Here we obtained a best F1score of 0.588 for sentence selection and a best meanDice score of 0.777 for data selection.The best performing methods described here pro-vide a good basis for further development of ourparallel fragment extraction methods, in particularconsidering that the methods start from nothing andobtain all knowledge about data-text relations in acompletely unsupervised way.
Our results showthat log likelihood ratios, which have been widelyused for measuring lexical association, but were sofar unproven for the data-text situation, can providea strong basis for identifying associations betweendata and text.ReferencesI.
Androutsopoulos, S. Kallonis, and V. Karkaletsis.2005.
Exploiting owl ontologies in the multilingualgeneration of object descriptions.
In Proceedings ofthe 10th European Workshop on Natural LanguageGenerationf (ENLG?05), pages 150?155.Anja Belz and Eric Kow.
2009.
System building costvs.
output quality in data-to-text generation.
In Pro-ceedings of the 12th European Workshop on NaturalLanguage Generation, pages 16?24.E.
Briscoe, J. Carroll, and J. Graham.
2006.
The sec-ond release of the rasp system.
In Proceedings of theCOLING/ACL 2006 Interactive Presentation Sessions.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 1:61?74.A.
Isard, J. Oberlander, I. Androutsopoulos, and C. Math-eson.
2003.
Speaking the users?
languages.
IEEE In-telligent Systems Magazine: Special Issue ?Advancesin Natural Language Processing, 18(1):40?45.Robert C. Moore.
2004.
On log-likelihood-ratios andthe significance of rare events.
In Proceedings of the9th Converence on Empirical Methods in Natural Lan-guage Processing (EMNLP?04), pages 333?340.Dragos Munteanu and Daniel Marcu.
2005.
Improvingmachine translation performance by exploiting non-parallel corpora.
Computational Linguistics, 31:477?504.108Dragos Stefan Munteanu and Daniel Marcu.
2006.
Ex-tracting parallel sub-sentential fragments from non-parallel corpora.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th annual meeting of the Association for Compu-tational Linguistics (COLING-ACL?06), pages 81?88,Morristown, NJ, USA.
Association for ComputationalLinguistics.F.
Portet, E. Reiter, A. Gatt, J.
Hunter, S. Sripada,Y.
Freer, and C. Sykes.
2009.
Automatic generation oftextual summaries from neonatal intensive care data.Artificial Intelligence, 173:789?816.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the 33rd annualmeeting on Association for Computational Linguistics,pages 320?322, Morristown, NJ, USA.
Association forComputational Linguistics.Reinhard Rapp.
1999.
Automatic identification of wordtranslations from unrelated english and german cor-pora.
In Proceedings of the 37th annual meeting of theAssociation for Computational Linguistics on Compu-tational Linguistics, pages 519?526, Morristown, NJ,USA.
Association for Computational Linguistics.Philip Resnik and Noah Smith.
2003.
The web as a par-allel corpus.
Computational Linguistics, 29:349?380.S.
Sripada, E. Reiter, J.
Hunter, and J. Yu.
2003.
Ex-ploiting a parallel text-data corpus.
In Proceedings ofCorpus Linguistics 2003, pages 734?743.Oliviero Stock, Massimo Zancanaro, Paolo Busetta adnCharles Callaway, Anbtonio Kru?ger, Michael Kruppa,Tsvi Kuflik, Elena Not, and Cesare Rocchi.
2007.Adaptive, intelligent presentation of information forthe museum visitor in PEACH.
User Modeling andUser-Adapted Interaction, 17(3):257?304.109
