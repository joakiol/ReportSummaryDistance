Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 148?151,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsNon-Expert Evaluation of Summarization Systems is RiskyDan GillickUniversity of California, BerkeleyComputer Science Divisiondgillick@cs.berkeley.eduYang LiuUniversity of Texas, DallasDepartment of Computer Scienceyangl@hlt.utdallas.eduAbstractWe provide evidence that intrinsic evalua-tion of summaries using Amazon?s Mechan-ical Turk is quite difficult.
Experiments mir-roring evaluation at the Text Analysis Con-ference?s summarization track show that non-expert judges are not able to recover systemrankings derived from experts.1 IntroductionAutomatic summarization is a particularly difficulttask to evaluate.
What makes a good summary?What information is relevant?
Is it possible to sepa-rate information content from linguistic quality?Besides subjectivity issues, evaluation is time-consuming.
Ideally, a judge would read the originalset of documents before deciding how well the im-portant aspects are conveyed by a summary.
A typ-ical 10-document problem could reasonably involve25 minutes of reading or skimming and 5 more min-utes for assessing a 100-word summary.
Since sum-mary output can be quite variable, at least 30 top-ics should be evaluated to get a robust estimate ofperformance.
Assuming a single judge evaluates allsummaries for a topic (more redundancy would bebetter), we get a rough time estimate: 17.5 hours toevaluate two systems.Thus it is of great interest to find ways of speedingup evaluation while minimizing subjectivity.
Ama-zon?s Mechanical Turk (MTurk) system has beenused for a variety of labeling and annotation tasks(Snow et al, 2008), but such crowd-sourcing has notbeen tested for summarization.We describe an experiment to test whether MTurkis able to reproduce system-level rankings thatmatch expert opinion.
Unlike the results of othercrowd-sourcing annotations for natural languagetasks, we find that non-expert judges are unable toprovide expert-like scores and tend to disagree sig-nificantly with each other.This paper is organized as follows: Section 2 in-troduces the particular summarization task and datawe use in our experiments; Section 3 describes thedesign of our Human Intelligence Task (HIT).
Sec-tion 4 shows experimental results and gives someanalysis.
Section 5 reviews our main findings andprovides suggestions for researchers wishing to con-duct their own crowd-sourcing evaluations.2 TAC Summarization TaskTopic: Peter JenningsDescription: Describe Peter Jennings?
lung cancer and itseffects.Reference: Peter Jennings?s announcement April 5, 2005,that he had lung cancer left his colleagues at ABC News sad-dened and dismayed.
He had been ?World News Tonight?anchorman since 1983.
By the end of the week, ABC had re-ceived 3,400 e-mails offering him prayers and good wishes.A former heavy smoker, Jennings had not been well for sometime and was unable to travel abroad to cover foreign events.However, his diagnosis came as a surprise to him.
ABC an-nounced that Jennings would continue to anchor the newsduring chemotherapy treatment, but he was unable to do so.Table 1: An example topic and reference summary fromthe TAC 2009 summarization task.Our data comes from the submissions to the TextAnalysis Conference (TAC) summarization track in2009 (Dang, 2009).
The main task involved 44query-focused topics, each requiring a system toproduce a 100-word summary of 10 related newsdocuments.
Experts provided four reference sum-maries for each topic.
Table 1 shows an example.148Score Difference0 1 2 3 meanOQ 119 92 15 0 0.54LQ 117 82 20 7 0.63Table 2: Identical summaries often were given differentscores by the same expert human judge at TAC 2009.Counts of absolute score differences are shown for Over-all Quality (OQ) and Linguistic Quality (LQ).2.1 Agreement and consistencyIn the official TAC evaluation, each summary wasjudged by one of eight experts for ?Overall Quality?and ?Linguistic Quality?
on a 1 (?very poor?)
to 10(?very good?)
scale.
Unfortunately, the lack of re-dundant judgments means we cannot estimate inter-annotator agreement.
However, we note that out ofall 4576 submitted summaries, there are 226 pairsthat are identical, which allows us to estimate anno-tator consistency.
Table 2 shows that an expert an-notator will give the same summary the same scorejust over half the time.2.2 Evaluation without source documentsOne way to dramatically speed up evaluation is touse the experts?
reference summaries as a gold stan-dard, leaving the source documents out entirely.This is the idea behind automatic evaluation withROUGE (Lin, 2004), which measures ngram over-lap with the references, and assisted evaluation withPyramid (Nenkova and Passonneau, 2004), whichmeasures overlap of facts or ?Semantic ContentUnits?
with the references.
The same idea has alsobeen employed in various manual evaluations, forexample by Haghighi and Vanderwende (2009), todirectly compare the summaries of two different sys-tems.
The potential bias introduced by such abbre-viated evaluation has not been explored.3 HIT designThe overall structure of the HIT we designed forsummary evaluation is as follows: The worker isasked to read the topic and description, and thentwo reference summaries (there is no mention of thesource documents).
The candidate summary appearsnext, followed by instructions to provide scores be-tween 1 (very poor) and 10 (very good) in each cat-egory1.
Mouse-over on the category names provides1Besides Overall Quality and Linguistic Quality, we includeInformation Content, to encourage judges to distinguish be-extra details, copied with slight modifications fromDang (2007).Our initial HIT design asked workers to performa head-to-head comparison of two candidate sum-maries, but we found this unsatisfactory for a num-ber of reasons.
First, many of the resulting scoresdid not obey the transitive property: given sum-maries x, y, and z, a single worker showed a pref-erence for y > x and z > y, but also x > z.Second, while this kind of head-to-head evalua-tion may be useful for system development, we arespecifically interested here in comparing non-expertMTurk evaluation with expert TAC evaluation.We went through a few rounds of revisions to thelanguage in the HIT after observing worker feed-back.
Specifically, we found it was important to em-phasize that a good summary not only responds tothe topic and description, but also conveys the infor-mation in the references.3.1 Quality controlOnly workers with at least a 96% HIT approval rat-ing2 were allowed access to this task.
We moni-tored results manually and blocked workers (reject-ing their work) if they completed a HIT in under 25seconds.
Such suspect work typically showed uni-form scores (usually all 10s).
Nearly 30% of HITswere rejected for this reason.To encourage careful work, we included this notein our HITs: ?High annotator consistency is impor-tant.
If the scores you provide deviate from the av-erage scores of other annotators on the same HIT,your work will be rejected.
We will award bonusesfor particularly good work.?
We gave a few smallbonuses ($0.50) to workers who left thoughtful com-ments.3.2 CompensationWe experimented with a few different compensationlevels and observed a somewhat counter-intuitive re-sult.
Higher compensation ($.10 per HIT) yieldedlower quality work than lower compensation ($.07per HIT), judging by the number of HITs we re-jected.
It seems that lower compensation attractsworkers who are less interested in making money,and thus willing to spend more time and effort.There is a trade-off, though, as there are fewer work-ers willing to do the task for less money.tween content and readability.2MTurk approval ratings calculated as the fraction of HITsapproved by requesters.149Sys TAC MTurkOQ LQ OQ LQ CA 5.16 5.64 7.03 7.27 7.27B 4.84 5.27 6.78 6.97 6.78C 4.50 4.93 6.51 6.85 6.49D 4.20 4.09 6.15 6.59 6.50E 3.91 4.70 6.19 6.54 6.58F 3.64 6.70 7.06 7.78 6.56G 3.57 3.43 5.82 6.33 6.28H 3.20 5.23 5.75 6.06 5.62Table 3: Comparison of Overall Quality (OQ) and Lin-guistic Quality (LQ) scores between the TAC and MTurkevaluations.
Content (C) is evaluated by MTurk workersas well.
Note that system F is the lead baseline.4 Experiments and AnalysisTo assess how well MTurk workers are able to em-ulate the work of expert judges employed by TAC,we chose a subset of systems and analyze the resultsof the two evaluations.
The systems were chosen torepresent the entire range of average Overall Qual-ity scores.
System F is a simple lead baseline, whichgenerates a summary by selecting the first sentencesup to 100 words of the most recent document.
Therest of the systems were submitted by various trackparticipants.
The MTurk evaluation included two-times redundancy.
That is, each summary was eval-uated by two different people.
The cost for the fullevaluation, including 44 topics, 8 systems, and 2xredundancy, at $.07 per HIT, plus 10% commissionfor Amazon, was $55.Table 3 shows average scores for the two evalu-ations.
The data suggest that the MTurk judges arebetter at evaluating Linguistic Quality than Contentor Overall Quality.
In particular, the MTurk judgesappear to have difficulty distinguishing LinguisticQuality from Content.
We will defend these claimswith more analysis, below.4.1 Worker variabilityThe first important question to address involves theconsistency of the workers.
We cannot compareagreement between TAC and MTurk evaluations, butthe MTurk agreement statistics suggest considerablevariability.
In Overall Quality, the mean score differ-ence between two workers for the same HIT is 2.4(the standard deviation is 2.0).
The mean is 2.2 forLinguistic Quality (the standard deviation is 1.5).In addition, the TAC judges show more similaritywith each other?as if they are roughly in agreementabout what makes a good summary.
We computeeach judge?s average score and look at the standarddeviation of these averages for the two groups.
TheTAC standard deviation is 1.0 (ranging from 3.0 to6.1), whereas the MTurk standard deviation is 2.3(ranging from 1.0 to 9.5).
Note that the averagenumber of HITs performed by each MTurk workerwas just over 5.Finally, we can use regression analysis to showwhat fraction of the total score variance is capturedby judges, topics, and systems.
We fit linear modelsin R using binary indicators for each judge, topic,and system.
Redundant evaluations in the MTurkset are removed for unbiased comparison with theTAC set.
Table 4 shows that the differences betweenthe TAC and MTurk evaluations are quite striking:Taking the TAC data alone, the topics are the majorsource of variance, whereas the judges are the majorsource of variance in the MTurk data.
The systemsaccount for only a small fraction of the variance inthe MTurk evaluation, which makes system rankingmore difficult.Eval Judges Topics SystemsTAC 0.28 0.40 0.13MTurk 0.44 0.13 0.05Table 4: Linear regression is used to model Overall Qual-ity scores as a function of judges, topics, and systems, re-spectively, for each data set.
The R2 values, which givethe fraction of variance explained by each of the six mod-els, are shown.4.2 Ranking comparisonsThe TAC evaluation, while lacking redundant judg-ments, was a balanced experiment.
That is, eachjudge scored every system for a single topic.
Thesame is not true for the MTurk evaluation, and asa result, the average per-system scores shown inTable 3 may be biased.
As a result, and becausewe need to test multiple system-level differences si-multaneously, a simple t-test is not quite sufficient.We use Tukey?s Honestly Significant Differences(HSD), explained in detail by Yandell (1997), to as-sess statistical significance.Tukey?s HSD test computes significance intervalsbased on the range of the sample means rather thanindividual differences, and includes an adjustment tocorrect for imbalanced experimental designs.
The Rimplementation takes as input a linear model, so we150Eval RankingTAC (OQ) A B C DA EB FC GC HDMTurk (OQ) F A B C EF GF DB HBTAC (LQ) F AF BF HF CF EA DB GEMTurk (LQ) F A BF CF DF EF HC GCMTurk (C) A B E F D C GA HDTable 5: Systems are shown in rank order from highest(left) to lowest (right) for each scoring metric: Over-all Quality (OQ), Linguistic Quality (LQ), and Content(C).
The superscripts indicate the rightmost system thatis significantly different (at 95% confidence) accordingto Tukey?s HSD test.model scores using binary indicators for (J)udges,(T)opics, and (S)ystems (see equation 1), and mea-sure significance in the differences between systemcoefficients (?k).score = ?+?i?iJi +?j?jTj +?k?kSk (1)Table 5 shows system rankings for the two evalu-ations.
The most obvious discrepancy between theTAC and MTurk rankings is system F, the base-line.
Both TAC and MTurk judges gave F the high-est scores for Linguistic Quality, a reasonable resultgiven its construction, whereas the other summariestend to pull sentences out of context.
But the MTurkjudges also gave F the highest scores in OverallQuality, suggesting that readability is more impor-tant to amateur judges than experts, or at least easierto identify.
Content appears the most difficult cate-gory for the MTurk judges, as few significant scoredifferences emerge.
Even with more redundancy, itseems unlikely that MTurk judges could produce aranking resembling the TAC Overall Quality rank-ing using this evaluation framework.5 DiscussionThrough parallel evaluations by experts at TAC andnon-experts on MTurk, we have shown two mainresults.
First, as expected, MTurk workers pro-duce considerably noisier work than experts.
Thatis, more redundancy is required to achieve statisti-cal significance on par with expert judgments.
Thisfinding matches prior work with MTurk.
Second,MTurk workers are unlikely to produce a score rank-ing that matches expert rankings for Overall Quality.This seems to be the result of some confusion in sep-arating content from readability.What does this mean for future evaluations?
Ifwe want to assess overall summary quality?that is,balancing content and linguistic quality like expertjudges do?we will need to redesign the task fornon-experts.
Perhaps MTurk workers will be bet-ter able to understand Nenkova?s Pyramid evaluation(2004), which is designed to isolate content.
Extrin-sic evaluation, where judges use the summary to an-swer questions derived from the source documentsor the references, as done by Callison-Burch forevaluation of Machine Translation systems (2009),is another possibility.Finally, our results suggest that anyone conduct-ing an evaluation of summarization systems usingnon-experts should calibrate their results by askingtheir judges to score summaries that have alreadybeen evaluated by experts.AcknowledgmentsThanks to Benoit Favre for discussing the evaluationformat and to the anonymous reviewers for helpful,detailed feedback.ReferencesC.
Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazons Me-chanical Turk.
Proceedings of EMNLP.H.T.
Dang.
2007.
Overview of DUC 2007.
In Proceed-ings of the Document Understanding Conference.H.T.
Dang.
2009.
Overview of the TAC 2009 opinionquestion answering and summarization tasks.
In Pro-ceedings of Text Analysis Conference (TAC 2009).A.
Haghighi and L. Vanderwende.
2009.
Exploring con-tent models for multi-document summarization.
InProceedings of HLT-NAACL.C.Y.
Lin.
2004.
Rouge: A package for automatic evalu-ation of summaries.
In Proceedings of the Workshop:Text Summarization Branches Out.A.
Nenkova and R. Passonneau.
2004.
Evaluating con-tent selection in summarization: The pyramid method.In Proceedings of HLT-NAACL.R.
Snow, B. O?Connor, D. Jurafsky, and A.Y.
Ng.
2008.Cheap and fast?but is it good?
: Evaluating non-expert annotations for natural language tasks.
In Pro-ceedings of EMNLP.B.S.
Yandell.
1997.
Practical data analysis for designedexperiments.
Chapman & Hall/CRC.151
