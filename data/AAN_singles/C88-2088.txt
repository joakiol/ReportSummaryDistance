STRATEGIES FOR EFFECTIVE PARAPHRASINGMarie MeteerVarda ShakedBBN Laboratories, Inc.10 Moulton StreetCambridge, Massachusetts 02238USAAB S'll'RAC'I'in this paper we present a new dimension to paraphrasing textin which characteristics of the original text motivate strategies foreffective pacaphrasing.
Our system combines two existing robustcomponents: the IRIJS-.II natural language underst~mding systemand the SPOKESMAN generation system.
We describe thearchitectur(: of the system and enhancements made to thesecomponents ofacilitate paraphrasing.
We particularly look at howlevels of representation in these two systems are used by specialistsin the paraphraser which define potential problems and paraphrasingstrategies.
Finally, we look at the role of paraphrasing in acooperative dialog system.
We will focus here on paraphrasing inthe coutext of natural language interfaces and particularly on howmultiple int::rpretations introduced by various kinds of ambiguitycan be contrasted in paraphrases u ing both sentence structure andhighlighting and folmating the text itself.1.
~\[NTRODUCTION lWhile tecimieally paraphrasing is simply the task of restatingthe meaning of a text in a different form, it is crucial to consider thepurpose of the paraphrase in order to motivate particular strategiesfor changinl: the text.
If the t)oint of the paraphrase is to clarify theoriginal texi, its in a natural language (NL) interface to a database(DB) or cx~rt  system application, then disambiguating the que Wand choosing more precise lcxical items (perhaps closer to thestructure of the actual Dt3, expert system, o1' other underlyingapplication) are essential strategies.
If the point is to summarizeinformation, then strategies for evaluating the relative importance ofthe information presenlcd in the text m'e necessary.
If the point ismerely to re:;tate the text ~t_.f~r~l lil2 than the original, perhaps merelyto exercise the system, then one must use strategies which considerwhat structures and lexical items were actually found by the parser.Oar motivation for work on strategies for effective paraphrasingcomes front the recent availablility of NI, interfaces as commercialproducts.
As the underlying systems that a NL interface mustinteract with increase in number and sophistication, the range of NLinteractions will increase as well.
Paraphrasers developed in thepast (e.g.
McKeown's Co-op and BBN's Parlance'rMNL Interface)were all limited in that each used only a single strategy forparaphrasing regardless of what problems may have been present inthe original query.
(We diseussthese ystems in detail in Section6.)
Our approach is to develop a variety of strategies which may beemployed in different situations.
We introduce anew dimension toparaphrasing text in which characteristics of the original text plus theoverall context (inch~ding the goal of the system) mofiwtte strategiesfor effective paraphrasing.Our focus here will be on paraphrasing anlbiguous queries in aninteractive dialog system, whc~e conlrasting nmltiple interpretationsis essential.
In order to ground our discussion, we first look brieflyat a range of ambiguity types.
We then provide an overview of thearchitecture and description of the two major components: theIRUS-II'rM mlderstanding system and the Spokesman generationsystem.
We look closely at the aspects of these systems that weaugmented t0r the paraphrasiug task and provide a detailed exampleof how the system appreciates multiple interpretations and uses thatinformation to govern decision making in generation.
Next wediscuss the role of paraphrasing in a cooperative dialog system, andin the final section we conta'ast our approach with other work inparaphrasing.I We would like to Ihank Lance Ramshaw tot' his invaluable help inunderstanding dieinner workings of RUS and suggestions of where it could beaugmented forout' purposes, and Dawn MacLaughlin for her implementation ofPal~rot, the init~d versio, of our paraphraser.
We would also like tx) thank RalphWeisclmdel, D~mafis Ayuso, and David iglcDonald for their helpful comments ofd~afl.s of this paper and Lya Bates tot early inspirations.2.
PROBLEMS AND STRATEGIESAmbiguity is one of the more difficult problems to detect andcorrect.
In this section we look at three kinds of ambiguity: lexical,structural and contextual, and discuss potential strategies aparaphraser might use to eliminate the ambiguity.1) LEXICAL AMBIGUITIES ale introduced when a lexical item canrefer to more than one thing.
In the following example "Manhattan"can refer to either the borough of New York City or the ship:Wtutt is the latitude arm longitude of Manhattun?The paraphraser must appreciate he ambiguity of that noun phrase,decide how to disambiguate it, and decide how much of the contextto include in the paraphrase.
One strategey would be to repeat heentire query, disambiguating the noun phrase by using the type andname of the object:Do you mean what & the latitude atul longitude of the cityManhattanor what is the latitude and longitude of the ship Manhattan?However, if the query is long, the result could be quitecumbersome.
A different strategy, highlighting and formatting thetext to contrast he differences, can serve to direct the user'sattention to the part that is ambiguous:Do you mean list the latitude and longitude of the city Manhattanor the ship Manhattan?2) STRUCTURAL AMBIGUITIES are caused when there are multipleparses for a sentence.
Conjunction is a typical source of structuralambiguity.
Modifiers of conjoined NPs may distribute over eachNP or modify only the closest NP.
Consider, for example, thefollowfi~g query:Display the forested lu'lLv and rivers.This query has only one interpretation in which the premodifier"forested" modifies only the noun "hills".
In contrast, he followingquery has two interpretations:Display the C1 carriers and frigatesIn one interpretation, the premodifier "CI" may apply only to thenoun "carrier"; in the other, "CI" applies to both "carriers" and"frigates".
Each interpretation requires a different paraphrasestrategy.
In the case where the premodifier distributes, theambiguity may be eliminated by repeating the modifier: Disl)lay theC1 carr&rs and C1 frigates.
When it does not distribute, there arethree potential s xategies:--changing the order of the conjuncts: Display the frigates andC1 carr&rs.--hatrodueing explicit quantifiers: Display the C1 carriers and allthe frigates.--moving premodifiers to postmodifiers: Display the carrierswhich are C1 arm the frigates.3) CONTEXTUAL AMBIGUITIES are introduced when the query isunderspecified for the underlying system it is working with.
Forexample if the context includes a map and the possibility of naturallanguage or table output, the query Which carriers are C1?
couldmean either list or display.This work was supported by the Strategic Computing Program, DARPAcontract munber N000014-85-C-00016.431U nd erstanding ~ - ~ ' ~ U  n d er ly~n g~'~- - .
.~  Gone ration- ~ , /  "k~ Program ~ -"~-~- ....expression' Paraphraser - - - -~-~Translate WML to text structure ~ .
;..,~'*~~~i i~uaS~u re 1 , : , : : : : : : : : I I  .
.
.
.
.
.
\[ .
.
.
.
.
.
.
.
Surfaeq Structure I/TEXT TEXT" ~ Flow of information through the paraphraser~.1~ Flow of information through understanding and generation componentsFIGURE 1 ARCI-I1TE~E OF THE PARAPIIRASER3.
ARCHITECTUREAs tile examples above illustrate, the information eeded tonotice problems uch as ambiguity in a query is quite varied, and thestrategies needed to generate a motivated paraphrase must beemployed at various levels in the generation process.
Adistinguishing feature of our system is that it works in cooperationwith existing understanding and generation components and allowsthe paraphraser access to multiple levels of their processing.
Thismultilevel design allows the understanding system to appreciateambiguities and vagueness at lexical, structural, and contextuallevels, and the generation system to "affect he text's organization,syntactic structure, lexical items and even to format and highlight thefinal text.Figure 1 shows an overview of the architecture of the system.In this section, we first describe the understanding and generationsystems independently, focusing on how the Problem Recognizersand Paraphrasing Strategies have been incorporated into thecomponents.
We then look at the paraphraser itself and how itevolved.3.1 THE UNDERSTANDING COMPONENT:IRUS-I I(TM)IRUS- I l tm (Weischedel, et al 1987) is a robnst NLunderstanding system that interfaces to a variety of underlyingsystems, such as DB management systems, expert systems andother application programs.
It is capable of handling a very widerange of English constructions including ill-folaned ones.3.1.1 IRUS-II  - Components and Design PrincipalsIRUS-II has two major processing levels which distinguish thelinmfistic processing from the details of the particular underlyingsy~ems it is used with.
The first level, the "Front End", integratessyntactic and semantic processing.
The major domain-independent"Front End" modules include a parser and associated grammar ofEnglish, a semantic interpreter, and a subsystem for resolvinganaphora nd ellipsis.
These modules simultaneously parse anEnglish text into a syntactic structural description and construct aformal semantic representation f its meaning in a higher orderintensional logic language called the World Model Language(WML).
The syntactic processor is the RUS Parser/Grammarwhich is based on the ATN formalism.
Constants in the WML areconcepts and predicates from a hierarchical domain modelrepresented in NIKL (Moser 1983).The more domain-dependent modules of the Front End are thelexicon, domain model, and a set of semantic Interpretation Rules(IRules).
'The lexicon contains information about parts of speech,and syntactic and morphological features needed for parsing, andword and phrase substitutes (such as abbreviations).
An IRuledefines, for a word or (semantic) class of words, the semanticallyacceptable English phrases that can occm' having that word as a headof the phrase, and in addition defines the semantic interpretation fan accepted phrase.
Thus, when tile parser proposes (i.e.,TRANSMITs) an intermediate syntactic phrase structure, thesemantic interpreter uses the mules that are associated with the headof that phrase to determine whether the proposed structure isinterpretable and to specify its interpretation.
Since semanticprocessing is integrated with syntactic processing, the 1Rules serveto block a semantically anomalous phrase as soon as it is proposedby the parser.
The semantic representation f a phrase is constructedonly when the phrase is believed complete.The task of the "Back End" component of 1RUS-II is to take aWML expression and compute the correct command or set ofcommands to one or more underlying systemsin order to obtain theresult requested by tile user.
This problem is decomposed into thefollowing steps:* The WML expression is simplified and then graduallytranslated into the Application System Interface Langauge(ASlL).
* The particular underlying system or systems that need to beaccessed are identified.
* The ASIL is transformed into underlying system(s) code toexecute the query.While the constants in WML and ASIL are domain-dependent, theconstants in ASIL-to-code translation system(s) code are bothdomain dependent and underlying-system dependent.3.1.2 Ambiguity Handling by the IRUS.
I I  System -OverviewIn this section, we briefly describe how various kinds ofambiguities are currently handled in IRUS-II.
There are at least hefollowing kinds of ambiguities that may occur in natural language:Semantie ambiguity (lexical, phrasal, referring expressions),structural ambiguity, quantifier scope ambiguity and collectivereading ambiguity.
In cases of semantic ambiguity, multiple WMLsare generated from the same syntactic parse path.
For example,when a word (e.g., "Manhattan") belongs to more than onesemantic lass in the domain model (e.g, CITY, VESSEL), twoWMLs are generated from the same syntactic parse path, eachreferring to a different semantic class.
Similm'ly, premodified nouns(e.g., "Hawaii ships") generate multiple WMLs, each created as aresult of multiple IRules assigning several interpretations to therelation between the elements (e.g., "Ships whose home port isHawaii", "Ships whose destination is Hawaii", or "Ships whosecurrent location is Hawaii")./432Strnctu~al ambiguities are caused by mulliple syntactic, interprcta~ioas nd result i ,  alternative parse paths in the RUSparser/grammar.
IRUS.II identifies these ambiguities byS(?luendally attempting topm~e file text, with each attempt followinga different parse path.
Note in these cases each syntactic parse pathnmy also have multiple semantic interpretmious.3 Jo3  )t',nhance~nenk~ to \]\[RIJSo\]\[I for Ef fect ivePa raplh~oa,,~ng'lhougb \[ILliS41 ~ pmdnces multiple inteq)letations (WMLs) fora variety of ambiguous entences~ it was not originally designedwith the intent of paraphrasing those interpretations.
While eachindividual WML could be paraphrased separately, a more useflllapproach would be to combine closely related interpretations i to asingle paraphrase that highlights the contrasts between theinterpretations.
The need to keep associations between multiple:interpretations motivated file lollowing enhmmements to the IRUS--IIsystem:* P~'cd~fined ambiguity specialists that detect and annotatepotel~tial problems presented by the input text are"distributed" in the parser/grammar nd the semanticinterpreter.
For e?ample, when the parser TRANSMITs thephras,: "Manhattan" to the semantic interpreter as a head of aNI?, two semm~tic classes, CITY and VESSEL, will beasst~;iaied with that NP.
At this point, the Lexical AmbiguitySpecialist records the lexieal item "Manhattan" as theambiguity soume mid the two different classes.
* After recording the potential ambiguity source, eachambiguity specialist monitors a prcdefined sequence ofTRANSMITs associated with that source, and records thedifl:en ~nt intermediate WML expressions resulting from theseTRANSMfYs.
For exmnple, the Lexical Ambiguity Specialistxm~nitors the TRANSMITs of "Manhatten" as a head noun ofthe NP.
Ill ibis case, there will be two applicable 1Rules, onedefining "Marthattan" as a CITY attd the other defining"Manhattan" as a VESSEI.
Both interpretations arescmal~tically acceptable, resulting in two intermediate WMLs,which are then recorded by tile specialist.
Upon completionof the inlntt text, two WMLs will be created and this recordwill I~ used to annotate them with their espective differencesthat resulted fi'om a common ambiguity source.
'We look at the details of the specialists on one particular example inSection 4,3?2 Ti~e Ceneratiou ;;yslem: ~POKESMANThe Spokesman gcnetation system also has two majorcomponents: a text planner and a linguistic realization component,MUMBLE4t6 (Mercer et al 1987).
Both components are builtwithin the framework of "multilevel, description directed control"(McDonald 1983).
In this framework, decisions are organized intolevels according to the kind of reference knowledge brought to beat"(e.g.
event or argmnent structure, syntactic structnre, morphology).,At each level, a representation f the utterance is constructed which\]both captures the decisions made so ~ar and constrains the futuredecision inaldng.
The l~p~esentation at each level also serves as ritecontrol ot the mapping to the next level ~ff representation.The text plmmcr must establish what information the utteranceits to include and what wording and organization it must have inorder to insore that the information is understood with the intendedperspectives.
The intermediate l vel of representation i  thisconlponent is tile text strt~cture, which is a tree-like representation,of the orgma~zation f discourse level constituents.
The stntcture ispopulated with model level objects (i.e.
ti'om the applicationsprogram) and "discourse objects" (compositional objects created for1the particulac utterance) and the ~elations between these objects.
Thetext strnctar~ is extended incrementally in two ways:1) expanding nodes whose contents are composite objects byusing predefined templates associated with the object types(such as expanding an "event" object by making its argumentssubnodes);2) adding units into the slfuctuw at new n(?les.
The units may beselected li'om an already positioned composite unit or they maybe individuals handed m the orcheslrator by an independentlych'ivcn selection process.Once the text structure is complete, it is traversed &;pth firstbeginning with file root node.
At each node, the mapping processchooses the linguistic resource (lexical item, syntactic relation suchas restrictive modifim, etc.)
that is to realize the object which is thecontent of that node.
Templates associated with these objects definethe set of possibilities and provide procedures for building itsportion of tile next level of representation, the "message level",which is the input specification for the linguistic realizationcomponent, MUMBLE-86.The input specification to MUMBLE-86 specifies what is to besaid and constrains how it is to be said.
MUMBLE-86 handles therealization of the elements in the input specification (e.g.
choosingbetween the ships ate assigned, which are assigned, or assigneddepending on whether the linguistic ontext requires a fldl clause,postmodifier, or premodifier), the positioning of elements in the text(e.g.
choosing where to place an adverbial phrase), and thenecessary morphological operations (e.g.
subject-verb agreement).In order to make these decisions, MUMBLE-86 maintains anexplicit representation f the linguistic context in the form of an~mnotated surface structure.
Labels on positions provide bothsyntactic onstraints for choosing the appropriate phrase and adefinition of which links may be broken to add more structure.
Thisstructure is traversed epth first as it is built, guiding the furtherrealization of embedded elements and the attachment of newelements.
When a word is reached by the traversal process, it issent to the morphology process, which uses the lingusitic ontext oexecute the appropriate morphological operations.
Then the word ispassed to the word stream to be output and the traversal processcontinues through the surface structure.3.3 Parrot and PollyOur first implementation f the paraphraser was simply a parrotwhich used the output of the parser (tile WML) as input to tilegenerator.
The text planner in this case consists of a set oftranslation flmctions which build text structure and populate it wilheoml)osite objects built from WML subexpressions and theconstants in the WML (concepts and roles from IKUS-I I 'shierarchical domain model).
The translation to text structure usesboth explicit and implicit information fiom the WML.
The firstoperator in a WML represents he speech act of the utterance.
Fo*example, BRING-ABOUT indicates explicitly that the matrix clauseshould be a command and implicitly that it should be in the presenttense and the agent is the system.
The IOTA operator indicates thatthe reference isdefinite and POWER indicates it is plural.A second set of templates map these objects to the inputspecification for the linguistic omponent, determining the choice oflexical heads, argument s ructm'es, and attachment relations (such asrestrictive-modifier or clausal-adjunct).Interestingly, PARROT turned out to be a conceptual parrot,rather than a verbatim one.
For example, the phrase the bridge onthe river is interpreted as the following WML expression.
Thedomain model predicate CROSS represents he role between bridgeand river since IRUS interprets "on" in this particular context interms of the CROSS 1elation:(IOTA JX 124 BRIDGE (CROSS JX 124 (IOTA JX236 RIVER)))This is "parroted" as the bridge which crosses the river.
While insome cases this direct translation of the WML produces anacceptable phrase, in other cases the results are less desirable.
Forexample, named objects are represented by an expression of theform (IOTA van type (NAME vat none)), which, tremslated directly,would produce the river which is named Hudson.
Such phrasesmake the generated text unnecessarily cumbersome.
Our solution inPARROT was to implement an optimization at the point when thecomplex object is built and placed in the text structure that uses thename as tile head of the complex object rather than the type.
(Melish, 1987, discusses imilar optimizations in generating fromplans.
)While PARROT allowed us to establish a link from text in to textout, it is clear this aioproach is insufficient to do more sophisticatedparaphrasing.
POLLY, as we call our "smart" 1)araphraser, takesadvantage of the extra information provided by IRUS-II in order tocontrol the decision making in generation.One of the most common places in which the system mustchoose carefully which realization to use is when tile input isambiguous and the paraphrase must contrast the two meanings.
Forexample, if a semantic ambiguity is caused by an ambiguous name,33as in Where is Diego Garcia (where Diego Garcia is both asubmarine and a port), the type information must be included in theparaphrase:Do you mean where is the port Diego Garciaor the submarine Diego Garcia.Note, with the optimization of PARROT described above, thissentence could not be disamiguated.In order to generate this paraphrase contrasting the twointerpretations, the system needs to know what part is ambiguous attwo different points in the generation process: in the text plannerwhen selecting the information to include (both the type and thename) and at the final stage when the text is being output (to changethe font).
Our use of explicit active representations allows thesystem to mark the contrast only once, at the highest level, the textstructure.
This constraint is then passed through the levels and canaffect decisions at any of the lower levels.
Thus the system makesuse of the information provided by the understanding system whenit is available and ensures it will still be available when needed andwon't be considered inparts of the utterance where it is not relevant.4.
Paraphrasing Syntactic Ambiguities - an ExampleTo elucidate the description above, we will return to an earlierexample of a query with an ambiguous conjunction construction:Display all carriers and frigates in the Indian Ocean.
This sentencehas two possible interpretations:1) Display all carriers in the Indian Ocean and all frigates in theIndian Ocean.2) DLplay all frigates in the Indian Ocean at~t all the carriers.In this example we show (1) how the Problem Recognizers discoverthat there are two interpretations and what the particular differencesare; and (2) how the Paraphrasing Strategies use that information ithe translation to text structure and the generation of the paraphrase.4.1 Phase 1: The Problem RecognizersAs we discussed earlier, problem recognizing specialists havebeen embedded in the understanding system.
Here we look at theNP Conjunction Ambiguity specialist and the two parse paths thatcorrespond to the parses resulting from a NP conjunction ambiguity(see Figure 2 below).434The first task of this specialist is to annotate the parse pathwhen a NP conjunction is encountered by the parser.
In IRUS-II,when the RUS parser has completed the processing of the first NPthe frigates and the conjunction word and, it attempts (among otheralternatives) to parse the next pltrase as a NP.
At this point theConjunction Ambiguity Specialist annotates that parse path with aNP-CONJUNCTION-AMBIGUITY tag (depicted in Figure 2 with* at the first NPLIST/ state in both parse paths 1 and 2).
Thisannotation will allow the different interpretations that may resultfrom this NP conjunction to be grouped later according to theircommon ambiguity source.
(Note that if we were not using anATN, appropriate annotations could still be made using structurebuilding rules associated with the grammar rules).
The paraphrasercan then organize its paraphrases according to a group of relatedambiguous interpretation,;.
As previously stated, presenting closelyrelated interpretations simultaneously is more effective thanpresenting randomly generated paraphrases that correspond toarbitrary parse paths.The second task of the NP Conjunction Ambiguity specialist isto monitor those TRANSMITs to the semantic interpreter fliat mayresult in multiple intelpretations (WMLs) from the same source ofambiguity.
Thus, starting from when the possible ambiguity hasbeen noticed, this specialist will monitor the TRANSMITs to all themodifiers of the NPs.
In our example, the NP ConjunctionAmbiguity specialist monitors the TRANSMITs of the prepositionalphrase (PP) in the Indian Ocean to all NPs annotatexi with the NP-.CONJUNCTION-AMBIGUITY tag (TRANSMITs are illustratedwith **), which include the TRANSMITs of that PP as apostmodifer to each of the conjoined NPs (parse path 1) as well asto only the second NP (parse path 2).
Since the PP in the IndianOcean is semantically acceptable as a postmodifer in both parsepaths, two intermediate WMLs are be created:Intermediate WML- 1:(SETOF (IOTA ?JfX19 (POWER CARRIER)(UNITS.LOCATION ?J X 19 IO))(IOTA ?JX20 (POWER FRIGATE)(UNITS.LOCATION ?JX20 IO)))Intermediate WML-2:(SETOF (IOTA ?JX19 (POWER CARRIER))(IOTA ?JX20 (POWER FTGGATE)(ONITS.LOCATION ?JX20 IO)))Each intermediate WML contains a SETOF operator with twoargmnents hat represent a pair of conjoined NPs.
In IntermediateWML-1 both arguments have the UNITS.LOCATION restriction,and in Intermediate WML=2 only the second argument has thatPARSE PATH 1push\[ TRANSMIT ~L~P r~.
.
.
all carriers andnp/ nplnp nplist/ ( ~ .
!
~ s ~ ~ ~  ~ ~ n  plist)'~ush ~PO stm?ds?in the Indian OceanPARSE PATH 2pushl  .
.
.
.
.
.
.
.
.all carriers ano , ~ .~-  ..f r ioa tes  ' ' T  -~  , ~ I push.
.
.
.
_L_ p?pin the Indian Ocean* Set conjunction ambiguity tag** Conjunction ambiguity specialist monitors tagged transmits to semantic interpreterFIGURE 2 PARSE PATHSrestriction.
The NP Conjunction Ambiguity specialist annotatesthose intenn,~diate WMLs, and the parser proceeds to complete theprocessing of the inpttt ext.
In our example, two final WMLs aregenerated, one for each of the two SETOF expressions thatoriginated from rite same NP.CONJUNCTION-AMBIGUITYsource :WMI.r 1: (ttR1NG-ABOUT((INTENSION(EXISTS ?JX18 LIST(OBJECT.OF ?JXl8<lntcrm-WML- 1 >)))TiM E WORI ,D))WMi ~-2: (IIRING-ABOUT((INTENSION(EXISTS ?JX18 LIST(OBJEC.T.OF ?.IX 18<Interm-WML-2>)))TIME WCIRLD))ANNOTATION:(NP-.CON.!
UNCTION-AMBIGUITY(Porse.-Path-.1 Interps (WML-1 <Interm-WML-l>))(P~tse-Patb-2 lnterps (WML-2 <intelm-WML-2>)))More complex sentences that contain postmodified NPeo~tjnnctioz~ may have additional interpretations.
For instance, thesentence The carriers were destroyed by frigates and subs in thelmlian Ocean may have a third interpretation in which the PP in theIndian Ocean modifies the whole clause.
Another more complexexample is" The carriers were ck,stroyed by 3 fi-igates attd subs in theIndian Ocean, in which ambiguity specialists for NP conjunction,PP clause aUachment mad quantifier SCOl:fing will interact.
This kindof interaction among specialists i a topic for our current researchon effective paraphrasing.4.7, Phase 2: 'l~rm~slating from WML to Text StructureOnce the l~roblen't Recognizers have annotated the WML, thetext planne~ t;d,:es over to translate the imensional logic expressioninto the hie~'archical text structure which organizes the objects and~'elations SlW.cified.
In this example, since the input was ambiguousm M there are two WMLs, there are two possible strategies forparaphz~tsing which apply at this step:(1) Paraphrase of each interpretation separately (as discussed inSecl ion 2).
(2) C.ombiae them into a single paraphrase using formatting andhighlighting to contrast the differences:Di,wlay th,~ carriers in the Indian Ocean and the frigates inthe Indian Oceanor the carriers in the Indian Ocean and all thefr igates.We will focus here on the second strategy, that which combines theinterpretations.
The text planner will begin by translating one of theWMLS and when it reaches the subexpression that is annotated asbeing ambiguous, it will build a text structure object representing thedisjunction of those subexpressions.As discussed in Section 3.2, the translation to text structureuses both explicit and implicit information from the WML.
In thiscase, the translation of the first operator, BRING-ABOUT builds acomplex-event object marked as a command in the present tense andthe agent is set to *you*.
The domain model concept DISPLAYprovides the matrix verb (see text structure in Figure 3).When the translation reaches the SETOF expression, aCOORDINATE-RELATION object is built containing bothsubexpressions with the relation DISJUNCTION.
It is also annotated"emphasize-contrast" to guide the later decision making.
As thisnode and its children are expanded, the annotation is passed own.Wizen the translation reaches the individual conjtmcts in theexpression, it uses the annotation to decide how to expand the textstructure for that object.
In the case where the modifier distributes,the annotation blocks any optimization that may lead to anambiguity, and ensures both conjuncts will be modifiexl; in the casewhere it does not distribute, there are two possible strategies toeliminate the ambiguity: 21) Manipt,lating the order of the conjuncts in the text structure:--If only one of the conjuncts is modified attd the modifier isrealizable as a premodifier, then that conjunct should beplaced second.--If only one of the conjuncts is modified and the modifier isrealizable as a postmodifier, then that conjunct should beplaced first.In this case, the paraphrase would be: Display the frigates in theImIian Ocean and carriers.2) Adding a quantifer, such as "all", to the conjunct withoutmodification by adding an adjtmct DO to the second conjunct,which would result in the paraphrase: Display all the carriersand the frigates in the Indi,'m Ocean.We use a combinalion of these strategies.
Figure 3 shows tbe partialtext stuctare built for this expression 3.2 Note that in this task of paraphrasing queries, where it is crucial that heparaphrase b unambiguious, these are strategies the generator should applyregardless ofwhether the original was ambiguous or not, as anthiguity may havebeen introduced into a conjunction by some other strategy, such as lexicalchoice.3 Objects labeled DO in tile diagram indicate discourse objects which have beencreated for this utterance.
Objects labeled DM are obieets from the domainmodel.
The creation of discourse objects allows objects to be annotated withtheir roles and other information not contained in the domain model (tense,number) and introduces objects which can be referred back to anaphorically withpronouns (e.g.
"they" for the DO dominating the conjuncts).~ /  <e entdisplay><DO agent *you*> #<DO patient~ j  <:diOsjredal:o: ' coord ,~:emphasize-contrast>>(~#<DO re la t ion  ' coord inate  .
.
...!
,~ :conjunction.
,~ /  :e rnphas ize -cont ras~object... _ ~ D O  object....~ '  ~-.~e.
rn ph asize-co n trast> / ~.,....,,~em ph asize -co n t rast >Qhead d janet#<DM carrier> #<DM location #<DM frigate> #<DM locationcarrier I0> frigate I0>>FIGURF.
3: TEXT STRUCTURE FOR GENERATION435Once this level is complete, it is traversed and the linguisticresources, uch as the lexical heads and major syntactic ategories,are chosen and represented in the input specification to the lingusiticrealization component, MUMBLE-86, which produces the final text.5.
USING TIlE PARAPHRASER IN A COOPERATIVEDIALOG SYSTEMThe work presented here has focused on developing strategiesfor paraphrasing in order to resolve ambiguity.
However, in anactual NL dialog system, choosing when and how to use thiscapability can be based on other considerations.
In this section weaddress ome practical issues and some related work we have donein the integration of our paraphraser into a Man-Machine hltel-face.The presentation of a paraphrase can be useful even in caseswhere no ambiguity has been detected, as it allows the user to verifythat the system's interpretation does not differ from the intendedinterpretation.
This is particularly useful for new users who need tobe reassured of the system's performance.
This feature should beunder the user's control, though, since frequent users of the systemmay only want to see paraphrases when the system finds multipleinterpretations.Paraphrasing can also be incorporated in cooperative r sponsesin order to make any presuppositions explicit.
Consider thefollowing exchange:U: Display all the carriers.S: <icons displayed on map>U: Which are within 500 miles of Hawaii?S: Carriers Midway, Coral Sea, and Saratoga.U: Which have the highest readiness ratings?S: Of the carriers within 500 miles of Hawaii, Midway andSaratoga re e l .Incorporating elided elements fi'om previous queries in the responsemakes clear which set is being considered for the cun'ent answer.Another sort of paraphrase, which we term "diagnosticresponses", can be used when the system is unable to find anyinterpretation f the user's query, due to ill-fonnedness, novel useof language, or simply inadequate information in the underlyingprogram.
As in paraphrasing, the generator uses structures built bythe understanding component to generate a focused response.
Forexample, ametaphorical use of "commander" torefer to ships, as inthe following query will violate the semantic restrictions on thearguments to the verb "assign".
When IRUS-II fails to find asemantic interpretation, it saves its state, which can then be used bythe generator to produce an appropriate r sponse:U: Which commanders are assigned to SPA 2?S: 1 don't understand how commanders can beassigned.6.
COMPARISON WITtl OTHER WORKA similar approach to ours is McKeown's Co-op system(McKeown, 1983).
It too functions in an interactive environment.However, it is limited in several ways:1) Since the system it worked with was limited to data basequeries, it could only paraphrase questions.
This is not only alimitation in functionality, but affects the linguistic ompetenceas well: the input had to be simple WH- questions with SVOstructure, no complex sentences orcomplicated adjuncts.2) It had only one strategy to change the text: given and new 4,which fronted noun phrases with relative clauses orprepositional phrases that appeared in the later parts of thesentence (essentially the verb phrase).
For example Whichprogrammers worked on oceanography projects in 1972?would be paraphrased: Assuming that there were oceanographyprojects in 1972, which programmers worked on thoseprojects?3) Since its only strategy involved complex noun phrases, if therewere no complex noun phrases in the query, it would be"paraphrased" exactly as the original.4 A related problem is that its notion of given and new was very simplistic: itis purely based on syntactic riteria of the incoming sentenceand does notconsider other criteria such as definiteness or context.436Lowden and de Roeck (1985) also adch'ess the problem ofparaphrasing in the context of data base query.
However, whilethey assume some parse of a qumy has.
taken place, the workfocuses entirely on the generation portion of the problem.
In fact,'they define paraphrasing as providing a "mapping between anunderlying t'ormal representation a d an NL text."
They discuss indetail how text formatting can improve clarity and a solid underlyinglinguistic framework (in theh' case lexical functional grammar) caninsure grammaticality, llowever, while they state that a parapla'aseshould be unambiguous, they do not address how to recognizewhen a query is ambiguous or how to generate an unambiguousquery.The BBN Parlaneerra NL Interface.is one of the most robust NI,interfaces in existance.
Its paraphraser integrates both the system'sconceptual and procedural understanding of NL queries.
Thisapproach is based on the observation that users need to be shownthe conceptual denotation of a word or phrase (e.g., "clericalemployee") with its denotation i  the underlying database system(e.g., an employee whose EEO category is 3 or an employee whosejob title is "secretary").
Thus, the Parlance paraphrases incortyoratereferences to specific fields and values in the underlying data basesystem.
So, while the text can be cumbersome, it has the advantageof more directly capturing what the system understood.
Due toefficiency considerations and limitations on the space for output, thePut'lance paraphraser p esents the paraphases one at a time, allowingthe user to confirm or reject the curt'cut interpretation, rather thanpresenting all paraptn'ases at the stone time.
The system allows theuser to refer back to previously presented interpretations, but as isthe case with the other paraphrasers, elated interpretations are notcontrasted.7.
CONCLUSIONIn addition to being useful in current interactive naturallanguage interfaces, the paraphrase task provides an excellentcontext to explore interesting issues in both natural anguageunderstanding and generation as well as paraphrasing itself.
In thenext phase of our research we plan to look at quantifier scopeambiguities, lexical choice, and the interaction between multipleproblems and strategies for improvement.8.
REFERENCEStIinrichs, Fxhard, Damafis Ayuso, Remko Scha (1987) "The Syntaxand Semantic of the JANUS Semantic hUerpretation Language",Technical Report Section of BUN Report No.
6522, BBNLaboratories, pgs.
27-33.Lowden, Barry G. T., and Anne De Roeck (1985) "GeneratingEnglish Paraphrases from Relational Query Expressions", vol.4, no.
4, p.337-348.McKeown, Kathleen R. (1983) "Paraphrasing Questions UsingGiven and New Information", American Journal ofComputational Linguistics, vol.
9. no.
1, Jan-Mar 1983, p.1-10.McDonald, David D. (1983) "Description Directed Contror' ,Computers and Mathematics 9(1), Reprinted in Grosz, et aL(eds.
), Readings in Natural Language Processing, MorganKanfmann Publishers, California, 1986, p. 519-538.Meteer, Marie M.,David D. McDonald, Scott Anderson, DavidForster, Linda Gay, Alison Heutmer, Penelope Sibun (1987)Mumble-86: Design and Implementation University ofMassadmsetts Technic'd Report 87-87, 173 pages.Moser, Margaret (1983) "An Overview of NIKL", Technical ReportSection of BBN Report No.
5421, BUN Laboratories.Weischedel, Ralph, Edward Walker, Damafis Ayuso, Jos de Bruin,Kimbede Koile, Lance Ramshaw, Varda Sh~ed (1986) "Out ofthe Laboratory: A case study with the IRUS natural anguageinterface", in Research and Development in Natural LanguageUnderstanding as part of the Strategic Computing Program,BBN Labs Technical Report number 6463, pgs.
13-26.Weischedel, Ralph, D. Ayuso, A. Haas, E. Hinrichs, R. Scha, V.Shaked (1987) Research and Development in Natural LanguageUnderstanding as part of the Strategic Computing Program,BBN Labs Tedmieal Report number 6522.
