Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1792?1797,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsDetecting Non-compositional MWE Components using WiktionaryBahar Salehi,?
?Paul Cook?and Timothy Baldwin???
NICTA Victoria Research Laboratory?
Department of Computing and Information SystemsThe University of MelbourneVictoria 3010, Australiabsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.netAbstractWe propose a simple unsupervised ap-proach to detecting non-compositionalcomponents in multiword expressionsbased on Wiktionary.
The approach makesuse of the definitions, synonyms and trans-lations in Wiktionary, and is applicable toany type of MWE in any language, assum-ing the MWE is contained in Wiktionary.Our experiments show that the proposedapproach achieves higher F-score thanstate-of-the-art methods.1 IntroductionA multiword expression (MWE) is a combina-tion of words with lexical, syntactic or seman-tic idiosyncrasy (Sag et al., 2002; Baldwin andKim, 2009).
An MWE is considered (semanti-cally) ?non-compositional?
when its meaning isnot predictable from the meaning of its compo-nents.
Conversely, compositional MWEs are thosewhose meaning is predictable from the meaningof the components.
Based on this definition, acomponent is compositional within an MWE, if itsmeaning is reflected in the meaning of the MWE,and it is non-compositional otherwise.Understanding which components are non-compositional within an MWE is important inNLP applications in which semantic informationis required.
For example, when searching forspelling bee, we may also be interested in docu-ments about spelling, but not those which containonly bee.
For research project, on the other hand,we are likely to be interested in documents whichcontain either research or project in isolation, andfor swan song, we are only going to be interestedin documents which contain the phrase swan song,and not just swan or song.In this paper, we propose an unsupervised ap-proach based on Wikitionary for predicting whichcomponents of a given MWE have a composi-tional usage.
Experiments over two widely-useddatasets show that our approach outperforms state-of-the-art methods.2 Related WorkPrevious studies which have considered MWEcompositionality have focused on either the iden-tification of non-compositional MWE token in-stances (Kim and Baldwin, 2007; Fazly et al.,2009; Forthergill and Baldwin, 2011; Muzny andZettlemoyer, 2013), or the prediction of the com-positionality of MWE types (Reddy et al., 2011;Salehi and Cook, 2013; Salehi et al., 2014).
Theidentification of non-compositional MWE tokensis an important task when a word combinationsuch as kick the bucket or saw logs is ambiguousbetween a compositional (generally non-MWE)and non-compositional MWE usage.
Approacheshave ranged from the unsupervised learning oftype-level preferences (Fazly et al., 2009) to su-pervised methods specific to particular MWE con-structions (Kim and Baldwin, 2007) or applica-ble across multiple constructions using featuressimilar to those used in all-words word sensedisambiguation (Forthergill and Baldwin, 2011;Muzny and Zettlemoyer, 2013).
The predictionof the compositionality of MWE types has tradi-tionally been couched as a binary classificationtask (compositional or non-compositional: Bald-win et al.
(2003), Bannard (2006)), but more re-cent work has moved towards a regression setup,where the degree of the compositionality is pre-dicted on a continuous scale (Reddy et al., 2011;Salehi and Cook, 2013; Salehi et al., 2014).
In ei-ther case, the modelling has been done either overthe whole MWE (Reddy et al., 2011; Salehi andCook, 2013), or relative to each component withinthe MWE (Baldwin et al., 2003; Bannard, 2006).In this paper, we focus on the binary classificationof MWE types relative to each component of the1792MWE.The work that is perhaps most closely related tothis paper is that of Salehi and Cook (2013) andSalehi et al.
(2014), who use translation data topredict the compositionality of a given MWE rel-ative to each of its components, and then combinethose scores to derive an overall compositionalityscore.
In both cases, translations of the MWE andits components are sourced from PanLex (Bald-win et al., 2010; Kamholz et al., 2014), and ifthere is greater similarity between the translatedcomponents and MWE in a range of languages,the MWE is predicted to be more compositional.The basis of the similarity calculation is unsuper-vised, using either string similarity (Salehi andCook, 2013) or distributional similarity (Salehi etal., 2014).
However, the overall method is su-pervised, as training data is used to select thelanguages to aggregate scores across for a givenMWE construction.
To benchmark our method,we use two of the same datasets as these two pa-pers, and repurpose the best-performing methodsof Salehi and Cook (2013) and Salehi et al.
(2014)for classification of the compositionality of eachMWE component.3 MethodologyOur basic method relies on analysis of lexicaloverlap between the component words and the def-initions of the MWE in Wiktionary, in the man-ner of Lesk (1986).
That is, if a given componentcan be found in the definition, then it is inferredthat the MWE carries the meaning of that compo-nent.
For example, the Wiktionary definition ofswimming pool is ?An artificially constructed poolof water used for swimming?, suggesting that theMWE is compositional relative to both swimmingand pool.
If the MWE is not found in Wiktionary,we use Wikipedia as a backoff, and use the firstparagraph of the (top-ranked) Wikipedia article asa proxy for the definition.As detailed below, we further extend the basicmethod to incorporate three types of informationfound in Wiktionary: (1) definitions of each wordin the definitions, (2) synonyms of the words in thedefinitions, and (3) translations of the MWEs andcomponents.3.1 Definition-based SimilarityThe basic method uses Boolean lexical overlap be-tween the target component of the MWE and adefinition.
A given MWE will often have multipledefinitions, however, begging the question of howto combine across them, for which we propose thefollowing three methods.First Definition (FIRSTDEF): Use only thefirst-listed Wiktionary definition for the MWE,based on the assumption that this is the predom-inant sense.All Definitions (ALLDEFS): In the case thatthere are multiple definitions for the MWE, cal-culate the lexical overlap for each independentlyand take a majority vote; in the case of a tie, labelthe component as non-compositional.Idiom Tag (ITAG): In Wiktionary, there is fa-cility for users to tag definitions as idiomatic.1If,for a given MWE, there are definitions tagged asidiomatic, use only those definitions; if there areno such definitions, use the full set of definitions.3.2 Synonym-based Definition ExpansionIn some cases, a component is not explicitly men-tioned in a definition, but a synonym does occur,indicating that the definition is compositional inthat component.
In order to capture synonym-based matches, we optionally look for synonymsof the component word in the definition,2and ex-pand our notion of lexical overlap to include thesesynonyms.For example, for the MWE china clay, the defi-nition is kaolin, which includes neither of the com-ponents.
However, we find the component wordclay in the definition for kaolin, as shown below.A fine clay, rich in kaolinite, used in ce-ramics, paper-making, etc.This method is compatible with the threedefinition-based similarity methods describedabove, and indicated by the +SYN suffix (e.g.FIRSTDEF+SYN is FIRSTDEF with synonym-based expansion).3.3 TranslationsA third information source in Wiktionary that canbe used to predict compositionality is sense-leveltranslation data.
Due to the user-generated na-ture of Wiktionary, the set of languages for which1Although the recall of these tags is low (Muzny andZettlemoyer, 2013).2After removing function words, based on a stopword list.1793ENC EVPCWordNet 91.1% 87.5%Wiktionary 96.7% 96.2%Wiktionary+Wikipedia 100.0% 96.2%Table 1: Lexical coverage of WordNet, Wik-tionary and Wiktionary+Wikipedia over our twodatasets.translations are provided varies greatly across lexi-cal entries.
Our approach is to take whatever trans-lations happen to exist in Wiktionary for a givenMWE, and where there are translations in that lan-guage for the component of interest, use the LCS-based method of Salehi and Cook (2013) to mea-sure the string similarity between the translationof the MWE and the translation of the compo-nents.
Unlike Salehi and Cook (2013), however,we do not use development data to select the opti-mal set of languages in a supervised manner, andinstead simply take the average of the string simi-larity scores across the available languages.
In thecase of more than one translation in a given lan-guage, we use the maximum string similarity foreach pairing of MWE and component translation.Unlike the definition and synonym-based ap-proach, the translation-based approach will pro-duce real rather than binary values.
To combinethe two approaches, we discretise the scores givenby the translation approach.
In the case of dis-agreement between the two approaches, we labelthe given MWE as non-compositional.
This re-sults in higher recall and lower precision for thetask of detecting compositionality.3.4 An Analysis of Wiktionary CoverageA dictionary-based method is only as good as thedictionary it is applied to.
In the case of MWEcompositionality analysis, our primary concern islexical coverage in Wiktionary, i.e., what propor-tion of a representative set of MWEs is containedin Wiktionary.
We measure lexical coverage rela-tive to the two datasets used in this research (de-scribed in detail in Section 4), namely 90 En-glish noun compounds (ENCs) and 160 Englishverb particle constructions (EVPCs).
In each case,we calculated the proportion of the dataset thatis found in Wiktionary, Wiktionary+Wikipedia(where we back off to a Wikipedia document in thecase that a MWE is not found in Wiktionary) andWordNet (Fellbaum, 1998).
The results are foundin Table 1, and indicate perfect coverage in Wik-tionary+Wikipedia for the ENCs, and very highcoverage for the EVPCs.
In both cases, the cov-erage of WordNet is substantially lower, althoughstill respectable, at around 90%.4 DatasetsAs mentioned above, we evaluate our method overthe same two datasets as Salehi and Cook (2013)(which were later used, in addition to a thirddataset of German noun compounds, in Salehiet al.
(2014)): (1) 90 binary English noun com-pounds (ENCs, e.g.
spelling bee or swimmingpool); and (2) 160 English verb particle construc-tions (EVPCs, e.g.
stand up and give away).
Ourresults are not directly comparable with those ofSalehi and Cook (2013) and Salehi et al.
(2014),however, who evaluated in terms of a regressiontask, modelling the overall compositionality of theMWE.
In our case, the task setup is a binary clas-sification task relative to each of the two compo-nents of the MWE.The ENC dataset was originally constructed byReddy et al.
(2011), and annotated on a contin-uous [0, 5] scale for both overall compositional-ity and the component-wise compositionality ofeach of the modifier and head noun.
The samplingwas random in an attempt to make the dataset bal-anced, with 48% of compositional English nouncompounds, of which 51% are compositional inthe first component and 60% are compositional inthe second component.
We generate discrete la-bels by discretising the component-wise composi-tionality scores based on the partitions [0, 2.5] and(2.5, 5].
On average, each NC in this dataset has1.4 senses (definitions) in Wiktionary.The EVPC dataset was constructed by Ban-nard (2006), and manually annotated for com-positionality on a binary scale for each of thehead verb and particle.
For the 160 EVPCs,76% are verb-compositional and 48% are particle-compositional.
On average, each EVPC in thisdataset has 3.0 senses (definitions) in Wiktionary.5 ExperimentsThe baseline for each dataset takes the form oflooking for a user-annotated idiom tag in the Wik-tionary lexical entry for the MWE: if there is an id-iomatic tag, both components are considered to benon-compositional; otherwise, both componentsare considered to be compositional.
We expectthis method to suffer from low precision for two1794MethodFirst Component Second ComponentPrecision Recall F-score Precision Recall F-scoreBaseline 66.7 68.2 67.4 66.7 83.3 74.1LCS 60.0 77.7 67.7 81.6 68.1 64.6DS 62.1 88.6 73.0 80.5 86.4 71.2DS+DSL2 62.5 92.3 74.5 78.4 89.4 70.6LCS+DS+DSL2 66.3 87.5 75.4 82.1 80.6 70.1FIRSTDEF 59.4 93.2 72.6 54.2 88.9 67.4ALLDEFS 59.5 100.0 74.6 52.9 100.0 69.2ITAG 60.3 100.0 75.2 54.5 100.0 70.6FIRSTDEF+SYN 64.9 84.1 73.3 63.8 83.3 72.3ALLDEFS+SYN 64.5 90.9 75.5 60.4 88.9 71.9ITAG+SYN 64.5 90.9 75.5 61.8 94.4 74.7FIRSTDEF+SYNCOMB(LCS+DS+DSL2)82.9 85.3 84.1 81.9 80.0 69.8ALLDEFS+SYNCOMB(LCS+DS+DSL2)81.2 88.1 84.5 87.3 80.6 73.3ITAG+SYNCOMB(LCS+DS+DSL2)81.0 88.1 84.1 88.0 81.1 73.9Table 2: Compositionality prediction results over the ENC dataset, relative to the first component (themodifier noun) and the second component (the head noun)reasons: first, the guidelines given to the annota-tors of our datasets might be different from whatWiktionary contributors assume to be an idiom.Second, the baseline method assumes that for anynon-compositional MWE, all components must beequally non-compositional, despite the wealth ofMWEs where one or more components are com-positional (e.g.
from the Wiktionary guidelinesfor idiom inclusion,3computer chess, basketballplayer, telephone box).We also compare our method with: (1) ?LCS?,the string similarity-based method of Salehi andCook (2013), in which 54 languages are used;(2) ?DS?, the monolingual distributional similaritymethod of Salehi et al.
(2014); (3) ?DS+DSL2?,the multilingual distributional similarity methodof Salehi et al.
(2014), including supervised lan-guage selection for a given dataset, based on cross-validation; and (4) ?LCS+DS+DSL2?, wherebythe first three methods are combined using a su-pervised support vector regression model.
Ineach case, the continuous output of the modelis equal-width discretised to generate a binaryclassification.
We additionally present results forthe combination of each of the six methods pro-posed in this paper with LCS, DS and DSL2, us-ing a linear-kernel support vector machine (rep-resented with the suffix ?COMB(LCS+DS+DSL2)?
fora given method).
The results are based on cross-3http://en.wiktionary.org/wiki/Wiktionary:Idioms_that_survived_RFDvalidation, and for direct comparability, the parti-tions are exactly the same as Salehi et al.
(2014).Tables 2 and 3 provide the results when our pro-posed method for detecting non-compositionalityis applied to the ENC and EVPC datasets, respec-tively.
The inclusion of translation data was foundto improve all of precision, recall and F-scoreacross the board for all of the proposed methods.For reasons of space, results without translationdata are therefore omitted from the paper.Overall, the simple unsupervised methods pro-posed in this paper are comparable with the unsu-pervised and supervised state-of-the-art methodsof Salehi and Cook (2013) and Salehi et al.
(2014),with ITAG achieving the highest F-score for theENC dataset and for the verb components of theEVPC dataset.
The inclusion of synonyms boostsresults in most cases.When we combine each of our proposed meth-ods with the string and distributional similar-ity methods of Salehi and Cook (2013) andSalehi et al.
(2014), we see substantial improve-ments over the comparable combined method of?LCS+DS+DSL2?
in most cases, demonstratingboth the robustness of the proposed methods andtheir complementarity with the earlier methods.
Itis important to reinforce that the proposed meth-ods make no language-specific assumptions andare therefore applicable to any type of MWE andany language, with the only requirement being thatthe MWE of interest be listed in the Wiktionary for1795MethodFirst Component Second ComponentPrecision Recall F-score Precision Recall F-scoreBaseline 24.6 36.8 29.5 59.6 40.5 48.2LCS 36.5 49.2 39.3 61.5 63.7 60.3DS 32.8 34.1 33.5 80.9 19.6 29.7DS+DSL2 31.8 72.4 44.2 74.8 27.5 36.6LCS+DS+DSL2 36.1 62.6 45.8 77.9 42.8 49.2FIRSTDEF 24.8 84.2 38.3 54.5 94.0 69.0ALLDEFS 25.0 97.4 39.8 53.6 97.6 69.2ITAG 26.2 89.5 40.5 54.6 91.7 68.4FIRSTDEF+SYN 32.9 65.8 43.9 60.4 65.5 62.9ALLDEFS+SYN 28.4 81.6 42.1 62.5 77.4 69.1ITAG+SYN 30.5 65.8 41.7 57.8 61.9 59.8FIRSTDEF+SYNCOMB(LCS+DS+DSL2)34.0 65.3 44.7 83.6 67.3 65.4ALLDEFS+SYNCOMB(LCS+DS+DSL2)37.4 70.9 48.9 80.4 65.9 63.0ITAG+SYNCOMB(LCS+DS+DSL2)35.6 70.9 47.4 83.5 64.9 64.2Table 3: Compositionality prediction results over the EVPC dataset, relative to the first component (thehead verb) and the second component (the particle)that language.6 Error AnalysisWe analysed all items in each dataset where thesystem score differed from that of the humanannotators.
For both datasets, the majority ofincorrectly-labelled items were compositional butpredicted to be non-compositional by our sys-tem, as can be seen in the relatively low preci-sion scores in Tables 2 and 3.
In many of thesecases, the prediction based on definitions and syn-onyms was compositional but the prediction basedon translations was non-compositional.
In suchcases, we arbitrarily break the tie by labelling theinstance as non-compositional, and in doing sofavour recall over precision.Some of the incorrectly-labelled ENCs havea gold-standard annotation of around 2.5, or inother words are semi-compositional.
For exam-ple, the compositionality score for game in gameplan is 2.82/5, but our system labels it as non-compositional; a similar thing happens with figureand the EVPC figure out.
Such cases demonstratethe limitation of approaches to MWE composi-tionality that treat the problem as a binary clas-sification task.On average, the EVPCs have three senses,which is roughly twice the number for ENCs.
Thismakes the prediction of compositionality harder,as there is more information to combine across (aneffect that is compounded with the addition of syn-onyms and translations).
In future work, we hopeto address this problem by first finding the sensewhich matches best with the sentences given to theannotators.7 ConclusionWe have proposed an unsupervised approach forpredicting the compositionality of an MWE rel-ative to each of its components, based on lexi-cal overlap using Wiktionary, optionally incorpo-rating synonym and translation data.
Our experi-ments showed that the various instantiations of ourapproach are superior to previous state-of-the-artsupervised methods.
All code to replicate the re-sults in this paper has been made publicly avail-able at https://github.com/bsalehi/wiktionary_MWE_compositionality.AcknowledgementsWe thank the anonymous reviewers for theirinsightful comments and valuable suggestions.NICTA is funded by the Australian government asrepresented by Department of Broadband, Com-munication and Digital Economy, and the Aus-tralian Research Council through the ICT Centreof Excellence programme.ReferencesTimothy Baldwin and Su Nam Kim.
2009.
Multiwordexpressions.
In Nitin Indurkhya and Fred J. Dam-1796erau, editors, Handbook of Natural Language Pro-cessing.
CRC Press, Boca Raton, USA, 2nd edition.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical modelof multiword expression decomposability.
In Pro-ceedings of the ACL-2003 Workshop on MultiwordExpressions: Analysis, Acquisition and Treatment,pages 89?96, Sapporo, Japan.Timothy Baldwin, Jonathan Pool, and Susan M.Colowick.
2010.
PanLex and LEXTRACT: Trans-lating all words of all languages of the world.
InProceedings of the 23rd International Conference onComputational Linguistics: Demonstrations, pages37?40, Beijing, China.Colin James Bannard.
2006.
Acquiring Phrasal Lexi-cons from Corpora.
Ph.D. thesis, University of Ed-inburgh.Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.2009.
Unsupervised type and token identification ofidiomatic expressions.
Computational Linguistics,35(1):61?103.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,USA.Richard Forthergill and Timothy Baldwin.
2011.Fleshing it out: A supervised approach to MWE-token and MWE-type classification.
In Proceedingsof the 5th International Joint Conference on NaturalLanguage Processing (IJCNLP 2011), pages 911?919, Chiang Mai, Thailand.David Kamholz, Jonathan Pool, and Susan Colowick.2014.
PanLex: Building a resource for panlinguallexical translation.
In Proceedings of the Ninth In-ternational Conference on Language Resources andEvaluation (LREC?14), pages 3145?3150, Reyk-javik, Iceland.Su Nam Kim and Timothy Baldwin.
2007.
Detectingcompositionality of English verb-particle construc-tions using semantic similarity.
In Proceedings ofthe 7th Meeting of the Pacific Association for Com-putational Linguistics (PACLING 2007), pages 40?48, Melbourne, Australia.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: How to tell apine cone from an ice cream cone.
In Proceedings ofthe 5th Annual International Conference on SystemsDocumentation, pages 24?26, Ontario, Canada.Grace Muzny and Luke Zettlemoyer.
2013.
Auto-matic idiom identification in Wiktionary.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1417?1421, Seattle, USA.Siva Reddy, Diana McCarthy, and Suresh Manandhar.2011.
An empirical study on compositionality incompound nouns.
In Proceedings of IJCNLP, pages210?218, Chiang Mai, Thailand.Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger.
2002.
Multiword ex-pressions: A pain in the neck for NLP.
In Pro-ceedings of the 3rd International Conference onIntelligent Text Processing Computational Linguis-tics (CICLing-2002), pages 189?206, Mexico City,Mexico.Bahar Salehi and Paul Cook.
2013.
Predictingthe compositionality of multiword expressions usingtranslations in multiple languages.
In Proceedingsof the Second Joint Conference on Lexical and Com-putational Semantics, volume 1, pages 266?275, At-lanta, USA.Bahar Salehi, Paul Cook, and Timothy Baldwin.
2014.Using distributional similarity of multi-way transla-tions to predict multiword expression composition-ality.
In Proceedings of the 14th Conference of theEACL (EACL 2014), pages 472?481, Gothenburg,Sweden.1797
