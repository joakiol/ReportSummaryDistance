Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69?77,Baltimore, Maryland USA, June 27, 2014.c?2014 Association for Computational LinguisticsChallenges in Automating Maze DetectionEric MorleyCSLUOHSUPortland, OR 97239morleye@gmail.comAnna Eva HallinDepartment of CommunicativeSciences and DisordersNew York UniversityNew York, NYae.hallin@nyu.eduBrian RoarkGoogle ResearchNew York, NY 10011roarkbr@gmail.comAbstractSALT is a widely used annotation ap-proach for analyzing natural languagetranscripts of children.
Nine annotatedcorpora are distributed along with scoringsoftware to provide norming data.
We ex-plore automatic identification of mazes ?SALT?s version of disfluency annotations?
and find that cross-corpus generalizationis very poor.
This surprising lack of cross-corpus generalization suggests substantialdifferences between the corpora.
This isthe first paper to investigate the SALT cor-pora from the lens of natural language pro-cessing, and to compare the utility of dif-ferent corpora collected in a clinical set-ting to train an automatic annotation sys-tem.1 IntroductionAssessing a child?s linguistic abilities is a criticalcomponent of diagnosing developmental disorderssuch as Specific Language Impairment or AutismSpectrum Disorder, and for evaluating progressmade with remediation.
Structured instruments(?tests?)
that elicit brief, easy to score, responsesto a sequence of items are a popular way of per-forming such assessment.
An example of a struc-tured instrument is the CELF-4, which includesnineteen multi-item subtests with tasks such asobject naming, word definition, reciting the daysof the week, or repeating sentences (Semel et al.,2003).
Over the past two decades, researchershave discussed the limitations of standardized testsand how well they tap into different language im-pairments.
Many have advocated the potentialbenefits of language sample analysis (LSA) (John-ston, 2006; Dunn et al., 1996).
The analysis ofnatural language samples may be particularly ben-eficial for language assessment in ASD, wherepragmatic and social communication issues areparamount yet may be hard to assess in a conven-tional test format (Tager-Flusberg et al., 2009).At present, the expense of LSA prevents it frombeing more widely used.
Heilmann (2010), whilearguing that LSA is not too time-consuming, esti-mates that each minute of spoken language takesfive to manually transcribe and annotate.
At thisrate, it is clearly impractical for clinicians to per-form LSA on hours of speech.
Techniques fromnatural language processing could be used to buildtools to automatically annotate transcripts, thus fa-cilitating LSA.Here, we evaluate the utility of a set of anno-tated corpora for automating a key annotation inthe de facto standard annotation schema for LSA:the Systematic Analysis of Language Transcripts(SALT) (Miller et al., 2011).
SALT comprises ascheme for coding transcripts of recorded speech,together with software that tallies these codes,computes scores describing utterance length anderror counts, among a range of other standard mea-sures, and compares these scores with normativesamples.
SALT codes indicate bound morphemes,several types of grammatical errors (for exampleusing a pronoun of the wrong gender or case), andmazes, which are defined as ?filled pauses, falsestarts, and repetitions and revisions of words, mor-phemes and phrases?
(Miller et al., 2011, p. 48).Mazes have sparked interest in the child lan-guage disorders literature for several reasons.They are most often analyzed from a languageprocessing perspective where the disruptions areviewed as a consequence of monitoring, detect-ing and repairing language, potentially includingspeech errors (Levelt, 1993; Postma and Kolk,1993; Rispoli et al., 2008).
Several studies havefound that as grammatical complexity and utter-ance length increase, the number of mazes in-creases in typically developing children and chil-dren with language impairments (MacLachlan and69Chapman, 1988; Nippold et al., 2008; Reuter-ski?old Wagner et al., 2000; Wetherell et al., 2007).Mazes in narrative contexts have been shownto differ between typical children and childrenwith specific language impairment (MacLachlanand Chapman, 1988; Thordardottir and Weismer,2001), though others have not found reliable groupdifferences (Guo et al., 2008; Scott and Windsor,2000).
Furthermore, outside the potential useful-ness of looking at mazes in themselves, mazes al-ways have to be detected and excluded in orderto calculate other standard LSA measures suchas mean length of utterance and type or tokencounts.
Mazes also must be excluded when ana-lyzing speech errors, since some mazes are in factself-corrections of language or speech errors.Thus, automatically delimiting mazes could beclinically useful in several ways.
First, if mazescan be automatically detected, standard measuressuch as token and type counts can be calculatedwith ease, as noted above.
Automatic maze detec-tion could also be a first processing step for au-tomatically identifying errors: error codes cannotappear in mazes, and certain grammatical errorsmay be easier to identify once mazes have beenexcised.
Finally, after mazes have been identified,further analysis of the mazes themselves (e.g.
thenumber of word in mazes, and the placement ofmazes in the sentence) can provide supplementaryinformation about language formulation abilitiesand word retrieval abilities (Miller et al., 2011, p.87-89).We use the corpora included with the SALTsoftware to train maze detectors.
These are thecorpora that the software uses to compute refer-ence counts.
These corpora share several charac-teristics we expect to be typical of clinical data:they were collected under a diverse set of circum-stances; they were annotated by different groups;the annotations ostensibly follow the same guide-lines; and the annotations were not designed withautomation in mind.
We will investigate whetherwe can extract usable generalizations from theavailable data, and explore how well the auto-mated system performs, which will be of interestto clinicians looking to expedite LSA.2 BackgroundHere we provide an overview of SALT and mazeannotations.
We are not aware of any attemptsto automate maze detection, although maze de-tection closely resembles the well-established taskof edited word detection.
We also provide anoverview of the corpora included with the SALTsoftware, which are the ones we will use to trainmaze detectors.2.1 SALT and Maze AnnotationsThe approach used in SALT has been in wide usefor nearly 30 years (Miller and Chapman, 1985),and now also exists as a software package1pro-viding transcription and coding support along withtools for aggregating statistics for manual codesover the annotated corpora and comparing withage norms.
The SALT software is not the focus ofthis investigation, so we do not discuss it further.Following the SALT guidelines, speech shouldbe transcribed orthographically and verbatim.
Thetranscript must include and indicate: the speakerof each utterance, partial words or stuttering, over-lapping speech, unintelligible words, and any non-speech sounds from the speaker.
Even atypicallanguage, for example neologisms (novel words)or grammatical errors (for example ?her went?
)should be written as such.There are three broad categories of SALT anno-tations: indicators of 1) certain bound morphemes,2) errors, and 3) mazes.
In general, verbal suffixesthat are visible in the surface form (for example-ing in ?going?)
and clitics that appear with an un-modified root (so for example -n?t in ?don?t?, butnot the -n?t in ?won?t?)
must be indicated.
SALTincludes various codes to indicate grammatical er-rors including, but not limited to: overgeneral-ization errors (?goed?
), extraneous words, omit-ted words or morphemes, and inappropriate ut-terances (e.g.
answering a yes/no question with?fight?).
For more information on these standardannotations, we refer the reader to the SALT man-ual (Miller et al., 2011).Here, we are interested in automatically delim-iting mazes.
In SALT, filled pauses, repetitionsand revisions are included in the umberella term?mazes?
but the manual does not include defini-tions for any of these categories.
In SALT, mazesare simply delimited by parentheses; they have nointernal structure, and cannot be nested.
Contigu-ous spans of maze words are delimited by a singleset of parentheses, as in the following utterance:(1) (You have you have um there/?s only)there/?s ten people1http://www.saltsoftware.com/70To be clear, we define the task of automatically ap-plying maze detections as taking unannotated tran-scripts of speech as input, and then outputting abinary tag for each word that indicates whether ornot it is in a maze.2.2 Edited Word DetectionAlthough we are not aware of any previous workon automating maze detection, there is a well-established task in natural language processingthat is quite similar: edited word detection.
Thegoal of edited word detection is to identify wordsthat have been revised or deleted by the speaker,for example ?to Dallas?
in the utterance ?I want togo to Dallas, um I mean to Denver.?.
Many in-vestigations have approached edited word detec-tion from what Nakatani et al.
(1993) have termed?speech-first?
perspective, meaning that edited de-tection is performed with features from the speechsignal in addition to a transcript.
These ap-proaches, however, are not applicable to the SALTcorpora, because they only contain transcripts.
Asa result, we must adopt a text-first approach tomaze detection, using only features extracted froma transcript.The text-first approach to edited word detec-tion is well established.
One of the first investi-gations taking a text-first approach was conductedby Charniak and Johnson (2001).
There, theyused boosted linear classifiers to identify editedwords.
Later, Johnson and Charniak (2004) im-proved upon the linear classifiers?
performancewith a tree adjoining grammar based noisy chan-nel model.
Zwarts and Johnson (2011) improvethe noisy channel model by adding in a rerankerthat leverages features extracted with the help of alarge language model.Qian and Liu (2013) have developed what iscurrently the best-performing edited word detec-tor, and it takes a text-first approach.
Unlike thedetector proposed by Zwarts and Johnson, Qianand Liu?s does not rely on any external data.
Theirdetector operates in three passes.
In the first pass,filler words (?um?, ?uh?, ?I mean?, ?well?, etc.)
aredetected.
In the second and third passes, editedwords are detected.
The reason for the three passesis that in addition to extracting features (mostlywords and part of speech tags) from the raw tran-script, the second and third steps use features ex-tracted from the output of previous steps.
An ex-ample of such features is adjacent words from theutterance with filler words and some likely editedwords removed.3 Overview of SALT CorporaWe explore nine corpora included with the SALTsoftware.
Table 1 has a high level overview ofthese corpora, showing where each was collected,the age ranges of the speakers, and the size of eachcorpus both in terms of transcripts and utterances.Note that only utterances spoken by the child arecounted, as we throw out all others.Table 1 shows several divisions among the cor-pora.
We see that one group of corpora comesfrom New Zealand, while the majority come fromNorth America.
All of the corpora, except for Ex-pository, include children at very different stagesof language development.Four research groups were responsible for thetranscriptions and annotations of the corpora inTable 1.
One group produced the CONVERSA-TION, EXPOSITORY, NARRATIVESSS, and NAR-RATIVESTORYRETELL corpora.
Another wasresponsible for all of the corpora from NewZealand.
Finally, the ENNI and GILLAMNT cor-pora were transcribed and annotated by two dif-ferent groups.
For more details on these cor-pora, how they were collected, and the anno-tators, we refer the reader to the SALT web-site at http://www.saltsoftware.com/resources/databases.html.Some basic inspection reveals that the corporacan be put into three groups based on the me-dian utterance lengths, and the distribution of ut-Table 1: Description of SALT corporaCorpus Transcripts Utterances Age Range Speaker LocationCONVERSATION 584 82,643 2;9 ?
13;3 WI & CAENNI 377 56,108 3;11 ?
10;0 CanadaEXPOSITORY 242 4,918 10;7 ?
15;9 WIGILLAMNT 500 40,102 5;0 ?
11;11 USANARRATIVESSS 330 16,091 5;2 ?
13;3 WI & CANARRATIVESTORYRETELL 500 14,834 4;4 ?
12;8 WI & CANZCONVERSATION 248 25,503 4;5 ?
7;7 NZNZPERSONALNARRATIVE 248 20,253 4;5 ?
7;7 NZNZSTORYRETELL 264 2,574 4;0 ?
7;7 NZ71terance2lengths, following the groups Figure 1,with the EXPOSITORY and CONVERSATION cor-pora in their own groups.
Note that the countsin Figure 1 are of all of the words in each ut-terance, including those in mazes.
We see thatthe corpora in Group A have a modal utterancelength ranging from seven to ten words.
There aremany utterances in these corpora that are shorteror longer than the median length.
Compared tothe corpora in Group A, those in Group B havea shorter modal utterance length, and fewer longutterances.
In Figure 1, we see that the CONVER-SATION corpus consists mostly of very short utter-ances.
At the other extreme is the EXPOSITORYcorpus, which resembles the corpora in Group Ain terms of modal utterance length, but which gen-erally contains longer utterances than any of theother corpora.4 Maze Detection Experiments4.1 Maze DetectorWe carry out our experiments in automatic mazedetection using a statistical maze detector thatlearns to identify mazes from manually labeleddata using features extracted from words and auto-matically predicted part of speech tags.
The mazedetector uses the feature set shown in Table 2.This set of features is identical to the ones used bythe ?filler word?
detector in Qian and Liu?s disflu-ency detector (2013).
We also use the same clas-2All of these corpora are reported to have been segmentedinto c-units, which is defined as ?an independent clause withits modifiers?
(Miller et al., 2011).Table 2: Feature templates for maze word detection, follow-ing Qian and Liu (2013).
We extract all of the above featuresfrom both words and POS tags, albeit separately.
t0indicatesthe current word or POS tag, while t?1is the previous oneand t1is the following.
The function I(a, b) is 1 if a and bare identical, and otherwise 0. y?1is the tag predicted for theprevious word.Category FeaturesUnigrams t?2, t?1, t0, t1, t2Bigrams t?1t0, t0t1Trigrams t?2t?1t0, t?1t0t1, t0t1t2Logic Unigrams I(ti, t0), I(pi, p0);?4 ?
i ?
4; i 6= 0Logic Bigrams I(ti?2ti?1, t?1t0)I(titi+1, t0ti+1);?4 ?
i ?
4; i 6= 0Predicted tag y?1(a) Group A(b) Group B(c) OthersFigure 1: Histograms of utterance length (including wordsin mazes) in SALT corporasifier as the second and third steps of their system:the Max Margin Markov Network ?M3N?
classi-fier in the pocketcrf toolkit (available at http://code.google.com/p/pocketcrf/).
TheM3N classifier is a kernel-based classifier that isable to leverage the sequential nature the data inthis problem (Taskar et al., 2003).
We use the fol-lowing label set: S-O (not in maze); S-M (sin-gle word maze); B-M (beginning of multi-word72maze); I-M (in multi-word maze); and E-M (endof multi-word maze).
The M3N classifier allowsus to set a unique penalty for each pair of con-fused labels, for example penalizing an erroneousprediction of S-O (failing to identify maze words)more heavily than spurious predictions of mazewords (all -M labels).
This ability is particularlyuseful for maze detection because maze words areso infrequent compared to words that are not inmazes.4.2 EvaluationWe split each SALT corpus into training, develop-ment, and test partitions.
Each training partitioncontains 80% of the utterances the corpus, whilethe development and test partitions each contain10% of the utterances.
We use the developmentportion of each corpus to set the penalty matrixsystem to roughly balance precision and recall.We evaluate maze detection in terms of bothtagging performance and bracketing performance,both of which are standard forms of evaluationfor various tasks in the Natural Language Pro-cessing literature.
Tagging performance captureshow effectively maze detection is done on a word-by-word basis, while bracketing performance de-scribes how well each maze is identified in its en-tirety.
For both tagging and bracketing perfor-mance, we count the number of true and falsepositives and negatives, as illustrated in Figure 2.In tagging performance, each word gets countedonce, while in bracketing performance we com-pare the predicted and observed maze spans.
Weuse these counts to compute the following metrics:(P)recision =tptp + fp(R)ecall =tptp + fnF1 =2PRP + RNote that partial words and punctuation are bothignored in evaluation.
We exclude punctuation be-cause punctuation does not need to be includedin mazes: it is not counted in summary statistics(e.g.
MLU, word count, etc.
), and punctuation er-rors are not captured by the SALT error codes.We exclude partial words because they are alwaysin mazes, and therefore can be detected triviallywith a simple rule.
Furthermore, because par-tial words are excluded from evaluation, the per-formance metrics are comparable across corpora,even if they vary widely in the frequency of partialwords.For both space and clarity, we do not presentthe complete results of every experiment in thispaper, although they are available online3.
In-stead, we present the complete baseline results,and then report F1 scores that are significantlybetter than the baseline.
We establish statisticalsignificance by using a randomized paired-sampletest (see Yeh (2000) or Noreen (1989)) to com-pare the baseline system (system A) and the pro-posed system (system B).
First, we compute thedifference d in F1 score between systems A and B.Then, we repeatedly construct a random set of pre-dictions for each input item by choosing betweenthe outputs of system A and B with equal proba-bility.
We compute the F1 score of these randompredictions, and if it exceeds the F1 score of thebaseline system by at least d, we count the itera-tion as a success.
The significance level is at mostthe number of successes divided by one more thanthe number of trials (Noreen, 1989).4.3 Baseline ResultsFor each corpus, we train the maze detector onthe training partition and test it on the devel-opment partition.
The results of these runs arein Table 3, which also includes the rank of thesize of each corpus (1 = biggest, 9 = smallest).We see immediately that our maze detector per-forms far better on some corpora than on oth-ers, both in terms of tagging and bracketing per-formance.
We note that maze detection perfor-mance is not solely determined by corpus size:tagging performance is substantially worse on thelargest corpus (CONVERSATION) than the small-3http://bit.ly/1dtFTPlFigure 2: Tagging and bracketing evaluation for maze detection.
TP = True Positive, FP = False Positive, TN = True Negative,FN = False NegativePred.
( and then it ) oh and then it ( um ) put his wings out .Gold ( and then it oh ) and then it ( um ) put his wings out .Tag TP ?3 FN TN ?3 TP TN ?4Brack.
FP, FN TP73Tagging BracketingCorpus Size Rank P R F1 P R F1CONVERSATION 1 0.821 0.779 0.800 0.716 0.729 0.723ENNI 2 0.923 0.882 0.902 0.845 0.837 0.841EXPOSITORY 8 0.703 0.680 0.691 0.620 0.615 0.618GILLAMNT 3 0.902 0.907 0.904 0.827 0.843 0.835NARRATIVESSS 6 0.781 0.768 0.774 0.598 0.679 0.636NARRATIVESTORYRETELL 7 0.799 0.774 0.786 0.627 0.671 0.649NZCONVERSATION 4 0.832 0.835 0.838 0.707 0.757 0.731NZPERSONALNARRATIVE 5 0.842 0.835 0.838 0.707 0.757 0.731NZSTORYRETELL 9 0.905 0.862 0.883 0.773 0.780 0.776Table 3: Baseline maze detection performance on development sections of SALT corpora: corpus-specific modelsest (NZSTORYRETELL).4.4 Generic ModelWe train a generic model for maze detection onall of the training portions of the nine SALT cor-pora.
We use the combined development sectionsof all of the corpora to tune the loss matrix for bal-anced precision and recall.
We then test the re-sulting model on the development section of eachSALT corpus, and evaluate in terms of tagging andbracketing accuracy.We find that the generic model performs worsethan the baseline in terms of both tagging andbracketing performance on six of the nine corporacorpora.
The generic model significantly improvestagging (F1=0.925, p ?
0.0022) on the NZSTO-RYRETELL corpus, but the improvement in brack-eting performance is not significant (p ?
0.1635).There is improvement of both tagging (F1=0.805,p ?
0.0001) and bracketing (F1=0.677, p ?0.0025) performance on the NARRATIVESSS cor-pus.
The generic model does not perform betterthan the baseline corpus-specific models on anyother corpora.The poor performance of the generic model issomewhat surprising, as it is trained with far moredata than any of the corpus-specific models.
Inmany tasks in natural language processing, in-creasing the amount of training data improves theresulting model, although this is not necessarilythe case if the additional data is noisy or out-of-domain.
This suggests two possibilities: 1) thelanguage in the corpora varies substantially, per-haps due to the speakers?
ages or the activity thatwas transcribed; and 2) the maze annotations areinconsistent between corpora.4.5 Multi-Corpus ModelsIt is possible that poor performance of the genericmodel relative to the baseline corpus-specificmodels can be attributed to systematic differencesbetween the SALT corpora.
We may be able totrain a model for a set of corpora that share particu-lar characteristics that can outperform the baselinemodels because such a model could leverage moretraining data.
We first evaluate a model for corporathat contain transcripts collected from children ofsimilar ages.
We also evaluate task-specific mod-els, specifically a maze-detection model for storyretellings, and another for conversations.
Thesetwo types of models could perform well if chil-dren of similar ages or performing similar tasksproduce mazes in a similar manner.
Finally, wetrain models for each group of annotators to seewhether systematic variation in annotation stan-dards between research groups could be respon-sible for the generic model?s poor performance.We train all of these models similarly to thegeneric model: we pool the training sections ofthe selected corpora, train the model, then test onthe development section of each selected corpus.We use the combined development sections of theselected corpora to tune the penalty matrix to bal-ance precision and recall.Again, we only report F1 scores that are higherthan the baseline model?s, and we test whetherthe improvement is statistically significant.
Wedo not report results where just the precision orjust the recall exceeds the baseline model perfor-mance, but not F1, because these are typically theresult of model imbalance, favoring precision atthe expense of recall or vice versa.
Bear in mindthat we roughly balance precision and recall on thecombined development sets, not each corpus?s de-velopment set individually.4.5.1 Age-Specific ModelWe train a single model on the following cor-pora: ENNI, GILLAMNT, NARRATIVESSS, andNARRATIVESTORYRETELL.
As shown in Ta-ble 1, these corpora contain transcripts collectedfrom children roughly aged 4-12.
In three of thefour corpora, the age-based model performs worsethan the baseline.
The only exception is NAR-74RATIVESTORYRETELL, for which the age-basedmodel outperforms the baseline in terms of bothtagging (F1=0.794, p ?
0.0673) and bracketing(F1=0.679, p ?
0.0062).4.5.2 Task-Specific ModelsWe construct two task-specific models for mazedetection: one for conversations, and the otherfor narrative tasks.
A conversational modeltrained on the CONVERSATION and NZCON-VERSATION corpora does not improve perfor-mance on either corpus relative to the base-line.
A model for narrative tasks trained on theENNI, GILLAMNT, NARRATIVESSS, NARRA-TIVESTORYRETELL, NZPERSONALNARRATIVEand NZSTORYRETELL corpora only improvesperformance on one of these, relative to the base-line.
Specifically, the narrative task model im-proves performance on the NARRATIVESSS cor-pus both in terms of tagging (F1=0.797, p ?0.0005) and bracketing (F1=0.693, p ?
0.0002).4.5.3 Research Group-Specific ModelsThere are two groups of researchers that haveannotated multiple corpora: a group in NewZealand, which annotated the NZCONVERSA-TION, NZPERSONALNARRATIVE, and NZSTO-RYRETELL corpora; and another group in Wis-consin, which annotated the CONVERSATION,EXPOSITORY, NARRATIVESSS, and NARRA-TIVESTORYRETELL corpora.
We trained re-search group-specific models, one for each ofthese groups.Overall, these models do not improve perfor-mance.
The New Zealand research group modeldoes not significantly improve performance on anyof the corpora they annotated, relative to the base-line.
The Wisconsin research group model yieldssignificant improvement on the NARRATIVESSScorpus, both in terms of tagging (F1=0.803, p ?0.0001) and bracketing (F1=0.699, p ?
0.0001)performance.
Performance on the CONVERSA-TION and EXPOSITORY corpora is lower withthe Wisconsin research group model than withthe corpus-specific baseline models, while perfor-mance on NARRATIVESTORYRETELL is essen-tially the same with the two models.5 DiscussionWe compared corpus-specific models for maze de-tection to more generic models applicable to mul-tiple corpora, and found that the generic modelsperformed worse than the corpus-specific ones.This was surprising because the more genericmodels were able to leverage more training datathan the corpus specific ones, and more trainingdata typically improves the performance of data-driven models such as our maze detector.
Theseresults strongly suggest that there are substantialdifferences between the nine SALT corpora.We suspect there are many areas in which theSALT corpora diverge from one another.
Onesuch area may be the nature of the language: per-haps the language differs so much between eachof the corpora that it is difficult to learn a modelappropriate for one corpus from any of the oth-ers.
Another potential source of divegence is intranscription, which does not always follow theSALT guidelines (Miller et al., 2011).
Two of theidiosyncracies we have observed are: more thanthree X?s (or a consonant followed by multipleX?s) to indicate unintelligble language, instead ofthe conventional X, XX, and XXX for unintelligi-ble words, phrases, and utterances, respectively;and non-canonical transcriptions of what appearto be filled pauses, including ?uhm?
and ?umhm?.These idiosyncracies could be straightforward tonormalize using automated methods, but doing sorequires that they be identified to begin with.
Fur-thermore, although these idiosyncracies may ap-pear to be minor, taken together they may actuallybe substantial.Another potential source of variation betweencorpora is likely in the maze annotations them-selves.
SALT?s definition of mazes, ?filled pauses,false starts, and repetitions and revisions of words,morphemes and phrases?
(Miller et al., 2011, p.48), is very short, and none of the componentsis defined in the SALT manual.
In contrast, theDisfluency Annotation Stylebook for SwitchboardCorpus (Meteer et al., 1995) describes a systemof disfluency annotations over approximately 25pages, devoting two pages to filled pauses and fiveto restarts.
The Switchboard disfluency annota-tions are much richer than SALT maze annota-tions, and we are not suggesting that they are ap-propriate for a clinical setting.
However, betweenthe stark contrast in detail of the two annotationsystems?
guidelines, and our finding that cross-corpus models for maze detection perform poorly,we recommend that SALT?s definition of mazesand their components be elaborated and clarified.This would be of benefit not just to those trying to75automate the application of SALT annotations, butalso to clinicians who use SALT and depend uponconsistently annotated transcripts.There are two clear tasks for future research thatbuild upon these results.
First, maze detection per-formance can surely be improved.
We note, how-ever, that evaluating maze detectors in terms of F1score may not always be appropriate if such a de-tector is used in a pipeline.
For example, theremay be a minimum acceptable level of precisionfor a maze detector used in a preprocessing stepto applying SALT error codes so that maze exci-sion does not create additional errors.
In such ascenario, the goal would be to maximize recall ata given level of precision.The second task suggested by this paper is to ex-plore the hypothesized differences within and be-tween corpora.
Such exploration could ultimatelyresult in more rigorous, communicable guidelinesfor maze annotations, as well as other annotationsand conventions in SALT.
If there are systematicdifferences in maze annotations across the SALTcorpora, such exploration could suggest ways ofmaking the annotations consistent without com-pletely redoing them.AcknowledgmentsWe would like to thank members of the ASD re-search group at the Center for Spoken LanguageUnderstanding at OHSU, for useful input into thisstudy: Jan van Santen, Alison Presmanes Hill,Steven Bedrick, Emily Prud?hommeaux, KyleGorman and Masoud Rouhizadeh.
This researchwas supported in part by NIH NIDCD awardR01DC012033 and NSF award #0826654.
Anyopinions, findings, conclusions or recommenda-tions expressed in this publication are those of theauthors and do not reflect the views of the NIH orNSF.ReferencesEugene Charniak and Mark Johnson.
2001.
Edit detec-tion and parsing for transcribed speech.
In Proceed-ings of the second meeting of the North AmericanChapter of the Association for Computational Lin-guistics on Language technologies, pages 1?9.
As-sociation for Computational Linguistics.Michelle Dunn, Judith Flax, Martin Sliwinski, andDorothy Aram.
1996.
The use of spontaneous lan-guage measures as criteria for identifying childrenwith specific language impairment: An attempt toreconcile clinical and research incongruence.
Jour-nal of Speech and Hearing research, 39(3):643.Ling-yu Guo, J Bruce Tomblin, and Vicki Samel-son.
2008.
Speech disruptions in the narrativesof english-speaking children with specific languageimpairment.
Journal of Speech, Language, andHearing Research, 51(3):722?738.John J Heilmann.
2010.
Myths and realities of lan-guage sample analysis.
SIG 1 Perspectives on Lan-guage Learning and Education, 17(1):4?8.Mark Johnson and Eugene Charniak.
2004.
A tag-based noisy-channel model of speech repairs.
InProceedings of the 42nd Meeting of the Associationfor Computational Linguistics (ACL?04), Main Vol-ume, pages 33?39, Barcelona, Spain, July.Judith R Johnston.
2006.
Thinking about child lan-guage: Research to practice.
Thinking Publications.Willem JM Levelt.
1993.
Speaking: From intention toarticulation, volume 1.
MIT press, Cambridge, MA.Barbara G MacLachlan and Robin S Chapman.
1988.Communication breakdowns in normal and lan-guage learning-disabled children?s conversation andnarration.
Journal of Speech and Hearing Disor-ders, 53(1):2.Marie W Meteer, Ann A Taylor, Robert MacIntyre,and Rukmini Iyer.
1995.
Dysfluency annotationstylebook for the switchboard corpus.
University ofPennsylvania.Jon Miller and Robin Chapman.
1985.
Systematicanalysis of language transcripts.
Madison, WI: Lan-guage Analysis Laboratory.Jon F Miller, Karen Andriacchi, and Ann Nockerts.2011.
Assessing language production using SALTsoftware: A clinician?s guide to language sampleanalysis.
SALT Software, LLC.Christine Nakatani and Julia Hirschberg.
1993.
Aspeech-first model for repair detection and correc-tion.
In Proceedings of the 31st Annual Meetingof the Association for Computational Linguistics,pages 46?53, Columbus, Ohio, USA, June.
Associ-ation for Computational Linguistics.Marilyn A Nippold, Tracy C Mansfield, Jesse L Billow,and J Bruce Tomblin.
2008.
Expository discoursein adolescents with language impairments: Exam-ining syntactic development.
American Journal ofSpeech-Language Pathology, 17(4):356?366.Eric W Noreen.
1989.
Computer intensive methodsfor testing hypotheses.
an introduction.
1989.
JohnWiley & Sons, 2(5):33.Albert Postma and Herman Kolk.
1993.
The covertrepair hypothesis: Prearticulatory repair processes innormal and stuttered disfluencies.
Journal of Speechand Hearing Research, 36(3):472.76Xian Qian and Yang Liu.
2013.
Disfluency detectionusing multi-step stacked learning.
In Proceedings ofthe 2013 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 820?825, At-lanta, Georgia, June.
Association for ComputationalLinguistics.Christina Reuterski?old Wagner, Ulrika Nettelbladt, Bir-gitta Sahl?en, and Claes Nilholm.
2000.
Conver-sation versus narration in pre-school children withlanguage impairment.
International Journal of Lan-guage & Communication Disorders, 35(1):83?93.Matthew Rispoli, Pamela Hadley, and Janet Holt.2008.
Stalls and revisions: A developmental per-spective on sentence production.
Journal of Speech,Language, and Hearing Research, 51(4):953?966.Cheryl M Scott and Jennifer Windsor.
2000.
Generallanguage performance measures in spoken and writ-ten narrative and expository discourse of school-agechildren with language learning disabilities.
Journalof Speech, Language & Hearing Research, 43(2).Eleanor Messing Semel, Elisabeth Hemmersam Wiig,and Wayne Secord.
2003.
Clinical evaluation oflanguage fundamentals.
The Psychological Corpo-ration, A Harcourt Assessment Company, Toronto,Canada, fourth edition.Helen Tager-Flusberg, Sally Rogers, Judith Cooper,Rebecca Landa, Catherine Lord, Rhea Paul, Ma-bel Rice, Carol Stoel-Gammon, Amy Wetherby, andPaul Yoder.
2009.
Defining spoken language bench-marks and selecting measures of expressive lan-guage development for young children with autismspectrum disorders.
Journal of Speech, Languageand Hearing Research, 52(3):643.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003.Maximum-margin markov networks.
In Neural In-formation Processing Systems (NIPS).Elin T Thordardottir and Susan Ellis Weismer.
2001.Content mazes and filled pauses in narrative lan-guage samples of children with specific languageimpairment.
Brain and cognition, 48(2-3):587?592.Danielle Wetherell, Nicola Botting, and Gina Conti-Ramsden.
2007.
Narrative in adolescent specificlanguage impairment (sli): A comparison with peersacross two different narrative genres.
InternationalJournal of Language & Communication Disorders,42(5):583?605.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Pro-ceedings of the 18th conference on Computationallinguistics-Volume 2, pages 947?953.
Associationfor Computational Linguistics.Simon Zwarts and Mark Johnson.
2011.
The impactof language models and loss functions on repair dis-fluency detection.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages703?711, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.77
