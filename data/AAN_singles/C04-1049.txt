Talking Robots With LEGO MindStormsAlexander KollerSaarland UniversitySaarbru?cken, Germanykoller@coli.uni-sb.deGeert-Jan M. KruijffSaarland UniversitySaarbru?cken, Germanygj@coli.uni-sb.deAbstractThis paper shows how talking robots can be builtfrom off-the-shelf components, based on the LegoMindStorms robotics platform.
We present fourrobots that students created as final projects in aseminar we supervised.
Because Lego robots are soaffordable, we argue that it is now feasible for anydialogue researcher to tackle the interesting chal-lenges at the robot-dialogue interface.1 IntroductionEver since Karel ?Capek introduced the word ?robot?in his 1921 novel Rossum?s Universal Robots andthe subsequent popularisation through Issac Asi-mov?s books, the idea of building autonomousrobots has captured people?s imagination.
The cre-ation of an intelligent, talking robot has been the ul-timate dream of Artificial Intelligence from the verystart.Yet, although there has been a tremendousamount of AI research on topics such as controland navigation for robots, the issue of integrat-ing dialogue capabilities into a robot has only re-cently started to receive attention.
Early successeswere booked with Flakey (Konolige et al, 1993),a voice-controlled robot which roamed the corri-dors of SRI.
Since then, the field of socially in-teractive robots has established itself (see (Fong etal., 2003)).
Often-cited examples of such inter-active robots that have a capability of communi-cating in natural language are the humanoid robotROBOVIE (Kanda et al, 2002) and robotic mu-seum tour guides like RHINO (Burgard et al, 1999)(Deutsches Museum Bonn), its successor MINERVAtouring the Smithsonian in Washington (Thrun etal., 2000), and ROBOX at the Swiss National Ex-hibition Expo02 (Siegwart and et al 2003).
How-ever, dialogue systems used in robotics appear tobe mostly restricted to relatively simple finite-state,query/response interaction.
The only robots in-volving dialogue systems that are state-of-the-art incomputational linguistics (and that we are aware of)are those presented by Lemon et al (2001), Sidneret al (2003) and Bos et al (2003), who equippeda mobile robot with an information state based dia-logue system.There are two obvious reasons for this gap be-tween research on dialogue systems in robotics onthe one hand, and computational linguistics on theother hand.
One is that the sheer cost involvedin buying or building a robot makes traditionalrobotics research available to only a handful of re-search sites.
Another is that building a talking robotcombines the challenges presented by robotics andnatural language processing, which are further ex-acerbated by the interactions of the two sides.In this paper, we address at least the first prob-lem by demonstrating how to build talking robotsfrom affordable, commercial off-the-shelf (COTS)components.
We present an approach, tested in aseminar taught at the Saarland University in Win-ter 2002/2003, in which we combine the LegoMindStorms system with COTS software for speechrecognition/synthesis and dialogue modeling.The Lego MindStorms1 system extends the tra-ditional Lego bricks with a central control unit (theRCX), as well as motors and various kinds of sen-sors.
It provides a severely limited computationalplatform from a traditional robotics point of view,but comes at a price of a few hundred, rather thantens of thousands of Euros per kit.
Because Mind-Storms robots can be flexibly connected to a dia-logue system running on a PC, this means that af-fordable robots are now available to dialogue re-searchers.We present four systems that were built by teamsof three students each under our supervision, anduse off-the-shelf components such as the Mind-Storms kits, a dialogue system, and a speech recog-niser and synthesis system, in addition to commu-nications software that we ourselves wrote to linkall the components together.
It turns out that using1LEGO and LEGO MindStorms are trademarks of theLEGO Company.this accessible technology, it is possible to createbasic but interesting talking robots in limited time(7 weeks).
This is relevant not only for future re-search, but can also serve as a teaching device thathas shown to be extremely motivating for the stu-dents.
MindStorms are a staple in robotics educa-tion (Yu, 2003; Gerovich et al, 2003; Lund, 1999),but to our knowledge, they have never been used aspart of a language technology curriculum.The paper is structured as follows.
We firstpresent the basic setup of the MindStorms systemand the software architecture.
Then we present thefour talking robots built by our students in some de-tail.
Finally, we discuss the most important chal-lenges that had to be overcome in building them.We conclude by speculating on further work in Sec-tion 5.2 ArchitectureLego MindStorms robots are built around a pro-grammable microcontroller, the RCX.
This unit,which looks like an oversized yellow Lego brick,has three ports each to attach sensors and motors,an infrared sender/receiver for communication withthe PC, and 32 KB memory to store the operatingsystem, a programme, and data.Figure 1: Architecture of a talking Lego robot.Our architecture for talking robots (Fig.
1) con-sists of four main modules: a dialogue system, aspeech client with speech recognition and synthesiscapabilities, a module for infrared communicationbetween the PC and the RCX, and the programmethat runs on the RCX itself.
Each student team hadto specify a dialogue, a speech recognition gram-mar, and the messages exchanged between PC andRCX, as well as the RCX control programme.
Allother components were off-the-shelf systems thatwere combined into a larger system by us.The centrepiece of the setup is the dialoguesystem.
We used the DiaWiz system by CLTFigure 2: The dialogue system.Sprachtechnologie GmbH2, a proprietary frame-work for defining finite-state dialogues (McTear,2002).
It has a graphical interface (Fig.
2) that al-lows the user to draw the dialogue states (shownas rectangles in the picture) and connect them viaedges.
The dialogue system connects to an arbitrarynumber of ?clients?
via sockets.
It can send mes-sages to and receive messages from clients in eachdialogue state, and thus handles the entire dialoguemanagement.
While it was particularly convenientfor us to use the CLT system, it could probably re-placed without much effort by a VoiceXML-baseddialogue manager.The client that interacts most directly with theuser is a module for speech recognition and synthe-sis.
It parses spoken input by means of a recogni-tion grammar written in the Java Speech GrammarFormat, 3 and sends an extremely shallow semanticrepresentation of the best recognition result to thedialogue manager as a feature structure.
The out-put side can be configured to either use a speechsynthesiser, or play back recorded WAV files.
Ourimplementation assumes only that the recognitionand synthesis engines are compliant with the JavaSpeech API 4.The IR communication module has the task ofconverting between high-level messages that the di-2http://www.clt-st.de3http://java.sun.com/products/java-media/speech/forDevelopers/JSGF/4http://java.sun.com/products/java-media/speech/Figure 3: A robot playing chess.alogue manager and the RCX programme exchangeand their low-level representations that are actuallysent over the IR link, in such a way that the userneed not think about the particular low-level details.The RCX programme itself is again implemented inJava, using the Lejos system (Bagnall, 2002).
Sucha programme is typically small (to fit into the mem-ory of the microcontroller), and reacts concurrentlyto events such as changes in sensor values and mes-sages received over the infrared link, mostly by con-trolling the motors and sending messages back tothe PC.3 Some Robots3.1 Playing ChessThe first talking robot we present plays chessagainst the user (Fig.
3).
It moves chess pieces ona board by means of a magnetic arm, which it canmove up and down in order to grab and release apiece, and can place the arm under a certain posi-tion by driving back and forth on wheels, and to theright and left on a gear rod.The dialogue between the human player and therobot is centred around the chess game: The humanspeaks the move he wants to make, and the robotconfirms the intended move, and announces checkand checkmate.
In order to perform the moves forthe robot, the dialogue manager connects to a spe-cialised client which encapsulates the GNU Chesssystem.5 In addition to computing the moves thatthe robot will perform, the chess programme is alsoused in disambiguating elliptical player inputs.Figure 4 shows the part of the chess dialoguemodel that accepts a move as a spoken commandfrom the player.
The Input node near the top waitsfor the speech recognition client to report that it5http://www.gnu.org/software/chess/chess.htmlFigure 4: A small part of the Chess dialogue.<cmd> = [<move>] <piece> <to> <squareTo>| ...<squareTo> = <colTo> <rowTo><colTo> = [a wie] anton {colTo:a} |[b wie] berta {colTo:b} | ...<rowTo> = eins {rowTo:1} |zwei {rowTo:2} | ...Figure 5: A small part of the Chess grammar.understood a player utterance as a command.
Anexcerpt from the recogniser grammar is shown inFig.
5: The grammar is a context-free grammar inJSGF format, whose production rules are annotatedwith tags (in curly brackets) representing a veryshallow semantics.
The tags for all production rulesused in a parse tree are collected into a table.The dialogue manager then branches depend-ing on the type of the command given by theuser.
If the command specified the piece and targetsquare, e.g.
?move the pawn to e4?, the recogniserwill return a representation like {piece="pawn"colTo="e" rowTo="4"}, and the dialogue willcontinue in the centre branch.
The user can alsospecify the source and target square.If the player confirms that the move commandwas recognised correctly, the manager sends themove description to the chess client (the ?sendmove?
input nodes near the bottom), which can dis-ambiguate the move description if necessary, e.g.by expanding moves of type ?move the pawn toe4?
to moves of type ?move from e2 to e4?.
Notethat the reference ?the pawn?
may not be globallyunique, but if there is only one possible referent thatcould perform the requested move, the chess clientresolves this automatically.The client then sends a message to the RCX,which moves the piece using the robot arm.
It up-dates its internal data structures, as well as the GNUChess representations, computes a move for itself,and sends this move as another message to the RCX.While the dialogue system as it stands already of-fers some degree of flexibility with regard to movephrasings, there is still plenty of open room for im-provements.
One is to use even more context infor-mation, in order to understand commands like ?takeit with the rook?.
Another is to incorporate recentwork on improving recognition results in the chessdomain by certain plausibility inferences (Gabsdil,2004).3.2 Playing a Shell GameFigure 6 introduces Luigi Legonelli.
The robot rep-resents a charismatic Italian shell-game player, andengages a human player in style: Luigi speaks Ger-man with a heavy Italian accent, lets the humanplayer win the first round, and then tries to pull sev-eral tricks either to cheat or to keep the player inter-ested in the game.Figure 6: A robot playing a shell game.Luigi?s Italian accent was obtained by feedingtransliterated German sentences to a speech synthe-sizer with an Italian voice.
Although the resultingaccent sounded authentic, listeners who were unfa-miliar with the accent had trouble understanding it.For demonstration purposes we therefore decided touse recorded speech instead.
To this end, the Italianstudent on the team lent his voice for the differentsentences uttered by Luigi.The core of Luigi?s dialogue model reflects theprogress of game play in a shell game.
At the start,Luigi and the player settle on a bet (between 1 and10 euros), and Luigi shows under which shell thecoin is.
Then, Luigi manipulates the shells (seealso below), moving them (and the coin) around theboard, and finally asks the player under which shellthe player believes the coin is.
Upon the player?sguess Luigi lifts the shell indicated by the player,and either loudly exclaims the unfairness of life (ifhe has lost) or kindly inquires after the player?svisual capacities (in case the player has guessedwrong).
At the end of the turn, Luigi asks the playerwhether he wants to play again.
If the player wouldlike to stop, Luigi tries to persuade the player tostay; only if the player is persistent, Luigi will endthe game and beat a hasty retreat.
(1) rob ?Ciao, my name is Luigi Legonelli.Do you feel like a little game?
?usr ?Yes ... ?rob ?The rules are easy.
I move da cuppa,you know, cuppa?
You look, say wherecoin is.
How much money you bet?
?usr ?10 Euros.
?rob (Luigi moves the cups/shells)rob ?So, where is the coin?
What do youthink, where?s the coin?
?usr ?Cup 1?rob ?Mamma mia!
You have won!
Whotold you, where is coin?!
Anothergame?
Another game!
?usr ?No.
?rob ?Come!
Play another game!
?usr ?No.
?rob ?Okay, ciao signorina!
Police, muchpolice!
Bye bye!
?The shells used in the game are small cups with ametal top (a nail), which enables Luigi to pick themup using a ?hand?
constructed around a magnet.The magnet has a downward oriented, U-shapedconstruction that enables Luigi to pick up two cupsat the same time.
Cups then get moved aroundthe board by rotating the magnet.
By magnetizingthe nail at the top of the cup, not only the cup butalso the coin (touched by the tip of the nail) can bemoved.
When asked to show whether the coin is un-der a particular shell, one of Luigi?s tricks is to keepthe nail magnetized when lifting a cup ?
thus alsolifting the coin, giving off the impression that therewas no coin under the shell.The Italian accent, the android shape of the robot,and the ?authentic?
behavior of Luigi all contributedto players genuinely getting engaged in the game.After the first turn, having won, most players ac-knowledged that this is an amusing Lego construc-tion; when they were tricked at the end of the sec-ond turn, they expressed disbelief; and when weshowed them that Luigi had deliberately cheatedthem, astonishment.
At that point, Luigi had ceasedto be simply an amusing Lego construction and hadachieved its goal as an entertainment robot that canimmerse people into its game.3.3 Exploring a pyramidThe robot in Figure 7, dubbed ?Indy?, is inspiredby the various robots that have been used to explorethe Great Pyramids in Egypt (e.g.
Pyramid Rover6,UPUAUT7).
It has a digital videocamera (webcam)and a lamp mounted on it, and continually transmitsimages from inside the pyramid.
The user, watch-ing the images of the videocamera on a computerscreen, can control the robot?s movements and theangle of the camera by voice.Figure 7: A robot exploring a pyramid.Human-robot interaction is crucial to the explo-ration task, as neither user nor robot has a com-plete picture of the environment.
The robot is awareof the environment through an (all-round) array oftouch-sensors, enabling it to detect e.g.
openings inwalls; the user receives a more detailed picture, but6http://www.newscientist.com/news/news.jsp?id=ns999928057http://www.cheops.orgonly of the environment straight ahead of the robot(due to the frontal orientation of the camera).The dialogue model for Indy defines the possibleinteraction that enables Indy and the user to jointlyexplore the environment.
The user can initiate a di-alogue to control the camera and its orientation (byletting the robot turn on the spot, in a particular di-rection), or to instruct the robot to make particularmovements (i.e.
turn left or right, stop).3.4 Traversing a labyrinthA variation on the theme of human-robot interactionin navigation is the robot in Figure 8.
Here, the userneeds to guide a robot through a labyrinth, specifiedby thick black lines on a white background.
Thetask that the robot and the human must solve col-laboratively is to pick up objects randomly strewnabout the maze.
The robot is able to follow the blacklines lines (the ?path?)
by means of an array of threelight sensors at its front.Figure 8: A robot traversing a labyrinth.Both the user and the robot can take the initia-tive in the dialogue.
The robot, capable of spottingcrossings (and the possibilities to go straight, leftand/or right), can initiate a dialogue asking for di-rections if the user had not instructed the robot be-forehand; see Example 2.
(2) rob (The robot arrives at a crossing; itrecognizes the possibility to go eitherstraight or left; there are no current in-structions)rob ?I can go left or straight ahead; whichway should I go?
?usr ?Please go right.
?rob ?I cannot go right here.usr ?Please go straight.
?rob ?Okay.
?The user can give the robot two different types ofdirections: in-situ directions (as illustrated in Ex-ample 2) or deictic directions (see Example 3 be-low).
This differentiates the labyrinth robot fromthe pyramid robot described in ?3.3, as the lattercould only handle in-situ directions.
(3) usr ?Please turn left at the next crossing.
?rob ?Okay?rob (The robot arrives at a crossing; itrecognizes the possibility to go eitherstraight or left; it was told to go left atthe next crossing)rob (The robot recognizes it can go leftand does so, as instructed)4 DiscussionThe first lesson we can learn from the work de-scribed above is that affordable COTS products indialogue and robotics have advanced to the pointthat it is feasible to build simple but interesting talk-ing robots with limited effort.
The Lego Mind-Storms platform, combined with the Lejos system,turned out to be a flexible and affordable roboticsframework.
More ?professional?
robots have thedistinct advantage of more interesting sensors andmore powerful on-board computing equipment, andare generally more physically robust, but LegoMindStorms is more than suitable for robotics ex-perimentation under controlled circumstances.Each of the robots was designed, built, and pro-grammed within twenty person-weeks, after an ini-tial work phase in which we created the basic in-frastructure shown in Figure 1.
One prerequisite ofthis rather efficient development process was thatthe entire software was built on the Java platform,and was kept highly modular.
Speech software ad-hering to the Java Speech API is becoming avail-able, and plugging e.g.
a different JSAPI-compliantspeech recogniser into our system is now a matterof changing a line in a configuration file.However, building talking robots is still a chal-lenge that combines the particular problems of dia-logue systems and robotics, both of which introducesituations of incomplete information.
The dialogueside has to robustly cope with speech recognition er-rors, and our setup inherits all limitations inherent infinite-state dialogue; applications having to do e.g.with information seeking dialogue would be betterserved with a more complex dialogue model.
Onthe other hand, a robot lives in the real world, andhas to deal with imprecisions in measuring its po-sition, unexpected obstacles, communications withthe PC breaking off, and extremely limited sensoryinformation about its surroundings.5 ConclusionThe robots we developed together with our stu-dents were toy robots, looked like toy robots, andcould (given the limited resources) only deal withtoy examples.
However, they confirmed that thereare affordable COTS components on the marketwith which we can, even in a limited amount oftime, build engaging talking robots that capture theessence of various (potential) real-life applications.The chess and shell game players could be used asentertainment robots.
The labyrinth and pyramidrobots could be extended into tackling real-worldexploration or rescue tasks, in which robots searchfor disaster victims in environments that are toodangerous for rescuers to venture into.8 Dialoguecapabilities are useful in such applications not justto communicate with the human operator, but alsopossibly with disaster victims, to check their condi-tion.Moreover, despite the small scale of these robots,they show genuine issues that could provide in-teresting lines of research at the interface betweenrobotics and computational linguistics, and in com-putational linguistics as such.
Each of our robotscould be improved dramatically on the dialogue sidein many ways.
As we have demonstrated that theequipment for building talking robots is affordabletoday, we invite all dialogue researchers to join usin making such improvements, and in investigat-ing the specific challenges that the combination ofrobotics and dialogue bring about.
For instance, arobot moves and acts in the real world (rather thana carefully controlled computer system), and suffersfrom uncertainty about its surroundings.
This limitsthe ways in which the dialogue designer can use vi-sual context information to help with reference res-olution.Robots, being embodied agents, present a hostof new challenges beyond the challenges we facein computational linguistics.
The interpretation oflanguage needs to be grounded in a way that isboth based in perception, and on conceptual struc-tures to allow for generalization over experiences.Naturally, this problem extends to the acquisitionof language, where approaches such as (Nicolescuand Mataric?, 2001; Carbonetto and Freitos, 2003;Oates, 2003) have focused on basing understandingentirely in sensory data.Another interesting issue concerns the interpreta-tion of deictic references.
Research in multi-modal8See also http://www.rescuesystem.org/robocuprescue/interfaces has addressed the issue of deictic refer-ence, notably in systems that allow for pen-input(see (Oviatt, 2001)).
Embodied agents raise thecomplexity of the issues by offering a broader rangeof sensory input that needs to be combined (cross-modally) in order to establish possible referents.Acknowledgments.
The authors would like tothank LEGO and CLT Sprachtechnologie for pro-viding free components from which to build ourrobot systems.
We are deeply indebted to our stu-dents, who put tremendous effort into designing andbuilding the presented robots.
Further informationabout the student projects (including a movie) isavailable at the course website, http://www.coli.uni-sb.de/cl/courses/lego-02.ReferencesBrian Bagnall.
2002.
Core Lego Mindstorms Pro-gramming.
Prentice Hall, Upper Saddle RiverNJ.Johan Bos, Ewan Klein, and Tetsushi Oka.
2003.Meaningful conversation with a mobile robot.
InProceedings of the 10th EACL, Budapest.W.
Burgard, A.B.
Cremers, D. Fox, D. Ha?hnel,G.
Lakemeyer, D. Schulz, W. Steiner, andS.
Thrun.
1999.
Experiences with an interactivemuseum tour-guide robot.
Artificial Intelligence,114(1-2):3?55.Peter Carbonetto and Nando de Freitos.
2003.
Whycan?t Jose?
talk?
the problem of learning se-mantic associations in a robot environment.
InProceedings of the HLT-NAACL 2003 Workshopon Learning Word Meaning from Non-LinguisticData, pages 54?61, Edmonton, Canada.Terrence W Fong, Illah Nourbakhsh, and KerstinDautenhahn.
2003.
A survey of socially interac-tive robots.
Robotics and Autonomous Systems,42:143?166.Malte Gabsdil.
2004.
Combining acoustic confi-dences and pragmatic plausibility for classifyingspoken chess move instructions.
In Proceedingsof the 5th SIGdial Workshop on Discourse andDialogue.Oleg Gerovich, Randal P. Goldberg, and Ian D.Donn.
2003.
From science projects to the en-gineering bench.
IEEE Robotics & AutomationMagazine, 10(3):9?12.Takayuki Kanda, Hiroshi Ishiguro, Tetsuo Ono, Mi-chita Imai, and Ryohei Nakatsu.
2002.
Develop-ment and evaluation of an interactive humanoidrobot ?robovie?.
In Proceedings of the IEEE In-ternational Conference on Robotics and Automa-tion (ICRA 2002), pages 1848?1855.Kurt Konolige, Karen Myers, Enrique Ruspini,and Alessandro Saffiotti.
1993.
Flakey in ac-tion: The 1992 aaai robot competition.
Techni-cal Report 528, AI Center, SRI International, 333Ravenswood Ave., Menlo Park, CA 94025, Apr.Oliver Lemon, Anne Bracy, Alexander Gruenstein,and Stanley Peters.
2001.
A multi-modal dia-logue system for human-robot conversation.
InProceedings NAACL 2001.Henrik Hautop Lund.
1999.
AI in children?s playwith LEGO robots.
In Proceedings of AAAI 1999Spring Symposium Series, Menlo Park.
AAAIPress.Michael McTear.
2002.
Spoken dialogue technol-ogy: enabling the conversational user interface.ACM Computing Surveys, 34(1):90?169.Monica N. Nicolescu and Maja J. Mataric?.
2001.Learning and interacting in human-robot do-mains.
IEEE Transactions on Systems, Man andCybernetics, 31.Tim Oates.
2003.
Grounding word meaningsin sensor data: Dealing with referential un-certainty.
In Proceedings of the HLT-NAACL2003 Workshop on Learning Word Meaning fromNon-Linguistic Data, pages 62?69, Edmonton,Canada.Sharon L. Oviatt.
2001.
Advances in the robustprocessing of multimodal speech and pen sys-tems.
In P. C. Yuen, Y.Y.
Tang, and P.S.
Wang,editors, Multimodal InterfacesB for Human Ma-chine Communication, Series on Machine Per-ception and Artificial Intelligence, pages 203?218.
World Scientific Publisher, London, UnitedKingdom.Candace L. Sidner, Christopher Lee, and Neal Lesh.2003.
Engagement by looking: Behaviors forrobots when collaborating with people.
In Pro-ceedings of the 7th workshop on the semanticsand pragmatics of dialogue (DIABRUCK).R.
Siegwart and et al 2003.
Robox at expo.02:A large scale installation of personal robots.Robotics and Autonomous Systems, 42:203?222.S.
Thrun, M. Beetz, M. Bennewitz, W. Burgard,A.B.
Cremers, F. Dellaert, D. Fox, D. Ha?hnel,C.
Rosenberg, N. Roy, J. Schulte, and D. Schulz.2000.
Probabilistic algorithms and the interactivemuseum tour-guide robot minerva.
InternationalJournal of Robotics Research, 19(11):972?999.Xudong Yu.
2003.
Robotics in education: Newplatforms and environments.
IEEE Robotics &Automation Magazine, 10(3):3.
