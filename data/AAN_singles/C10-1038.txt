Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 331?339,Beijing, August 2010Enriching Dictionaries with Images from the Internet- Targeting Wikipedia and a Japanese Semantic Lexicon: Lexeed -Sanae FujitaNTT Communication Science Lab.sanae@cslab.kecl.ntt.co.jpMasaaki NagataNTT Communication Science Lab.nagata.masaaki@lab.ntt.co.jpAbstractWe propose a simple but effective methodfor enriching dictionary definitions withimages based on image searches.
Vari-ous query expansion methods using syn-onyms/hypernyms (or related words) areevaluated.
We demonstrate that ourmethod is effective in obtaining high-precision images that complement dictio-nary entries, even for words with abstractor multiple meanings.1 IntroductionThe Internet is an immense resource for images.If we can form connections between these im-ages and dictionary definitions, we can createrich dictionary resources with multimedia infor-mation.
Such dictionaries have the potential toprovide educational (Popescu et al, 2006), cross-langauge information retrieval (Hayashi et al,2009) or assistive communication tools especiallyfor children, language learners, speakers of differ-ent languages, and people with disabilities suchas dyslexia (Mihalcea and Leong, 2008; Goldberget al, 2009).Additionally, a database of typical images con-nected to meanings has the potential to fill thegaps between images and meanings (semanticgap).
There are many studies which aim to crossthe semantic gap (Ide and Yanai, 2009; Smeulderset al, 2000; Barnard et al, 2003) from the pointof view of image recognition.
However the se-mantic classes of target images are limited (e.g.Caltech-101, 2561).
Yansong and Lapata (2008)tried to construct image databases annotated withkeywords from Web news images with their cap-tions and articles, though the semantic coverage is1http://www.vision.caltech.edu/Image Datasets/Caltech101,256/unknown.
In this paper, we aim to supply sev-eral suitable images for dictionary definitions.
Wepropose a simple but effective method based on anInternet image search.There have been several studies related to sup-plying images for a dictionary or thesaurus.
Bondet al (2009) applied images obtained from theOpen Clip Art Library (OCAL) to Japanese Word-Net.2 They obtained candidate images by compar-ing the hierarchical structures of OCAL and Word-Net, and then judged whether or not the image wassuitable for the synset by hand.
OCAL benefitsfrom being in the public domain; however, it can-not cover a wide variety of meanings because ofthe limited number of available images.Fujii and Ishikawa (2005) collected imagesand text from the Internet by querying lemma,and linked them to an open encyclopedia, CY-CLONE.3 They guessed the meaning of the im-ages by disambiguating the surrounding text.
Thisis a straightforward approach, but it is difficult touse it to collect images with minor meanings, be-cause in most cases the Internet search queryinglemma only provides images related to the mostcommon meaning.
For example, lemma y?arch may mean ??architecture??
or ?
?home run?
?in Japanese, but a lemma search provided no im-age of the latter at least in the top 500.There are some resources which link images totarget synsets selected from WordNet (Fellbaum,1998).
For example, PicNet (Borman et al, 2005),ImageNet (Deng et al, 2009) and image ontology(Popescu et al, 2006, 2007; Zinger et al, 2006)collect candidate images from the Internet.
PicNetand ImageNet ask Web users to judge their suitabil-ity, and Zinger et al (2006); Popescu et al (2007)automatically filtered out unsuitable images us-ing visual characteristics.
These approaches can2http://nlpwww.nict.go.jp/wn-ja/3http://cyclone.cl.cs.titech.ac.jp/331?????????????????????????????
?INDEX y?
arch (POS: noun)SENSE 1???????????
?DEFINITION ?1 k?1 G41 D08m1 W89 6G3 m?1 T?2Buildings with bow-shaped top.
Or its architectural style.EXAMPLE G2 ?1 H?=Gy?1 @wo4 ?
dThat bridge has 2 arches.HYPERNYM m1 building,T?2 styleSEM.
CLASS ?865:house (main building)?
(?
?2:concrete?
),?2435:pattern, method?
(?
?1000:abstract?)??????????????IMAGE?
?SENSE 3?????????
?DEFINITION ?1 @?
?D1 ???
?1  A home run in baseball.EXAMPLE ??
?1 %?
{?4 ???
?2 D?U3 Gy?3 ki<4 8A batter blasted the ball over the right-field wall.HYPERNYM ?
?D1 honruidaSYNONYM ???
?1 home run, DOMAIN ?1 baseballSEM.
CLASS ?1680:sport?
(?
?1000:abstract?)?????????????IMAGE????????????????????????????????
?Figure 1: Simplified Entry for Lexeed & Hinoki:y?
archcollect a large number of highly accurate images.However, target synsets are limited at present, andthe coverage of polysemous words is unknown.We present a comparison with ImageNet and im-age ontology (Popescu et al, 2006) in ?
3.In this paper, to cover a broad range of mean-ings, we use an Internet search.
In advance, we ex-pand the number of queries per meaning using in-formation extracted from definition sentences.
In?
3, we investigate the usability and effectivenessof several types of information targeting two dif-ferent types of dictionaries, a Japanese SemanticLexicon: Lexeed and a Web Dictionary: JapaneseWikipedia4 (?
2).
We show that our method is sim-ple but effective.
We also analyze senses that aredifficult to portray using images.2 Resources2.1 Japanese Semantic Lexicon: LexeedWe use Lexeed, a Japanese Semantic Lexicon(Kasahara et al, 2004) as a target dictionary (seeFigure 1).
Lexeed includes the 29,000 most famil-iar words in Japanese, split into 48,000 senses.Each entry contains the word itself and its partof speech (POS) along with definition and ex-ample sentences and links to the Goi-Taikei (GT)Japanese Ontology (Ikehara et al, 1997).
In ad-dition, we extracted related words such as hyper-nyms, synonyms, and domains, from the defini-4http://ja.wikipedia.org/Table 1: Size of Lexeed and Japanese Wikipedia(disambiguation)Lexeed Wikipedia SharedNo.
LemmaEntries 29,272 33,299 2,228Senses 48,009 197,9121 19,703Ave.
Senses/Entry 1.6 5.9 8.8Max.
Senses/Entry 57 320 148Monosemous 19,080 74 2Ave.
Words/Definition2 14.4 10.7 11.01From the all 215,883 lists, we extracted lists showingsenses obtained by heuristics (see lines 2,3,4,6,7,9 and10 for Figure 2).2Analyzed by Mecab, http://mecab.sourceforge.net/tions (called Hinoki Ontology).
The images in Fig-ure 1 are samples provided using our method.2.2 Web Dictionary :Japanese WikipediaWe used Wikipedia?s disambiguation pages,5 as atarget dictionary (see Figure 2).
A disambigua-tion page lists articles (eg.
?
?European Union??,?
?Ehime University??)
associated with the samelemma (eg.
?EU?).
Our goal is to provide imagesfor each article listed.
As shown in Figure 2, theyinclude various writing styles.2.3 Comparison of Lexeed and WikipediaTable 1 shows the sizes of Lexeed and Wikipedia?sdisambiguation pages, and the shared entries.Shared entries are rare, and account for less than5Version 20091011.332Original (in Japanese)1 ???EU??
?2 * [[AJ?]]3 * [[Europa Universalis]]???
- [[??????{?????z?]]G[[???????????
]]4 * [[??d?
]](Ehime University) - [[??z]][[???]]Dd?
?G[[ ?d?
]]5 ???Eu??
?6 * [[?}??}?]]G?
?d7 * [[????y?]]
- ?
"?H8 ???eu??
?9 * [[.eu]] - AJ?G[[ 9??{?
]]10 * [[????
]]G[[ISO 639|ISO 639-1????
]]Gloss1 ???EU??
?2 * [[European Union]]3 * [[Europa Universalis]] series - a [[histori-cal computer game]] by [[Paradox Interactive]]4 * [[Ehime University]] - a [[National Univer-sity]] in [[Matsuyama]],[[Ehime Prefecture]]5 ???Eu??
?6 * [[Europium]]?s chemical element symbol7 * [[euphonium]] - a brass instrument8 ???eu??
?9 * [[.eu]] - [[country-code top-level domain]]for the European Union10 * [[ISO 639|ISO 639-1 language code]] of[[Basque]][[ ]] shows a link in Wikipedia.
And we assign each line a number for easy citation.Figure 2: Simplified Example of Wikipedia?s Disambiguation Page: ?EU (disambiguation)?10 % of the total 67.
As regards Lexeed, 16,685entries (57 %) do not appear in any of Wikipedia?slemmas, not only in disambiguation pages.8As shown in Table 1, Wikipedia has manysenses, but most of them are proper nouns.
Forexample, in Lexeed,????
sunflower is monose-mous, but in Wikipedia, 67 senses are listed,including 65 proper nouns besides ??plant?
?and ?
?sunflower oil??.
On the other hand,in Wikipedia, y?
arch has only one sense,??architecture??
corresponding to Lexeed?s y?1 arch, and has no disambiguation page.As mentioned above, Lexeed and Wikipedia havevery different types of entries and senses.
Thisresearch aims to investigate the possibility ofsupplying appropriate images for such differentsenses, and a method for obtaining better images.3 Experiment to Supply Images forWord SensesIn this paper, we propose a simple method forsupplying appropriate images for each dictionarysense of a word.
We collect candidate imagesfrom the Internet by using a querying imagesearch.
To obtain images even for minor senses,we expand the query by appending queries ex-6Shared lemmas are 6I buckwheat noodle, ?{??cycle,???}
owl, etc.7Lemmas only in Wikipedia are {???
Aesop, ?
?Biot/Veoh,?Gi fall name, etc.8Lemmas only in Lexeed are? pay later, ???
?humorous,e> selection, etc.tracted from definitions for each sense.In this paper, we investigated two main typesof expansion, that is, the appending of mainlysynonyms (SYN), and related words including hy-pernyms (LNK).
For information retrieval, queryexpansion using synonyms has been adopted inseveral studies (Voorhees, 1994; Fang and Zhai,2006; Unno et al, 2008).
Our LNK is similar tomethods used in Deng et al (2009), but we notethat their goal is not to give images to polysemouswords (which is our intention).
Popescu et al(2006) also used synonyms (all terms in a synset)and hypernyms (immediate supertype in WordNet),but they did not investigate the effectiveness ofeach expansion and they forcus only on selectedobject synsets.3.1 Experimental and Evaluation MethodWe collected five candidate images for each sensefrom the Internet by querying an image search en-gine.9 Then we manually evaluated the suitabil-ity of the image for explaining the target sense.The evaluator determined whether or not the im-age was appropriate (T), acceptable (M), or inap-propriate (F).
The evaluator also noted the reasonsfor F.Figure 3 shows an example for8WF' onion.As shown in Figure 3, the evaluator determined T,M or F for each candidate image.9We used Google AJAX images API,http://code.google.com/intl/ja/apis/ajaxsearch/333(1) (2) (3) (4) (5)T (Appropriate) F (Inappropriate) M (Acceptable) T (Appropriate) T (Appropriate)Figure 3: Examples of Candidate Images and Evaluations for8WF' onionTable 2: Data for Hinoki OntologyType No.
% ExampleLemma Related WordHypernym 47,054 69.1 y?1 arch T?Synonym 14,068 20.6 y?3 arch ????
homerDomain 1,868 2.7 y?3 arch ?
baseballHyponym 757 1.1 7c61 buy and sell 7d sellMeronym 686 1.0 ?+1 lean ?
? fish meatAbbreviation 383 0.6 ?2 A(sia) y?y AsiaOther name 216 0.3 F0-X2 shave ?????
plug outletOther 3102 4.6 ^X?&1 papillote ?
fishTotal 68,134 100For an image that is related but that does not ex-plain the sense, the evaluation is F. For example,for 8WF' onion, the images of onion dishessuch as (2) in Figure 3 are F. On the other hand,the images that show onions themselves such as(1), (4) and (5) in Figure 3 are T. With (3) in Fig-ure 3, the image may show the onion itself or afield of onions, therefore the evaluation is M.One point of judgment, specifically between Tand M, is whether the image is typical or not.
With8WF' onion, most typical images are similar to(1), (4) and (5).
The image (3) may not be typi-cal but is helpful for understanding, and (2) maylead to a misunderstanding if this is the only im-age shown to the dictionary user.
This is why (3)is judged to be M and (2) is judged to be F.We evaluated 200 target senses for Lexeed, and100 for Wikipedia.103.2 Experiment: LexeedIn this paper, we expand queries using the Hi-noki Ontology (Bond et al, 2004), which includesrelated words extracted from the definition sen-tences.
Table 2 shows the data for the Hinoki On-tology.For SYN, we expand queries using synonyms,abbreviations, other names in Table 2, and vari-10We performed an image search in September 2009 forLexeed, and in December 2009 for Wikipedia.ant spellings found in the dictionary.
On the otherhand, for LNK, we use all the remaining rela-tions, namely hypernyms, domains, etc.
Addi-tionally, we use only normal spellings with no ex-pansion, when the target words are monosemous(MONO).
One exception should be noted.
Whenthe normal spelling employs hiragana (Japanesesyllabary characters), we expand it using a vari-ant spelling.
For example,AlU dragonfly is ex-panded by the variant spelling??
dragonfly.To investigate the trends and difficulties basedon various conditions, we split the Lexeed sensesinto four types, namely, concrete and monose-mous (MC), or polysemous (PC), not concrete andmonosemous (MA), or polysemous (PA).
We se-lected 50 target senses for evaluation randomlyfor each type.
The target senses were randomlyselected without distinguishing them in terms oftheir POS.Note that we regard the sense as being some-thing concrete that is linked to GT?s seman-tic classes subsumed by ?2:concrete?, such as8WF' onion (?
?677:crop/harvest/farmproducts?
?
?2:concrete?
).3.3 Results and Discussion: LexeedTable 3 shows the ratio of T (appropriate), M (ac-ceptable) and F (inappropriate) images for the tar-get sense.
We calculated the ratio using all fivecandidate images, for example, in Figure 3, the334ratio of appropriate images is 60 % (three of five).In Table 3, the baseline shows a case where thequery only involves the lemma (normal spelling).As shown in Table 3, SYN has higher precisionthan LNK.
This means that SYN can focus onthe appropriate sense.
With polysemous words(PC, PA), expansion works more effectively, andhelps to supply appropriate images for each sense.However, with MC, both LNK and SYN have lessprecision.
This is because the target senses ofMC are majorities, so expansion is adversely af-fected.
Although MONO alone has good precision,because hiragana is often used as readings andhas high ambiguity, appending the variant spellinghelps us to focus on the appropriate sense.Here, we focus on LNK of PC, and then analyzethe reasons for F (Table 5).
In Table 5, in 24.3%of cases it is ?difficult to portray the sense usingimages?
(The numbers of senses for which it is?difficult to portray the sense using images?
are,3 of MC, 9 of PC, 10 of MA, and 16 of PA. Weinvestigate such senses in more detail in ?
3.4.
).For such senses, no method can provide suit-able images, as might be expected.
Therefore, weexclude targets where it is ?difficult to portray thesense using images?, then we recalculated the ra-tio of appropriate images.
Table 4 shows the ca-pability of our proposed method for senses thatcan be explored using images.
This leads to 66.3% precision (15.3% improvement) even for mostdifficult target type, PA.Again, when we look at Table 5, reasons 2-5(33.3 %) will be improved.
In particular, ?hy-pernym leads to ambiguity?
makes up more than10%.
Hypernyms sometimes work well, butsometimes they lead to other words included inthe hypernyms.
For example, appending the hy-pernym ?
foods to 0 boiled-dried fishleads to images of ?foods made with boiled-driedfish?.
This is why SYN obtained better resultsthan LNK.
Then, with ?expanded by minor sense?and when the original sense is dominant major-ity, expansion reduced the precision.
Therefore,we should expand using only words with majorsenses.3.4 Discussion: Senses can/cannot be shownby imagesAs described above, the target senses are ran-domly selected without being distinguished bytheir POS, because we also want to investigate thefeatures of senses that can be shown by images.Table 6 shows the ratio of senses judged as ?diffi-cult to portray the sense using images?
(labeled as?Not Shown?)
for each POS.
As regards POS, themajority of selected senses are nouns, followedby verbal nouns and verbs.
We expected that themajority of nouns and verbal nouns whould be?Shown?, but did not expect that a majority ofverb is also ?Shown?.
Other POSs are too rareto judge, although they tend to fall in the ?NotShown?
category.Furthermore, in Table 7, for nouns and verbalnouns, we show the ratio of senses for each type(?Concrete?
or ?not Concrete?)
judged in termsof ?difficult to portray the sense using images?.We classified the senses into ?Concrete?
or ?notConcrete?
based on GT?s semantic classes, as de-scribed in ?
3.2.Table 6: Ratio of Senses judged as ?difficult toportray the sense using images?
for each POSPOS Shown Not Shown TotalNo.
% No.
% No.Noun 132 85.2 23 14.8 155Verbal Noun 15 78.9 4 21.1 19Verb 9 81.8 2 18.2 11Affix 4 57.1 3 42.9 7Pronoun 0 0 2 100 2Adjective 1 50 1 50 2Adverb 0 0 2 100 2Interjection 1 100 0 0 1Conjunction 0 0 1 100 1Total 162 81 38 19 200Table 7: Ratio of Concrete/Not Concrete Sensesjudged as ?difficult to portray the sense using im-ages?
: for Nouns and Verbal NounsType Shown Not Shown TotalNo.
% No.
% No.Concrete 114 90.5 12 9.5 126Not Concrete 33 68.8 15 31.3 48Total 147 84.5 27 15.5 174335Table 3: Ratio of Appropriate Images for Sense (Precision): LexeedTarget Expanding F (Inappropriate) T (Appropriate) M (Acceptable) T+MType Method No.
% No.
% No.
% No.
% TotalSYN 18 24.0 36 48.0 21 28.0 57 76.0 75Mono- LNK 82 33.5 112 45.7 51 20.8 163 66.5 245semous MONO 42 16.8 181 72.4 27 10.8 208 83.2 250Con- (MC) baseline 46 18.4 171 68.4 33 13.2 204 81.6 250Poly- SYN 94 38.7 88 36.2 61 25.1 149 61.3 243crete semous LNK 111 44.4 92 36.8 47 18.8 139 55.6 250(PC) baseline 180 72.0 53 21.2 17 6.8 70 28.0 250SYN 32 42.7 21 28.0 22 29.3 43 57.3 75not Mono- LNK 138 57.5 54 22.5 48 20.0 102 42.5 240semous MONO 98 40.0 98 40.0 49 20.0 147 60.0 245Con- (MA) baseline 112 44.8 86 34.4 52 20.8 138 55.2 250Poly- SYN 122 49.0 64 25.7 63 25.3 127 51.0 249crete semous LNK 150 60.2 52 20.9 47 18.9 99 39.8 249(PA) baseline 201 80.7 36 14.5 12 4.8 48 19.3 249Table 4: Ratio of Appropriate Images for Sense (Precision), excluding senses that are difficult to portrayusing images: LexeedTarget Expanding F (Inappropriate) T (Appropriate) M (Acceptable) T+MType Method No.
% No.
% No.
% No.
% TotalSYN 15 21.4 36 51.4 19 27.1 55 78.6 70Mono- LNK 71 30.9 112 48.7 47 20.4 159 69.1 230Con- semous MONO 29 12.3 180 76.6 26 11.1 206 87.7 235(MC) baseline 35 14.9 170 72.3 30 12.8 200 85.1 235Poly- SYN 61 30.8 85 42.9 52 26.3 137 69.2 198crete semous LNK 84 40.0 89 42.4 37 17.6 126 60.0 210(PC) baseline 139 67.8 53 25.9 13 6.3 66 32.2 205SYN 17 34.0 20 40.0 13 26.0 33 66.0 50not Mono- LNK 101 51.8 54 27.7 40 20.5 94 48.2 195semous MONO 65 33.3 94 48.2 36 18.5 130 66.7 195Con- (MA) baseline 72 36 85 42.5 43 21.5 128 64.0 809Poly- SYN 57 33.7 63 37.3 49 29 112 66.3 169crete semous LNK 81 47.9 52 30.8 36 21.3 88 52.1 169(PA) baseline 122 72.2 36 21.3 11 6.5 47 27.8 169Table 5: Reasons for F: PC, LNK:LexeedNo.
Reason No.
% Example1 difficult to portray the sense 27 24.3 ,e meusing images ?
?humble expressions used for oneself?
?2 hypernym leads to ambiguity 12 10.8 0 boiled-dried fish (?
?
foods)3 expanded by minor sense 11 9.9 ???
link (?????
links, usually means lynx)4 no expansion is better 8 7.2 ?????
cameraman (??
staff)5 original sense is TOO minor 6 5.4 ?
lake (??
lake),?
usually means sea6 Other 47 42.3Total 111 100336As shown in Table 7, 90.5 % of ?Concrete?nouns are judged as ?Shown?, and only 9.5 % ofsenses are judged as ?Not Shown?
11.
However68.8 % of ?not Concrete?
nouns are also judgedas ?Shown?.Therefore, both POS and type (?Concrete?
or?not Concrete?)
are helpful, but not perfect fea-tures as regards knowing the sense is ?difficult toportray the sense using images?.
In future workwe will undertake further analysis to determinethe critical features.3.5 Experiment: WikipediaFor LNK we use the Wikipedia hyperlinks (shownas [[ ]] in Fig 2).
95.5 % of all senses include [[ ]],85.4 % linked to an actual page, and [[ ]] appeared0.95 times per sense.
Note that we do not use timeexpression links such as [[2010]] and [[1990s]].With SYN, we use synonyms extracted withheuristics.
Table 8 shows the main rules that weused to extract synonyms.
We extracted synonymsfor 98.0 % of 197,912 senses.Then we randomly selected 50 target senses forevaluation from lemmas shared/unshared by Lex-eed.3.6 Results and Discussion: WikipediaWe do not show the baseline in Table 9, but it is al-ways below 10%.
For all target senses, expansionprovides more suitable images.
Because there areso many senses in Wikipedia, no target sense isin the majority.
As shown in Table 9, there arefew differences between SYN and LNK, becausemost of the synonyms used for SYN are also links.However, SYN has slightly superior precision asregards T (Appropriate), which means the processof extracting synonyms helped to reject links thatwere poorly with the target senses.Also in Lexeed, expansion using synonyms(SYN) had higher precision than hypernyms (LNK).Because we do not know the total number of suit-able images for the target senses on the Internet,we cannot estimate the recall with this evaluationmethod.
However, we speculate that hypernyms11For example, ?
?
conference ( ?
?373:organization, etc.?
?
?2:concrete?
), )bhcparental surrogate ( ?
?342:agent/representative?
??2:concrete?
), and so on.provide higher recall.
Deng et al (2009) under-took expansion using hypernyms and this may bean appropriate way to obtain many more imagesfor each sense.
However, because our aim is em-ploy several suitable images for each sense, highprecision is preferable to high recall.Now, we focus on LNK shared by Lexeed, andthen we analyze the reasons for F (Table 10).
Incontrast to Lexeed, no sense is classified as ?dif-ficult to portray the sense using images?.
How-ever, there are many senses where it is difficultto decide what kind of images ?explain the tar-get sense?.
For example, in Table 10, with?maybe T (Appropriate)?, the target sense was apersonal name and the image was his/her repre-sentative work.
In this paper, for personal names,only the images of the person are judged to be T,despite the fact that supplying images of represen-tative work for novelists or artists may be suitable.In this study, we obtained five images per sense,but only one image was sufficient for some senses,for example, an image of an album cover for thename of an album.
In contrast, several differenttypes of images are needed for some senses.
Forexample, for the name of a city, images of maps,landscapes, city offices, symbols of the city, etc.are all suitable.
Therefore, it may be better to esti-mate a rough class first, such as the name of an al-bum, artist and place, and then obtain preassignedtypes of images.4 ConclusionsThe goal of this work was to supply several suit-able images for dictionary definitions.
The tar-get dictionaries were Lexeed and Wikipedia, whichhave very different characteristics.
To cover awide range of senses, we collected candidateimages from the Internet by querying an im-age search engine.
Then, to obtain suitable anddifferent images for each sense, we expandedthe queries by appending related words extractedfrom the definition sentences.
In this paper, wetried two types of expansion, one mainly usingsynonyms (SYN), and one mainly using hyper-nyms or related links (LNK).The results show that SYN provided better pre-cision than LNK, especially for Lexeed.
Also, queryexpansion provided a substantial improvement for337Table 8: Rules for Extracting Synonyms for SYN: WikipediaExampleRule Lemma Definition sentenceshead parts separated by hyphen (- or ?)
EU [[euphonium]] - a brass instrument (line 7 in Figure 2)whole definitions appear as a chunk EU [[European Union]] (line 2 in Figure 2)parts indicated byarrow (g) {?
dog One of [[Oriental Zodiac]]g[[?
dog]]quotation key words,??
See etc.
{?
dog [[Chinese character]]?s [[radical parts]], See [[u inu-bu]]parts in parentheses or ?
?
includingwhole lemma Einstein ?Albert Einstein?alphameric characters, for katakana lemma ???
?samba?characters of alpha-numeral lemma CS ?????g?
(computer science)underlined parts show the extracted synonyms.Table 9: Ratio of Appropriate Images for Sense (Precision): WikipediaTarget Expanding F (Inappropriate) T (Appropriate) M (Acceptable) T+MType Method No.
% No.
% No.
% No.
% TotalShared by SYN 98 40.8 119 49.6 23 9.6 142 59.2 240Lexeed LNK 92 41.8 107 48.6 21 9.5 128 58.2 220NOT shared SYN 100 41.2 103 42.4 40 16.5 143 58.8 243by Lexeed LNK 96 41.0 93 39.7 45 19.2 138 59.0 234Table 10: Reasons for F: Shared by Lexeed, LNK: WikipediaNo.
Reason No.
% ExampleLemma Links7 lack of queries 14 15.2 N!
fue (reading) ? Hue, city name in Vietnam(available words in def.
)8 inappropriate queries 10 10.9 ???
? regular w?
?3g active roster(available words in def.
)2 hypernym lead to ambiguity 5 5.4 ?????
cache ????????
geocaching9 maybe T (Appropriate) 5 5.4 ??
? monkey ??????
Monkey Punch6 Other 58 63Total 92 100polysemous words.
Our proposed method is sim-ple but effective for our purpose, that is supplyingsuitable and different images for each sense.In future work we intend to analyze senses thatare difficult/easy to portray using images in moredetail, using not only semantic charactaristics butalso visual features(Csurka et al, 2004).
We alsointend to improve the expansion method.
One wayto achieve this is to filter out expansions with mi-nor senses.
As for Wikipedia, we should approxi-mate the class first, such as the name of an album,artist and place, then obtain preassigned types ofimages.338ReferencesKobus Barnard, Pinar Duygulu, Nando de Freitas, DavidForsyth, David Blei, and Michael I. Jordan.
2003.
Match-ing Words and Pictures.
Journal of Machine LearningResearch, Vol.
3, pp.
1107?1135.Francis Bond, Hitoshi Isahara, Sanae Fujita, Kiyotaka Uchi-moto, Takayuki Kuribayashi, and Kyoko Kanzaki.
2009.Enhancing the Japanese WordNet.
In The 7th Workshopon Asian Language Resources, in conjunction with ACL-IJCNLP-2009, pp.
1?8.Francis Bond, Eric Nichols, Sanae Fujita, and TakaakiTanaka.
2004.
Acquiring an Ontology for a FundamentalVocabulary.
In Proceedings of the 20th International Con-ference on Computational Linguistics: COLING-2004,pp.
1319?1325.Andy Borman, Rada Mihalcea, and Paul Tarau.
2005.
Pic-Net: Pictorial Representations for Illustrated SemanticNetworks.
In Proceedings of the AAAI Spring Symposiumon Knowledge Collection from Volunteer Contributors.Gabriela Csurka, Cedric Bray, Chris Dance, and Lixin Fan.2004.
Visual categorization with bags of keypoints.
InECCV International Workshop on Statistical Learning inComputer Vision, pp.
59?74.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, andLi Fei-Fei.
2009.
ImageNet: A Large-Scale HierarchicalImage Database.
In IEEE Computer Vision and PatternRecognition (CVPR).Hui Fang and ChengXiang Zhai.
2006.
Semantic termmatching in axiomatic approaches to information re-trieval.
In Proceedings of the 29th Annual InternationalACM SIGIR Conference on Research and Development inInformaion Retrieval, pp.
115?122.
ACM.Christine Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Atsushi Fujii and Tetsuya Ishikawa.
2005.
Image Retrievaland Disambiguation for Encyclopedic Web Search.
InProceedings of the International Joint Conference on Ar-tificial Intelligence: IJCAI-2005, pp.
1598?1599.Andrew B. Goldberg, Jake Rosin, Xiaojin Zhu, andCharles R. Dyer.
2009.
Toward Text-to-Picture Synthe-sis.
In NIPS 2009 Mini-Symposia on Assistive MachineLearning for People with Disabilities.Yoshihiko Hayashi, Savas Bora, and Masaaki Nagata.
2009.Utilizing Images for Assisting Cross-language Informa-tion Retrieval on the Web.
In International Workshop onWeb Information Retrieval Support Systems, pp.
100?103.Ichiro Ide and Keiji Yanai.
2009.
Crossing the Semantic Gap: Towards the Understanding of Image and Video Con-tents.
Journal of Japanese Society for Artificial Intelli-gence, Vol.
24, No.
5, pp.
691?699.
(in Japanese).Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, AkioYokoo, Hiromi Nakaiwa, Kentaro Ogura, YoshifumiOoyama, and Yoshihiko Hayashi.
1997.
Goi-Taikei?
A Japanese Lexicon.
Iwanami Shoten, Tokyo.
5volumes/CD-ROM.Kaname Kasahara, Hiroshi Sato, Francis Bond, TakaakiTanaka, Sanae Fujita, Tomoko Kanasugi, and ShigeakiAmano.
2004.
Construction of a Japanese Semantic Lexi-con: Lexeed.
In IEICE Technical Report: 2004-NLC-159,pp.
75?82.
(in Japanese).Rada Mihalcea and Chee Wee Leong.
2008.
Toward commu-nicating simple sentences using pictorial representations.Machine Translation, Vol.
22, No.
3, pp.
153?173.Adrian Popescu, Christophe Millet, and Pierre-AlainMoe?llic.
2007.
Ontology Driven Content Based ImageRetrieval.
In Proceedings of the ACM International Con-ference on Image and Video Retrieval.Adrian Popescu, Christophe Millet, Pierre-Alain Moe?llic,Patrick He`de, and Gregory Grefenstette.
2006.
AutomaticConstruction of a Grounded Multimedia Ontology of Ob-jects to Illustrate Concepts in a Learning Process.
In NET-TIES 2006 Conference: Advanced Educational Technolo-gies for a Future e-Europe.Arnold W.M.
Smeulders, Marcel Worring, Simone Santini,Amarnath Gupta, and Ramesh Jain.
2000.
Content-basedImage Retrieval at the End of the Early Years.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, Vol.
22, No.
12, pp.
1349?1380.Yuya Unno, Yusuke Miyao, and Jyunichi Tujii.
2008.
In-formation Retrieval using Automatically Extracted para-phrases.
In Proceedings of the 14th Annual Meeting ofThe Association for Natural Language Processing: NLP-2008, pp.
123?126.
(in Japanese).Ellen M. Voorhees.
1994.
Query Expansion using Lexical-Semantic Relations.
In Proceedings of the 17th AnnualInternational ACM SIGIR Conference on Research andDevelopment in Informaion Retrieval, pp.
61?69.Feng Yansong and Mirella Lapata.
2008.
Automatic imageannotation using auxiliary text information.
In Proceed-ings of ACL-08: HLT, pp.
272?280.
Association for Com-putational Linguistics.Svitlana Zinger, Christophe Millet, Benoit Mathieu, GregoryGrefenstette, Patrick He`de, and Pierre-Alain Moe?llic.2006.
Clustering and semantically filtering web imagesto create a large-scale image ontology.
In SPIE 18thAnnual Symposium Electronic Imaging, Internet ImagingVII, Vol.
6061, pp.
89?97.339
