Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 51?66,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsControlled Ascent: Imbuing Statistical MT with Linguistic KnowledgeWilliam D. Lewis and Chris QuirkMicrosoft ResearchOne Microsoft WayRedmond, WA 98052{wilewis,chrisq}@microsoft.comAbstractWe explore the intersection of rule-based and sta-tistical approaches in machine translation, with aparticular focus on past and current work here atMicrosoft Research.
Until about ten years ago,the only machine translation systems worth usingwere rule-based and linguistically-informed.
Alongcame statistical approaches, which use large cor-pora to directly guide translations toward expres-sions people would actually say.
Rather than mak-ing local decisions when writing and conditioningrules, goodness of translation was modeled numer-ically and free parameters were selected to opti-mize that goodness.
This led to huge improvementsin translation quality as more and more data wasconsumed.
By necessity, the pendulum is swing-ing towards the inclusion of linguistic features inMT systems.
We describe some of our statisticaland non-statistical attempts to incorporate linguis-tic insights into machine translation systems, show-ing what is currently working well, and what isn?t.We also look at trade-offs in using linguistic knowl-edge (?rules?)
in pre- or post-processing by lan-guage pair, with a particular eye on the return oninvestment as training data increases in size.1 IntroductionMachine translation has undergone severalparadigm shifts since its original conception.Early work considered the problem as cryptog-raphy, imagining that a word replacement ciphercould find the word correspondences between twolanguages.
Clearly Weaver was decades ahead ofhis time in terms of both computational powerand availability of data: only now is this approachgaining some traction (Knight, 2013)1 At the time,however, this direction did not appear promising,and work turned toward rule-based approaches.Effective translation needs to handle a broadrange of phenomena.
Word substitution ciphersmay address lexical selection, but there are manyadditional complexities: morphological normal-ization in the source language, morphological in-flection in the target language, word order differ-ences, and sentence structure differences, to name1For the original 1949 Translation memorandum byWeaver see (Weaver, 1955).a few.
Many of these could be captured, at leastto a first degree of approximation, by rule-basedapproaches.
A single rule might capture the factthat English word order is predominantly SVOand Japanese word order is predominantly SOV.While many exceptions exist, such rules handlemany of the largest differences between languagesrather effectively.
Therefore, rule-based systemsthat did a reasonable job of addressing morpho-logical and syntactic differences between sourceand target dominated the marketplace for decades.With the broader usage of computers, greateramounts of electronic data became available tosystems.
Example-based machine translationsystems, which learn corpus-specific translationsbased on data, began to show substantial improve-ments in the core problem of lexical selection.This task was always quite difficult for rule-basedapproaches: finding the correct translation in con-text requires a large amount of knowledge.
Inpractice, nearby words are effective disambigua-tors once a large amount of data has been captured.Phrasal statistical machine translation systemsformalized many of the intuitions in example-based machine translation approaches, replacingheuristic selection functions with robust statisticalestimators.
Effective search techniques developedoriginally for speech recognition were strong start-ing influences in the complicated realm of MT de-coding.
Finally, large quantities of parallel dataand even larger quantities of monolingual data al-lowed such phrasal methods to shine even in broaddomain translation.Translations were still far from perfect, though.Phrasal systems capture local context and local re-ordering well, but struggle with global reordering.Over the past decade, statistical machine transla-tion has begun to be influenced by linguistic infor-mation once again.
Syntactic models have shownsome of the most compelling gains.
Many sys-tems leverage the syntactic structure of either the51source or the target sentences to make better deci-sions about reordering and lexical selection.Our machine translation group has been an ac-tive participant in many of these latest develop-ments.
The first MSR MT system used deep lin-guistic features, often with great positive effect.Inspired by the successes and failures of this sys-tem, we invested heavily in syntax-based SMT.However, our current statistical systems are stilllinguistically impoverished in comparison.This paper attempts to document importantlessons learned, highlight current best practices,and identify promising future directions for im-proving machine translation.
A brief review ofour earlier generation of machine translation tech-nology sets the stage; this older system remainsrelevant given renewed interest in semantics (e.g.,http://amr.isi.edu/).
Next we describe some ofour statistical and non-statistical attempts to in-corporate linguistic insights into machine transla-tion systems, showing what is currently workingwell, and what is not.
We also look at trade-offsin using linguistic knowledge (?rules?)
in pre- orpost-processing by language pair, with a particu-lar eye on the return on investment as training dataincreases in size.
Systems built on different ar-chitectures, particularly those incorporating somelinguistic information, may have different learn-ing curves on data.
The advent of social mediaand big data presents new challenges; we reviewsome effective research in this area.
We concludeby exploring promising directions for improvingtranslation quality, especially focusing on areasthat stand to benefit from linguistic information.2 Logical Form TranslationMachine translation research at Microsoft Re-search began in 1999.
Analysis components hadbeen developed to parse surface sentences intodeep logical forms: predicate-argument structuresthat normalized away many morphological andsyntactic differences.
This deep representationwas originally intended for information miningand question answering, allowing facts to rein-force one another, and simplifying question andanswer matching.
These same normalizationshelped make information more consistent acrosslanguages: machine translation was a clear poten-tial application.
Consider the deep representationsof the sentence pairs in Figure 1: many of the sur-face differences, such as word order and morpho-Figure 1: Example logical forms for three distinctinputs, demonstrating how differences in syntacticstructure may be normalized away.
In each case,the logical form is a graph of nodes such as ?be?and ?difficult?, and relations such as ?Tobj?
(typ-ical object) and ?Tsub?
(typical subject).
In addi-tion, nodes are marked with binary features calledbits, prefixed with a + symbol in the notation, thatcapture unstructured pieces of information such astense and number.logical inflection, are normalized away, potentiallyeasing the translation process.Substantial differences remained, however.Many words and phrases have non-compositionalcontextually-influenced translations.
Commercialsystems of the time relied on complex, hand-curated dictionaries to make this mapping.
Yetexample-based and statistical systems had alreadybegun to show promise, especially in the case ofdomain-specific translations.
Microsoft in par-ticular had large internal demand for ?technical?translations.
With increasing language coverageand continuing updates to product documentationand support articles came increasing translationcosts.
Producing translations tailored to this do-main would have been an expensive task for arule-based system; a corpus-based approach waspursed.This was truly a hybrid system.
Source and tar-get language surface sentences were parsed intodeep logical forms using rule-based analyzers.22These parsers were developed with a strong focus on cor-pora, though.
George Heidorn, Karen Jensen, and the NLPresearch group developed a toolchain for quickly parsing alarge bank of test sentences and comparing against the lastbest result.
The improvements and regressions resulting froma change to the grammar could be manually evaluated, andthe changes refined until the end result.
The end result was a52Figure 2: The process of learning translation in-formation from parallel data in the LF system.Likewise a rule-based target language generationcomponent could find a surface realization of adeep logical form.
However, the mapping fromsource language logical form fragments to targetlanguage logical form fragments was learned fromparallel data.2.1 Details of the LF-based systemTraining started with a parallel corpus.
First, thesource and target language sentences were parsed.Then the logical forms of the source and targetwere aligned (Menezes and Richardson, 2001).These aligned logical forms were partitioned intominimal non-compositional units, each consistingof some non-empty subset of the source and tar-get language nodes and relations.
Much like inexample-based or phrasal systems, both minimaland composed versions of these units were thenstored as possible translations.
A schematic of thethis data flow is presented in Figure 2.At runtime, an input sentence was first parsedinto a logical form.
Units whose source sidesmatched the logical form were gathered.
A heuris-tic search found a set of fragments that: (a) cov-ered every input node at least once, and (b) wereconsistent in their translation selections.
If somenode or relation was not uncovered, it was copiedfrom source to target.
The resulting target lan-guage logical form was then fed into a genera-tion component, which produced the final string.A schematic diagram is presented in Figure 3.This overview sweeps many fine details un-der the rug.
Many morphological and syntacticdistinctions were represented as binary features(?bits?)
in the LF; mapping bits was difficult.
Thedata driven but not statistical approach to parser development.Figure 3: The process of translating a new sen-tence in the LF system.logical form was a graph rather than a tree ?
in?John ate and drank?, John is the DSUB (deep sub-ject) of both eat and drink ?
which led to com-plications in transferring structure.
Many suchcomplications were often handled through rules;these rules grew more complex over time.
Corpus-based approaches efficiently learned many non-compositional and domain specific issues.2.2 Results and lessons learnedThe system was quite successful at the time.
MSRused human evaluation heavily, performing bothabsolute and relative quality evaluations.
In theabsolute case, human judges gave each transla-tion a score between 1 (terrible translation) and4 (perfect).
For relative evaluations, judges werepresented with two translations in randomized or-der, and were asked whether they preferred systemA, system B, or neither.
In its training domain,the LF-based system was able to show substantialimprovements over rule-based systems that domi-nated the market at the time.Much of these gains were due to domain- andcontext-sensitivity of the system.
Consider theSpanish verb ?activar?.
A fair gloss into En-glish is ?activate?, but the most appropriate trans-lation in context varies (?signal?, ?flag?, etc.).
Theexample-based approach was able to capture thosecontexts very effectively, leading to automatic do-main customization given only translation mem-ories.
This was a huge improvement over rule-based systems of the time.During this same era, however, statistical ap-proaches (Och and Ney, 2004) were showing greatpromise.
Therefore, we ran a comparison be-tween the LF-based system and a statistical system53(a) Effecitve LF translation.
Note how the LF system is able to translate ?se lleveban a cabo?
even though that particularsurface form was not present in the training data.SRC: La tabla muestra adema?s do?nde se llevaban a cabo esas tareas en Windows NT versio?n 4.0.REF: The table also shows where these tasks were performed in Windows NT version 4.0.LF: The table shows where, in addition, those tasks were conducted on Windows NT version 4.0.STAT: The table also shows where llevaban to Windows NT version 4.0.
(b) Parsing errors may degrade translation quality; the parser interprted ?/?
as coordination.SRC: La sintaxis del operador / tiene las siguientes partes:REF: The / operator syntax has these parts:LF: The operator syntax it has the parts:STAT: The / operator syntax has these parts:(c) Graph-like structures for situations such as coordination are difficult to transfer (see the parenthesized group in particular);selecting the correct form at generation time is difficult in the absence of a target language model.SRC: Debe ser una consulta de seleccio?n (no una consulta de tabla de referencias cruzadas ni una consulta de accio?n).REF: Must be a select query (not a crosstab query or action query).LF: You must not be a select query neither not a query in table in cross-references nor not an action query.STAT: Must be a select query (not a crosstab query or an action query).Figure 4: Example source Spanish sentences, English reference translations of those sentences, transla-tions from the LF system, and translations from a statistical translation system without linguistic features.without linguistic information.
Both systems weretrained and tuned on the same data, and translatedthe same unseen test set.
The linguistic systemhad the additional knowledge sources at its dis-posal: morphological, lexical, syntactic, and se-mantic information.
Regardless, the systems per-formed nearly equally well on average.
Each haddistinct strengths and weaknesses, though.Often the success or failure of the LF-systemwas tied to the accuracy of its deep analysis.
Whenthese representations were accurate, they couldlead to effective generalizations and better trans-lations of rare phenomena.
Since surface wordswere lemmatized and syntactic differences nor-malized, unseen surface forms could still be trans-lated as long as their lemma was known (see Fig-ure 4(a)).
Yet mistakes in identifying the correctlogical form could lead to major translation errors,as in Figure 4(b).Likewise the lack of statistics in the com-ponents could cause problems.
Statistical ap-proaches found great benefits from the target lan-guage model.
Using a rule-based generation com-ponent made it difficult to leverage a target lan-guage model.
Often, even if a particular transla-tion was presented tens, hundreds, or thousandsof times in the data, the LF-based system couldnot produce it because the rule-based generationcomponent would not propose the common sur-face form, as in Figure 4(c).We drew several lessons from this system whendeveloping our next generation of machine trans-lation systems.
It was clear to us that syntactic rep-resentations can help translation, especially in re-ordering and lexical selection: appropriate repre-sentations allows better generalization.
However,over-generalization can lead to translation error, ascan parsing errors.3 The Next Generation MSR MTSystemsResearch in machine translation at Microsoft hasbeen strongly influenced by this prior experiencewith the LF system.
First we must notice thatthere is a huge space of possible translations.
Con-sider human reference translations: unless tied toa specific domain or area, they seldom agree com-pletely on lexical selection and word order.
If oursystem is to produce reasonable output, it shouldconsider a broad range of translation options, pre-ferring outputs most similar to language used byhumans.
Why do we say ?order of magnitude?rather than ?magnitude order?, or ?master of cer-emonies?
rather than ?ceremonies master??
Manychoices in language are fundamentally arbitrary,but we need to conform to those arbitrary deci-sions if we are to produce fluent and understand-able output.
Second, while there is leverage to begained from deep features, seldom do we have acomponent that identifies these features with per-54fect accuracy.
In practice it seems that the errorrate increases as the depth of component analy-sis increases.
Finally, we need a representationof ?good translations?
that is understandable by acomputer.
When forced to choose between twotranslations, the system needs to make a choice:an ordering.Therefore, our data-driven systems cruciallyrely on several components.
First, we must effi-ciently search a broad range of translations.
Sec-ond, we must rank according to both our linguisticintuitions and the patterns that emerge from data.We use a number of different systems basedon the availability of linguistic resources.
So-called phrasal statistic machine translation sys-tems, which model translations using no more thansequences of contiguous words, perform surpris-ingly well and require nothing but tokenization inboth languages.
In language pairs for which wehave a source language parser, a parse of the in-put sentence is used to guide reordering and helpselect relevant non-contiguous units; this is thetreelet system (Quirk and Menezes, 2006).
Re-gardless of which system we use, however, tar-get language models score the fluency of the out-put, and have a huge positive impact on translationquality.We are interested in means of incorporating lin-guistic intuition deeper into such a system.
As inthe case of the treelet system, this may define thebroad structure of the system.
However, there arealso more accessible ways of influencing existingsystems.
For instance, linguists may author fea-tures that identify promising or problematic trans-lations.
We describe one such attempt in the fol-lowing system.3.1 Like and DontLikeEven in our linguistically-informed treelet sys-tem (Quirk and Menezes, 2006), which uses syn-tax in its translation system, many of the individ-ual mappings are clearly bad, at least to a human.When working with linguistic experts, one gut re-sponse is to write rules that inspect the transla-tion mappings and discard those translation map-pings that appear dangerous.
Perhaps they seemto delete a verb, perhaps they use a speculative re-ordering rule ?
something makes them look bad toa linguist.
However, even if we are successful inremoving a poor translation choice, the remainingpossibilities may be even worse ?
or perhaps notranslation whatsoever remains.Instead, we can soften this notion.
Imagine thata linguist is able to say that this mapping is notpreferred because of some property.
Likewise, askilled linguist might be able to identify mappingsthat look particularly promising, and prefer thosemappings to others; see Figure 5 for an example.This begs the question: how much should weweight such influence?
Our answer is a corpusdriven one.
Each of these linguistic preferencesshould be noted, and the weight of these prefer-ences should be tuned with all others to optimizethe goodness of translation.
Already our statisti-cal system has a number of signals that attempt togauge translation quality: the translation modelsattempt to capture fidelity of translation; languagemodels focus on fluency; etc.
We use techniquessuch as MERT (Och, 2003) and PRO (Hopkinsand May, 2011) to tune the relative weight of thesesignals.
Why not tune indicators from linguists inthe same manner?When our linguists mark a mapping as +Like or+DontLike, we track that throughout the search.Each final translation incorporates a count of Likemappings and a count of DontLike mappings, justas it accumulates a language model score, trans-lation model scores, word penalties, and so on.These weights are tuned to optimize some approx-imate evaluation metric.
In Figure 6, the weightof Like and DontLike is shown for a number ofsystems, demonstrating how optimization may beused to tune the effect of hand-written rules.
Re-moving these features degrades the performanceof an MT system by at least 0.5 BLEU points,though the degradations are often even more visi-ble to humans.This mechanism has been used to capture anumber of effects in translation commonly missedby statistical methods.
It is crucial yet challengingto maintain negation during translation, especiallyin language pairs where negation is expressed dif-ferently: some languages use a free morpheme(Chinese tends to have a separate word), othersuse a bound morpheme (English may use pre-fixes), others require two separated morphemes(French has negation agreement); getting any ofthese wrong can lead to poor translations.
Rulesthat look at potentially distant words can helpscreen away negation errors.
Likewise rules canhelp ensure that meaning is preserved, by prevent-ing main verbs mapping to punctuation, or screen-55// don?t allow verb to be lostif (forany(NodeList(rMapping),[Cat=="Verb" & ?Aux(SynNode(InputNode))])) {list {segrec} bad_target=sublist(keeplist,[forall(NodeList,[pure_punk(Lemma) | coord_conjunction(foreign_language,Lemma)])]);if (bad_target) {segrec rec;foreach (rec; bad_target) {+DontLike(rec);}}}Figure 5: An example rule for marking mappings as ?DontLike?.
In this case, the rule searches forsource verbs that are not auxiliaries and that are translated into lemmas or punctuation.
Such translationsare marked as DontLike.Figure 6: A plot of the weights +Like map-ping count and +DontLike mapping count weightsacross language pairs.
Generally Like is assigneda positive weight (sometimes quite positive), andDontLike is assigned a negative weight.
In oursystem, weights are L1 normalized (the sum of theabsolute values of the weights is equal to one), sofeature weights greater than 0.1 are very influen-tial.ing out mappings that seem unlikely, especiallywhen those mappings involve unusual tokens.These two features are a rather coarse means ofintroducing linguistic feedback.
As our parame-ter estimation techniques scale to larger featuresmore effectively, we are considering using finer-grained feedback from linguists to say not onlythat they like or don?t like a particular mapping,but why.
The relative impact of each type of feed-back can be weighted: perhaps it is critical to pre-serve verbs, but not so important to handle defi-niteness.
Given recent successes in scaling param-eter estimation to larger and larger values, this areashows great promise.3.2 Linguistic component accuracyAnother crucial issue is the quality of the linguisticcomponents.
We would certainly hope that betterquality of linguistic analysis should lead to bet-ter quality translations.
Indeed, in certain circum-stances it appears that this correlation holds.In the case of the treelet system, we hope to de-rive benefit from linguistic features via a depen-dency tree.
To investigate the impact of the parsequality, we can degrade a Treebank-trained parserby limiting the amount of training data made avail-able.
As this decreases, the parser quality shoulddegrade.
If we hold all other information in theMT system fixed (parallel and monolingual train-ing data, training regimen, etc.
), then all differ-ences should be due to the changes in parse qual-ity.
Table 1 presents the results of an experimentof this form (Quirk and Corston-Oliver, 2006).
Asthe amount of training data increase, we see a sub-stantial increase in parse quality.Another way to mitigate parser error is to main-tain syntactic ambiguity through the translationprocess.
For syntax directed translation systems,this can be achieved by translating forests ratherthan single trees, ideally including the score of56English- English-System German JapanesePhrasal 31.7 32.9Right branching 31.4 28.0250 instances 32.8 34.12,500 instances 33.0 34.625,000 instances 33.7 35.739,082 instances 33.8 36.0Table 1: Comparison of BLEU scores as linguisticinformation is varied.
A phrasal system providesa baseline free of linguistic information.
Next weconsider a treelet system with a very weak base-line: a right branching tree is always proposed.This baseline is much worse than a simple phrasalsystem.
The final four rows evaluate the impactof a parser trained on increasing amounts of sen-tences from the English Penn Treebank.
Even witha tiny amount of training data, the system getssome benefit from syntactic information, and thereturns appear to increase with more training data.parse as part of the translation derivation.
In un-published results, we found that this made a sub-stantial improvement in translation quality; theeffect was corroborated in other syntax directedtranslation systems (Mi et al 2008).
Alterna-tively, allowing a neighborhood of trees similarto some predicted tree can handle ambiguity evenwhen the original parser does not maintain a for-est.
This also allows translation to handle phenom-ena that are systematically mis-parsed, as well ascases where the parser specification is not idealfor the translation task.
Recent work in this areahas show substantial improvements (Zhang et al2011).4 Evaluation4.1 Fact or Fiction: BLEU is Biased AgainstRule-Based or Linguistically-InformedSystems?It has generally been accepted as common wis-dom that BLEU favors statistical MT systems anddisfavors those that are linguistically informed orrule-based.
Surprisingly, the literature on the topicis rather sparse, with some notable exceptions(Riezler and Maxwell, 2005; Farru?s et al 2012;Carpuat and Simard, 2012).
We too have madethis assumption, and had a few years ago coinedthe term treelet penalty to indicate the degree bywhich BLEU favored our phrasal systems over ourtreelet systems.
We had noted on a few occa-sions that treelet systems had lower BLEU scoresthan our phrasal systems over the same data (the?penalty?
), but when compared against one an-other in human evaluation, there was little dif-ference, or often, treelet was favored.
A notablecase was on German-English, where we noted athree-point difference in BLEU between equiva-lent treelet and phrasal systems (favoring phrasal),and a ship/no-ship decision was dependent on theresulting human eval.
The general consensus ofthe team was that the phrasal system was markedlybetter, based on the BLEU result, and treelet sys-tem should be pulled.
However, after a human evalwas conducted, we discovered that the treelet sys-tem was significantly better than the phrasal.
Fromthat point forward, we talked about the treeletpenalty for German being three points, a ?fact?that has lived in the lore of our team ever since.What was really missing, however, was sys-tematic experimental evidence showing the differ-ences between treelet and phrasal systems.
Wetalked about the treelet penalty as a given, butthere was slow rumble of counter evidence sug-gesting that maybe the assumptions behind the?penalty?
were actually unfounded, or minimally,misinformed.One piece of evidence was from experimentsdone by Xiaodong He and an intern that showed aninteraction in quality differences between treeletand phrasal gated by the length of the sentence.Xiaodong was able to show that phrasal systemstended to do better on longer sentences and treeleton shorter: for Spanish-English, he showed a dif-ference in BLEU of 1.29 on ?short?
content on ageneral domain test set, and 1.77 for short contenton newswire content (the NIST08 test set).
TheBLEU difference diminished as the length of thecontent increased, until there was very little dif-ference (less than 1/2 point) for longer content.3An interaction between decoder type and sentencelength means that there might also be an interac-3These results were not published, but were provided tothe authors in a personal conversation with Xiaodong.
In arelated paper (He et al 2008), He and colleagues showedsignificant improvements in BLEU on a system combinationsystem, but no diffs in human eval.
Upon analysis, the re-searchers were able to show that the biggest benefit to BLEUwas in short content, but the same preference was not exhib-ited on the same content by the human evaluators.
In otherwords, the improvements observed in the short content thatBLEU favored had little impact on the overall impressions ofthe human evaluators.57tion between decoder type and test set, especiallyif particular test sets contain a lot of long-ish sen-tences, e.g., WMT and Europarl).
To the contrary,most IT text, which is quite common in Microsoft-specific localization content, tends to be shorter.The other was based on general impressionsbetween treelet and phrasal systems.
Becausetreelet systems are informed by dependency parsesbuilt over the source sentences (a parse can helpconstrain a search space of possible translations,and prune undesirable mappings e.g., constrain tonominal types when the source is a noun), and,as noted earlier, because the parses allow linguiststo pre- or post-process content based on observa-tions in the parse, we have tended to see more?fluent?
output in treelet than phrasal.
However,as the sizes of data have grown steadily over theyears, the quality of translations in our phrasal sys-tems have grown proportionally with the increasein data.
The question arose: is there also an in-teraction between the size of our training data anddecoder type?
In effect, does the quality of phrasalsystems catch-up to the quality of treelet systemswhen trained over very large sets of data?4.2 Treelet Penalty ExperimentsWe ran a set of experiments to measure the dif-ferences between treelet and phrasal systems overvarying sizes of data, in order to measure the sizeof the treelet penalty and its interaction with train-ing data size.
Our assumption was that a sucha penalty existed, and that the penalty decreasedas training data size increased, perhaps converg-ing on zero for very large systems.
Likewise,we wanted to test the interaction between decodertype and sentence length.We chose two languages to run these exper-iments on, Spanish and German, which we ranin both directions, that is, English-to-target (EX)and target-to-English (XE).
We chose Spanish andGerman for several reasons, first among them be-ing that we have high-quality parsers for both lan-guages, as we do for English.
Further, we havedone significant development work on pre- andpost-processing for both languages over the pastseveral years.
Both of these facts combined meantthat the treelet systems stood a real chance of be-ing strong contenders in the experiments againstthe equivalent phrasal systems.
Further, althoughthe languages are typologically close neighborsof English, the word order differences and highdistortion rates from English to or from Germanmight favor a parser-based approach.We had four baseline systems that were builtover very large sets of data.
For Spanish  En-glish, the baseline systems were trained on over22M sentence pairs; for German  English, thebaseline systems were trained on over 36M sen-tence pairs.4 We then created five samples of thebaseline data for each language pair, consisting of100K, 500K, 1M, 2M, and 5M sentence pairs (thesame samples were used for both EX and XE forthe respective pairs).
We then trained both treeletand phrasal systems in both directions (EX andXE) over each sample of data.
Language mod-els were trained on all systems over the target-sidedata.For dev data, we used development data fromthe 2010 WMT competition (Callison-Burch et al2010), and we used MERT (Och, 2003) to tuneeach system.
We tested each system against threedifferent test sets: two were from the WMT com-petitions of 2009 and 2010, and the other wasone locally constructed from 5000 sentences ofcontent translated by users of our production ser-vice (http://bing.com/translator), which we subse-quently had manually translated into the target lan-guages.
The former two test sets are somewhatnews focused; the latter is a random sample ofmiscellaneous translations, and is more generallyfocused.The results of the experiments are shown in Ta-bles 2 and 3, with the relevant graphs in Fig-ures 9 - 10.
The reader will note that in all cases?Spanish and German, EX and XE?the treelet sys-tems scored higher than the related phrasal sys-tems.
This result surprised us, since we thoughtthat treelet systems would score less than phrasalsystems, especially at lower data sizes.
That said,in the Spanish systems, there is a clear conver-gence as data sizes increased: on the WMT09test set for English-Spanish, for instance, the diffstarts at 1.46 BLEU (treelet minus phrasal) forthe 100K sentence system, with a steady conver-gence to near zero (0.12) for the full-data baseline.The other test sets show the same steady conver-gence, although they do not approach zero quiteas closely.
(One might ask whether they wouldconverge to zero with more training data.)
The4A sizable portion of the data for each were scraped fromthe Web, but there were other sources used as well, such asEuroparl, data from TAUS, MS internal localization data, UNcontent, WMT news content, etc.58other direction is even more dramatic: on all testsets the diffs converge on negative values, indi-cating that phrasal systems surpass the quality ofthe associated treelet systems at the largest datapoints.
This is a nice result since it shows, at leastin the case of Spanish, that there is an interac-tion between decoder type and the amount of data:treelet clearly does better at lower data amounts,but phrasal catches up with, and can even pass, thequality of equivalent treelet given sufficient data.With larger data, phrasal may, in fact, be favoredover treelet.The German systems do not tell quite as nice astory.
While it is still true that treelet has higherBLEU scores than phrasal throughout, and thatsystems trained using both decoders improve inquality as more data is added (and the trajectoryis similar), there is no observable convergence asdata size increases.
For German, then, we can onlysay that more data helps either decoder, but wecannot say that phrasal benefits from larger datamore than treelet.
Why the difference betweenSpanish and German?
We suspect there may be aninteraction with the parsers, in that two separateteams developed them.
Thus, it could be the factthat the strength of the respective parsers affectedhow ?linguistically informed?
particular systemsare.
There could also be an interaction with thenumber of word types vs. tokens in the Germandata?given German?s rampant compounding?which increases data sparsity, dampening effectsuntil much larger amounts of data are used.
Weare still in the process of running additional ex-periments to see if there are observable effects inGerman with much larger data sizes, or at least,to determine why German does not show the sameeffects as Spanish.Figure 7: English-Spanish BLEU graph across dif-ferent data sizes, Treelet vs. Phrasal.Since human evaluation is the gold standard weFigure 8: Spanish-English BLEU graph across dif-ferent data sizes, Treelet vs. Phrasal.Figure 9: English-German BLEU graph acrossdifferent data sizes, Treelet vs. Phrasal.Figure 10: German-English BLEU graph acrossdifferent data sizes, Treelet vs. Phrasal.59EX Treelet Phrasal Diff - T-PReq Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010100K 26.49 21.52 23.69 23.10 20.06 21.19 3.39 1.46 2.50500K 28.61 22.85 25.20 25.64 21.47 22.86 2.97 1.38 2.341M 30.52 24.82 27.74 28.36 24.17 26.28 2.16 0.65 1.462M 31.61 25.59 28.54 29.48 24.76 26.91 2.13 0.83 1.635M 32.86 26.37 30.14 30.89 25.84 28.56 1.97 0.53 1.5822M 33.80 27.01 30.61 32.55 26.89 30.12 1.25 0.12 0.49XE100K 27.72 21.76 23.21 26.18 20.80 21.78 1.54 0.96 1.43500K 29.89 22.86 24.89 28.16 22.15 23.44 1.73 0.71 1.451M 32.18 24.76 27.14 31.32 24.32 26.02 0.86 0.44 1.122M 33.31 25.44 28.09 32.77 25.26 27.38 0.54 0.18 0.715M 34.47 26.17 29.10 34.18 26.10 28.74 0.29 0.07 0.3622M 35.88 27.16 30.20 36.21 27.26 30.48 -0.33 -0.10 -0.28Table 2: BLEU Score results for the Spanish Treelet Penalty experimentsEX Treelet Phrasal Diff (T-P)Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010100K 18.98 11.13 12.19 18.22 10.81 11.53 0.76 0.32 0.66500K 22.13 13.18 14.33 21.09 12.74 13.68 1.04 0.44 0.651M 23.23 13.98 15.12 21.89 13.51 14.27 1.34 0.47 0.852M 23.72 14.77 15.87 23.11 14.04 15.03 0.61 0.73 0.845M 24.82 15.31 16.58 24.35 15.00 16.01 0.47 0.31 0.5736M 26.72 16.72 18.20 25.83 16.33 17.18 0.89 0.39 1.02XE100K 27.42 15.91 16.37 26.75 15.83 16.28 0.67 0.08 0.09500K 30.98 18.25 19.16 29.80 18.11 19.09 1.18 0.14 0.071M 32.30 19.16 20.40 31.26 19.06 20.18 1.04 0.10 0.222M 33.40 19.95 21.48 32.25 19.65 21.06 1.15 0.30 0.425M 34.86 21.14 22.55 33.91 20.67 22.13 0.95 0.47 0.4236M 37.31 22.72 24.97 36.08 21.99 23.85 1.23 0.73 1.12Table 3: BLEU Score results for the German Treelet Penalty experiments60seek to achieve with our quality measures, andsince BLEU is only weakly correlated with hu-man eval (Coughlin, 2003), we ran human evalsagainst both the English-Spanish and English-German output.
Performing human evaluationgives us two additional perspectives on the data:(1) do humans perceive a qualitative difference be-tween treelet and phrasal, as we see with BLEU,and (2), if the difference is perceptible, what is itsmagnitude relative to BLEU.
If the magnitude ofthe difference is much larger than that of BLEU,and especially does not show convergence in theSpanish cases, then we still have a strong casefor the Treelet Penalty.
In fact, if human evalu-ators perceive a difference Spanish cases on thefull data systems, the case where we show con-vergence, then the resulting differences could bedescribed as the penalty value.Unfortunately, our human evaluation data onthe Treelet Penalty effect was inconclusive.
Ourevaluations show a strong correlation betweenBLEU and human evaluation, something that is at-tested to in the literature (e.g., , the first paper onBLEU (Papineni et al 2002), and a deeper explo-ration in (Coughlin, 2003)).
However, the effectwe were looking for ?
that is, a difference betweenhuman evaluations across decoders ?
was not evi-dent.
In fact, the human evaluations followed thedifferences we saw in BLEU between the two de-coders very closely.
Figure 11 shows data pointsfor each data size for each decoder, plotting BLEUagainst human evaluation.
When we fit a regres-sion line against the data points for each decoder,we see complete overlap.5Figure 11: Scatterplot showing Treelet vs Phrasalsystems across different data sizes, plotting BLEU(Y) against Human Eval scores (X)5Clearly, the sample is very small, so the regression lineshould be taken with a grain of salt.
We would need a lotmore data to be able to draw any strong conclusions.In summary, we show a strong effect of treeletsystems performing better than phrasal systemstrained on the same data.
That difference, how-ever, generally diminishes as data sizes increase,and in the case of Spanish (both directions), thereis a convergence in very large data sizes.
Theseresults are not completely surprising, but still are anice systematic confirmation that linguistically in-formed systems really do better in lower-data en-vironments.
Without enough data, statistical sys-tems cannot learn the generalizations that mightotherwise be provided by a parse, or codified inrules.
What we failed to show, at least with Span-ish and German, is a confirmation of the existenceof the Treelet Penalty.
Given the small number ofsamples, a larger study which includes many morelanguage pairs and data sizes, may once and for allconfirm the Penalty.
Thus far, human evaluationsdo not show qualitative differences between thetwo decoders?at least, not divergent from BLEU.4.3 Interaction Between Decoder Type andSentence LengthWhen comparing the differences between de-coders, another area to pay special attention to issystematic differences in behavior as input contentis varied.
For example, we may expect a phrasaldecoder to do better on noisier, less grammaticaldata than a parser-informed decoder, since in thelatter case the parser may fail to parse; the failurecould ripple through subsequent processes, andthus lessen the quality of the output.
Likewise, aparser-informed decoder may do better on contentthat is short and easy to parse.
If we were to do acoarse-grained separation of data into length buck-ets, making the very gross assumption that shortequals easy-to-parse and long not, then we maysee some qualitative differences between the de-coders across these buckets.To see length-based effects across decodertypes, we designed a set of experiments on Ger-man and Spanish in both directions, where we sep-arated the WMT 2010 test data into length-basedword-count buckets: 0-10, 10-20, 20-30, 30-40,and 40+ words.
We then calculated the BLEUscores on each of these buckets, the results forwhich are shown in Figures 12.Treelet does better than phrasal in almost allconditions (except one).
That is not surprising,given the results we observed in Section 4.2.
Whatis interesting is to see how much stronger treelet61Figure 12: Treelet-Phrasal BLEU differences bybucket across language pairperforms on short content than phrasal: treeletdoes the best on the shortest content, with qualitydropping off anywhere between 10-30 words.One conclusion that can be drawn from thesedata is that treelet performs best on short con-tent precisely because the parser can easily parsethe content, and the parse is effective in inform-ing subsequent processes.
The most sustainedbenefit is observable in English-German, with abump up at 10-20, and a slow tapering off there-after.
Processing the structural divergence be-tween the two languages, especially when it comesto word order, may benefit more from a parse.
Inother words, the parser can help inform alignmentwhere there are long-distance distortion effects; aphrasal system?s view is too local to catch them.However, at longer sentence sizes, the absenceof good parses lessen the treelet advantage.
Infact, in English-German (and in Spanish-English)at 40+, there is no observable benefit of treeletover phrasal.65 The Data GapAll Statistical Machine Translation work relies ondata, and the manipulation of the data as a pre-process can often have significant effects down-stream.
?Data munging?, as we like to call it, isevery team?s ?secret sauce?, something that canoften lead to multi-point differences in BLEU.For most teams, the heuristics that are applied arefairly ad hoc, and highly dependent on the kind ofdata being consumed.
Since data sources are of-ten quite noisy, e.g., the Web, noise reduction is akey component of many of the heuristics.
Here is6The bump up at 40+ on English-Spanish and German-English is inexplicable, but may be attributable to the diffi-culty that either decoder has in processing such long content.There is also likely an interaction with statistical noise causeby such small sample sizes.a list of common heuristics applied to data.
Someof these are drawn from our own pre-processing,some are mentioned explicitly in other literature,in particular, (Denkowski et al 2012).?
Remove lines containing escape characters,invalid Unicode, and other non-linguisticnoise.?
Remove content that where the ratio of cer-tain content passes some threshold, e.g., al-phabetic/numeric ratio, script ratio (percent-age of characters in wrong form passes somethreshold, triggering removal).?
Normalize space, hyphens, quotes, etc.
tostandard forms.?
Normalize Unicode characters to canonicalforms, e.g., Form C, Form KC.?
In parallel data, measure the degree of ratioof length imbalance (e.g., character or wordcount) between source and target, as a test formisalignments.
Remove sentence pairs thatpass some threshold.?
Remove content where character count forany token, or token count across a sentence,exceeds some threshold (the assumption be-ing that really long content is of little benefitdue to complications it causes in downstreamprocessing).The point of data cleaning heuristics is to in-crease the value of training data.
Each data pointthat is noisy increases the chance of learningsomething that could be distracting or harmful.Likewise, each data point that is cleaned reducesthe level of data sparsity (e.g., through normaliza-tions or substitutions) and improves the chancesthat the models will be more robust.
Althoughit has been shown that increasing the amount oftraining data for SMT improves results (Brants etal., 2007), not all data is beneficial, and clean datais best of all.Crucially, most data munging is done throughheuristics, or rules, although thresholds or con-straints can be tuned by data.
A more sophis-ticated example of data cleaning is described in(Denkowski et al 2012) where the authors usedmachine learning methods for measuring qualityestimation to select the ?best?
portions of a cor-pus.
So, rather than training their SMT on an en-tire corpus, they trained an estimator that selected62the best portions, and used only those.
In their en-try in the 2012 WMT competition, they used only60% of the English-French Gigaword corpus7 andcame in first in the shared translation task for thepair.Another important aspect of data as it relates toSMT is task-dependence: what domain or genreof data will an SMT engine be applied to?
Forinstance, will an SMT engine be used to trans-late IT content, news content, subtitles, or Eu-roparl proceedings?
If the engine itself is trainedon data that is dissimilar to the desired goal, thenresults may be less than satisfying.
This is a com-mon problem in the field, and a cottage industryhas been built around customization and domain-adaptation, e.g., (Moore and Lewis, 2010; Axelrodet al 2011; Wang et al 2012).
In general, the so-lution is to adapt an SMT engine to the desireddomain using a set of seed data in that domain.A more difficult problem is when there is verylittle parallel data in the desired domain, which isa problem we will look at in the next section.5.1 Preprocessing Data to Make it MatchA little over a year ago, Facebook activated atranslation feature in their service, which directlycalled Bing Translator.
This feature has allowedusers to translate pages or posts not in their nativelanguage with a See Translation option.
An exam-ple is shown in Figure 13.The real problem with translating ?FB-speak?,or content from virtually any kind of social media,is the paucity of parallel data in the domain.
Thisflies in the face of the usual way problems are tack-led in SMT, that is, locate (lots of) relevant paralleldata, and then train up a decoder.
Outside of a fewslang dictionaries, there is almost no FB-like par-allel content available.Given the relatively formal nature of the textthat most of our engines are trained on, the mis-match between FB content and our translation en-gines often led to very poor translations.
Yet,given the absence of in-domain parallel data, itwas not possible for us to train-up FB-specificSMT engines.
We realized that our only optionwas to somehow manipulate the input to make itlook more like the content we trained our engineson.
Effectively, if we treated ?FB-speak?
as a di-alect of the source language, we could use distri-7The English-French Gigaword corpus is described in(Callison-Burch et al 2009)Regex Outputfrnd[sz] friendsplz+ pleaseyess* yesbe?c[uo][sz] becausenuff enoughwo?u?lda would havesrr+y sorryTable 5: Some example regexes to ?fix?
FaceBookcontentbutional queues of dialect-specific content to findthe counterparts in the majority dialect.Table 4 gives some examples of FB content onthe left, and the more formal representation of thesame on the right.
The reader will note some sys-tematic characteristics of the FB content as com-pared to the formal content (see also (Hassan andMenezes, 2013)).
Given the absence of paralleltraining data, we could ?correct?
the FB contentto make it look more like English, and then trans-late the ?corrected?
English through our engines.Our first inclination was to examine the logs ofthe most frequent words being translated by FBusers and use string substitutions or regexes (regu-lar expressions) to effect repairs.
We arrived veryquickly at a large set of simple repairs like thoseshown in Table 5.
We were able to achieve greaterthan 97% precision using a large table of substitu-tions for the most common translations (against aheld-out set of FB content).
However, there weretwo problems with the approach: (1) recall wasrelatively low, at 52.03%, and (2) the solution wasnot easily scalable to additional languages and sce-narios.To address these two deficiencies, we sought amore data-driven approach.
But we had to be cre-ative since our standard ?hammer?
of parallel datadid not exist.
Our intuition was that there weredistributional regularities in the FB content thatcould help discover a mapping for a given targetword, e.g., the distribution of plzzz in the FB con-tent would allow us to discover that it distributessimilarly to please in our non-FB content.
HanyHassan developed a TextCorrector tool that is, ashe put it (Hassan and Menezes, 2013), ?based onconstructing a lattice from possible normalizationcandidates and finding the best normalization se-quence according to an n-gram language modelusing a Viterbi decoder?, where he developed an63Figure 13: Two Facebook posts: the first translated, the second showing the See Translation optionFB Speak English Translation Commentgoooood morniiing good morning Extended characters for emphasis or dramatic effectwuz up bro What?s up brother ?Phonetic?
spelling to reflect local dialect or usagecm to c my luv Come to see my love Remove vowels in common words, sound-alike sequences4get, 2morrow forget, tomorrow Sound-alike number substitutionr u 4 real?
Are you for real?
Sound-alike letter and number substitutionsLMS Like my status Single ?word?
abbreviations forIDK I don?t know multi-word expressionsROFL Rolling on the floor laughingTable 4: FB Speak with English references?unsupervised approach to learn the normalizationcandidates from unlabeled text data.?
He then useda Random Walk strategy to walk a contextual sim-ilarity graph.
The two principal benefits of thisapproach is that it did not require parallel train-ing data?two large monolingual corpora are re-quired, one for the ?noisy?
data (i.e., FB content)and one for the clean data (i.e., our large supplyof language model training data)?nor did it re-quire labeled data (i.e., , the algorithm is unsu-pervised).
After several iterations over very largecorpora (tens of millions of sentences) he arrivedat a solution that had comparable precision to theregex method but had much higher recall.
The bestiteration achieved 96.51% precision (the regex ap-proach achieve 97.07% precision) and 72.38% re-call (regex: 52.03%).8 Crucially, as the size ofthe data increases, the TextCorrector continues toshow improvement.The end result was a much better User Expe-rience for FB users.
Rather than badly mangledtranslations, or worse, no translations at all, usersget translations generated by our standard, verylarge statistical engines (for English source, no-tably, our treelet engines).
An example Englishsource string is shown in Table 6, with transla-8For a complete description of TextCorrector, pleasesee (Hassan and Menezes, 2013).tions shown for both the corrected and uncorrectedsource.6 Conclusions and Future DirectionsA crucial lesson from the work on the FB correc-tions described in Section 5.1 is its analog to Ma-chine Learning as a whole: rule-based approachesoften achieve very high precision, but often at thesacrifice of recall.
The same is true in MachineTranslation: rule-based MT is often more accuratewhen it was accurate, resulting in more precise andgrammatical translations.
However, it tends to besomewhat brittle and does not do as well on casesnot explicitly coded for.
SMT, on the other hand,tends to be more malleable and adaptable, but of-ten less precise.
Tapping rule-based approachesin a statistical framework can really give us thebest of both worlds, giving us higher precision andhigher recall.Finding an appropriate mix is difficult, though.As in the case of parsing, we can see how errorscan substantially degrade translation quality, espe-cially if we only consider the single best analysis.By making our analysis components as robust aspossible, quantifying our degree of certainty withscoring mechanisms, and preserving ambiguity ofthe analysis, we can achieve a better return on in-64Language Unrepaired RepairedOriginal English i?l do cuz ma parnts r ma lyf I?ll do because my parents are my lifeTo Italian i ?
l fare cuz ma parnts r ma lyf lo far perch i miei genitori sono la mia vitaTo German i ?
l tun Cuz Ma Parnts R Ma lyf Ich werde tun, weil meine Eltern mein Leben sindTo Spanish traer hacer cuz ma parnts r ma lyf voy a hacer porque mis padres son mi vidaTable 6: One English FB sentence with and without normalizations, translated to various languagesvestment.
Making this linguistic information beincluded softly as features is a powerful way ofsurfacing linguistic generalizations to the systemwhile not forcing its hand.Some of the greatest successes in mixing lin-guistic and statistical methods have been in syn-tax.
There is much ground to cover still.
Mor-phology is integrated weakly into current SMTsystems, mostly as broad features (Jeong et al2010) though sometimes with more sophistica-tion (Chahuneau et al 2013).
Better integration ofmorphological features could have great effect, es-pecially in agglutinative languages such as Finnishand Turkish.Deeper models of semantics present a rich chal-lenge to the field.
As we proceed into deeper mod-els, picking the correct representation is a signifi-cant issue.
Humans can generally agree on words,mostly on morphology, and somewhat on syntax.But semantics touches on issues of meaning repre-sentation: how should we best represent semanticinformation?
Should we attempt to faithfully rep-resent all the information in the source language,or gather only a simple model that suffices to dis-ambiguate information?
Others are focusing onlexical semantics using continuous space repre-sentations (Mikolov et al 2013), a softer meansof representing meaning.Regardless of the details, one point is very clear:future work in MT will require dealing with data.Systems, whether statistical or rule-based, willneed to work with and learn from the increas-ing volumes of information available to comput-ers.
Effective hybrid systems will be no exception?
tempering the keen insights of experts with thenoisy wisdom of big data from the crowd holdsgreat promise.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proceedings of EMNLP, pages 355?362.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 858?867, Prague, Czech Republic,June.
Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.
Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metric-sMATR, pages 17?53, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Marine Carpuat and Michel Simard.
2012.
The troublewith smt consistency.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages442?449, Montre?al, Canada, June.
Association forComputational Linguistics.Victor Chahuneau, Noah A. Smith, and Chris Dyer.2013.
Knowledge-rich morphological priors forbayesian language models.
In Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 1206?1215, At-lanta, Georgia, June.
Association for ComputationalLinguistics.Deborah A. Coughlin.
2003.
Correlating automatedand human assessments of machine translation qual-ity.
In Proceedings of MT Summit IX, New Or-leans, Louisiana, USA, September.
The Associationfor Machine Translation in the Americas (AMTA).Michael Denkowski, Greg Hanneman, and Alon Lavie.2012.
The CMU-Avenue French-English Transla-tion System.
In Proceedings of the NAACL 2012Workshop on Statistical Machine Translation.Mireia Farru?s, Marta R.
Costa-jussa?, and MajaPopovic.
2012.
Study and correlation analysis oflinguistic, perceptual and automatic machine trans-lation evaluations.
Journal of the American Societyfor Information Science and Technology, 63(1):174?184, January.65Hany Hassan and Arul Menezes.
2013.
Social text nor-malization using contextual graph random walks.
InProceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.Xiaodong He, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore.
2008.
Indirect-hmm-based hypothesis alignment for combining outputsfrom machine translation systems.
In Proceedingsof EMNLP.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1352?1362, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,and Chris Quirk.
2010.
A discriminative lexiconmodel for complex morphology.
In The Ninth Con-ference of the Association for Machine Translationin the Americas (AMTA-2010).Kevin Knight.
2013.
Tutorial on decipherment.
InACL 2013, Sofia, Bulgaria, August.Arul Menezes and Stephen D. Richardson.
2001.
Abest-first alignment algorithm for automatic extrac-tion of transfer mappings from bilingual corpora.
InAssociation for Computational Linguistics.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL-08: HLT,pages 192?199, Columbus, Ohio, June.
Associationfor Computational Linguistics.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 746?751, Atlanta,Georgia, June.
Association for Computational Lin-guistics.Robert C. Moore and William D. Lewis.
2010.
Intel-ligent Selection of Language Model Training Data.In Proceedings of the ACL 2010 Conference ShortPapers, Uppsala, Sweden, July.Franz Josef Och and Hermann Ney.
2004.
TheAlignment Template Approach to Statistical Ma-chine Translation.
Computational Linguisitics,30(4):417?449, September.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st ACL, Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th ACL, Philadelphia, PA.Chris Quirk and Simon Corston-Oliver.
2006.
The im-pact of parse quality on syntactically-informed sta-tistical machine translation.
In Proceedings of the2006 Conference on Empirical Methods in NaturalLanguage Processing, pages 62?69, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Chris Quirk and Arul Menezes.
2006.
DependencyTreelet Translation: The convergence of statisticaland example-based Machine Translation?
MachineTranslation, 20:43?65.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshopon Intrinsic and Extrinsic Evaluation Measures forMachine Translation and/or Summarization, pages57?64, Ann Arbor, Michigan, June.
Association forComputational Linguistics.Wei Wang, Klaus Macherey, Wolfgang Macherey,Franz Och, and Peng Xu.
2012.
Improved do-main adaptation for statistical machine translation.In Proceedings of AMTA.Warren Weaver.
1955.
Translation.
In William N.Locke and A. Donald Booth, editors, MachineTranslation of Languages, pages 15?23.
MIT Press,Massachussets.Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.2011.
Binarized forest to string translation.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, pages 835?845, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.66
