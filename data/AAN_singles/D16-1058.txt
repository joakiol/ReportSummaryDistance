Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 606?615,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAttention-based LSTM for Aspect-level Sentiment ClassificationYequan Wang and Minlie Huang and Li Zhao* and Xiaoyan ZhuState Key Laboratory on Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Science and Technology, Tsinghua University, Beijing 100084, China*Microsoft Research Asiawangyequan@live.cn, aihuang@tsinghua.edu.cnlizo@microsoft.com, zxy-dcs@tsinghua.edu.cnAbstractAspect-level sentiment classification is a fine-grained task in sentiment analysis.
Since itprovides more complete and in-depth results,aspect-level sentiment analysis has receivedmuch attention these years.
In this paper, wereveal that the sentiment polarity of a sentenceis not only determined by the content but isalso highly related to the concerned aspect.For instance, ?The appetizers are ok, but theservice is slow.
?, for aspect taste, the polar-ity is positive while for service, the polarityis negative.
Therefore, it is worthwhile to ex-plore the connection between an aspect andthe content of a sentence.
To this end, wepropose an Attention-based Long Short-TermMemory Network for aspect-level sentimentclassification.
The attention mechanism canconcentrate on different parts of a sentencewhen different aspects are taken as input.
Weexperiment on the SemEval 2014 dataset andresults show that our model achieves state-of-the-art performance on aspect-level sentimentclassification.1 IntroductionSentiment analysis (Nasukawa and Yi, 2003), alsoknown as opinion mining (Liu, 2012), is a keyNLP task that receives much attention these years.Aspect-level sentiment analysis is a fine-grainedtask that can provide complete and in-depth results.In this paper, we deal with aspect-level sentimentclassification and we find that the sentiment polar-ity of a sentence is highly dependent on both con-tent and aspect.
For example, the sentiment polarityof ?Staffs are not that friendly, but the taste coversall.?
will be positive if the aspect is food but neg-ative when considering the aspect service.
Polaritycould be opposite when different aspects are consid-ered.Neural networks have achieved state-of-the-artperformance in a variety of NLP tasks such as ma-chine translation (Lample et al, 2016), paraphraseidentification (Yin et al, 2015), question answer-ing (Golub and He, 2016) and text summariza-tion (Rush et al, 2015).
However, neural net-work models are still in infancy to deal with aspect-level sentiment classification.
In some works, tar-get dependent sentiment classification can be ben-efited from taking into account target information,such as in Target-Dependent LSTM (TD-LSTM)and Target-Connection LSTM (TC-LSTM) (Tang etal., 2015a).
However, those models can only takeinto consideration the target but not aspect informa-tion which is proved to be crucial for aspect-levelclassification.Attention has become an effective mechanism toobtain superior results, as demonstrated in imagerecognition (Mnih et al, 2014), machine transla-tion (Bahdanau et al, 2014), reasoning about entail-ment (Rockta?schel et al, 2015) and sentence sum-marization (Rush et al, 2015).
Even more, neuralattention can improve the ability to read comprehen-sion (Hermann et al, 2015).
In this paper, we pro-pose an attention mechanism to enforce the modelto attend to the important part of a sentence, in re-sponse to a specific aspect.
We design an aspect-to-sentence attention mechanism that can concentrate606on the key part of a sentence given the aspect.We explore the potential correlation of aspect andsentiment polarity in aspect-level sentiment classifi-cation.
In order to capture important information inresponse to a given aspect, we design an attention-based LSTM.
We evaluate our approach on a bench-mark dataset (Pontiki et al, 2014), which containsrestaurants and laptops data.The main contributions of our work can be sum-marized as follows:?
We propose attention-based Long Short-Termmemory for aspect-level sentiment classifica-tion.
The models are able to attend differ-ent parts of a sentence when different aspectsare concerned.
Results show that the attentionmechanism is effective.?
Since aspect plays a key role in this task, wepropose two ways to take into account aspectinformation during attention: one way is toconcatenate the aspect vector into the sentencehidden representations for computing attentionweights, and another way is to additionally ap-pend the aspect vector into the input word vec-tors.?
Experimental results indicate that our ap-proach can improve the performance comparedwith several baselines, and further examplesdemonstrate the attention mechanism workswell for aspect-level sentiment classification.The rest of our paper is structured as follows:Section 2 discusses related works, Section 3 gives adetailed description of our attention-based propos-als, Section 4 presents extensive experiments to jus-tify the effectiveness of our proposals, and Section 5summarizes this work and the future direction.2 Related WorkIn this section, we will review related works onaspect-level sentiment classification and neural net-works for sentiment classification briefly.2.1 Sentiment Classification at Aspect-levelAspect-level sentiment classification is typicallyconsidered as a classification problem in the liter-ature.
As we mentioned before, aspect-level sen-timent classification is a fine-grained classificationtask.
The majority of current approaches attempt todetecting the polarity of the entire sentence, regard-less of the entities mentioned or aspects.
Traditionalapproaches to solve those problems are to manu-ally design a set of features.
With the abundance ofsentiment lexicons (Rao and Ravichandran, 2009;Perez-Rosas et al, 2012; Kaji and Kitsuregawa,2007), the lexicon-based features were built for sen-timent analysis (Mohammad et al, 2013).
Most ofthese studies focus on building sentiment classifierswith features, which include bag-of-words and sen-timent lexicons, using SVM (Mullen and Collier,2004).
However, the results highly depend on thequality of features.
In addition, feature engineeringis labor intensive.2.2 Sentiment Classification with NeuralNetworksSince a simple and effective approach to learn dis-tributed representations was proposed (Mikolov etal., 2013), neural networks advance sentiment anal-ysis substantially.
Classical models including Re-cursive Neural Network (Socher et al, 2011; Donget al, 2014; Qian et al, 2015), Recursive Neu-ral Tensor Network (Socher et al, 2013), Recur-rent Neural Network (Mikolov et al, 2010; Tanget al, 2015b), LSTM (Hochreiter and Schmidhuber,1997) and Tree-LSTMs (Tai et al, 2015) were ap-plied into sentiment analysis currently.
By utilizingsyntax structures of sentences, tree-based LSTMshave been proved to be quite effective for many NLPtasks.
However, such methods may suffer from syn-tax parsing errors which are common in resource-lacking languages.LSTM has achieved a great success in variousNLP tasks.
TD-LSTM and TC-LSTM (Tang etal., 2015a), which took target information into con-sideration, achieved state-of-the-art performancein target-dependent sentiment classification.
TC-LSTM obtained a target vector by averaging thevectors of words that the target phrase contains.However, simply averaging the word embeddings ofa target phrase is not sufficient to represent the se-mantics of the target phrase, resulting a suboptimalperformance.607Despite the effectiveness of those methods, it isstill challenging to discriminate different sentimentpolarities at a fine-grained aspect level.
Therefore,we are motivated to design a powerful neural net-work which can fully employ aspect information forsentiment classification.3 Attention-based LSTM with AspectEmbedding3.1 Long Short-term Memory (LSTM)Recurrent Neural Network(RNN) is an extension ofconventional feed-forward neural network.
How-ever, standard RNN has the gradient vanishingor exploding problems.
In order to overcomethe issues, Long Short-term Memory network(LSTM) was developed and achieved superior per-formance (Hochreiter and Schmidhuber, 1997).
Inthe LSTM architecture, there are three gates and acell memory state.
Figure 1 illustrates the architec-ture of a standard LSTM.LSTM LSTM LSTM?softmax?1 ?2 ??
?1 ?2 ?
?Figure 1: The architecture of a standard LSTM.
{w1, w2, .
.
.
, wN} represent the word vector in a sen-tence whose length is N .
{h1, h2, .
.
.
, hN} is the hiddenvector.More formally, each cell in LSTM can be com-puted as follows:X =[ht?1xt](1)ft = ?
(Wf ?X + bf ) (2)it = ?
(Wi ?X + bi) (3)ot = ?
(Wo ?X + bo) (4)ct = ft ?
ct?1 + it ?
tanh(Wc ?X + bc) (5)ht = ot ?
tanh(ct) (6)where Wi,Wf ,Wo ?
Rd?2d are the weighted ma-trices and bi, bf , bo ?
Rd are biases of LSTM to belearned during training, parameterizing the transfor-mations of the input, forget and output gates respec-tively.
?
is the sigmoid function and ?
stands forelement-wise multiplication.
xt includes the inputsof LSTM cell unit, representing the word embed-ding vectors wt in Figure 1.
The vector of hiddenlayer is ht.We regard the last hidden vector hN as the rep-resentation of sentence and put hN into a softmaxlayer after linearizing it into a vector whose length isequal to the number of class labels.
In our work, theset of class labels is {positive, negative, neutral}.3.2 LSTM with Aspect Embedding(AE-LSTM)Aspect information is vital when classifying the po-larity of one sentence given aspect.
We may get op-posite polarities if different aspects are considered.To make the best use of aspect information, we pro-pose to learn an embedding vector for each aspect.Vector vai ?
Rda is represented for the embed-ding of aspect i, where da is the dimension of aspectembedding.
A ?
Rda?|A| is made up of all aspectembeddings.
To the best of our knowledge, it is thefirst time to propose aspect embedding.3.3 Attention-based LSTM (AT-LSTM)The standard LSTM cannot detect which is the im-portant part for aspect-level sentiment classification.In order to address this issue, we propose to de-sign an attention mechanism that can capture thekey part of sentence in response to a given aspect.Figure 2 represents the architecture of an Attention-based LSTM (AT-LSTM).Let H ?
Rd?N be a matrix consisting of hid-den vectors [h1, .
.
.
, hN ] that the LSTM produced,where d is the size of hidden layers and N is thelength of the given sentence.
Furthermore, va rep-resents the embedding of aspect and eN ?
RN is avector of 1s.
The attention mechanism will producean attention weight vector ?
and a weighted hidden608LSTM LSTM LSTMWord Representation?AttentionAspect EmbeddingH?2 ??
??
?2 ??
????
??
??LSTM?1?1?
?r?Figure 2: The Architecture of Attention-based LSTM.
The aspect embeddings have been used to decide the attention weightsalong with the sentence representations.
{w1, w2, .
.
.
, wN} represent the word vector in a sentence whose length is N .
varepresents the aspect embedding.
?
is the attention weight.
{h1, h2, .
.
.
, hN} is the hidden vector.representation r.M = tanh([WhHWvva ?
eN]) (7)?
= softmax(wTM) (8)r = H?T (9)where, M ?
R(d+da)?N , ?
?
RN , r ?
Rd.Wh ?
Rd?d, Wv ?
Rda?da and w ?
Rd+da areprojection parameters.
?
is a vector consisting ofattention weights and r is a weighted representationof sentence with given aspect.
The operator in 7 (acircle with a multiplication sign inside, OP for shorthere) means: va?eN = [v; v; .
.
.
; v], that is, the op-erator repeatedly concatenates v for N times, whereeN is a column vector with N 1s.
Wvva ?
eN isrepeating the linearly transformed va as many timesas there are words in sentence.The final sentence representation is given by:h?
= tanh(Wpr + WxhN ) (10)where, h?
?
Rd, Wp and Wx are projection param-eters to be learned during training.
We find that thisworks practically better if we add WxhN into the fi-nal representation of the sentence, which is inspiredby (Rockta?schel et al, 2015).The attention mechanism allows the model tocapture the most important part of a sentence whendifferent aspects are considered.h?
is considered as the feature representation ofa sentence given an input aspect.
We add a linearlayer to convert sentence vector to e, which is a real-valued vector with the length equal to class number|C|.
Then, a softmax layer is followed to trans-form e to conditional probability distribution.y = softmax(Wsh?
+ bs) (11)where Ws and bs are the parameters for softmaxlayer.3.4 Attention-based LSTM with AspectEmbedding (ATAE-LSTM)The way of using aspect information in AE-LSTMis letting aspect embedding play a role in com-puting the attention weight.
In order to bettertake advantage of aspect information, we appendthe input aspect embedding into each word inputvector.
The structure of this model is illustratedin 3.
In this way, the output hidden representa-tions (h1, h2, ..., hN ) can have the information fromthe input aspect (va).
Therefore, in the followingstep that compute the attention weights, the inter-609LSTM LSTM LSTMWord Representation?AttentionAspect EmbeddingH?2 ??
??
?2 ??
????
??
??LSTM?1?1????
??
???
?Aspect Embedding?
?Figure 3: The Architecture of Attention-based LSTM with Aspect Embedding.
The aspect embeddings have been take as inputalong with the word embeddings.
{w1, w2, .
.
.
, wN} represent the word vector in a sentence whose length is N .
va represents theaspect embedding.
?
is the attention weight.
{h1, h2, .
.
.
, hN} is the hidden vector.dependence between words and the input aspect canbe modeled.3.5 Model TrainingThe model can be trained in an end-to-end way bybackpropagation, where the objective function (lossfunction) is the cross-entropy loss.
Let y be the tar-get distribution for sentence, y?
be the predicted sen-timent distribution.
The goal of training is to mini-mize the cross-entropy error between y and y?
for allsentences.loss = ?
?i?jyji logy?ji + ?||?||2 (12)where i is the index of sentence, j is the index ofclass.
Our classification is three way.
?
is the L2 -regularization term.
?
is the parameter set.Similar to standard LSTM, the parameter setis {Wi, bi,Wf , bf ,Wo, bo,Wc, bc,Ws, bs}.
Fur-thermore, word embeddings are the parameterstoo.
Note that the dimension of Wi,Wf ,Wo,Wcchanges along with different models.
If the aspectembeddings are added into the input of the LSTMcell unit, the dimension of Wi,Wf ,Wo,Wc will beenlarged correspondingly.
Additional parametersare listed as follows:AT-LSTM: The aspect embedding A is addedinto the set of parameters naturally.
In addition,Wh,Wv,Wp,Wx, w are the parameters of atten-tion.
Therefore, the additional parameter set of AT-LSTM is {A,Wh,Wv,Wp,Wx, w}.AE-LSTM: The parameters include the as-pect embedding A.
Besides, the dimension ofWi,Wf ,Wo,Wc will be expanded since the aspectvector is concatenated.
Therefore, the additional pa-rameter set consists of {A}.ATAE-LSTM: The parameter set consists of{A,Wh,Wv,Wp,Wx, w}.
Additionally, the dimen-sion of Wi,Wf ,Wo,Wc will be expanded with theconcatenation of aspect embedding.The word embedding and aspect embedding areoptimized during training.
The percentage of out-of-vocabulary words is about 5%, and they are ran-domly initialized from U(?
?, ?
), where ?
= 0.01.In our experiments, we use AdaGrad (Duchi etal., 2011) as our optimization method, which has610improved the robustness of SGD on large scalelearning task remarkably in a distributed environ-ment (Dean et al, 2012).
AdaGrad adapts the learn-ing rate to the parameters, performing larger updatesfor infrequent parameters and smaller updates forfrequent parameters.4 ExperimentWe apply the proposed model to aspect-level sen-timent classification.
In our experiments, all wordvectors are initialized by Glove1 (Pennington et al,2014).
The word embedding vectors are pre-trainedon an unlabeled corpus whose size is about 840 bil-lion.
The other parameters are initialized by sam-pling from a uniform distribution U(?
?, ?).
Thedimension of word vectors, aspect embeddings andthe size of hidden layer are 300.
The length of at-tention weights is the same as the length of sentence.Theano (Bastien et al, 2012) is used for implement-ing our neural network models.
We trained all mod-els with a batch size of 25 examples, and a momen-tum of 0.9, L2-regularization weight of 0.001 andinitial learning rate of 0.01 for AdaGrad.4.1 DatasetWe experiment on the dataset of SemEval 2014 Task42 (Pontiki et al, 2014).
The dataset consists ofcustomers reviews.
Each review contains a list ofaspects and corresponding polarities.
Our aim is toidentify the aspect polarity of a sentence with thecorresponding aspect.
The statistics is presented inTable 1.4.2 Task DefinitionAspect-level Classification Given a set of pre-identified aspects, this task is to determine thepolarity of each aspect.
For example, given asentence, ?The restaurant was too expensive.
?,there is an aspect price whose polarity is negative.The set of aspects is {food, price, service, ambi-ence, anecdotes/miscellaneous}.
In the dataset ofSemEval 2014 Task 4, there is only restaurantsdata that has aspect-specific polarities.
Table 21Pre-trained word vectors of Glove can be obtained fromhttp://nlp.stanford.edu/projects/glove/2The introduction about SemEval 2014 can be obtainedfrom http://alt.qcri.org/semeval2014/Asp.
Positive Negative NeuralTrain Test Train Test Train TestFo.
867 302 209 69 90 31Pr.
179 51 115 28 10 1Se.
324 101 218 63 20 3Am.
263 76 98 21 23 8An.
546 127 199 41 357 51Total 2179 657 839 222 500 94Table 1: Aspects distribution per sentiment class.
{Fo., Pr.,Se, Am., An.}
refer to {food, price, service, ambience, anec-dotes/miscellaneous}.
?Asp.?
refers to aspect.Models Three-way Pos./Neg.LSTM 82.0 88.3TD-LSTM 82.6 89.1TC-LSTM 81.9 89.2AE-LSTM 82.5 88.9AT-LSTM 83.1 89.6ATAE-LSTM 84.0 89.9Table 2: Accuracy on aspect level polarity classification aboutrestaurants.
Three-way stands for 3-class prediction.
Pos./Neg.indicates binary prediction where ignoring all neutral instances.Best scores are in bold.illustrates the comparative results.Aspect-Term-level Classification For a given setof aspects term within a sentence, this task is to de-termine whether the polarity of each aspect term ispositive, negative or neutral.
We conduct experi-ments on the dataset of SemEval 2014 Task 4.
Inthe sentences of both restaurant and laptop datasets,there are the location and sentiment polarity foreach occurrence of an aspect term.
For example,there is an aspect term fajitas whose polarity is neg-ative in sentence ?I loved their fajitas.
?.Experiments results are shown in Table 3 and Ta-ble 4.
Similar to the experiment on aspect-levelclassification, our models achieve state-of-the-artperformance.4.3 Comparison with baseline methodsWe compare our model with several baselines, in-cluding LSTM, TD-LSTM, and TC-LSTM.LSTM: Standard LSTM cannot capture any as-pect information in sentence, so it must get the same611?
(a ) the aspect of this sentence: service?
(b) the aspect of this sentence: foodFigure 4: Attention Visualizations.
The aspects of (a) and (b) are service and food respectively.
The color depth expresses theimportance degree of the weight in attention vector ?.
From (a), attention can detect the important words from the whole sentencedynamically even though multi-semantic phrase such as ?fastest delivery times?
which can be used in other areas.
From (b),attention can know multi-keypoints if more than one keypoint existing.Models Three-way Pos./Neg.LSTM 74.3 -TD-LSTM 75.6 -AE-LSTM 76.6 89.6ATAE-LSTM 77.2 90.9Table 3: Accuracy on aspect term polarity classification aboutrestaurants.
Three-way stands for 3-class prediction.
Pos./Neg.indicates binary prediction where ignoring all neutral instances.Best scores are in bold.Models Three-way Pos./Neg.LSTM 66.5 -TD-LSTM 68.1 -AE-LSTM 68.9 87.4ATAE-LSTM 68.7 87.6Table 4: Accuracy on aspect term polarity classification aboutlaptops.
Three-way stands for 3-class prediction.
Pos./Neg.
in-dicates binary prediction where ignoring all neutral instances.Best scores are in bold.sentiment polarity although given different aspects.Since it cannot take advantage of the aspect infor-mation, not surprisingly the model has worst per-formance.TD-LSTM: TD-LSTM can improve the perfor-mance of sentiment classifier by treating an aspectas a target.
Since there is no attention mechanism inTD-LSTM, it cannot ?know?
which words are im-portant for a given aspect.TC-LSTM: TC-LSTM extended TD-LSTM byincorporating a target into the representation of asentence.
It is worth noting that TC-LSTM per-forms worse than LSTM and TD-LSTM in Table 2.TC-LSTM added target representations, which wasobtained from word vectors, into the input of theLSTM cell unit.In our models, we embed aspects into anothervector space.
The embedding vector of aspects canbe learned well in the process of training.
ATAE-LSTM not only addresses the shortcoming of theunconformity between word vectors and aspect em-beddings, but also can capture the most importantinformation in response to a given aspect.
In ad-dition, ATAE-LSTM can capture the important anddifferent parts of a sentence when given differentaspects.4.4 Qualitative AnalysisIt is enlightening to analyze which words decide thesentiment polarity of the sentence given an aspect.We can obtain the attention weight ?
in Equation 8and visualize the attention weights accordingly.Figure 4 shows the representation of how atten-tion focuses on words with the influence of a givenaspect.
We use a visualization tool Heml (Deng612The   appetizers   are   ok,   but   the   service   is   slow.I  highly  recommend  it  for  not  just  its  superb  cuisine,  but  also  for  its  friendly  owners  and  staff.The  service,  however,  is  a  peg  or  two  below  the  quality  of  food  (horrible  bartenders),  andthe  clientele,  for  the  most  part,  are  rowdy,  loud-mouthed  commuters  (this  could  explain  thebad  attitudes  from  the  staff)  getting  loaded  for  an  AC/DC  concert  or  a  Knicks  game.aspect: service; polarity: negativeaspect: food; polarity: neutral(a)aspect: food; polarity: positive aspect: food; polarity: positive(b)aspect: food; polarity: positive aspect: service; polarity: positive aspect: ambience; polarity: negative(c)Figure 5: Examples of classification.
(a) is an instance with different aspects.
(b) represents that our model can focus on wherethe keypoints are and not disturbed by the privative word not.
(c) stands for long and complicated sentences.
Our model can obtaincorrect sentiment polarity.et al, 2014) to visualize the sentences.
The colordepth indicates the importance degree of the weightin attention vector ?, the darker the more important.The sentences in Figure 4 are ?I have to say theyhave one of the fastest delivery times in the city .
?and ?The fajita we tried was tasteless and burnedand the mole sauce was way too sweet.?.
The corre-sponding aspects are service and food respectively.Obviously attention can get the important parts fromthe whole sentence dynamically.
In Figure 4 (a),?fastest delivery times?
is a multi-word phrase, butour attention-based model can detect such phrasesif service can is the input aspect.
Besides, the atten-tion can detect multiple keywords if more than onekeyword is existing.
In Figure 4 (b), tastless and toosweet are both detected.4.5 Case StudyAs we demonstrated, our models obtain the state-of-the-art performance.
In this section, we will furthershow the advantages of our proposals through sometypical examples.In Figure 5, we list some examples from the testset which have typical characteristics and cannot beinferred by LSTM.
In sentence (a), ?The appetiz-ers are ok, but the service is slow.
?, there are twoaspects food and service.
Our model can discrimi-nate different sentiment polarities with different as-pects.
In sentence (b), ?I highly recommend it fornot just its superb cuisine, but also for its friendlyowners and staff.
?, there is a negation word not.
Ourmodel can obtain correct polarity, not affected bythe negation word who doesn?t represent negationhere.
In the last instance (c), ?The service, however,is a peg or two below the quality of food (horri-ble bartenders), and the clientele, for the most part,are rowdy, loud-mouthed commuters (this could ex-plain the bad attitudes from the staff) getting loadedfor an AC/DC concert or a Knicks game.
?, the sen-tence has a long and complicated structure so thatexisting parser may hardly obtain correct parsingtrees.
Hence, tree-based neural network modelsare difficult to predict polarity correctly.
While ourattention-based LSTM can work well in those sen-tences with the help of attention mechanism and as-pect embedding.5 Conclusion and Future WorkIn this paper, we have proposed attention-basedLSTMs for aspect-level sentiment classification.The key idea of these proposals are to learn aspect613embeddings and let aspects participate in computingattention weights.
Our proposed models can con-centrate on different parts of a sentence when dif-ferent aspects are given so that they are more com-petitive for aspect-level classification.
Experimentsshow that our proposed models, AE-LSTM andATAE-LSTM, obtain superior performance over thebaseline models.Though the proposals have shown potentials foraspect-level sentiment analysis, different aspects areinput separately.
As future work, an interestingand possible direction would be to model more thanone aspect simultaneously with the attention mech-anism.AcknowledgmentsThis work was partly supported by the NationalBasic Research Program (973 Program) undergrant No.2012CB316301/2013CB329403, the Na-tional Science Foundation of China under grantNo.61272227/61332007, and the Beijing HigherEducation Young Elite Teacher Project.
The workwas also supported by Tsinghua University BeijingSamsung TelecomR&DCenter Joint Laboratory forIntelligent Media Computing.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprintarXiv:1409.0473.Fre?de?ric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian Goodfellow, Arnaud Bergeron,Nicolas Bouchard, David Warde-Farley, and YoshuaBengio.
2012.
Theano: new features and speed im-provements.
arXiv preprint arXiv:1211.5590.Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,Matthieu Devin, Mark Mao, Andrew Senior, PaulTucker, Ke Yang, Quoc V Le, et al 2012.
Large scaledistributed deep networks.
In Advances in NeuralInformation Processing Systems, pages 1223?1231.Wankun Deng, Yongbo Wang, Zexian Liu, Han Cheng,and Yu Xue.
2014.
Hemi: a toolkit for illustratingheatmaps.
PloS one, 9(11):e111988.Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, MingZhou, and Ke Xu.
2014.
Adaptive recursive neuralnetwork for target-dependent twitter sentiment classi-fication.
In ACL (2), pages 49?54.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.David Golub and Xiaodong He.
2016.
Character-levelquestion answering with attention.
arXiv preprintarXiv:1604.00727.Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom.
2015.
Teaching machines to readand comprehend.
In Advances in Neural InformationProcessing Systems, pages 1684?1692.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Nobuhiro Kaji and Masaru Kitsuregawa.
2007.
Buildinglexicon for sentiment analysis frommassive collectionof html documents.
In EMNLP-CoNLL, pages 1075?1083.Guillaume Lample, Miguel Ballesteros, Sandeep Subra-manian, Kazuya Kawakami, and Chris Dyer.
2016.Neural architectures for named entity recognition.arXiv preprint arXiv:1603.01360.Bing Liu.
2012.
Sentiment analysis and opinion mining.Synthesis lectures on human language technologies,5(1):1?167.Tomas Mikolov, Martin Karafia?t, Lukas Burget, JanCernocky`, and Sanjeev Khudanpur.
2010.
Re-current neural network based language model.
InINTERSPEECH, volume 2, page 3.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Volodymyr Mnih, Nicolas Heess, Alex Graves, et al2014.
Recurrent models of visual attention.
InAdvances in Neural Information Processing Systems,pages 2204?2212.Saif M Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
arXiv preprintarXiv:1308.6242.Tony Mullen and Nigel Collier.
2004.
Sentiment analy-sis using support vector machines with diverse infor-mation sources.
In EMNLP, volume 4, pages 412?418.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Sen-timent analysis: Capturing favorability using natu-ral language processing.
In Proceedings of the 2ndinternational conference on Knowledge capture, pages70?77.
ACM.614Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for word rep-resentation.
Proceedings of the Empiricial Methodsin Natural Language Processing (EMNLP 2014),12:1532?1543.Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-cea.
2012.
Learning sentiment lexicons in spanish.
InLREC, volume 12, page 73.Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-ris Papageorgiou, Ion Androutsopoulos, and SureshManandhar.
2014.
Semeval-2014 task 4: Aspectbased sentiment analysis.
In Proceedings of the8th international workshop on semantic evaluation(SemEval 2014), pages 27?35.Qiao Qian, Bo Tian, Minlie Huang, Yang Liu, XuanZhu, and Xiaoyan Zhu.
2015.
Learning tag embed-dings and tag-specific composition functions in re-cursive neural network.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing, volume 1, pages1365?1374.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceedingsof the 12th Conference of the European Chapter ofthe Association for Computational Linguistics, pages675?682.
Association for Computational Linguistics.Tim Rockta?schel, Edward Grefenstette, Karl Moritz Her-mann, Toma?s?
Koc?isky`, and Phil Blunsom.
2015.
Rea-soning about entailment with neural attention.
arXivpreprint arXiv:1509.06664.Alexander M Rush, Sumit Chopra, and Jason We-ston.
2015.
A neural attention model for ab-stractive sentence summarization.
arXiv preprintarXiv:1509.00685.Richard Socher, Jeffrey Pennington, Eric H Huang, An-drew Y Ng, and Christopher D Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 151?161.
Association forComputational Linguistics.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts.
2013.
Recursive deep models forsemantic compositionality over a sentiment treebank.In EMNLP, volume 1631, page 1642.
Citeseer.Kai Sheng Tai, Richard Socher, and Christopher DManning.
2015.
Improved semantic representa-tions from tree-structured long short-term memorynetworks.
arXiv preprint arXiv:1503.00075.Duyu Tang, Bing Qin, Xiaocheng Feng, and TingLiu.
2015a.
Target-dependent sentiment classifica-tion with long short term memory.
arXiv preprintarXiv:1512.01100.Duyu Tang, Bing Qin, and Ting Liu.
2015b.
Docu-ment modeling with gated recurrent neural networkfor sentiment classification.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 1422?1432.Wenpeng Yin, Hinrich Schu?tze, Bing Xiang, and BowenZhou.
2015.
Abcnn: Attention-based convolutionalneural network for modeling sentence pairs.
arXivpreprint arXiv:1512.05193.615
