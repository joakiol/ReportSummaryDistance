Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 37?44,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsSearch right and thou shalt find ...Using Web Queries for Learner Error DetectionMichael Gamon Claudia LeacockMicrosoft Research Butler Hill GroupOne Microsoft Way P.O.
Box 935Redmond, WA 981052, USA Ridgefield, CT 06877, USAmgamon@microsoft.com Claudia.leacock@gmail.comAbstractWe investigate the use of web search queriesfor detecting errors in non-native writing.
Dis-tinguishing a correct sequence of words froma sequence with a learner error is a baselinetask that any error detection and correctionsystem needs to address.
Using a large corpusof error-annotated learner data, we investigatewhether web search result counts can be usedto distinguish correct from incorrect usage.
Inthis investigation, we compare a variety ofquery formulation strategies and a number ofweb resources, including two major searchengine APIs and a large web-based n-gramcorpus.1 IntroductionData-driven approaches to the detection and cor-rection of non-native errors in English have beenresearched actively in the past several years.
Sucherrors are particularly amenable to data-driven me-thods because many prominent learner writing er-rors involve a relatively small class of phenomenathat can be targeted with specific models, in par-ticular article and preposition errors.
Prepositionand determiner errors (most of which are articleerrors) are the second and third most frequent er-rors in the Cambridge Learner Corpus (after themore intractable problem of content word choice).By targeting the ten most frequent prepositionsinvolved in learner errors, more than 80% of pre-position errors in the corpus are covered.Typically, data-driven approaches to learner er-rors use a classifier trained on contextual informa-tion such as tokens and part-of-speech tags withina window of the preposition/article (Gamon et al2008, 2010, DeFelice and Pulman 2007, 2008, Hanet al 2006, Chodorow et al 2007, Tetreault andChodorow 2008).Language models are another source of evidencethat can be used in error detection.
Using languagemodels for this purpose is not a new approach, itgoes back to at least Atwell (1987).
Gamon et al(2008) and Gamon (2010) use a combination ofclassification and language modeling.
Once lan-guage modeling comes into play, the quantity ofthe training data comes to the forefront.
It has beenwell-established that statistical models improve asthe size of the training data increases (Banko andBrill 2001a, 2001b).
This is particularly true forlanguage models: other statistical models such as aclassifier, for example, can be targeted towards aspecific decision/classification, reducing the appe-tite for data somewhat, while language modelsprovide probabilities for any sequence of words - atask that requires immense training data resourcesif the language model is to consider increasinglysparse longer n-grams.Language models trained on data sources likethe Gigaword corpus have become commonplace,but of course there is one corpus that dwarfs anyother resource in size: the World Wide Web.
Thishas drawn the interest of many researchers in natu-ral language processing over the past decade.
Tomention just a few examples, Zhu and Rosenfeld(2001) combine trigram counts from the web withan existing language model where the estimates ofthe existing model are unreliable because of datasparseness.
Keller and Lapata (2003) advocate theuse of the web as a corpus to retrieve backoffprobabilities for unseen bigrams.
Lapata and Keller(2005) extend this method to a range of additionalnatural language processing tasks, but also cautionthat web counts have limitations and add noise.Kilgariff (2007) points out the shortcomings of37accessing the web as a corpus through search que-ries: (a) there is no lemmatization or part-of-speechtagging in search indices, so a linguistically mea-ningful query can only be approximated, (b) searchsyntax, as implemented by search engine provid-ers, is limited, (c) there is often a limit on the num-ber of automatic queries that are allowed by searchengines, (c) hit count estimates are estimates ofretrieved pages, not of retrieved words.
We wouldlike to add to that list that hit count estimates onthe web are just that -- estimates.
They are com-puted on the fly by proprietary algorithms, and ap-parently the algorithms also access different slicesof the web index, which causes a fluctuation overtime, as Tetrault and Chodorow (2009) point out.In 2006, Google made its web-based 5gram lan-guage model available through the Linguistic DataConsortium, which opens the possibility of usingreal n-gram statistics derived from the web direct-ly, instead of using web search as a proxy.In this paper we explore the use of the web as acorpus for a very specific task: distinguishing be-tween a learner error and its correction.
This is ob-viously not the same as the more ambitiousquestion of whether a system can be built to detectand correct errors on the basis of web counts alone,and this is a distinction worth clarifying.
Any sys-tem that successfully detects and corrects an errorwill need to accomplish three tasks1: (1) find a partof the user input that contains an error (error de-tection).
(2) find one or multiple alternativestring(s) for the alleged error (candidate genera-tion) and (3) score the alternatives and the originalto determine which alternative (if any) is a likelycorrection (error correction).
Here, we are onlyconcerned with the third task, specifically thecomparison between the incorrect and the correctchoice.
This is an easily measured task, and is alsoa minimum requirement for any language model orlanguage model approximation: if the model can-not distinguish an error from a well-formed string,it will not be useful.1 Note that these tasks need not be addressed by separate com-ponents.
A contextual classifier for preposition choice, forexample, can generate a probability distribution over a set ofprepositions (candidate generation).
If the original prepositionchoice has lower probability than one or more other preposi-tions, it is a potential error (error detection), and the preposi-tions with higher probability will be potential corrections(error correction).We focus on two prominent learner errors in thisstudy: preposition inclusion and choice and articleinclusion and choice.
These errors are among themost frequent learner errors (they comprise nearlyone third of all errors in the learner corpus used inthis study).In this study, we compare three web datasources: The public Bing API, Google API, and theGoogle 5-gram language model.
We also pay closeattention to strategies of query formulation.
Thequestions we address are summarized as follows:Can web data be used to distinguish learner er-rors from correct phrases?What is the better resource for web-data: theBing API, the Google API, or the Google 5-gram data?What is the best query formulation strategywhen using web search results for this task?How much context should be included in thequery?2 Related WorkHermet et al (2008) use web search hit counts forpreposition error detection and correction inFrench.
They use a set of confusable prepositionsto create a candidate set of alternative prepositionalchoices and generate queries for each of the candi-dates and the original.
The queries are producedusing linguistic analysis to identify both a govern-ing and a governed element as a minimum mea-ningful context.
On a small test set of 133sentences, they report accuracy of 69.9% using theYahoo!
search engine.Yi et al (2008) target article use and collocationerrors with a similar approach.
Their system firstanalyzes the input sentence using part-of-speechtagging and a chunk parser.
Based on this analysis,potential error locations for determiners and verb-noun collocation errors are identified.
Query gen-eration is performed at three levels of granularity:the sentence (or clause) level, chunk level andword level.
Queries, in this approach, are not exactstring searches but rather a set of strings combinedwith the chunk containing the potential errorthrough a boolean operator.
An example for achunk level query for the sentence "I am learningeconomics at university" would be "[economics]AND [at university] AND [learning]".
For article38errors the hit count estimates (normalized for querylength) are used directly.
If the ratio of the norma-lized hit count estimate for the alternative articlechoice to the normalized hit count estimate of theoriginal choice exceeds a manually determinedthreshold, the alternative is suggested as a correc-tion.
For verb-noun collocations, the situation ismore complex since the system does not automati-cally generate possible alternative choices fornoun/verb collocations.
Instead, the snippets (doc-ument summaries) that are returned by the initialweb search are analyzed and potential alternativecollocation candidates are identified.
They thensubmit a second round of queries to determinewhether the suggestions are more frequent than theoriginal collocation.
Results on a 400+ sentencecorpus of learner writing show 62% precision and41% recall for determiners, and 30.7% recall and37.3% precision for verb-noun collocation errors.Tetreault and Chodorow (2009) make use of theweb in a different way.
Instead of using global webcount estimates, they issue queries with a region-specific restriction and compare statistics acrossregions.
The idea behind this approach is that re-gions that have a higher density of non-nativespeakers will show significantly higher frequencyof erroneous productions than regions with a high-er proportion of native speakers.
For example, theverb-preposition combinations married to versusmarried with show very different counts in the UKversus France regions.
The ratio of counts for mar-ried to/married with in the UK is 3.28, whereas itis 1.18 in France.
This indicates that there is signif-icant over-use of married with among nativeFrench speakers, which serves as evidence that thisverb-preposition combination is likely to be an er-ror predominant for French learners of English.They test their approach on a list of known verb-preposition errors.
They also argue that, in a state-of-the-art preposition error detection system, recallon the verb-preposition errors under investigationis still so low that systems can only benefit fromincreased sensitivity to the error patterns that arediscoverable through the region web estimates.Bergsma et al(2009) are the closest to our work.They use the Google N-gram corpus to disambi-guate usage of 34 prepositions in the New YorkTimes portion of the Gigaword corpus.
They use asliding window of n-grams (n ranging from 2 to 5)across the preposition and collect counts for allresulting n-grams.
They use two different methodsto combine these counts.
Their SuperLM modelcombines the counts as features in a linear SVMclassifier, trained on a subset of the data.
TheirSumLM model is simpler, it sums all log countsacross the n-grams.
The preposition with the high-est score is then predicted for the given context.Accuracy on the New York Times data in these ex-periments reaches 75.4% for SuperLM and 73.7%for SumLM.Our approach differs from Bergsma et al inthree crucial respects.
First, we evaluate insertion,deletion, and substitution operations, not just subs-titution, and we extend our evaluation to articleerrors.
Second, we focus on finding the best querymechanism for each of these operations, whichrequires only a single query to the Web source.Finally, the focus of our work is on learner errordetection, so we evaluate on real learner data asopposed to well-formed news text.
This distinctionis important: in our context, evaluation on editedtext artificially inflates both precision and recallbecause the context surrounding the potential errorsite is error-free whereas learner writing can be,and often is, surrounded by errors.
In addition,New York Times writing is highly idiomatic whilelearner productions often include unidiomatic wordchoices, even though the choice may not be consi-dered an error.3 Experimental Setup3.1 Test DataOur test data is extracted from the Cambridge Uni-versity Press Learners?
Corpus (CLC).
Our ver-sion of CLC currently contains 20 million wordsfrom non-native English essays written as part ofone of Cambridge?s English language proficiencytests (ESOL) ?
at all proficiency levels.
The essaysare annotated for error type, erroneous span andsuggested correction.
We perform a number ofpreprocessing steps on the data.
First, we correctall errors that were flagged as being spelling errors.Spelling errors that were flagged as morphologyerrors were left alone.
We also changed confusablewords that are covered by MS Word.
In addition,we changed British English spelling to AmericanEnglish.
We then eliminate all annotations for non-pertinent errors (i.e.
non-preposition/article errors,or errors that do not involve any of the targetedprepositions), but we retain the original (errone-39ous) text for these.
This makes our task hardersince we will have to make predictions in text con-taining multiple errors, but it is more realistic giv-en real learner writing.
Finally, we eliminatesentences containing nested errors (where the an-notation of one error contains an annotation foranother error) and multiple article/preposition er-rors.
Sentences that were flagged for a replacementerror but contained no replacement were also elim-inated from the data.
The final set we use consistsof a random selection of 9,006 sentences from theCLC with article errors and 9,235 sentences withpreposition errors.3.2 Search APIs and CorporaWe examine three different sources of data to dis-tinguish learner errors from corrected errors.
First,we use two web search engine APIs, Bing andGoogle.
Both APIs allow the retrieval of a page-count estimate for an exact match query.
Sincethese estimates are provided based on proprietaryalgorithms, we have to treat them as a "black box".The third source of data is the Google 5-gram cor-pus (Linguistic Data Consortium 2006) which con-tains n-grams with n ranging from 1 to 5.
Thecount cutoff for unigrams is 200, for higher ordern-grams it is 40.3.3 Query FormulationThere are many possible ways to formulate an ex-act match (i.e.
quoted) query for an error and itscorrection, depending on the amount of contextthat is included on the right and left side of the er-ror.
Including too little context runs the risk ofmissing the linguistically relevant information fordetermining the proper choice of preposition ordeterminer.
Consider, for example, the sentence werely most of/on friends.
If we only include oneword to the left and one word to the right of thepreposition, we end up with the queries "most onfriends" and "most of friends" - and the web hitcount estimate may tell us that the latter is morefrequent than the former.
However, in this exam-ple, the verb rely determines the choice of preposi-tion and when it is included in the query as in "relymost on friends" versus "rely most of friends", theestimated hit counts might correctly reflect the in-correct versus correct choice of preposition.
Ex-tending the query to cover too much of the context,on the other hand, can lead to low or zero web hitestimates because of data sparseness - if we in-clude the pronoun we in the query as in "we relymost on friends" versus "we rely most of friends",we get zero web count estimates for both queries.Another issue in query formulation is whatstrategy to use for corrections that involve dele-tions and insertions, where the number of tokenschanges.
If, for example, we use queries of length3, the question for deletion queries is whether weuse two words to the left and one to the right of thedeleted word, or one word to the left and two to theright.
In other words, in the sentence we traveledto/0 abroad last year, should the query for the cor-rection (deletion) be "we traveled abroad" or "tra-veled abroad last"?Finally, we can employ some linguistic informa-tion to design our query.
By using part-of-speechtag information, we can develop heuristics to in-clude a governing content word to the left and thehead of the noun phrase to the right.The complete list of query strategies that wetested is given below.SmartQuery: using part-of-speech informationto include the first content word to the left and thehead noun to the right.
If the content word on theleft cannot be established within a window of 2tokens and the noun phrase edge within 5 tokens,select a fixed window of 2 tokens to the left and 2tokens to the right.FixedWindow Queries: include n tokens to theleft and m tokens to the right.
We experimentedwith the following settings for n and m: 1_1, 2_1,1_2, 2_2, 3_2, 2_3.
The latter two 6-grams wereonly used for the API?s, because the Google corpusdoes not contain 6-grams.FixedLength Queries: queries where the lengthin tokens is identical for the error and the correc-tion.
For substitution errors, these are the same asthe corresponding FixedWindow queries, but forsubstitutions and deletions we either favor the leftor right context to include one additional token tomake up for the deleted/inserted token.
We expe-rimented with trigrams, 4-grams, 5-grams and 6-grams, with left and right preference for each, theyare referred to as Left4g (4-gram with left prefe-rence), etc.403.4 Evaluation MetricsFor each query pair <qerror, qcorrection>, we produceone of three different outcomes:correct (the query results favor the correction ofthe learner error over the error itself):count(qcorrection) > count(qerror)incorrect (the query results favor the learner errorover its correction):count(qerror) >= count(qcorrection)where(count(qerror) &  0 ORcount(qcorrection) &  0)noresult:count(qcorrection) = count(qerror) = 0For each query type, each error (preposition or ar-ticle), each correction operation (deletion, inser-tion, substitution) and each web resource (BingAPI, Google API, Google N-grams) we collectthese counts and use them to calculate three differ-ent metrics.
Raw accuracy is the ratio of correctpredictions to all query pairs:!
"#$"%%&'"%( ) $%*''%*'' + ,-%*'' + -*'./&01We also calculate accuracy for the subset of querypairs where at least one of the queries resulted in asuccessful hit, i.e.
a non-zero result.
We call thismetric Non-Zero-Result-Accurracy (NZRA), it isthe ratio of correct predictions to incorrect predic-tions, ignoring noresults:2*-3.
'*!./&014%%&'"%( ) $%*''%*'' + ,-%*''Finally, retrieval ratio is the ratio of queries thatreturned non-zero results:4 ResultsWe show results from our experiments in Table 1 -Table 6.
Since space does not permit a full tabula-tion of all the individual results, we restrict our-selves to listing only those query types that achievebest results (highlighted) in at least one metric.Google 5-grams show significantly better resultsthan both the Google and Bing APIs.
This is goodnews in terms of implementation, because it freesthe system from the vagaries involved in relying onsearch engine page estimates: (1) the latency, (2)query quotas, and (3) fluctuations of page esti-mates over time.
The bad news is that the 5-gramcorpus has much lower retrieval ratio because, pre-sumably, of its frequency cutoff.
Its use also limitsthe maximum length of a query to a 5-gram (al-though neither of the APIs outperformed Google 5-grams when retrieving 6-gram queries).The results for substitutions are best, for fixedwindow queries.
For prepositions, the SmartQue-ries perform with about 86% NZRA while a fixedlength 2_2 query (targeted word with a ?2-tokenwindow) achieves the best results for articles, atabout 85% (when there was at least one non-zeromatch).
Retrieval ratio for the prepositions wasabout 6% lower than retrieval ratio for articles ?41% compared to 35%.The best query type for insertions was fixed-length LeftFourgrams with about 95% NZRA and71% retrieval ratio for articles and 89% and 78%retrieval ratio for prepositions.
However, Left-Fourgrams favor the suggested rewrites because,by keeping the query length at four tokens, theoriginal has more syntactic/semantic context.
If theoriginal sentence contains is referred as the and theannotator inserted to before as, the original querywill be is referred as the and the correction queryis referred to as.Conversely, with deletion, having a fixed win-dow favors the shorter rewrite string.
The bestquery types for deletions were: 2_2 queries for ar-ticles (94% NZRA and 46% retrieval ratio) andSmartQueries for prepositions (97% NZRA and52% retrieval ratio).
For prepositions the fixedlength 1_1 query performs about the same as theSmartQueries, but that query is a trigram (orsmaller at the edges of a sentence) whereas the av-erage length of SmartQueries is 4.7 words for pre-positions and 4.3 words for articles.
So while thecoverage for SmartQueries is much lower, thelonger query string cuts the risk of matching onfalse positives.The Google 5-gram Corpus differs from searchengines in that it is sensitive to upper and lowercase distinctions and to punctuation.
While intui-tively it seemed that punctuation would hurt n-gram performance, it actually helps because thepunctuation is an indicator of a clause boundary.
Arecent Google search for have a lunch and havelunch produced estimates of about 14 million webpages for the former and only 2 million for the lat-ter.
Upon inspecting the snippets for have a lunch,the next word was almost always a noun such asmenu, break, date, hour, meeting, partner, etc.
Therelative frequencies for have a lunch would bemuch different if a clause boundary marker were41required.
The 5-gram corpus also has sentenceboundary markers which is especially helpful toidentify changes at the beginning of a sentence.Query typenon-zero-result accuracy retrieval ratio raw accuracyB-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-NgrSmartQuery 0.8637 0.9548 0.9742 0.8787 0.8562 0.5206 0.7589 0.8176 0.50711_1 0.4099 0.9655 0.9721 0.9986 0.9978 0.9756 0.4093 0.9634 0.9484Table 1: Preposition deletions (1395 query pairs).Query typenon-zero-result accuracy retrieval ratio raw accuracyB-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-NgrLeft4g 0.7459 0.8454 0.8853 0.9624 0.9520 0.7817 0.7178 0.8048 0.69201_1 0.5679 0.2983 0.3550 0.9973 0.9964 0.9733 0.5661 0.2971 0.3456Right3g 0.6431 0.8197 0.8586 0.9950 0.9946 0.9452 0.6399 0.8152 0.8116Table 2: Preposition insertions (2208 query pairs).Query typenon-zero-result accuracy retrieval ratio raw accuracyB-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-NgrSmartQuery 0.7396 0.8183 0.8633 0.7987 0.7878 0.4108 0.5906 0.6446 0.50711_1=L3g=R3g 0.4889 0.6557 0.6638 0.9870 0.9856 0.9041 0.4826 0.6463 0.60011_2=R4g 0.6558 0.7651 0.8042 0.9178 0.9047 0.6383 0.6019 0.6921 0.5133Table 3: Preposition substitutions (5632 query pairs).Query typenon-zero-result accuracy retrieval ratio raw accuracyB-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr2_2 0.7678 0.9056 0.9386 0.8353 0.8108 0.4644 0.6414 0.7342 0.43591_1 0.3850 0.8348 0.8620 0.9942 0.9924 0.9606 0.3828 0.8285 0.82811_2 0.5737 0.8965 0.9097 0.9556 0.9494 0.7920 0.5482 0.8512 0.7205Table 4: Article deletions (2769 query pairs).Query typenon-zero-result accuracy retrieval ratio raw accuracyB-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-NgrLeft4g 0.8292 0.9083 0.9460 0.9505 0.9428 0.7072 0.7880 0.8562 0.66901_1 0.5791 0.3938 0.3908 0.9978 0.9975 0.9609 0.5777 0.3928 0.3755Left3g 0.6642 0.8983 0.8924 0.9953 0.9955 0.9413 0.6611 0.8942 0.8400Table 5: Article insertions (5520 query pairs).Query typenon-zero-result accuracy retrieval ratio raw accuracyB-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr2_2=Left5g=Right5g 0.6970 0.7842 0.8486 0.8285 0.8145 0.4421 0.5774 0.6388 0.37521_1=L3g=R3g 0.4385 0.7063 0.7297 0.9986 0.9972 0.9596 0.4379 0.7043 0.70011_2=R4g 0.5268 0.7493 0.7917 0.9637 0.9568 0.8033 0.5077 0.7169 0.6360Table 6: Article substitutions (717 query pairs).425 Error AnalysisWe manually inspected examples where thematches on the original string were greater thanmatches on the corrected string.
The results of thiserror analysis are shown in table 7.
Most of thetime, (1) the context that determined article or pre-position use and choice was not contained withinthe query.
This includes, for articles, cases wherearticle usage depends either on a previous mentionor on the intended sense of a polysemous headnoun.
Some other patterns also emerged.
Some-times (2) both and the original and the correctionseemed equally good in the context of the entiresentence, for example it?s very important to us andit?s very important for us.
In other cases, (3) therewas another error in the query string (recall that weretained all of the errors in the original sentencesthat were not the targeted error).
Then there is avery subjective category (4) where the relative n-gram frequencies are unexpected, for examplewhere the corpus has 171 trigrams guilty for youbut only 137 for guilty about you.
These often oc-cur when both of the frequencies are either lowand/or close.
This category includes cases where itis very likely that one of the queries is retrieving ann-gram whose right edge is the beginning of acompound noun (as in with the trigram have alunch).
Finally, (5) some of the ?corrections?
eitherintroduced an error into the sentence or the originaland ?correction?
were equally bad.
In this catego-ry, we also include British English article usagelike go to hospital.
For prepositions, (6) some ofthe corrections changed the meaning of the sen-tence ?
where the disambiguation context is oftennot in the sentence itself and either choice is syn-tactically correct, as in I will buy it from youchanged to I will buy it for you.Articles Prepsfreq ratio freq ratio1.N-gram does not con-tain necessary context 187 .58 183 .522.Original and correc-tion both good 39 .12 51 .113.Other error in n-gram 30 .9 35 .104.Unexpected ratio 36 .11 27 .095.Correction is wrong 30 .9 30 .086.Meaning changing na na 24 .07Table 7: Error analysisIf we count categories 2 and 5 in Table 7 as notbeing errors, then the error rate for articles drops20% and the error rate for prepositions drops 19%.A disproportionately high subcategory of querystrings that did not contain the disambiguating con-text (category 1) was at the edges of the sentence ?especially for the LeftFourgrams at the beginningof a sentence where the query will always be a bi-gram.6 Conclusion and Future WorkWe have demonstrated that web source counts canbe an accurate predictor for distinguishing betweena learner error and its correction - as long as thequery strategy is tuned towards the error type.Longer queries, i.e.
4-grams and 5-grams achievethe best non-zero-result accuracy for articles, whileSmartQueries perform best for preposition errors.Google N-grams across the board achieve the bestnon-zero-result accuracy, but not surprisingly theyhave the lowest retrieval ratio due to count cutoffs.Between the two search APIs, Bing tends to havebetter retrieval ratio, while Google achieves higheraccuracy.In terms of practical use in an error detectionsystem, a general "recipe" for a high precisioncomponent can be summarized as follows.
First,use the Google Web 5-gram Corpus as a websource.
It achieves the highest NZRA, and it avoidsmultiple problems with search APIs: results do notfluctuate over time, results are real n-gram countsas opposed to document count estimates, and a lo-cal implementation can avoid the high latency as-sociated with search APIs.
Secondly, carefullyselect the query strategy depending on the correc-tion operation and error type.We hope that this empirical investigation cancontribute to a more solid foundation for futurework in error detection and correction involvingthe web as a source for data.
While it is certainlynot sufficient to use only web data for this purpose,we believe that the accuracy numbers reported hereindicate that web data can provide a strong addi-tional signal in a system that combines differentdetection and correction mechanisms.
One can im-agine, for example, multiple ways to combine then-gram data with an existing language model.
Al-ternatively, one could follow Bergsma et al (2009)and issue not just a single pair of queries but a43whole series of queries and sum over the results.This would increase recall since at least some ofthe shorter queries are likely to return non-zeroresults.
In a real-time system, however, issuingseveral dozen queries per potential error locationand potential correction could cause performanceissues.
Finally, the n-gram counts can be incorpo-rated as one of the features into a system such asthe one described in Gamon (2010) that combinesevidence from various sources in a principled wayto optimize accuracy on learner errors.AcknowledgmentsWe would like to thank Yizheng Cai for makingthe Google web ngram counts available through aweb service and to the anonymous reviewers fortheir feedback.ReferencesEric Steven Atwell.
1987.
How to detect grammaticalerrors in a text without parsing it.
Proceedings of the3rd EACL, Copenhagen, Denmark, pp 38 - 45.Michele Banko and Eric Brill.
2001a.
Mitigating thepaucity-of-data problem: Exploring the effect oftraining corpus size on classifier performance fornatural language processing.
In James Allan, editor,Proceedings of the First International Conference onHuman Language Technology Research.
MorganKaufmann, San Francisco.Michele Banko and Eric Brill.
2001b.
Scaling to veryvery large corpora for natural language disambigua-tion.
In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics andthe 10th Conference of the European Chapter of theAssociation for Computational Linguistics, pp.
26?33, Toulouse, France.Shane Bergsma, Dekang Lin, and Randy Goebel.
2009.Web-scale n-gram models for lexical disambiguation.In Proceedings for the 21st International Joint Confe-rence on Artificial Intelligence, pp.
1507 ?
1512.Martin Chodorow, Joel Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involving pre-positions.
In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions, pp.
25-30.Rachele De Felice and Stephen G. Pulman.
2007.
Au-tomatically acquiring models of preposition use.
InProceedings of the Fourth ACL-SIGSEM Workshopon Prepositions, pp.
45-50.
Prague.Rachele De Felice and Stephen Pulman.
2008.
A clas-sifier-based approach to preposition and determinererror correction in L2 English.
COLING.
Manches-ter, UK.Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-der Klementiev, William Dolan, Dmitriy Belenko,and Lucy Vanderwende.
2008.
Using contextualspeller techniques and language modeling for ESLerror correction.
In Proceedings of IJCNLP, Hydera-bad, India.Michael Gamon.
2010.
Using mostly native data to cor-rect errors in learners?
writing.
In Proceedings ofNAACL.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineering,12(2), 115-129.Matthieu Hermet, Alain D?silets, Stan Szpakowicz.2008.
Using the web as a linguistic resource to auto-matically correct lexico-syntactic errors.
In Proceed-ings of the 6th Conference on Language Resourcesand Evaluation (LREC), pp.
874-878.Frank Keller and Mirella Lapata.
2003.
Using the webto obtain frequencies for unseen bigrams.
Computa-tional Linguistics, 29(3): 459-484.Adam Kilgariff.
2007.
Googleology is bad science.Computational Linguistics 33(1): 147-151.Mirella Lapata and Frank Keller.
2005.
Web-BasedModels for Natural Language Processing.
ACMTransactions on Speech and Language Processing(TSLP), 2(1):1-31.Linguistic Data Consortium.
2006.
Web 1T 5-gramversion 1.http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13 .Joel Tetreault and Martin Chodorow.
2008.
The ups anddowns of preposition error detection in ESL.COLING.
Manchester, UK.Joel Tetreault and Martin Chodorow.
2009.
Examiningthe use of region web counts for ESL error detection.Web as Corpus Workshop (WAC-5), San Sebastian,Spain.Xing Yi, Jianfeng Gao and Bill Dolan.
2008.
A web-based English proofing system for English as asecond language users.
In Proceedings of the ThirdInternational Joint Conference on Natural LanguageProcessing (IJCNLP).
Hyderabad, India.Zhu, X. and Rosenfeld, R. 2001.
Improving trigramlanguage modeling with the world wide web.
In Pro-ceedings of International Conference on AcousticsSpeech and Signal Processing.
Salt Lake City.44
