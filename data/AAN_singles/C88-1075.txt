Parsing Incomplete SentencesBernard LANGINRIAB.P.
105, 78153 Le Chesnay, Francelang@inria.inria, frAbstractAn efficient context-free parsing algorithm ispresentedthat can parse sentences with unknown parts of unknownlength.
It pa'oduees in finite form all possible parses (of-ten infinite in number) that could account for the missingparts.
The algorithm is a variation oa the constructiondue to Earley.
ltowever, its presentation is such that itcan readily be adapted to any chart parsing schema (top-down, bottom-up, etc...).1 In t roduct ionIt is often necessary in practical situations to attempt parsing anincorrect or incomplete input.
This may take many forms: e.g.missing or spurious words, misspelled or misunderstood or oth-erwise unknown words \[28\], missing or unidentified word bound-aries \[22,27\].
Specific techniques may be developed to deal withthese situations according to the requirements of the applicationarcs (e.g.
n~tural language processing, progrmrmfing languageparsing, tea:i-time or off-line processing).The con~lext-fi.ee (CF) parsing of a sentence with unknownwords hss been considered by other authors \[28\].
Very simply,an unknown word may be considered as a "special multi-part-of-speech word whose pa'ct of speech can be anything".
This multi-psi't-of-speech word need not be introduced in the CF grammarof the lang0age, but only implicitly in the construction of itsparser.
Thi;~ works very well with Earley-like (chart) parsersthat can simulate all possible parsing paths that could lead toa correct parse.In this paper, we deal with the more complex problem ofparsing a ser*.tence for wtfich one or several subparts of unknownlength are roissing.
Again we can use a chart parser to try allpossible parses on all possible inputs.
However the fact that thelength of th~ 1*fissing subsequence is unknown raises an addi-tional difficulty.
Many published chart parsers \[24,28,23,21\] areconstructed ~,ith the assumption that tim CF grammar of thelanguage ho~', no cyclic rules.
Tlfis hypothesis is reasonable forthe syntax ol natural (or programming) languages.
However theresulting simplification of the pm'ser construction does not allowits extension to parsing sentences with unknown subsequeneesof words.If the length (in words) of the missing subsequence wereknown, we could simply replace it with as many unknown words,a problem we know how to handle.
When this length is notknown, the tdgorithm has to simulate the parsing of an arbi-trary numbe~: of words, and thus may have to go several t im~tht'ough reduction by the same rules of the grammar 1 withoutever' touchinl; the stack present before scanning the unknownt~equenee, aml without reading the input beyond that sequence.If we consider the unknown sequence as a special input word,wc are in a situation that is analogous to that created by cyclicgrammars, i.~.
g~amrnars where a nonterminal may derive ontoIThis grammar oriented view of the computation of the autonmton isonly meant as a support for intuition.itself without producing any terminal.
This explains why tech-niques limited to non-cyclic grammars cannot deal with thisproblem.It may be noted that the problem is different fi'om that ofparsing in a word lattice \[22,27\] since all possible path in thelattice have a known bounded length, even when the latticecontains eparated unknown words, tIowever the technique pre-sented here combines well with word lattice parsing.The ability to parse unknown subsequences may be ~sefulto parse badly transmitted sentences, and sentences that arcinterrupted (e.g.
in a discussion) or otherwise left unfinished(e.g.
because the rest may be inferred from the context).
Itmay also be used in programming languages: for example theprogramming language SETL \[9\] allows some statements o beleft unfinished in some contexts.The next section contains an introduction to all-paths pars-ing.
In section 3 we give a more detailed account of our basicalgorithm and point at the features that allow the handlingof cyclic grammars.
Section 4 contains the modifications thatmake this algorltlml capable of parsing incomplete sentences.The fifll algorithm is given in appendix C, while two examplesare given in appendices A and B.2 All-Paths ParsingSince Earley's first paper \[10\], many adaptations or improve-ments of his ~flgorithm have been published \[6,5,24,28\].
Theyare usually variations following some chart parsing schema \[16\].In a previous paper \[18\], the author attempted to unify all theseresults by proposing an Earley-like construction for all-paths in-terpretation of (non-deterministic) Push-Down-Transducers(PDT).
The idea was that left-to-right parsing schemata mayusually be expressed as a construction technique for building arecognizing Push-Down-Automaton (PDA) from the CF gram-mar of the language.
This is quite apparent when comparingthe PDA constructions in \[12\] to the ctmrt sche,nata of \[16\]which are now a widely accepted reference.
Thns a construc-tion proposed for general PDTs is de facto applicable to mostleft-to-right parsing schemata, and allows in particular the useof well established PDT construction teclmiques (e.g.
prece-dence, LL(k), LR(k) \[8,14,2\]) for general CF parsing.In this earlier paper, our basic algorithm is proved correct,and its complexity is shown to be O(n3), i.e.
as good as thebest general parsing algorithms 2.
As is usual with Earley'sconstruction 3, the theoretical complexity bound is rarely at-tained, and the algorithm behaves linearly most of the time.Further optimizations are proposed in \[18\] that improve thisbehavior.Most published variants of Earley's algorithm, including Ear-ley's own, may be viewed as (a sometimes weaker form of) ourconstruction applied to some specific PDA or PDT.
This is the~Theoretically faster algorithms \[29,7\] can achieve O(n ~'4~6) but with anunacceptable constant fi~ctor.
Note also that we do not require the grammarto be in Chomsky Normal Form.SAnd unlike tabular algorithms such as Cocke-Younger-Kasami's \[13,15,30,11\].365explicit strategy of Tomita \[28\] in the special case of LALR(1)PDT construction technique.
A notable exception is the verygeneral approach of Shell \[25\], though it is very similar to aDatalog extension \[19\] of the algorithm presented here.An essential feature of all-paths parsing algorithms is to beable to produce all possible parses in a concise form, with asmuch sharing as possible of the common subparses.
This isrealized in many systems \[6,24,28\] by producing some kind ofshared-forest which is a representation f all parse-trees withvarious sharings of common subparts.
In the case of our al-gorithm, a parse is represented by the sequence of rules to beused in a left-to-right reduction of the input sentence to theinitial nonterminal of the gramnmr.
Sharing between all pos-sible parses is achieved by producing, instead of an extension-ally given set of possible parse sequences, a new CF grammarthat generates all possible parse sequences (possibly an infinitenumber if the grammar of the input language is cyclic, and ifthe parsed sentence is infinitely ambiguous).
With appropri-ate care, it is also possible to read this ontput grammar as ashared-forest (see appendix A).
However its meaningful inter-pretation as a shared-forest is dependent on the parsing schema(cf.
\[12,16\]) used in constructing the PDT that produces it asoutput.
Good definition and understanding of shared forestsis essential to properly define and handle the extra processingneeded to disambiguate a sentence, in the usual case when theambiguous CF grammar is uscd only as a parsing backbone\[24,26\].
The structure of shared forests is discussed in \[4\].Before and while following the next section, we suggest hatthe reader looks at Appendix A which contains a detailed exam-ple showing an output grammar and the corresponding sharedforest for a slightly ambiguous input sentence.3 The Basic Algor ithmA formal definition of the extended algorithm for possibly in-complete sentences i given in appendix C. The formal aspectof our presentation of the algorithm is justified by the fact thatit allows specialization of the given constructions to specificparsing schema without loss of the correctness and complex-ity properties, as well as the specialization of the optimizationtechniques ( ee \[18\]) established in the general case.
The exam-ples presented later were obtained with an adaptation of thisgeneral algorithm to bottom-up LALR(1) parsers \[8\].Our aim is to parse sentences in the language / :(G) gen-erated by a CF phrase structure grammar G = (V,\]E, YI,~)according to its syntax.
The notation used is V for the set ofnonterminal, \]E for the set of terminals, YI for the rules, andfor the initial nonterminal.We assume that, by some appropriate parser constructiontechnique (e.g.
\[14,8,2,1\]) we mechanically produce from thegrammar G a parser for the language ?
:(G) in the form of a(possibly non-deterministic) push-down transducer (PDT) TG.The output of each possible computation of the parser is a se-quence of rules in H a to be used in a left-to-right reduction ofthe input sentence (this is obviously equivalent to producing aparse-tree).We assume for the PDT 7G a very general formal defini-tion that can fit most usual PDT construction techniques.
Ito ois defined as an 8-tuple T(~ = (q ,  E, A, II, 6, q, $, F)  where: Qis the set of states, \]E is the set of input word symbols, & is theset of stack symbols, I I  is the set of output symbols (i.e.
rulesof G), ~l is the initial state, $ is the initial stack symbol, Fis the set of final states, 6 is a finite set of transitions of the4Implementations usually denote these rules by their index in the set II.form: (pAa~--~ qBu)  with p, qEq ,  A,BEZXU{E&},aE~U{e~) ,and  uE I I * .Let the PDT be in a configuration p = (p Aa a~ u) where pis the current state, Aa is the stack contents with h on the top,ax is the remaining input where the symbol a is the next to beshifted and x E ~E*, and u is the already produced output.
Theapplication of a transition r = (p A a ~ q B v) results in a newconfiguration p' = (q Ba x uv) where the terminal symbol a hasbeen scanned (i.e.
shifted), A has been popped and n has beenpushed, and v has been concatenated to the existing output u.If the terminal symbol a is replaced by e:~ in the transition, noinput symbol is scanned.
If A (resp.
B) is replaced by e~ thenno stack symbol is popped from (resp.
pushed on) the stack.Our algorithm consists in an Earley-like 5 simulation of thePDT TG.
Using the terminology of \[2\], the algorithm buildsan item set Si successively for each word symbol xi holdingposition i in the input sentence x.
An item is constituted of twomodes of the form (p A i) where p is a PDT state, A is a stacksymbol, and i is the index of an input symbol.
The item setSi contains items of the form ((p A i) (q B j))  .
These items areused as nonterminals of a grammar ~ = (S, I I ,  P, Uf), where 6'is the set of all items (i.e.
the union of St), and the rules inare constructed together with their left-hand-side item by thealgorithm.
The initial nonterminal Uf of ~ derives on the lastitems produced by a successful computation.The meaning of an item U = ((p A i) (q n j))  is the following:?
there are computations of the PDT on the given inputsentence that reach a configuration pt where the state isp, the stack top is A and the last symbol scanned is xi;?
the next stack symbol is then B and, for all these compu-tations, it was last on top in a configuration p where thestate was q and the last symbol scanned was xj;?
the rule sequences in l-I* derivable from U in the grammarare exactly those sequences output by the above definedcomput~:tions of the PDT between configurations p and p~.In simpler words, an item may be understood as a set ofdistinguished fl'agments of the possible PDT computations, thatare independent of the initial content of the stack, except for itstop element.
Item structures are used to share these fragmentsbetween all PDT computations that can use them, so as toavoid duplication of work.
In the output grammar an item isa nonterminal that may derive on the outputs produced by thecorresponding computation fragments.The items may also be read as an encoding of the possibleconfigurations that could be attained by the PDT on the giveninput, with sharing of common stack fragments (the same frag-ment may be reused several times for the same stack in the caseof cyclic grammars, or incomplete sentences).
In figure 1 werepresent a partial collection of items.
Each item is representedby its two modes as (Kh Kh,) without giving the internal struc-ture of modes as a triples (PDT-state ?
stack-symbol ?
input-index).
Each mode Kh actually stands for the triple (pa A h ih).We have added arrows from the second component of every item(Kh Kh,) to the first component of any item (Ku Kh,,).
Thischaining indicates in reverse the order in which the correspond-ing modes are encountered during a possible computation of thePDT.
In particular, the sequence of stack symbols of the firstmodes of the items in any such chain is a possible stack con-tent.
Ignoring the output, an item (Kh K^,) represent the setof PDT configurations where the current state is p~,, the nextinput symbol to be read has the index ih + 1, and the stack con-tent is formed of all the stack symbols to be found in the firstmode of all items of any chain of items beginning with (Kh Kh,).Hence, if the collection of items of figure 1 is produced by adynamic programming computation, it means that a standardnon-deterministic computation of the PDT could have reached5We assume the reader to be familiar with some variation of Earley'salgorithm.
Earley's original paper uses the word s~ate instead of i~em.366Figure 1: f~ems as shared representations of tack eozffigurationsstate I)1, having last read the input symbol of index il, andhaving buitt any of tile following stack configurations (amongothers), with tim stack top on the left hand side: A1A2As...,A1A2A3A7 .
., A1A2AaAfA6.
.
.
,  A1A2AsAsAs .
.
.
,  A1A2A4AaAbAs .
.
.
,A1A2A4AbAs.
.
.
,  and  so on.The transitions of tlm PDT are interpreted to produce newitems, and new associated rules in 5 ?
for the output grammar ~,as described in appendix C. When the same item is producedseveral times, only one copy is kept in the item set, but a newrule is produced each time.
This merging of identical itemsaccounts for the sharing of identical subeomputations.
The cot-responding rules with stone left-hand-side (i.e.
the multiply produeed item) account for santo of the sharing in the output (of.appendices A & B).
Sharing in the output also appears in theuse of the :,ame item in the right hand side of sevcral differentoutput rules.
This directly results from the non-determinism ofthe PDT computation, i.e.
the ambiguity of the input sentence.The critical feature of the algorithm for handling cyclic rules(i.e.
infinite ambiguity) is to be found in the handling of pap-ping transitions 6.
When applying a popping transition r =(p A eI:i ~ r e~.
z) to the item C = ((p A i) (q la j)) the alga-rithm mu,*t find all items Y = ( (q ,  j ) ( s  D k)), i.e.
all itemswith first mode (q B j), produced and build for each of then,a new itera V = ((r Jl i) (s D k)) together with the output rule(V -~ YUz) to be added to 70.
The subtle point is that theY-items must be all items with (q B j) as first mode, includingthose that, when j = i, may be built later in the computation(e.g.
because their existence depends on some other V-itembuilt in that step).4 Parsing Incomplete SentencesIn order to handle incomplete sentences, we extend the inputvocabulary with 2 symbols: "?"
standing for one unknown wordsymbol, and "*" standing for an unknown sequence of inputword symbols ~.Normally a scanning transition, say (p e a ~ r e z), is ap-plicable to ~tx~ item, say U = ((p A i) (q B j)) in ,-qi, only whena == xi+l, wlmre xi+, is the next input symbol to be shifted.
Itproduces a ,law item in 5:1+1 and a new rule in 7 ?, respectivelyV ~-: ((rA i+ l ) (q l l j ) )  and (V-+ Uz) for the above transitionand item.When the next input symbol to be shifted is xi+l = ?
(i.e.
theunknown input word symbol), then any scanning transition may6Popping transitions are also the critical place to look at for ensuringO(n a) worst ease complexity.7Several djacent "*" are equivalent toa single one.be applied as above independently of the input symbol requiredby the transition (provided that the transition is applicable withrespect o PDT state and stack symbol).When the next input symbol to be shifted is x~+l = * (i.e.
theunknowlt input subsequence), then the algorithm proceeds asfor the unknown word, except hat the new item V is created initem set 8~ instead of b'i+l, i.e.
V = ((r A i) (q B j)) in the caseof the abow; example.
Thus, in the presence of the unknownsymbol subsequence *, scanning transitions may be applied anynumber of times to the same computation thread, without shift-ing the input stream s .Scanning transitions are also used normally on input sym-bol xi+2 so as to produce also itetns in ,S~+:, for example theitem ((r A i+2) (q B j)), assuming a =-- xi+~ in the case of theabove example 9.
This is how computation proceeds beyond thel t l l known subscquenee.There is a remaining difficulty due to tile fact that it may behard to relate a parse sequence of rules in II to the input sen-tence because of the unknown mnber of input symbol actuallyassumed for all occm'rence of the unknown input subsequence.We solve this difficulty by including tile input word symbols intheir propel" place in parse sequences, which can thus be readas postfix polish encodings of tile parse tree.
In such a parsesequence, the symbol * is included a number of times equal tothe assumed length of the corresponding unknown input subse-qucnce(s) for that parse (cf.
appendix B).A last point concerns implification of the resulting ram-mar (~, or equivalently of the corresponding shared-parse-forest.In practice an unknown subseque, nce may stand for an arbi-trarily complex sequence of input word symbols, with a co lrcspondingly complex pars(" structure.
Since the subsequenceis unknown anyway, its hypothetical structures (:all be summa-rized by the nonterminal symbols that dominate it (thanks tocontext-fl'eeness).Hence the output parse grammar ~ produced by our algo-rithm may be simplified by replacing with the unknown subse-quence terminal *, all nonterminals (i.e.
items) that deri,e onlyon (occurrences of) this symbol.
However, to keep the outputreadable, wc usually qualify these * symbols with the appro-priate nonterminal of tile parsed language grammar G. Thesubstructures thus eliminated can be retrieved by arbitrary l~eof the original CF grammar of the parsed language, whici~ thuscomplements he simplified output gramma.P ?.
An example i,~;given in appendix B.5 Conc lus ionWe have shown that Earley's construction, when correctly ac-cepting cyclic grammars, may be used to parse incomplete sen-.tences.
The generality of the construction presented allows itsadaptation to any of the classical parsing schemata \[16\], andthe use of well established parser construction techniques toachieve fficiency.
The formal setting we have chosen is to ourknowledge the only one that has ever been used to provc thecorrectness of the constructed parse forest as well as that of therecognizer itself.
~?Ve believe it to be a good framework to studySNote that in such a situation; a rule X -~ aX of the language grammarG behaves as if it were a cyclic rule X --* X, since the parsing proceedsas if it were ignoring terminal symbols.
This does not lead to an infinitecomputation since ohly a finite number (proportional toi) of distinct itemscan be built in 8~.SWe assume, only for simplicity of exposition, that * is followed by anormal input word symbol.
Note also that 8i+1 is not built.l?If the input were reduced to the unknown subsequence alone, the outputgrammar ~ would be equivalent to the original grammar 151 of the inputlanguage (up to simple transformation).
The output parse sequences wouldthen simplify into a single occurrence of the symbol * qualified by the initialnonterminal I~ of the \]augusta grammar G.367the structure of parse forests \[4\], and to develop optimizationstrategies.Recent extensions of our approach to recursive queries inDatalog \[19\] and to Horn clauses \[20\] are an indication thatthese techniques may be applied effectively to more complexgrammatical setting, including unification based grammars andlogic based semantics processing.
More generally, dynamic pro-gramming approaches such as the one presented here shouldbe a privileged way of dealing with ill-formed input, since thevariety of possible rrors is the source of even more combina-torial problems than the natural ambiguity or non-determinismalready present in many "correct" sentences.Acknowledgements: Sylvie Billot is currently studyingthe implementation technology for the algorithms described here\[3,4\].
The examples in appendices A & B were produced withher prototype implementation.
The author gratefully acknowl-edges her commitment to have this implementation runningin time, as well as numerous discussions with her, V~roniqueDonzeau-Gouge, and Anne-Marie Vercoustre.References\[1\] Aho, A.V.
; Sethi, R.; and Ullman, J.D.
1986 Com-pilers -- Principles, Techniques and Tools.
Addison-Wesley.\[2\] Aho, A.V.
; and Ullman, J.D.
1972 The Theory ofParsing, Translation and Compiling.
Prentice-Hall,Englewood Cliffs, New Jersey.\[3\] Billot, S. 1986 Analyse Syntaxique Non-D~terministe.Rapport DEA, Universit~ d'Orl~ans la Source, andINRIA, France.\[4\] Billot, S.; and Lang, B.
1988 The structure of Shared: Forests in Ambiguous Parsing.
In preparation.\[5\] Bouckaert, M.; Pirotte, A.; and Snelling, M. 1975 Ef-ficient Parsing Algorithms for General C0ntext-FreeGrammars.
Information Sciences 8(1): 1-26.\[6\] Coeke, J.; and Schwartz, J.T.
1970 ProgrammingLanguages and Their Compilers.
Courant Instituteof Mathematical Sciences, New York University, NewYork.\[7\] Coppersmith, D.; and Winograd, S. 1982 On theAsymptotic Complexity of Matrix Multiplication.SIAM Journal on Computing, 11(3): 472-492.\[8\] DeRemer, F.L.
1971 Simple LR(k) Grammars.
Com-munications ACM 14(7): 453-460.\[9\] Donzeau-Gouge, V.; Dubois, C.; Facon, P.; and JeanF.
1987 Development ofa Programming Environmentfor SETL.
ESEC'87, Proc.
of the 1 "t European Soft-ware Engineering Conference, Strasbourg (France),pp.
23-34.\[10\] Earley, J.
1970 An Efficient Context-Free Parsing Al-gorithm.
Communications ACM 13(2): 94-102.\[ l l \]?Graham, S.L.
; Harrison, M.A.
; and Ruzzo W.L.1980 An Improved Context-Free Recognizer.
ACIdTransactions on Programming Languages arid Sys-tems 2(3): 415-462.\[12\] Griffiths, I.; an(l Petrick, S. 1965 On the Relative Effi-ciencies of Context-Frec Grammar Recognizers.
Com-munications ACM 8(5): 289-300.368\[13\] Hays, D.G.
1962 Automatic Language-Data Process-ing.
In Computer Applications in the Behavioral Sci-ences, (H. Borko ed.
), Prentice-Hall, pp.
394-423.\[14\] Ichbiah, J.D.
; and Morse, S.P.
1970 A Techniquefor Generating Almost Optimal Floyd-Evans Pro-ductions for Precedence Granmaars.
CommunicationsA CM 13(8): 501-508.\[15\] Kasami, J.
1965 An E~cient Recognition and Syn-tax Analysis Algorithm for Context-Free Languages.Report of Univ.
of Hawaii, also AFCRL-65-758,Air Force Cambridge Research Laboratory, Bedford(Massachusetts), also 1966, University of Illinois Co-ordinated Science Lab.
Report, No.
R-257.\[16\] Kay, M. 1980 Algorithm Schemata nd Data Struc-tures in Syntactic Processing.
Proceedings of the No-bel Symposium on Text Processing, Gothenburg.\[17\] Knuth, D.E.
1965 On the Translation of Languagesfrom Left to Right.
Information and Control, 8: 607-639.\[18\] Long, B.
1974 Deterministic Techniques for EfficientNon-deterministic Parsers.
Proc.
of the 2 na Collo-quium on Automata, Languages and Programming,J.
Loeckx (ed.
), S~rbrficken, Springer Lecture Notesin Computer Science 14: 255-269.Also: Rapport de Recherche 72, IRIA-Laboria, Roc-queneour t (France).\[19\] Long, B.
1988 Datalog Automata.
To appear in Proc.of the 3 rd Internat.
Conf.
on Data and KnowledgeBases, Jerusalem (Israel).\[20\] Long, B.
1988 Complete Evaluation of Horn Clauses,an Automata Theoretic Approach.
In preparation.\[21\] Li, T.; and Chun, H.W.
1987 A Massively ParallelNetwork-Based Natural Language Parsing System.Proc.
of 2 nd Int.
Conf.
on Computers and Applica-tions Beijing (Peking), : 401-408.\[22\] Nakagawa, S. 1987 Spoken Sentence Recognition byTime-Synchronous Parsing Algorithm of Context-Free Grammar'.
Proc.
ICASSP 87, Dallas (Texas),Vol.
2 : 829-832.\[23\] Phillips, J.D.
1986 A Simple Efficient Parser forPhrase-Structure Grammars.
Quarterly Newsletterof the Soc.
for the Study of Artificial Intelligence(AISBQ) 59: 14-19.Pratt, V.R.
1975 LINGOL - -  A Progress Report.
InProceedings Of the 4th IJCAI: 422-428.Shell, B.A.
1976 Observations on Context Free Pars-ing.
in Statistical Methods in Linguistics: 71-109,Stockholm (Sweden), Proe.
of Internat.
Conf.
onComputational Linguistics (COLING-76), Ottawa(Canna).Also: Technical Report TR 12-76, Ceat~r f~ Re-search in Computing T~mology, A ik~ Ccmaputa~tion Laboratory, Harvard Univ., Cambr~ (M~-sachusetts).Shiebcr, S.M.
1985 Using Restriction to ExtendParsing Algorithms for Complex-Feature-Based For-malisms.
Proceedings of the 23 ,~ Annual Meeting ofthe Association for Computational 15inguistics: 145-152.\[24\]\[25\]\[26\]\[27\] '?omita, M. 1986 An Efficient Word Lattice Pars-ing Algorithm fox" Continuous Speech Recognition.
InProceedings of IEEE-17~CE~ASJ I~terua~ional Con-.fereuee on Aco~tstlc,, Speech, and Signal Processing(ICASSP 86), Vol.
3: 1.569-1572.\[28\] Tomita, M. 1987 An Efficient Augmented-Context-?roe P~.rsing Algoxithm.
Compufational Lingui~tical:~(1.2): :~-~6.\[29\] geliant, L.G.
1975 GenerM Context-Free Recognition~n Le~ than Cubic Time.
dournM o$ Computer and3ystcm Sc~en~:ea, 10: 308-315.\[3i~\] 'gotmger, D.~.
1967 Recognition and Parsing ofl\]ontext-Free Language~ in Time n 3. litformafion and9outrol, 10(2): 189-208A 3:im~)ie xample wi~,hout unknowninput subsequenceTbi,'~ first simple exanrple, without unknown input, is intendedto fiunilia~:ize the ' with our rem:u~r constructions.A.~I  Craxnxnar  o f  the  ana lyzed  language' l ' i~ia grmr.m~' is taken fl'om \[28\].Nonterndna\]s are in C~l)ital letters, and termimtls are inlower ea~u,.. '?1.,e lh'zt r~le i~ treed for initialization and lmn--dling of tim delinfitez' symbol $.
The $ delimiters are implicitin ~:b,~., r~e~aal input sentenc?~.
(4) itP : :~ de~ n(5) ~P : :~ t~P PP(6) '?P ::~, pr(~p hip(7) VP ::,~ v ~PThis inpn:; eo~'re~pondu (for example) to the sentence:*~:i: ea.,\] a ~lan w i~h a mirror":~ALY~:t:S \[IF: (~ v do'~ ~ prep dot zt).
,&oi i  {71*~t~;'~x~; gr a~.~iar  in :educed by  the  parserThe gr~J~o~,~,~r output bg the paxser is given in figure 2.
Theinitial nol~te~mhLM is ~he left-hand side of the fh'st rule.
l~brre~l~l)i\]i~;:~ t, he nonternfi:mfl/items have bemn given computerg*'xte~n.t(~/names, (ff the fens at.x, where :c is an integer.
At thispoint we.
have forgotten ~he ixdermd structm'e of the items corre-?
spending ~o ?~heix' 'o\]e in the pa.~sing process.
All other symbolsare ternfi~M.
Integer terminals correspond to rule numbers ofthe input language grammar (-~ (see.
section A.1 above), and theothe," tex'Jx,hm\]f~ are symboh~ of the parsed language, i.e.
symbolsin ~\].
Not, ~.
the ~.mbig~ity fi)r nonterminM at;3.nt0 ::= nt l  nt2 nt l4  ::= detnt l  ::= $ nt l5  ::= nat2 ::= at3 nt28 nt l6  ::= nt l7 6nt3 ::= nt4 2 nt l7  ::= ntl8 nt l9nt3 ::= nt23 1 ntl8 : :=prepnt4 ::= nt5 ntl6 ntl9 ::= nt20 4nt5 : :=  nt6 1 nt20 : :=  nt21 nt22at6 ::= at7 nt9 nt21 ::= dotnt7 ::= at8 3 nt22 ::= nat8 ::= n at23 ::= nt7 nt24at9 ::= nt l0 7 nt24 ::= nt25 7nt l0 ::= nt l l  nt12 at25 ::= nt l l  nt26nt l l  :::= v nt26 ::= nt27 5nt l2 ::= nt l3 4 nt27 ::= ntl2 nt l6nt l3 ::= nt l4  nt l5 nt28 ::= $Figm'e 2: The output grammar.01 24 23i i- J I11 12~,4  11\] 19~4v I ,,r,,,,, I13 2014 15  21 22det  rl det  nFigure 3: Graph of the output grammar.NP4v det  npp6prep det  nFigure 4: The shared parse forest369Ao4 Simplified output grammarThis is a simplified form of the grammar in which some of thestructm'e that makes it readable as a shared-forest has been lost(though it could be retrieved).
However it preserves all sharingof common subparses.
This is the justification for having somany rules, while only 2 parse sequences may be generated bythat grarmnar.ntO : :=  $ nt3 $nt8 ::= nt7 nt11 nt12 7 1 nt16 2nt3 ::= nt7 nt l l  nt l2 nt16 5 7 1nt7 : := n 3nt11 : := vnt12 ::= det n 4nt16 ::= prep det n 4 6The 2 parses of the input, which are defined by this gram-maI'~ are:$ n 3 v det n 4 7 1 prep det n 4 6 2 $$ n 3 v det n 4 prep det n 4 6 5 7 1 $Here again the 2 symbols $ must be read as delimiters.A.5 Parse  forest  bu i l t  f rom that  g rammarTo explain the construction of the shared forest, we first buildin figure 3 a graph from the grammar of section A.3.
Here thegraph is acyclic, but with an incomplete input, it could havecycles.
Each node corresponds to one terminal or nonterminalof the grammar in section A.3, and is labeled by it.
The labelsat the right of small dashes are input grammar ule nmnbers(eft section A.1).
Note the ambiguity of node nt3 representedby an ellipse joining the two possible parses.From the graph of figure 3, we can trivially derive tim shared-forest given in figure 4.For readability, we present this shared-forest in a simplifiedforra.
Actually the sons of a node need sometimes to be repre-sented as a binary Lisp like list, so as to allow proper sharingof some of the sons.
Each node includes a label which is a non-terminal of the grammar Q, and for each possible derivation(several in case of ambiguity, e.g.
the top node of figure 4) thereis the number of the grammar ule used for that derivation.The constructions in this section are purely virtual, andare not actually necessary in an implementation.
The data-structure representing the grammar of section A.3 may be di-rectly interpreted and used as a shared-forest.B Example  wi th an unknown inputsubsequenceB.1 Grammar of the analyzed languageThe grammar is the same as in appendix A.1-3o2 Input  sentenceThis input corresponds (for example) to the sentence:~...
SaW , .
.
mi r ror  ~where the first " .
.
. "
are known to be one word, and the last" .
.
o" may be any number of words, i.e.
:ANALYSIS OF: (?
v * n)B.3 Output grammar produced bythe  parserNote that the nodes that derive on (several) symbol(s) ?
havebeen replaced by * for simplification as indicated at the end ofsection 4.
This explnins the gaps in the numbering of nonter-minals.IFigure 5: Shared=forest for an incomplete Sentence.s2s PP2n NP PP prep\NP5 /'/:NP prep detFigure 6: A parse tree extracted from the forest.ntO ::~ ntl nt2 nt26 ::~ * nt27ntl ::- $ nt27 ::ffi nt28nt2 ::ffi nt3 nt38 nt27 ::~ nt32 5nt3 ::~ nt4 2 nt28 ::~ nt29 4nt3 ::ffi nt33 i nt28 ::- nt31 3nt4 ::= nt5 nt25 nt29 ::- * n~30nt5 ::ffi nt6 2 nt30 ::- nnt5 ::- nt17 1 nt31 ::- nnt6 ::- nt5 * nt32 ::- * nt25nt17 ::- nt18 nt20 nt33 ::u nt18 nt34nt18 ::- nt19 3 nt34 ::- nt35 7nt l9  : : -  ?
n t35  : : -  n t22  nt36nt20 ::- nt21 7 nt36 ::- nt28nt21 ::ffi nt22 * nt36 ::- nt37 5nt22 ::- v n?37 i : : -  * n?~5n?25 ::- n?26 6 n?38 i : : -  $B.4 Simplified output grammarntO ::= $ nt3 $ nt28 ::i ~ * n 4nt3 ::= nt5 nt25 2 nt28 ::~ n 3nt3 ::= nt l8  nt22 nt36 7 I nt25 ::~ * nt27 6n?5 ::- n?5 * 2 ntl8 ::~ ?
3nt5 ::= nt l8  nt22 * 7 I nt22 ::u vnt27 ::= nt28 nt36 ::= nt28nt27 ::- * nt25 5 nt36 ::~ * nt25 5370A parse of the input, chosen in the infinite set of possibleparses defined by this grammar, is the following (see figure 6):$ ?
8 v*  7 1 .
2 **  **  a46  5 62  $This itt not ~'eally a complete parse since, due to the first sim-plification of the grammar, some * symbols stand for a missingnontermil~d, i.e.
for any parse of a string derived from thisnontermil~d.
For example the first ?
stand for the nontermlnalNp and cmdd be replaced by "* 3" or by "* * 4 * * 3 6 5".B ,5  Parse  shared- fo res t  bu i l t  f rom that  g ram-I~laFThe outpu~ grammars given above are not optimal with respectto sharing.
Mainly the nonterminals nt27 and st36 should bethe same (they do generate the same parse fragments).
Alsothe .terminal n should appear only once.
We give in figure 5a stmred-ibrest corresponding to this grammar, build as in theprevio~ example of appendix A, were we have improved theshax'ing by merging at27 mxd st36 so as to improve readability.We do not give the intermediate graph representing tha outputgrannnar us we did in appendix A.Our implementation is currently being improved to directlyachieve better sharing.In figure 6 we give one parse-tree extracted from the shared-forest of fig~rc 5. it corresponds to the parse sequence given asexample in scction B.4 above.
Note that, like the correspondingparse sequence, this is not a complete parse tree, since it Iresnontermir~\]s labeling its leaves.
A complete parse tree may beobtained by completing arbitrarily these leaves according to theoriginal grv.mmar of the language as defined in section A.1.C The  a lgor i thmThe length of this algorithm is due to its generality.
Fewer typesof transitions axe usually needed with specific implementations,typically only one for scanning transitions.Coxmneats are prefixed with "--".....
Begin parse with input sequence x of length n~e~A:  - -  Initialization:= :  o o o o((q$ O) (q$ 0));:::: (0  ~ e);So ::~ {6};"p :=: {~};i :=: O;step-B: - -  Iteration- -  initial i tem- -  f irst rule of output grammar- -  initialize item-set ,.go- -  rules of output grammar--- input-scanner index is set- -  before the first input symbolloop - -  while i < n (el, exit in step-B.$)i f  xi+t # *~tepoB.l: - -  Normal  completion of item-set St--- with non-scanning transitions.
:l:or nve.vy item U = ( (pA i ) (q l~ j ) )  in  8/ do~:or avery noa-scanuing transltion r in $ dowe distinguish five cases, according to r:~ :  -~ stack-flee transitionif r=(pee  ~-~ rez )then V := ( ( rA i ) (qB j ) ) ;& := &u{V};v := v u {(v --, uz)};~ :  - -  push transitionif r=(pee  ~-) rcz )then V := ( ( rC i ) (pA i ) ) ;s, := &u{v};v := v u ( (v  --, z)};case-B.l,3: - -  pop transitioni f  r=(pAe ~-+ rex)then~or every i tem Y = ((q B j)  (s D k)) in  Sjdo V := ((r n i) (s D k)) ;& := &u{v) ;V := PU {(V --+ YCz)};case-B.l.~: - -  pop-push transitioni f  r=(pAe ~ rCz)then V := ( ( rC i ) (qB j ) ) ;& := &u{V};v := v u {(v -~ Uz)};case-B.1.~:- -  Other non-scanning transitions are ignoredelse - -~ .--- t.e.
the next input symbol- is the unknown subsequenee:s tep-B* .h  - -  Completion of item-set Si- -  with non-scanning transitions- -  and with dummy scanning transitions.--- This step is similar to step-B.
1,- -  but considering all transitions as non-scanning.for every item U=((pA i ) (qB j ) )  in Si dofor every transition v in 6 do- -  we distinguish five eases, according to r:case-B*.1.~:i~ ~ =(r ,~e  ~ re~)  o~- ~ =(p~.~ ~ ~'~)then V := ((r A i) (q 13 j ) ) ;,s', := &u{v) ;p := ~, u {(v - ,  u , )} ;- -  and so on as in step.B.
lstep-B.2: - -  Exit for main loopi f  i = n then ex i t  loop; ~- go to step-Ch := i+1;while X h = *  do h := h+l ;step-B.3: - -  Initialization of item-set Sh&:=?
;for every item u = ((p A i) (q B j ))  in e dofor every scanning transition r in ~ do- -  Proceed by eases as in step.B.1,- -  but with scanning transitions, and- -  adding the new items to Sh instead of St.- - -  See for example the following case:fase-B.$.2:i f  r=(pea  ~-~ rcz )  with xh =a or xh=?then V := ((r C h) (p A i)) ;& := &u{v};~, := , ,  u {(v - ,  z)};~ :  - -  Inerementat ion of scanning index ii := h;end loop;step-C: - -  Terminationfo r  every i tem U =: ((f t n) (q $ O)) in ansuch that  f 6 F do7 :) := 7 ~ U (Uf --~ U) ; - -  Ut is the initial nonterminal  of 9.- -  End of parse371
