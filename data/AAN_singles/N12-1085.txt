2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 667?676,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsGrammatical structures for word-level sentiment detectionAsad B. SayeedMMCI Cluster of ExcellenceSaarland University66123 Saarbru?cken, Germanyasayeed@coli.uni-sb.deJordan Boyd-Graber,Bryan Rusk, Amy Weinberg{iSchool / UMIACS, Dept.
of CS, CASL}University of MarylandCollege Park, MD 20742 USA{jbg@umiacs,brusk@,aweinberg@casl}.umd.eduAbstractExisting work in fine-grained sentiment anal-ysis focuses on sentences and phrases but ig-nores the contribution of individual words andtheir grammatical connections.
This is becauseof a lack of both (1) annotated data at the wordlevel and (2) algorithms that can leverage syn-tactic information in a principled way.
We ad-dress the first need by annotating articles fromthe information technology business press viacrowdsourcing to provide training and testingdata.
To address the second need, we proposea suffix-tree data structure to represent syntac-tic relationships between opinion targets andwords in a sentence that are opinion-bearing.We show that a factor graph derived from thisdata structure acquires these relationships witha small number of word-level features.
Wedemonstrate that our supervised model per-forms better than baselines that ignore syntac-tic features and constraints.1 IntroductionThe terms ?sentiment analysis?
and ?opinion mining?cover a wide body of research on and development ofsystems that can automatically infer emotional statesfrom text (after Pang and Lee (2008) we use the twonames interchangeably).
Sentiment analysis plays alarge role in business, politics, and is itself a vibrantresearch area (Bollen et al, 2010).Effective sentiment analysis for texts such asnewswire depends on the ability to extract who(source) is saying what (target).
Fine-grained sen-timent analysis requires identifying the sources andtargets directly relevant to sentiment bearing expres-sions (Ruppenhofer et al, 2008).
For example, con-sider the following sentence from a major informa-tion technology (IT) business journal:Lloyd Hession, chief security officer at BTRadianz in New York, said that virtualiza-tion also opens up a slew of potential net-work access control issues.There are three entities in the sentence that have thecapacity to express an opinion: Lloyd Hession, BTRadianz, and New York.
These are potential opinionsources.
There are also a number of mentioned con-cepts that could serve as the topic of an opinion inthe sentence, or target.
These include all the sources,but also ?virtualization?, ?network access control?,?network?, and so on.The challenging task is to discriminate betweenthese mentions and choose the ones that are rele-vant to the user.
Furthermore, such a system mustalso indicate the content of the opinion itself.
Thismeans that we are actually searching for all triples{source, target, opinion} in this sentence (Kim andHovy, 2006) and throughout each document in thecorpus.
In this case, we want to identify that LloydHession is the source of an opinion, ?slew of networkissues,?
about a target, virtualization.
Providing suchfine-grained annotations would enrich informationextraction, question answering, and corpus explo-ration applications by letting users see who is sayingwhat with what opinion (Wilson et al, 2005; Stoy-anov and Cardie, 2006).We motivate the need for a grammatically-focusedapproach to fine-grained opinion mining and situate it667within the context of existing work in Section 2.
Wepropose a supervised technique for learning opinion-target relations from dependency graphs in a way thatpreserves syntactic coherence and semantic compo-sitionality.
In addition to being theoretically sound?
a lacuna identified in many sentiment systems1?
such approaches improve downstream sentimenttasks (Moilanen and Pulman, 2007).There are multiple types of downstream tasks thatpotentially require the retrieval of {source, target,opinion} relations on a sentence-by-sentence basis.An increasingly significant application area is in theuse of large corpora in social science.
This area ofresearch requires the exploration and aggregation ofdata about the relationships between discourses, orga-nizations, and people.
For example, the IT businesspress data that we use in this work belongs to a largerresearch program (Tsui et al, 2009; Sayeed et al,2010) of exploring industry opinion leadership.
ITbusiness press text is one type of text in which manyentities and opinions can appear intermingled withone another in a small amount of text.Another application for fine-grained sentiment re-lation retrieval of this type is paraphrasing, whereattribution of which opinion belongs to which entitiesmay be important for producing useful and accurateoutput, since source and target identification errorscan change the entire meaning of an output text.Unlike previous approaches that ignore syntax, weuse a sentence?s syntactic structure to build a proba-bilistic model that encodes whether a word is opinionbearing as a latent variable.
We build a data structurewe call a ?syntactic relatedness trie?
(Section 3) thatserves as the skeleton for a graphical model over thesentiment relevance of words (Section 4).
This ap-proach allows us to learn features that predict opinionbearing constructions from grammatical structures.Because of a dearth of resources for this fine-grainedtask, we also develop new crowdsourcing techniquesfor labeling word-level, syntactically informed sen-1Alm (2011) recently argued that work on sentiment anal-ysis needs to de-emphasize the goal of building systems thatare ?high-performing?
by traditional measures, because the fieldrisks sacrificing ?opportunities that may lead to a more thoroughunderstanding of language uses and users?
in relation to subjec-tive phenomena.
The work we present in this paper thereforefocuses on extracting meaningful features as an investment infuture work that directly improves retrieval performance.timent (Section 5).
We use inference techniques touncover grammatical patterns that connect opinion-expressing words and target entities (Section 6) per-forming better than using syntactically uninformedmethods.2 Background and existing workWe call opinion mining ?fine-grained?
when it re-trieves many different {source, target, opinion}triples per document.
This is particularly challengingwhen there are multiple triples even within a sen-tence.
There is considerable work on identifying thesource of an opinion.
However, it is much harderto find obvious features that tell us whether ?virtual-ization?
is the target of an opinion.
The most recenttarget identification techniques use machine learningto determine the presence of a target from knownopinionated language (Jakob and Gurevych, 2010).Even when targets are identified we must decide ifan opinion is expressed, since not all target mentionswill necessarily be accompanied by opinion expres-sions.
Returning to the first example sentence, wecould say that the negative opinion about virtualiza-tion is expressed by the words ?slew?
and ?issues?.A system that could automatically make this dis-covery must draw on grammatical relationships be-tween targets and the opinion bearing words.
Parsersreveal these relationships, but the relationships areoften indirect.
The variability of language preventsa complete enumeration of all intervening items thatmake the relationships indirect, but examples includenegation and intensifiers, which change opinion, andsentiment-neutral words, which fill syntactic or stylis-tic needs.
In this paper, we cope with the variabilityof expression by using supervised machine learningto generalize across observations and learn which fea-tures best enable us to identify opinionated language.Existing work in this area often uses semanticframes and role labeling (Kim and Hovy, 2006; Choiet al, 2006), but resources typically used in thesetasks (e.g.
FrameNet) are not exhaustive.
More gen-eral approaches (Ruppenhofer et al, 2008) describesemantic and discourse contexts of opinion sourcesand targets cannot recognize them.When techniques do identify targets via syntax,they often only use grammar as a feature in an oth-erwise syntax-agnostic model.
Some work of this668nature merely identifies targets without providing thesyntactic evidence necessary to find domain-relevantopinionated language (Jakob and Gurevych, 2010),relying on lists of opinion keywords.
There is alsowork (Qiu et al, 2011) that uses predefined heuristicsover dependency parses to identify both targets andopinion keywords but does not acquire new syntacticheuristics.
Other work (Nakagawa et al, 2010) is sim-ilar to ours in that it uses factor graph modeling overa dependency parse formalism, but it assumes thatopinionated language is known a priori and focuseson polarity classification, while our work tackles themore fundamental problem of identifying the opin-ionated language itself.Little work has been done to perform target andopinion-expression extraction jointly, especially in away that extracts features for downstream processing.This dearth persists despite evidence that such infor-mation improves sentiment analysis (Moilanen andPulman, 2007).An advantage of our proposed approach is that wecan use dependency paths in order to capture situa-tions where the relations are non-compositional orsemantically motivated.
In Section 5, we describe adata set that has the additional property that opinionis expressed in ways that require external pragmaticknowledge of the domain.
An advantage of arbi-trary, non-local dependencies is that we can treat thisknowledge as part of the model we learn via long-distance chains, which can capture pragmatics.3 Syntactic relatedness triesWe now describe how we build the syntactic related-ness trie (SRT) that forms the scaffolding for the prob-abilistic models needed to identify sentiment-bearingwords via syntactic constraints extracted from a de-pendency parse (Ku?bler et al, 2009).We use the Stanford Parser (de Marneffe and Man-ning, 2008) to produce a dependency graph and con-sider the resulting undirected graph structure overwords.
We construct a trie for each possible targetword in a sentence (it is possible for a sentence toinduce multiple tries if the sentence contains multi-ple potential targets).
Each trie encodes paths fromthe possible target word to other words, and eachpath represents a sequence of words connected byundirected edges in the parse.3.1 Encoding Dependencies in an SRTSRTs enable us to encode the connections betweena single linguistic object of interest?in this appli-cation, a possible target word?and a set of relatedobjects.
SRTs are data structures consisting of nodesand edges.This description is very similar to the definitionof a dependency parse.
The key difference is thatwhile a token only appears once as a node in a de-pendency parse, an SRT can contain multiple nodesthat originate from the same token.
This encodes thepossible connections between an opinion target andopinion-conveying words.The object of interest is the opinion target, definedas the SRT root node (e.g.
in Figure 1 ?policy?
is aknown target, so it becomes the root of an SRT).
EachSRT edge corresponds to a grammatical relationshipbetween words and is labeled with that relationship.We use the notation aR??
b to signify that node a hasthe relationship (?role?)
R with b.
We say in this casethat node b is a descendent of node a with the roleR.
The directed edges constitute a trie or suffix treethat represents the fact that multiple paths may shareelements that all provide evidence for the relevanceof multiple leaves.
2In the remainder of this section we describe thenecessary steps to create a training corpus for fine-grained sentiment analysis.
We provide an exampleof how to create an SRT from a dependency parse andthen to attach latent variable assignments to an SRTbased on human annotations in a way that respectssyntactic constraints.3.2 Using sentiment flow to label an SRTOur goal is to discriminate between parts of the struc-ture that are relevant to target-opinion word relationsand those that are not.
We use the term sentimentflow (shortened to ?flow?
when space is an issue)for relevant sentiment-bearing words in the SRT andinert for the remainder of the sentence.
We use theterm ?flow?
because our invariant (section 3.3) con-strains a sentiment flow in a SRT to be a contiguoussubgraph; this corresponds to linguistic intuitionsthat, for example, in the sentence ?Linux with Wine2The SRT will be used to create an undirected graphicalmodel; the notion of directedness refers to the traversal of pathsused to construct the SRT.669the dominantrolethe european climate protectionpolicyhasbenefitsoureconomypolicypolicypolicyprotectionrolerolehasdominantbenefitsDependency ParsePaths for "policy" SRTFigure 1: Dependency parse example.
A dependencyparse (top) is used to generate a syntactic relatednesstrie for all possible targets of a sentiment-bearingexpression.
For the target word ?policy?, there are anumber of paths (colors are consistent in paths to beadded to the SRT and in the dependency parse) thatconnect it to other words; once extracted, these pathswill be inserted into a target-specific SRT.is very usable?, {?Linux?, ?is?, ?very?}
could notbe part of a sentiment flow without also including{?usable?
}.Now that we have the structure of the model, weneed training data: sentences where sentiment bear-ing words have been labeled.
We describe how to gofrom sentiment-labeled words to valid flows usingthis sentence from the MPQA:The dominant role of the European climateprotection policy has benefits for our econ-omy.In this sentence, the target word ?policy?
is con-nected to multiple sentiment-bearing words via pathsin the dependency parse (Figure 1).
We can representthese relationships using paths through the graph asin Figure 2(a).
(For clarity, we do not show somepaths.
)Suppose that an annotator decides that ?protec-tion?
and ?benefits?
are directly expressing an opin-ion about the policy, but ?dominant?
is ambiguous (ithas some negative connotations).
The nodes ?protec-tion?
and ?benefits?
are a flow, and the ?dominant?policyprotectionrolehas benefitsdominantpolicyprotectionrolehas benefitsdominantpolicyprotectionrolehas benefitsdominantpolicyprotectionrolehas benefitsdominant(a)(b)(c)(d)Figure 2: Labeled SRTs rooted on the target word?policy?
; green-filled nodes represent words that arepart of a sentiment flow and nodes with a red outlinerepresent inert nodes.
(a) Initial labels for SRT (e.g.as provided by annotators) (b) propagating labels toyield a valid sentiment flow (c) a change of ?role?
toinert also renders its children inert (d) a change of?dominant?
to be part of a sentiment flow also causesits parents to be part of a flow.node is inert.
However, there is considerable overlapbetween the ?dominant?
path and the ?benefits?
path.That is the motivation for combining them into a triestructure and labeling them in such a way that thepath remains a flow until there is no path element thatleads to a flow leaf (Figure 2).In other words, we want the path elements com-mon to a flow path and an inert path to reinforcesentiment flow.
The transition from flow to inert islearned by the classifier.We enforce this requirement through the procedureshown in Figure 2, which is equivalent to finding thedepth first search tree of the dependency graph andapplying the node-labeling scheme as above.3.3 InvariantAnything that follows a node with an inert label isby definition not reachable from the root of the tree.670Consequently, any node that is part of a sentimentflow that follows an inert node is not reachable alonga path and is actually inert itself.
We specify thisdirectly as an invariant on the data structure:Invariant: no node descending from anode labeled inert can be labeled as a partof a sentiment flow.This specifies that flow labels spread out from theroot of the SRT.
Our inference algorithm requiresthat we be able to change the labels of nodes fortest data, thus we need to define invariant-respectingoperations for switching labels from flow to flow andvice-versa.
A flow label switched to inert will requireall the descendents of that particular node to switchto inert as well as in figure 2(c).
Similarly, an inertlabel switched to flow will require all of the ancestorsof that node to switch to flow as in 2(d).4 Encoding SRTs as a factor graphIn this section, we develop supervised machine learn-ing tools to produce a labeled SRT from unlabeled,held-out data in a single, unified model, without per-mitting the sorts of inconsistencies that may be ad-mitted by using a local classifier at each node.4.1 Sampling labelsA factor graph (Kschischang et al, 1998) is a rep-resentation of a joint probability distribution in theform of a graph with two types of vertices: vari-able vertices and factor vertices.
Given a set of vari-ables Z = {z1 .
.
.
zn}, we connect them via factorsF = {f1 .
.
.
fm}.
Factors are functions that repre-sent relationships, i.e.
probabilistic dependencies,among the variables; the product of all factors givesthe complete joint distribution p. Each factor fi cantake as input some corresponding subset of variablesYi from Z.
We can then write the relationship asfollows:p(Z) ?
?mk=1 fk(Yk)Our goal is to discover the values for the variablesthat best explain a dataset.
While there are manyapproaches for inference in statistical models, weturn to MCMC methods (Neal, 1993) to discover theunderlying structure of the model.
More specifically,we seek a posterior distribution over latent variablesparentnodechild1child2child3hgfFigure 3: Graphical model of SRT factorsthat partition words in a sentence into flow and in-ert groups; we estimate this posterior using Gibbssampling (Finkel et al, 2005).The sampler requires an initial state that respectsthe invariant.
Our initial setting is produced by iterat-ing through all labels in the SRT forest and randomlysetting them as either flow or inert with uniformprobability.A Gibbs sampler samples new variable assign-ments from the conditional distribution, treating thevariable assignments for all other variables fixed.However, the assignment of a single node is highlycoupled with its neighbors, so a block sampler is usedto propose changes to groups nodes that respect theflow labeling of the overall assignments.
This wasimplemented by changing the proposal distributionused by the FACTORIE framework (McCallum et al,2009).We can thus represent a node and its contributionto the overall score using the graph in Figure 3.
Thisgraph contains the given node, its parent, and a vari-able number of children.
The factors that go into thelabeling decision for each node are thus constrainedto a small, computationally tractable space aroundthe given node.
This graph contains three factors:?
g represents a function over features of the givennode itself, or ?node features.??
f represents a function over a bigram of featurestaken from the parent node and the given node,or ?parent-node?
features.?
h represents a function over a combination fea-tures on the node and features of all its children,or ?node-child?
features.We provide further details about these factors in thenext section.671In addition to the latent value associated with eachword, we associate each node with features derivedfrom the dependency parse: the word from the sen-tence itself, the part-of-speech (POS) tag assignedby the Stanford parser, and the label of the incomingdependency edge.
We treat the edge labels from theoriginal dependency parse as a feature of the node.We can represent the set of possible observed lin-guistic feature classes as the set of features ?.
Fig-ure 3 induces a scoring function with contributionsof each node to the score(label|node) =????
(f(parent?, node?|label)g(node?|label)h(node?, child1?, .
.
.
, childn?|label)).After assignments for the latent variables are sampled,the weights for the factors (which when combinedcreate individual factors f that define the joint) mustbe learned.
This is accomplished via the sample-rankalgorithm (Wick et al, 2009).5 Data sourceOur goal is to identify opinion-bearing words and tar-gets using supervised machine learning techniques.Sentiment corpora with sub-sentential annotations,such as the Multi-Perspective Question-Answering(MPQA) corpus (Wilson and Wiebe, 2005) and theJ.
D. Power and Associates (JDPA) blog post cor-pus (Kessler et al, 2010), exist, but most of theseannotations are at a phrase level.
Within a phrase,however, some words may contribute more than oth-ers to the statement of an opinion.
We developed ourown annotations to discover such distinctions3.
Wedescribe these briefly here; more information aboutthe development of the data source can be found inSayeed et al (2011).5.1 Information technology business pressOur work is part of a larger collaboration with so-cial scientists to study the diffusion of informationtechnology (IT) innovations through society by iden-tifying opinion leaders and IT-relevant opinionatedlanguage Rogers (2003).
Thus, we focus on a col-lection of articles from the IT professional maga-zine, Information Week, from the years 1991 to 2008.3To download the corpus, visit http://www.umiacs.umd.edu/?asayeed/naacl12data/.This consists of 33K articles including news bulletinsand opinion columns.
Our IT concept target list (59terms) comes from our application.
Thus, we con-struct a trie for each appearance of any of these possi-ble target terms.
We consider this list of target termsto be complete, which allows us to focus on discover-ing opinion-bearing text associated with these targets.5.2 Crowdsourced annotation processOur process for obtaining gold standard data involvesmultiple levels of human annotation including oncrowdsourcing platforms Hsueh et al (2009).There are 75K sentences with IT concept mentions,only a minority of which express relevant opinions.Hired undergraduate students searched a random se-lection of these sentences and found 219 that containthese opinions.
We used cosine-similarity to rank theremaining sentences against the 219.We then needed to identify which of the wordscontained an opinion.
We excluded all words thatwere common function words (e.g.,?the?, ?in?)
butleft negations.
We engineered tasks so that onlya randomly-selected five or six words appear high-lighted for classification in order to limit annotatorboredom.
We called this group a ?highlight group?.The virtualization example would look like this:Lloyd Hession, chief security officer at BTRadianz in New York, said that virtual-ization also opens up a slew of potentialnetwork access control issues.In the virtualization example, the worker would seethat virtualization is highlighted as the IT concepttarget.
Other words are highlighted as candidates thatthe worker must classify as being opinion-relevant to?virtualization?.
Each highlight group corresponds toa syntactic relatedness trie (Section 3).A task was presented to a worker in the form ofa highlight group and some list boxes that representclasses for the highlighted words: ?positive?, ?nega-tive?, ?not opinion-relevant?, and ?ambiguous?.
Theworker was required to drag each highlighted can-didate word to exactly one of the boxes.
As we arenot doing opinion polarity classification, the ?posi-tive?
and ?negative?
boxes were intended as a formof misdirection intended to avoid having the workerconsider what an opinion is; we treated this input asa single ?opinion-relevant?
category.672Three or more users annotated each highlightgroup, and an aggregation scheme was applied af-terwards: ?ambiguous?
answers were rolled into ?notopinion-relevant?
and ties were dropped.
Our qual-ity control process involved filtering out workerswho performed poorly on a small subset of gold-standard answers We annotated 30 evaluation units todetermine that our process retrieved opinion-relevantwords at 85% precision and 74% recall.Annotators labeled 700 highlight groups for theresults in this paper.
The total cost of this exercisewas approximately 250 USD, which includes the feescharged by Amazon and CrowdFlower.
These lasthighlight groups were converted to SRTs and dividedinto training and testing groups, 465 and 196 SRTsrespectively, with a small number lost to fatal errorsin the Stanford parser.6 Experiments and discussionDuring the training phase, we evaluate the qualityof a candidate labeling based on label accuracy.
Weneed to identify both flow nodes and inert nodes inorder to distinguish between relevant and irrelevantsubcomponents.
We thus also employ precision andrecall as performance metrics.An example of how this works can be seen by com-paring figure 2(b) to figure 2(d), viewing the formeras the gold standard and the latter as a hypotheticalsystem output.
If we run the evaluation over thatsingle SRT and treat flow as the positive class, wefind that 3 true positives, 1 false positive, 2 false neg-atives, and no true negatives.
There are 6 labels intotal.
That yields 0.50 accuracy, 0.75 precision, 0.60recall, and 0.67 F-measure.We run every experiment (training a model andtesting on held-out data) 10 times and take the meanaverage and range of all measures.
F-measure iscalculated for each run and averaged post hoc.6.1 ExperimentsOur baseline system is the initial setting of the labelsfor the sampler: uniform random assignment of flowlabels, respecting the invariant.
This leads to a largeclass imbalance in favor of inert as any switch toinert converts all nodes downstream from the root toconvert to inert, while a switch to flow causes onlyone ancestor branch to convert to flow.Our next systems involve combinations of our SRTfactors with the observed linguistic features.
All ourexperiments include the factor g that pertains only tothe features of the node.
Then we add factor f?theparent-node ?bigram?
features?and finally factor h,the variable-length node-child features.
We also ex-periment with including and excluding combinationsof POS, role, and word features.
We also exploredmodels that only made local decisions, ignoring theconsistency constraints over sentiment flows.
Al-though such models cannot be used in techniquessuch as Nakagawa et al?s polarity classifier, theyfunction as a baseline and inform whether syntacticconstraints help performance.We ran the inferencer for 200 iterations to train amodel with a particular factor-feature combination.We use the learned model to predict the labels onthe held-out testing data by running the inferencealgorithm (sampling labels only) for 50 iterations.6.2 DiscussionWe present a sampling of possible feature-factor com-binations in table 1 in order to show trends in theperformance of the system.Unsurprisingly, the invariant-respecting baselinehad very high precision but low recall.
Simply includ-ing the node-only g factor with all features increasesthe recall while hurting precision.
On removing wordfeatures, recall increases without changing precision.This suggests that some words in some SRTs are as-sociated with flow labels in the training data, but notas much in the testing data.Including parent-node f features with the g fea-tures yields higher precision and lower recall, sug-gesting that parent-node word features support preci-sion.
Including all features on all factors (f , g, and h)preserves most of the precision but improves recall.Excluding h features increases recall slightly morethan it hurts precision.
Excluding both word featuresfor all factors and role h features hurts all measures.The accuracy measure, however, does show over-all improvement with the inclusion of more feature-factor combinations.
In particular, the node-child hfactor does appear to have an effect on the perfor-mance.
The presence of some combinations of childword, POS tags, and roles appear to provide someindication of the flow labeling of some of the nodes.The best models in terms of accuracy include all or673Experiment Features Invariant?
Precision Recall F AccuracyBaseline N/AYes 0.78 ?
0.05 0.06 ?
0.01 0.11 ?
0.02 0.51 ?
0.01No 0.50 ?
0.00 0.49 ?
0.00 0.50 ?
0.00 0.50 ?
0.00Node onlyAllYes 0.63 ?
0.10 0.34 ?
0.10 0.42 ?
0.07 0.54 ?
0.03No 0.51 ?
0.00 0.88 ?
0.03 0.65 ?
0.01 0.51 ?
0.01All but wordYes 0.63 ?
0.16 0.40 ?
0.22 0.42 ?
0.19 0.53 ?
0.03No 0.57 ?
0.04 0.56 ?
0.17 0.55 ?
0.07 0.55 ?
0.03Parent, nodeParent: all but wordYes 0.71 ?
0.06 0.21 ?
0.04 0.31 ?
0.05 0.55 ?
0.01Node: allAll Yes 0.84 ?
0.07 0.11 ?
0.04 0.19 ?
0.06 0.53 ?
0.01Full graphParent: all but wordYes 0.59 ?
0.06 0.39 ?
0.11 0.46 ?
0.07 0.54 ?
0.03Node: all but wordChildren: POS onlyParent: allYes 0.67 ?
0.05 0.39 ?
0.08 0.47 ?
0.06 0.59 ?
0.02Node: allChildren: all but wordAllYes 0.70 ?
0.05 0.35 ?
0.08 0.46 ?
0.07 0.59 ?
0.02No 0.70 ?
0.03 0.20 ?
0.05 0.36 ?
0.06 0.56 ?
0.01Table 1: Performance using different feature combinations, including some without enforcing the invariant.Mean averages and standard deviation for 10 runs.almost all of the features.Our non-invariant-respecting baseline unsurpris-ingly was nearly 50% on all measures.
Including thenode-only features dramatically increases recall, lessif we exclude word features.
The word features ap-pear to have an effect on recall just as in the invariant-respecting case with node-only features.
With allfeatures, precision is dramatically improved, but witha large cost to recall.
However, it underperformsthe equivalent invariant-respecting model in recall,F-measure, and accuracy.Though these invariant-violating models are un-constrained in the way they label the graph, ourinvariant-respecting models still outperform them.A coherent path contains more information than anincoherent one; it is important to find negating andintensifying elements in context.
Our SRT invariantallows us to achieve better performance and will bemore useful to downstream tasks.Finally, it appears that using more factors and lin-guistic features promotes stability in performanceand decreases sensitivity to the initial setting.6.3 Manual inspectionOne pattern that prominently stood out in the testingdata with the full-graph model was the misclassifica-tion of flow labels as inert in the vicinity of Stanforddependency labels such as conj and.
These kindsof labels have high ?fertility?
; the labels immediatelyfollowing them in the SRT could be a variety of types,creating potential data sparsity issues.This problem could be resolved by making somefeatures transparent to the learner.
For example, ifnode q has an incoming conj and dependency edgelabel, then q?s parent could also be directly connectedto q?s children, as a conjunction should be linguisti-cally transparent to the status of the children in thesentiment flow.There are many fewer incidents of inert labels be-ing classified as flow.
There are paths through anSRT where a flow candidate word is the ancestor ofan inert candidate word from the set of crowdsourcedcandidates.
The model sometimes appears to ?over-shoot?
the flow candidate.
Considering that recall isalready fairly low, attempts to address this problemrisks making the model too conservative.
One poten-tial solution is to prune or separate paths that containmultiple flow candidates.6.3.1 Paths foundWe examined the labeling on the held-out testingdata of the best-performing model of the full graphsystem with all linguistic features.
For example, con-sider the following highlight group:But Microsoft?s informal approach may not beenough as the number of blogs at the companygrows, especially since the line between ?personal?Weblogs and those done as part of the job can behard to distinguish.In this case, the Turkers decided that ?distinguish?expressed a negative opinion about blogs, in the sense674that something that was difficult to distinguish wasa problem: the modifier ?hard?
is what makes itnegative.
The system found an entirely flow path thatconnected these attributes into a single unit:Blog:flow prepof?????
number:flow nsubj???
?grows:flow ccomp?????
hard:flow xcomp????
?distinguish:flowIn this path, ?blog?
and ?distinguish?
are both con-nected to one another by ?hard?, giving ?distinguish?its negative spin.
There are two non-local dependen-cies in this example: xcomp, ccomp.
Very often,more than one unique path connects the concept tothe opinion candidate word.7 Conclusions and future workIn this work, we have applied machine learning toproduce a robust modeling of syntactic structure foran information extraction application.
A solution tothe problem of modeling these structures requires thedevelopment of new techniques that model complexlinguistic relationships in an application-dependentway.
We have shown that we can mine these relation-ships without being overcome by the data-sparsityissues that typically stymie learning over complexlinguistic structure.The limitations on these techniques ultimately findtheir root in the difficulty in modeling complex syn-tactic structures that simultaneously exclude irrel-evant portions of the structure while maintainingconnected relations.
Our technique uses a structure-labelling scheme that enforces connectedness.
En-forcing connected structure is not only necessary toproduce useful results but also to improve accuracy.Further performance gains might be possible by en-riching the feature set.
For example, the POS tagsetused by the Stanford parser contains multiple verbtags that represent different English tenses and num-bers.
For the purpose of sentiment relations, it ispossible that the differences between verb tags aretoo small to matter and are causing data sparsity is-sues.
Thus, we could additional features that ?backoff?
to general verb tags.AcknowledgementsThis paper is based upon work supported by theUS National Science Foundation under Grant IIS-0729459.
Additional support came from the Clusterof Excellence ?Multimodal Computing and Innova-tion?, Germany.
Jordan Boyd-Graber is also sup-ported by US National Science Foundation GrantNSF grant #1018625 and the Army Research Labora-tory through ARL Cooperative Agreement W911NF-09-2-0072.
Any opinions, findings, conclusions, orrecommendations expressed are the authors?
and donot necessarily reflect those of the sponsors.ReferencesAlm, C. O.
(2011).
Subjective natural language prob-lems: Motivations, applications, characterizations,and implications.
In ACL (Short Papers).Bollen, J., Mao, H., and Zeng, X.-J.
(2010).
Twit-ter mood predicts the stock market.
CoRR,abs/1010.3003.Choi, Y., Breck, E., and Cardie, C. (2006).
Joint ex-traction of entities and relations for opinion recog-nition.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP).de Marneffe, M.-C. and Manning, C. D. (2008).
Thestanford typed dependencies representation.
InCrossParser ?08: Coling 2008: Proceedings ofthe workshop on Cross-Framework and Cross-Domain Parser Evaluation, Morristown, NJ, USA.Association for Computational Linguistics.Finkel, J. R., Grenager, T., and Manning, C. (2005).Incorporating non-local information into informa-tion extraction systems by gibbs sampling.
InProceedings of the 43rd Annual Meeting on Asso-ciation for Computational Linguistics, ACL ?05,pages 363?370, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Hsueh, P.-Y., Melville, P., and Sindhwani, V. (2009).Data quality from crowdsourcing: a study of anno-tation selection criteria.
In Proceedings of theNAACL HLT 2009 Workshop on Active Learn-ing for Natural Language Processing, HLT ?09,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Jakob, N. and Gurevych, I.
(2010).
Extracting opin-ion targets in a single and cross-domain settingwith conditional random fields.
In EMNLP.Kessler, J. S., Eckert, M., Clark, L., and Nicolov,N.
(2010).
The 2010 ICWSM JDPA sentment675corpus for the automotive domain.
In 4th Int?lAAAI Conference on Weblogs and Social MediaData Workshop Challenge (ICWSM-DWC 2010).Kim, S.-M. and Hovy, E. (2006).
Extracting opinions,opinion holders, and topics expressed in onlinenews media text.
In SST ?06: Proceedings of theWorkshop on Sentiment and Subjectivity in Text,pages 1?8, Morristown, NJ, USA.
Association forComputational Linguistics.Kschischang, F. R., Frey, B. J., and andrea Loeliger,H.
(1998).
Factor graphs and the sum-product algo-rithm.
IEEE Transactions on Information Theory,47:498?519.Ku?bler, S., McDonald, R., and Nivre, J.
(2009).
De-pendency parsing.
Synthesis Lectures on HumanLanguage Technologies, 2(1).McCallum, A., Schultz, K., and Singh, S. (2009).Factorie: Probabilistic programming via impera-tively defined factor graphs.
In Neural InformationProcessing Systems (NIPS).Moilanen, K. and Pulman, S. (2007).
Sentiment com-position.
In Proceedings of the Recent Advances inNatural Language Processing International Con-ference (RANLP-2007), Borovets, Bulgaria.Nakagawa, T., Inui, K., and Kurohashi, S. (2010).
De-pendency tree-based sentiment classification usingcrfs with hidden variables.
In HLT-NAACL.Neal, R. M. (1993).
Probabilistic inference usingMarkov chain Monte Carlo methods.
TechnicalReport CRG-TR-93-1, University of Toronto.Pang, B. and Lee, L. (2008).
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2).Qiu, G., Liu, B., Bu, J., and Chen, C. (2011).
Opin-ion word expansion and target extraction throughdouble propagation.
Computational linguistics,37(1):9?27.Rogers, E. M. (2003).
Diffusion of Innovations, 5thEdition.
Free Press.Ruppenhofer, J., Somasundaran, S., and Wiebe, J.(2008).
Finding the sources and targets of sub-jective expressions.
In Calzolari, N., Choukri, K.,Maegaard, B., Mariani, J., Odjik, J., Piperidis, S.,and Tapias, D., editors, Proceedings of the SixthInternational Language Resources and Evaluation(LREC?08), Marrakech, Morocco.
European Lan-guage Resources Association (ELRA).Sayeed, A.
B., Nguyen, H. C., Meyer, T. J., andWeinberg, A.
(2010).
Expresses-an-opinion-about:using corpus statistics in an information extractionapproach to opinion mining.
In Proceedings of the23rd International Conference on ComputationalLinguistics, COLING ?10.Sayeed, A.
B., Rusk, B., Petrov, M., Nguyen, H. C.,Meyer, T. J., and Weinberg, A.
(2011).
Crowd-sourcing syntactic relatedness judgements for opin-ion mining in the study of information technologyadoption.
In Proceedings of the Association forComputational Linguistics 2011 workshop on Lan-guage Technology for Cultural Heritage, SocialSciences, and the Humanities (LaTeCH).
Associa-tion for Computational Linguistics.Stoyanov, V. and Cardie, C. (2006).
Partially su-pervised coreference resolution for opinion sum-marization through structured rule learning.
InEMNLP ?06: Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 336?344, Morristown, NJ, USA.Association for Computational Linguistics.Tsui, C.-J., Wang, P., Fleischmann, K., Oard, D.,and Sayeed, A.
(2009).
Understanding IT innova-tions by computational analysis of discourse.
InInternational conference on information systems.Wick, M., Rohanimanesh, K., Culotta, A., and Mccal-lum, A.
(2009).
SampleRank: Learning preferencefrom atomic gradients.
In NIPS WS on Advancesin Ranking.Wilson, T. and Wiebe, J.
(2005).
Annotating attribu-tions and private states.
In CorpusAnno ?05: Pro-ceedings of the Workshop on Frontiers in CorpusAnnotations II, Morristown, NJ, USA.
Associationfor Computational Linguistics.Wilson, T., Wiebe, J., and Hoffmann, P. (2005).
Rec-ognizing contextual polarity in phrase-level senti-ment analysis.
In HLT/EMNLP.676
