An Intelligent Procedure AssistantBuilt Using REGULUS 2 and ALTERFManny Rayner, Beth Ann Hockey, Jim Hieronymus, John Dowding, Greg AistResearch Institute for Advanced Computer Science (RIACS)NASA Ames Research CenterMoffet Field, CA 94035{mrayner,bahockey,jimh,jdowding,aist}@riacs.eduSusana EarlyDeAnza College/NASA Ames Research Centersearly@mail.arc.nasa.govAbstractWe will demonstrate the latest version ofan ongoing project to create an intelli-gent procedure assistant for use by as-tronauts on the International Space Sta-tion (ISS).
The system functionality in-cludes spoken dialogue control of nav-igation, coordinated display of the pro-cedure text, display of related pictures,alarms, and recording and playback ofvoice notes.
The demo also exempli-fies several interesting component tech-nologies.
Speech recognition and lan-guage understanding have been devel-oped using the Open Source REGULUS2 toolkit.
This implements an approachto portable grammar-based language mod-elling in which all models are derivedfrom a single linguistically motivated uni-fication grammar.
Domain-specific CFGlanguage models are produced by firstspecialising the grammar using an au-tomatic corpus-based method, and thencompiling the resulting specialised gram-mars into CFG form.
Translation betweenlanguage centered and domain centeredsemantic representations is carried out byALTERF, another Open Source toolkit,which combines rule-based and corpus-based processing in a transparent way.1 IntroductionAstronauts aboard the ISS spend a great deal of theirtime performing complex procedures.
This often in-volves having one crew member reading the proce-dure aloud, while while the other crew member per-forms the task, an extremely expensive use of as-tronaut time.
The Intelligent Procedure Assistant isdesigned to provide a cheaper alternative, whereby avoice-controlled system navigates through the pro-cedure under the control of the astronaut perform-ing the task.
This project has several challengingfeatures including: starting the project with no tran-scribed data for the actual target input language, andrapidly changing coverage and functionality.
Weare using REGULUS 2 and ALTERF to address thesechallenges.
Together, they provide an example-based framework for constructing the portion of thesystem from recognizer through intepretation thatallows us to make rapid changes and take advan-tage of both rule-base and corpus-based informationsources.
In this way, we have been able to extractmaximum utility out of the small amounts of datainitial available to the project and also smoothly ad-just as more data has been accumulated in the courseof the project.The following sections describe the procedure as-sistant application and domain, REGULUS 2 and AL-TERF.2 Application and domainThe system, an early version of which was describedin (Aist et al, 2002), is a prototype intelligent voiceenabled personal assistant, intended to support astro-nauts on the International Space Station in carryingout complex procedures.
The first production ver-sion is tentatively scheduled for introduction sometime during 2004.
The system reads out each pro-cedure step as it reaches it, using a TTS engine, andalso shows the corresponding text and supplemen-tary images in a visual display.
Core functionalityconsists of the following types of commands:?
Navigation: moving to the following step orsubstep (?next?, ?next step?, ?next substep?
),going back to the preceding step or substep(?previous?, ?previous substep?
), moving to anamed step or substep (?go to step three?, ?goto step ten point two?).?
Visiting non-current steps, either to preview fu-ture steps or recall past ones (?read step four?,?read note before step nine?).
When this func-tionality is invoked, the non-current step is dis-played in a separate window, which is closedon returning to the current step.?
Recording, playing and deleting voice notes(?record voice note?, ?play voice note on stepthree point one?, ?delete voice note on substeptwo?).?
Setting and cancelling alarms (?set alrm forfive minutes from now?, ?cancel alarm at tentwenty one?).?
Showing or hiding pictures (?show the smallwaste water bag?, ?hide the picture?).?
Changing the TTS volume (?increase/decreasevolume?).?
Querying status (?where are we?, ?list voicenotes?, ?list alarms?).?
Undoing and correcting commands (?go back?,?no I said increase volume?, ?I meant stepfour?
).The system consists of a set of modules, writtenin several different languages, which communicatewith each other through the SRI Open Agent Ar-chitecture (Martin et al, 1998).
Speech recogni-tion is carried out using the Nuance Toolkit (Nuance,2003).3 REGULUS 2REGULUS 2 (Rayner et al, 2003; Regulus, 2003)is an Open Source environment that supports effi-cient compilation of typed unification grammars intospeech recognisers.
The basic intent is to providea set of tools to support rapid prototyping of spo-ken dialogue applications in situations where littleor no corpus data exists.
The environment has al-ready been used to build over half a dozen appli-cations with vocabularies of between 100 and 500words.The core functionality provided by the REGU-LUS 2 environment is compilation of typed unifi-cation grammars into annotated context-free gram-mar language models expressed in Nuance Gram-mar Specification Language (GSL) notation (Nu-ance, 2003).
GSL language models can be con-verted into runnable speech recognisers by invokingthe Nuance Toolkit compiler utility, so the net resultis the ability to compile a unification grammar intoa speech recogniser.Experience with grammar-based spoken dialoguesystems shows that there is usually a substantialoverlap between the structures of grammars for dif-ferent domains.
This is hardly surprising, since theyall ultimately have to model general facts about thelinguistic structure of English and other natural lan-guages.
It is consequently natural to consider strate-gies which attempt to exploit the overlap betweendomains by building a single, general grammar validfor a wide variety of applications.
A grammar of thiskind will probably offer more coverage (and hencelower accuracy) than is desirable for any given spe-cific application.
It is however feasible to addressthe problem using corpus-based techniques whichextract a specialised version of the original generalgrammar.REGULUS implements a version of the grammarspecialisation scheme which extends the Explana-tion Based Learning method described in (Rayneret al, 2002).
There is a general unification gram-mar, loosely based on the Core Language Enginegrammar for English (Pulman, 1992), which hasbeen developed over the course of about ten individ-ual projects.
The semantic representations producedby the grammar are in a simplified version of theCore Language Engine?s Quasi Logical Form nota-tion (van Eijck and Moore, 1992).A grammar built on top of the general grammar istransformed into a specialised Nuance grammar inthe following processing stages:1.
The training corpus is converted into a ?tree-bank?
of parsed representations.
This is doneusing a left-corner parser representation of thegrammar.2.
The treebank is used to produce a specialisedgrammar in REGULUS format, using the EBLalgorithm (van Harmelen and Bundy, 1988;Rayner, 1988).3.
The final specialised grammar is compiled intoa Nuance GSL grammar.4 ALTERFALTERF (Rayner and Hockey, 2003) is another OpenSource toolkit, whose purpose is to allow a cleancombination of rule-based and corpus-driven pro-cessing in the semantic interpretation phase.
Thereis typically no corpus data available at the startof a project, but considerable amounts at the end:the intention behind ALTERF is to allow us to shiftsmoothly from an initial version of the system whichis entirely rule-based, to a final version which islargely data-driven.ALTERF characterises semantic analysis as a taskslightly extending the ?decision-list?
classificationalgorithm (Yarowsky, 1994; Carter, 2000).
We startwith a set of semantic atoms, each representing aprimitive domain concept, and define a semanticrepresentation to be a non-empty set of semanticatoms.
For example, in the procedure assistant do-main we represent the utterancesplease speak upshow me the sample syringeset an alarm for five minutes from nowno i said go to the next steprespectively as{increase volume}{show, sample syringe}{set alrm, 5, minutes}{correction, next step}where increase volume, show,sample syringe, set alrm, 5, minutes,correction and next step are semanticatoms.
As well as specifying the permitted semanticatoms themselves, we also define a target modelwhich for each atom specifies the other atoms withwhich it may legitimately combine.
Thus here, forexample, correction may legitimately combinewith any atom, but minutes may only combinewith correction, set alrm or a number.1.Training data consists of a set of utterances, ineither text or speech form, each tagged with its in-tended semantic representation.
We define a set offeature extraction rules, each of which associates anutterance with zero or more features.
Feature ex-traction rules can carry out any type of processing.In particular, they may involve performing speechrecognition on speech data, parsing on text data, ap-plication of hand-coded rules to the results of pars-ing, or some combination of these.
Statistics arethen compiled to estimate the probability p(a | f)of each semantic atom a given each separate featuref , using the standard formulap(a | f) = (Naf + 1)/(Nf + 2)where Nf is the number of occurrences in the train-ing data of utterances with feature f , and N af is thenumber of occurrences of utterances with both fea-ture f and semantic atom a.The decoding process follows (Yarowsky, 1994)in assuming complete dependence between the fea-tures.
Note that this is in sharp contrast with theNaive Bayes classifier (Duda et al, 2000), which as-sumes complete independence.
Of course, neitherassumption can be true in practice; however, as ar-gued in (Carter, 2000), there are good reasons forpreferring the dependence alternative as the betteroption in a situation where there are many featuresextracted in ways that are likely to overlap.We are given an utterance u, to which we wish toassign a representation R(u) consisting of a set ofsemantic atoms, together with a target model com-prising a set of rules defining which sets of seman-1The current system post-processes Alterf semantic atomlists to represent domain dependancies between semanticatoms more directly before passing on the result.
e.g.
(correction, set alrm, 5, minutes) is repack-aged as (correction(set alrm(time(0,5))))tic atoms are consistent.
The decoding process pro-ceeds as follows:1.
Initialise R(u) to the empty set.2.
Use the feature extraction rules and the statis-tics compiled during training to find the set ofall triples ?f, a, p?
where f is a feature associ-ated with u, a is a semantic atom, and p is theprobability p(a | f) estimated by the trainingprocess.3.
Order the set of triples by the value of p, withthe largest probabilities first.
Call the orderedset T .4.
Remove the highest-ranked triple ?f, a, p?
fromT .
Add a to R(u) iff the following conditionsare fulfilled:?
p ?
pt for some pre-specified thresholdvalue pt.?
Addition of a to R(u) results in a setwhich is consistent with the target model.5.
Repeat step (4) until T is empty.Intuitively, the process is very simple.
We justwalk down the list of possible semantic atoms, start-ing with the most probable ones, and add them tothe semantic representation we are building up whenthis does not conflict with the consistency rules inthe target model.
We stop when the atoms suggestedare too improbable, that is, they have probabilies be-low a cut-off threshold.5 Summary and structure of demoWe have described a non-trivial spoken language di-alogue application built using generic Open Sourcetools that combine rule-based and corpus-drivenprocessing.
We intend to demo the system with par-ticular reference to these tools, displaying intermedi-ate results of processing and showing how the cover-age can be rapidly reconfigured in an example-basedfashion.ReferencesG.
Aist, J. Dowding, B.A.
Hockey, and J. Hieronymus.2002.
An intelligent procedure assistant for astro-naut training and support.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (demo track), Philadelphia, PA.D.
Carter.
2000.
Choosing between interpretations.
InM.
Rayner, D. Carter, P. Bouillon, V. Digalakis, andM.
Wire?n, editors, The Spoken Language Translator.Cambridge University Press.R.O.
Duda, P.E.
Hart, and H.G.
Stork.
2000.
PatternClassification.
Wiley, New York.D.
Martin, A. Cheyer, and D. Moran.
1998.
Buildingdistributed software systems with the open agent ar-chitecture.
In Proceedings of the Third InternationalConference on the Practical Application of IntelligentAgents and Multi-Agent Technology, Blackpool, Lan-cashire, UK.Nuance, 2003. http://www.nuance.com.
As of 25 Febru-ary 2003.S.G.
Pulman.
1992.
Syntactic and semantic process-ing.
In H. Alshawi, editor, The Core Language En-gine, pages 129?148.
MIT Press, Cambridge, Mas-sachusetts.M.
Rayner and B.A.
Hockey.
2003.
Transparent com-bination of rule-based and data-driven approaches in aspeech understanding architecture.
In Proceedings ofthe 10th EACL, Budapest, Hungary.M.
Rayner, B.A.
Hockey, and J. Dowding.
2002.
Gram-mar specialisation meets language modelling.
In Pro-ceedings of the 7th International Conference on Spo-ken Language Processing (ICSLP), Denver, CO.M.
Rayner, B.A.
Hockey, and J. Dowding.
2003.
Anopen source environment for compiling typed unifica-tion grammars into speech recognisers.
In Proceed-ings of the 10th EACL (demo track), Budapest, Hun-gary.M.
Rayner.
1988.
Applying explanation-based general-ization to natural-language processing.
In Proceedingsof the International Conference on Fifth GenerationComputer Systems, pages 1267?1274, Tokyo, Japan.Regulus, 2003. http://sourceforge.net/projects/regulus/.As of 24 April 2003.J.
van Eijck and R. Moore.
1992.
Semantic rules forEnglish.
In H. Alshawi, editor, The Core LanguageEngine, pages 83?116.
MIT Press.T.
van Harmelen and A. Bundy.
1988.
Explanation-based generalization = partial evaluation (researchnote).
Artificial Intelligence, 36:401?412.D.
Yarowsky.
1994.
Decision lists for lexical ambiguityresolution.
In Proceedings of the 32nd Annual Meet-ing of the Association for Computational Linguistics,pages 88?95, Las Cruces, New Mexico.
