Automatic Measuring of English Language Proficiency using MTEvaluation TechnologyKeiji YasudaATR Spoken Language TranslationResearch LaboratoriesDepartment of SLR2-2-2 Hikaridai,?Keihanna Science City?Kyoto 619-0288 Japankeiji.yasuda@atr.jpFumiaki SugayaKDDI R&D Laboratories2-1-15, Ohara, Kamifukuoka-city,Saitama, 356-8502, Japanfsugaya@kddilabs.jpEiichiro SumitaATR Spoken Language TranslationResearch LaboratoriesDepartment of NLR2-2-2 Hikaridai,?Keihanna Science City?Kyoto 619-0288 Japaneiichiro.sumita@atr.jpToshiyuki TakezawaATR Spoken Language TranslationResearch LaboratoriesDepartment of SLR2-2-2 Hikaridai,?Keihanna Science City?Kyoto 619-0288 Japantoshiyuki.takezawa@atr.jpGenichiro KikuiATR Spoken Language TranslationResearch LaboratoriesDepartment of SLR2-2-2 Hikaridai,?Keihanna Science City?Kyoto 619-0288 Japangenichiro.kikui@atr.jpSeiichi YamamotoATR Spoken Language TranslationResearch Laboratories2-2-2 Hikaridai,?Keihanna Science City?Kyoto 619-0288 Japanseiichi.yamamoto@atr.jpAbstractAssisting in foreign language learning is one ofthe major areas in which natural language pro-cessing technology can contribute.
This paperproposes a computerized method of measuringcommunicative skill in English as a foreign lan-guage.
The proposed method consists of twoparts.
The first part involves a test sentenceselection part to achieve precise measurementwith a small test set.
The second part is the ac-tual measurement, which has three steps.
Stepone asks proficiency-known human subjects totranslate Japanese sentences into English.
Steptwo gauges the match between the translationsof the subjects and correct translations basedon the n-gram overlap or the edit distance be-tween translations.
Step three learns the rela-tionship between proficiency and match.
By re-gression it finds a straight-line fitting for thescatter plot representing the proficiency andmatches of the subjects.
Then, it estimates pro-ficiency of proficiency-unknown users by usingthe line and the match.
Based on this approach,we conducted experiments on estimating theTest of English for International Communica-tion (TOEIC) score.
We collected two sets ofdata consisting of English sentences translatedfrom Japanese.
The first set consists of 330 sen-tences, each translated to English by 29 subjectswith varied English proficiency.
The second setconsists of 510 sentences translated in a similarmanner by a separate group of 18 subjects.
Wefound that the estimated scores correlated withthe actual scores.1 IntroductionFor effective second language learning, it is ab-solutely necessary to test proficiency in the sec-ond language.
This testing can help in selectingeducational materials before learning, checkinglearners?
understanding after learning, and soon.To make learning efficient, it is important toachieve testing with a short turnaround time.Computer-based testing is one solution for this,and several kinds of tests have been developed,including CASEC (CASEC, 2004) and TOEFL-CBT (TOEFL, 2004).
However, these tests aremainly based on cloze testing or multiple-choicequestions.
Consequently, they require labourcosts for expert examination designers to makethe questions and the alternative ?detractor?answers.In this paper, we propose a method for the au-tomatic measurement of English language pro-ficiency by applying automatic evaluation tech-niques.
The proposed method selects adequatetest sentences from an existing corpus.
Then,it automatically evaluates the translations oftest sentences done by users.
The core tech-nology of the proposed method, i.e., the auto-matic evaluation of translations, was developedin research aiming at the efficient developmentof Machine Translation (MT) technology (Su etal., 1992; Papineni et al, 2002; NIST, 2002).In the proposed method, we apply these MTevaluation technologies to the measurement ofhuman English language proficiency.
The pro-posed method focuses on measuring the commu-nicative skill of structuring sentences, which isindispensable for writing and speaking.
It doesnot measure elementary capabilities includingvocabulary or grammar.
This method also pro-poses a test sentence selection scheme to enableefficient testing.Section 2 describes several automatic evalua-tion methods applied to the proposed method.Section 3 introduces the proposed evaluationscheme.
Section 4 shows the evaluation resultsobtained by the proposed method.
Section 5concludes the paper.2 MT Evaluation TechnologiesIn this section, we briefly describe automaticevaluation methods of translation.
These meth-ods were proposed to evaluate MT output, butthey are applicable to translation by humans.All of these methods are based on the sameidea, that is, to compare the target transla-tion for evaluation with high-quality referencetranslations that are usually done by skilledtranslators.
Therefore, these methods require acorpus of high-quality human reference transla-tions.
We call these translations as ?references?.2.1 DP-based MethodThe DP score between a translation output andreferences can be calculated by DP matching(Su et al, 1992; Takezawa et al, 1999).
First,we define the DP score between sentence (i.e.,word array) Wa and sentence Wb by the follow-ing formula.SDP (Wa,Wb) = T ?
S ?
I ?DT (1)where T is the total number of words in Wa, S isthe number of substitution words for comparingWa to Wb, I is the number of inserted words forcomparing Wa to Wb, and D is the number ofdeleted words for comparing Wa to Wb.Using Equation 1, (Si(j)), that is, the testsentence unit DP-score of the translation of testsentence j done by subject i, can be calculatedby the following formula.SDPi(j) =maxk=1 to Nref{SDP (Wref(k)(j),Wsub(i)(j)), 0}(2)where Nref is the number of references,Wref(k)(j) is the k-th reference of the test sen-tence j, and Wsub(i)(j) is the translation of thetest sentence j done by subject i.Finally, SDPi , which is the test set unit DP-score of subject i, can be calculated by the fol-lowing formula.SDPi =1NsentNsent?j=1SDPi(j) (3)where Nsent is the number of test sentences.2.2 N-gram-based MethodPapineni et al (2002) proposed BLEU, which isan automatic method for evaluating MT qual-ity using N -gram matching.
The National Insti-tute of Standards and Technology also proposedan automatic evaluation method called NIST(2002), which is a modified method of BLEU.In this research we use two kinds of units toapply BLEU and NIST.
One is a test sentenceunit and the other is a test set unit.
The unit ofutterance corresponds to the unit of ?segment?in the original BLEU and NIST studies (Pap-ineni et al, 2002; NIST, 2002).Equation 4 is the test sentence unit BLEUscore formulation of the translation of test sen-tence j done by subject i.SBLEUi (j) =exp{ N?n=1wn log(pn)?max(L?refLsys ?
1, 0)}(4)wherepn =?C?{Candidates}?n?gram?
{C} Countclip (n?gram)?C?{Candidates}?n?gram?
{C} Count(n?gram)wn = N?1andL?ref = the number of words in the referencetranslation that is closest in length to thetranslation being scoredLsys = the number of words in the transla-tion being scoredEquation 5 is the test sentence unit NISTscore formulation of the translation of test sen-tence j done by subject i.SNISTi(j) =?Nn=1{?all w1...wn in sys outputinfo(w1...wn)?all w1...wn in sys output(1)}?exp{?
log2[min(LsysLref , 1)]}(5)whereinfo(w1 .
.
.
wn) =log2( the number of occurence of w1...wn?1the number of occurence of w1...wn)Lref = the average number of words in a ref-erence translation, averaged over all refer-ence translationsLsys = the number of words in the transla-tion being scoredand ?
is chosen to make the brevity penalty fac-tor=0.5 when the number of words in the sys-tem translation is 2/3 of the average numberof words in the reference translation.
For Equa-tions 4 and 7, N indicates the maximum n-gramlength.
In this research we set N to 4 for BLEUand to 5 for NIST.We may consider the unit of the test set cor-responding to the unit of ?document?
or ?sys-tem?
in BLEU and NIST.
However, we use for-mulations for the test set unit scores that aredifferent from those of the original BLEU andNIST.Calculate correlationbetween TOEIC score andsentence unit automatic scoreReferences translatedby bilingualsEnglish writing byproficiency-knownhuman subjectsEnglish sentencesby proficiencyJapanese test setAutomatic evaluation(sentence unit evaluation)CorpusSelect test sentencesbased on correlationFigure 1: Flow of Test Set SelectionThe test set unit scores of BLEU and NISTare calculated by Equations 6 and 7.SBLEUi =1NsentNsent?j=1SBLEUi(j) (6)SNISTi =1NsentNsent?j=1SNISTi(j) (7)3 The Proposed MethodThe proposed method described in this paperconsists of two parts.
One is the test set selec-tion part and the other is the actual measure-ment part.
The measurement part is dividedinto two phases: a parameter-estimation phaseand a testing phase.
Here, we use the term ?sub-jects?
to refer to the human subjects in the testset selection part and the parameter-estimationphase of the measurement part; we use ?users?to refer to the humans in the testing phase ofthe measurement part.Regression analysis usingproficiency and automaticscoresReferences translatedby bilingualsEnglish writing byproficiency-knownhuman subjectsEnglish sentencesby proficiencyJapanese test setRegressioncoefficientAutomatic evaluation(Test set unit evaluation)English writing by a userAutomatic evaluationEstimation of EnglishproficiencyEnglish sentencesAutomatic scoreEnglishproficiency?Testing phase?Corpus?Parameter-estimation phase?Figure 2: Flow of English Proficiency MeasurmentWe employ the Test of English for Interna-tional Communication (TOEIC, 2004) as an ob-jective measure of English proficiency.3.1 Test Sentence Selection MethodFigure 1 shows the flow of the test sentence se-lection.
We first calculate the test sentenceunit automatic score by using Equation 2, 4 or5 for each test sentence and subject.
Second,for each test sentence, we calculate the correla-tion between the automatic scores and subjects?TOEIC scores.
Finally, using the above results,we choose the test sentences that give high cor-relation.3.2 Method of Measuring EnglishProficiencyFigure 2 shows the flow of measuring Englishproficiency.
In the parameter-estimation phase,for each subject, we first calculate the test setunit automatic score by using Equation 3, 6 or7.
Next, we apply regression analysis using theautomatic scores and subjects?
TOEIC scores.In the testing phase, we calculate a user?sTOEIC score using the automatic score of theuser and the regression line calculated in theparameter-estimation phase.4 Experiments4.1 Experimental Conditions4.1.1 Test setsFor the experiments, we employ two differ-ent test sets.
One is BTEC (Basic TravelExpression Corpus) (Takezawa et al, 2002)and the other is SLTA1 (Takezawa, 1999).Both BTEC and SLTA1 are parts of bilingualcorpora that have been collected for researchon speech translation systems.
However, theyhave different features.
A detailed analysisof these corpora was done by Kikui et al(2003).
Here, we briefly explain these test sets.In this study, we use the Japanese side as atest set and the English side as a reference forautomatic evaluation.BTECBTEC was designed to cover expressions forevery potential subject in travel conversation.This test set was collected by investigating?phrasebooks?
that contain Japanese/Englishsentence pairs that experts consider useful fortourists traveling abroad.
One sentence con-tains 8 words on average.
The test set for thisexperiment consists of 510 sentences from theBTEC corpus.The total number of examinees is 18, andthe range of their TOEIC scores is between the400s and 900s.
Every hundred-point range has3 examinees.SLTA1SLTA1 consists of 330 sentences in 23 conver-sations from the ATR bilingual travel conver-sation database (Takezawa, 1999).
One sen-tence contains 13 words on average.
This corpuswas collected by simulated dialogues betweenJapanese and English speakers through a pro-fessional interpreter.
The topics of the conver-sations are mainly hotel conversations, such asreservations, enquiries and so on.The total number of examinees is 29, and therange of their TOEIC score is between the 300sand 800s.
Excluding the 600s, every hundred-point range has 5 examinees.4.1.2 ReferenceFor the automatic evaluation, we collected 16references for each test sentence.
One of themis from the English side of the test set, and theremaining 15 were translated by 5 bilinguals (3references by 1 bilingual).4.2 Experimental Results4.2.1 Experimental Results of Test SetSelectionFigures 3 and 4 show the correlation betweenthe test sentence unit automatic score and thesubjects?
TOEIC score.
Here, the automaticscore is calculated using Equation 2, 4 or 5.
Fig-ure 3 shows the results on BTEC, and Fig.
4shows the results on SLTA1.
In these fig-ures, the ordinate represents the correlation.The filled circles indicate the results using theDP-based automatic evaluation method.
Thegray circles indicate the results using BLEU.The empty circles indicate the results usingNIST.
Looking at these figures, we find thatthe three automatic evaluation methods showa similar tendency.
Comparing BTEC andSLTA1, BTEC contains more cumbersome testsentences.
In BTEC, about 20% of the test sen-tences give a correlation of less than 0.
Mean-while, in the SLTA1, this percentage is about10%.-1-0.8-0.6-0.4-0.200.20.40.60.810 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510Test sentence (sorted by correlation)CorrelationDPBLEUNISTFigure 3: Correlation between test sentence unitautomatic scores and subjects?
TOEIC scores(BTEC)Table 1 shows examples of low-correlated testsentences.
As shown in the table, BTEC con-tains more short and frequently used expres-sions than does SLTA1.
This kind of expres-sion is thought to be too easy for testing, sothis low-correlation phenomenon is thought tooccur.
SLTA1 still contains a few sentences ofthis kind (?Example 1?
of SLTA1 in the ta-ble).
Additionally, there is another contributingfactor explaining the low correlation in SLTA1.Looking at ?Example 2?
of SLTA1 in the ta-ble, this expression is not very easy to translate.For this test sentence, several expressions canbe produced as an English translation.
Thus,automatic evaluation methods cannot evaluatecorrectly due to the insufficient variety of ref-erences.
Considering these results, this methodcan remove inadequate test sentences due notonly to the easiness of the test sentence butalso to the difficulty of the automatic evalua-tion.
Figures 5 and 6 show the relationshipbetween the number of test sentences and cor-relation.
This correlation is calculated betweenthe test set unit automatic scores and the sub-jects?
TOEIC scores.
Here, the automatic scoreis calculated using Equation 3, 6 or 7.
Figure5 shows the results on BTEC, and Fig.
6 showsthe results on SLTA1.In these figures, the abscissa represents thenumber of test sentences, i.e., Nsent in Equa-tions 3, 6 and 7, and the ordinate representsthe correlation.
Definitions of the circles arethe same as those in the previous figure.
Here,the test sentence selection is based on the cor-relation shown in Figs.
3 and 4.Comparing Fig.
5 to Fig.
6, in the case ofTable 1: Example of low-correlated test sentencesJapanese EnglishExample 1??????
?Good night.Example 2???????????
?Can I see a menu, please?Example 1?????????????????
?Yes, with my Mastercard pleaseExample 2??????????????????????????????????????????????
?I wish I could take that but we have a limited budget sohow much will that cost?SLTA1BTEC-1-0.8-0.6-0.4-0.200.20.40.60.810 30 60 90 120 150 180 210 240 270 300 330Test sentence (sorted by correlation)CorrelationDPBLEUNISTFigure 4: Correlation between test sentence unitautomatic scores and subjects?
TOEIC scores(SLTA1)0.60.650.70.750.80.850.90.9510 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510Number of test sentencesCorrelationDPBLEUNISTFigure 5: Correlation between test set unitautomatic scores and subjects?
TOEIC scores(BTEC)using the full test set (510 test sentences forBTEC, 330 test sentences for SLTA1), the cor-relation of BTEC is lower than that of SLTA1.As we mentioned above, the ratio of the low-correlated test sentences in BTEC is higher thanthat of SLTA1 (See Figs.
3 and 4).
This issueis thought to cause a decrease in the correlationshown in Fig.
5.
However, by applying the se-0.60.650.70.750.80.850.90.9510 30 60 90 120 150 180 210 240 270 300 330Number of test sentencesCorrelationDPBLEUNISTFigure 6: Correlation between test set unitautomatic scores and subjects?
TOEIC scores(SLTA1)501001502002503003500 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510Number of test sentencesStandarderrorDPBLEUNISTFigure 7: Standard error (BTEC)lection based on sentence unit correlation, theseobstructive test sentences can be removed.
Thispermits the selection of high-correlated small-sized test sets.
In these figures, the highest cor-relations are around 0.95.4.2.2 Experimental Results of EnglishProficiency MeasurementFor the experiments on English proficiency mea-surement, we carried out a leave-one-out crossvalidation test.
The leave-one-out cross valida-501001502002503003500 30 60 90 120 150 180 210 240 270 300 330Number of test sentencesStandarderrorDPBLEUNISTFigure 8: Standard error (SLTA1)tion test is conducted not only for the measure-ment of the English proficiency but also for thetest set selection.To evaluate the proficiency measurement bythe proposed method, we calculate the standarderror of the results of a leave-one-out cross val-idation test.
The following formula is the defi-nition of the standard error.
?E =????
1NuserNuser?i=1(Ti ?Ai)2 (8)where Nuser is the number of users, Ti is theactual TOEIC score of user i, and Ai is user i?sestimated TOEIC score by using the proposedmethod.Figures 7 and 8 show the relationship betweenthe number of test sentences and the standarderror.In these figures, the abscissa represents thenumber of test sentences, and the ordinate rep-resents the standard error.
Definitions of thecircles are the same as in the previous figure.Here, the test sentence selection is based on thecorrelation shown in Figs.
3 and 4.Looking at Figs.
7 and 8, we can observe dif-ferences between the standard errors of BTECand SLTA1.
This is thought to be due to thedifference of the number of subjects in the ex-periments (for the leave-one-out cross valida-tion test, 17 subjects with BTEC and 28 sub-jects with SLTA1).
Even though these wereclosed experiments, the results in Figs.
5 and6 show an even higher correlation with BTECthan with SLTA1 at the highest point.
There-fore, there is room for improvement by increas-ing the number of subjects with BTEC.In the test using 30 to 60 test sentences inFigs.
7 and 8, the standard errors are muchsmaller than in the test using the full test set(510 test sentences for BTEC, 330 test sentencesfor SLTA1).
These results imply that the testset selection works very well and that it enablesprecise testing using a smaller size test set.5 ConclusionWe proposed an automatic measurementmethod for English language proficiency.
Theproposed method applies automatic MT evalu-ation to measure human English language pro-ficiency.
This method focuses on measuring thecommunicative skill of structuring sentences,which is indispensable in writing and speaking.However, it does not measure elementary capa-bilities such as vocabulary and grammar.
Themethod also involves a new test sentence selec-tion scheme to enable efficient testing.In the experiments, we used TOEIC as an ob-jective measure of English language proficiency.We then applied some currently available auto-matic evaluation methods: BLEU, NIST and aDP-based method.
We carried out experimentson two test sets: BTEC and SLTA1.
Accord-ing to the experimental results, the proposedmethod gave a good measurement result on asmall-sized test set.
The standard error of mea-surement is around 120 points on the TOEICscore with BTEC and less than 100 TOEICpoints score with SLTA1.
In both cases, theoptimum size of the test set is 30 to 60 test sen-tences.The proposed method still needs humanlabour to make the references.
To obtain higherportability, we will apply an automatic para-phrase scheme (Finch et al, 2002; Shimohataand Sumita, 2002) to make the references auto-matically.6 AcknowledgementsThe research reported here was supported inpart by a contract with the National Instituteof Information and Communications Technol-ogy entitled ?A study of speech dialogue trans-lation technology based on a large corpus?.ReferencesCASEC.
2004.
Computer AssessmentSystem for English Communication.http://www.ets.org/toefl/.A.
Finch, T. Watanabe, and E. Sumita.
2002.?Paraphrasing by Statistical Machine Trans-lation?.
In Proceedings of the 1st Forum onInformation Technology (FIT2002), volumeE-53, pages 187?188.G.
Kikui, E. Sumita, T. Takezawa, andS.
Yamamoto.
2003.
?Creating Corpora forSpeech-to-Speech Translation?.
In Proceed-ings of EUROSPEECH, pages 381?384.NIST.
2002.
Automatic Evaluationof Machine Translation Quality Us-ing N-gram Co-Occurence Statistics.http://www.nist.gov/speech/tests/mt/mt2001/resource/.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.
Bleu: a method for auto-matic evaluation of machine translation.
InProceedings of the 40th Annual Meeting ofthe Association for Computational Linguis-tics (ACL), pages 311?318.M.
Shimohata and E. Sumita.
2002.
?Auto-matic Paraphrasing Based on Parallel Corpusfor Normalization?.
In Proceedings of Inter-national Conference on Language Resourcesand Evaluation (LREC), pages 453?457.K.-Y.
Su, M.-W. Wu, and J.-S. Chang.
1992.A new quantitative quality measure for ma-chine translation systems.
In Proceedings ofthe 14th International Conference on Com-putational Linguistics(COLING), pages 433?439.T.
Takezawa, F. Sugaya, A. Yokoo, and S. Ya-mamoto.
1999.
A new evaluation method forspeech translation systems and a case studyon ATR-MATRIX from Japanese to English.In Proceeding of Machine Translation Summit(MT Summit), pages 299?307.T.
Takezawa, E. Sumita, F. Sugaya, H. Ya-mamoto, and S. Yamamoto.
2002.
?Toward aBroad-Coverage Bilingual Corpus for SpeechTranslation of Travel Conversations in theReal World?.
In Proceedings of InternationalConference on Language Resources and Eval-uation (LREC), pages 147?152.T.
Takezawa.
1999.
Building a bilingual travelconversation database for speech translationresearch.
In Proceedings of the 2nd Inter-national Workshop on East-Asian LanguageResources and Evaluation ?
Oriental CO-COSDA Workshop ?99 ?, pages 17?20.TOEFL.
2004.
Test of English as a ForeignLanguage.
http://www.ets.org/toefl/.TOEIC.
2004.
Test of Englishfor International Communication.http://www.ets.org/toeic/.
