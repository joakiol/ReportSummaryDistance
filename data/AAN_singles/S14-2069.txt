Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 400?405,Dublin, Ireland, August 23-24, 2014.LIPN: Introducing a new Geographical Context Similarity Measure and aStatistical Similarity Measure Based on the Bhattacharyya CoefficientDavide Buscaldi, Jorge J.
Garc?
?a Flores, Joseph Le Roux, Nadi TomehLaboratoire d?Informatique de Paris Nord, CNRS (UMR 7030)Universit?e Paris 13, Sorbonne Paris Cit?e, Villetaneuse, France{buscaldi,jgflores,joseph.le-roux,nadi.tomeh}@lipn.univ-paris13.frBel?em Priego SanchezLaboratoire LDI (Lexique, Dictionnaires, Informatique)Universit?e Paris 13, Sorbonne Paris Cit?e, Villetaneuse, FranceLKE, FCC, BUAP, San Manuel, Puebla, Mexicobelemps@gmail.comAbstractThis paper describes the system used bythe LIPN team in the task 10, Multilin-gual Semantic Textual Similarity, at Sem-Eval 2014, in both the English and Span-ish sub-tasks.
The system uses a sup-port vector regression model, combiningdifferent text similarity measures as fea-tures.
With respect to our 2013 partici-pation, we included a new feature to takeinto account the geographical context anda new semantic distance based on theBhattacharyya distance calculated on co-occurrence distributions derived from theSpanish Google Books n-grams dataset.1 IntroductionAfter our participation at SemEval 2013 withLIPN-CORE (Buscaldi et al., 2013) we found thatgeography has an important role in discriminatingthe semantic similarity of sentences (especially inthe case of newswire).
If two events happened ina different location, their semantic relatedness isusually low, no matter if the events are the same.Therefore, we worked on a similarity measure ableto capture the similarity between the geographiccontexts of two sentences.
We tried also to rein-force the semantic similarity features by introduc-ing a new measure that calculates word similari-ties on co-occurrence distributions extracted fromGoogle Books bigrams.
This measure was intro-duced only for the Spanish runs, due to time con-straints.
The regression model used to integratethe features was the ?-Support Vector RegressionThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence de-tails:http://creativecommons.org/licenses/by/4.0/model (?-SVR) (Sch?olkopf et al., 1999) imple-mentation provided by LIBSVM (Chang and Lin,2011), with a radial basis function kernel with thestandard parameters (?
= 0.5).
We describe allthe measures in Section 2; the results obtained bythe system are detailed in Section 3.2 Similarity MeasuresIn this section we describe the measures used asfeatures in our system.
The description of mea-sures already used in our 2013 participation is lessdetailed than the description of the new ones.
Ad-ditional details on the measures may be found in(Buscaldi et al., 2013).
When POS tagging andNE recognition were required, we used the Stan-ford CoreNLP1for English and FreeLing23.1 forSpanish.2.1 WordNet-based Conceptual SimilarityThis measure has been introduced in order to mea-sure similarities between concepts with respect toan ontology.
The similarity is calculated as fol-lows: first of all, words in sentences p and q arelemmatised and mapped to the related WordNetsynsets.
All noun synsets are put into the set ofsynsets associated to the sentence, Cpand Cq, re-spectively.
If the synsets are in one of the otherPOS categories (verb, adjective, adverb) we lookfor their derivationally related forms in order tofind a related noun synset: if there exists one, weput this synset in Cp(or Cq).
No disambigua-tion process is carried out, so we take all possiblemeanings into account.Given Cpand Cqas the sets of concepts con-tained in sentences p and q, respectively, with1http://www-nlp.stanford.edu/software/corenlp.shtml2http://nlp.lsi.upc.edu/freeling/400|Cp| ?
|Cq|, the conceptual similarity between pand q is calculated as:ss(p, q) =?c1?Cpmaxc2?Cqs(c1, c2)|Cp|where s(c1, c2) is a conceptual similarity mea-sure.
Concept similarity can be calculated in dif-ferent ways.
We used a variation of the Wu-Palmerformula (Wu and Palmer, 1994) named ?Proxi-Genea3?, introduced by (Dudognon et al., 2010),which is inspired by the analogy between a familytree and the concept hierarchy in WordNet.
TheProxiGenea3 measure is defined as:s(c1, c2) =11 + d(c1) + d(c2)?
2 ?
d(c0)where c0is the most specific concept that ispresent both in the synset path of c1and c2(that is,the Least Common Subsumer or LCS).
The func-tion returning the depth of a concept is noted withd.2.2 IC-based SimilarityThis measure has been proposed by (Mihalcea etal., 2006) as a corpus-based measure which usesResnik?s Information Content (IC) and the Jiang-Conrath (Jiang and Conrath, 1997) similarity met-ric.
This measure is more precise than the oneintroduced in the previous subsection because ittakes into account also the importance of conceptsand not only their relative position in the hierarchy.We refer to (Buscaldi et al., 2013) and (Mihalceaet al., 2006) for a detailed description of the mea-sure.
The idf weights for the words were calcu-lated using the Google Web 1T (Brants and Franz,2006) frequency counts, while the IC values usedare those calculated by Ted Pedersen (Pedersen etal., 2004) on the British National Corpus3.2.3 Syntactic DependenciesThis measure tries to capture the syntactic simi-larity between two sentences using dependencies.Previous experiments showed that converting con-stituents to dependencies still achieved best resultson out-of-domain texts (Le Roux et al., 2012), sowe decided to use a 2-step architecture to obtainsyntactic dependencies.
First we parsed pairs ofsentences with the LORG parser4.
Second we con-3http://www.d.umn.edu/ tpederse/similarity.html4https://github.com/CNGLdlab/LORG-Releaseverted the resulting parse trees to Stanford depen-dencies5.Given the sets of parsed dependencies DpandDq, for sentence p and q, a dependency d ?
Dxis a triple (l, h, t) where l is the dependency label(for instance, dobj or prep), h the governor andt the dependant.
The similarity measure betweentwo syntactic dependencies d1= (l1, h1, t1) andd2= (l2, h2, t2) is the levenshtein distance be-tween the labels l1and l2multiplied by the aver-age of idfh?
sWN(h1, h2) and idft?
sWN(t1, t2),where idfhand idftare the inverse document fre-quencies calculated on Google Web 1T for thegovernors and the dependants (we retain the max-imum for each pair), respectively, and sWNis cal-culated using formula ??.
NOTE: This measurewas used only in the English sub-task.2.4 Information Retrieval-based SimilarityLet us consider two texts p and q, an IR system Sand a document collection D indexed by S. Thismeasure is based on the assumption that p and qare similar if the documents retrieved by S for thetwo texts, used as input queries, are ranked simi-larly.Let be Lp= {dp1, .
.
.
, dpK} and Lq={dq1, .
.
.
, dqK}, dxi?
D the sets of the top Kdocuments retrieved by S for texts p and q, respec-tively.
Let us define sp(d) and sq(d) the scores as-signed by S to a document d for the query p andq, respectively.
Then, the similarity score is calcu-lated as:simIR(p, q) = 1??d?Lp?Lq?(sp(d)?sq(d))2max(sp(d),sq(d))|Lp?
Lq|if |Lp?
Lq| 6= ?, 0 otherwise.For the participation in the English sub-task weindexed a collection composed by the AQUAINT-26and the English NTCIR-87document collec-tions, using the Lucene84.2 search engine withBM25 similarity.
The Spanish index was cre-ated using the Spanish QA@CLEF 2005 (agenciaEFE1994-95, El Mundo 1994-95) and multiUN5We used the default built-in converter provided with theStanford Parser (2012-11-12 revision).6http://www.nist.gov/tac/data/data desc.html#AQUAINT-27http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8-databases.php8http://lucene.apache.org/core401(Eisele and Chen, 2010) collections.
The K valuewas set to 70 after a study detailed in (Buscaldi,2013).2.5 N-gram Based SimilarityThis measure tries to capture the fact that similarsentences have similar n-grams, even if they arenot placed in the same positions.
The measure isbased on the Clustered Keywords Positional Dis-tance (CKPD) model proposed in (Buscaldi et al.,2009) for the passage retrieval task.The similarity between a text fragment p andanother text fragment q is calculated as:simngrams(p, q) =?
?x?Qh(x, P )?ni=1wid(x, xmax)Where P is the set of the heaviest n-grams in pwhere all terms are also contained in q; Q is theset of all the possible n-grams in q, and n is thetotal number of terms in the longest sentence.
Theweights for each term wiare calculated as wi=1 ?log(ni)1+log(N)where niis the frequency of termtiin the Google Web 1T collection, and N is thefrequency of the most frequent term in the GoogleWeb 1T collection.
The weight for each n-gram(h(x, P )), with |P | = j is calculated as:h(x, P ) ={?jk=1wkif x ?
P0 otherwiseThe function d(x, xmax) determines the minimumdistance between a n-gram x and the heaviest onexmaxas the number of words between them.2.6 Geographical Context SimilarityWe observed that in many sentences, especiallythose extracted from news corpora, the compati-bility of the geographic context between the sen-tences is an important clue to determine if the sen-tences are related or not.
This measure tries tomeasure if the two sentences refer to events thattook place in the same geographical area.
We builta database of geographically-related entities, usinggeo-WordNet (Buscaldi and Rosso, 2008) and ex-panding it with all the synsets that are related to ageographically grounded synset.
This implies thatalso adjectives and verbs may be used as clues forthe identification of the geographical context of asentence.
For instance, ?Afghan?
is associated to?Afghanistan?, ?Sovietize?
to ?Soviet Union?, etc.The Named Entities of type PER (Person) are alsoused as clues: we use Yago9to check whether theNE corresponds to a famous leader or not, and inthe affirmative case we include the related nationto the geographical context of the sentence.
For in-stance, ?Merkel?
is mapped to ?Germany?.
GivenGpand Gqthe sets of places found in sentences pand q, respectively, the geographical context simi-larity is calculated as follows:simgeo(p, q) = 1?logK??
?1 +?x?Gpminy?Gqd(x, y)max(|Gp|, |Gq|)??
?Where d(x, y) is the spherical distance in Km.
be-tween x and y, and K is a normalization factor setto 10000 Km.
to obtain similarity values between1 and 0.2.7 2-grams ?Spectral?
DistanceThis measure is used to calculate the seman-tic similarity of two words on the basis of theircontext, according to the distributional hypothe-sis.
The measure exploits bi-grams in the GoogleBooks n-gram collection10and is based on the dis-tributional hypothesis, that is, ?words that tend toappear in similar contexts are supposed to havesimilar meanings?.
Given a word w, we calcu-late the probability of observing a word x know-ing that it is preceded by w as p(x|w) = p(w ?x)/p(w) = c(?wx?)/c(?w?
), where c(?wx?)
isthe number of bigrams ?w x?
observed in GoogleBooks (counting all publication years) 2-gramsand c(?w?)
is the number of occurrences of w ob-served in Google Books 1-grams.
We calculatealso the probability of observing a word y know-ing that it is followed by w as p(y|w) = p(w ?y)/p(w) = c(?yw?)/c(?w?).
In such a way, wemay obtain for a word witwo probability distri-butions Dwipand Dwifthat can be compared to thedistributions obtained in the same way for anotherword wj.
Therefore, we calculate the distance oftwo words comparing the distribution probabilitiesbuilt in this way, using the Bhattacharyya coeffi-cient:9http://www.mpi-inf.mpg.de/yago-naga/yago/10https://books.google.com/ngrams/datasets402sf(wi, wj) = ?
log(?x?X?Dwif(x) ?Dwjf(x))sp(wi, wj) = ?
log(?x?X?Dwip(x) ?Dwjp(x))the resulting distance between wiand wjis cal-culated as the average between sf(wi, wj) andsp(wi, wj).
All words in sentence p are comparedto the words of sentence q using this similarityvalue.
The words that are semantically closer arepaired; if a word cannot be paired (average dis-tance with any of the words in the other sentence> 10), then it is left unpaired.
The value used asthe final feature is the averaged sum of all distancescores.2.8 Other MeasuresIn addition to the above text similarity measures,we used also the following common measures:CosineCosine distance calculated betweenp = (wp1, .
.
.
, wpn) and q = (wq1, .
.
.
, wqn), thevectors of tf.idf weights associated to sentencesp and q, with idf values calculated on Google Web1T.Edit DistanceThis similarity measure is calculated using theLevenshtein distance on characters between thetwo sentences.Named Entity OverlapThis is a per-class overlap measure (in this way,?France?
as an Organization does not match?France?
as a Location) calculated using the Dicecoefficient between the sets of NEs found, respec-tively, in sentences p and q.3 Results3.1 SpanishIn order to train the Spanish model, we trans-lated automatically all the sentences in the EnglishSemEval 2012 and 2013 using Google Translate.We also built a corpus manually using definitionsfrom the RAE11(Real Academia Espa?nola de laLengua).
The definitions were randomly extractedand paired at different similarity levels (taking into11http://www.rae.es/account the Dice coefficient calculated on the def-initions bag-of-words).
Three annotators gave in-dependently their similarity judgments on thesepaired definitions.
A total of 200 definitions wereannotated for training.
The official results for theSpanish task are shown in Table 1.
In Figure 1 weshow the results obtained by taking into accounteach individual feature as a measure of similaritybetween texts.
These results show that the combi-nation was always better than the single features(as expected), and the feature best able to capturesemantic similarity alone was the cosine distance.In Table 2 we show the results of the ablationtest, which shows that the features that most con-tributed to improve the results were the IR-basedsimilarity for the news dataset and the cosine dis-tance for the Wikipedia dataset.
The worst featurewas the NER overlap (not taking into account itwould have allowed us to gain 2 places in the finalrankings).Wikipedia News OverallLIPN-run1 0.65194 0.82554 0.75558LIPN-run2 0.71647 0.8316 0.7852LIPN-run3 0.71618 0.80857 0.77134Table 1: Spanish results (Official runs).The differences between the three submit-ted runs are only in the training set used.LIPN-run1 uses all the training data availabletogether, LIPN-run3 uses a training set com-posed by the translated news for the news datasetand the RAE training set for the Wikipedia dataset;finally, the best run LIPN-run2 uses the sametraining sets of run3 together to build a singlemodel.3.2 EnglishOur participation in the English task was ham-pered by some technical problems which did notallow us to complete the parsing of the tweet datain time.
As a consequence of this and some er-rors in the scripts launched to finalize the experi-ments, the submitted results were incomplete andwe were able to detect the problem only after thesubmission.
We show in Table 3 the official re-sults of run1 with the addition of the results on theOnWN dataset calculated after the participation tothe task.403Figure 1: Spanish task: results taking into account the individual features as semantic similarity mea-sures.Ablated feature Wikipedia News Overall diffLIPN-run2 (none) 0.7165 0.8316 0.7852 0.00%1:CKPD 0.7216 0.8318 0.7874 0.22%2:WN 0.7066 0.8277 0.7789 ?0.63%3:Edit Dist 0.708 0.8242 0.7774 ?0.78%4:Cosine 0.6849 0.8235 0.7677 ?1.75%5:NER overlap 0.7338 0.8341 0.7937 0.85%6:Mihalcea-JC 0.7103 0.8301 0.7818 ?0.34%7:IRsim 0.7161 0.8026 0.7677 ?1.74%8:geosim 0.7185 0.8325 0.7865 0.14%9:Spect.
Dist 0.7243 0.8311 0.7880 0.28%Table 2: Spanish task: ablation test.Dataset CorrelationComplete (official + OnWN) 0.6687Complete (only official) 0.5083deft-forum 0.4544deft-news 0.6402headlines 0.6527images 0.8094OnWN (unofficial) 0.8039tweet-news 0.5507Table 3: English results (Official run + unofficialOnWN).4 Conclusions and Future WorkThe introduced measures were studied on theSpanish subtask, observing a limited contribu-tion from geographic context similarity and spec-tral distance.
The IR-based measure introducedin 2013 proved to be an important feature fornewswire-based datasets as in the 2013 Englishtask, even when trained on a training set derivedfrom automatic translation, which include manyerrors.
Our participation in the English subtaskwas inconclusive due to the technical faults experi-enced to produce our results.
We will neverthelesstake into account the lessons learned in this partic-ipation for future ones.AcknowledgementsPart of this work has been carried out with the sup-port of LabEx-EFL (Empirical Foundation of Lin-guistics) strand 5 (computational semantic analy-sis).
We are also grateful to CoNACyT (ConsejoNAcional de Ciencia y Tecnologia) for support to404this work.ReferencesThorsten Brants and Alex Franz.
2006.
Web 1t 5-gramcorpus version 1.1.Davide Buscaldi and Paolo Rosso.
2008.
Geo-WordNet: Automatic Georeferencing of WordNet.In Proceedings of the International Conference onLanguage Resources and Evaluation, LREC 2008,Marrakech, Morocco.Davide Buscaldi, Paolo Rosso, Jos?e Manuel G?omez,and Emilio Sanchis.
2009.
Answering ques-tions with an n-gram based passage retrieval engine.Journal of Intelligent Information Systems (JIIS),34(2):113?134.Davide Buscaldi, Joseph Le Roux, Jorge J. Garcia Flo-res, and Adrian Popescu.
2013.
Lipn-core: Seman-tic text similarity using n-grams, wordnet, syntac-tic analysis, esa and information retrieval based fea-tures.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 1: Pro-ceedings of the Main Conference and the SharedTask: Semantic Textual Similarity, pages 162?168,Atlanta, Georgia, USA, June.
Association for Com-putational Linguistics.Davide Buscaldi.
2013.
Une mesure de similarit?es?emantique bas?ee sur la recherche d?information.
In5`eme Atelier Recherche d?Information SEmantique- RISE 2013, pages 81?91, Lille, France, July.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technol-ogy, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Damien Dudognon, Gilles Hubert, and Bachelin JhonnVictorino Ralalason.
2010.
Proxig?en?ea : Unemesure de similarit?e conceptuelle.
In Proceedings ofthe Colloque Veille Strat?egique Scientifique et Tech-nologique (VSST 2010).Andreas Eisele and Yu Chen.
2010.
Multiun:A multilingual corpus from united nation docu-ments.
In Daniel Tapias, Mike Rosner, Ste-lios Piperidis, Jan Odjik, Joseph Mariani, BenteMaegaard, Khalid Choukri, and Nicoletta Calzo-lari (Conference Chair), editors, Proceedings of theSeventh conference on International Language Re-sources and Evaluation, pages 2868?2872.
Euro-pean Language Resources Association (ELRA), 5.J.J.
Jiang and D.W. Conrath.
1997.
Semantic similar-ity based on corpus statistics and lexical taxonomy.In Proc.
of the Int?l.
Conf.
on Research in Computa-tional Linguistics, pages 19?33.Joseph Le Roux, Jennifer Foster, Joachim Wagner,Rasul Samad Zadeh Kaljahi, and Anton Bryl.2012.
DCU-Paris13 Systems for the SANCL 2012Shared Task.
In The NAACL 2012 First Workshopon Syntactic Analysis of Non-Canonical Language(SANCL), pages 1?4, Montr?eal, Canada, June.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In Proceedingsof the 21st national conference on Artificial intelli-gence - Volume 1, AAAI?06, pages 775?780.
AAAIPress.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity: measuring the re-latedness of concepts.
In Demonstration Papers atHLT-NAACL 2004, HLT-NAACL?Demonstrations?04, pages 38?41, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Bernhard Sch?olkopf, Peter Bartlett, Alex Smola, andRobert Williamson.
1999.
Shrinking the tube: anew support vector regression algorithm.
In Pro-ceedings of the 1998 conference on Advances in neu-ral information processing systems II, pages 330?336, Cambridge, MA, USA.
MIT Press.Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the 32ndannual meeting on Association for ComputationalLinguistics, ACL ?94, pages 133?138, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.405
