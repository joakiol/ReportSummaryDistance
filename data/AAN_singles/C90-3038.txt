NEURAL NETWORK APPROACH TO WORD CATEGORY PREDICTIONFOR ENGLISH TEXTSMasami NAKAMURA, Katsuteru MARUYAMA f, Takeshi KAWABATA f?, Kiyohiro SHIKANO t i tATR Interpreting Telephony Research LaboratoriesSeika-chou, Souraku-gun, Kyoto 619-02, JAPANe-mail masami@atr-la.atr.co.jpAbstractWord category prediction is used to implement anaccurate word recognition system.
Traditional statisticalapproaches require considerable training data to estimatethe probabilities ofword sequences, and many parametersto memorize probabilities.
Tosolve this problem, NETgram,which is the neural network for word category prediction, isproposed.
Training results how that the perfornmnce of timNETgram is comparable to that of the statistical model;although the NETgram requires fewer parameters than the~;tatisticat model.
Also the NETgram performs effectively?or unknown data, i.e., the NETgram interpolates sparsetraining data.
Results of analyzing the hidden layer showthat the word categories are classified into linguisticallyti~ignificant groups.
The results of applying the NETgram toHMM English word recognition show that the NETgramimproves the word recognition rate fi'om 81.0% to 86.9%.1.
IntroductionFor the realization of an interpreting telephony system,an accurate word recognition system is necessary.
Becauseit is difficult to recognize English words using only theiracoustical characteristics, an accurate word recognitionsystem needs certain linguistic information.
Errors in wordrecognition results for sentences uttered in isolationinclude the tbllowing types of errors recoverable usinglinguistic infi)rmation.
(a) Local syntax errors.
(b) Global syntax errors.
(c) Semantics and context errors.Many errors arise with one-syllable words such as ( I, by) and ( the, be ).
More than half of these errors can berecovered by use of local syntax rules.
The Trigramlanguage model is an extremely rough approximation f alanguage, but it is a practical and useful model from theJ Research and Development Department, NITSUF, O CorporatAo~l1 N'ffFBasic Research \[,aboratm'ies"J"l"t N'UI' ttuman I terface \[,aboratoriesviewpoint of entropy.
At the very least, the trigram modelis useful as a preprocessor for a linguistic processor whichwill be able to deal with syntax, semantics and context.Text Mr. Hawksly said yesterday he wouldCategory NP NP VBD NR PPS MDCategory 5 \] 51 79 55 66 46No.Bigram :1 /"" ~' -~""prediction \] / :' ' ............. ~.Fig.
1 Word Category PredictionUsing Brown Corpus Text DataThe trigram model using the appearance probabilitiesof the following Word was efficiently applied to improveword recognition results 111\]\[121.
However, the traditionalstatistical pproach requires considerable training samplesto estimate the probabilities of word sequence andconsiderable memory capacity to process theseprobabilities.
Additionally, it is difficult to predict unseendata which never appeared in tile training data.Neural networks are interesting devices which canlearn general characteristics or rules from limited sampledata.
Neural networks are particularly useful in patternrecognition.
In symbol processing, NETtalk \[3\], whichproduces phonemes from English text, has been usedsuccessfully.
Now a neural network is being applied to wordcategory prediction \[4\].This paper describes the NETgram, which is a neuralnetwork for word category prediction i  text.
The NETgramis constructed by a trained Bigram network with two hiddenlayers, so that each bidden layer can learn the coarse-codedfeatures of the input or output word category.
Also, theNETgram can easily be expanded from Bigran) to N-gramnetwork without exponentially increasing the number ofparameters.
Tire NETgram is tested by t i 'a inin~experiments with the Brown Corpus English Text Databasei 213\[51.
The NETgram is applied to IIMM English wordrecognition resulting in an improvement of its recognitionperformance.2.
Word Category Predict ion Neural  Net (N ETgram)The basic Bigram network in the NETgram is a 4-layerfeed-forward network, as shown in Fig.2, which has 2hidden layers.
Because this network is trained for the nextword category as the output for an input word category,hidden layers are expected to learn some l inguist icstructure from the relationship between one word categoryand the next in the text.
The Trigram network in theNETgram has a structure such that, as the number ofnext word category\[ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1L ___ ( !IiO0  .
.
.
.
.
.
0 ' ,.
.
.
.
.
.I.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Jpresent word categoryOutput Layer89 unitsHidden layer 216 unitsHidden layer 116 unitsInput Layer89 unitsFig.2 NETgram ( Basic Bigram Network ): ,oo?
.
, ,~?
.
.
?
, , , ,N th word" I  Output !I Unit<s9) ,, iI HL2" Io , ? "
~  lu ' i t~06)  I " I .~i I .
.
.
LU' i t<lm I'1 Ifiput I '."
Input .
.
.
.
iU nits(89) I Units(89):{N-3)th Word " (N-2)th Word (N-1)th Word!4-gram Trigram Bigram networkFig.3 NETgram ( Trigram, 4-gram Network )grams increases, every new input block produced is fullyconnected to the lower hidden layer of one basic Bigramnetwork.
The link weight is set at wt' as shown in Fig.3.When expanding from Trigram network to 4-gram network,one lower hidden layer block is added and the first andsecond input blocks are fully connected to one lower hiddenlayer block, and the second and third input blocks are fullyconnected to the other lower hidden layer block.3.
How to Train NETgramIlow to train a NE'Pgram, e.g.
a Trigram network, isshown in Fig.4.
As input data, word categories in theBrown Corpus text\[5\] are given, in order, from the firstword in the sentence to the last, In one input block, only oneunit corresponding to the word category number is turnedON (1); The others are turned OFF (0).
As output data,only one unit corresponding to the next word categorynumber is trained by ON (1).
The others are trained by OFF(0).
The tra in ing a lgor i thm is the Back-Propagationalgorithm\[6l, which uses the gradient descent to changelink-weights in order to reduce the difference between thenetwork output vectors and the desired output vectors.First, the basic Bigram network is trained.
Next, theTrigram networks are trained with the llnk weight valuestrained by the basic Bigram network as initial values.This task is a many-to-many mapping problem.
Thus, it .is difficult to train because the updating direction of thelink weight vector easily fluctuates.
In a two-sentence.~ko .
o??
?0 1 0i O .
.
.
.
.
.
0 .... O!
oo,pot~_1 .
.
.
.
.
.
.
.
5~ .
.
.
.
.
J!9_~ LayerH idden LayersL_!
.
.
.
.
.
.
.
.
_s3 .
.
.
.
.
8_9_2 L_t .
.
.
.
.
.
.
.
.
.
.
79___8_9_2 Layer~.
.
.~?
, o , ,  , ' li _..dFig.4 ttow to Train NETgram (Trigram Model)214training experiment of about 50 words, we have confirmedthat the output values of tbe basic Bigram networkconverge on the next occurrence probability distribution.ttowever, for many training data, considerable time isrequired for training.
There fore ,  in order to increasetraining speed, we use the next werd category occurrenceprobability distribution calculated for 1,024 sentences(about 24,000 words) as output training data in the basicBigram network.
Of course, in Trigram and 4-gramtraining, we use the next one-word category as outputtraining data.4.
Training llcsnlts4.1.
Basic Bigram NetworkWord category prediction results show that NETgrmn(the basic Bigram network) is comparal)le to the statisticalBigram model.Next, we consider whether the hidden layer hasobtained some linguistic structure.
We Calculated thesimilarity of every two lower hidden layer (HIA) outputvectors for 89 word categories and clustered them.Similarity S is calculated by(M(Ci),M(Cj))S(ci,cj) = (4.1)I1 M(Ci)it II M(Qi)I1where MfOi) is the lower hidden layer (ILL1) output vectorof the input word category CL (M(Ci),M(C~\])) is the immrproduct of M(Ci) and M(Cj).
It M(Ci) II is the norm of M(Ci).The clustering result is shown in Fig.5.
Clustering by thethreshold of similarity, 0.985, the word categories areclassified into linguistically significant groups, which arethe HAVE', verb group, BE verb group, subjective pronoungroup, group whose categories should be before a noun, andothers.
Therefore the NETgram can learn linguisticstructure naturally.4.2.
Trigram NetworkWord category prediction results are shown in Fig.6.The NETgram (Trigram network) is comparable to thestatistical Trigram model for test data.Furthermore, the NETgram performs effectively forunseen data whicil never appeared in the training data,although the statistical Trigram can not predict the nextword category for unseen data.
That is to say, NETgramsinterpolate sparse training data in the same way deletedinterpolation \[71 does.CA'FE-GORY"3"g3qV~37 HVD40 EIV716 BEDZ20 BER21 BEZ19 BEN14 BE17BEG38 I-IVG~ffPPS--86 WPS67 PPSS"2gD-r---45 JJT58 OD42 JJ48 NN$61 PP$52 NP$13 AT78 VB80 VBG06 ,1 lAP22 CC09 ABN10 ABX75 RP81 VBN89 DUM23 CD43 J JR47 NN noun,single)55 NR lome, west79 VBD (verb, past)32 VSZ (verb, -s, -es)~2 DTS these55 PPO me, him, it19 NNS (noun,plural)t0 RB (adverb)~0NNS$ men's}1 N PP ATR, Tom~thers othersEXAMPLE Thresho ld  of  S imi lar i ty(part of speech) 1.000 0.995 0.990 0.985 0.980had - - -1  J :has : \  - - - -~--  T_Y2-L _ ~_L_  - -WaS .
.
.
.
.
.
.
.
/ !are .
.
.
.
.
.
.
~ i 'een Q?
)ibei.~ .
.
.
.
.
.
.
~ jh ~  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7fie, Jt .
.
.
.
.
.
.
.
.
.
.
~whe,whicb .
.
.
.
.
.
.
.
.
.
.
.
.
.
~ I ;I,we,thev .
.
.
.
.
.
.
.
~ ;biggest .
.
.
.
~ / I  I F F l ;first, 2nd .
.
.
.
J i l l |  I \ [ i(adjective) .
.
.
.
.
.
.
.
1 I I t t | idog's - - - - - -ATR's .
.
.
.
.
.
.
1 I I I ~ I !a the .
.
.
.
.
.
.
~_  I I ; !
(verb, base ~-- t  \] \[ ;(~erU.
~.g) -2  _~ i i i\]many, next .
.
.
.
~ ', iand, or - -~!
!half, all ~ -  I ~both -I Iabout, off  ~ - -  i(verb, -ed) ~ _ ~  i(dummy)one, 2 I--t- ', comp.adj.
)Prediction Rate0.8 -.
.
.
.
.
.
.
.
.
.
.
.
.
C)NETgram for test dataStatistical Model for test data0.60 .4  .,...
,,,'""'"data as Trigram data.0.2I _Fig.5 Clustering Result of iliA Output Vectors of NETgram( Bigram )l 1 l _ _ J0 \] 2 3 4 5Number of Prediction Candidate CategoriesFig.6 NETgram (Trigran0 Prediction Rates21534.3.
Differences between the statistical model and theN ETgramWe discuss differences between two approaches, theconventional statistical model and the NETgraln.
Theconventional statistical model is based on the table-lookup.In the case of the Trigram nlodel, next appearanceprobabilities are computed frmn the histogram, countingthe next word category for the two word categories in thetraining sentences.
The probabilities are put in an 89*89*89size table.
Thus, the 89 appearance probabilities of the nextword category are obtained from the 89*89*89 size tableusing the argument of 89*89 symbol permutation.B 89.s9 ---> R 89 B ; binary spaceR ; real spaceIn order to get 89 prediction values for the next wordcategory, the trained NETgram procedure isas follows :First, encode from an 89*89 symbol permutation to a16-dimensional nalogue code.
( h'om the inputlayer to the hidden layer 1 )Second, transform the 16-dimensional nalogue code toa 16-dimensiotial nalogue code of the nextword's 89 prediction values.
( from hidden layer1 to hidden layer 2 )Finally, decode the 16-dimensional nalogue code to 89prediction values of the next word's 89prediction values.
( fl'om hidden layer 2 tooutput layer )B89"89 __> R 16 .__> R 16 ___> R 89The values of each space are output values of theNETgram units of each layer.
These mappings are uniquelydetermined by link-weight values of the NETgram.
That isto say, each layer unit value is computed by summinglower-connected unit values multiplied by link-weights andpassing through the nonlinear function (sigmoid function).These two approaches need the following memory area(number of parameters).
'Statistical model 89?89X89 = 704,969(max number of table elements}"NETgram (89+89)X16+16X16+16X89 +121 = 5,193(number of link-weights)(121 ; offset parameters)Thus, the parameters of the statistical model ar(~89?89X89 probabilities.
In practice, there are ninny 0values in 89 X 89 X 89 probabilities and the size of the tablecan be reduced using a particular technique, tlowever, thisdepends on the kind of task and the number of' trainingdata.
On the other band, the NETgram can produce89?89?89 prediction values using link-weight valuesmemorized as parameters.Next, concerning'the data representation, the statisticalmodel does not use input data structures because it is basedon the table-lookup which get probabilities directly bysymbol series input.
On the other hand, the NETgramextracts a feature related to the distance between wordcategories from symbol series input into 16-dimensionalanalogue code.
16-dimensional analogue codes are describedin 4.1 as the feature of the NETgram hidden layer in theBigram nmdel.
Thus, the NETgram interpolates sparsetraining data in the process of bigram and trigram training.From the viewpoint of data coding, The NETgramcompresses data from 89-dimensional binary space into 16-dimensional real space.4.4.
4-gram Network4-gram prediction rates of the NETgram trained by2,048 are not much higher than the trigram prediction ratesof that trained by 1,024 sentences.
The statistical modelexperiment results how that more than 6,000 sentences arenecessary as training data in order for the 4-gramprediction rates to equal the trigram prediction rates of theNETgram trained by 1,024 sentences.
Futhermore, thetrigram prediction rates of the statistical model increase astim training sentences increase, up to a max of 16,000training sentences.
The NETgram compensates for thesparse 4-gram data through the interpolation effect.However, it is clear that the 4-gram prediction NETgramneeds far more than 16,000 training sentences in order tobetter the performance of the trigram prediction.
Trainingfor so many sentences was not possible because of thelimited database and considerable computing required.5.
Applying the NETgram to Speech RecognitionThe algorithm for applying the NETgram to speechrecognition is shown in Fig.7.
HMM refers to the ltiddenMarkov Model which is a technique for speechrecognition\[l\] \[8\] [9\].216Acoustic lin___quisticKeyboard Conversation Speech Data Brown Col pus Toxt Data, *  Traininq ~, ~ Test "** ~* Training ~.Data" ~ ,, Data ?
~ ~ Data * " ' l ' "  "Recoqnit oTraining ~ jFig.
7 I mprovenmnt of 11MM English Word RecognitionUsing the NETgram5.1.
Formulat ionl.,et w~ show a word just after wi_ 1 and just before wi+ I.Let Ci show one of the word categories to which the word wibelongs.
The same we,'d belonging to a different category isregarded as a different word.
The tr igram probability of wiis calculated using the following approximations.t '(wi/wi.2 wi !
)~-~ P(wi/Ci-2 Ci-1):= P(Ci/Ci-2 Ci4)X {P(w i / C i.2 Ci-1) / P( Ci / Ci-2 Ci-1) }:= P(Ci/Ci-2 Ci-1) {P(wi) / P(Ci) } (5.1)Word tr igram probabi l i t ies are approximated usingcategory tr igram probabilities as follows :P(wi/wi.2 wi.1) ~ P(wi/Ci-2 Ci-1) (5.2)The probability of w~ is denoted by the preceding two-word sequence, wi.2, Wi.l, and is approximated by theirpreceding two-category sequence.P(wi / Ci-2 Ci-1) / P(Ci / Ci-2 Ci-l) = I'(wi) / P(Ci) (5.3)The probability ratio of wi and Ci given by Ci-2 C i j  isnearly equal to the total probability ratio ofwi  and C i.To eah.
'ulate the above probab i l i ty ,  the t r ig ramprobability of word category, P(C i / Ci-2 Ci-1), and wordoccurrence probability, P(wi)/P(Ci), are required.
The wordprobability, P(wi) / P(Ci), is prestored in tim dietio,mry ofword wi for each wm'd category.To avoid the multit)lication of probabilities% tbe loglikelihood, STi, is defined as :STi = IolIP(Ci/Ci-2 Ci-l) ?
log(P(wi)/P(C~)) (5,4)'rhe.
first term is retrieved from the tr igram of wordcategories and the second term is retrieved from the worddictionary.The maximum likelihood of a word, SW, is given by thesum of word likelihood values of a n-word sequence.
The j-th word candidate in the i-th word of a sentence is denotedby wij.
The likelihood of" wij , SWi,i, is defined as the sum oftwo types of likelihood which are the log-likelihood of theItMM output probability, SHia, and the tr igram likelihood,STi j .
Thus, the likelihood of wij is described as follows :SWi.j = (1-o~) .
SHi j  + ~o .
S'l'ij (55)where a~ is the weighting parameter  to adjust  thescaling of two kinds of likelihood.The maximum sentence l ikel ihood values, G, aredenoted by the following equations :Go,; = SWod ( i = O) (5.6)Gij -- max( S Wij  -~ Gi.1, k ) ( i  v= 0 ) (5.7)kWhen the length era sentence is N, the maximum valueof GN.i j  is regarded as the maximum likelihood of the wordsequence.
The back-tracing of wij gives the optimal wordsequence.In this paper, the best-ten candidates in the tlMM wordrecognition results are used.
As the same word belonging toa different category is regarded as a different word, thereare ten or more word candidates.5.2.
Engl ish Word Recogni t ion ResultsThe exper iment  task is to t rans la te  keyboardconversations which include 377 English sentences (2,834words) uttered word by word by one mate native speaker.The sentences are composed of 542 different words.
HMMphone models are trained using 190 sentences (1,487 words)without phone labels.The tr igram models, the NETgram and the statisticalmodel, are trained using using 512 and 1,024 sentences ofthe Brown Corpus Text l)atabase.
One sentence is about 24words long.Engl ish word recognition results for 18'7 sentences(l,347 words) of keyboa,'d eonversatim~s u ing HMM andthe tr igram models are shown in Table 1.
The recognitionrate in the experiment using only flMM is 81.0% Using the217NETgram, the recognition rates have been in)proved about5 or 6 %.
The number of recognition errors decreases usingN l,\]Tgram.Table 1 tlMM English Word Recognition Ratesusing NETgrana or Statistical model (%)Model Training NETgram StatisticalSentences Model!I 512 86.3 85.5Trigram1,024 86.9 85.4?
i i i iThe results of analyzing the hidden layer after trainingshowed that the word categories; were classified into somelinguistically significant groups, that is to say, theNlgTgram learns a linguistic structure.Next, the NETgram was applied to ttMM English wordrecognition, and it was shown that the NETgram caneffectively correct word recognition errors in text.
The wordrecognition rate using tlMM is 81.0%.
The NETgramtrained by 1,024 sentences improves the word recognitionrate to 86.9%.
The NETgram performs better than timstatistical trigram model when data is in*;ufficient oestimate the correct probabilities ofa word sequence.Comparing the NETgram and the statistical trigrammodel, the performance of the NETgram is higher than thatof the statistical trigram in the case of training dataconsisting of 512 and 1,024 sentences.
Furthermore, thestatistical trigram model cannot learn word sequenceswhich do not appear as a trigram in the training datm Thus,the prediction value o. c that word sequence is zero.
TheNETgram does not make such fatal mistakes.Additional results for 4,096 and 30,000 sentences showthat recognition rates are 86.9% and 87.2% using theN ETgram, and 86.6% and 87,7% using the statistical model.It is confirmed that the NETgram performs better than thestatistical trigram nmdel when data is insufficient oestimate the correct probabilities Tberefore, even iftraining data were insufficient to estimate accurate trigramprobabilities, the NETgram pe,'forms effectively.
That is tosay, the NETgram interpolates sparse trigram trainingdata using bigram training memory.6.
ConclusionIn this paper we have presented the NETgram, a neuralnetwork for N-gram word category prediction in text.
TheNETgram can easily be expanded from Bigram to N-gramnetwork without exponentially increasing the number ofparameters.The training results showed that the Trigram wordcategory prediction ability of the NETgram was comparableto that of the statistical Trigram model although theNgTgram requires fewer parameters than the statisticalmodel.
We also confirmed that NETgrams performedeffectively for unknown data which never appeared in thetraining data, that is to say, NETgrams interpolate sparsetraining data naturally.AcknowledgementThe authors would like to express their gratitude to Dr.Akira Kurematsu, president of ATR Interpret ingTelephony Research Laboratories, which made thisresearch possible, for his encouragement a d support.
Weare also indebted to the members of the Speech ProcessingDepartment a ATR, for their hell) in the various tages ofthis research.References\[11 I,'.Jetinek, "Continuous Speech Recognition byStatistical Methods", Proceedings ofthe iI'~EI'\], Vol.64, No.4(1976.4)121 K.Shikano, "hnprovenmnt of Word Recognition Resultby Trigram Model", \[CASSP 87, 29.2(1987.4)\[al T.J.Sejnowski, C.R.Rosenberg, "NETtalk, A ParallelNetwork that l,earns to Read Aloud", Teeh.
Report, TheJohns llopkins University EESS -86-01 (1986)\[41 M.Nakamura, K.Shikano, "A Study of English WordCategory Prediction Based on Neural Networks", ICASSP89, SI 3.10(1989.5)151 13town University, "Brown Corpus", Tech: Report,Brown University (1967)16l D.E.Rumelhart, G.E.tllnton, R.d.Williams, "ParallelDistributed Processing", M.I.T.
Press (1986)\[7\] F.Jelinek, R.Mercer, "Interpolated Estimation ofMarker Source Parameters from Sparse Data", PatternRecognition i  Practice, ed.
E,S.
Gelsema nd L. N. Kanal,North ttolland (1980)\[81 S.E.Levinson, L.R.Rabiner, M.M.Sondhi, "AnIntroduction to the Application of the Theory ofProbabilistie Functions of a Marker Process to AutomaticSpeech Recognition", Bell Syst.
Teeh.
J.
62(4), pp1035-1074(1983)\[91 T.1Ianazawa, G.Kawabata, K.Shikano, "Study ofSeparate Vector Quantization for HMM PhonemeRecognition", ASJ 2-P-8(1988.10)2 't.8
