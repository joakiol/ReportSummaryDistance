Parametric Models of Linguistic Count DataMartin JanscheDepartment of LinguisticsThe Ohio State UniversityColumbus, OH 43210, USAjansche@acm.orgAbstractIt is well known that occurrence countsof words in documents are often mod-eled poorly by standard distributions likethe binomial or Poisson.
Observed countsvary more than simple models predict,prompting the use of overdispersed mod-els like Gamma-Poisson or Beta-binomialmixtures as robust alternatives.
Anotherdeficiency of standard models is due to thefact that most words never occur in a givendocument, resulting in large amounts ofzero counts.
We propose using zero-inflated models for dealing with this, andevaluate competing models on a NaiveBayes text classification task.
Simplezero-inflated models can account for prac-tically relevant variation, and can be easierto work with than overdispersed models.1 IntroductionLinguistic count data often violate the simplistic as-sumptions of standard probability models like thebinomial or Poisson distribution.
In particular, theinadequacy of the Poisson distribution for model-ing word (token) frequency is well known, and ro-bust alternatives have been proposed (Mosteller andWallace, 1984; Church and Gale, 1995).
In the caseof the Poisson, a commonly used robust alternativeis the negative binomial distribution (Pawitan, 2001,?4.5), which has the ability to capture extra-Poissonvariation in the data, in other words, it is overdis-persed compared with the Poisson.
When a smallset of parameters controls all properties of the dis-tribution it is important to have enough parametersto model the relevant aspects of one?s data.
Sim-ple models like the Poisson or binomial do not haveenough parameters for many realistic applications,and we suspect that the same might be true of log-linear models.
When applying robust models likethe negative binomial to linguistic count data likeword occurrences in documents, it is natural to askto what extent the extra-Poisson variation has beencaptured by the model.
Answering that question isour main goal, and we begin by reviewing some ofthe classic results of Mosteller and Wallace (1984).2 Word Frequency in Fixed-Length TextsIn preparation of their authorship study of The Fed-eralist, Mosteller and Wallace (1984, ?2.3) investi-gated the variation of word frequency across con-tiguous passages of similar length, drawn from pa-pers of known authorship.
The occurrence frequen-cies of any in papers by Hamilton (op.
cit., Ta-ble 2.3?3) are repeated here in Figure 1: out of atotal of 247 passages there are 125 in which theword any does not occur; it occurs once in 88 pas-sages, twice in 26 passages, etc.
Figure 1 also showsthe counts predicted by a Poisson distribution withmean 0.67.
Visual inspection (?chi by eye?)
indi-cates an acceptable fit between the model and thedata, which is confirmed by a ?2 goodness-of-fittest.
This demonstrates that certain words seem tobe adequately modeled by a Poisson distribution,whose probability mass function is shown in (1):Poisson(?
)(x) = ?xx!1exp?
(1)12650751001250 1 2 3 4 5887frequency(numberofpassages)occurrences of "any" [Hamilton]observedPoisson(0.67)Figure 1: Occurrence counts of any in Hamilton pas-sages: raw counts and counts predicted under a Pois-son model.For other words the Poisson distribution gives amuch worse fit.
Take the occurrences of were in pa-pers by Madison, as shown in Figure 2 (ibid.).
Wecalculate the ?2 statistic for the counts expected un-der a Poisson model for three bins (0, 1, and 2?5, toensure that the expected counts are greater than 5)and obtain 6.17 at one degree of freedom (numberof bins minus number of parameters minus one),which is enough to reject the null hypothesis thatthe data arose from a Poisson(0.45) distribution.
Onthe other hand, the ?2 statistic for a negative bino-mial distribution NegBin(0.45,1.17) is only 0.013for four bins (0, 1, 2, and 3?5), i. e., again 1 degreeof freedom, as two parameters were estimated fromthe data.
Now we are very far from rejecting the nullhypothesis.
This provides some quantitative back-ing for Mosteller and Wallace?s statement that ?eventhe most motherly eye can scarcely make twins ofthe [Poisson vs. empirical] distributions?
for certainwords (op.
cit., 31).The probability mass function of the negative bi-nomial distribution, using Mosteller and Wallace?sparameterization, is shown in (2):NegBin(?
,?
)(x) = ?xx!?(?
+ x)(?
+?)?+x???(?)
(2)If one recalls that the Gamma function is well be-haved and thatexp?
= lim???(1+??
)?= lim???(?
+?)???
,it is easy to see that NegBin(?
,?)
converges toPoisson(? )
for ?
constant and ?
?
?.
On the other52550751001251501790 1 2 3 4 511858167frequency(numberofpassages)occurrences of "were" [Madison]observedPoisson(0.45)NegBin(0.45, 1.17)Figure 2: Occurrence counts of were in Madisonpassages: raw counts and counts predicted underPoisson and negative binomial models.hand, small values of ?
drag the mode of the nega-tive binomial distribution towards zero and increaseits variance, compared with the Poisson.As more and more probability mass is concen-trated at 0, the negative binomial distribution startsto depart from the empirical distribution.
One canalready see this tendency in Mosteller and Wallace?sdata, although they themselves never comment onit.
The problem with a huge chunk of the proba-bility mass at 0 is that one is forced to say that theoutcome 1 is still fairly likely and that the probabil-ity should drop rapidly from 2 onwards as the term1/x!
starts to exert its influence.
This is often at oddswith actual data.Take the word his in papers by Hamilton andMadison (ibid., pooled from individual sections ofTable 2.3?3).
It is intuitively clear that his maynot occur at all in texts that deal with certain as-pects of the US Constitution, since many aspects ofconstitutional law are not concerned with any sin-gle (male) person.
For example, Federalist No.
23(The Necessity of a Government as Energetic as theOne Proposed to the Preservation of the Union, ap-prox.
1800 words, by Hamilton) does not contain asingle occurrence of his, whereas Federalist No.
72(approx.
2000 words, a continuation of No.
71 TheDuration in Office of the Executive, also by Hamil-ton) contains 35 occurrences.
The difference is thatNo.
23 is about the role of a federal government inthe abstract, and Nos.
71/72 are about term limits foroffices filled by (male) individuals.
We might there-fore expect the occurrences of his to vary more, de-4052001004826120 1 2 3713918frequency(numberofpassages)occurrences of "his" [Hamilton, Madison]observedNegBin(0.54, 0.15)NegBin(0.76, 0.11)0.34 NegBin(1.56, 0.89)Figure 3: Occurrence counts of his in Hamilton andMadison passages (NB: y-axis is logarithmic).pending on topic, than any or were.The overall distribution of his is summarized inFigure 3; full details can be found in Table 1.
Ob-serve the huge number of passages with zero oc-currences of his, which is ten times the number ofpassages with exactly one occurrence.
Also noticehow the negative binomial distribution fitted usingthe Method of Maximum Likelihood (MLE model,first line in Figure 3, third column in Table 1) over-shoots at 1, but underestimates the number of pas-sages with 2 and 3 occurrences.The problem cannot be solved by trying to fit thetwo parameters of the negative binomial based onthe observed counts of two points.
The second linein Figure 3 is from a distribution fitted to match theobserved counts at 0 and 1.
Although it fits those twopoints perfectly, the overall fit is worse than that ofthe MLE model, since it underestimates the observedcounts at 2 and 3 more heavily.The solution we propose is illustrated by the thirdline in Figure 3.
It accounts for only about a thirdof the data, but covers all passages with one or moreoccurrences of his.
Visual inspection suggests that itprovides a much better fit than the other two models,if we ignore the outcome 0; a quantitative compari-son will follow below.
This last model has relaxedthe relationship between the probability of the out-come 0 and the probabilities of the other outcomes.In particular, we obtain appropriate counts for theoutcome 1 by pretending that the outcome 0 oc-curs only about 71 times, compared with an actual405 observed occurrences.
Recall that the modelaccounts for only 34% of the data; the remainingNegBin ZINBobsrvd expctd expctd0 405 403.853 405.0001 39 48.333 40.2072 26 21.686 24.2063 18 12.108 14.8684 5 7.424 9.2235?6 9 8.001 9.3617?14 7 6.996 5.977?2 statistic 6.447 2.952df 4 3?2 cumul.
prob 0.832 0.601?
logL(??)
441.585 439.596Table 1: Occurrence counts of his in Hamilton andMadison passages.counts for the outcome 0 are supplied entirely bya second component whose probability mass is con-centrated at zero.
The expected counts under the fullmodel are found in the rightmost column of Table 1.The general recipe for models with large countsfor the zero outcome is to construe them as two-component mixtures, where one component is a de-generate distribution whose entire probability massis assigned to the outcome 0, and the other compo-nent is a standard distribution, call it F (?).
Such anonstandard mixture model is sometimes known asa ?modified?
distribution (Johnson and Kotz, 1969,?8.4) or, more perspicuously, as a zero-inflated dis-tribution.
The probability mass function of a zero-inflated F distribution is given by equation (3),where 0 ?
z ?
1 (z < 0 may be allowable subjectto additional constraints) and x?
0 is the Kroneckerdelta ?x,0.ZIF (z,?
)(x) = z (x?
0)+(1?
z)F (?
)(x) (3)It corresponds to the following generative process:toss a z-biased coin; if it comes up heads, generate 0;if it comes up tails, generate according to F (?).
Ifwe apply this to word frequency in documents, whatthis is saying is, informally: whether a given wordappears at all in a document is one thing; how oftenit appears, if it does, is another thing.This is reminiscent of Church?s statement that?
[t]he first mention of a word obviously dependson frequency, but surprisingly, the second doesnot.?
(Church, 2000) However, Church was con-cerned with language modeling, and in particularcache-based models that overcome some of the limi-tations introduced by a Markov assumption.
In sucha setting it is natural to make a distinction betweenthe first occurrence of a word and subsequent occur-rences, which according to Church are influencedby adaptation (Church and Gale, 1995), referringto an increase in a word?s chance of re-occurrenceafter it has been spotted for the first time.
Forempirically demonstrating the effects of adaptation,Church (2000) worked with nonparametric methods.By contrast, our focus is on parametric methods, andunlike in language modeling, we are also interestedin words that fail to occur in a document, so it is nat-ural for us to distinguish between zero and nonzerooccurrences.In Table 1, ZINB refers to the zero-inflated neg-ative binomial distribution, which takes a parame-ter z in addition to the two parameters of its nega-tive binomial component.
Since the negative bino-mial itself can already accommodate large fractionsof the probability mass at 0, we must ask whether theZINB model fits the data better than a simple nega-tive binomial.
The bottom row of Table 1 shows thenegative log likelihood of the maximum likelihoodestimate ??
for each model.
Log odds of 2 in favor ofZINB are indeed sufficient (on Akaike?s likelihood-based information criterion; see e. g. Pawitan 2001,?13.5) to justify the introduction of the additionalparameter.
Also note that the cumulative ?2 proba-bility of the ?2 statistic at the appropriate degrees offreedom is lower for the zero-inflated distribution.It is clear that a large amount of the observedvariation of word occurrences is due to zero infla-tion, because virtually all words are rare and manywords are simply not ?on topic?
for a given docu-ment.
Even a seemingly innocent word like his turnsout to be ?loaded?
(and we are not referring to gen-der issues), since it is not on topic for certain dis-cussions of constitutional law.
One can imagine thatthis effect is even more pronounced for taboo words,proper names, or technical jargon (cf.
Church 2000).Our next question is whether the observed variationis best accounted for in terms of zero-inflation oroverdispersion.
We phrase the discussion in terms ofa practical task for which it matters whether a wordis on topic for a document.3 Word Frequency Conditional onDocument LengthWord occurrence counts play an important role indocument classification under an independent fea-ture model (commonly known as ?Naive Bayes?
).This is not entirely uncontroversial, as many ap-proaches to document classification use binary in-dicators for the presence and absence of each word,instead of full-fledged occurrence counts (see Lewis1998 for an overview).
In fact, McCallum andNigam (1998) claim that for small vocabulary sizesone is generally better off using Bernoulli indicatorvariables; however, for a sufficiently large vocab-ulary, classification accuracy is higher if one takesword frequency into account.Comparing different probability models in termsof their effects on classification under a Naive Bayesassumption is likely to yield very conservative re-sults, since the Naive Bayes classifier can performaccurate classifications under many kinds of adverseconditions and even when highly inaccurate prob-ability estimates are used (Domingos and Pazzani,1996; Garg and Roth, 2001).
On the other hand, anevaluation in terms of document classification hasthe advantages, compared with language modeling,of computational simplicity and the ability to benefitfrom information about non-occurrences of words.Making a direct comparison of overdispersed andzero-inflated models with those used by McCal-lum and Nigam (1998) is difficult, since McCal-lum and Nigam use multivariate models ?
for whichthe ?naive?
independence assumption is different(Lewis, 1998) ?
that are not as easily extended tothe cases we are concerned about.
For example,the natural overdispersed variant of the multinomialmodel is the Dirichlet-multinomial mixture, whichadds just a single parameter that globally controlsthe overall variation of the entire vocabulary.
How-ever, Church, Gale and other have demonstrated re-peatedly (Church and Gale, 1995; Church, 2000)that adaptation or ?burstiness?
are clearly propertiesof individual words (word types).
Using joint inde-pendent models (one model per word) brings us backinto the realm of standard independence assump-tions, makes it easy to add parameters that controloverdispersion and/or zero-inflation for each wordindividually, and simplifies parameter estimation.02040608010010 100 1000 10000 100000classificationaccuracy(percent)vocabulary size (number of word types)NewsgroupsBinomialBernoulliFigure 4: A comparison of event models for differ-ent vocabulary sizes on the Newsgroup data set.So instead of a single multinomial distributionwe use independent binomials, and instead of amultivariate Bernoulli model we use independentBernoulli models for each word.
The overall jointmodel is clearly wrong since it wastes probabilitymass on events that are known a priori to be impos-sible, like observing documents for which the sum ofthe occurrences of each word is greater than the doc-ument length.
On the other hand, it allows us to takethe true document length into account while usingonly a subset of the vocabulary, whereas on McCal-lum and Nigam?s approach one has to either com-pletely eliminate all out-of-vocabulary words andadjust the document length accordingly, or else mapout-of-vocabulary words to an unknown-word tokenwhose observed counts could then easily dominate.In practice, using joint independent models doesnot cause problems.
We replicated McCallum andNigam?s Newsgroup experiment1 and did not findany major discrepancies.
The reader is encour-aged to compare our Figure 4 with McCallum andNigam?s Figure 3.
Not only are the accuracy fig-ures comparable, we also obtained the same criti-cal vocabulary size of 200 words below which theBernoulli model results in higher classification ac-curacy.The Newsgroup data set (Lang, 1995) is a strati-1Many of the data sets used by McCallum and Nigam (1998)are available at http://www.cs.cmu.edu/~TextLearning/datasets.html.fied sample of approximately 20,000 messages to-tal, drawn from 20 Usenet newsgroups.
The factthat 20 newsgroups are represented in equal pro-portions makes this data set well suited for compar-ing different classifiers, as class priors are uniformand baseline accuracy is low at 5%.
Like McCal-lum and Nigam (1998) we used (Rain)bow (McCal-lum, 1996) for tokenization and to obtain the word/document count matrix.
Even though we followedMcCallum and Nigam?s tokenization recipe (skip-ping message headers, forming words from contigu-ous alphabetic characters, not using a stemmer), ourtotal vocabulary size of 62,264 does not match Mc-Callum and Nigam?s figure of 62,258, but does comereasonably close.
Also following McCallum andNigam (1998) we performed a 4:1 random split intotraining and test data.
The reported results were ob-tained by training classification models on the train-ing data and evaluating on the unseen test data.We compared four models of token frequency.Each model is conditional on the document length n(but assumes that the parameters of the distributiondo not depend on document length), and is derivedfrom the binomial distributionBinom(p)(x | n) =(nx)px (1?
p)n?x, (4)which we view as a one-parameter conditionalmodel, our firstmodel: x represents the token counts(0?
x?
n); and n is the length of the document mea-sured as the total number of token counts, includingout-of-vocabulary items.The second model is the Bernoulli model, whichis derived from the binomial distribution by replac-ing all non-zero counts with 1:Bernoulli(p)(x | n)= Binom(p)(?xx+1?|?nn+1?
)(5)Our third model is an overdispersed binomialmodel, a ?natural?
continuous mixture of binomi-als with the integrated binomial likelihood ?
i. e. theBeta density (6), whose normalizing term involvesthe Beta function ?
as the mixing distribution.Beta(?,?
)(p) = p??1(1?
p)??1B(?,? )
(6)The resulting mixture model (7) is known as thePo?lya?Eggenberger distribution (Johnson and Kotz,1969) or as the beta-binomial distribution.
It hasbeen used for a comparatively small range of NLPapplications (Lowe, 1999) and certainly deservesmore widespread attention.BetaBin(?,?
)(x | n)=?
10Binom(p)(x | n) Beta(?,?
)(p) dp=(nx)B(x+?,n?
x+?
)B(?,? )
(7)As was the case with the negative binomial (whichis to the Poisson as the beta-binomial is to the bino-mial), it is convenient to reparameterize the distribu-tion.
We choose a slightly different parameterizationthan Lowe (1999); we follow Ennis and Bi (1998)and use the identitiesp = ?/(?
+?
),?
= 1/(?
+?
+1).To avoid confusion, we will refer to the distributionparameterized in terms of p and ?
as BB:BB(p,?)
= BetaBin(p1?
??
, (1?
p)1?
??
)(8)After reparameterization the expectation and vari-ance areE[x;BB(p,?
)(x | n)] = n p,Var[x;BB(p,?
)(x | n)] = n p (1?
p) (1+(n?1) ?
).Comparing this with the expectation and variance ofthe standard binomial model, it is obvious that thebeta-binomial has greater variance when ?
> 0, andfor ?
= 0 the beta-binomial distribution coincideswith a binomial distribution.Using the method of moments for estimation isparticularly straightforward under this parameteri-zation (Ennis and Bi, 1998).
Suppose one sampleconsists of observing x successes in n trials (x occur-rences of the target word in a document of length n),where the number of trials may vary across samples.Now we want to estimate parameters based on a se-quence of s samples ?x1,n1?, .
.
.
,?xs,ns?.
We equatesample moments with distribution moments?ini p?
=?ixi,?ini p?
(1?
p?)
(1+(ni?1) ??)
=?i(xi?ni p?
)2,and solve for the unknown parameters:p?
=?i xi?i ni, (9)??
= ?i(xi?ni p?)2/(p?
(1?
p?))?
?i ni?i n2i ?
?i ni.
(10)In our experience, the resulting estimates are suf-ficiently close to the maximum likelihood esti-mates, while method-of-moment estimation is muchfaster than maximum likelihood estimation, whichrequires gradient-based numerical optimization2 inthis case.
Since we estimate parameters for up to400,000 models (for 20,000 words and 20 classes),we prefer the faster procedure.
Note that themaximum likelihood estimates may be suboptimal(Lowe, 1999), but full-fledged Bayesian methods(Lee and Lio, 1997) would require even more com-putational resources.The fourth and final model is a zero-inflated bino-mial distribution, which is derived straightforwardlyvia equation (3):ZIBinom(z, p)(x | n)= z (x?
0)+(1?
z)Binom(p)(x | n)=???z+(1?
z)(1?
p)n if x = 0(1?
z)(nx)px (1?
p)n?x if x > 0(11)Since the one parameter p of a single binomialmodel can be estimated directly using equation (9),maximum likelihood estimation for the zero-inflatedbinomial model is straightforward via the EM al-gorithm for finite mixture models.
Figure 5 showspseudo-code for a single EM update.Accuracy results of Naive Bayes document classi-fication using each of the four word frequency mod-els are shown in Table 2.
One can observe that thedifferences between the binomial models are small,2Not that there is anything wrong with that.
In fact, we cal-culated the MLE estimates for the negative binomial models us-ing a multidimensional quasi-Newton algorithm.1: Z?
0; X ?
0; N?
02: {E step}3: for i?
1 to s do4: if xi = 0 then5: z?i?
z/(z+(1?
p)ni)6: Z?
Z+ z?i7: X ?
X +(1?
z?i) xi8: N?
X +(1?
z?i)ni9: else {xi 6= 0, z?i = 0}10: X ?
X + xi11: N?
N +ni12: end if13: end for14: {M step}15: z?
Z/s16: p?
X/NFigure 5: Maximum likelihood estimation of ZI-Binom parameters z and p: Pseudo-code for a singleEM iteration that updates the two parameters.but even small effects can be significant on a test setof about 4,000 messages.
More importantly, notethat the beta-binomial and zero-inflated binomialmodels outperform both the simple binomial and theBernoulli, except on unrealistically small vocabu-laries (intuitively, 20 words are hardly adequate fordiscriminating between 20 newsgroups, and thosewords would have to be selected much more care-fully).
In light of this we can revise McCallum andNigam?s McCallum and Nigam (1998) recommen-dation to use the Bernoulli distribution for small vo-cabularies.
Instead we recommend that neither theBernoulli nor the binomial distributions should beused, since in all reasonable cases they are outper-formed by the more robust variants of the binomialdistribution.
(The case of a 20,000 word vocabularyis quickly declared unreasonable, since most of thewords occur precisely once in the training data, andso any parameter estimate is bound to be unreliable.
)We want to knowwhether the differences betweenthe three binomial models could be dismissed as achance occurrence.
The McNemar test (Dietterich,1998) provides appropriate answers, which are sum-marized in Table 3.
As we can see, the classifi-cation results under the zero-inflated binomial andbeta-binomial models are never significantly differ-Bernoulli Binom ZIBinom BetaBin20 30.94 28.19 29.48 29.9350 45.28 44.04 44.85 45.15100 53.36 52.57 53.84 54.16200 59.72 60.15 60.47 61.16500 66.58 68.30 67.95 68.581,000 69.31 72.24 72.46 73.202,000 71.45 75.92 76.35 77.035,000 73.80 80.64 80.51 80.1910,000 74.18 82.61 82.58 82.5820,000 74.05 83.70 83.06 83.06Table 2: Accuracy of the four models on the News-group data set for different vocabulary sizes.Binom Binom ZIBinomZIBinom BetaBin BetaBin20 7 750 7 7100 7 7200 75001,000 72,000 75,00010,00020,000 7Table 3: Pairwise McNemar test results.
A 7 in-dicates a significant difference of the classificationresults when comparing a pair of of models.ent, in most cases not even approaching significanceat the 5% level.
A classifier based on the beta-binomial model is significantly different from onebased on the binomial model; the difference for avocabulary of 20,000 words is marginally significant(the ?2 value of 3.8658 barely exceeds the criticalvalue of 3.8416 required for significance at the 5%level).
Classification based on the zero-inflated bi-nomial distribution differs most from using a stan-dard binomial model.
We conclude that the zero-inflated binomial distribution captures the relevantextra-binomial variation just as well as the overdis-persed beta-binomial distribution, since their classi-fication results are never significantly different.The differences between the four models can beseen more visually clearly on the WebKB data set707580859020k10k5k2k1k5002001005020classificationaccuracy(percent)vocabulary size (number of word types)WebKB 4BernoulliBinomialZIBinomBetaBinFigure 6: Accuracy of the four models on the Web-KB data set as a function of vocabulary size.
(McCallum and Nigam, 1998, Figure 4).
Evaluationresults for Naive Bayes text classification using thefour models are displayed in Figure 6.
The zero-inflated binomial model provides the overall high-est classification accuracy, and clearly dominates thebeta-binomial model.
Either one should be preferredover the simple binomial model.
The early peakand rapid decline of the Bernoulli model had alreadybeen observed by McCallum and Nigam (1998).We recommend that the zero-inflated binomialdistribution should always be tried first, unless thereis substantial empirical or prior evidence againstit: the zero-inflated binomial model is computation-ally attractive (maximum likelihood estimation us-ing EM is straightforward and numerically stable,most gradient-based methods are not), and its z pa-rameter is independently meaningful, as it can be in-terpreted as the degree to which a given word is ?ontopic?
for a given class of documents.4 ConclusionWe have presented theoretical and empirical evi-dence for zero-inflation among linguistic count data.Zero-inflated models can account for increased vari-ation at least as well as overdispersed models onstandard document classification tasks.
Given thecomputational advantages of simple zero-inflatedmodels, they can and should be used in place of stan-dard models.
For document classification, an eventmodel based on a zero-inflated binomial distribu-tion outperforms conventional Bernoulli and bino-mial models.AcknowledgementsThanks to Chris Brew and three anonymous review-ers for valuable feedback.
Cue the usual disclaimers.ReferencesKenneth W. Church.
2000.
Empirical estimates of adaptation:The chance of two Noriegas is closer to p/2 than p2.
In18th International Conference on Computational Linguis-tics, pages 180?186.
ACL Anthology C00-1027.Kenneth W. Church and William A. Gale.
1995.
Poisson mix-tures.
Natural Language Engineering, 1:163?190.Thomas G. Dietterich.
1998.
Approximate statistical testsfor comparing supervised classification learning algorithms.Neural Computation, 10:1895?1924.Pedro Domingos and Michael J. Pazzani.
1996.
Beyond in-dependence: Conditions for the optimality of the simpleBayesian classifier.
In 13th International Conference on Ma-chine Learning, pages 105?112.Daniel M. Ennis and Jian Bi.
1998.
The beta-binomial model:Accounting for inter-trial variation in replicated differenceand preference tests.
Journal of Sensory Studies, 13:389?412.Ashutosh Garg and Dan Roth.
2001.
Understanding probabilis-tic classifiers.
In 12th European Conference on MachineLearning, pages 179?191.Norman L. Johnson and Samuel Kotz.
1969.
Discrete Distribu-tions, volume 1.
Wiley, New York, NY, first edition.Ken Lang.
1995.
Newsweeder: Learning to filter netnews.
In12th International Conference on Machine Learning, pages331?339.Jack C. Lee and Y. L. Lio.
1997.
A note on Bayesian estima-tion and prediction for the beta-binomial model.
Journal ofStatistical Computation and Simulation, 63:73?91.David D. Lewis.
1998.
Naive (Bayes) at forty: The indepen-dence assumption in information retrieval.
In 10th EuropeanConference on Machine Learning, pages 4?15.Stephen A. Lowe.
1999.
The beta-binomial mixture model forword frequencies in documents with applications to informa-tion retrieval.
In 6th European Conference on Speech Com-munication and Technology, pages 2443?2446.Andrew McCallum and Kamal Nigam.
1998.
A comparisonof event models for naive Bayes text classification.
In AAAIWorkshop on Learning for Text Categorization, pages 41?48.Andrew Kachites McCallum.
1996.
Bow: A toolkit for sta-tistical language modeling, text retrieval, classification andclustering.
http://www.cs.cmu.edu/~mccallum/bow/.Frederick Mosteller and David L. Wallace.
1984.
AppliedBayesian and Classical Inference: The Case of The Fed-eralist Papers.
Springer, New York, NY, second edition.Yudi Pawitan.
2001.
In All Likelihood: Statistical Modellingand Inference Using Likelihood.
Oxford University Press,New York, NY.
