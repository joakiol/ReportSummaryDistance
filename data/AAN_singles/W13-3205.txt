Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 40?49,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsTransducing Sentences to Syntactic Feature Vectors:an Alternative Way to ?Parse?
?Fabio Massimo ZanzottoUniversity of Rome ?Tor Vergata?Via del Politecnico 100133 Roma, Italyfabio.massimo.zanzotto@uniroma2.itLorenzo Dell?ArcipreteUniversity of Rome ?Tor Vergata?Via del Politecnico 100133 Roma, Italylorenzo.dellarciprete@gmail.comAbstractClassification and learning algorithms usesyntactic structures as proxies betweensource sentences and feature vectors.
Inthis paper, we explore an alternative pathto use syntax in feature spaces: the Dis-tributed Representation ?Parsers?
(DRP).The core of the idea is straightforward:DRPs directly obtain syntactic feature vec-tors from sentences without explicitly pro-ducing symbolic syntactic interpretations.Results show that DRPs produce featurespaces significantly better than those ob-tained by existing methods in the sameconditions and competitive with those ob-tained by existing methods with lexical in-formation.1 IntroductionSyntactic processing is widely considered an im-portant activity in natural language understand-ing (Chomsky, 1957).
Research in natural lan-guage processing (NLP) exploits this hypothesisin models and systems.
Syntactic features improveperformance in high level tasks such as questionanswering (Zhang and Lee, 2003), semantic rolelabeling (Gildea and Jurafsky, 2002; Pradhan etal., 2005; Moschitti et al 2008; Collobert et al2011), paraphrase detection (Socher et al 2011),and textual entailment recognition (MacCartney etal., 2006; Wang and Neumann, 2007; Zanzotto etal., 2009).Classification and learning algorithms are keycomponents in the above models and in currentNLP systems, but these algorithms cannot directlyuse syntactic structures.
The relevant parts ofphrase structure trees or dependency graphs areexplicitly or implicitly stored in feature vectors.To fully exploit syntax in learning classi-fiers, kernel machines (Cristianini and Shawe-Taylor, 2000) use graph similarity algorithms(e.g., (Collins and Duffy, 2002) for trees) as struc-tural kernels (Ga?rtner, 2003).
These structural ker-nels allow to exploit high-dimensional spaces ofsyntactic tree fragments by concealing their com-plexity.
These feature spaces, although hidden,still exist.
Then, even in kernel machines, sym-bolic syntactic structures act only as proxies be-tween the source sentences and the syntactic fea-ture vectors.In this paper, we explore an alternative wayto use syntax in feature spaces: the DistributedRepresentation Parsers (DRP).
The core of theidea is straightforward: DRPs directly bridgethe gap between sentences and syntactic featurespaces.
DRPs act as syntactic parsers and fea-ture extractors at the same time.
We leverage onthe distributed trees recently introduced by Zan-zotto&Dell?Arciprete (2012) and on multiple lin-ear regression models.
Distributed trees are smallvectors that encode the large vectors of the syn-tactic tree fragments underlying the tree kernels(Collins and Duffy, 2002).
These vectors effec-tively represent the original vectors and lead toperformances in NLP tasks similar to tree kernels.Multiple linear regression allows to learn linearDRPs from training data.
We experiment with thePenn Treebank data set (Marcus et al 1993).
Re-sults show that DRPs produce distributed trees sig-nificantly better than those obtained by existingmethods, in the same non-lexicalized conditions,and competitive with those obtained by existingmethods with lexical information.
Finally, DRPsare extremely faster than existing methods.The rest of the paper is organized as fol-lows.
First, we present the background of our40idea (Sec.
2).
Second, we fully describe ourmodel (Sec.
3).
Then, we report on the experi-ments (Sec.
4).
Finally, we draw some conclusionsand outline future work (Sec.
5)2 BackgroundClassification and learning algorithms for NLPtasks treat syntactic structures t as vectors in fea-ture spaces ~t ?
Rm.
Each feature generally rep-resents a substructure ?i.
In simple weightingschemes, feature values are 1 if ?i is a substruc-ture of t and 0 otherwise.
Different weightingschemes are used and possible.
Then, learning al-gorithms exploit these feature vectors in differentways.
Decision tree learners (Quinlan, 1993) electthe most representative feature at each iteration,whereas kernel machines (Cristianini and Shawe-Taylor, 2000) exploit similarity between pairs ofinstances, s(t1, t2).
This similarity is generallymeasured as the dot product between the two vec-tors, i.e.
s(t1, t2) = ~t1 ?
~t2.The use of syntactic features changed when treekernels (Collins and Duffy, 2002) appeared.
Treekernels gave the possibility to fully exploit featurespaces of tree fragments.
Until then, learning al-gorithms could not treat these huge spaces.
It isinfeasible to explicitly represent that kind of fea-ture vectors and to directly compute similaritiesthrough dot products.
Tree kernels (Collins andDuffy, 2002), by computing similarities betweentwo trees with tree comparison algorithms, exactlydetermine dot products of vectors in these targetspaces.
After their introduction, different tree ker-nels have been proposed (e.g., (Vishwanathan andSmola, 2002; Culotta and Sorensen, 2004; Mos-chitti, 2006)).
Their use spread in many NLPtasks (e.g., (Zhou et al 2007; Wang and Neu-mann, 2007; Moschitti et al 2008; Zanzotto etal., 2009; Zhang and Li, 2009)) and in other areaslike biology (Vert, 2002; Hashimoto et al 2008)and computer security (Du?ssel et al 2008; Rieckand Laskov, 2007; Bockermann et al 2009).Tree kernels have played a very important rolein promoting the use of syntactic information inlearning classifiers, but this method obfuscated thefact that syntactic trees are ultimately used as vec-tors in learning algorithms.
To work with theidea of directly obtaining rich syntactic featurevectors from sentences, we need some techniquesto make these high-dimensional vectors again ex-plicit, through smaller but expressive vectors.A solution to the above problem stems fromthe recently revitalized research in DistributedRepresentations (DR) (Hinton et al 1986; Ben-gio, 2009; Collobert et al 2011; Socher et al2011; Zanzotto and Dell?Arciprete, 2012).
Dis-tributed Representations, studied in opposition tosymbolic representations (Rumelhart and Mcclel-land, 1986), are methods for encoding data struc-tures such as trees into vectors, matrices, or high-order tensors.
The targets of these representa-tions are generally propositions, i.e., flat tree struc-tures.
The Holographic Reduced Representations(HRR), proposed by Plate (1994), produce nearlyorthogonal vectors for different structures by com-bining circular convolution and randomly gener-ated vectors for basic components (as in (Ander-son, 1973; Murdock, 1983)).Building on HRRs, Distributed Trees (DT) havebeen proposed to encode deeper trees in low di-mensional vectors (Zanzotto and Dell?Arciprete,2012).
DTs approximate the feature space of treefragments defined for the tree kernels (Collins andDuffy, 2002) and guarantee similar performancesof classifiers in NLP tasks such as question classi-fication and textual entailment recognition.
Thus,Distributed Trees are good representations of syn-tactic trees, that we can use in our definition ofdistributed representation parsers (DRPs).3 Distributed Representation ParsersIn this section, first, we sketch the idea of Dis-tributed Representation ?Parsers?
(DRPs).
Then,we review the distributed trees as a way to repre-sent trees in low dimensional vectors.
Finally, wedescribe how to build DRPs by mixing a functionthat encodes sentences in vectors and a linear re-gressor that can be induced from training data.3.1 The IdeaThe approach to using syntax in learning algo-rithms generally follows two steps: first, parsesentences s with a symbolic parser (e.g., (Collins,2003; Charniak, 2000; Nivre et al 2007)) andproduce symbolic trees t; second, use an en-coder to build syntactic feature vectors.
Fig-ure 1 sketches this idea when the final vectorsare the distributed trees;t ?
Rd (Zanzotto andDell?Arciprete, 2012)1.
In this case, the last step1To represent a distributed tree for a tree t, we use thenotation;t to stress that this small vector is an approximationof the original high-dimensional vector ~t in the space of tree41s?We booked the flight?
?SymbolicParser(SP)?tSPPPNPPRPWeVPaa!!VbookedNPQDTtheNNflight?DistributedTreeEncoder(DT)?
;t ?
Rd??????0.00024350.00232...?0.007325?????
?Distributed Representation Parser (DRP);s ?
RdD ????????0.00172450.0743869...0.0538474???????
PFigure 1: ?Parsing?
with distributed structures in perspectiveis the Distributed Tree Encoder (DT).Our proposal is to build a Distributed Represen-tation ?Parser?
(DRP) that directly maps sentencess into the final vectors.
We choose the distributedtrees;t as these reduced vectors fully represent thesyntactic trees.
A DRP acts as follows (see Fig-ure 1): first, a function D encodes sentence s intoa distributed vector ;s ?
Rd; second, a functionP transforms the input vector ;s into a distributedtree;t .
This second step is a vector to vector trans-formation and, in a wide sense, ?parses?
the inputsentence.Given an input sentence s, a DRP is then afunction defined as follows:;t = DRP (s) = P (D(s)) (1)In this paper, we design some functions D and wepropose a linear function P , designed to be a re-gressor that can be induced from training data.
Inthis study, we use a space with d dimensions forboth sentences ;s and distributed trees;t , but, ingeneral, these spaces can be of different size.3.2 Syntactic Trees as Distributed VectorsWe here report on the distributed trees2 (Zan-zotto and Dell?Arciprete, 2012) to describe howthese vectors represent syntactic trees and how thedot product between two distributed trees approxi-mates the tree kernel defined by Collins and Duffy(2002).fragments.2For the experiments, we used the implemen-tation of the distributed tree encoder available athttp://code.google.com/p/distributed-tree-kernels/Given a tree t, the corresponding distributed tree;t is defined as follows:DT (t) =??i?S(t)?i;?
i (2)where S(t) is the set of the subtrees ?i of t,;?
iis the small vector corresponding to tree fragment?i and ?i is the weight of subtree ?i in the finalfeature space.
As in (Collins and Duffy, 2002), theset S(t) contains tree fragments ?
such that theroot of ?
is any non-terminal node in t and, if ?contains node n, it must contain all the siblings ofn in t (see, for example, Slex(t) in Figure 2).
Theweight ?i is defined as:?i =????
?|?i|?1 if |?i| > 1 and ?
6= 01 if |?i| = 10 if ?
= 0(3)where |?i| is the number of non-terminal nodes oftree fragment ?i and ?
is the traditional parame-ter used to penalize large subtrees.
For ?
= 0,?i has a value 1 for productions and 0 other-wise.
If different tree fragments are associatedto nearly orthonormal vectors, the dot product;t1 ?
;t2 approximates the tree kernel (Zanzotto andDell?Arciprete, 2012).A key feature of the distributed tree fragments;?
is that these vectors are built compositionallyfrom a set N of nearly orthonormal random vec-tors;n , associated to node labels n. Given a sub-tree ?
, the related vector is obtained as:;?
= ;n1 ?
;n2 ?
.
.
.?
;nk42Sno lex(t) = {SQNP VP,VPZV NP,NPPRP,SZNPPRPVP ,SHHNP VPZV NP,VPHHV NPc#DT NN, .
.
.
}Slex(t) = Sno lex(t) ?
{SZNPPRPWeVP,VPaa!
!VbookedNPc#DT NN,VPaa!!VbookedNPc#DTtheNN,VPaa!
!VbookedNPQDT NNflight, .
.
.
}Figure 2: Subtrees of the tree t in Figure 1where node vectors ;n i are ordered according to adepth-first visit of subtree ?
and ?
is a vector com-position operation, specifically the shuffled circu-lar convolution3 .
This function guarantees thattwo different subtrees have nearly orthonormalvectors (see (Zanzotto and Dell?Arciprete, 2012)for more details).
For example, the fifth tree ?5 ofset Sno lex(t) in Figure 2 is:;?
5 =;S ?
(;NP ?
(;V P ?
(;V ?
;NP )))We experiment with two tree fragment sets:the non-lexicalized set Sno lex(t), where tree frag-ments do not contain words, and the lexicalizedset Slex(t), including all the tree fragments.
Anexample is given in Figure 2.3.3 The ModelTo build a DRP, we need to define the encoderD and the transformer P .
In the following, wepresent a non-lexicalized and a lexicalized modelfor the encoder D and we describe how we canlearn the transformer P by means of a linear re-gression model.3.3.1 Sentence EncodersEstablishing good models to encode input sen-tences into vectors is the most difficult challenge.The models should consider the kind of informa-tion that can lead to a correct syntactic interpre-tation.
Only in this way, the distributed repre-sentation parser can act as a vector transformingmodule.
Unlike in models such as (Socher et al2011), we want our encoder to represent the wholesentence as a fixed size vector.
We propose a non-lexicalized model and a lexicalized model.3The shuffled circular convolution ?
is defined as ~a?~b =s1(~a)?
s2(~b) where ?
is the circular convolution and s1 ands2 are two different random permutations of vector elements.Non-lexicalized model The non-lexicalizedmodel relies only on the pos-tags of the sentencess: s = p1 .
.
.
pn where pi is the pos-tag associatedwith the i-th token of the sentence.
In the follow-ing we discuss how to encode this information ina Rd space.
The basic model D1(s) is the one thatconsiders the bag-of-postags, that is:D1(s) =?i;p i (4)where ;p i ?
N is the vector for label pi, takenfrom the set of nearly orthonomal random vectorsN .
It is basically in line with the bag-of-wordmodel used in random indexing (Sahlgren, 2005).Due to the commutative property of the sum andsince vectors in N are nearly orthonormal: (1)two sentences with the same set of pos-tags havethe same vector; and, (2) the dot product betweentwo vectors, D1(s1) and D1(s2), representing sen-tences s1 and s2, approximately counts how manypos-tags the two sentences have in common.
Thevector for the sentence in Figure 1 is then:D1(s) =;PRP +;V +;DT +;NNThe general non-lexicalized model that takesinto account all n-grams of pos-tags, up to lengthj, is then the following:Dj(s) = Dj?1(s) +?i;p i ?
.
.
.?
;p i+j?1where ?
is again the shuffled circular convolution.An n-gram pi .
.
.
pi+j?1 of pos-tags is representedas;p i ?
.
.
.
?
;p i+j?1.
Given the properties ofthe shuffled circular convolution, an n-gram ofpos-tags is associated to a versor, as it composesj versors, and two different n-grams have nearlyorthogonal vectors.
For example, vector D3(s)for the sentence in Figure 1 is:43D3(s) =;PRP +;V +;DT +;NN +;PRP ?
;V +;V ?
;DT +;DT ?
;NN +;PRP ?
;V ?
;DT +;V ?
;DT ?
;NNLexicalized model Including lexical informa-tion is the hardest part of the overall model, asit makes vectors denser in information.
Herewe propose an initial model that is basically asthe non-lexicalized model, but includes a vectorrepresenting the words in the unigrams.
Theequation representing sentences as unigrams is:Dlex1 (s) =?i;p i ?
;wiVector ;wi represents word wi and is taken from theset N of nearly orthonormal random vectors.
Thisguarantees that Dlex1 (s) is not lossy.
Given a pairword-postag (w, p), it is possible to know if thesentence contains this pair, as Dlex1 (s)?;p?
;w ?
1if (w, p) is in sentence s and Dlex1 (s)?
;p ?
;w ?
0otherwise.
Other vectors for representing words,e.g., distributional vectors or those obtained aslook-up tables in deep learning architectures (Col-lobert and Weston, 2008), do not guarantee thispossibility.The general equation for the lexicalized versionof the sentence encoder follows:Dlexj (s) = Dlexj?1(s) +?i;p i ?
.
.
.?
;p i+j?1This model is only an initial proposal in orderto take into account lexical information.3.3.2 Learning Transformers with LinearRegressionThe transformer P of the DRP (see Equation 1)can be seen as a linear regressor:;t = P;s (5)where P is a square matrix.
This latter can be esti-mated having training sets (T,S) of oracle vectorsand sentence input vectors (;t i,;s i) for sentencessi.
Interpreting these sets as matrices, we need tosolve a linear set of equations, i.e.
: T = PS.An approximate solution can be computed us-ing Principal Component Analysis and PartialLeast Square Regression4.
This method relies on4An implementation of this method is available within theR statistical package (Mevik and Wehrens, 2007).Moore-Penrose pseudo-inversion (Penrose, 1955).Pseudo-inverse matrices S+ are obtained usingsingular value decomposition (SVD).
Matriceshave the property SS+ = I.
Using the itera-tive method for computing SVD (Golub and Ka-han, 1965), we can obtain different approxima-tions S+(k) of S+ considering k singular values.
Fi-nal approximations of DRP s are then: P(k) =TS+(k).Matrices P are estimated by pseudo-invertingmatrices representing input vectors for sentencesS.
Given the different input representations forsentences, we can then estimate different DRPs:DRP1 = TS+1 , DRP2 = TS+2 , and so on.
Weneed to estimate the best k in a separate parameterestimation set.4 ExperimentsWe evaluated three issues for assessing DRP mod-els: the performance of DRPs in reproducing or-acle distributed trees (Sec.
4.2); the quality of thetopology of the vector spaces of distributed treesinduced by DRPs (Sec.
4.3); and the computationrun time of DRPs (Sec.
4.4).
Section 4.1 describesthe experimental set-up.4.1 Experimental Set-upData We derived the data sets from the WallStreet Journal (WSJ) portion of the English PennTreebank data set (Marcus et al 1993), usinga standard data split for training (sections 2-21PTtrain with 39,832 trees) and for testing (section23 PT23 with 2,416 trees).
We used section 24PT24 with 1,346 trees for parameter estimation.We produced the final data sets of distributedtrees with three different ?
values: ?=0, ?=0.2,and ?=0.4.
For each ?, we have two ver-sions of the data sets: a non-lexicalized version(no lex), where syntactic trees are consideredwithout words, and a lexicalized version (lex),where words are considered.
Oracle trees t aretransformed into oracle distributed trees ;o usingthe Distributed Tree Encoder DT (see Figure 1).We experimented with two sizes of the distributedtrees space Rd: 4096 and 8192.We have designed the data sets to determinehow DRPs behave with ?
values relevant forsyntax-sensitive NLP tasks.
Both tree kernels anddistributed tree kernels have the best performancesin tasks such as question classification, seman-tic role labeling, or textual entailment recognition44with ?
values in the range 0?0.4.System Comparison We compared the DRPsagainst the existing way of producing distributedtrees (based on the recent paper described in(Zanzotto and Dell?Arciprete, 2012)): distributedtrees are obtained using the output of a sym-bolic parser (SP) that is then transformed into adistributed tree using the DT with the appropri-ate ?.
We refer to this chain as the DistributedSymbolic Parser (DSP ).
The DSP is then thechain DSP (s) = DT (SP (s)) (see Figure 1).As for the symbolic parser, we used Bikel?s ver-sion (Bikel, 2004) of Collins?
head-driven statisti-cal parser (Collins, 2003).
For a correct compar-ison, we used the Bikel?s parser with oracle part-of-speech tags.
We experimented with two ver-sions: (1) a lexicalized method DSPlex, i.e., thenatural setting of the Collins/Bikel parser, and (2)a fully non-lexicalized version DSPno lex that ex-ploits only part-of-speech tags.
We obtained thislast version by removing words in input sentencesand leaving only part-of-speech tags.
We trainedthese DSP s on PTtrain.Parameter estimation DRPs have two basic pa-rameters: (1) parameter k of the pseudo-inverse,that is, the number of considered eigenvectors (seeSection 3.3.2) and (2) the maximum length j of then-grams considered by the encoder Dj (see Sec-tion 3.3.1).
We performed the parameter estima-tion on the datasets derived from section PT24 bymaximizing a pseudo f-measure.
Section 4.2 re-ports both the definition of the measure and theresults of the parameter estimation.4.2 Parsing PerformanceThe first issue to explore is whether DRP s areactually good ?distributed syntactic parsers?.
Wecompare DRP s against the distributed symbolicparsers by evaluating how well these ?distributedsyntactic parsers?
reproduce oracle distributedtrees.Method A good DRP should produce dis-tributed trees that are similar to oracle distributedtrees.
To capture this, we use the cosine similaritybetween the system and the oracle vectors:cos(;t ,;o ) =;t ?
;o||;t ||||;o ||where;t is the system?s distributed tree and ;ois the oracle distributed tree.
We compute thesedim Model ?
= 0 ?
= 0.2 ?
= 0.44096DRP1 0.6285 0.5697 0.542DRP2 0.8011 0.7311 0.631DRP3 0.8276?
0.7552 ?
0.6506?DRP4 0.8171 0.744 0.6419DRP5 0.8045 0.7342 0.631DSPno lex 0.654 0.5884 0.4835DSPlex 0.815 0.7813 0.71218192DRP3 0.8335?
0.7605?
0.6558?DSPno lex 0.6584 0.5924 0.4873DSPlex 0.8157 0.7815 0.7123Table 1: Average similarity on PT23 of theDRPs (with different j) and the DSP on the non-lexicalized data sets with different ?s and with thetwo dimensions of the distributed tree space (4096and 8192).
?
indicates significant difference wrt.DSPno lex (p << .005 computed with the Stu-dent?s t test)Model ?
= 0 ?
= 0.2 ?
= 0.4DRP3 0.7192 0.6406 0.0646DSPlex 0.9073 0.8564 0.6459Table 2: Average similarity on PT23 of the DRP3and the DSPlex on the lexicalized data sets withdifferent ?s on the distributed tree space with 4096dimensionsthe cosine similarity at the sentence-based (i.e.,vector-based) granularity.
Results report averagevalues.Estimated parameters We estimated parame-ters k and j by training the different DRP s onthe PTtrain set and by maximizing the similarityof the DRP s on PT24.
The best pair of param-eters is j=3 and k=3000.
For completeness, wereport also the best k values for the five differentj we experimented with: k = 47 for j=1 (the lin-early independent vectors representing pos-tags),k = 1300 for j=2, k = 3000 for j=3, k = 4000for j=4, and k = 4000 for j=5.
For comparison,some resulting tables report results for the differ-ent values of j.Results Table 1 reports the results of the first setof experiments on the non-lexicalized data sets.The first block of rows (seven rows) reports the av-erage cosine similarity of the different methods onthe distributed tree spaces with 4096 dimensions.The second block (the last three rows) reports theperformance on the space with 8192 dimensions.The average cosine similarity is computed on thePT23 set.
Although we already selected j=3 asthe best parameterization (i.e.
DRP3), the first45Output Model ?
= 0 ?
= 0.2 ?
= 0.4No lexDRP3 0.9490 0.9465 0.9408DSPno lex 0.9033 0.9001 0.8932DSPlex 0.9627 0.9610 0.9566Lex DRP3 0.9642 0.9599 0.0025DSPlex 0.9845 0.9817 0.9451Table 3: Average Spearman?s Correlation: dim4096 between the oracle?s vector space and thesystems?
vector spaces (100 trials on lists of 1000sentence pairs).five rows of the first block report the results of theDRPs for five values of j.
This gives an idea ofhow the different DRPs behave.
The last two rowsof this block report the results of the two DSPs.We can observe some important facts.
First,DRP s exploiting 2-grams, 3-grams, 4-grams, and5-grams of part-of-speech tags behave signifi-cantly better than the 1-grams for all the valuesof ?.
Distributed representation parsers need in-puts that keep trace of sequences of pos-tags ofsentences.
But these sequences tend to confusethe model when too long.
As expected, DRP3behaves better than all the other DRPs.
Second,DRP3 behaves significantly better than the com-parable traditional parsing chain DSPno lex thatuses only part-of-speech tags and no lexical in-formation.
This happens for all the values of ?.Third, DRP3 behaves similarly to DSPlex for?=0.
Both parsers use oracle pos tags to emit sen-tence interpretations but DSPlex also exploits lex-ical information that DRP3 does not access.
For?=0.2 and ?=0.4, the more informed DSPlex be-haves significantly better than DRP3.
But DRP3still behaves significantly better than the compa-rable DSPno lex.
All these observations are validalso for the results obtained for 8192 dimensions.Table 2 reports the results of the second set ofexperiments on the lexicalized data sets performedon a 4192-dimension space.
The first row reportsthe average cosine similarity of DRP3 trained onthe lexicalized model and the second row reportsthe results of DSPlex.
In this case, DRP3 is notbehaving well with respect to DSPlex.
The addi-tional problem DRP3 has is that it has to repro-duce input words in the output.
This greatly com-plicates the work of the distributed representationparser.
But, as we report in the next section, thispreliminary result may be still satisfactory for ?=0and ?=0.2.Figure 3: Topology of the resulting spaces derivedwith the three different methods: similarities be-tween sentences4.3 Kernel-based PerformanceThis experiment investigates how DRP s preservethe topology of the oracle vector space.
This cor-relation is an important quality factor of a dis-tributed tree space.
When using distributed treevectors in learning classifiers, whether ;oi ?
;oj inthe oracle?s vector space is similar to;ti ?
;tj inthe DRP?s vector space is more important thanwhether ;oi is similar to;ti (see Figure 3).
Sen-tences that are close using the oracle syntactic in-terpretations should also be close using DRP vec-tors.
The topology of the vector space is more rel-evant than the actual quality of the vectors.
Theexperiment on the parsing quality in the previoussection does not properly investigate this property,as the performance of DRPs could be not sufficientto preserve distances among sentences.Method We evaluate the coherence of the topol-ogy of two distributed tree spaces by measuringthe Spearman?s correlation between two lists ofpairs of sentences (si, sj), ranked according to thesimilarity between the two sentences.
If the twolists of pairs are highly correlated, the topologyof the two spaces is similar.
The different meth-ods and, thus, the different distributed tree spacesare compared against the oracle vector space (seeFigure 3).
Then, the first list always represents theoracle vector space and ranks pairs (si, sj) accord-ing to ;o i ?
;o j .
The second list instead representsthe space obtained with a DSP or a DRP.
Thus, itis respectively ranked with;t?i ?
;t?j or;ti ?
;tj .
In thisway, we can comparatively evaluate the quality ofthe distributed tree vectors of our DRP s with re-spect to the other methods.
We report average andstandard deviation of the Spearman?s correlationon 100 runs over lists of 1000 pairs.
We used thetesting set PT23 for extracting vectors.46Figure 4: Running time with respect to the sen-tence length (dimension = 4092)Results Table 3 reports results both on the non-lexicalized and on the lexicalized data set.
Forthe non-lexicalized data set we report three meth-ods (DRP3, DSPno lex, and DSPlex) and for thelexicalized dataset we report two methods (DRP3and DSPlex).
Columns represent different valuesof ?.
Experiments are carried out on the 4096-dimension space.
For the non-lexicalized data set,distributed representation parsers behave signifi-cantly better than DSPno lex for all the values of?.
The upper-bound of DSPlex is not so far.
Forthe harder lexicalized data set, the difference be-tween DRP3 and DSPlex is smaller than the onebased on the parsing performance.
Thus, we havemore evidence of the fact that we are in a goodtrack.
DRP s can substitute the DSP in generatingvector spaces of distributed trees that adequatelyapproximate the space defined by an oracle.4.4 Running TimeIn this last experiment, we compared the runningtime of the DRP with respect to the DSP .
Theanalysis has been done on a dual-core processorand both systems are implemented in the sameprogramming language, i.e.
Java.
Figure 4 plotsthe running time of the DRP , the SP , and thefull DSP = DT ?
SP .
The x-axis represents thesentence length in words and the y-axis representsthe running time in milliseconds.
The distance be-tween SP and DSP shrinks as the plot is in a log-arithmic scale.
Figure 5 reports the average co-sine similarity of DRP , DSPlex, and DSPno lex,with respect to the sentence length, on the non-lexicalized data set with ?=0.4.We observe that DRP becomes extremely con-venient for sentences larger than 10 words (seeFig.
4) and the average cosine similarity differencebetween the different methods is nearly constantfor the different sentence lengths (see Fig.
5).
Thistest already makes DRPs very appealing methodsfor real time applications.
But, if we consider thatFigure 5: Average similarity with ?=0.4 with re-spect to the sentence length (dimension = 4092)DRPs can run completely on Graphical ProcessingUnits (GPUs), as dealing only with matrix prod-ucts, fast-Fourier transforms, and random genera-tors, we can better appreciate the potentials of theproposed methods.5 Conclusions and Future WorkWe presented Distributed Representation Parsers(DRP) as a novel path to use syntactic structuresin feature spaces.
We have shown that these?parsers?
can be learnt using training data and thatDRPs are competitive with respect to traditionalmethods of using syntax in feature spaces.This novel path to use syntactic structures infeature spaces opens interesting and unexploredpossibilities.
First, DRPs tackle the issue of com-putational efficiency of structural kernel methods(Rieck et al 2010; Shin et al 2011) from anotherperspective.
DRPs could reduce structural kernelcomputations to extremely efficient dot products.Second, the tight integration of parsing and featurevector generation lowers the computational cost ofproducing distributed representations from trees,as circular convolution is not applied on-line.Finally, DRPs can contribute to treat syntax indeep learning models in a uniform way.
Deeplearning models (Bengio, 2009) are completelybased on distributed representations.
But whenapplied to natural language processing tasks (e.g.,(Collobert et al 2011; Socher et al 2011)), syn-tactic structures are not represented in the neuralnetworks in a distributed way.
Syntactic informa-tion is generally used by exploiting symbolic parsetrees, and this information positively impacts per-formances on final applications, e.g., in paraphrasedetection (Socher et al 2011) and in semantic rolelabeling (Collobert et al 2011).
Building on theresults presented here, an interesting line of re-search is then the integration of distributed repre-sentation parsers and deep learning models.47ReferencesJames A. Anderson.
1973.
A theory for the recognitionof items from short memorized lists.
PsychologicalReview, 80(6):417 ?
438.Yoshua Bengio.
2009.
Learning deep architectures forai.
Foundations and Trends in Machine Learning,2(1):1?127.Daniel M. Bikel.
2004.
Intricacies of collins?
parsingmodel.
Comput.
Linguist., 30:479?511, December.Christian Bockermann, Martin Apel, and MichaelMeier.
2009.
Learning sql for database intrusion de-tection using context-sensitive modelling.
In Detec-tion of Intrusions andMalware & Vulnerability As-sessment (DIMVA), pages 196?205.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proc.
of the 1st NAACL, pages132?139, Seattle, Washington.Naom Chomsky.
1957.
Aspect of Syntax Theory.
MITPress, Cambridge, Massachussetts.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
In Pro-ceedings of ACL02.Michael Collins.
2003.
Head-driven statistical mod-els for natural language parsing.
Comput.
Linguist.,29(4):589?637.R.
Collobert and J. Weston.
2008.
A unified architec-ture for natural language processing: Deep neuralnetworks with multitask learning.
In InternationalConference on Machine Learning, ICML.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
J. Mach.
Learn.
Res., 12:2493?2537,November.Nello Cristianini and John Shawe-Taylor.
2000.
AnIntroduction to Support Vector Machines and OtherKernel-based Learning Methods.
Cambridge Uni-versity Press, March.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedingsof the 42nd Annual Meeting on Association for Com-putational Linguistics, ACL ?04, Stroudsburg, PA,USA.
Association for Computational Linguistics.Patrick Du?ssel, Christian Gehl, Pavel Laskov, and Kon-rad Rieck.
2008.
Incorporation of application layerprotocol syntax into anomaly detection.
In Proceed-ings of the 4th International Conference on Infor-mation Systems Security, ICISS ?08, pages 188?202,Berlin, Heidelberg.
Springer-Verlag.Thomas Ga?rtner.
2003.
A survey of kernels for struc-tured data.
SIGKDD Explorations.Daniel Gildea and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Computational Lin-guistics, 28(3):245?288.Gene Golub and William Kahan.
1965.
Calculat-ing the singular values and pseudo-inverse of a ma-trix.
Journal of the Society for Industrial and Ap-plied Mathematics, Series B: Numerical Analysis,2(2):205?224.Kosuke Hashimoto, Ichigaku Takigawa, Motoki Shiga,Minoru Kanehisa, and Hiroshi Mamitsuka.
2008.Mining significant tree patterns in carbohydratesugar chains.
Bioinformatics, 24:i167?i173, Au-gust.G.
E. Hinton, J. L. McClelland, and D. E. Rumel-hart.
1986.
Distributed representations.
In D. E.Rumelhart and J. L. McClelland, editors, Paral-lel Distributed Processing: Explorations in the Mi-crostructure of Cognition.
Volume 1: Foundations.MIT Press, Cambridge, MA.Bill MacCartney, Trond Grenager, Marie-Catherinede Marneffe, Daniel Cer, and Christopher D. Man-ning.
2006.
Learning to recognize features of validtextual entailments.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Main Conference, pages 41?48, New York City,USA, June.
Association for Computational Linguis-tics.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of en-glish: The penn treebank.
Computational Linguis-tics, 19:313?330.Bjrn-Helge Mevik and Ron Wehrens.
2007.
Thepls package: Principal component and partial leastsquares regression in r. Journal of Statistical Soft-ware, 18(2):1?24, 1.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role label-ing.
Computational Linguistics, 34(2):193?224.Alessandro Moschitti.
2006.
Efficient ConvolutionKernels for Dependency and Constituent SyntacticTrees.
In Proceedings of The 17th European Con-ference on Machine Learning, Berlin, Germany.Bennet B. Murdock.
1983.
A distributed memorymodel for serial-order information.
PsychologicalReview, 90(4):316 ?
338.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, Glsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Roger Penrose.
1955.
A generalized inverse for matri-ces.
In Proc.
Cambridge Philosophical Society.T.
A.
Plate.
1994.
Distributed Representations andNested Compositional Structure.
Ph.D. thesis.48Sameer Pradhan, Wayne Ward, Kadri Hacioglu,James H. Martin, and Daniel Jurafsky.
2005.
Se-mantic role labeling using different syntactic views.In ACL ?05: Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 581?588.
Association for Computational Lin-guistics, Morristown, NJ, USA.J.
Quinlan.
1993.
C4:5:programs for Machine Learn-ing.
Morgan Kaufmann, San Mateo.Konrad Rieck and Pavel Laskov.
2007.
Languagemodels for detection of unknown attacks in networktraffic.
Journal in Computer Virology, 2:243?256.10.1007/s11416-006-0030-0.Konrad Rieck, Tammo Krueger, Ulf Brefeld, andKlaus-Robert Mu?ller.
2010.
Approximate tree ker-nels.
J. Mach.
Learn.
Res., 11:555?580, March.David E. Rumelhart and James L. Mcclelland.
1986.Parallel Distributed Processing: Explorations in theMicrostructure of Cognition : Foundations (ParallelDistributed Processing).
MIT Press, August.Magnus Sahlgren.
2005.
An introduction to randomindexing.
In Proceedings of the Methods and Appli-cations of Semantic Indexing Workshop at the 7th In-ternational Conference on Terminology and Knowl-edge Engineering TKE, Copenhagen, Denmark.Kilho Shin, Marco Cuturi, and Tetsuji Kuboyama.2011.
Mapping kernels for trees.
In Lise Getoorand Tobias Scheffer, editors, Proceedings of the28th International Conference on Machine Learning(ICML-11), ICML ?11, pages 961?968, New York,NY, USA, June.
ACM.Richard Socher, Eric H. Huang, Jeffrey Pennington,Andrew Y. Ng, and Christopher D. Manning.
2011.Dynamic pooling and unfolding recursive autoen-coders for paraphrase detection.
In Advances inNeural Information Processing Systems 24.Jean-Philippe Vert.
2002.
A tree kernel to anal-yse phylogenetic profiles.
Bioinformatics, 18(suppl1):S276?S284, July.S.
V. N. Vishwanathan and Alexander J. Smola.
2002.Fast kernels for string and tree matching.
In SuzannaBecker, Sebastian Thrun, and Klaus Obermayer, ed-itors, NIPS, pages 569?576.
MIT Press.Rui Wang and Gu?nter Neumann.
2007.
Recognizingtextual entailment using sentence similarity based ondependency tree skeletons.
In Proceedings of theACL-PASCAL Workshop on Textual Entailment andParaphrasing, pages 36?41, Prague, June.
Associa-tion for Computational Linguistics.F.M.
Zanzotto and L. Dell?Arciprete.
2012.
Dis-tributed tree kernels.
In Proceedings of Interna-tional Conference on Machine Learning, pages 193?200.Fabio Massimo Zanzotto, Marco Pennacchiotti, andAlessandro Moschitti.
2009.
A machine learningapproach to textual entailment recognition.
NATU-RAL LANGUAGE ENGINEERING, 15-04:551?582.Dell Zhang and Wee Sun Lee.
2003.
Question classi-fication using support vector machines.
In Proceed-ings of the 26th annual international ACM SIGIRconference on Research and development in infor-maion retrieval, SIGIR ?03, pages 26?32, New York,NY, USA.
ACM.Min Zhang and Haizhou Li.
2009.
Tree kernel-basedSVM with structured syntactic knowledge for BTG-based phrase reordering.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 698?707, Singapore, Au-gust.
Association for Computational Linguistics.GuoDong Zhou, Min Zhang, DongHong Ji, andQiaoMing Zhu.
2007.
Tree kernel-based relationextraction with context-sensitive structured parsetree information.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 728?736.49
