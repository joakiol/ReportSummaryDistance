Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 953?960Manchester, August 2008Using Syntactic Information for Improving Why-Question AnsweringSuzan Verberne, Lou Boves, Nelleke Oostdijk and Peter-Arno CoppenDepartment of LinguisticsRadboud University Nijmegens.verberne@let.ru.nlAbstractIn this paper, we extend an existing para-graph retrieval approach to why-questionanswering.
The starting-point is a systemthat retrieves a relevant answer for 73%of the test questions.
However, in 41%of these cases, the highest ranked relevantanswer is not ranked in the top-10.
Weaim to improve the ranking by adding a re-ranking module.
For re-ranking we con-sider 31 features pertaining to the syntacticstructure of the question and the candidateanswer.
We find a significant improvementover the baseline for both success@10 andMRR@150.
The most important featuresfor re-ranking are the baseline score, thepresence of cue words, the question?s mainverb, and the relation between question fo-cus and document title.1 IntroductionRecently, some research has been directed at prob-lems involved in why-question answering (why-QA).
About 5% of all questions asked to QAsystems are why-questions (Hovy et al, 2002).They need a different approach from factoid ques-tions, since their answers cannot be stated in a sin-gle phrase.
Instead, a passage retrieval approachseems more suitable.
In (Verberne et al, 2008),we proposed an approach to why-QA that is basedon paragraph retrieval.
We reported mediocre per-formance and suggested that adding linguistic in-formation may improve ranking power.c?Suzan Verberne, 2008.
Licensed under the CreativeCommons Attribution-Noncommercial-Share Alike 3.0 Un-ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.In the present paper, we implement a simi-lar paragraph retrieval approach and extend it byadding a re-ranking module based on structural lin-guistic information.
Our aim is to find out whethersyntactic knowledge is relevant for discovering re-lations between question and answer, and if so,which type of information is the most beneficial.In the following sections, we first discuss relatedwork (section 2).
In sections 3 and 4, we introducethe data that we used for development purposesand the baseline retrieval and ranking method thatwe implemented.
In section 5, we present our re-ranking method and the results obtained, followedby a discussion in section 6, and directions for fur-ther research in section 7.2 Related workA substantial amount of work has been done inimproving QA by adding syntactic information(Tiedemann, 2005; Quarteroni et al, 2007; Hi-gashinaka and Isozaki, 2008).
All these studiesshow that syntactic information gives a small butsignificant improvement on top of the traditionalbag-of-words (BOW) approaches.The work of (Higashinaka and Isozaki, 2008)focuses on the problem of ranking candidate an-swer paragraphs for Japanese why-questions.
Theyfind a success@10 score of 70.3% with an MRRof 0.328.
They conclude that their system forJapanese is the best-performing fully implementedwhy-QA system.
In (Tiedemann, 2005), passageretrieval for Dutch factoid QA is enriched withsyntactic information from dependency structures.The baseline approach, using only the BOW, re-sulted in an MRR of 0.342.
With the addition ofsyntactic structure, MRR improved to 0.406.The work by (Quarteroni et al, 2007) consid-ers the problem of answering definition questions.953They use predicate-argument structures (PAS) forimproved answer ranking.
Their results show thatPAS make a very small contribution compared toBOW only (F-scores 70.7% vs. 69.3%).The contribution of this paper is twofold: (1) weconsider the relatively new problem of why-QA forEnglish and (2) we not only improve a simple pas-sage retrieval approach by adding syntactic infor-mation but we also perform extensive feature se-lection in order to find out which syntactic featurescontribute to answer ranking and to what extent.3 DataAs data for developing and testing our systemfor why-QA, we use the Webclopedia question setby (Hovy et al, 2002).
This set contains ques-tions that were asked to the online QA systemanswers.com.
805 of these questions are why-questions.
As answer corpus, we use the off-lineWikipedia XML corpus, which consists of 659,388articles (Denoyer and Gallinari, 2006).
We manu-ally inspect a sample of 400 of the Webclopediawhy-questions.
Of these, 93 have an answer in theWikipedia corpus.
Manual extraction of one rele-vant answer for each of these questions results in aset of 93 why-questions and their reference answer.We also save the title of the Wikipedia article inwhich each of the answers is embedded, in orderto be able to evaluate document retrieval togetherwith answer retrieval.4 Paragraph retrieval for why-QA4.1 Baseline methodWe index the Wikipedia XML corpus using theWumpus Search Engine (Buttcher, 2007).
InWumpus, queries can be formulated in the GCLformat, which is especially geared to retrievingXML items.
Since we consider paragraphs as re-trieval units, we let the engine retrieve text frag-ments marked with ?p?
as candidate answers.We implement a baseline method for questionanalysis in which first stop words are removed1.Also, any punctuation is removed from the ques-tion.
What remains is a set of question contentwords.
Next, we automatically create a query foreach question that retrieves paragraphs containing(a subset of) these question terms.
For ranking1To this end the stop word list is used that can be foundat http://marlodge.supanet.com/museum/ funcword.html.
Weuse all categories except the numbers and the word whythe paragraphs retrieved, we use the QAP algo-rithm created by MultiText, which has been im-plemented in Wumpus.
QAP is a passage scor-ing algorithm specifically developed for QA tasks(Buttcher et al, 2004).
For each question, we re-trieve and rank the top 150 of highest scoring an-swer candidates.4.2 Evaluation methodFor evaluation of the results, we perform manualassessment of all answers retrieved, starting at thehighest-ranked answer and ending as soon as weencounter a relevant answer2.
Then we count theproportion of questions that have at least one rele-vant answer in the top n of the results for n = 10and n = 150, giving us success@10 and suc-cess@150.
For the highest ranked relevant answerper question, we determine the reciprocal rank(RR).
If there is no relevant answer retrieved bythe system at n = 150, the RR is 0.
Over all ques-tions, we calculate the Mean RR (MRR@150).We also measure the performance of our systemfor document retrieval: the proportion of questionsfor which at least one of the answers in the top 10comes from the reference document (success@10for document retrieval) and the MRR@150 for thehighest position of the reference document3.4.3 Results and discussionTable 1: Baseline results for the why passage retrieval sys-tem for answer retrieval and document retrieval in terms ofsuccess@10, success@150 and MRR@150S@10 S@150 MRR@150Answer retrieval 43.0% 73.1% 0.260Document retrieval 61.8% 82.2% 0.365There are two possible directions for improvingour system: (1) by improving retrieval and (2) byimproving ranking.
Since success@150 is 73.1%,for 68 of the 93 questions in our set at least onerelevant answer is retrieved in the top 150.
For theother 25 questions, the reference answer was notincluded in the long list of 150 results.In the present paper we focus on improving an-swer ranking.
The results show that for 30.1% of2We don?t need to assess the tail since we are only in-terested in the highest-ranked relevant answer for calculatingMRR3Note that we consider as relevant all documents in whicha relevant answer is embedded.
So the relevant document withthe highest rank is either the reference document or the doc-ument in which the relevant answer with the highest rank isembedded.954the questions4, a relevant answer is retrieved butis not placed in the top 10 by the ranking algo-rithm.
For these 28 questions in our set, re-rankingmay be an option.
Since re-ranking will not im-prove the results for the questions for which thereis no relevant answer in the top-150, the maximumsuccess@10 that we can achieve by re-ranking is73.1% for answer paragraphs and 82.8% for docu-ments.5 Answer re-rankingBefore we can decide on our re-ranking approach,we take a closer look at the ranking method that isapplied in the baseline system.
The QAP algorithmincludes the following variables: (1) term overlapbetween query and passage, (2) passage length and(3) total corpus frequency for each term (Buttcheret al, 2004).
Let us consider three example ques-tions from our collection to see the strengths andweaknesses of these variables.1.
Why do people sneeze?2.
Why do women live longer than men on average?3.
Why are mountain tops cold?In (1), the corpus frequencies of the questionterms people and sneeze ensure that the relativelyunique term sneeze is weighted heavier for rankingthan the very common noun people.
This matchesthe goal of the query, which is finding an explana-tion for sneezing.
However, in (2), the frequencyvariables used by QAP do not reflect the impor-tance of the terms.
Thus, women, live, longer andaverage are considered to be of equal importance,while obviously the latter term is only peripheral tothe goal of the query.
This cannot be derived fromits corpus frequency, but may be inferred from itssyntactic function in the question: an adverbial onsentence level.
In (3), mountain and tops are in-terpreted as two distinct terms by the baseline sys-tem, whereas the interpretation of mountain topsas compound item is more appropriate.Examples 2 and 3 above show that a question-answer pair may contain more information thanis represented by the frequency variables imple-mented in the QAP algorithm.
Our aim is to findout which features from a question-answer pairconstitute the information that discloses a relationbetween the question and its answer.
Moreover, weaim at weighting these features in such a way thatwe can optimize ranking performance.473.1%?
43.0%5.1 Features for re-rankingAs explained above, baseline ranking is based onterm overlap.
The features that we propose forre-ranking are also based on term overlap, but in-stead of considering all question content words in-discriminately in one overlap function, we select asubset of question terms for each of the re-rankingfeatures.
By defining different subsets based onsyntactic functions and categories, we can investi-gate which syntactic features of the question, andwhich parts of the answer are most important forre-ranking.The following subsections list the syntactic fea-tures that we consider.
Each feature consists of twoitem sets: a set of question items and a set of an-swer items.
The value that is assigned to a featureis a function of the intersection between these twosets.
For a set of question items Q and a set ofanswer items A, the proportion P of their intersec-tion is:P =|Q ?
A|+ |A ?
Q||Q|+ |A|(1)Our approach to composing the set of features isdescribed in subsections 5.1.1 to 5.1.4 below.
Welabel the features using the letter f followed by anumber so that we can back-reference to them.5.1.1 The syntactic structure of the questionExample 2 in the previous section shows thatsome syntactic functions in the question may bemore important than other functions.
Since we donot know as yet which syntactic functions are themost important, we include both heads (f1) andmodifiers (f2) as item sets.
We also include thefour main syntactic constituents for why-questions:subject (f4), main verb (f6), nominal predicate (f8)and direct object (f10) to be matched against theanswer terms.
For these features, we add a vari-ant where as answer items only words/phrases withthe same syntactic function are included (f5, f7, f9,f11).Example 3 in the previous section exemplifiesthe potential relevance of noun phrases (f3).5.1.2 The semantic structure of the questionThe features f12 to f15 come from earlier dataanalyses that we performed.
We saw that oftenthere is a link between a specific part of the ques-tion and the title of the document in which the ref-erence answer is found.
For example, the answerto the question ?Why did B.B.
King name his gui-tar Lucille??
is in the Wikipedia article with the ti-955tle B.B.
King.
The answer document and the ques-tion apparently share the same topic (B.B.
King).In analogy to linguistically motivated approachesto factoid QA (Ferret et al, 2002) we introduce theterm question focus for this topic.The focus is often the syntactic subject of thequestion.
From our data, we found the follow-ing two exceptions to this general rule: (1) If thesubject is semantically poor, the question focus isthe (verbal or nominal) predicate: ?Why do peo-ple sneeze?
?, and (2) in case of etymology ques-tions (which cover about 10% of why-questions),the focus is the subject complement of the pas-sive sentence: ?Why are chicken wings calledBuffalo Wings??
In all other cases, the questionfocus is the grammatical subject: ?Why do catssleep so much?
?We include a feature (f13) for matching wordsfrom the question focus to words from the docu-ment title.
We also add a feature (f12) for the re-lation between all question words and words fromthe document title, and a feature (f14) for the rela-tion between question focus words and all answerwords.5.1.3 SynonymsFor each of the features f1 to f15, we add analternative feature (f16 to f30) covering the set ofall WordNet synonyms for all items in the origi-nal feature.
Note that the original words are nolonger included for these features; we only includethe terms from their synonym sets.
For synonyms,we apply a variant of equation 1 in which |Q ?
A|is interpreted as the number of question items thathave at least one synonym in the set of answeritems and |A ?
Q| as the number of answer itemsthat occur in at least one of the synonym sets of thequestion items.5.1.4 Cue wordsFinally, we add a closed set of cue words thatoften occur in answers to why-questions5 (f31).5.2 Extracting feature values from the dataFor the majority of features we need the syntacticstructure of the input question, and for some of thefeatures also of the answer.
We experimented withtwo different parsers for these tasks: a develop-5These cue words come from earlier work that we did onthe analysis of why-answers: because, since, therefore, why,in order to, reason, reasons, due to, cause, caused, causing,called, namedment version of the Pelican parser6 and the EP4IRdependency parser (Koster, 2003).Given a question-answer pair and the parse treesof both question and answer, we extract valuesfrom each parser?s output for all features in sec-tion 5.1 by means of a Perl script.Our script has access to the following externalcomponents: A stop word list (see section 4.1), afixed set of cue words, the CELEX Lemma lexi-con (Burnage et al, 1990), all WordNet synonymsets, and a list of pronouns and semantically poornouns7.Given one question-answer pair, the featureextraction script performs the following actions.Based on the question?s parse tree, it extracts thesubject, main verb, direct object (if present) andnominal predicate (if present) from the question.The script decides on question focus using therules suggested in section 5.1.2.
For the answer, itextracts the document title.
From the parse treescreated for the answer paragraph, it extracts allsubjects, all verbs, all direct objects, and all nomi-nal predicates.For each feature, the script composes the re-quired sets of question items and answer items.
Allitems are lowercased and punctuation is removed.In multi-word items, spaces are replaced by un-derscores before stop words are removed from thequestion and the answer.
Then the script calculatesthe proportion of the intersection of the two sets foreach feature following equation 18.Whether or not to lemmatize the items beforematching them is open to debate.
In the litera-ture, there is some discussion on the benefit oflemmatization for information extraction (Bilottiet al, 2004).
Lemmatization can be problematicin the case of proper names (which are not alwaysrecognizable by capitalization) and noun phrasesthat are fixed expressions such as sailors of old.Noun phrases are involved not only in the NP fea-ture (f3), but also in our features involving sub-ject, direct object, nominal predicate and questionfocus.
Therefore, we decided only to lemmatizeverbs (for features f6 and f7) in the current versionof our system.For each question-answer pair in our data set,we extract all feature values using our script.
We6The Pelican parser is a constituency parser that is cur-rently being developed at Nijmegen University.
See alsohttp://lands.let.ru.nl/projects/pelican/7These are the nouns humans and people8A multi-word term is counted as one item956use three different settings for feature extraction:(1) feature extraction from gold standard con-stituency parse trees of the questions in accordancewith the descriptive model of the Pelican parser9;(2) feature extraction from the constituency parsetrees of the questions generated by Pelican10; and(3) feature extraction from automatically gener-ated dependency parse trees from EP4IR.Our training and testing method using the ex-tracted feature values is explained in the next sec-tion.5.3 Re-ranking methodAs the starting point for re-ranking we run thebaseline system on the complete set of 93 ques-tions and retrieve 150 candidate answers per ques-tion, ranked by the QAP algorithm.
As describedin section 5.2, we use two different parsers.
Ofthese, Pelican has a more detailed descriptivemodel and gives better accuracy (see section 6.3 onparser evaluation) but EP4IR is at present more ro-bust for parsing long sentences and large amountsof text.
Therefore, we parse all answers (93 times150 paragraphs) with EP4IR only.
The questionsare parsed by both Pelican and EP4IR.As presented in section 5.1, we have 31 re-ranking features.
To these, we add the score thatwas assigned by QAP, which makes 32 featuresin total.
We aim to weight the feature values insuch a way that their contribution to the overallsystem performance is optimal.
We set each fea-ture weight as an integer between 0 and 10, whichmakes the number of possible weighting configu-rations 1132.
In order to choose the optimal con-figuration from this huge set of possible configura-tions, we use a genetic algorithm11 (Goldberg andHolland, 1988).
The variable that we optimize dur-ing training is MRR.
We tune the feature weightsover 100 generations of 1000 individuals.
For eval-uation, we apply cross valuation on five question9Pelican aims at producing all possible parse trees for agiven sentence.
A linguist can then decide on the correct parsetree given the context.
We created the gold standard for eachquestion by manually selecting the correct parse tree from theparse trees generated by the parser.10For this setting, we run the Pelican parser with the optionof only giving one parse (the most likely according to Pelican)per question.
As opposed to the gold standard setting, we donot perform manual selection of the correct parse.11We chose to work with a genetic algorithm because weare mainly interested in feature selection and ranking.
Weare currently experimenting with Support Vector Machines(SVM) to see whether the results obtained from using the ge-netic algorithm are good enough for reliable feature selection.folds: in five turns, we train the feature weights onfour of the five folds and evaluate them on the fifth.We use the feature values that come from thegold standard parse trees for training the featureweights, because the benefit of a syntactic itemtype can only be proved if the extraction of thatitem from the data is correct.
At the testing stage,we re-rank the 93 questions using all three fea-ture extraction settings: feature values extractedfrom gold standard parse trees, feature values ex-tracted with Pelican and feature values extractedwith EP4IR.
We again regard the distribution ofquestions over the five folds: we re-rank the ques-tions in fold five according to the weights found bytraining on folds one to four.5.4 Results from re-rankingTable 2 on the next page shows the results for thethree feature extraction settings.Using the Wilcoxon Signed-Rank Test we findthat all three re-ranking conditions give signifi-cantly better results than the baseline (Z = ?1.91,P = 0.0281 for paired reciprocal ranks).
The dif-ferences between the three re-ranking conditionsare, however, not significant12.5.5 Which features made the improvement?If we plot the weights that were chosen for the fea-tures in the five folds, we see that for some featuresvery different weights were chosen in the differentfolds.
Apparently, for these features, the weightvalues do not generalize over the five folds.
In or-der to only use reliable features, we only considerfeatures that get similar weights over all five folds:their weight values have a standard deviation < 2and an average weight > 0.
We find that of the32 features, 21 are reliable according to this def-inition.
Five of these features make a substantialcontribution to the re-ranking score (table 3).
Be-hind each feature is its reference number from sec-tion 5.1 and its average weight on a scale of 0 to10.Moreover, there are three other features that to alimited extent contribute to the overall score (table4).Thirteen other reliable features get a weight < 1.5assigned during training and thereby slightly con-tribute to the re-ranking score.12The slightly lower success and MRR scores for re-ranking with gold standard parse trees compared to Pelicanparse trees can be explained by the absence of the gold stan-dard for one question in our set.957Table 2: Re-ranking results for three different parser settings in terms of success@10, success@150 and MRR@150.Answer/paragraph retrieval Document retrievalVersion S@10 S@150 MRR S@10 S@150 MRRBaseline 43.0% 73.1% 0.260 61.8% 82.8% 0.365Re-ranking w/ gold standard parse trees 54.4% 73.1% 0.370 63.1% 82.8% 0.516Re-ranking w/ Pelican parse trees 54.8% 73.1% 0.380 64.5% 82.8% 0.518Re-ranking w/ EP4IR parse trees 53.8% 73.1% 0.349 63.4% 82.8% 0.493Table 3: Features that substantially contribute to the re-ranking score, with their average weightQuestion focus synonyms to doctitle (f28) 9.2Question verb synonyms to answer verbs (f22) 9Cue words (f31) 9QAP 8.8Question focus to doctitle (f13) 7.8Table 4: Features that to a limited extent contribute to there-ranking score, with their average weightQuestion subject to answer subjects (f5) 2.2Question nominal predicate synonyms (f23) 1.8Question object synonyms to answer objects (f26) 1.86 DiscussionOur re-ranking method scores significantly betterthan the baseline, with use of a small subset ofthe 32 features.
It reaches a success@10 scoreof 54.8% with an MRR@150 of 0.380 for answerretrieval.
This compares to the MRR of 0.328that Higashinaka and Isozaki found for why-QAand the MRR of 0.406 that Tiedemann reachesfor syntactically enhanced factoid-QA (see sec-tion 2), showing that our method performs reason-able well.
However, the MRR of 0.380 also showsthat a substantial part of the problem of why-QA isstill to be solved.6.1 Error analysisFor analysis of our results, we counted for howmany questions the ranking was improved, and forhow many the ranking deteriorated.
First of all,ranking remained equal for 35 questions (37.6%).25 of these are the questions for which no rele-vant answer was retrieved by the baseline systemat n = 150 (26.9% of questions).
For these ques-tions the ranking obviously remained equal (RR is0) after re-ranking.
For the other 10 questions forwhich ranking did not change, RR was 1 and re-mained 1.
Apparently, re-ranking does not affectexcellent rankings.For two third (69%) of the remaining questions,ranking improved and for one third (31%), it dete-riorated.
There are eleven questions for which thereference answer was ranked in the top 10 by thebaseline system but it drops out of the top 10 byre-ranking.
On the other hand, there are 22 ques-tions for which the reference answer enters the top10 by re-ranking the answers, leading to an overallimprovement in success@10.If we take a look at the eleven questions forwhich the reference answer drops out of the top10 by re-ranking, we see that these are all caseswhere there is no lexical overlap between the ques-tion focus and the document title.
The importanceof features 13 and 28 in the re-ranking weightsworks against the reference answer for these ques-tions.
Here are three examples (question focus asdetected by the feature extraction script is under-lined):1.
Why do neutral atoms have the same number of protonsas electrons?
(answer in ?Oxidation number?)2.
Why do flies walk on food?
(answer in ?Insect Habitat?)3.
Why is Wisconsin called the Badger State?
(answer in?Wisconsin?
)In example 1, the reference answer is outrankedby answer paragraphs from documents with one ofthe words neutral and atoms in its title.
In example2, there is actually a semantic relation between thequestion focus (flies) and the document title (in-sect); however, this relation is not synonymy buthyperonymy and therefore not included in our re-ranking features.
One could dispute the definitionof question focus for etymology questions (exam-ple 3), but there are simply more cases where thesubject complement of the question leads to doc-ument title than cases where its subject (such asWinsconsin) does.6.2 Feature selection analysisWe think that the outcome of the feature selection(section 5.5) is very interesting.
We are not sur-prised that the original score assigned by QAP isstill important in the re-ranking module: the fre-quency variables apparently do provide useful in-formation on the relevance of a candidate answer.We also see that the presence of cue words(f31) gives useful information in re-ranking an-958swer paragraphs.
In fact, incorporating the pres-ence of cue words is a first step towards recogniz-ing that a paragraph is potentially an answer to awhy-question.
We feel that identifying a paragraphas a potential answer is the most salient problemof why-QA, since answers cannot be recognizedby simple semantic-syntactic units such as namedentities as is the case for factoid QA.
The currentresults show that surface patterns (the literal pres-ence of items from a fixed set of cue words) are afirst step in the direction of answer selection.More interesting than the baseline score and cuewords are the high average weights assigned tothe features f13 and f28.
These two features referto the relation between question focus and docu-ment title.
As explained in section 5.1.2, we al-ready had the intuition that there is some relationbetween the question focus of a why-question andthe document title.
The high weights that are as-signed to the question focus features show that ourprocedure for extracting question focus is reliable.The importance of question focus for why-QA isespecially interesting because it is a question fea-ture that is specific to why-questions and does notsimilarly apply to factoids or other question types.Moreover, the link from the question focus to thedocument title shows that Wikipedia as an answersource can provide QA systems with more infor-mation than a collection of plain texts without doc-ument structure does.From the other features discussed in section 5.5,we learn that all four main question constituentscontribute to the re-ranking score, but that syn-onyms of the main verb make the highest contri-bution (f22).
Subject (f5), object (f26) and nomi-nal predicate (f23) make a lower contribution.
Wesuspect that this may be due to our decision to onlylemmatize verbs, and not nouns (see section 5.2).It could be that since lemmatization leads to morematches, a feature can make a higher contributionif its items are lemmatized.6.3 The quality of the syntactic descriptionsWe already concluded in the previous section thatour feature extraction module is very well capableof extracting the question focus, since f13 and f28get assigned high weights by training.
However,in the training stage, we used gold standard parsetrees.
In this section we evaluate the two automaticsyntactic parsers Pelican and EP4IR, in order tobe able to come up with fruitful suggestions forimproving our system in the future.As a measure for parser evaluation, we con-sider constituent extraction: how well do bothparsers perform in identifying and delimiting thefour main constituents from a why-question: sub-ject, main verb, direct object and nominal pred-icate?
As the gold standard for this experimentwe use manually verified constituents that wereextracted from the gold standard parse trees.
Weadapt our feature extraction script so that it printseach of the four constituents per question.
Then wecalculate the recall score for each parser for eachconstituent type.Recall is the number of correctly identified con-stituents of a specific type divided by the totalnumber of constituents of this type in the goldstan-dard parse tree.
This total number is not exactly 93for all constituent types: only 34 questions have adirect object in their main clause and 31 questionshave a nominal predicate.
The results of this exer-cise are in Table 5.Table 5: Recall for constituent extraction (in %)subjs verbs objs preds allPelican 79.6 94.6 64.7 71.0 82.1EP4IR 63.4 64.5 44.1 48.4 59.4We find that over all constituent types, Peli-can reaches significantly better recall scores thanEP4IR (Z = 5.57; P < 0.0001 using theWilcoxon Signed-Rank Test).Although Pelican gives much better results onconstituent extraction than EP4IR, the results onthe re-ranking task do not differ significantly.
Themost plausible explanation for this is that the highaccuracy of the Pelican parses is undone by thepoor syntactic analysis on the answer side, whichis in all settings performed by EP4IR.7 Future directionsIn section 4.3, we mentioned two directions for im-proving our pipeline system: improving retrievaland improving ranking.
Recently we have beenworking on optimizing the retrieval module of ourpipeline system by investigating the influence ofdifferent retrieval modules and passage segmenta-tion strategies on the retrieval performance.
Thiswork has resulted in a better passage retrieval mod-ule in terms of success@150.
Details on these ex-periments are in (Khalid and Verberne, 2008).Moreover, we have been collecting a larger datacollection in order to do make feature selection for959our re-ranking experiments more reliable and lessdepending on specific cases in our dataset.
Thiswork has resulted in a total set of 188 why-questionanswer pairs.
We are currently using this datacollection for further research into improving ourpipeline system.In the near future, we aim to investigate whattype of information is needed for further improv-ing our system for why-QA.
With the addition ofsyntactic information our system reaches an MRRscore of 0.380.
This compares to the MRR scoresreached by other syntactically enhanced QA sys-tems (see section 2).
However, an MRR of 0.380also shows that a substantial part of the problemof why-QA is still to be solved.
We are currentlyinvestigating what type information is needed forfurther system improvement.Finally, we also plan experiments with a numberof dependency parsers to be used instead of EP4IRfor the syntactic analysis of the answer para-graphs.
Current experiments with Charniak (Char-niak, 2000) show better constituent extraction thanwith EP4IR.
It is still to be seen whether this alsoinfluences the overall performance of our system.8 ConclusionWe added a re-ranking step to an existing para-graph retrieval method for why-QA.
For re-ranking, we took the score assigned to a questionanswer pair by the ranking algorithm QAP in thebaseline system, and weighted it with a number ofsyntactic features.
We experimented with 31 fea-tures and trained the feature weights on a set of 93why-questions with 150 answers provided by thebaseline system for each question.
Feature valuesfor training the weights for the 31 features wereextracted from gold standard parse trees for eachquestion answer pair.We evaluated the feature weights on automat-ically parsed questions and answers, in five folds.We found a significant improvement over the base-line for both success@10 and MRR@150.
Themost important features were the baseline score,the presence of cue words, the question?s mainverb, and the relation between question focus anddocument title.We think that, although syntactic informationgives a significant improvement over baseline pas-sage ranking, more improvement is still to begained from other types of information.
Investi-gating the type of information needed is part of ourfuture directions.ReferencesBilotti, M.W., B. Katz, and J. Lin.
2004.
What worksbetter for question answering: Stemming or mor-phological query expansion.
Proc.
IR4QA at SIGIR2004.Burnage, G., R.H. Baayen, R. Piepenbrock, and H. vanRijn.
1990.
CELEX: A Guide for Users.Buttcher, S., C.L.A.
Clarke, and G.V.
Cormack.
2004.Domain-Specific Synonym Expansion and Valida-tion for Biomedical Information Retrieval.Buttcher, S. 2007.
The Wumpus Search Engine.http://www.wumpus-search.org/.Charniak, E. 2000.
A maximum-entropy-inspiredparser.
ACM International Conference ProceedingSeries, 4:132?139.Denoyer, L. and P. Gallinari.
2006.
The WikipediaXML corpus.
ACM SIGIR Forum, 40(1):64?69.Ferret, O., B. Grau, M. Hurault-Plantet, G. Illouz,L.
Monceaux, I. Robba, and A. Vilnat.
2002.
Find-ing an answer based on the recognition of the ques-tion focus.
Proc.
of TREC 2001, pages 500?250.Goldberg, D.E.
and J.H.
Holland.
1988.
Genetic Algo-rithms and Machine Learning.
Machine Learning,3(2):95?99.Higashinaka, R. and H. Isozaki.
2008.
Corpus-basedQuestion Answering for why-Questions.
In Proc.
ofIJCNLP, vol.1, pages 418?425.Hovy, E.H., U. Hermjakob, and D. Ravichandran.2002.
A Question/Answer Typology with SurfaceText Patterns.
In Proc.
of HLT 2002.Khalid, M. and S. Verberne.
2008.
Passage Retrievalfor Question Answering using Sliding Windows.
InProc.
of IR4QA at COLING 2008.Koster, CHA.
2003.
Head-modifier frames for every-one.
Proc.
of SIGIR 2003, page 466.Quarteroni, S., A. Moschitti, S. Manandhar, andR.
Basili.
2007.
Advanced Structural Represen-tations for Question Classification and Answer Re-ranking.
In Proc.
of ECIR 2007, volume 4425, pages234?245.Tiedemann, J.
2005.
Improving passage retrieval inquestion answering using NLP.
In Proc.
of EPIA2005.Verberne, S., L. Boves, N. Oostdijk, and P.A.
Coppen.2008.
Evaluating paragraph retrieval for why-QA.In Proc.
of ECIR 2008.960
