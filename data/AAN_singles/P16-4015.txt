Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics?System Demonstrations, pages 85?90,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational Linguisticsccg2lambda: A Compositional Semantics SystemPascual Mart?
?nez-G?omez4pascual.mg@aist.go.jpKoji Mineshima1mineshima.koji@ocha.ac.jpYusuke Miyao2yusuke@nii.ac.jpDaisuke Bekki1,2bekki@is.ocha.ac.jp1Ochanomizu UniversityTokyo, Japan2NIITokyo, Japan4AIRC, AISTTokyo, JapanAbstractWe demonstrate a simple and easy-to-usesystem to produce logical semantic rep-resentations of sentences.
Our softwareoperates by composing semantic formu-las bottom-up given a CCG parse tree.
Ituses flexible semantic templates to spec-ify semantic patterns.
Templates for En-glish and Japanese accompany our soft-ware, and they are easy to understand, useand extend to cover other linguistic phe-nomena or languages.
We also providescripts to use our semantic representationsin a textual entailment task, and a visu-alization tool to display semantically aug-mented CCG trees in HTML.1 IntroductionWe are motivated by NLP problems that bene-fit from any degree of computer language un-derstanding or semantic parsing.
Two prominentexamples are Textual Entailment and Question-Answering, where the most successful ap-proaches (Abzianidze, 2015; Berant et al, 2013)require symbolic representations of the semanticsof sentences.
We are inspired by the theoreti-cal developments in the formal semantics litera-ture, where higher-order logical (HOL) formulasare used to derive meaning representations (MR);despite what is typically believed in the NLP com-munity, Mineshima et al (2015) demonstrated thatHOL can be used effectively at a reasonable speed.In this paper, we describe ccg2lambda, oursystem to obtain MRs given derivations (trees) of aCombinatory Categorial Grammar (CCG) (Steed-man, 2000).
In order to obtain the MRs, our sys-tem is guided by the combinatory characteristicsof CCG derivations and a list of manually de-signed semantic templates.
The linguistic intu-itions behind the design of those semantic tem-plates and the evaluation of the MRs that they pro-duce is detailed in Mineshima et al (2015), andis not repeated here.
In that paper, we tackled aTextual Entailment task, where the meanings ofpremises and conclusions were represented sym-bolically, and their entailment relation was judgedwith a theorem prover of higher-order logics.
Withthis system, we obtained a state-of-the-art perfor-mance on the FraCaS dataset (Cooper et al, 1994).ccg2lambda and the accompanying semantictemplates are open source1.
Semantic templatesare already available for English and Japanese,and they are easily extensible to other linguisticphenomena and other languages for which CCGparsers are available.
Here we describe how to useccg2lambda and how to specify semantic tem-plates for other researchers to extend our work.2 Related WorkThe most similar system to ours is Boxer (Bos etal., 2004), which outputs first order formulas givenCCG trees.
Our system can additionally producehigher-order formulas, which are more expressiveand potentially accurate (Mineshima et al, 2015).There are three prominent textbook systemsfor computational semantics, that of Bird et al(2009), Blackburn and Bos (2005) and van Ei-jck and Unger (2010).
These three systems, to-gether with the Lambda Calculator2(Champollionet al, 2007) are excellent educational resourcesthat are very accessible to beginner linguists ingeneral, and semanticists in particular.
The devel-opment of ccg2lambda is inspired by these sys-tems, in that we aimed to produce a software that iseasy to understand, use and extend with only basicknowledge of formal semantics and lambda calcu-1https://github.com/mynlp/ccg2lambda2http://lambdacalculator.com/85lus.
However, these systems are mainly developedfor educational purposes and are not connected tofully fledged parsers, hence not immediately us-able as a component of larger NLP systems.We have developed ccg2lambda to processtrees that are produced by wide-coverage CCGparsers (e.g.
C&C and Jigg3).
Other seman-tic parsers such as those developed by Bos et al(2004), Abzianidze (2015) and Lewis and Steed-man (2013) also connect to wide-coverage CCGparsers, but they do not emphasize easy accessi-bility or extensibility.
NL2KR (Vo et al, 2015) isan interactive system with powerful generalizationcapabilities, but it does not allow fine-grained lexi-con specifications (only CCG categories) and doesnot output machine readable semantics.
Instead,ccg2lambda produces XML machine-readableMRs, which make our system easy to integrate inlarger logic or statistical NLP systems.3 System OverviewAlthough our main system contribution is a se-mantic parser, we use the problem of textual en-tailment as an end-to-end task.
Figure 1 schemati-cally shows the several components of our system.The first stage is to parse sentences into CCGtrees (see Figure 2 for an example).
Our systemcurrently supports the C&C parser (Clark and Cur-ran, 2004) for English, and Jigg (Noji and Miyao,2016) for Japanese.The second stage is the semantic composition,where MRs are constructed compositionally overCCG trees using lambda calculus, thus allowinghigher-order logics if necessary.
To this end, oursystem is guided by the compositional rules of theCCG tree and the semantic templates provided bythe user.
In Section 4 we describe in detail howthese semantic templates are specified and howthey control the semantic outputs.
The outputof this stage is a Stanford CoreNLP-style XMLfile (Manning et al, 2014) where each sentencehas three XML nodes: <tokens>, <ccg> and<semantics>.
Thus, sentence semantics cansimply be read off the root node of the CCG tree.In the case of recognizing textual entailment,the third stage is the theorem construction, defini-tion of predicate types, and execution with a logicprover.
This stage is not essential to our system,but it is added to this paper to show the usefulnessof our semantic representations in an NLP task.3https://github.com/mynlp/jigg4 Semantic Compositionccg2lambda receives CCG trees and outputs(possibly higher-order) logic formulas.
To thatend, we use i) the combinatory characteristics ofCCG trees to guide the semantic compositions,and ii) a list of semantic templates to assign a pre-cise meaning to CCG constituents.See Figure 2 for an example of CCG deriva-tion for the sentence ?Some woman ordered tea?,augmented with its semantics.
Nodes have CCGsyntactic categories (e.g.
N or S\NP ), which iswhat our system receives as input.
On the samefigure, we have added the logical semantic repre-sentations (e.g.
?x.woman(x)) below the syntac-tic categories.
Our system outputs these logicalformulas.
For clarity, leaves also display the to-ken base forms.
The symbols <,> and lex standfor left and right function application rules, andthe type-shift rule in C&C, respectively.
Theserules and the syntactic categories guide the seman-tic composition, provided with semantic templatesthat describe the specific semantics.4.1 Semantic templatesSemantic templates are defined declaratively ina YAML4file, typically by a formal semanticistafter an appropriate linguistic analysis.
A tem-plate applies to a node of the CCG derivationtree if certain conditions are met.
Each templatehas two required attributes: semantics and (syn-tactic) category.
The attribute semantics is alambda term in NLTK semantics format (Garretteand Klein, 2009).
In case a template applies on aCCG leaf (that is, a word), the lambda term in thetemplate is applied on the base form of the word,and ?-reduction is performed.
For example, thesemantic template?
semantics : \E.\x.E(x)category : Napplying on a leaf whose base word is ?woman?and its syntactic category is N , would producethe expression (?E.
?x.E(x))(woman) which is?-reduced to ?x.woman(x).
Here, the base form?woman?
substitutes all occurrences of the vari-able E in the semantics expression.In case a template applies on a CCG inner node(a node with children), the lambda abstraction isapplied on the semantics of the children, in order.4http://www.yaml.org/spec/86Figure 1: System pipeline for recognizing textual entailment.
Syntactic structures of sentences are ob-tained with a CCG parser.
Then, we perform the semantic composition using semantic templates.
Theresulting meaning representations are used to perform various logical inferences with a theorem prover.SomeNP/N?F?G.?x.
(Fx ?Gx)womanN?x.woman(x)NP?G.
?x(woman(x) ?G(x))>ordered(S\NP )/NP?Q1?Q2.Q2(?x.Q1(?y.order(x, y)))teaN?y.tea(y)NP?F.?y.
(tea(y) ?
F (y))lexS\NP?Q2.Q2(?x.?y.
(tea(y) ?
order(x, y)))>S?x.
(woman(x) ?
?y.
(tea(y) ?
order(x, y)))<Figure 2: CCG derivation tree of the sentence ?Some woman ordered tea?, with its semantics (simplifiedfor illustrative purposes).
The actual output of ccg2lambda with our provided templates is in Figure 6.For example, in Figure 2, the template?
semantics : \E.\F.?y.
(E(y) ?
F (y))category : NPrule : lexproduces a type-raise from N to NP , and whenapplied to the CCG node whose child?s semanticsare ?y.tea(y), it will produce, after ?-reduction,the formula ?F.?y.
(tea(y) ?
F (y)).
Here, thechild?s semantics ?y.tea(y) substitute all occur-rences of the variable E. The newly composed se-mantic representation ?F.?y.
(tea(y)?F (y)) nowexpects another predicate (a verb) as an argumentF (i.e.
?order?
), which will be filled in the nextstep of the composition.The category attribute of a semantic templatemay also specify conditions on the feature struc-tures of CCG nodes (which are provided by theCCG parser), in which case templates apply if thesyntactic category matches and the feature struc-ture subsumes that of the CCG node.
For example,if the semantic template specifies a syntactic cat-egory NP[dcl = true], it matches a CCG nodewith a category NP[dcl = true] or a categoryNP[dcl = true, adj = true].Other conditions for matching templates toCCG nodes can be specified by adding more at-tributes to the semantic template.
In the exam-ple above, the attribute rule : lex is used tospecify the combination rule of that inner CCGnode.
In practice, any XML attribute of a CCGnode can be used to specify matching conditions,which means that users of ccg2lambda can en-rich CCG trees with arbitrary annotations such asNamed Entities or Events and use them as match-ing conditions when defining semantic templateswithout modifying the software.
It is also possibleto specify attributes of the children of the targetCCG node.
These conditions are always prefixedby the string child, followed by the branch in-dex 0 or 1.
For example, a semantic template withthe attribute child1 child0 pos : NN matchesa node whose right child?s (child1) left child?s(child0) POS tag is an NN.
Moreover, paths tochild nodes can be left unspecified, by using thekeyword child any X : Y; in this case, any childwhose attribute X has value Y will be matched bythe template.
If more than one template matches aCCG node, the first appearing template is selected.4.2 System Usage and OutputThe command for the semantic composition is:# python semparse.py ccgtrees.xmltemplates.yaml semantics.xmlwhere ccgtrees.xml is a Jigg?s XML styleCCG tree, templates.yaml contains thesemantic templates, and semantics.xml is theXML output of the system.
We also provide ascript to convert C&C XML trees into Jigg?s XMLstyle.
The output of semparse.py follows theconventions of Stanford coreNLP (see Figure 3).However, we follow Jigg?s style to represent871 <r o o t>2 <s e n t e n c e s>3 <s e n t e n c e>4 <t o k e n s>5 <t o k e n base=?
t e a ?
s u r f =?
t e a ?
pos=?NN?
/>6 <t o k e n .
.
.
/>7 </ t o k e n s>8 <ccg>9 <span id =?
s1 ?
c h i l d =?
s2 ?
c a t e g o r y =?N?r u l e =?
l e x ?
/>10 <span .
.
.
/>11 </ ccg>12 <s e m a n t i c s>13 <span id =?
s1 ?
c h i l d =?
s2 ?
sem=?\y .
t e a ( y ) ?t y p e =?
t e a : E n t i t y ?> Prop ?
/>14 <span .
.
.
/>15 </ s e m a n t i c s>16 </ s e n t e n c e>17 </ s e n t e n c e s>18 </ r o o t>Figure 3: XML output of the semantic composi-tion.
Span nodes of the semantics tag contain log-ical semantic representations of that constituent.element characteristics as XML node attributes.For example, the base and surface forms, and thePOS tag of a token are all represented as XMLattributes in a <token> tag.Our semantic composition produces the<semantics> tag, which has as many childrennodes (<span>) as the CCG tree, the same spanidentifiers and structure.
However, semanticspans also have a ?sem?
attribute encoding thesemantics (using NLTK?s semantics format) thathave been composed for that constituent.
Anexample of a resulting semantic logic formula inNLTK semantics format is:\F.exists y.
( tea(y) & F (y))Note that predicates are prefixed with an under-score to avoid collisions with reserved predicatesin NLTK semantics format or in a potential prover.Semantic spans also provide the type of singlepredicates (attribute ?type?).
For instance, the typeof the predicate tea is a function that receives anentity as an argument, and produces a proposition:tea : Entity?
PropTypes are automatically inferred using NLTK se-mantics functionality.
However, it is possible toforce arbitrary types in a semantic template byadding the attribute ?coq type?.
For example, wecan specify the type for a transitive verb as:?
semantics : .
.
.category : (S\NP)/NPcoq type : Entity?
Entity?
PropWe can activate these types with the flag--arbi-types in the call to semparse.py.5 Textual EntailmentThe logical formulas that ccg2lambda outputscan be used in a variety of applications.
In thisdemonstration, we use them to recognize textualentailment, an NLP problem that often requiresprecise language understanding.
We assume thatthe user inputs a file with one sentence per line.All sentences are assumed to be premises, exceptthe last sentence, which is assumed to be the con-clusion.
An entailment problem example is:premise1: All women ordered coffee or tea.premise2: Some woman did not order coffee.conclusion: Some woman ordered tea.Contrarily to other textual entailment systemsbased on logics (Angeli and Manning, 2014; Mac-Cartney and Manning, 2007), we do not assumesingle-premise problems, which makes our systemmore general.
The MRs of the problem above are:p1: ?x.(woman(x)?
?y.
((tea(y) ?
coffee(y)) ?
order(x, y)))p2: ?x.
(woman(x) ?
??y.
(coffee(y) ?
order(x, y)))c : ?x.
(woman(x) ?
?y.
(tea(y) ?
order(x, y)))We build a theorem by concatenating mean-ing representations of the premises {p1, .
.
.
, pn}and the conclusion c with the implication opera-tor, which is a convenience in theorem proving:Theorem : p1?
.
.
.?
pn?
c. (1)And then, we define predicate types as:Parameter tea : Entity ?
Prop.Parameter order : Entity ?
Entity ?
Prop.Finally, we pipe the theorem and type definitionsto Coq (Cast?eran and Bertot, 2004), an interactivehigher-order prover that we run fully automatedwith the use of some tactics (including arithmeticsand equational reasoning), as described in Mi-neshima et al (2015).
We return the label yes (en-tailment) if the conclusion can be logically provedfrom the premises, no if the negated conclusioncan be proved, and unknown otherwise.The recognition of textual entailment can beperformed with the following command:# python prove.py semantics.xmlwhere the entailment judgment (yes, no, unknown)is printed to standard output.
Moreover, the flag--graph out allows to specify an HTML fileto print a graphical visualization of the CCG treestructure of sentences, their semantic composition(every constituent annotated with a component ofthe formula), and the prover script.
An excerpt ofthe visualization is shown in Section 6.88Figure 4: Visualization of the semantic output of ccg2lambda for the sentence ?All women orderedcoffee or tea.?
where logical semantic representations appear below their respective CCG nodes.Figure 5: Visualization of the semantic output of ccg2lambda for the sentence ?Some woman did notorder coffee.?
where logical semantic representations appear below their respective CCG nodes.Figure 6: Visualization of the semantic output of ccg2lambda for the sentence ?Some woman orderedtea.?
where logical semantic representations appear below their respective CCG nodes.6 VisualizationFor visualization purposes, we provide a separatescript that can be called as:# python visualize.py semantics.xml> semantics.htmlwhich produces a file semantics.html withan HTML graphical representation of the CCGtree, augmented at every node with the seman-tics composed up to that node (see Figures 4, 5and 6 for an excerpt).
These semantic representa-tions are obtained with the semantic templates thataccompany our software and that were developedand evaluated in Mineshima et al (2015).
Thetrivial propositions ?TrueP?
have no effect and ap-pear in the formulas in place of potential modifiers(such as adjectives or adverbs) of more complexsentences.
The visualization can be configured todisplay the root on top, change colors and sizes ofthe syntactic categories, feature structures, logicalformulas and base forms at the leaves.7 Future Work and ConclusionAs an extension to ccg2lambda, it would bevaluable to produce (possibly scored) N-best listsof logical formulas, instead of the current single1-best.
Moreover, our current semantic templatesdo not cover all syntactic categories that C&C orJigg produce, and we need a good default combi-nation mechanism.
Other minor enhancements areto produce logical formulas for each CCG deriva-tion in an N-best list, and to allow features otherthan the base form to become predicates.In this paper we have demonstrated our sys-tem to convert CCG trees to logic MRs.
It oper-ates by composing semantics bottom-up, guidedby the combinatory characteristics of the CCGderivation and semantic templates provided by theuser.
In this release, semantic templates for En-glish and Japanese are also included.
As Mi-neshima et al (2015) has shown, the MRs obtainedby ccg2lambda are useful to recognize textual89entailment.
We believe that these easy-to-produceMRs can be useful to NLP tasks that require pre-cise language understanding or that benefit fromusing MRs as features in their statistical systems.AcknowledgmentsThis work was supported by CREST, JST.ReferencesLasha Abzianidze.
2015.
A tableau prover for natu-ral logic and language.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 2492?2502, Lisbon, Portu-gal, September.
Association for Computational Lin-guistics.Gabor Angeli and Christopher D. Manning.
2014.NaturalLI: Natural logic inference for commonsense reasoning.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 534?545, Doha, Qatar,October.
Association for Computational Linguistics.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 1533?1544, Seattle, Wash-ington, USA, October.
Association for Computa-tional Linguistics.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media, Inc.Patrick Blackburn and Johan Bos.
2005.
Represen-tation and Inference for Natural Language: A FirstCourse in Computational Semantics.
CSLI.Johan Bos, Stephen Clark, Mark Steedman, James RCurran, and Julia Hockenmaier.
2004.
Wide-coverage semantic representations from a CCGparser.
In Proceedings of the 20th international con-ference on Computational Linguistics, pages 1240?1246.
Association for Computational Linguistics.Pierre Cast?eran and Yves Bertot.
2004.
Interac-tive Theorem Proving and Program Development.Coq?Art: The Calculus of Inductive Constructions.Springer Verlag.Lucas Champollion, Joshua Tauberer, and MaribelRomero.
2007.
The Penn Lambda Calculator: Ped-agogical software for natural language semantics.In Tracy Holloway King, editor, Proceedings of theGrammar Engineering Across Frameworks (GEAF07) Workshop, pages 106?127, Stanford.
CSLI Publi-cations.Stephen Clark and James R Curran.
2004.
Parsing theWSJ using CCG and log-linear models.
In Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, pages 104?111.
Associ-ation for Computational Linguistics.Robin Cooper, Richard Crouch, Jan van Eijck, ChrisFox, Josef van Genabith, Jan Jaspers, Hans Kamp,Manfred Pinkal, Massimo Poesio, Stephen Pulman,et al 1994.
FraCaS?a framework for computationalsemantics.
deliverable, D6.Dan Garrette and Ewan Klein.
2009.
An extensibletoolkit for computational semantics.
In Proceed-ings of the Eighth International Conference on Com-putational Semantics, IWCS-8 ?09, pages 116?127,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Mike Lewis and Mark Steedman.
2013.
Combin-ing distributional and logical semantics.
Transac-tions of the Association for Computational Linguis-tics, 1:179?192.Bill MacCartney and Christopher D Manning.
2007.Natural logic for textual inference.
In Proceedingsof the ACL-PASCAL Workshop on Textual Entail-ment and Paraphrasing, pages 193?200.
Associa-tion for Computational Linguistics.Christopher Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proceedings of 52nd AnnualMeeting of the Association for Computational Lin-guistics: System Demonstrations, pages 55?60, Bal-timore, Maryland, June.
Association for Computa-tional Linguistics.Koji Mineshima, Pascual Mart?
?nez-G?omez, YusukeMiyao, and Daisuke Bekki.
2015.
Higher-order log-ical inference with compositional semantics.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 2055?2061, Lisbon, Portugal, September.
Association forComputational Linguistics.Hiroshi Noji and Yusuke Miyao.
2016.
Jigg: Aframework for an easy natural language process-ing pipeline.
In Proceedings of ACL 2016 SystemDemonstrations, Berlin, Germany, August.
Associa-tion for Computational Linguistics.Mark Steedman.
2000.
The Syntactic Process.
MITPress.Jan van Eijck and Christina Unger.
2010.
Compu-tational Semantics with Functional Programming.Cambridge University Press.Nguyen Vo, Arindam Mitra, and Chitta Baral.
2015.The NL2KR platform for building natural languagetranslation systems.
In Proceedings of the 53rd An-nual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 899?908, Beijing, China, July.Association for Computational Linguistics.90
