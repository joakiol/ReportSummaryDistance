Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 812?817,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsThe Impact of Listener Gaze on Predicting Reference ResolutionNikolina Koleva1Mart?
?n Villalba2Maria Staudte11Embodied Spoken Interaction Group, Saarland University, Saarbr?ucken, Germany2Department of Linguistics, University of Potsdam, Potsdam, Germany{nikkol | masta}@coli.uni-saarland.de{martin.villalba | alexander.koller}@uni-potsdam.deAlexander Koller2AbstractWe investigate the impact of listener?sgaze on predicting reference resolution insituated interactions.
We extend an ex-isting model that predicts to which entityin the environment listeners will resolvea referring expression (RE).
Our modelmakes use of features that capture whichobjects were looked at and for how long,reflecting listeners?
visual behavior.
Weimprove a probabilistic model that consid-ers a basic set of features for monitoringlisteners?
movements in a virtual environ-ment.
Particularly, in complex referentialscenes, where more objects next to the tar-get are possible referents, gaze turns out tobe beneficial and helps deciphering listen-ers?
intention.
We evaluate performance atseveral prediction times before the listenerperforms an action, obtaining a highly sig-nificant accuracy gain.1 IntroductionSpeakers tend to follow the listener?s behavior inorder to determine whether their communicatedmessage was received and understood.
This phe-nomenon is known as grounding, it is well estab-lished in the dialogue literature (Clark, 1996), andit plays an important role in collaborative tasksand goal?oriented conversations.
Solving a col-laborative task in a shared environment is an ef-fective way of studying the alignment of commu-nication channels (Clark and Krych, 2004; Hannaand Brennan, 2007).In situated spoken conversations ambiguous lin-guistic expressions are common, where additionalmodalities are available.
While Gargett et al.
(2010) studied instruction giving and following invirtual environments, Brennan et al.
(2013) ex-amined pedestrian guidance in outdoor real envi-ronments.
Both studies investigate the interactionof human interlocutors but neither study exploitslisteners?
eye movements.
In contrast, Koller etal.
(2012) designed a task in which a natural lan-guage generation (NLG) system gives instructionsto a human player in virtual environment whoseeye movements were tracked.
They outperformedsimilar systems in both successful reference res-olution and listener confusion.
Engonopoulos etal.
(2013) attempted to predict the resolution ofan RE, achieving good performance by combiningtwo probabilistic log?linear models: a semanticmodel Psemthat analyzes the semantics of a giveninstruction, and an observational model Pobsthatinspects the player?s behavior.
However, they didnot include listener?s gaze.
They observed that theaccuracy for Pobsreaches its highest point at a rel-atively late stage in an interaction.
Similar obser-vations are reported by Kennington and Schlangen(2014): they compare listener gaze and an incre-mental update model (IUM) as predictors for theresolution of an RE, noting that gaze is more ac-curate before the onset of an utterance, whereasthe model itself is more accurate afterwards.In this paper we report on the extension of thePobsmodel to also consider listener?s visual be-haviour.
More precisely we implement featuresthat encode listener?s eye movement patterns andevaluate their performance on a multi?modal datacollection.
We show that such a model as ittakes an additional communication channel pro-vides more accurate predictions especially whendealing with complex scenes.
We also expand onconcepts from the IUM, by applying the conclu-sions drawn from its behaviour to a dynamic taskwith a naturalistic interactive scenario.2 Problem definitionWe address the research question of how to auto-matically predict an RE resolution, i.e., answer-ing the question of which entity in a virtual en-vironment has been understood by the listener af-812ter receiving an instruction.
While the linguisticmaterial in instructions carries a lot of informa-tion, even completely unambiguous descriptionsmay be misunderstood.
A robust NLG systemshould be capable of detecting misunderstandingsand preventing its users from making mistakes.Language comprehension is mirrored by inter-locutors?
non verbal behavior, and this can helpwhen decoding the listener?s interpretation.
Pre-cise automatic estimates may be crucial when de-veloping a real?time NLG system, as such a mech-anism would be more robust and capable at avoid-ing misunderstandings.
As mentioned in section 1,Engonopoulos et al.
(2013) propose two statisticalmodels to solve that problem: a semantic modelPsembased on the linguistic content, and an ob-servation model Pobsbased on listener behaviorfeatures.More formally, let?s assume a system generatesan expression r that aims to identify a target ob-ject otamong a setO of possible objects, i.e.
thoseavailable in the scene view.
Given the state of theworld s at time point t, and the observed listener?sbehavior ?
(t) of the user at time t ?
tb(wheretbdenotes the end of an interaction), we estimatedthe conditional probability p(op|r, s, ?
(t)) that in-dicates how probable it is that the listener resolvedr to op.
This probability can be also expressed asfollows:P (op|r, s, ?
(t)) ?Psem(op|r, s)Pobs(op|?
(t))P (op)Following Engonopoulos et al.
(2013) we makethe simplifying assumption that the distribution ofthe probability among the possible targets is uni-form and obtain:P (op|r, s, ?
(t)) ?
Psem(op|r, s)Pobs(op|?
(t))We expect an NLG system to compute and out-put an expression that maximizes the probabilityof op.
Due to the dynamic nature of our scenar-ios, we also require the probability value to be up-dated at certain time intervals throughout an in-teraction.
Tracking the probability changes overtime, an NLG system could proactively react tochanges in its environment.
Henderson and Smith(2007) show that accounting for both fixation lo-cation and duration are key to identify a player?sfocus of attention.The technical contribution of this paper is to ex-tend the Pobsmodel of Engonopoulos et al.
(2013)with gaze features to account for these variables.3 Episodes and feature functionsThe data for our experiment was obtained fromthe GIVE Challenge (Koller et al., 2010), an inter-active task in a 3D virtual environment in whicha human player (instruction follower, IF) is navi-gated through a maze, locating and pressing but-tons in a predefined order aiming to unlock a safe.While pressing the wrong button in the sequencesdoesn?t always have negative effects, it can alsolead to restarting or losing the game.
The IF re-ceives instructions from either another player oran automated system (instruction giver, IG).
TheIF?s behavior was recorded every 200ms, alongwith the IG?s instructions and the state of thevirtual world.
The result is an interaction cor-pus comprising over 2500 games and spanningover 340 hours of interactions.
These interactionswere mainly collected during the GIVE-2 and theGIVE-2.5 challenges.
A laboratory study con-ducted by Staudte et al.
(2012) comprises a datacollection that contains eye-tracking records forthe IF.
Although the corpus contains both success-ful and unsuccessful games, we have decided toconsider only the successful ones.We define an episode over this corpus as a typ-ically short sequence of recorded behavior states,beginning with a manipulation instruction gener-ated by the IG and ending with a button press bythe IF (at time point tb).
In order to make surethat the recorded button press is a direct responseto the IG?s instruction, an episode is defined suchthat it doesn?t contain further utterances after thefirst one.
Both the target intended by the IG (ot)and the one selected by the IF (op) were recorded.Figure 1: The structure of the interactions.Figure 1 depicts the structure of an episodewhen eye-tracking data is available.
Each episode813can be seen as a sequence of interaction states(s1, .
.
.
, sn), and each state has a set of visibleobjects ({o1, o2, o3, o10, o12}).
We then computethe subset of fixated objects ({o2, o3, o12}).
Weupdate both sets of visible and fixated objects dy-namically in each interaction state with respect tothe change in visual scene and the correspondingrecord of the listener?s eye movements.We developed feature functions over theseepisodes.
Along with the episode?s data, eachfunction takes two parameters: an object opforwhich the function is evaluated, and a parameterd seconds that defines how much of the episode?sdata is the feature allowed to analyze.
Each featurelooks only at the behavior that happens in the timeinterval ?d to 0.
Henceforth we refer to the valueof a feature function over this interval as its valueat time ?d.
The value of a feature function evalu-ated on episodes with length less than d seconds isundefined.4 Prediction modelsGiven an RE uttered by an IG, the semantic modelPsemestimates the probability for each possibleobject in the environment to have been understoodas the referent, ranks all candidates and selects themost probable one in a current scene.
This prob-ability represents the semantics of the utterance,and is evaluated at a single time point immediatelyafter the instruction (e.g.
?press the blue button?
)has been uttered.
The model takes into accountfeatures that encode the presence or absence of ad-jectives carrying information about the spatial orcolor properties (like the adjective ?blue?
), alongwith landmarks appearing as post modifiers of thetarget noun.In contrast to the semantic model, the observa-tional model Pobsevaluates the changes in the vi-sual context and player?s behavior after an instruc-tion has been received.
The estimated probabil-ity is updated constantly before an action, as thelistener in our task?oriented interactions is con-stantly in motion, altering the visual context.
Themodel evaluates the distance of the listener posi-tion to a potential target, whether it is visible ornot, and also how salient an object is in that par-ticular time window.As we have seen above, eye movements pro-vide useful information indicating language com-prehension, and also how to map a semantic repre-sentation to an entity in a shared environment.
In-terlocutors constantly interact with their surround-ing and point to specific entities with gestures.Gaze behaviour is also driven by the current stateof an interaction.
Thus, we extend the basic setof Pobsfeatures and implement eye?tracking fea-tures that capture gaze information.
We call thisthe extended observational model PEobsand con-sider the following additional features:1.
Looked at: feature counts the number ofinteraction states in which an object hasbeen fixated at least once during the currentepisode.2.
Longest Sequence: detects the longest con-tinuous sequence of interaction states inwhich a particular object has been fixated.3.
Linear Distance: returns the euclidean dis-tance dist on screen between the gaze cursorand the center of an object.4.
Inv-Squared Distance: returns11+dist2.5.
Update Fixated Objects: expands the list offixated objects in order to consider the IF?sfocus of attention.
It successively searches in10 pixel steps and stops as soon as an objectis found (the threshold is 100 pixels).
Thisfeature evaluates to 1 if the list of fixated ob-jects is been expanded and 0 otherwise.When training our model at time ?dtrain, wegenerate a feature matrix.
Given a trainingepisode, each possible (located in the same room)object opis added as a new row, where each col-umn contains the value of a different feature func-tion for opover this episode at time ?dtrain.
Fi-nally, the row based on the target selected by theIF is marked as a positive example.
We then traina log-linear model, where the weights assignedto each feature function are learned via optimiza-tion with the L-BFGS algorithm.
By training ourmodel to correctly predict a target button basedonly on data observed up until ?dtrainsecondsbefore the actual action tb, we expect our model toreliably predict which button the user will select.Analogously, we define accuracy at testing time?dtestas the percentage of correctly predicted tar-get objects when predicting over episodes at time?dtest.
This pair of training and test parameters isdenoted as the tuple (dtrain, dtest).8145 DatasetWe evaluated the performance of our improvedmodel over data collected by Staudte et al.
(2012)using the GIVE Challenge platform.
Both trainingand testing were performed over a subset of thedata obtained during a collection task involvingworlds created by Gargett et al.
(2010), designedto provide the task with varying levels of diffi-culty.
This corpus provides recorded eye-trackingdata, collected with a remote faceLAB system.
Incontrast, the evaluation presented by Engonopou-los et al.
(2013) uses only games collected for theGIVE 2 and GIVE 2.5 challenges, for which noeye-tracking data is available.
Here, we do not in-vestigate the performance of Psemand concentrateon the direct comparison between Pobsand PEobsin order to find out if and when eye?tracking canimprove the prediction of an RE resolution.We further filtered our corpus in order to re-move noisy games following Koller et al.
(2012),considering only interactions for which the eye-tracker calibration detected inspection of either thetarget or another button object in at least 75% of allreferential scenes in an interaction.
The resultingcorpus comprises 75 games, for a combined lengthof 8 hours.
We extracted 761 episodes from thiscorpus, amounting to 47m 58s of recorded interac-tions, with an average length per episode of 3.78seconds (?
= 3.03sec.).
There are 261 episodesshorter than 2 sec., 207 in the 2-4 sec.
range, 139in the 4-6 sec.
range, and 154 episodes longer than6 sec.6 Evaluation and resultsThe accuracy of our probabilistic models dependson the parameters (dtrain, dtest).
At differentstages of an interaction the difficulty to predict anintended target varies as the visual context changesand in particular the number of visible objects.
Asthe weights of the features are optimized at time?dtrain, it would be expected that testing also attime ?dtest= ?dtrainyields the highest accu-racy.
However, the difficulty to make a predic-tion decreases as tb?
dtestapproaches tb, i.e.
asthe player moves towards the intended target.
Weexpect that testing at ?dtrainworks best, but weneed to be able to update continuously.
Thus wealso evaluate at other timepoints and test severalcombinations of the (dtrain, dtest) parameters.Given the limited amount of eye-tracking dataavailable in our corpus, we replaced the cross-corpora-challenge test setting from the originalPobsstudy with a ten fold cross validation setup.As training and testing were performed over in-stances of a certain minimum length according to(dtrain, dtest), we first removed all instances withlength less than max(dtrain, dtest), and then per-form the cross validation split.
In this way weensure that the number of instances in the foldsare not unbalanced.
Moreover, each instance wasclassified as easy or hard depending on the num-ber of visible objects at time tb.
An instancewas considered easy if no more than three objectswere visible at that point, or hard otherwise.
For?dtest= 0, 59.5% of all instances are consideredhard, but this proportion decreases as ?dtestin-creases.
At ?dtest= ?6, the number of hard in-stances amounts to 72.7%.We evaluated both the original Pobsmodel andthe PEobsmodel on the same data set.
We also cal-culated accuracy values for each feature function,in order to test whether a single function could out-perform Pobs.
We included as baselines two ver-sions of Pobsusing only the features InRoom andVisual Salience proposed by Engonopoulos et al.
(2013).The accuracy results on Figure 2 show our ob-servations for?6 ?
?dtrain?
?2 and?dtrain??dtest?
0.
The graph shows that PEobsperformssimilarly as Pobson the easy instances, i.e.
theeye-tracking features are not contributing in thosescenarios.
However, PEobsshows a consistent im-provement on the hard instances over Pobs.For each permutation of the training and test-ing parameters (dtrain, dtest), we obtain a set ofepisodes that fulfil the length criteria for the givenparameters.
We apply Pobsand PEobson the ob-tained set of instances and measure two corre-sponding accuracy values.
We compared the ac-curacy values of Pobsand PEobsover all 25 differ-ent (dtrain, dtest) pairs, using a paired samples t-test.
The test indicated that the PEobsperformance(M = 83.72, SD = 3.56) is significantly betterthan the Pobsperformance (M = 79.33, SD =3.89), (t(24) = 9.51, p < .001, Cohen?s d =1.17).
Thus eye-tracking features seem to be par-ticularly helpful for predicting to which entity anRE is resolved in hard scenes.The results also show a peak in accuracy nearthe -3 seconds mark.
We computed a 2x2 con-tingency table that contrasts correct and incorrectpredictions for Pobsand PEobs, i.e.
whether oiwas815Figure 2: Accuracy as a function of training and testing time.classified as target object or not.
Data for this ta-ble was collected from all episode judgements formodels trained at times in the [?6 sec.,?3 sec.
]range and tested at -3 seconds.
McNemar?s testshowed that the marginal row and column frequen-cies are significantly different (p < 0.05).
Thispeak is related to the average required time be-tween an utterance and the resulting target manip-ulation.
This result shows that our model is moreaccurate precisely at points in time when we ex-pect fixations to a target object.7 ConclusionIn this paper we have shown that listener?s gazeis useful by showing that accuracy improves overPobsin the context of predicting the resolution ofan RE.
In addition, we observed that our modelPEobsproves to be more robust than Pobswhen thetime interval between the prediction (tb?
dtest)and the button press (tb) increases, i.e.
gaze isespecially beneficial in an early stage of an in-teraction.
This approach shows significant ac-curacy improvement on hard referential sceneswhere more objects are visible.We have also established that gaze is particu-larly useful when combined with some other sim-ple features, as the features that capture listenersvisual behaviour are not powerful enough to out-perform even the simplest baseline.
Gaze onlybenefits the model when it is added on top of fea-tures that capture the visual context, i.e.
the currentscene.The most immediate future line of research isthe combination of our PEobsmodel with the se-mantic model Psem, in order to test the impact ofthe extended features in a combined model.
If suc-cessful, such a model could provide reliable pre-dictions for a significant amount of time before anaction takes place.
This is of particular importancewhen it comes to designing a system that auto-matically generates and online outputs feedback toconfirm correct and reject incorrect intentions.Testing with users in real time is also an areafor future research.
An implementation of the Pobsmodel is currently in the test phase, and an exten-sion for the PEobsmodel would be the immediatenext step.
The model could be embedded in anNLG system to improve the automatic languagegeneration in such scenarios.Given that our work refers only to NLG sys-tems, there?s no possible analysis of speaker?sgaze.
However, it may be interesting to askwhether a human IG could benefit from the pre-dictions of PEobs.
We could study whether pre-dictions based on the gaze (mis-)match betweenboth interlocutors are more effective than simplypresenting the IF?s gaze to the IG and trusting theIG to correctly interpret this data.
If such a sys-tem proved to be effective, it could point misun-derstandings to the IG before either of the partici-pants becomes aware of them.AcknowledgementsThis work was funded by the Cluster of Excel-lence on ?Multimodal Computing and Interaction?of the German Excellence Initiative and the SFB632 ?Information Structure?.816ReferencesSusan E. Brennan, Katharina S. Schuhmann, andKarla M. Batres.
2013.
Entrainment on the moveand in the lab: The walking around corpus.
In Pro-ceedings of the 35th Annual Conference of the Cog-nitive Science Society, Berlin, Germany.Herbert H. Clark and Meredyth A. Krych.
2004.Speaking while monitoring addressees for under-standing.
Journal of Memory and Language,50(1):62?81, January.Herbert H. Clark.
1996.
Using Language.
CambridgeUniversity Press, May.Nikos Engonopoulos, Mart?
?n Villalba, Ivan Titov, andAlexander Koller.
2013.
Predicting the resolutionof referring expressions from user behavior.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP), Seattle.Andrew Gargett, Konstantina Garoufi, AlexanderKoller, and Kristina Striegnitz.
2010.
The give-2 corpus of giving instructions in virtual environ-ments.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10), Valletta, Malta, May.
European Lan-guage Resources Association (ELRA).Joy E. Hanna and Susan E. Brennan.
2007.
Speakers?eye gaze disambiguates referring expressions earlyduring face-to-face conversation.
Journal of Mem-ory and Language, 57(4):596?615, November.John M. Henderson and Tim J. Smith.
2007.
Howare eye fixation durations controlled during sceneviewing?
further evidence from a scene onset delayparadigm.
Visual Cognition, 17(6-7):1055?1082.Casey Kennington and David Schlangen.
2014.
Com-paring listener gaze with predictions of an incremen-tal reference resolution model.
RefNet workshop onPsychological and Computational Models of Refer-ence Comprehension and Production.Alexander Koller, Kristina Striegnitz, Andrew Gargett,Donna Byron, Justine Cassell, Robert Dale, JohannaMoore, and Jon Oberlander.
2010.
Report on theSecond NLG Challenge on Generating Instructionsin Virtual Environments (GIVE-2).
In Proceedingsof the 6th International Natural Language Genera-tion Conference (INLG).Alexander Koller, Maria Staudte, Konstantina Garoufi,and Matthew Crocker.
2012.
Enhancing referen-tial success by tracking hearer gaze.
In Proceed-ings of the 13th Annual Meeting of the Special In-terest Group on Discourse and Dialogue, SIGDIAL?12, pages 30?39, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Maria Staudte, Alexander Koller, Konstantina Garoufi,and Matthew Crocker.
2012.
Using listener gaze toaugment speech generation in a virtual 3D environ-ment.
In Proceedings of the 34th Annual Meeting ofthe Cognitive Science Society (CogSci), Sapporo.817
