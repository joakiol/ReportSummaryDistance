TnT - -  A S ta t i s t i ca l  Par t -o f -Speech  TaggerThors ten  BrantsSaarland UniversityComputat iona l  LinguisticsD-66041 Saarbriicken, Germanythors t  en@co l i ,  un i - sb ,  deAbst rac tTrigrams'n'Tags (TnT) is an efficient statisticalpart-of-speech tagger.
Contrary to claims foundelsewhere in the literature, we argue that a taggerbased on Markov models performs at least as well asother current approaches, including the MaximumEntropy framework.
A recent comparison has evenshown that TnT performs ignificantly better for thetested corpora.
We describe the basic model of TnT,the techniques used for smoothing and for handlingunknown words.
Furthermore, we present evalua-tions on two corpora.1 In t roduct ionA large number of current language processing sys-tems use a part-of-speech tagger for pre-processing.The tagger assigns a (unique or ambiguous) part-of-speech tag to each token in the input and passes itsoutput o the next processing level, usually a parser.Furthermore, there is a large interest in part-of-speech tagging for corpus annotation projects, whocreate valuable linguistic resources by a combinationof automatic processing and human correction.For both applications, a tagger with the highestpossible accuracy is required.
The debate aboutwhich paradigm solves the part-of-speech taggingproblem best is not finished.
Recent comparisonsof approaches that can be trained on corpora (vanHalteren et al, 1998; Volk and Schneider, 1998) haveshown that in most cases statistical aproaches (Cut-ting et al, 1992; Schmid, 1995; Ratnaparkhi, 1996)yield better results than finite-state, rule-based, ormemory-based taggers (Brill, 1993; Daelemans etal.,1996).
They are only surpassed by combinations ofdifferent systems, forming a "voting tagger".Among the statistical approaches, the MaximumEntropy framework has a very strong position.
Nev-ertheless, a recent independent comparison of 7 tag-gets (Zavrel and Daelemans, 1999) has shown thatanother approach even works better: Markov mod-els combined with a good smoothing technique andwith handling of unknown words.
This tagger, TnT,not only yielded the highest accuracy, it also was thefastest both in training and tagging.The tagger comparison was organized as a "black-box test": set the same task to every tagger andcompare the outcomes.
This paper describes themodels and techniques used by TnT together withthe implementation.The reader will be surprised how simple the under-lying model is.
The result of the tagger comparisonseems to support he maxime "the simplest is thebest".
However, in this paper we clarify a numberof details that are omitted in major previous pub-lications concerning tagging with Markov models.As two examples, (Rabiner, 1989) and (Charniaket al, 1993) give good overviews of the techniquesand equations used for Markov models and part-of-speech tagging, but they are not very explicit in thedetails that are needed for their application.
We ar-gue that it is not only the choice of the general modelthat determines the result of the tagger but also thevarious "small" decisions on alternatives.The aim of this paper is to give a detailed ac-count of the techniques used in TnT.
Additionally,we present results of the tagger on the NEGRA cor-pus (Brants et al, 1999) and the Penn Treebank(Marcus et al, 1993).
The Penn Treebank resultsreported here for the Markov model approach areat least equivalent to those reported for the Maxi-mum Entropy approach in (Ratnaparkhi, 1996).
Fora comparison to other taggers, the reader is referredto (Zavrel and Daelemans, 1999).2 Arch i tec ture2.1 The Under ly ing ModelTnT uses second order Markov models for part-of-speech tagging.
The states of the model representtags, outputs represent the words.
Transition prob-abilities depend on the states, thus pairs of tags.Output probabilities only depend on the most re-cent category.
To be explicit, we calculateargmax P(tilti-1, ti-2)P(wilti P(tr+l ItT)ti...iT(i)for a given sequence of words w I .
.
.
W T of length T.tl... tT are elements of the tagset, the additional224tags t - l ,  to, and tT+l are beginning-of-sequenceand end-of-sequence markers.
Using these additionaltags, even if they stem from rudimentary process-ing of punctuation marks, slightly improves taggingresults.
This is different from formulas presentedin other publications, which just stop with a "looseend" at the last word.
If sentence boundaries arenot marked in the input, TnT adds these tags if itencounters one of \[.!?
;\] as a token.Transition and output probabilities are estimatedfrom a tagged corpus.
As a first step, we use themaximum likelihood probabilities /5 which are de-rived from the relative frequencies:Unigrams: /5(t3) = f(t3) (2)Nf(t2, t3) (3)Bigrams: P(t31t~)= f(t2)f(ta, t2, t3)Trigrams: /5(t3ltx,t~) - f ( t l , t2)  (4)Lexical: /5(w3 It3) - /(w3, t3) (5)f(t3)for all tl, t2, t3 in the tagset and w3 in the lexi-con.
N is the total number of tokens in the trainingcorpus.
We define a maximum likelihood probabil-ity to be zero if the corresponding nominators anddenominators are zero.
As a second step, contex-tual frequencies are smoothed and lexical frequencesare completed by handling words that are not in thelexicon (see below).2.2 Smooth ingTrigram probabilities generated from a corpus usu-ally cannot directly be used because of the sparse-data problem.
This means that there are not enoughinstances for each trigram to reliably estimate theprobability.
Furthermore, setting a probability tozero because the corresponding trigram never oc-cured in the corpus has an undesired effect.
It causesthe probability of a complete sequence to be set tozero if its use is necessary for a new text sequence,thus makes it impossible to rank different sequencescontaining a zero probability.The smoothing paradigm that delivers the bestresults in TnT is linear interpolation of unigrams,bigrams, and trigrams.
Therefore, we estimate atrigram probability as follows:P(t3ltl ,  t2) = AlP(t3) + Ag_/5(t31t2) + A3/5(t3\[t1, t2)(6)/5 are maximum likelihood estimates of the proba-bilities, and A1 + A2 + A3 = 1, so P again representprobability distributions.We use the context-independent variant of linearinterpolation, i.e., the values of the As do not dependon the particular trigram.
Contrary to intuition,this yields better esults than the context-dependentvariant.
Due to sparse-data problems, one cannot es-timate a different set of As for each trigram.
There-fore, it is common practice to group trigrams by fre-quency and estimate tied sets of As.
However, weare not aware of any publication that has investi-gated frequency groupings for linear interpolation ipart-of-speech tagging.
All groupings that we havetested yielded at most equivalent results to context-independent linear interpolation.
Some groupingseven yielded worse results.
The tested groupingsincluded a) one set of As for each frequency valueand b) two classes (low and high frequency) on thetwo ends of the scale, as well as several groupingsin between and several settings for partitioning theclasses.The values of Ax, A2, and A3 are estimated bydeleted interpolation.
This technique successivelyremoves each trigram from the training corpus andestimates best values for the As from all other n-grams in the corpus.
Given the frequency countsfor uni-, bi-, and trigrams, the weights can be veryefficiently determined with a processing time linearin the number of different rigrams.
The algorithmis given in figure 1.
Note that subtracting 1 meanstaking unseen data into account.
Without this sub-traction the model would overfit the training dataand would generally ield worse results.2.3 Handling of Unknown WordsCurrently, the method of handling unknown wordsthat seems to work best for inflected languages isa suffix analysis as proposed in (Samuelsson, 1993).Tag probabilities are set according to the word's end-ing.
The suffix is a strong predictor for word classes,e.g., words in the Wall Street Journal part of thePenn Treebank ending in able are adjectives (JJ) in98% of the cases (e.g.
fashionable, variable), the restof 2% are nouns (e.g.
cable, variable).The probability distribution for a particular suf-fix is generated from all words in the training setthat share the same suffix of some predefined max-imum length.
The term suffix as used here means"final sequence of characters of a word" which is notnecessarily a linguistically meaningful suffix.Probabilities are smoothed by successive abstrac-tion.
This calculates the probability of a tag tgiven the last m letters li of an n letter word:P(t l ln - r ,+l , .
.
.
ln) .
The sequence of increasinglymore general contexts omits more and more char-acters of the suffix, such that P(t l ln_m+2,.
.
.
, ln) ,P(t \ [ ln-m+3,.
.
.
, l~) ,  .
.
.
,  P(t) are used for smooth-ing.
The recursiou formula isP(t l l ,_ i+l,  .
.
.
ln)= P( t l ln - i+ l , .
.
.
In) + O iP ( t l l~- , .
.
.
,  ln) (7)1 +0~225set  )%1 ---- )%2 = )%3 = 0fo reach  t r ig ram tl,t2,t3 with  f(t l ,t2,t3) > 0depending on the maximum of the fo l low ing  three  va lues :f(tl ,t2,ts)-- 1 case f (h,t2)- I  " increment )%3 by f ( t l , t2 , t3)f(t2,t3)-I case f(t2)- I  " increment )%2 by f ( t l , t2 , t s )f ( t3 ) - - icase N-1  " increment )%1 by f(t l ,t2,t3)endendnormalize )%1, )%2, )%3Figure 1: Algorithm for calculting the weights for context-independent li ear interpolation )%1, )%2, )%3 whenthe n-gram frequencies are known.
N is the size of the corpus?
If the denominator in one of the expressionsis 0, we define the result of that expression to be 0.for i = m. .
.
0, using the maximum likelihood esti-mates/5  from frequencies in the lexicon, weights Oiand the initializationP(t) =/5(t) .
(8)The maximum likelihood estimate for a suffix oflength i is derived from corpus frequencies byP(t i /~- i+l ,  .. l~) = f (t ,  1~-~+1,... l~) ?
(9 )For the Markov model, we need the inverse condi-tional probabilities P ( / , - i+ l , .
.
.
lnlt) which are ob-tained by Bayesian inversion.A theoretical motivated argumentation uses thestandard eviation of the maximum likelihood prob-abilities for the weights 0i (Samuelsson, 1993)?This leaves room for interpretation.1) One has to identify a good value for m, thelongest suffix used?
The approach taken for TnT isthe following: m depends on the word in question.We use the longest suffix that we can find in thetraining set (i.e., for which the frequency is greaterthan or equal to 1), but at most 10 characters.
Thisis an empirically determined choice?2) We use a context-independent approach for 0i,as we did for the contextual weights )%i.
It turnedout to be a good choice to set al 0i to the standarddeviation of the unconditioned maximum likelihoodprobabilities of the tags in the training corpus, i.e.,we set1$Oi = ~--~(/5(tj) - 15)2 (10)s 1 j= lfor all i = 0 .
.
.
m - 1, using a tagset of s tags andthe average$/5 = 1 ~/5(t j )  (11)8j----IThis usually yields values in the range 0.03 .. .
0.10.3) We use different estimates for uppercase andlowercase words, i.e., we maintain two different suffixtries depending on the capitalization of the word.This information improves the tagging results?4) Another freedom concerns the choice of thewords in the lexicon that should be used for suf-fix handling.
Should we use all words, or are someof them better suited than others?
Accepting thatunknown words are most probably infrequent, onecan argue that using suffixes of infrequent words inthe lexicon is a better approximation for unknownwords than using suffixes of frequent words.
There-fore, we restrict the procedure of suffix handling towords with a frequency smaller than or equal to somethreshold value.
Empirically, 10 turned out to be agood choice for this threshold.2.4 Cap i ta l i za t ionAdditional information that turned out to be use-ful for the disambiguation process for several cor-pora and tagsets is capitalization information.
Tagsare usually not informative about capitalization, butprobability distributions of tags around capitalizedwords are different from those not capitalized.
Theeffect is larger for English, which only capitalizesproper names, and smaller for German, which capi-talizes all nouns.We use flags ci that are true if wi is a capitalizedword and false otherwise?
These flags are added tothe contextual probability distributions.
Instead ofP(tsItl,t2) (12)we useP(t3, c3\[tl , cl, t2, c2) (13)and equations (3) to (5) are updated accordingly.This is equivalent o doubling the size of the tagsetand using different ags depending on capitalization.2262.5 Beam SearchThe processing time of the Viterbi algorithm (Ra-biner, 1989) can be reduced by introducing a beamsearch.
Each state that receives a 5 value smallerthan the largest 5 divided by some threshold valueis excluded from further processing.
While theViterbi algorithm is guaranteed to find the sequenceof states with the highest probability, this is nolonger true when beam search is added.
Neverthe-less, for practical purposes and the right choice of0, there is virtually no difference between the algo-rithm with and without a beam.
Empirically, a valueof 0 = 1000 turned out to approximately double thespeed of the tagger without affecting the accuracy.The tagger currently tags between 30~000 and60,000 tokens per second (including file I/O) on aPentium 500 running Linux.
The speed mainly de-pends on the percentage of unknown words and onthe average ambiguity rate.3 Eva luat ionWe evaluate the tagger's performance under severalaspects.
First of all, we determine the tagging ac-curacy averaged over ten iterations.
The overall ac-curacy, as well as separate accuracies for known andunknown words are measured.Second, learning curves are presented, that indi-cate the performance when using training corpora ofdifferent sizes, starting with as few as 1,000 tokensand ranging to the size of the entire corpus (minusthe test set).An important characteristic of statistical taggersis that they not only assign tags to words but alsoprobabilities in order to rank different assignments.We distinguish reliable from unreliable assignmentsby the quotient of the best and second best assign-ments 1.
All assignments for which this quotient islarger than some threshold are regarded as reliable,the others as unreliable.
As we will see below, accu-racies for reliable assignments are much higher.The tests are performed on partitions of the cor-pora that use 90% as training set and 10% as testset, so that the test data is guaranteed to be unseenduring training.
Each result is obtained by repeat-ing the experiment 10 times with different partitionsand averaging the single outcomes.In all experiments, contiguous test sets are used.The alternative is a round-robin procedure that putsevery 10th sentence into the test set.
We argue thatcontiguous test sets yield more realistic results be-cause completely unseen articles are tagged.
Usingthe round-robin procedure, parts of an article are al-ready seen, which significantly reduces the percent-age of unknown words.
Therefore, we expect even1 By definition, this quotient is co if there is only one pos-sible tag for a given word.higher results when testing on every 10th sentenceinstead of a contiguous et of 10%.In the following, accuracy denotes the number ofcorrectly assigned tags divided by the number of to-kens in the corpus processed.
The tagger is allowedto assign exactly one tag to each token.We distinguish the overall accuracy, taking intoaccount all tokens in the test corpus, and separateaccuracies for known and unknown tokens.
The lat-ter are interesting, since usually unknown tokens aremuch more difficult to process than known tokens,for which a list of valid tags can be found in thelexicon.3.1 Tagging the  NEGRA corpusThe German NEGRA corpus consists of 20,000 sen-tences (355,000 tokens) of newspaper texts (Frank-furter Rundschau) that are annotated with parts-of-speech and predicate-argument structures (Skut etal., 1997).
It was developed at the Saarland Univer-sity in Saarbrficken 2.
Part of it was tagged at theIMS Stuttgart.
This evaluation only uses the part-of-speech annotation and ignores structural annota-tions.Tagging accuracies for the NEGRA corpus areshown in table 2.Figure 3 shows the learning curve of the tagger,i.e., the accuracy depending on the amount of train-ing data.
Training length is the nmnber of tokensused for training.
Each training length was testedten times, training and test sets were randomly cho-sen and disjoint, results were averaged.
The traininglength is given on a logarithmic scale.It is remarkable that tagging accuracy for knownwords is very high even for very small training cot-pora.
This means that we have a good chance ofgetting the right tag if a word is seen at least onceduring training.
Average percentages of unknowntokens are shown in the bottom line of each diagram.We exploit the fact that the tagger not only de-termines tags, but also assigns probabilities.
If thereis an alternative that has a probability "close to"that of the best assignment, his alternative can beviewed as almost equally well suited.
The notion of"close to" is expressed by the distance of probabil-ities, and this in turn is expressed by the quotientof probabilities.
So, the distance of the probabili-ties of a best tag tbest and an alternative tag tartis expressed by P(tbest)/p(tau), which is some valuegreater or equal to 1 since the best tag assignmenthas the highest probability.Figure 4 shows the accuracy when separating as-signments with quotients larger and smaller thanthe threshold (hence reliable and unreliable assign-ments).
As expected, we find that accuracies for2For availability, please checkh~tp ://w~.
col i. uni-sb, de/s fb378/negra-corpus777 227Table 2: Part-of-speech tagging accuracy for the NEGRA corpus, averaged over 10 test runs, training andtest set are disjoint.
The table shows the percentage of unknown tokens, separate accuracies and standarddeviations for known and unknown tokens, as well as the overall accuracy.percentageunknownsNEGRA corpus 11.9%knownace .97.7% 0.23unknownacc.
(x89.0% 0.72overallaCE.
o"96.7% 0.29NEGRA Corpus :  POS Learn ing  Curve1009O8070 /S6O50 , i1 2 550.8 46.4 41.4I i i I I I i10 20 50 100 200 320 50036.0 30.7 23.0 18.3 14.3 11.9 11).3Overallmin =78.1%max=96.7%e Knownrain =95.7%max=97.7%- - -a- - -  Unknownrain =61.2%max=89.0%I1000 x 1000 Training LengthSt  avg.
percentage unknownFigure 3: Learning curve for tagging the NEGRA corpus.
The training sets of variable sizes as well as testsets of 30,000 tokens were randomly chosen.
Training and test sets were disjoint, the procedure was repeated10 times and results were averaged.
Percentages of unknowns for 500k and 1000k training are determinedfrom an untagged extension.NEGRA Corpus: Accuracy of  re l iab le  assignments100999897Af " / :.
Reliable rain =96.7% max=99.4%96 I i i i i i i i i i i2 5 10 20 50 100 500 2000 10000 threshold 0100 97.9 95.1 92.7 90.3 86.8 84.1 81.0 76.1 71.9 68.3 64.1 62.0 % cases reliable- 53.5 62.9 69.6 74.5 79.8 82.7 85.2 88.0 89.6 90.8 91.8 92.2 acc.
of complementFigure 4: Tagging accuracy for the NEGRA corpus when separating reliable and unreliable assignments.
Thecurve shows accuracies for reliable assignments.
The numbers at the bottom line indicate the percentage ofreliable assignments and the accuracy of the complement set (i.e., unreliable assignments).228Table 5: Part-of-speech tagging accuracy for the Penn Treebank.
The table shows the percentage of unknowntokens, separate accuracies and standard deviations for known and unknown tokens, as well as the overallaccuracy.I percentage knownunknowns acc.
aPenn Treebank 2.9% 97.0% 0.15unknownaCC.
O"85.5% 0.69overallaCE.
O"96.7% 0.151009O8070 <60Penn Treebank: POS Learn ing  Curve/50 I I ~ i I I I i I1 2 5 10 20 50 100 200 50050.3 42.8 33.4 26.8 20.2 13.2 9.8 7.0 4.4Overallrain =78.6%max=96.7%Knownrain =95.2%max=97.0%Unknownmin =62.2%max=85.5%I1000 ?
1000 Training Length2.9 avg.
percentage unknownFigure 6: Learning curve for tagging the Penn Treebank.
The training sets of variable sizes as well as test setsof 100,000 tokens were randomly chosen.
Training and test sets were disjoint, the procedure was repeated10 times and results were averaged.Penn Treebank: Accuracy of reliable assignments100999897Overallrain =96.6%max=99.4%96 i i i i t i i i i i i i2 5 10 20 50 100 500 2000 10000 threshold 0100 97.7 94.6 92.2 89.8 86.3 83.5 80.4 76.6 73.8 71.0 67.2 64.5 % cases reliable- 53.5 62.8 68.9 73.9 79.3 82.6 85.2 87.5 88.8 89.8 91.0 91.6 acc.
of complementFigure 7: Tagging accuracy for the Penn Treebank when separating reliable and unreliable assignments.
Thecurve shows accuracies for reliable assignments.
The numbers at the bottom line indicate the percentage ofreliable assignments and the accuracy of the complement set.229reliable assignments are much higher than for unre-liable assignments.
This distinction is, e.g., usefulfor annotation projects during the cleaning process,or during pre-processing, sothe tagger can emit mul-tiple tags if the best tag is classified as unreliable.3.2 Tagging the Penn TreebankWe use the Wall Street Journal as contained in thePenn Treebank for our experiments.
The annotationconsists of four parts: 1) a context-free structureaugmented with traces to mark movement and dis-continuous constituents, 2) phrasal categories thatare annotated as node labels, 3) a small set of gram-matical functions that are annotated as extensions tothe node labels, and 4) part-of-speech tags (Marcuset al, 1993).
This evaluation only uses the part-of-speech annotation.The Wall Street Journal part of the Penn Tree-bank consists of approx.
50,000 sentences (1.2 mil-lion tokens).Tagging accuracies for the Penn Treebank areshown in table 5.
Figure 6 shows the learning curveof the tagger, i.e., the accuracy depending on theamount of training data.
Training length is the num-ber of tokens used for training.
Each training lengthwas tested ten times.
Training and test sets weredisjoint, results are averaged.
The training length isgiven on a logarithmic scale.
As for the NEGRA cor-pus, tagging accuracy is very high for known tokenseven with small amounts of training data.We exploit the fact that the tagger not only de-termines tags, but also assigns probabilities.
Figure7 shows the accuracy when separating assignmentswith quotients larger and smaller than the threshold(hence reliable and unreliable assignments).
Again,we find that accuracies for reliable assignments aremuch higher than for unreliable assignments.3.3 Summary of Part-of-Speech TaggingResultsAverage part-of-speech tagging accuracy is between96% and 97%, depending on language and tagset,which is at least on a par with state-of-the-art re-sults found in the literature, possibly better.
Forthe Penn Treebank, (Ratnaparkhi, 1996) reports anaccuracy of 96.6% using the Maximum Entropy ap-proach, our much simpler and therefore faster HMMapproach delivers 96.7%.
This comparison eeds tobe re-examined, since we use a ten-fold crossvalida-tion and averaging of results while Ratnaparkhi onlymakes one test run.The accuracy for known tokens is significantlyhigher than for unknown tokens.
For the Germannewspaper data, results are 8.7% better when theword was seen before and therefore is in the lexicon,than when it was not seen before (97.7% vs. 89.0%).Accuracy for known tokens is high even with verysmall amounts of training data.
As few as 1000 to-kens are sufficient o achieve 95%-96% accuracy forthem.
It is important for the tagger to have seen aword at least once during training.Stochastic taggers assign probabilities to tags.
Weexploit the probabilities to determine reliability ofassignments.
For a subset hat is determined uringprocessing by the tagger we achieve accuracy ratesof over 99%.
The accuracy of the complement set ismuch lower.
This information can, e.g., be exploitedin an annotation project to give an additional treat-ment to the unreliable assignments, or to pass se-lected ambiguities to a subsequent processing step.4 Conc lus ionWe have shown that a tagger based on Markov mod-els yields state-of-the-art results, despite contraryclaims found in the literature.
For example, theMarkov model tagger used in the comparison of (vanHalteren et al, 1998) yielded worse results than allother taggers.
In our opinion, a reason for the wrongclaim is that the basic algorithms leave several deci-sions to the implementor.
The rather large amountof freedom was not handled in detail in previous pub-lications: handling of start- and end-of-sequence, theexact smoothing technique, how to determine theweights for context probabilities, details on handlingunknown words, and how to determine the weightsfor unknown words.
Note that the decisions we madeyield good results for both the German and the En-glish Corpus.
They do so for several other corporaas well.
The architecture remains applicable to alarge variety of languages.According to current tagger comparisons (vanHalteren et al, 1998; Zavrel and Daelemans, 1999),and according to a comparsion of the results pre-sented here with those in (Ratnaparkhi, 1996), theMaximum Entropy framework seems to be the onlyother approach yielding comparable results to theone presented here.
It is a very interesting futureresearch topic to determine the advantages of eitherof these approaches, to find the reason for their highaccuracies, and to find a good combination of both.TnT is freely available to universities and re-lated organizations for research purposes (seehttp ://www.
coli.
uni-sb, de/-thorsten/tnt).AcknowledgementsMany thanks go to Hans Uszkoreit for his sup-port during the development of TnT.
Most of thework on TnT was carried out while the authorreceived a grant of the Deutsche Forschungsge-meinschaft in the Graduiertenkolleg Kognitionswis-senschaft Saarbriicken.
Large annotated corpora rethe pre-requisite for developing and testing part-of-speech taggers, and they enable the generation ofhigh-quality language models.
Therefore, I would230like to thank all the people who took the effortto annotate the Penn Treebank, the Susanne Cor-pus, the Stuttgarter Referenzkorpus, the NEGRACorpus, the Verbmobil Corpora, and several others.And, last but not least, I would like to thank theusers of TnT who provided me with bug reports andvaluable suggestions for improvements.Re ferencesThorsten Brants, Wojciech Skut, and Hans Uszko-reit.
1999.
Syntactic annotation of a Germannewspaper corpus.
In Proceedings of the ATALATreebank Workshop, pages 69-76, Paris, France.Eric Brill.
1993.
A Corpus-Based Approach to Lan-guage Learning.
Ph.D. Dissertation, Departmentof Computer and Information Science, Universityof Pennsylvania.Eugene Charniak, Curtis Hendrickson, Neil Ja-cobson, and Mike Perkowitz.
1993.
Equationsfor part-of-speech tagging.
In Proceedings of theEleventh National Con\[erence on Artificial In-telligence, pages 784-789, Menlo Park: AAAIPress/MIT Press.Doug Cutting, Julian Kupiec, Jan Pedersen, andPenelope Sibun.
1992.
A practical part-of-speechtagger.
In Proceedings of the 3rd Conferenceon Applied Natural Language Processing (ACL),pages 133-140.Walter Daelemans, Jakub Zavrel, Peter Berck, andSteven Gillis.
1996.
Mbt: A memory-based partof speech tagger-generator.
In Proceedings of theWorkshop on Very Large Corpora, Copenhagen,Denmark.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Compu-tational Linguistics, 19(2):313-330.Lawrence R. Rabiner.
1989.
A tutorial on Hid-den Markov Models and selected applications inspeech recognition.
In Proceedings o\] the IEEE,volume 77(2), pages 257-285.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingso\] the Conference on Empirical Methods in Nat-ural Language Processing EMNLP-96, Philadel-phia, PA.Christer Samuelsson.
1993.
Morphological tag-ging based entirely on Bayesian inference.
In9th Nordic Conference on Computational Lin-guistics NODALIDA-93, Stockholm University,Stockholm, Sweden.Helmut Schmid.
1995.
Improvements in part-of-speech tagging with an application to German.In Helmut Feldweg and Erhard Hinrichts, editors,Lexikon und Text.
Niemeyer, Tfibingen.Wojciech Skut, Brigitte Krenn, Thorsten Brants,and Hans Uszkoreit.
1997.
An annotation schemefor free word order languages.
In Proceedings ofthe Fifth Conference on Applied Natural LanguageProcessing ANLP-97, Washington, DC.Hans van Halteren, Jakub Zavrel, and Walter Daele-mans.
1998.
Improving data driven wordclass tag-ging by system combination.
In Proceedings of theInternational Conference on Computational Lin-guistics COLING-98, pages 491-497, Montreal,Canada.Martin Volk and Gerold Schneider.
1998.
Compar-ing a statistical and a rule-based tagger for ger-man.
In Proceedings of KONVENS-98, pages 125-137, Bonn.Jakub Zavrel and Walter Daelemans.
1999.
Eval-uatie van part-of-speech taggers voor bet cor-pus gesproken ederlands.
CGN technical report,Katholieke Universiteit Brabant, Tilburg.231
