Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2071?2076,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsNatural Language Model Re-usability for Scaling to Different DomainsYoung-Bum Kim, Alexandre Rochette and Ruhi SarikayaMicrosoft Corporation, Redmond, WAybkim,alrochet,ruhi.sarikaya@microsoft.comAbstractNatural language understanding is the coreof the human computer interactions.
How-ever, building new domains and tasks thatneed a separate set of models is a bottle-neck for scaling to a large number of do-mains and experiences.
In this paper, wepropose a practical technique that addressesthis issue in a web-scale language understand-ing system: Microsoft?s personal digital as-sistant Cortana.
The proposed technique usesa constrained decoding method with a uni-versal slot tagging model sharing the sameschema as the collection of slot taggers builtfor each domain.
The proposed approach al-lows reusing of slots across different domainsand tasks while achieving virtually the sameperformance as those slot taggers trained perdomain fashion.1 IntroductionRecently there has been tremendous investment intothe personal digital assistants by big technologycompanies (Sarikaya, 2015; Sarikaya et al, 2016).Apple?s SIRI, Google Now, Microsoft?s Cortana andAmazon?s Alexa are examples of such systems.
Nat-ural language understanding (Gupta et al, 2006; Turand De Mori, 2011)is at the core of these systemsproviding natural communication between the userand the agent.
These systems support a number ofscenarios including creating reminders, setting upalarms, note taking, scheduling meetings, findingand consuming entertainment (i.e.
movie, music,games), finding places of interest and getting driv-ing directions to them.
The domains supported bythese systems are on the order of tens (not in hun-dreds) and adding new domains and experiences isa scaling problem that has not been solved yet (Tur,2006; Jeong and Lee, 2009; El-Kahky et al, 2014;Kim et al, 2015d).The primary reason behind this is that each do-main requires potentially a new schema, intentsand slots extracted from the natural language query.That, in turn requires collecting and annotating newdata, which is the most expensive step in terms oftime and money, and building a new set of domainspecific models to support the new domains and sce-narios.
Slot modeling in particular is the most de-manding component in terms of the difficulty of an-notation and modeling.In this study, we propose a new approach that re-duces the cost of scaling natural language under-standing to a large number of domains and expe-riences without significantly sacrificing the accu-racy.
The approach consists of a universal slot tag-ging method and a runtime technique called con-strained decoding that performs the decoding ac-cording a specific schema.
The proposed approach,heavily enables reusing existing slot modeling datathat are collected and annotated for potentially dif-ferent domains and applications, for building newdomains and applications.
The new domains can beexpressed in terms of existing semantic schema.The rest of the paper is organized as follows.
Inthe next section, we talk about universal slot mod-eling.
In section 3, we present the constrained de-coding technique.
We describe the experimental setup, results and analysis in section 4 followed by theconclusions and future directions in section 5.20712 Universal Slot TaggingThe first step is to build a universal slot tagger, a sin-gle model that can handle all the domains an agent(e.g.
Cortana) can support.
In Table 1, we show asubset of the domains that are supported by Cortanafor experimentation purposes.2.1 Universal Slot Tagger TrainingTo train the universal slot tagger, we consider twosimple and intuitive approaches: Binary and All-in-one.Suppose we have a combined k slots across do-mains and also have access to the labeled data, Bi-nary approach trains k binary classifier one for eachslot type.
For each binary slot tagger targeting a spe-cific slot type, the labeled data is programaticallymapped to create a new labeled data set, where onlythe target label is kept while all the other labels aremapped ?other?
label.
All-in-one approach simplytrains a single model by aggregating queries acrossall domains.2.2 Slot Tagging AmbiguityUniversal slot tagging model has an advantage,which can share schema across all domains usedfor training time.
In spite of the advantage, thereare ambiguity problems caused by combining all do-mains (and the underlying data) into a single model.The problems can be grouped into two categories:?
Imbalanced training data distribution: Theamount of training data varies across domains.Universal slot model may have bias towardspredicting the slots in domains with largertraining data.
For example, slots with less train-ing data (e.g.
app name in MEDIACONTROLdomain could be overwhelmed by slots withlarge training data (e.g.
place name in PLACESdomain).?
Domain-specific schema: In practice, the do-mains the system handles are not constructedat the same time.
They are designed for dif-ferent application back-ends, requirements andscenarios at different points in time.
In otherwords, the semantic schema for a domain is de-signed without considering other domains.
InFigure 1: Constrained Lattice: Disabling nodes and transitionwhile decoding the lattice to honor given constraints of domainschema.ALARM domain, the slot indicating time is sub-divided into sub-slots such as start time repre-senting starting time, duration representing du-ration for an alarm.
In contrast, in PLACES do-main, there is only a single slot indicating time(time).3 Constrained DecodingSlot tagging is considered as a sequence learningproblem (Deoras and Sarikaya, 2013; Li et al,2009; Xu and Sarikaya, 2014; Celikyilmaz et al,2015; Kim et al, 2015b; Kim et al, 2015c; Kimet al, 2015a).
In sequence learning, given a sam-ple query x1 .
.
.
xn, the decoding problem is to findthe most likely slot sequence among all the possiblesequences, y1 .
.
.
yn:f(x1 .
.
.
xn) = argmaxy1...ynp(x1 .
.
.
xn, y1 .
.
.
yn)Here, we assume that output space in training issame as those in test time.However, in our problem, output (slot) space intest time can be different from those in trainingtime.
At test time, we may observe different slot se-quences than what is observed in the training time.This is not an issue for the Binary approach, sincewe can use the output of the selected taggers neededfor the new domain.
We simply use general decod-ing approach with each of the selected taggers.
Note2072that a given query is run as many times as the num-bers of slot types covered in a given domain.For All-in-One technique, we consider two possi-ble approaches: Post-Filter and Constrained Decod-ing.
With Post-Filter, we simply provide the besthypothesis generated by the slot tagger that meetsthe domain schema constraints, by computing thefull n-best of slots and filtering out the slot typesthat do not meet the target domain schema.
WithConstrained Decoding, given a schema y?
?
y forthe target domain, we first define a constrained lat-tice lattice Y(x, y?)
= Y(x1, y?)?
.
.
.
?Y(xn, y?
), asshown in Figure 1.
Then, we perform the decodingin the constrained lattice:f(x1 .
.
.
xn, y?)
= argmaxY(x,y?
)p(x1 .
.
.
xn, y1 .
.
.
yn)4 ExperimentsIn this section, we conducted a series of experimentsto evaluate the proposed techniques on datasets ob-tained from real usage.4.1 Experimental SetupTo test the effectiveness of the proposed approach,we apply it to a suite of 16 Cortana domains forslot tagging tasks.
The data statistics and shortdescriptions are shown in Table 1.
As the ta-ble indicates, the domains have different granu-larity and diverse semantics.
Note that we keepdomain-specific slots such as alarm state, but thereare enough shared labels across domains.
For exam-ple, ALARM domain, there are 6 shared slots among8 slots.
There are 62 slots appearing more than onedomain.
Especially, some basic slots such as time,date,place name,person name,location and productappear in most domains.4.2 Slot TaggersIn all our experiments, we trained Conditional Ran-dom Fields (CRFs)(Lafferty et al, 2001) and usedn-gram features up to n = 3, regular expression,lexicon features, and Brown Clusters (Brown et al,1992).
With these features, we compare the follow-ing methods for slot tagging1:1For parameter estimation, we used L-BFGS (Liu and No-cedal, 1989) with 100 as the maximum iteration count and 1.0for the L2 regularization parameter.?
In-domain: Train a domain specific model us-ing the domain specific data covering the slotssupported in that domain.?
Binary: Train a binary classifier for each slottype, combine the slots needed for a given do-main schema.?
Post: Train a single model with all domain data,take the one-best parse of the tagger and filter-out slots outside the domain schema.?
Const: Train a single model with all domaindata and then perform constrained decoding us-ing a domain specific schema.4.3 ResultsFor the first scenario, we assume that test domainsemantic schema is a subset of training domainschema.
The results of this scenario are shown inTable 2.
We consider In-domain as a plausible up-per bound on the performance, yielding 94.16% ofF1 on average.
First, Binary has the lowest perfor-mance of 75.85%.
We believe that when we traina binary classifier for each slot type, the other slotsthat provide valuable contextual information for theslot sequence are ignored.
This leads to degradationin tagging accuracy.
Post improves F1 scores acrossdomains, resulting into 86.50% F1 on average.
Notethat this technique does not handle ambiguities anddata distribution mismatch due to combining mul-tiple domain specific data with different data sizes.Finally, Const lead to consistent gains across all do-mains, achieving 93.36%, which almost matches theIn-domain performance.
The reason why Const per-forms better than Binary is that Const constrains thebest path search to the target domain schema.
It doesnot consider the schema elements that are outsidethe target domain schema.
By doing so, it addressesthe training data distribution issue as well as overlapon various schema elements.For the second scenario, we consider a new setof test domains not covered in the training set, asshown in Table 3.
The amount of training data forthe test domains are limited (< 5K).
These domainslack training data for the location and app nameslots.
When we use universal slot tagger with con-strained decoding Const yields 94.30%.
On average,Const increases F1-score by 1.41 percentage points,2073#slots #sharedslots #train #test DescriptionALARM 8 6 160K 16K Set alrmsCALENDAR 21 17 100K 10K Set meeting in calendarCOMM.
21 14 700K 70K Make a call&send msgMYSTUFF 20 16 24K 2.5K find&open a documentONDEVICE 10 8 227K 24k Set up a phonePLACES 31 22 478K 45K Find location & infoREMIND 17 13 153K 14K Remind to-do listWEATHER 9 5 281K 26K Ask weatherTRANSIT 16 16 0 2k Ask bus schedule & infoMEDIACONT.
15 15 0 10k Set up a music playerENTERTAIN.
18 12 130k 13k Find&play movie&musicORDERFOOD 15 15 2.5k 2k Order foodRESERVATIONS 21 19 3k 2k Reserve restaurantTAXI 17 17 0 2k Book a cabEVENTS 7 7 2k 1k Book an event ticketSHOWTIMES 15 15 2k 1k Book a movie ticketTable 1: The overview of data we used and descriptions.Domain In-domain Binary Post ConstALARM 96.24 76.49 91.86 95.33CALENDAR 91.79 75.62 80.58 90.19COMM.
95.06 84.17 88.19 94.76ENTER.
96.05 85.39 90.42 95.84MYSTUFF 88.34 51.3 80.6 87.51ONDEVICE 97.65 70.16 77.8 96.43PLACES 92.39 75.27 87.63 91.36REMIND 91.53 72.67 88.98 91.1WEATHER 98.37 91.56 92.45 97.73Average 94.16 75.85 86.50 93.36Table 2: Performance for universal models.Domain In-domain ConstORDERFOOD 93.62 95.63RESERVATIONS 93.03 94.58EVENTS 92.82 94.28SHOWTIMES 92.07 92.69Average 92.89 94.30Table 3: Performance for prototype domains.TAXI TRANSIT MEDIAC.
AVG.Const 90.86 99.5 93.08 94.48Table 4: Results across new domains.corresponding a 20% decrease in relative error.
Webelieve that universal slot tagger learns to tag theseslots from data available in PLACES and ENTER-TAINMENT domains.For the last scenario shown in Table 4, we assumethat we do not have training data for the test do-mains.
The Const performs reasonably well, yield-ing 94.48% on average.
Interestingly, for the TRAN-SIT domain, we can get alost perfect tagging per-formance of 99.5%.
Note that all tags in TRANSITand TAXI domains are fully covered by our universalmodels, but the MEDIACONTROL domain is par-tially covered.4.4 DiscussionBy using the proposed technique, we maximizethe reuse of existing data labeled for different do-mains and applications.
The proposed techniqueallows mixing and matching of slots across differ-ent domains to create new domains.
For exam-ple, we can tag the slots in the SHOWTIMES do-main, which involves finding a movie to watch byusing movie titles, actor names from the ENTER-TAINMENT domain, and the location of the the-ater by using location, place name slots from thePLACES domain.
If the new domain needs somenew slots that are not covered by the universal tag-ger, then some examples queries could be annotatedand added to the universal slot tagger training data toretrain the models.
Instead of maintaining a separate2074slot tagger for each domain, one needs to maintain asingle slot tagger.
The new slots added can be usedby future domains and applications.5 ConclusionsWe proposed a solution for scaling domains and ex-periences potentially to a large number of use casesby reusing existing data labeled for different do-mains and applications.
The universal slot taggingcoupled with constrained decoding achieves almostas good a performance as those slot taggers built ina domain specific fashion.
This approach enablescreation of virtual domains through any combina-tion of slot types covered in the universal slot taggerschema, reducing the need to collect and annotatethe same slot types multiple times for different do-mains.
One of the future directions of research is toextend the same idea to the intent modeling, wherewe can re-use intent data built for different applica-tions and domains for a new domain.
Also, we planto extend the constrained decoding idea to slot tag-ging with neural networks (Kim et al, 2016), whichachieved gains over CRFs.ReferencesPeter F Brown, Peter V Desouza, Robert L Mercer, Vin-cent J Della Pietra, and Jenifer C Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional linguistics, 18(4):467?479.Asli Celikyilmaz, Dilek Hakkani-Tur, Panupong Pasupat,and Ruhi Sarikaya.
2015.
Enriching word embed-dings using knowledge graph for semantic tagging inconversational dialog systems.
AAAI - Associationfor the Advancement of Artificial Intelligence, Jan-uary.Anoop Deoras and Ruhi Sarikaya.
2013.
Deep beliefnetwork markov model sequence classification spokenlanguage understanding.
In Interspeech.Ali El-Kahky, Xiaohu Liu, Ruhi Sarikaya, Gokhan Tur,Dilek Hakkani-Tur, and Larry Heck.
2014.
Extendingdomain coverage of language understanding systemsvia intent transfer between domains using knowledgegraphs and search query click logs.
In 2014 IEEE In-ternational Conference on Acoustics, Speech and Sig-nal Processing (ICASSP), pages 4067?4071.
IEEE.Narendra Gupta, Gokhan Tur, Dilek Hakkani-Tur, Srini-vas Bangalore, Giuseppe Riccardi, and Mazin Gilbert.2006.
The at&t spoken language understanding sys-tem.
IEEE Transactions on Audio, Speech, and Lan-guage Processing, 14(1):213?222.Minwoo Jeong and Gary Geunbae Lee.
2009.
Multi-domain spoken language understanding with transferlearning.
Speech Communication, 51(5):412?424.Young-Bum Kim, Minwoo Jeong, Karl Stratos, and RuhiSarikaya.
2015a.
Weakly supervised slot tagging withpartially labeled sequences from web search click logs.In Proceedings of the Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies (NAACL-HLT), pages 84?92.
Association for ComputationalLinguistics.Young-Bum Kim, Karl Stratos, Xiaohu Liu, and RuhiSarikaya.
2015b.
Compact lexicon selection withspectral methods.
In Proceedings of Association forComputational Linguistics (ACL), pages 806?811.
As-sociation for Computational Linguistics.Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya.2015c.
Pre-training of hidden-unit crfs.
In Proceed-ings of the Association for Computational Linguistics(ACL), pages 192?198.
Association for ComputationalLinguistics.Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and Min-woo Jeong.
2015d.
New transfer learning techniquesfor disparate label sets.
In ACL.
Association for Com-putational Linguistics.Young-Bum Kim, Karl Stratos, Minjoon Seo, and RuhiSarikaya.
2016.
Domainless adaptation by con-strained decoding on a schema lattice.
In Proceedingsof the International Conference on Computational Lin-guistics (Coling).
Association for Computational Lin-guistics.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In ICML, pages 282?289.Xiao Li, Ye-Yi Wang, and Alex Acero.
2009.
Extractingstructured information from user queries with semi-supervised conditional random fields.
In Proceedingsof the 32nd international ACM SIGIR conference onResearch and development in information retrieval.Dong C Liu and Jorge Nocedal.
1989.
On the lim-ited memory bfgs method for large scale optimization.Mathematical programming, 45(1-3):503?528.Ruhi Sarikaya, Paul Crook, Alex Marin, Minwoo Jeong,Jean-Philippe Robichaud, Asli Celikyilmaz, Young-Bum Kim, Alexandre Rochette, Omar Zia Khan, Xi-uahu Liu, Daniel Boies, Tasos Anastasakos, ZhallehFeizollahi, Nikhil Ramesh, Hisami Suzuki, RomanHolenstein, Elizabeth Krawczyk, and Vasiliy Radoste.2016.
An overview of end-to-end language under-standing and dialog management for personal digital2075assistants.
In IEEE Workshop on Spoken LanguageTechnology.Ruhi Sarikaya.
2015.
The technology powering personaldigital assistants.
Keynote at Interspeech, Dresden,Germany.Gokhan Tur and Renato De Mori.
2011.
Spoken lan-guage understanding: Systems for extracting semanticinformation from speech.
John Wiley & Sons.Gokhan Tur.
2006.
Multitask learning for spokenlanguage understanding.
In In Proceedings of theICASSP, Toulouse, France.Puyang Xu and Ruhi Sarikaya.
2014.
Targeted featuredropout for robust slot filling in natural language un-derstanding.
In ISCA - International Speech Commu-nication Association, September.2076
