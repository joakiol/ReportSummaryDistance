Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1?9,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsAn Assessment of the Accuracy of Automatic Evaluation in SummarizationKarolina OwczarzakInformation Access DivisionNational Institute of Standards and Technologykarolina.owczarzak@gmail.comJohn M. ConroyIDA Center for Computing Sciencesconroy@super.orgHoa Trang DangInformation Access DivisionNational Institute of Standards and Technologyhoa.dang@nist.govAni NenkovaUniversity of Pennsylvanianenkova@seas.upenn.eduAbstractAutomatic evaluation has greatly facilitatedsystem development in summarization.
At thesame time, the use of automatic evaluationhas been viewed with mistrust by many, as itsaccuracy and correct application are not wellunderstood.
In this paper we provide an as-sessment of the automatic evaluations used formulti-document summarization of news.
Weoutline our recommendations about how anyevaluation, manual or automatic, should beused to find statistically significant differencesbetween summarization systems.
We identifythe reference automatic evaluation metrics?ROUGE 1 and 2?that appear to best emu-late human pyramid and responsiveness scoreson four years of NIST evaluations.
We thendemonstrate the accuracy of these metrics inreproducing human judgements about the rel-ative content quality of pairs of systems andpresent an empirical assessment of the rela-tionship between statistically significant dif-ferences between systems according to man-ual evaluations, and the difference accordingto automatic evaluations.
Finally, we present acase study of how new metrics should be com-pared to the reference evaluation, as we searchfor even more accurate automatic measures.1 IntroductionAutomatic evaluation of content selection in sum-marization, particularly the ROUGE evaluationtoolkit (Lin and Hovy, 2003), has been enthusias-tically adopted by researchers since its introductionin 2003.
It is now standardly used to report results inpublications; however we have a poor understandingof the accuracy of automatic evaluation.
How oftendo we publish papers where we report an improve-ment according to automatic evaluation, but never-theless, a standard manual evaluation would have ledus to different conclusions?
In our work we directlyaddress this question, and hope that our encouragingfindings contribute to a better understanding of thestrengths and shortcomings of automatic evaluation.The aim of this paper is to give a better assessmentof the automatic evaluation metrics for content se-lection standardly used in summarization research.We perform our analyses on data from the 2008-2011 Text Analysis Conference (TAC)1 organizedby the National Institute of Standards and Technol-ogy (NIST).
We choose these datasets because inearly evaluation initiatives, the protocol for manualevaluation changed from year to year in search ofstable manual evaluation approaches (Over et al,2007).
Since 2008, however, the same evaluationprotocol has been applied by NIST assessors and weconsider it to be the model that automatic metricsneed to emulate.We start our discussion by briefly presenting themanual procedure for comparing systems (Section2) and how these scores should be best used to iden-tify significant differences between systems over agiven test set (Section 3).
Then, we embark on ourdiscussion of the accuracy of automatic evaluationand its ability to reproduce manual scoring.To begin our analysis, we assess the accuracy ofcommon variants of ROUGE on the TAC 2008-2011datasets (Section 4.1).
There are two aspects of eval-uation that we pay special attention to:Significant difference Ideally, all system compar-isons should be performed using a test for sta-1http://www.nist.gov/tac/1tistical significance.
As both manual metricsand automatic metrics are noisy, a statisticalhypothesis test is needed to estimate the prob-ability that the differences observed are whatwould be expected if the systems are compa-rable in their performance.
When this proba-bility is small (by convention 0.05 or less) wereject the null hypothesis that the systems?
per-formance is comparable.It is important to know if scoring a system viaan automatic metric will lead to conclusionsabout the relative merits of two systems differ-ent from what one would have concluded on thebasis of manual evaluation.
We report very en-couraging results, showing that automatic met-rics rarely contradict manual metrics, and somemetrics never lead to contradictions.
For com-pleteness, given that most papers do not reportsignificance, we also compare the agreementbetween manual and automatic metrics withouttaking significance into account.Type of comparison Established manual evalua-tions have two highly desirable properties: (1)they can tell apart good automatic systems frombad automatic systems and (2) they can differ-entiate automatic summaries from those pro-duced by humans with high accuracy.
Bothproperties are essential.
Obviously, choosingthe better system in development cycles is keyin eventually improving overall performance.Being able to distinguish automatic from man-ual summaries is a general sanity test 2 that anyevaluation adopted for wide use is expected topass?it is useless to report system improve-ments when it appears that automatic methodsare as good as human performance3.
As we willsee, there is no single ROUGE variant that hasboth of these desirable properties.Finally, in Section 5, we discuss ways to compareother automatic evaluation protocols with the refer-2For now, automatic systems do not have the performanceof humans, thus, the ability to distinguish between human andautomatically generated summaries is an exemplar of the widerproblem of distinguishing high quality summaries from others.3Such anomalous findings, when using automatic evalua-tion, have been reported for some summarization genres suchas summarization of meetings (Galley, 2006).ence ROUGE metrics we have established.
We de-fine standard tests for significance that would iden-tify evaluations that are significantly more accuratethan the current reference measures, thus warrant-ing wider adoption for future system developmentand reporting of results.
As a case study we applythese to the TAC AESOP (Automatically EvaluatingSummaries of Peers) task which called for the devel-opment of novel evaluation techniques that are moreaccurate than ROUGE evaluations.2 Manual evaluationBefore automatic evaluation methods are developed,it is necessary to establish a desirable manual eval-uation which the automatic methods will need to re-produce.
The type of summarization task must alsobe precisely specified?single- or multi-documentsummarization, summarization of news, meetings,academic articles, etc.
Saying that an automaticevaluation correlates highly with human judgementin general, is disturbingly incomplete, as the sameautomatic metric can predict some manual evalu-ation scores for some summarization tasks well,while giving poor correlation with other manualscores for certain tasks (Lin, 2004; Liu and Liu,2010).In our work, we compare automatic metrics withthe manual methods used at TAC: Pyramid and Re-sponsiveness.
These manual metrics primarily aimto assess if the content of the summary is appro-priately chosen to include only important informa-tion.
They do not deal directly with the linguisticquality of the summary?how grammatical are thesentences or how well the information in the sum-mary is organized.
Subsequently, in the experimentsthat we present in later sections, we do not addressthe assessment of automatic evaluations of linguisticquality (Pitler et al, 2010), but instead analyze theperformance of ROUGE and other related metricsthat aim to score summary content.The Pyramid evaluation (Nenkova et al, 2007) re-lies on multiple human-written gold-standard sum-maries for the input.
Annotators manually identifyshared content across the gold-standards regardlessof the specific phrasing used in each.
The pyra-mid score is based on the ?popularity?
of informa-tion in the gold-standards.
Information that is shared2across several human gold-standards is given higherweight when a summary is evaluated relative to thegold-standard.
Each evaluated summary is assigneda score which indicates what fraction of the mostimportant information for a given summary size isexpressed in the summary, where importance is de-termined by the overlap in content across the humangold-standards.The Responsiveness metric is defined for query-focused summarization, where the user?s informa-tion need is clearly stated in a short paragraph.
Inthis situation, the human assessors are presentedwith the user query and a summary, and are askedto assign a score that reflects to what extent the sum-mary satisfies the user?s information need.
There areno human gold-standards, and the linguistic qualityof the summary is to some extent incorporated in thescore, because information that is presented in a con-fusing manner may not be seen as relevant, while itcould be interpreted by the assessor more easily inthe presence of a human gold-standard.
Given thatall standard automatic evaluation procedures com-pare a summary with a set of human gold-standards,it is reasonable to expect that they will be more accu-rate in reproducing results from Pyramid evaluationthan results from Responsiveness judgements.3 Comparing systemsEvaluation metrics are used to determine the rela-tive quality of a summarization system in compari-son to one or more systems, which is either anotherautomatic summarizer, or a human reference sum-marizer.
Any evaluation procedure assigns a scoreto each summary.
To identify which of the two sys-tems is better, we could simply average the scoresof summaries produced by each system in the testset, and compare these averages.
This approach isstraightforward; however, it gives no indication ofthe statistical significance of the difference betweenthe systems.
In system development, engineers maybe willing to adopt new changes only if they leadto significantly better performance that cannot be at-tributed to chance.Therefore, in order to define more precisely whatit means for a summarization system to be ?bet-ter?
than another for a given evaluation, we employstatistical hypothesis testing comparisons of sum-marization systems on the same set of documents.Given an evaluation of two summarization systemsA and B we have the following:Definition 1.
We say a summarizer A ?signifi-cantly outperforms?
summarizer B for a givenevaluation score if the null hypothesis of the fol-lowing paired test is rejected with 95% confidence.Given two vectors of evaluation scores x and y,sampled from the corresponding random vari-ables X and Y, measuring the quality of sum-marizer A and B, respectively, on the same col-lection of document sets, with the median of xgreater than the median of y,H0 : The median of X ?
Y is 0.Ha : The median of X ?
Y is not 0.We apply this test using human evaluation met-rics, such as pyramid and responsiveness, as well asautomatic metrics.
Thus, when comparing two sum-marization systems we can, for example, say systemA significantly outperforms system B in responsive-ness if the null hypothesis can be rejected.
If the nullhypothesis cannot be rejected, we say system A doesnot significantly perform differently than system B.A complicating factor when the differences be-tween systems are tested for significance, is thatsome inputs are simply much harder to summarizethan others, and there is much variation in scoresthat is not due to properties of the summarizersthat produced the summaries but rather properties ofthe input text that are summarized (Nenkova, 2005;Nenkova and Louis, 2008).Given this variation in the data, the most appropri-ate approach to assess significance in the differencebetween system is to use paired rank tests such asa paired Wilcoxon rank-sum test, which is equiva-lent to the Mann-Whitney U test.
In these tests, thescores of the two systems are compared only for thesame input and ranks are used instead of the actualdifference in scores assigned by the evaluation pro-cedures.
Prior studies have shown that paired testsfor significance are indeed able to discover consid-erably more significant differences between systemsthan non-paired tests, in which the noise of input dif-ficulty obscures the actual difference in system per-3formance (Rankel et al, 2011).
For this paper, weperform all testing using the Wilcoxon sign rank test.4 How do we identify a good metric?If we treat manual evaluation metrics as our goldstandard, then we require that a good automatic met-ric mirrors the distinctions made by such a man-ual metric.
An automatic metric for summarizationevaluation should reliably predict how well a sum-marization system would perform relative to othersummarizers if a human evaluation were performedon the summaries.
An automatic metric would hopeto answer the question:Would summarizer A significantly outper-form summarizer B when evaluated by ahuman?We address this question by evaluating how wellan automatic metric agrees with a human metric inits judgements in the following cases:?
all comparisons between different summariza-tion systems?
all comparisons between systems and humansummarizers.Depending on the application, we may record thecounts of agreements and disagreements or we maynormalize these counts to estimate the probabilitythat an automatic evaluation metric will agree with ahuman evaluation metric.4.1 Which is the best ROUGE variantIn this section, we set out to identify which of themost widely-used versions of ROUGE have highestaccuracy in reproducing human judgements aboutthe relative merits of pairs of systems.
We exam-ine ROUGE-1, ROUGE-2 and ROUGE-SU4.
Forall experiments we use stemming and for each ver-sion we test scores produced both with and withoutremoving stopwords.
This corresponds to six differ-ent versions of ROUGE that we examine in detail.ROUGE outputs several scores including preci-sion, recall, and an F-measure.
However, the mostinformative score appears to be recall as reportedwhen ROUGE was first introduced (Lin and Hovy,2003).
Given that in the data we work with, sum-maries are produced for a specified length in words (and all summaries are truncated to the predefinedlength), recall on the task does not allow for artifi-cially high scores which would result by producinga summary of excessive length.The goal of our analysis is to identify which of theROUGE variants is most accurate in correctly pre-dicting which of two participating systems is the bet-ter one according to the manual pyramid and respon-siveness scores.
We use the data for topic-focusedsummarization from the TAC summarization trackin 2008-20114.Table 1 gives the overview of the 2008-2011 TACSummarization data, including the number of top-ics and participants.
For each topic there were fourreference (model) summaries, written by one of theeight assessors; as a result, there were eight human?summarizers,?
but each produced summaries onlyfor half of the topics.year topics automatic human references/summarizers summarizers topic2008 48 58 8 42009 44 55 8 42010 46 43 8 42011 44 50 8 4Table 1: Data in TAC 2008-2011 Summarization track.We compare each pair of participating systemsbased on the manual evaluation score.
For each pair,we are interested in identifying the system that isbetter.
We consider both the case when an appropri-ate test for statistical significance has been applied topick out the better system as well as the case wheresimply the average scores of systems over the test setare compared.
The latter use of evaluations is mostcommon in research papers on summarization; how-ever, in summarization system development, testingfor significance is important because a difference insummarizer scores that is statistically significant ismuch more likely to reflect a true difference in qual-ity between the two systems.Therefore, we look at agreement betweenROUGE and manual metrics in two ways:?
agreement about significant differences be-tween summarizers, according to a paired4In all these years systems also competed on producing up-date summaries.
We do not report results on this task for thesake of simplifying the discussion.4Auto only Human-AutomaticPyr Resp Pyr Respdiff no diff contr diff no diff contr diff no diff contr diff no diff contrr1m 91 59 0.85 87 51 1.34 91 75 0.06 91 100 0.45r1ms 90 59 0.83 84 50 3.01 91 75 0.06 90 100 0.45r2m 91 68 0.19 88 60 0.47 75 75 0.62 75 100 1.02r2ms 88 72 0 84 62 0.65 73 75 1.56 72 100 1.95r4m 91 64 0.62 87 56 0.91 82 75 0.43 82 100 0.83r4ms 90 64 0.04 85 55 1.15 83 75 0.81 83 100 1.20Table 2: Average percentage agreement between ROUGE and manual metrics about significant differences on TAC2008-2011 data.
r1 = ROUGE-1, r2 = ROUGE-2, r4 = ROUGE-SU4, m = stemmed, s = stopwords removed; diff =agreement on significant differences, no diff = agreement on lack of significant differences, contr = contradictions.Auto only Human-AutomaticPyr Resp Pyr Respmetric sig all sig all sig all sig allr1m 77 87 70 82 90 99 90 99r1ms 77 88 69 80 90 98 90 98r2m 81 89 75 83 75 94 75 94r2ms 81 89 74 81 72 93 72 93r4m 80 88 73 82 82 96 82 96r4ms 79 89 71 81 83 96 83 96Table 3: Average agreement between ROUGE and manual metrics on TAC 2008-2011 data.
r1 = ROUGE-1, r2 =ROUGE-2, r4 = ROUGE-SU4, m = stemmed, s = stopwords removed; sig = agreement on significant differences, all= agreement on all differences.Wilcoxon test.
No adjustments for multiplecomparisons are made.?
agreement about any differences between sum-marizers (whether significant on not).Agreements occur when the two evaluation met-rics make the same distinction between System Aand System B: A is significantly better than B, A issignificantly worse than B, or A and B are not sig-nificantly different from each other.
Contradictionsoccur when both metrics find a significant differencebetween A and B, but in opposite directions; this isa much more serious case than a mere lack of agree-ment (i.e., when one metric says A and B are notsignificantly different, and the other metric finds asignificant difference).Table 2 shows the average percentage agreementbetween ROUGE and Pyramid/Responsivenesswhen it comes to identifying significant differencesor lack thereof.
Column diff shows the recallof significant differences between pairs of systems(i.e., how many significant differences determinedby Pyramid/Responsiveness are found by ROUGE);column no diff gives the recall of the cases wherethere are no significant differences between two sys-tems according to Pyramid/Responsiveness.There are a few instances of contradictions, aswell, but their numbers are fairly small.
?Auto only?refers to comparisons between automatic summariz-ers only; ?Human-Automatic?
refers to cases whena human summarizer is compared to an automaticsummarizer.
There are fewer human summarizers,so there are fewer ?Human-Automatic?
comparisonsthan ?Auto only?
ones.There are a few exceptional cases where the hu-man summarizer is not significantly better than theautomatic summarizers, even according to the man-ual evaluation, which accounts for the uniform val-ues in the ?no difference?
column (this is proba-bly because the comparison is performed for muchfewer test inputs).Table 3 combines the number of agreements inthe ?difference?
and ?no difference?
columns fromTable 2 into the sig column, which shows accu-racy: in checking system pairs for significant differ-ences, in how many cases does ROUGE make thesame decision as the manual metric (there is/isn?ta significant difference between A and B).
Ta-ble 3 also gives the number of agreements aboutany differences between systems, not only thosethat reached statistical significance; in other words,agreements on system pairwise rankings.
In both5tables we see that removing stopwords often de-creases performance of ROUGE, although not al-ways.
Also, there is no clear winner in the ROUGEcomparison: while ROUGE-2 with stemming is thebest at distinguishing among automatic summariz-ers, ROUGE-1 is the most accurate when it comesto human?automatic comparisons.
To reflect this,we adopt both ROUGE-1 and ROUGE-2 (with stem-ming, without removing stopwords) as our referenceautomatic metrics for further comparisons.Reporting pairwise accuracy of automatic evalua-tion measures has several advantages over reportingcorrelations between manual and automatic metrics.In correlation analysis, we cannot obtain any senseof how accurate the measure is in identifying statis-tically significant differences.
In addition, pairwiseaccuracy is more interpretable than correlations andgives some provisional indication about how likelyit is that we are drawing a wrong conclusion whenrelying on automatic metric to report results.Table 3 tells us that when statistical significanceis not taken into account, in 89% of cases ROUGE-2 scores will lead to the same conclusion about therelative merits of systems as the expensive Pyramidevaluation.
In 83% of cases the conclusions willagree with the Responsiveness evaluation.
The accu-racy of identifying significant differences is worse,dropping by about 10% for both Pyramid and Re-sponsiveness.Finally, we would like to get empirical estimatesof the relationship between the size of the differencein ROUGE-2 scores between two systems and theagreement between manual and ROUGE-2 evalua-tion.
The goal is to check if it is the case that ifone system scores higher than another by x ROUGEpoints, then it would be safe to assume that a manualevaluation would have led to the same conclusion.Figure 1 shows a histogram of differences inROUGE-2 scores.
The pairs for which this differ-ence was significant are given in red and for thosewhere the difference is not significant are given inblue.
The histogram clearly shows that in general,the size of improvement cannot be used to replace atest for significance.
Even for small differences inROUGE score (up to 0.007) there are about 15 pairsout of 200 for which the difference is in fact signif-icant according to Pyramid or Responsiveness.
Asthe difference in ROUGE-2 scores between the twosystems increases, there are more significant differ-ences.
For differences greater than 0.05, all differ-ences are significant.Figure 2 shows the histograms of differences inROUGE-2 scores, split into cases where the pairwiseranking of systems according to ROUGE agreeswith manual evaluation (blue) and disagrees (red).For score differences smaller than 0.013, about halfof the times ROUGE-2 would be wrong in identify-ing which system in the pair is the better one accord-ing to manual evaluations.
For larger differences thenumber of disagreements drops sharply.
For thisdataset, a difference in ROUGE-2 scores of morethan 0.04 always corresponds to an improvement inthe same direction according to the manual metrics.5 Looking for better metricsIn the preceding sections, we established thatROUGE-2 is the best ROUGE variant for compar-ing two automatic systems, and ROUGE-1 is best indistinguishing between humans and machines.
Ob-viously, it is of great interest to develop even bet-ter automatic evaluations.
In this section, we out-line a simple procedure for deciding if a new au-tomatic evaluation is significantly better than a ref-erence measure.
For this purpose, we consider theautomatic metrics from the TAC 2011 AESOP task,which called for the development of better automaticmetrics for summarization evaluation NIST ( 2011).For each automatic evaluation metric, we estimatethe probability that it agrees with Pyramid or Re-sponsiveness.
Figure 3 gives the estimated proba-bility of agreement with Pyramid and Overall Re-sponsiveness for all AESOP 2011 metrics with anagreement of 0.6 or more.
The metrics are plot-ted with error bars giving the 95% confidence in-tervals for the probability of agreement with themanual evaluations.
The red-dashed line is theperformance of the reference automatic evaluation,which is ROUGE-2 for machine only and ROUGE-1 for comparing machines and human summariz-ers.
Metrics whose 95% confidence interval is be-low this line are significantly worse (as measuredby the z-test approximation of a binomial test) thanthe baseline.
Conversely, those whose 95% con-fidence interval is above the red line are signifi-cantly better than the baseline.
Thus, just ROUGE-6Figure 1: Histogram of the differences in ROUGE-2 score versus significant differences as determined by Pyramid(left) or Responsiveness (right).Figure 2: Histogram of the differences in ROUGE-2 score versus differences as determined by Pyramid (left) orResponsiveness (right).BE (the MINIPAR variant of ROUGE-BE), one ofNIST?s baselines for AESOP, significantly outper-formed ROUGE-2 for predicting pyramid compar-isons; and 4 metrics: ROUGE-BE, DemokritosGR2,catholicasc1, and CLASSY1, all significantly out-perform ROUGE-2 for predictiong responsivenesscomparisons.
Descriptions of these metrics as wellas the other proposed metrics can be found in theTAC 2011 proceedings (NIST, 2011).Similarly, Figure 4 gives the estimated probabil-ity when the comparison is made between humanand machine summarizers.
Here, 10 metrics are sig-nificantly better than ROUGE-1 in predicting com-parisons between automatic summarization systemsand human summarizers in both pyramid and re-sponsiveness.
The ROUGE-SU4 and ROUGE-BEbaselines are not shown here but their performancewas approximately 57% and 46% respectively.If we limit the comparisons to only those wherea significant difference was measured by Pyramidand also Overall Responsiveness, we get the plotsgiven in Figure 5 for comparing automatic summa-rization systems.
(The corresponding plot for com-parisons between machines and humans is omittedas all differences are significant.)
The results showthat there are 6 metrics that are significantly betterthan ROUGE-2 for correctly predicting when a sig-nificant difference in pyramid scores occurs, and 3metrics that are significantly better than ROUGE-2for correctly predicting when a significant differencein responsiveness occurs.6 DiscussionIn this paper we provided a thorough assessmentof automatic evaluation in summarization of news.We specifically aimed to identify the best variantof ROUGE on several years of TAC data and dis-covered that ROUGE-2 recall with stemming andstopwords not removed, provides the best agreementwith manual evaluations.
The results shed positivelight on the automatic evaluation, as we find thatROUGE-2 agrees with manual evaluation in almost90% of the case when statistical significance is notcomputed, and about 80% when it is.
However,these numbers are computed in a situation wheremany very different systems are compared?some7Figure 3: Pyramid and Responsiveness Agreement of AESOP 2011 Metrics for automatic summarizers.Figure 4: Pyramid and Responsiveness Significant Difference Agreement of AESOP 2011 Metrics for all summarizers.8Figure 5: Pyramid and Responsiveness Significant Difference Agreement of AESOP 2011 Metrics for automaticsummarizers.very good, others bad.
We examine the size of dif-ference in ROUGE score and identify that for differ-ences less than 0.013 a large fraction of the conclu-sions drawn by automatic evaluation will contradictthe conclusion drawn by a manual evaluation.
Fu-ture studies should be more mindful of these find-ings when reporting results.Finally, we compare several alternative automaticevaluation measures with the reference ROUGEvariants.
We discover that many new proposals arebetter than ROUGE in distinguishing human sum-maries from machine summaries, but most are thesame or worse in evaluating systems.
The Basic El-ements evaluation (ROUGE-BE) appears to be thestrongest contender for an automatic evaluation toaugment or replace the current reference.ReferencesPaul Over and Hoa Dang and Donna Harman.
2007.DUC in context.
Inf.
Process.
Manage.
43(6), 1506?1520.Chin-Yew Lin and Eduard H. Hovy.
2003.
Auto-matic Evaluation of Summaries Using N-gram Co-occurrence Statistics.
Proceeding of HLT-NAACL.Michel Galley.
2006.
A Skip-Chain Conditional Ran-dom Field for Ranking Meeting Utterances by Impor-tance.
Proceeding of EMNLP, 364?372.Feifan Liu and Yang Liu.
2010.
Exploring correlationbetween ROUGE and human evaluation on meetingsummaries.
Trans.
Audio, Speech and Lang.
Proc.,187?196.C.Y.
Lin.
2004.
Looking for a Few Good Metrics: Au-tomatic Summarization Evaluation - How Many Sam-ples are Enough?
Proceedings of the NTCIR Work-shop 4.Ani Nenkova and Rebecca J. Passonneau and KathleenMcKeown.
2007.
The Pyramid Method: Incorporat-ing human content selection variation in summariza-tion evaluation.
TSLP 4(2).Emily Pitler and Annie Louis and Ani Nenkova.
2010.Automatic Evaluation of Linguistic Quality in Multi-Document Summarization.
Proceedings of ACL, 544?554.Ani Nenkova.
2005.
Automatic Text Summarization ofNewswire: Lessons Learned from the Document Un-derstanding Conference.
AAAI, 1436?1441.Ani Nenkova and Annie Louis.
2008.
Can You Summa-rize This?
Identifying Correlates of Input Difficultyfor Multi-Document Summarization.
ACL, 825?833.Peter Rankel and John M. Conroy and Eric Slud and Di-anne P. O?Leary.
2011.
Ranking Human and MachineSummarization Systems.
Proceedings of EMNLP,467?473.National Institute of Standards and Technology.2011.
Text Analysis Workshop Proceedingshttp://www.nist.gov/tac/publications/index.html.9
