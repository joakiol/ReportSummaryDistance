Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 607?615,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsBilingual Sentiment Consistency for Statistical Machine TranslationBoxing Chen and Xiaodan ZhuNational Research Council Canada1200 Montreal Road, Ottawa, Canada, K1A 0R6{Boxing.Chen, Xiaodan.Zhu}@nrc-cnrc.gc.caAbstractIn this paper, we explore bilingual sentimentknowledge for statistical machine translation(SMT).
We propose to explicitly model theconsistency of sentiment between the sourceand target side with a lexicon-based approach.The experiments show that the proposed mod-el significantly improves Chinese-to-EnglishNIST translation over a competitive baseline.1 IntroductionThe expression of sentiment is an interesting andintegral part of human languages.
In written textsentiment is conveyed by senses and in speech alsovia prosody.
Sentiment is associated with bothevaluative (positive or negative) and potency (de-gree of sentiment) ?
involving two of the threemajor semantic differential categories identified byOsgood et al.
(1957).Automatically analyzing the sentiment of mono-lingual text has attracted a large bulk of research,which includes, but is not limited to, the early ex-ploration of (Turney, 2002; Pang et al., 2002; Hat-zivassiloglou & McKeown, 1997).
Since then,research has involved a variety of approaches andbeen conducted on various type of data, e.g., prod-uct reviews, news, blogs, and the more recent so-cial media text.As sentiment has been an important concern inmonolingual settings, better translation of suchinformation between languages could be of interestto help better cross language barriers, particularlyfor sentiment-abundant data.
Even when we ran-domly sampled a subset of sentence pairs from theNIST Open MT1 training data, we found that about48.2% pairs contain at least one sentiment word onboth sides, and 22.4% pairs contain at least one1 http://www.nist.gov/speech/tests/mtintensifier word on both sides, which suggests anon-trivial percent of sentences may potentiallyinvolve sentiment in some degree2.# snt.pairs% snt.
withsentiment words% snt.
withintensifiers103,369 48.2% 22.4%Table 1.
Percentages of sentence pairs that contain sen-timent words on both sides or intensifiers3 on both sides.One expects that sentiment has been implicitlycaptured in SMT through the statistics learnedfrom parallel corpus, e.g., the phrase tables in aphrase-based system.
In this paper, we are interest-ed in explicitly modeling sentiment knowledge fortranslation.
We propose a lexicon-based approachthat examines the consistency of bilingual subjec-tivity, sentiment polarity, intensity, and negation.The experiments show that the proposed approachimproves the NIST Chinese-to-English translationover a strong baseline.In general, we hope this line of work will helpachieve better MT quality, especially for data withmore abundant sentiment, such as social media text.2 Related WorkSentiment analysis and lexicon-based approach-es Research on monolingual sentiment analysis canbe found under different names such as opinion,stance, appraisal, and semantic orientation, amongothers.
The overall goal is to label a span of texteither as positive, negative, or neutral ?
some-times the strength of sentiment is a concern too.2 The numbers give a rough idea of sentiment coverage; itwould be more ideal if the estimation could be conducted onsenses instead of words, which, however, requires reliablesense labeling and is not available at this stage.
Also, accord-ing to our human evaluation on a smaller dataset, two thirds ofsuch potentially sentimental sentences do convey sentiment.3 The sentiment and intensifier lexicons used to acquire thesenumbers are discussed in Section 3.2.607The granularities of text have spanned from wordsand phrases to passages and documents.Sentiment analysis has been approached mainlyas an unsupervised or supervised problem, alt-hough the middle ground, semi-supervised ap-proaches, exists.
In this paper, we take a lexicon-based, unsupervised approach to considering sen-timent consistency for translation, although thetranslation system itself is supervised.
The ad-vantages of such an approach have been discussedin (Taboada et al., 2011).
Briefly, it is good at cap-turing the basic sentiment expressions common todifferent domains, and certainly it requires no bi-lingual sentiment-annotated data for our study.
Itsuits our purpose here of exploring the basic roleof sentiment for translation.
Also, such a methodhas been reported to achieve a good cross-domainperformance (Taboada et al., 2011) comparablewith that of other state-of-the-art models.Translation for sentiment analysis A very inter-esting line of research has leveraged labeled data ina resource-rich language (e.g., English) to helpsentiment analysis in a resource-poorer language.This includes the idea of constructing sentimentlexicons automatically by using a translation dic-tionary (Mihalcea et al., 2007), as well as the ideaof utilizing parallel corpora or automatically trans-lated documents to incorporate sentiment-labeleddata from different languages (Wan, 2009; Mihal-cea et al., 2007).Our concern here is different ?
instead of uti-lizing translation for sentiment analysis; we areinterested in the SMT quality itself, by modelingbilingual sentiment in translation.
As mentionedabove, while we expect that statistics learned fromparallel corpora have implicitly captured sentimentin some degree, we are curious if better modelingis possible.Considering semantic similarity in translationThe literature has included interesting ideas of in-corporating different types of semantic knowledgefor SMT.
A main stream of recent efforts havebeen leveraging semantic roles (Wu and Fung,2009; Liu and Gildea, 2010; Li et al., 2013) to im-prove translation, e.g., through improving reorder-ing.
Also, Chen et al.
(2010) have leveraged sensesimilarity between source and target side as addi-tional features.
In this work, we view a differentdimension, i.e., semantic orientation, and show thatincorporating such knowledge improves the trans-lation performance.
We hope this work would addmore evidences to the existing literature of lever-aging semantics for SMT, and shed some light onfurther exploration of semantic consistency, e.g.,examining other semantic differential factors.3 Problem & Approach3.1 Consistency of sentimentIdeally, sentiment should be properly preserved inhigh-quality translation.
An interesting study con-ducted by Mihalcea et al.
(2007) suggests that inmost cases the sentence-level subjectivity is pre-served by human translators.
In their experiments,one English and two Romanian native speakerswere asked to independently annotate the senti-ment of English-Romanian sentence pairs from theSemCor corpus (Miller et al., 1993), a balancedcorpus covering a number of topics in sports, poli-tics, fashion, education, and others.
These humansubjects were restricted to only access and annotatethe sentiment of their native-language side of sen-tence pairs.
The sentiment consistency was ob-served by examining the annotation on both sides.Automatic translation should conform to such aconsistency too, which could be of interest formany applications, particularly for sentiment-abundant data.
On the other hand, if consistency isnot preserved for some reason, e.g., alignmentnoise, enforcing consistency may help improve thetranslation performance.
In this paper, we explorebilingual sentiment consistency for translation.3.2 Lexicon-based bilingual sentiment analysisTo capture bilingual sentiment consistency, we usea lexicon-based approach to sentiment analysis.Based on this, we design four groups of features torepresent the consistency.The basic idea of the lexicon-based approach isfirst identifying the sentiment words, intensifiers,and negation words with lexicons, and then calcu-lating the sentiment value using manually designedformulas.
To this end, we adapted the approachesof (Taboada et al., 2011) and (Zhang et al., 2012)so as to use the same formulas to analyze the sen-timent on both the source and the target side.The English and Chinese sentiment lexicons weused are from (Wilson et al.
2005) and (Xu and Lin,2007), respectively.
We further use 75 English in-608tensifiers listed in (Benzinger, 1971; page 171) and81 Chinese intensifiers from (Zhang et al., 2012).We use 17 English and 13 Chinese negation words.Similar to (Taboada et al., 2011) and (Zhang etal., 2012), we assigned a numerical score to eachsentiment word, intensifier, and negation word.More specifically, one of the five values: -0.8, -0.4,0, 0.4, and 0.8, was assigned to each sentimentword in both the source and target sentiment lexi-cons, according to the strength information anno-tated in these lexicons.
The scores indicate thestrength of sentiment.
Table 2 lists some examples.Similarly, one of the 4 values, i.e., -0.5, 0.5, 0.7and 0.9, was manually assigned to each intensifierword, and a -0.8 or -0.6 to the negation words.
Allthese scores will be used below to modify and shiftthe sentiment value of a sentiment unit.Sentiment words Intensifiers Negation wordsimpressive (0.8)good (0.4)actually (0.0)worn (-0.4)depressing (-0.8)extremely (0.9)very (0.7)pretty (0.5)slightly (-0.5)not (-0.8)rarely (-0.6)Table 2: Examples of sentiment words and their senti-ment strength; intensifiers and their modify rate; nega-tion words and their negation degree.Each sentiment word and its modifiers (negationwords and intensifiers) form a sentiment unit.
Wefirst found all sentiment units by identifying senti-ment words with the sentiment lexicons and theirmodifiers with the corresponding lexicon in a 7-word window.
Then, for different patterns of sen-timent unit, we calculated the sentiment valuesusing the formulas listed in Table 3, where theseformulas are adapted from (Taboada et al., 2011)and (Zhang et al., 2012) so as to be applied to bothlanguages.Sen.unitSen.
valueformulaExampleSen.valuews S(ws) good 0.40wnws D(wn)S(ws) not good -0.32wiws (1+R(wi))S(ws) very good 0.68wnwiws (1+ D(wn)R(wi))S(ws) not very good 0.176wiwnws D(wn)(1+R(wi))S(ws) very not good4 -0.544Table 3: Heuristics used to compute the lexicon-basedsentiment values for different types of sentiment units.4 The expression ?very not good?
is ungrammatical in English.However, in Chinese, it is possible to have this kind of expres-sion, such as ?????
?, whose transliteration is ?very notbeautiful?, meaning ?very ugly?.For notation, S(ws) stands for the strength ofsentiment word ws, R(wi) is degree of the intensifi-er word wi, and D(wn) is the negation degree of thenegation word wn.Above, we have calculated the lexicon basedsentiment value (LSV) for any given unit ui, andwe call it lsv(ui) below.
If a sentence or phrase scontains multiple sentiment units, its lsv-score is amerge of the individual lsv-scores of all its senti-ment units:)))((()( 1 iN ulsvbasismergslsv ?
(1)where the function basis(.)
is a normalization func-tion that performs on each lsv(ui).
For example, thebasis(.)
function could be a standard sign functionthat just examines if a sentiment unit is positive ornegative, or simply an identity function (using thelsv-scores directly).
The merg(.)
is a function thatmerge the lsv-scores of individual sentiment units,which may take several different forms below inour feature design.
For example, it can be a meanfunction to take the average of the sentiment units?lsv-scores, or a logic OR function to examine if asentence or phrase contains positive or negativeunits (depending on the basis function).
It can alsobe a linear function that gives different weights todifferent units according to further knowledge, e.g.,syntactic information.
In this paper, we only lever-age the basic, surface-level analysis5.In brief, our model here can be thought of as aunification and simplification of both (Taboada etal., 2011) and (Zhang et al., 2012), for our bilin-gual task.
We suspect that better sentiment model-ing may further improve the general translationperformance or the quality of sentiment in transla-tion.
We will discuss some directions we think in-teresting in the future work section.3.3 Incorporating sentiment consistency intophrase-based SMTIn this paper, we focus on exploring sentimentconsistency for phrase-based SMT.
However, theapproach might be used in other translationframework.
For example, consistency may be con-sidered in the variables used in hierarchical transla-tion rules (Chiang, 2005).5 Note that when sentiment-annotated training data are availa-ble, merg(.)
can be trained, e.g., if assuming it to be the wide-ly-used (log-) linear form.609We will examine the role of sentiment con-sistency in two ways: designing features for thetranslation model and using them for re-ranking.Before discussing the details of our features, webriefly recap phrase-based SMT for completeness.Given a source sentence f, the goal of statisticalmachine translation is to select a target languagestring e which maximizes the posterior probabilityP(e|f).
In a phrase-based SMT system, the transla-tion unit is the phrases, where a "phrase" is a se-quence of words.
Phrase-based statistical machinetranslation systems are usually modeled through alog-linear framework (Och and Ney, 2002) by in-troducing the hidden word alignment variable a(Brown et al., 1993).
)),~,~((maxarg~ 1,* ?
??
Mm mmae afeHe ?
(2)where e~ is a string of phrases in the target lan-guage, f~ is the source language string,),~,~( afeHm  are feature functions, and weightsm?
are typically optimized to maximize the scoringfunction (Och, 2003).3.4 Feature designIn Section 3.2 above, we have discussed our lexi-con-based approach, which leverages lexicon-based sentiment consistency.
Below, we describethe specific features we designed for our experi-ments.
For a phrase pair ( ef ~,~ ) or a sentence pair(f, e)6, we propose the following four groups ofconsistency features.Subjectivity The first group of features is designedto check the subjectivity of a phrase or a sentencepair (f, e).
This set of features examines if thesource or target side contains sentiment units.
Asthe name suggests, these features only capture ifsubjectivity exists, but not if a sentiment is positive,negative, or neutral.
We include four binary fea-tures that are triggered in the following condi-tions?satisfaction of each condition gives thecorresponding feature a value of 1 and otherwise 0.?
F1: if neither side of the pair (f, e) contains atleast one sentiment unit;6 For simplicity, we hereafter use the same notation (f, e) torepresent both a phrase pair and a sentence pair, when no con-fusion arises.?
F2: if only one side contains sentiment units;?
F3: if the source side contains sentimentunits;?
F4: if the target side contains sentiment units.Sentiment polarity The second group of featurescheck the sentiment polarity.
These features arestill binary; they check if the polarities of thesource and target side are the same.?
F5: if the two sides of the pair (f, e) have thesame polarity;?
F6: if at least one side has a neutral senti-ment;?
F7: if the polarity is opposite on the twosides, i.e., one is positive and one is negative.Note that examining the polarity on each sidecan be regarded as a special case of applying Equa-tion 1 above.
For example, examining the positivesentiment corresponds to using an indicator func-tion as the basis function: it takes a value of 1 ifthe lsv-score of a sentiment unit is positive or 0otherwise, while the merge function is the logicOR function.
The subjectivity features above canalso be thought of similarly.Sentiment intensity The third group of features isdesigned to capture the degree of sentiment andthese features are numerical.
We designed twotypes of features in this group.Feature F8 measures the difference of the LSVscores on the two sides.
As shown in Equation (3),we use a mean function7  as our merge functionwhen computing the lsv-scores with Equation (1),where the basis function is simply the identityfunction.?
??
ni iulsvnslsv 01 )(1)((3)Feature F9, F10, and F11 are the second type inthis group of features, which compute the ratio ofsentiment units on each side and examine their dif-ference.?
F8: |)()(|),( 118 elsvflsvefH ???
F9: |)()(|),(9 elsvflsvefH ??
?
?7 We studied several different options but found the averagefunction is better than others for our translation task here, e.g.,better than giving more weight to the last unit.610?
F10: |)()(|),(10 elsvflsvefH ??
???
F11: |)()(|),(11 elsvflsvefH ????
??lsv+(.)
calculates the ratio of a positive sentimentunits in a phrase or a sentence, i.e., the number ofpositive sentiment units divided by the total num-ber of words of the phrase or the sentence.
It corre-sponds to a special form of Equation 1, in whichthe basis function is an indicator function as dis-cussed above, and the merge function adds up allthe counts and normalizes the sum by the length ofthe phrase or the sentence concerned.
Similarly,lsv-(.)
calculates the ratio of negative units andlsv+-(.)
calculates that for both types of units.
Thelength of sentence here means the number of wordtokens.
We experimented with and without remov-ing stop words when counting them, and found thatdecision has little impact on the performance.
Wealso used the part-of-speech (POS) information inthe sentiment lexicons to help decide if a word is asentiment word or not, when we extract features;i.e., a word is considered to have sentiment only ifits POS tag also matches what is specified in thelexicons8.
Using POS tags, however, did not im-prove our translation performance.Negation The fourth group of features checks theconsistency of negation words on the source andtarget side.
Note that negation words have alreadybeen considered in computing the lsv-scores ofsentiment units.
One motivation is that a negationword may appear far from the sentiment word itmodifies, as mentioned in (Taboada et al., 2011)and may be outside the window we used to calcu-late the lsv-score above.
The features here addi-tionally check the counts of negation words.
Thisgroup of features is binary and triggered by thefollowing conditions.?
F12: if neither side of the pair (f, e) containnegation words;?
F13: if both sides have an odd number ofnegation words or both sides have an evennumber of them;?
F14:  if both sides have an odd number ofnegation words not appearing outside anysentiment units, or if both sides have an evennumber of such negation words;8 The Stanford POS tagger (Toutanova et al., 2003) wasused to tag phrase and sentence pairs for this purpose.?
F15: if both sides have an odd number ofnegation words appearing in all sentimentunits, or if both sides have an even numberof such negation words.4 Experiments4.1 Translation experimental settingsExperiments were carried out with an in-housephrase-based system similar to Moses (Koehn etal., 2007).
Each corpus was word-aligned usingIBM model 2, HMM, and IBM model 4, and thephrase table was the union of phrase pairs extract-ed from these separate alignments, with a lengthlimit of 7.
The translation model was smoothed inboth directions with Kneser-Ney smoothing (Chenet al., 2011).
We use the hierarchical lexicalizedreordering model (Galley and Manning, 2008),with a distortion limit of 7.
Other features includelexical weighting in both directions, word count, adistance-based RM, a 4-gram LM trained on thetarget side of the parallel data, and a 6-gram Eng-lish Gigaword LM.
The system was tuned withbatch lattice MIRA (Cherry and Foster, 2012).We conducted experiments on NIST Chinese-to-English translation task.
The training data are fromNIST Open MT 2012.
All allowed bilingual corpo-ra were used to train the translation model and re-ordering models.
There are about 283M targetword tokens.
The development (dev) set comprisedmainly data from the NIST 2005 test set, and alsosome balanced-genre web-text from NIST trainingdata.
Evaluation was performed on NIST 2006 and2008, which have 1,664 and 1,357 sentences,39.7K and 33.7K source words respectively.
Fourreferences were provided for all dev and test sets.4.2 ResultsOur evaluation metric is case-insensitive IBMBLEU (Papineni et al., 2002), which performsmatching of n-grams up to n = 4; we report BLEUscores on two test sets NIST06 and NIST08.
Fol-lowing (Koehn, 2004), we use the bootstrapresampling test to do significance testing.
In Table4-6, the sign * and ** denote statistically signifi-cant gains over the baseline at the p < 0.05 and p <0.01 level, respectively.611NIST06 NIST08 Avg.Baseline 35.1 28.4 31.7+feat.
group1 35.6** 29.0** 32.3+feat.
group2 35.3* 28.7* 32.0+feat.
group3 35.3 28.7* 32.0+feat.
group4 35.5* 28.8* 32.1+feat.
group1+2 35.8** 29.1** 32.5+feat.
group1+2+3 36.1** 29.3** 32.7+feat.
group1+2+3+4 36.2** 29.4** 32.8Table 4: BLEU(%) scores on two original test sets fordifferent feature combinations.
The sign * and ** indi-cate statistically significant gains over the baseline atthe p < 0.05 and p < 0.01 level, respectively.Table 4 summarizes the results of the baselineand the results of adding each group of featuresand their combinations.
We can see that each indi-vidual feature group improves the BLEU scores ofthe baseline, and most of these gains are signifi-cant.
Among the feature groups, the largest im-provement is associated with the first featuregroup, i.e., the subjectivity features, which sug-gests the significant role of modeling the basic sub-jectivity.
Adding more features results in furtherimprovement; the best performance was achievedwhen using all these sentiment consistency fea-tures, where we observed a 1.1 point improvementon the NIST06 set and a 1.0 point improvement onthe NIST08 set, which yields an overall improve-ment of about 1.1 BLEU score.To further observe the results, we split each ofthe two (i.e., the NIST06 and NIST08) test setsinto three subsets according to the ratio of senti-ment words in the reference.
We call them low-sen, mid-sen and high-sen subsets, denoting lower,middle, and higher sentiment-word ratios, respec-tively.
The three subsets contain roughly equalnumber of sentences.
Then we merged the twolow-sen subsets together, and similarly the twomid-sen and high-sen subsets together, respective-ly.
Each subset has roughly 1007 sentences.low-sen mid-sen high-senbaseline 33.4 32.3 29.3+all feat.
34.4** 33.5** 30.4**improvement 1.0 1.2 1.1Table 5: BLEU(%) scores on three sub test sets withdifferent sentiment ratios.Table 5 shows the performance of baseline andthe system with sentiment features (the last systemof Table 4) on these subsets.
First, we can see thatboth systems perform worse as the ratio of senti-ment words increases.
This probably indicates thattext with more sentiment is harder to translate thantext with less sentiment.
Second, it is interestingthat the largest improvement is seen on the mid-sensub-set.
The larger improvement on the mid-sen/high-sen subsets than on the low-sen may indi-cate the usefulness of the proposed features in cap-turing sentiment information.
The lowerimprovement on high-sen than on mid-sen proba-bly indicates that the high-sen subset is hard any-way and using simple lexicon-level features is notsufficient.Sentence-level reranking Above, we have incor-porated sentiment features into the phrase tables.To further confirm the usefulness of the sentimentconsistency features, we explore their role for sen-tence-level reranking.
To this end, we re-rank1000-best hypotheses for each sentence that weregenerated with the baseline system.
All the senti-ment features were recalculated for each hypothe-sis.
We then re-learned the weights for thedecoding and sentiment features to select the besthypothesis.
The results are shown in Table 6.
Wecan see that sentiment features improve the per-formance via re-ranking.
The improvement is sta-tistically significant, although the absoluteimprovement is less than that obtained by incorpo-rating the sentiment features in decoding.
Not thatas widely known, the limited variety of candidatesin reranking may confine the improvement thatcould be achieved.
Better models on the sentencelevel are possible.
In addition, we feel that ensur-ing sentiment and its target to be correctly paired isof interest.
Note that we have also combined thelast system in Table 4 with the reranking systemhere; i.e., sentiment consistency was incorporatedin both ways, but we did not see further improve-ment, which suggests that the benefit of the senti-ment features has mainly been captured in thephrase tables already.feature NIST06 NIST08 Avg.baseline 35.1 28.4 31.7+ all feat.
35.4* 28.9** 32.1Table 6: BLEU(%) scores on two original test sets onsentence-level sentiment features.612Human evaluation We conducted a human evalu-ation on the output of the baseline and the systemthat incorporates all the proposed sentiment fea-tures (the last system in Table 4).
For this purpose,we randomly sampled 250 sentences from the twoNIST test sets according to the following condi-tions.
First, the selected sentences should containat least one sentiment word?in this evaluation, wetarget the sentences that may convey some senti-ment.
Second, we do not consider sentences short-er than 5 words or longer than 50 words; or whereoutputs of the baseline system and the system withsentiment feature were identical.
The 250 selectedsentences were split into 9 subsets, as we have 9human evaluators (none of the authors of this papertook part in this experiment).
Each subset contains26 randomly selected sentences, which are 234sentences in total.
The other 16 sentences are ran-domly selected to serve as a common data set: theyare added to each of the 9 subsets in order to ob-serve agreements between the 9 annotators.
Inshort, each human evaluator was presented with 42evaluation samples.
Each sample is a tuple contain-ing the output of the baseline system, that of thesystem considering sentiment, and the referencetranslation.
The two automatic translations werepresented in a random order to the evaluators.As in (Callison-Burch et al., 2012), we per-formed a pairwise comparison of the translationsproduced by the systems.
We asked the annotatorsthe following two questions Q1 and Q2:?
Q1(general preference): For any reason,which of the two translations do you preferaccording to the provided references, other-wise mark ?no preference???
Q2 (sentiment preference):  Does the refer-ence contains sentiment?
If so, in terms ofthe translations of the sentiment, which ofthe two translations do you prefer, otherwisemark ?no preference?
?We computed Fleiss?s Kappa (Fleiss, 1971) onthe common set to measure inter-annotator agree-ment,all?
.
Then, we excluded one and only oneannotator at a time to compute i?
(Kappa scorewithout i-th annotator, i.e., from the other eight).Finally, we removed the annotation of the two an-notators whose answers were most different fromthe others?
: i.e., annotators with the biggestiall ??
?
values.
As a result, we got a Kappa score0.432 on question Q1 and 0.415 on question Q2,which both mean moderate agreement.base win bsc win equal totalTranslation 58(31.86%)82(45.05%)42(23.09%)182Sentiment 30(22.39%)49(36.57%)55(41.04%)134Table 7: Human evaluation preference for outputs frombaseline vs. system with sentiment features.This left 7 files from 7 evaluators.
We threwaway the common set in each file, leaving 182pairwise comparisons.
Table 6 shows that the eval-uators preferred the output from the system withsentiment features 82 times, the output from thebaseline system 58 times, and had no preferencethe other 42 times.
This indicates that there is ahuman preference for the output from the systemthat incorporated the sentiment features over thosefrom the baseline system at the p<0.05 significancelevel (in cases where people prefer one of them).For question Q2, the human annotators regarded 48sentences as conveying no sentiment according tothe provided reference, although each of them con-tains at least one sentiment word (a criterion wedescribed above in constructing the evaluation set).Among the remaining 134 sentences, the humanannotators preferred the proposed system 49 timesand the baseline system 30 times, while they markno-preference 55 times.
The result shows a humanpreference for the proposed model that considerssentiment features at the p<0.05 significance level(in the cases where the evaluators did mark a pref-erence).4.3 ExamplesWe have also manually examined the translationsgenerated by our best model (the last model of Ta-ble 4, named BSC below) and the baseline model(BSL), and we attribute the improvement to twomain reasons: (1) checking sentiment consistencyon a phrase pair helps punish low-quality phrasepairs caused by word alignment error, (2) suchconsistency checking also improves the sentimentof the translation to better match the sentiment ofthe source.613(1)Phr.
pairsREFBSLBSC??
||| talks   vs.
??
||| peace talks?
help the palestinians and the israelis to resume peace talks ??
help the israelis and palestinians to resumption of the talks ??
help the israelis and palestinians to resume peace talks ?(2)Phr.
pairsREFBSLBSC??
||| war    vs.
??
||| preparing for?
the national team is preparing for matches with palestine and Iraq ??
the national team 's match with the palestinians and the iraq war ??
the national team preparing for the match with the palestinian and iraq ?(3)REFBSLBSC?
in china we have top-quality people , ever-improving facilities ??
we have talents in china , an increasing number of facilities ??
we have outstanding talent in china , more and better facilities ?(4)REFBSLBSC?
continue to strive for that ??
continue to struggle ??
continue to work hard to achieve ?Table 8: Examples that show how sentiment helps improve our baseline model.
REF is a reference translation, BSLstands for baseline model, and BSC (bilingual sentiment consistency) is the last model of Table 4.In the first two examples of Table 8, the firstline shows two phrase pairs that are finally chosenby the baseline and BSC system, respectively.
Thenext three lines correspond to a reference (REF),translation from BSL, and that from the BSC sys-tem.
The correct translations of ????
should be?peace negotiations?
or ?peace talks?, which havea positive sentiment, while the word ?talks?doesn?t convey sentiment at all.
By punishing thephrase pair ???
||| talks?, the BSC model wasable to generate a better translation.
In the secondexample, the correct translation of ????
shouldbe ?prepare for?, where neither side conveys sen-timent.
The incorrect phrase pair ???
||| war?
isgenerated from incorrect word alignment.
Since?war?
is a negative word in our sentiment lexicon,checking sentiment consistency helps down-weightsuch incorrect translations.
Note also that the in-correct phrase pair ???
||| war?
is not totally irra-tional, as the literal translation of ???
?
is?prepare for war?.Similarly, in the third example, ?outstanding tal-ent?
is closer with respect to sentiment to the refer-ence ?top-quality people?
than ?talent?
is; ?moreand better?
is closer with respect to sentiment tothe reference ?ever-improving?
than ?an increasingnumber?
is.
These three examples also help us un-derstand the benefit of the subjectivity featuresdiscussed in Section 3.4.
In the fourth example,?work hard to achieve?
has a positive sentiment,same as ?strive?, while ?struggle?
is negative.
Wecan see that the BSC model is able to preserve theoriginal sentiment better (the 9 human evaluatorswho were involved in our human evaluation (Sec-tion 4.3) all agreed with this).5 Conclusions and future workWe explore lexicon-based sentiment consistencyfor statistical machine translation.
By incorporatinglexicon-based subjectivity, polarity, intensity, andnegation features into the phrase-pair translationmodel, we observed a 1.1-point improvement ofBLEU score on NIST Chinese-to-English transla-tion.
Among the four individual groups of features,subjectivity consistency yields the largest im-provement.
The usefulness of the sentiment fea-tures has also been confirmed when they are usedfor re-ranking, for which we observed a 0.4-pointimprovement on the BLEU score.
In addition, hu-man evaluation shows the preference of the humansubjects towards the translations generated by theproposed model, in terms of both the general trans-lation quality and the sentiment conveyed.In the paper, we propose a lexicon-based ap-proach to the problem.
It is possible to employmore complicated models.
For example, with theinvolvement of proper sentiment-annotated data, ifavailable, one may train a better sentiment-analysismodel even for the often-ungrammatical phrasepairs or sentence candidates.
Another direction wefeel interesting is ensuring that sentiment and itstarget are not only better translated but also betterpaired, i.e., their semantic relation is preserved.This is likely to need further syntactic or semanticanalysis at the sentence level, and the semantic rolelabeling work reviewed in Section 2 is relevant.614ReferencesC.
Banea, R. Mihalcea, J. Wiebe and S. Hassan.
2008.Multilingual subjectivity analysis using machinetranslation.
In Proc.
of EMNLP.E.
M. Benzinger.
1971.
Intensifiers in current English.PhD.
Thesis.
University of Florida.P.
F. Brown, S. Della Pietra, V. Della J. Pietra, and R.Mercer.
1993.
The mathematics of Machine Transla-tion: Parameter estimation.
Computational Linguis-tics, 19(2): 263-312.C.
Callison-Burch, P. Koehn, C. Monz, R. Soricut, andL.
Specia.
2012.
Findings of the 2012 Workshop onStatistical Machine Translation.
In Proc.
of WMT.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
of ACL, 263?270.B.
Chen, G. Foster, and R. Kuhn.
2010.
Bilingual SenseSimilarity for Statistical Machine Translation.
InProc.
of ACL, 834-843.B.
Chen, R. Kuhn, G. Foster, and H. Johnson.
2011.Unpacking and transforming feature functions: Newways to smooth phrase tables.
In Proc.
of MT Sum-mit.C.
Cherry and G. Foster.
2012.
Batch tuning strategiesfor statistical machine translation.
In Proc.
ofNAACL.J.
L. Fleiss.
1971.
Measuring nominal scale agreementamong many raters.
Psychological Bulletin, 76(5):378?382.M.
Galley and C. D. Manning.
2008.
A simple and ef-fective hierarchical phrase reordering model.
In Proc.of EMNLP: 848?856.V.
Hatzivassiloglou and K. McKeown.
1997.
Predictingthe semantic orientation of adjectives.
In Proc.
ofEACL: 174-181.P.
Koehn.
2004.
Statistical significance tests for ma-chine translation evaluation.
In Proc.
of EMNLP:388?395.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,R.
Zens, C. Dyer, O. Bojar, A. Constantin and E.Herbst.
2007.
Moses: Open Source Toolkit for Statis-tical Machine Translation.
In Proc.
of ACL, 177-180.J.
Li, P. Resnik and H. Daume III.
2013.
Modeling Syn-tactic and Semantic Structures in HierarchicalPhrase-based Translation.
In Proc.
of NAACL, 540-549.D.
Liu and D. Gildea.
2010.
Semantic role features formachine translation.
In Proc.
of COLING,  716?724.R.
Mihalcea, C. Banea and J. Wiebe.
2007.
Learningmultilingual subjective language via cross-lingualprojections.
In Proc.
of ACL.F.
J. Och and H. Ney.
2002.
Discriminative Trainingand Maximum Entropy Models for Statistical Ma-chine Translation.
In Proc.
of ACL.F.
J. Och.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proc.
of ACL.C.
E. Osgood, G. J. Suci, and  P. H. Tannenbaum.
1957.The measurement of meaning.
University of IllinoisPress.B.
Pang, L. Lee, S. Vaithyanathan.
2002.
Thumbs up?
:sentiment classification using machine learning tech-niques.
In Proc.
of EMNLP, 79-86.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of ma-chinetranslation.
In Proc.
of ACL, 311?318.M.
Taboada, M. Tofiloski, J. Brooke, K. Voll, and M.Stede.
2011.
Lexicon-Based Methods for SentimentAnalysis.
Computational Linguistics.
37(2): 267-307.K.
Toutanova, D. Klein, C. Manning, and Y. Singer.2003.
Feature-Rich Part-of-Speech Tagging with aCyclic Dependency Network.
In Proc.
of HLT-NAACL, 252-259.P.
Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifi-cation of reviews.
In Proc.
of ACL, 417-424.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM basedword alignment in statistical translation.
In Proc.
ofCOLING.X.
Wan.
2009.
Co-Training for Cross-Lingual Senti-ment Classification.
In proc.
of ACL, 235-243.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recogniz-ing Contextual Polarity in Phrase-Level SentimentAnalysis.
In Proc.
of EMNLP.D.
Wu and P. Fung.
2009.
Semantic Roles for SMT: AHybrid Two-Pass Model.
In Proc.
of NAACL, 13-16.L.
Xu and H. Lin.
2007.
Ontology-Driven AffectiveChinese Text Analysis and Evaluation Method.
InLecture Notes in Computer Science Vol.
4738, 723-724, Springer.C.
Zhang, P. Liu, Z. Zhu, and M. Fang.
2012.
A Senti-ment Analysis Method Based on a Polarity Lexicon.Journal of Shangdong University (Natural Science).47(3): 47-50.615
