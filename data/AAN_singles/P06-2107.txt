Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 835?841,Sydney, July 2006. c?2006 Association for Computational LinguisticsStatistical phrase-based models for interactive computer-assistedtranslationJesu?s Toma?s and Francisco CasacubertaInstituto Tecnolo?gico de Informa?ticaUniversidad Polite?cnica de Valencia46071 Valencia, Spain{jtomas,fcn}@upv.esAbstractObtaining high-quality machine transla-tions is still a long way off.
A post-editing phase is required to improve theoutput of a machine translation system.An alternative is the so called computer-assisted translation.
In this framework, ahuman translator interacts with the sys-tem in order to obtain high-quality trans-lations.
A statistical phrase-based ap-proach to computer-assisted translation isdescribed in this article.
A new decoder al-gorithm for interactive search is also pre-sented, that combines monotone and non-monotone search.
The system has beenassessed in the TransType-2 project forthe translation of several printer manuals,from (to) English to (from) Spanish, Ger-man and French.1 IntroductionComputers have become an important tool to in-crease the translator?s productivity.
In a more ex-tended framework, a machine translation (MT)system can be used to obtain initial versions of thetranslations.
Unfortunately, the state of the art inMT is far from being perfect, and a human trans-lator must edit this output in order to achieve high-quality translations.Another possibility is computer-assisted trans-lation (CAT).
In this framework, a human trans-lator interacts with the system in order to obtainhigh-quality translations.
This work follows theapproach of interactive CAT initially suggestedby (Foster et al, 1996) and developed in theTransType2 project (SchlumbergerSema S.A.
etal., 2001; Barrachina et al, 2006).
In this frame-work, the system suggests a possible translationof a given source sentence.
The human translatorcan accept either the whole suggestion or accept itonly up to a certain point (that is, a character pre-fix of this suggestion).
In the latter case, he/shecan type one character after the selected prefix inorder to direct the system to the correct translation.The accepted prefix and the new corrected charac-ter can be used by the system to propose a newsuggestion to complete the prefix.
The process isrepeated until the user completely accepts the sug-gestion proposed by the system.
Figure 1 showsan example of a possible CAT system interaction.Statistical machine translation (SMT) is an ad-equate framework for CAT since the MT mod-els used can be learnt automatically from a train-ing bilingual corpus and the search proceduresdeveloped for SMT can be adapted efficiently tothis new interactive framework (Och et al, 2003).Phrase-based models have proved to be very ad-equate statistical models for MT (Toma?s et al,2005).
In this work, the use of these models hasbeen extended to interactive CAT.The organization of the paper is as follows.The following section introduces the statistical ap-proach to MT and section 3 introduces the sta-tistical approach to CAT.
In section 4, we reviewthe phrase-based translation model.
In section 5,we describe the decoding algorithm used in MT,and how it can be adapted to CAT.
Finally, wewill present some experimental results and conclu-sions.2 Statistical machine translationThe goal of SMT is to translate a given source lan-guage sentence sJ1 = s1...sJ to a target sentencetI1 = t1...tI .
The methodology used (Brown etal., 1993) is based on the definition of a functionPr(tI1|sJ1 ) that returns the probability that tI1 is a835source Transferir documentos explorados a otro directoriointeraction-0 Move documents scanned to other directoryinteraction-1 Move s canned documents to other directoryinteraction-2 Move scanned documents to a nother directoryinteraction-3 Move scanned documents to another f olderacceptance Move scanned documents to another folderFigure 1: Example of CAT system interactions to translate the Spanish source sentence into English.
Ininteraction-0, the system suggests a translation.
In interaction-1, the user accepts the first five characters?Move ?
and presses the key s , then the system suggests completing the sentence with ?canneddocuments to other directory?.
Interactions 2 and 3 are similar.
In the final interaction, theuser completely accepts the present suggestion.translation of a given sJ1 .
Once this function is es-timated, the problem can be reduced to search asentence t?I?1 that maximizes this probability for agiven sJ1 .t?I?1 = argmaxI,tI1Pr(tI1|sJ1 ) = argmaxI,tI1Pr(tI1)Pr(sJ1 |tI1)(1)Equation 1 summarizes the following three mat-ters to be solved: First, an output language modelis needed to distinguish valid sentences from in-valid sentences in the target language, Pr(tI1).Second, a translation model, Pr(sJ1 |tI1).
Finally,the design of an algorithm to search for the sen-tence t?I1 that maximizes this product.3 Statistical computer-assistedtranslationIn a CAT scenario, the source sentence sJ1 and agiven prefix of the target sentence ti1 are given.This prefix has been validated by the user (using aprevious suggestion by the system plus some cor-rected words).
Now, we are looking for the mostprobable words that complete this prefix.t?I?i+1 = argmaxI,tIi+1Pr(tIi+1|sJ1 , ti1)= argmaxI,tIi+1Pr(tI1)Pr(sJ1 |tI1) (2)This formulation is very similar to the previouscase, but in this one, the search is constrainedto the set of possible suffixes tIi+1 instead ofthe whole target sentences tI1.
Therefore, thesame techniques (translation models, decoder al-gorithm, etc.)
which have been developed forSMT can be used in CAT.Note that the statistical models are defined atword level.
However, the CAT interface describedin the first section works at character level.
Thisis not a problem: the transformation can be per-formed in an easy way.Another important issue is the computationaltime required by the system to produce a new sug-gestion.
In the CAT framework, real-time is re-quired.4 Phrase-based modelsThe usual statistical translation models can beclassified as single-word based alignment models.Models of this kind assume that an input word isgenerated by only one output word (Brown et al,1993).
This assumption does not correspond to thecharacteristics of natural language; in some cases,we need to know a word group in order to obtain acorrect translation.One initiative for overcoming the above-mentioned restriction of single-word models isknown as the template-based approach (Och,2002).
In this approach, an entire group of adja-cent words in the source sentence may be alignedwith an entire group of adjacent target words.
Asa result, the context of words has a greater influ-ence and the changes in word order from sourceto target language can be learned explicitly.
Atemplate establishes the reordering between twosequences of word classes.
However, the lexicalmodel continues to be based on word-to-word cor-respondence.A simple alternative to these models has beenproposed, the phrase-based (PB) approach (Toma?sand Casacuberta, 2001; Marcu and Wong, 2002;Zens et al, 2002).
The principal innovation of thephrase-based alignment model is that it attempts tocalculate the translation probabilities of word se-quences (phrases) rather than of only single words.These methods explicitly learn the probability of a836sequence of words in a source sentence (s?)
beingtranslated as another sequence of words in the tar-get sentence (t?
).To define the PB model, we segment the sourcesentence sJ1 into K phrases (s?K1 ) and the targetsentence tI1 into K phrases (t?K1 ).
A uniform prob-ability distribution over all possible segmentationsis assumed.
If we assume a monotone alignment,that is, the target phrase in position k is producedonly by the source phrase in the same position(Toma?s and Casacuberta, 2001) we get:Pr(sJ1 |tI1) ?
?K,t?K1 ,s?K1K?k=1p(s?k|t?k) (3)where the parameter p(s?|t?)
estimates the probabil-ity of translating the phrase t?
into the phrase s?.A phrase can be comprised of a single word (butempty phrases are not allowed).
Thus, the con-ventional word to word statistical dictionary is in-cluded.If we permit the reordering of the target phrases,a hidden phrase level alignment variable, ?K1 , isintroduced.
In this case, we assume that the targetphrase in position k is produced only by the sourcephrase in position ?k.Pr(sJ1 |tI1) ?
?K,t?K1 ,s?K1 ,?K1K?k=1p(?k|?k?1)?p(s?k|t?
?k)(4)where the distortion model p(?k| ?k?1) (the prob-ability of aligning the target segment k with thesource segment ?k) depends only on the previousalignment ?k?1 (first order model).
For the dis-tortion model, it is also assumed that an alignmentdepends only on the distance of the two phrases(Och and Ney, 2000):p(?k|?k?1) = p|??k??
?k?1 |0 .
(5)There are different approaches to the parameterestimation.
The first one corresponds to a di-rect learning of the parameters of equations 3 or4 from a sentence-aligned corpus using a max-imum likelihood approach (Toma?s and Casacu-berta, 2001; Marcu and Wong, 2002).
The sec-ond one is heuristic and tries to use a word-aligned corpus (Zens et al, 2002; Koehn et al,2003).
These alignments can be obtained fromsingle-word models (Brown et al, 1993) using theavailable public software GIZA++ (Och and Ney,2003).
The latter approach is used in this research.5 Decoding in interactive machinetranslationThe search algorithm is a crucial part of a CATsystem.
Its performance directly affects the qual-ity and efficiency of translation.
For CAT searchwe propose using the same algorithm as in MT.Thus, we first describe the search in MT.5.1 Search for MTThe aim of the search in MT is to look fora target sentence tI1 that maximizes the productP (tI1) ?
P (sJ1 |tI1).
In practice, the search is per-formed to maximise a log-linear model of Pr(tI1)and Pr(tI1|sJ1 )?
that allows a simplification of thesearch process and better empirical results in manytranslation tasks (Toma?s et al, 2005).
Parameter?
is introduced in order to adjust the importanceof both models.
In this section, we describe twosearch algorithms which are based on multi-stack-decoding (Berger et al, 1996) for the monotoneand for the non-monotone model.The most common statistical decoder algo-rithms use the concept of partial translation hy-pothesis to perform the search (Berger et al,1996).
In a partial hypothesis, some of the sourcewords have been used to generate a target prefix.Each hypothesis is scored according to the trans-lation and language model.
In our implementa-tion for the monotone model, we define a hypoth-esis search as the triple (J ?, tI?1 , g), where J ?
is thelength of the source prefix we are translating (i.e.sJ ?1 ); the sequence of I ?
words, tI?1 , is the targetprefix that has been generated and g is the score ofthe hypothesis (g = Pr(tI?1 ) ?
Pr(tI?1 |sJ?1 )?
).The translation procedure can be described asfollows.
The system maintains a large set of hy-potheses, each of which has a corresponding trans-lation score.
This set starts with an initial emptyhypothesis.
Each hypothesis is stored in a differ-ent stack, according to the source words that havebeen considered in the hypothesis (J ?).
The al-gorithm consists of an iterative process.
In eachiteration, the system selects the best scored par-tial hypothesis to extend in each stack.
The exten-sion consists in selecting one (or more) untrans-lated word(s) in the source and selecting one (ormore) target word(s) that are attached to the exist-ing output prefix.
The process continues severaltimes or until there are no more hypotheses to ex-tend.
The final hypothesis with the highest scoreand with no untranslated source words is the out-837put of the search.The search can be extended to allow for non-monotone translation.
In this extension, severalreorderings in the target sequence of phrases arescored with a corresponding probability.
We de-fine a hypothesis search as the triple (w, tI?1 , g),where w = {1..J} is the coverage set that defineswhich positions of source words have been trans-lated.
For a better comparison of hypotheses, thestore of each hypothesis in different stacks accord-ing to their value of w is proposed in (Berger et al,1996).
The number of possible stacks can be veryhigh (2J ); thus, the stacks are created on demand.The translation procedure is similar to the previousone: In each iteration, the system selects the bestscored partial hypothesis to extend in each createdstack and extends it.5.2 Search algorithms for iterative MT.The above search algorithm can be adapted to theiterative MT introduced in the first section, i.e.given a source sentence sJ1 and a prefix of the tar-get sentence ti1, the aim of the search in iterativeMT is to look for a suffix of the target sentencet?I?i+1 that maximises the product Pr(tI1)?Pr(sJ1 |tI1)(or the log-linear model: Pr(tI?1 ) ?Pr(tI?1 |sJ?1 )?).
Asimple modification of the search algorithm is nec-essary.
When a hypothesis is extended, if the newhypothesis is not compatible with the fixed targetprefix, ti1, then this hypothesis is not considered.Note that this prefix is a character sequence and ahypothesis is a word sequence.
Thus, the hypothe-sis is converted to a character sequence before thecomparison.In the CAT scenario, speed is a critical aspect.In the PB approach monotone search is more effi-cient than non-monotone search and obtains simi-lar translation results for the tasks described in thisarticle (Toma?s and Casacuberta, 2004).
However,the use of monotone search in the CAT scenariopresents a problem: If a user introduces a prefixthat cannot be obtained in a monotone way fromthe source, the search algorithm is not able to com-plete this prefix.
In order to solve this problem,but without losing too much efficiency, we use thefollowing approach: Non-monotone search is usedwhile the target prefix is generated by the algo-rithm.
Monotone search is used while new wordsare generated.Note that searching for a prefix that we alreadyknow may seem useless.
The real utility of thisphase is marking the words in the target sentencethat have been used in the translation of the givenprefix.A desirable feature of the iterative machinetranslation system is the possibility of producinga list of target suffixes, instead of only one (Civeraet al, 2004).
This feature can be easily obtainedby keeping the N -best hypotheses in the last stack.In practice these N -best hypotheses are too simi-lar.
They differ only in one or two words at the endof the sentence.
In order to solve this problem, thefollowing procedure is performed: First, generatea hypotheses list using the N -best hypotheses ofa regular search.
Second, add to this list, new hy-potheses formed by a single translation-word froma non-translated source word.
Third, add to thislist, new hypotheses formed by a single word witha high probability according to the target languagemodel.
Finally, sort the list maximising the diver-sity at the beginning of the suffixes and select thefirst N hypotheses.6 Experimental results6.1 Evaluation criteriaFour different measures have been used in the ex-periments reported in this paper.
These measuresare based on the comparison of the system outputwith a single reference.?
Word Error Rate (WER): Edit distance interms of words between the target sentenceprovided by the system and the referencetranslation (Och and Ney, 2003).?
Character Error Rate (CER): Edit distance interms of characters between the target sen-tence provided by the system and the refer-ence translation (Civera et al, 2004).?
Word-Stroke Ratio (WSR): Percentage ofwords which, in the CAT scenario, must bechanged in order to achieve the reference.?
Key-Stroke Ratio (KSR): Number of key-strokes that are necessary to achieve the ref-erence translation divided by the number ofrunning characters (Och et al, 2003) 1.1In others works, an extra keystroke is added in the lastiteration when the user accepts the sentence.
We do not addthis extra keystroke.
Thus, the KSR obtained in the interac-tion example of Figure 1, is 3/40.838time (ms) WSR KSR10 33.9 11.240 30.9 9.8100 30.0 9.3500 27.8 8.513000 27.5 8.3Table 2: Translation results obtained for sev-eral average response time in the Spanish/English?XRCE?
task.WER and CER measure the post-editing ef-fort to achieve the reference in an MT scenario.On the other hand, WSR and KSR measure theinteractive-editing effort to achieve the referencein a CAT scenario.
WER and CER measures havebeen obtained using the first suggestion of theCAT system, when the validated prefix is void.6.2 Task descriptionIn order to validate the approach described in thispaper a series of experiments were carried out us-ing the XRCE corpus.
They involve the translationof technical Xerox manuals from English to Span-ish, French and German and from Spanish, Frenchand German to English.
In this research, we usethe raw version of the corpus.
Table 1 shows somestatistics of training and test corpus.6.3 ResultsTable 2 shows the WSR and KSR obtained for sev-eral average response times, for Spanish/Englishtranslations.
We can control the response timechanging the number of iterations in the search al-gorithm.
Note that real-time restrictions cause asignificant degradation of the performance.
How-ever, in a real CAT scenario long iteration timescan render the system useless.
In order to guar-antee a fast human interaction, in the remainingexperiments of the paper, the mean iteration timeis constrained to about 80 ms.Table 3 shows the results using monotonesearch and combining monotone and non-monotone search.
Using non-monotone searchwhile the given prefix is translated improves theresults significantly.Table 4 compares the results when the systemproposes only one translation (1-best) and whenthe system proposes five alternative translations(5-best).
Results are better for 5-best.
However, inthis configuration the user must read five differentmonotone non-monotoneWSR KSR WSR KSREnglish/Spanish 36.1 11.2 28.7 8.9Spanish/English 32.2 10.4 30.0 9.3English/French 66.0 24.9 60.7 22.6French/English 64.5 23.6 61.6 22.2English/German 71.0 27.1 67.6 25.6German/English 66.4 23.6 62.0 21.9Table 3: Comparison of monotone and non-monotone search in ?XRCE?
corpora.1-best 5-bestWSR KSR WSR KSREnglish/Spanish 28.7 8.9 28.4 7.3Spanish/English 30.0 9.3 29.7 7.6English/French 60.7 22.6 59.8 18.8French/English 61.6 22.2 60.7 17.6English/German 67.6 25.6 67.1 20.9German/English 62.0 21.9 61.6 16.5Table 4: CAT results for the ?XRCE?
task for 1-best hypothesis and 5-best hypothesis.alternatives before choosing.
It is still to be shownif this extra time is compensated by the fewer keystrokes needed.Finally, in table 5 we compare the post-editingeffort in an MT scenario (WER and CER) and theinteractive-editing effort in a CAT scenario (WSRand KSR).
These results show how the number ofcharacters to be changed, needed to achieve thereference, is reduced by more than 50%.
The re-duction at word level is slight or none.
Note thatresults from English/Spanish are much better thanfrom English/French and English/German.
Thisis because a large part of the English/Spanish testcorpus has been obtained from the index of thetechnical manual, and this kind of text is easier totranslate.It is not clear how these theoretical gains trans-late to practical gains, when the system is used byreal translators (Macklovitch, 2004).7 Related workSeveral CAT systems have been proposed in theTransType projects (SchlumbergerSema S.A.
etal., 2001):In (Foster et al, 2002) a maximum entropy ver-sion of IBM2 model is used as translation model.It is a very simple model in order to achieve rea-839English/Spanish English/German English/FrenchTrain Sent.
pairs (K) 56 49 53Run.
words (M) 0.6/0.7 0.6/0.5 0.6/0.7Vocabulary (K) 26/30 25/27 25/37Test Sent.
pairs (K) 1.1 1.0 1.0Run.
words (K) 8/9 9/10 11/10Perplexity 107/60 93/169 193/135Table 1: Statistics of the ?XRCE?
corpora English to/from Spanish, German and French.
Trigram modelswere used to compute the test perplexity.WER CER WSR KSREnglish/Spanish 31.1 21.7 28.7 8.9Spanish/English 34.9 24.7 30.0 9.3English/French 61.6 49.2 60.7 22.6French/English 58.0 48.2 61.6 22.2English/German 68.0 56.9 67.6 25.6German/English 59.5 50.6 62.0 21.9Table 5: Comparison of post-editing effort inMT scenario (WER/CER) and the interactive-editing effort in CAT scenario (WSR/KSR).
Non-monotone search and 1-best hypothesis is used.sonable interaction times.
In this approach, thelength of the proposed extension is variable infunction of the expected benefit of the humantranslator.In (Och et al, 2003) the Alignment-Templatestranslation model is used.
To achieve fast responsetime, it proposes to use a word hypothesis graph asan efficient search space representation.
This wordgraph is precalculated before the user interactions.In (Civera et al, 2004) finite state transduc-ers are presented as a candidate technology in theCAT paradigm.
These transducers are inferred us-ing the GIATI technique (Casacuberta and Vidal,2004).
To solve the real-time constraints a wordhypothesis graph is used.
The N -best configura-tion is proposed.In (Bender et al, 2005) the use of a word hy-pothesis graph is compared with the direct use ofthe translation model.
The combination of twostrategies is also proposed.8 ConclusionsPhrase-based models have been used for interac-tive CAT in this work.
We show how SMT can beused, with slight adaptations, in a CAT system.
Aprototype has been developed in the framework ofthe TransType2 project (SchlumbergerSema S.A.et al, 2001).The experimental results have proved that thesystems based on such models achieve a good per-formance, possibly, allowing a saving of humaneffort with respect to the classical post-editing op-eration.
However, this fact must be checked byactual users.The main critical aspect of the interactive CATsystem is the response time.
To deal with this is-sue, other proposals are based on the constructionof a word graphs.
This method can reduce the gen-eration capability of the fully fledged translationmodel (Och et al, 2003; Bender et al, 2005).
Themain contribution of the present proposal is a newdecoding algorithm, that combines monotone andnon-monotone search.
It runs fast enough and theconstruction of word graph is not necessary.AcknowledgmentsThis work has been partially supported by theSpanish project TIC2003-08681-C02-02 the ISTProgramme of the European Union under grantIST-2001-32091.
The authors wish to thank theanonymous reviewers for their criticisms and sug-gestions.ReferencesS.
Barrachina, O. Bender, F. Casacuberta, J. Civera,E.
Cubel, S. Khadivi, A. Lagarda, H. Net, J. Toma?s,E.Vidal, and J.M.
Vilar.
2006.
Statistical ap-proaches to computer-assisted translation.
In prepa-ration.O.
Bender, S. Hasan, D. Vilar, R. Zens, and H. Ney.2005.
Comparison of generation strategies for inter-active machine translation.
In Proceedings of EAMT2005 (10th Annual Conference of the European As-sociation for Machine Translation), pages 30?40,Budapest, Hungary, May.840A.
L. Berger, P. F. Brown, S. A. Della Pietra, V. J. DellaPietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.1996.
Language translation apparatus and methodof using context-based translation models.
UnitedStates Patent, No.
5510981, April.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?311.F.
Casacuberta and E. Vidal.
2004.
Machine transla-tion with inferred stochastic finite-state transducers.Computational Linguistics, 30(2):205?225.J.
Civera, J. M. Vilar, E. Cubel, A. L. Lagarda, S. Bar-rachina, E. Vidal, F. Casacuberta, D.
Pico?, andJ.
Gonza?lez.
2004.
From machine translation tocomputer assisted translation using finite-state mod-els.
In Proceedings of the 2004 Conference on Em-pirical Methods in Natural Language Processing(EMNLP04), Barcelona, Spain.G.
Foster, P. Isabelle, and P. Plamondon.
1996.
Wordcompletion: A first step toward target-text mediatedIMT.
In COLING ?96: The 16th Int.
Conf.
on Com-putational Linguistics, pages 394?399, Copenhagen,Denmark, August.G.
Foster, P. Langlais, and G. Lapalme.
2002.
User-friendly text prediction for translators.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP02), pages 148?155, Philadelphia, USA, July.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Human Language Tech-nology and North American Association for Com-putational Linguistics Conference (HLT/NAACL),pages 48?54, Edmonton, Canada, June.E Macklovitch.
2004.
The contribution of end-usersto the transtype2 project.
volume 3265 of Lec-ture Notes in Computer Science, pages 197?207.Springer-Verlag.D.
Marcu and W. Wong.
2002.
A phrase-based jointprobability model for statistical machine transla-tion.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, Philadel-phia, USA, July.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In Proc.
of the 38th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 440?447, Hong Kong, October.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51, March.F.
J. Och, R. Zens, and H. Ney.
2003.
Efficient searchfor interactive statistical machine translation.
InProceedings of the 10th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL), pages 387.?393, Budapest, Hun-gary, April.F.
J. Och.
2002.
Statistical Machine Translation:From Single-Word Models to Alignment Templates.Ph.D.
thesis, Computer Science Department, RWTHAachen, Germany, October.SchlumbergerSema S.A., Intituto Tecnolo?gico de In-forma?tica, Rheinisch Westfa?lische TechnischeHochschule Aachen Lehrstul fu?r Informatik VI,Recherche Applique?e en Linguistique InformatiqueLaboratory University of Montreal, Celer Solu-ciones, Socie?te?
Gamma, and Xerox Research CentreEurope.
2001.
TT2.
TransType2 - computerassisted translation.
Project technical annex.J.
Toma?s and F. Casacuberta.
2001.
Monotone statis-tical translation using word groups.
In Procs.
of theMachine Translation Summit VIII, pages 357?361,Santiago de Compostela, Spain.J.
Toma?s and F. Casacuberta.
2004.
Statistical machinetranslation decoding using target word reordering.In Structural, Syntactic, and Statistical Pattern Re-congnition, volume 3138 of Lecture Notes in Com-puter Science, pages 734?743.
Springer-Verlag.J.
Toma?s, J. Lloret, and F. Casacuberta.
2005.Phrase-based alignment models for statistical ma-chine translation.
In Pattern Recognition and Im-age Analysis, volume 3523 of Lecture Notes in Com-puter Science, pages 605?613.
Springer-Verlag.R.
Zens, F. J. Och, and H. Ney.
2002.
Phrase-basedstatistical machine translation.
Advances in Artifi-cial Inteligence, LNAI 2479(25):18?32, September.841
