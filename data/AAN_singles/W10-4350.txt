Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 265?268,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsMPOWERS: a Multi Points Of VieW Evaluation Refinement StudioMarianne Laurent, Philippe BretierOrange LabsLannion, France{marianne.laurent, philippe.bretier} @orange-ftgroup.comAbstractWe present our Multi Point Of vieW Eval-uation Refinement Studio (MPOWERS),an application framework for Spoken Di-alogue System evaluation that implementsdesign conventions in a user-friendly in-terface.
It ensures that all evaluator-usersmanipulate a unique shared corpus of datawith a shared set of parameters to de-sign and retrieve their evaluations.
Ittherefore answers both the need for con-vergence among the evaluation practicesand the consideration of several analyti-cal points of view addressed by the evalu-ators involved in Spoken Dialogue Systemprojects.
After introducing the system ar-chitecture, we argue the solution?s addedvalue in supporting a both data-driven andgoal-driven process.
We conclude with fu-ture works and perspectives of improve-ment upheld by human processes.1 IntroductionThe evaluation of Spoken Dialogue Systems(SDS) is a twofold issue.
On the one hand, the lackof convention on evaluation criteria and the manydifferent evaluation needs and situations alongwith SDS projects lead to nomadic evaluation set-ups and interpretations.
We inventoried seven jobfamilies contributing to these projects: the market-ing people, the business managers, the technicaland ergonomics experts, the hosting providers, thecontracting owners as well as the actual human op-erators which integrate SDS in their activity (Lau-rent et al, 2010).
Various experimental proto-cols for data collection and analytical data pro-cessing flourish in the domain.
On the other hand,however they may not share evaluation needs andmethods, the various potential evaluators need tocooperate inside and across projects.
This claimsfor a convergence of evaluation practices towardstandardized methodologies.
The domain has puta lot of efforts toward the definition of commensu-rable metrics (Paek, 2007) for comparative evalu-ations and improved transparency over communi-cations on systems?
performances.Nonetheless, we believe that no one-size-fits-allsolution may cover all evaluation needs (Laurentand Bretier, 2010).
We therefore work onto therationalization - not the standardization - of eval-uation practices.
By rationalization, we refer to thedefinition of common norms to describe the eval-uation protocols; common thinking models andvocabulary, for evaluators to make their proce-dures explicit.
Our Multi Points Of VieW Evalu-ation Refinement Studio (MPOWERS) facilitatesthe design, from a unique corpus of parameters, ofpersonalized evaluations adapted to the particularcontexts.
It does not compete with workbencheslike MeMo (Mo?ller et al, 2006) or WITcHCRafT(Schmitt et al, 2010) for which the overall evalu-ation process is predefined within the tool.The following section details the solution archi-tecture.
Then, we present the MPOWERS?s pur-poses, emphasizing on its added value for evalua-tors.
Last, we explain the technical and process-related aspects that must support the system.2 Architecture of the systemThe application is built on a classical Business In-telligence (BI) solution that aims to provide de-cision makers with personalized information (SeeFig.
1).
We store, in a single datamart, param-eters retrieved from heterogeneous sources: inter-action logs, user questionnaires and third-party an-notations relative to the evaluation campaigns ar-ranged on the evaluated system(s).
Then, data arecleaned, transformed and aggregated into Key Per-formance Indicators (KPIs).
It guarantees that theindicators used across teams and projects are de-fined, calculated and maintained in the same place.265Figure 1: The MPOWERS architectureOn the upper layer, evaluators define and retrievepersonalized reports and dashboards.We use the Let?s Go!
System corpus shared bythe Carnegie Mellon University.
It contains logfiles generated since from 2003 from the Pitts-burgh?s telephone-based bus information systemlog files, one per module composing the system,and a summary HTML file.
At our stage of theproject the html summary allows the calculationof a satisfying number of parameters to supportthe system development and refinement.
We com-pute the dialogue duration, the number of systemand user turns, the number of barge-ins, the ratiobetween user and system turn number, the numberof help requests and of no-matches per call and theratio of successful interactions.The application relies on the SpagoBI 2.6 opensource solution1.
Once parametrized, it enablesnon-technical stakeholders to retrieve personal-ized KPIs reports based on shared resources.
Fornow, it delivers basic dashboards for two userprofiles.
One focuses on the service monitoringfor marketing people and business managers andthe other one provides the development team withusability-related performance figures (see fig.
4).The unique datamart guarantees all users to workfrom similar data.
Its population requires parsingroutines to identify and extract the relevant data.3 Evaluation process and added valueBy automating tractable tasks, MPOWERS sup-ports the evaluator-users in their evaluation pro-cess driven by decision-making objectives.
Assketched in figure 2, our application-supportedprocess is slightly modified from the one definedby Stufflebeam (1980): a process through whichone defines, obtains and delivers useful pieces ofinformation that enable to settle between the alter-1http://www.spagoworld.org/native possible decisions.Figure 2: Evaluation process with MPOWERS(grey-tinted stages are supported by the system)Custom-made Python2 routines enable to ex-tract relevant data from the log files.
They provideCSV3 formatted files to be converted into SQLscripts.
The datamart is designed to be graduallypopulated from successive evaluation campaignson one or several SDS.
As data may originatesfrom diverse sources, it arrays in different formatsand often displays different parameters.
Adaptedad hoc routines permit the manipulation into con-sistent format.
We anticipate the use of separate ta-bles in the datamart from comparative evaluationsons distinct systems.The retrieval of KPIs in SpagoBI requiresdatasets pre-parametrized over SQL-Queries.They describe the SDS?s performance and be-haviour.
We defined the parameters relative tothe system performance according to the ITU-TRec.
P.Sup24 (2005).
Yet, unless input corpora aredefined accordingly not all the recommendation?sparameters can be implemented.
Three modes todisplay these datasets are proposed to evaluators:?
A summary of high-level KPIs provides ageneral view on the evaluated system with?red-light indicators?
(see fig.
3).
Links tomore detailed charts or analysis tools are dis-played next to each of them.2http://www.python.org/3Comma-Separated Value266Figure 3: High-level KPIs with link to more detailed documents.
Please note that the success ratio iscalculated via an ad-hoc query and does not necessarily corresponds to the user being or not satisfied.Figure 4: Dashboard dedicated to a high-level view on usability performance.267?
Visual dashboards display pre-processed dataaccording to pre-defined evaluation profiles(see fig.
4).?
Tools for in-depth individual analysis Fil-tered queries permit evaluators to individu-ally adjust their analysis according to localevaluation objectives.
Queries can be storedfor later use or saved in PDF documents fordistribution to non-MPOWERS users.End-users, i.e.
the evaluators, are limited to dis-play the results and proceed to in-depth queries.An administrator access allows for prior data pro-cessing and the configuration of datasets, KPIs anddashboards.
With collaborative enhancement pur-poses, the application supports communication be-tween users with built-in discussion threads infor-mation feeds and shared to-do-lists to suggest andnegotiate future configurations.These distinct outlooks on the corpus arecomplementary.
They combine a high-levelview on the service?s behaviour and performancewith detailed personalised analysis.
Whatevertheir layouts, every information displayed to theevaluators-users is retrieved from a unique corpusand from the same SQL-queries.
Therefore, evenif all evaluators consider distinct features on theevaluated service, our framework brings consis-tency to their evaluation practices.4 Future workMPOWERS is on its first development stages.Several perspectives of enhancement are planned.First, it requires to be augmented with more KPIsand in-depth analytical features.
Second, as itonly manipulates automated log files, user ques-tionnaires and third-party annotations are expectedto enrich its evaluation possibilities.
Third, we in-tent MPOWERS to perform comparative evalua-tions between distinct services in the future.
Andlast, the framework would benefit from being em-ployed within real evaluators?
daily activity.5 ConclusionThe paper presents a platform that supports theSDS project stakeholders in their evaluation task.While advocating for a rationalization of evalua-tion practices among project teams and across or-ganizations, it promotes the existence of differentcohabiting points of view instead of disregardingthem.
When most evaluation contributions coverthe overall evaluation process, from experimentaldata collection set-ups to guidance for interpreta-tion, we limit to a user-centric framework, whereevaluators remain in charge of the evaluation de-sign.
We actually provide them with an opera-tional framework and unified tools to design andprocess their evaluations.
This may help initiateindividual, as well as community-wide, gradualrefinements of methodologies.AcknowledgmentsThe demo makes use of the Let?s Go!
log filesprovided by the Carnegie Mellon University.
Wethank Telecom Bretagne, Q. Jin, X. Chen, S.Zarrad, F. Agez and A. Bolze for their contribu-tion in the platform deployment.ReferencesM.
Eskenazi, A. W. Black, A. Raux, and B. Langner.2008.
Let?s Go Lab: a platform for evaluation ofspoken dialog systems with real world users.
In In-terspeech 2008, Brisbane, Australia.M.
Laurent and P. Bretier.
2010.
Ad-hoc evaluationsalong the lifecycle of industrial spoken dialogue sys-tems: heading to harmonisation?
In LREC 2010,Malta.M.
Laurent, I. Kanellos, and P. Bretier.
2010.
Con-sidering the subjectivity to rationalise evaluation ap-proaches: the example of Spoken Dialogue Systems.In QoMEx?10, Trondheim, Norway.S.
Mo?ller, R. Englert, K.-P. Engelbrecht, V. Hafner,A.
Jameson, A. Oulasvirta, A. Raake, and N. Rei-thinger.
2006.
MeMo: towards automatic usabilityevaluation of spoken dialogue services by user error.9th International Conference on Spoken Language.T.
Paek.
2007.
Toward evaluation that leads to bestpractices: reconciling dialog evaluation in researchand industry.
In Workshop on Bridging the Gap:Academic and Industrial Research in Dialog Tech-nologies, pages 40?47, New York.
ACL, Rochester.ITU-T Rec.
P.Sup24.
2005.
Parameters describing theinteraction with spoken dialogue systems.A.
Schmitt, G. Bertrand, T. Heinroth, W. Minker, andJ.
Liscombe.
2010.
Witchcraft: A workbench forintelligent exploration of human computer conver-sations.
In LREC 2010, Malta.D.
L. Stufflebeam.
1980.
L?e?valuation en e?ducation etla prise de de?cision.
Ottawa.268
