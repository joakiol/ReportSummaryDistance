Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 31?37,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsMeasuring the Productivity of Determinerless PPsFlorian Do?mges, Tibor Kiss, Antje Mu?ller, Claudia RochSprachwissenschaftliches InstitutRuhr-Universita?t Bochumflorian.doemges@rub.detibor@linguistics.rub.deantje.mueller@rub.declaudia.roch@rub.deAbstractWe determine the productivity of determin-erless PPs in German quantitatively, restrict-ing ourselves to the preposition unter.
Thestudy is based on two German newspa-per corpora, comprising some 210 millionwords.
The problematic construction, i.e.unter followed by a determinerless singularnoun occurs some 16.000 times in the cor-pus.
To clarify the empirical productivityof the construction, we apply a productivitymeasure developed by Baayen (2001) to thesyntactic domain by making use of statisti-cal models suggested in Evert (2004).
Wecompare two different models and suggest agradient descent search for parameter esti-mation.
Our results show that the combina-tion of unter+noun must in fact be character-ized as productive, and hence that a syntactictreatment is required.Kiss (2006),Kiss (2007),Li (1992), Zipf (1949)1 IntroductionThe combination of a preposition with a singularcount noun, illustrated in (1) with the prepositionunter, is a frequent construction in written and spo-ken German.
From a theoretical perspective, con-structions like (1) are interesting since they seemto violate the near universal rule that determinersshould accompany singular count nouns if the lan-guage in question shows determiners at all (cf.
Him-melmann (1998)).unter Vorbehalt (with reservation),(1)unter Androhung (on pain),unter Lizenz (under licence),unter Vorwand (pretending)Baldwin et al (2006) follow a tradition of En-glish grammar and call constructions like (1) deter-minerless PPs (D-PP), defined as PPs whose NP-complement consists of a singular count noun with-out an accompanying determiner (as e.g.
Englishby bus, in mind ).
It has been claimed that D-PPsare mostly idiomatic and not productive.
Hence,computational grammars often include D-PPs onlyas stock phrases or listed multiword expressions anddo not offer a grammatical treatment.
However, bothclaims have to be doubted seriously.
Kiss (2006,2007) shows that the class of D-PPs does not con-tain more idiomatic phrases than a typical phrasalcategory should and also argues against a ?light Phypothesis?
which allows a pseudo-compositionaltreatment of D-PPs by ignoring the semantics of thepreposition altogether.
Trawinski (2003), Baldwinet al (2006), as well as Trawinski et al (2006) offergrammatical treatments of D-PPs, or at least of somesubclasses of D-PPs.
Interestingly, (Baldwin et al(2006), 175f.)
take the productivity of a subclassof D-PPs for granted and propose a lexical entry forprepositions which select determinerless N?s as theircomplement.
While we are sympathetic to a syn-tactic treatment of D-PPs in a computational gram-mar, we think that the productivity of such construc-tions must be considered more closely.
The analysisof Baldwin et al (2006) allows the unlimited com-bination of prepositions meeting their lexical spec-ification with a determinerless N projection.
This31assumption is not in line with speaker?s intuitionswith regard to producing or judging such construc-tions.
As has been pointed out by Kiss (2006, 2007),speakers of German can neither freely produce se-quences consisting of unter and determinerless Nprojections (typically a noun) nor can they judgesuch constructions in isolation.
In addition, not evenvery similar nouns can be interchanged in a D-PP,as can be witnessed by comparing near-synonymsVoraussetzung and Pra?misse which both translate asprerequisite, or as provided in the examples in (2).The examples in (2) illustrate that Voraussetzungcannot be replaced by Pra?misse in a D-PP (2a, b),while it can be replaced as a head noun in a fullPP (2c, d).
While the contrast in (2) casts doubt ona productive analysis on the basis of the speakersknowledge of language, the present paper will showthat unter+noun has to be classified as productivefrom an empirical perspective.a.
Auch Philippe Egli besteht auf einer(2)eigenen Handschrift - unterVoraussetzung des Einversta?ndnissesdes Ensembles.b.
* Auch Philippe Egli besteht auf einereigenen Handschrift - unter Pra?missedes Einversta?ndnisses des Ensembles.c.
Auch Philippe Egli besteht auf einereigenen Handschrift - unter derVoraussetzung des Einversta?ndnissesdes Ensembles.d.
Auch Philippe Egli besteht auf einereigenen Handschrift - unter derPra?misse des Einversta?ndnisses desEnsembles.
?Philippe Egli insists on his individual wayof dealing with the issue, provided theensemble agrees.
?Our investigation is based of a corpus analysis ofD-PPs, consisting of the preposition unter and a fol-lowing noun, and employs a quantitative measureof productivity, first developed by Harald Baayento analyze morphological productivity.
The pre-liminary conclusion to be drawn from this resultwill be that empirical and intuitive productivity ofunter+noun sequences do not match.In applying Baayen?s productivity measure tosyntactic sequences, however, we are faced witha serious problem.
Baayen?s productivity measureP (N) is based on the expectation of the hapaxlegomena ?
E[V1] ?
occurring in a vocabulary ofsize N, i.e.
P (N) = E[V1]N .1101001  10  100  1000Cardinalities of the frequency classesFigure 1: Cardinalities of the frequency classes.
Thefrequency of each type was counted, then the typeswere grouped into classes of equal frequency.
Thenumber of types in each class was counted.
The fre-quency values m are assigned to the x-axis, the sizeof the class Vm to the y-axis.
Both are scaled loga-rithmically.Since we cannot derive the expectation of the ha-pax legomena directly from the corpus, we have toapproximate it by use of regression models.
To sim-plify matters somewhat, Baayen?s models can onlybe applied to unigrams, while we have to considerbigrams ?
the preposition and the adjacent noun.
Tocircumvent this problem, Kiss (2006,2007) calcu-lated P (N) on the basis of the empirical distribu-tion of V1 as N gets larger.
Evert (2004) offers re-gression models to determine E[V1] for n-grams andsuggests two different models, the Zipf-Mandelbrot32model (ZM) and the finite Zipf-Mandelbrot model(fZM).
The difference between these two modelsis that fZM assumes a finite vocabulary.
In thepresent paper, we apply Evert?s models to sequencesof unter+noun.
We differ from Evert?s proposal inestimating the free parameter ?
in both models onthe basis of the gradient descent algorithm.
Contraryto Evert?s assumptions, we will show that the resultsof the ZM model are much closer to the empiricalobservations than the results of the fZM model.The paper is structured as follows.
Section 2 de-scribes the empirical basis of the experiment, a cor-pus study of unter+textnounsg sequences.
Section3 introduces the models suggested by Evert (2004).Section 3.1 introduces the models, section 3.2 showshow the free parameter is estimated by making useof the gradient descent algorithm.
The results arecompared in section 3.3.2 Corpus StudyThe present study is based on two German corpora,with a total of 213 million words: the NZZ-corpus1995-1998 (Neue Zu?rcher Zeitung) and the FRR-corpus 1997-1999 (Frankfurter Rundschau).
Mak-ing use of the orthographic convention that nounsare capitalized in German, we have automaticallyextracted 12.993 types, amouting to some 71.000tokens of unter and a following noun.
From these12.993 types, we have removed all candidates wherethe noun is a proper noun, or realized as a plural,or as member of a support verb construction.
Also,we have excluded typical stock phrases and all massnouns.
The extraction process was done both man-ually (proper nouns, mass nouns, support verb con-structions) and automatically (plurals, mass nouns).As a result of the extraction process, a total num-ber of 1.103 types remained, amounting to 16.444tokens.
The frequency of every type was determinedand types with the same frequency were groupedinto classes.
65 equivalence classes were establishedaccording to their frequency m (cf.
Figure 1).
Thenumber of elements in every class was counted andthe various count results were associated with thevariables Vm = V1, V2, .
.
.
, V2134.3 LNRE Model RegressionBaayen (2001) uses the term LNRE models (largenumber of rare events) to describe a class of mod-els that allow the determination of the expectationwith a small set of parameters.
Evert (2004) pro-poses two LNRE models with are based on Zipf?sLaw (Zipf(1949), Li (1992)) to identify the expec-tations E[V1], .
.
.
, E[Vmax].
Both models are basedon the Zipf-Mandelbrot law.Zipf?s Law (Zipf(1949), Li (1992)) posits that thefrequency of the r-most frequent type is proportionalto 1r .
The distribution of random texts displays astrong similarity to the results expected according toZipf?s Law (cp.
Li (1992)).
Mandelbrot (1962) etal.
explain this phenomenon by Zipf?s Principle ofLeast Effort.Rouault (1978) shows that the probability of typeswith a low frequency asymptotically behaves asposited by the Zipf-Mandelbrot Law?i =C(i + b)awith a > 1 and b > 0.The models are introduced in section 3.1.
Bothrequire a parameter ?, whose value was determinedby employing a gradient descent algorithm imple-mented in Perl.
The optimal value for the free pa-rameter was found by constructing an error functionto minimise ?.
The calculation was carried out forboth models, but better results are produced if theassumption is given up that the vocabulary is finite.3.1 Finite and general Zipf-Mandelbrot modelsEvert (2004) proposes the finite Zipf-Mandelbrotmodel (fZM) and the general Zipf-Mandelbrotmodel (ZM) for modelling the expectations of thefrequency classes Vm, i.e.
E[V1], .
.
.
, E[Vmax] andthe expected vocabulary size, i.e.
the expectationof the different types E[V ].
The two models makedifferent assumptions about the probability distribu-tions of the frequency classes.
The fZM assumesthat there is a minimal probability A ?
defined as?A : ?i : A ?
?i.
This amounts to the assumptionthat the vocabulary size itself is finite.
Hence, it canbe expected according to the fZM model that the setof observed types does not increase once N ?
1A isreached.
In the general ZM model, there is no suchminimal probability.Assuming a fZM model, Evert (2004) proposesthe following results to estimate the expectation of33the frequency classes E[Vm] and the expected vo-cabulary size E[V ].
In the following equations,B stands for the maximum probability, defined as?i : B ?
?i.E[Vm] =1 ?
?(B1??
?A1??)
?m!
?N?
?
?(m?
?,N ?
A) (3)E[V ] = 1 ?
?(B1??
?A1??)
?N?
?
?
(1 ?
?,N ?A)?
+1 ?
?(B1??
?A1??)
?
?
?
A?
?
(1 ?
e?N ?A) (4)As can be witnessed from the formulae given, N ,A, and B are already known or directly derivablefrom our observations, leaving us with the determi-nation of the free parameter ?.Using the general Zipf-Mandelbrot model, we endwith the following estimations, again suggested byEvert (2004):E[Vm] =1 ?
?B1??
?m!
?N?
?
?(m?
?)
(5)E[V ] = 1 ?
?B1??
?N?
?
?
(1 ?
?)?
(6)As there is no minimal probability, we are leftwith the maximal probability B, the token size N,and again a free parameter ?.3.2 Parameter estimation through gradientdescentSince the expectation of the frequency classes in (3)and (5) depend on the free parameter ?, this pa-rameter must be estimated in a way that minimisesthe deviation of expected and observed values.
Wemeasure the deviation with a function that takes intoaccount all observed frequencies and their expectedvalues.
A function satisfying these criteria can befound by treating observed frequency classes and ex-pectations as real-valued vectors in a vector space.OT = (V, V1, V2, .
.
.
, V2134) ?
R66 (7)ET (?)
=(E(V )(?
), E(V1)(?
), .
.
.
, E(V2134)(?))
?
R66 (8)1101001  10  100  1000Cardinalities of the frequency classesFigure 2: The application of the fZM LNRE Modelcombined with Rouault?s estimation method leads toa strong deviation from the observed data.
The ob-served data is depicted as a solid line, the data fromthe model as a dotted line.
The frequency values mare assigned to the x-axis, the size of the class Vmrespectively the expected size E(Vm) to the y-axis.Both are scaled logarithmically.A natural choice for a measure of error is thequadratic norm of the difference vector between ob-servation and expectation.
As we have no infor-34mation about the relationship between different fre-quencies we assume that the covariance matrix is theunit matrix.These thoughts result in the following error func-tion:g(?)
= (E(V )(?)
?
V )2+?m=1,...,2134(E(Vm)(?)
?
Vm)2 (9)The minimal ?
is equal to the root of the deriva-tive of the error function with respect to ?.
Thederivative of the error function is:?g??
= 2?E(V )??
(E(V )(?)
?
V )+2?m=1,...,2134?E(Vm)??
(E(Vm)(?)
?
Vm) (10)One way to find the minimum ??
=argmin?
g(?)
would be to derive the expectedvalues with respect to ?
and solve g?(??)
= 0 for?.
As there is no way known to the authors toaccomplish this in a symbolic way, the use of anumeric method to calculate ??
is advised.We chose to find ??
by employing a gradient de-scent method and approximating ?g??
by evaluatingg(?)
in small steps ??
(i) and calculating ?g(k)??
(k) =g(?0+Pkj=1 ??
(j))?g(?0+Pk?1j=1 ??(j))??
(k) , where k is num-ber of the iteration.In the vicinity of a minimum ?g??(?)
decreases un-til it vanishes at ?
?.After every iteration the new ??
(k) is chosen bytaking under consideration the change of ?g(k)??
(k) andthe sign of ??(k?
1).
If ?g(k)??
(k) increased, the sign of??
(k ?
1) is inverted: ??
(k) = ???
(k ?
1).To prevent the algorithm from oscillat-ing around the minimum the last two valuesg(?0 +?k?2j=1 ??
(j)) and g(?0 +?k?1j=1 ??
(j)) aresaved.When a step would result in returning to a previ-ous value g(?0 +?k?1j=1 ??
(j) + ??
(k)) = g(?0 +?k?2j=1 ??
(j)), the step size is multiplied by a con-stant 0 < ?
?
1: ??
(k) = ???
(k ?
1).
The al-gorithm is stopped when the absolute value of thestep size drops under a predetermined threshold:|??
(k)| < ?threshold.3.3 ResultsInterestingly, ?
as determined by gradient descenton the basis of a fZM leads to a value of 0.666,which does not match well with our observations,as can be witnessed in Figure 2.1101001  10  100  1000Cardinalities of the frequency classesFigure 3: The ZM LNRE Model leads to a far betterresult with less deviation from the observation.
Theobserved data is depicted as a solid line, the datafrom the model as a dotted line.
The frequency val-ues m are assigned to the x-axis, the size of the classVm respectively the expected size E(Vm) to the y-axis.
Both are scaled logarithmically.A gradient descent search on the basis of the ZMmodel delivered a value of ?
= 0.515, a much betterapproximation (with a ?2-Value of 4.514), as can be35witnessed from Figure 3.
The value thus reachedalso converges with the estimation procedure for ?suggested by Rouault (1978), and taken up by Evert(2004), i.e.
?
= V1V .
Consequently, we assume aZM model for estimating of expected frequencies.00.050.10.150.20  0.2  0.4  0.6  0.8  1Estimated ProductivityObserved ProductivityFigure 4: The parts of the corpus were appendedto each other and after every step the productivityP (N) was calculated directly from the data as wellas from the fitted ZM model.
The percentage ofthe corpus is assigned to the x-axis, the productiv-ity P (N) is assigned to the y-axis.
The productivityvalues that were deduced directly from data are plot-ted as a dotted line, the productivity values from theZM model are plotted as a solid line.To chart the productivity of sequences of the formunter+noun, we have divided our corpus into sixsmaller parts and sampled V , N , and V1 at theseparts.
The distribution of the observations thusgained can be found in Figure 4, together with theexpectations derived from the ZM model.
We ob-serve that both distributions are strikingly similarand converge at the values for the full corpus.N V1 E[V1] P (N)542 74 96.66 0.1821068 104 123.47 0.1182151 169 166.41 0.0794262 282 249.93 0.0596222 384 332.19 0.0548365 469 400.43 0.04816444 746 748.81 0.022Table 1: Overview of the observed and expectednumbers of hapax legomena and the associated pro-ductivity value at different corpus sizes.In a broader perspective, Figure 4 shows that thecombination of unter+noun is a productive process,when its empirical distribution is considered.
Aswas already pointed out in section 1, this findingis at odds with speaker?s intuitions about combina-tions of unter+noun.
Assuming that this result canbe extended to other subclasses of D-PPs, we wouldsuggest restricting lexical specifications for preposi-tions to subclasses of nouns, depending on the perti-nent preposition.
Future research will have to showwhether such clear-cut subclasses can be identifiedby looking more closely at the empirical findings,other whether we are confronted with a continuum,which would require alternative rule types.ReferencesHarald Baayen.
2001.
Word Frequency Distributions.Kluwer, Dordrecht.Timothy Baldwin, John Beavers, Leonoor van der Beek,Francis Bond, Dan Flickinger, and Ivan A.
Sag.
2006.In Search of a Systematic Treatment of DeterminerlessPPs.
In Patrick Saint-Dizier, editor, Syntax and Se-mantics of Prepositions, pages 163?179.
Springer.Stefan Evert.
2004.
A Simple LNRE Model for Ran-dom Character Sequences.
In Proceedings of the7mes Journees Internationales d?Analyse Statistiquedes Donnees Textuelles, pages 411?422.Nikolaus Himmelmann.
1998.
Regularity in Irregular-ity: Article Use in Adpositional Phrases.
LinguisticTypology, 2:315?353.Tibor Kiss.
2006.
Do we need a grammar of irregularsequences?
In Miriam Butt, editor, Proceedings ofKONVENS, pages 64?70, Konstanz.36Tibor Kiss.
2007.
Produktivita?t und Idiomatizita?tvon Pra?position-Substantiv-Sequenzen.
forthcomingin Zeitschrift fu?r Sprachwissenschaft.W.
Li.
1992.
Random texts exhibit zipf?s-law-like wordfrequency distribution.
IEEE Transactions on Infor-mation Theory.B.
Mandelbrot.
1962.
On the theory of word frequenciesand on related Markovian models of discourse.
Amer-ican Mathematical Society.A.
Rouault.
1978.
Lois de Zipf et sources markoviennes.Annales de l?Institut H. Poincare.Beata Trawinski, Manfred Sailer, and Jan-Philipp Soehn.2006.
Combinatorial Aspects of Collocational Prepo-sitional Phrases.
In Patrick Saint-Dizier, editor, Syn-tax and Semantics of Prepositions, pages 181?196.Springer.Beata Trawinski.
2003.
The Syntax of Complex Preposi-tions in German: An HPSG Approach.
In Proceedingsof GLIP, volume 5, pages 155?166.G.
K. Zipf.
1949.
Human Behavior and the Principle ofLeast Effort.
Addison-Wesley, Campridge.37
