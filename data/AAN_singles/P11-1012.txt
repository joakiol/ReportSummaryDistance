Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 112?122,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsQuery Weighting for Ranking Model AdaptationPeng Cai1, Wei Gao2, Aoying Zhou1, and Kam-Fai Wong2,31East China Normal University, Shanghai, Chinapengcai2010@gmail.com, ayzhou@sei.ecnu.edu.cn2The Chinese University of Hong Kong, Shatin, N.T., Hong Kong{wgao, kfwong}@se.cuhk.edu.hk3Key Laboratory of High Confidence Software Technologies, Ministry of Education, ChinaAbstractWe propose to directly measure the impor-tance of queries in the source domain to thetarget domain where no rank labels of doc-uments are available, which is referred toas query weighting.
Query weighting is akey step in ranking model adaptation.
Asthe learning object of ranking algorithms isdivided by query instances, we argue thatit?s more reasonable to conduct importanceweighting at query level than document level.We present two query weighting schemes.The first compresses the query into a queryfeature vector, which aggregates all documentinstances in the same query, and then con-ducts query weighting based on the query fea-ture vector.
This method can efficiently esti-mate query importance by compressing querydata, but the potential risk is information lossresulted from the compression.
The secondmeasures the similarity between the sourcequery and each target query, and then com-bines these fine-grained similarity values forits importance estimation.
Adaptation exper-iments on LETOR3.0 data set demonstratethat query weighting significantly outperformsdocument instance weighting methods.1 IntroductionLearning to rank, which aims at ranking documentsin terms of their relevance to user?s query, has beenwidely studied in machine learning and informationretrieval communities (Herbrich et al, 2000; Fre-und et al, 2004; Burges et al, 2005; Yue et al,2007; Cao et al, 2007; Liu, 2009).
In general,large amount of training data need to be annotatedby domain experts for achieving better ranking per-formance.
In real applications, however, it is timeconsuming and expensive to annotate training datafor each search domain.
To alleviate the lack oftraining data in the target domain, many researchershave proposed to transfer ranking knowledge fromthe source domain with plenty of labeled data to thetarget domain where only a few or no labeled data isavailable, which is known as ranking model adapta-tion (Chen et al, 2008a; Chen et al, 2010; Chen etal., 2008b; Geng et al, 2009; Gao et al, 2009).Intuitively, the more similar an source instanceis to the target instances, it is expected to be moreuseful for cross-domain knowledge transfer.
Thismotivated the popular domain adaptation solutionbased on instance weighting, which assigns largerweights to those transferable instances so that themodel trained on the source domain can adapt moreeffectively to the target domain (Jiang and Zhai,2007).
Existing instance weighting schemes mainlyfocus on the adaptation problem for classification(Zadrozny, 2004; Huang et al, 2007; Jiang and Zhai,2007; Sugiyama et al, 2008).Although instance weighting scheme may be ap-plied to documents for ranking model adaptation,the difference between classification and learning torank should be highlighted to take careful consider-ation.
Compared to classification, the learning ob-ject for ranking is essentially a query, which con-tains a list of document instances each with a rel-evance judgement.
Recently, researchers proposedlistwise ranking algorithms (Yue et al, 2007; Caoet al, 2007) to take the whole query as a learningobject.
The benchmark evaluation showed that list-112Target domainSource Domaind1(s1) d2(s1)d3(s1)d1(s2)d2(s2)d3(s2)d2(t1)d1(t2)d2(t2) d3(t2)d3(t1)d1(t1)(a) Instance based weightingd2(s1)d1(s1)d3(s1)d1(s2)d2(s2)d3(s2)qs2qs1d3(t1)d2(t1)d1(t1)d1(t2)d2(t2)d3(t2)qt1qt2Target domainSource Domain(b) Query based weightingFigure 1: The information about which document instances belong to the same query is lost in document instanceweighting scheme.
To avoid losing this information, query weighting takes the query as a whole and directly measuresits importance.wise approach significantly outperformed pointwiseapproach, which takes each document instance as in-dependent learning object, as well as pairwise ap-proach, which concentrates learning on the order ofa pair of documents (Liu, 2009).
Inspired by theprinciple of listwise approach, we hypothesize thatthe importance weighting for ranking model adapta-tion could be done better at query level rather thandocument level.Figure 1 demonstrates the difference between in-stance weighting and query weighting, where thereare two queries qs1 and qs2 in the source domainand qt1 and qt2 in the target domain, respectively,and each query has three retrieved documents.
InFigure 1(a), source and target domains are repre-sented as a bag of document instances.
It is worthnoting that the information about which documentinstances belong to the same query is lost.
Toavoid this information loss, query weighting schemeshown as Figure 1(b) directly measures importanceweight at query level.Instance weighting makes the importance estima-tion of document instances inaccurate when docu-ments of the same source query are similar to thedocuments from different target queries.
Take Fig-ure 2 as a toy example, where the document in-stance is represented as a feature vector with fourfeatures.
No matter what weighting schemes areused, it makes sense to assign high weights to sourcequeries qs1 and qs2 because they are similar to tar-get queries qt1 and qt2, respectively.
Meanwhile, thesource query qs3 should be weighted lower because<d1s1>=( 5, 1, 0 ,0 )<d2s1>=( 6, 2, 0 ,0 )<d1s2>=( 0, 0, 5, 1)<d2s2>=( 0, 0, 6, 2)<d1s3>=( 5, 1, 0, 0)<d2s3>=( 0, 0, 6, 2)<d1t1>=(5, 1, 0 ,0 )<d2t1>=(6, 2, 0 ,0 )<d1t2>=( 0, 0, 5, 1)<d2t2>=( 0, 0, 6, 2)qs1qs2qs3qt1qt2Figure 2: A toy example showing the problem of docu-ment instance weighting scheme.it?s not quite similar to any of qt1 and qt2 at querylevel, meaning that the ranking knowledge from qs3is different from that of qt1 and qt2 and thus lessuseful for the transfer to the target domain.
Unfor-tunately, the three source queries qs1, qs2 and qs3would be weighted equally by document instanceweighting scheme.
The reason is that all of theirdocuments are similar to the two document instancesin target domain despite the fact that the documentsof qs3 correspond to their counterparts from differenttarget queries.Therefore, we should consider the source queryas a whole and directly measure the query impor-tance.
However, it?s not trivial to directly estimate113a query?s weight because a query is essentially pro-vided as a matrix where each row represents a vectorof document features.
In this work, we present twosimple but very effective approaches attempting toresolve the problem from distinct perspectives: (1)we compress each query into a query feature vec-tor by aggregating all of its document instances, andthen conduct query weighting on these query featurevectors; (2) we measure the similarity between thesource query and each target query one by one, andthen combine these fine-grained similarity values tocalculate its importance to the target domain.2 Instance Weighting Scheme ReviewThe basic idea of instance weighting is to put largerweights on source instances which are more simi-lar to target domain.
As a result, the key problemis how to accurately estimate the instance?s weightindicating its importance to target domain.
(Jiangand Zhai, 2007) used a small number of labeled datafrom target domain to weight source instances.
Re-cently, some researchers proposed to weight sourceinstance only using unlabeled target instances (Shi-modaira, 2000; Sugiyama et al, 2008; Huang et al,2007; Zadrozny, 2004; Gao et al, 2010).
In thiswork, we also focus on weighting source queriesonly using unlabeled target queries.
(Gao et al, 2010; Ben-David et al, 2010) pro-posed to use a classification hyperplane to separatesource instances from target instances.
With the do-main separator, the probability that a source instanceis classified to target domain can be used as the im-portance weight.
Other instance weighting methodswere proposed for the sample selection bias or co-variate shift in the more general setting of classifierlearning (Shimodaira, 2000; Sugiyama et al, 2008;Huang et al, 2007; Zadrozny, 2004).
(Sugiyama etal., 2008) used a natural model selection procedure,referred to as Kullback-Leibler divergence Impor-tance Estimation Procedure (KLIEP), for automat-ically tuning parameters, and showed that its impor-tance estimation was more accurate.
The main ideais to directly estimate the density function ratio oftarget distribution pt(x) to source distribution ps(x),i.e.
w(x) = pt(x)ps(x) .
Then model w(x) can be used toestimate the importance of source instances.
Modelparameters were computed with a linear model byminimizing the KL-divergence from pt(x) to its esti-mator p?t(x).
Since p?t(x) = w?
(x)ps(x), the ultimateobjective only contains model w?
(x).For using instance weighting in pairwise rank-ing algorithms, the weights of document instancesshould be transformed into those of documentpairs (Gao et al, 2010).
Given a pair of documents?xi, xj?
and their weights wi and wj , the pairwiseweight wij could be estimated probabilistically aswi ?wj .
To consider query factor, query weight wasfurther estimated as the average value of the weightsover all the pairs, i.e., wq = 1M?i,j wij , where Mis the number of pairs in query q. Additionally, totake the advantage of both query and document in-formation, a probabilistic weighting for ?xi, xj?
wasmodeled by wq ?
wij .
Through the transformation,instance weighting schemes for classification can beapplied to ranking model adaptation.3 Query WeightingIn this section, we extend instance weighting to di-rectly estimate query importance for more effec-tive ranking model adaptation.
We present twoquery weighting methods from different perspec-tives.
Note that although our methods are based ondomain separator scheme, other instance weightingschemes such as KLIEP (Sugiyama et al, 2008) canalso be extended similarly.3.1 Query Weighting by Document FeatureAggregationOur first query weighting method is inspired by therecent work on local learning for ranking (Geng etal., 2008; Banerjee et al, 2009).
The query can becompressed into a query feature vector, where eachfeature value is obtained by the aggregate of its cor-responding features of all documents in the query.We concatenate two types of aggregates to constructthe query feature vector: the mean ??
= 1|q|?|q|i=1 f?iand the variance ??
= 1|q|?|q|i=1(f?i ?
??
)2, where f?iis the feature vector of document i and |q| denotesthe number of documents in q .
Based on the ag-gregation of documents within each query, we canuse a domain separator to directly weight the sourcequeries with the set of queries from both domains.Given query data sets Ds = {qis}mi=1 and Dt ={qjt }nj=1 respectively from the source and target do-114Algorithm 1 Query Weighting Based on Document Feature Aggregation in the QueryInput:Queries in the source domain, Ds = {qis}mi=1;Queries in the target domain, Dt = {qjt }nj=1;Output:Importance weights of queries in the source domain, IWs = {Wi}mi=1;1: ys = ?1, yt = +1;2: for i = 1; i ?
m; i + + do3: Calculate the mean vector ?
?i and variance vector ?
?i for qis;4: Add query feature vector q?is = (?
?i, ?
?i, ys) to D?s ;5: end for6: for j = 1; j ?
n; j + + do7: Calculate the mean vector ?
?j and variance vector ?
?j for qjt ;8: Add query feature vector q?jt = (?
?j , ?
?j , yt) to D?t;9: end for10: Find classification hyperplane Hst which separates D?s from D?t;11: for i = 1; i ?
m; i + + do12: Calculate the distance of q?is to Hst, denoted as L(q?is);13: Wi = P (qis ?
Dt) = 11+exp(??L(q?is)+?
)14: Add Wi to IWs;15: end for16: return IWs;mains, we use algorithm 1 to estimate the proba-bility that the query qis can be classified to Dt, i.e.P (qis ?
Dt), which can be used as the importance ofqis relative to the target domain.
From step 1 to 9,D?sand D?t are constructed using query feature vectorsfrom source and target domains.
Then, a classifi-cation hyperplane Hst is used to separate D?s fromD?t in step 10.
The distance of the query featurevector q?is from Hst are transformed to the probabil-ity P (qis ?
Dt) using a sigmoid function (Platt andPlatt, 1999).3.2 Query Weighting by Comparing Queriesacross DomainsAlthough the query feature vector in algorithm 1 canapproximate a query by aggregating its documents?features, it potentially fails to capture important fea-ture information due to the averaging effect duringthe aggregation.
For example, the merit of featuresin some influential documents may be canceled outin the mean-variance calculation, resulting in manydistorted feature values in the query feature vectorthat hurts the accuracy of query classification hy-perplane.
This urges us to propose another queryweighting method from a different perspective ofquery similarity.Intuitively, the importance of a source query tothe target domain is determined by its overall sim-ilarity to every target query.
Based on this intu-ition, we leverage domain separator to measure thesimilarity between a source query and each one ofthe target queries, where an individual domain sep-arator is created for each pair of queries.
We esti-mate the weight of a source query using algorithm 2.Note that we assume document instances in the samequery are conditionally independent and all queriesare independent of each other.
In step 3, D?qis is con-structed by all the document instances {x?k} in queryqis with the domain label ys.
For each target queryqjt , we use the classification hyperplane Hij to es-timate P (x?k ?
D?qjt), i.e.
the probability that eachdocument x?k of qis is classified into the document setof qjt (step 8).
Then the similarity between qis and qjtis measured by the probability P (qis ?
qjt ) at step 9.Finally, the probability of qis belonging to the targetdomain P (qis ?
Dt) is calculated at step 11.It can be expected that algorithm 2 will generate115Algorithm 2 Query Weighting by Comparing Source and Target QueriesInput:Queries in source domain, Ds = {qis}mi=1;Queries in target domain, Dt = {qjt }nj=1;Output:Importance weights of queries in source domain, IWs = {Wi}mi=1;1: ys = ?1, yt = +1;2: for i = 1; i ?
m; i + + do3: Set D?qis={x?k, ys)}|qis|k=1;4: for j = 1; j ?
n; j + + do5: Set D?qjt={x?k?
, yt)}|qjt |k?=1;6: Find a classification hyperplane Hij which separates D?qis from D?qjt;7: For each k, calculate the distance of x?k to Hij , denoted as L(x?k);8: For each k, calculate P (x?k ?
D?qjt) = 11+exp(??L(x?k)+?)
;9: Calculate P (qis ?
qjt ) = 1|qis|?|qis|k=1 P (x?k ?
D?qjt);10: end for11: Add Wi = P (qis ?
Dt) = 1n?nj=1 P (qis ?
qjt ) to IWs;12: end for13: return IWs;more precise measures of query similarity by utiliz-ing the more fine-grained classification hyperplanefor separating the queries of two domains.4 Ranking Model Adaptation via QueryWeightingTo adapt the source ranking model to the target do-main, we need to incorporate query weights into ex-isting ranking algorithms.
Note that query weightscan be integrated with either pairwise or listwise al-gorithms.
For pairwise algorithms, a straightforwardway is to assign the query weight to all the documentpairs associated with this query.
However, documentinstance weighting cannot be appropriately utilizedin listwise approach.
In order to compare queryweighting with document instance weighting, weneed to fairly apply them for the same approach ofranking.
Therefore, we choose pairwise approach toincorporate query weighting.
In this section, we ex-tend Ranking SVM (RSVM) (Herbrich et al, 2000;Joachims, 2002) ?
one of the typical pairwise algo-rithms for this.Let?s assume there are m queries in the data setof source domain, and for each query qi there are?
(qi) number of meaningful document pairs that canbe constructed based on the ground truth rank labels.Given ranking function f , the objective of RSVM ispresented as follows:min12||w?||2 + Cm?i=1?
(qi)?j=1?ij (1)subject to zij ?
f(w?, x?j(1)qi ?
x?j(2)qi ) ?
1 ?
?ij?ij ?
0, i = 1, .
.
.
,m; j = 1, .
.
.
, ?
(qi)where x?j(1)qi and x?j(2)qi are two documents with dif-ferent rank label, and zij = +1 if x?j(1)qi is labeledmore relevant than x?j(2)qi ; or zij = ?1 otherwise.Let ?
= 12C and replace ?ij with Hinge Loss func-tion (.
)+, Equation 1 can be turned to the followingform:min ?||w?||2+m?i=1?
(qi)?j=1(1 ?
zij ?
f(w?, x?j(1)qi ?
x?j(2)qi ))+(2)Let IW (qi) represent the importance weight ofsource query qi.
Equation 2 is extended for inte-grating the query weight into the loss function in a116straightforward way:min ?||w?||2+m?i=1IW (qi) ??
(qi)?j=1(1 ?
zij ?
f(w?, x?j(1)qi ?
x?j(2)qi ))+where IW (.)
takes any one of the weightingschemes given by algorithm 1 and algorithm 2.5 EvaluationWe evaluated the proposed two query weightingmethods on TREC-2003 and TREC-2004 web trackdatasets, which were released through LETOR3.0 asa benchmark collection for learning to rank by (Qinet al, 2010).
Originally, different query tasks weredefined on different parts of data in the collection,which can be considered as different domains for us.Adaptation takes place when ranking tasks are per-formed by using the models trained on the domainsin which they were originally defined to rank thedocuments in other domains.
Our goal is to demon-strate that query weighting can be more effectivethan the state-of-the-art document instance weight-ing.5.1 Datasets and SetupThree query tasks were defined in TREC-2003 andTREC-2004 web track, which are home page finding(HP), named page finding (NP) and topic distilla-tion (TD) (Voorhees, 2003; Voorhees, 2004).
In thisdataset, each document instance is represented by 64features, including low-level features such as termfrequency, inverse document frequency and docu-ment length, and high-level features such as BM25,language-modeling, PageRank and HITS.
The num-ber of queries of each task is given in Table 1.The baseline ranking model is an RSVM directlytrained on the source domain without using anyweighting methods, denoted as no-weight.
We im-plemented two weighting measures based on do-main separator and Kullback-Leibler divergence, re-ferred to DS and KL, respectively.
In DS measure,three document instance weighting methods basedon probability principle (Gao et al, 2010) wereimplemented for comparison, denoted as doc-pair,doc-avg and doc-comb (see Section 2).
In KL mea-sure, there is no probabilistic meaning for KLweightQuery Task TREC 2003 TREC 2004Topic Distillation 50 75Home Page finding 150 75Named Page finding 150 75Table 1: The number of queries in TREC-2003 andTREC-2004 web trackand the doc-comb based on KL is not interpretable,and we only present the results of doc-pair and doc-avg for KL measure.
Our proposed query weight-ing methods are denoted by query-aggr and query-comp, corresponding to document feature aggrega-tion in query and query comparison across domains,respectively.
All ranking models above were trainedonly on source domain training data and the labeleddata of target domain was just used for testing.For training the models efficiently, we imple-mented RSVM with Stochastic Gradient Descent(SGD) optimizer (Shalev-Shwartz et al, 2007).
Thereported performance is obtained by five-fold crossvalidation.5.2 Experimental ResultsThe task of HP and NP are more similar toeach other whereas HP/NP is rather different fromTD (Voorhees, 2003; Voorhees, 2004).
Thus,we carried out HP/NP to TD and TD to HP/NPranking adaptation tasks.
Mean Average Precision(MAP) (Baeza-Yates and Ribeiro-Neto, 1999) isused as the ranking performance measure.5.2.1 Adaptation from HP/NP to TDThe first set of experiments performed adaptationfrom HP to TD and NP to TD.
The results of MAPare shown in Table 2.For the DS-based measure, as shown in the table,query-aggr works mostly better than no-weight,doc-pair, doc-avg and doc-comb, and query-comp per-forms the best among the five weighting methods.T-test on MAP indicates that the improvement ofquery-aggr over no-weight is statistically significanton two adaptation tasks while the improvement ofdocument instance weighting over no-weight is sta-tistically significant only on one task.
All of theimprovement of query-comp over no-weight, doc-pair,doc-avg and doc-comb are statistically signifi-cant.
This demonstrates the effectiveness of query117Model Weighting method HP03 to TD03 HP04 to TD04 NP03 to TD03 NP04 to TD04no-weight 0.2508 0.2086 0.1936 0.1756DSdoc-pair 0.2505 0.2042 0.1982?
0.1708doc-avg 0.2514 0.2019 0.2122??
0.1716doc-comb 0.2562 0.2051 0.2224???
0.1793query-aggr 0.2573 0.2106???
0.2088 0.1808??
?query-comp 0.2816???
0.2147???
0.2392???
0.1861??
?KLdoc-pair 0.2521 0.2048 0.1901 0.1761doc-avg 0.2534 0.2127?
0.1904 0.1777doc-comb - - - -query-aggr 0.1890 0.1901 0.1870 0.1643query-comp 0.2548?
0.2142?
0.2313???
0.1807?Table 2: Results of MAP for HP/NP to TD adaptation.
?, ?, ?
and boldface indicate significantly better than no-weight,doc-pair, doc-avg and doc-comb, respectively.
Confidence level is set at 95%weighting compared to document instance weight-ing.Furthermore, query-comp can perform better thanquery-aggr.
The reason is that although documentfeature aggregation might be a reasonable represen-tation for a set of document instances, it is possiblethat some information could be lost or distorted inthe process of compression.
By contrast, more ac-curate query weights can be achieved by the morefine-grained similarity measure between the sourcequery and all target queries in algorithm 2.For the KL-based measure, similar observationcan be obtained.
However, it?s obvious that DS-based models can work better than the KL-based.The reason is that KL conducts weighting by densityfunction ratio which is sensitive to the data scale.Specifically, after document feature aggregation, thenumber of query feature vectors in all adaptationtasks is no more than 150 in source and target do-mains.
It renders the density estimation in query-aggr is very inaccurate since the set of samples istoo small.
As each query contains 1000 documents,they seemed to provide query-comp enough samplesfor achieving reasonable estimation of the densityfunctions in both domains.5.2.2 Adaptation from TD to HP/NPTo further validate the effectiveness of queryweighting, we also conducted adaptation from TDto HP and TD to NP .
MAP results with significanttest are shown in Table 3.We can see that document instance weightingschemes including doc-pair, doc-avg and doc-combcan not outperform no-weight based on MAP mea-sure.
The reason is that each query in TD has 1000retrieved documents in which 10-15 documents arerelevant whereas each query in HP or NP only con-sists 1-2 relevant documents.
Thus, when TD servesas the source domain, it leads to the problem thattoo many document pairs were generated for train-ing the RSVM model.
In this case, a small numberof documents that were weighted inaccurately canmake significant impact on many number of docu-ment pairs.
Since query weighting method directlyestimates the query importance instead of documentinstance importance, both query-aggr and query-comp can avoid such kind of negative influence thatis inevitable in the three document instance weight-ing methods.5.2.3 The Analysis on Source Query WeightsAn interesting problem is which queries in thesource domain are assigned high weights and whyit?s the case.
Query weighting assigns each sourcequery with a weight value.
Note that it?s not mean-ingful to directly compare absolute weight valuesbetween query-aggr and query-comp because sourcequery weights from distinct weighting methods havedifferent range and scale.
However, it is feasibleto compare the weights with the same weightingmethod.
Intuitively, if the ranking model learnedfrom a source query can work well in target do-main, it should get high weight.
According to thisintuition, if ranking models fq1s and fq2s are learned118model weighting scheme TD03 to HP03 TD04 to HP04 TD03 to NP03 TD04 to NP04no-weight 0.6986 0.6158 0.5053 0.5427DSdoc-pair 0.6588 0.6235?
0.4878 0.5212doc-avg 0.6654 0.6200 0.4736 0.5035doc-comb 0.6932 0.6214?
0.4974 0.5077query-aggr 0.7179???
0.6292???
0.5198???
0.5551??
?query-comp 0.7297???
0.6499???
0.5203???
0.6541??
?KLdoc-pair 0.6480 0.6107 0.4633 0.5413doc-avg 0.6472 0.6132 0.4626 0.5406doc-comb ?
?
?
?query-aggr 0.6263 0.5929 0.4597 0.4673query-comp 0.6530??
0.6358???
0.4726 0.5559??
?Table 3: Results of MAP for TD to HP/NP adaptation.
?, ?, ?
and boldface indicate significantly better than no-weight,doc-pair, doc-avg and doc-comb, respectively.
Confidence level is set as 95%.from queries q1s and q2s respectively, and fq1s per-forms better than fq2s , then the source query weightof q1s should be higher than that of q2s .For further analysis, we compare the weight val-ues between each source query pair, for which wetrained RSVM on each source query and evaluatedthe learned model on test data from target domain.Then, the source queries are ranked according to theMAP values obtained by their corresponding rank-ing models.
The order is denoted as Rmap.
Mean-while, the source queries are also ranked with re-spect to their weights estimated by DS-based mea-sure, and the order is denoted as Rweight.
We hopeRweight is correlated as positively as possible withRmap.
For comparison, we also ranked these queriesaccording to randomly generated query weights,which is denoted as query-rand in addition to query-aggr and query-comp.
The Kendall?s ?
= P?QP+Qis used to measure the correlation (Kendall, 1970),where P is the number of concordant query pairsand Q is the number of discordant pairs.
It?snoted that ?
?s range is from -1 to 1, and the largervalue means the two ranking is better correlated.The Kendall?s ?
by different weighting methods aregiven in Table 4 and 5.We find that Rweight produced by query-aggr andquery-comp are all positively correlated with Rmapand clearly the orders generated by query-comp aremore positive than those by query-aggr.
This isanother explanation why query-comp outperformsquery-aggr.
Furthermore, both are far better thanweighting TD03 to HP03 TD04 to HP04doc-pair 28,835 secs 21,640 secsquery-aggr 182 secs 123 secsquery-comp 15,056 secs 10,081 secsTable 6: The efficiency of weighting in seconds.query-rand because theRweight by query-rand is ac-tually independent of Rmap.5.2.4 EfficiencyIn the situation where there are large scale data insource and target domains, how to efficiently weighta source query is another interesting problem.
With-out the loss of generality, we reported the weightingtime of doc-pair, query-aggr and query-comp fromadaptation from TD to HP using DS measure.
Asdoc-avg and doc-comb are derived from doc-pair,their efficiency is equivalent to doc-pair.As shown in table 6, query-aggr can efficientlyweight query using query feature vector.
The reasonis two-fold: one is the operation of query documentaggregation can be done very fast, and the other isthere are 1000 documents in each query of TD or HP,which means that the compression ratio is 1000:1.Thus, the domain separator can be found quickly.
Inaddition, query-comp is more efficient than doc-pairbecause doc-pair needs too much time to find theseparator using all instances from source and targetdomain.
And query-comp uses a divide-and-conquermethod to measure the similarity of source query toeach target query, and then efficiently combine these119Weighting method HP03 to TD03 HP04 to TD04 NP03 to TD03 NP04 to TD04query-aggr 0.0906 0.0280 0.0247 0.0525query-comp 0.1001 0.0804 0.0711 0.1737query-rand 0.0041 0.0008 -0.0127 0.0163Table 4: The Kendall?s ?
of Rweight and Rmap in HP/NP to TD adaptation.Weighting method TD03 to HP03 TD04 to HP04 TD03 to NP03 TD04 to NP04query-aggr 0.1172 0.0121 0.0574 0.0464query-comp 0.1304 0.1393 0.1586 0.0545query-rand ?0.0291 0.0022 0.0161 -0.0262Table 5: The Kendall?s ?
of Rweight and Rmap in TD to HP/NP adaptation.fine-grained similarity values.6 Related WorkCross-domain knowledge transfer has became animportant topic in machine learning and natural lan-guage processing (Ben-David et al, 2010; Jiangand Zhai, 2007; Blitzer et al, 2006; Daume?
IIIand Marcu, 2006).
(Blitzer et al, 2006) pro-posed model adaptation using pivot features to buildstructural feature correspondence in two domains.
(Pan et al, 2009) proposed to seek a common fea-tures space to reduce the distribution difference be-tween the source and target domain.
(Daume?
III andMarcu, 2006) assumed training instances were gen-erated from source domain, target domain and cross-domain distributions, and estimated the parameterfor the mixture distribution.Recently, domain adaptation in learning to rankreceived more and more attentions due to the lackof training data in new search domains.
Existingranking adaptation approaches can be grouped intofeature-based (Geng et al, 2009; Chen et al, 2008b;Wang et al, 2009; Gao et al, 2009) and instance-based (Chen et al, 2010; Chen et al, 2008a; Gao etal., 2010) approaches.
In (Geng et al, 2009; Chen etal., 2008b), the parameters of ranking model trainedon the source domain was adjusted with the smallset of labeled data in the target domain.
(Wang et al,2009) aimed at ranking adaptation in heterogeneousdomains.
(Gao et al, 2009) learned ranking mod-els on the source and target domains independently,and then constructed a stronger model by interpo-lating the two models.
(Chen et al, 2010; Chen etal., 2008a) weighted source instances by using smallamount of labeled data in the target domain.
(Gao etal., 2010) studied instance weighting based on do-main separator for learning to rank by only usingtraining data from source domain.
In this work, wepropose to directly measure the query importance in-stead of document instance importance by consider-ing information at both levels.7 ConclusionWe introduced two simple yet effective queryweighting methods for ranking model adaptation.The first represents a set of document instanceswithin the same query as a query feature vector,and then directly measure the source query impor-tance to the target domain.
The second measuresthe similarity between a source query and each tar-get query, and then combine the fine-grained simi-larity values to estimate its importance to target do-main.
We evaluated our approaches on LETOR3.0dataset for ranking adaptation and found that: (1)the first method efficiently estimate query weights,and can outperform the document instance weight-ing but some information is lost during the aggrega-tion; (2) the second method consistently and signifi-cantly outperforms document instance weighting.8 AcknowledgementP.
Cai and A. Zhou are supported by NSFC (No.60925008) and 973 program (No.
2010CB731402).W.
Gao and K.-F. Wong are supported by national863 program (No.
2009AA01Z150).
We also thankanonymous reviewers for their helpful comments.120ReferencesRicardo A. Baeza-Yates and Berthier Ribeiro-Neto.1999.
Modern Information Retrieval.Somnath Banerjee, Avinava Dubey, Jinesh Machchhar,and Soumen Chakrabarti.
2009.
Efficient and accu-rate local learning for ranking.
In SIGIR workshop :Learning to rank for information retrieval, pages 1?8.Shai Ben-David, John Blitzer, Koby Crammer, AlexKulesza, Fernando Pereira, and Jennifer WortmanVaughan.
2010.
A theory of learning from differentdomains.
Machine Learning, 79(1-2):151?175.John Blitzer, Ryan Mcdonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of EMNLP.C.
Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,N.
Hamilton, and G. Hullender.
2005.
Learning torank using gradient descent.
In Proceedings of ICML,pages 89?96.Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, andHang Li.
2007.
Learning to rank: from pairwise ap-proach to listwise approach.
In Proceedings of ICML,pages 129 ?
136.Depin Chen, Jun Yan, Gang Wang, Yan Xiong, WeiguoFan, and Zheng Chen.
2008a.
Transrank: A novelalgorithm for transfer of rank learning.
In Proceedingsof ICDM Workshops, pages 106?115.Keke Chen, Rongqing Lu, C.K.
Wong, Gordon Sun,Larry Heck, and Belle Tseng.
2008b.
Trada: Treebased ranking function adaptation.
In Proceedings ofCIKM.Depin Chen, Yan Xiong, Jun Yan, Gui-Rong Xue, GangWang, and Zheng Chen.
2010.
Knowledge transferfor cross domain learning to rank.
Information Re-trieval, 13(3):236?253.Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26(1):101?126.Y.
Freund, R. Iyer, R. Schapire, and Y.
Singer.
2004.An efficient boosting algorithm for combining prefer-ences.
Journal of Machine Learning Research, 4:933?969.Jianfeng Gao, Qiang Wu, Chris Burges, Krysta Svore,Yi Su, Nazan Khan, Shalin Shah, and Hongyan Zhou.2009.
Model adaptation via model interpolation andboosting for web search ranking.
In Proceedings ofEMNLP.Wei Gao, Peng Cai, Kam Fai Wong, and Aoying Zhou.2010.
Learning to rank only using training data fromrelated domain.
In Proceedings of SIGIR, pages 162?169.Xiubo Geng, Tie-Yan Liu, Tao Qin, Andrew Arnold,Hang Li, and Heung-Yeung Shum.
2008.
Query de-pendent ranking using k-nearest neighbor.
In Proceed-ings of SIGIR, pages 115?122.Bo Geng, Linjun Yang, Chao Xu, and Xian-Sheng Hua.2009.
Ranking model adaptation for domain-specificsearch.
In Proceedings of CIKM.R.
Herbrich, T. Graepel, and K. Obermayer.
2000.Large Margin Rank Boundaries for Ordinal Regres-sion.
MIT Press, Cambridge.Jiayuan Huang, Alexander J. Smola, Arthur Gretton,Karsten M. Borgwardt, and Bernhard Scho?lkopf.2007.
Correcting sample selection bias by unlabeleddata.
In Proceedings of NIPS, pages 601?608.Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in nlp.
In Proceedings ofACL.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of SIGKDD,pages 133?142.Maurice Kendall.
1970.
Rank Correlation Methods.Griffin.Tie-Yan Liu.
2009.
Learning to rank for informationretrieval.
Foundations and Trends in Information Re-trieval, 3(3):225?331.Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, andQiang Yang.
2009.
Domain adaptation via transfercomponent analysis.
In Proceedings of IJCAI, pages1187?1192.John C. Platt and John C. Platt.
1999.
Probabilistic out-puts for support vector machines and comparisons toregularized likelihood methods.
In Advances in LargeMargin Classifiers, pages 61?74.
MIT Press.Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li.
2010.
Letor:A benchmark collection for research on learning torank for information retrieval.
Information Retrieval,13(4):346?374.S.
Shalev-Shwartz, Y.
Singer, and N. Srebro.
2007.
Pe-gasos: Primal estimated sub-gradient solver for svm.In Proceedings of the 24th International Conferenceon Machine Learning, pages 807?814.Hidetoshi Shimodaira.
2000.
Improving predictive in-ference under covariate shift by weighting the log-likelihood function.
Journal of Statistical Planningand Inference, 90:227?244.Masashi Sugiyama, Shinichi Nakajima, HisashiKashima, Paul von Bu?nau, and Motoaki Kawan-abe.
2008.
Direct importance estimation withmodel selection and its application to covariateshift adaptation.
In Proceedings of NIPS, pages1433?1440.Ellen M. Voorhees.
2003.
Overview of trec 2003.
InProceedings of TREC-2003, pages 1?13.Ellen M. Voorhees.
2004.
Overview of trec 2004.
InProceedings of TREC-2004, pages 1?12.Bo Wang, Jie Tang, Wei Fan, Songcan Chen, Zi Yang,and Yanzhu Liu.
2009.
Heterogeneous cross domainranking in latent space.
In Proceedings of CIKM.121Y.
Yue, T. Finley, F. Radlinski, and T. Joachims.
2007.A support vector method for optimizing average preci-sion.
In Proceedings of SIGIR, pages 271?278.Bianca Zadrozny Zadrozny.
2004.
Learning and evalu-ating classifiers under sample selection bias.
In Pro-ceedings of ICML, pages 325?332.122
