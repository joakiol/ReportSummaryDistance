Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 11?17,Vancouver, October 2005. c?2005 Association for Computational LinguisticsParsing Linear Context-Free Rewriting SystemsH?kan BurdenDept.
of LinguisticsG?teborg Universitycl1hburd@cling.gu.sePeter Ljungl?fDept.
of Computing ScienceG?teborg Universitypeb@cs.chalmers.seAbstractWe describe four different parsing algorithmsfor Linear Context-Free Rewriting Systems(Vijay-Shanker et al, 1987).
The algorithmsare described as deduction systems, and possi-ble optimizations are discussed.The only parsing algorithms presented for linear context-free rewriting systems (LCFRS; Vijay-Shanker et al,1987) and the equivalent formalism multiple context-freegrammar (MCFG; Seki et al, 1991) are extensions of theCKY algorithm (Younger, 1967), more designed for theirtheoretical interest, and not for practical purposes.
Thereason for this could be that there are not many imple-mentations of these grammar formalisms.
However, sincea very important subclass of the Grammatical Framework(Ranta, 2004) is equivalent to LCFRS/MCFG (Ljungl?f,2004a; Ljungl?f, 2004b), there is a need for practicalparsing algorithms.In this paper we describe four different parsing algo-rithms for Linear Context-Free Rewriting Systems.
Thealgorithms are described as deduction systems, and pos-sible optimizations are discussed.1 Introductory definitionsA record is a structure ?
= {r1 = a1; .
.
.
; rn = an},where all ri are distinct.
That this can be seen as a setof feature-value pairs.
This means that we can define asimple version of record unification ?1 unionsq ?2 as the union?1?
?2, provided that there is no r such that ?1.r 6= ?2.r.We sometimes denote a sequence X1, .
.
.
, Xn by themore compact ~X .
To update the ith record in a list ofrecords, we write ~?
[i := ?].
To substitute a variableBk for a record ?k in any data structure ?, we write?
[Bk/?k].1.1 Decorated Context-Free GrammarsThe context-free approximation described in section 4uses a form of CFG with decorated rules of the formf : A ?
?, where f is the name of the rule, and ?
is asequence of terminals and categories subscripted with in-formation needed for post-processing of the context-freeparse result.
In all other respects a decorated CFG can beseen as a straight-forward CFG.1.2 Linear Context-Free Rewriting SystemsA linear context-free rewriting system (LCFRS; Vijay-Shanker et al, 1987) is a linear, non-erasing multiplecontext-free grammar (MCFG; Seki et al, 1991).
AnMCFG rule is written1A ?
f [B1 .
.
.
B?]
:= { r1 = ?1; .
.
.
; rn = ?n }where A and Bi are categories, f is the name of the rule,ri are record labels and ?i are sequences of terminals andargument projections of the form Bi.r.
The languageL(A) of a category A is a set of string records, and isdefined recursively asL(A) = { ?
[B1/?1, .
.
.
, B?/??]
|A ?
f [B1 .
.
.
B?]
:= ?,?1 ?
L(B1), .
.
.
, ??
?
L(B?)
}It is the possibility of discontinuous constituents thatmakes LCFRS/MCFG more expressive than context-freegrammars.
If the grammar only consists of single-labelrecords, it generates a context-free language.Example A small example grammar is shown in figure 1,and generates the languageL(S) = { s shm | s ?
(a ?
b)?
}where shm is the homomorphic mapping such thateach a in s is translated to c, and each b is translatedto d. Examples of generated strings are ac, abcd andbbaddc.
However, neither abc nor abcdabcd will be1We borrow the idea of equating argument categories andvariables from Nakanishi et al (1997) , but instead of tuples weuse the equivalent notion of records for the linearizations.11Figure 1: An example grammar describing the language{ s shm | s ?
(a ?
b)?
}S ?
f [A] := { s = A.p A.q }A ?
g[A1 A2] := { p = A1.p A2.p; q = A1.q A2.q }A ?
ac[ ] := { p = a; q = c }A ?
bd[ ] := { p = b; q = d }generated.
The language is not context-free sinceit contains a combination of multiple and crossedagreement with duplication.If there is at most one occurrence of each possible pro-jection Ai.r in a linearization record, the MCFG rule islinear.
If all rules are linear the grammar is linear.
A ruleis erasing if there are argument projections that have norealization in the linearization.
A grammar is erasing ifit contains an erasing rule.
It is possible to transform anerasing grammar to non-erasing form (Seki et al, 1991).Example The example grammar is both linear and non-erasing.
However, given that grammar, the ruleE ?
e[A] := { r1 = A.p; r2 = A.p }is both non-linear (since A.p occurs more than once)and erasing (since it does not mention A.q).1.3 RangesGiven an input string w, a range ?
is a pair of indices,(i, j) where 0 ?
i ?
j ?
|w| (Boullier, 2000).
The en-tire string w = w1 .
.
.
wn spans the range (0, n).
Theword wi spans the range (i ?
1, i) and the substringwi+1, .
.
.
, wj spans the range (i, j).
A range with identi-cal indices, (i, i), is called an empty range and spans theempty string.A record containing label-range pairs,?
= { r1 = ?1, .
.
.
, rn = ?n }is called a range record.
Given a range ?
= (i, j), theceiling of ?
returns an empty range for the right index,d?e = (j, j); and the floor of ?
does the same for theleft index b?c = (i, i).
Concatenation of two ranges isnon-deterministic,(i, j) ?
(j?, k) = { (i, k) | j = j?
}.1.3.1 Range restrictionIn order to retrieve the ranges of any substring s in asentence w = w1 .
.
.
wn we define range restriction of swith respect to w as ?s?w = { (i, j) | s = wi+1 .
.
.
wj },i.e.
the set of all occurrences of s in w. If w is understoodfrom the context we simply write ?s?.Range restriction of a linearization record ?
is written??
?, which is a set of records, where every terminal tokens is replaced by a range from ?s?.
The range restriction oftwo terminals next to each other fails if range concatena-tion fails for the resulting ranges.
Any unbound variablesin ?
are unaffected by range restriction.Example Given the string w = abba, range restrictingthe terminal a yields?a?w = { (0, 1), (3, 4) }Furthermore,?aA.r a bB.q?w ={ (0, 1)A.r (0, 2)B.q, (3, 4)A.r (0, 2)B.q }The other possible solutions fail since they cannotbe range concatenated.2 Parsing as deductionThe idea with parsing as deduction (Shieber et al, 1995)is to deduce parse items by inference rules.
A parse itemis a representation of a piece of information that the pars-ing algorithm has acquired.
An inference rule is written?1 .
.
.
?nC?where ?
is the consequence of the antecedents ?1 .
.
.
?n,given that the side conditions in C hold.2.1 Parsing decorated CFGDecorated CFG can be parsed in a similar way as stan-dard CFG.
For our purposes it suffices to say that the al-gorithm returns items of the form,[f : A/?
?
B1/?1 .
.
.
Bn/?n ?
]saying that A spans the range ?, and each daughter Bispans ?i.The standard inference rule combine might look likethis for decorated CFG:Combine[f : A/?
?
?
?
Bx ?
][g : B/??
?
.
.
.
?
]???
?
?
?
??
[f : A/?
?
?
Bx/???
?
?
]Note that the subscript x in Bx is the decoration that willonly be used in post-processing.123 The Na?ve algorithmSeki et al (1991) give an algorithm for MCFG, which canbe seen as an extension of the CKY algorithm (Younger,1967).
The problem with that algorithm is that it has tofind items for all daughters at the same time.
We modifythis basic algorithm to be able to find one daughter at thetime.There are two kinds of items.
A passive item [A; ?
]has the meaning that the category A has been found span-ning the range record ?.
An active item for the ruleA ?
f [ ~B ~B?]
:= ?
has the form[A ?
f [ ~B ?
~B?
]; ?
; ~?
]in which the categories to the left of the dot, ~B, have beenfound with the linearizations in the list of range records~?.
?
is the result of substituting the projections in ?
withranges for the categories found in ~B.3.1 Inference rulesThere are three inference rules, Predict, Combine andConvert.PredictA ?
f [ ~B] := ??
?
???
[A ?
f [ ?
~B]; ?
; ]Prediction gives an item for every rule in the gram-mar, where the range restriction ?
is what has beenfound from the beginning.
The list of daughters isempty since none of the daughters in ~B have beenfound yet.Combine[A ?
f [ ~B ?
Bk ~B?
]; ?
; ~?
][Bk; ?k]??
?
?
[Bk/?k][A ?
f [ ~B Bk ?
~B?
]; ??
; ~?, ?k]An active item looking for Bk and a passive itemthat has found Bk can be combined into a new activeitem.
In the new item we substitute Bk for ?k inthe linearization record.
We also add ?k to the newitem?s list of daughters.Convert[A ?
f [ ~B ?
]; ?
; ~?]?
?
?
[A; ?
]Every fully instantiated active item is converted intoa passive item.
Since the linearization record ?is fully instantiated, it is equivalent to the rangerecord ?.Figure 2: The example grammar converted to a decoratedCFGf : (S.s) ?
(A.p) (A.q)g : (A.p) ?
(A.p)1 (A.p)2g : (A.q) ?
(A.q)1 (A.q)2ac : (A.p) ?
aac : (A.q) ?
bbd : (A.p) ?
cbd : (A.q) ?
dThe subscripted numbers are for distinguishing the twocategories from each other, since they are equivalent.Here A.q is a context-free category of its own, not arecord projection.4 The Approximative algorithmParsing is performed in two steps in the approximativealgorithm.
First we parse the sentence using a context-free approximation.
Then the resulting context-free chartis recovered to a LCFRS chart.The LCFRS is converted by creating a decoratedcontext-free rule for every row in a linearization record.Thus, the ruleA ?
f [ ~B] := { r1 = ?1; .
.
.
; rn = ?n }will give n context-free rules f : A.ri ?
?i.
The ex-ample grammar from figure 1 is converted to a decoratedCFG in figure 2.Parsing is now initiated by a context-free parsing algo-rithm returning decorated items as in section 2.1.
Sincethe categories of the decorated grammar are projectionsof LCFRS categories, the final items will be of the form[f : (A.r)/?
?
.
.
.
(B.r?)x/??
.
.
.
?
]Since the decorated CFG is over-generating, the re-turned parse chart is unsound.
We therefore need to re-trieve the items from the decorated CFG parse chart andcheck them against the LCFRS to get the discontinuousconstituents and mark them for validity.The initial parse items are of the form,[A ?
f [ ~B]; r = ?
; ~?
]where ~?
is extracted from a corresponding decorated item[f : (A.r)/?
?
?
], by partitioning the daughters in ?such that ?i = { r = ?
| (B.r)i/?
?
?
}.
In other words,?i will consist of all r = ?
such that B.r is subscriptedby i in the decorated item.Example Given ?
= (A.p)2/??
(B.q)1/???
(A.q)2/???
?,we get the two range records ?1 = {q = ???}
and?2 = {p = ??
; q = ????
}.13Apart from the initial items, we use three kinds of parseitems.
From the initial parse items we first build LCFRSitems, of the form[A ?
f [ ~B]; ?
?
ri .
.
.
rn; ~?
]where ri .
.
.
rn is a list of labels, ~?
is a list of | ~B| rangerecords, and ?
is a range record for the labels r1 .
.
.
ri?1.In order to recover the chart we use mark items[A ?
f [ ~B ?
~B?
]; ?
; ~?
?
~??
]The idea is that ~?
has been verified as range records span-ning the daughters ~B.
When all daughters have been ver-ified, a mark item is converted to a passive item [A; ?
].4.1 Inference rulesThere are five inference rules, Pre-Predict, Pre-Combine,Mark-Predict, Mark-Combine and Convert.Pre-PredictA ?
f [ ~B] := {r1 = ?1; .
.
.
; rn = ?n}~??
= { }, .
.
.
, { }[A ?
f [ ~B]; ?
r1 .
.
.
rn; ~??
]Every rule A ?
f [ ~B] is predicted as an LCFRSitem.
Since the context-free items contain informa-tion about ?1 .
.
.
?n, we only need to use the labelsr1, .
.
.
, rn.
~??
is a list of | ~B| empty range records.Pre-Combine[R; ?
?
r ri .
.
.
rn; ~?
][R; r = ?
; ~??]~???
?
~?
unionsq ~??
[R; {?
; r = ?}
?
ri .
.
.
rn; ~???
]If there is an initial parse item for the rule R with la-bel r, we can combine it with an LCFRS item look-ing for r, provided the daughters?
range records canbe unified.Mark-Predict[A ?
[ ~B]; ?
?
; ~?
][A ?
[ ?
~B]; ?
; ?
~?
]When all record labels have been found, we can startto check if the items have been derived in a valid wayby marking the daughters?
range records for correct-ness.Mark-Combine[A ?
f [ ~B ?
Bi ~B?
]; ?
; ~?
?
?i ~??
][Bi; ?i][A ?
f [ ~B Bi ?
~B?
]; ?
; ~?
?i ?
~??
]Record ?i is correct if there is a correct passive itemfor category Bi that has found ?i.Convert[A ?
f [ ~B ?
]; ?
; ~?
?
][A; ?
]An item that has marked all daughters as correct isconverted to a passive item.5 The Active algorithmThe active algorithm parses without using any context-free approximation.
Compared to the Na?ve algorithmthe dot is used to traverse the linearization record of arule instead of the categories in the right-hand side.For this algorithm we use a special kind of range,?
?, which denotes simultaneously all empty ranges (i, i).Range restricting the empty string gives ???
= ??.
Con-catenation is defined as ????
= ????
= ?.
Both the ceilingand the floor of ??
are identities, d?
?e = b?
?c = ?
?.There are two kinds of items.
Passive items [A; ?]
saythat we have found category A inside the range record ?.An active item for the ruleA ?
f [ ~B] := {?
; r = ??
; ?
}is of the form[A ?
f [ ~B]; ?, r = ?
?
?, ?
; ~?
]where ?
is a range record corresponding to the lineariza-tion rows in ?
and ?
has been recognized spanning ?.We are still looking for the rest of the row, ?, and the re-maining linearization rows ?.
~?
is a list of range recordscontaining information about the daughters ~B.5.1 Inference rulesThere are five inference rules, Predict, Complete, Scan,Combine and Convert.PredictA ?
f [ ~B] := {r = ?
; ?}~??
= { }, .
.
.
, { }[A ?
f [ ~B]; {}, r = ??
?
?, ?
; ~??
]For every rule in the grammar, predict a correspond-ing item that has found the empty range.
~??
is a listof | ~B| empty range records since nothing has beenfound yet.Complete[R; ?, r = ?
?
?, {r?
= ?
; ?
}; ~?
][R; {?
; r = ?
}, r?
= ??
?
?,?
; ~?
]When an item has found an entire linearization rowwe continue with the next row by starting it off withthe empty range.14Scan[R; ?, r = ?
?
s?, ?
; ~?]??
?
?
?
?s?
[R; ?, r = ??
?
?, ?
; ~?
]When the next symbol to read is a terminal, its rangerestriction is concatenated with the range for whatthe row has found so far.Combine[A ?
f [ ~B]; ?, r = ?
?
Bi.r?
?, ?
; ~?
][Bi; ??]??
?
?
?
??.r?
?i ?
??
[A ?
f [ ~B]; ?, r = ??
?
?, ?
; ~?
[i := ??
]]If the next thing to find is a projection on Bi, andthere is a passive item where Bi is the category,where ??
is consistent with ?i, we can move the dotpast the projection.
?i is updated with ?
?, since itmight contain more information about the ith daugh-ter.Convert[A ?
f [ ~B]; ?, r = ?
?
?, {}; ~?
][A; {?
; r = ?
}]An active item that has fully recognized all its lin-earization rows is converted to a passive item.6 The Incremental algorithmAn incremental algorithm reads one token at the time andcalculates all possible consequences of the token beforethe next token is read2.
The Active algorithm as describedabove is not incremental, since we do not know in whichorder the linearization rows of a rule are recognized.
Tobe able to parse incrementally, we have to treat the lin-earization records as sets of feature-value pairs, insteadof a sequence.The items for a rule A ?
f [ ~B] := ?
have the sameform as in the Active algorithm:[A ?
f [ ~B]; ?, r = ?
?
?, ?
; ~?
]However, the order between the linearization rows doesnot have to be the same as in ?.
Note that in this algo-rithm we do not use passive items.
Also note that sincewe always know where in the input we are, we cannotmake use of a distinguished ?-range.
Another conse-quence of knowing the current input position is that thereare fewer possible matches for the Combine rule.2See e.g.
the ACL 2004 workshop ?Incremental Parsing:Bringing Engineering and Cognition Together?.6.1 Inference rulesThere are four inference rules, Predict, Complete, Scanand Combine.PredictA ?
f [ ~B] := {?
; r = ?
; ?
}0 ?
k ?
|w|[A ?
f [ ~B]; {}, r = (k, k) ?
?, {?;?
}; ~??
]An item is predicted for every linearization row rand every input position k.
~??
is a list of | ~B| emptyrange records.Complete[R; ?, r = ?
?
?, {?
; r?
= ?
; ?
}; ~?
]d?e ?
k ?
|w|[R; {?
; r = ?
}, r?
= (k, k) ?
?, {?;?
}; ~?
]Whenever a linearization row r is fully traversed, wepredict an item for every remaining linearization rowr?
and every remaining input position k.Scan[R; ?, r = ?
?
s?, ?
; ~?]??
?
?
?
?s?
[R; ?, r = ??
?
?, ?
; ~?
]If the next symbol in the linearization row is a termi-nal, its range restriction is concatenated to the rangefor the partially recognized row.Combine[R; ?, r = ?
?
Bi.r?
?, ?
; ~?
][Bi ?
.
.
.
; ?
?, r?
= ??
?
?, .
.
.
; .
.
.]???
?
?
?
??
?i ?
{??
; r?
= ??
}[R; ?, r = ???
?
?, ?
; ~?
[i := {??
; r?
= ??
}]]If the next item is a record projection Bi.r?, andthere is an item for Bi which has found r?, thenmove the dot forward.
The information in ?i mustbe consistent with the information found for the Biitem, {??
; r?
= ??
}.7 DiscussionWe have presented four different parsing algorithms forLCFRS/MCFG.
The algorithms are described as deduc-tion systems, and in this final section we discuss somepossible optimizations, and complexity issues.157.1 Different prediction strategiesThe Predict rule in the above described algorithms is verycrude, predicting an item for each rule in the grammar(for the Incremental algorithm even for each input po-sition).
A similar context-free prediction rule is calledbottom-up Earley by Sikkel and Nijholt (1997).
Suchcrude predictions are only intended for educational pur-poses, since they lead to lots of uninteresting items, andwaste of computing power.
For practical purposes thereare two standard context-free prediction strategies, top-down and bottom-up (see e.g.
Wir?n (1992)) and they canbe adapted to the algorithms presented in this paper.The main idea is that an item for the rule A ?
f [ ~B]with the linearization row r = ?
is only predicted if.
.
.
(Top-down prediction) .
.
.
there is another item lookingfor A.r.
(Bottom-up prediction) .
.
.
there is an passive item thathas found the first symbol in ?.For a more detailed description of these prediction strate-gies, see Ljungl?f (2004a).7.2 Efficiency and complexity of the algorithmsThe theoretical time complexity for these algorithms isnot better than what has been presented earlier.3 Thecomplexity arguments are similar and the reader is re-ferred to Seki et al (1991).However, theoretical time complexity does not saymuch about practical performance, as is already clearfrom context-free parsing, where the theoretical timecomplexity has remained the same ever since the firstpublications (Kasami, 1965; Younger, 1967).
There aretwo main ways of improving the efficiency of existingalgorithms, which can be called refinement and filtering(Sikkel and Nijholt, 1997).
First, one wants to be ableto locate existing parse items efficiently, e.g.
by indexingsome properties in a hash table.
This is often done byrefining the parse items or inference rules, increasing thenumber of items or deduction steps.
Second, it is desir-able to reduce the number of parse items, which can bedone by filtering out redundant parts of an algorithm.The algorithms presented in this paper can all be seenas refinements and filterings of the basic algorithm ofSeki et al (1991):The na?ve algorithm is a refinement of the basic algo-rithm, since single items and deduction steps are de-composed into several different items and smallerdeduction steps.3Nakanishi et al (1997) reduce the parsing problem toboolean matrix multiplication, but this can be considered apurely theoretical result.The approximative algorithm is both a refinement anda filtering of the na?ve algorithm; a refinement sincethe inference rules Pre-Predict and Pre-Combine areadded, and a filtering since there will hopefully beless items for Mark-Predict and Mark-Combine totake care of.The active algorithm is a refinement of the na?ve algo-rithm, since the Combine rule is divided into therules Complete, Scan and Combine.The incremental algorithm is finally a refinement ofthe active algorithm, since Predict and Completecan select from any possible remaining linearizationrow, and not just the following.Furthermore, the different prediction strategies (top-down and bottom-up), become filterings of the algo-rithms, since they reduce the number of parse items.7.3 Implementing and testing the algorithmsThe algorithms presented in this paper have been im-plemented in the programming language Haskell, for in-clusion in the Grammatical Framework system (Ranta,2004).
These implementations are described by Bur-den (2005).
We have also started to implement a selectionof the algorithms in the programming language Prolog.Preliminary results suggest that the Active algorithmwith bottom-up prediction is a good candidate for parsinggrammars written in the Grammatical Framework.
Fora normal sentence in the English resource grammar thespeedup is about 20 times when compared to context-freeparsing and filtering of the parse trees.
In the future weplan to test the different algorithms more extensively.AcknowledgmentsThe authors are supported by the EU project TALK (Talkand Look, Tools for Ambient Linguistic Knowledge),IST-507802.ReferencesPierre Boullier.
2000.
Range concatenation grammars.In 6th International Workshop on Parsing Technolo-gies, pages 53?64, Trento, Italy.H?kan Burden.
2005.
Implementations of parsing al-gorithms for linear multiple context-free grammars.Master?s thesis, G?teborg University, Gothenburg,Sweden.Tadao Kasami.
1965.
An efficient recognition and syntaxalgorithm for context-free languages.
Technical Re-port AFCLR-65-758, Air Force Cambridge ResearchLaboratory, Bedford, MA.16Peter Ljungl?f.
2004a.
Expressivity and Complexityof the Grammatical Framework.
Ph.D. thesis, G?te-borg University and Chalmers University of Technol-ogy, Gothenburg, Sweden.Peter Ljungl?f.
2004b.
Grammatical Framework andmultiple context-free grammars.
In 9th Conference onFormal Grammar, Nancy, France.Ryuichi Nakanishi, Keita Takada, and Hiroyuki Seki.1997.
An efficient recognition algorithm for multi-ple context-free languages.
In MOL5: 5th Meeting onthe Mathematics of Language, pages 119?123, Saar-br?cken, Germany.Aarne Ranta.
2004.
Grammatical Framework, a type-theoretical grammar formalism.
Journal of FunctionalProgramming, 14(2):145?189.Hiroyuki Seki, Takashi Matsumara, Mamoru Fujii, andTadao Kasami.
1991.
On multiple context-free gram-mars.
Theoretical Computer Science, 88:191?229.Stuart Shieber, Yves Schabes, and Fernando Pereira.1995.
Principles and implementation of deductiveparsing.
Journal of Logic Programming, 24(1?2):3?36.Klaas Sikkel and Anton Nijholt.
1997.
Parsing ofcontext-free languages.
In G. Rozenberg and A. Sa-lomaa, editors, The Handbook of Formal Languages,volume II, pages 61?100.
Springer-Verlag, Berlin.K.
Vijay-Shanker, David Weir, and Aravind Joshi.
1987.Characterizing structural descriptions produced by var-ious grammatical formalisms.
In 25th Meeting of theAssociation for Computational Linguistics.Mats Wir?n.
1992.
Studies in Incremental Natural-Language Analysis.
Ph.D. thesis, Link?ping Univer-sity, Link?ping, Sweden.Daniel H Younger.
1967.
Recognition of context-free languages in time n3.
Information and Control,10(2):189?208.17
