Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 541?545,Dublin, Ireland, August 23-24, 2014.Sensible: L2 Translation Assistance by Emulating the ManualPost-Editing ProcessLiling Tan, Anne-Kathrin Schumann, Jose M.M.
Martinez and Francis Bond1Universit?at des Saarland / Campus, Saarbr?ucken, GermanyNanyang Technological University1/ 14 Nanyang Drive, Singaporealvations@gmail.com, anne.schumann@mx.uni-saarland.de,j.martinez@mx.uni-saarland.de, bond@ieee.orgAbstractThis paper describes the Post-Editor Z sys-tem submitted to the L2 writing assis-tant task in SemEval-2014.
The aim oftask is to build a translation assistancesystem to translate untranslated sentencefragments.
This is not unlike the taskof post-editing where human translatorsimprove machine-generated translations.Post-Editor Z emulates the manual pro-cess of post-editing by (i) crawling and ex-tracting parallel sentences that contain theuntranslated fragments from a Web-basedtranslation memory, (ii) extracting the pos-sible translations of the fragments indexedby the translation memory and (iii) apply-ing simple cosine-based sentence similar-ity to rank possible translations for the un-translated fragment.1 IntroductionIn this paper, we present a collaborative submis-sion between Saarland University and NanyangTechnological University to the L2 Translation As-sistant task in SemEval-2014.
Our team nameis Sensible and the participating system is Post-Editor Z (PEZ).The L2 Translation Assistant task concerns thetranslation of an untranslated fragment from a par-tially translated sentence.
For instance, given asentence, ?Ich konnte B?arbel noch on the borderin einen letzten S-Bahn-Zug nach Westberlin set-zen.
?, the aim is to provide an appropriate transla-tion for the underline phrase, i.e.
an der Grenze.The aim of the task is not unlike the task ofpost-editing where human translators correct er-rors provided by machine-generated translations.This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/The main difference is that in the context of post-editing the source text is provided.
A transla-tion workflow that incorporates post-editing be-gins with a source sentence, e.g.
?I could stillsit on the border in the very last tram to WestBerlin.?
and the human translator is provided witha machine-generated translation with untranslatedfragments such as the previous example and some-times ?fixing?
the translation would simply re-quire substituting the appropriate translation forthe untranslated fragment.2 Related Tasks and PreviousApproachesThe L2 writing assistant task lies between thelines of machine translation and crosslingual wordsense disambiguation (CLWSD) or crosslinguallexical substitution (CLS) (Lefever and Hoste,2013; Mihalcea et al.
2010).While CLWSD systems resolve the correctsemantics of the translation by providing thecorrect lemma in the target language, CLS at-tempts to provide also the correct form of thetranslation with the right morphology.
Machinetranslation tasks focus on producing translationsof whole sentences/documents while crosslingualword sense disambiguation targets a single lexicalitem.Previously, CLWSD systems have tried distri-butional semantics and string matching methods(Tan and Bond, 2013), unsupervised clustering ofword alignment vectors (Apidianaki, 2013) andsupervised classification-based approaches trainedon local context features for a window of threewords containing the focus word (van Gompel,2010; van Gompel and van den Bosch, 2013; Rud-nick et al., 2013).
Interestingly, Carpuat (2013)approached the CLWSD task with a Statistical MTsystem .Short of concatenating outputs of CLWSD /CLS outputs and dealing with a reordering issue541and responding to the task organizers?
call to avoidimplementing a full machine translation systemto tackle the task, we designed PEZ as an Auto-matic Post-Editor (APE) that attempts to resolveuntranslated fragments.3 Automatic Post-EditorsAPEs target various types of MT errors from de-terminer selection (Knight and Chander, 1994) togrammatical agreement (Mare?cek et al., 2011).Untranslated fragments from machine translationsare the result of out-of-vocabulary (OOV) words.Previous approaches to the handling of un-translated fragments include using a pivot lan-guage to translate the OOV word(s) into a thirdlanguage and then back into to the source lan-guage, thereby extracting paraphrases to OOV(Callison-burch and Osborne, 2006), combiningsub-lexical/constituent translations of the OOVword(s) to generate the translation (Huang et al.,2011) or finding paraphrases of the OOV wordsthat have available translations (Marton et al.,2009; Razmara et al., 2013).1However the simplest approach to handle un-translated fragments is to increase the size of par-allel data.
The web is vast and infinite, a humantranslator would consult the web when encounter-ing a word that he/she cannot translate easily.
Themost human-like approach to post-editing a for-eign untranslated fragment is to do a search onthe web or a translation memory and choose themost appropriate translation of the fragment fromthe search result given the context of the machinetranslated sentence.4 MotivationWhen post-editing an untranslated fragment, a hu-man translator would (i) first query a translationmemory or parallel corpus for the untranslatedfragment in the source language, (ii) then attemptto understand the various context that the fragmentcan occur in and (iii) finally he/she would sur-mise appropriate translations for the untranslatedfragment based on semantic and grammatical con-straints of the chosen translations.1in MT, evaluation is normally performed using automaticmetrics based on automatic evaluation metrics that comparesscores based on string/word similarity between the machine-generated translation and a reference output, simply remov-ing OOV would have improved the metric ?scores?
of thesystem (Habash, 2008; Tan and Pal, 2014).The PEZ system was designed to emulate themanual post-editing process by (i) first crawlinga web-based translation memory, (ii) then extract-ing parallel sentences that contain the untranslatedfragments and the corresponding translations ofthe fragments indexed by the translation memoryand (iii) finally ranking them based on cosine sim-ilarity of the context words.5 System DescriptionThe PEZ system consists of three components,viz (i) a Web Translation Memory (WebTM)crawler, (ii) the XLING reranker and (iii) a longestngram/string match module.5.1 WebTM CrawlerGiven the query fragment and the context sen-tence, ?Die Frau kehrte alone nach Lima zur?uck?,the crawler queries www.bab.la and returnssentences containing the untranslated fragmentwith various possible tranlsations, e.g:?
isoliert : Darum sollten wir den Kaffee nichtisoliert betrachten.?
alleine : Die Kommission kann nun aber f?urihr Verhalten nicht alleine die Folgen tragen.?
Allein : Allein in der Europischen Unionsind.The retrieval mechanism is based on thefact that the target translations of the queriedword/phrase are bolded on a web-based TM andthus they can be easily extracted by manipulatingthe text between <bold>...</bold> tags.
Al-though the indexed translations were easy to ex-tract, there were few instances where the transla-tions were embedded betweeen the bold tags onthe web-based TM.5.2 XLING RerankerXLING is a light-weight cosine-based sentencesimilarity script used in the previous CLWSDshared task in SemEval-2013 (Tan and Bond,2013).
Given the sentences from the WebTMcrawler, the reranker first removes all stopwordsfrom the sentences and then ranks the sentencesbased on the number of overlapping stems.In situations where there are no overlappingcontent words from the sentences, XLING fallsback on the most common translation of the un-translated fragment.542en-de en-es fr-en nl-enacc wac rec acc wac rec acc wac rec acc wac recWebTM 0.160 0.184 0.647 0.145 0.175 0.470 0.055 0.067 0.210 0.092 0.099 0.214XLING 0.152 0.178 0.647 0.141 0.171 0.470 0.055 0.067 0.210 0.088 0.095 0.214PEZ 0.162 0.233 0.878 0.239 0.351 0.819 0.081 0.116 0.321 0.115 0.152 0.335Table 1: Results for Best Evaluation of the System Runs.5.3 Longest Ngram/String MatchesDue to the low coverage of the indexed trans-lations on the web TM, it is necessary to ex-tract more candidate translations.
Assuming lit-tle knowledge about the target language, humantranslator would find parallel sentences containingthe untranslated fragment and resort to finding re-peating phrases that occurs among the target lan-guage sentences.For instance, when we query the phrase historybook from the context ?Von ihr habe ich mehr gel-ernt als aus manchem history book.
?, the longestngram/string matches module retrieves several tar-get language sentences without any indexed trans-lation:?
Ich weise darauf hin oder nehme an, dassdies in den Geschichtsb?uchern auch soerw?ahnt wird.?
Wenn die Geschichtsb?ucher geschrieben wer-den wird unser Zeitalter, denke ich, wegendrei Dingen erinnert werden.?
Ich bin sicher, Pr?asident Mugabe hat sichnun einen Platz in den Geschichtsb?ucherngesichert, wenn auch aus den falschenGr?unden.?
In den Geschichtsb?uchern wird f?ur jedeneinzelnen Tag der letzten mehr als 227 Jahrean Gewalttaten oder Tragdien auf dem eu-rop?aischen Kontinent erinnert.By simply spotting the repeating word/stringfrom the target language sentences it is pos-sible to guess that the possible candidatesfor ?history book?
are Geschichtsb?ucher orGeschichtsb?uchern.
Computationally, this canbe achieved by looking for the longest matchingngrams or the longest matching string across thetarget language sentences fetched by the WebTMcrawler.5.4 System RunsWe submitted three system runs to the L2 writingassistant task in Semeval-2014.1.
WebTM: a baseline configuration which out-puts the most frequent indexed translation ofthe untranslated fragment from the Web TM.2.
XLING: reranks the WebTM outputs basedon cosine similarity.3.
PEZ: similar to the XLING but when theWebTM fetches no output, the system looksfor longest common substring and reranks theoutputs based on cosine similarity.6 EvaluationThe evaluation of the task is based on three met-rics, viz.
absolute accuracy (acc), word-based ac-curacy (wac) and recall (rec).Absolute accuracy measures the number offragments that match the gold translation of theuntranslated fragments.
Word-based accuracy as-signs a score according to the longest consecutivematching substring between output fragment andreference fragment; it is computed as such:wac =|longestmatch(output,reference)|max(|output|,|reference|)Recall accounts for the number of fragments forwhich output was given (regardless of whether itwas correct).7 ResultsTable 1 presents the results for the best evalua-tion scores of the PEZ system runs for the En-glish to German (en-de), English to Spanish (en-es), French to English (fr-en) and Dutch to English(nl-en) evaluations.
Figure 1 presents the word ac-curacy of the system runs for both best and out-of-five (oof) evaluation2.The results show that using the longestngram/string improves the recall and subsequentlythe accuracy and word accuracy of the system.However, this is not true when guessing untrans-lated fragments from L1 English to L2.
This isdue to the low recall of the system when search-ing for the untranslated fragment in French and2Please refer to http://goo.gl/y9f5Na for resultsof other competing systems543Figure 1: Word Accuracy of System Runs (best onthe left, oof on the right).Dutch, where the English words/phases indexed inthe TM is much larger than other languages.8 Error AnalysisWe manually inspected the English-German out-puts from the PEZ system and identified severalparticularities of the outputs that account for thelow performance of the system for this languagepair.8.1 Weird Expressions in the TMWhen attempting to translate Nevertheless in thecontext of ?Nevertheless hat sich die neue Bun-desrepublik Deutschland unter amerikanischemDruck an der militrischen Einmischung auf demBalkan beteiligt.?
where the gold translation isTrotzdem or Nichtsdestotrotz.
The PEZ system re-trieves the following sentence pairs that contains ararely used expression nichtsdestoweniger from aliterally translated sentence pair in the TM:?
EN: But nevertheless it is a fact that nobodycan really recognize their views in the report.?
DE: Aber nichtsdestoweniger kann sich nie-mand so recht in dem Bericht wiederfinden.Another example of weird expression is whentranslating ?husband?
in the context of ?In derSilvesternacht sind mein husband und ich auf dieBahnhofstra?e gegangen.?.
PEZ provided a lesseruse yet valid translation Gemahl instead of thegold translation Mann.
In this case, it is also amatter of register where in a more formal registerone will use Gemahl instead of Mann.8.2 Missing / Additional Words fromMatchesWhen extracting candidate translations from theTM index or longest ngram/string, there are sev-eral matches where the PEZ system outputs a par-tial phrase or phrases with additional tokens thatcause the disparity between the absolute accuracyand word accuracy.
An instance of missing wordsis as follows:?
Input: Eine genetische Veranlagungplays a decisive role.?
PEZ: Eine genetische Veranlagungeine entscheidende rolle.?
Gold: Eine genetische Veranlagungspielt (dabei) eine entscheidende rolle.For the addition of superfluous words is as fol-lows:?
Input: Ger?ate wie Handys sindnot permitted wenn sie nicht unterrichtlichenBelangen dienen.?
PEZ: Ger?ate wie Handys sind es verboten,wenn sie nicht unterrichtlichen Belangendienen.?
Gold: Ger?ate wie Handys sind verbotenwenn sie nicht unterrichtlichen Belangen di-enen.8.3 Case SensitivityFor the English-German evaluation , there are sev-eral instances where the PEZ system produces thecorrect translation of the phrase but in lower casesand this resulted in poorer accuracy.
This is uniqueto German target language and possibly contribut-ing to the lower scores as compared to the English-Spanish evaluation.9 ConclusionIn this paper, we presented the PEZ automaticpost-editor system in the L2 writing assistant taskin SemEval-2014.
The PEZ post-editing system isa resource lean approach to provide translation foruntranslated fragments based on no prior trainingdata and simple string manipulations from a web-based translation memory.The PEZ system attempts to emulate the pro-cess of a human translator post-editing out-of-vocabulary words from a machine-generated544translation.
The best configuration of the PEZ sys-tem involves a simple string search for the longestcommon ngram/string from the target languagesentences without having word/phrasal alignmentand also avoiding the need to handle word reorder-ing for multi-token untranslated fragments.AcknowledgementsThe research leading to these results has receivedfunding from the People Programme (Marie CurieActions) of the European Union?s Seventh Frame-work Programme FP7/2007-2013/ under REAgrant agreement n?317471.ReferencesMarianna Apidianaki.
2013.
Limsi : Cross-lingualword sense disambiguation using translation senseclustering.
In Second Joint Conference on Lexi-cal and Computational Semantics (*SEM), Volume2: Proceedings of the Seventh International Work-shop on Semantic Evaluation (SemEval 2013), pages178?182, Atlanta, Georgia, USA, June.Chris Callison-burch and Miles Osborne.
2006.
Im-proved statistical machine translation using para-phrases.
In In Proceedings of HLT/NAACL-2006,pages 17?24.Marine Carpuat.
2013.
Nrc: A machine translation ap-proach to cross-lingual word sense disambiguation(semeval-2013 task 10).
In Second Joint Conferenceon Lexical and Computational Semantics (*SEM),Volume 2: Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 188?192, Atlanta, Georgia, USA, June.Nizar Habash.
2008.
Four techniques for online han-dling of out-of-vocabulary words in arabic-englishstatistical machine translation.
In ACL, pages 57?60.Chung-Chi Huang, Ho-Ching Yen, Ping-Che Yang,Shih-Ting Huang, and Jason S. Chang.
2011.
Usingsublexical translations to handle the oov problem inmachine translation.
ACM Trans.
Asian Lang.
Inf.Process., 10(3):16.Kevin Knight and Ishwar Chander.
1994.
Automatedpostediting of documents.
In AAAI, pages 779?784.David Mare?cek, Rudolf Rosa, Petra Galu?s?c?akov?a, andOnd?rej Bojar.
2011.
Two-step translation withgrammatical post-processing.
In Proceedings of theSixth Workshop on Statistical Machine Translation,WMT ?11, pages 426?432, Stroudsburg, PA, USA.Yuval Marton, Chris Callison-Burch, and PhilipResnik.
2009.
Improved statistical machine trans-lation using monolingually-derived paraphrases.
InEMNLP, pages 381?390.Majid Razmara, Maryam Siahbani, Reza Haffari, andAnoop Sarkar.
2013.
Graph propagation for para-phrasing out-of-vocabulary words in statistical ma-chine translation.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1105?1115, Sofia, Bulgaria, August.Alex Rudnick, Can Liu, and Michael Gasser.
2013.Hltdi: Cl-wsd using markov random fields forsemeval-2013 task 10.
In Second Joint Conferenceon Lexical and Computational Semantics (*SEM),Volume 2: Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 171?177, Atlanta, Georgia, USA, June.Liling Tan and Francis Bond.
2013.
Xling: Match-ing query sentences to a parallel corpus using topicmodels for wsd.
In Second Joint Conference on Lex-ical and Computational Semantics (*SEM), Volume2: Proceedings of the Seventh International Work-shop on Semantic Evaluation (SemEval 2013), pages167?170, Atlanta, Georgia, USA, June.Liling Tan and Santanu Pal.
2014.
Manawi: Usingmulti-word expressions and named entities to im-prove machine translation.
In Proceedings of theNinth Workshop on Statistical Machine Translation,Baltimore, USA, August.Maarten van Gompel and Antal van den Bosch.
2013.Wsd2: Parameter optimisation for memory-basedcross-lingual word-sense disambiguation.
In SecondJoint Conference on Lexical and Computational Se-mantics (*SEM), Volume 2: Proceedings of the Sev-enth International Workshop on Semantic Evalua-tion (SemEval 2013), pages 183?187, Atlanta, Geor-gia, USA, June.Maarten van Gompel.
2010.
Uvt-wsd1: A cross-lingual word sense disambiguation system.
In Pro-ceedings of the 5th International Workshop on Se-mantic Evaluation, SemEval ?10, pages 238?241,Stroudsburg, PA, USA.545
