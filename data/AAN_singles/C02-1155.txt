Multi-Dimensional Text ClassificationThanaruk THEERAMUNKONGIT Program, SIIT, Thammasat UniversityP.O.
Box 22 Thammasat Rangsit Post Office,Pathumthani, Thailand, 12121ping@siit.tu.ac.thVerayuth LERTNATTEEIT Program, SIIT, Thammasat UniversityP.O.
Box 22 Thammasat Rangsit Post Office,Pathumthani, Thailand, 12121verayuth@siit.tu.ac.thAbstractThis paper proposes a multi-dimensionalframework for classifying text documents.In this framework, the concept of multi-dimensional category model is introducedfor representing classes.
In contrast withtraditional flat and hierarchical categorymodels, the multi-dimensional categorymodel classifies each text document in acollection using multiple predefined sets ofcategories, where each set corresponds to adimension.
Since a multi-dimensional modelcan be converted to flat and hierarchicalmodels, three classification strategies arepossible, i.e., classifying directly based onthe multi-dimensional model and classifyingwith the equivalent flat or hierarchicalmodels.
The efficiency of these threeclassifications is investigated on two datasets.
Using k-NN, na?ve Bayes and centroid-based classifiers, the experimental resultsshow that the multi-dimensional-based andhierarchical-based classification performsbetter than the flat-based classifications.1 IntroductionIn the past, most of previous works on textclassification focus on classifying textdocuments into a set of flat categories.
The taskis to classify documents into a predefined set ofcategories (or classes) (Lewis and Ringuetee,1994; Eui-Hong and Karypis, 2000) where thereare no structural relationships among thesecategories.
Many existing databases areorganized in this type of flat structure, such asReuters newswire, OHSUMED and TREC.
Toimprove classification accuracy, a variety oflearning techniques are developed, includingregression models (Yang and Chute, 1992),nearest neighbour classification (Yang and Liu,1999), Bayesian approaches (Lewis andRinguetee, 1994; McCallum et al, 1998),decision trees (Lewis and Ringuetee 1994),neural networks (Wiener et al,1995) andsupport vector machines (Dumais and Chen,2000).
However, it is very difficult to browse orsearch documents in flat categories when thereare a large number of categories.
As a moreefficient method, one possible natural extensionto flat categories is to arrange documents intopic hierarchy instead of a simple flat structure.When people organize extensive data sets intofine-grained classes, topic hierarchy is oftenemployed to make the large collection of classes(categories) more manageable.
This structure isknown as category hierarchy.
Many popular searchengines and text databases apply this structure, suchas Yahoo, Google Directory, Netscape search andMEDLINE.
There are many recent worksattempting to automate text classification based onthis category hierarchy (McCallum et al, 1998;Chuang W. T. et al, 2000).
However, with a largenumber of classes or a large hierarchy, the problemof sparse training data per class at the lower levels inthe hierarchy raises and results in decreasingclassification accuracy of lower classes.
As anotherproblem, the traditional category hierarchy may betoo rigid for us to construct since there exist severalpossible category hierarchies for a data set.To cope with these problems, this paperproposes a new framework, called multi-dimensional framework, for text classification.The framework allows multiple pre-defined setsof categories (viewed as multiple dimensions)instead of a single set of categories like flatcategories.
While each set of classes with sometraining examples (documents) attached to eachclass, represents a criterion to classify a new textdocument based on such examples, multiple setsof classes enable several criteria.
Documents areclassified based on these multiple criteria(dimensions) and assigned a class per criterion(dimension).
Two merits in the multi-dimensional approach are (1) the support ofmultiple viewpoints of classification, (2) asolution to data sparseness problem.
Theefficiency of multi-dimensional classification isinvestigated using three classifiers: k-NN, na?veBayes and centroid-based methods.2 Multi-Dimensional Category Modelfor Text ClassificationCategory is a powerful tool to manage a largenumber of text documents.
By grouping textdocuments into a set of categories, it is possiblefor us to efficiently keep or search forinformation we need.
At this point, the structureof categories, called category model, becomesone of the most important factors that determinethe efficiency of organizing text documents.
Inthe past, two traditional category models, calledflat and hierarchical category models, wereapplied in organizing text documents.
However,these models have a number of disadvantages asfollows.
For the flat category model, when thenumber of categories becomes larger, it faceswith difficulty of browsing or searching thecategories.
For the hierarchical category model,constructing a good hierarchy is a complicatedtask.
In many cases, it is not intuitive todetermine the upward/downward relationsamong categories.
There are several possiblehierarchies for the same document set.
Sincehierarchies in the hierarchical category modelare static, browsing and searching documentsalong the hierarchy are always done in a fixorder, from the root to a leaf node.
Therefore,the searching flexibility is lost.As an alternative to flat and hierarchicalcategory models, the multi-dimensional categoryis introduced.
So far the concept of multi-dimensional data model has been very wellknown in the field of database technology.
Themodel was shown to be powerful in modeling adata warehouse or OLAP to allow users to store,view and utilize relational data efficiently(Jiawei and Micheline, 2001).
This sectiondescribes a way to apply multi-dimensional datamodel to text classification, so called multi-dimensional category.
The proposed model is anextension of flat category model, wheredocuments are not classified into a single set ofcategories, instead they are classified intomultiple sets.
Each set of categories can beviewed as a dimension in the sense thatdocuments may be classified into different kindsof categories.
For example in Figure 1, a set ofnews issues (documents) can be classified intothree dimensions, say TOPIC, ZONE andTOPIC, each including {sports, economics,politics, social, entertainment, science andtechnology}, {domestic, intra-continental, inter-continental} and {good news, bad news, neutralnews}, respectively.
A news issue in a Thailandnewspaper titled ?Airplanes attacked WorldTrader Center?
can be classified into ?socialnews?, ?inter-continental?, ?bad news?
in thefirst, second and third dimensions, respectively.sports economicspolitics socialentertainment S & TTOPIC dimensionintra-continentalZONE dimensioninter-continentaldomesticbad newsMOOD dimensionneutral newsgood newsFigure 1.
Three-dimension category model forclassifying news documentssportsdomesticbad newssportsdomesticgood newssportsdomesticneural newsS&Tinter-conbad newsS&Tinter-congood newsS&Tinter-conneural newsFigure 2.
Flat category model for the model in Figure 1Comparing with flat and/or hierarchicalcategory models, the multi-dimensional modelhas the following merits.
First, it is more naturalthan flat model in the sense that a documentcould be classified basing on not a singlecriterion (one dimension) but multiple criteria(multiple dimensions).
Secondly, in contrastwith hierarchical model, it is possible for us tobrowse or search documents flexibly without theorder constraint defined in the structure.
Lastly,the multi-dimensional category model can bebasically transformed to and represented by flatcategory or hierarchical category models, eventhe converses are not always intuitive.In the previous example, the correspondingflat and hierarchical models for the multi-dimensional model in Figure 1 are illustratedFigure 2 and 3, respectively.
The total number ofderived flat categories equals to the product ofthe number of categories in each dimension, i.e.,54(=6x3x3).
In the derived hierarchical model,the number of leaf categories is also equivalentto 54 but there exist 24 (=6+6x3) internalcategories.
Note that the figure shows only onepossible hierarchy where the dimensions orderedby TOPIC, ZONE and MOOD.
However, thereare totally 6 (=3!)
possible hierarchies for themodel in Figure 1.From a viewpoint of category representation,the fact that the derived flat model enumeratesall combinations among categories, makes therepresentation of a class be more precise thanthe class in multi-dimensional model.
However,from the viewpoint of relationship constraints inthese models, the derived flat category modelignores the relationship among categories whilethe derived hierarchical model explicitlydeclares such relationship in a rigid manner, andthe multi-dimensional model is a compromisebetween these two previous models.
Thesedifferent aspects affect classification efficiencyas shown in the next section.3 Multi-Dimensional ClassificationDescribed in the previous section, a multi-dimensional category model can be transformedinto flat and hierarchical category models.
As aresult, there are three different classificationstrategies: flat-based, hierarchical-based andmulti-dimensional-based methods.3.1 Flat-based classificationThe na?ve method to classify documents accordingto a multi-dimensional model is flat-basedclassification.
After transforming a multi-dimensional category model to flat category model,traditional flat classification is applied directly to thederived flat categories.
The granularity of thederived flat categories is finer than the originalmulti-dimensional categories since all combinationsof classes in the dimensions are enumerated.
Thisfact implies that a flat category represents the classmore precisely than a multi-dimensional categoryand then one can expect high classificationaccuracy.
However, on the other hand, the numberof training data (documents) per class is reduced.
Asa consequence, flat classification may face with thesparseness problem of training data.
This may causea classifier harder to classify and then reduceclassification accuracy.
In the view ofcomputational cost, a test document has to becompare to all enumerated classes, resulting in highcomputation.3.2 Hierarchical-based classificationThe second method is to transform a multi-dimensional category model to a hierarchicalcategory model and then apply the standardhierarchical classification on the derivedhierarchical model.
There are several possiblemodels generated from a multi-dimensionalmodel due to the order of dimensions asdescribed in section 2.
The classification is heldalong the hierarchy from the root to a leaf.
Thedecision of the class, which a document belongsSportsS&Tgoodbadneutraldomes.intra.inter.goodbadneutraldomes.intra.inter.Figure 3.
Hierarchical category model for themodel in Figure 1to, is made in step by step.
The classifications ofdifferent levels occupy different granularities oftraining data.
Nodes at the level closed to theroot will have coarser granularity.
This makessuch nodes represent classes less imprecisely butthere are more training data (documents) forthese nodes.
On the other hand, nodes nearleaves will have finer granularity and then havemore precise representation but have lesstraining data.
The classification accuracy variedwith the order of dimensions in the hierarchy.3.3 Multi-dimensional-based  classificationIt is possible to directly classify a documentusing the multi-dimensional category model.The class of the document for each dimension isdetermined independently.
We called this multi-dimensional-based classification.
Comparedwith flat-based classification, the granularity ofmulti-dimensional classification is coarser.
Foreach dimension, it classifies a document basedon categories in that dimension instead ofclassifying it into the set of finer categories asdone in flat classification.
Although the multi-dimensional category is not precisely representany finer categories, the number of training data(documents) per class is relatively high.
As aconsequence, multi-dimensional classificationgains high accuracy for each dimension andresults in high accuracy for the overallclassification accuracy when there are a smallnumber of training data.
It also performs fasterthan flat-based classification since there arefewer classes needed to be compared.4 ImplementationTo investigate efficiency of text classification onthe multidimensional category model, threewell-known classification algorithms called k-nearest neighbors (k-NN), na?ve Bayesian (NB)and centroid-based (CB) approaches are applied.4.1 k-NN ClassifierAs a similarity-based method, the k-nearestneighbor classifier (k-NN) is applied to our textclassification.
First, the classifier calculates kmost similar documents (i.e., k nearestneighbors) of the test document being classified.The similarity of this document to a class iscomputed by summing up the similarities ofdocuments among the k documents, whoseclasses are equivalent to such class.
The testdocument is assigned the class that has thehighest similarity to the document.
Twoparameters involved are the definition ofsimilarity and the number k. While the standardsimilarity is defined as tf?idf, a variant(0.5+0.5tf/tfmax)?idf that performed better in ourpreliminary experiments, is applied in this work.The parameter k is determined by experiments asshown in the next section.4.2 Na?ve Bayes ClassifierThe standard na?ve Bayesian (NB) is applied asa statistical approach to our text classification inthis work.
For each document, the classifier firstcalculates the posterior probability P(ci|d) ofclass ci that the document belongs to differentclasses and assigns it to the class with thehighest posterior probability.
Basically, adocument d can be represented by a bag ofwords {w1, w2, ?, wn} in that document (i.e., avector of occurrence frequencies of words in thedocument).
NB assumes that the effect of aword?s occurrence on a given class isindependent of other words?
occurrence.
Withthis assumption, a NB classifier finds the mostprobable class ci ?
C, called a maximum aposteriori (MAP) cMAP for the document, whereC={c1, c2, ?, ck}  is a set of predefined classes.
(1)4.3 Centroid-based ClassifierApplied in our implementation is a variant ofcentroid-based classification (CB) with differentweight methods from the standard weighting tf-idf.
A centroid-based classifier (CB) is amodified version of k-NN classifier.
Instead ofcomparing the test document with all trainingdocuments, CB calculates a centroid (a vector)for all training documents in each class andcompares the test document with these centroidsto find the most probable (similar) class.
Asimple centroid-based classifier represents adocument with a vector each dimension ofwhich expresses a term in the document with aweight of tf?idf.
The resultant vector is??
?=nj iijcniincMAPcPcwPwwwPcPcwwwPcii12121)()|(maxarg}),...,,({)()|},...,,({maxargnormalized with the document length to a unit-length vector.
A different version of a centroidvector is so-called a prototype vector (Chuang,W.
T. et al, 2000).
Instead of normalizing eachvector in the class before calculating a centroid,the prototype vector is calculated by normalizingthe summation of all vectors of documents in theclass.
Both methods utilizing centroid-based andprototype vectors obtained high classificationaccuracy with small time complexity.
In ourimplementation, we use a variant of theprototype vector that does not apply the standardtf-idf but use either of the following weightingformulas.
These weighting formulas, we calledCB1 and CB2, were empirically proved to workwell in (Theeramunkong and Lertnattee, 2001).
(2)icsd stands for inter-class standard deviation,tfrms is the root mean square of document termfrequency in a class, and sd means standarddeviation.
After this weighting, a prototypevector is constructed for each class.
Due to thelength limitation of the paper, we ignore thedetail of this formula but the full description canbe found in (Theeramunkong and Lertnattee, 2001).5 Experimental ResultsTwo data sets, WebKB and Drug informationcollection (DI) are used for evaluating our multi-dimensional model.
These two data sets can beviewed as a two-dimensional category model asfollows.
Composed of 8,145 web pages, theWebKB data set is a collection of web pages ofcomputer science departments in fouruniversities with some additional pages fromother universities.
The original collection isdivided into seven classes (1st dimension): student,faculty, staff, course, project, department andothers.
Focusing on each class, five subclasses(2nd dimension) are defined according to theuniversity a web page belongs to: Cornell,Texas, Washington, Wisconsin andmiscellaneous.
In our experiment, we use thefour most popular classes: student, faculty,course and project.
This includes 4,199 webpages.
Drug information, the second data set, isa collection of web documents that have beencollected from www.rxlist.com.
This collectionis composed of 4,480 web pages providinginformation about widely used drugs in seventopics (1st dimension): adverse drug reaction,clinical pharmacology, description, indications,overdose, patient information, and warning.There exists exactly one page for each drug ineach class, i.e., the number of recorded drugs is640 (=4480/7).
Moreover We manually groupedthe drugs according to major pharmacologicalactions, resulting in five classes (2nd dimension):chemotherapy (Chem), neuro-muscular system(NMS), cardiovascular & hematopoeitic (CVS),hormone (Horm) and respiratory system (Resp).The multi-dimensional classification is testedusing four algorithms: k-NN, NB and twocentroid-based classifiers (CB1 and CB2).
In thek-NN, the parameter k is set to 20 for WebKB,and set to 35 for DI.
For the centroid-basedmethod, the applied weighting systems are thoseshown in Section 4.3.
All experiments wereperformed with 10-fold cross validation.
That is,90% of documents are kept as a training setwhile the rest 10% are used for testing.
Theperformance was measured by classificationaccuracy defined as the ratio between thenumber of documents assigned with correctclasses and the total number of test documents.As a preprocess, some stop words (e.g., a, an,the) and all tags (e.g., <B>, </HTML>) wereomitted from documents to eliminate the affectof these common words and typographic words.In the rest, first the results on flat andhierarchical classification on the data sets areshown, followed by that of multi-dimensionalclassification.
Finally overall discussion is given.5.1 Flat-based ClassificationIn this experiment, test documents are classifiedinto the most specified classes say D12, whichare the combinations of two dimensions, D1 andD2.
Therefore, the number of classes equals tothe product of the number of classes in eachdimension.
That is 20 (=5?4) classes forWebKB and 35 (=7?5) classes for DI.
A testdocument was assigned the class that gained thehighest score from the classifier applied.
Table 1displays the classification accuracy of flatclassification on WebKB and DI data sets.
Here,two measures, two-dimension and single-dimension accuracy, are taken into account.or(CB1) (CB2)sdtficsdidftfrms ???sdtfidftfrms?
?WebKB DID12D1 D12D2 D12 D12D1 D12D2 D12k-NN 68.02 84.69 57.32 79.46 66.14 60.04NB 80.23 78.76 62.66 93.75 73.97 69.61CB1 77.54 91.52 71.59 96.14 72.08 69.42CB2 71.52 89.12 63.42 89.49 80.58 73.28Table 1.
Flat classification accuracy (%)In the table, D12 shows the two-dimensionaccuracy where the test document is completelyassigned to the correct class.
D12D1 and D12D2, the single-dimension accuracy, mean theaccuracy of the first and second dimensionswhere the classes in D1 and D2 dimensions aregenerated from the result class D12, respectively.The result shows that the centroid-basedclassifiers perform better than k-NN and NB.CB1 and CB2 works well on WebKB and DI,respectively.
Even low two-dimension accuracy,high single-dimension accuracy is obtained.5.2 Hierarchical-based ClassificationSince there are two dimensions in the dataset, hierarchical-based classification can be heldin two different ways according to theclassifying order.
In the first version, documentsare classified based on the first dimension todetermine the class to which those documentsbelong.
They are further classified againaccording to the second dimension using themodel of that class.
The other version classifiesdocuments based on the second dimension firstand then the first dimension.
The results areshown in Table 2.
In the tables, D1, D2 and D*12mean the accuracy of the first dimension, thesecond dimension and the two-dimensionaccuracy, respectively.
D1+D*12=>D2 expressesthe accuracy of the second dimension that usedthe result from the first dimension duringclassifying the second dimension.
D2+D*12=>D1represents the accuracy of the second dimensionthat used the result from the first dimensionduring classifying the first dimension.From the results, we found that the centroid-based classifiers also perform better than k-NNand NB, and CB1 works well on WebKB whileCB2 gains the highest accuracy on DI.
In almostcases, the hierarchical-based classificationperforms better than the flat-based classification.Moreover, an interesting observation is thatclassifying on the worse dimension before thebetter one yields a better result.WebKB DID1 D1+D*12=>D2 D*12 D1 D1+D*12=>D2 D*12k-NN 69.85 84.31 58.61 80.20 73.17 60.20NB 80.54 78.85 62.42 95.00 73.35 70.38CB1 80.42 91.28 73.90 96.23 73.44 69.26CB2 76.04 88.59 66.87 91.43 80.09 74.24WebKB DID2+D*12=>D1 D2 D*12 D2+D*12=>D1 D2 D*12k-NN 67.42 83.34 56.04 79.29 76.36 61.25NB 79.92 87.45 69.35 93.08 83.75 78.33CB1 77.99 90.02 70.18 95.60 73.44 70.36CB2 71.44 92.36 65.78 88.33 84.87 76.05Table 2.
Hierarchical classification accuracy(%)(upper: D1 before D2 , lower: D2 before D1)5.3 Multi-dimensional ClassificationIn the last experiment, multi-dimensionalclassification is investigated.
Documents areclassified twice based on two dimensionsindependently.
The results of the first andsecond dimensions are combined to be thesuggested class for a test document.
Theclassification accuracy of multi-dimensionalclassification is shown in Table 3.WebKB DID1 D2 D1+D2D1+2 D1 D2 D1+D2D1+2k-NN 69.85 83.34 57.37 80.20 76.36 61.85NB 80.54 87.45 69.66 95.00 83.75 79.51CBC1 80.42 90.02 72.52 96.23 73.44 70.05CBC2 76.04 92.36 69.90 91.43 84.87 77.99Table 3.
Multi-dimensional.classification accuracy (%)In the tables, D1 and D2 mean the accuracy ofthe first and second dimensions, respectively.D1+D2D1+2  is the two-dimension accuracy ofthe class which is the combination of classessuggested in the first dimension and the seconddimension.
From the results, we found that CB1performs well on WebKB but NB gains thehighest accuracy on DI.
The multi-dimensionalclassification outperforms flat classification inmost cases but sometime the hierarchical-basedclassification performs well.5.4 Overall Evaluation and DiscussionTwo accuracy criteria are (1) all dimensions arecorrect or (2) some dimensions are correct.
Theclassification accuracy based on the firstcriterion is shown in all previous tables as thetwo-dimension accuracy.
As the secondcriterion, the classification accuracy can beevaluated when some dimensions are correct.The result is summarized in Table 4.
The multi-dimensional classification outperforms other twomethods for WebKB but the hierarchical-basedclassification sometimes works better for DI.WebKB DIF H1 H2 M F H1 H2 Mk-NN 72.80 77.08 75.38 78.28 76.36 76.69 77.83 76.59NB 83.86 79.70 83.69 89.38 79.50 84.18 88.42 84.00CB1 84.11 85.85 84.01 84.84 84.53 84.84 84.52 85.22CB2 85.04 82.32 81.90 88.15 80.32 85.76 86.60 84.20Table 4.
Classification accuracy (%) when somedimensions are correct.From this result, some observations can begiven as follows.
There are two tradeoff factorsthat affect classification accuracy of multi-dimensional category model: training set sizeand the granularity of classes.
The flat-basedclassification in the multi-dimensional modeldeals with the finest granularity of classesbecause all combinations of classes frompredefined dimensions are combined to form alarge set of classes.
Although this preciserepresentation of classes may increase theaccuracy, the flat-based classification sufferswith sparseness problem where the number oftraining data per class is reduced.
The accuracy islow when the training set is small.
The multi-dimensional-based classification copes with thecoarsest granularity of the classes.
Therefore thenumber of training document per class is larger thanflat-based classification approach but therepresentation of classes is not exact.
However, Itworks well when we have a relatively small trainingset.
The hierarchical-based classification occupies amedium granularity of classes.
However, the size oftraining set is smaller than multi-dimensionalapproach at the low level of the hierarchy.
It workswell when the training set is medium.6 ConclusionIn this paper, a multi-dimensional framework ontext classification was proposed.
The frameworkapplies a multi-dimensional category forrepresenting classes, in contrast with traditionalflat and hierarchical category models.Classifying text documents based on a multi-dimensional category model can be performedusing the multi-dimensional-based classificationor the flat and hierarchical classifications.
Byexperiments on two data sets and threealgorithms, k-NN, na?ve Bayes and centroid-based methods, the results show that the multi-dimensional-based and hierarchical-basedclassifications outperform flat-based one.ReferencesChuang W. T. et al  (2000), A Fast Algorithm forHierarchical Text Classification.
DataWarehousing and Knowledge Discovery,  409-418.Dumais S. T. and Chen H. (2000) HierarchicalClassification of Web Content, In Proc.
of the 23rdInternational ACM SIGIR, pp.
256-263.Eui-Hong H. and Karypis G. (2000) Centroid-BasedDocument Classification: Analysis & ExperimentalResults.
In Proc.
of European Conference onPKDD, pp.
424-431.Jiawei H. and Micheline K. (2001) Data Mining: Conceptsand Techniques.
Morgan Kaufmann publishers.Lewis D. D. and Ringuette M. (1994) A Comparisonof Two Learning Algorithms for TextCategorization.
In Proc.
of Third AnnualSymposium on Document Analysis andInformation Retrieval, pages 81-93.McCallum A. et al (1998) Improving TextClassification by Shrinkage in a Hierarchy ofClasses, In Proc.
of the 15th InternationalConference on Machine Learning, pp.
359-367.Theeramunkong T. and Lertnattee V. (2001)  ImprovingCentroid-Based Text Classification Using Term-Distribution-Based Weighting System andClustering.
In Proc.
of International Symposiumon Communications and Information Technology(ISCIT 2001), pp.
33-36.Wiener E. D. et al (1995) A Neural Network Approach toTopic Spotting.
In Proc.
of SDAIR-95, the 4th AnnualSymposium on Document Analysis and InformationRetrieval.
pp.
317-332.Yang Y. and Chute C. G. (1992) A Linear LeastSquare Fit Mapping Method for InformationRetrieval from Natural Language Texts.
In Proc.of the 14th International Conference onComputational Linguistics, pp.
358-362.Yang, Y. and Liu X.
(1999) A Re-examination of TextCategorization Methods.
In Proc.
of the 22nd ACMSIGIR Conference, 42-49.
