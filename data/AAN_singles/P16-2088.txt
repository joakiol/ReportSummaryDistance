Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 543?548,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Novel Measure for Coherence in Statistical Topic ModelsFred Morstatter and Huan LiuArizona State UniversityTempe, Arizona, USA{fred.morstatter, huan.liu}@asu.eduAbstractBig data presents new challenges for un-derstanding large text corpora.
Topic mod-eling algorithms help understand the un-derlying patterns, or ?topics?, in data.
Re-searchersauthor often read these topics inorder to gain an understanding of the un-derlying corpus.
It is important to evaluatethe interpretability of these automaticallygenerated topics.
Methods have previ-ously been designed to use crowdsourcingplatforms to measure interpretability.
Inthis paper, we demonstrate the necessity ofa key concept, coherence, when assessingthe topics and propose an effective methodfor its measurement.
We show that theproposed measure of coherence captures adifferent aspect of the topics than existingmeasures.
We further study the automa-tion of these topic measures for scalabil-ity and reproducibility, showing that thesemeasures can be automated.1 IntroductionBig data poses new challenges in analyzing textcorpora.
Topic modeling algorithms have recentlygrown to popularity for their ability to help dis-cover the underlying topics in a corpus.
Topicwords are the words selected to represent a topic.They have been shown to be useful in the ar-eas of machine learning, text analysis (Grim-mer and Stewart, 2013), and social media analy-sis (O?Connor et al, 2010), among others.
Topicmodels can be used as predictive models to clas-sify new documents in the context of the train-ing corpus.
They are evaluated by measuring theirpredictive performance on a held-out set of docu-ments.
Topic models can also be inspected man-ually by a human to understand the themes ofthe underlying corpus.
A widely adopted way issuggested by (Chang et al, 2009): it measuresthe quality of a topic by inspecting how far topicwords are from some random words.
The idea isthat the quality of a topic can be measured by howfar topic words are from some random words.
Inother words, if human evaluators can consistentlyseparate random words from topic words, thesetopics are good, otherwise, they are not good.
Anadvantage of this measure is that it can be easilyimplemented to deploy on a crowd-sourcing plat-form like Amazon?s Mechanical Turk.Assuming that random words represent randomtopics, we can name the above method ?between-topic?
measure.
In this paper, we hypothesizethat this measure considers just one important as-pect in assessing the quality of statistical topics.Specifically, we investigate the topic interpretabil-ity by examining the ?coherence?
of a topic gener-ated by topic modeling algorithms, i.e., how closetopic words are within a topic.
Thus, this mea-sure is a ?within-topic?
measure.
Two immedi-ate challenging questions are: (1) without know-ing ground truth of topic coherence, how can wedesign an equally effective method like ?between-topic?
measure for crowd-sourcing evaluation?and (2) how different is this ?within-topic?
coher-ence measure from the existing ?between-topic?measure?
We elaborate how we answer these twochallenges by starting with some related work,showing how the ?between-topic?
measure facesdifficulty in measuring coherence, and presentingour proposal of a coherence measure.2 Related WorkTopic modeling is pervasive, and has been widelyaccepted across many communities such as ma-chine learning and social sciences (Ramage et al,2009; Schmidt, 2012; Yang et al, 2011).
One of543the reasons for the wide appreciation of these al-gorithms is their ability to find underlying topicsin enormous sets of data (Blei, 2012).
More re-cently topic modeling has been widely applied tosocial media data (Kireyev et al, 2009; Joseph etal., 2012; Morstatter et al, 2013), e.g.
(Yin et al,2011; Hong et al, 2012; Pozdnoukhov and Kaiser,2011) focus on identifying topics in geographicalTwitter datasets.
In (Kumar et al, 2013; Mimnoet al, 2011), the authors had to employ subject-matter experts to assess topic quality.
These man-ual topic labels can be supplemented with auto-matic labeling algorithms (Maiya et al, 2013).While these works attempt to ensure topic qual-ity by employing domain experts, these are highlydomain-specific cases.
The measures we discussgoing forward are more general, and can be ap-plied to topic models trained with text data.The most important point of comparison be-tween our work and others lies in the Model Pre-cision measure proposed in (Chang et al, 2009).The insight of this measure is that a good topicis one whose top few words are distant, or highlyseparate, from randomly-selected words.
Theirtask works by showing several human participants,or Turker, the top 5 words from a topic and onerandomly-chosen, low-ranking ?intruded?
word.The humans are then asked to select the word thatthey think was intruded.
The measure then esti-mates the topic?s quality by calculating the numberof times the humans correctly guessed the intrudedword.
While Word Intrusion provides insight intoa topic?s interpretability, the key assumption is thattopic goodness comes only from the top words be-ing separate from a randomly-selected word.
Thismeasure does not offer any insight about the co-herence of the top words.
We propose a new mea-sure which complements Word Intrusion by mea-suring distance within a topic.
(Lau et al, 2014) built a machine learning algo-rithm to automatically detect the intruded word ina topic.
Methods for evaluating topic models wereproposed in (Wallach et al, 2009).
We investigatethe applicability of this measure in our work.3 Model Precision QuandaryModel Precision works by asking the user tochoose the word that does not fit within the restof the set.
We are measuring the top words in thetopic by comparing them to an outlier.
While thismethod has merit, it does not help us understandthe coherence within the top words for the topic.A diagram illustrating this phenomenon isshown in Figure 1.
In Figure 1(a), we see a co-herent topic.
This topic is coherent because all 5 ofthe top words are close together, while the intrudedword is far away.
In Figure 1(b) we see a topicthat is less coherent because the fifth word lies ata distance from the first four.
In both cases, ModelPrecision gives us the intruder word in the topic, asseen in Figures 1(c), and 1(d).
While this is the de-sired performance of Model Precision, it leaves uswith no understanding of the coherence of the topwords of the topic.
Results are masked by the out-lier, and do not give information about the intra-cluster distance, or coherence of the topic.In light of this, we look for a way to separatetopics not just by their distance from an outlier,but also by the distance within the top words in thetopic.
The next section of this paper investigates amethod which can measure not just the intruderword, but also the coherence of the top words inthe topic.
In this way we separate topics such asthose shown in Figure 1 based on the coherence oftheir top words.4 Word Intrusion Choose TwoIn this section we propose a new experiment thatmeasures the interpretability of the top words ofa topic.
This experiment sets up the task as be-fore: we select the top five words from a topic,and inject one low-probability word.
The key dif-ference is that we ask the Turker to select two in-truded words among the six.The intuition behind this experiment is that theTurkers?
first choice will be the intruded word,just as in Model Precision.
However, their secondchoice is what makes the topic?s quality clear.
In acoherent topic the Turkers won?t be able to distin-guish a second word as all of the words will seemsimilar.
A graphical representation of this phe-nomenon is shown in Figure 1(e).
In the case ofan incoherent, a strong ?second-place?
contenderwill emerge as the Turkers identify a 2nd intruderword, as in Figure 1(f).4.1 Experimental SetupTo perform this experiment, we inject one low-probability word for each topic, and we ask theTurkers to select two words that do not fit withinthe group.
We show the six words to the Turker inrandom order with the following prompt:544(a) Coherent Topic (b) Less-Coherent Topic(c) Coherent Topic: Model Precision (d) Less-Coherent Topic: Model Precision(e) Coherent Topic: Model Precision Choose Two (f) Less-Coherent Topic: Model Precision Choose TwoFigure 1: Comparison between Model Precision, and Model Precision Choose Two for a toy topic.Circles represent the top words and triangles represent intruded words.
Model Precision Choose Twocan distinguish the less-coherent topic.You will be shown six words.
Four words belong to-gether, and two of them do not.
Choose two words thatdo not belong in the group.Coherent topics will cause the Turkers?
re-sponses regarding the second intruded word to beunpredictable.
Thus, our measure of the good-ness of the topic should be the predictability ofthe Turkers?
second choice.
We propose a newmeasure called ?Model Precision Choose Two?to measure this.
Model Precision Choose Two(MPCT) measures this spread as the peakedness ofthe probability distribution.
We define MPCTmkfor topic k on model m as:MPCTmk= H(pturk(wmk,1), ..., pturk(wmk,5)),(1)where H(?)
is the Shannon entropy (Cover andThomas, 2006), wmkis the vector of the top wordsin topic k generated by model m, and pturk(wmk,i)is the probability that a Turker selects wmk,i.
Thismeasures the strength of the second-place candi-date, with higher values indicating a smoother,more even distribution, and lower values indicat-ing Turkers gravitation towards a second word.The intuition behind choosing entropy is thatit will measure the unpredictability in the Turkerselections.
That is, if the Turkers are confusedabout which second word to choose, then theiranswers will be scattered amongst the remainingfive words.
As a result, the entropy will be high.Conversely, if the second word is obvious, theTurkers will begin to congregate around that sec-ond choice, meaning that their answers will be fo-cused.
As a result, the entropy will be low.
Be-cause entropy is able to measure the confusion ofthe Turkers responses about the second word, weuse it directly in the design of our measure.4.2 DataThe data used in this study consists of articles fromEnglish Wikipedia.
We sample 10,000 articlesuniformly at random from across the dataset.
Weselected articles containing more than 50 words.In preprocessing we stripped case, removed punc-tuation, stopwords, and words consisting entirelyof numbers.
This process yields a corpus con-taining 10,000 documents, 4,200,174 tokens, and545Table 1: Example topics showing the variance ofMPCT when MP = 1.0.MPCT Top Five Words Intruded Word0.202 canada, canadian, north, ontario, http shipping0.373 language, century, word, english, greek drew0.407 river, highway, road, north, route berea0.569 born, children, family, life, father boatsman0.795 design, engine, model, power, system resynthesized0.946 railway, station, road, line, route anagarika1.000 film, series, show, television, films bubblegrunge196,219 types.The topic modeling algorithm used is latentDirichlet alocation (LDA) (Blei et al, 2003).
Tobuild the models used in the experiments, werun LDA on the Wikipedia corpus using valuesof K = {10, 25, 50, 100} with the Mallet pack-age (McCallum, 2002).
This yields 4 models and185 total topics.
The model generated by eachvalue of K is denoted by m in the equations.4.3 Experimental ResultsThe results of this experiment, aggregated bymodel, are shown in Figure 2.
We see that as thevalue of K increases, the median score for MPCTstays roughly the same.
We compute the Spear-man?s ?
correlation coefficient (Spearman, 1904)between the MP and MPCT measures, and findthat the measures have ?
= 0.09.
This lack of cor-relation indicates that this measure is assessing adifferent dimension of the topics.To help explain these results, we provide someexamples of topics that received different MPCTscores with a perfect separateness (MP) score inTable 1.
We see that although all of the topicshave perfect scores along this dimension, their co-hesiveness score varies.
This is due to the Turkers?agreement about the second intruded word.5 Automating Model PrecisionChoose TwoThe crowdsourced experiments carried out in thispaper provide a complementary understanding ofhow humans understand the topics that are gener-ated using statistical topic models.
One drawbackof these methods lies in the difficulty of repro-ducing these experiments.
This difficulty comesfrom two sources: 1) the monetary cost of employ-ing the Turkers to solve the HITs, and 2) the timecost to build the surveys and to collect the results.To overcome these issues, we propose automatedmethods that can estimate the topics?
performance0.000.250.500.751.0010 25 50 100No.
Topics (K)ModelPrecision Choose 2Model Precision Choose 2Figure 2: Model Precision Choose Two across thefour models used in this work.
Higher scores arebetter.
We see that as K increases, the medianscore does not improve noticeably.along these different dimensions.
These measurescan be used by future researchers to automaticallygauge their topics.We test several automated measures for theirability to predict the outcome of the crowdsourcedmeasures.
To test these measures, we calculate theSpearman?s ?
between the automated measure ofthe topic and the crowdsourced measure.
The au-tomated measures we propose are as follows:1.
Topic Size: LDA assigns a topic label to eachtoken in the dataset.
Topic size measures thenumber of tokens assigned to the topic by theLDA model, where more tokens indicates alarger topic.
This has been tested in (Mimnoet al, 2011).2.
Topic Entropy: The entropy of the entireprobability distribution for the topic.
Highentropy indicates a flat distribution of proba-bilities, while low entropy indicates a peakeddistribution around the first few words.3.
Mimno Co-Occurrence: Measures the fre-quency of the top words co-occurring withinthe same document.
Proposed in (Mimno etal., 2011), and measured as:MCO(w) =|w|?j=2j?1?k=1logD(wj,wk) + 1D(wk),(2)546Table 2: Performance of automated measures inapproximating the crowdsourced experiments.
Allvalues are Spearman?s ?
correlation coefficientswith the crowdsourced measure.Automated Measure MPCT1.
Topic Size -0.5722.
Topic Entropy -0.5393.
Mimno -0.4384.
No.
Word Senses -0.4565.
Avg.
Pairwise JCD -0.8446.
Mean-Link JCD -0.4347.
NPMI -0.582where w is the vector of the top 20 wordsin the topic, and D(?)
returns the number oftimes the words co-occur in any document inthe corpus.4.
No.
Word Senses: The total number of wordsenses, according to WordNet, of the top fivewords in the topic.
This varies slightly fromthe measure proposed in (Chang et al, 2009),where the authors also consider the intrudedword.
Because the intruded word is generallyfar away, we exclude it from our calculation.5.
Avg.
Pairwise Jiang-Conrath Distance:The Jiang-Conrath (Jiang and Conrath, 1997)distance (JCD) is a measure of semantic sim-ilarity, or coherence, that considers the low-est common subsumer according to Word-Net.
Here we compute the average JCD ofall(52)= 10 pairs of the top five wordsof the topic.
This approach was introducedby (Chang et al, 2009), however we modifyit slightly to only consider the top five wordsin the topic.6.
Mean-Link JCD: Using the JCD measureas before, we compute the average distancefrom the intruded word to each of the top 5words from the topic.7.
Normalized Pointwise Mutual Informa-tion (NPMI): NPMI measures the associa-tion between the top words in a topic.
Itis normalized to yield a score of 1 in thecase of perfect association.
This measure wasfirst introduced by (Bouma, 2009).
We usethe calculation adapted for the problem ofestimating a topic?s performance introducedin (Lau et al, 2014).We calculate the correlation between all au-tomated methods and MPCT, shown in Table 2.MPCT is best predicted using the Avg.
PairwiseJCD measure.
The implications of this result areimportant: MPCT is best predicted by JCD, a mea-sure that approximates the coherence of topics.Furthermore the correlations are negative, indicat-ing that a low average distance (and thus, a highsemantic similarity) indicates a high performancealong this automated measure.6 Conclusion and Future WorkIn this work we define a new measure for the per-formance of statistical topic models.
We show thatthis measure gauges a different aspect of the top-ics than the traditional model precision measure.Finally, we identify automated measures that canapproximate the crowdsourced measures for bothinterpretability and coherence.
This measure canbe used by future researchers to complement theiranalysis of statistical topics.
The results from ourexperiments indicate that Word Intrusion ChooseTwo is different from Word Intrusion, with almostno correlation between the two measures.Furthermore, we propose automatic measuresthat can replace the crowdsourced measures.
Thisis important as it allows for both scalability andreproducibility, as experiments using crowdsourc-ing are costly in terms of both time and money.We find that measures based on the interpretabil-ity of topics can best approximate the ModelPrecision Choose Two measure, indicating thatthis measure favors topics whose top words aremore semantically similar, furthering our claimthat this measure is assessing the coherence of thetopic.
Code and data to reproduce Model Preci-sion Choose Two can be found at http://bit.ly/mpchoose2.While model precision choose two offers a newway to understand topics, there may be othersthat could help to reveal other dimensions of topicquality.
Future work is to find other measures forthe semantic properties of topic modeling algo-rithms.
Furthermore, the automated measures wediscover to approximate the crowdsourced onesmay be incorporated into a topic modeling algo-rithm that can better produce interpretable topics.AcknowledgmentsThis work is sponsored, in part, by Office of NavalResearch (ONR) grant N000141410095.547ReferencesD.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
La-tent Dirichlet Allocation.
The Journal of MachineLearning Research, 3:993?1022.David Blei.
2012.
Topic modeling and digital humani-ties.
Journal of Digital Humanities, 2(1):8?11.Gerlof Bouma.
2009.
Normalized (pointwise) mutualinformation in collocation extraction.
Proceedingsof GSCL, pages 31?40.Jonathan Chang, Jordan L. Boyd-Graber, Sean Gerrish,Chong Wang, and David M Blei.
2009.
Readingtea leaves: How humans interpret topic models.
InNIPS, pages 288?296.T.
M. Cover and J.
A. Thomas.
2006.
Elements ofInformation Theory.
Wiley InterScience, Hoboken,New Jersey.Justin Grimmer and Brandon M Stewart.
2013.
Text asdata: The promise and pitfalls of automatic contentanalysis methods for political texts.
Political Analy-sis.Liangjie Hong, Amr Ahmed, Siva Gurumurthy,Alexander J. Smola, and Kostas Tsioutsiouliklis.2012.
Discovering geographical topics in the twit-ter stream.
In Proceedings of the 21st internationalconference on World Wide Web, WWW ?12, pages769?778, New York, NY, USA.
ACM.Jay J Jiang and David W Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical tax-onomy.
arXiv preprint cmp-lg/9709008.Kenneth Joseph, Chun How Tan, and Kathleen M.Carley.
2012.
Beyond ?local?, ?categories?
and?friends?
: clustering foursquare users with latent?topics?.
In Proceedings of the 2012 ACM Con-ference on Ubiquitous Computing, UbiComp ?12,pages 919?926, New York, NY, USA.
ACM.K Kireyev, L Palen, and K Anderson.
2009.
Appli-cations of Topics Models to Analysis of Disaster-Related Twitter Data.
In NIPS Workshop on Ap-plications for Topic Models: Text and Beyond, vol-ume 1.Shamanth Kumar, Fred Morstatter, Reza Zafarani, andHuan Liu.
2013.
Whom Should I Follow?
: Identi-fying Relevant Users During Crises.
In Proceedingsof the 24th ACM Conference on Hypertext and So-cial Media, HT ?13, pages 139?147, New York, NY,USA.
ACM.Jey Han Lau, David Newman, and Timothy Baldwin.2014.
Machine reading tea leaves: Automaticallyevaluating topic coherence and topic model quality.In Proceedings of the European Chapter of the As-sociation for Computational Linguistics.Arun S Maiya, John P Thompson, Francisco Loaiza-Lemos, and Robert M Rolfe.
2013.
Exploratoryanalysis of highly heterogeneous document collec-tions.
In Proceedings of the 19th ACM SIGKDD in-ternational conference on Knowledge discovery anddata mining, pages 1375?1383.
ACM.Andrew McCallum.
2002.
Mallet: A machine learningfor language toolkit.
http://mallet.cs.umass.edu.David Mimno, Hanna M. Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing semantic coherence in topic models.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,pages 262?272, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Fred Morstatter, J?urgen Pfeffer, Huan Liu, and Kath-leen M. Carley.
2013.
Is the Sample Good Enough?Comparing Data from Twitters Streaming API withTwitters Firehose.
Proceedings of ICWSM.Brendan O?Connor, Michel Krieger, and David Ahn.2010.
Tweetmotif: Exploratory search and topicsummarization for twitter.
In ICWSM.Alexei Pozdnoukhov and Christian Kaiser.
2011.Space-time dynamics of topics in streaming text.In Proc.
of the 3rd ACM SIGSPATIAL Int?l Work-shop on Location-Based Social Networks, LBSN?11, pages 1?8, New York, NY, USA.
ACM.Daniel Ramage, Evan Rosen, Jason Chuang, Christo-pher D Manning, and Daniel A McFarland.
2009.Topic modeling for the social sciences.
In NIPS2009 Workshop on Applications for Topic Models:Text and Beyond, volume 5.Benjamin M Schmidt.
2012.
Words alone: Disman-tling topic models in the humanities.
Journal of Dig-ital Humanities, 2(1):49?65.Charles Spearman.
1904.
The proof and measurementof association between two things.
The Americanjournal of psychology, 15(1):72?101.Hanna M Wallach, Iain Murray, Ruslan Salakhutdinov,and David Mimno.
2009.
Evaluation methods fortopic models.
In ICML, pages 1105?1112.
ACM.Tze-I Yang, Andrew J Torget, and Rada Mihalcea.2011.
Topic modeling on historical newspapers.
InProceedings of the 5th ACL-HLT Workshop on Lan-guage Technology for Cultural Heritage, Social Sci-ences, and Humanities, pages 96?104.
Associationfor Computational Linguistics.Zhijun Yin, Liangliang Cao, Jiawei Han, ChengxiangZhai, and Thomas Huang.
2011.
Geographicaltopic discovery and comparison.
In Proceedingsof the 20th international conference on World wideweb, WWW ?11, pages 247?256, New York, NY,USA.
ACM.548
