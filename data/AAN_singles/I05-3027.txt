A Conditional Random Field Word Segmenterfor Sighan Bakeoff 2005Huihsin TsengDept.
of LinguisticsUniversity of ColoradoBoulder, CO 80302tseng@colorado.eduPichuan Chang, Galen Andrew,Daniel Jurafsky, Christopher ManningStanford Natural Language Processing GroupStanford UniversityStanford, CA 94309{pichuan, pupochik, jurafsky, manning}@stanford.eduAbstractWe present a Chinese word seg-mentation system submitted to theclosed track of Sighan bakeoff 2005.Our segmenter was built using a condi-tional random field sequence modelthat provides a framework to use alarge number of linguistic features suchas character identity, morphologicaland character reduplication features.Because our morphological featureswere extracted from the training cor-pora automatically, our system was notbiased toward any particular variety ofMandarin.
Thus, our system does notoverfit the variety of Mandarin mostfamiliar to the system's designers.
Ourfinal system achieved a F-score of0.947 (AS), 0.943 (HK), 0.950 (PK)and 0.964 (MSR).1 IntroductionThe 2005 Sighan Bakeoff included four dif-ferent corpora, Academia Sinica (AS), CityUniversity of Hong Kong (HK), Peking Univer-sity (PK), and Microsoft Research Asia (MSR),each of which has its own definition of a word.In the 2003 Sighan Bakeoff (Sproat & Emer-son 2003), no single model performed well onall corpora included in the task.
Rather, systemstended to do well on corpora largely drawn froma set of similar Mandarin varieties to the onethey were originally developed for.
Across cor-pora, variation is seen in both the lexicons andalso in the word segmentation standards.
Weconcluded that, for future systems, generaliza-tion across such different Mandarin varieties iscrucial.
To this end, we proposed a new modelusing character identity, morphological andcharacter reduplication features in a conditionalrandom field modeling framework.2 AlgorithmOur system builds on research into condi-tional random field (CRF), a statistical sequencemodeling framework first introduced by Laffertyet al (2001).
Work by Peng et al (2004) firstused this framework for Chinese word segmen-tation by treating it as a binary decision task,such that each character is labeled either as thebeginning of a word or the continuation of one.Gaussian priors were used to prevent overfittingand a quasi-Newton method was used for pa-rameter optimization.The probability assigned to a label sequencefor a particular sequence of characters by a CRFis given by the equation below:( ) ( )?????
?= ??
?Cc kc cXYkkXZXYP f ,,exp)(1| ?
?Y is the label sequence for the sentence, X isthe sequence of unsegmented characters, Z(X) isa normalization term, fk is a feature function, andc indexes into characters in the sequence beinglabeled.A CRF allows us to utilize a large number ofn-gram features and different state sequence168based features and also provides an intuitiveframework for the use of morphological features.3 Feature engineering3.1 FeaturesThe linguistic features used in our model fallinto three categories: character identity n-grams,morphological and character reduplication fea-tures.For each state, the character identity features(Ng & Low 2004, Xue & Shen 2003, Goh et al2003) are represented using feature functionsthat key off of the identity of the character in thecurrent, proceeding and subsequent positions.Specifically, we used four types of unigram fea-ture functions, designated as C0 (current charac-ter), C1 (next character), C-1 (previous character),C-2 (the character two characters back).
Fur-thermore, four types of bi-gram features wereused, and are notationally designated here asconjunctions of the previously specified unigramfeatures, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0.Given that unknown words are normallymore than one character long, when representingthe morphological features as feature functions,such feature functions keyed off the morpho-logical information extracted from both the pro-ceeding state and the current state.
Our morpho-logical features are based upon the intuition re-garding unknown word features given in Gao etal.
(2004).
Specifically, their idea was to useproductive affixes and characters that only oc-curred independently to predict boundaries ofunknown words.
To construct a table containingaffixes of unknown words, rather than usingthreshold-filtered affix tables in a separate un-known word model as was done in Gao et al(2004), we first extracted rare words from a cor-pus and then collected the first and last charac-ters to construct the prefix and suffix tables.
Forthe table of individual character words, we col-lected an individual character word table foreach corpus of the characters that always oc-curred alone as a separate word in the given cor-pus.
We also collected a list of bi-grams fromeach training corpus to distinguish knownstrings from unknown.
Adopting all the featurestogether in a model and using the automaticallygenerated morphological tables prevented oursystem from manually overfitting the Mandarinvarieties we are most familiar with.The tables are used in the following ways:1) C-1+C0 unknown word feature functionswere created for each specific pair of charactersin the bi-gram tables.
Such feature functions areactive if the characters in the respective statesmatch the corresponding feature function?scharacters.
These feature functions are designedto distinguish known strings from unknown.2) C-1, C0, and C1 individual character featurefunctions were created for each character in theindividual character word table, and are likewiseactive if the respective character matches thefeature function?s character.3) C-1 prefix feature functions are definedover characters in the prefix table, and fire if thecharacter in the proceeding state matches thefeature function?s character.4) C0 suffix feature functions are definedover suffix table characters, and fire if the char-acter in the current state matches the featurefunction?s character.Additionally, we also use reduplication fea-ture functions that are active based on the repeti-tion of a given character.
We used two such fea-ture functions, one that fires if the previous andthe current character, C-1 and C0, are identicaland one that does so if the subsequent and theprevious characters, C-1 and C1, are identical.Most features appeared in the first-order tem-plates with a few of character identity features inthe both zero-order and first-order templates.We also did normalization of punctuations dueto the fact that Mandarin has a huge variety ofpunctuations.Table 1 shows the number of data featuresand lambda weights in each corpus.Table 1 The number of features in each corpus# of data features # of lambda weightsAS 2,558,840 8,076,916HK 2,308,067 7,481,164PK 1,659,654 5,377,146MSR 3,634,585 12,468,8903.2 Experiments3.2.1 Results on Sighan bakeoff 2003Experiments done while developing this sys-tem showed that its performance was signifi-cantly better than that of Peng et al (2004).As seen in Table 2, our system?s F-score was0.863 on CTB (Chinese Treebank from Univer-169sity of Pennsylvania) versus 0.849 F on Peng etal.
(2004).
We do not at present have a goodunderstanding of which aspects of our systemgive it superior performance.Table 2 Comparisons of Peng et al (2004) and our F-score on the closed track in Sighan bakeoff 2003SighanBakeoff 2003Our F-score F-scorePeng et al (2004)CTB 0.863 0.849AS 0.970 0.956HK 0.947 0.928PK 0.953 0.9413.2.2 Results on Sighan bakeoff 2005Our final system achieved a F-score of 0.947(AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).This shows that our system successfully general-ized and achieved state of the art performanceon all four corpora.Table 3 Performance of the features cumulatively,starting with the n-gram.F-score AS HK PK MSRn-gram 0.943 0.946 0.950 0.961n-gram (PU fixed)  0.953+Unk&redupl 0.947 0.943 0.950 0.964+Unk&redupl(PU fixed)0.952Table 3 lists our results on the four corpora.We give our results using just character identitybased features; character identity features plusunknown words and reduplication features.
Ourunknown word features only helped on AS andMSR.
Both of these corpora have words thathave more characters than HK and PK.
This in-dicates that our unknown word features weremore useful for corpora with segmentation stan-dards that tend to result in longer words.In the HK corpus, when we added in un-known word features, our performance dropped.However, we found that the testing data usesdifferent punctuation than the training set.
Oursystem could not distinguish new word charac-ters from new punctuation, since having a com-plete punctuation list is considered externalknowledge for closed track systems.
If the newpunctuation were not unknown to us, our per-formance on HK data would have gone up to0.952 F and the unknown word features wouldhave not hurt the system too much.Table 4 present recalls (R), precisions (P), f-scores (F) and recalls on both unknown (Roov)and known words (Riv).Table 4 Detailed performances of each corpusR P F Roov RivAS 0.950 0.943 0.947?
0.718?
0.960HK 0.941 0.946 0.943?
0.698?
0.961HK(PU-fix)0.952 0.952 0.952 0.791 0.965PK 0.946 0.954 0.950?
0.787?
0.956MSR 0.962 0.966 0.964?
0.717?
0.9683.3 Error analysisOur system performed reasonably well onmorphologically complex new words, such as???
(CABLE in AS) and ???
(MUR-DER CASE in PK), where ?
(LINE) and ?
(CASE) are suffixes.
However, it over-generalized to words with frequent suffixes suchas ??
(it should be ?
?
?to burn some-one?
in PK) and ??
(it should be?
?
?
?to look backward?
in PK).
For the corpora thatconsidered 4 character idioms as a word, oursystem combined most of new idioms together.This differs greatly from the results that onewould likely obtain with a more traditionalMaxMatch based technique, as such an algo-rithm would segment novel idioms.One short coming of our system is that it isnot robust enough to distinguish the differencebetween ordinal numbers and numbers withmeasure nouns.
For example, ??
(3rd year)and ??
(three years) are not distinguishableto our system.
In order to avoid this problem, itmight require having more syntactic knowledgethan was implicitly given in the training data.Finally, some errors are due to inconsisten-cies in the gold segmentation of non-hanzi char-acter.
For example, ?Pentium4?
is a word, but?PC133?
is two words.
Sometimes, ?8?
is aword, but sometimes it is segmented into twowords.1704 ConclusionOur system used a conditional random fieldsequence model in conjunction with characteridentity features, morphological features andcharacter reduplication features.
We extractedour morphological information automatically toprevent overfitting Mandarin from particularMandarin-speaking area.
Our final systemachieved a F-score of 0.947 (AS), 0.943 (HK),0.950 (PK) and 0.964 (MSR).5 AcknowledgmentThanks to Kristina Toutanova for her gener-ous help and to Jenny Rose Finkel who devel-oped such a great conditional random fieldpackage.
This work was funded by the Ad-vanced Research and Development Activity'sAdvanced Question Answering for IntelligenceProgram, National Science Foundation awardIIS-0325646 and a Stanford Graduate Fellow-ship.ReferencesLafferty, John, A. McCallum, and F. Pereira.
2001.Conditional Random Field: Probabilistic Modelsfor Segmenting and Labeling Sequence Data.
InICML 18.Gao, Jianfeng Andi Wu, Mu Li, Chang-Ning Huang,Hongqiao Li, Xinsong Xia and Haowei Qin.
2004.Adaptive Chinese word segmentation.
In ACL-2004.Goh, Chooi-Ling, Masayuki Asahara, Yuji Matsu-moto.
2003.
Chinese unknown word identificationusing character-based tagging and chunking.
InACL 2003 Interactive Poster/Demo Sessions.Ng, Hwee Tou and Jin Kiat Low.
2004.
Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once?Word-Based or Character-Based?
In EMNLP 9.Peng, Fuchun, Fangfang Feng and AndrewMcCallum.
2004.
Chinese segmentation and newword detection using conditional random fields.
InCOLING 2004.Sproat, Richard and Tom Emerson.
2003.
The firstinternational Chinese word segmentation bakeoff.In SIGHAN 2.Xue, Nianwen and Libin Shen.
2003.
Chinese WordSegmentation as LMR Tagging.
In SIGHAN 2.171
