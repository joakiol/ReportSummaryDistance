Answering Denition Questions via Temporally-Anchored Text SnippetsMarius Pas?caGoogle Inc.1600 Amphitheatre ParkwayMountain View, California 94043mars@google.comAbstractA lightweight extraction method derives textsnippets associated to dates from the Web.The snippets are organized dynamically intoanswers to definition questions.
Experi-ments on standard test question sets showthat temporally-anchored text snippets allowfor efficiently answering definition ques-tions at accuracy levels comparable to thebest systems, without any need for complexlexical resources, or specialized processingmodules dedicated to finding definitions.1 IntroductionIn the field of automated question answering (QA),a variety of information sources and multiple extrac-tion techniques can all contribute to producing rele-vant answers in response to natural-language ques-tions submitted by users.
Yet the nature of the infor-mation source which is mined for answers, togetherwith the scope of the questions, have the most sig-nificant impact on the overall architecture of a QAsystem.
When compared to the average queries sub-mitted in a decentralized information seeking envi-ronment such as Web search, fact-seeking questionstend to specify better the nature of the informationbeing sought by the user, whether it is the name ofthe longest river in some country, or the name ofthe general who defeated the Spanish Armada.
Inorder to understand the structure and the linguisticclues encoded in natural-language questions, manyQA systems employ sophisticated techniques, thusderiving useful information such as terms, relationsamong terms, the type of the expected answers (e.g.,cities vs. countries vs. presidential candidates), andother semantic constraints (e.g., the elections from1978 rather than any other year).One class of questions whose characteristics placethem closer to exploratory queries, rather than stan-dard fact-seeking questions, are definition questions.Seeking information about an entity or a concept,questions such as ?Who is Caetano Veloso??
of-fer little guidance as to what particular techniquescould be used in order to return relevant informationfrom a large text collection.
In fact, the same usermay choose to submit a definition question or a sim-pler exploratory query (Caetano Veloso), and stilllook for text snippets capturing relevant propertiesof the question concept.
Various studies (Chen et al,2006; Han et al, 2006) illustrate the challenges in-troduced by definition questions.
As such questionshave a less irregular form than other open-domainquestions, recognizing their type is relatively eas-ier (Hildebrandt et al, 2004).
Conversely, the iden-tification of relevant documents and the extractionof answers to definition questions are more labori-ous, and the impact on the architecture of QA sys-tems is quite significant.
Indeed, separate, dedicatedmodules, or even end-to-end systems are specifi-cally built for answering definition questions (Kla-vans and Mures?an, 2001; Hildebrandt et al, 2004;Greenwood and Saggion, 2004).
The importance ofdefinition questions among other question categoriesis confirmed by their inclusion among the evalua-tion queries from the QA track of TREC evalua-tions (Voorhees and Tice, 2000).This paper investigates the impact of temporally-411anchored text snippets derived from the Web, inanswering definition questions and, more gener-ally, exploratory queries.
Section 2 describes alightweight mechanism for extracting text snippetsand associated dates from sentences in Web docu-ments.
Section 3 assesses the coverage of the ex-tracted snippets.
As shown in Section 4, relevantevents, in which the question concept was involved,can be captured by matching the queries on the textsnippets, and organizing the snippets around the as-sociated dates.
Section 5 describes discusses the roleof the extracted text snippets in answering two setsof definition questions.2 Temporally Anchored Text SnippetsAll experiments rely on the unstructured text in ap-proximately one billion documents in English from a2003 Web repository snapshot of the Google searchengine.
Pre-processing of the documents consistsin HTML tag removal, simplified sentence bound-ary detection, tokenization and part-of-speech tag-ging with the TnT tagger (Brants, 2000).
No othertools or lexical resources are employed.A sequence of sentence tokens represents a po-tential date if it consists of: single year (four-digitnumbers, e.g., 1929); or simple decade (e.g., 1930s);or month name and year (e.g., January 1929); ormonth name, day number and year (e.g., January15, 1929).
Dates occurring in text in any other for-mat are ignored.
To avoid spurious matches, suchas 1929 people, potential dates are discarded if theyare immediately followed by a noun or noun modi-fier, or immediately preceded by a noun.To convert document sentences into a few textsnippets associated with dates, the overall structureof sentences is roughly approximated.
Deep textanalysis may be desirable but simply not feasible onthe Web.
As a lightweight alternative, the proposedextraction method approximates the occurrence andboundaries of text snippets through the following setof lexico-syntactic patterns:(P1): ?Date [,|-|(|nil] [when] Snippet [,|-|)|.]?
(P2): ?
[StartSent] [In|On] Date [,|-|(|nil] Snippet [,|-|)|.]?
(P3): ?
[StartSent] Snippet [in|on] Date [EndSent]?
(P4): ?
[Verb] [OptionalAdverb] [in|on] Date?The first extraction pattern, P1, targets sentenceswith adverbial relative clauses introduced by wh-adverbs and preceded by a date, e.g.
:?By [Date 1910], when [Snippet Korea was an-nexed to Japan], the Korean population in Americahad grown to 5,008?.Comparatively, P2 and P3 match sentences thatstart or end in a simple adverbial phrase containinga date.
In the case of P4, the occurrence of rele-vant dates within sentences is approximated by verbsfollowed by a simple adverbial phrase containing adate.
P4 marks the entire sentence as a potentialnugget because it lacks the punctuation clues in theother three patterns.The patterns must satisfy additional constraints inorder to match a sentence.
These constraints con-stitute heuristics to avoid, rather than solve, com-plex linguistic phenomena.
Thus, a nugget is alwaysdiscarded if it does not contain a verb, or containsany pronoun.
Furthermore, the snippets in P2 andP3 must start with, and the nugget in P4 must con-tain a noun phrase, which in turn is approximated bythe occurrence of a noun, adjective or determiner.The combination of patterns and constraints is byno means definitive or error-free.
It is a practicalsolution to achieve graceful degradation on largeamounts of data, reduce the extraction errors, andimprove the usefulness of the extracted snippets.
Assuch, it emphasizes robustness at Web scale, withouttaking advantage of existing specification languagesfor representing events and temporal expressions oc-curring in text (Pustejovsky et al, 2003), and forgo-ing the potential benefits of more complex methodsthat extract temporal relations from relatively cleantext collections (Mani et al, 2006).3 Coverage of Text SnippetsA concept such as a particular actor, country or or-ganization usually occurs within more than one ofthe extracted text snippets.
In fact, the set of textsnippets containing the concept, together with theassociated dates, often represents an extract-based,simple temporal summary of the events in which theconcept has been involved.
Starting from this ob-servation, a task-based evaluation of the coverageof the extracted text snippets consists in verifyingto what extent they capture the condensed historyof several countries.
Since any country must havebeen involved in some historical timeline of events,a reference timeline is readily available in an exter-41205101520253035404550556065707580859095100GambiaBurundiComorosDjiboutiEritreaEthiopiaKenyaRwandaSeychellesSomaliaTanzaniaUgandaCameroonEquatorial GuineaGabonAlgeriaCeutaEgyptLibyaMelillaMoroccoSudanTunisiaWesternSaharaCount/PercentageTotal reference snippets (count)Matched reference snippets (percentage)Figure 1: Percentage of reference snippets with corresponding extracted snippetsnal resource, e.g., encyclopedia, as an excerpt cov-ering a condensed history of the country.
The refer-ence timeline is compared against the text snippetscontaining a country such as Ethiopia.
To this ef-fect, the text snippets containing a given country asa phrase are retained, ordered in increasing order oftheir associated dates, and evaluated against the ref-erence timeline.Both the test set of countries and the gold stan-dard are collected from Wikipedia (Remy, 2002).The test set comprises countries from Africa.
SinceAfrican countries have fewer extracted snippets thanother countries, the evaluation results provide moreuseful, lower bounds rather than average or best-case.
Due to limited human resources available forthis evaluation, the test countries are a subset of theAfrican countries in Wikipedia, selected in the orderin which they are listed on the site.
They cover allEastern, Central and Northern Africa.
The CentralAfrican Republic, the Republic of the Congo, andSao Tome and Principe are discarded and Gambiaadded, leading to a test set of 24 country names.
Thesource of the reference timelines is the condensedhistory article that is part of the main descriptionpage of each country in Wikipedia.The evaluation procedure is concerned only withrecall, but is still highly subjective.
It requiresthe manual division of the reference text into datedevents.
In addition, the assessor must decide whichdetails surrounding an event are significant, andmust be matched into the extracted snippets in orderto get any credit.
The actual evaluation consists inmatching each dated event from the reference time-line into the extracted timeline.
During matching,the extracted snippets are analyzed by hand to decidewhich snippets, if any, capture the reference event,significant details around it, and the time stamp.On average, 1173 text snippets are returned percountry name, with a median of 733 snippets.
Fig-ure 1 summarizes the comparison of reference snip-pets and extracted snippets.
The continuous linecorresponds to the total number of reference snip-pets that were manually identified in the referencetimeline; Melilla has the smallest such number (2),whereas Sudan has the largest (24).
The dottedline in Figure 1 represents the percentage of refer-ence snippets that have at least one match into theextracted snippets, thus evaluating recall.
An av-erage of 72% of the reference snippets have suchmatches.
For 5 queries, there are matches for allreference snippets.
The worst case occurs for Equa-torial Guinea, for which only two out of the 11 ref-erence snippets can be matched.
Based on the re-sults, we conclude that the text snippets and the as-sociated dates provide a good coverage in the caseof information about countries.
The snippets can beretrieved as answers to questions asking about dates(When, What year) as described in (Pas?ca, 2007), oras answers to definition questions as discussed be-low.4134 Answering Definition QuestionsInput definition questions are uniformly handled asBoolean queries, after the removal of stop words aswell as question-specific terms (Who etc.).
Thus,questions such as ?Who is Caetano Veloso??
and?Who won the Nobel Peace Prize??
are consistentlyconverted into conjunctive queries corresponding toCaetano Veloso and won Nobel Peace Prize respec-tively.
The score assigned to a matching text snippetis higher, if the snippet occurs in a larger numberof documents.
Similarly, the score is higher if thesnippet contains fewer non-stop terms in addition tothe question term matches, or the average distancein the snippet between pairs of query term matchesis lower.
A side effect of the latter heuristic is toboost the snippets in which the query terms occur asa phrase, rather than as scattered term matches.When they are associated to a common date, re-trieved snippets transfer their relevance score ontothe date, in the form of the sum of the individualsnippet scores.
The dates are ranked in decreas-ing order of their relevance scores, and those withthe highest scores are returned as responses to thequestion, together with the top associated snippets.Within a set of text snippets associated to a date, thesnippets are also ranked relatively to one another,such that each returned date is accompanied by itstop supporting snippets.
The ranking within a set ofsnippets associated to a date is a two-pass procedure.First, the snippets are scanned to count the numberof occurrences of non-stop unigrams within the en-tire set.
Second, a snippet is weighted with respectto others based on how many of the unigrams it con-tains, and the individual scores of those unigrams.In the output, the snippets act as useful, implicittext-based justifications of why the dates may berelevant or not.
As such, they implement a practi-cal method of fusing together bits (snippets) of in-formation collected from unrelated documents.
Insome cases, the snippets show why a returned result(date) is relevant.
For example, 1990 is relevant tothe query Germany unied because ?East and WestGermany were unied?
according to the top snippet.In other cases, the text snippets quickly reveal whythe result is related to the query even though it maynot match the original user?s intent.
For instance, auser may ask the question ?When was the Taj Mahalbuilt??
with the well-known monument in mind, inwhich case the irrelevance of the date 1903 is self-explanatory based on one of its supporting snippets,?the lavish Taj Mahal Hotel was built?.5 EvaluationThe answers returned by the system are ranked indecreasing order of their scores.
By convention, ananswer to a definition question comprises a returneddate, plus the top matching text snippets that pro-vide support for that date.
Ideally, a snippet shouldonly contain the desired answer and nothing else.
Inpractice, a snippet is deemed correct if it containsthe ideal answer, although it may contain some otherextraneous information.5.1 Objective EvaluationA thorough evaluation of answers to definition ques-tions would be complex, prone to subjective as-sessments, and would involve significant human la-bor (Voorhees, 2003).
Therefore, the quality of thetext snippets in the context of definition questionsis tested on a set, DefQa1, containing the 23 ?Whois/was [ProperName]??
questions from the TRECQA track from 1999 through 2002.
In this case, eachreturned answer consists of a date and the first sup-porting text snippet.Table 1 contains a sample of the test questions.The right column shows actual text snippets re-trieved for the definition questions, together with theassociated date and the rank of that date within theoutput.
In an objective evaluation strictly based onthe answer keys of the gold standard, the MRR scoreover the DefQa1 set is 0.596.
The score is quite high,given that the answer keys prefer the genus of thequestion concept, rather than other types of infor-mation.
For instance, the answer keys for the TRECquestions Q222:?Who is Anubis??
and Q253:?Whois William Wordsworth??
mark poet and ?Egyptiangod?
as correct answers respectively, thus empha-sizing the genus of the question concepts Anubisand William Wordsworth.
This explains the strongreliance in previous work on hand-written patternsand dictionary-based techniques for detecting textfragments encoding the genus and differentia of thequestion concept (Lin, 2002; Xu et al, 2004).414Question (Rank) Relevant Date: Associated FactQ218: Who was (1) 1893: First patented in 1893 by Whitcomb Judson, the Clasp Locker was notoriously unreliableWhitcomb Judson?
and expensive(2) 1891: the zipper was invented by Whitcomb JudsonQ239: Who is (1) February 21 1936: Barbara Jordan was born in Houston, TexasBarbara Jordan?
(2) January 17 1996: Barbara Jordan died in Austin, Texas, at the age of 59(4) 1973: Barbara Jordan was diagnosed with multiple sclerosis and was confined to a wheelchair(5) 1976: Barbara Jordan became the first African-American Woman to deliver a keynote address ata political convention(7) 1966: Barbara Jordan became the first black representative since 1883 to win an election tothe Texas legislature(8) 1972: Barbara Jordan was elected to the US CongressQ253: Who is (1) 1770: William Wordsworth was born in 1770 in the town of Cockermouth, EnglandWilliam Wordsworth?
(2) April 7 1770: William Wordsworth was born(4) 1798: Romanticism officially began, when William Wordsworth and Samuel Taylor Coleridgeanonymously published Lyrical Ballads(5) 1802: William Wordsworth married Mary Hutchinson at Brompton church(7) 1795: Coleridge met the poet William Wordsworth(8) April 23 1850: William Wordsworth died(11) 1843: William Wordsworth (1770-1850) was made Poet Laureate of BritainQ346: Who is (1) 1902: Langston Hughes was born in Joplin, MissouriLangston Hughes?
(2) May 22 1967: Langston Hughes died of cancer(5) 1994: The Collected Poems of Langston Hughes was publishedQ351: Who is (1) 1927: aviation hero Charles Lindbergh was honored with a ticker-tape parade in New York CityCharles Lindbergh?
(2) 1932: Charles Lindbergh?s infant son was kidnapped and murdered(3) February 4 1902: Charles Lindbergh was born in Detroit(5) August 26 1974: Charles Lindbergh died(7) May 21 1927: Charles Lindbergh landed in Paris(8) May 20 1927: Charles Lindbergh took off from Long Island(9) May 1927: an airmail pilot named Charles Lindbergh made the first solo flight across the AtlanticOceanQ419: Who was (1) 1977: Goodall founded the Jane Goodall Institute for Wildlife ResearchJane Goodall?
(2) April 3 1934: Jane Goodall was born in London, England(3) 1960: Dr Jane Goodall began studying chimpanzees in east Africa(8) 1985: Jane Goodall ?s twenty-five years of anthropological and conservation research waspublishedTable 1: Temporally-anchored text snippets returned as answers to definition questions5.2 Subjective EvaluationBeyond the snippets that happen to contain the genusof the question concept, the output constitutes sup-plemental results to what other definition QA sys-tems may offer.
The intuition is that prominent factsassociated with the question concept provide use-ful, if not direct answers to the corresponding def-inition question, with the twist of presenting themtogether with the associated date.
For instance, thefirst answer to Q239:?Who is Barbara Jordan??
re-veals her date of birth and is associated with thefirst retrieved date, February 21 1936.
In the objec-tive evaluation, this answer is marked as incorrect.However, some users may find this snippet useful,although they may still prefer the seventh or eighthtext snippets from Table 1 as primary answers, asthey mention Barbara Jordan?s election to a statelegislature in 1966, and to the Congress in 1972.
Asan alternative evaluation, the top five matching snip-pets for each of the top ten dates are inspected man-ually, and answers such as the birth year of a personare subjectively marked as correct.
Overall, 59.1%of the snippets returned for the DefQa1 questions aredeemed correct, which shows that the answers cap-ture useful properties of the question concepts.5.3 Alternative Objective EvaluationA separate objective evaluation was conducted ona set, DefQa2, containing the 24 definition ques-tions asking for information about various people,from the TREC QA track from 2004.
Although cor-rectness assessments are still subjective, they bene-fit from a more rigorous evaluation procedure.
Foreach question, the gold standard consists of sets ofresponses classified according to their importanceinto two classes, namely vital nuggets, containing415information that the assessors feel must be returnedfor the overall output to be good, and non-vital, con-taining information that is acceptable in the outputbut not necessary.Following the official 2004 evaluation proce-dure (Voorhees, 2004), a returned text snippet isconsidered vital, non-vital, or incorrect based onwhether it conceptually matches a vital, non-vitalanswer, or none of the answers specified in the goldstandard for that question.
The overall recall isthe average of individual recall values per question,which are computed as the number of returned vi-tal answers, divided by the number of vital answersfrom the gold standard for a given question.
In thiscase, a returned answer is formed by a date andits top three associated text snippets.
If a vital an-swer from the gold standard matches any of the threesnippets of a returned answer, then the returned an-swer is vital.The overall recall value over DefQa2 is 0.46.
Thecorresponding F-measure, which gives three timesmore importance to recall than to precision as speci-fied in the official evaluation procedure, is 0.39.
Thescore measures favorably against the top three F-measure scores of 0.46, 0.40, and 0.37 reported inthe official 2004 evaluation (Voorhees, 2004).
Thetwo better scores were obtained by systems that relyextensively on human-generated knowledge fromresources such as WordNet (Zhang et al, 2005) andspecific Web glossaries (Cui et al, 2007).
In com-parison, the text snippets retrieved in this paper pro-vide relevant answers to definition questions withthe added benefit of providing a temporal anchorfor each answer, and without using any complex lin-guistic resources and tools.The scores per question vary widely, with the re-trieved snippets containing none of the vital answersfor six questions, all vital answers for other six, andsome fraction of the vital answers for the remain-ing questions.
For example, one of the retrievedtext snippets is ?US Air Force Colonel Eileen MarieCollins was the rst woman to command a spaceshuttle mission?.
The snippet is classified as vitalfor the question about Eileen Marie Collins, since itconceptually matches a vital answer from the goldstandard, namely ?rst woman space shuttle com-mander?.
Again, even though the standard evalua-tion does not require a temporal anchor for an an-swer to be correct, we feel that the dates associatedto the retrieved snippets provide very useful, addi-tional, condensed information.
In the case of EileenMarie Collins, the above-mentioned vital answer isaccompanied by the date 1999, when the missiontook place.6 Related WorkPrevious approaches to answering definition ques-tions from large text collections can be classifiedaccording to the kind of techniques for the extrac-tion of answers.
A significant body of work is ori-ented towards mining descriptive phrases or sen-tences, as opposed to other types of semantic in-formation, for the given question concepts.
To thiseffect, the use of hand-written lexico-syntactic pat-terns and regular expressions, targeting the genusand possibly the differentia of the question concept,is widespread, whether employed for mining defini-tions in English (Liu et al, 2003; Hildebrandt et al,2004) or other languages such as Japanese (Fujii andIshikawa, 2004), from local text collections (Xu etal., 2004) or from the Web (Blair-Goldensohn et al,2004; Androutsopoulos and Galanis, 2005).
Com-paratively, the small set of patterns used here targetstext snippets that are temporally-anchored.
There-fore the text snippets provide answers to definitionanswers without actually employing any specializedmodule for seeking specific information such as thegenus of the question concept.Several studies propose unsupervised extractionmethods as an alternative to using hand-written pat-terns for definition questions (Androutsopoulos andGalanis, 2005; Cui et al, 2007).
Previous workoften relies on external resources as an importantor even essential guide towards the desired out-put.
Such resources include WordNet (Prager etal., 2001) for finding the genus of the question con-cept; large dictionaries such as Merriam Webster,for ready-to-use definitions (Xu et al, 2004; Hilde-brandt et al, 2004); and encyclopedias, for collect-ing words that are likely to occur in potential defini-tions (Fujii and Ishikawa, 2004; Xu et al, 2004).
Incomparison, the experiments reported in this paperdo not require any external lexical resource.4167 ConclusionWithout specifically targeting definitions,temporally-anchored text snippets extracted fromthe Web provide very useful answers to definitionquestions, as measured on standard test questionsets.
Since the snippets tend to capture importantevents involving the question concepts, rather thanphrases that describe the question concept, theycan be employed as either standalone answers, orsupplemental results in conjunction with answersextracted with other techniques.ReferencesI.
Androutsopoulos and D. Galanis.
2005.
A practically unsu-pervised learning method to identify single-snippet answersto definition questions on the Web.
In Proceedings of theHuman Language Technology Conference (HLT-EMNLP-05), pages 323?330, Vancouver, Canada.S.
Blair-Goldensohn, K. McKeown, and A. Schlaikjer, 2004.New Directions in Question Answering, chapter AnsweringDefinitional Questions: a Hybrid Approach, pages 47?58.MIT Press, Cambridge, Massachusetts.T.
Brants.
2000.
TnT - a statistical part of speech tagger.In Proceedings of the 6th Conference on Applied NaturalLanguage Processing (ANLP-00), pages 224?231, Seattle,Washington.Y.
Chen, M. Zhou, and S. Wang.
2006.
Reranking answersfor definitional QA using language modeling.
In Proceed-ings of the 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Association forComputational Linguistics (COLING-ACL-06), pages 1081?1088, Sydney, Australia.H.
Cui, M. Kan, and T. Chua.
2007.
Soft pattern matchingmodels for definitional question answering.
ACM Transac-tions on Information Systems, 25(2).A.
Fujii and T. Ishikawa.
2004.
Summarizing encyclope-dic term descriptions on the Web.
In Proceedings of the20th International Conference on Computational Linguistics(COLING-04), pages 645?651, Geneva, Switzerland.M.
Greenwood and H. Saggion.
2004.
A pattern based ap-proach to answering factoid, list and definition questions.In Proceedings of the 7th Content-Based Multimedia Infor-mation Access Conference (RIAO-04), pages 232?243, Avi-gnon, France.K.
Han, Y.
Song, and H. Rim.
2006.
Probabilistic model fordefinitional question answering.
In Proceedings of the 29thACM Conference on Research and Development in Informa-tion Retrieval (SIGIR-06), pages 212?219, Seattle, Washing-ton.W.
Hildebrandt, B. Katz, and J. Lin.
2004.
Answering defini-tion questions with multiple knowledge sources.
In Proceed-ings of the 2004 Human Language Technology Conference(HLT-NAACL-04), pages 49?56, Boston, Massachusetts.J.
Klavans and Smaranda Mures?an.
2001.
Evaluation ofDefinder: A system to mine definitions from consumer-oriented medical text.
In Proceedings of the 1st ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL-01), pages201?203, Roanoke, Virginia.C.Y.
Lin.
2002.
The effectiveness of dictionary and web-based answer reranking.
In Proceedings of the 19th Interna-tional Conference on Computational Linguistics (COLING-02), pages 1?7, Taipei, Taiwan.B.
Liu, C. Chin, and H.T.
Ng.
2003.
Mining topic-specificconcepts and definitions on the Web.
In Proceedings of the12th International World Wide Web Conference (WWW-03),pages 251?260, Budapest, Hungary.I.
Mani, M. Verhagen, B. Wellner, C. Lee, and J. Pustejovsky.2006.
Machine learning of temporal relations.
In Proceed-ings of the 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Association forComputational Linguistics (COLING-ACL-06), pages 753?760, Sydney, Australia.M.
Pas?ca.
2007.
Lightweight Web-based fact repositoriesfor textual question answering.
In Proceedings of the 16thACM Conference on Information and Knowledge Manage-ment (CIKM-07), Lisboa, Portugal.J.
Prager, D. Radev, and K. Czuba.
2001.
Answering what-isquestions by virtual annotation.
In Proceedings of the 1stHuman Language Technology Conference (HLT-01), pages1?5, San Diego, California.J.
Pustejovsky, J. Castano, R. Ingria, R. Sauri, R. Gaizauskas,A.
Setzer, and G. Katz.
2003.
TimeML: Robust specifica-tion of event and temporal expressions in text.
In Proceed-ings of the 5th International Workshop on Computational Se-mantics (IWCS-5), Tilburg, Netherlands.M.
Remy.
2002.
Wikipedia: The free encyclopedia.
OnlineInformation Review, 26(6):434.E.M.
Voorhees and D.M.
Tice.
2000.
Building a question-answering test collection.
In Proceedings of the 23rd In-ternational Conference on Research and Development inInformation Retrieval (SIGIR-00), pages 200?207, Athens,Greece.E.
Voorhees.
2003.
Evaluating answers to definition questions.In Proceedings of the 2003 Human Language TechnologyConference (HLT-NAACL-03), pages 109?111, Edmonton,Canada.E.M.
Voorhees.
2004.
Overview of the TREC-2004 QuestionAnswering track.
In Proceedings of the 13th Text REtrievalConference (TREC-8), Gaithersburg, Maryland.
NIST.J.
Xu, R. Weischedel, and A. Licuanan.
2004.
Evaluation ofan extraction-based approach to answering definitional ques-tions.
In Proceedings of the 27th ACM Conference on Re-search and Development in Information Retrieval (SIGIR-04), pages 418?424, Sheffield, United Kingdom.Z.
Zhang, Y. Zhou, X. Huang, and L. Wu.
2005.
Answeringdefinition questions using Web knowledge bases.
In Pro-ceedings of the 2nd International Joint Conference on Natu-ral Language Processing (IJCNLP-05), pages 498?506, JejuIsland, Korea.417
