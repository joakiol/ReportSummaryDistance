Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 112?119,New York, June 2006. c?2006 Association for Computational LinguisticsWord Alignment via Quadratic AssignmentSimon Lacoste-JulienUC Berkeley, Berkeley, CA 94720slacoste@cs.berkeley.eduBen TaskarUC Berkeley, Berkeley, CA 94720taskar@cs.berkeley.eduDan KleinUC Berkeley, Berkeley, CA 94720klein@cs.berkeley.eduMichael I. JordanUC Berkeley, Berkeley, CA 94720jordan@cs.berkeley.eduAbstractRecently, discriminative word alignment methodshave achieved state-of-the-art accuracies by extend-ing the range of information sources that can beeasily incorporated into aligners.
The chief advan-tage of a discriminative framework is the abilityto score alignments based on arbitrary features ofthe matching word tokens, including orthographicform, predictions of other models, lexical contextand so on.
However, the proposed bipartite match-ing model of Taskar et al (2005), despite beingtractable and effective, has two important limita-tions.
First, it is limited by the restriction thatwords have fertility of at most one.
More impor-tantly, first order correlations between consecutivewords cannot be directly captured by the model.
Inthis work, we address these limitations by enrich-ing the model form.
We give estimation and infer-ence algorithms for these enhancements.
Our bestmodel achieves a relative AER reduction of 25%over the basic matching formulation, outperform-ing intersected IBM Model 4 without using anyoverly compute-intensive features.
By includingpredictions of other models as features, we achieveAER of 3.8 on the standard Hansards dataset.1 IntroductionWord alignment is a key component of most end-to-end statistical machine translation systems.
Thestandard approach to word alignment is to constructdirectional generative models (Brown et al, 1990;Och and Ney, 2003), which produce a sentence inone language given the sentence in another lan-guage.
While these models require sentence-alignedbitexts, they can be trained with no further super-vision, using EM.
Generative alignment models do,however, have serious drawbacks.
First, they requireextensive tuning and processing of large amountsof data which, for the better-performing models, isa non-trivial resource requirement.
Second, condi-tioning on arbitrary features of the input is difficult;for example, we would like to condition on the or-thographic similarity of a word pair (for detectingcognates), the presence of that pair in various dic-tionaries, the similarity of the frequency of its twowords, choices made by other alignment systems,and so on.Recently, Moore (2005) proposed a discrimina-tive model in which pairs of sentences (e, f) andproposed alignments a are scored using a linearcombination of arbitrary features computed from thetuples (a, e, f).
While there are no restrictions onthe form of the model features, the problem of find-ing the highest scoring alignment is very difficultand involves heuristic search.
Moreover, the param-eters of the model must be estimated using averagedperceptron training (Collins, 2002), which can beunstable.
In contrast, Taskar et al (2005) cast wordalignment as a maximum weighted matching prob-lem, in which each pair of words (ej , fk) in a sen-tence pair (e, f) is associated with a score sjk(e, f)reflecting the desirability of the alignment of thatpair.
Importantly, this problem is computationallytractable.
The alignment for the sentence pair is thehighest scoring matching under constraints (such asthe constraint that matchings be one-to-one).
Thescoring model sjk(e, f) can be based on a rich fea-ture set defined on word pairs (ej , fk) and their con-text, including measures of association, orthogra-phy, relative position, predictions of generative mod-els, etc.
The parameters of the model are estimatedwithin the framework of large-margin estimation; inparticular, the problem turns out to reduce to the112solution of a (relatively) small quadratic program(QP).
The authors show that large-margin estimationis both more stable and more accurate than percep-tron training.While the bipartite matching approach is a use-ful first step in the direction of discriminative wordalignment, for discriminative approaches to com-pete with and eventually surpass the most sophisti-cated generative models, it is necessary to considermore realistic underlying statistical models.
Note inparticular two substantial limitations of the bipartitematching model of Taskar et al (2005): words havefertility of at most one, and there is no way to incor-porate pairwise interactions among alignment deci-sions.
Moving beyond these limitations?while re-taining computational tractability?is the next majorchallenge for discriminative word alignment.In this paper, we show how to overcome both lim-itations.
First, we introduce a parameterized modelthat penalizes different levels of fertility.
While thisextension adds very useful expressive power to themodel, it turns out not to increase the computa-tional complexity of the aligner, for either the pre-diction or the parameter estimation problem.
Sec-ond, we introduce a more thoroughgoing extensionwhich incorporates first-order interactions betweenalignments of consecutive words into the model.
Wedo this by formulating the alignment problem as aquadratic assignment problem (QAP), where in ad-dition to scoring individual edges, we also definescores of pairs of edges that connect consecutivewords in an alignment.
The predicted alignment isthe highest scoring quadratic assignment.QAP is an NP-hard problem, but in the range ofproblem sizes that we need to tackle the problem canbe solved efficiently.
In particular, using standardoff-the-shelf integer program solvers, we are able tosolve the QAP problems in our experiments in undera second.
Moreover, the parameter estimation prob-lem can also be solved efficiently by making use ofa linear relaxation of QAP for the min-max formu-lation of large-margin estimation (Taskar, 2004).We show that these two extensions yield signif-icant improvements in error rates when comparedto the bipartite matching model.
The addition of afertility model improves the AER by 0.4.
Model-ing first-order interactions improves the AER by 1.8.Combining the two extensions results in an improve-ment in AER of 2.3, yielding alignments of betterquality than intersected IBM Model 4.
Moreover,including predictions of bi-directional IBM Model4 and model of Liang et al (2006) as features, weachieve an absolute AER of 3.8 on the English-French Hansards alignment task?the best AER re-sult published on this task to date.2 ModelsWe begin with a quick summary of the maximumweight bipartite matching model in (Taskar et al,2005).
More precisely, nodes V = Vs ?
V t cor-respond to words in the ?source?
(Vs) and ?tar-get?
(V t) sentences, and edges E = {jk : j ?Vs, k ?
V t} correspond to alignments between wordpairs.1 The edge weights sjk represent the degreeto which word j in one sentence can be translatedusing the word k in the other sentence.
The pre-dicted alignment is chosen by maximizing the sumof edge scores.
A matching is represented using aset of binary variables yjk that are set to 1 if wordj is assigned to word k in the other sentence, and 0otherwise.
The score of an assignment is the sum ofedge scores: s(y) = ?jk sjkyjk.
For simplicity, letus begin by assuming that each word aligns to one orzero words in the other sentence; we revisit the issueof fertility in the next section.
The maximum weightbipartite matching problem, arg maxy?Y s(y), canbe solved using combinatorial algorithms for min-cost max-flow, expressed in a linear programming(LP) formulation as follows:max0?z?1?jk?Esjkzjk (1)s.t.
?j?Vszjk ?
1, ?k ?
V t;?k?Vtzjk ?
1, ?j ?
Vs,where the continuous variables zjk are a relax-ation of the corresponding binary-valued variablesyjk.
This LP is guaranteed to have integral (andhence optimal) solutions for any scoring functions(y) (Schrijver, 2003).
Note that although the aboveLP can be used to compute alignments, combina-torial algorithms are generally more efficient.
For1The source/target designation is arbitrary, as the modelsconsidered below are all symmetric.113thebackbone ofoureconomydee?pinedorsalea`notree?conomiethebackbone ofoureconomydee?pinedorsalea`notree?conomie(a) (b)Figure 2: An example fragment that requires fertilitygreater than one to correctly label.
(a) The guess ofthe baseline M model.
(b) The guess of the M+Ffertility-augmented model.example, in Figure 1(a), we show a standard con-struction for an equivalent min-cost flow problem.However, we build on this LP to develop our exten-sions to this model below.
Representing the predic-tion problem as an LP or an integer LP provides aprecise (and concise) way of specifying the modeland allows us to use the large-margin frameworkof Taskar (2004) for parameter estimation describedin Section 3.For a sentence pair x, we denote position pairs byxjk and their scores as sjk.
We let sjk = w>f(xjk)for some user provided feature mapping f and ab-breviate w>f(x,y) = ?jk yjkw>f(xjk).
We caninclude in the feature vector the identity of the twowords, their relative positions in their respective sen-tences, their part-of-speech tags, their string similar-ity (for detecting cognates), and so on.2.1 FertilityAn important limitation of the model in Eq.
(1) isthat in each sentence, a word can align to at mostone word in the translation.
Although it is commonthat words have gold fertility zero or one, it is cer-tainly not always true.
Consider, for example, thebitext fragment shown in Figure 2(a), where back-bone is aligned to the phrase e?pine dorsal.
In thisfigure, outlines are gold alignments, square for surealignments, round for possibles, and filled squaresare target algnments (for details on gold alignments,see Section 4).
When considering only the surealignments on the standard Hansards dataset, 7 per-cent of the word occurrences have fertility 2, and 1percent have fertility 3 and above; when consideringthe possible alignments high fertility is much morecommon?31 percent of the words have fertility 3and above.One simple fix to the original matching model isto increase the right hand sides for the constraintsin Eq.
(1) from 1 to D, where D is the maximumallowed fertility.
However, this change results inan undesirable bimodal behavior, where maximumweight solutions either have all words with fertil-ity 0 or D, depending on whether most scores sjkare positive or negative.
For example, if scores tendto be positive, most words will want to collect asmany alignments as they are permitted.
What themodel is missing is a means for encouraging thecommon case of low fertility (0 or 1), while allowinghigher fertility when it is licensed.
This end can beachieved by introducing a penalty for having higherfertility, with the goal of allowing that penalty tovary based on features of the word in question (suchas its frequency or identity).In order to model such a penalty, we introduceindicator variables zdj?
(and zd?k) with the intendedmeaning: node j has fertility of at least d (and nodek has fertility of at least d).
In the following LP, weintroduce a penalty of?2?d?D sdj?zdj?
for fertilityof node j, where each term sdj?
?
0 is the penaltyincrement for increasing the fertility from d ?
1 tod:max0?z?1?jk?Esjkzjk (2)??j?Vs,2?d?Dsdj?zdj?
??k?Vt,2?d?Dsd?kzd?ks.t.
?j?Vszjk ?
1 +?2?d?Dzd?k, ?k ?
V t;?k?Vtzjk ?
1 +?2?d?Dzdj?, ?j ?
Vs.We can show that this LP always has integral so-lutions by a reduction to a min-cost flow problem.The construction is shown in Figure 1(b).
To ensurethat the new variables have the intended semantics,we need to make sure that sdj?
?
sd?j?
if d ?
d?,so that the lower cost zdj?
is used before the highercost zd?j?
to increase fertility.
This restriction im-114(a) (b) (c)Figure 1: (a) Maximum weight bipartite matching as min-cost flow.
Diamond-shaped nodes represent flowsource and sink.
All edge capacities are 1, with edges between round nodes (j, k) have cost ?sjk, edgesfrom source and to sink have cost 0.
(b) Expanded min-cost flow graph with new edges from source and tosink that allow fertility of up to 3.
The capacities of the new edges are 1 and the costs are 0 for solid edgesfrom source and to sink, s2j?, s2?k for dashed edges, and s3j?, s3?k for dotted edges.
(c) Three types of pairsof edges included in the QAP model, where the nodes on both sides correspond to consecutive words.formorethan ayeardepuisplusdeunanformorethan ayeardepuisplusdeunan(a) (b)Figure 3: An example fragment with a monotonicgold alignment.
(a) The guess of the baseline Mmodel.
(b) The guess of the M+Q quadratic model.plies that the penalty must be monotonic and convexas a function of the fertility.To anticipate the results that we report in Sec-tion 4, adding fertility to the basic matching modelmakes the target algnment of the backbone examplefeasible and, in this case, the model correctly labelsthis fragment as shown in Figure 2(b).2.2 First-order interactionsAn even more significant limitation of the modelin Eq.
(1) is that the edges interact only indi-rectly through the competition induced by the con-straints.
Generative alignment models like theHMM model (Vogel et al, 1996) and IBM models 4and above (Brown et al, 1990; Och and Ney, 2003)directly model correlations between alignments ofconsecutive words (at least on one side).
For exam-ple, Figure 3 shows a bitext fragment whose goldalignment is strictly monotonic.
This monotonicityis quite common ?
46% of the words in the hand-aligned data diagonally follow a previous alignmentin this way.
We can model the common local align-ment configurations by adding bonuses for pairs ofedges.
For example, strictly monotonic alignmentscan be encouraged by boosting the scores of edgesof the form ?
(j, k), (j + 1, k + 1)?.
Another trend,common in English-French translation (7% on thehand-aligned data), is the local inversion of nounsand adjectives, which typically involves a pair ofedges ?
(j, k + 1), (j + 1, k)?.
Finally, a word in onelanguage is often translated as a phrase (consecutivesequence of words) in the other language.
This pat-tern involves pairs of edges with the same origin onone side: ?
(j, k), (j, k+1)?
or ?
(j, k), (j+1, k)?.
Allthree of these edge pair patterns are shown in Fig-ure 1(c).
Note that the set of such edge pairs Q ={jklm : |j ?
l| ?
1, |k ?
m| ?
1} is of linear sizein the number of edges.Formally, we add to the model variables zjklmwhich indicate whether both edge jk and lm are inthe alignment.
We also add a corresponding scoresjklm, which we assume to be non-negative, sincethe correlations we described are positive.
(Nega-tive scores can also be used, but the resulting for-mulation we present below would be slightly differ-ent.)
To enforce the semantics zjklm = zjkzlm, weuse a pair of constraints zjklm ?
zjk; zjklm ?
zlm.Since sjklm is positive, at the optimum, zjklm =115min(zjk, zlm).
If in addition zjk, zlm are integral (0or 1), then zjklm = zjkzlm.
Hence, solving the fol-lowing LP as an integer linear program will find theoptimal quadratic assignment for our model:max0?z?1?jk?Esjkzjk +?jklm?Qsjklmzjklm (3)s.t.
?j?Vszjk ?
1, ?k ?
V t;?k?Vtzjk ?
1, ?j ?
Vs;zjklm ?
zjk, zjklm ?
zlm, ?jklm ?
Q.Note that we can also combine this extension withthe fertility extension described above.To once again anticipate the results presented inSection 4, the baseline model of Taskar et al (2005)makes the prediction given in Figure 3(a) becausethe two missing alignments are atypical translationsof common words.
With the addition of edge pairfeatures, the overall monotonicity pushes the align-ment to that of Figure 3(b).3 Parameter estimationTo estimate the parameters of our model, we fol-low the large-margin formulation of Taskar (2004).Our input is a set of training instances {(xi,yi)}mi=1,where each instance consists of a sentence pair xiand a target algnment yi.
We would like to findparameters w that predict correct alignments on thetraining data: yi = arg maxy?i?Yiw>f(xi, y?i) for each i,where Yi is the space of matchings for the sentencepair xi.In standard classification problems, we typicallymeasure the error of prediction, `(yi, y?i), using thesimple 0-1 loss.
In structured problems, where weare jointly predicting multiple variables, the loss isoften more complex.
While the F-measure is a nat-ural loss function for this task, we instead chose asensible surrogate that fits better in our framework:weighted Hamming distance, which counts the num-ber of variables in which a candidate solution y?
dif-fers from the target output y, with different penaltyfor false positives (c+) and false negatives (c?
):`(y, y?)
=?jk[c+(1 ?
yjk)y?jk + c?
(1 ?
y?jk)yjk].We use an SVM-like hinge upper bound onthe loss `(yi, y?i), given by maxy?i?Yi [w>fi(y?i) +`i(y?i) ?
w>fi(yi)], where `i(y?i) = `(yi, y?i), andfi(y?i) = f(xi, y?i).
Minimizing this upper boundencourages the true alignment yi to be optimal withrespect to w for each instance i:min||w||??
?imaxy?i?Yi[w>fi(y?i) + `i(y?i)] ?
w>fi(yi),where ?
is a regularization parameter.In this form, the estimation problem is a mixtureof continuous optimization over w and combinato-rial optimization over yi.
In order to transform itinto a more standard optimization problem, we needa way to efficiently handle the loss-augmented in-ference, maxy?i?Yi [w>fi(y?i) + `i(y?i)].
This opti-mization problem has precisely the same form as theprediction problem whose parameters we are tryingto learn ?
maxy?i?Yi w>fi(y?i) ?
but with an addi-tional term corresponding to the loss function.
Ourassumption that the loss function decomposes overthe edges is crucial to solving this problem.
We omitthe details here, but note that we can incorporate theloss function into the LPs for various models we de-scribed above and ?plug?
them into the large-marginformulation by converting the estimation probleminto a quadratic problem (QP) (Taskar, 2004).
ThisQP can be solved using any off-the-shelf solvers,such as MOSEK or CPLEX.2 An important differ-ence that comes into play for the estimation of thequadratic assignment models in Equation (3) is thatinference involves solving an integer linear program,not just an LP.
In fact the LP is a relaxation of the in-teger LP and provides an upper bound on the valueof the highest scoring assignment.
Using the LP re-laxation for the large-margin QP formulation is anapproximation, but as our experiments indicate, thisapproximation is very effective.
At testing time, weuse the integer LP to predict alignments.
We havealso experimented with using just the LP relaxationat testing time and then independently rounding eachfractional edge value, which actually incurs no lossin alignment accuracy, as we discuss below.2When training on 200 sentences, the QP we obtain containsroughly 700K variables and 300K constraints and is solved inroughly 10 minutes on a 2.8 GHz Pentium 4 machine.
Aligningthe whole training set with the flow formulation takes a fewseconds, whereas using the integer programming (for the QAPformulation) takes 1-2 minutes.116thehon.memberforVerdunwouldnothavedenigrated mypositionlede?pute?deVerdunneauraitpasde?pre?cie?mapositionthehon.memberforVerdunwouldnothavedenigrated mypositionlede?pute?deVerdunneauraitpasde?pre?cie?mapositionthehon.memberforVerdunwouldnothavedenigrated mypositionlede?pute?deVerdunneauraitpasde?pre?cie?maposition(a) (b) (c)Figure 4: An example fragment with several multiple fertility sure alignments.
(a) The guess of the M+Qmodel with maximum fertility of one.
(b) The guess of the M+Q+F quadratic model with fertility twopermitted.
(c) The guess of the M+Q+F model with lexical fertility features.4 ExperimentsWe applied our algorithms to word-level alignmentusing the English-French Hansards data from the2003 NAACL shared task (Mihalcea and Pedersen,2003).
This corpus consists of 1.1M automaticallyaligned sentences, and comes with a validation set of37 sentence pairs and a test set of 447 sentences.
Thevalidation and test sentences have been hand-aligned(see Och and Ney (2003)) and are marked with bothsure and possible alignments.
Using these align-ments, alignment error rate (AER) is calculated as:(1 ?
|A ?
S| + |A ?
P ||A| + |S|)?
100%.Here, A is a set of proposed index pairs, S is thesure gold pairs, and P is the possible gold pairs.For example, in Figure 4, proposed alignments areshown against gold alignments, with open squaresfor sure alignments, rounded open squares for possi-ble alignments, and filled black squares for proposedalignments.The input to our algorithm is a small number oflabeled examples.
In order to make our results morecomparable with Moore (2005), we split the origi-nal set into 200 training examples and 247 test ex-amples.
We also trained on only the first 100 tomake our results more comparable with the exper-iments of Och and Ney (2003), in which IBM model4 was tuned using 100 sentences.
In all our experi-ments, we used a structured loss function that penal-ized false negatives 10 times more than false posi-tives, where the value of 10 was picked by using avalidation set.
The regularization parameter ?
wasalso chosen using the validation set.4.1 Features and resultsWe parameterized all scoring functions sjk, sdj?,sd?k and sjklm as weighted linear combinations offeature sets.
The features were computed fromthe large unlabeled corpus of 1.1M automaticallyaligned sentences.In the remainder of this section we describe theimprovements to the model performance as variousfeatures are added.
One of the most useful featuresfor the basic matching model is, of course, the set ofpredictions of IBM model 4.
However, computingthese features is very expensive and we would like tobuild a competitive model that doesn?t require them.Instead, we made significant use of IBM model 2 asa source of features.
This model, although not veryaccurate as a predictive model, is simple and cheapto construct and it is a useful source of features.The Basic Matching Model: Edge Features Inthe basic matching model of Taskar et al (2005),called M here, one can only specify features on pairsof word tokens, i.e.
alignment edges.
These features117include word association, orthography, proximity,etc., and are documented in Taskar et al (2005).
Wealso augment those features with the predictions ofIBM Model 2 run on the training and test sentences.We provided features for model 2 trained in eachdirection, as well as the intersected predictions, oneach edge.
By including the IBM Model 2 features,the performance of the model described in Taskar etal.
(2005) on our test set (trained on 200 sentences)improves from 10.0 AER to 8.2 AER, outperformingunsymmetrized IBM Model 4 (but not intersectedmodel 4).As an example of the kinds of errors the baselineM system makes, see Figure 2 (where multiple fer-tility cannot be predicted), Figure 3 (where a prefer-ence for monotonicity cannot be modeled), and Fig-ure 4 (which shows several multi-fertile cases).The Fertility Model: Node Features To addresserrors like those shown in Figure 2, we increasedthe maximum fertility to two using the parameter-ized fertility model of Section 2.1.
The model learnscosts on the second flow arc for each word via fea-tures not of edges but of single words.
The score oftaking a second match for a word w was based onthe following features: a bias feature, the proportionof times w?s type was aligned to two or more wordsby IBM model 2, and the bucketed frequency of theword type.
This model was called M+F.
We also in-cluded a lexicalized feature for words which werecommon in our training set: whether w was everseen in a multiple fertility alignment (more on thisfeature later).
This enabled the system to learn thatcertain words, such as the English not and Frenchverbs like aurait commonly participate in multiplefertility configurations.Figure 5 show the results using the fertility exten-sion.
Adding fertility lowered AER from 8.5 to 8.1,though fertility was even more effective in conjunc-tion with the quadratic features below.
The M+F set-ting was even able to correctly learn some multiplefertility instances which were not seen in the trainingdata, such as those shown in Figure 2.The First-Order Model: Quadratic FeaturesWith or without the fertility model, the model makesmistakes such as those shown in Figure 3, whereatypical translations of common words are not cho-sen despite their local support from adjacent edges.In the quadratic model, we can associate featureswith pairs of edges.
We began with features whichidentify each specific pattern, enabling trends ofmonotonicity (or inversion) to be captured.
We alsoadded to each edge pair the fraction of times thatpair?s pattern (monotonic, inverted, one to two) oc-curred according each version of IBM model 2 (for-ward, backward, intersected).Figure 5 shows the results of adding the quadraticmodel.
M+Q reduces error over M from 8.5 to 6.7(and fixes the errors shown in Figure 3).
When boththe fertility and quadratic extensions were added,AER dropped further, to 6.2.
This final model iseven able to capture the diamond pattern in Figure 4;the adjacent cycle of alignments is reinforced by thequadratic features which boost adjacency.
The ex-ample in Figure 4 shows another interesting phe-nomenon: the multi-fertile alignments for not andde?pute?
are learned even without lexical fertility fea-tures (Figure 4b), because the Dice coefficients ofthose words with their two alignees are both high.However the surface association of aurait with haveis much higher than with would.
If, however, lexi-cal features are added, would is correctly aligned aswell (Figure 4c), since it is observed in similar pe-riphrastic constructions in the training set.We have avoided using expensive-to-compute fea-tures like IBM model 4 predictions up to this point.However, if these are available, our model can im-prove further.
By adding model 4 predictions to theedge features, we get a relative AER reduction of27%, from 6.5 to 4.5.
By also including as featuresthe posteriors of the model of Liang et al (2006), weachieve AER of 3.8, and 96.7/95.5 precision/recall.It is comforting to note that in practice, the burdenof running an integer linear program at test time canbe avoided.
We experimented with using just the LPrelaxation and found that on the test set, only about20% of sentences have fractional solutions and only0.2% of all edges are fractional.
Simple rounding3of each edge value in the LP solution achieves thesame AER as the integer LP solution, while usingabout a third of the computation time on average.3We slightly bias the system on the recall side by rounding0.5 up, but this doesn?t yield a noticeable difference in the re-sults.118Model Prec Rec AERGenerativeIBM 2 (E?F) 73.6 87.7 21.7IBM 2 (F?E) 75.4 87.0 20.6IBM 2 (intersected) 90.1 80.4 14.3IBM 4 (E?F) 90.3 92.1 9.0IBM 4 (F?E) 90.8 91.3 9.0IBM 4 (intersected) 98.0 88.1 6.5Discriminative (100 sentences)Matching (M) 94.1 88.5 8.5M + Fertility (F) 93.9 89.4 8.1M + Quadratic (Q) 94.4 91.9 6.7M + F + Q 94.8 92.5 6.2M + F + Q + IBM4 96.4 94.4 4.5Discriminative (200 sentences)Matching (M) 93.4 89.7 8.2M + Fertility (F) 93.6 90.1 8.0M + Quadratic (Q) 95.0 91.1 6.8M + F + Q 95.2 92.4 6.1M + F + Q + IBM4 96.0 95.0 4.4Figure 5: AER on the Hansards task.5 ConclusionWe have shown that the discriminative approach toword alignment can be extended to allow flexiblefertility modeling and to capture first-order inter-actions between alignments of consecutive words.These extensions significantly enhance the expres-sive power of the discriminative approach; in partic-ular, they make it possible to capture phenomena ofmonotonicity, local inversion and contiguous fertil-ity trends?phenomena that are highly informativefor alignment.
They do so while remaining compu-tationally efficient in practice both for prediction andfor parameter estimation.Our best model achieves a relative AER reduc-tion of 25% over the basic matching formulation,beating intersected IBM Model 4 without the useof any compute-intensive features.
Including Model4 predictions as features, we achieve a further rela-tive AER reduction of 32% over intersected Model4 alignments.
By also including predictions of an-other model, we drive AER down to 3.8.
We arecurrently investigating whether the improvement inAER results in better translation BLEU score.
Al-lowing higher fertility and optimizing a recall bi-ased cost function provide a significant increase inrecall relative to the intersected IBM model 4 (from88.1% to 94.4%), with only a small degradation inprecision.
We view this as a particularly promisingaspect of our work, given that phrase-based systemssuch as Pharaoh (Koehn et al, 2003) perform betterwith higher recall alignments.ReferencesP.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, andP.
S. Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):79?85.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms.
In Proc.
EMNLP.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT-NAACL 2003.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In HLT-NAACL.R.
Mihalcea and T. Pedersen.
2003.
An evaluation exer-cise for word alignment.
In Proceedings of the HLT-NAACL 2003 Workshop, Building and Using parallelTexts: Data Driven Machine Translation and Beyond,pages 1?6, Edmonton, Alberta, Canada.Robert C. Moore.
2005.
A discriminative framework forbilingual word alignment.
In Proc.
HLT/EMNLP.F.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?52.A.
Schrijver.
2003.
Combinatorial Optimization: Poly-hedra and Efficiency.
Springer.B.
Taskar, S. Lacoste-Julien, and D. Klein.
2005.
A dis-criminative matching approach to word alignment.
InEMNLP.B.
Taskar.
2004.
Learning Structured Prediction Mod-els: A Large Margin Approach.
Ph.D. thesis, StanfordUniversity.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In COLING16, pages 836?841.119
