Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 29?38,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsA Performance Study of Cube Pruning for Large-Scale HierarchicalMachine TranslationMatthias Huck1 and David Vilar2 and Markus Freitag1 and Hermann Ney11 Human Language Technology and Pattern 2 DFKI GmbHRecognition Group, RWTH Aachen University Alt-Moabit 91cD-52056 Aachen, Germany D-10559 Berlin, Germany<surname>@cs.rwth-aachen.de david.vilar@dfki.deAbstractIn this paper, we empirically investigate theimpact of critical configuration parameters inthe popular cube pruning algorithm for decod-ing in hierarchical statistical machine transla-tion.
Specifically, we study how the choiceof the k-best generation size affects trans-lation quality and resource requirements inhierarchical search.
We furthermore exam-ine the influence of two different granular-ities of hypothesis recombination.
Our ex-periments are conducted on the large-scaleChinese?English and Arabic?English NISTtranslation tasks.
Besides standard hierarchi-cal grammars, we also explore search with re-stricted recursion depth of hierarchical rulesbased on shallow-1 grammars.1 IntroductionCube pruning (Chiang, 2007) is a widely usedsearch strategy in state-of-the-art hierarchical de-coders.
Some alternatives and extensions to theclassical algorithm as proposed by David Chianghave been presented in the literature since, e.g.
cubegrowing (Huang and Chiang, 2007), lattice-basedhierarchical translation (Iglesias et al 2009b; deGispert et al 2010), and source cardinality syn-chronous cube pruning (Vilar and Ney, 2012).
Stan-dard cube pruning remains the commonly adopteddecoding procedure in hierarchical machine transla-tion research at the moment, though.
The algorithmhas meanwhile been implemented in many publiclyavailable toolkits, as for example in Moses (Koehnet al 2007; Hoang et al 2009), Joshua (Li etal., 2009a), Jane (Vilar et al 2010), cdec (Dyer etal., 2010), Kriya (Sankaran et al 2012), and Niu-Trans (Xiao et al 2012).
While the plain hierar-chical approach to machine translation (MT) is onlyformally syntax-based, cube pruning can also be uti-lized for decoding with syntactically or semanticallyenhanced models, for instance those by Venugopalet al(2009), Shen et al(2010), Xie et al(2011),Almaghout et al(2012), Li et al(2012), Williamsand Koehn (2012), or Baker et al(2010).Here, we look into the following key aspects of hi-erarchical phrase-based translation with cube prun-ing:?
Deep vs. shallow grammar.?
k-best generation size.?
Hypothesis recombination scheme.We conduct a comparative study of all combinationsof these three factors in hierarchical decoding andpresent detailed experimental analyses with respectto translation quality and search efficiency.
We fo-cus on two tasks which are of particular interest tothe research community: the Chinese?English andArabic?English NIST OpenMT translation tasks.The paper is structured as follows: We briefly out-line some important related work in the followingsection.
We subsequently give a summary of thegrammars used in hierarchical phrase-based trans-lation, including a presentation of the difference be-tween a deep and a shallow-1 grammar (Section 3).Essential aspects of hierarchical search with thecube pruning algorithm are explained in Section 4.We show how the k-best generation size is defined(we apply the limit without counting recombined29candidates), and we present the two different hy-pothesis recombination schemes (recombination Tand recombination LM).
Our empirical investiga-tions and findings constitute the major part of thiswork: In Section 5, we first accurately describe oursetup, then conduct a number of comparative exper-iments with varied parameters on the two translationtasks, and finally analyze and discuss the results.
Weconclude the paper in Section 6.2 Related WorkHierarchical phrase-based translation (HPBT) wasfirst proposed by Chiang (2005).
Chiang also in-troduced the cube pruning algorithm for hierarchicalsearch (Chiang, 2007).
It is basically an adaptationof one of the k-best parsing algorithms by Huangand Chiang (2005).
Good descriptions of the cubepruning implementation in the Joshua decoder havebeen provided by Li and Khudanpur (2008) and Liet al(2009b).
Xu and Koehn (2012) implementedhierarchical search with the cube growing algorithmin Moses and compared its performance to Moses?cube pruning implementation.
Heafield et alre-cently developed techniques to speed up hierarchicalsearch by means of an improved language model in-tegration (Heafield et al 2011; Heafield et al 2012;Heafield et al 2013).3 Probabilistic SCFGs for HPBTIn hierarchical phrase-based translation, a proba-bilistic synchronous context-free grammar (SCFG)is induced from a bilingual text.
In addition to con-tinuous lexical phrases, hierarchical phrases withusually up to two gaps are extracted from the word-aligned parallel training data.Deep grammar.
The non-terminal set of a stan-dard hierarchical grammar comprises two symbolswhich are shared by source and target: the initialsymbol S and one generic non-terminal symbol X .Extracted rules of a standard hierarchical grammarare of the form X ?
?
?, ?,?
?
where ?
?, ??
is abilingual phrase pair that may contain X , i.e.
?
?
({X } ?
VF )+ and ?
?
({X } ?
VE)+, where VFand VE are the source and target vocabulary, respec-tively.
The ?
relation denotes a one-to-one corre-spondence between the non-terminals in ?
and in ?.A non-lexicalized initial rule and a special glue rulecomplete the grammar.
We denote standard hierar-chical grammars as deep grammars here.Shallow-1 grammar.
Iglesias et al(2009a) pro-pose a limitation of the recursion depth for hierar-chical rules with shallow grammars.
In a shallow-1grammar, the generic non-terminal X of the stan-dard hierarchical approach is replaced by two dis-tinct non-terminals XH and XP .
By changing theleft-hand sides of the rules, lexical phrases are al-lowed to be derived from XP only, hierarchicalphrases from XH only.
On all right-hand sides ofhierarchical rules, the X is replaced by XP .
Gapswithin hierarchical phrases can thus be filled withcontinuous lexical phrases only, not with hierarchi-cal phrases.
The initial and glue rules are adjustedaccordingly.4 Hierarchical Search with Cube PruningHierarchical search is typically carried out with aparsing-based procedure.
The parsing algorithm isextended to handle translation candidates and to in-corporate language model scores via cube pruning.The cube pruning algorithm.
Cube pruning op-erates on a hypergraph which represents the wholeparsing space.
This hypergraph is built employ-ing a customized version of the CYK+ parsing al-gorithm (Chappelier and Rajman, 1998).
Giventhe hypergraph, cube pruning expands at most kderivations at each hypernode.1 The pseudocodeof the k-best generation step of the cube pruningalgorithm is shown in Figure 1.
This function iscalled in bottom-up topological order for all hy-pernodes.
A heap of active derivations A is main-tained.
A initially contains the first-best derivationsfor each incoming hyperedge (line 1).
Active deriva-tions are processed in a loop (line 3) until a limit kis reached or A is empty.
If a candidate deriva-tion d is recombinable, the RECOMBINE auxiliaryfunction recombines it and returns true; otherwise(for non-recombinable candidates) RECOMBINE re-turns false.
Non-recombinable candidates are ap-pended to the list D of k-best derivations (line 6).This list will be sorted before the function terminates1The hypergraph on which cube pruning operates can beconstructed based on other techniques, such as tree automata,but CYK+ parsing is the dominant approach.30(line 8).
The PUSHSUCC auxiliary function (line 7)updates A with the next best derivations following dalong the hyperedge.
PUSHSUCC determines thecube order by processing adjacent derivations in aspecific sequence (of predecessor hypernodes alongthe hyperedge and phrase translation options).2k-best generation size.
Candidate derivations aregenerated by cube pruning best-first along the in-coming hyperedges.
A problem results from the lan-guage model integration, though: As soon as lan-guage model context is considered, monotonicityproperties of the derivation cost can no longer beguaranteed.
Thus, even for single-best translation,k-best derivations are collected to a buffer in a beamsearch manner and finally sorted according to theircost.
The k-best generation size is consequently acrucial parameter to the cube pruning algorithm.Hypothesis recombination.
Partial hypotheseswith states that are indistinguishable from each otherare recombined during search.
We define two no-tions of when to consider two derivations as indis-tinguishable, and thus when to recombine them:Recombination T. The T recombination schemerecombines derivations that produce identicaltranslations.Recombination LM.
The LM recombinationscheme recombines derivations with identicallanguage model context.Recombination is conducted within the loop ofthe k-best generation step of cube pruning.
Re-combined derivations do not increment the gener-ation count; the k-best generation limit is thus ef-fectively applied after recombination.3 In general,more phrase translation candidates per hypernodeare being considered (and need to be rated with thelanguage model) in the recombination LM schemecompared to the recombination T scheme.
The morepartial hypotheses can be recombined, the more it-erations of the inner code block of the k-best gen-eration loop are possible.
The same internal k-best2See Vilar (2011) for the pseudocode of the PUSHSUCCfunction and other details which are omitted here.3Whether recombined derivations contribute to the genera-tion count or not is a configuration decision (or implementa-tion decision).
Please note that some publicly available toolkitscount recombined derivations by default.Input: a hypernode and the size k of the k-best listOutput: D, a list with the k-best derivations1 let A?
heap({(e,1|e|) | e ?
incoming edges)})2 let D ?
[ ]3 while |A| > 0 ?
|D| < k do4 d?
pop(A)5 if not RECOMBINE(D, d) then6 D ?
D ++ [d]7 PUSHSUCC(d,A)8 sort DFigure 1: k-best generation with the cube pruning al-gorithm.generation size results in a larger search space for re-combination LM.
We will examine how the overallnumber of loop iterations relates to the k-best gener-ation limit.
By measuring the number of derivationsas well as the number of recombination operationson our test sets, we will be able to give an insightinto how large the fraction of recombinable candi-dates is for different configurations.5 ExperimentsWe conduct experiments which evaluate perfor-mance in terms of both translation quality andcomputational efficiency, i.e.
translation speed andmemory consumption, for combinations of deepor shallow-1 grammars with the two hypothesisrecombination schemes and an exhaustive rangeof k-best generation size settings.
Empirical re-sults are presented on the Chinese?English andArabic?English 2008 NIST tasks (NIST, 2008).5.1 Experimental SetupWe work with parallel training corpora of 3.0 MChinese?English sentence pairs (77.5 M Chinese /81.0 M English running words after preprocessing)and 2.5 M Arabic?English sentence pairs (54.3 MArabic / 55.3 M English running words after prepro-cessing), respectively.
Word alignments are createdby aligning the data in both directions with GIZA++and symmetrizing the two trained alignments (Ochand Ney, 2003).
When extracting phrases, we applyseveral restrictions, in particular a maximum lengthof ten on source and target side for lexical phrases,a length limit of five on source and ten on targetside for hierarchical phrases (including non-terminalsymbols), and no more than two gaps per phrase.31Table 1: Data statistics for the test sets.
Numbers havebeen replaced by a special category symbol.Chinese MT08 Arabic MT08Sentences 1 357 1 360Running words 34 463 45 095Vocabulary 6 209 9 387The decoder loads only the best translation optionsper distinct source side with respect to the weightedphrase-level model scores (100 for Chinese, 50 forArabic).
The language models are 4-grams withmodified Kneser-Ney smoothing (Kneser and Ney,1995; Chen and Goodman, 1998) which have beentrained with the SRILM toolkit (Stolcke, 2002).During decoding, a maximum length constraintof ten is applied to all non-terminals except the ini-tial symbol S .
Model weights are optimized withMERT (Och, 2003) on 100-best lists.
The op-timized weights are obtained (separately for deepand for shallow-1 grammars) with a k-best gen-eration size of 1 000 for Chinese?English and of500 for Arabic?English and kept for all setups.We employ MT06 as development sets.
Trans-lation quality is measured in truecase with BLEU(Papineni et al 2002) on the MT08 test sets.Data statistics for the preprocessed source sides ofboth the Chinese?English MT08 test set and theArabic?English MT08 test set are given in Table 1.Our translation experiments are conducted withthe open source translation toolkit Jane (Vilar etal., 2010; Vilar et al 2012).
The core imple-mentation of the toolkit is written in C++.
Wecompiled with GCC version 4.4.3 using its -O2optimization flag.
We employ the SRILM li-braries to perform language model scoring in thedecoder.
In binarized version, the language mod-els have a size of 3.6G (Chinese?English) and 6.2G(Arabic?English).
Language models and phrase ta-bles have been copied to the local hard disks of themachines.
In all experiments, the language modelis completely loaded beforehand.
Loading time ofthe language model and any other initialization stepsare not included in the measured translation time.Phrase tables are in the Jane toolkit?s binarized for-mat.
The decoder initializes the prefix tree struc-ture, required nodes get loaded from secondary stor-age into main memory on demand, and the loadedcontent is being cleared each time a new input sen-tence is to be parsed.
There is nearly no overheaddue to unused data in main memory.
We do notrely on memory mapping.
Memory statistics arewith respect to virtual memory.
The hardware wasequipped with RAM well beyond the requirementsof the tasks, and sufficient memory has been re-served for the processes.5.2 Experimental ResultsFigures 2 and 3 depict how the Chinese?Englishand Arabic?English setups behave in terms oftranslation quality.
The k-best generation size incube pruning is varied between 10 and 10 000.The four graphs in each plot illustrate the resultswith combinations of deep grammar and recombi-nation scheme T, deep grammar and recombinationscheme LM, shallow grammar and recombinationscheme T, as well as shallow grammar and recom-bination scheme LM.
Figures 4 and 5 show the cor-responding translation speed in words per second forthese settings.
The maximum memory requirementsin gigabytes are given in Figures 6 and 7.
In orderto visualize the trade-offs between translation qual-ity and resource consumption somewhat better, weplotted translation quality against time requirementsin Figures 8 and 9 and translation quality againstmemory requirements in Figures 10 and 11.
Transla-tion quality and model score (averaged over all sen-tences; higher is better) are nicely correlated for allconfigurations, as can be concluded from Figures 12through 15.5.3 DiscussionChinese?English.
For Chinese?English trans-lation, the system with deep grammar performs gen-erally a bit better with respect to quality than theshallow one, which accords with the findings ofother groups (de Gispert et al 2010; Sankaran etal., 2012).
The LM recombination scheme yieldsslightly better quality than the T scheme, and withthe shallow-1 grammar it outperforms the T schemeat any given fixed amount of time or memory allo-cation (Figures 8 and 10).Shallow-1 translation is up to roughly 2.5 timesfaster than translation with the deep grammar.
How-ever, the shallow-1 setups are considerably sloweddown at higher k-best sizes as well, while the ef-fort pays off only very moderately.
Overall, the322323.52424.52525.510  100  1000  10000BLEU [%]k-best generation sizeNIST Chinese-to-English (MT08)deep, recombination Tdeep, recombination LMshallow-1, recombination Tshallow-1, recombination LMFigure 2: Chinese?English translation quality (truecase).42.54343.54444.54510  100  1000  10000BLEU [%]k-best generation sizeNIST Arabic-to-English (MT08)deep, recombination Tdeep, recombination LMshallow-1, recombination Tshallow-1, recombination LMFigure 3: Arabic?English translation quality (truecase).012345678910  100  1000  10000wordsper secondk-best generation sizeNIST Chinese-to-English (MT08)deep, recombination Tdeep, recombination LMshallow-1, recombination Tshallow-1, recombination LMFigure 4: Chinese?English translation speed.02468101214161810  100  1000  10000wordsper secondk-best generation sizeNIST Arabic-to-English (MT08)deep, recombination Tdeep, recombination LMshallow-1, recombination Tshallow-1, recombination LMFigure 5: Arabic?English translation speed.081624324010  100  1000  10000gigabytesk-best generation sizeNIST Chinese-to-English (MT08)deep, recombination Tdeep, recombination LMshallow-1, recombination Tshallow-1, recombination LMFigure 6: Chinese?English memory requirements.081624324010  100  1000  10000gigabytesk-best generation sizeNIST Arabic-to-English (MT08)deep, recombination Tdeep, recombination LMshallow-1, recombination Tshallow-1, recombination LMFigure 7: Arabic?English memory requirements.332323.52424.52525.50.125 0.25  0.5  1  2  4  8  16  32BLEU [%]seconds per wordNIST Chinese-to-English (MT08)deep, recombination Tdeep, recombination LMshallow-1, recombination Tshallow-1, recombination LMFigure 8: Trade-off between translation quality and speedfor Chinese?English.42.54343.54444.5450.125 0.25  0.5  1  2  4  8  16  32BLEU [%]seconds per wordNIST Arabic-to-English (MT08)deep, recombination Tdeep, recombination LMshallow-1, recombination Tshallow-1, recombination LMFigure 9: Trade-off between translation quality and speedfor Arabic?English.2323.52424.52525.58  16  32  64BLEU [%]gigabytesNIST Chinese-to-English (MT08)deep, recombination Tdeep, recombination LMshallow-1, recombination Tshallow-1, recombination LMFigure 10: Trade-off between translation quality and mem-ory requirements for Chinese?English.42.54343.54444.54516  32  64  128BLEU [%]gigabytesNIST Arabic-to-English (MT08)deep, recombination Tdeep, recombination LMshallow-1, recombination Tshallow-1, recombination LMFigure 11: Trade-off between translation quality and mem-ory requirements for Arabic?English.shallow-1 grammar at a k-best size between 100 and1 000 seems to offer a good compromise of qualityand efficiency.
Deep translation with k = 2000 andthe LM recombination scheme promises high qual-ity translation, but note the rapid memory consump-tion increase beyond k = 1000 with the deep gram-mar.
At k ?
1 000, memory consumption is not anissue in both deep and shallow systems, but transla-tion speed starts to drop at k > 100 already.Arabic?English.
Shallow-1 translation producescompetitive quality for Arabic?English translation(de Gispert et al 2010; Huck et al 2011).
TheLM recombination scheme boosts the BLEU scoresslightly.
The systems with deep grammar are sloweddown strongly with every increase of the k-best size.Their memory consumption likewise inflates early.We actually stopped running experiments with deepgrammars for Arabic?English at k = 7000 for theT recombination scheme, and at k = 700 for the LMrecombination scheme because 124G of memory didnot suffice any more for higher k-best sizes.
Thememory consumption of the shallow systems staysnearly constant across a large range of the surveyedk-best sizes, but Figure 11 reveals a plateau wheremore resources do not improve translation quality.Increasing k from 100 to 2 000 in the shallow setupwith LM recombination provides half a BLEU point,but reduces speed by a factor of more than 10.342323.52424.52525.5-8.7 -8.65 -8.6 -8.55 -8.5 -8.45 -8.4BLEU [%]average model scoreNIST Chinese-to-English (MT08)deep, recombination Tdeep, recombination LMFigure 12: Relation of translation quality and averagemodel score for Chinese?English (deep grammar).42.54343.54444.545-6.6 -6.5 -6.4 -6.3 -6.2 -6.1BLEU [%]average model scoreNIST Arabic-to-English (MT08)deep, recombination Tdeep, recombination LMFigure 13: Relation of translation quality and averagemodel score for Arabic?English (deep grammar).2323.52424.52525.5-9.4 -9.35 -9.3 -9.25 -9.2 -9.15 -9.1BLEU [%]average model scoreNIST Chinese-to-English (MT08)shallow-1, recombination Tshallow-1, recombination LMFigure 14: Relation of translation quality and averagemodel score for Chinese?English (shallow-1 grammar).42.54343.54444.545-12.1 -12 -11.9 -11.8 -11.7 -11.6BLEU [%]average model scoreNIST Arabic-to-English (MT08)shallow-1, recombination Tshallow-1, recombination LMFigure 15: Relation of translation quality and averagemodel score for Arabic?English (shallow-1 grammar).Actual amount of derivations.
We measured theamount of hypernodes (Table 2), the amount of actu-ally generated derivations after recombination, andthe amount of generated candidate derivations in-cluding recombined ones?or, equivalently, loop it-erations in the algorithm from Figure 1?for se-lected limits k (Tables 3 and 4).
The ratio of theaverage amount of derivations per hypernode afterand before recombination remains consistently atlow values for all recombination T setups.
For thesetups with LM recombination scheme, this recom-bination factor rises with larger k, i.e.
the fractionof recombinable candidates increases.
The increaseis remarkably pronounced for Arabic?English withdeep grammar.
The steep slope of the recombina-tion factor may be interpreted as an indicator for un-desired overgeneration of the deep grammar on theArabic?English task.6 ConclusionWe systematically studied three key aspects of hier-archical phrase-based translation with cube pruning:Deep vs. shallow-1 grammars, the k-best generationsize, and the hypothesis recombination scheme.
Ina series of empirical experiments, we revealed thetrade-offs between translation quality and resourcerequirements to a more fine-grained degree than thisis typically done in the literature.35Table 2: Average amount of hypernodes per sentence and average length of the preprocessed input sentences on theNIST Chinese?English (MT08) and Arabic?English (MT08) tasks.Chinese?English Arabic?Englishdeep shallow-1 deep shallow-1avg.
#hypernodes per sentence 480.5 200.7 896.4 308.4avg.
source sentence length 25.4 33.2Table 3: Detailed statistics about the actual amount of derivations on the NIST Chinese?English task (MT08).deeprecombination T recombination LMavg.
#derivations avg.
#derivations avg.
#derivations avg.
#derivationsper hypernode per hypernode per hypernode per hypernodek (after recombination) (incl.
recombined) factor (after recombination) (incl.
recombined) factor10 10.0 11.7 1.17 10.0 18.2 1.82100 99.9 120.1 1.20 99.9 275.8 2.761000 950.1 1142.3 1.20 950.1 4246.9 4.4710000 9429.8 11262.8 1.19 9418.1 72008.4 7.65shallow-1recombination T recombination LMavg.
#derivations avg.
#derivations avg.
#derivations avg.
#derivationsper hypernode per hypernode per hypernode per hypernodek (after recombination) (incl.
recombined) factor (after recombination) (incl.
recombined) factor10 9.7 11.3 1.17 9.6 13.6 1.41100 90.8 105.2 1.16 90.4 168.6 1.861000 707.3 811.3 1.15 697.4 2143.4 3.0710000 6478.1 7170.4 1.11 6202.8 34165.6 5.51Table 4: Detailed statistics about the actual amount of derivations on the NIST Arabic?English task (MT08).deeprecombination T recombination LMavg.
#derivations avg.
#derivations avg.
#derivations avg.
#derivationsper hypernode per hypernode per hypernode per hypernodek (after recombination) (incl.
recombined) factor (after recombination) (incl.
recombined) factor10 10.0 18.3 1.83 10.0 71.5 7.15100 98.0 177.4 1.81 98.0 1726.0 17.62500 482.1 849.0 1.76 482.1 14622.1 30.331000 961.8 1675.0 1.74 ?
?
?shallow-1recombination T recombination LMavg.
#derivations avg.
#derivations avg.
#derivations avg.
#derivationsper hypernode per hypernode per hypernode per hypernodek (after recombination) (incl.
recombined) factor (after recombination) (incl.
recombined) factor10 9.6 12.1 1.26 9.6 16.6 1.73100 80.9 105.2 1.30 80.2 193.8 2.421000 690.1 902.1 1.31 672.1 2413.0 3.5910000 5638.6 7149.5 1.27 5275.1 31283.6 5.9336AcknowledgmentsThis work was partly achieved as part of the QuaeroProgramme, funded by OSEO, French State agencyfor innovation.
This material is also partly basedupon work supported by the DARPA BOLT projectunder Contract No.
HR0011-12-C-0015.
Any opin-ions, findings and conclusions or recommendationsexpressed in this material are those of the authorsand do not necessarily reflect the views of theDARPA.
The research leading to these results hasreceived funding from the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement no 287658.ReferencesHala Almaghout, Jie Jiang, and Andy Way.
2012.
Ex-tending CCG-based Syntactic Constraints in Hierar-chical Phrase-Based SMT.
In Proc.
of the AnnualConf.
of the European Assoc.
for Machine Translation(EAMT), pages 193?200, Trento, Italy, May.Kathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie Dorr, Nathaniel Filardo, LoriLevin, Scott Miller, and Christine Piatko.
2010.Semantically-Informed Syntactic Machine Transla-tion: A Tree-Grafting Approach.
In Proc.
of the Conf.of the Assoc.
for Machine Translation in the Americas(AMTA), Denver, CO, USA, October/November.Jean-Ce?dric Chappelier and Martin Rajman.
1998.
AGeneralized CYK Algorithm for Parsing StochasticCFG.
In Proc.
of the First Workshop on Tabulation inParsing and Deduction, pages 133?137, Paris, France,April.Stanley F. Chen and Joshua Goodman.
1998.
An Em-pirical Study of Smoothing Techniques for LanguageModeling.
Technical Report TR-10-98, ComputerScience Group, Harvard University, Cambridge, MA,USA, August.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Proc.
ofthe Annual Meeting of the Assoc.
for ComputationalLinguistics (ACL), pages 263?270, Ann Arbor, MI,USA, June.David Chiang.
2007.
Hierarchical Phrase-Based Trans-lation.
Computational Linguistics, 33(2):201?228,June.Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,Eduardo R. Banga, and William Byrne.
2010.
Hierar-chical Phrase-Based Translation with Weighted Finite-State Transducers and Shallow-n Grammars.
Compu-tational Linguistics, 36(3):505?533.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec:A Decoder, Alignment, and Learning framework forfinite-state and context-free translation models.
InProc.
of the ACL 2010 System Demonstrations, pages7?12, Uppsala, Sweden, July.Kenneth Heafield, Hieu Hoang, Philipp Koehn, TetsuoKiso, and Marcello Federico.
2011.
Left LanguageModel State for Syntactic Machine Translation.
InProc.
of the Int.
Workshop on Spoken Language Trans-lation (IWSLT), pages 183?190, San Francisco, CA,USA, December.Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012.Language Model Rest Costs and Space-Efficient Stor-age.
In Proc.
of the 2012 Joint Conf.
on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, EMNLP-CoNLL ?12, pages 1169?1178, Jeju Island, Korea,July.Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013.Grouping Language Model Boundary Words to SpeedK-Best Extraction from Hypergraphs.
In Proc.
of theHuman Language Technology Conf.
/ North AmericanChapter of the Assoc.
for Computational Linguistics(HLT-NAACL), Atlanta, GA, USA, June.Hieu Hoang, Philipp Koehn, and Adam Lopez.
2009.A Unified Framework for Phrase-Based, Hierarchical,and Syntax-Based Statistical Machine Translation.
InProc.
of the Int.
Workshop on Spoken Language Trans-lation (IWSLT), pages 152?159, Tokyo, Japan, Decem-ber.Liang Huang and David Chiang.
2005.
Better k-bestParsing.
In Proc.
of the 9th Int.
Workshop on ParsingTechnologies, pages 53?64, October.Liang Huang and David Chiang.
2007.
Forest Rescoring:Faster Decoding with Integrated Language Models.
InProc.
of the Annual Meeting of the Assoc.
for Com-putational Linguistics (ACL), pages 144?151, Prague,Czech Republic, June.Matthias Huck, David Vilar, Daniel Stein, and HermannNey.
2011.
Advancements in Arabic-to-English Hier-archical Machine Translation.
In 15th Annual Confer-ence of the European Association for Machine Trans-lation, pages 273?280, Leuven, Belgium, May.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009a.
Rule Filtering by Pat-tern for Efficient Hierarchical Translation.
In Proc.
ofthe 12th Conf.
of the Europ.
Chapter of the Assoc.
forComputational Linguistics (EACL), pages 380?388,Athens, Greece, March.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009b.
Hierarchical Phrase-Based Translation with Weighted Finite State Trans-37ducers.
In Proc.
of the Human Language TechnologyConf.
/ North American Chapter of the Assoc.
for Com-putational Linguistics (HLT-NAACL), pages 433?441,Boulder, CO, USA, June.Reinhard Kneser and Hermann Ney.
1995.
ImprovedBacking-Off for M-gram Language Modeling.
InProc.
of the International Conf.
on Acoustics, Speech,and Signal Processing, volume 1, pages 181?184, De-troit, MI, USA, May.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open Source Toolkit forStatistical Machine Translation.
In Proc.
of the AnnualMeeting of the Assoc.
for Computational Linguistics(ACL), pages 177?180, Prague, Czech Republic, June.Zhifei Li and Sanjeev Khudanpur.
2008.
A ScalableDecoder for Parsing-Based Machine Translation withEquivalent Language Model State Maintenance.
InProceedings of the Second Workshop on Syntax andStructure in Statistical Translation, SSST ?08, pages10?18, Columbus, OH, USA, June.Zhifei Li, Chris Callison-Burch, Chris Dyer, SanjeevKhudanpur, Lane Schwartz, Wren Thornton, JonathanWeese, and Omar Zaidan.
2009a.
Joshua: An OpenSource Toolkit for Parsing-Based Machine Transla-tion.
In Proc.
of the Workshop on Statistical MachineTranslation (WMT), pages 135?139, Athens, Greece,March.Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur, andWren Thornton.
2009b.
Decoding in Joshua: OpenSource, Parsing-Based Machine Translation.
ThePrague Bulletin of Mathematical Linguistics, (91):47?56, January.Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef vanGenabith.
2012.
Using Syntactic Head Information inHierarchical Phrase-Based Translation.
In Proc.
of theWorkshop on Statistical Machine Translation (WMT),pages 232?242, Montre?al, Canada, June.NIST.
2008.
Open Machine Translation 2008 Evalua-tion.
http://www.itl.nist.gov/iad/mig/tests/mt/2008/.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51, March.Franz Josef Och.
2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
In Proc.
of the An-nual Meeting of the Assoc.
for Computational Linguis-tics (ACL), pages 160?167, Sapporo, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for Automatic Eval-uation of Machine Translation.
In Proc.
of the AnnualMeeting of the Assoc.
for Computational Linguistics(ACL), pages 311?318, Philadelphia, PA, USA, July.Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.2012.
Kriya - An end-to-end Hierarchical Phrase-based MT System.
The Prague Bulletin of Mathemat-ical Linguistics, (97):83?98, April.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2010.String-to-Dependency Statistical Machine Translation.Computational Linguistics, 36(4):649?671, Decem-ber.Andreas Stolcke.
2002.
SRILM ?
an Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.on Spoken Language Processing (ICSLP), volume 3,Denver, CO, USA, September.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference Grammars:Softening Syntactic Constraints to Improve Statisti-cal Machine Translation.
In Proc.
of the HumanLanguage Technology Conf.
/ North American Chap-ter of the Assoc.
for Computational Linguistics (HLT-NAACL), pages 236?244, Boulder, CO, USA, June.David Vilar and Hermann Ney.
2012.
Cardinalitypruning and language model heuristics for hierarchi-cal phrase-based translation.
Machine Translation,26(3):217?254, September.David Vilar, Daniel Stein, Matthias Huck, and HermannNey.
2010.
Jane: Open Source Hierarchical Transla-tion, Extended with Reordering and Lexicon Models.In Proc.
of the Workshop on Statistical Machine Trans-lation (WMT), pages 262?270, Uppsala, Sweden, July.David Vilar, Daniel Stein, Matthias Huck, and HermannNey.
2012.
Jane: an advanced freely available hierar-chical machine translation toolkit.
Machine Transla-tion, 26(3):197?216, September.David Vilar.
2011.
Investigations on Hierarchi-cal Phrase-Based Machine Translation.
Ph.D. the-sis, RWTH Aachen University, Aachen, Germany,November.Philip Williams and Philipp Koehn.
2012.
GHKMRule Extraction and Scope-3 Parsing in Moses.
InProc.
of the Workshop on Statistical Machine Transla-tion (WMT), pages 388?394, Montre?al, Canada, June.Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.
2012.NiuTrans: An Open Source Toolkit for Phrase-basedand Syntax-based Machine Translation.
In Proc.
ofthe ACL 2012 System Demonstrations, pages 19?24,Jeju, Republic of Korea, July.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A NovelDependency-to-String Model for Statistical MachineTranslation.
In Proc.
of the Conf.
on Empirical Meth-ods for Natural Language Processing (EMNLP), pages216?226, Edinburgh, Scotland, UK, July.Wenduan Xu and Philipp Koehn.
2012.
Extending HieroDecoding in Moses with Cube Growing.
The PragueBulletin of Mathematical Linguistics, (98):133?142,October.38
