Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 886?897,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsWhat Can We Get From 1000 Tokens?A Case Study of Multilingual POS Tagging For Resource-Poor LanguagesLong Duong,12Trevor Cohn,1Karin Verspoor,1Steven Bird,1and Paul Cook11Department of Computing and Information Systems,The University of Melbourne2National ICT Australia, Victoria Research Laboratorylduong@student.unimelb.edu.au{t.cohn, karin.verspoor, sbird, paulcook}@unimelb.edu.auAbstractIn this paper we address the problemof multilingual part-of-speech tagging forresource-poor languages.
We use par-allel data to transfer part-of-speech in-formation from resource-rich to resource-poor languages.
Additionally, we use asmall amount of annotated data to learn to?correct?
errors from projected approachsuch as tagset mismatch between lan-guages, achieving state-of-the-art perfor-mance (91.3%) across 8 languages.
Ourapproach is based on modest data require-ments, and uses minimum divergence clas-sification.
For situations where no uni-versal tagset mapping is available, wepropose an alternate method, resultingin state-of-the-art 85.6% accuracy on theresource-poor language Malagasy.1 IntroductionPart-of-speech (POS) tagging is a crucial task fornatural language processing (NLP) tasks, provid-ing basic information about syntax.
SupervisedPOS tagging has achieved great success, reach-ing as high as 95% accuracy for many languages(Petrov et al., 2012).
However, supervised tech-niques need manually annotated data, and thisis either lacking or limited in most resource-poor languages.
Fully unsupervised POS taggingis not yet useful in practice due to low accu-racy (Christodoulopoulos et al., 2010).
In this pa-per, we propose a semi-supervised method to nar-row the gap between supervised and unsupervisedapproaches.
We demonstrate that even a smallamount of supervised data leads to substantial im-provement.Our method is motivated by the availability ofparallel data.
Thanks to the development of mul-tilingual documents from government projects,book translations, multilingual websites, and soforth, parallel data between resource-rich andresource-poor languages is relatively easy to ac-quire.
This parallel data provides the bridge thatpermits us to transfer POS information from aresource-rich to a resource-poor language.Systems that make use of cross-lingual tagprojection typically face several issues, includ-ing mismatches between the tagsets used for thelanguages, artifacts from noisy alignments andcross-lingual syntactic divergence.
Our approachcompensates for these issues by training on asmall amount of annotated data on the target side,demonstrating that only 1k tokens of annotateddata is sufficient to improve performance.We first tag the resource-rich language using asupervised POS tagger.
We then project POS tagsfrom the resource-rich language to the resource-poor language using parallel word alignments.The projected labels are noisy, and so we usevarious heuristics to select only ?good?
trainingexamples.
We train the model in two stages.First, we build a maximum entropy classifier Ton the (noisy) projected data.
Next, we traina supervised classifier P on a small amount ofannotated data (1,000 tokens) in the target lan-guage, using a minimum divergence techniqueto incorporate the first model, T .
Comparedwith the state of the art (T?ackstr?om et al., 2013),we make more-realistic assumptions (e.g.
relyingon a tiny amount of annotated data rather thana huge crowd-sourced dictionary) and use lessparallel data, yet achieve a better overall result.We achieved 91.3% average accuracy over 8 lan-guages, exceeding T?ackstr?om et al.
(2013)?s resultof 88.8%.The test data we employ makes use of map-pings from language-specific POS tag inventoriesto a universal tagset (Petrov et al., 2012).
How-ever, such a mapping might not be available forresource-poor languages.
Therefore, we also pro-886pose a variant of our method which removes theneed for identical tagsets between the projectionmodel T and the correction model P , based ona two-output maximum entropy model over tagpairs.
Evaluating on the resource-poor languageMalagasy, we achieved 85.6% accuracy, exceed-ing the state-of-the-art of 81.2% (Garrette et al.,2013).2 Background and Related WorkThere is a wealth of prior work on multilingualPOS tagging.
The simplest approach takes advan-tage of the typological similarities that exist be-tween languages pairs such as Czech and Russian,or Serbian and Croatian.
They build the tagger?
or estimate part of the tagger ?
on one lan-guage and apply it to the other language (Reddyand Sharoff, 2011, Hana et al., 2004).Yarowsky and Ngai (2001) pioneered the use ofparallel data for projecting tag information froma resource-rich language to a resource-poor lan-guage.
Duong et al.
(2013b) used a similar methodon using sentence alignment scores to rank thegoodness of sentences.
They trained a seed modelfrom a small part of the data, then applied thismodel to the rest of the data using self-trainingwith revision.Das and Petrov (2011) also used parallel databut additionally exploited graph-based label prop-agation to expand the coverage of labelled tokens.Each node in the graph represents a trigram in thetarget language.
Each edge connects two nodeswhich have similar context.
Originally, only somenodes received a label from direct label projection,and then labels were propagated to the rest of thegraph.
They only extracted the dictionary fromthe graph because the labels of nodes are noisy.They used the dictionary as the constraints for afeature-based HMM tagger (Berg-Kirkpatrick etal., 2010).
Both Duong et al.
(2013b) and Das andPetrov (2011) achieved 83.4% accuracy on the testset of 8 European languages.Goldberg et al.
(2008) pointed out that, with thepresence of a dictionary, even an incomplete one,a modest POS tagger can be built using simplemethods such as expectation maximization.
Thisis because most of the time, words have a verylimited number of possible tags, thus a dictionarythat specifies the allowable tags for a word helpsto restrict the search space.
With a gold-standarddictionary, Das and Petrov (2011) achieved an ac-curacy of approximately 94% on the same 8 lan-guages.
The effectiveness of a gold-standard dic-tionary is undeniable, however it is costly to buildone, especially for resource-poor languages.
Li etal.
(2012) used the dictionary from Wiktionary,1acrowd-sourced dictionary.
They scored 84.8% ac-curacy on the same 8 languages.
Currently, Wik-tionary covers over 170 languages, but the cov-erage varies substantially between languages and,unsurprisingly, it is poor for resource-poor lan-guages.
Therefore, relying on Wiktionary is noteffective for building POS taggers for resource-poor languages.T?ackstr?om et al.
(2013) combined both tokeninformation (from direct projected data) and typeconstraints (from Wiktionary?s dictionary) to formthe state-of-the-art multilingual tagger.
They builta tag lattice and used these token and type con-straints to prune it.
The remaining paths are thetraining data for a CRF tagger.
They achieved88.8% accuracy on the same 8 languages.Table 1 summarises the performance of theabove models across all 8 languages.
Note thatthese methods vary in their reliance on externalresources.
Duong et al.
(2013b) use the least, i.e.only the Europarl Corpus (Koehn, 2005).
Das andPetrov (2011) additionally use the United NationParallel Corpus.
Li et al.
(2012) didn?t use any par-allel text but used Wiktionary instead.
T?ackstr?omet al.
(2013) exploited more parallel data than Dasand Petrov (2011) and also used a dictionaryfrom Li et al.
(2012).Another approach for resource-poor languagesis based on the availability of a small amountof annotated data.
Garrette et al.
(2013) built aPOS tagger for Kinyarwanda and Malagasy.
Theydidn?t use parallel data but instead exploited fourhours of manual annotation to build?4,000 tokensor ?3,000 word-types of annotated data.
Thesetokens or word-types were used to build a tag dic-tionary.
They employed label propagation for ex-panding the coverage of this dictionary in a sim-ilar vein to Das and Petrov (2011), but they alsoused an external dictionary.
They built trainingexamples using the combined dictionary and thentrained the tagger on this data.
They achieved81.9% and 81.2% accuracy for Kinyarwanda andMalagasy respectively.
Note that their usage of anexternal dictionary compromises their claim of us-ing only 4 hours of annotation.1http://www.wiktionary.org/887da nl de el it pt es sv AverageDas and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4Duong et al.
(2013b) 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4Li et al.
(2012) 83.3 86.3 85.4 79.2 86.5 84.5 86.4 86.1 84.8T?ackstr?om et al.
(2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages?
Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish(sv) ?
evaluated on CoNLL data (Buchholz and Marsi, 2006).The method we propose in this paper is similarin only using a small amount of annotation.
How-ever, we directly use the annotated data to trainthe model rather than using a dictionary.
We arguethat with a proper ?guide?, we can take advantageof very limited annotated data.2.1 Annotated dataOur annotated data mainly comes from CoNLLshared tasks on dependency parsing (Buchholzand Marsi, 2006).
The language specific tagsetsare mapped into the universal tagset.
We willuse this annotated data mainly for evaluation.
Ta-ble 2 shows the size of annotated data for eachlanguage.
The 8 languages we are consideringin this experiment are not actually resource-poorlanguages.
However, running on these 8 lan-guages makes our system comparable with pre-viously proposed methods.
Nevertheless, we tryto use as few resources as possible, in order tosimulate the situation for resource-poor languages.Later in Section 6 we adapt the approach for Mala-gasy, a truly resource-poor language.2.2 Universal tagsetWe employ the universal tagset from (Petrov etal., 2012) for our experiment.
It consists of 12common tags: NOUN, VERB, ADJ (adjective),ADV (adverb), PRON (pronoun), DET (deter-miner and article), ADP (preposition and post-position), CONJ (conjunctions), NUM (numeri-cal), PRT (particle), PUNC (punctuation) and X(all other categories including foreign words andabbreviations).
Petrov et al.
(2012) provide themapping from each language-specific tagset to theuniversal tagset.The idea of using the universal tagset is of greatuse in multilingual applications, enabling compar-ison across languages.
However, the mapping isnot always straightforward.
Table 2 shows the sizeof the annotated data for each language, the num-ber of tags presented in the data, and the list oftags that are not matched.
We can see that only 8tags are presented in the annotated data for Dan-ish, i.e, 4 tags (DET, PRT, PUNC, and NUM) aremissing.2Thus, a classifier using all 12 tags willbe heavily penalized in the evaluation.Li et al.
(2012) considered this problem andtried to manually modify the Danish mappings.Moreover, PRT is not really a universal tag sinceit only appears in 3 out of the 8 languages.
Planket al.
(2014) pointed out that PRT often gets con-fused with ADP even in English.
We will latershow that the mapping problem causes substantialdegradation in the performance of a POS taggerexploiting parallel data.
The method we presenthere is more target-language oriented: our modelis trained on the target language, in this way, onlyrelevant information from the source language isretained.
Thus, we automatically correct the map-ping, and other incompatibilities arising from in-correct alignments and syntactic divergence be-tween the source and target languages.Lang Size(k) # Tags Not Matchedda 94 8 DET, PRT, PUNC, NUMnl 203 11 PRTde 712 12el 70 12it 76 11 PRTpt 207 11 PRTes 89 11 PRTsv 191 11 DETAVG 205Table 2: The size of annotated data fromCoNLL (Buchholz and Marsi, 2006), and thenumber of tags included and missing for 8 lan-guages.2Many of these are mistakes in the mapping, however,they are indicative of the kinds of issues expected in low-resource languages.8883 Directly Projected Model (DPM)In this section we describe a maximum entropytagger that only uses information from directlyprojected data.3.1 Parallel dataWe first collect Europarl data having English asthe source language, an average of 1.85 millionparallel sentences for each of the 8 language pairs.In terms of parallel data, we use far less data com-pared with other recent work.
Das and Petrov(2011) used Europarl and the ODS United Na-tion dataset, while T?ackstr?om et al.
(2013) addi-tionally used parallel data crawled from the web.The amount of parallel data is crucial for align-ment quality.
Since DPM uses alignments to trans-fer tags from source to target language, the per-formance of DPM (and other models that exploitprojection) largely depends on the quantity of par-allel data.
The ?No LP?
model of Das and Petrov(2011), which only uses directly projected labels(without label propagation), scored 81.3% for 8languages.
However, using the same model butwith more parallel data, T?ackstr?om et al.
(2013)scored 84.9% on the same test set.3.2 Label projectionWe use the standard alignment tool Giza++ (Ochand Ney, 2003) to word align the parallel data.
Weemploy the Stanford POS tagger (Toutanova et al.,2003) to tag the English side of the parallel dataand then project the label to the target side.
It hasbeen confirmed in many studies (T?ackstr?om et al.,2013, Das and Petrov, 2011, Toutanova and John-son, 2008) that directly projected labels are noisy.Thus we need a method to reduce the noise.
Weemploy the strategy of Yarowsky and Ngai (2001)of ranking sentences using a their alignment scoresfrom IBM model 3.Firstly, we want to know how noisy the pro-jected data is.
Thus, we use the test data to builda simple supervised POS tagger using the TnTtagger (Brants, 2000) which employs a second-order Hidden Markov Model (HMM).
We tag theprojected data and compare the label from directprojection and from the TnT tagger.
The labelsfrom the TnT Tagger are considered as pseudo-gold labels.
Column ?Without Mapping?
from Ta-ble 3 shows the average accuracy for the first n-sentences (n = 60k, 100k, 200k, 500k) for 8 lan-guages according to the ranking.
Column ?Cov-erage?
shows the percentages of projected label(the other tokens are Null aligned).
We can seethat when we select more data, both coverage andaccuracy fall.
In other words, using the sentencealignment score, we can rank sentences with highcoverage and accuracy first.
However, even afterranking, the accuracy of projected labels is lessthan 80% demonstrating how noisy the projectedlabels are.Table 3 (column ?With Mapping?)
additionallyshows the accuracy using simple tagset mapping,i.e.
mapping each tag to the tag it is assigned mostfrequently in the test data.
For example DET, PRT,PUNC, NUM, missing from Danish gold data, willbe matched to PRON, X, X, ADJ respectively.
Thissimple matching yields a ?
4% (absolute) im-provement in average accuracy.
This illustrates theimportance of handling tagset mapping carefully.3.3 The modelIn this section, we introduce a maximum entropytagger exploiting the projected data.
We select thefirst 200k sentences from Table 3 for this experi-ment.
This number represents a trade-off betweensize and accuracy.
More sentences provide moreinformation but at the cost of noisier data.
Duonget al.
(2013b) also used sentence alignment scoresto rank sentences.
Their model stabilizes after us-ing 200k sentences.
We conclude that 200k sen-tences is enough and capture most informationfrom the parallel data.Features DescriptionsW@-1 Previous wordW@+1 Next wordW@0 Current wordCAP First character is capitalizedNUMBER Is numberPUNCT Is punctuationSUFFIX@k Suffix up to length 3 (k <= 3)WC Word classTable 4: Feature template for a maximum entropytaggerWe ignore tokens that don?t have labels, whicharise from null alignments and constitute approxi-mately 14% of the data.
The remaining data (?1.4million tokens) are used to train a maximum en-tropy (MaxEnt) model.
MaxEnt is one of thesimplest forms of probabilistic classifier, and isappropriate in this setting due to the incomplete889Data Size (k) Coverage (%) Without Mapping With Mapping60 91.5 79.9 84.2100 89.1 79.4 83.6200 86.1 79.1 82.9500 82.4 78.0 81.5Table 3: The coverage, and POS tagging accuracy with and without tagset mapping of directly projectedlabels, averaged over 8 languages for different data sizesModel da nl de el it pt es sv AvgAll features 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2- Word Class 64.7 82.6 86.6 79.0 82.8 84.6 82.2 76.9 79.9- Suffix 64.0 82.8 86.3 78.1 81.0 85.9 82.3 76.2 79.6- Prev, Next Word 62.6 82.5 87.4 79.0 81.9 86.5 82.2 74.8 79.6- Cap, Num, Punct 64.0 81.9 84.0 78.0 79.1 86.3 81.8 75.6 78.8Table 5: The accuracy of Directed Project Model (DPM) with different feature sets, removing one featureset at a timesequence data.
While sequence models such asHMMs or CRFs can provide more accurate mod-els of label sequences, they impose a more strin-gent training requirement.3We also experimentedwith a first-order linear chain CRF trained on con-tiguous sub-sequences but observed ?
4% (abso-lute) drop in performance.The maximum entropy classifier estimates theprobability of tag t given a word w asP (t|w) =1Z(w)expD?j=1?jfj(w, t) ,where Z(w) =?texp?Dj=1?jfj(w, t) is thenormalization factor to ensure the probabilitiesP (t|w) sum to one.
Here fjis a feature functionand ?jis the weight for this feature, learned aspart of training.
We use Maximum A Posteriori(MAP) estimation to maximize the log likelihoodof the training data, D = {wi, ti}Ni=1, subject to azero-mean Gaussian regularisation term,L = logP (?
)N?i=1P (t(i)|w(i))= ?D?j=1?2j2?2+N?i=1D?j=1?jfj(wi, ti)?
logZ(wi)where the regularisation term limits over-fitting,an important concern when using large feature3T?ackstr?om et al.
(2013) train a CRF on incomplete data,using a tag dictionary heuristic to define a ?gold standard?lattice over label sequences.sets.
For our experiments we set ?2= 1.
We useL-BFGS which performs gradient ascent to maxi-mize L. Table 4 shows the features we consideredfor building the DPM.
We use mkcls, an unsu-pervised method for word class induction which iswidely used in machine translation (Och, 1999).We run mkcls to obtain 100 word classes, usingonly the target language side of the parallel data.Table 5 shows the accuracy of the DPM evalu-ated on 8 languages (?All features model?).
DPMperforms poorly on Danish, probably because ofthe tagset mapping issue discussed above.
TheDPM result of 80.2% accuracy is encouraging,particularly because the model had no explicit su-pervision.To see what features are meaningful for ourmodel, we remove features in turn and reportthe result.
The result in Table 5 disagrees withT?ackstr?om et al.
(2013) on the word class features.They reported a gain of approximately 3% (ab-solute) using the word class.
However, it seemsto us that these features are not especially mean-ingful (at least in the present setting).
Possiblereasons for the discrepancy are that they train theword class model on a massive quantity of exter-nal monolingual data, or their algorithms for wordclustering are better (Uszkoreit and Brants, 2008).We can see that the most informative features areCapitalization, Number and Punctuation.
Thismakes sense because in languages such as Ger-man, capitalization is a strong indicator of NOUN.Number and punctuation features ensure that weclassify NUM and PUNCT tags correctly.8904 Correction ModelIn this section we incorporate the directly pro-jected model into a second correction modeltrained on a small supervised sample of 1,000 an-notated tokens.
Our DPM model is not very accu-rate; as we have discussed it makes many errors,due to invalid or inconsistent tag mappings, noisyalignments, and cross-linguistic syntactic diver-gence.
However, our aim is to see how effectivelywe can exploit the strengths of the DPM modelwhile correcting for its inadequacies using directsupervision.
We select only 1,000 annotated to-kens to reflect a low resource scenario.
A smallsupervised training sample is a more realistic formof supervision than a tag dictionary (noisy or oth-erwise).
Although used in most prior work, a tagdictionary for a new language requires significantmanual effort to construct.
Garrette and Baldridge(2013) showed that a 1,000 token dataset could becollected very cheaply, requiring less than 2 hoursof non-expert time.Our correction model makes use of a mini-mum divergence (MD) model (Berger et al., 1996),a variant of the maximum entropy model whichbiases the target distribution to be similar to astatic reference distribution.
The method has beenused in several language applications includingmachine translation (Foster, 2000) and parsing(Plank and van Noord, 2008, Johnson and Riezler,2000).
These previous approaches have used var-ious sources of reference distribution, e.g., incor-porating information from a simpler model (John-son and Riezler, 2000) or combining in- and out-of-domain models (Plank and van Noord, 2008).Plank and van Noord (2008) concluded that thismethod for adding prior knowledge only workswith high quality reference distributions, other-wise performance suffers.In contrast to these previous approaches, weconsider the specific setting where both thelearned model and the reference model so=P (t|w) are both maximum entropy models.
In thiscase we show that the MD setup can be simplifiedto a regularization term, namely a Gaussian priorwith a non-zero mean.
We model the classificationprobability, P?
(t|w) as the product between a basemodel and a maximum entropy classifier,P?
(t|w) ?
P (t|w) expD?j=1?jfj(w, t)where here we use the DPM model as base modelP (t|w).
Under this setup, where P?uses the samefeatures as P , and both are log-linear models, thissimplifies toP?
(t|w) ?
exp?
?D?j=1?jfj(w, t) +D?j=1?jfj(w, t)???
expD?j=1(?j+ ?j) fj(w, t) (1)where the constant of proportionality is Z?
(w) =?texp?Dj=1(?j+ ?j) fj(w, t).
It is clear thatEquation (1) also defines a maximum entropy clas-sifier, with parameters ?j= ?j+ ?j, and conse-quently this might seem to be a pointless exercise.The utility of this approach arises from the prior:MAP training with a zero mean Gaussian priorover ?
is equivalent to a Gaussian prior over theaggregate weights, ?j?
N (?j, ?2).
This priorenforces parameter sharing between the two mod-els by penalising parameter divergence from theunderlying DPM model ?.
The resulting trainingobjective isLcorr= logP (t|w, ?)?12?2D?j=1(?j?
?j)2which can be easily optimised using standardgradient-based methods, e.g., L-BFGS.
The con-tribution of the regulariser is scaled by the constant12?2.4.1 Regulariser sensitivityCareful tuning of the regularisation term ?2is crit-ical for the correction model, both to limit over-fitting on the very small training sample of 1,000tokens, and to control the extent of the influenceof the DPM model over the correction model.A larger value of ?2lessens the reliance on theDPM and allows for more flexible modelling ofthe training set, while a small value of ?2forcesthe parameters to be close to the DPM estimates atthe expense of data fit.
We expect the best valueto be somewhere between these extremes, and useline-search to find the optimal value for ?2.
Forthis purpose, we hold out 100 tokens from the1,000 instance training set, for use as our devel-opment set for hyper-parameter selection.From Figure 1, we can see that the model per-forms poorly on small values of ?2.
This is under-standable because the small ?2makes the model891lllll l l l ll l0.01 0.1 1 10 70 100 1000100001e+051e+061e+07Variance808488Accuracy (%)l Average AccFigure 1: Sensitivity of regularisation parameter?2against the average accuracy measured on 8languages on the development settoo similar to DPM, which is not very accurate(80.2%).
At the other extreme, if ?2is large, theDPM model is ignored, and the correction modelis equivalent with the supervised model (?
88%accuracy).
We select the value of ?2= 70, whichmaximizes the accuracy on the development set.4.2 The modelUsing the value of ?2= 70, we retrain the modelon the whole 1,000-token training set and evalu-ate the model on the rest of the annotated data.Table 6 shows the performance of DPM, Super-vised model, Correction model and the state-of-the-art model (T?ackstr?om et al., 2013).
The super-vised model trains a maximum entropy tagger us-ing the same features as in Table 4 on this 1000 to-kens.
The only difference between the supervisedmodel and the correction model is that in the cor-rection model we additionally incorporate DPM asthe prior.The supervised model performs surprisinglywell confirming that our features are meaning-ful in distinguishing between tags.
This modelachieves high accuracy on Danish compared withother languages probably because Danish is eas-ier to learn since it contains only 8 tags.
Despitethe fact that the DPM is not very accurate, the cor-rection model consistently outperforms the super-vised model on all considered languages, approx-imately 4.3% (absolute) better on average.
Thisshows that our method of incorporating DPM tothe model is efficient and robust.The correction model performs much bet-ter than the state-of-the-art for 7 languages butll ll l ll ll l l100 300 500 700 1000150020005000100001500050000Data Size65758595Accuracy (%)l Correction ModelSupervised ModelFigure 2: Learning curve for correction model andsupervised model: the x-axis is the size of data(number of tokens); the y-axis is the average ac-curacy measured on 8 languages; the dashed lineshows the data condition reported in Table 6slightly worse for 1 language.
On average weachieve 91.3% accuracy compared with 88.8%for the state-of-the-art, an error rate reduction of22.3%.
This is despite using fewer resources andonly modest supervision.5 AnalysisTagset mismatch In the correction model, weimplicitly resolve the mismatched tagset issue.DPM might contain tags that don?t appear in thetarget language or generally are errors in the map-ping.
However, when incorporating DPM into thecorrection model, only the feature weight of tagsthat appear in the target language are retained.
Ingeneral, because we don?t explicitly do any map-ping between languages, we might have trouble ifthe tagset size of the target language is bigger thanthe source language tagset.
However, this is notthe case for our experiment because we choose En-glish as the source-side and English has the full 12tags.Learning curve We investigate the impact ofthe number of available annotated tokens on thecorrection model.
Figure 2 shows the learningcurve of the correction model and the supervisedmodel.
We can clearly see the differences be-tween 2 models when the size of training data issmall.
For example, at 100 tokens, the differenceis very large, approximately 18% (absolute), it isalso 6% (absolute) better than DPM.
This differ-ence diminishes as we add more data.
This makesense because when we add more data, the super-vised model become stronger, while the effective-892Model da nl de el it pt es sv AvgDPM 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2T?ackstr?om et al.
(2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8Supervised model 90.1 84.6 89.6 88.2 81.4 87.6 88.9 85.4 87.0Correction Model 92.1 91.1 92.5 92.1 89.9 92.5 91.6 88.7 91.3DPM (with dict) 65.2 83.9 87.0 79.1 83.5 87.1 83.0 77.5 80.8Correction Model (with dict) 93.3 92.2 93.7 93.2 92.2 93.1 92.8 90.0 92.6Table 6: The comparison of our Directly Projected Model, Supervised Model, Correction Model and thestate-of-the-art system (T?ackstr?om et al., 2013).
The best performance for each language is shown inbold.
The models that are built with a dictionary are provided for reference.ness of the DPM prior on the correction model iswearing off.
An interesting observation is that thecorrection model is always better, even when weadd massive amounts of annotated data.
At 50,000tokens, when the supervised model reaches 96%accuracy, the correction model is still 0.3% (abso-lute) better, reaching 96.3%.
It means that evenat that high level of confidence, some informa-tion can still be added from DPM to the correc-tion model.
This improvement probably comesfrom the observation that the ambiguity in onelanguage is explained through the alignment.
Italso suggests that this method could improve theperformance of a supervised POS tagger even forresource-rich languages.Our methods are also relevant for annotationprojects for resource-poor languages.
Assumingthat it is very costly to annotate even 100 tokens,applying our methods can save annotation effortbut maintain high performance.
For example, wejust need 100 tokens to match the accuracy of a su-pervised method trained on 700 tokens, or we justneed 500 tokens to match the performance withnearly 2,000 tokens of supervised learning.Our method is simple, but particularly suitablefor resource-poor languages.
We need a smallamount of annotated data for a high performancePOS tagger.
For example, we need only around300 annotated tokens to reach the same accuracyas the state-of-the-art unsupervised POS tagger(88.8%).Tag dictionary Although, it is not our objec-tive to rely on the dictionary, we are interestedin whether the gains from the correction modelstill persist when the DPM performance is im-proved.
We attempt to improve DPM, followingthe method of Li et al.
(2012) by building a tag dic-tionary using Wiktionary.
This dictionary is thenused as a feature which fires for word-tag pairingspresent in the dictionary.
We expect that when weadd this additional supervision, the DPM modelshould perform better.
Table 6 shows the perfor-mance of DPM and the correction model when in-corporating the dictionary.
The DPM model onlyincreases 0.6% absolute but the correction modelincreases 1.3%.
Additionally, it shows that ourmodel can improve further by incorporating exter-nal information where available.CRF Our approach of using simple classifiersbegs the question of whether better results couldbe obtained using sequence models, such as con-ditional random fields (CRFs).
As mentioned pre-viously, a CRF is not well suited for incompletedata.
However, as our second ?correction?
modelis trained on complete sequences, we now con-sider using a CRF in this stage.
The training al-gorithm is as follows: first we estimate the DPMfeature weights on the incomplete data as before,and next we incorporate the feature weights into aCRF trained on the 1,000 annotated tokens.
This iscomplicated by the different feature sets betweenthe MaxEnt classifier and the CRF, however theclassifier uses a strict subset of the CRF features.Thus, we use the minimum divergence prior forthe token level features, and a standard zero-meanprior for the sequence features.
That is, the ob-jective function of the CRF correction model be-comes:Lcorrcrf= logP (t|w, ?)?12?21?j?F1(?j?
?j)2?12?22?j?F2?2j(2)where F1is the set of features referring to onlyone label as in the DPM maxent model and F2is the set of features over label pairs.
The unionof F = F1?
F2is the set of all features forthe CRF.
We perform grid search using held out893data as before for ?21and ?22.
The CRF correc-tion model scores 88.1% compared with 86.5% ofthe supervised CRF model trained on the 1,000tokens.
Clearly, this is beneficial, however, theCRF correction model still performs worse thanthe MaxEnt correction model (91.3%).
We are notsure why but one reason might be overfitting ofthe CRF, due to its large feature set and tiny train-ing sample.
Moreover, this CRF approach is or-thogonal to T?ackstr?om et al.
(2013): we could usetheir CRF model as the DPM model and train theCRF correction model using the same minimumdivergence method, presumably resulting in evenhigher performance.6 Two-output modelGarrette and Baldridge (2013) also use only asmall amount of annotated data, evaluating ontwo resource-poor languages Kinyarwanda (KIN)and Malagasy (MLG).
As a simple baseline, wetrained a maxent supervised classifier on this data,achieving competitive results of 76.4% and 80.0%accuracy compared with their published resultsof 81.9% and 81.2% for KIN and MLG, respec-tively.
Note that the Garrette and Baldridge (2013)method is much more complicated than this base-line, and additionally uses an external dictionary.We want to further improve the accuracy ofMLG using parallel data.
Applying the techniquefrom Section 4 will not work directly, due to thetagset mismatch (the Malagasy tagset contains 24tags) which results in highly different feature sets.Moreover, we don?t have the language expertiseto manually map the tagset.
Thus, in this section,we propose a method capable of handling tagsetmismatch.
For data, we use a parallel English-Malagasy corpus of ?100k sentences,4and thePOS annotated dataset developed by Garrette andBaldridge (2013), which comprises 4230 tokensfor training and 5300 tokens for testing.6.1 The modelTraditionally, MaxEnt classifiers are trained us-ing a single label.5The method we propose istrained with pairs of output labels: one for the4http://www.ark.cs.cmu.edu/global-voices/5Or else a sequence of labels, in the case of a conditionalrandom field (Lafferty et al., 2001).
However, even in thiscase, each token is usually assigned a single label.
An excep-tion is the factorial CRF (Sutton et al., 2007), which modelsseveral co-dependent sequences.
Our approach is equivalentto a factorial CRF without edges between tags for adjacenttokens in the input.Malagasy tag (tM) and one for the universal tag(tU), which are both predicted conditioned on aMalagasy word (wM) in context.
Our two-outputmodel is defined asP (tM, tU|wM) =1Z(wM)exp(D?j=1?jfMj(w, tM)+E?j=1?jfUj(w, tU) +F?j=1?jfBj(w, tM, tU))(3)where fM, fU, fBare the feature functions con-sidering tMonly, tUonly, and over both outputstMand tUrespectively, and Z(wM) is the parti-tion function.
We can think of Eq.
(3) as the com-bination of 3 models: the Malagasy maxent super-vised model, the DPM model, and the tagset map-ping model.
The central idea behind this model isto learn to predict not just the MLG tags, as in astandard supervised model, but also to learn themapping between MLG and the noisy projecteduniversal tags.
Framing this as a two output modelallows for information to flow both ways, such thatconfident taggings in either space can inform theother, and accordingly the mapping weights ?
areoptimised to maximally exploit this effect.One important question is how to obtain la-belled data for training the two-output model, asour small supervised sample of MLG text is onlyannotated for MLG labels tM.
We resolve thisby first learning the DPM model on the projectedlabels, after which we automatically label ourcorrection training set with predicted tags fromthe DPM model.
That is, we augment the an-notated training data from (tM, wM) to become(tM, tU, wM).
This is then used to train the two-output maxent classifier, optimising a MAP ob-jective using standard gradient descent.
Note thatit would be possible to apply the same minimumdivergence technique for the two-output maxentmodel.
In this case the correction model wouldinclude a regularization term over the ?
to bias to-wards the DPM parameters, while ?
and ?
woulduse a zero-mean regularizer.
However, we leavethis for future work.Table 7 summarises the performance of thestate-of-the-art (Garrette et al., 2013), the super-vised model and the two-output maxent modelevaluated on the Malagasy test set.
The two-outputmaxent model performs much better than the su-pervised model, achieving ?5.3% (absolute) im-894Model Accuracy (%)Garrette et al.
(2013) 81.2MaxEnt Supervised 80.02-output MaxEnt (Universal tagset) 85.32-output MaxEnt (Penn tagset) 85.6Table 7: The performance of different models forMalagasy.provement.
An interesting property of this ap-proach is that we can use different tagsets for theDPM.
We also tried the original Penn treebanktagset which is much larger than the universaltagset (48 vs. 12 tags).
We observed a small im-provement reaching 85.6%, suggesting that somepertinent information is lost in the universal tagset.All in all, this is a substantial improvement overthe state-of-the-art result of 81.2% (Garrette et al.,2013) and an error reduction of 23.4%.7 ConclusionIn this paper, we thoroughly review the work onmultilingual POS tagging of the past decade.
Wepropose a simple method for building a POS tag-ger for resource-poor languages by taking advan-tage of parallel data and a small amount of anno-tated data.
Our method also efficiently resolvesthe tagset mismatch issue identified for some lan-guage pairs.
We carefully choose and tune themodel.
Comparing with the state-of-the-art, weare using the more realistic assumption that asmall amount of labelled data can be made avail-able rather than requiring a crowd-sourced dic-tionary.
We use less parallel data which as wepointed out in section 3.1, could have been a hugedisadvantage for us.
Moreover, we did not exploitany external monolingual data.
Importantly, ourmethod is simpler but performs better than previ-ously proposed methods.
With only 1,000 anno-tated tokens, less than 1% of the test data, we canachieve an average accuracy of 91.3% comparedwith 88.8% of the state-of-the-art (error reductionrate ?22%).
Across the 8 languages we are sub-stantially better at 7 and slightly worse at one.
Ourmethod is reliable and could even be used to im-prove the performance of a supervised POS tagger.Currently, we are building the tagger and eval-uating through several layers of mapping.
Eachlayer might introduce some noise which accumu-lates and leads to a biased model.
Moreover,the tagset mappings are not available for manyresource-poor languages.
We therefore also pro-posed a method to automatically match betweentagsets based on a two-output maximum entropymodel.
On the resource-poor language Mala-gasy, we achieved the accuracy of 85.6% com-pared with the state-of-the-art of 81.2% (Garretteet al., 2013).
Unlike their method, we didn?t use anexternal dictionary but instead use a small amountof parallel data.In future work, we would like to improve theperformance of DPM by collecting more paralleldata.
Duong et al.
(2013a) pointed out that usinga different source language can greatly alter theperformance of the target language POS tagger.We would like to experiment with different sourcelanguages other than English.
We assume that wehave 1,000 tokens for each language.
Thus, for the8 languages we considered we will have 8,000 an-notated tokens.
Currently, we treat each languageindependently, however, it might also be interest-ing to find some way to incorporate informationfrom multiple languages simultaneously to buildthe tagger for a single target language.AcknowledgmentsWe would like to thank Dan Garreette, JasonBaldridge and Noah Smith for Malagasy and Kin-yarwanda datasets.
This work was supported bythe University of Melbourne and National ICTAustralia (NICTA).
NICTA is funded by the Aus-tralian Federal and Victoria State Governments,and the Australian Research Council through theICT Centre of Excellence program.
Dr Cohn is therecipient of an Australian Research Council Fu-ture Fellowship (project number FT130101105).895ReferencesTaylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Proceeding ofHLT-NAACL, pages 582?590.Adam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
COMPU-TATIONAL LINGUISTICS, 22:39?71.Thorsten Brants.
2000.
TnT: A statistical part-of-speech tagger.
In Proceedings of the Sixth Con-ference on Applied Natural Language Processing(ANLP ?00), pages 224?231, Seattle, Washington,USA.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsuper-vised pos induction: How far have we come?
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 575?584.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-basedprojections.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies - Volume1, HLT ?11, pages 600?609.Long Duong, Paul Cook, Steven Bird, and PavelPecina.
2013a.
Increasing the quality and quan-tity of source language data for Unsupervised Cross-Lingual POS tagging.
Proceedings of the Sixth In-ternational Joint Conference on Natural LanguageProcessing, pages 1243?1249.
Asian Federation ofNatural Language Processing.Long Duong, Paul Cook, Steven Bird, and PavelPecina.
2013b.
Simpler unsupervised POS taggingwith bilingual projections.
Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics (Volume 2: Short Papers), pages634?639.
Association for Computational Linguis-tics.George Foster.
2000.
A maximum entropy/minimumdivergence translation model.
In Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics, pages 45?52.Dan Garrette and Jason Baldridge.
2013.
Learning apart-of-speech tagger from two hours of annotation.pages 138?147, June.Dan Garrette, Jason Mielens, and Jason Baldridge.2013.
Real-world semi-supervised learning of pos-taggers for low-resource languages.
pages 583?592,August.Yoav Goldberg, Meni Adler, and Michael Elhadad.2008.
Em can find pretty good hmm pos-taggers(when given a good start.
In In Proc.
ACL, pages746?754.Jiri Hana, Anna Feldman, and Chris Brew.
2004.A resource-light approach to Russian morphology:Tagging Russian using Czech resources.
In Pro-ceedings of the 2004 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP ?04),pages 222?229, Barcelona, Spain, July.Mark Johnson and Stefan Riezler.
2000.
Exploit-ing auxiliary distributions in stochastic unification-based grammars.
In Proceedings of the 1st NorthAmerican Chapter of the Association for Computa-tional Linguistics Conference, NAACL 2000, pages154?161.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofthe Tenth Machine Translation Summit (MT SummitX), pages 79?86, Phuket, Thailand.
AAMT.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth Inter-national Conference on Machine Learning, ICML?01, pages 282?289.Shen Li, Jo?ao V. Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?12,pages 1389?1398.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29(1):19?51, March.Franz Josef Och.
1999.
An efficient method for deter-mining bilingual word classes.
In Proceedings of theNinth Conference on European Chapter of the As-sociation for Computational Linguistics, EACL ?99,pages 71?76.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceed-ings of the Eight International Conference on Lan-guage Resources and Evaluation (LREC?12), Istan-bul, Turkey, may.
European Language ResourcesAssociation (ELRA).Barbara Plank and Gertjan van Noord.
2008.
Ex-ploring an auxiliary distribution based approachto domain adaptation of a syntactic disambigua-tion model.
In Coling 2008: Proceedings of theWorkshop on Cross-Framework and Cross-DomainParser Evaluation, CrossParser ?08, pages 9?16.Barbara Plank, Dirk Hovy, and Anders S?gaard.
2014.Learning part-of-speech taggers with inter-annotatoragreement loss.
In Proceedings of the 14th Confer-ence of the European Chapter of the Association for896Computational Linguistics, pages 742?751, Gothen-burg, Sweden, April.Siva Reddy and Serge Sharoff.
2011.
Cross lan-guage POS taggers (and other tools) for Indian lan-guages: An experiment with Kannada using Teluguresources.
In Proceedings of IJCNLP workshop onCross Lingual Information Access: ComputationalLinguistics and the Information Need of Multilin-gual Societies.
(CLIA 2011 at IJNCLP 2011), Chi-ang Mai, Thailand, November.Charles Sutton, Andrew McCallum, and KhashayarRohanimanesh.
2007.
Dynamic conditional randomfields: Factorized probabilistic models for labelingand segmenting sequence data.
J. Mach.
Learn.Res., 8:693?723, May.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
Transactions of the Association for Computa-tional Linguistics, 1:1?12.Kristina Toutanova and Mark Johnson.
2008.
Abayesian lda-based model for semi-supervised part-of-speech tagging.
In J.C. Platt, D. Koller, andY.
Singer a nd S.T.
Roweis, editors, Advances inNeural Information Processing Systems 20, pages1521?1528.
Curran Associates, Inc.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology -Volume 1 (NAACL ?03), pages 173?180, Edmonton,Canada.Jakob Uszkoreit and Thorsten Brants.
2008.
Dis-tributed word clustering for large scale class-basedlanguage modeling in machine translation.
In InACL International Conference Proceedings.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual POS taggers and NP bracketers via robustprojection across aligned corpora.
In Proceedings ofthe Second Meeting of the North American Chapterof the Association for Computational Linguistics onLanguage technologies, NAACL ?01, pages 1?8.897
