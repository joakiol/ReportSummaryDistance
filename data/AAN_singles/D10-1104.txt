Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068?1076,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsAbstractIn this paper we develop an approach to tacklethe problem of verb selection for learners ofEnglish as a second language (ESL) by usingfeatures from the output of Semantic Role La-beling (SRL).
Unlike existing approaches toverb selection that use local features such asn-grams, our approach exploits semantic fea-tures which explicitly model the usage contextof the verb.
The verb choice highly dependson its usage context which is not consistentlycaptured by local features.
We then combinethese semantic features with other local fea-tures under the generalized perceptron learn-ing framework.
Experiments on both in-domain and out-of-domain corpora show thatour approach outperforms the baseline andachieves state-of-the-art performance.11 IntroductionVerbs in English convey actions or states of being.In addition, they also communicate sentiments andimply circumstances, e.g., in ?He got [gained] thescholarship after three interviews.
?, the verb?gained?
may indicate that the ?scholarship?
wascompetitive and required the agent?s efforts; incontrast, ?got?
sounds neutral and less descriptive.
* This work has been done while the author was visiting Mi-crosoft Research Asia.Since verbs carry multiple important functions,misusing them can be misleading, e.g., the nativespeaker could be confused when reading ?I likelooking [reading] books?.
Unfortunately, accord-ing to (Gui and Yang, 2002; Yi et al, 2008), morethan 30% of the errors in the Chinese Learner Eng-lish Corpus (CLEC) are verb choice errors.
Hence,it is useful to develop an approach to automaticallydetect and correct verb selection errors made byESL learners.However, verb selection is a challenging taskbecause verbs often exhibit a variety of usages andeach usage depends on a particular context, whichcan hardly be adequately described by convention-al n-gram features.
For instance, both ?made?
and?received?
can complete ?I have __ a telephonecall.
?, where the usage context can be representedas ?made/received a telephone call?
; however, in?I have __ a telephone call from my boss?, theprepositional phrase ?from my boss?
becomes acritical part of the context, which now cannot bedescribed by n-gram features, resulting in only?received?
being suitable.Some researchers (Tetreault and Chodorow,2008) exploited syntactic information and n-gramfeatures to represent verb usage context.
Yi et al(2008) introduced an unsupervised web-basedproofing method for correcting verb-noun colloca-tion errors.
Brockett et al (2006) employed phrasalStatistical Machine Translation (SMT) techniquesto correct countability errors.
None of their meth-ods incorporated semantic information.SRL-based Verb Selection for ESL1,2Xiaohua Liu, 3Bo Han*, 4Kuan Li*, 5Stephan Hyeonjun Stiller and 2Ming Zhou1School of Computer Science and TechnologyHarbin Institute of Technology2Microsoft Research Asia3Department of Computer Science and Software EngineeringThe University of Melbourne4College of Computer ScienceChongqing University5Computer Science DepartmentStanford University{xiaoliu,  mingzhou, v-kuli}@microsoft.comb.han@pgrad.unimelb.edu.ausstiller@stanford.edu1068Unlike the other papers, we derive features fromthe output of an SRL (M?rquez, 2009) system toexplicitly model verb usage context.
SRL is gener-ally understood as the task of identifying the argu-ments of a given verb and assigning them semanticlabels describing the roles they play.
For example,given a sentence ?I want to watch TV tonight?
andthe target predicate ?watch?, the output of SRLwill be something like ?I [A0] want to watch [tar-get predicate] TV [A1] tonight [AM-TMP].
?,meaning that the action ?watch?
is conducted bythe agent ?I?, on the patient ?TV?, and the actionhappens ?tonight?.We believe that SRL results are excellent fea-tures for characterizing verb usage context forthree reasons: (i) Intuitively, the predicate-argument structures generated by SRL systemscapture major relationships between a verb and itscontextual participants and consequently largelydetermine whether or not the verb usage is proper.For example, in ?I want to watch a match tonight.?,?match?
is the patient of ?watch?, and ?watch ?match?
forms a collocation, suggesting ?watch?
isappropriately used.
(ii) Predicate-argument struc-tures abstract away syntactic differences in sen-tences with similar meanings, and therefore canpotentially filter out lots of noise from the usagecontext.
For example, consider ?I want to watch afootball match on TV tonight?
: if ?match?
is suc-cessfully identified as the agent of ?watch?,?watch ?
football?, which is unrelated to the us-age of ?watch?
in this case, can be easily excludedfrom the usage context.
(iii) Research on SRL hasmade great achievements, including human-annotated training corpora and state-of-the-art sys-tems, which can be directly leveraged.Taking an English sentence as input, our methodfirst generates correction candidates by replacingeach verb with verbs in its pre-defined confusionset; then for every candidate, it extracts SRL-derived features; finally our method scores everycandidate using a linear function trained by thegeneralized perceptron learning algorithm (Collins,2002) and selects the best candidate as output.Experimental results show that SRL-derived fea-tures are effective in verb selection, but we alsoobserve that noise in SRL output adversely in-creases feature space dimensions and the numberof false suggestions.
To alleviate this issue, we uselocal features, e.g., n-gram-related features, andachieve state-of-the-art performance when all fea-tures are integrated.Our contributions can be summarized as follows:1.
We propose to exploit SRL-derived fea-tures to explicitly model verb usage con-text.2.
We propose to use the generalized percep-tron framework to integrate SRL-derived(and other) features  and achieve state-of-the-art performance on both in-domain andout-of-domain test sets.Our paper is organized as follows: In the nextsection, we introduce related work.
In Section 3,we describe our method.
Experimental results andanalysis on both in-domain and out-of-domain cor-pora are presented in Section 4.
Finally, we con-clude our paper with a discussion of future work inSection 5.2 Related WorkSRL results are used in various tasks.
Moldovan etal.
(2004) classify the semantic relations of nounphrases based on SRL.
Ye and Baldwin (2006)apply semantic role?related information to verbsense disambiguation.
Narayanan and Harabagiu(2004) use semantic role structures for questionanswering.
Surdeanu et al (2003) employ predi-cate-argument structures for information extrac-tion.However, in the context of ESL error detectionand correction, little study has been carried out onclearly exploiting semantic information.
Brockettet al (2006) propose the use of the phrasal statisti-cal machine translation (SMT) technique to identi-fy and correct ESL errors.
They devise severalheuristic rules to generate synthetic data from ahigh-quality newswire corpus and then use the syn-thetic data together with their original counterpartsfor SMT training.
The SMT approach on the artifi-cial data set achieves encouraging results for cor-recting countability errors.
Yi et al (2008) use webfrequency counts to identify and correct determinerand verb-noun collocation errors.
Compared withthese methods, our approach explicitly modelsverb usage context by leveraging the SRL output.The SRL-based semantic features are integrated,along with the local features, into the generalizedperceptron model.10693 Our ApproachOur method can be regarded as a pipeline consist-ing of three steps.
Given as input an English sen-tence written by ESL learners, the system firstchecks every verb and generates correction candi-dates by replacing each verb with its confusion set.Then a feature vector that represents verb usagecontext is derived from the outputs of an SRL sys-tem and then multiplied with the feature weightvector trained by the generalized perceptron.
Final-ly, the candidate with the highest score is selectedas the output.3.1 FormulationWe formulate the task as a process of generatingand then selecting correction candidates:?
?
?
?sScores sGENs 'maxarg* ??
(1)Here 's  denotes the input sentence for proofing,?
?
'sGEN  is the set of correction candidates, and?
?sScore  is the linear model trained by the percep-tron learning algorithm, which will be discussed insection 3.4.We call every target verb in 's  a checkpoint.For example, ?sees?
is a checkpoint in ?Jane seesTV every day.?.
Correction candidates are generat-ed by replacing each checkpoint with its confu-sions.
Table 1 shows a sentence with onecheckpoint and the corresponding correction can-didates.Input Jane sees TV every day.Candidates Jane watches TV every day.Jane looks TV every day.
?Table 1.
Correction candidate list.One state-of-the-art SRL system (Riedel andMeza-Ruiz, 2008) is then utilized to extract predi-cate-argument structures for each verb in the input,as illustrated in Table 2.Semantic features are generated by combiningthe predicate with each of its arguments; e.g.,?watches_A0_Jane?, ?sees_A0_Jane?, ?watch-es_A1_TV?
and ?sees_A1_TV?
are semantic fea-tures derived from the semantic roles listed in Ta-ble 2.Sentence Semantic rolesJane sees TV every day Predicate: sees;A0: Jane;A1: TV;Jane watches TV everydayPredicate: watches;A0: Jane;A1: TV;Table 2.
Examples of SRL outputs.At the training stage, each sentence is labeled bythe SRL system.
Each correction candidate s  isrepresented as a feature vector dRs ??
)( , whered  is the total number of features.
The featureweight vector is denoted as dRw??
, and ?
?sScoreis computed as follows:?
?
wssScore ????
)(                        (2)Finally, ?
?sScore  is applied to each candidate,and *s , the one with the highest score, is selectedas the output, as shown in Table 3.Correction candidate Score*s  Jane watches TV every day.
10.8Jane looks TV every day.
0.8Jane reads TV every day.
0.2?
?Table 3.
Correction candidate scoring.In the above framework, the basic idea is togenerate correction candidates with the help of pre-defined confusion sets and apply the global linearmodel to each candidate to compute the degree ofits fitness to the usage context that is representedas features derived from SRL results.To make our idea practical, we need to solve thefollowing three subtasks: (i) generating the confu-sion set that includes possible replacements for agiven verb; (ii) representing the context with se-mantic features and other complementary features;and (iii) training the feature weight.
We will de-scribe our solutions to those subtasks in the rest ofthis section.10703.2 Generation of Verb Confusion SetsVerb confusion sets are used to generate correctioncandidates.
Due to the great number of verbs andtheir diversified usages, manually collecting allverb confusions in all scenarios is prohibitivelytime-consuming.
To focus on the study of the ef-fectiveness of semantic role features, we restrictour research scope to correcting verb selection er-rors made by Chinese ESL learners and select fiftyrepresentative verbs which are among the mostfrequent ones and account for more than 50% ofESL verb errors in the CLEC data set.
For everyselected verb we manually compile a confusion setusing the following data sources:1.
Encarta treasures.
We extract all the syno-nyms of verbs from the Microsoft Encarta Diction-ary, and this forms the major source for ourconfusion sets.2.
English-Chinese Dictionaries.
ESL learnersmay get interference from their mother tongue (Liuet al, 2000).
For example, some Chinese peoplemistakenly say ?see newspaper?, partially becausethe translation of ?see?
co-occurs with ?newspa-per?
in Chinese.
Therefore English verbs in thedictionary sharing more than two Chinese mean-ings are collected.
For example, ?see?
and ?read?are in a confusion set because they share the mean-ings of both ???
(?to see?, ?to read?)
and ????
(?to grasp?)
in Chinese.3.
An SMT translation table.
We extract para-phrasing verb expressions from a phrasal SMTtranslation table learnt from parallel corpora (Ochand Ney, 2004).
This may help us use the implicitsemantics of verbs that SMT can capture but a dic-tionary cannot, such as the fact that the verbNote that verbs in any confusion set that we arenot interested in are dropped, and that the verb it-self is included in its own confusion set.
We leaveit to our future work to automatically constructverb confusions.3.3 Verb Usage Context FeaturesThe verb usage context1 refers to its surroundingtext, which influences the way one understands theexpression.
Intuitively, verb usage context can takethe form of a collocation, e.g., ?watch ?
TV?
in ?Isaw [watched] TV yesterday.?
; it can also simplybe idioms, e.g., we say ?kick one?s habit?
insteadof ?remove one?s habit?.We use features derived from the SRL output torepresent verb usage context.
The SRL system ac-cepts a sentence as input and outputs all argumentsand the semantic roles they play for every verb inthe sentence.
For instance, given the sentence ?Ihave opened an American bank account in Bos-ton.?
and the predicate ?opened?, the output ofSRL is listed in Table 4, where A0 and A1 are twocore roles, representing the agent and patient of anaction, respectively, and other roles starting with?AM-?are adjunct roles, e.g., AM-LOC indicatesthe location of an action.
Predicate-argument struc-tures keep the key participants of a given verbwhile dropping other unrelated words from its us-age context.
For instance, in ?My teacher said Chi-nese is not easy to learn.
?, the SRL systemrecognizes that ?Chinese?
is not the A1-argumentof ?said?.
So ?say _ Chinese?, which is irrelevantto the usage of said, is not extracted as a feature.The SRL system, however, may outputerroneous predicate-argument structures, whichnegatively affect the performance of verbselection.
For instance,  for the sentence ?Hehasn?t done anything but take [make] a lot ofmoney?, ?lot?
is incorrectly identified as the patientof ?take?, making it hard to select ?make?
as theproper verb even though ?make money?
forms asound collocation.
To tackle this issue, we uselocal textual features, namely features related to n-gram, chunk and chunk headword, as shown inTable 5.
Back-off features are generated byreplacing the word with its POS tag to alleviatedata sparseness.1 http://en.wikipedia.org/wiki/Context_(language_use)I have made[opened] an American bank account in Boston .
[A0][Predicate][A1] [AM-LOC]Table 4.
An example of SRL output.1071Table 5.
An example of feature set.3.4 Perceptron LearningWe choose the generalized perceptron algorithm asour training method because of its easy implemen-tation and its capability of incorporating variousfeatures.
However, there are still two concernsabout this perceptron learning approach: its inef-fectiveness in dealing with inseparable samplesand its ignorance of weight normalization that po-tentially limits its ability to generalize.
In section4.4 we show that the training error rate drops sig-nificantly to a very low level after several roundsof training, suggesting that the correct candidatescan almost be separated from others.
We also ob-serve that our method performs well on an out-of-domain test corpus, indicating the good generaliza-tion ability of this method.
We leave it to our fu-ture work to replace perceptron learning with othermodels like Support Vector Machines (Vapnik,1995).In Figure 1,is  is the ith correct sentence withinthe training data.
T and N represent the number oftraining iterations and training examples, respec-tively.
)( isGEN  is the function that outputs all thepossible corrections for the input sentence is  witheach checkpoint substituted by one of its confu-sions, as described in Section 3.1.
We observe thatthe generated candidates sometimes contain rea-sonable outputs for the verb selection task, whichshould be removed.
For instance, in ??
reporterscould not take [make] notes or tape the conversa-tion?, both ?take?
and ?make?
are suitable verbs inthis context.
To fix this issue, we trained a trigramlanguage model using SRILM (Stolcke, 2002) onLDC data21, and calculated the logarithms of thelanguage model score for the original sentence andits artificial manipulations.
We only kept manipu-lations with a language model score that is t lowerthan that of the original sentence.
We experimen-tally set t = 5.Inputs: training examples is , i=1?NInitialization: 0?w?Algorithm:For r= 1.. T, i= 1..NCalculatewso isGens ????
?
)(maxarg )(If os i ?
)()( osww o ?????
?
?Outputs: w?Figure 1.
The perceptron algorithm, adapted from Co-lins (2002).?
in Figure 1 is the feature extraction function.)(o?
and )( is?
are vectors extracted from the out-put and oracle, respectively.
A vector field is filledwith 1 if the corresponding feature exists, or 0 oth-erwise; w?
is the feature weight vector, where posi-tive elements suggest that the correspondingfeatures support the hypothesis that the candidateis correct.The training process is to update w?
, when theoutput differs from the oracle.
For example, wheno is ?I want to look TV?
and is  is ?I want to watchTV?, w?
will be updated.We use the averaged Perceptron algorithm (Col-lins, 2002) to alleviate overfitting on the trainingdata.
The averaged perceptron weight vector isdefined as???
?TrNriwTN ..1,..1i,1 ???
(3)where riw ,?
is the weight vector immediately af-ter the ith sentence in the  rth iteration.2 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?cata-logId=LDC2005T12Local: trigramshave_openedhave_opened_aopened_an_AmericanPRP_VBP_openedVBP_opened_DTopened_DT_JJLocal: chunkhave_openedopened_an_American_investment_bank_accountPRP_openedopened_NNSemantic: SRL derived featuresA0_I_openedopened_A1_accountopened_AM-LOC_in...10724 ExperimentsIn this section, we compare our approach with theSMT-based approach.
Furthermore, we study thecontribution of predicate-argument-relatedfeatures, and the performances on verbs withvarying distance to their arguments.4.1 Experiment PreparationThe training corpus for perceptron learning wastaken from LDC2005T12.
We randomly selectednewswires containing target verbs from the NewYork Times as the training data.
We then used theOpenNLP package31to extract sentences from thenewswire text and to parse them into the corre-sponding tokens, POS tags, and chunks.
The SRLsystem is built according to Riedel and Meza-Ruiz(2008), using the CoNLL-2008 shared task data fortraining.
We assume that the newswire data is ofhigh quality and free of linguistic errors, and final-ly we gathered 20000 sentences that contain any ofthe target verbs we were focusing on.
We experi-mentally set the number of training rounds to T =50.We constructed two sets of testing data for in-domain and out-of-domain test purposes, respec-tively.
To construct the in-domain test data, wefirst collected all the sentences that contain any ofthe verbs we were interested in from the previousunused LDC dataset; then we replaced any targetverb in our list with a verb in its confusion set;next, we used the language-model-based pruningstrategy described in 3.4 to drop possibly correctmanipulations from the test data; and finally werandomly sampled 5000 sentences for testing.To build the out-of-domain test dataset, wegathered 186 samples that contained errors relatedto the verbs we were interested in from Englishblogs written by Chinese and from the CLEC cor-pus, which were then corrected by an English na-tive speaker.
Furthermore, for every errorinvolving the verbs in our target list, both the verband the word that determines the error are markedby the English native speaker.4.2 BaselineWe built up a phrasal SMT system with the wordre-ordering feature disabled, since our task onlyconcerns the substitution of the target verb.
To3 http://opennlp.sourceforge.net/construct the training corpus, we followed the ideain Brockett et al (2006), and applied a similarstrategy described in section 3.4 to the SRL sys-tem?s training data to generate aligned pairs.4.3 Evaluation MetricWe employed the following metrics adapted from(Yi et al, 2008): revised precision (RP), recall ofthe correction (RC) and false alarm (FA).sCheckpoint All of #Proofings Correct of #RP ?
(4)RP reflects how many outputs are correct usag-es.
The output is regarded as a correct suggestion ifand only if it is exactly the same as the answer.Paraphrasing scenarios, for example, the case thatthe output is ?take notes?
and the answer is ?makenotes?, are counted as errors.Errors Total of#Proofings Modified Correct of# RC ?
(5)RC indicates how many erroneous sentences arecorrected among all the errors.
It measures the sys-tem?s coverage of verb selection errors.sCheckpoint All of#sCheckpoint Modified Incorrect of# FA ?
(6)FA is related to the cases where a correct verb ismistakenly replaced by an inappropriate one.
The-se false suggestions are likely to disturb or evenannoy users, and thus should be avoided as muchas possible.4.4 Results and AnalysisThe training error curves of perceptron learningwith different feature sets are shown in Figure 2.They drop to a low error rate and then stabilizeafter a few number of training rounds, indicatingthat most of the cases are linearly separable andthat perceptron learning is applicable to the verbselection task.We conducted feature selection by dropping fea-tures that occur less than N times.
Here N was ex-perimentally set to 5.
We observe that, after featureselection, some useful features such as?watch_A1_TV?
and ?see_A1_TV?
were kept, butsome noisy features like ?Jane_A0_sees?
and?Jane_A0_watches?
were removed, suggesting theeffectiveness of this feature selection approach.1073Figure 2.
Training error curves of the perceptron.We tested the baseline and our approach on thein-domain and out-of-domain corpora.
The resultsare shown in Table 7 and 8, respectively.In the in-domain test, the SMT-based approachhas the highest false alarm rate, though its outputwith word insertions or deletions is not consideredwrong if the substituted verb is correct.
Our ap-proach, regardless of what feature sets are used,outperforms the SMT-based approach in terms ofall metrics, showing the effectiveness of percep-tron learning for the verb selection task.
Under theperceptron learning framework, we can see that thesystem using only SRL-related features has higherrevised precision and recall of correction, but alsoa slightly higher false alarm rate than the systembased on only local features.
When local featuresand SRL-derived features are integrated together,the state-of-the-art performance is achieved with a5% increase in recall, and minor changes in preci-sion and false alarm.In the out-of-domain test, the SMT-based ap-proach performs much better than in the in-domaintest, especially in terms of false alarm rate, indicat-ing the SMT-based approach may favor short sen-tences.
However, its recall drops greatly.
We ob-serve similar performance differences between thesystems with different feature sets under the sameperceptron learning framework, reaffirming theusefulness of the SRL-based features for verb se-lection.We also conducted significance test.
The resultsconfirm that the improvements (SRL+Local vs.SMT-based) are statistically significant (p-value <0.001) for both the open-domain and the in-domainexperiments.Furthermore, we studied the performance of oursystem on verbs with varying distance to their ar-guments on the out-of-domain test corpus.Local d<=2 2<d<=4 d>4RP 64.3% 60.3% 59.4%RC 34.6% 33.1% 28.9%FA 3.0% 6.3% 5.0%SRL d<=2 2<d<=4 d>4RP 65.1% 60.1% 62.1%RC 40.3% 34.0% 36.9%FA 5.0% 6.7% 6.3%Table 9.
Performance on verbs with different distance totheir arguments on out-of-domain test data.Table 9 shows that the system with only SRL-derived features performs significantly better thanthe system with only local features on the verbwhose usage depends on a distant argument, i.e.,one where the number of words between the predi-cate and the argument is larger than 4.
To under-stand the reason, consider the following sentence:?It's raining outside.
Please wear[take] theblack raincoat with you.
?SMT-based Our methodSRL Local SRL + LocalRP 48.4% 64.5% 62.2% 66.4%RC 23.5% 40.2% 32.9% 46.4%FA 13.3% 5.6% 4.2% 6.8%Table 7.
In-domain test results.SMT-based Our methodSRL Local SRL + LocalRP 50.7% 64.0% 62.6% 65.5%RC 13.5% 39.0% 33.3% 44.0%FA 6.1% 5.5% 4.0% 6.5%Table 8.
Out-of-domain test results.1074Intuitively, ?wear?
and ?take?
seem to fill theblank well, since they both form a collocation with?raincoat?
; however, when ?with [AM-MNR] you?is considered as part of the context, ?wear?
nolonger fits it and ?take?
wins.
In this case, the long-distance feature devised from AM-MNR helps se-lect the suitable verb, while the trigram featurescannot because they cannot represent the long dis-tance verb usage context.We also find some typical cases that are beyondthe reach of the SRL-derived features.
For instance,consider ?Everyone doubts [suspects] that Tom isa spy.?.
Both of the verbs can be followed by aclause.
However, the SRL system regards ?is?, thepredicate of the clause, as the patient, resulting infeatures like ?doubt_A1_is?
and ?suspect_A1_is?,which capture nothing about verb usage context.However, if we consider the whole clause ?sus-pect_Tom is a spy?
as the patient, this could resultin a very sparse feature that would be filtered.
Inthe future, we will combine word-level and phrase-level SRL systems to address this problem.Besides its incapability of handling verb selec-tion errors involving clauses, the SRL-derived fea-tures fail to work when verb selection depends ondeep meanings that cannot be captured by currentshallow predicate-argument structures.
For exam-ple, in ?He was wandering in the park, spending[killing] his time watching the children playing.
?,though ?spending?
and ?killing?
fit the syntacticstructure and collocation agreement, and expressthe meaning ?to allocate some time doing some-thing?, the word ?wandering?
suggests that ?kill-ing?
may be more appropriate.
Current SRLsystems cannot represent the semantic connectionbetween two predicates and thus are helpless forthis case.
We argue that the performance of oursystem can be improved along with the progress ofSRL.5 Conclusions and Future WorkVerb selection is challenging because verb usagehighly depends on the usage context, which is hardto capture and represent.
In this paper, we proposeto utilize the output of an SRL system to explicitlymodel verb usage context.
We also propose to usethe generalized perceptron learning framework tointegrate SRL-derived features with other features.Experimental results show that our method outper-forms the SMT-based system and achieves state-of-the-art performance when SRL-related featuresand other local features are integrated.
We alsoshow that, for cases where the particular verb us-age mainly depends on its distant arguments, a sys-tem with only SRL-derived features performsmuch better than the system with only local fea-tures.In the future, we plan to automatically constructconfusion sets, expand our approach to more verbsand test our approach on a larger size of real data.We will try to combine the outputs of several SRLsystems to make our system more robust.
We alsoplan to further validate the effectiveness of theSRL-derived features under other learning methodslike SVMs.AcknowledgmentWe thank the anonymous reviewers for their valu-able comments.
We also thank Changning Huang,Yunbo Cao, Dongdong Zhang, Henry Li and MuLi for helpful discussions.ReferencesFrancis Bond, Kentaro Ogura, and Satoru Ikehara.
1994.Countability and number in Japanese to English ma-chine translation.
Proc.
of the 15th conference onComputational Linguistics, pages 32-38.Chris Brockett, William B. Dolan, and Michael Gamon.2006.
Correcting ESL errors using phrasal SMTtechniques.
Proc.
of the 21st International Confer-ence on Computational Linguistics and the 44th An-nual Meeting on Association for ComputationalLinguistics, pages 249-256.Michael Collins.
2002.
Discriminative training methodsfor hidden Markov models: theory and experimentswith perceptron algorithms.
Proc.
of the ACL-02Conference on Empirical Methods in Natural Lan-guage Processing, pages 1-8.Jens Eeg-Olofsson and Ola Knutsson.
2003.
AutomaticGrammar Checking for Second Language Learners ?the Use of Prepositions.
Proc.
of NoDaliDa.Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-dre Klementiev, William B. Dolan, Dmitrtiy Belen-ko, and Lucy Vanderwende.
2008.
Using ContextualSpeller Techniques and Language Modeling for ESLError Correction.
Proc.
of the International JointConference on Natural Language Processing.Shichun Gui and Huizhong Yang.
2002.
Chinese Learn-er English Corpus.
Shanghai Foreign Languages Ed-ucation Press, Shanghai, China.1075Julia E. Heine.
1998.
Definiteness predictions for Japa-nese noun phrases.
Proc.
of the 36th Annual Meetingof the Association for Computational Linguistics and17th International Conference on ComputationalLinguistics, pages 519-525.John Lee and Stephanie Seneff.
2008.
Correcting mis-use of verb forms.
Proc.
of the 46th Annual Meetingon Association for Computational Linguistics, pages174-182.Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun andChangning Huang.
2000.
PENS: A Machine-aidedEnglish Writing System for Chinese Users.
Proc.
ofthe 38th Annual Meeting on Association for Compu-tational Linguistics, pages 529-536.Llu?s M?rquez.
2009.
Semantic Role Labeling Past,Present and Future, Tutorial of ACL-IJCNLP 2009.Dan Moldovan, Adriana Badulescu, Marta Tatu, DanielAntohe and Roxana Girju.
2004.
Models for the se-mantic classification of noun phrases.
Proc.
of theHLT-NAACL Workshop on Computational LexicalSemantics, pages 60-67.Srini Narayanan and Sanda Harabagiu.
2004.
Questionanswering based on semantic structures.
Proc.
of the20th International Conference on ComputationalLinguistics, pages 693-701.Franz J. Och and Hermann Ney.
2004.
The AlignmentTemplate Approach to Statistical Machine Transla-tion.
Journal of Computational Linguistics, 30(4),pages 417-449.Sebastian Riedel and Ivan Meza-Ruiz.
2008.
Collectivesemantic role labelling with Markov Logic.
Proc.
ofthe Twelfth Conference on Computational NaturalLanguage Learning, pages 193-197.Andreas Stolcke.
2002.
SRILM -- An Extensible Lan-guage Modeling Toolkit.
Proc.
of International Con-ference on Spoken Language Processing, pages: 901-904.Mihai Surdeanu, Lluis M?rquez, Xavier Carreras, andPere R. Comas.
2007.
Combination strategies for se-mantic role labeling.
Journal of Artificial IntelligenceResearch, page 105-151.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argument struc-tures for information extraction.
Proc.
of the 41stAnnual Meeting on Association for ComputationalLinguistics, pages 8-15.Joel R. Tetreault and Martin Chodorow.
2008.
The upsand downs of preposition error detection in ESL writ-ing.
Proc.
of the 22nd international Conference onComputational Linguistics, pages 865-872.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag, New York.Patrick Ye and Timothy Baldwin.
2006.
Verb SenseDisambiguation Using Selectional PreferencesExtracted with a State-of-the-art Semantic RoleLabeler.
Proc.
of the Australasian LanguageTechnology Workshop, pages 141-148.Xing Yi, Jianfeng Gao, and William B. Dolan.
2008.
AWeb-based English Proofing System for English as aSecond Language Users.
Proc.
of International JointConference on Natural Language Processing, pages619-624.1076
