BLEU in characters:towards automatic MT evaluation in languageswithout word delimitersEtienne Denoualetienne.denoual@atr.jpATR ?
Spoken language communication research labsKeihanna gakken tosi, 619-0288 Kyoto, JapanYves Lepageyves.lepage@atr.jpAbstractAutomatic evaluation metrics for Ma-chine Translation (MT) systems, suchas BLEU or NIST, are now well estab-lished.
Yet, they are scarcely used forthe assessment of language pairs likeEnglish-Chinese or English-Japanese,because of the word segmentation prob-lem.
This study establishes the equiv-alence between the standard use ofBLEU in word n-grams and its appli-cation at the character level.
The useof BLEU at the character level elimi-nates the word segmentation problem:it makes it possible to directly comparecommercial systems outputting unseg-mented texts with, for instance, statisti-cal MT systems which usually segmenttheir outputs.1 IntroductionAutomatic evaluation metrics for Machine Trans-lation (MT) systems, such as BLEU (PAPINENIet al, 2001) or NIST (DODDINGTON, 2002), arenow well established.
They serve as quality as-sessment methods or comparison tools and are afast way of measuring improvement.
Althoughit is claimed that such objective MT evaluationmethods are language-independent, they are usu-ally only applied to English, as they basically relyon word counts.
In fact, the organisers of cam-paigns like NIST (PRZYBOCKI, 2004)1, TIDES2or IWSLT (AKIBA et al, 2004)3, prefer to evalu-ate outputs of machine translation systems whichare already segmented into words before apply-ing such objective evaluation methods.
The con-sequence of this state of affairs is that evaluationcampaigns of English to Japanese or English toChinese machine translation systems for instance,are not, to our knowledge, widely seen or re-ported.2 Overview2.1 The word segmentation problemAs statistical machine translation systems basi-cally rely on the notion of words through theirlexicon models (BROWN et al, 1993), they areusually capable of outputting sentences alreadysegmented into words when they translate intolanguages like Chinese or Japanese.
But this isnot necessarily the case with commercial systems.For instance, Systran4 does not output segmentedtexts when it translates into Chinese or Japanese.As such, comparing systems that translate intolanguages where words are not an immediategiven in unprocessed texts, is still hindered bythe human evaluation bottleneck.
To compare theperformance of different systems, segmentationhas to be performed beforehand.1http://www.nist.gov/speech/tests/mt-/doc/mt04 evalplan.v2.1.pdf2http://www.nist.gov/speech/tests/mt-/mt tides01 knight.pdf3http://www.slt.atr.jp/IWSLT2004-/archives/000619.html4http://www.systranbox.com/systran-/box.79One can always apply standard word segmen-tation tools (for instance, The Peking Univer-sity Segmenter for Chinese (DUAN et al, 2003)or ChaSen for Japanese (MATSUMOTO et al,1999)), and then apply objective MT evaluationmethods.
However, the scores obtained would bebiased by the error rates of the segmentation toolson MT outputs5.
Indeed, MT outputs still differfrom standard texts, and their segmentation maylead to a different performance.
Consequently, itis difficult to directly and fairly compare scoresobtained for a system outputting non-segmentedsentences with scores obtained for a system de-livering sentences already segmented into words.2.2 BLEU in charactersNotwithstanding the previous issue, it is unde-niable that methods like BLEU or NIST havebeen adopted by the MT community as theymeasure complementary characteristics of trans-lations: namely fluency and adequacy (AKIBA etal., 2004, p. 7).
Although far from being per-fect, they definitely are automatic, fast, and cheap.For all these reasons, one cannot easily ask theMT community to give up their practical know-how related to such measures.
It is preferable tostate an equivalence with well established mea-sures than to merely look for some correlationwith human scores, which would indeed amountto propose yet another new evaluation method.Characters are always an immediate given inany electronic text of any language, which is notnecessarily the case for words.
Based on this ob-servation, this study shows the effect of shiftingfrom the level of words to the level of charac-ters, i.e., of performing all computations in char-acters instead of words.
According to what wassaid above, the purpose is not to look for anycorrelation with human scores, but to establishan equivalence between BLEU scores obtained intwo ways: on characters and on words.Intuitively a high correlation should exist.
Thecontrary would be surprising.
However, theequivalence has yet to be determined, along withthe corresponding numbers of characters andwords for which the best correlation is obtained.5Such error rates are around 5% to 10% for standardtexts.
An evaluation of the segmentation tool is in fact re-quired.
on MT outputs alone.3 Experimental setupThe most popular off-the-shelf objective methodscurrently seem to be BLEU and NIST.
As NISTwas a modification of the original definition ofBLEU, the work reported here concentrates onBLEU.
Also, according to (BRILL and SORICUT,2004), BLEU is a good representative of a classof automatic evaluation methods with the focuson precision6.3.1 Computation of a BLEU scoreFor a given maximal order N , a baselineBLEUwN score is the product of two factors:a brevity penalty and the geometric average ofmodified n-gram precisions computed for all n-grams up to N .BLEUwN score = BP ?
N???
?N?n=1pnThe brevity penalty is the exponential of therelative variation in length against the closest ref-erence:BP ={1 if |C| > |Rclosest|e1?r/c if |C| ?
|Rclosest|where C is the candidate and Rclosest is the closestreference to the candidate according to its length.|S| is the length of a sentence S in words.
Using aconsistent notation, we note as |S|W the numberof occurrences of the (sub)string W in the sen-tence S, so that |S|w1...wn is the number of occur-rences of the word n-gram w1 .
.
.
wn in the sen-tence S.With the previous notations, a modified n-gramprecision for the order n is the ratio of two sums7:pn =?w1...wn?Cmin(|C|w1...wn , maxR(|R|w1...wn))?w1...wn?C|C|w1...wn?
the numerator gives the number of n-gramsof the candidate appearing in the references,6ROUGE (LIN and HOVY, 2003) would be a representa-tive of measures with the focus on recall.7We limit ourselves to the cases where one candidate orone reference is one sentence.80limited to the maximal number of occur-rences of the n-gram considered in a singlereference8;?
the denominator gives the total number of n-grams in the candidate.We leave the basic definition of BLEU un-touched.
The previous formulae can be appliedto character n-grams instead of word n-grams.In the sequel of this paper, for a given order N ,the measure obtained using words will be calledBLEUwN , whereas the measure in characters fora given order M will be noted BLEUcM .3.2 The test dataWe perform our study on English because a lan-guage for which the segmentation is obvious andundisputable is required.
On Japanese or Chinese,this would not be the case, as different segmentersdiffer in their results on the same texts9.The experiments presented in this paper rely ona data set consisting of 510 Japanese sentencestranslated into English by 4 different machinetranslation systems, adding up to 2, 040 candidatetranslations.
For each sentence, a set of 13 refer-ences had been produced by hand in advance.Different BLEU scores in words and characterswere computed for each of the 2, 040 English can-didate sentences, with their corresponding 13 ref-erence sentences.4 Results: equivalence BLEUwN /BLEUcMTo investigate the equivalence of BLEUwN andBLEUcM , we use three methods: we look forthe best correlation, the best agreement in judge-ments between the two measures, and the bestbehaviour, according to an intrinsic property ofBLEU.4.1 Best correlationFor some given order N , our goal is to determinethe value of M for which the BLEUcM scores (in8This operation is referred to as clipping in the originalpaper (PAPINENI et al, 2001).9Although we already applied the method in characterson unsegmented Japanese or Chinese MT outputs, this is notthe object of the present study, which, again, is to show theequivalence between BLEU in words and characters.characters) are best correlated with the scores ob-tained with BLEUwN .
To this end, we computefor all possible Ns and Ms all Pearson?s correla-tions between scores obtained with BLEUwN andBLEUcM .
We then select for each N , that Mwhich gives a maximum in correlation.
The re-sults10 are shown in Table 1.
For N = 4 words,the best M is 17 characters.4.2 Best agreement in judgementSimilar to the previous method, we compute forall possible Ms and Ns all Kappa coefficients be-tween BLEUwN and BLEUcM and then select,for each given N , that M which gives a maxi-mum.
The justification for such a procedure is asfollows.All BLEU scores fall between 0 and 1, there-fore it is always possible to recast them on a scaleof grades.
We arbitrarily chose 10 grades, rang-ing from 0 to 9, to cover the interval [ 0 , 1 ] withten smaller intervals of equal size.
A grade of 0corresponds to the interval [ 0 , 0.1 [, and so on,up to grade 9 which corresponds to [ 0.9 , 1 ].
Asentence with a BLEU score of, say 0.435, will beassigned a grade of 4.By recasting BLEU scores as described above,they become judgements into discrete grades, sothat computing two different BLEU scores firstin words and then in characters for the samesentence, is tantamount to asking two differentjudges to judge the same sentence.
A well-established technique to assess the agreement be-tween two judges being the computation of theKappa coefficient, we use this technique to mea-sure the agreement between any BLEUwN andany BLEUcM .The maximum in the Kappa coefficients isreached for the values11 given in Table 1.
ForN = 4 words, the best M is 18 characters.10The average ratio M/N obtained is 4.14, which is notthat distant from the average word length in our data set:3.84 for the candidate sentences.Also, for N = 4, we computed all values of Ms for eachsentence length.
See Table 2.11Except for N = 3, where the value obtained (14) isquite different from that obtained with Pearson?s correlation(10), the values obtained with Kappa coefficients atmost dif-fer by 1.814.3 Best analogical behaviourBLEU depends heavily on the geometric averageof modified n-gram precision scores.
Therefore,because one cannot hope to find a given n-gram ina sentence if neither of the two included (n?
1)-grams is found in the same sentence, the follow-ing property holds for BLEU:For any given N , for any given candi-date, for any given set of references,BLEUwN ?
BLEUw(N?1)The left graph of Figure 2 shows the correspon-dence of BLEUw4 and BLEUw3 scores for thedata set.
Indeed all points are found on the di-agonal or below.Using the property above, we are interestedin finding experimentally the value M such thatBLEUcM ?
BLEUw(N?1) is true for almost allvalues.
Such a value M can then be considered tobe the equivalent in characters for the value N inwords.Here we look incrementally for the M allowingBLEUcM to best mimic BLEUwN , that is leavingat least 90% of the points on or under the diag-onal.
For N = 4, as the graph in the middle ofFigure 2 illustrates, such a situation is first en-countered for M = 18.
The graph on the rightside shows the corresponding layout of the scoresfor the data set.
This indeed tends to confirm thatthe M for which BLEUcM displays a similar be-haviour to BLEUw4 is around 18.5 The standard case of systemevaluation5.1 BLEUw4 ' BLEUc18According to the previous results, it is possible tofind some M for some given N for which thereis a high correlation, a good agreement in judge-ment and an analogy of behaviour between mea-sures in characters and in words.
For the mostwidely used value of N , 4, the corresponding val-ues in characters were 17 according to correlation,18 according to agreement in judgement, and 18according to analogical behaviour.
We thus de-cide to take 18 as the number of characters cor-responding to 4 words (see Figure 1 for plots ofscores in words against scores in characters).5.2 Ranking systemsWe recomputed the overall BLEU scores of thefour MT systems whose data we used, with theusual BLEUw4 and its corresponding method incharacters, BLEUc18.
Table 3 shows the averagevalues obtained on the four systems.When going from words to characters, the val-ues decrease by an average of 0.047.
This isexplained as follows: a sentence of less than Nunits, has necessarily a BLEU score of 0 for N -grams in this unit.
Table 4 shows that, in our data,there are more sentences of less than 18 characters(350) than sentences of less than 4 words (302).Thus, there are more 0 scores with characters, andthis explains the decrease in system scores whengoing from words to characters.On the whole, Table 3 shows that happilyenough, shifting from words to characters in theapplication of the standard BLEU measure leavesthe ranking unchanged12.6 ConclusionWe studied the equivalence of applying the BLEUformula in character M -grams instead of wordN -grams.
Our study showed a high correlation,a good agreement in judgement, and an analogyof behaviour for definite corresponding values ofM and N .
For the most widely used value of N ,4, we determined a corresponding value in char-acters of 18.Consequently, this study paves the way to theapplication of BLEU (in characters) in objec-tive evaluation campaigns of automatic transla-tion into languages without word delimiters, likeChinese or Japanese, as it avoids any problemwith segmentation.AcknowledgementsThe research reported here was supported in partby a contract with the National Institute of Infor-mation and Communications Technology entitled?A study of speech dialogue translation technol-ogy based on a large corpus?.12(ZHANG et al, 2004) reported confidence intervals ofaround 2% (i.e., in this case, ?0.01) for BLEU, so that sys-tem 2 and 3 are undistinguishable by BLEUw4.82ReferencesYasuhiro AKIBA, Marcello FEDERICO, NorikoKANDO, Hiromi NAKAIWA, Michael PAUL, andJun?ichi TSUJII.
2004.
Overview of the IWSLT04evaluation campaign.
In Proc.
of the InternationalWorkshop on Spoken Language Translation, pages1?12, Kyoto, Japan.Eric BRILL and Radu SORICUT.
2004.
A unifiedframework for automatic evaluation using n-gramco-occurence statistics.
In Proceedings of ACL2004, pages 613?620, Barcelone.Peter E. BROWN, Vincent J. DELLA PIETRA,Stephen A. DELLA PIETRA, and Robert L. MER-CER.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
ComputationalLinguistics, Special Issue on Using Large Corpora:II, 19(2):263?311.George DODDINGTON.
2002.
Automatic evalua-tion of machine translation quality using N-gramco-occurrence statistics.
In Proceedings of HumanLanguage Technology, pages 128?132, San Diego.Huiming DUAN, Xiaojing BAI, Baobao CHANG, andShiwen YU.
2003.
Chinese word segmentation atPeking University.
In Qing Ma and Fei Xia, edi-tors, Proceedings of the Second SIGHAN Workshopon Chinese Language Processing, pages 152?155.Chin-Yew LIN and Eduard HOVY.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL 2003, pages 71?78, Edmonton.Y.
MATSUMOTO, A. KITAUCHI, T. YAMASHITA,Y.
HIRANO, H. MATSUDA, and M. HASAHARA.1999.
Japanese morphological analysis systemChaSen version 2.0.
Technical report NAIST-IS-TR99009, Nara Institute of Technology.Kishore PAPINENI, Salim ROUKOS, Todd WARD, andWei-Jing ZHU.
2001.
Bleu: a method for automaticevaluation of machine translation.
Research reportRC22176, IBM.Mark PRZYBOCKI.
2004.
The 2004 NIST machinetranslation evaluation plan (MT-04).Ying ZHANG, Stefan VOGEL, and Alex WAIBEL.2004.
Interpreting BLEU/NIST scores: how muchimprovement do we need to have a better system?In Proceedings of LREC 2004, volume V, pages2051?2054, Lisbonne.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91BLEUc18BLEUw4Figure 1: BLEUw4 in ordinates against BLEUc18 in abscissae.83Figure 2: On the left, experimental scores for BLEUw4 versus BLEUw3: all points are on the diagonalor below.
On the right, BLEUc18 scores versus BLEUw3: 90% of the points are on the diagonal orbelow.
In the middle, proportion of BLEUcM scores under BLEUw3 for M varying from 1 to 30.Table 1: Equivalent Ns and Ms for BLEUwN and BLEUcM obtained by different methods.BLEUw1 BLEUw2 BLEUw3 BLEUw4Pearson?s correlation (best M ) 0.89 (5) 0.90 (8) 0.85 (10) 0.83 (17)Kappa value (best M ) 0.17 (5) 0.29 (9) 0.34 (14) 0.35 (18)best M for analogical behaviourwrt to (N ?
1) (threshold = 90%) (9) (14) (18)Table 2: Correlation of BLEUw4 scores with BLEUc18 scores by sentence length.sentence length 4 5 6 7 8 9 10 > 10points 12.9% 18.2% 13.6% 13.4% 7.5% 6.5% 5.0% 8.1%average BLEUw4 score 0.188 0.300 0.252 0.364 0.345 0.318 0.321 0.015std.
dev.
?0.389 ?0.416 ?0.376 ?0.382 ?0.363 ?0.3150 ?0.346 ?0.291local best M 16 17 16 19 17 17 16 12Pearson?s correlation 0.827 0.795 0.797 0.824 0.899 0.894 0.952 0.919global best M 18Pearson?s correlation 0.788 0.794 0.779 0.805 0.883 0.871 0.929 0.861Table 3: Overall BLEU scores for 4 different systems in BLEUw4 and BLEUc18.system 1 system 2 system 3 system 4overall BLEUw4 score 0.349 > 0.305 ?
0.312 > 0.232overall BLEUc18 score 0.292 > 0.279 > 0.267 > 0.183difference in scores ?0.057 ?0.036 ?0.045 ?0.049Table 4: Distribution of the 510 sentences by lengths in words and characters.length < 4 words ?
4 words total< 18 characters 266 84 350?
18 characters 37 123 160total 302 208 51084
