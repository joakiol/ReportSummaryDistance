Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 47?51,Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational LinguisticsAn initial study of topical poetry segmentationChris FournierUniversity of OttawaOttawa, ON, Canadacfour037@eecs.uottawa.caAbstractThis work performs some basic research upontopical poetry segmentation in a pilot studydesigned to test some initial assumptions andmethodologies.
Nine segmentations of thepoem titled Kubla Khan (Coleridge, 1816,pp.
55-58) are collected and analysed, pro-ducing low but comparable inter-coder agree-ment.
Analyses and discussions of these cod-ings focus upon how to improve agreementand outline some initial results on the natureof topics in this poem.1 IntroductionTopical segmentation is the division of a text byplacing boundaries between segments.
Within a seg-mentation, each segment should represent a coherentand cohesive topic.
The decision to place a boundarybetween two segments of text is subjective and mustoften be determined manually.
The factors involvedin performing this subjective task are poorly under-stood, which motivates this work to begin the basicresearch required to understand this phenomenon.For literature, topical segmentations have beenproduced for a short story (Kozima, 1993) and anovel (Kazantseva and Szpakowicz, 2012).
Poetry,however, has had little attention in terms of topicalsegmentation.
Brooke et al(2012) collected seg-mentations of poetry that sought to delineate whichvoices communicate various segments of The Waste-land by T.S.
Elliot (1888-1965), but a voice seg-ment does not necessarily correlate with a topicalsegment.
Because The Wasteland?s defining featureis its voice-shifts, more data is required to under-stand the variety of topical segments that could existwithin poetry besides those delineated by changingvoice ?
which this work aims to provide.11Available at http://nlp.chrisfournier.ca/This work?s goal is to begin to provide someinitial information about what constitutes a topicin poetry by analysing the Romantic-era poem ti-tled Kubla Khan (Coleridge, 1816, pp.
55-58) bySamuel Taylor Coleridge (1772?1834).
Chosen forits beauty, variety, short length (54 lines), and lack ofstrict adherence to a prescribed structure (e.g., son-nets, odes, etc.
), it is assumed that this purportedfragment of a dream will contain a wide variety ofdifferent topics (as judged by manual coders).This work aims to discover from reader?s interpre-tations of topical segmentation in poetry the:?
Structure of these topics (e.g., are they linear,hierarchical, or something else?);?
Types and variety of topics (e.g., do topics shiftwhen there are changes in time, place, descrip-tion, exposition, etc.
); and?
Relationship between poetic features and topi-cal boundaries (e.g., do stanzas correlate withtopical boundaries?
).Unfortunately, this work is simply a pilot studyand it cannot make any generalizations about poetryoverall, but inferences can be made about this singlepoem and its topical structure.2 Related WorkTopical Segmentation Topical segmentation ofexpository texts such as popular science magazinearticles have been well studied by Hearst (1993,1994, 1997) while developing the automatic topi-cal segmenter named TextTiling.
On a parallel track,Kozima (1993) segmented a simplified version of O.Henry?s (William Sydney Porter; 1862?1910) shortstory titled Springtime a` la Carte (Thornley, 1816).Both bodies of work focused upon using lexical co-hesion to model where topic boundaries occur andcollected manual segmentations to study.
This data,47however, was never analysed for the types of seg-ments contained, but only for the presence or ab-sence of topic boundaries at specific positions.Kazantseva and Szpakowicz (2012) delved deeperinto topical segmentation of literature by collectingsegmentations of Wilkie Collins?
(1824?1883) ro-mantic novel The Moonstone (Collins, 1868).
Inthe novel, 20 of its chapters were segmented indi-vidually by 27 annotators (in groups of 4?6) intoepisodes.
Episodes were defined as ?topically con-tinuous spans of text demarcated by the most percep-tible shifts of topic in the chapter?
(Kazantseva andSzpakowicz, 2012, p. 213).
This work also analysedthe boundaries placed by the coders themselves, butnot the types of segments that they produced.Brooke et al(2012) collected voice-switch seg-mentations of The Wasteland by T.S.
Elliot (1888-1965).
Although voices are not topics, voice switch-ing could constitute topical boundaries.
Segmenta-tions from 140 English literature undergraduate stu-dents and 6 expert readings were collected and usedto compose one authoritative reference segmentationto test a large number automatic segmenters upon.Agreement and Comparison Inter-coder agree-ment coefficients measure the agreement between agroup of human judges (i.e.
coders) and whethertheir agreement is greater than chance.
Low coeffi-cient values indicate that a task may have restrictedcoders such that their responses do not represent anempirical model of the task, or the task instructionsdid not sufficiently define the task.
High coefficientvalues indicate the degree of reliability and repli-cability of a coding scheme and the coding collec-tion methodology (Carletta, 1996).
Although thereis much debate about what coefficient value repre-sents adequate agreement, any coefficient value canbe used to compare studies of the same task that usedifferent coding schemes or methodologies.Many inter-coder agreement coefficients exist, butthis work uses Fleiss?
multi-pi (pi?, Fleiss 1971; oc-casionally referred to as K by Siegel and Castellan1988) to measure agreement because it generalizesindividual coder performance to give a better pic-ture of the replicability of a study.
Specifically, anadaptation of the proposal by Fournier and Inkpen(2012, pp.
154?156) for computing pi?
is used thatis detailed by Fournier (2013).Fournier (2013) modifies the work of Fournierand Inkpen (2012) to provide a more discriminativemeasure of similarity between segmentations calledboundary similarity (B) ?
an edit distance basedmeasure which is unbiased, more consistent, andmore intuitive than traditional segmentation compar-ison methods such as Pk (Beeferman and Berger,1999, pp.
198?200) and WindowDiff (Pevznerand Hearst, 2002, p. 10).
Using the inter-coderagreement formulations provided in Fournier andInkpen (2012), Fournier (2013) provides B-basedinter-coder agreement coefficients including Fleiss?multi-pi (referred to as pi?B) which can discern be-tween low/high agreement while still awarding par-tial credit for near misses.3 Study DesignThis work is a small study meant to inform futurelarger studies on topical poetry segmentation.
Tothat end, a single 54 line poem, Kubla Khan (Co-leridge, 1816, pp.
55-58), is segmented.
Writtenin four stanzas (originally published in two) com-posed of tetra and penta-meter iambs, this well stud-ied work appears to show a large variety of topicalsegment breaks, including time, place, scenery, nar-ration, exposition, etc.
Stripped of its indentationand with its stanzas compressed into one long se-quence of numbered lines, this poem was presentedto segmenters to divide into topics.Objectives The objective of this study is to iden-tify whether topics in poems fit well into a lineartopic structure (i.e., boundaries cannot overlap) andto test the annotation instructions used.
Addition-ally, a survey of the types and variety of topics is de-sirable to inform whether more than one boundarytype might be needed to model segment boundaries(and to inspire statistical features for training an au-tomatic topical poetry segmenter).
Finally, the re-lationship between poem features and topic bound-aries is of interest; specifically, for this initial work,do stanzas correlate with topical boundaries?Subjects Nine subjects were recruited using Ama-zon?s Mechanical Turk from the United States whohad an exemplary work record (i.e., were ?Mas-ter Tukers?).
Segment text summaries were anal-ysed for correct language use to ensure that coders48demonstrated English language proficiency.Granularity Segmentations were solicited at theline level (arbitrarily assuming that a topic will notchange within a line, but may between lines).
Thislevel is assumed to be fine enough to partition seg-ments accurately while still being coarse enough tomake the task short (only 54 lines can be divided intosegments).
Because there may be a great number oftopics found in the poem by readers, it is assumedthat a nearly missed boundary would only be thosethat are adjacent to another (i.e., nt for B is set to 2).Collection procedure Segmenters were asked toread the poem and to divide it into topical segmentswhere a topic boundary could represent a changein time, scenery, or any other detail that the readerdeems important.
A short example coding was alsoprovided to augment the instructions.
Along withline number spans, a single sentence description ofthe segment was requested (for segment type analy-sis and to verify coder diligence and thoughtfulness)and overall comments on the task were solicited.4 Study Results and AnalysisTime The 9 subjects took 35.1556?18.6796 min-utes to read and segment the poem.2 Each was remu-nerated $8 USD, or $18.91?
11.03 USD per hour.Segmentations The 9 coders placed 17.6667 ?6.2716 boundaries within the 54 lines of the poem.The number of segmentations produced by eachcoder is shown in Figure 1a, along with the meanand standard deviation (SD).Agreement The segmentations provided by the 9coders in this study have an inter-coder agreementcoefficient value of pi?B = 0.3789.
This value is low,but it is only slightly below that of Hearst (1997)(0.4405) and Kazantseva and Szpakowicz (2012)(0.20, 0.18, 0.40, 0.38, 0.23 for each of the 5 groups)as reported in Fournier (2013).
This value is also notunexpected given the different coding behaviours(e.g., boundary placement frequency) in Figure 1a.Similarity Using Boundary Similarity (B), taking1 ?
B can yield a simple distance function between2One coder took far less time because they submitted part oftheir answers via email and time was not accurately recorded.segmentations.
Because of the low agreement ofthis study, it is assumed that there must be subsetsof coders who agree more with each other than withothers (i.e., clusters).
Using 1?B as a distance func-tion between segmentations, hierarchical agglom-erative clustering was used to obtain the clustersshown in Figure 1b.
Computing inter-coder agree-ment for these clusters produces subsets with signif-icantly higher than overall agreement (Table 1).Labels Taking the single-sentence descriptions ofeach topic, an attempt was made to label them asbelonging to one or more of these categories:1.
Exposition (e.g, story/plot development);2.
Event (e.g., an action or event occurred);3.
Place (Location is stated or changed);4.
Description (of an entity; can be specific):a) Scenery b) Person c) Sound d) Comparison(simile or metaphor)5.
Statement (to the reader).These labels were decided by the author while read-ing the segmentations and were iteratively con-structed until they suitably described the one-linesegment topic summaries.
Using Jaccard similarity,the labels placed on each position were comparedto those of each other coder to obtain mean similar-ity of each line, as plotted in Figure 1c.
This showsthat in terms of topic types, actual agreement variesby position.
The portions with the highest agree-ment are at the beginning of the poem and containscenery description which appear to have been easyto agree upon (type-wise).
Overall, mean label sim-ilarity between all coders was 0.5330 ?
0.4567, butsome of the identified clusters exhibited even highersimilarity (Table 1).Feature correlations There is some evidence tosuggest that boundaries between the four stanzas atlines 11?12, 30?31, and 36?37 correlate with top-ical shifts because 6/9, 9/9, and 9/9 (respectively)coders placed boundaries at these locations.
There islittle evidence to suggest that the indentation of line5 and lines 31?34 (not shown) correlate with topicalshifts because only 1/9 and 5/9 (respectively) codersplaced boundaries between these segments.Topical structure One of the coders commentedthat they felt that the segments should overlap and490 1 2 3 4 5 6 7 8Coder051015202530Boundariesplaced(quantity) 20261810261692311(a) Coder boundaries placed withmean and SD1 0 2 4 7 3 5 6 8Coder0.00.10.20.30.40.50.60.7MeanDistance(1?B)(b) Hierarchical agglomerative clustersusing 1?
B as a distance metric0 10 20 30 40 50Line0.00.20.40.60.81.0MeanLabelJaccardSimilarity(c) Mean Jaccard similarity of topiclabel types per lineFigure 1: Various analyses of the 9 manual segmentations of Kubla KhanCoders {4, 7} {0, 2} {6, 8} {1, 0, 2} {1, 0, 2, 4, 7} {3, 5, 6, 8} {5, 6, 8}pi?B 0.3704 0.6946 0.7625 0.5520 0.4474 0.4764 0.5389E(J) 0.491?
0.495 0.460?
0.439 0.685?
0.464 0.508?
0.452 0.512?
0.467 0.593?
0.425 0.580?
0.432Table 1: Inter-coder agreement (pi?B) and mean Jaccard topic label similarity (with WD) for coder clusterscoded so.
These codings were adjusted by the au-thor to not overlap for analysis, but the coder?s com-ment highlights that perhaps these segments shouldbe able to overlap, or that linear segmentation maynot be an adequate model for topics in poetry.5 DiscussionGiven the low (but comparable) inter-coder agree-ment values of this study, it is evident that some vari-ables are not properly being controlled by the proce-dure used herein.
Before a larger study is performed,the issue of low agreement must be explained; somehypotheses for this are that:1.
Coders may have been of varying levels of ed-ucation, English proficiency, or motivation;2.
Instructions may have not been clear or exhaus-tive in terms of the potential topics types;3.
A linear segmentation not allowing for overlapmay artificially constrain coders; and4.
The poem selected may simply be inherentlydifficult to interpret and thus segment.This study has, however, catalogued a numberof topic labels which can be used to better educatecoders about the types of topical segments that ex-ist, which could lead to obtaining higher inter-coderagreement.
Pockets of agreement do exist, as shownin the clusters and their agreement and topic labelsimilarity values (Table 1).
If more data is collected,but inter-coder agreement stays steady, perhaps in-stead these clusters will remain and become morepopulated.
Maybe these clusters will reveal that theproblem was modelled correctly, but that there issimply a difference between the coders that was notpreviously known.
Such a difference could be spot-ted using clustering, but what the actual difference ismay remain a mystery unless more biographical de-tails are available (e.g., sex, age, education, Englishproficiency, reading preferences, etc.
).6 Conclusions and Future WorkAlthough Kubla Khan is a beautiful poem, its topicalsegmentation is vexing.
Low inter-coder agreementexemplified by this study indicates that the method-ology used to investigate topical poetry segmenta-tion may require some modifications, or more bio-graphical details must be sought to identify the causeof the low agreement.
Clustering was able to iden-tify pockets of high agreement and similarity, butthe nature of these clusters is largely unknown ?what biographical details or subjective opinions ofthe task separate these groups?Future work will continue with subsequent pilotstudies to attempt to raise the level of inter-coderagreement or to explain the low agreement by look-ing for clusters of coders who agree (and attemptingto explain the relationships between coders in theseclusters).
Also, more poems need to be analysed tomake generalisations about poetry overall.
The re-lationships between topical segments in poetry andother poetic features such as rhyme, meter, and ex-pert opinions are also worth investigation.50ReferencesBeeferman, Doug and Adam Berger.
1999.
Statisti-cal models for text segmentation.
Machine Learn-ing 34:177?210.Brooke, Julian, Adam Hammond, and Graeme Hirst.2012.
Unsupervised Stylistic Segmentation of Po-etry with Change Curves and Extrinsic Features.In Proceedings of the 1st NAACL-HLT Workshopon Computational Linguistics for Literature.
As-sociation for Computational Linguistics, Strouds-burg, PA, USA, pages 26?35.Carletta, Jean.
1996.
Assessing Agreement on Clas-sification Tasks: The Kappa Statistic.
Computa-tional Linguistics 22(2):249?254.Coleridge, Samuel Taylor.
1816.
Christabel, KublaKhan, and the Pains of Sleep.
John Murray.Collins, Wilkie.
1868.
The Moonstone.
TinsleyBrothers.Fleiss, Joseph L. 1971.
Measuring nominal scaleagreement among many raters.
PsychologicalBulletin 76:378?382.Fournier, Chris.
2013.
Evaluating Text Segmenta-tion using Boundary Edit Distance.
In Proceed-ings of 51st Annual Meeting of the Association forComputational Linguistics.
Association for Com-putational Linguistics, Stroudsburg, PA, USA.Fournier, Chris and Diana Inkpen.
2012.
Segmen-tation Similarity and Agreement.
In Proceedingsof Human Language Technologies: The 2012 An-nual Conference of the North American Chap-ter of the Association for Computational Linguis-tics.
Association for Computational Linguistics,Stroudsburg, PA, USA, pages 152?161.Hearst, Marti A.
1993.
TextTiling: A QuantitativeApproach to Discourse.
Technical report, Uni-versity of California at Berkeley, Berkeley, CA,USA.Hearst, Marti A.
1994.
Context and Structured inAutomated Full-Text Information Access Contextand Structure in Automated Full-Text InformationAccess.
Ph.D. thesis, University of CaliforniaBerkeley.Hearst, Marti A.
1997.
TextTiling: Segmenting Textinto Multi-paragraph Subtopic Passages.
Compu-tational Linguistics 23:33?64.Kazantseva, Anna and Stan Szpakowicz.
2012.
Top-ical Segmentation: a Study of Human Perfor-mance.
In Proceedings of Human Language Tech-nologies: The 2012 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics.
Association for Com-putational Linguistics, Stroudsburg, PA, USA,pages 211?220.Kozima, Hideki.
1993.
Text segmentation based onsimilarity between words.
In Proceedings of the31st Annual Meeting of the Association for Com-putational Linguistics.
Association for Computa-tional Linguistics, Stroudsburg, PA, USA, ACL?93, pages 286?288.Pevzner, Lev and Marti A. Hearst.
2002.
A critiqueand improvement of an evaluation metric for textsegmentation.
Computational Linguistics 28:19?36.Siegel, Sidney and N. J. Castellan.
1988.
Non-parametric Statistics for the Behavioral Sciences,McGraw-Hill, New York, USA, chapter 9.8.
2edition.Thornley, G. C., editor.
1816.
British and Ameri-can Short Stories.
Longman Simplified EnglishSeries.
Longman.51
