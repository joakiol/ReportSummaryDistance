Information Retrieval Capable of Visualization and High PrecisionQing Ma1,2 and Kousuke Enomoto11Ryukoku University / 2NICT, Japanqma@math.ryukoku.ac.jpMasaki Murata and Hitoshi IsaharaNICT, Japan{murata,isahara}@nict.go.jpAbstractWe present a neural-network based self-organizing approach that enables vi-sualization of the information retrievalwhile at the same time improving itsprecision.
In computer experiments,two-dimensional documentary maps inwhich queries and documents weremapped in topological order accord-ing to their similarities were created.The ranking of the results retrieved us-ing the maps was better than that ofthe results obtained using a conven-tional TFIDF method.
Furthermore, theprecision of the proposed method wasmuch higher than that of the conven-tional TFIDF method when the processwas focused on retrieving highly rel-evant documents, suggesting that theproposed method might be especiallysuited to information retrieval tasks inwhich precision is more critical than re-call.1 IntroductionInformation retrieval (IR) has been studied sincean earlier stage [e.g., (Menzel, 1966)] and sev-eral kinds of basic retrieval models have been pro-posed (Salton and Buckley, 1988) and a numberof improved IR systems based on these modelshave been developed by adopting various NLPtechniques [e.g., (Evans and Zhai, 1996; Mitraet al, 1997; Mandara, et al, 1998; Murata, etal., 2000)].
However, an epoch-making techniquethat surpasses the TFIDF weighted vector spacemodel, the main approach to IR at present, has notyet been invented and IR is still relatively impre-cise.
There are also challenges presenting a largenumber of retrieval results to users in a visual andintelligible form.Our aim is to develop a high-precision, visualIR system that consists of two phases.
The firstphase is carried out using conventional IR tech-niques in which a large number of related docu-ments are gathered from newspapers or websitesin response to a query.
In the second phase thevisualization of the retrieval results and pickingare performed.
The visualization process clas-sifies the query and retrieval results and placesthem on a two-dimensional map in topologicalorder according to the similarity between them.To improve the precision of the retrieval process,the picking process involves further selection of asmall number of highly relevant documents basedon the classification results produced by the visu-alization process.This paper presents a new approach by usingthe self-organizing map (SOM) proposed by Ko-honen (Kohonen, 1997) for this second IR phase1.To enable the second phase to be slotted into apractical IR system as described above, visual-1There have been a number of studies of SOM on datamining and visualization [e.g., (Kohonen, et al, 2000)] sincethe WEBSOM was developed in 1996.
To our knowledge,however, these works mainly focused on confirming the ca-pabilities of SOM in the self-organization and/or in the vi-sualization.
In this study, we slot the SOM-based processinginto a practical IR system that enables visualization of theIR while at the same time improving its precision.
The an-other feature of our study differing from others is that weperformed comparative studies with TFIDF-based IR meth-ods, the major approach to IR in NLP field.138ization and picking should be carried out for asingle query and set of related documents.
Inthis paper, however, for the purpose of evaluatingthe proposed system, correct answer data, consist-ing of multiple queries and related documents asused in the 1999 IR contest, IREX (Murata, etal., 2000), was used.
The procedure of the sec-ond IR-phase in this paper is therefore as follows.Given a set of queries and related documents, adocumentary map is first automatically createdthrough self-organization.
This map provides vis-ible and continuous retrieval results in which allqueries and documents are placed in topologicalorder according to their similarity2.
The docu-mentary map provides users with an easy methodof finding documents related to their queries andalso enables them to see the relationships betweendocuments with regard to the same query, or eventhe relationships between documents across dif-ferent queries.
In addition, the documents relatedto a query can be ranked by simply calculatingthe Euclidean distances between the points of thequeries and the points of the documents in themap and then choosing the N closest documentsin ranked order as the retrieval results for eachquery.
If a small N is set, then the retrieval resultsare limited to the most highly relevant documents,thus improving the retrieval precision.Computer experiments showed that meaning-ful two-dimensional documentary maps could becreated; The ranking of the results retrieved us-ing the map was better than that of the results ob-tained using a conventional TFIDF method.
Fur-thermore, the precision of the proposed methodwas much higher than that of the conventionalTFIDF method when the retrieval process focusedon retrieving the most highly relevant documents,which indicates that the proposed method mightbe particularly useful for picking the best docu-ments, thus greatly improving the IR precision.2 Self-organizing documentary mapsand ranking related documentsA SOM can be visualized as a two-dimensionalarray of nodes on which a high-dimensional in-2For a specific query, other queries and documents in themap are considered to be irrelevant (i.e., documents unre-lated to the query).
This map is therefore equivalent to amap consisting of one query and related and unrelated docu-ments, which will be adopted in the practical IR system thatwe aim to develop.put vector can be mapped in an orderly mannerthrough a learning process.
After the learning, ameaningful nonlinear coordinate system for dif-ferent input features is created over the network.This learning process is competitive and unsuper-vised and is called a self-organizing process.Self-organizing documentary maps are ones inwhich given queries and all related documentsin the collection are mapped in order of similar-ity, i.e., queries and documents with similar con-tent are mapped to (or best-matched by) nodesthat are topographically close to one another, andthose with dissimilar content are mapped to nodesthat are topographically far apart.
Ranking is theprocedure of ranking documents related to eachquery from the map by calculating the Euclideandistances between the points of the queries andthe points of the documents in the map and choos-ing the N closest documents as the retrieval result.2.1 DataThe queries are those used in a dry run of the1999 IREX contest and the documents relating tothe queries are original Japanese newspaper arti-cles used in the contest as the correct answers.
Inthis study, only nouns (including Japanese verbalnouns) were selected for use.2.2 Data codingSuppose we have a set of queries:Q = {Q i (i = 1, ?
?
?
, q)}, (1)where q is the total number of queries, and a setof documents:A = {Ai j (i = 1, ?
?
?
, q, j = 1, ?
?
?
, ai)},(2)where ai is the total number of documents relatedto Q i.
For simplicity, where there is no need todistinguish between queries and documents, weuse the same term ?documents?
and the same no-tation Di to represent either a query Q i or a doc-ument Ai j.
That is, we define a new setD = {Di (i = 1, ?
?
?
, d)} = Q?A (3)which includes all queries and documents.
Here,d is the total number of queries and documents,i.e.,d = q +q?i=1ai.
(4)139Each document, Di, can then be defined by theset of nouns it contains asDi = {noun(i)1 , w(i)1 , ?
?
?
, noun(i)ni , w(i)ni }, (5)where noun(i)k (k = 1, ?
?
?
, ni) are all differentnouns in the document Di and w(i)k is a weightrepresenting the importance of noun(i)k (k =1, ?
?
?
, ni) in document Di.
The weights are com-puted by their tf or tfidf values.
That is,w(i)j = tf(i)j or tf(i)j idfj .
(6)In the case of using tf, the weights are normalizedsuch thatw(i)1 + ?
?
?+ w(i)ni = 1.
(7)Also, when using the Japanese thesaurus, Bun-rui Goi Hyou (The National Institute for JapaneseLanguage, 1964) (BGH for short), synonymousnouns in the queries are added to the sets ofnouns from the queries shown in Eq.
(5) and theirweights are set to be the same as those of the orig-inal nouns.Suppose we have a correlative matrix whose el-ement dij is some metric of correlation, or a sim-ilarity distance, between the documents Di andDj ; i.e., the smaller the dij , the more similar thetwo documents.
We can then code document Diwith the elements in the i-th row of the correlativematrix asV (Di) = [di1, di2, ?
?
?
, did]T .
(8)The V (Di) ?
<d is the input to the SOM.
There-fore, the method to compute the similarity dis-tance dij is the key to creating the maps.
Notethat the individual dij of vector V (Di) only re-flects the relationships between a pair of docu-ments when they are considered independently.To establish the relationships between the doc-ument Di and all other documents, representa-tions such as vector V (Di) are required.
Evenif we have these high-dimensional vectors forall the documents, it is still difficult to estab-lish their global relationships.
We therefore needto use an SOM to reveal the relationships be-tween these high-dimensional vectors and repre-sent them two-dimensionally.
In other words, therole of the SOM is merely to self-organize vec-tors; the quality of the maps created depends onthe vectors provided.In computing the similarity distance dij be-tween documents, we take two factors into ac-count: (1) the larger the number of commonnouns in two documents, the more similar the twodocuments should be (i.e., the shorter the simi-larity distance); (2) the distance between any twoqueries should be based on their application to theIR processing; i.e., by considering the procedureused to rank the documents relating to each queryfrom the map.
For this reason, the document-similarity distance between queries should be setto the largest value.
To satisfy these two factors,dij is calculated as follows:dij =????????????
?1 if both Di and Djare queries1?
|Cij ||Di|+|Dj |?|Cij | not the case mentionedabove and i 6= j0, if i=j(9)where |Di| and |Dj | are values (the numbers ofelements) of sets of documents Di and Dj de-fined by Eq.
(5) and |Cij | is the value of the in-tersection Cij of the two sets Di and Dj .
|Cij |is therefore some metric of document similarity(the inverse of the similarity distance dij) betweendocuments Di and Dj which is normalized by|Di|+|Dj |?|Cij |.
Before describing the methodsfor computing them, we first rewrite the definitionof documents given by Eq.
(5) for Di and Dj asfollows.Di = {(c1, w(i)c1 , ?
?
?
, cl, w(i)cl ),(n(i)1 , w(i)1 , ?
?
?
, n(i)mi , w(i)mi)}, (10)andDj = {(c1, w(j)c1 , ?
?
?
, cl, w(j)cl ),(n(j)1 , w(j)1 , ?
?
?
, n(j)mj , w(j)mj )}, (11)where ck (k = 1, ?
?
?
, l) are the common nouns ofdocuments Di and Dj and n(i)k (k = 1, ?
?
?
,mi)and n(j)k (k = 1, ?
?
?
,mj) are nouns of documentsDi and Dj which differ from each other.
By com-paring Eq.
(5) and Eqs.
(10) and (11), we know140that l+mi +mj = ni + nj .
Thus, |Di| (or |Dj |)of Eq.
(9) can be calculated as follows.|Di| =l?k=1w(i)ck +mi?k=1w(i)k .
(12)For calculating |Cij |, on the other hand, since theweights (of either common or different nouns)generally differ between two documents, we de-vised four methods which are expressed as fol-lows.Method A:|Cij | =l?k=1max(w(i)ck , w(j)ck ).
(13)Method B:|Cij | =l?k=1w(i)ck + w(j)ck2 .
(14)Method C:|Cij | =???????????
?lk=1 max(w(i)ck , w(j)ck ) if one is a queryand the otheris a document?lk=1w(i)ck+w(j)ck2 .
if both aredocuments(15)Method D:|Cij | =???????????
?lk=1 max(w(i)ck , w(j)ck ) if one is a queryand the otheris a document?lk=1 min(w(i)ck , w(j)ck ).
if both aredocuments(16)Note that we need not consider the case whereboth are queries for calculating |Cij | because thishas been considered independently as shown byEq.
(9).3 Experimental Results3.1 DataSix queries Q i (i = 1, ?
?
?
, q, q = 6) and 433documents Ai j (i = 1, ?
?
?
, q, q = 6, j =1, ?
?
?
, ai and?qi=1 ai = 433) used in the dry runTable 1: Distribution of documents used in theexperimentsa1 a2 a3 a4 a5 a6?6i=1 ai80 89 42 108 49 65 433of the 1999 IREX contest were used for our ex-periments.
The distribution of these documents,i.e., the number ai (i = 1, ?
?
?
, q, q = 6) of docu-ments related to each query, is shown in Table 1.It should be noted that since the proposed IRapproach will be slotted into a practical IR sys-tem in the second phase in which a small number(say below 1,000, or even below 500) of the re-lated documents should have been collected, thisexperimental scale is definitely a practical one.3.2 SOMWe used a SOM of a 40?40 two-dimensional ar-ray.
Since the total number d of queries and doc-uments to be mapped was 439, i.e., d = q +?6i=1 ai = 439, the number of dimensions of in-put n was 439.
In the ordering phase, the numberof learning steps T was set at 10,000, the initialvalue of the learning rate ?
(0) at 0.1, and the ini-tial radius of the neighborhood ?
(0) at 30.
In thefine adjustment phase, T was set at 15,000, ?
(0)at 0.01, and ?
(0) at 5.
The initial reference vec-tors mi(0) consisted of random values between 0and 1.0.3.3 ResultsWe first performed a preliminary experiment andanalysis to determine which of the four methodswas the optimal one for calculating |Cij | shownin Eqs.
(13)-(16).
Table 2 shows the IR precision,i.e., the precision of the ranking results obtainedfrom the self-organized documentary maps cre-ated using the four methods.
The IR precision wascalculated by follows.P = 1qq?i=1#related to Q i in the retrieved ai documentsai ,(17)where q is the total number of queries, # meansnumber, and ai is the total number of documentsrelated to Q i as shown in Table 1.In the case of using tf values as weights ofnouns, method B obviously did not work.
Al-141Table 2: IR precision for the four methods for cal-culating |Cij |Weight MethodAMethodBMethodCMethodDtf 0.33 0.20 0.41 0.45tfidf 0.85 0.76 0.91 0.78though the similarity between queries was manda-torily set to the largest value, all six queries weremapped in almost the same position, thus produc-ing the poorest result.
We consider the reason forthis was as follows.
In general, the number ofwords in a query is much smaller than the num-ber of words in the documents, and the numberof queries is much smaller than the number ofdocuments collected.
As described in section 2,each query was defined by a vector consisting ofall similarities between the query and five otherqueries and all documents in the collection.
Wethink that using the average weights of words ap-pearing in the queries and documents to calculatethe similarities between queries and documents,as in method B, tends to produce similar vectorsfor the queries.
All of these query vectors are thenmapped to almost the same position.
With codingmethod A, because the larger of the two weightsof a query and a document is used, the same prob-lem could also arise in practice.
There were no es-sential differences between coding methods C andD, which were almost equally precise.
Neither ofthese methods have the shortcomings describedabove for methods A and B.
However, when tfidfvalues were used as the weights of the nouns, evenmethods A and B worked quite well.
Therefore, ifwe use tfidf values as the weights of the nouns, wemay use either of the four methods.
Based on thisanalysis and the preliminary experimental resultthat method C and D had highest precisions in thecases of using tf and tfidf values as weights of thenouns, respectively, we used methods C and D forcalculating |Cij | in all the remaining experiments.Table 3 shows the IR precision obtained usingvarious methods.
From this table we can see thatthe proposed method in the case of SOM (w=tfidf,C), i.e., using method C for calculating |Cij |, us-ing tfidf values as the weights of nouns, and notusing the Japanese thesaurus (BGH), in the caseof SOM (w=tfidf, D), i.e., using method D, us-ing tfidf values, and not using the BGH, and inTable 3: IR precision obtained using variousmethodsTFIDF TFIDF(BGH)SOM(w=tf,D)SOM(w=tfidf,C)SOM(w=tfidf,C,BGH)SOM(w=tfidf,D)SOM(w=tfidf,D,BGH)0.67 0.75 0.45 0.91 0.77 0.78 0.73Table 4: IR precision for top N related documentsN TFIDF TFIDF(BGH)SOM(w=tf,D)SOM(w=tfidf,C)SOM(w=tfidf,C,BGH)SOM(w=tfidf,D)SOM(w=tfidf,D,BGH)10 0.83 0.88 0.75 1.0 0.97 1.0 0.9720 0.79 0.86 0.68 0.99 0.95 0.98 0.9730 0.73 0.84 0.62 0.99 0.94 0.97 0.9140 0.71 0.82 0.58 0.98 0.90 0.97 0.87the case of SOM (w=tfidf, C, BGH), i.e., usingmethod C, using tfidf values, and using the BGHproduced the highest, second highest, and thirdhighest precision, respectively, of all the methodsincluding the conventional TFIDF method.
Whenthe BGH was used, however, the IR precision ofthe proposed method dropped inversely, whereasthat of the conventional TFIDF improved.
Thelower precision of the proposed method when us-ing BGH might be due to the calculation of thedenominator of Eq.
(9); this will be investigatedin future study.Table 4 shows the IR precision obtained usingvarious methods when the retrieval process is fo-cused on the top N related documents.
From thistable we can see that the IR precision of the pro-posed method, no matter whether the BGH wasused or not, or whether method C or D was usedfor calculating |Cij |, was much higher than thatof the conventional TFIDF method when the pro-cess was focused on retrieving the most relevantdocuments.
This result demonstrated that the pro-posed method might be especially useful for pick-ing highly relevant documents, thus greatly im-proving the precision of IR.Figure 1 shows the left-top area of a self-organized documentary map obtained using theproposed method in the case of SOM (w=tfidf,D)3.
From this map, we can see that query Q 43Note that the map obtained using the proposed methodin the case of SOM (w=tfidf, C), which had the highest IRprecision, was better than this.142Figure 1: Left-top area of self-organized docu-mentary mapand its related documents A4 ?
(where * denotesan Arabic numeral), Q 2 and its related docu-ments A2 ?
were mapped in positions near eachother.
Similar results were obtained for the otherqueries which were not mapped in the area of thefigure.
This map provides visible and continu-ous retrieval results in which all queries and docu-ments are placed in topological order according totheir similarities.
The map provides an easy wayof finding documents related to queries and alsoshows the relationships between documents withregard to the same query and even the relation-ships between documents across different queries.Finally, it should be noted that each map thatconsists of 400 to 500 documents was obtained in10 minutes by using a personal computer with a3GHZ CPU of Pentium 4.4 ConclusionThis paper described a neural-network based self-organizing approach that enables information re-trieval to be visualized while improving its preci-sion.
This approach has a practical use by slot-ting it into a practical IR system as the second-phase processor.
Computer experiments of practi-cal scale showed that two-dimensional documen-tary maps in which queries and documents aremapped in topological order according to theirsimilarities can be created and that the rankingof the results retrieved using the created mapsis better than that produced using a conventionalTFIDF method.
Furthermore, the precision of theproposed method was much higher than that ofthe conventional TFIDF method when the pro-cess was focused on retrieving the most relevantdocuments, suggesting that the proposed methodmight be especially suited to information retrievaltasks in which precision is more important thanrecall.In future work, we first plan to re-confirm theeffectiveness of using the BGH and to further im-prove the IR accuracy of the proposed method.We will then begin developing a practical IR sys-tem capable of visualization and high precisionusing a two-phase IR procedure.
In the first phase,a large number of related documents are gath-ered from newspapers or websites in response toa query presented using conventional IR; the sec-ond phase involves visualization of the retrievalresults and picking the most relevant results.ReferencesH.
Menzel.
1966.
Information needs and uses in scienceand technology.
Annual Review of Information Scienceand Technology, 1, pp.
41-69.G.
Salton and C. Buckley.
1988.
Term-weighting ap-proaches in automatic text retrieval.
InformationProcessing & Management, 24(5), pp.
513-523.D.
A. Evans and C. Zhai.
1996.
Noun-phrase analysis inunrestricted text for information retrieval.
ACL?96, pp.17-24.M.
Mitra, C. Buckley, A. Singhal, and C. Cardie, C.1997.
An analysis of statistical and syntactic phrases.RIAO?97, pp.
200-214.R.
Mandara, T. Tokunana, and H. Tanaka 1998.
The useof WordNet in information retrieval.
COLING-ACL?98Workshop: Usage of WordNet in Natural LanguageProcessing Systems, pp.
31-37.M.
Murata, Q. Ma, K. Uchimoto, H. Ozaku, M. Uchiyama,and H. Hitoshi 2000.
Japanese probabilistic informa-tion retrieval using location and category information.IRAL?2000.T.
Kohonen 1997.
Self-organizing maps.
Springer, 2ndEdition.T.
Kohonen, S. Kaski, K. Lagus, J. Salojarrvi, J. Honkela,V.
Paatero, and A. Saarela.
2000.
Self Organization ofa Massive Document Collection.
IEEE Trans.
NeuralNetworks, 11, 3, pp.
574-585.The National Institute for Japanese Language.
1964.
BunruiGoi Hyou (Japanese Thesaurus).
Dainippon-tosho.143
