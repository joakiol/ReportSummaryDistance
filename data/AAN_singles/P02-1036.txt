Dynamic programming for parsing and estimation ofstochastic unication-based grammars?Stuart GemanDivision of Applied MathematicsBrown Universitygeman@dam.brown.eduMark JohnsonCognitive and Linguistic SciencesBrown UniversityMark Johnson@Brown.eduAbstractStochastic unification-based grammars(SUBGs) define exponential distributionsover the parses generated by a unification-based grammar (UBG).
Existing algo-rithms for parsing and estimation requirethe enumeration of all of the parses of astring in order to determine the most likelyone, or in order to calculate the statis-tics needed to estimate a grammar froma training corpus.
This paper describes agraph-based dynamic programming algo-rithm for calculating these statistics fromthe packed UBG parse representations ofMaxwell and Kaplan (1995) which doesnot require enumerating all parses.
Likemany graphical algorithms, the dynamicprogramming algorithm?s complexity isworst-case exponential, but is often poly-nomial.
The key observation is that byusing Maxwell and Kaplan packed repre-sentations, the required statistics can berewritten as either the max or the sum ofa product of functions.
This is exactlythe kind of problem which can be solvedby dynamic programming over graphicalmodels.?
We would like to thank Eugene Charniak, MiyaoYusuke, Mark Steedman as well as Stefan Riezler and the teamat PARC; naturally all errors remain our own.
This research wassupported by NSF awards DMS 0074276 and ITR IIS 0085940.1 IntroductionStochastic Unification-Based Grammars (SUBGs)use log-linear models (also known as exponential orMaxEnt models and Markov Random Fields) to de-fine probability distributions over the parses of a uni-fication grammar.
These grammars can incorporatevirtually all kinds of linguistically important con-straints (including non-local and non-context-freeconstraints), and are equipped with a statisticallysound framework for estimation and learning.Abney (1997) pointed out that the non-context-free dependencies of a unification grammar requirestochastic models more general than Probabilis-tic Context-Free Grammars (PCFGs) and MarkovBranching Processes, and proposed the use of log-linear models for defining probability distributionsover the parses of a unification grammar.
Un-fortunately, the maximum likelihood estimator Ab-ney proposed for SUBGs seems computationally in-tractable since it requires statistics that depend onthe set of all parses of all strings generated by thegrammar.
This set is infinite (so exhaustive enumer-ation is impossible) and presumably has a very com-plex structure (so sampling estimates might take anextremely long time to converge).Johnson et al (1999) observed that parsing andrelated tasks only require conditional distributionsover parses given strings, and that such conditionaldistributions are considerably easier to estimate thanjoint distributions of strings and their parses.
Theconditional maximum likelihood estimator proposedby Johnson et al requires statistics that depend onthe set of all parses of the strings in the training cor-Computational Linguistics (ACL), Philadelphia, July 2002, pp.
279-286.Proceedings of the 40th Annual Meeting of the Association forpus.
For most linguistically realistic grammars thisset is finite, and for moderate sized grammars andtraining corpora this estimation procedure is quitefeasible.However, our recent experiments involve trainingfrom the Wall Street Journal Penn Tree-bank, andrepeatedly enumerating the parses of its 50,000 sen-tences is quite time-consuming.
Matters are onlymade worse because we have moved some of theconstraints in the grammar from the unification com-ponent to the stochastic component.
This broadensthe coverage of the grammar, but at the expense ofmassively expanding the number of possible parsesof each sentence.In the mid-1990s unification-based parsers weredeveloped that do not enumerate all parses of a stringbut instead manipulate and return a ?packed?
rep-resentation of the set of parses.
This paper de-scribes how to find the most probable parse andthe statistics required for estimating a SUBG fromthe packed parse set representations proposed byMaxwell III and Kaplan (1995).
This makes it pos-sible to avoid explicitly enumerating the parses ofthe strings in the training corpus.The methods proposed here are analogues ofthe well-known dynamic programming algorithmsfor Probabilistic Context-Free Grammars (PCFGs);specifically the Viterbi algorithm for finding themost probable parse of a string, and the Inside-Outside algorithm for estimating a PCFG from un-parsed training data.1 In fact, because Maxwell andKaplan packed representations are just Truth Main-tenance System (TMS) representations (Forbus andde Kleer, 1993), the statistical techniques describedhere should extend to non-linguistic applications ofTMSs as well.Dynamic programming techniques havebeen applied to log-linear models before.Lafferty et al (2001) mention that dynamicprogramming can be used to compute the statisticsrequired for conditional estimation of log-linearmodels based on context-free grammars wherethe properties can include arbitrary functions ofthe input string.
Miyao and Tsujii (2002) (which1However, because we use conditional estimation, alsoknown as discriminative training, we require at least some dis-criminating information about the correct parse of a string inorder to estimate a stochastic unification grammar.appeared after this paper was accepted) is the closestrelated work we know of.
They describe a techniquefor calculating the statistics required to estimate alog-linear parsing model with non-local propertiesfrom packed feature forests.The rest of this paper is structured as follows.The next section describes unification grammarsand Maxwell and Kaplan packed representation.The following section reviews stochastic unifica-tion grammars (Abney, 1997) and the statisticalquantities required for efficiently estimating suchgrammars from parsed training data (Johnson et al,1999).
The final substantive section of this papershows how these quantities can be defined directlyin terms of the Maxwell and Kaplan packed repre-sentations.The notation used in this paper is as follows.
Vari-ables are written in upper case italic, e.g., X,Y , etc.,the sets they range over are written in script, e.g.,X ,Y , etc., while specific values are written in lowercase italic, e.g., x, y, etc.
In the case of vector-valuedentities, subscripts indicate particular components.2 Maxwell and Kaplan packedrepresentationsThis section characterises the properties of unifica-tion grammars and the Maxwell and Kaplan packedparse representations that will be important for whatfollows.
This characterisation omits many detailsabout unification grammars and the algorithm bywhich the packed representations are actually con-structed; see Maxwell III and Kaplan (1995) for de-tails.A parse generated by a unification grammar is anite subset of a set F of features.
Features are parsefragments, e.g., chart edges or arcs from attribute-value structures, out of which the packed representa-tions are constructed.
For this paper it does not mat-ter exactly what features are, but they are intendedto be the atomic entities manipulated by a dynamicprogramming parsing algorithm.
A grammar definesa set ?
of well-formed or grammatical parses.
Eachparse ?
?
?
is associated with a string of wordsY (?)
called its yield.
Note that except for trivialgrammars F and ?
are infinite.If y is a string, then let ?
(y) = {?
?
?|Y (?)
=y} and F(y) = ????
(y){f ?
?}.
That is, ?
(y) isthe set of parses of a string y and F(y) is the set offeatures appearing in the parses of y.
In the gram-mars of interest here ?
(y) and hence also F(y) arefinite.Maxwell and Kaplan?s packed representations of-ten provide a more compact representation of theset of parses of a sentence than would be obtainedby merely listing each parse separately.
The intu-ition behind these packed representations is that formost strings y, many of the features in F(y) occurin many of the parses ?(y).
This is often the casein natural language, since the same substructure canappear as a component of many different parses.Packed feature representations are defined interms of conditions on the values assigned to a vec-tor of variables X .
These variables have no directlinguistic interpretation; rather, each different as-signment of values to these variables identifies a setof features which constitutes one of the parses inthe packed representation.
A condition a on X isa function from X to {0, 1}.
While for uniformitywe write conditions as functions on the entire vec-tor X , in practice Maxwell and Kaplan?s approachproduces conditions whose value depends only on afew of the variables in X , and the efficiency of thealgorithms described here depends on this.A packed representation of a finite set of parses isa quadruple R = (F ?, X,N, ?
), where:?
F ?
?
F(y) is a finite set of features,?
X is a finite vector of variables, where eachvariable X` ranges over the finite set X`,?
N is a finite set of conditions on X called theno-goods,2 and?
?
is a function that maps each feature f ?
F ?to a condition ?f on X .A vector of values x satises the no-goods N iffN(x) = 1, where N(x) = ??
?N ?(x).
Each xthat satisfies the no-goods identies a parse ?
(x) ={f ?
F ?|?f (x) = 1}, i.e., ?
is the set of featureswhose conditions are satisfied by x.
We require thateach parse be identified by a unique value satisfying2The name ?no-good?
comes from the TMS literature, andwas used by Maxwell and Kaplan.
However, here the no-goodsactually identify the good variable assignments.the no-goods.
That is, we require that:?x, x?
?
X if N(x) = N(x?)
= 1 and?
(x) = ?(x?)
then x = x?
(1)Finally, a packed representation R represents theset of parses ?
(R) that are identified by valuesthat satisfy the no-goods, i.e., ?
(R) = {?
(x)|x ?X , N(x) = 1}.Maxwell III and Kaplan (1995) describes a pars-ing algorithm for unification-based grammars thattakes as input a string y and returns a packed rep-resentation R such that ?
(R) = ?
(y), i.e., R rep-resents the set of parses of the string y.
The SUBGparsing and estimation algorithms described in thispaper use Maxwell and Kaplan?s parsing algorithmas a subroutine.3 Stochastic Unification-Based GrammarsThis section reviews the probabilistic frameworkused in SUBGs, and describes the statistics thatmust be calculated in order to estimate the pa-rameters of a SUBG from parsed training data.For a more detailed exposition and descriptionsof regularization and other important details, seeJohnson et al (1999).The probability distribution over parses is definedin terms of a finite vector g = (g1, .
.
.
, gm) ofproperties.
A property is a real-valued function ofparses ?.
Johnson et al (1999) placed no restric-tions on what functions could be properties, permit-ting properties to encode arbitrary global informa-tion about a parse.
However, the dynamic program-ming algorithms presented here require the informa-tion encoded in properties to be local with respect tothe features F used in the packed parse representa-tion.
Specifically, we require that properties be de-fined on features rather than parses, i.e., each featuref ?
F is associated with a finite vector of real values(g1(f), .
.
.
, gm(f)) which define the property func-tions for parses as follows:gk(?)
=?f?
?gk(f), for k = 1 .
.
.
m. (2)That is, the property values of a parse are the sumof the property values of its features.
In the usualcase, some features will be associated with a singleproperty (i.e., gk(f) is equal to 1 for a specific valueof k and 0 otherwise), and other features will be as-sociated with no properties at all (i.e., g(f) = 0).This requires properties be very local with re-spect to features, which means that we give up theability to define properties arbitrarily.
Note how-ever that we can still encode essentially arbitrarylinguistic information in properties by adding spe-cialised features to the underlying unification gram-mar.
For example, suppose we want a property thatindicates whether the parse contains a reduced rela-tive clauses headed by a past participle (such ?gar-den path?
constructions are grammatical but oftenalmost incomprehensible, and alternative parses notincluding such constructions would probably be pre-ferred).
Under the current definition of properties,we can introduce such a property by modifying theunderlying unification grammar to produce a certain?diacritic?
feature in a parse just in case the parse ac-tually contains the appropriate reduced relative con-struction.
Thus, while properties are required to belocal relative to features, we can use the ability ofthe underlying unification grammar to encode essen-tially arbitrary non-local information in features tointroduce properties that also encode non-local in-formation.A Stochastic Unification-Based Grammar is atriple (U, g, ?
), where U is a unification grammarthat defines a set ?
of parses as described above,g = (g1, .
.
.
, gm) is a vector of property functions asjust described, and ?
= (?1, .
.
.
, ?m) is a vector ofnon-negative real-valued parameters called propertyweights.
The probability P?(?)
of a parse ?
?
?
is:P?(?)
=W?(?
)Z?, where:W?(?)
=m?j=1?gj(?
)j , andZ?
=?????W?(??
)Intuitively, if gj(?)
is the number of times that prop-erty j occurs in ?
then ?j is the ?weight?
or ?cost?
ofeach occurrence of property j and Z?
is a normal-ising constant that ensures that the probability of allparses sums to 1.Now we discuss the calculation of several impor-tant quantities for SUBGs.
In each case we showthat the quantity can be expressed as the value thatmaximises a product of functions or else as the sumof a product of functions, each of which dependson a small subset of the variables X .
These are thekinds of quantities for which dynamic programminggraphical model algorithms have been developed.3.1 The most probable parseIn parsing applications it is important to be able toextract the most probable (or MAP) parse ??
(y) ofstring y with respect to a SUBG.
This parse is:??
(y) = argmax???(y)W?(?
)Given a packed representation (F ?, X,N, ?)
for theparses ?
(y), let x?
(y) be the x that identifies ??
(y).Since W?(??
(y)) > 0, it can be shown that:x?
(y) = argmaxx?XN(x)m?j=1?gj(?
(x))j= argmaxx?XN(x)m?j=1??f??
(x) gj(f)j= argmaxx?XN(x)m?j=1??f?F?
?f (x)gj(f)j= argmaxx?XN(x)m?j=1?f?F ??
?f (x)gj(f)j= argmaxx?XN(x)?f?F ???m?j=1?gj(f)j??
?f (x)= argmaxx?X???N?
(x)?f?F ?h?,f (x) (3)where h?,f (x) = ?mj=1 ?gj(f)j if ?f (x) = 1 andh?,f (x) = 1 if ?f (x) = 0.
Note that h?,f (x) de-pends on exactly the same variables in X as ?f does.As (3) makes clear, finding x?
(y) involves maximis-ing a product of functions where each function de-pends on a subset of the variables X .
As explainedbelow, this is exactly the kind of maximisation thatcan be solved using graphical model techniques.3.2 Conditional likelihoodWe now turn to the estimation of the propertyweights ?
from a training corpus of parsed data D =(?1, .
.
.
, ?n).
As explained in Johnson et al (1999),one way to do this is to find the ?
that maximises theconditional likelihood of the training corpus parsesgiven their yields.
(Johnson et al actually maximiseconditional likelihood regularized with a Gaussianprior, but for simplicity we ignore this here).
If yi isthe yield of the parse ?i, the conditional likelihoodof the parses given their yields is:LD(?)
=n?i=1W?(?i)Z?(?
(yi))where ?
(y) is the set of parses with yield y and:Z?
(S) =???SW?(?
).Then the maximum conditional likelihood estimate??
of ?
is ??
= argmax?
LD(?
).Now calculating W?
(?i) poses no computationalproblems, but since ?
(yi) (the set of parses for yi)can be large, calculating Z?(?
(yi)) by enumeratingeach ?
?
?
(yi) can be computationally expensive.However, there is an alternative method for calcu-lating Z?(?
(yi)) that does not involve this enumera-tion.
As noted above, for each yield yi, i = 1, .
.
.
, n,Maxwell?s parsing algorithm returns a packed fea-ture structure Ri that represents the parses of yi, i.e.,?
(yi) = ?(Ri).
A derivation parallel to the one for(3) shows that for R = (F ?, X,N, ?):Z?(?
(R)) =?x?X???N?
(x)?f?F ?h?,f (x) (4)(This derivation relies on the isomorphism betweenparses and variable assignments in (1)).
It turns outthat this type of sum can also be calculated usinggraphical model techniques.3.3 Conditional ExpectationsIn general, iterative numerical procedures are re-quired to find the property weights ?
that maximisethe conditional likelihood LD(?).
While there area number of different techniques that can be used,all of the efficient techniques require the calculationof conditional expectations E?
[gk|yi] for each prop-erty gk and each sentence yi in the training corpus,where:E?
[g|y] =????(y)g(?)P?(?|y)=????
(y) g(?)W?(?)Z?(?
(y))For example, the Conjugate Gradient algorithm,which was used by Johnson et al, requires the cal-culation not just of LD(?)
but also its derivatives?LD(?)/??k.
It is straight-forward to show:?LD(?)?
?k= LD(?
)?kn?i=1(gk(?i) ?
E?
[gk|yi]) .We have just described the calculation of LD(?
),so if we can calculate E?
[gk|yi] then we can calcu-late the partial derivatives required by the ConjugateGradient algorithm as well.Again, let R = (F ?, X,N, ?)
be a packed repre-sentation such that ?
(R) = ?(yi).
First, note that(2) implies that:E?
[gk|yi] =?f?F ?gk(f) P({?
: f ?
?
}|yi).Note that P({?
: f ?
?
}|yi) involves the sum ofweights over all x ?
X subject to the conditionsthat N(x) = 1 and ?f (x) = 1.
Thus P({?
: f ??
}|yi) can also be expressed in a form that is easyto evaluate using graphical techniques.Z?(?(R))P?({?
: f ?
?
}|yi)=?x?X?f (x)???N?
(x)?f ?
?F ?h?,f ?
(x) (5)4 Graphical model calculationsIn this section we briefly review graphical modelalgorithms for maximising and summing productsof functions of the kind presented above.
It turnsout that the algorithm for maximisation is a gener-alisation of the Viterbi algorithm for HMMs, andthe algorithm for computing the summation in (5)is a generalisation of the forward-backward algo-rithm for HMMs (Smyth et al, 1997).
Viewedabstractly, these algorithms simplify these expres-sions by moving common factors over the max orsum operators respectively.
These techniques arenow relatively standard; the most well-known ap-proach involves junction trees (Pearl, 1988; Cow-ell, 1999).
We adopt the approach approach de-scribed by Geman and Kochanek (2000), which isa straightforward generalization of HMM dynamicprogramming with minimal assumptions and pro-gramming overhead.
However, in principle any ofthe graphical model computational algorithms canbe used.The quantities (3), (4) and (5) involve maximisa-tion or summation over a product of functions, eachof which depends only on the values of a subset ofthe variables X .
There are dynamic programmingalgorithms for calculating all of these quantities, butfor reasons of space we only describe an algorithmfor finding the maximum value of a product of func-tions.
These graph algorithms are rather involved.It may be easier to follow if one reads Example 1before or in parallel with the definitions below.To explain the algorithm we use the following no-tation.
If x and x?
are both vectors of length mthen x =j x?
iff x and x?
disagree on at most theirjth components, i.e., xk = x?k for k = 1, .
.
.
, j ?1, j + 1, .
.
.
m. If f is a function whose domainis X , we say that f depends on the set of variablesd(f) = {Xj |?x, x?
?
X , x =j x?, f(x) 6= f(x?
)}.That is, Xj ?
d(f) iff changing the value of Xj canchange the value of f .The algorithm relies on the fact that the variablesin X = (X1, .
.
.
, Xn) are ordered (e.g., X1 pre-cedes X2, etc.
), and while the algorithm is correctfor any variable ordering, its efficiency may varydramatically depending on the ordering as describedbelow.
Let H be any set of functions whose do-mains are X .
We partition H into disjoint subsetsH1, .
.
.
,Hn+1, where Hj is the subset of H that de-pend on Xj but do not depend on any variables or-dered before Xj , and Hn+1 is the subset of H that donot depend on any variables at all (i.e., they are con-stants).3 That is, Hj = {H ?
H|Xj ?
d(H),?i <j Xi 6?
d(H)} and Hn+1 = {H ?
H|d(H) = ?
}.As explained in section 3.1, there is a set of func-tions A such that the quantities we need to calculatehave the general form:Mmax = maxx?X?A?AA(x) (6)x?
= argmaxx?X?A?AA(x).
(7)Mmax is the maximum value of the product expres-sion while x?
is the value of the variables at which themaximum occurs.
In a SUBG parsing application x?identifies the MAP parse.3Strictly speaking this does not necessarily define a parti-tion, as some of the subsetsHj may be empty.The procedure depends on two sequences of func-tions Mi, i = 1, .
.
.
, n + 1 and Vi, i = 1, .
.
.
, n.Informally, Mi is the maximum value attained bythe subset of the functions A that depend on one ofthe variables X1, .
.
.
, Xi, and Vi gives informationabout the value of Xi at which this maximum is at-tained.To simplify notation we write these functions asfunctions of the entire set of variables X , but usu-ally depend on a much smaller set of variables.
TheMi are real valued, while each Vi ranges over Xi.Let M = {M1, .
.
.
,Mn}.
Recall that the sets offunctions A and M can be both be partitioned intodisjoint subsets A1, .
.
.
,An+1 and M1, .
.
.
,Mn+1respectively on the basis of the variables each Aiand Mi depend on.
The definition of the Mi andVi, i = 1, .
.
.
, n is as follows:Mi(x) = maxx??Xs.t.
x?=ix?A?AiA(x?)?M?MiM(x?)
(8)Vi(x) = argmaxx??Xs.t.
x?=ix?A?AiA(x?)?M?MiM(x?
)Mn+1 receives a special definition, since there is novariable Xn+1.Mn+1 =???A?An+1A?????M?Mn+1M??
(9)The definition of Mi in (8) may look circular (sinceM appears in the right-hand side), but in fact it isnot.
First, note that Mi depends only on variablesordered after Xi, so if Mj ?
Mi then j < i. Morespecifically,d(Mi) =??
?A?Aid(A) ??M?Mid(M)??
\ {Xi}.Thus we can compute the Mi in the orderM1, .
.
.
,Mn+1, inserting Mi into the appropriate setMk, where k > i, when Mi is computed.We claim that Mmax = Mn+1.
(Note that Mn+1and Mn are constants, since there are no variablesordered after Xn).
To see this, consider the tree Twhose nodes are the Mi, and which has a directededge from Mi to Mj iff Mi ?
Mj (i.e., Mi appearsin the right hand side of the definition (8) of Mj).T has a unique root Mn+1, so there is a path fromevery Mi to Mn+1.
Let i ?
j iff there is a pathfrom Mi to Mj in this tree.
Then a simple inductionshows that Mj is a function from d(Mj) to a max-imisation over each of the variables Xi where i ?
jof ?i?j,A?Ai A.Further, it is straightforward to show that Vi(x?)
=x?i (the value x?
assigns to Xi).
By the same argu-ments as above, d(Vi) only contains variables or-dered after Xi, so Vn = x?n.
Thus we can evaluatethe Vi in the order Vn, .
.
.
, V1 to find the maximisingassignment x?.Example 1 Let X = { X1, X2, X3, X4, X5,X6, X7} and set A = {a(X1, X3), b(X2, X4),c(X3, X4, X5), d(X4, X5), e(X6, X7)}.
We canrepresent the sharing of variables in A by means of aundirected graph GA, where the nodes of GA are thevariables X and there is an edge in GA connectingXi to Xj iff ?A ?
A such that both Xi, Xj ?
d(A).GA is depicted below.    X1 X3 X5 X6X2 X4 X7r r rrrrrStarting with the variable X1, we compute M1and V1:M1(x3) = maxx1?X1a(x1, x3)V1(x3) = argmaxx1?X1a(x1, x3)We now proceed to the variable X2.M2(x4) = maxx2?X2 b(x2, x4)V2(x4) = argmaxx2?X2b(x2, x4)Since M1 belongs to M3, it appears in the denitionof M3.M3(x4, x5) = maxx3?X3c(x3, x4, x5)M1(x3)V3(x4, x5) = argmaxx3?X3c(x3, x4, x5)M1(x3)Similarly, M4 is dened in terms of M2 and M3.M4(x5) = maxx4?X4 d(x4, x5)M2(x4)M3(x4, x5)V4(x5) = argmaxx4?X4d(x4, x5)M2(x4)M3(x4, x5)Note that M5 is a constant, reecting the fact thatin GA the node X5 is not connected to any node or-dered after it.M5 = maxx5?X5 M4(x5)V5 = argmaxx5?X5M4(x5)The second component is dened in the same way:M6(x7) = maxx6?X6 e(x6, x7)V6(x7) = argmaxx6?X6e(x6, x7)M7 = maxx7?X7 M6(x7)V7 = argmaxx7?X7M6(x7)The maximum value for the product M8 = Mmax isdened in terms of M5 and M7.Mmax = M8 = M5M7Finally, we evaluate V7, .
.
.
, V1 to nd the maximis-ing assignment x?.x?7 = V7x?6 = V6(x?7)x?5 = V5x?4 = V4(x?5)x?3 = V3(x?4, x?5)x?2 = V2(x?4)x?1 = V1(x?3)We now briefly consider the computational com-plexity of this process.
Clearly, the number of stepsrequired to compute each Mi is a polynomial of or-der |d(Mi)|+1, since we need to enumerate all pos-sible values for the argument variables d(Mi) andfor each of these, maximise over the set Xi.
Fur-ther, it is easy to show that in terms of the graph GA,d(Mj) consists of those variables Xk, k > j reach-able by a path starting at Xj and all of whose nodesexcept the last are variables that precede Xj .Since computational effort is bounded above by apolynomial of order |d(Mi)|+ 1, we seek a variableordering that bounds the maximum value of |d(Mi)|.Unfortunately, finding the ordering that minimisesthe maximum value of |d(Mi)| is an NP-completeproblem.
However, there are several efficient heuris-tics that are reputed in graphical models communityto produce good visitation schedules.
It may be thatthey will perform well in the SUBG parsing applica-tions as well.5 ConclusionThis paper shows how to apply dynamic program-ming methods developed for graphical models toSUBGs to find the most probable parse and to ob-tain the statistics needed for estimation directly fromMaxwell and Kaplan packed parse representations.i.e., without expanding these into individual parses.The algorithm rests on the observation that so longas features are local to the parse fragments used inthe packed representations, the statistics required forparsing and estimation are the kinds of quantitiesthat dynamic programming algorithms for graphicalmodels can perform.
Since neither Maxwell and Ka-plan?s packed parsing algorithm nor the proceduresdescribed here depend on the details of the underly-ing linguistic theory, the approach should apply tovirtually any kind of underlying grammar.Obviously, an empirical evaluation of the algo-rithms described here would be extremely useful.The algorithms described here are exact, but be-cause we are working with unification grammarsand apparently arbitrary graphical models we can-not polynomially bound their computational com-plexity.
However, it seems reasonable to expectthat if the linguistic dependencies in a sentence typ-ically factorize into largely non-interacting cliquesthen the dynamic programming methods may offerdramatic computational savings compared to currentmethods that enumerate all possible parses.It might be interesting to compare these dy-namic programming algorithms with a standardunification-based parser using a best-first searchheuristic.
(To our knowledge such an approach hasnot yet been explored, but it seems straightforward:the figure of merit could simply be the sum of theweights of the properties of each partial parse?s frag-ments).
Because such parsers prune the search spacethey cannot guarantee correct results, unlike the al-gorithms proposed here.
Such a best-first parsermight be accurate when parsing with a trained gram-mar, but its results may be poor at the beginningof parameter weight estimation when the parameterweight estimates are themselves inaccurate.Finally, it would be extremely interesting to com-pare these dynamic programming algorithms tothe ones described by Miyao and Tsujii (2002).
Itseems that the Maxwell and Kaplan packed repre-sentation may permit more compact representationsthan the disjunctive representations used by Miyaoet al, but this does not imply that the algorithmsproposed here are more efficient.
Further theoreti-cal and empirical investigation is required.ReferencesSteven Abney.
1997.
Stochastic Attribute-Value Grammars.Computational Linguistics, 23(4):597?617.Robert Cowell.
1999.
Introduction to inference for Bayesiannetworks.
In Michael Jordan, editor, Learning in Graphi-cal Models, pages 9?26.
The MIT Press, Cambridge, Mas-sachusetts.Kenneth D. Forbus and Johan de Kleer.
1993.
Building problemsolvers.
The MIT Press, Cambridge, Massachusetts.Stuart Geman and Kevin Kochanek.
2000.
Dynamic program-ming and the representation of soft-decodable codes.
Tech-nical report, Division of Applied Mathematics, Brown Uni-versity.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, andStefan Riezler.
1999.
Estimators for stochastic ?unification-based?
grammars.
In The Proceedings of the 37th AnnualConference of the Association for Computational Linguis-tics, pages 535?541, San Francisco.
Morgan Kaufmann.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.Conditional Random Fields: Probabilistic models for seg-menting and labeling sequence data.
In Machine Learn-ing: Proceedings of the Eighteenth International Conference(ICML 2001), Stanford, California.John T. Maxwell III and Ronald M. Kaplan.
1995.
A methodfor disjunctive constraint satisfaction.
In Mary Dalrymple,Ronald M. Kaplan, John T. Maxwell III, and Annie Zae-nen, editors, Formal Issues in Lexical-Functional Grammar,number 47 in CSLI Lecture Notes Series, chapter 14, pages381?481.
CSLI Publications.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximum entropyestimation for feature forests.
In Proceedings of HumanLanguage Technology Conference 2002, March.Judea Pearl.
1988.
Probabalistic Reasoning in Intelligent Sys-tems: Networks of Plausible Inference.
Morgan Kaufmann,San Mateo, California.Padhraic Smyth, David Heckerman, and Michael Jordan.
1997.Probabilistic Independence Networks for Hidden MarkovModels.
Neural Computation, 9(2):227?269.
