Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 696?703,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsSparse Information Extraction:Unsupervised Language Models to the RescueDoug Downey, Stefan Schoenmackers, and Oren EtzioniTuring Center, Department of Computer Science and EngineeringUniversity of Washington, Box 352350Seattle, WA 98195, USA{ddowney,stef,etzioni}@cs.washington.eduAbstractEven in a massive corpus such as the Web, asubstantial fraction of extractions appear in-frequently.
This paper shows how to assessthe correctness of sparse extractions by uti-lizing unsupervised language models.
TheREALM system, which combines HMM-based and n-gram-based language models,ranks candidate extractions by the likeli-hood that they are correct.
Our experimentsshow that REALM reduces extraction errorby 39%, on average, when compared withprevious work.Because REALM pre-computes languagemodels based on its corpus and does not re-quire any hand-tagged seeds, it is far morescalable than approaches that learn mod-els for each individual relation from hand-tagged data.
Thus, REALM is ideally suitedfor open information extraction where therelations of interest are not specified in ad-vance and their number is potentially vast.1 IntroductionInformation Extraction (IE) from text is far from in-fallible.
In response, researchers have begun to ex-ploit the redundancy in massive corpora such as theWeb in order to assess the veracity of extractions(e.g., (Downey et al, 2005; Etzioni et al, 2005;Feldman et al, 2006)).
In essence, such methods uti-lize extraction patterns to generate candidate extrac-tions (e.g., ?Istanbul?)
and then assess each candi-date by computing co-occurrence statistics betweenthe extraction and words or phrases indicative ofclass membership (e.g., ?cities such as?
).However, Zipf?s Law governs the distribution ofextractions.
Thus, even the Web has limited redun-dancy for less prominent instances of relations.
In-deed, 50% of the extractions in the data sets em-ployed by (Downey et al, 2005) appeared onlyonce.
As a result, Downey et al?s model, and re-lated methods, had no way of assessing which ex-traction is more likely to be correct for fully half ofthe extractions.
This problem is particularly acutewhen moving beyond unary relations.
We refer tothis challenge as the task of assessing sparse extrac-tions.This paper introduces the idea that language mod-eling techniques such as n-gram statistics (Manningand Schu?tze, 1999) and HMMs (Rabiner, 1989) canbe used to effectively assess sparse extractions.
Thepaper introduces the REALM system, and highlightsits unique properties.
Notably, REALM does notrequire any hand-tagged seeds, which enables it toscale to Open IE?extraction where the relations ofinterest are not specified in advance, and their num-ber is potentially vast (Banko et al, 2007).REALM is based on two key hypotheses.
TheKnowItAll hypothesis is that extractions that oc-cur more frequently in distinct sentences in thecorpus are more likely to be correct.
For exam-ple, the hypothesis suggests that the argument pair(Giuliani, New York) is relatively likely to beappropriate for the Mayor relation, simply becausethis pair is extracted for the Mayor relation rela-tively frequently.
Second, we employ an instance ofthe distributional hypothesis (Harris, 1985), which696can be phrased as follows: different instances ofthe same semantic relation tend to appear in sim-ilar textual contexts.
We assess sparse extractionsby comparing the contexts in which they appear tothose of more common extractions.
Sparse extrac-tions whose contexts are more similar to those ofcommon extractions are judged more likely to becorrect based on the conjunction of the KnowItAlland the distributional hypotheses.The contributions of the paper are as follows:?
The paper introduces the insight that the sub-field of language modeling provides unsuper-vised methods that can be leveraged to assesssparse extractions.
These methods are morescalable than previous assessment techniques,and require no hand tagging whatsoever.?
The paper introduces an HMM-based tech-nique for checking whether two arguments areof the proper type for a relation.?
The paper introduces a relational n-grammodel for the purpose of determining whethera sentence that mentions multiple argumentsactually expresses a particular relationship be-tween them.?
The paper introduces a novel language-modeling system called REALM that combinesboth HMM-based models and relational n-gram models, and shows that REALM reduceserror by an average of 39% over previous meth-ods, when applied to sparse extraction data.The remainder of the paper is organized as fol-lows.
Section 2 introduces the IE assessment task,and describes the REALM system in detail.
Section3 reports on our experimental results followed by adiscussion of related work in Section 4.
Finally, weconclude with a discussion of scalability and withdirections for future work.2 IE AssessmentThis section formalizes the IE assessment task anddescribes the REALM system for solving it.
An IEassessor takes as input a list of candidate extractionsmeant to denote instances of a relation, and outputsa ranking of the extractions with the goal that cor-rect extractions rank higher than incorrect ones.
Acorrect extraction is defined to be a true instance ofthe relation mentioned in the input text.More formally, the list of candidate extrac-tions for a relation R is denoted as ER ={(a1, b1), .
.
.
, (am, bm)}.
An extraction (ai, bi) isan ordered pair of strings.
The extraction is correctif and only if the relation R holds between the argu-ments named by ai and bi.
For example, for R =Headquartered, a pair (ai, bi) is correct iff thereexists an organization ai that is in fact headquarteredin the location bi.1ER is generated by applying an extraction mech-anism, typically a set of extraction ?patterns?, toeach sentence in a corpus, and recording the results.Thus, many elements of ER are identical extractionsderived from different sentences in the corpus.This task definition is notable for the minimalinputs required?IE assessment does not requireknowing the relation name nor does it require hand-tagged seed examples of the relation.
Thus, an IEAssessor is applicable to Open IE.2.1 System OverviewIn this section, we describe the REALM system,which utilizes language modeling techniques to per-form IE Assessment.REALM takes as input a set of extractions ER,and outputs a ranking of those extractions.
Thealgorithm REALM follows is outlined in Figure 1.REALM begins by automatically selecting from ERa set of bootstrapped seeds SR intended to serve ascorrect examples of the relation R. REALM utilizesthe KnowItAll hypothesis, setting SR equal to theh elements in ER extracted most frequently fromthe underlying corpus.
This results in a noisy set ofseeds, but the methods that use these seeds are noisetolerant.REALM then proceeds to rank the remaining(non-seed) extractions by utilizing two language-modeling components.
An n-gram language modelis a probability distribution P (w1, ..., wn) over con-secutive word sequences of length n in a corpus.Formally, if we assume a seed (s1, s2) is a correctextraction of a relation R, the distributional hypoth-esis states that the context distribution around theseed extraction, P (w1, ..., wn|wi = s1, wj = s2)for 1 ?
i, j ?
n tends to be ?more similar?
to1For clarity, our discussion focuses on relations betweenpairs of arguments.
However, the methods we propose can beextended to relations of any arity.697P (w1, ..., wn|wi = e1, wj = e2) when the extrac-tion (e1, e2) is correct.
Naively comparing contextdistributions is problematic, however, because thearguments to a relation often appear separated byseveral intervening words.
In our experiments, wefound that when relation arguments appear togetherin a sentence, 75% of the time the arguments areseparated by at least three words.
This implies thatn must be large, and for sparse argument pairs it isnot possible to estimate such a large language modelaccurately, because the number of modeling param-eters is proportional to the vocabulary size raised tothe nth power.
To mitigate sparsity, REALM utilizessmaller language models in its two components as ameans of ?backing-off?
from estimating context dis-tributions explicitly, as described below.First, REALM utilizes an HMM to estimatewhether each extraction has arguments of the propertype for the relation.
Each relation R has a setof types for its arguments.
For example, the rela-tion AuthorOf(a, b) requires that its first ar-gument be an author, and that its second be somekind of written work.
Knowing whether extractedarguments are of the proper type for a relation canbe quite informative for assessing extractions.
Thechallenge is, however, that this type information isnot given to the system since the relations (and thetypes of the arguments) are not known in advance.REALM solves this problem by comparing the dis-tributions of the seed arguments and extraction ar-guments.
Type checking mitigates data sparsity byleveraging every occurrence of the individual extrac-tion arguments in the corpus, rather than only thosecases in which argument pairs occur near each other.Although argument type checking is invalu-able for extraction assessment, it is not suf-ficient for extracting relationships between ar-guments.
For example, an IE system us-ing only type information might determine thatIntel is a corporation and that Seattle isa city, and therefore erroneously conclude thatHeadquartered(Intel, Seattle) is cor-rect.
Thus, REALM?s second step is to employ ann-gram-based language model to assess whether theextracted arguments share the appropriate relation.Again, this information is not given to the system,so REALM compares the context distributions of theextractions to those of the seeds.
As described inREALM(Extractions ER = {e1, ..., em})SR = the h most frequent extractions in ERUR = ER - SRTypeRankings(UR)?
HMM-T(SR, UR)RelationRankings(UR)?
REL-GRAMS(SR, UR)return a ranking of ER with the elements of SR at thetop (ranked by frequency) followed by the elements ofUR = {u1, ..., um?h} ranked in ascending order ofTypeRanking(ui) ?RelationRanking(ui).Figure 1: Pseudocode for REALM at run-time.The language models used by the HMM-T andREL-GRAMS components are constructed in a pre-processing step.Section 2.3, REALM employs a relational n-gramlanguage model in order to accurately compare con-text distributions when extractions are sparse.REALM executes the type checking and relationassessment components separately; each componenttakes the seed and non-seed extractions as argumentsand returns a ranking of the non-seeds.
REALM thencombines the two components?
assessments into asingle ranking.
Although several such combinationsare possible, REALM simply ranks the extractions inascending order of the product of the ranks assignedby the two components.
The following subsectionsdescribe REALM?s two components in detail.We identify the proper nouns in our corpus us-ing the LEX method (Downey et al, 2007).
In ad-dition to locating the proper nouns in the corpus,LEX also concatenates each multi-token proper noun(e.g.,Los Angeles) together into a single token.Both of REALM?s components construct languagemodels from this tokenized corpus.2.2 Type Checking with HMM-TIn this section, we describe our type-checking com-ponent, which takes the form of a Hidden MarkovModel and is referred to as HMM-T. HMM-T ranksthe set UR of non-seed extractions, with a goal ofranking those extractions with arguments of propertype for R above extractions containing type errors.Formally, let URi denote the set of the ith argumentsof the extractions in UR.
Let SRi be defined simi-larly for the seed set SR.Our type checking technique exploits the distri-butional hypothesis?in this case, the intuition that698Intel , headquartered in Santa+ClaraFigure 2: Graphical model employed by HMM-T.
Shown is the case in which k = 2.
Corpuspre-processing results in the proper noun SantaClara being concatenated into a single token.extraction arguments in URi of the proper type willlikely appear in contexts similar to those in whichthe seed arguments SRi appear.
In order to iden-tify terms that are distributionally similar, we traina probabilistic generative Hidden Markov Model(HMM), which treats each token in the corpus asgenerated by a single hidden state variable.
Here, thehidden states take integral values from {1, .
.
.
, T},and each hidden state variable is itself generated bysome number k of previous hidden states.2 For-mally, the joint distribution of the corpus, repre-sented as a vector of tokens w, given a correspond-ing vector of states t is:P (w|t) =?iP (wi|ti)P (ti|ti?1, .
.
.
, ti?k) (1)The distributions on the right side of Equation 1can be learned from a corpus in an unsupervisedmanner, such that words which are distributed sim-ilarly in the corpus tend to be generated by simi-lar hidden states (Rabiner, 1989).
The generativemodel is depicted as a Bayesian network in Figure 2.The figure also illustrates the one way in which ourimplementation is distinct from a standard HMM,namely that proper nouns are detected a priori andmodeled as single tokens (e.g., Santa Clara isgenerated by a single hidden state).
This allowsthe type checker to compare the state distributionsof different proper nouns directly, even when theproper nouns contain differing numbers of words.To generate a ranking of UR using the learnedHMM parameters, we rank the arguments ei accord-ing to how similar their state distributions P (t|ei)2Our implementation makes the simplifying assumption thateach sentence in the corpus is generated independently.are to those of the seed arguments.3 Specifically, wedefine a function:f(e) =?ei?eKL(?w?
?SRiP (t|w?
)|SRi|, P (t|ei)) (2)where KL represents KL divergence, and the outersum is taken over the arguments ei of the extractione.
We rank the elements of UR in ascending order off(e).HMM-T has two advantages over a more tradi-tional type checking approach of simply countingthe number of times in the corpus that each extrac-tion appears in a context in which a seed also ap-pears (cf.
(Ravichandran et al, 2005)).
The firstadvantage of HMM-T is efficiency, as the traditionalapproach involves a computationally expensive stepof retrieving the potentially large set of contexts inwhich the extractions and seeds appear.
In our ex-periments, using HMM-T instead of a context-basedapproach results in a 10-50x reduction in the amountof data that is retrieved to perform type checking.Secondly, on sparse data HMM-T has the poten-tial to improve type checking accuracy.
For exam-ple, consider comparing Pickerington, a sparsecandidate argument of the type City, to the seedargument Chicago, for which the following twophrases appear in the corpus:(i) ?Pickerington, Ohio?
(ii) ?Chicago, Illinois?In these phrases, the textual contexts surroundingChicago and Pickerington are not identical,so to the traditional approach these contexts offerno evidence that Pickerington and Chicagoare of the same type.
For a sparse token likePickerington, this is problematic because thetoken may never occur in a context that preciselymatches that of a seed.
In contrast, in the HMM, thenon-sparse tokens Ohio and Illinois are likelyto have similar state distributions, as they are boththe names of U.S. States.
Thus, in the state spaceemployed by the HMM, the contexts in phrases (i)and (ii) are in fact quite similar, allowing HMM-T to detect that Pickerington and Chicagoare likely of the same type.
Our experiments quan-tify the performance improvements that HMM-T of-3The distribution P (t|ei) for any ei can be obtained fromthe HMM parameters using Bayes Rule.699fers over the traditional approach for type checkingsparse data.The time required to learn HMM-T?s parametersscales proportional to T k+1 times the corpus size.Thus, for tractability, HMM-T uses a relatively smallstate space of T = 20 states and a limited k valueof 3.
While these settings are sufficient for typechecking (e.g., determining that Santa Clara isa city) they are too coarse-grained to assess relationsbetween arguments (e.g., determining that SantaClara is the particular city in which Intel isheadquartered).
We now turn to the REL-GRAMScomponent, which performs the latter task.2.3 Relation Assessment with REL-GRAMSREALM?s relation assessment component, calledREL-GRAMS, tests whether the extracted argumentshave a desired relationship, but given REALM?s min-imal input it has no a priori information about therelationship.
REL-GRAMS relies instead on the dis-tributional hypothesis to test each extraction.As argued in Section 2.1, it is intractable to buildan accurate language model for context distributionssurrounding sparse argument pairs.
To overcomethis problem, we introduce relational n-gram mod-els.
Rather than simply modeling the context distri-bution around a given argument, a relational n-grammodel specifies separate context distributions for anarguments conditioned on each of the other argu-ments with which it appears.
The relational n-grammodel allows us to estimate context distributions forpairs of arguments, even when the arguments do notappear together within a fixed window of n words.Further, by considering only consecutive argumentpairs, the number of distinct argument pairs in themodel grows at most linearly with the number ofsentences in the corpus.
Thus, the relational n-grammodel can scale.Formally, for a pair of arguments (e1, e2), a re-lational n-gram model estimates the distributionsP (w1, ..., wn|wi = e1, e1 ?
e2) for each 1 ?
i ?n, where the notation e1 ?
e2 indicates the eventthat e2 is the next argument to either the right or theleft of e1 in the corpus.REL-GRAMS begins by building a relational n-gram model of the arguments in the corpus.
Fornotational convenience, we represent the model?sdistributions in terms of ?context vectors?
for eachpair of arguments.
Formally, for a given sentencecontaining arguments e1 and e2 consecutively, wedefine a context of the ordered pair (e1, e2) to beany window of n tokens around e1.
Let C ={c1, c2, ..., c|C|} be the set of all contexts of all ar-gument pairs found in the corpus.4 For a pair of ar-guments (ej , ek), we model their relationship usinga |C| dimensional context vector v(ej ,ek), whose i-thdimension corresponds to the number of times con-text ci occurred with the pair (ej , ek) in the corpus.These context vectors are similar to document vec-tors from Information Retrieval (IR), and we lever-age IR research to compare them, as described be-low.To assess each extraction, we determine how sim-ilar its context vector is to a canonical seed vec-tor (created by summing the context vectors of theseeds).
While there are many potential methodsfor determining similarity, in this work we rank ex-tractions by decreasing values of the BM25 dis-tance metric.
BM25 is a TF-IDF variant intro-duced in TREC-3(Robertson et al, 1992), whichoutperformed both the standard cosine distance anda smoothed KL divergence on our data.3 Experimental ResultsThis section describes our experiments on IE assess-ment for sparse data.
We start by describing ourexperimental methodology, and then present our re-sults.
The first experiment tests the hypothesis thatHMM-T outperforms an n-gram-based method onthe task of type checking.
The second experimenttests the hypothesis that REALM outperforms multi-ple approaches from previous work, and also outper-forms each of its HMM-T and REL-GRAMS compo-nents taken in isolation.3.1 Experimental MethodologyThe corpus used for our experiments consisted of asample of sentences taken from Web pages.
Froman initial crawl of nine million Web pages, we se-lected sentences containing relations between propernouns.
The resulting text corpus consisted of about4Pre-computing the set C requires identifying in advancethe potential relation arguments in the corpus.
We consider theproper nouns identified by the LEX method (see Section 2.1) tobe the potential arguments.700three million sentences, and was tokenized as de-scribed in Section 2.
For tractability, before and afterperforming tokenization, we replaced each token oc-curring fewer than five times in the corpus with oneof two ?unknown word?
markers (one for capital-ized words, and one for uncapitalized words).
Thispreprocessing resulted in a corpus containing aboutsixty-five million total tokens, and 214,787 uniquetokens.We evaluated performance on four relations:Conquered, Founded, Headquartered, andMerged.
These four relations were chosen becausethey typically take proper nouns as arguments, andincluded a large number of sparse extractions.
Foreach relationR, the candidate extraction listER wasobtained using TEXTRUNNER (Banko et al, 2007).TEXTRUNNER is an IE system that computes an in-dex of all extracted relationships it recognizes, in theform of (object, predicate, object) triples.
For eachof our target relations, we executed a single queryto the TEXTRUNNER index for extractions whosepredicate contained a phrase indicative of the rela-tion (e.g., ?founded by?, ?headquartered in?
), andthe results formed our extraction list.
For each rela-tion, the 10 most frequent extractions served as boot-strapped seeds.
All of the non-seed extractions weresparse (no argument pairs were extracted more thantwice for a given relation).
These test sets containeda total of 361 extractions.3.2 Type Checking ExperimentsAs discussed in Section 2.2, on sparse data HMM-Thas the potential to outperform type checking meth-ods that rely on textual similarities of context vec-tors.
To evaluate this claim, we tested the HMM-Tsystem against an N-GRAMS type checking methodon the task of type-checking the arguments to a re-lation.
The N-GRAMS method compares the contextvectors of extractions in the same way as the REL-GRAMS method described in Section 2.3, but is notrelational (N-GRAMS considers the distribution ofeach extraction argument independently, similar toHMM-T).
We tagged an extraction as type correct iffboth arguments were valid for the relation, ignoringwhether the relation held between the arguments.The results of our type checking experiments areshown in Table 1.
For all types, HMM-T outper-forms N-GRAMS, and HMM-T reduces error (mea-Type HMM-T N-GRAMSConquered 0.917 0.767Founded 0.827 0.636Headquartered 0.734 0.589Merged 0.920 0.854Average 0.849 0.712Table 1: Type Checking Performance.
Listed is areaunder the precision/recall curve.
HMM-T outper-forms N-GRAMS for all relations, and reduces theerror in terms of missing area under the curve by46% on average.sured in missing area under the precision/recallcurve) by 46%.
The performance difference on eachrelation is statistically significant (p < 0.01, two-sampled t-test), using the methodology for measur-ing the standard deviation of area under the preci-sion/recall curve given in (Richardson and Domin-gos, 2006).
N-GRAMS, like REL-GRAMS, employsthe BM-25 metric to measure distributional similar-ity between extractions and seeds.
Replacing BM-25 with cosine distance cuts HMM-T?s advantageover N-GRAMS, but HMM-T?s error rate is still 23%lower on average.3.3 Experiments with REALMThe REALM system combines the type checkingand relation assessment components to assess ex-tractions.
Here, we test the ability of REALM toimprove the ranking of a state of the art IE system,TEXTRUNNER.
For these experiments, we evalu-ate REALM against the TEXTRUNNER frequency-based ordering, a pattern-learning approach, and theHMM-T and REL-GRAMS components taken in iso-lation.
The TEXTRUNNER frequency-based order-ing ranks extractions in decreasing order of their ex-traction frequency, and importantly, for our task thisordering is essentially equivalent to that produced bythe ?Urns?
(Downey et al, 2005) and Pointwise Mu-tual Information (Etzioni et al, 2005) approachesemployed in previous work.The pattern-learning approach, denoted as PL, ismodeled after Snowball (Agichtein, 2006).
The al-gorithm and parameter settings for PL were thosemanually tuned for the Headquartered relationin previous work (Agichtein, 2005).
A sensitivityanalysis of these parameters indicated that the re-701Conquered Founded Headquartered Merged AverageAvg.
Prec.
0.698 0.578 0.400 0.742 0.605TEXTRUNNER 0.738 0.699 0.710 0.784 0.733PL 0.885 0.633 0.651 0.852 0.785PL+ HMM-T 0.883 0.722 0.727 0.900 0.808HMM-T 0.830 0.776 0.678 0.864 0.787REL-GRAMS 0.929 (39%) 0.713 0.758 0.886 0.822REALM 0.907 (19%) 0.781 (27%) 0.810 (35%) 0.908 (38%) 0.851 (39%)Table 2: Performance of REALM for assessment of sparse extractions.
Listed is area under the preci-sion/recall curve for each method.
In parentheses is the percentage reduction in error over the strongestbaseline method (TEXTRUNNER or PL) for each relation.
?Avg.
Prec.?
denotes the fraction of correctexamples in the test set for each relation.
REALM outperforms its REL-GRAMS and HMM-T componentstaken in isolation, as well as the TEXTRUNNER and PL systems from previous work.sults are sensitive to the parameter settings.
How-ever, we found no parameter settings that performedsignificantly better, and many settings performedsignificantly worse.
As such, we believe our re-sults reasonably reflect the performance of a patternlearning system on this task.
Because PL performsrelation assessment, we also attempted combiningPL with HMM-T in a hybrid method (PL+ HMM-T)analogous to REALM.The results of these experiments are shown in Ta-ble 2.
REALM outperforms the TEXTRUNNER andPL baselines for all relations, and reduces the miss-ing area under the curve by an average of 39% rel-ative to the strongest baseline.
The performancedifferences between REALM and TEXTRUNNER arestatistically significant for all relations, as are differ-ences between REALM and PL for all relations ex-cept Conquered (p < 0.01, two-sampled t-test).The hybrid REALM system also outperforms eachof its components in isolation.4 Related WorkTo our knowledge, REALM is the first system to uselanguage modeling techniques for IE Assessment.Redundancy-based approaches to pattern-basedIE assessment (Downey et al, 2005; Etzioni et al,2005) require that extractions appear relatively fre-quently with a limited set of patterns.
In contrast,REALM utilizes all contexts to build a model of ex-tractions, rather than a limited set of patterns.
Ourexperiments demonstrate that REALM outperformsthese approaches on sparse data.Type checking using named-entity taggers hasbeen previously shown to improve the precision ofpattern-based IE systems (Agichtein, 2005; Feld-man et al, 2006), but the HMM-T type-checkingcomponent we develop differs from this work in im-portant ways.
Named-entity taggers are limited inthat they typically recognize only small set of types(e.g., ORGANIZATION, LOCATION, PERSON),and they require hand-tagged training data for eachtype.
HMM-T, by contrast, performs type check-ing for any type.
Finally, HMM-T does not requirehand-tagged training data.Pattern learning is a common technique for ex-tracting and assessing sparse data (e.g.
(Agichtein,2005; Riloff and Jones, 1999; Pas?ca et al, 2006)).Our experiments demonstrate that REALM outper-forms a pattern learning system closely modeled af-ter (Agichtein, 2005).
REALM is inspired by pat-tern learning techniques (in particular, both use thedistributional hypothesis to assess sparse data) butis distinct in important ways.
Pattern learning tech-niques require substantial processing of the corpusafter the relations they assess have been specified.Because of this, pattern learning systems are un-suited to Open IE.
Unlike these techniques, REALMpre-computes language models which allow it to as-sess extractions for arbitrary relations at run-time.In essence, pattern-learning methods run in time lin-ear in the number of relations whereas REALM?s runtime is constant in the number of relations.
Thus,REALM scales readily to large numbers of relationswhereas pattern-learning methods do not.702A second distinction of REALM is that its typechecker, unlike the named entity taggers employedin pattern learning systems (e.g., Snowball), can beused to identify arbitrary types.
A final distinction isthat the language models REALM employs requirefewer parameters and heuristics than pattern learn-ing techniques.Similar distinctions exist between REALM and arecent system designed to assess sparse extractionsby bootstrapping a classifier for each target relation(Feldman et al, 2006).
As in pattern learning, con-structing the classifiers requires substantial process-ing after the target relations have been specified, anda set of hand-tagged examples per relation, makingit unsuitable for Open IE.5 ConclusionsThis paper demonstrated that unsupervised languagemodels, as embodied in the REALM system, are aneffective means of assessing sparse extractions.Another attractive feature of REALM is its scal-ability.
Scalability is a particularly important con-cern forOpen Information Extraction, the task of ex-tracting large numbers of relations that are not spec-ified in advance.
Because HMM-T and REL-GRAMSboth pre-compute language models, REALM can bequeried efficiently to perform IE Assessment.
Fur-ther, the language models are constructed indepen-dently of the target relations, allowing REALM toperform IE Assessment even when relations are notspecified in advance.In future work, we plan to develop a probabilisticmodel of the information computed by REALM.
Wealso plan to evaluate the use of non-local context forIE Assessment by integrating document-level mod-eling techniques (e.g., Latent Dirichlet Allocation).AcknowledgementsThis research was supported in part by NSF grantsIIS-0535284 and IIS-0312988, DARPA contractNBCHD030010, ONR grant N00014-05-1-0185 aswell as a gift from Google.
The first author is sup-ported by an MSR graduate fellowship sponsored byMicrosoft Live Labs.
We thank Michele Banko, JeffBilmes, Katrin Kirchhoff, and Alex Yates for helpfulcomments.ReferencesE.
Agichtein.
2005.
Extracting Relations From LargeText Collections.
Ph.D. thesis, Department of Com-puter Science, Columbia University.E.
Agichtein.
2006.
Confidence estimation methods forpartially supervised relation extraction.
In SDM 2006.M.
Banko, M. Cararella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfrom the web.
In Procs.
of IJCAI 2007.D.
Downey, O. Etzioni, and S. Soderland.
2005.
A Prob-abilistic Model of Redundancy in Information Extrac-tion.
In Procs.
of IJCAI 2005.D.
Downey, M. Broadhead, and O. Etzioni.
2007.
Locat-ing complex named entities in web text.
In Procs.
ofIJCAI 2007.O.
Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theweb: An experimental study.
Artificial Intelligence,165(1):91?134.R.
Feldman, B. Rosenfeld, S. Soderland, and O. Etzioni.2006.
Self-supervised relation extraction from theweb.
In ISMIS, pages 755?764.Z.
Harris.
1985.
Distributional structure.
In J. J. Katz,editor, The Philosophy of Linguistics, pages 26?47.New York: Oxford University Press.C.
D. Manning and H. Schu?tze.
1999.
Foundations ofStatistical Natural Language Processing.M.
Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.2006.
Names and similarities on the web: Fact extrac-tion in the fast lane.
In Procs.
of ACL/COLING 2006.L.
R. Rabiner.
1989.
A tutorial on hidden markov modelsand selected applications in speech recognition.
Pro-ceedings of the IEEE, 77(2):257?286.D.
Ravichandran, P. Pantel, and E. H. Hovy.
2005.
Ran-domized Algorithms and NLP: Using Locality Sensi-tive Hash Functions for High Speed Noun Clustering.In Procs.
of ACL 2005.M.
Richardson and P. Domingos.
2006.
Markov LogicNetworks.
Machine Learning, 62(1-2):107?136.E.
Riloff and R. Jones.
1999.
Learning Dictionaries forInformation Extraction by Multi-level Boot-strapping.In Procs.
of AAAI-99, pages 1044?1049.S.
E. Robertson, S. Walker, M. Hancock-Beaulieu,A.
Gull, and M. Lau.
1992.
Okapi at TREC-3.
InText REtrieval Conference, pages 21?30.703
