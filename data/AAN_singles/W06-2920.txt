Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 149?164, New York City, June 2006. c?2006 Association for Computational LinguisticsCoNLL-X shared task on Multilingual Dependency ParsingSabine BuchholzSpeech Technology GroupCambridge Research LabToshiba Research EuropeCambridge CB2 3NH, UKsabine.buchholz@crl.toshiba.co.ukErwin MarsiCommunication & CognitionTilburg University5000 LE Tilburg, The Netherlandse.c.marsi@uvt.nlAbstractEach year the Conference on Com-putational Natural Language Learning(CoNLL)1 features a shared task, in whichparticipants train and test their systems onexactly the same data sets, in order to bet-ter compare systems.
The tenth CoNLL(CoNLL-X) saw a shared task on Multi-lingual Dependency Parsing.
In this pa-per, we describe how treebanks for 13 lan-guages were converted into the same de-pendency format and how parsing perfor-mance was measured.
We also give anoverview of the parsing approaches thatparticipants took and the results that theyachieved.
Finally, we try to draw gen-eral conclusions about multi-lingual pars-ing: What makes a particular language,treebank or annotation scheme easier orharder to parse and which phenomena arechallenging for any dependency parser?AcknowledgementMany thanks to Amit Dubey and Yuval Kry-molowski, the other two organizers of the sharedtask, for discussions, converting treebanks, writingsoftware and helping with the papers.21see http://ilps.science.uva.nl/?erikt/signll/conll/2Thanks also to Alexander Yeh for additional help with thepaper reviews.
His work was made possible by the MITRE Cor-poration?s Sponsored Research Program.1 IntroductionPrevious CoNLL shared tasks focused on NP chunk-ing (1999), general chunking (2000), clause iden-tification (2001), named entity recognition (2002,2003), and semantic role labeling (2004, 2005).
Thisshared task on full (dependency) parsing is the log-ical next step.
Parsing is an important preprocess-ing step for many NLP applications and thereforeof considerable practical interest.
It is a complextask and as it is not straightforwardly mappable to a?classical?
segmentation, classification or sequenceprediction problem, it also poses theoretical chal-lenges to machine learning researchers.During the last decade, much research has beendone on data-driven parsing and performance has in-creased steadily.
For training these parsers, syntac-tically annotated corpora (treebanks) of thousandsto tens of thousands of sentences are necessary; soinitially, research has focused on English.
Dur-ing the last few years, however, treebanks for otherlanguages have become available and some parsershave been applied to several different languages.See Section 2 for a more detailed overview of re-lated previous research.So far, there has not been much comparison be-tween different dependency parsers on exactly thesame data sets (other than for English).
One of thereasons is the lack of a de-facto standard for an eval-uation metric (labeled or unlabeled, separate root ac-curacy?
), for splitting the data into training and test-ing portions and, in the case of constituency tree-banks converted to dependency format, for this con-version.
Another reason are the various annotation149schemes and logical data formats used by differenttreebanks, which make it tedious to apply a parser tomany treebanks.
We hope that this shared task willimprove the situation by introducing a uniform ap-proach to dependency parsing.
See Section 3 for thedetailed task definition and Section 4 for informationabout the conversion of all 13 treebanks.In this shared task, participants had two to threemonths3 to implement a parsing system that could betrained for all these languages and four days to parseunseen test data for each.
19 participant groups sub-mitted parsed test data.
Of these, all but one parsedall 12 required languages and 13 also parsed the op-tional Bulgarian data.
A wide variety of parsingapproaches were used: some are extensions of pre-viously published approaches, others are new.
SeeSection 5 for an overview.Systems were scored by computing the labeledattachment score (LAS), i.e.
the percentage of?scoring?
tokens for which the system had predictedthe correct head and dependency label.
Punctuationtokens were excluded from scoring.
Results acrosslanguages and systems varied widely from 37.8%(worst score on Turkish) to 91.7% (best score onJapanese).
See Section 6 for detailed results.However, variations are consistent enough to al-low us to draw some general conclusions.
Section 7discusses the implications of the results and analyzesthe remaining problems.
Finally, Section 8 describespossible directions for future research.2 Previous researchTesnie`re (1959) introduced the idea of a dependencytree (a ?stemma?
in his terminology), in whichwords stand in direct head-dependent relations, forrepresenting the syntactic structure of a sentence.Hays (1964) and Gaifman (1965) studied the for-mal properties of projective dependency grammars,i.e.
those where dependency links are not allowed tocross.
Mel?c?uk (1988) describes a multistratal de-pendency grammar, i.e.
one that distinguishes be-tween several types of dependency relations (mor-phological, syntactic and semantic).
Other theoriesrelated to dependency grammar are word grammar3Some though had significantly less time: One participantregistered as late as six days before the test data release (reg-istration was a prerequisite to obtain most of the data sets) andstill went on to submit parsed test data in time.
(Hudson, 1984) and link grammar (Sleator and Tem-perley, 1993).Some relatively recent rule-based full depen-dency parsers are Kurohashi and Nagao (1994) forJapanese, Oflazer (1999) for Turkish, Tapanainenand Ja?rvinen (1997) for English and Elworthy(2000) for English and Japanese.While phrase structure parsers are usually evalu-ated with the GEIG/PARSEVAL measures of preci-sion and recall over constituents (Black et al, 1991),Lin (1995) and others have argued for an alterna-tive, dependency-based evaluation.
That approach isbased on a conversion from constituent structure todependency structure by recursively defining a headfor each constituent.The same idea was used by Magerman (1995),who developed the first ?head table?
for the PennTreebank (Marcus et al, 1994), and Collins (1996),whose constituent parser is internally based on prob-abilities of bilexical dependencies, i.e.
dependenciesbetween two words.
Collins (1997)?s parser andits reimplementation and extension by Bikel (2002)have by now been applied to a variety of languages:English (Collins, 1999), Czech (Collins et al, 1999),German (Dubey and Keller, 2003), Spanish (Cowanand Collins, 2005), French (Arun and Keller, 2005),Chinese (Bikel, 2002) and, according to Dan Bikel?sweb page, Arabic.Eisner (1996) introduced a data-driven depen-dency parser and compared several probability mod-els on (English) Penn Treebank data.
Kudo andMatsumoto (2000) describe a dependency parser forJapanese and Yamada and Matsumoto (2003) an ex-tension for English.
Nivre?s parser has been testedfor Swedish (Nivre et al, 2004), English (Nivre andScholz, 2004), Czech (Nivre and Nilsson, 2005),Bulgarian (Marinov and Nivre, 2005) and ChineseCheng et al (2005), while McDonald?s parser hasbeen applied to English (McDonald et al, 2005a),Czech (McDonald et al, 2005b) and, very recently,Danish (McDonald and Pereira, 2006).3 Data format, task definitionThe training data derived from the original treebanks(see Section 4) and given to the shared task partic-ipants was in a simple column-based format that is150an extension of Joakim Nivre?s Malt-TAB format4for the shared task and was chosen for its processingsimplicity.
All the sentences are in one text file andthey are separated by a blank line after each sen-tence.
A sentence consists of one or more tokens.Each token is represented on one line, consisting of10 fields.
Fields are separated from each other by aTAB.5 The 10 fields are:1) ID: Token counter, starting at 1 for each newsentence.2) FORM: Word form or punctuation symbol.For the Arabic data only, FORM is a concatenationof the word in Arabic script and its transliteration inLatin script, separated by an underscore.
This rep-resentation is meant to suit both those that do andthose that do not read Arabic.3) LEMMA: Lemma or stem (depending on theparticular treebank) of word form, or an underscoreif not available.
Like for the FORM, the values forArabic are concatenations of two scripts.4) CPOSTAG: Coarse-grained part-of-speechtag, where the tagset depends on the treebank.5) POSTAG: Fine-grained part-of-speech tag,where the tagset depends on the treebank.
It is iden-tical to the CPOSTAG value if no POSTAG is avail-able from the original treebank.6) FEATS: Unordered set of syntactic and/ormorphological features (depending on the particu-lar treebank), or an underscore if not available.
Setmembers are separated by a vertical bar (|).7) HEAD: Head of the current token, which iseither a value of ID, or zero (?0?)
if the token linksto the virtual root node of the sentence.
Note thatdepending on the original treebank annotation, theremay be multiple tokens with a HEAD value of zero.8) DEPREL: Dependency relation to the HEAD.The set of dependency relations depends on the par-ticular treebank.
The dependency relation of a to-ken with HEAD=0 may be meaningful or simply?ROOT?
(also depending on the treebank).9) PHEAD: Projective head of current token,which is either a value of ID or zero (?0?
), or an un-derscore if not available.
The dependency structure4http://w3.msi.vxu.se/ nivre/research/MaltXML.html5Consequently, field values cannot contain TABs.
In theshared task data, field values are also not supposed to con-tain any other whitespace (although unfortunately some spacesslipped through in the Spanish data).resulting from the PHEAD column is guaranteed tobe projective (but is not available for all data sets),whereas the structure resulting from the HEAD col-umn will be non-projective for some sentences ofsome languages (but is always available).10) PDEPREL: Dependency relation to thePHEAD, or an underscore if not available.As should be obvious from the description above,our format assumes that each token has exactly onehead.
Some dependency grammars, and also sometreebanks, allow tokens to have more than one head,although often there is a distinction between primaryand optional secondary relations, e.g.
in the DanishDependency Treebank (Kromann, 2003), the DutchAlpino Treebank (van der Beek et al, 2002b) andthe German TIGER treebank (Brants et al, 2002).For this shared task we decided to ignore any ad-ditional relations.
However the data format couldeasily be extended with additional optional columnsin the future.
Cycles do not occur in the shared taskdata but are scored as normal if predicted by parsers.The character encoding of all data files is Unicode(specifically UTF-8), which is the only encoding tocover all languages and therefore ideally suited formultilingual parsing.While the training data contained all 10 columns(although sometimes only with dummy values, i.e.underscores), the test data given to participants con-tained only the first 6.
Participants?
parsers thenpredicted the HEAD and DEPREL columns (anypredicted PHEAD and PDEPREL columns were ig-nored).
The predicted values were compared to thegold standard HEAD and DEPREL.6 The officialevaluation metric is the labeled attachment score(LAS), i.e.
the percentage of ?scoring?
tokens forwhich the system has predicted the correct HEADand DEPREL.
The evaluation script defines a non-scoring token as a token where all characters of theFORM value have the Unicode category property?Punctuation?.76The official scoring script eval.pl, data sets for somelanguages and instructions on how to get the rest, the softwareused for the treebank conversions, much documentation, fullresults and other related information will be available from thepermanent URL http://depparse.uvt.nl (also linkedfrom the CoNLL web page).7See man perlunicode for the technical details and theshared task website for our reasons for this decision.
Notethat an underscore and a percentage sign also have the Unicode?Punctuation?
property.151We tried to take a test set that was representativeof the genres in a treebank and did not cut throughtext samples.
We also tried to document how weselected this set.8 We aimed at having roughly thesame size for the test sets of all languages: 5,000scoring tokens.
This is not an exact requirement aswe do not want to cut sentences in half.
The rel-atively small size of the test set means that evenfor the smallest treebanks the majority of tokens isavailable for training, and the equal size means thatfor the overall ranking of participants, we can sim-ply compute the score on the concatenation of alltest sets.4 Treebanks and their conversionIn selecting the treebanks, practical considerationswere the major factor.
Treebanks had to be actuallyavailable, large enough, have a license that allowedfree use for research or kind treebank providers whotemporarily waived the fee for the shared task, andbe suitable for conversion into the common formatwithin the limited time.
In addition, we aimed at abroad coverage of different language families.9 Asa general rule, we did not manually correct errors intreebanks if we discovered some during the conver-sion, see also Buchholz and Green (2006), althoughwe did report them to the treebank providers andseveral got corrected by them.4.1 Dependency treebanksWe used the following six dependency treebanks:Czech: Prague Dependency Treebank10 (PDT)(Bo?hmova?
et al, 2003); Arabic: Prague Arabic De-pendency Treebank11 (PADT) (Hajic?
et al, 2004;Smrz?
et al, 2002); Slovene: Slovene DependencyTreebank12 (SDT) (Dz?eroski et al, 2006); Danish:8See the shared task website for a more detailed discussion.9That was also the reason why we decided not to includea fifth Germanic language (English) although the freely avail-able SUSANNE treebank (Sampson, 1995) or possibly the PennTreebank would have qualified otherwise.10Many thanks to Jan Hajic?
for granting the temporary li-cense for CoNLL-X and talking to LDC about it, to Christo-pher Cieri for arranging distribution through LDC and to TonyCastelletto for handling the distribution.11Many thanks to Yuval Krymolowski for converting the tree-bank, Otakar Smrz?
for valuable help during the conversion andthanks again to Jan Hajic?, Christopher Cieri and Tony Castel-letto.12Many thanks to the SDT people for granting the speciallicense for CoNLL-X and to Tomaz?
Erjavec for converting theDanish Dependency Treebank13 (Kromann, 2003);Swedish: Talbanken0514 (Teleman, 1974; Einars-son, 1976; Nilsson et al, 2005); Turkish: Metu-Sabanc?
treebank15 (Oflazer et al, 2003; Atalay etal., 2003).The conversion of these treebanks was the easi-est task as the linguistic representation was alreadywhat we needed, so the information only had to beconverted from SGML or XML to the shared taskformat.
Also, the relevant information had to be dis-tributed appropriately over the CPOSTAG, POSTAGand FEATS columns.For the Swedish data, no predefined distinctioninto coarse and fine-grained PoS was available, sothe two columns contain identical values in our for-mat.
For the Czech data, we sampled both our train-ing and test data from the official ?training?
partitionbecause only that one contains gold standard PoStags, which is also what is used in most other datasets.
The Czech DEPREL values include the suf-fixes to mark coordination, apposition and parenthe-sis, while these have been ignored during the con-version of the much smaller Slovene data.
For theArabic data, sentences with missing annotation werefiltered out during the conversion.The Turkish treebank posed a special problembecause it analyzes each word as a sequence ofone or more inflectional groups (IGs).
Each IGconsists of either a stem or a derivational suffixplus all the inflectional suffixes belonging to thatstem/derivational suffix.
The head of a whole wordis not just another word but a specific IG of anotherword.16 One can easily map this representation toone in which the head of a word is a word but thattreebank for us.13Many thanks to Matthias Trautner Kromann and assistantsfor creating the DDT and releasing it under the GNU GeneralPublic License and to Joakim Nivre, Johan Hall and Jens Nils-son for the conversion of DDT to Malt-XML.14Many thanks to Jens Nilsson, Johan Hall and Joakim Nivrefor the conversion of the original Talbanken to Talbanken05and for making it freely available for research purposes and toJoakim Nivre again for prompt and proper respons to all ourquestions.15Many thanks to Bilge Say and Kemal Oflazer for grant-ing the license for CoNLL-X and answering questions and toGu?ls?en Eryig?it for making many corrections to the treebank anddiscussing some aspects of the conversion.16This is a bit like saying that in ?the usefulness of X forY?, ?for Y?
links to ?use-?
and not to ?usefulness?.
Only thatin Turkish, ?use?, ?full?
and ?ness?
each could have their owninflectional suffixes attached to them.152mapping would lose information and it is not clearwhether the result is linguistically meaningful, prac-tically useful, or even easier to parse because in theoriginal representation, each IG has its own PoS andmorphological features, so it is not clear how that in-formation should be represented if all IGs of a wordare conflated.
We therefore chose to represent eachIG as a separate token in our format.
To make theresult a connected dependency structure, we definedthe HEAD of each non-word-final IG to be the fol-lowing IG and the DEPREL to be ?DERIV?.
We as-signed the stem of the word to the first IG?s LEMMAcolumn, with all non-first IGs having LEMMA ?
?,and the actual word form to the last IG, with all non-last IGs having FORM ?
?.
As already mentioned inSection 3, the underscore has the punctuation char-acter property, therefore non-last IGs (whose HEADand DEPREL were introduced by us) are not scoringtokens.
We also attached or reattached punctuation(see the README available at the shared task web-site for details.
)4.2 Phrase structure with functions for allconstituentsWe used the following five treebanks of this type:German: TIGER treebank17 (Brants et al, 2002);Japanese: Japanese Verbmobil treebank18 (Kawataand Bartels, 2000); Portuguese: The Bosque partof the Floresta sinta?
(c)tica19 (Afonso et al, 2002);Dutch: Alpino treebank20 (van der Beek et al,2002b; van der Beek et al, 2002a); Chinese: Sinica17Many thanks to the TIGER team for allowing us to use thetreebank for the shared task and to Amit Dubey for convertingthe treebank.18Many thanks to Yasuhiro Kawata, Julia Bartels and col-leagues from Tu?bingen University for the construction of theoriginal Verbmobil treebank for Japanese and to Sandra Ku?blerfor providing the data and granting the special license forCoNLL-X.19Many thanks to Diana Santos, Eckhard Bick and otherFloresta sint(c)tica project members for creating the treebankand making it publicly available, for answering many questionsabout the treebank (Diana and Eckhard), for correcting prob-lems and making new releases (Diana), and for sharing scriptsand explaining the head rules implemented in them (Eckhard).Thanks also to Jason Baldridge for useful discussions and toBen Wing for independently reporting problems which Dianathen fixed.20Many thanks to Gertjan van Noord and the other people atthe University of Groningen for creating the Alpino Treebankand releasing it for free, to Gertjan van Noord for answering allour questions and for providing extra test material and to Antalvan den Bosch for help with the memory-based tagger.treebank21 (Chen et al, 2003).Their conversion to dependency format requiredthe definition of a head table.
Fortunately, in con-trast to the Penn Treebank for which the head ta-ble is based on POS22 we could use the gram-matical functions annotated in these treebanks.Therefore, head rules are often of the form: thehead child of a VP/clause is the child with theHD/predicator/hd/Head function.
The DEPRELvalue for a token is the function of the biggest con-stituent of which this token is the lexical head.
If theconstituent comprising the complete sentence didnot have a function, we gave its lexical head tokenthe DEPREL ?ROOT?.For the Chinese treebank, most functions are notgrammatical functions (such as ?subject?, ?object?
)but semantic roles (such as ?agent?, ?theme?).
Forthe Portuguese treebank, the conversion was compli-cated by the fact that a detailed specification existedwhich tokens should be the head of which other to-kens, e.g.
the finite verb must be the head of thesubject and the complementzier but the main verbmust be the head of the complements and adjuncts.23Given that the Floresta sinta?
(c)tica does not use tra-ditional VP constituents but rather verbal chunks(consisting mainly of verbs), a simple Magerman-Collins-style head table was not sufficient to derivethe required dependency structure.
Instead we useda head table that defined several types of heads (syn-tactic, semantic) and a link table that specified whatlinked to which type of head.24Another problem existed with the Dutch tree-bank.
Its original PoS tag set is very coarse andthe PoS and the word stem information is not veryreliable.25 We therefore decided to retag the tree-bank automatically using the Memory-Based Tag-ger (MBT) (Daelemans et al, 1996) which uses avery fine-grained tag set.
However, this created aproblem with multiwords.
MBT does not have theconcept of multiwords and therefore tags all of their21Many thanks to Academia Sinica for granting the tempo-rary license for CoNLL-X, to Keh-Jiann Chen for answeringour questions and to Amit Dubey for converting the treebank.22containing rules such as: the head child of a VP is the left-most ?to?, or else the leftmost past tense verb, or else etc.23Eckhard Bick, p.c.24See the conversion script bosque2MALT.py and theREADME file at the shared task website for details.25http://www.let.rug.nl/vannoord/trees/Papers/diffs.pdf153components individually.
As Alpino does not pro-vide an internal structure for multiwords, we hadto treat multiwords as one token.
However, wethen lack a proper PoS for the multiword.
Aftermuch discussion, we decided to assign each multi-word the CPOSTAG ?MWU?
(multiword unit) anda POSTAG which is the concatenation of the PoSof all the components as predicted by MBT (sepa-rated by an underscore).
Likewise, the FEATS area concatenation of the morphological features of allcomponents.
This approach resulted in many dif-ferent POSTAG values for the training set and evenin unseen values in the test set.
It remains to betested whether our approach resulted in data sets bet-ter suited for parsing than the original.4.3 Phrase structure with some functionsWe used two treebanks of this type: Spanish:Cast3LB26 (Civit Torruella and Mart??
Anton?
?n,2002; Navarro et al, 2003; Civit et al, 2003); Bul-garian: BulTreeBank27 (Simov et al, 2002; Simovand Osenova, 2003; Simov et al, 2004; Osenova andSimov, 2004; Simov et al, 2005).Converting a phrase structure treebank with onlya few functions to a dependency format usually re-quires linguistic competence in the treebank?s lan-guage in order to create the head table and miss-ing function labels.
We are grateful to Chanev etal.
(2006) for converting the BulTreeBank to theshared task format and to Montserrat Civit for pro-viding us with a head table and a function mappingfor Cast3LB.284.4 Data set characteristicsTable 1 shows details of all data sets.
FollowingNivre and Nilsson (2005) we use the following def-inition: ?an arc (i, j) is projective iff all nodes oc-curring between i and j are dominated by i (wheredominates is the transitive closure of the arc rela-26Many thanks to Montserrat Civit and Toni Mart??
for allow-ing us to use Cast3LB for CoNLL-X and to Amit Dubey forconverting the treebank.27Many thanks to Kiril Simov and Petya Osenova for allow-ing us to use the BulTreeBank for CoNLL-X.28Although unfortunately, due to a bug, the function list wasnot used and the Spanish data in the shared task ended up withmany DEPREL values being simply ?
?.
By the time we dis-covered this, the test data release date was very close and wedecided not to release new bug-fixed training material that late.tion)?.295 ApproachesTable 2 tries to give an overview of the wide varietyof parsing approaches used by participants.
We referto the individual papers for details.
There are severaldimensions along which to classify approaches.5.1 Top-down, bottom-upPhrase structure parsers are often classified in termsof the parsing order: top-down, bottom-up or var-ious combinations.
For dependency parsing, thereseem to be two different interpretations of the term?bottom-up?.
Nivre and Scholz (2004) uses thisterm with reference to Yamada and Matsumoto(2003), whose parser has to find all children of atoken before it can attach that token to its head.We will refer to this as ?bottom-up-trees?.
An-other use of ?bottom-up?
is due to Eisner (1996),who introduced the notion of a ?span?.
A spanconsists of a potential dependency arc r betweentwo tokens i and j and all those dependency arcsthat would be spanned by r, i.e.
all arcs betweentokens k and l with i ?
k, l ?
j. Parsing inthis order means that the parser has to find all chil-dren and siblings on one side of a token before itcan attach that token to a head on the same side.This approach assumes projective dependency struc-tures.
Eisner called this approach simply ?bottom-up?, while Nivre, whose parser implicitly also fol-lows this order, called it ?top-down/bottom-up?
todistinguish it from the pure ?bottom-up(-trees)?
or-der of Yamada and Matsumoto (2003).
To avoidconfusion, we will refer to this order as ?bottom-up-spans?.5.2 Unlabeled parsing versus labelingGiven that the parser needs to predict the HEAD aswell as the DEPREL value, different approaches arepossible: predict the (probabilities of the) HEADsof all tokens first, or predict the (probabilities ofthe) DEPRELs of all tokens first, or predict theHEAD and DEPREL of one token before predict-ing these values for the next token.
Within thefirst approach, each dependency can be labeled in-dependently (Corston-Oliver and Aue, 2006) or a29Thanks to Joakim Nivre for explaining this.154Ar Ch Cz Da Du Ge Ja Po Sl Sp Sw Tu Bulang.
fam.
Sem.
Sin.
Sla.
Ger.
Ger.
Ger.
Jap.
Rom.
Sla.
Rom.
Ger.
Ura.
Sla.genres 1: ne 6 3 8+ 5+ 1: ne 1: di 1: ne 1: no 9 4+ 8 12annotation d c+f d d dc+f dc+f c+f dc+f d c(+f) dc+f/d d c+ttraining datatokens (k) 54 337 1249 94 195 700 151 207 29 89 191 58 190%non-scor.
8.8 a0.8 14.9 13.9 11.3 11.5 11.6 14.2 17.3 12.6 11.0 b33.1 14.4units (k) 1.5 57.0 72.7 5.2 13.3 39.2 17.0 9.1 1.5 3.3 11.0 5.0 12.8tokens/unit c37.2 d5.9 17.2 18.2 14.6 17.8 e8.9 22.8 18.7 27.0 17.3 11.5 14.8LEMMA f(+) ?
+ ?
+ ?
?
+ + + ?
+ ?CPOSTAGs 14 13+9 12 10 13 g52 20 15 11 15 37 14 11POSTAGs 19 h294+9 63 24 i302 52 77 21 28 38 37 30 53FEATS 19 ?
61 47 81 ?
4 146 51 33 ?
82 50DEPRELs 27 82 78 52 26 46 7 55 25 21 56 25 18D.s H.=0 15 1 14 1 1 1 1 6 6 1 1 1 1%HEAD=0 5.5 16.9 6.7 6.4 8.9 6.3 18.6 5.1 5.9 4.2 6.5 13.4 7.9%H.
preced.
82.9 24.8 50.9 75.0 46.5 50.9 8.9 60.3 47.2 60.8 52.8 6.2 62.9%H.
follow.
11.6 58.2 42.4 18.6 44.6 42.7 72.5 34.6 46.9 35.1 40.7 80.4 29.2H.=0/unit 1.9 1.0 1.0 1.0 1.2 1.0 1.5 1.0 j0.9 1.0 1.0 1.0 1.0%n.p.
arcs 0.4 0.0 1.9 1.0 5.4 2.3 k1.1 1.3 1.9 l0.1 1.0 1.5 0.4%n.p.
units 11.2 0.0 23.2 15.6 36.4 27.8 5.3 18.9 22.2 1.7 9.8 11.6 5.4test datascor.
tokens 4990 4970 5000 5010 4998 5008 5003 5009 5004 4991 5021 5021 5013%new form 17.3 9.3 5.2 18.1 20.7 6.5 0.96 11.6 22.0 14.7 18.0 41.4 14.5%new lem.
4.3 n/a 1.8 n/a 15.9 n/a n/a 7.8 9.9 9.7 n/a 13.2 n/aTable 1: Characteristics of the data sets for the 13 languages (abbreviated by their first two letters): language family (Semitic,Sino-Tibetan, Slavic, Germanic, Japonic (or language isolate), Romance, Ural-Altaic); number of genres, and genre if only one(news, dialogue, novel); type of annotation (d=dependency, c=constituents, dc=discontinuous constituents, +f=with functions,+t=with types).
For the training data: number of tokens (times 1000); percentage of non-scoring tokens; number of parse tree units(usually sentences, times 1000); average number of (scoring and non-scoring) tokens per parse tree unit; whether a lemma or stemis available; how many different CPOSTAG values, POSTAG values, FEATS components and DEPREL values occur for scoringtokens; how many different values for DEPREL scoring tokens with HEAD=0 can have (if that number is 1, there is one designatedlabel (e.g.
?ROOT?)
for tokens with HEAD=0); percentage of scoring tokens with HEAD=0, a head that precedes or a head thatfollows the token (this nicely shows which languages are predominantly head-initial or head-final); the average number of scoringtokens with HEAD=0 per parse tree unit; the percentage of (scoring and non-scoring) non-projective relations and of parse treeunits with at least one non-projective relation.
For the test data: number of scoring tokens; percentage of scoring tokens with aFORM or a LEMMA that does not occur in the training data.afinal punctuation was deliberately left out during the conversion (as it is explicitly excluded from the tree structure)bthe non-last IGs of a word are non-scoring, see Section 4.1cin many cases the parse tree unit in PADT is not a sentence but a paragraphdin many cases the unit in Sinica is not a sentence but a comma-separated clause or phraseethe treebank consists of transcribed dialogues, in which some sentences are very short, e.g.
just ?Hai.?
(?Yes.?
)fonly part of the Arabic data has non-underscore values for the LEMMA columngno mapping from fine-grained to coarse-grained tags was available; same for Swedishh9 values are typos; POSTAGs also encode subcategorization information for verbs and some semantic information for con-junctions and nouns; some values also include parts in square brackets which in hindsight should maybe have gone to FEATSidue to treatment of multiwordsjprobably due to some sentences consisting only of non-scoring tokens, i.e.
punctuationkthese are all disfluencies, which are attached to the virtual root nodelfrom co-indexed items in the original treebank; same for Bulgarian155algorithm ver.
hor.
search lab.
non-proj learner pre post optall pairsMcD MST/Eisner b-s irr.
opt/approx.
2nd + a MIRA ?
?
?Cor MST/Eisner b-s irr.
optimal 2nd ?
BPMb+ME [SVM] + c ?
?Shi MST/CLE irr.
irr.
optimal 1st +, CLE MIRA ?
?
?Can own algorithm irr.
irr.
approx.(?)
int.
+ d TiMBL ?
?
+Rie ILP irr.
irr.
increment.
int.
+ e MIRA ?
?
+Bic CG-inspired mpf mpf backtrack(?)
int.
+ f MLE(?)
+ g + h ?stepwiseDre hagi/Eisner/rerank b-s irr.
best 1st exh 2nd ?
MLE ?
?
+ jLiu own algorithm b-t mpf det./local int.
?
MLE ?
?
?Car Eisner b-s irr.
approx.
int.
?
perceptron ?
?
?stepwise: classifier-basedAtt Y&M b-t for.
determin.
int.
+ k ME [MBL,SVM,...] stem ?
?Cha Y&M b-t for.
local 2nd ?
l perceptron (SNoW) proj ?
?Yur own algorithm b-s irr.
determin.
int.
?
decision list (GPA)m ?
?
?Che chunker+Nivre b-s for.
determin.
int.n ?
SVM + ME [CRF] ?
?
?Niv Nivre b-s for.
determin.
int.
+, ps-pr SVM proj deproj +Joh Nivre+MST/CLE b-s f+bo N-best int.p +, CLE SVM (LIBSVM) ?
?Wu Nivre+root parser b-s f/bq det.[+exh.]
int.
?
[+] SVM (SVMLight) ?
[+] r ?otherSch PCFG/CKY b-t irr.
opt.
int.
+, traces MLE [ME] d2c c2d ?Table 2: Overview of parsing approaches taken by participating groups (identified by the first three lettersof the first author): algorithm (Y&M: Yamada and Matsumoto (2003), ILP: Integer Linear Programming),vertical direction (irrelevant, mpf: most probable first, bottom-up-spans, bottom-up-trees), horizontal direc-tion (irrelevant, mpf: most probable first, forward, backward), search (optimal, approximate, incremental,best-first exhaustive, deterministic), labeling (interleaved, separate and 1st step, separate and 2nd step),non-projective (ps-pr: through pseudo-projective approach), learner (ME: Maximum Entropy; learners inbrackets were explored but not used in the official submission), preprocessing (projectivize, d2c: dependen-cies to constituents), postprocessing (deprojectivize, c2d: constituents to dependencies), learner parameteroptimization per languageanon-projectivity through approximate search, used for some languagesb20 averaged perceptrons combined into a Bayes Point Machinecintroduced a single POS tag ?aux?
for all Swedish auxiliary and model verbsdby having no projectivity constrainteselective projectivity constraint for Japanesefseveral approaches to non-projectivitygusing some FEATS components to create some finer-grained POSTAG valueshreattachment rules for some types of non-projectivityihead automaton grammarjdetermined the maximally allowed distance for relationskthrough special parser actionslpseudo-projectivizing training data onlymGreedy Prepend Algorithmnbut two separate learners used for unlabeled parsing versus labelingoboth foward and backward, then combined into a single tree with CLEpbut two separate SVMs used for unlabeled parsing versus labelingqforward parsing for Japanese and Turkish, backward for the restrattaching remaining unattached tokens through exhaustive search (not for submitted runs)156sequence classifier can label all children of a tokentogether (McDonald et al, 2006).
Within the thirdapproach, HEAD and DEPREL can be predicted si-multaneously, or in two separate steps (potentiallyusing two different learners).5.3 All pairsAt the highest level of abstraction, there are two fun-damental approaches, which we will call ?all pairs?and ?stepwise?.
In an ?all pairs?
approach, everypossible pair of two tokens in a sentence is consid-ered and some score is assigned to the possibilityof this pair having a (directed) dependency relation.Using that information as building blocks, the parserthen searches for the best parse for the sentence.This approach is one of those described in Eisner(1996).
The definition of ?best?
parse depends onthe precise model used.
That model can be one thatdefines the score of a complete dependency tree asthe sum of the scores of all dependency arcs in it.The search for the best parse can then be formalizedas the search for the maximum spanning tree (MST)(McDonald et al, 2005b).
If the parse has to be pro-jective, Eisner?s bottom-up-span algorithm (Eisner,1996) can be used for the search.
For non-projectiveparses, McDonald et al (2005b) propose using theChu-Liu-Edmonds (CLE) algorithm (Chu and Liu,1965; Edmonds, 1967) and McDonald and Pereira(2006) describe an approximate extension of Eis-ner?s algorithm.
There are also alternatives to MSTwhich allow imposing additional constraints on thedependency structure, e.g.
that at most one depen-dent of a token can have a certain label, such as ?sub-ject?, see Riedel et al (2006) and Bick (2006).
Bycontrast, Canisius et al (2006) do not even enforcethe tree constraint, i.e.
they allow cycles.
In a vari-ant of the ?all pairs?
approach, only those pairs oftokens are considered that are not too distant (Cani-sius et al, 2006).5.4 StepwiseIn a stepwise approach, not all pairs are considered.Instead, the dependency tree is built stepwise andthe decision about what step to take next (e.g.
whichdependency to insert) can be based on informationabout, in theory all, previous steps and their results(in the context of generative probabilistic parsing,Black et al (1993) call this the history).
Stepwiseapproaches can use an explicit probability modelover next steps, e.g.
a generative one (Eisner, 1996;Dreyer et al, 2006), or train a machine learner topredict those.
The approach can be deterministic (ateach point, one step is chosen) or employ varioustypes of search.
In addition, parsing can be done ina bottom-up-constituent or a bottom-up-spans fash-ion (or in another way, although this was not done inthis shared task).
Finally, parsing can start at the firstor the last token of a sentence.
When talking aboutlanguages that are written from left to right, this dis-tinction is normally referred to as left-to-right ver-sus right-to-left.
However, for multilingual parsingwhich includes languages that are written from rightto left (Arabic) or sometimes top to bottom (Chi-nese, Japanese) this terminology is confusing be-cause it is not always clear whether a left-to-rightparser for Arabic would really start with the left-most (i.e.
last) token of a sentence or, like for otherlanguages, with the first (i.e.
rightmost).
In general,starting with the first token (?forward?)
makes moresense from a psycholinguistic point of view but start-ing with the last (?backward?)
might be beneficialfor some languages (possibly related to them beinghead-initial versus head-final languages).
The pars-ing order directly determines what information willbe available from the history when the next decisionneeds to be made.
Stepwise parsers tend to inter-leave the prediction of HEAD and DEPREL.5.5 Non-projectivityAll data sets except the Chinese one contain somenon-projective dependency arcs, although their pro-portion varies from 0.1% to 5.4%.
Participants tookthe following approaches to non-projectivity:?
Ignore, i.e.
predict only projective parses.
De-pending on the way the parser is trained, itmight be necessary to at least projectivize thetraining data (Chang et al, 2006).?
Always allow non-projective arcs, by not im-posing any projectivity constraint (Shimizu,2006; Canisius et al, 2006).?
Allow during parsing under certain conditions,e.g.
for tokens with certain properties (Riedelet al, 2006; Bick, 2006) or if no alternativeprojective arc has a score above the threshold157(Bick, 2006) or if the classifier chooses a spe-cial action (Attardi, 2006) or the parser predictsa trace (Schiehlen and Spranger, 2006).?
Introduce through post-processing, e.g.through reattachment rules (Bick, 2006) orif the change increases overall parse treeprobability (McDonald et al, 2006).?
The pseudo-projective approach (Nivre andNilsson, 2005): Transform non-projectivetraining trees to projective ones but encodethe information necessary to make the inversetransformation in the DEPREL, so that this in-verse transformation can also be carried out onthe test trees (Nivre et al, 2006).5.6 Data columns usedTable 3 shows which column values havebeen used by participants.
Nobody used thePHEAD/PDEPREL column in any way.
It is likelythat those who did not use any of the other columnsdid so mainly for practical reasons, such as thelimited time and/or the difficulty to integrate it intoan existing parser.5.6.1 FORM versus LEMMALemma or stem information has often been ig-nored in previous dependency parsers.
In the sharedtask data, it was available in just over half the datasets.
Both LEMMA and FORM encode lexical in-formation.
There is therefore a certain redundancy.Participants have used these two columns in differ-ent ways:?
Use only one (see Table 3).?
Use both, in different features.
Typically, a fea-ture selection routine and/or the learner itself(through weights) will decide about the impor-tance of the resulting features.?
Use a variant of the FORM as a substitute fora missing LEMMA.
Bick (2006) used the low-ercased FORM if the LEMMA is not available,Corston-Oliver and Aue (2006) a prefix and At-tardi (2006) a stem derived by a rule-based sys-tem for Danish, German and Swedish.form lem.
cpos pos featsMcD ++ a + b ??
+ +, co+cr.pr.Cor + + + c ++ +, co+cr.pr.dShi + ?
+ ?
?Can + ?
?
+ ?Rie + e + + + f + cr.pr.Bic (+) + + g + (+)Dre ++ h + rer.
rer.
?Liu (+) + ++ + ?Car ++ + ++ + + comp.Att (+) + + ?
(+)Cha ?
+ ?
+ + atomicYur + + + + + comp.Che + + + + + atomic?Niv + + + + + comp.Joh + ?
+ + + comp.Wu + ?
+ + ?Sch ?
(+)i ?
(+) (+)Table 3: Overview of data columns used by partici-pating groups.
???
: a column value was not used atall.
?+?
: used in at least some features.
?(+)?
: Vari-ant of FORM used only if LEMMA is missing, oronly parts of FEATS used.
?++?
: used more exten-sively than another column containing related infor-mation (where FORM and LEMMA are related, asare CPOSTAG and POSTAG), e.g.
also in combina-tion features or features for context tokens in addi-tion to features for the focus token(s).
?rer.?
: usedin the reranker only.
For the last column: atomic,comp.
= components, cr.pr.
= cross-product.aalso prefix and suffix for labelerbinstead of form for Arabic and Spanishcinstead of POSTAG for Dutch and Turkishdfor labeler; unlab.
parsing: only some for global featuresealso prefixfalso 1st character of POSTAGgonly as backoffhreranker: also suffix; if no lemma, use prefix of FORMiLEMMA, POSTAG, FEATS only for back-off smoothing5.6.2 CPOSTAG versus POSTAGAll data sets except German and Swedish had dif-ferent values for CPOSTAG and POSTAG, althoughthe granularity varied widely.
Again, there are dif-ferent approaches to dealing with the redundancy:?
Use only one for all languages.158?
Use both, in different features.
Typically, a fea-ture selection routine and/or the learner itself(through weights) will decide about the impor-tance of the resulting features.?
Use one or the other for each language.5.6.3 Using FEATSBy design, a FEATS column value has internalstructure.
Splitting it at the ?|?30 results in a set ofcomponents.
The following approaches have beenused:?
Ignore the FEATS.?
Treat the complete FEATS value as atomic, i.e.do not split it into components.?
Use only some components, e.g.
Bick (2006)uses only case, mood and pronoun subclass andAttardi (2006) uses only gender, number, per-son and case.?
Use one binary feature for each component.This is likely to be useful if grammatical func-tion is indicated by case.?
Use one binary feature for each cross-productof the FEATS components of i and the FEATScomponents of j.
This is likely to be useful foragreement phenomena.?
Use one binary feature for each FEATS com-ponent of i that also exists for j.
This is a moreexplicit way to model agreement.5.7 Types of featuresWhen deciding whether there should be a depen-dency relation between tokens i and j, all parsersuse at least information about these two tokens.
Inaddition, the following sources of information canbe used (see Table 4): token context (tc): a limitednumber (determined by the window size) of tokensdirectly preceding or following i or j; children: in-formation about the already found children of i andj; siblings: in a set-up where the decision is not ?isthere a relation between i and j?
but ?is i the head ofj?
or in a separate labeling step, the siblings of i arethe already found children of j; structural context30or for Dutch, also at the ?
?tc ch si sc di in gl co ac la opMcD + l + l ?
l l + ?
l (+)aCor + l b l + p ?
+ + ?
?
(+)cShi + ?
?
?
+ ?
?
+ ?
+ ?Can + ?
?
?
+ ?
?
?
?
?
?Rie + ?
+ d ?
?
?
?
+ ?
+ e +Bic + + f + g ?
+ + h ?
+ ?
++ (+)iDre r r + r + r ?
+ ?
r rLiu ?
+ ?
+ + ?
?
+ ?
?
?Car + ?
+ ?
+ + ?
+ ?
+ ?Att ?
+ + + ?
?
?
?
+ + (+)jCha + + ?
l ?
?
?
+ + ?
?Yur + + ?
?
?
?
?
?
?
?
+Che ?
+ + + + ?
?
?
?
?
?Niv + + ?
+ ?
?
?
?
?
+ +Joh + + ?
+ ?
?
?
?
?
+ ?Wu + + ?
+ ?
?
?
+ ?
+ ?Sch ?
+ ?
?
?
?
?
?
?
+ ?Table 4: Overview of features used by participatinggroups.
See the text for the meaning of the columnabbreviations.
For separate HEAD and DEPREL as-signment: p: only for unlabeled parsing, l: only forlabeling, r: only for reranking.aFORM versus LEMMAbnumber of tokens governed by childcPOSTAG versus CPOSTAGdfor arity constraintefor arity constraintffor ?full?
head constraintgfor uniqueness constrainthfor barrier constraintiof constraintsjPOS window size(sc) other than children/siblings: neighboring sub-trees/spans, or ancestors of i and j; distance from ito j; information derived from all the tokens in be-tween i and j (e.g.
whether there is an interveningverb or how many intervening commas there are);global features (e.g.
does the sentence contain a fi-nite verb); explicit feature combinations (dependingon the learner, these might not be necessary, e.g.
apolynomial kernel routinely combines features); forclassifier-based parsers: the previous actions, i.e.classifications; whether information about labels isused as input for other decisions.
Finally, the pre-cise set of features can be optimized per language.1596 ResultsTable 5 shows the official results for submittedparser outputs.31 The two participant groups withthe highest total score are McDonald et al (2006)and Nivre et al (2006).
As both groups had muchprior experience in multilingual dependency pars-ing (see Section 2), it is not too surprising that theyboth achieved good results.
It is surprising, how-ever, how similar their total scores are, given thattheir approaches are quite different (see Table 2).The results show that experiments on just one or twolanguages certainly give an indication of the useful-ness of a parsing approach but should not be takenas proof that one algorithm is better for ?parsing?
(ingeneral) than another that performs slightly worse.The Bulgarian scores suggest that rankings wouldnot have been very different had it been the 13thobligatory languages.Table 6 shows that the same holds had we used an-other evaluation metric.
Note that a negative numberin both the third and fifth column indicates that er-rors on HEAD and DEPREL occur together on thesame token more often than for other parsers.
Fi-nally, we checked that, had we also scored on punc-tuation tokens, total scores as well as rankings wouldonly have shown very minor differences.7 Result analysis7.1 Across data setsThe average LAS over all data sets varies between56.0 for Turkish and 85.9 for Japanese.
Top scoresvary between 65.7 for Turkish and 91.7 for Japanese.In general, there is a high correlation between thebest scores and the average scores.
This means thatdata sets are inherently easy or difficult, no mat-ter what the parsing approach.
The ?easiest?
one isclearly the Japanese data set.
However, it would bewrong to conclude from this that Japanese in generalis easy to parse.
It is more likely that the effect stemsfrom the characteristics of the data.
The JapaneseVerbmobil treebank contains dialogue within a re-stricted domain (making business appointments).
As31Unfortunately, urgent other obligations prevented two par-ticipants (John O?Neil and Kenji Sagae) from submitting a pa-per about their shared task work.
Their results are indicated bya smaller font.
Sagae used a best-first probabilistic version ofY&M (p.c.
).LAS unlabeled label acc.McD 80.3 = 86.6 ?1 86.7Niv 80.2 = 85.5 +1 86.8O?N 78.4 = 85.3 ?1 85.0Rie 77.9 = 85.0 ?1 84.9Sag 77.8 ?2 83.7 +2 85.6Che 77.7 +1 84.6 = 84.2Cor 76.9 +1 84.4 ?1 84.0Cha 76.8 = 83.5 +1 84.1Joh 74.9 ?1 80.4 = 83.7Car 74.7 +1 81.2 = 83.5Wu 71.7 ?1 78.4 ?1 79.1Can 70.8 +1 78.4 ?1 78.6Bic 70.0 = 77.5 a+2 80.3Dre 65.2 ?1 74.5 ?1 75.2Yur 65.0 ?1 73.5 ?2 70.9Liu 63.3 ?2 70.7 = 73.6Sch 62.8 = 72.1 b+3 75.7Att 61.2 c+4 76.2 = 70.7Shi 34.2 = 38.7 = 39.7Table 6: Differences in ranking depending on theevaluation metric.
The second column repeats theofficial metric (LAS).
The third column shows howthe ranking for each participant changes (or not: ?=?
)if the unlabeled attachment scores, as shown in thefourth column, are used.
The fifth column showshow the ranking changes (in comparison to LAS) ifthe label accuracies, as shown in the sixth column,are used.aIn Bick?s method, preference is given to the assignment ofdependency labels.bSchiehlen derived the constituent labels for his PCFG ap-proach from the DEPREL values.cDue to the bug (see footnote with Table 5).can be seen in Table 1, there are very few newFORM values in the test data, which is an indica-tion of many dialogues in the treebank being sim-ilar.
In addition, parsing units are short on aver-age.
Finally, the set of DEPREL values is very smalland consequently the ratio between (C)POSTAG andDEPREL values is extremely favorable.
It wouldbe interesting to apply the shared task parsers tothe Kyoto University Corpus (Kurohashi and Nagao,1997), which is the standard treebank for Japaneseand has also been used by Kudo and Matsumoto160Ar Ch Cz Da Du Ge Ja Po Sl Sp Sw Tu Tot SD BuMcD 66.9 85.9 80.2 84.8 79.2 87.3 90.7 86.8 73.4 82.3 82.6 63.2 80.3 8.4 87.6Niv 66.7 86.9 78.4 84.8 78.6 85.8 91.7 87.6 70.3 81.3 84.6 65.7 80.2 8.5 87.4O?N 66.7 86.7 76.6 82.8 77.5 85.4 90.6 84.7 71.1 79.8 81.8 57.5 78.4 9.4 85.2Rie 66.7 90.0 67.4 83.6 78.6 86.2 90.5 84.4 71.2 77.4 80.7 58.6 77.9 10.1 0.0Sag 62.7 84.7 75.2 81.6 76.6 84.9 90.4 86.0 69.1 77.7 82.0 63.2 77.8 9.0 0.0Che 65.2 84.3 76.2 81.7 71.8 84.1 89.9 85.1 71.4 80.5 81.1 61.2 77.7 8.7 86.3Cor 63.5 79.9 74.5 81.7 71.4 83.5 90.0 84.6 72.4 80.4 79.7 61.7 76.9 8.5 83.4Cha 60.9 85.1 72.9 80.6 72.9 84.2 89.1 84.0 69.5 79.7 82.3 60.5 76.8 9.4 0.0Joh 64.3 72.5 71.5 81.5 72.7 80.4 85.6 84.6 66.4 78.2 78.1 63.4 74.9 7.7 0.0Car 60.9 83.7 68.8 79.7 67.3 82.4 88.1 83.4 68.4 77.2 78.7 58.1 74.7 9.7 83.3Wu 63.8 74.8 59.4 78.4 68.5 76.5 90.1 81.5 67.8 73.0 71.7 55.1 71.7 9.7 79.7Can 57.6 78.4 60.9 77.9 74.6 77.6 87.4 77.4 59.2 68.3 79.2 51.1 70.8 11.1 78.7Bic 55.4 76.2 63.0 74.6 69.5 74.7 84.8 78.2 64.3 71.4 74.1 53.9 70.0 9.3 79.2Dre 53.4 71.6 60.5 66.6 61.6 71.0 82.9 75.3 58.7 67.6 67.6 46.1 65.2 9.9 74.8Yur 52.4 72.7 51.9 71.6 62.8 63.8 84.4 70.4 55.1 69.6 65.2 60.3 65.0 9.5 73.5Liu 50.7 75.3 58.5 77.7 59.4 68.1 70.8 71.1 57.2 65.1 63.8 41.7 63.3 10.4 67.6Sch 44.4 66.2 53.3 76.1 72.1 68.7 83.4 71.0 50.7 47.0 71.1 49.8 62.8 13.0 0.0Att 53.8 54.9 59.8 66.4 58.2 69.8 65.4 75.4 57.2 67.4 68.8 37.8 a61.2 9.9 72.9Shi 62.8 0.0 0.0 75.8 0.0 0.0 0.0 0.0 64.6 73.2 79.5 54.2 34.2 36.3 0.0Av 59.9 78.3 67.2 78.3 70.7 78.6 85.9 80.6 65.2 73.5 76.4 56.0 80.0SD 6.5 8.8 8.9 5.5 6.7 7.5 7.1 5.8 6.8 8.4 6.5 7.7 6.3Table 5: Labeled attachment scores of parsers on the 13 test sets.
The total score (Tot) and standard devia-tions (SD) from the average per participant are calculated over the 12 obligatory languages (i.e.
excludingBulgarian).
Note that due to the equal sizes of the test sets for all languages, the total scores, i.e.
the LASover the concatenation of the 12 obligatory test sets, are identical (up to the first decimal digit) to the averageLAS over the 12 test sets.
Averages and standard deviations per data set are calculated ignoring zero scores(i.e.
results not submitted).
The highest score for each column and those not significantly worse (p < 0.05)are shown in bold face.
Significance was computed using the official scoring script eval.pl and DanBikel?s Randomized Parsing Evaluation Comparator, which implements stratified shuffling.aAttardi?s submitted results contained an unfortunate bug which caused the DEPREL values of all tokens with HEAD=0 tobe an underscore (which is scored as incorrect).
Using the simple heuristic of assigning the DEPREL value that most frequentlyoccured with HEAD=0 in training would have resulted in a total LAS of 67.5.
(2000), or to the domain-restricted Japanese dia-logues of the ATR corpus (Lepage et al, 1998).32Other relatively ?easy?
data sets are Portuguese(2nd highest average score but, interestingly, thethird-longest parsing units), Bulgarian (3rd), Ger-man (4th) and Chinese (5th).
Chinese also has thesecond highest top score33 and Chinese parsing units32Unfortunately, both these treebanks need to be bought, sothey could not be used for the shared task.
Note also thatJapanese dependency parsers often operate on ?bunsetsus?
in-stead of words.
Bunsetsus are related to chunks and consist ofa content word and following particles (if any).33Although this seems to be somewhat of a mystery com-pared to the ranking according to the average scores.
Riedel etare the shortest.
and Chinese parsing units are theshortest.
We note that all ?easier?
data sets offerlarge to middle-sized training sets.The most difficult data set is clearly the Turkishone.
It is rather small, and in contrast to Arabicand Slovene, which are equally small or smaller, itcovers 8 genres, which results in a high percentageof new FORM and LEMMA values in the test set.It is also possible that parsers get confused by thehigh proportion (one third!)
of non-scoring tokensal.
(2006)?s top score is more than 3% absolute above the sec-ond highest score and they offer no clear explanation for theirsuccess.161and the many tokens with ?
?
as either the FORM orLEMMA.
There is a clear need for further researchto check whether other representations result in bet-ter performance.The second-most difficult data set is Arabic.
It isquite small and has by far the longest parsing units.The third-most difficult data set is Slovene.
It hasthe smallest training set.
However, its average aswell as top score far exceed those for Arabic andTurkish, which are larger.
Interestingly, although thetreebank text comes from a single source (a transla-tion of Orwell?s novel ?1984?
), there is quite a highproportion of new FORM and LEMMA values in thetest set.
The fourth-most difficult data set is Czechin terms of the average score and Dutch in terms ofthe top score.
The diffence in ranking for Czech isprobably due to the fact that it has by far the largesttraining set and ironically, several participants couldnot train on all data within the limited time, or elsehad to partition the data and train one model for eachpartition.
Likely problems with the Dutch data setare: noisy (C)POSTAG and LEMMA, (C)POSTAGfor multiwords, and the highest proportion of non-projectivity.Factors that have been discussed so far are: thesize of the training data, the proportion of newFORM and LEMMA values in the test set, the ra-tio of (C)POSTAG to DEPREL values, the averagelength of the parsing unit the proportion of non-projective arcs/parsing units.
It would be interest-ing to derive a formula based on those factors thatfits the shared task data and see how well it pre-dicts results on new data sets.
One factor that seemsto be irrelevant is the head-final versus head-initialdistinction, as both the ?easiest?
and the most dif-ficult data sets are for head-final languages.
Thereis also no clear proof that some language familiesare easier (with current parsing methods) than oth-ers.
It would be interesting to test parsers on theHebrew treebank (Sima?an et al, 2001), to compareperformance to Arabic, the other Semitic languagein the shared task, or on the Hungarian Szeged Cor-pus (Csendes et al, 2004), for another agglutinativelanguage.7.2 Across participantsFor most parsers, their ranking for a specific lan-guage differs at most a few places from their over-all ranking.
There are some outliers though.
Forexample, Johansson and Nugues (2006) and Yuret(2006) are seven ranks higher for Turkish than over-all, while Riedel et al (2006) are five ranks lower.Canisius et al (2006) are six and Schiehlen andSpranger (2006) even eight ranks higher for Dutchthan overall, while Riedel et al (2006) are six rankslower for Czech and Johansson and Nugues (2006)also six for Chinese.
Some of the higher rankingscould be related to native speaker competence andresulting better parameter tuning but other outliersremain a mystery.
Even though McDonald et al(2006) and Nivre et al (2006) obtained very simi-lar overall scores, a more detailed look at their per-formance shows clear differences.
Taken over all 12obligatory languages, both obtain a recall of morethan 89% on root tokens (i.e.
those with HEAD=0)but Nivre?s precision on them is much lower thanMcDonald?s (80.91 versus 91.07).
This is likely tobe an effect of the different parsing approaches.7.3 Across part-of-speech tagsWhen breaking down by part-of-speech the resultsof all participants on all data sets, one can observesome patterns of ?easy?
and ?difficult?
parts-of-speech, at least in so far as tag sets are compara-ble across treebanks.
The one PoS that everybodygot 100% correct are the German infinitival mark-ers (tag PTKZU; like ?to?
in English).
Accuracy onthe Swedish equivalent (IM) is not far off at 98%.Other easy PoS are articles, with accuracies in thenineties for German, Dutch, Swedish, Portugueseand Spanish.
As several participants have remarkedin their papers, prepositions are much more difficult,with typical accuracies in the fifties or sixties.
Simi-larly, conjunctions typically score low, with accura-cies even in the forties for Arabic and Dutch.8 Future researchThere are many directions for interesting researchbuilding on the work done in this shared task.
Oneis the question which factors make data sets ?easy?or difficult.
Another is finding out how much ofparsing performance depends on annotations suchas the lemma and morphological features, whichare not yet routinely part of treebanking efforts.
Inthis respect, it would be interesting to repeat ex-162periments with the recently released new version ofthe TIGER treebank which now contains this in-formation.
One line of research that does not re-quire additional annotation effort is defining or im-proving the mapping from coarse-grained to fine-grained PoS tags.34 Another is harvesting and usinglarge-scale distributional data from the internet.
Wealso hope that by combining parsers we can achieveeven better performance, which in turn would facili-tate the semi-automatic enlargement of existing tree-banks and possibly the detection of remaining er-rors.
This would create a positive feedback loop.Finally one must not forget that almost all of theLEMMA, (C)POSTAG and FEATS values and evenpart of the FORM column (the multiword tokensused in many data sets and basically all tokeniza-tion for Chinese and Japanese, where words are nor-mally not delimited by spaces) have been manuallycreated or corrected and that the general parsing taskhas to integrate automatic tokenization, morphologi-cal analysis and tagging.
We hope that the resourcescreated and lessons learned during this shared taskwill be valuable for many years to come but alsothat they will be extended and improved by othersin the future, and that the shared task website willgrow into an informational hub on multilingual de-pendency parsing.ReferencesA.
Arun and F. Keller.
2005.
Lexicalization in crosslinguisticprobabilistic parsing: The case of French.
In Proc.
of the43rd Annual Meeting of the ACL.D.
Bikel.
2002.
Design of a multi-lingual, parallel-processingstatistical parsing engine.
In Proc.
of the Human LanguageTechnology Conf.
(HLT).E.
Black, S. Abney, D. Flickenger, et al 1991.
A procedure forquantitatively comparing the syntactic coverage of Englishgrammars.
In Speech and Natural Language: Proceedingsof a Workshop Held at Pacific Grove, California.E.
Black, F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, andS.
Roukos.
1993.
Towards history-based grammars: Usingricher models for probabilistic parsing.
In Proc.
of the 31rdAnnual Meeting of the ACL.S.
Buchholz and D. Green.
2006.
Quality control of treebanks:documenting, converting, patching.
In LREC 2006 work-shop on Quality assurance and quality measurement for lan-guage and speech resources.34For the Swedish Talbanken05 corpus, that work has beendone after the shared task (see the treebank?s web site).A.
Chanev, K. Simov, P. Osenova, and S. Marinov.
2006.
De-pendency conversion and parsing of the BulTreeBank.
InProc.
of the LREC-Workshop Merging and Layering Lin-guistic Information.Y.
Cheng, M. Asahara, and Y. Matsumoto.
2005.
Chinesedeterministic dependency analyzer: Examining effects ofglobal features and root node finder.
In Proc.
of SIGHAN-2005.Y.J.
Chu and T.H.
Liu.
1965.
On the shortest arborescence of adirected graph.
Science Sinica, 14:1396?1400.M.
Collins, J. Hajic, L. Ramshaw, and C. Tillmann.
1999.A statistical parser for Czech.
In Proc.
of the 37th AnnualMeeting of the ACL.M.
Collins.
1996.
A new statistical parser based on bigramlexical dependencies.
In Proc.
of the 34th Annual Meetingof the ACL.M.
Collins.
1997.
Three generative, lexicalised models for sta-tistical parsing.
In Proc.
of the 35th Annual Meeting of theACL.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.B.
Cowan and M. Collins.
2005.
Morphology and reranking forthe statistical parsing of Spanish.
In Proc.
of the Joint Conf.on Human Language Technology and Empirical Methods inNatural Language Processing (HLT/EMNLP).D.
Csendes, J. Csirik, and T. Gyimo?thy.
2004.
The Szeged cor-pus: a POS tagged and syntactically annotated Hungariannatural language corpus.
In Proc.
of the 5th Intern.
Work-shop on Linguistically Interpreteted Corpora (LINC).W.
Daelemans, J. Zavrel, P. Berck, and S. Gillis.
1996.
MBT:A memory-based part of speech tagger-generator.
In Proc.of the 4th Workshop on Very Large Corpora (VLC).A.
Dubey and F. Keller.
2003.
Probabilistic parsing for Germanusing sister-head dependencies.
In Proc.
of the 41st AnnualMeeting of the ACL.J.
Edmonds.
1967.
Optimum branchings.
Journal of Researchof the National Bureau of Standards, 71B:233?240.J.
Einarsson.
1976.
Talbankens skriftspra?kskonkordans.J.
Eisner.
1996.
Three new probabilistic models for depen-dency parsing: An exploration.
In Proc.
of the 16th Intern.Conf.
on Computational Linguistics (COLING), pages 340?345.D.
Elworthy.
2000.
A finite-state parser with dependency struc-ture output.
In Proc.
of the 6th Intern.
Workshop on ParsingTechnologies (IWPT).H.
Gaifman.
1965.
Dependency systems and phrase-structuresystems.
Information and Control, 8:304?337.D.
Hays.
1964.
Dependency theory: A formalism and someobservations.
Language, 40:511?525.R.
Hudson.
1984.
Word Grammar.
Blackwell.163T.
Kudo and Y. Matsumoto.
2000.
Japanese dependency struc-ture analysis based on support vector machines.
In Proc.
ofthe Joint Conf.
on Empirical Methods in Natural LanguageProcessing and Very Large Corpora (EMNLP/VLC).S.
Kurohashi and M. Nagao.
1994.
KN parser: Japanese depen-dency/case structure analyzer.
In Proceedings of the Work-shop on Sharable Natural Language, pages 48?55.S.
Kurohashi and M. Nagao.
1997.
Kyoto University text cor-pus project.
In Proc.
of the 5th Conf.
on Applied NaturalLanguage Processing (ANLP), pages 115?118.Y.
Lepage, S. Ando, S. Akamine, and H. Iida.
1998.
An anno-tated corpus in Japanese using Tesnie`re?s structural syntax.In ACL-COLING Workshop on Processing of Dependency-Based Grammars, pages 109?115.D.
Lin.
1995.
A dependency-based method for evaluatingbroad-coverage parsers.
In Proc.
of the International JointConference on Artificial Intelligence (IJCAI).D.
Magerman.
1995.
Statistical decision-tree models for pars-ing.
In Proc.
of the 33rd Annual Meeting of the ACL, pages276?283.M.
Marcus, G. Kim, M. Marcinkiewicz, R. Mac-Intyre, A. Bies,M.
Ferguson, K. Katz, and B. Schasberger.
1994.
The Penntreebank: Annotating predicate argument structure.
In Proc.of the Workshop on Human Language Technology (HLT).S.
Marinov and J. Nivre.
2005.
A data-driven dependencyparser for Bulgarian.
In Proc.
of the 4th Workshop on Tree-banks and Linguistic Theories (TLT), pages 89?100.R.
McDonald and F. Pereira.
2006.
Online learning of approx-imate dependency parsing algorithms.
In Proc.
of the 11thConf.
of the European Chapter of the ACL (EACL).R.
McDonald, K. Crammer, and F. Pereira.
2005a.
Onlinelarge-margin training of dependency parsers.
In Proc.
of the43rd Annual Meeting of the ACL.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005b.Non-projective dependency parsing using spanning tree al-gorithms.
In Proc.
of the Joint Conf.
on Human LanguageTechnology and Empirical Methods in Natural LanguageProcessing (HLT/EMNLP).I.
Mel?c?uk.
1988.
Dependency Syntax: Theory and Practice.The SUNY Press, Albany, N.Y.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective dependencyparsing.
In Proc.
of the 43rd Annual Meeting of the ACL,pages 99?106.J.
Nivre and M. Scholz.
2004.
Deterministic dependency pars-ing of English text.
In Proc.
of the 20th Intern.
Conf.
onComputational Linguistics (COLING), pages 64?70.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-based depen-dency parsing.
In Proc.
of the 8th Conf.
on ComputationalNatural Language Learning (CoNLL), pages 49?56.K.
Oflazer.
1999.
Dependency parsing with an extended finitestate approach.
In Proc.
of the 37th Annual Meeting of theACL, pages 254?260.G.
Sampson.
1995.
English for the Computer: The SUSANNECorpus and analytic scheme.
Clarendon Press.K.
Sima?an, A. Itai, Y.
Winter, A. Altman, and N. Nativ.
2001.Building a tree-bank of modern Hebrew text.
In JournalTraitement Automatique des Langues (t.a.l.)
?
Special Issueon Natural Language Processing and Corpus Linguistics.D.
Sleator and D. Temperley.
1993.
Parsing English with a linkgrammar.
In Proc.
of the 3rd Intern.
Workshop on ParsingTechnologies (IWPT).P.
Tapanainen and T. Ja?rvinen.
1997.
A non-projective depen-dency parser.
In Proc.
of the 5th Conf.
on Applied NaturalLanguage Processing (ANLP).U.
Teleman, 1974.
Manual fo?r grammatisk beskrivning av taladoch skriven svenska (MAMBA).L.
Tesnie`re.
1959.
Ele?ment de syntaxe structurale.
Klinck-sieck, Paris.H.
Yamada and Y. Matsumoto.
2003.
Statistical dependencyanalysis with support vector machines.
In Proc.
of the 8th In-tern.
Workshop on Parsing Technologies (IWPT), pages 195?206.Papers by participants of CoNLL-X (this volume)G. Attardi.
2006.
Experiments with a multilanguage non-projective dependency parser.E.
Bick.
2006.
LingPars, a linguistically inspired, language-independent machine learner for dependency treebanks.S.
Canisius, T. Bogers, A. van den Bosch, J. Geertzen, andE.
Tjong Kim Sang.
2006.
Dependency parsing by infer-ence over high-recall dependency predictions.M.
Chang, Q.
Do, and D. Roth.
2006.
A pipeline model forbottom-up dependency parsing.S.
Corston-Oliver and A. Aue.
2006.
Dependency parsing withreference to Slovene, Spanish and Swedish.M.
Dreyer, D. Smith, and N. Smith.
2006.
Vine parsing andminimum risk reranking for speed and precision.R.
Johansson and P. Nugues.
2006.
Investigating multilingualdependency parsing.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Multilingualdependency analysis with a two-stage discriminative parser.J.
Nivre, J.
Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006.Labeled pseudo-projective dependency parsing with supportvector machines.S.
Riedel, R.
C?ak?c?, and I. Meza-Ruiz.
2006.
Multi-lingual de-pendency parsing with incremental integer linear program-ming.M.
Schiehlen and K. Spranger.
2006.
Language indepen-dent probabilistic context-free parsing bolstered by machinelearning.N.
Shimizu.
2006.
Maximum spanning tree algorithm for non-projective labeled dependency parsing.D.
Yuret.
2006.
Dependency parsing as a classication problem.164
