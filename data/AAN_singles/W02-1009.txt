Transformational Priors Over GrammarsJason Eisner <jason@cs.jhu.edu>Johns Hopkins University, 3400 N. Charles St., NEB 224, Baltimore, MDAbstractThis paper proposes a novel class of PCFG parameterizationsthat support linguistically reasonable priors over PCFGs.
Toestimate the parameters is to discover a notion of relatednessamong context-free rules such that related rules tend to haverelated probabilities.
The prior favors grammars in which therelationships are simple to describe and have few major excep-tions.
A basic version that bases relatedness on weighted editdistance yields superior smoothing of grammars learned fromthe Penn Treebank (20% reduction of rule perplexity over thebest previous method).1 A Sketch of the Concrete ProblemThis paper uses a new kind of statistical model to smooththe probabilities of PCFG rules.
It focuses on ?flat?
or?dependency-style?
rules.
These resemble subcategoriza-tion frames, but include adjuncts as well as arguments.The verb put typically generates 3 dependents?asubject NP at left, and an object NP and goal PP at right:?
S?
NP put NP PP: Jim put [the pizza] [in the oven]But put may also take other dependents, in other rules:?
S?
NP Adv put NP PP: Jim often put [a pizza] [in the oven]?
S?
NP put NP PP PP: Jim put soup [in an oven] [at home]?
S?
NP put NP: Jim put [some shares of IBM stock]?
S?
NP put Prt NP: Jim put away [the sauce]?
S?
TO put NP PP: to put [the pizza] [in the oven]?
S?
NP put NP PP SBAR: Jim put it [to me] [that .
.
.
]These other rules arise if put can add, drop, reorder,or retype its dependents.
These edit operations on rulesare semantically motivated and quite common (Table 1).We wish to learn contextual probabilities for the editoperations, based on an observed sample of flat rules.
InEnglish we should discover, for example, that it is quitecommon to add or delete PP at the right edge of a rule.These contextual edit probabilities will help us guess thetrue probabilities of novel or little-observed rules.However, rules are often idiosyncratic.
Our smooth-ing method should not keep us from noticing (givenenough evidence) that put takes a PP more often thanmost verbs.
Hence this paper?s proposal is a Bayesiansmoothing method that allows idiosyncrasy in the gram-mar while presuming regularity to be more likely a priori.The model will assign a positive probability to eachof the infinitely many formally possible rules.
The fol-lowing bizarre rule is not observed in training, and seemsvery unlikely.
But there is no formal reason to rule it out,and it might help us parse an unlikely test sentence.
Sothe model will allow it some tiny probability:?
S?
NP Adv PP put PP PP PP NP AdjP S2 Background and Other ApproachesA PCFG is a conditional probability function p(RHS |LHS).1 For example, p(V NP PP | VP) gives the proba-bility of the rule VP?
V NP PP.
With lexicalized non-terminals, it has form p(Vput NPpizza PPin | VPput).Usually one makes an independence assumption anddefines this as p(Vput NP PP | VPput) times factors thatchoose dependent headwords pizza and in accordingto the selectional preferences of put.
This paper is aboutestimating the first factor, p(Vput NP PP | VPput).In supervised learning, it is simplest to use a max-imum likelihood estimate (perhaps with backoff fromput).
Charniak (1997) calls this a ?Treebank grammar?and gambles that assigning 0 probability to rules unseenin training data will not hurt parsing accuracy too much.However, there are four reasons not to use a Treebankgrammar.
First, ignoring unseen rules necessarily sacri-fices some accuracy.
Second, we will show that it im-proves accuracy to flatten the parse trees and use flat,dependency-style rules like p(NP put NP PP | Sput);this avoids overly strong independence assumptions, butit increases the number of unseen rules and so makesTreebank grammars less tenable.
Third, backing off fromthe word is a crude technique that does not distinguishamong words.2 Fourth, one would eventually like to re-duce or eliminate supervision, and then generalization isimportant to constrain the search to reasonable grammars.To smooth the distribution p(RHS | LHS), one can de-fine it in terms of a set of parameters and then estimatethose parameters.
Most researchers have used an n-grammodel (Eisner, 1996; Charniak, 2000) or more generalMarkov model (Alshawi, 1996) to model the sequenceof nonterminals in the RHS.
The sequence Vput NP PPin our example is then assumed to be emitted by someMarkov model of VPput rules (again with backoff fromput).
Collins (1997, model 2) uses a more sophisticatedmodel in which all arguments in this sequence are gener-ated jointly, as in a Treebank grammar, and then a Markovprocess is used to insert adjuncts among the arguments.While Treebank models overfit the training data,Markov models underfit.
A simple compromise (novel tothis paper) is a hybrid Treebank/Markov model, whichbacks off from a Treebank model to a Markov.
Likethis paper?s main proposal, it can learn well-observed id-iosyncratic rules but generalizes when data are sparse.31Nonstandardly, this allows infinitely many rules with p>0.2One might do better by backing off to word clusters, whichCharniak (1997) did find provided a small benefit.3Carroll and Rooth (1998) used a similar hybrid techniqueAssociation for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
63-70.Proceedings of the Conference on Empirical Methods in NaturalThese models are beaten by our rather different model,transformational smoothing, which learns commonrules and common edits to them.
The comparison is adirect one, based on the perplexity or cross-entropy ofthe trained models on a test set of S?
?
?
?
rules.4A subtlety is that two annotation styles are possible.
Inthe Penn Treebank, put is the head of three constituents(V, VP, and S, where underlining denotes a head child)and joins with different dependents at different levels:?
[S [NP Jim] [VP [V put] [NP pizza] [PP in the oven]]]In the flattened or dependency version that we prefer,each word joins with all of its dependents at once:?
[S [NP Jim] put [NP pizza] [PP in the oven]]A PCFG generating the flat structure must estimatep(NP put NP PP | Sput).
A non-flat PCFG addsthe dependents of put in 3 independent steps, so in ef-fect it factors the flat rule?s probability into 3 suppos-edly independent ?subrule probabilities,?
p(NP VPput |Sput) ?
p(Vput NP PP | VPput) ?
p(put | Vput).Our evaluation judges the estimates of flat-rule prob-abilities.
Is it better to estimate these directly, or as aproduct of estimated subrule probabilities?5 Transforma-tional smoothing is best applied to the former, so that theedit operations can freely rearrange all of a word?s depen-dents.
We will see that the Markov and Treebank/Markovmodels also work much better this way?a useful finding.3 The Abstract Problem: Designing PriorsThis section outlines the Bayesian approach to learningprobabilistic grammars (for us, estimating a distributionover flat CFG rules).
By choosing among the manygrammars that could have generated the training data, thelearner is choosing how to generalize to novel sentences.To guide the learner?s choice, one can explicitly spec-ify a prior probability distribution p(?)
over possiblegrammars ?, which themselves specify probability dis-tributions over strings, rules, or trees.
A learner shouldseek ?
that maximizes p(?)
?
p(D | ?
), where D is theset of strings, rules, or trees observed by the learner.
Thefirst factor favors regularity (?pick an a priori plausiblegrammar?
), while the second favors fitting the idiosyn-crasies of the data, especially the commonest data.6to evaluate rule distributions that they acquired from anautomatically-parsed treebank.4All the methods evaluated here apply also to full PCFGs,but verb-headed rules S ?
?
?
?
present the most varied, inter-esting cases.
Many researchers have tried to learn verb subcat-egorization, though usually not probabilistic subcategorization.5In testing the latter case, we sum over all possible internalbracketings of the rule.
We do train this case on the true internalbracketing, but it loses even with this unfair advantage.6This approach is called semi-Bayesian or Maximum A Pos-Priors can help both unsupervised and supervisedlearning.
(In the semi-supervised experiments here, train-ing data is not raw text but a sparse sample of flat rules.
)Indeed a good deal of syntax induction work has beencarried out in just this framework (Stolcke and Omohun-dro, 1994; Chen, 1996; De Marcken, 1996; Gru?nwald,1996; Osborne and Briscoe, 1997).
However, all suchwork to date has adopted rather simple prior distributions.Typically, it has defined p(?)
to favor PCFGs whose rulesare few, short, nearly equiprobable, and defined over asmall set of nonterminals.
Such definitions are conve-nient, especially when specifying an encoding for MDL,but since they treat all rules alike, they may not be gooddescriptions of linguistic plausibility.
For example, theywill never penalize the absence of a predictable rule.A prior distribution can, however, be used to encodevarious kinds of linguistic notions.
After all, a prior isreally a soft form of Universal Grammar: it gives thelearner enough prior knowledge of grammar to overcomeChomsky?s ?poverty of the stimulus?
(i.e., sparse data).?
A preference for small or simple grammars, as above.?
Substantive preferences, such as a preference for verbsto take 2 nominal arguments, or to allow PP adjuncts.?
Preferences for systematicity, such as a preference forthe rules to be consistently head-initial or head-final.This paper shows how to design a prior that favors acertain kind of systematicity.
Lexicalized grammars fornatural languages are very large?each word specifies adistribution over all possible dependency rules it couldhead?but they tend to have internal structure.
The newprior prefers grammars in which a rule?s probability canbe well-predicted from the probabilities of other rules, us-ing linguistic transformations such as edit operations.For example, p(NP Adv w put NP PP | Sw) cor-relates with p(NP w NP PP | Sw).
Both numbers arehigh for w = put, medium for w = fund, and low forw = sleep.
The slope of the regression line has to dowith the rate of preverbal Adv-insertion in English.The correlation is not perfect (some verbs are espe-cially prone to adverbial modification), which is why wewill only model it with a prior.
To just the extent that evi-dence aboutw is sparse, the prior will cause the learner tosmooth the two probabilities toward the regression line.4 Patterns Worth ModelingBefore spelling out our approach, let us do a sanity check.A frame is a flat rule whose headword is replaced withteriori learning, since it is equivalent to maximizing p(?
| D).It is also equivalent to Minimum Description Length (MDL)learning, which minimizes the total number of bits `(?
)+`(D |?)
needed to encode grammar and data, because one can choosean encoding scheme where `(x) = ?
log2 p(x), or conversely,define probability distributions by p(x) = 2?`(x).MI ?
?
MI ?
?
MI ?
?9.01 [NP ADJP-PRD] [NP RB ADJP-PRD] 4.76 [TO S] [ S] 5.54 [TO NP PP] [NP TO NP]8.65 [NP ADJP-PRD] [NP PP-LOC-PRD] 4.17 [TO S] [TO NP PP] 5.25 [TO NP PP] [NP MD NP .
]8.01 [NP ADJP-PRD] [NP NP-PRD] 2.77 [TO S] [TO NP] 4.67 [TO NP PP] [NP MD NP]7.69 [NP ADJP-PRD] [NP ADJP-PRD .]
6.13 [TO NP] [TO NP SBAR-TMP] 4.62 [TO NP PP] [TO ]8.49 [NP NP-PRD] [NP NP-PRD .]
5.72 [TO NP] [TO NP PP PP] 3.19 [TO NP PP] [TO NP]7.91 [NP NP-PRD] [NP ADJP-PRD .]
5.36 [TO NP] [NP MD RB NP] 2.05 [TO NP PP] [ NP]7.01 [NP NP-PRD] [NP ADJP-PRD] 5.16 [TO NP] [TO NP PP PP-TMP] 5.08 [ NP] [ADVP-TMP NP]8.45 [NP ADJP-PRD .]
[NP PP-LOC-PRD] 5.11 [TO NP] [TO NP ADVP] 4.86 [ NP] [ADVP NP]8.30 [NP ADJP-PRD .]
[NP NP-PRD .]
4.85 [TO NP] [TO NP PP-LOC] 4.53 [ NP] [ NP PP-LOC]8.04 [NP ADJP-PRD .]
[NP NP-PRD] 4.84 [TO NP] [MD NP] 3.50 [ NP] [ NP PP]7.01 [NP ADJP-PRD .]
[NP ADJP-PRD] 4.49 [TO NP] [NP TO NP] 3.17 [ NP] [ S]7.01 [NP SBAR] [NP SBAR .
?]
4.36 [TO NP] [NP MD S] 2.28 [ NP] [NP NP]4.75 [NP SBAR] [NP SBAR .]
4.36 [TO NP] [NP TO NP PP] 1.89 [ NP] [NP NP .
]6.94 [NP SBAR .]
[?
NP SBAR .]
4.26 [TO NP] [NP MD NP PP] 2.56 [NP NP] [NP NP .
]5.94 [NP SBAR .]
[NP SBAR .
?]
4.26 [TO NP] [TO NP PP-TMP] 2.20 [NP NP] [ NP]5.90 [NP SBAR .]
[S , NP .]
4.21 [TO NP] [TO PRT NP] 4.89 [NP NP .]
[NP ADVP-TMP NP .
]5.82 [NP SBAR .]
[NP ADVP SBAR .]
4.20 [TO NP] [NP MD NP] 4.57 [NP NP .]
[NP ADVP NP .
]4.68 [NP SBAR .]
[ SBAR] 3.99 [TO NP] [TO NP PP] 4.51 [NP NP .]
[NP NP PP-TMP]4.50 [NP SBAR .]
[NP SBAR] 3.69 [TO NP] [NP MD NP .]
3.35 [NP NP .]
[NP S .
]3.23 [NP SBAR .]
[NP S .]
3.60 [TO NP] [TO ] 2.99 [NP NP .]
[NP NP]2.07 [NP SBAR .]
[NP ] 3.56 [TO NP] [TO PP] 2.96 [NP NP .]
[NP NP PP .
]1.91 [NP SBAR .]
[NP NP .]
2.56 [TO NP] [NP NP PP] 2.25 [NP NP .]
[ NP PP]1.63 [NP SBAR .]
[NP NP] 2.04 [TO NP] [NP S] 2.20 [NP NP .]
[ NP]4.52 [NP S] [NP S .]
1.99 [TO NP] [NP NP] 4.82 [NP S .]
[ S]4.27 [NP S] [ S] 1.69 [TO NP] [NP NP .]
4.58 [NP S .]
[NP S]3.36 [NP S] [NP ] 1.68 [TO NP] [NP NP PP .]
3.30 [NP S .]
[NP ]2.66 [NP S] [NP NP .]
1.03 [TO NP] [ NP] 2.93 [NP S .]
[NP NP .
]2.37 [NP S] [NP NP] 4.75 [S , NP .]
[NP SBAR .]
2.28 [NP S .]
[NP NP]Table 1: The most predictive pairs of sentential frames.
If S?
?
occurs in training data at least 5 times with a given headword inthe position, then S?
?
also tends to appear at least once with that headword.
MI measures the mutual information of thesetwo events, computed over all words.
When MI is large, as here, the edit distance between ?
and ?
tends to be strikingly small (1or 2), and certain linguistically plausible edits are extremely common.the variable ?
?
(corresponding tow above).
Table 1 il-lustrates that in the Penn Treebank, if frequent rules withframe ?
imply matching rules with frame ?, there areusually edit operations (section 1) to easily turn ?
into ?.How about rare rules, whose probabilities are most inneed of smoothing?
Are the same edit transformationsthat we can learn from frequent cases (Table 1) appropri-ate for predicting the rare cases?
The very rarity of theserules makes it impossible to create a table like Table 1.However, rare rules can be measured in the aggregate,and the result suggests that the same kinds of transforma-tions are indeed useful?perhaps even more useful?inpredicting them.
Let us consider the set R of 2,809,545possible flat rules that stand at edit distance 1 from the setof S ?
?
?
?
rules observed in our English training data.That is, a rule such as Sput ?
NP put NP is in R if itdid not appear in training data itself, but could be derivedby a single edit from some rule that did appear.A bigram Markov model (section 2) was used to iden-tify 2,714,763 rare rules in R?those that were predictedto occur with probability < 0.0001 given their head-words.
79 of these rare rules actually appeared in adevelopment-data set of 1423 rules.
The bigram modelwould have expected only 26.2 appearances, given thelexical headwords in the test data set.
The difference isstatistically significant (p < 0.001, bootstrap test).In other words, the bigram model underpredicts theedit-distance ?neighbors?
of observed rules by a factorof 3.7 One can therefore hope to use the edit transforma-tions to improve on the bigram model.
For example, the7Similar results are obtained when we examine just one par-ticular kind of edit operation, or rules of one particular length.Delete Y transformation recognizes that if ?
?
?
X Y Z ?
?
?has been observed, then ?
?
?
X Z ?
?
?
is plausible even ifthe bigram X Z has not previously been observed.Presumably, edit operations are common because theymodify a rule in semantically useful ways, allowing thefiller of a semantic role to be expressed (Insert), sup-pressed (Delete), retyped (Substitute), or heavy-shifted(Swap).
Such ?valency-affecting operations?
have re-peatedly been invoked by linguists; they are not confinedto English.8 So a learner of an unknown language canreasonably expect a priori that flat rules related by editoperations may have related probabilities.However, which edit operations varies by language.Each language defines its own weighted, contextual,asymmetric edit distance.
So the learner will have to dis-cover how likely particular edits are in particular con-texts.
For example, it must learn the rates of prever-bal Adv-insertion and right-edge PP-insertion.
Evidenceabout these rates comes mainly from the frequent rules.5 A Transformation ModelThe form of our new model is shown in Figure 1.
Thevertices are flat context-free rules, and the arcs betweenthem represent edit transformations.
The set of arcs leav-8Carpenter (1991) writes that whenever linguists run into theproblem of systematic redundancy in the syntactic lexicon, theydesign a scheme in which lexical entries can be derived fromone another by just these operations.
We are doing the samething.
The only twist that the lexical entries (in our case, flatPCFG rules) have probabilities that must also be derived, so wewill assume that the speaker applies these operations (randomlyfrom the hearer?s viewpoint) at various rates to be learned.exp ?1Z1 exp ?2+?8Z1To fund NPexp ?3+?5+?8Z2exp ?0Z4exp ?0Z8exp ?3+?5+?9Z6exp ?2+?9Z5exp ?1Z50.0002STARTHALT START(merge)To merge NPTo merge PP NPTo merge NP PPHALTTo fund NP PPTo fund PP NPHALTHALT START(fund)exp ?7+?8Z3exp ?0Z30.0011exp ?0Z7exp ?7+?9Z7exp ?3+?4Z2exp ?0Z2exp ?0Z6exp ?3+?4Z6exp ?6Z3exp ?6Z7?0 halts ?3 inserts PP ?6 deletes PP ?8 yields To fund NP PP?1 chooses To NP ?4 inserts PP before NP ?7 moves NP right past PP ?9 yields To merge NP PP?2 chooses To NP PP ?5 inserts PP before right edgeFigure 1: A fragment of a transformation model.
Vertices are possible context-free rules (their left-hand sides, Sfund ?
andSmerge ?
, are omitted to avoid visual clutter).
Arc probabilities are determined log-linearly, as shown, from a real-valued vector ?of feature weights.
The Z values are chosen so that the arcs leaving each vertex have total probability 1.
Dashed arrows representarcs not shown here (there are hundreds from each vertex, mainly insertions).
Also, not all features are shown (see Table 2).ing any given vertex has total probability 1.
The learner?sjob is to discover the probabilities.Fortunately, the learner does not have to learn a sep-arate probability for each of the (infinitely) many arcs,since many of the arcs represent identical or similar edits.As shown in Figure 1, an arc?s probability is determinedfrom meaningful features of the arc, using a conditionallog-linear model of p(arc | source vertex).
The learneronly has to learn the finite vector ?
of feature weights.Arcs that represent similar transformations have similarfeatures, so they tend to have similar probabilities.This transformation model is really a PCFG with un-usual parameterization.
That is, for any value of ?, itdefines a language-specific probability distribution overall possible context-free rules (graph vertices).
To sam-ple from this distribution, take a random walk from thespecial vertex START to the special vertex HALT.
Therule at the last vertex reached before HALT is the sample.This sampling procedure models a process where thespeaker chooses an initial rule and edits it repeatedly.The random walk might reach Sfund ?
To fund NPin two steps and simply halt there.
This happenswith probability 0.0011 ?
exp ?1Z1 ?exp ?0Z2.
Or, havingarrived at Sfund ?
To fund NP, it might transformit into Sfund ?
To fund PP NP and then further toSfund ?
To fund NP PP before halting.Thus, p?
(Sfund ?
To fund NP PP) denotes theprobability that the random walk somehow reachesSfund ?
To fund NP PP and halts there.
Condi-tionalizing this probability gives p?
(To NP PP |Sfund), as needed for the PCFG.99The experiments of this paper do not allow transformationsGiven ?, it is nontrivial to solve for the probability dis-tribution over grammar rules e. Let I?
(e) denote the flowto vertex e. This is defined to be the total probability ofall paths from START to e. Equivalently, it is the expectednumber of times e would be visited by a random walkfrom START.
The following recurrence defines p?(e):10I?
(e) = ?e,START +?e?I?(e?)
?
p(e?
?
e) (1)p?
(e) = I?
(e) ?
p(e?
HALT) (2)Since solving the large linear system (1) would be pro-hibitively expensive, in practice we use an approximaterelaxation algorithm (Eisner, 2001) that propagates flowthrough the graph until near-convergence.
In general thismay underestimate the true probabilities somewhat.Now consider how the parameter vector ?
affects thedistribution over rules, p?
(e), in Figure 1:?
By raising the initial weight ?1, one canincrease the flow to Sfund ?
To fund NP,Smerge ?
To merge NP, and the like.
By equa-tion (2), this also increases the probability of these rules.But the effect also feeds through the graph to increasethe flow and probability at those rules?
descendants inthe graph, such as Smerge ?
To merge NP PP.So a single parameter ?1 controls a whole complex ofrule probabilities (roughly speaking, the infinitival transi-tives).
The model thereby captures the fact that, althoughthat change the LHS or headword of a rule, so it is trivial to findthe divisor p?
(Sfund): in Figure 1 it is 0.0011.
But in general,LHS-changing transformations can be useful (Eisner, 2001).10Where ?x,y = 1 if x = y, else ?x,y = 0.rules are mutually exclusive events whose probabilitiessum to 1, transformationally related rules have positivelycorrelated probabilities that rise and fall together.?
The exception weight ?9 appears on all and only thearcs to Smerge ?
To merge NP PP.
That rule haseven higher probability than predicted by PP-insertion asabove (since merge, unlike fund, actually tends to sub-categorize for PPwith).
To model its idiosyncratic prob-ability, one can raise ?9.
This ?lists?
the rule speciallyin the grammar.
Rules derived from it also increase inprobability (e.g., Smerge ?
To Adv merge NP PP),since again the effect feeds through the graph.?
The generalization weight ?3 models the strength ofthe PP-insertion relationship.
Equations (1) and (2) im-ply that p?
(Sfund ?
To fund NP PP) is modeled asa linear combination of the probabilities of that rule?sparents in the graph.
?3 controls the coefficient ofp?
(Sfund ?
To fund NP) in this linear combination,with the coefficient approaching zero as ?3 ?
??.?
Narrower generalization weights such as ?4 and ?5control where PP is likely to be inserted.
To learn thefeature weights is to learn which features of a transfor-mation make it probable or improbable in the language.Note that the vertex labels, graph topology, and arcparameters are language independent.
That is, Figure 1is supposed to represent Universal Grammar: it tells alearner what kinds of generalizations to look for.
Thelanguage-specific part is ?, which specifies which gener-alizations and exceptions help to model the data.6 The PriorThe model has more parameters than data.
Why?
Beyondthe initial weights and generalization weights, in practicewe allow one exception weight (e.g., ?8, ?9) for each rulethat appeared in training data.
(This makes it possible tolearn arbitrary exceptions, as in a Treebank grammar.
)Parameter estimation is nonetheless possible, using aprior to help choose among the many values of ?
that doa reasonable job of explaining the training data.
The priorconstrains the degrees of freedom: while many parame-ters are available in principle, the prior will ensure thatthe data are described using as few of them as possible.The point of reparameterizing a PCFG in terms of ?,as in Figure 1, is precisely that only one parameter isneeded per linguistically salient property of the PCFG.Making ?3 > 0 creates a broadly targeted transforma-tion.
Making ?9 6= 0 or ?1 6= 0 lists an idiosyncratic rule,or class of rules, together with other rules derived fromthem.
But it takes more parameters to encode less sys-tematic properties, such as narrowly targeted edit trans-formations (?4, ?5) or families of unrelated exceptions.A natural prior for the parameter vector ?
?
Rk istherefore specified in terms of a variance ?2.
We simplysay that the weights ?1, ?2, .
.
.
?k are independent sam-ples from the normal distribution with mean 0 and vari-ance ?2 > 0 (Chen and Rosenfeld, 1999):?
?
N(0, ?2)?N(0, ?2)?
?
?
?
?N(0, ?2) (3)or equivalently, that ?
is drawn from a multivariate Gaus-sian with mean ~0 and diagonal covariance matrix ?2I ,i.e., ?
?
N(~0, ?2I).This says that a priori, the learner expects most fea-tures in Figure 1 to have weights close to zero, i.e., to beirrelevant.
Maximizing p(?)
?
p(D | ?)
means findinga relatively small set of features that adequately describethe rules and exceptions of the grammar.
Reducing thevariance ?2 strengthens this bias toward simplicity.For example, if Sfund ?
To fund NP PP andSmerge ?
To fund NP PP are both observed moreoften than the current p?
distribution predicts, then thelearner can follow either (or both) of two strategies: raise?8 and ?9, or raise ?3.
The former strategy fits the trainingdata only; the latter affects many disparate arcs and leadsto generalization.
The latter strategy may harm p(D | ?
)but is preferred by the prior p(?)
because it uses one pa-rameter instead of two.
If more than two words act likemerge and fund, the pressure to generalize is stronger.7 Perturbation ParametersIn experiments, we have found that a slight variation onthis model gets slightly better results.
Let ?e denote theexception weight (if any) that allows one to tune the prob-ability of rule e. We eliminate ?e and introduce a differentparameter pie, called a perturbation, which is used in thefollowing replacements for equations (1) and (2):I?
(e) = ?e,START +?e?I?(e?)
?
exppie ?
p(e?
?
e)(4)p?
(e) = I?
(e) ?
exppie ?
p(e?
HALT)/Z (5)where Z is a global normalizing factor chosen so that?e p?
(e) = 1.
The new prior on pie is the same as theold prior on ?e.Increasing either ?e or pie will raise p?
(e); the learnermay do this to account for observations of e in trainingdata.
The probabilities of other rules consequently de-crease so that?e p?
(e) = 1.
When pie is raised, allrules?
probabilities are scaled down slightly and equally(because Z increases).
When ?e is raised, e steals proba-bility from its siblings,11 but these are similar to e so tendto appear in test data if e is in training data.
Raising ?ewithout disproportionately harming e?s siblings requiresmanipulation of many other parameters, which is discour-aged by the prior and may also suffer from search error.We speculate that this is why pie works better.11Raising the probability of an arc from e?
to e decreases theprobabilities of arcs from e?
to siblings of e, as they sum to 1.
(Insert) (Insert, target)(Insert, left) (Insert, target, left)(Insert, right) (Insert, target, right)(Insert, left, right)(Insert, side) (Insert, side, target)(Insert, side, left) (Insert, side, target, left)(Insert, side, right) (Insert, side, target, right)(Insert, side, left, right)If the arc insertsAdv after TOin TO fund PP,thentarget=Advleft=TOright=?
?side=left of headTable 2: Each Insert arc has 14 features.
The features of anygiven arc are found by instantiating the tuples above, as shown.Each instantiated tuple has a weight specified in ?.S?
?
?
?
rules only train dev testTreebank sections 0?15 16 17sentences 15554 1343 866rule tokens 18836 1588 973rule types 11565 1317 795frame types 2722 564 365headword types 3607 756 504novel rule tokens 51.6% 47.8%novel frame tokens 8.9% 6.3%novel headword tokens 10.4% 10.2%novel rule types 61.4% 57.5%novel frame types 24.6% 16.4%novel headword types 20.9% 18.8%nonterminal types 78# transformations applicable to 158n?1 158n?1 158n?1rule with RHS length = nTable 3: Properties of the experimental data.
?Novel?
meansnot observed in training.
?Frame?
was defined in section 4.8 Evaluation12To evaluate the quality of generalization, we used pre-parsed training data D and testing data E (Table 3).Each dataset consisted of a collection of flat rules such asSput ?
NP put NP PP extracted from the Penn Tree-bank (Marcus et al, 1993).
Thus, p(D | ?,pi) andp(E | ?,pi) were each defined as a product of rule prob-abilities of the form p?,pi(NP put NP PP | Sput).The learner attempted to maximize p(?,pi) ?
p(D |?,pi) by gradient ascent.
This amounts to learning thegeneralizations and exceptions that related the trainingrules D. The evaluation measure was then the perplex-ity on test data, ?
log2 p(E | ?,pi)/|E| .
To get a good(low) perplexity score, the model had to assign reason-able probabilities to the many novel rules in E (Table 3).For many of these rules, even the frame was novel.Note that although the training data was preparsed intorules, it was not annotated with the paths in Figure 1 thatgenerated those rules, so estimating ?
and pi was still anunsupervised learning problem.The transformation graph had about 14 features per arc(Table 2).
In the finite part of the transformation graphthat was actually explored (including bad arcs that com-pete with good ones), about 70000 distinct features wereencountered, though after training, only a few hundred12See (Eisner, 2001) for full details of data preparation,model structure, parameter initialization, backoff levels for thecomparison models, efficient techniques for computing the ob-jective and its gradient, and more analysis of the results.Treebank/Markovbasic Katz one-countaflat non-flatb flat flat non-flat(a) Treebank ?
?1-gram 1774.9 86435.1 340.9 160.0 193.22-gram 135.2 199.3 127.2 116.2 174.73-gram 136.5 177.4 132.7 123.3 174.8Collinsc 363.0 494.5 197.9transformation 108.6averagedd 102.3(b) 1-gram 1991.2 96318.8 455.1 194.3 233.12-gram 162.2 236.6 153.2 138.8 205.63-gram 161.9 211.0 156.8 145.7 208.1Collins 414.5 589.4 242.0transformation 124.8averaged 118.0aBack off from Treebank grammar with Katz vs. one-countbackoff (Chen and Goodman, 1996) (Note: One-count was al-ways used for backoff within the n-gram and Collins models.
)bSee section 2 for discussioncCollins (1997, model 2)dAverage of transformation model with best other modelTable 4: Perplexity of the test set under various models.
(a) Fulltraining set.
(b) Half training set (sections 0?7 only).feature weights were substantial, and only a few thousandwere even far enough from zero to affect performance.There was also a parameter pie for each observed rule e.Results are given in Table 4a, which compares thetransformation model to various competing models dis-cussed in section 2.
The best (smallest) perplexities ap-pear in boldface.
The key results:?
The transformation model was the winner, reducingperplexity by 20% over the best model replicated fromprevious literature (a bigram model).?
Much of this improvement could be explained bythe transformation model?s ability to model exceptions.Adding this ability more directly to the bigram model,using the new Treebank/Markov approach of section 2,also reduced perplexity from the bigram model, by 6%or 14% depending on whether Katz or one-count backoffwas used, versus the transformation model?s 20%.?
Averaging the transformation model with the best com-peting model (Treebank/bigram) improved it by an addi-tional 6%.
So using transformations yields a total per-plexity reduction of 12% over Treebank/bigram, and 24%over the best previous model from the literature (bigram).?
What would be the cost of achieving such a perplexityimprovement by additional annotation?
Training the av-eraged model on only the first half of the training set, withno further tuning of any options (Table 4b), yielded a testset perplexity of 118.0.
So by using transformations, wecan achieve about the same perplexity as the best modelwithout transformations (Treebank/bigram, 116.2), usingonly half as much training data.?
Furthermore, comparing Tables 4a and 4b shows thatthe transformation model had the most graceful perfor-mance degradation when the dataset was reduced in size.1e?10 1e?07 1e?04 1e?011e?101e?071e?041e?01p(rule| headword):averagedtransf.5e?04 5e?03 5e?02 5e?015e?045e?035e?025e?01p(rule | headword): Treebank/bigram0.001 0.010 0.100 1.0000.0010.0100.1001.000Figure 2: Probabilities of test set flat rules under the averaged model, plotted against the corresponding probabilities under thebest transformation-free model.
Improvements fall above the main diagonal; dashed diagonals indicate a factor of two.
The threelog-log plots (at different scales!)
partition the rules by the number of training observations: 0 (left graph), 1 (middle), ?
2 (right).This is an encouraging result for the use of the methodin less supervised contexts (although results on a noisydataset would be more convincing in this regard).?
The competing models from the literature are best usedto predict flat rules directly, rather than by summing overtheir possible non-flat internal structures, as has beendone in the past.
This result is significant in itself.
Ex-tending Johnson (1998), it shows the inappropriateness ofthe traditional independence assumptions that build up aframe by several rule expansions (section 2).Figure 2 shows that averaging the transformationmodel with the Treebank/bigram model improves the lat-ter not merely on balance, but across the board.
In otherwords, there is no evident class of phenomena for whichincorporating transformations would be a bad idea.?
Transformations particularly helped raise the estimatesof the low-probability novel rules in test data, as hoped.?
Transformations also helped on test rules that hadbeen observed once in training with relatively infrequentwords.
(In other words, the transformation model doesnot discount singletons too much.)?
Transformations hurt slightly on balance for rules ob-served more than once in training, but the effect was tiny.All these differences are slightly exaggerated if one com-pares the transformation model directly with the Tree-bank/bigram model, without averaging.The transformation model was designed to use editoperations in order to generalize appropriately from aword?s observed frames to new frames that are likely toappear with that word in test data.
To directly test themodel?s success at such generalization, we compared itto the bigram model on a pseudo-disambiguation task.Each instance of the task consisted of a pair of rulesfrom test data, expressed as (word, frame) pairs (w1, f1)and (w2, f2), such that f1 and f2 are ?novel?
frames thatdid not appear in training data (with any headword).Each model was then asked: Does f1 go with w1 andf2 with w2, or vice-versa?
In other words, which is big-ger, p(f1 | w1) ?
p(f2 | w2) or p(f2 | w1) ?
p(f1 | w2)?Since the frames were novel, the model had to makethe choice according to whether f1 or f2 looked morelike the frames that had actually been observed with w1in the past, and likewise w2.
What this means dependson the model.
The bigram model takes two frames tolook alike if they contain many bigrams in common.
Thetransformation model takes two frames to look alike ifthey are connected by a path of probable transformations.The test data contained 62 distinct rules (w, f) inwhich f was a novel frame.
This yielded 62?612 = 1891pairs of rules, leading to 1811 task instances after obvi-ous ties were discarded.13Baseline performance on this difficult task is 50% (ran-dom guess).
The bigram model chose correctly in 1595of the 1811 instances (88.1%).
Parameters for ?memo-rizing?
specific frames do not help on this task, which in-volves only novel frames, so the Treebank/bigram modelhad the same performance.
By contrast, the transforma-tion model got 1669 of 1811 correct (92.2%), for a more-than-34% reduction in error rate.
(The development setshowed similar results.)
However, since the 1811 taskinstances were derived non-independently from just 62novel rules, this result is based on a rather small sample.9 DiscussionThis paper has presented a nontrivial way to reparameter-ize a PCFG in terms of ?deep?
parameters representingtransformations and exceptions.
A linguistically sensibleprior was natural to define over these deep parameters.Famous examples of ?deep reparameterization?
are theFourier transform in speech recognition and the SVDtransform for Latent Semantic Analysis in IR.
Like ourtechnique, they are intended to reveal significant structurethrough the leading parameters while relegating noise andexceptions to minor parameters.
Such representations13An obvious tie is an instance where f1 = f2, or whereboth w1 and w2 were novel headwords.
(The 62 rules included11 with novel headwords.)
In such cases, neither the bigram northe transformation model has any basis for making its decision:the probabilities being compared will necessarily be equal.make it easier to model the similarity or probability of theobjects at hand (waveforms, documents, or grammars).Beyond the fact that it shows at least a good perplex-ity improvement (it has not yet been applied to a realtask), an exciting ?big idea?
aspect of this work is itsflexibility in defining linguistically sensible priors overgrammars.
Our reparameterization is made with refer-ence to a user-designed transformation graph (Figure 1).The graph need not be confined to edit distance transfor-mations, or to the simple features of Table 2 (used herefor comparability with the Markov models), which con-dition a transformation?s probability on local context.In principle, the approach could be used to capturea great many linguistic phenomena.
Figure 1 could beextended with more ambitious transformations, such asgapping, gap-threading, and passivization.
The flat rulescould be annotated with internal structure (as in TAG) andthematic roles.
Finally, the arcs could bear further fea-tures.
For example, the probability of unaccusative move-ment (someone sank the boat?
the boat sank) should de-pend on whether the headword is a change-of-state verb.Indeed, Figure 1 can be converted to any lexicalizedtheory of grammar, such as categorial grammar, TAG,LFG, HPSG, or Minimalism.
The vertices represent lex-ical entries and the arcs represent probabilistic lexical re-dundancy rules or metarules (see footnote 8).
The trans-formation model approach is therefore a full stochas-tic treatment of lexicalized syntax?
apparently the firstto treat lexical redundancy rules, although (Briscoe andCopestake, 1999) give an ad hoc approach.
See (Eisner,2001; Eisner, 2002a) for more discussion.It is worthwhile to compare the statistical approachhere with some other approaches:?
Transformation models are similar to graphical mod-els: they allow similar patterns of deductive and abduc-tive inference from observations.
However, the verticesof a transformation graph do not represent different ran-dom variables, but rather mutually exclusive values of thesame random variable, whose probabilities sum to 1.?
Transformation models incorporate conditional log-linear (maximum entropy) models.
As an alternative,one could directly build a conditional log-linear modelof p(RHS | LHS).
However, such a model would learnprobabilities, not relationships.
A feature weight wouldnot really model the strength of the relationship betweentwo frames e, e?
that share that feature.
It would only in-fluence both frames?
probabilities.
If the probability of ewere altered by some unrelated factor (e.g., an exceptionweight), then the probability of e?
would not respond.?
A transformation model can be regarded as a proba-bilistic FSA that consists mostly of -transitions.
(Rulesare only emitted on the arcs to HALT.)
This perspectiveallows use of generic methods for finite-state parameterestimation (Eisner, 2002b).
We are strongly interested inimproving the speed of such methods and their ability toavoid local maxima, which are currently the major diffi-culty with our system, as they are for many unsupervisedlearning techniques.
We expect to further pursue trans-formation models (and simpler variants that are easier toestimate) within this flexible finite-state framework.The interested reader is encouraged to look at (Eisner,2001) for a much more careful and wide-ranging discus-sion of transformation models, their algorithms, and theirrelation to linguistic theory, statistics, and parsing.
Chap-ter 1 provides a good overview.
For a brief article high-lighting the connection to linguistics, see (Eisner, 2002a).ReferencesHiyan Alshawi.
1996.
Head automata for speech translation.In Proceedings of ICSLP, Philadelphia, PA.T.
Briscoe and A. Copestake.
1999.
Lexical rules in constraint-based grammar.
Computational Linguistics, 25(4):487?526.Bob Carpenter.
1991.
The generative power of categorial gram-mars and head-driven phrase structure grammars with lexicalrules.
Computational Linguistics, 17(3):301?313.Glenn Carroll and Mats Rooth.
1998.
Valence induction with ahead-lexicalized PCFG.
In Proceedings of EMNLP.Eugene Charniak.
1997.
Statistical parsing with a context-freegrammar and word statistics.
In Proc.
of AAAI, 598?603.Eugene Charniak.
2000.
A maximum-entropy inspired parser.In Proceedings of NAACL.Stanley Chen and Joshua Goodman.
1996.
An empirical studyof smoothing techniques.
In Proceedings of ACL.Stanley F. Chen and Ronald Rosenfeld.
1999.
A Gaussian priorfor smoothing maximum entropy models.
Technical ReportCMU-CS-99-108, Carnegie Mellon University, February.Stanley Chen.
1996.
Building Probabilistic Models for NaturalLanguage.
Ph.D. thesis, Harvard University.Michael J. Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In Proceedings of ACL/EACL, 16?23.Carl De Marcken.
1996.
Unsupervised Language Acquisition.Ph.D.
thesis, MIT.Jason Eisner.
1996.
Three new probabilistic models for depen-dency parsing: An exploration.
Proc.
of COLING, 340?345.Jason Eisner.
2001.
Smoothing a Probabilistic Lexicon via Syn-tactic Transformations.
Ph.D. thesis, Univ.
of Pennsylvania.Jason Eisner.
2002a.
Discovering syntactic deep structure viaBayesian statistics.
Cognitive Science, 26(3), May.Jason Eisner.
2002b.
Parameter estimation for probabilisticfinite-state transducers.
In Proceedings of the 40th ACL.P.
Gru?nwald.
1996.
A minimum description length approachto grammar inference.
In S. Wermter et al, eds., Symbolic,Connectionist and Statistical Approaches to Learning forNLP, no.
1040 in Lecture Notes in AI, pages 203?216.Mark Johnson.
1998.
PCFG models of linguistic tree represen-tations.
Computational Linguistics, 24(4):613?632.Beth Levin.
1993.
English Verb Classes and Alternations: APreliminary Investigation.
University of Chicago Press.M.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.
1993.Building a large annotated corpus of English: The Penn Tree-bank.
Computational Linguistics, 19(2):313?330.Miles Osborne and Ted Briscoe.
1997.
Learning stochastic cat-egorial grammars.
In Proceedings of CoNLL, 80?87.
ACL.A.
Stolcke and S.M.
Omohundro.
1994.
Inducing probabilisticgrammars by Bayesian model merging.
In Proc.
of ICGI.
