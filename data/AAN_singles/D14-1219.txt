Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2049?2060,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsDiscriminative Reranking of Discourse Parses Using Tree KernelsShafiq Joty and Alessandro MoschittiALT Research GroupQatar Computing Research Institute{sjoty,amoschitti}@qf.org.qaAbstractIn this paper, we present a discrimina-tive approach for reranking discourse treesgenerated by an existing probabilistic dis-course parser.
The reranker relies on treekernels (TKs) to capture the global depen-dencies between discourse units in a tree.In particular, we design new computa-tional structures of discourse trees, whichcombined with standard TKs, originatenovel discourse TKs.
The empirical evalu-ation shows that our reranker can improvethe state-of-the-art sentence-level parsingaccuracy from 79.77% to 82.15%, a rel-ative error reduction of 11.8%, which inturn pushes the state-of-the-art document-level accuracy from 55.8% to 57.3%.1 IntroductionClauses and sentences in a well-written text areinterrelated and exhibit a coherence structure.Rhetorical Structure Theory (RST) (Mann andThompson, 1988) represents the coherence struc-ture of a text by a labeled tree, called discoursetree (DT) as shown in Figure 1.
The leaves cor-respond to contiguous clause-like units called ele-mentary discourse units (EDUs).
Adjacent EDUsand larger discourse units are hierarchically con-nected by coherence relations (e.g., ELABORA-TION, CAUSE).
Discourse units connected by a re-lation are further distinguished depending on theirrelative importance: nuclei are the core parts of therelation while satellites are the supportive ones.Conventionally, discourse analysis in RST in-volves two subtasks: (i) discourse segmentation:breaking the text into a sequence of EDUs, and(ii) discourse parsing: linking the discourse unitsto form a labeled tree.
Despite the fact that dis-course analysis is central to many NLP appli-cations, the state-of-the-art document-level dis-course parser (Joty et al., 2013) has an f -scoreof only 55.83% using manual discourse segmen-tation on the RST Discourse Treebank (RST-DT).Although recent work has proposed rich lin-guistic features (Feng and Hirst, 2012) and pow-erful parsing models (Joty et al., 2012), discourseparsing remains a hard task, partly because theseapproaches do not consider global features andlong range structural dependencies between DTconstituents.
For example, consider the human-annotated DT (Figure 1a) and the DT generated bythe discourse parser of Joty et al.
(2013) (Figure1b) for the same text.
The parser makes a mistakein finding the right structure: it considers only e3as the text to be attributed to e2, where all the textspans from e3to e6(linked by CAUSE and ELAB-ORATION) compose the statement to be attributed.Such errors occur because existing systems do notencode long range dependencies between DT con-stituents such as those between e3and e4?6.Reranking models can make the global struc-tural information available to the system as fol-lows: first, a base parser produces several DThypotheses; and then a classifier exploits the en-tire information in each hypothesis, e.g., the com-plete DT with its dependencies, for selecting thebest DT.
Designing features capturing such globalproperties is however not trivial as it requires theselection of important DT fragments.
This meansselecting subtree patterns from an exponential fea-ture space.
An alternative approach is to implicitlygenerate the whole feature space using tree kernels(TKs) (Collins and Duffy, 2002; Moschitti, 2006).In this paper, we present reranking models fordiscourse parsing based on Support Vector Ma-chines (SVMs) and TKs.
The latter allows usto represent structured data using the substructurespace thus capturing structural dependencies be-tween DT constituents, which is essential for ef-fective discourse parsing.
Specifically, we madethe following contributions.
First, we extend the2049Topic-CommentAttributionCauseElaborationElaborationeeeee e2345 61(a) A human-annotated discourse tree.BackgroundAttributionCauseElaborationElaborationee e ee e2 3 45 61(b) A discourse tree generated by Joty et al.
(2013).Figure 1: Example of human-annotated and system-generated discourse trees for the text [what?s more,]e1[he believes]e2[seasonal swings in the auto industry this year aren?t occurring at the same time in the past,]e3[because of production and pric-ing differences]e4[that are curbing the accuracy of seasonal adjustments]e5] [built into the employment data.
]e6Horizontallines indicate text segments; satellites are connected to their nuclei by curved arrows.existing discourse parser1(Joty et al., 2013) toproduce a list of k most probable parses for eachinput text, with associated probabilities that definethe initial ranking.Second, we define a set of discourse tree ker-nels (DISCTK) based on the functional composi-tion of standard TKs with structures representingthe properties of DTs.
DISCTK can be used forany classification task involving discourse trees.Third, we use DISCTK to define kernels forreranking and use them in SVMs.
Our rerankerscan exploit the complete DT structure using TKs.They can ascertain if portions of a DT are compat-ible, incompatible or simply not likely to coexist,since each substructure is an exploitable feature.In other words, problematic DTs are expected tobe ranked lower by our reranker.Finally, we investigate the potential of our ap-proach by computing the oracle f -scores for bothdocument- and sentence-level discourse parsing.However, as demonstrated later in Section 6, fordocument-level parsing, the top k parses oftenmiss the best parse.
For example, the oracle f -scores for 5- and 20-best document-level parsingare only 56.91% and 57.65%, respectively.
Thusthe scope of improvement for the reranker is rathernarrow at the document level.
On the other hand,the oracle f -score for 5-best sentence-level dis-course parsing is 88.09%, where the base parser(i.e., 1-best) has an oracle f -score of 79.77%.Therefore, in this paper we address the followingtwo questions: (i) how far can a reranker improvethe parsing accuracy at the sentence level?
and(ii) how far can this improvement, if at all, pushthe (combined) document-level parsing accuracy?To this end, our comparative experiments on1Available from http://alt.qcri.org/tools/RST-DT show that the sentence-level reranker canimprove the f -score of the state-of-the-art from79.77% to 82.15%, corresponding to a relativeerror reduction of 11.8%, which in turn pushesthe state-of-the-art document-level f -score from55.8% to 57.3%, an error reduction of 3.4%.In the rest of the paper, after introducing the TKtechnology in Section 2, we illustrate our novelstructures, and how they lead to the design ofnovel DISCTKs in Section 3.
We present the k-best discourse parser in Section 4.
In Section 5, wedescribe our reranking approach using DISCTKs.We report our experiments in Section 6.
We brieflyoverview the related work in Section 7, and finally,we summarize our contributions in Section 8.2 Kernels for Structural RepresentationTree kernels (Collins and Duffy, 2002; Shawe-Taylor and Cristianini, 2004; Moschitti, 2006) area viable alternative for representing arbitrary sub-tree structures in learning algorithms.
Their ba-sic idea is that kernel-based learning algorithms,e.g., SVMs or perceptron, only need the scalarproduct between the feature vectors representingthe data instances to learn and classify; and kernelfunctions compute such scalar products in an effi-cient way.
In the following subsections, we brieflydescribe the kernel machines and three types oftree kernels (TKs), which efficiently compute thescalar product in the subtree space, where the vec-tor components are all possible substructures ofthe corresponding trees.2.1 Kernel MachinesKernel Machines (Cortes and Vapnik, 1995), e.g.,SVMs, perform binary classification by learninga hyperplane H(~x) = ~w ?
~x + b = 0, where2050Chapter 2.
Background 15abc eg)abc egbc eg c eFigure 2.4: A tree (left) and all of its proper subtrees (right).abc eg)abc egab gc egbc eFigure 2.5: A tree (left) and all of its subset trees (right).Proper Subtree A proper subtree ticomprises node vialong with all of its de-scendants (see figure 2.4 for an example of a tree along with all its proper subtrees).Subset Tree A subset tree is a subtree for which the following constraint is sat-isfied: either all of the children of a node belong to the subset tree or none of them.The reason for adding such a constraint can be understood by considering the factthat subset trees were defined for measuring the similarity of parse trees in naturallanguage applications.
In that context a node along with all of its children representa grammar production.
Figure 2.5 gives an example of a tree along with some of itssubset trees.Figure 2: A tree with its STK subtrees; STKbalso includesleaves as features.~x ?
Rnis the feature vector representation of anobject o ?
O to be classified and ~w ?
Rnandb ?
R are parameters learned from the trainingdata.
One can train such machines in the dualspace by rewri ing the model parameter ~w as a l n-ear combination of training examples, i.e., ~w =?i=1..lyi?i~xi, where yiis equal to 1 for positiveexam les and ?1 for negative example , ?i?
R+and ~xi?i ?
{1, .., l} are the training instances.Then, we can use the data object oi?
O directlyin the hyperplane equation considering their map-ping function ?
: O ?
Rn, as follows: H(o) =?i=1..lyi?i~xi?~x+b =?i=1..lyi?i?
(oi) ??
(o)+b =?i=1..lyi?iK(oi, o) + b, where the productK(oi, o) = ??
(oi) ?
?(o)?
is the kernel function(e.g., TK) associated with the mapping ?.2.2 Tree KernelsConvolution TKs compute the number of com-mon tree fragments between two trees T1and T2without explicitly considering the whole fragmentspace.
A TK function over T1and T2is defined as:TK(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2),where NT1and NT2are the sets of the nodes ofT1and T2, respectively, and ?
(n1, n2) is equalto the number of common fragments rooted inthe n1and n2nodes.2The computation of ?function depends on the shape of fragments,conversely, a different ?
determines the richnessof the kernel space and thus different tree kernels.In the following, we briefly describe two existingand well-known tree kernels.
Please see severaltutorials on kernels (Moschitti, 2013; Moschitti,2012; Moschitti, 2010) for more details.3Syntactic Tree Kernels (STK) produce fragmentssuch that each of their nodes includes all or noneof its children.
Figure 2 shows a tree T and itsthree fragments (do not consider the single nodes)in the STK space on the left and right of the ar-2To get a similarity score between 0 and 1, it iscommon to apply a normalization in the kernel space,i.e.TK(T1,T2)?TK(T1,T1)?TK(T2,T2).3Tutorials notes available at http://disi.unitn.it/moschitti/14 Chapter 2.
Backgroundabc eg2 331Figure 2.2: A positional Tree.
The number over an arc represents the position ofthe node with respect to its parent.abc eg)abc egab gc eg agbc eabc eabeFigure 2.3: A tree (left) and some of its subtrees (right).node.
The maximum out-degree of a tree is the highest index of all the nodes of thetree.
The out-degree of a node for an ordered tree corresponds to the number of itschildren.
The depth of a node viwith respect to one of its ascendants vjis definedas the number of nodes comprising the path from vjto vi.
When not specified, thenode with respect to the depth is computed, is the root.A tree can be decomposed in many types of substructures.Subtree A subtree t is a subset of nodes in the tree T , with corresponding edges,which forms a tree.
A subtree rooted at node viwill be indicated with ti, while asubtree rooted at a generic node v will be indicated by t(v).
When t is used in acontext where a node is expected, t refers to the root node of the subtree t. Theset of subtrees of a tree will be indicated by NT.
When clear from the context NTmay refer to specific type of subtrees.
Figure 2.3 gives an example of a tree togetherwith its subtrees.
Various types of subtrees can be defined for a tree T .Figure 3: A tree with its PTK fragments.row, respectively.
STK(T ,T ) counts the numberof co mon fragments, which in this case is theumber of subtrees of T , i.e., three.
In the figure,we also show three single nodes, c, e, and g, i.e.,the leaves of T , which are computed by a vari-ant of the kernel, that we call STKb.
The com-putati nal complexity of STK is O(|NT1||NT2|),but the average running time tends to be linear(i.e.
O(|NT1| + |NT2|)) for syntactic trees (Mos-chitti, 2006).Partial Tree Kernel (PTK) generates a richer setof tree fragments.
Given a target tree T , PTKcan generate any subset of connected nodes of T ,whose edges are in T .
For example, Figure 3shows a tree with its nine fragments including allsingle nodes (i.e., the leaves of T ).
PTK is moregeneral than STK as its fragments can include anysubsequence of children of a target node.
The timecomplexity of PTK is O(p?2|NT1||NT2|), wherep is the largest subsequence of children that onewants to consider and ?
is the maximal out-degreeobserved in the two trees.
However, the averagerunning time again tends to be linear for syntactictrees (Moschitti, 2006).3 Discourse Tree Kernels (DISCTK)Engineering features that can capture the depen-dencies between DT constituents is a difficult task.In principle, any dependency between words, rela-tions and structures (see Figure 1) can be an im-portant feature for discourse parsing.
This maylead to an exponential number of features, whichmakes the feature engineering process very hard.The standard TKs described in the previous sec-tion serve as a viable option to get useful sub-tree features automatically.
However, the defini-tion of the input to a TK, i.e., the tree represent-ing a training instance, is extremely important asit implicitly affects the subtree space generated bythe TK, where the target learning task is carriedout.
This can be shown as follows.
Let ?M()be a mapping from linguistic objects oi, e.g., adiscourse parse, to a meaningful tree Ti, and let?TK() be a mapping into a tree kernel space us-2051(a) JRN(b) SRNFigure 4: DISCTK trees: (a) Joint Relation-Nucleus (JRN), and (b) Split Relation Nucleus (SRN).ing one of the TKs described in Section 2.2, i.e.,TK(T1, T2) = ?TK(T1) ?
?TK(T2).
If we applyTK to the objects oitransformed by ?M(), weobtain TK(?M(o1), ?M(o2)) = ?TK(?M(o1)) ??TK(?M(o2))=(?TK??M)(o1)?(?TK?
?M)(o2)= DiscTK(o1, o2), which is a new kernel4in-duced by the mapping ?DiscTK=(?TK?
?M).We define two different mappings ?Mto trans-form the discourse parses generated by the baseparser into two different tree structures: (i) theJoint Relation-Nucleus tree (JRN), and (ii) theSplit Relation Nucleus tree (SRN).3.1 Joint Relation-Nucleus Tree (JRN)As shown in Figure 4a, JRN is a direct mappingof the parser output, where the nuclearity statuses(i.e., satellite or nucleus) of the connecting nodesare attached to the relation labels.5For example,the root BACKGROUNDSatellite?Nucleusin Figure4a denotes a Background relation between a satel-lite discourse unit on the left and a nucleus unit onthe right.
Text spans (i.e., EDUs) are representedas sequences of Part-of-Speech (POS) tags con-nected to the associated words, and are groupedunder dummy SPAN nodes.
We experiment withtwo lexical variations of the trees: (i) All includesall the words in the EDU, and (ii) Bigram includesonly the first and last two words in the EDU.When JRN is used with the STK kernel, an ex-ponential number of fragments are generated.
Forexample, the upper row of Figure 5 shows two4People interested in algorithms may like it more design-ing a complex algorithm to compute(?TK??M).
However,the design of ?Mis conceptually equivalent and more effec-tive from an engineering viewpoint.5This is a common standard followed by the parsers.smallest (atomic) fragments and one subtree com-posed of two atomic fragments.
Note that muchlarger structures encoding long range dependen-cies are also part of the feature space.
These frag-ments can reveal if the discourse units are orga-nized in a compatible way, and help the rerankerto detect the kind of errors shown earlier in Fig-ure 1b.
However, one problem with JRN repre-sentation is that since the relation nodes are com-posed of three different labels, the generated sub-trees tend to be sparse.
In the following, we de-scribe SRN that attempts to solve this issue.3.2 Split Relation Nucleus Tree (SRN)SRN is not very different from JRN as shown inFigure 4b.
The only difference is that instead ofattaching the nuclearity statuses to the relation la-bels, in this representation we assign them to theirrespective discourse units.
When STK kernel isapplied to SRN it again produces an exponentialnumber of fragments.
For example, the lower rowof Figure 5 shows two atomic fragments and onesubtree composed of two atomic fragments.
Com-paring the two examples in Figure 5, it is easyto understand that the space of subtrees extractedfrom SRN is less sparse than that of JRN.Note that, as described in Secion 2.2, when thePTK kernel is applied to JRN and SRN trees, it cangenerate a richer feature space, e.g., features thatare paths containing relation labels (e.g., BACK-GROUND - CAUSE - ELABORATION or ATTRIBU-TION - CAUSE - ELABORATION).4 Generation of k-best Discourse ParsesIn this section we describe the 1-best discourseparser of Joty et al.
(2013), and how we extend2052Figure 5: Fragments from JRN in Figure 4a (upper row) and SRN in Figure 4b (lower row).it to k-best discourse parsing.Joty et al.
(2013) decompose the problem ofdocument-level discourse parsing into two stagesas shown in Figure 6.
In the first stage, the intra-sentential discourse parser produces discoursesubtrees for the individual sentences in a docu-ment.
Then the multi-sentential parser combinesthe sentence-level subtrees and produces a DT forthe document.
Both parsers have the same twocomponents: a parsing model and a parsing al-gorithm.
The parsing model explores the searchspace of possible DTs and assigns a probability toevery possible DT.
Then the parsing algorithm se-lects the most probable DT(s).
While two separateparsing models are employed for intra- and multi-sentential parsing, the same parsing algorithm isused in both parsing conditions.
The two-stageparsing exploits the fact that sentence boundariescorrelate very well with discourse boundaries.
Forexample, more than 95% of the sentences in RST-DT have a well-formed discourse subtree in thefull document-level discourse tree.The choice of using two separate models forintra- and multi-sentential parsing is well justifiedfor the following two reasons: (i) it has been ob-served that discourse relations have different dis-tributions in the two parsing scenarios, and (ii) themodels could independently pick their own infor-mative feature sets.
The parsing model used forintra-sentential parsing is a Dynamic ConditionalRandom Field (DCRF) (Sutton et al., 2007) shownin Figure 7.
The observed nodes Ujat the bottomlayer represent the discourse units at a certain levelof the DT; the binary nodes Sjat the middle layerpredict whether two adjacent units Uj?1and Ujshould be connected or not; and the multi-classnodes Rjat the top layer predict the discourserelation between Uj?1and Uj.
Notice that themodel represents the structure and the label of aDT constituent jointly, and captures the sequentialdependencies between the DT constituents.
Sincethe chain-structured DCRF model does not scaleup to multi-sentential parsing of long documents,ModelA lgorithmSentences segmentedinto EDUsDocument-leveldiscourse treeModelA lgorithmMulti-sentential parserIntra-sentential parserFigure 6: The two-stage document-level discourse parserproposed by Joty et al.
(2013).U UU U U2223 j t-1 tSS S S SR R R R R33 jj t-1t-1 tU1tRelation sequenceStructuresequenceUnit sequence at level iFigure 7: The intra-sentential parsing model.the multi-sentential parsing model is a CRF whichbreaks the chain structure of the DCRF model.The parsing models are applied recursively atdifferent levels of the DT in their respective pars-ing scenarios (i.e., intra- and multi-sentential),and the probabilities of all possible DT con-stituents are obtained by computing the posteriormarginals over the relation-structure pairs (i.e.,P (Rj, Sj=1|U1, ?
?
?
, Ut,?
), where ?
are modelparameters).
These probabilities are then used ina CKY-like probabilistic parsing algorithm to findthe globally optimal DT for the given text.Let Ubxand Uexdenote the beginning andend EDU Ids of a discourse unit Ux, andR[Ubi, Uem, Uej] refers to a coherence relationR that holds between the discourse unit con-taining EDUs Ubithrough Uemand the unitcontaining EDUs Uem+1 through Uej.
Given ndiscourse units, the parsing algorithm uses theupper-triangular portion of the n?n dynamicprogramming table A, where cell A[i, j] (fori < j) stores:A[i, j] = P (r?
[Ubi, Uem?, Uej]), where(m?, r?)
= argmaxi?m<j ; RP (R[Ubi, Uem, Uej])?A[i,m]?A[m+ 1, j] (1)20531 1 22 23Br1r3r2r2r3r4Cr2r1e1e2r4e3e4Figure 8: The B and C dynamic programming tables (left), and the corresponding discourse tree (right).In addition to A, which stores the probability ofthe most probable constituents of a DT, the pars-ing algorithm also simultaneously maintains twoother tables B and C for storing the best structure(i.e., Uem?)
and the relations (i.e., r?)
of the corre-sponding DT constituents, respectively.
For exam-ple, given 4 EDUs e1?
?
?
e4, the B and C tables atthe left side in Figure 8 together represent the DTshown at the right.
More specifically, to generatethe DT, we first look at the top-right entries in thetwo tables, and find B[1, 4] = 2 and C[1, 4] = r2,which specify that the two discourse units e1:2ande3:4should be connected by the relation r2(theroot in the DT).
Then, we see how EDUs e1ande2should be connected by looking at the entriesB[1, 2] and C[1, 2], and find B[1, 2] = 1 andC[1, 2] = r1, which indicates that these two unitsshould be connected by the relation r1(the leftpre-terminal).
Finally, to see how EDUs e3and e4should be linked, we look at the entriesB[3, 4] andC[3, 4], which tell us that they should be linked bythe relation r4(the right pre-terminal).It is straight-forward to generalize the above al-gorithm to produce k most probable DTs.
Whenfilling up the dynamic programming tables, ratherthan storing a single best parse for each subtree,we store and keep track of k-best candidates si-multaneously.
More specifically, each cell in thedynamic programming tables (i.e., A, B and C)should now contain k entries (sorted by their prob-abilities), and for each such entry there should be aback-pointer that keeps track of the decoding path.The algorithm works in polynomial time.
Forn discourse units and M number of relations, the1-best parsing algorithm has a time complexity ofO(n3M) and a space complexity of O(n2), wherethe k-best version has a time and space complexi-ties ofO(n3Mk2log k) andO(n2k), respectively.There are cleverer ways to reduce the complexity(e.g., see (Huang and Chiang, 2005) for three suchways).
However, since the efficiency of the algo-rithm did not limit us to produce k-best parses forlarger k, it was not a priority in this work.5 Kernels for Reranking Discourse TreesIn Section 3, we described DISCTK, which essen-tially can be used for any classification task involv-ing discourse trees.
For example, given a DT, wecan use DISCTK to classify it as correct vs. in-correct.
However, such classification is not com-pletely aligned to our purpose, since our goal isto select the best (i.e., the most correct) DT fromk candidate DTs; i.e., a ranking task.
We adopta preference reranking technique as described in(Moschitti et al., 2006; Dinarelli et al., 2011).5.1 Preference RerankerPreference reranking (PR) uses a classifier C ofpairs of hypotheses ?hi, hj?, which decides if hi(i.e., a candidate DT in our case) is better thanhj.
We generate positive and negative examples totrain the classifier using the following approach.The pairs ?h1, hi?
constitute positive examples,where h1has the highest f -score accuracy on theRelation metric (to be described in Section 6) withrespect to the gold standard among the candidatehypotheses, and vice versa, ?hi, h1?
are consideredas negative examples.
At test time, C classifies allpairs ?hi, hj?
generated from the k-best hypothe-ses.
A positive decision is a vote for hi, and a neg-ative decision is a vote for hj.
Also, the classifierscore can be used as a weighted vote.
Hypothesesare then ranked according to the number (sum) ofthe (weighted) votes they get.6We build our reranker using simple SVMs.76As shown by Collins and Duffy (2002), only the classifi-cation of k hypotheses (paired with the empty one) is neededin practice, thus the complexity is only O(k).7Structural kernels, e.g., TKs, cannot be used in more ad-vanced algorithms working in structured output spaces, e.g.,SVMstruct.
Indeed, to our knowledge, no one could suc-cessfully find a general and exact solution for the argmaxequation, typically part of such advanced models, when struc-tural kernels are used.
Some approximate solutions for sim-ple kernels, e.g., polynomial or gaussian kernels, are given in(Joachims and Yu, 2009), whereas (Severyn and Moschitti,2011; Severyn and Moschitti, 2012) provide solutions forusing the cutting-plane algorithm (which requires argmaxcomputation) with structural kernels but in binary SVMs.2054Since in our problem a pair of hypotheses ?hi, hj?constitutes a data instance, we now need to definethe kernel between the pairs.
However, notice thatDISCTK only works on a single pair.Considering that our task is to decide whetherhiis better than hj, it can be convenient torepresent the pairs in terms of differences be-tween the vectors of the two hypotheses, i.e.,?K(hi)?
?K(hj), where K (i.e., DISCTK) is de-fined between two hypotheses (not on two pairsof hypotheses).
More specifically, to computethis difference implicitly, we can use the follow-ing kernel summation: PK(?h1, h2?, ?h?1, h?2?)
=(?K(h1) ?
?K(h2)) ?
(?K(h?1) ?
?K(h?2)) =K(h1, h?1)+K(h2, h?2)?K(h1, h?2)?K(h2, h?1).In general, Preference Kernel (PK) works wellbecause it removes many identical features by tak-ing differences between two huge implicit TK-vectors.
In our reranking framework, we also in-clude traditional feature vectors in addition to thetrees.
Therefore, each hypothesis h is representedas a tuple ?T,~v?
composed of a tree T and a fea-ture vector ~v.
We then define a structural kernel(i.e., similarity) between two hypotheses h andh?as follows: K(h, h?)
= DiscTK(T, T?)
+FV (~v,~v?
), where DISCTK maps the DTs T andT?to JRN or SRN and then applies STK, STKborPTK defined in Sections 2.2 and 3, and FV is astandard kernel, e.g., linear, polynomial, gaussian,etc., over feature vectors (see next section).5.2 Feature VectorsWe also investigate the impact of traditional(i.e., not subtree) features for reranking discourseparses.
Our feature vector comprises two types offeatures that capture global properties of the DTs.Basic Features.
This set includes eight globalfeatures.
The first two are the probability andthe (inverse) rank of the DT given by the baseparser.
These two features are expected to helpthe reranker to perform at least as good as the baseparser.
The other six features encode the structuralproperties of the DT, which include depth of theDT, number of nodes connecting two EDUs (i.e.,SPANs in Figure 4), number of nodes connectingtwo relational nodes, number of nodes connectinga relational node and an EDU, number of nodesthat connects a relational node as left child and anEDU as right child, and vice versa.Relation Features.
We encode the relations inthe DT as bag-of-relations (i.e., frequency count).This will allow us to assess the impact of a flat rep-resentation of the DT.
Note that more importantrelational features would be the subtree patternsextracted from the DT.
However, they are alreadygenerated by TKs in a simpler way.
See (Pighinand Moschitti, 2009; Pighin and Moschitti, 2010)for a way to extract the most relevant features froma model learned in the kernel space.6 ExperimentsOur experiments aim to show that reranking ofdiscourse parses is a promising research direction,which can improve the state-of-the-art.
To achievethis, we (i) compute the oracle accuracy of the k-best parser, (ii) test different kernels for rerankingdiscourse parses by applying standard kernels toour new structures, (iii) show the reranking perfor-mance using the best kernel for different numberof hypotheses, and (iv) show the relative impor-tance of features coming from different sources.6.1 Experimental SetupData.
We use the standard RST-DT corpus (Carl-son et al., 2002), which comes with discourse an-notations for 385 articles (347 for training and 38for testing) from the Wall Street Journal.
We ex-tracted sentence-level DTs from a document-levelDT by finding the subtrees that exactly span overthe sentences.
This gives 7321 and 951 sentencesin the training and test sets, respectively.
Follow-ing previous work, we use the same 18 coarser re-lations defined by Carlson and Marcu (2001).We create the training data for the reranker in a5-fold cross-validation fashion.8Specifically, wesplit the training set into 5 equal-sized folds, andtrain the parsing model on 4 folds and apply to therest to produce k most probable DTs for each text.Then we generate and label the pairs (by compar-ing with the gold) from the k most probable treesas described in Section 5.1.
Finally, we merge the5 labeled folds to create the full training data.SVM Reranker.
We use SVM-light-TK to trainour reranking models,9which enables the useof tree kernels (Moschitti, 2006) in SVM-light(Joachims, 1999).
We build our new kernels forreranking exploiting the standard built-in TK func-tions, such as STK, STKband PTK.
We applied8Note that our earlier experiments with a 2-fold cross vali-dation process yielded only 50% of our current improvement.9http://disi.unitn.it/moschitti/Tree-Kernel.htm2055a linear kernel to standard feature vectors as itshowed to be the best on our development set.Metrics.
The standard procedure to evaluate dis-course parsing performance is to compute Pre-cision, Recall and f -score of the unlabeled andlabeled metrics proposed by Marcu (2000b).10Specifically, the unlabeled metric Span measureshow accurate the parser is in finding the rightstructure (i.e., skeleton) of the DT, while the la-beled metrics Nuclearity and Relation measure theparser?s ability to find the right labels (nuclearityand relation) in addition to the right structure.
Op-timization of the Relation metric is considered tobe the hardest and the most desirable goal in dis-course parsing since it gives aggregated evaluationon tree structure and relation labels.
Therefore,we measure the oracle accuracy of the k-best dis-course parser based on the f -scores of the Relationmetric, and our reranking framework aims to op-timize the Relation metric.11Specifically, the ora-cle accuracy for k-best parsing is measured as fol-lows: ORACLE =?Ni=1maxkj=1f?scorer(gi,hji)N, whereN is the total number of texts (sentences or docu-ments) evaluated, giis the gold DT annotation fortext i, hjiis the jthparse hypothesis generated bythe k-best parser for text i, and f -scorer(gi, hji) isthe f -score accuracy of hypothesis hjion the Re-lation metric.
In all our experiments we report thef -scores of the Relation metric.6.2 Oracle AccuracyTable 1 presents the oracle scores of the k-best intra-sentential parser PAR-S on the standardRST-DT test set.
The 1-best result correspondsto the accuracy of the base parser (i.e., 79.77%).The 2-best shows dramatic oracle-rate improve-ment (i.e., 4.65% absolute), suggesting that thebase parser often generates the best tree in itstop 2 outputs.
5-best increases the oracle scoreto 88.09%.
Afterwards, the increase in accuracyslows down, achieving, e.g., 90.37% and 92.57%at 10-best and 20-best, respectively.The results are quite different at the documentlevel as Table 2 shows the oracle scores of the k-best document-level parser PAR-D.12The results10Precision, Recall and f -score are the same when the dis-course parser uses manual discourse segmentation.
Since allour experiments in this paper are based on manual discoursesegmentation, we only report the f -scores.11It is important to note that optimizing Relation metricmay also result in improved Nuclearity scores.12For document-level parsing, Joty et al.
(2013) pro-k 1 2 5 10 15 20PAR-S 79.77 84.42 88.09 90.37 91.74 92.57Table 1: Oracle scores as a function of k of k-best sentence-level parses on RST-DT test set.k 1 2 5 10 15 20PAR-D 55.83 56.52 56.91 57.23 57.54 57.65Table 2: Oracle scores as a function of k of k-bestdocument-level parses on RST-DT test set.suggest that the best tree is often missing in thetop k parses, and the improvement in oracle-rate isvery little as compared to the sentence-level pars-ing.
The 2-best and the 5-best improve over thebase accuracy by only 0.7% and 1.0%, respec-tively.
The improvement becomes even lower forlarger k. For example, the gain from 20-best to30-best parsing is only 0.09%.
This is not sur-prising because generally document-level DTs arebig with many constituents, and only a very fewof them change from k-best to k+1-best parsing.These small changes do not contribute much tothe overall f -score accuracy.13In summary, theresults in Tables 1 and 2 demonstrate that a k-bestreranker can potentially improve the parsing accu-racy at the sentence level, but may not be a suit-able option for improving parsing at the documentlevel.
In the following, we report our results forreranking sentence-level discourse parses.6.3 Performance of Different DISCTKsSection 3 has pointed out that different DISCTKscan be obtained by specifying the TK type (e.g.,STK, STKb, PTK) and the mapping ?M(i.e.,JRN, SRN) in the overall kernel function(?TK??M)(o1)?(?TK??M)(o2).
Table 3 reports the per-formance of such model compositions using the 5-best hypotheses on the RST-DT test set.
Addition-ally, it also reports the accuracy for the two ver-sions of JRN and SRN, i.e., Bigram and All.
Fromthese results, we can note the following.Firstly, the kernels generally perform better onBigram than All lexicalization.
This suggests thatusing all the words from the text spans (i.e., EDUs)produces sparse models.pose two approaches to combine intra- and multi-sententialparsers, namely 1S-1S (1 Sentence-1 Subtree) and Slidingwindow.
In this work we extend 1S-1S to k-best document-level parser PAR-D since it is not only time efficient but italso achieves better results on the Relation metric.13Note that Joty et al.
(2012; 2013) report lower f -scoresboth at the sentence level (i.e., 77.1% as opposed to our79.77%) and at the document level (i.e., 55.73% as opposedto our 55.83%).
We fixed a crucial bug in their (1-best) pars-ing algorithm, which accounts for the improved performance.2056?TK?
?MJRN SRNBigram All Bigram AllSTK 81.28 80.04 82.15 80.04STKb81.35 80.28 82.18 80.25PTK 81.63 78.50 81.42 78.25Table 3: Reranking performance of different discourse treekernels on different representations.Secondly, while the tree kernels perform sim-ilarly on the JRN representation, STK performssignificantly better (p-value < 0.01) than PTKon SRN.14This result is interesting as it pro-vides indications of the type of DT fragments use-ful for improving parsing accuracy.
As pointedout in Section 2.2, PTK includes all featuresgenerated by STK, and additionally, it includesfragments whose nodes can have any subsets ofthe children they have in the original DT.
Sincethis does not improve the accuracy, we speculatethat complete fragments, e.g., [CAUSE [ATTRI-BUTION][ELABORATION]] are more meaningfulthan the partial ones, e.g., [CAUSE [ATTRIBU-TION]] and [CAUSE [ELABORATION]], whichmay add too much uncertainty on the signatureof the relations contained in the DT.
We verifiedthis hypothesis by running an experiment withPTK constraining it to only generate fragmentswhose nodes preserve all or none of their children.The accuracy of such fragments approached theones of STK, suggesting that relation informationshould be used as a whole for engineering features.Finally, STKbis slightly (but not significantly)better than STK suggesting that the lexical infor-mation is already captured by the base parser.Note that the results in Table 3 confirms manyother experiments we carried out on several devel-opment sets.
For any run: (i) STK always performsas well as STKb, (ii) STK is always better thanPTK, and (iii) SRN is always better than JRN.
Inwhat follows, we show the reranking performancebased on STK applied to SRN with Bigram.6.4 Insights on DISCTK-based RerankingTable 4 reports the performance of our reranker(RR) in comparison with the oracle (OR) accuracyfor different values of k, where we also show thecorresponding relative error rate reduction (ERR)with respect to the baseline.
To assess the general-ity of our approach, we evaluated our reranker onboth the standard test set and the entire training setusing 5-fold cross validation.1514Statistical significance is verified using paired t-test.15The reranker was trained on 4 folds and tested on the restBaseline Basic feat.
+ Rel.
feat.
+ Tree79.77 79.84 79.81 82.15Table 5: Comparison of features from different sources for5-best discourse reranking.
(Joty et al., 2013) With RerankerPAR-D 55.8 57.3Table 6: Document-level parsing results with 5-bestsentence-level discourse reranker.We note that: (i) the best result on the standardtest set is 82.15% for k = 4 and 5, which givesan ERR of 11.76%, and significantly (p-value <0.01) outperforms the baseline, (ii) the improve-ment is consistent when we move from standardtest set to 5-folds, (iii) the best result on the 5-foldsis 80.86 for k = 6, which is significantly (p-value< 0.01) better than the baseline 78.57, and givesan ERR of 11.32%.
We also experimented withother values of k in both training and test sets (alsoincreasing k only in the test set), but we could notimprove over our best result.
This suggests thatoutperforming the baseline (which in our case isthe state of the art) is rather difficult.16In this respect, we also investigated the im-pact of traditional ranking methods based on fea-ture vectors, and compared it with our TK-basedmodel.
Table 5 shows the 5-best reranking accu-racy for different feature subsets.
The Basic fea-tures (Section 5.2) alone do not significantly im-prove over the Baseline.
The only relevant fea-tures are the probability and the rank of each hy-pothesis, which condense all the information ofthe local model (TKs models always used them).Similarly, adding the relations as bag-of-relations in the vector (Rel.
feat.)
does not pro-vide any gain, whereas the relations encoded inthe tree fragments (Tree) gives improvement.
Thisshows the importance of using structural depen-dencies for reranking discourse parses.Finally, Table 6 shows that if we use oursentence-level reranker in the document-levelparser of Joty et al.
(2013), the accuracy of the lat-ter increases from 55.8% to 57.3%, which is a sig-nificant improvement (p < 0.01), and establishesa new state-of-the-art for document-level parsing.6.5 Error AnalysisWe looked at some examples where our rerankerfailed to identify the best DT.
Unsurprisingly, it16The human agreement on sentence-level parsing is 83%.2057Standard test set 5-folds (average)k=1 k=2 k=3 k=4 k=5 k=6 k=1 k=2 k=3 k=4 k=5 k=6RR 79.77 81.08 81.56 82.15 82.15 82.11 78.57 79.76 80.28 80.68 80.80 80.86ERR - 6.48 8.85 11.76 11.76 11.57 - 5.88 8.45 10.43 11.02 11.32OR 79.77 84.42 86.55 87.68 88.09 88.75 78.57 83.20 85.13 86.49 87.35 88.03Table 4: Reranking performance (RR) in comparison with oracle (OR) accuracy for different values of k on the standardtestset and 5-folds of RST-DT.
Second row shows the relative error rate reduction (ERR).happens many times for small DTs containingonly two or three EDUs, especially when the re-lations are semantically similar.
Figure 9 presentssuch a case, where the reranker fails to rank theDT with Summary ahead of the DT with Elabo-ration.
Although we understand that the rerankerlacks enough structural context to distinguish thetwo relations in this example, we expected that in-cluding the lexical items (e.g., (CFD)) in our DTrepresentation could help.
However, similar shortparenthesized texts are also used to elaborate asin Senate Majority Leader George Mitchell (D.,Maine), where the text (D., Maine) (i.e., Democratfrom state Maine) elaborates its preceding text.This confuses our reranker.
We also found er-ror examples where the reranker failed to distin-guish between Background and Elaboration, andbetween Cause and Elaboration.
This suggeststhat we need rich semantic representation of thetext to improve our reranker further.7 Related WorkEarly work on discourse parsing applied hand-coded rules based on discourse cues and surfacepatterns (Marcu, 2000a).
Supervised learning wasfirst attempted by Marcu (2000b) to build a shift-reduce discourse parser.
This work was then con-siderably improved by Soricut and Marcu (2003).They presented probabilistic generative models forsentence-level discourse parsing based on lexico-syntactic patterns.
Sporleder and Lapata (2005)investigated the necessity of syntax in discourseanalysis.
More recently, Hernault et al.
(2010)presented the HILDA discourse parser that itera-tively employs two SVM classifiers in pipeline tobuild a DT in a greedy way.
Feng and Hirst (2012)improved the HILDA parser by incorporating richlinguistic features, which include lexical seman-tics and discourse production rules.Joty et al.
(2013) achieved the best prior resultsby (i) jointly modeling the structure and the la-bel of a DT constituent, (ii) performing optimalrather than greedy decoding, and (iii) discriminat-ing between intra- and multi-sentential discourseparsing.
However, their model does not con-Same-UnitSummarybegins trading today.On the Big Board, Crawford & Co., Atlanta, (CFD)ElaborationFigure 9: An error made by our reranker.sider long range dependencies between DT con-stituents, which are encoded by our kernels.
Re-garding the latter, our work is surely inspired by(Collins and Duffy, 2002), which uses TK for syn-tactic parsing reranking or in general discrimina-tive reranking, e.g., (Collins and Koo, 2005; Char-niak and Johnson, 2005; Dinarelli et al., 2011).However, such excellent studies do not regarddiscourse parsing, and in absolute they achievedlower improvements than our methods.8 Conclusions and Future WorkIn this paper, we have presented a discriminativeapproach for reranking discourse trees generatedby an existing discourse parser.
Our reranker usestree kernels in SVM preference ranking frame-work to effectively capture the long range struc-tural dependencies between the constituents of adiscourse tree.
We have shown the reranking per-formance for sentence-level discourse parsing us-ing the standard tree kernels (i.e., STK and PTK)on two different representations (i.e., JRN andSRN) of the discourse tree, and compare it withthe traditional feature vector-based approach.
Ourresults show that: (i) the reranker improves onlywhen it considers subtree features computed bythe tree kernels, (ii) SRN is a better representationthan JRN, (iii) STK performs better than PTK forreranking discourse trees, and (iv) our best resultoutperforms the state-of-the-art significantly.In the future, we would like to apply ourreranker to the document-level parses.
However,this will require a better hypotheses generator.AcknowledgmentsThis research is part of the Interactive sYstemsfor Answer Search (Iyas) project, conducted bythe Arabic Language Technologies (ALT) groupat Qatar Computing Research Institute (QCRI)within the Qatar Foundation.2058ReferencesLynn Carlson and Daniel Marcu.
2001.
Discourse Tag-ging Reference Manual.
Technical Report ISI-TR-545, University of Southern California InformationSciences Institute.Lynn Carlson, Daniel Marcu, and Mary EllenOkurowski.
2002.
RST Discourse Treebank (RST-DT) LDC2002T07.
Linguistic Data Consortium,Philadelphia.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt DiscriminativeReranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics, ACL?05, pages 173?180, NJ, USA.
ACL.Michael Collins and Nigel Duffy.
2002.
New RankingAlgorithms for Parsing and Tagging: Kernels overDiscrete Structures, and the Voted Perceptron.
InACL.Michael Collins and Terry Koo.
2005.
DiscriminativeReranking for Natural Language Parsing.
Comput.Linguist., 31(1):25?70, March.Corinna Cortes and Vladimir Vapnik.
1995.
SupportVector Networks.
Machine Learning, 20:273?297.Marco Dinarelli, Alessandro Moschitti, and GiuseppeRiccardi.
2011.
Discriminative Reranking forSpoken Language Understanding.
IEEE Transac-tions on Audio, Speech and Language Processing(TASLP), 20:526539.Vanessa Feng and Graeme Hirst.
2012.
Text-level Dis-course Parsing with Rich Linguistic Features.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics, ACL ?12,pages 60?68, Jeju Island, Korea.
ACL.Hugo Hernault, Helmut Prendinger, David A. duVerle,and Mitsuru Ishizuka.
2010.
HILDA: A DiscourseParser Using Support Vector Machine Classification.Dialogue and Discourse, 1(3):1?33.Liang Huang and David Chiang.
2005.
Better K-best Parsing.
In Proceedings of the Ninth Inter-national Workshop on Parsing Technology, Parsing?05, pages 53?64, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Thorsten Joachims and Chun-Nam John Yu.
2009.Sparse Kernel SVMs via Cutting-Plane Training.Machine Learning, 76(2-3):179?193.
ECML.Thorsten Joachims.
1999.
Making large-Scale SVMLearning Practical.
In Advances in Kernel Methods- Support Vector Learning.Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng.2012.
A Novel Discriminative Framework forSentence-Level Discourse Analysis.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?12,pages 904?915, Jeju Island, Korea.
ACL.Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, andYashar Mehdad.
2013.
Combining Intra- andMulti-sentential Rhetorical Parsing for Document-level Discourse Analysis.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, ACL ?13, Sofia, Bulgaria.
ACL.William Mann and Sandra Thompson.
1988.
Rhetor-ical Structure Theory: Toward a Functional Theoryof Text Organization.
Text, 8(3):243?281.Daniel Marcu.
2000a.
The Rhetorical Parsing of Un-restricted Texts: A Surface-based Approach.
Com-putational Linguistics, 26:395?448.Daniel Marcu.
2000b.
The Theory and Practice ofDiscourse Parsing and Summarization.
MIT Press,Cambridge, MA, USA.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2006.
Semantic Role Labeling via Tree Ker-nel Joint Inference.
In Proceedings of the TenthConference on Computational Natural LanguageLearning (CoNLL-X), pages 61?68, New York City,June.
Association for Computational Linguistics.Alessandro Moschitti.
2006.
Efficient ConvolutionKernels for Dependency and Constituent SyntacticTrees.
In 17th European Conference on MachineLearning, pages 318?329.
Springer.Alessandro Moschitti.
2010.
Kernel Engineering forFast and Easy Design of Natural Language Applica-tions.
In COLING (Tutorials), pages 1?91.Alessandro Moschitti.
2012.
State-of-the-Art Kernelsfor Natural Language Processing.
In Tutorial Ab-stracts of ACL 2012, page 2, Jeju Island, Korea, July.Association for Computational Linguistics.Alessandro Moschitti.
2013.
Kernel-based Learningto Rank with Syntactic and Semantic Structures.
InSIGIR, page 1128.Daniele Pighin and Alessandro Moschitti.
2009.
Re-verse Engineering of Tree Kernel Feature Spaces.
InEMNLP, pages 111?120.Daniele Pighin and Alessandro Moschitti.
2010.
OnReverse Feature Engineering of Syntactic Tree Ker-nels.
In Proceedings of the Fourteenth Confer-ence on Computational Natural Language Learning,pages 223?233, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Aliaksei Severyn and Alessandro Moschitti.
2011.Fast Support Vector Machines for Structural Ker-nels.
In ECML/PKDD (3), pages 175?190.Aliaksei Severyn and Alessandro Moschitti.
2012.Fast Support Vector Machines for Convolution TreeKernels.
Data Min.
Knowl.
Discov., 25(2):325?357.John Shawe-Taylor and Nello Cristianini.
2004.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press.2059Radu Soricut and Daniel Marcu.
2003.
Sentence LevelDiscourse Parsing Using Syntactic and Lexical In-formation.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology - Volume 1, NAACL?03, pages 149?156,Edmonton, Canada.
ACL.Caroline Sporleder and Mirella Lapata.
2005.
Dis-course Chunking and its Application to SentenceCompression.
In Proceedings of the conferenceon Human Language Technology and EmpiricalMethods in Natural Language Processing, HLT-EMNLP?05, pages 257?264, Vancouver, BritishColumbia, Canada.
ACL.Charles Sutton, Andrew McCallum, and KhashayarRohanimanesh.
2007.
Dynamic Conditional Ran-dom Fields: Factorized Probabilistic Models for La-beling and Segmenting Sequence Data.
Journal ofMachine Learning Research (JMLR), 8:693?723.2060
