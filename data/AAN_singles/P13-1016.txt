Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 155?165,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDistortion Model Considering Rich Contextfor Statistical Machine TranslationIsao Goto?,?
Masao Utiyama?
Eiichiro Sumita?Akihiro Tamura?
Sadao Kurohashi?
?National Institute of Information and Communications Technology?Kyoto Universitygoto.i-es@nhk.or.jp{mutiyama, eiichiro.sumita, akihiro.tamura}@nict.go.jpkuro@i.kyoto-u.ac.jpAbstractThis paper proposes new distortion mod-els for phrase-based SMT.
In decoding, adistortion model estimates the source wordposition to be translated next (NP) giventhe last translated source word position(CP).
We propose a distortion model thatcan consider the word at the CP, a wordat an NP candidate, and the context of theCP and the NP candidate simultaneously.Moreover, we propose a further improvedmodel that considers richer context by dis-criminating label sequences that specifyspans from the CP to NP candidates.
Itenables our model to learn the effect ofrelative word order among NP candidatesas well as to learn the effect of distancesfrom the training data.
In our experiments,our model improved 2.9 BLEU points forJapanese-English and 2.6 BLEU points forChinese-English translation compared tothe lexical reordering models.1 IntroductionEstimating appropriate word order in a target lan-guage is one of the most difficult problems forstatistical machine translation (SMT).
This is par-ticularly true when translating between languageswith widely different word orders.To address this problem, there has been a lotof research done into word reordering: lexicalreordering model (Tillman, 2004), which is oneof the distortion models, reordering constraint(Zens et al, 2004), pre-ordering (Xia and Mc-Cord, 2004), hierarchical phrase-based SMT (Chi-ang, 2007), and syntax-based SMT (Yamada andKnight, 2001).In general, source language syntax is useful forhandling long distance word reordering.
However,obtaining syntax requires a syntactic parser, whichis not available for many languages.
Phrase-basedSMT (Koehn et al, 2007) is a widely used SMTmethod that does not use a parser.Phrase-based SMT mainly1 estimates word re-ordering using distortion models2.
Therefore, dis-tortion models are one of the most important com-ponents for phrase-based SMT.
On the other hand,there are methods other than distortion models forimproving word reordering for phrase-based SMT,such as pre-ordering or reordering constraints.However, these methods also use distortion mod-els when translating by phrase-based SMT.
There-fore, distortion models do not compete againstthese methods and are commonly used with them.If there is a good distortion model, it will improvethe translation quality of phrase-based SMT andbenefit to the methods using distortion models.In this paper, we propose two distortion mod-els for phrase-based SMT.
In decoding, a distor-tion model estimates the source word position tobe translated next (NP) given the last translatedsource word position (CP).
The proposed modelsare the pair model and the sequence model.
Thepair model utilizes the word at the CP, a word atan NP candidate site, and the words surroundingthe CP and the NP candidates (context) simultane-ously.
In addition, the sequence model, which isthe further improved model, considers richer con-text by identifying the label sequence that spec-ify the span from the CP to the NP.
It enablesour model to learn the effect of relative word or-der among NP candidates as well as to learn theeffect of distances from the training data.
Ourmodel learns the preference relations among NP1A language model also supports the estimation.2In this paper, reordering models for phrase-based SMT,which are intended to estimate the source word position tobe translated next in decoding, are called distortion models.This estimation is used to produce a hypothesis in the targetlanguage word order sequentially from left to right.155kinou  kare  wa  pari  de  hon  wo  kattahe   bought   books   in   Paris   yesterdaySource:Target:Figure 1: An example of left-to-right translationfor Japanese-English.
Boxes represent phrasesand arrows indicate the translation order of thephrases.candidates.
Our model consists of one probabilis-tic model and does not require a parser.
Exper-iments confirmed the effectiveness of our methodfor Japanese-English and Chinese-English transla-tion, using NTCIR-9 Patent Machine TranslationTask data sets (Goto et al, 2011).2 Distortion Model for Phrase-BasedSMTA Moses-style phrase-based SMT generates targethypotheses sequentially from left to right.
There-fore, the role of the distortion model is to esti-mate the source phrase position to be translatednext whose target side phrase will be located im-mediately to the right of the already generated hy-potheses.
An example is shown in Figure 1.
InFigure 1, we assume that only the kare wa (En-glish side: ?he?)
has been translated.
The targetword to be generated next will be ?bought?
and thesource word to be selected next will be its corre-sponding Japanese word katta.
Thus, a distortionmodel should estimate phrases including katta asa source phrase position to be translated next.To explain the distortion model task in more de-tail, we need to redefine more precisely two terms,the current position (CP) and next position (NP) inthe source sentence.
CP is the source sentence po-sition corresponding to the rightmost aligned tar-get word in the generated target word sequence.NP is the source sentence position correspondingto the leftmost aligned target word in the targetphrase to be generated next.
The task of the distor-tion model is to estimate the NP3 from NP candi-dates (NPCs) for each CP in the source sentence.43NP is not always one position, because there may be mul-tiple correct hypotheses.4This definition is slightly different from that of existingmethods such as Moses and (Green et al, 2010).
In existingmethods, CP is the rightmost position of the last translatedsource phrase and NP is the leftmost position of the sourcephrase to be translated next.
Note that existing methods dokinou1kare2wa3pari4de5hon6wo7katta8he   bought   books   in   Paris   yesterday(a)kinou1kare2wa3pari4de5ni6satsu7hon8wo9katta10he   bought   two   books   in   Paris   yesterday(b)kinou1kare2wa3hon4wo5karita6ga7kanojo8wa9katta10he   borrowed   books  yesterday  but  she  bought(c)kinou1kare2wa3kanojo4ga5katta6hon7wo8karita9yesterday  he  borrowed  the  books  that  she  bought(e)kinou1kare2wa3hon4wo5katta6ga7kanojo8wa9karita10he   bought   books   yesterday   but   she   borrowed(d)???~?????~???~??
?~CP NPFigure 2: Examples of CP and NP for Japanese-English translation.
The upper sentence is thesource sentence and the sentence underneath is atarget hypothesis for each example.
The NP is inbold, and the CP is in bold italics.
The point of anarrow with a ?
mark indicates a wrong NP candi-date.Estimating NP is a difficult task.
Figure 2 showssome examples.
The superscript numbers indicatethe word position in the source sentence.In Figure 2 (a), the NP is 8.
However, in Fig-ure 2 (b), the word (kare) at the CP is the same as(a), but the NP is different (the NP is 10).
Fromthese examples, we see that distance is not the es-sential factor in deciding an NP.
And it also turnsout that the word at the CP alone is not enough toestimate the NP.
Thus, not only the word at the CPbut also the word at a NP candidate (NPC) shouldbe considered simultaneously.In (c) and (d) in Figure 2, the word (kare) at theCP is the same and karita (borrowed) and katta(bought) are at the NPCs.
Karita is the word atthe NP and katta is not the word at the NP for(c), while katta is the word at the NP and karitais not the word at the NP for (d).
From these ex-amples, considering what the word is at the NPnot consider word-level correspondences.156is not enough to estimate the NP.
One of the rea-sons for this difference is the relative word orderbetween words.
Thus, considering relative wordorder is important.In (d) and (e) in Figure 2, the word (kare) at theCP and the word order between katta and karitaare the same.
However, the word at the NP for(d) and the word at the NP for (e) are different.From these examples, we can see that selectinga nearby word is not always correct.
The differ-ence is caused by the words surrounding the NPCs(context), the CP context, and the words betweenthe CP and the NPC.
Thus, these should be con-sidered when estimating the NP.In summary, in order to estimate the NP, the fol-lowing should be considered simultaneously: theword at the NP, the word at the CP, the relativeword order among the NPCs, the words surround-ing NP and CP (context), and the words betweenthe CP and the NPC.There are distortion models that do not requirea parser for phrase-based SMT.
The linear dis-tortion cost model used in Moses (Koehn et al,2007), whose costs are linearly proportional tothe reordering distance, always gives a high costto long distance reordering, even if the reorder-ing is correct.
The MSD lexical reordering model(Tillman, 2004; Koehn et al, 2005; Galley andManning, 2008) only calculates probabilities forthe three kinds of phrase reorderings (monotone,swap, and discontinuous), and does not considerrelative word order or words between the CP andthe NPC.
Thus, these models are not sufficient forlong distance word reordering.Al-Onaizan and Papineni (2006) proposed adistortion model that used the word at the CP andthe word at an NPC.
However, their model did notuse context, relative word order, or words betweenthe CP and the NPC.Ni et al (2009) proposed a method that adjuststhe linear distortion cost using the word at the CPand its context.
Their model does not simultane-ously consider both the word specified at the CPand the word specified at the NPCs.Green et al (2010) proposed distortion mod-els that used context.
Their model (the outboundmodel) estimates how far the NP should be fromthe CP using the word at the CP and its con-text.5 Their model does not simultaneously con-5They also proposed another model (the inbound model)sider both the word specified at the CP and theword specified at an NPC.
For example, the out-bound model considers the word specified at theCP, but does not consider the word specified at anNPC.
Their models also do not consider relativeword order.In contrast, our distortion model solves theaforementioned problems.
Our distortion modelsutilize the word specified at the CP, the word spec-ified at an NPC, and also the context of the CPand the NPC simultaneously.
Furthermore, our se-quence model considers richer context includingthe relative word order among NPCs and also in-cluding all the words between the CP and the NPC.In addition, unlike previous methods, our modelslearn the preference relations among NPCs.3 Proposed MethodIn this section, we first define our distortion modeland explain our learning strategy.
Then, we de-scribe two proposed models: the pair model andthe sequence model that is the further improvedmodel.3.1 Distortion Model and Learning StrategyFirst, we define our distortion model.
Let i be aCP, j be an NPC, S be a source sentence, andX bethe random variable of the NP.
In this paper, dis-tortion probability is defined as P (X = j|i, S),which is the probability of an NPC j being the NP.Our distortion model is defined as the model cal-culating the distortion probability.Next, we explain the learning strategy for ourdistortion model.
We train this model as a dis-criminative model that discriminates the NP fromNPCs.
Let J be a set of word positions in S otherthan i.
We train the distortion model subject to?j?JP (X = j|i, S) = 1.The model parameters are learned to maximize thedistortion probability of the NP among all of theNPCs J in each source sentence.
This learningstrategy is a kind of preference relation learning(Evgniou and Pontil, 2002).
In this learning, thethat estimates reverse direction distance.
Each NPC is re-garded as an NP, and the inbound model estimates how farthe corresponding CP should be from the NP using the wordat the NP and its context.157distortion probability of the actual NP will be rel-atively higher than those of all the other NPCs J .This learning strategy is different from that of(Al-Onaizan and Papineni, 2006; Green et al,2010).
For example, Green et al (2010) trainedtheir outbound model subject to ?c?C P (Y =c|i, S) = 1, where C is the set of the nine distor-tion classes6 and Y is the random variable of thecorrect distortion class that the correct distortion isclassified into.
Distortion is defined as j ?
i ?
1.Namely, the model probabilities that they learnedwere the probabilities of distortion classes in allof the training data, not the relative preferencesamong the NPCs in each source sentence.3.2 Pair ModelThe pair model utilizes the word at the CP, theword at an NPC, and the context of the CP and theNPC simultaneously to estimate the NP.
This canbe done by our distortion model definition and thelearning strategy described in the previous section.In this work, we use the maximum entropymethod (Berger et al, 1996) as a discriminativemachine learning method.
The reason for thisis that a model based on the maximum entropymethod can calculate probabilities.
However, ifwe use scores as an approximation of the distor-tion probabilities, various discriminative machinelearning methods can be applied to build the dis-tortion model.Let s be a source word and sn1 = s1s2...sn bea source sentence.
We add a beginning of sen-tence (BOS) marker to the head of the source sen-tence and an end of sentence (EOS) marker to theend, so the source sentence S is expressed as sn+10(s0 = BOS, sn+1 = EOS).
Our distortion modelcalculates the distortion probability for an NPCj ?
{j|1 ?
j ?
n + 1 ?
j ?= i} for each CPi ?
{i|0 ?
i ?
n}P (X = j|i, S) = 1Ziexp(wTf (i, j, S, o, d))(1)whereo ={0 (i < j)1 (i > j) , d =????
?0 (|j ?
i| = 1)1 (2 ?
|j ?
i| ?
5)2 (6 ?
|j ?
i|),6(?
?,?8], [?7,?5], [?4,?3], ?2, 0, 1, [2, 3], [4, 6],and [7,?).
In (Green et al, 2010), ?1 was used as one ofdistortion classes.
However, ?1 represents the CP in our def-inition, and CP is not an NPC.
Thus, we shifted all of thedistortion classes for negative distortions by ?1.Template?o?, ?o, sp?1, ?o, ti?, ?o, tj?, ?o, d?, ?o, sp, sq?2,?o, ti, tj?, ?o, ti?1, ti, tj?, ?o, ti, ti+1, tj?,?o, ti, tj?1, tj?, ?o, ti, tj , tj+1?, ?o, si, ti, tj?,?o, sj , ti, tj?1 p ?
{p|i?
2 ?
p ?
i + 2 ?
j ?
2 ?
p ?
j + 2}2 (p, q) ?
{(p, q)|i ?
2 ?
p ?
i + 2 ?
j ?
2 ?
q ?j + 2 ?
(|p?
i| ?
1 ?
|q ?
j| ?
1)}Table 1: Feature templates.
t is the part of speechof s.w is a weight parameter vector, each elementof f(?)
is a binary feature function, and Zi =?j?
{j|1?j?n+1 ?
j ?=i}(numerator of Equation 1)is a normalization factor.
o is an orientation of i toj and d is a distance class.The binary feature function that constitutes anelement of f(?)
returns 1 when its feature ismatched and if else, returns 0.
Table 1 shows thefeature templates used to produce the features.
Afeature is an instance of a feature template.In Equation 1, i, j, and S are used by the featurefunctions.
Thus, Equation 1 can utilize featuresconsisting of both si, which is the word specifiedat i, and sj , which is the word specified at j, orboth the context of i and the context of j simulta-neously.
Distance is considered using the distanceclass d. Distortion is represented by distance andorientation.
The pair model considers distortionusing six joint classes of d and o.3.3 Sequence ModelThe pair model does not consider relative word or-der among NPCs or all the words between the CPand an NPC.
In this section, we propose a furtherimproved model, the sequence model, which con-siders richer context including relative word orderamong NPCs and also including all the words be-tween the CP and an NPC.In (c) and (d) in Figure 2, karita (borrowed) andkatta (bought) occur in the source sentences.
Thepair model considers the effect of distances usingonly the distance class d. If these positions arein the same distance class, the pair model cannotconsider the differences in distances.
In this case,these are conflict instances during training and itis difficult to distinguish the NP for translation.Now to explain how to consider the relativeword order by the sequence model.
The sequencemodel considers the relative word order by dis-criminating the label sequence corresponding tothe NP from the label sequences corresponding to158Label DescriptionC A position is the CP.I A position is a position between the CPand the NPC.N A position is the NPC.Table 2: The ?C, I, and N?
label set.LabelsequenceID1 N C3 C N4 C I N5 C I I N6 C I I I N7 C I I I I N8 C I I I I I N9 C I I I I I I N10 C I I I I I I I N11 C I I I I I I I I NBOS0kinou1kare2wa3hon4wo5karita6ga7kanojo8wa9katta10EOS11(yesterday)(he)(book)(borrowed)(she)(bought)Source sentenceFigure 3: Example of label sequences that specifyspans from the CP to each NPC for the case ofFigure 2 (c).
The labels (C, I, and N) in the boxesare the label sequences.each NPC in each sentence.
Each label sequencecorresponds to one NPC.
Therefore, if we identifythe label sequence that corresponds to the NP, wecan obtain the NP.
The label sequences specify thespans from the CP to each NPC using three kindsof labels indicating the type of word positions inthe spans.
The three kinds of labels, ?C, I, and N,?are shown in Table 2.
Figure 3 shows examplesof the label sequences for the case of Figure 2 (c).In Figure 3, the label sequences are represented byboxes and the elements of the sequences are labels.The NPC is used as the label sequence ID for eachlabel sequence.The label sequence can treat relative word or-der.
For example, the label sequence ID of 10 inFigure 3 knows that karita exists to the left of theNPC of 10.
This is because karita6 carries a la-bel I while katta10 carries a label N, and a positionwith label I is defined as relatively closer to the CPthan a position with label N. By utilizing the labelsequence and corresponding words, the model canreflect the effect of karita existing between the CPand the NPC of 10 on the probability.For the sequence model, karita (borrowed) andkatta (bought) in (c) and (d) in Figure 2 are notconflict instances in training, whereas they areconflict instances in training for the pair model.The reason is as follows.
In order to make theprobability of the NPC of 10 smaller than the NPCof 6, instead of making the weight parameters forthe features with respect to the word at the positionof 10 with label N smaller than the weight param-eters for the features with respect to the word atthe position of 6 with label N, the sequence modelcan give negative weight parameters for the fea-tures with respect to the word at the position of 6with label I.We use a sequence discrimination techniquebased on CRF (Lafferty et al, 2001) to identify thelabel sequence that corresponds to the NP.
Thereare two differences between our task and the CRFtask.
One difference is that CRF discriminates la-bel sequences that consist of labels from all of thelabel candidates, whereas we constrain the labelsequences to sequences where the label at the CPis C, the label at an NPC is N, and the labels be-tween the CP and the NPC are I.
The other dif-ference is that CRF is designed for discriminat-ing label sequences corresponding to the same ob-ject sequence, whereas we do not assign labels towords outside the spans from the CP to each NPC.However, when we assume that another label suchas E has been assigned to the words outside thespans and there are no features involving label E,CRF with our label constraints can be applied toour task.
In this paper, the method designed todiscriminate label sequences corresponding to thedifferent word sequence lengths is called partialCRF.The sequence model based on partial CRF is de-rived by extending the pair model.
We introducethe label l and extend the pair model to discrimi-nating the label sequences.
There are two exten-sions to the pair model.
One extension uses la-bels.
We suppose that label sequences specify thespans from the CP to each NPC.
We conjoined allthe feature templates in Table 1 with an additionalfeature template ?li, lj?
to include the labels intofeatures where li is the label corresponding to theposition of i.
The other extension uses sequence.In the pair model, the position pair of (i, j) is usedto derive features.
In contrast, to descriminate la-bel sequences in the sequence model, the positionpairs of (i, k), k ?
{k|i < k ?
j ?
j ?
k < i}159and (k, j), k ?
{k|i ?
k < j ?
j < k ?
i}are used to derive features.
Note that in the featuretemplates in Table 1, i and j are used to specifytwo positions.
When features are used for the se-quence model, one of the positions is regarded ask.The distortion probability for an NPC j beingthe NP given a CP i and a source sentence S iscalculated as:P (X = j|i, S) =1Ziexp( ?k?M?
{j}wTf (i, k, S, o, d, li, lk)+?k?M?
{i}wTf (k, j, S, o, d, lk, lj))(2)whereM ={{m|i < m < j} (i < j){m|j < m < i} (i > j)and Zi = ?j?
{j|1?j?n+1 ?
j ?=i}(numerator ofEquation 2) is a normalization factor.
Since j isused as the label sequence ID, discriminating jalso means discriminating label sequence IDs.The first term in exp(?)
in Equation 2 considersall of the word pairs located at i and other posi-tions in the sequence, and also their context.
Thesecond term in exp(?)
in Equation 2 considers allof the word pairs located at j and other positionsin the sequence, and also their context.By designing our model to discriminate amongdifferent length label sequences, our model cannaturally handle the effect of distances.
Many fea-tures are derived from a long label sequence be-cause it will contain many labels between the CPand the NPC.
On the other hand, fewer featuresare derived from a short label sequence because ashort label sequence will contain fewer labels be-tween the CP and the NPC.
The bias from thesedifferences provides important clues for learningthe effect of distances.77Note that the sequence model does not only considerlarger context than the pair model, but that it also considerslabels.
The pair model does not discriminate labels, whereasthe sequence model uses label N and label I for the positionsexcept for the CP, depending on each situation.
For example,in Figure 3, at position 6, label N is used in the label sequenceID of 6, but label I is used in the label sequence IDs of 7 to11.
Namely, even if they are at the same position, the labelsin the label sequences are different.
The sequence model dis-criminates the label differences.BOS  kare  wa  pari  de  hon  wo  katta  EOSBOS  he  bought  books  in  Paris  EOSSource:Target:training dataFigure 4: Examples of supervised training data.The lines represent word alignments.
The Englishside arrows point to the nearest word aligned onthe right.3.4 Training Data for DiscriminativeDistortion ModelTo train our discriminative distortion model, su-pervised training data is needed.
The training datais built from a parallel corpus and word alignmentsbetween corresponding source words and targetwords.
Figure 4 shows examples of training data.We select the target words aligned to the sourcewords sequentially from left to right (target sidearrows).
Then, the order of the source words inthe target word order is decided (source side ar-rows).
The source sentence and the source sidearrows are the training data.4 ExperimentIn order to confirm the effects of our distortionmodel, we conducted a series of Japanese to En-glish (JE) and Chinese to English (CE) translationexperiments.84.1 Common SettingsWe used the patent data for the Japanese to En-glish and Chinese to English translation subtasksfrom the NTCIR-9 Patent Machine TranslationTask (Goto et al, 2011).
There were 2,000 sen-tences for the test data and 2,000 sentences for thedevelopment data.Mecab9 was used for the Japanese morpholog-ical analysis.
The Stanford segmenter10 and tag-ger11 were used for Chinese segmentation andPOS tagging.
The translation model was trainedusing sentences of 40 words or less from the train-ing data.
So approximately 2.05 million sen-tence pairs consisting of approximately 54 million8We conducted JE and CE translation as examples oflanguage pairs with different word orders and of languageswhere there is a great need for translation into English.9http://mecab.sourceforge.net/10http://nlp.stanford.edu/software/segmenter.shtml11http://nlp.stanford.edu/software/tagger.shtml160Japanese tokens whose lexicon size was 134k and50 million English tokens whose lexicon size was213k were used for JE.
And approximately 0.49million sentence pairs consisting of 14.9 millionChinese tokens whose lexicon size was 169k and16.3 million English tokens whose lexicon sizewas 240k were used for CE.
GIZA++ and grow-diag-final-and heuristics were used to obtain wordalignments.
In order to reduce word alignment er-rors, we removed articles {a, an, the} in Englishand particles {ga, wo, wa} in Japanese before per-forming word alignments because these functionwords do not correspond to any words in the otherlanguages.
After word alignment, we restored theremoved words and shifted the word alignment po-sitions to the original word positions.
We used 5-gram language models that were trained using theEnglish side of each set of bilingual training data.We used an in-house standard phrase-basedSMT system compatible with the Moses decoder(Koehn et al, 2007).
The SMT weighting param-eters were tuned by MERT (Och, 2003) using thedevelopment data.
To stabilize the MERT results,we tuned three times by MERT using the first halfof the development data and we selected the SMTweighting parameter set that performed the best onthe second half of the development data based onthe BLEU scores from the three SMT weightingparameter sets.We compared systems that used a commonSMT feature set from standard SMT features anddifferent distortion model features.
The com-mon SMT feature set consists of: four translationmodel features, phrase penalty, word penalty, anda language model feature.
The compared differentdistortion model features are: the linear distortioncost model feature (LINEAR), the linear distortioncost model feature and the six MSD bidirectionallexical distortion model (Koehn et al, 2005) fea-tures (LINEAR+LEX), the outbound and inbounddistortion model features discriminating nine dis-tortion classes (Green et al, 2010) (9-CLASS), theproposed pair model feature (PAIR), and the pro-posed sequence model feature (SEQUENCE).4.2 Training for the Proposed ModelsOur distortion model was trained as follows: Weused 0.2 million sentence pairs and their wordalignments from the data used to build the trans-lation model as the training data for our distortionmodels.
The features that were selected and usedwere the ones that had been counted12, using thefeature templates in Table 1, at least four timesfor all of the (i, j) position pairs in the trainingsentences.
We conjoined the features with threetypes of label pairs ?C, I?, ?I,N?, or ?C,N?
as in-stances of the feature template ?li, lj?
to producefeatures for SEQUENCE.
The L-BFGS method(Liu and Nocedal, 1989) was used to estimate theweight parameters of maximum entropy models.The Gaussian prior (Chen and Rosenfeld, 1999)was used for smoothing.4.3 Training for the Compared ModelsFor 9-CLASS, we used the same training data asfor our distortion models.
Let ti be the part ofspeech of si.
We used the following feature tem-plates to produce features for the outbound model:?si?2?, ?si?1?, ?si?, ?si+1?, ?si+2?, ?ti?, ?ti?1, ti?,?ti, ti+1?, and ?si, ti?.
These feature templates corre-spond to the components of the feature templatesof our distortion models.
In addition to these fea-tures, we used a feature consisting of the relativesource sentence position as the feature used by(Green et al, 2010).
The relative source sentenceposition is discretized into five bins, one for eachquintile of the sentence.
For the inbound model13,i of the feature templates was changed to j. Fea-tures occurring four or more times in the train-ing sentences were used.
The maximum entropymethod with Gaussian prior smoothing was usedto estimate the model parameters.The MSD bidirectional lexical distortion modelwas built using all of the data used to build thetranslation model.4.4 Results and DiscussionWe evaluated translation quality based on the case-insensitive automatic evaluation score BLEU-4(Papineni et al, 2002).
We used distortion lim-its of 10, 20, 30, and unlimited (?
), which limitedthe number of words for word reordering to a max-imum number.
Table 3 presents our main results.The proposed SEQUENCE outperformed the base-lines for both Japanese to English and Chinese toEnglish translation.
This demonstrates the effec-tiveness of the proposed SEQUENCE.
The scoresof the proposed SEQUENCE were higher than those12When we counted features for selection, we only countedfeatures that were from the feature templates of ?si, sj?,?ti, tj?, ?si, ti, tj?, and ?sj , ti, tj?
in Table 1 when j was notthe NP, in order to avoid increasing the number of features.13The inbound model is explained in footnote 5.161Japanese-English Chinese-EnglishDistortion limit 10 20 30 ?
10 20 30 ?LINEAR 27.98 27.74 27.75 27.30 29.18 28.74 28.31 28.33LINEAR+LEX 30.25 30.37 30.17 29.98 30.81 30.24 30.16 30.139-CLASS 30.74 30.98 30.92 30.75 31.80 31.56 31.31 30.84PAIR 31.62 32.36 31.96 32.03 32.51 32.30 32.25 32.32SEQUENCE 32.02 32.96 33.29 32.81 33.41 33.44 33.35 33.41Table 3: Evaluation results for each method.
The values are case-insensitive BLEU scores.
Bold numbersindicate no significant difference from the best result in each language pair using the bootstrap resamplingtest at a significance level ?
= 0.01 (Koehn, 2004).Japanese-English Chinese-EnglishHIER 30.47 32.66Table 4: Evaluation results for hierarchical phrase-based SMT.of the proposed PAIR.
This confirms the effective-ness for considering relative word order and wordsbetween the CP and an NPC.
The proposed PAIRoutperformed 9-CLASS, confirming that consider-ing both the word specified at the CP and the wordspecified at the NPC simultaneously was more ef-fective than that of 9-CLASS.For translating between languages with widelydifferent word orders such as Japanese and En-glish, a small distortion limit is undesirable be-cause there are cases where correct translationscannot be produced with a small distortion limit,since the distortion limit prunes the search spacethat does not meet the constraint.
Therefore,a large distortion limit is required to translatecorrectly.
For JE translation, our SEQUENCEachieved significantly better results at distortionlimits of 20 and 30 than that at a distortion limitof 10, while the baseline systems of LINEAR,LINEAR+LEX, and 9-CLASS did not achieve this.This indicate that SEQUENCE could treat longdistance reordering candidates more appropriatelythan the compared methods.We also tested hierarchical phrase-based SMT(Chiang, 2007) (HIER) using the Moses imple-mentation.
The common data was used to trainHIER.
We used unlimited max-chart-span for thesystem setting.
Results are given in Table 4.
OurSEQUENCE outperformed HIER.
The gain for JEwas large but the gain for CE was modest.
Sincephrase-based SMT is generally faster in decod-ing speed than hierarchical phrase-based SMT,achieving better or comparable scores is worth-DistortionProbabilityFigure 5: Average probabilities for large distortionfor Japanese-English translation.while.To investigate the tolerance for sparsity of thetraining data, we reduced the training data forthe sequence model to 20,000 sentences for JEtranslation.14 SEQUENCE using this model witha distortion limit of 30 achieved a BLEU scoreof 32.22.15 Although the score is lower than thescore of SEQUENCE with a distortion limit of 30in Table 3, the score was still higher than thoseof LINEAR, LINEAR+LEX, and 9-CLASS for JEin Table 3.
This indicates that the sequence modelalso works even when the training data is not large.This is because the sequence model considers notonly the word at the CP and the word at an NPCbut also rich context, and rich context would be ef-fective even for a smaller set of training data.14We did not conduct experiments using larger trainingdata because there would have been a very high computa-tional cost to build models using the L-BFGS method.15To avoid effects from differences in the SMT weightingparameters, we used the same SMT weighting parameters forSEQUENCE, with a distortion limit of 30, in Table 3.162To investigate how well SEQUENCE learns theeffect of distance, we checked the average distor-tion probabilities for large distortions of j ?
i?
1.Figure 5 shows three kinds of probabilities for dis-tortions from 3 to 20 for Japanese-English transla-tion.
One is the average distortion probabilitiesin the Japanese test sentences for each distortionfor SEQUENCE, and another is this for PAIR.
Thethird (CORPUS) is the probabilities for the actualdistortions in the training data that were obtainedfrom the word alignments used to build the trans-lation model.
The probability for a distortion forCORPUS was calculated by the number of the dis-tortion divided by the total number of distortionsin the training data.Figure 5 shows that when a distance class fea-ture used in the model was the same (e.g., distor-tions from 5 to 20 were the same distance classfeature), PAIR produced average distortion prob-abilities that were almost the same.
In contrast,the average distortion probabilities for SEQUENCEdecreased when the lengths of the distortions in-creased, even if the distance class feature wasthe same, and this behavior was the same as thatof CORPUS.
This confirms that the proposedSEQUENCE could learn the effect of distances ap-propriately from the training data.165 Related WorksWe discuss related works other than discussed inSection 2.
Xiong et al (2012) proposed a modelpredicting the orientation of an argument with re-spect to its verb using a parser.
Syntactic struc-tures and predicate-argument structures are usefulfor reordering.
However, orientations do not han-dle distances.
Thus, our distortion model does notcompete against the methods predicting orienta-tions using a parser and would assist them if used16We also checked the average distortion probabilities forthe 9-CLASS outbound model in the Japanese test sentencesfor Japanese-English translation.
We averaged the averageprobabilities for distortions in a distortion span of [4, 6] andalso averaged those in a distortion span of [7, 20], where thedistortions in each span are in the same distortion class.
Theaverage probability for [4, 6] was 0.058 and that for [7, 20]was 0.165.
From CORPUS, the average probabilities in thetraining data for each distortion in [4, 6] were higher thanthose for each distortion in [7, 20].
However, the conversewas true for the comparison between the two average prob-abilities for the outbound model.
This is because the sumof probabilities for distortions from 7 and above was largerthan the sum of probabilities for distortions from 4 to 6 in thetraining data.
This comparison indicates that the 9-CLASSoutbound model could not appropriately learn the effects oflarge distances for JE translation.together.There are word reordering constraint methodsusing ITG (Wu, 1997) for phrase-based SMT(Zens et al, 2004; Yamamoto et al, 2008; Feng etal., 2010).
These methods consider sentence levelconsistency with respect to ITG.
The ITG con-straint does not consider distances of reorderingand was used with other distortion models.
Ourdistortion model does not consider sentence levelconsistency, so our distortion model and ITG con-straint methods are thought to be complementary.There are tree-based SMT methods (Chiang,2007; Galley et al, 2004; Liu et al, 2006).
Inmany cases, tree-based SMT methods do not usethe distortion models that consider reordering dis-tance apart from translation rules because it is nottrivial to use distortion scores considering the dis-tances for decoders that do not generate hypothe-ses from left to right.
If it could be applied to thesemethods, our distortion model might contribute totree-based SMT methods.
Investigating the effectswill be for future work.6 ConclusionThis paper described our distortion models forphrase-based SMT.
Our sequence model simplyconsists of only one probabilistic model, but it canconsider rich context.
Experiments indicate thatour models achieved better performance and thesequence model could learn the effect of distancesappropriately.
Since our models do not require aparser, they can be applied to many languages.
Fu-ture work includes application to other languagepairs, incorporation into ITG constraint methodsand other reordering methods, and application totree-based SMT methods.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Dis-tortion models for statistical machine translation.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 529?536, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Adam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Comput.Linguist., 22(1):39?71, March.Stanley F. Chen and Ronald Rosenfeld.
1999.
A gaus-sian prior for smoothing maximum entropy models.Technical report.163David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Theodoros Evgniou and Massimiliano Pontil.
2002.Learning preference relations from data.
NeuralNets Lecture Notes in Computer Science, 2486:23?32.Yang Feng, Haitao Mi, Yang Liu, and Qun Liu.
2010.An efficient shift-reduce decoding algorithm forphrased-based machine translation.
In Coling 2010:Posters, pages 285?293, Beijing, China, August.Coling 2010 Organizing Committee.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 848?856, Honolulu, Hawaii, October.
As-sociation for Computational Linguistics.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translationrule?
In Daniel Marcu Susan Dumais and SalimRoukos, editors, HLT-NAACL 2004: Main Proceed-ings, pages 273?280, Boston, Massachusetts, USA,May 2 - May 7.
Association for Computational Lin-guistics.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K. Tsou.
2011.
Overview of the patentmachine translation task at the NTCIR-9 workshop.In Proceedings of NTCIR-9, pages 559?578.Spence Green, Michel Galley, and Christopher D.Man-ning.
2010.
Improved models of distortion costfor statistical machine translation.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 867?875, LosAngeles, California, June.
Association for Compu-tational Linguistics.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh System Descrip-tion for the 2005 IWSLT Speech Translation Evalu-ation.
In Proceedings of the International Workshopon Spoken Language Translation.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic,June.
Association for Computational Linguistics.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof 18th International Conference on Machine Learn-ing, pages 282?289.D.C.
Liu and J. Nocedal.
1989.
On the limited memorymethod for large scale optimization.
MathematicalProgramming B, 45(3):503?528.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 609?616, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-hesan Niranjan.
2009.
Handling phrase reorder-ings for machine translation.
In Proceedings of theACL-IJCNLP 2009 Conference Short Papers, pages241?244, Suntec, Singapore, August.
Associationfor Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan, July.
Association for ComputationalLinguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318.Christoph Tillman.
2004.
A unigram orienta-tion model for statistical machine translation.
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Short Papers, pages 101?104, Boston, Massachusetts, USA, May 2 - May 7.Association for Computational Linguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In Proceedings of Coling 2004, pages 508?514, Geneva, Switzerland, Aug 23?Aug 27.
COL-ING.Deyi Xiong, Min Zhang, and Haizhou Li.
2012.
Mod-eling the translation of predicate-argument structurefor smt.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 902?911, JejuIsland, Korea, July.
Association for ComputationalLinguistics.164Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedingsof 39th Annual Meeting of the Association for Com-putational Linguistics, pages 523?530, Toulouse,France, July.
Association for Computational Lin-guistics.Hirofumi Yamamoto, Hideo Okuma, and EiichiroSumita.
2008.
Imposing constraints from the sourcetree on ITG constraints for SMT.
In Proceedings ofthe ACL-08: HLT Second Workshop on Syntax andStructure in Statistical Translation (SSST-2), pages1?9, Columbus, Ohio, June.
Association for Com-putational Linguistics.Richard Zens, Hermann Ney, Taro Watanabe, and Ei-ichiro Sumita.
2004.
Reordering constraints forphrase-based statistical machine translation.
In Pro-ceedings of Coling 2004, pages 205?211, Geneva,Switzerland, Aug 23?Aug 27.
COLING.165
