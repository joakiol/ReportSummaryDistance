Learning Word Senses With Feature Selection and Order IdentificationCapabilitiesZheng-Yu Niu, Dong-Hong JiInstitute for Infocomm Research21 Heng Mui Keng Terrace119613 Singapore{zniu, dhji}@i2r.a-star.edu.sgChew-Lim TanDepartment of Computer ScienceNational University of Singapore3 Science Drive 2117543 Singaporetancl@comp.nus.edu.sgAbstractThis paper presents an unsupervised word senselearning algorithm, which induces senses of targetword by grouping its occurrences into a ?natural?number of clusters based on the similarity of theircontexts.
For removing noisy words in feature set,feature selection is conducted by optimizing a clus-ter validation criterion subject to some constraint inan unsupervised manner.
Gaussian mixture modeland Minimum Description Length criterion are usedto estimate cluster structure and cluster number.Experimental results show that our algorithm canfind important feature subset, estimate model or-der (cluster number) and achieve better performancethan another algorithm which requires cluster num-ber to be provided.1 IntroductionSense disambiguation is essential for many lan-guage applications such as machine translation, in-formation retrieval, and speech processing (Ide andVe?ronis, 1998).
Almost all of sense disambigua-tion methods are heavily dependant on manuallycompiled lexical resources.
However these lexicalresources often miss domain specific word senses,even many new words are not included inside.Learning word senses from free text will help usdispense of outside knowledge source for definingsense by only discriminating senses of words.
An-other application of word sense learning is to helpenriching or even constructing semantic lexicons(Widdows, 2003).The solution of word sense learning is closely re-lated to the interpretation of word senses.
Differentinterpretations of word senses result in different so-lutions to word sense learning.One interpretation strategy is to treat a word senseas a set of synonyms like synset in WordNet.
Thecommittee based word sense discovery algorithm(Pantel and Lin, 2002) followed this strategy, whichtreated senses as clusters of words occurring in sim-ilar contexts.
Their algorithm initially discoveredtight clusters called committees by grouping topn words similar with target word using average-link clustering.
Then the target word was assignedto committees if the similarity between them wasabove a given threshold.
Each committee that thetarget word belonged to was interpreted as one ofits senses.There are two difficulties with this committeebased sense learning.
The first difficulty is aboutderivation of feature vectors.
A feature for targetword here consists of a contextual content word andits grammatical relationship with target word.
Ac-quisition of grammatical relationship depends onthe output of a syntactic parser.
But for some lan-guages, ex.
Chinese, the performance of syntacticparsing is still a problem.
The second difficulty withthis solution is that two parameters are required tobe provided, which control the number of commit-tees and the number of senses of target word.Another interpretation strategy is to treat a wordsense as a group of similar contexts of target word.The context group discrimination (CGD) algorithmpresented in (Schu?tze, 1998) adopted this strategy.Firstly, their algorithm selected important contex-tual words using ?2 or local frequency criterion.With the ?2 based criterion, those contextual wordswhose occurrence depended on whether the am-biguous word occurred were chosen as features.When using local frequency criterion, their algo-rithm selected top n most frequent contextual wordsas features.
Then each context of occurrences oftarget word was represented by second order co-occurrence based context vector.
Singular value de-composition (SVD) was conducted to reduce the di-mensionality of context vectors.
Then the reducedcontext vectors were grouped into a pre-definednumber of clusters whose centroids corresponded tosenses of target word.Some observations can be made about their fea-ture selection and clustering procedure.
One ob-servation is that their feature selection uses onlyfirst order information although the second order co-occurrence data is available.
The other observationis about their clustering procedure.
Similar withcommittee based sense discovery algorithm, theirclustering procedure also requires the predefinitionof cluster number.
Their method can capture bothcoarse-gained and fine-grained sense distinction asthe predefined cluster number varies.
But from apoint of statistical view, there should exist a parti-tioning of data at which the most reliable, ?natural?sense clusters appear.In this paper, we follow the second order repre-sentation method for contexts of target word, sinceit is supposed to be less sparse and more robust thanfirst order information (Schu?tze, 1998).
We intro-duce a cluster validation based unsupervised fea-ture wrapper to remove noises in contextual words,which works by measuring the consistency betweencluster structures estimated from disjoint data sub-sets in selected feature space.
It is based on theassumption that if selected feature subset is impor-tant and complete, cluster structure estimated fromdata subset in this feature space should be stableand robust against random sampling.
After deter-mination of important contextual words, we use aGaussian mixture model (GMM) based clusteringalgorithm (Bouman et al, 1998) to estimate clusterstructure and cluster number by minimizing Min-imum Description Length (MDL) criterion (Ris-sanen, 1978).
We construct several subsets fromwidely used benchmark corpus as test data.
Experi-mental results show that our algorithm (FSGMM )can find important feature subset, estimate clusternumber and achieve better performance comparedwith CGD algorithm.This paper is organized as follows.
In section2 we will introduce our word sense learning al-gorithm, which incorporates unsupervised featureselection and model order identification technique.Then we will give out the experimental results ofour algorithm and discuss some findings from theseresults in section 3.
Section 4 will be devoted toa brief review of related efforts on word sense dis-crimination.
In section 5 we will conclude our workand suggest some possible improvements.2 Learning Procedure2.1 Feature selectionFeature selection for word sense learning is to findimportant contextual words which help to discrim-inate senses of target word without using class la-bels in data set.
This problem can be generalizedas selecting important feature subset in an unsuper-vised manner.
Many unsupervised feature selectionalgorithms have been presented, which can be cate-gorized as feature filter (Dash et al, 2002; Talav-era, 1999) and feature wrapper (Dy and Brodley,2000; Law et al, 2002; Mitra et al, 2002; Modhaand Spangler, 2003).In this paper we propose a cluster valida-tion based unsupervised feature subset evaluationmethod.
Cluster validation has been used to solvemodel order identification problem (Lange et al,2002; Levine and Domany, 2001).
Table 1 givesout our feature subset evaluation algorithm.
If somefeatures in feature subset are noises, the estimatedcluster structure on data subset in selected featurespace is not stable, which is more likely to be theartifact of random splitting.
Then the consistencybetween cluster structures estimated from disjointdata subsets will be lower.
Otherwise the estimatedcluster structures should be more consistent.
Herewe assume that splitting does not eliminate some ofthe underlying modes in data set.For comparison of different clustering structures,predictors are constructed based on these clusteringsolutions, then we use these predictors to classifythe same data subset.
The agreement between classmemberships computed by different predictors canbe used as the measure of consistency between clus-ter structures.
We use the stability measure (Langeet al, 2002) (given in Table 1) to assess the agree-ment between class memberships.For each occurrence, one strategy is to constructits second order context vector by summing the vec-tors of contextual words, then let the feature selec-tion procedure start to work on these second ordercontextual vectors to select features.
However, sincethe sense associated with a word?s occurrence is al-ways determined by very few feature words in itscontexts, it is always the case that there exist morenoisy words than the real features in the contexts.So, simply summing the contextual word?s vectorstogether may result in noise-dominated second or-der context vectors.To deal with this problem, we extend the featureselection procedure further to the construction ofsecond order context vectors: to select better featurewords in contexts to construct better second ordercontext vectors enabling better feature selection.Since the sense associated with a word?s occur-rence is always determined by some feature wordsin its contexts, it is reasonable to suppose that theselected features should cover most of occurrences.Formally, let coverage(D,T ) be the coverage rateof the feature set T with respect to a set of con-texts D, i.e., the ratio of the number of the occur-rences with at least one feature in their local con-texts against the total number of occurrences, thenwe assume that coverage(D,T ) ?
?
.
In practice,we set ?
= 0.9.This assumption also helps to avoid the bias to-ward the selection of fewer features, since withfewer features, there are more occurrences withoutfeatures in contexts, and their context vectors willbe zero valued, which tends to result in more stablecluster structure.Let D be a set of local contexts of occurrences oftarget word, then D = {di}Ni=1, where di representslocal context of the i-th occurrence, and N is thetotal number of this word?s occurrences.W is used to denote bag of words occurring incontext set D, then W = {wi}Mi=1, where wi de-notes a word occurring in D, and M is the totalnumber of different contextual words.Let V denote a M ?
M second-order co-occurrence symmetric matrix.
Suppose that the i-th, 1 ?
i ?
M , row in the second order matrix corre-sponds to word wi and the j-th , 1 ?
j ?
M , col-umn corresponds to word wj , then the entry speci-fied by i-th row and j-th column records the numberof times that word wi occurs close to wj in corpus.We use v(wi) to represent the word vector of con-textual word wi, which is the i-th row in matrix V .HT is a weight matrix of contextual word subsetT , T ?
W .
Then each entry hi,j represents theweight of word wj in di, wj ?
T , 1 ?
i ?
N .
Weuse binary term weighting method to derive contextvectors: hi,j = 1 if word wj occurs in di, otherwisezero.Let CT = {cTi }Ni=1 be a set of context vectors infeature space T , where cTi is the context vector ofthe i-th occurrence.
cTi is defined as:cTi =?j(hi,jv(wj)), wj ?
T, 1 ?
i ?
N. (1)The feature subset selection in word set W can beformulated as:T?
= argmaxT{criterion(T,H, V, q)}, T ?
W, (2)subject to coverage(D,T ) ?
?
, where T?
is the op-timal feature subset, criterion is the cluster valida-tion based evaluation function (the function in Ta-ble 1), q is the resampling frequency for estimateof stability, and coverage(D,T ) is the proportionof contexts with occurrences of features in T .
Thisconstrained optimization results in a solution whichmaximizes the criterion and meets the given con-straint at the same time.
In this paper we use se-quential greedy forward floating search (Pudil et al,1994) in sorted word list based on ?2 or local fre-quency criterion.
We set l = 1, m = 1, where l isplus step, and m is take-away step.2.2 Clustering with order identificationAfter feature selection, we employ a Gaussian mix-ture modelling algorithm, Cluster (Bouman et al,Table 1: Unsupervised Feature Subset Evaluation Algorithm.Intuitively, for a given feature subset T , we iteratively split dataset into disjoint halves, and compute the agreement of cluster-ing solutions estimated from these sets using stability measure.The average of stability over q resampling is the estimation ofthe score of T .Function criterion(T , H , V , q)Input parameter: feature subset T , weight matrix H ,second order co-occurrence matrix V , resamplingfrequency q;(1) ST = 0;(2) For i = 1 to q do(2.1) Randomly split CT into disjoint halves, denotedas CTA and CTB ;(2.2) Estimate GMM parameter and cluster number on CTAusing Cluster, and the parameter set is denoted as ?
?A;The solution ?
?A can be used to construct a predictor?A;(2.3) Estimate GMM parameter and cluster number on CTBusing Cluster, and the parameter set is denoted as ?
?B ,The solution ?
?B can be used to construct a predictor?B ;(2.4) Classify CTB using ?A and ?B ;The class labels assigned by ?A and ?B are denotedas LA and LB ;(2.5) ST+ = maxpi 1|CTB |?i 1{pi(LA(cTBi)) = LB(cTBi)},where pi denotes possible permutation relating indicesbetween LA and LB , and cTBi ?
CTB ;(3) ST = 1qST ;(4) Return ST ;1998), to estimate cluster structure and cluster num-ber.
Let Y = {yn}Nn=1 be a set of M dimen-sional vectors to be modelled by GMM.
Assumingthat this model has K subclasses, let pik denote theprior probability of subclass k, ?k denote the M di-mensional mean vector for subclass k, Rk denotethe M ?M dimensional covariance matrix for sub-class k, 1 ?
k ?
K. The subclass label for pixelyn is represented by xn.
MDL criterion is usedfor GMM parameter estimation and order identifi-cation, which is given by:MDL(K, ?)
= ?N?n=1log (pyn|xn(yn|?))
+12L log (NM),(3)pyn|xn(yn|?)
=K?k=1pyn|xn(yn|k, ?
)pik, (4)L = K(1 +M + (M + 1)M2 )?
1, (5)The log likelihood measures the goodness of fit ofa model to data sample, while the second term pe-nalizes complex model.
This estimator works by at-tempting to find a model order with minimum codelength to describe the data sample Y and parameterset ?.If the cluster number is fixed, the estimation ofGMM parameter can be solved using EM algorithmto address this type of incomplete data problem(Dempster et al, 1977).
The initialization of mix-ture parameter ?
(1) is given by:pi(1)k =1Ko (6)?
(1)k = yn, where n = b(k?
1)(N ?
1)/(Ko?
1)c+1 (7)R(1)k =1N ?Nn=1ynytn (8)Ko is a given initial subclass number.Then EM algorithm is used to estimate model pa-rameters by minimizing MDL:E-step: re-estimate the expectations based on pre-vious iteration:pxn|yn(k|yn, ?
(i)) =pyn|xn(yn|k, ?
(i))pik?Kl=1(pyn|xn(yn|l, ?
(i))pil), (9)M-step: estimate the model parameter ?
(i) tomaximize the log-likelihood in MDL:Nk =N?n=1pxn|yn(k|yn, ?
(i)) (10)pik = NkN (11)?k =1NkN?n=1ynpxn|yn(k|yn, ?
(i)) (12)Rk = 1NkN?n=1(yn ?
?k)(yn ?
?k)tpxn|yn(k|yn, ?
(i))(13)pyn|xn(yn|k, ?
(i)) =1(2pi)M/2 |Rk|?1/2 exp{?}
(14)?
= ?12(yn ?
?k)tR?1k (yn ?
?k) (15)The EM iteration is terminated when the changeof MDL(K, ?)
is less than ?:?
= 1100(1 +M +(M + 1)M2 )log(NM) (16)For inferring the cluster number, EM algorithmis applied for each value of K, 1 ?
K ?
Ko, andthe value K?
which minimizes the value of MDLis chosen as the correct cluster number.
To makethis process more efficient, two cluster pair l and mare selected to minimize the change in MDL crite-ria when reducing K to K ?
1.
These two clustersl and m are then merged.
The resulting parameterset is chosen as an initial condition for EM iterationwith K ?
1 subclasses.
This operation will avoid acomplete minimization with respect to pi, ?, and Rfor each value of K.Table 2: Four ambiguous words, their senses and frequencydistribution of each sense.Word Sense Percentagehard not easy (difficult) 82.8%(adjective) not soft (metaphoric) 9.6%not soft (physical) 7.6%interest money paid for the use of money 52.4%a share in a company or business 20.4%readiness to give attention 14%advantage, advancement or favor 9.4%activity that one gives attention to 3.6%causing attention to be given to 0.2%line product 56%(noun) telephone connection 10.6%written or spoken text 9.8%cord 8.6%division 8.2%formation 6.8%serve supply with food 42.6%(verb) hold an office 33.6%function as something 16%provide a service 7.8%3 Experiments and Evaluation3.1 Test dataWe constructed four datasets from hand-tagged cor-pus 1 by randomly selecting 500 instances for eachambiguous word - ?hard?, ?interest?, ?line?, and?serve?.
The details of these datasets are given inTable 2.
Our preprocessing included lowering theupper case characters, ignoring all words that con-tain digits or non alpha-numeric characters, remov-ing words from a stop word list, and filtering outlow frequency words which appeared only once inentire set.
We did not use stemming procedure.The sense tags were removed when they were usedby FSGMM and CGD.
In evaluation procedure,these sense tags were used as ground truth classes.A second order co-occurrence matrix for Englishwords was constructed using English version ofXinhua News (Jan. 1998-Dec. 1999).
The win-dow size for counting second order co-occurrencewas 50 words.3.2 Evaluation method for feature selectionFor evaluation of feature selection, we used mutualinformation between feature subset and class labelset to assess the importance of selected feature sub-set.
Our assessment measure is defined as:M(T ) = 1|T |?w?T?l?Lp(w, l)log p(w, l)p(w)p(l) , (17)where T is the feature subset to be evaluated, T ?W , L is class label set, p(w, l) is the joint distri-bution of two variables w and l, p(w) and p(l) aremarginal probabilities.
p(w, l) is estimated based1http://www.d.umn.edu/?tpederse/data.htmlon contingency table of contextual word set W andclass label set L. Intuitively, if M(T1) > M(T2),T1 is more important than T2 since T1 contains moreinformation about L.3.3 Evaluation method for clustering resultWhen assessing the agreement between clusteringresult and hand-tagged senses (ground truth classes)in benchmark data, we encountered the difficultythat there was no sense tag for each cluster.In (Lange et al, 2002), they defined a permu-tation procedure for calculating the agreement be-tween two cluster memberships assigned by differ-ent unsupervised learners.
In this paper, we appliedtheir method to assign different sense tags to onlymin(|U |, |C|) clusters by maximizing the accuracy,where |U | is the number of clusters, and |C| is thenumber of ground truth classes.
The underlying as-sumption here is that each cluster is considered asa class, and for any two clusters, they do not sharesame class labels.
At most |C| clusters are assignedsense tags, since there are only |C| classes in bench-mark data.Given the contingency table Q between clustersand ground truth classes, each entry Qi,j gives thenumber of occurrences which fall into both the i-th cluster and the j-th ground truth class.
If |U | <|C|, we constructed empty clusters so that |U | =|C|.
Let ?
represent a one-to-one mapping functionfrom C to U .
It means that ?
(j1) 6= ?
(j2) if j1 6=j2 and vice versa, 1 ?
j1, j2 ?
|C|.
Then ?
(j)is the index of the cluster associated with the j-thclass.
Searching a mapping function to maximizethe accuracy of U can be formulated as:??
= argmax?|C|?j=1Q?
(j),j .
(18)Then the accuracy of solution U is given byAccuracy(U) =?j Q??
(j),j?i,j Qi,j.
(19)In fact,?i,j Qi,j is equal to N , the number ofoccurrences of target word in test set.3.4 Experiments and resultsFor each dataset, we tested following procedures:CGDterm:We implemented the context groupdiscrimination algorithm.
Top max(|W | ?20%, 100) words in contextual word list was se-lected as features using frequency or ?2 based rank-ing.
Then k-means clustering2 was performed oncontext vector matrix using normalized Euclideandistance.
K-means clustering was repeated 5 times2We used k-means function in statistics toolbox of Matlab.and the partition with best quality was chosen as fi-nal result.
The number of clusters used by k-meanswas set to be identical with the number of groundtruth classes.
We tested CGDterm using variousword vector weighting methods when deriving con-text vectors, ex.
binary, idf , tf ?
idf .CGDSV D: The context vector matrix was de-rived using same method in CGDterm.
Then k-means clustering was conducted on latent seman-tic space transformed from context vector matrix,using normalized Euclidean distance.
Specifically,context vectors were reduced to 100 dimensions us-ing SVD.
If the dimension of context vector wasless than 100, all of latent semantic vectors withnon-zero eigenvalue were used for subsequent clus-tering.
We also tested it using different weightingmethods, ex.
binary, idf , tf ?
idf .FSGMM : We performed cluster validationbased feature selection in feature set used by CGD.Then Cluster algorithm was used to group targetword?s instances using Euclidean distance measure.?
was set as 0.90 in feature subset search procedure.The random splitting frequency is set as 10 for es-timation of the score of feature subset.
The initialsubclass number was 20 and full covariance matrixwas used for parameter estimation of each subclass.For investigating the effect of different contextwindow size on the performance of three proce-dures, we tested these procedures using various con-text window sizes: ?1, ?5, ?15, ?25, and all ofcontextual words.
The average length of sentencesin 4 datasets is 32 words before preprocessing.
Per-formance on each dataset was assessed by equation19.The scores of feature subsets selected byFSGMM and CGD are listed in Table 3 and4.
The average accuracy of three procedures withdifferent feature ranking and weighting method isgiven in Table 5.
Each figure is the average over 5different context window size and 4 datasets.
Wegive out the detailed results of these three proce-dures in Figure 1.
Several results should be notedspecifically:From Table 3 and 4, we can find that FSGMMachieved better score on mutual information (MI)measure than CGD over 35 out of total 40 cases.This is the evidence that our feature selection pro-cedure can remove noise and retain important fea-tures.As it was shown in Table 5, with both ?2 andfreq based feature ranking, FSGMM algorithmperformed better than CGDterm and CGDSV D ifwe used average accuracy to evaluate their per-formance.
Specifically, with ?2 based featureranking, FSGMM attained 55.4% average accu-racy, while the best average accuracy of CGDtermand CGDSV D were 40.9% and 51.3% respec-tively.
With freq based feature ranking, FSGMMachieved 51.2% average accuracy, while the best av-erage accuracy of CGDterm and CGDSV D were45.1% and 50.2%.The automatically estimated cluster numbers byFSGMM over 4 datasets are given in Table 6.The estimated cluster number was 2 ?
4 for ?hard?,3 ?
6 for ?interest?, 3 ?
6 for ?line?, and 2 ?
4for ?serve?.
It is noted that the estimated clusternumber was less than the number of ground truthclasses in most cases.
There are some reasons forthis phenomenon.
First, the data is not balanced,which may lead to that some important features can-not be retrieved.
For example, the fourth sense of?serve?, and the sixth sense of ?line?, their corre-sponding features are not up to the selection criteria.Second, some senses can not be distinguished usingonly bag-of-words information, and their differencelies in syntactic information held by features.
Forexample, the third sense and the sixth sense of ?in-terest?
may be distinguished by syntactic relation offeature words, while the bag of feature words occur-ring in their context are similar.
Third, some sensesare determined by global topics, rather than localcontexts.
For example, according to global topics, itmay be easier to distinguish the first and the secondsense of ?interest?.Figure 2 shows the average accuracy over threeprocedures in Figure 1 as a function of contextwindow size for 4 datasets.
For ?hard?, the per-formance dropped as window size increased, andthe best accuracy(77.0%) was achieved at win-dow size 1.
For ?interest?, sense discriminationdid not benefit from large window size and thebest accuracy(40.1%) was achieved at window size5.
For ?line?, accuracy dropped when increas-ing window size and the best accuracy(50.2%) wasachieved at window size 1.
For ?serve?, the per-formance benefitted from large window size and thebest accuracy(46.8%) was achieved at window size15.In (Leacock et al, 1998), they used Bayesian ap-proach for sense disambiguation of three ambiguouswords, ?hard?, ?line?, and ?serve?, based on cuesfrom topical and local context.
They observed thatlocal context was more reliable than topical contextas an indicator of senses for this verb and adjective,but slightly less reliable for this noun.
Comparedwith their conclusion, we can find that our resultis consistent with it for ?hard?.
But there is somedifferences for verb ?serve?
and noun ?line?.
ForTable 3: Mutual information between feature subset and classlabel with ?2 based feature ranking.Word Cont.
Size of MI Size of MIwind.
feature ?10?2 feature ?10?2size subset subsetof CGD ofFSGMMhard 1 18 6.4495 14 8.10705 100 0.4018 80 0.430015 100 0.1362 80 0.141625 133 0.0997 102 0.1003all 145 0.0937 107 0.0890interest 1 64 1.9697 55 2.06395 100 0.3234 89 0.335515 157 0.1558 124 0.153125 190 0.1230 138 0.1267all 200 0.1163 140 0.1191line 1 39 4.2089 32 4.64565 100 0.4628 84 0.487115 183 0.1488 128 0.142925 263 0.1016 163 0.0962all 351 0.0730 192 0.0743serve 1 22 6.8169 20 6.70435 100 0.5057 85 0.522715 188 0.2078 164 0.209425 255 0.1503 225 0.1536all 320 0.1149 244 0.1260Table 4: Mutual information between feature subset and classlabel with freq based feature ranking.Word Cont.
Size of MI Size of MIwind.
feature ?10?2 feature ?10?2size subset subsetof CGD ofFSGMMhard 1 18 6.4495 14 8.10705 100 0.4194 80 0.483215 100 0.1647 80 0.177425 133 0.1150 102 0.1259all 145 0.1064 107 0.1269interest 1 64 1.9697 55 2.70515 100 0.6015 89 0.830915 157 0.2526 124 0.349525 190 0.1928 138 0.2982all 200 0.1811 140 0.2699line 1 39 4.2089 32 4.46065 100 0.6895 84 0.781615 183 0.2301 128 0.292925 263 0.1498 163 0.2181all 351 0.1059 192 0.1630serve 1 22 6.8169 20 7.00215 100 0.7045 85 0.842215 188 0.2763 164 0.341825 255 0.1901 225 0.2734all 320 0.1490 244 0.2309?serve?, the possible reason is that we do not useposition of local word and part of speech informa-tion, which may deteriorate the performance whenlocal context(?
5 words) is used.
For ?line?, thereason might come from the feature subset, whichis not good enough to provide improvement whenTable 5: Average accuracy of three procedures with varioussettings over 4 datasets.Algorithm Feature Feature Averageranking weighting accuracymethod methodFSGMM ?2 binary 0.554CGDterm ?2 binary 0.404CGDterm ?2 idf 0.407CGDterm ?2 tf ?
idf 0.409CGDSVD ?2 binary 0.513CGDSVD ?2 idf 0.512CGDSVD ?2 tf ?
idf 0.508FSGMM freq binary 0.512CGDterm freq binary 0.451CGDterm freq idf 0.437CGDterm freq tf ?
idf 0.447CGDSVD freq binary 0.502CGDSVD freq idf 0.498CGDSVD freq tf ?
idf 0.485Table 6: Automatically determined mixture component num-ber.Word Context Model Modelwindow order ordersize with ?2 with freqhard 1 3 45 2 215 2 325 2 3all 2 3interest 1 5 45 3 415 4 625 4 6all 3 4line 1 5 65 4 315 5 425 5 4all 3 4serve 1 3 35 3 415 3 325 3 3all 2 4context window size is no less than 5.4 Related WorkBesides the two works (Pantel and Lin, 2002;Schu?tze, 1998), there are other related efforts onword sense discrimination (Dorow and Widdows,2003; Fukumoto and Suzuki, 1999; Pedersen andBruce, 1997).In (Pedersen and Bruce, 1997), they described anexperimental comparison of three clustering algo-rithms for word sense discrimination.
Their featuresets included morphology of target word, part ofspeech of contextual words, absence or presence ofparticular contextual words, and collocation of fre-0 1 5 15 25 all0.40.50.60.70.80.9Hard datasetAccuracy0 1 5 15 25 all0.20.30.40.50.6AccuracyInterest dataset0 1 5 15 25 all0.20.30.40.50.60.7Line datasetAccuracy0 1 5 15 25 all0.30.350.40.450.50.550.6Serve datasetAccuracyFigure 1: Results for three procedures over 4 datases.
Thehorizontal axis corresponds to the context window size.
Solidline represents the result of FSGMM + binary, dashed linedenotes the result of CGDSVD + idf , and dotted line is theresult of CGDterm + idf .
Square marker denotes ?2 basedfeature ranking, while cross marker denotes freq based featureranking.0 1 5 15 25 all0.30.350.40.450.50.550.60.650.70.750.8AverageAccuracyHard datasetInterest datasetLine datasetServe datasetFigure 2: Average accuracy over three procedures in Figure1 as a function of context window size (horizontal axis) for 4datasets.quent words.
Then occurrences of target word weregrouped into a pre-defined number of clusters.
Sim-ilar with many other algorithms, their algorithm alsorequired the cluster number to be provided.In (Fukumoto and Suzuki, 1999), a term weightlearning algorithm was proposed for verb sense dis-ambiguation, which can automatically extract nounsco-occurring with verbs and identify the number ofsenses of an ambiguous verb.
The weakness of theirmethod is to assume that nouns co-occurring withverbs are disambiguated in advance and the numberof senses of target verb is no less than two.The algorithm in (Dorow and Widdows, 2003)represented target noun word, its neighbors andtheir relationships using a graph in which each nodedenoted a noun and two nodes had an edge betweenthem if they co-occurred with more than a givennumber of times.
Then senses of target word wereiteratively learned by clustering the local graph ofsimilar words around target word.
Their algorithmrequired a threshold as input, which controlled thenumber of senses.5 Conclusion and Future WorkOur word sense learning algorithm combined twonovel ingredients: feature selection and order iden-tification.
Feature selection was formalized as aconstrained optimization problem, the output ofwhich was a set of important features to determineword senses.
Both cluster structure and cluster num-ber were estimated by minimizing a MDL crite-rion.
Experimental results showed that our algo-rithm can retrieve important features, estimate clus-ter number automatically, and achieve better per-formance in terms of average accuracy than CGDalgorithm which required cluster number as input.Our word sense learning algorithm is unsupervisedin two folds: no requirement of sense tagged data,and no requirement of predefinition of sense num-ber, which enables the automatic discovery of wordsenses from free text.In our algorithm, we treat bag of words in lo-cal contexts as features.
It has been shown thatlocal collocations and morphology of target wordplay important roles in word sense disambiguationor discrimination (Leacock et al, 1998; Widdows,2003).
It is necessary to incorporate these morestructural information to improve the performanceof word sense learning.ReferencesBouman, C. A., Shapiro, M., Cook, G. W., Atkins,C.
B., & Cheng, H. (1998) Cluster: AnUnsupervsied Algorithm for Modeling Gaus-sian Mixtures.
http://dynamo.ecn.purdue.edu/?bouman/software/cluster/.Dash, M., Choi, K., Scheuermann, P., & Liu, H. (2002)Feature Selection for Clustering - A Filter Solution.Proc.
of IEEE Int.
Conf.
on Data Mining(pp.
115?122).Dempster, A. P., Laird, N. M., & Rubin, D. B.
(1977)Maximum likelihood from incomplete data using theEM algorithm.
Journal of the Royal Statistical Soci-ety, 39(B).Dorow, B, & Widdows, D. (2003) Discovering Corpus-Specific Word Senses.
Proc.
of the 10th Conf.
of theEuropean Chapter of the Association for Computa-tional Linguistics, Conference Companion (researchnotes and demos)(pp.79?82).Dy, J. G., & Brodley, C. E. (2000) Feature Subset Selec-tion and Order Identification for Unsupervised Learn-ing.
Proc.
of the 17th Int.
Conf.
on Machine Learn-ing(pp.
247?254).Fukumoto, F., & Suzuki, Y.
(1999) Word Sense Disam-biguation in Untagged Text Based on Term WeightLearning.
Proc.
of the 9th Conf.
of European Chapterof the Association for Computational Linguistics(pp.209?216).Ide, N., & Ve?ronis, J.
(1998) Word Sense Disambigua-tion: The State of the Art.
Computational Linguistics,24:1, 1?41.Lange, T., Braun, M., Roth, V., & Buhmann, J. M. (2002)Stability-Based Model Selection.
Advances in NeuralInformation Processing Systems 15.Law, M. H., Figueiredo, M., & Jain, A. K. (2002) Fea-ture Selection in Mixture-Based Clustering.
Advancesin Neural Information Processing Systems 15.Leacock, C., Chodorow, M., & Miller A. G. (1998) Us-ing Corpus Statistics and WordNet Relations for SenseIdentification.
Computational Linguistics, 24:1, 147?165.Levine, E., & Domany, E. (2001) Resampling Methodfor Unsupervised Estimation of Cluster Validity.
Neu-ral Computation, Vol.
13, 2573?2593.Mitra, P., Murthy, A. C., & Pal, K. S. (2002) Unsu-pervised Feature Selection Using Feature Similarity.IEEE Transactions on Pattern Analysis and MachineIntelligence, 24:4, 301?312.Modha, D. S., & Spangler, W. S. (2003) Feature Weight-ing in k-Means Clustering.
Machine Learning, 52:3,217?237.Pantel, P. & Lin, D. K. (2002) Discovering Word Sensesfrom Text.
Proc.
of ACM SIGKDD Conf.
on Knowl-edge Discovery and Data Mining(pp.
613-619).Pedersen, T., & Bruce, R. (1997) Distinguishing WordSenses in Untagged Text.
Proceedings of the 2ndConference on Empirical Methods in Natural Lan-guage Processing(pp.
197?207).Pudil, P., Novovicova, J., & Kittler, J.
(1994) FloatingSearch Methods in Feature Selection.
Pattern Recog-nigion Letters, Vol.
15, 1119-1125.Rissanen, J.
(1978) Modeling by Shortest Data Descrip-tion.
Automatica, Vol.
14, 465?471.Schu?tze, H. (1998) Automatic Word Sense Discrimina-tion.
Computational Linguistics, 24:1, 97?123.Talavera, L. (1999) Feature Selection as a PreprocessingStep for Hierarchical Clustering.
Proc.
of the 16th Int.Conf.
on Machine Learning(pp.
389?397).Widdows, D. (2003) Unsupervised methods for devel-oping taxonomies by combining syntactic and statisti-cal information.
Proc.
of the Human Language Tech-nology / Conference of the North American Chapterof the Association for Computational Linguistics(pp.276?283).
