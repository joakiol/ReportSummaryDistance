Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 1?8,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsCache-Augmented Latent Topic Language Models for Speech RetrievalJonathan WintrodeCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MDjcwintr@cs.jhu.eduAbstractWe aim to improve speech retrieval perfor-mance by augmenting traditional N-gram lan-guage models with different types of topiccontext.
We present a latent topic modelframework that treats documents as arisingfrom an underlying topic sequence combinedwith a cache-based repetition model.
We ana-lyze our proposed model both for its ability tocapture word repetition via the cache and forits suitability as a language model for speechrecognition and retrieval.
We show this model,augmented with the cache, captures intuitiverepetition behavior across languages and ex-hibits lower perplexity than regular LDA onheld out data in multiple languages.
Lastly, weshow that our joint model improves speech re-trieval performance beyond N-grams or latenttopics alone, when applied to a term detectiontask in all languages considered.1 IntroductionThe availability of spoken digital media continuesto expand at an astounding pace.
According toYouTube?s publicly released statistics, between Au-gust 2013 and February 2015 content upload rateshave tripled from 100 to 300 hours of video perminute (YouTube, 2015).
Yet the information con-tent therein, while accessible via links, tags, or otheruser-supplied metadata, is largely inaccessible viacontent search within the speech.Speech retrieval systems typically rely onLarge Vocabulary Continuous Speech Recognition(LVSCR) to generate a lattice of word hypothesesfor each document, indexed for fast search (Millerand others, 2007).
However, for sites like YouTube,localized in over 60 languages (YouTube, 2015), thelikelihood of high accuracy speech recognition inmost languages is quite low.Our proposed solution is to focus on topic infor-mation in spoken language as a means of dealingwith errorful speech recognition output in many lan-guages.
It has been repeatedly shown that a task liketopic classification is robust to high (40-60%) worderror rate systems (Peskin, 1996; Wintrode, 2014b).We would leverage the topic signal?s strength for re-trieval in a high volume, multilingual digital mediaprocessing environment.The English word topic, defined as a particu-lar ?subject of discourse?
(Houghton-Mifflin, 1997),arises from the Greek root, ?opio?
, meaning a physi-cal ?place?
or ?location?.
However, the semantic con-cepts of a particular subject are not disjoint from thephysical location of the words themselves.The goal of this particular work is to jointly modeltwo aspects of topic information, local context (rep-etition) and broad context (subject matter), whichwe previously treated in an ad hoc manner (Win-trode and Sanjeev, 2014) in a latent topic frame-work.
We show that in doing so we can achieve bet-ter word retrieval performance than language mod-els with only N-gram context on a diverse set of spo-ken languages.2 Related WorkThe use of both repetition and broad topic con-text have been exploited in a variety of ways bythe speech recognition and retrieval communities.Cache-based or adaptive language models were1some of the first approaches to incorporate informa-tion beyond a short N-gram history (where N is typ-ically 3-4 words).Cache-based models assume the probability of aword in a document d is influenced both by theglobal frequency of that word and N-gram context aswell as by the N-gram frequencies of d (or precedingcache ofK words).
Although most words are rare atthe corpus level, when they do occur, they occur inbursts.
Thus a local estimate, from the cache, maybe more reliable than the global estimate.
Jelinek(1991) and Kuhn (1990) both successfully appliedthese types of models for speech recognition, andRosenfeld (1994), using what he referred to as ?trig-ger pairs?, also realized significant gains in WER.More recently, recurrent neural network languagemodels (RNNLMs) have been introduced to capturemore of these ?long-term dependencies?
(Mikolovet al, 2010).
In terms of speech retrieval, recent ef-forts have looked at exploiting repeated keywords atsearch time, without directly modifying the recog-nizer (Chiu and Rudnicky, 2013; Wintrode, 2014a).Work within the information retrieval (IR) com-munity connects topicality with retrieval.
Hearst andPlaunt (1993) reported that the ?subtopic structur-ing?
of documents can improve full-document re-trieval.
Topic models such as Latent Dirichlet Al-location (LDA) (Blei et al, 2003) or ProbabilisticLatent Semantic Analysis (PLSA) (Hofmann, 2001)are used to the augment the document-specific lan-guage model in probabilistic, language-model basedIR (Wei and Croft, 2006; Chen, 2009; Liu and Croft,2004; Chemudugunta et al, 2007).
In all thesecases, topic information was helpful in boosting re-trieval performance above baseline vector space orN-gram models.Our proposed model closely resembles that fromChemudugunta et al (2007), with our notions ofbroad and local context corresponding to their ?gen-eral and specific?
aspects.
The unigram cache caseof our model should correspond to their ?specialwords?
model, however we do not constrain ourcache component to only unigrams.With respect to speech recognition, Florian andYarowsky (Florian and Yarowsky, 1999) and Khu-danpur and Wu (Khudanpur and Wu, 1999) usevector-space clustering techniques to approximatethe topic content of documents and augment aAlgorithm 1 Cache-augmented generative processfor all t ?
T dodraw ?(t)?
Dirichlet(?
)for all d ?
D dodraw ?(d)?
Dirichlet(?
)draw ?(d)?
Beta(?0, ?1)for wd,i, 1 ?
i ?
|d| dodraw kd,i?
Bernoulli(?
(d))if kd,i= 0 thendraw zd,i?
?
(d)draw wd,i?
?
(t=zd,i)elsedraw wd,i?
Cache(d,W?i)end ifbaseline N-gram model with topic-specific N-gramcounts.
Clarkson and Robinson (1997) proposeda similar application of cache and mixture mod-els, but only demonstrate small perplexity improve-ments.
Similar approaches use latent topic models toinfer a topic mixture of the test document (soft clus-tering) with significant recognition error reductions(Heidel et al, 2007; Hsu and Glass, 2006; Liu andLiu, 2008; Huang and Renals, 2008).
Instead of in-terpolating with a traditional backoff model, Chienand Chueh (2011) use topic models with and with-out a dynamic cache to good effect as a class-basedlanguage model.We build on the cluster-oriented results, particu-larly Khudanpur and Wu (1997) and Wintrode andKhudanpur (2014), but within an explicit frame-work, jointly capturing both types of topic informa-tion that many have leveraged individually.3 Cache-augmented Topic ModelWe propose a straightforward extension of the LDAtopic model (Blei et al, 2003; Steyvers and Griffiths,2007), allowing words to be generated either from alatent topic or from a document-level cache.
At eachword position we flip a biased coin.
Based on theoutcome we either generate a latent topic and thenthe observed word, or we pick a new word directlyfrom the cache of already observed words.
Thus wewould jointly learn the underlying topics and the ten-dency towards repetition.As with LDA, we assume each corpus is drawnfrom T latent topics.
Each topic is denoted ?
(t), a2multinomial random variable in the size of the vo-cabulary where ?
(t)vis the probability P (wv|t).
Foreach document we draw ?
(d), where ?
(d)tis the prob-ability P (t|d).We introduce two additional sets of variables, ?
(d)and kd,i.
The state kd,iis a Bernoulli variable indi-cating whether a word wd,iis drawn from the cacheor from the latent topic state.
?
(d)is the documentspecific prior on the cache state kd,i.Algorithm 1 gives the generative process explic-itly.
We choose a Beta prior ?
(d)for the Bernoullivariables kd,i.
As with the Dirichlet priors, this al-lows for a straightforward formulation of the jointprobability P (W,Z,K,?,?, ?
), from which we de-rive densities for Gibbs sampling.
A plate diagramis provided in Figure 1, illustrating the dependenceboth on latent variables and the cache of previousobservations.We implement our model as a collapsed Gibbssampler extending Java classes from the Mallet topicmodeling toolkit (McCallum, 2002).
We use theGibbs sampler for parameter estimation (trainingdata) and inference (held-out data).
We also lever-age Mallet?s hyperparameter re-estimation (Wallachet al, 2009), which we apply to ?, ?, and ?.4 Language ModelingOur primary goal in constructing this model is toapply it to language models for speech recognitionand retrieval.
Given an LVCSR system with a stan-dard N-gram language model (LM), we now de-scribe how we incorporate the inferred topic andcache model parameters of a new document into thebase LM for subsequent recognition tasks on thatspecific document.We begin by estimating model parameters on atraining corpus: topics ?
(t), cache proportions ?
(d),and hyperparameters, ?, ?, and ?
(the Beta hyperpa-rameter).
In our experiments we restrict the trainingset to the LVCSR acoustic and language model train-ing.
This restriction is required by the Babel task,not the model.
Using other corpora or text resourcescertainly should be considered for other tasks.To apply the model during KWS, we first decodea new audio document d with the base LM, PLandextract the most likely observed word sequence Wfor inference.
The inference process gives us the es-wzk?(d)????
(z)?W?i|d|DTFigure 1: Cache-augmented model plate diagram.timates for ?
(d)and ?
(d), which we then use to com-pute document-specific and cache-augmented lan-guage models.From a language modeling perspective we treatthe multinomials ?
(t)as unigram LM?s and use theinferred topic proportions ?
(d)as a set of mixtureweights.
From these we compute the document-specific unigram model for d (Eqn.
1).
This serves tocapture what we have referred to as the broad topiccontext.We incorporate both Pdas well as the cache Pc(local context) into the base model PLusing linearinterpolation of probabilities.
Word histories are de-noted hifor brevity.
For our experiments we firstcombine Pdwith the N-gram model (Eqn.
2).
Wethen interpolate with the cache model to get a jointtopic and cache language model (Eqn.
4).Pd(wi) =T?t=1?(d)t?
?
(t)i(1)PLd(wi) =?Pd(wi) + (1?
?)
?
PL(wi) (2)Pdc(wi) =?(d)Pc(wi)+(1?
?
(d)) ?
Pd(wi)(3)PLdc(wi|hi) =?(d)Pc(wi|hi)+(1?
?
(d)) ?
PLd(wi|hi)(4)We expect the inferred document cache probabil-ity ?
(d)to serve as a natural interpolation weightwhen combining document-specific unigram modelPdcand cache.
We consider alternatives to per-document ?
(d)as part of the speech retrieval eval-uation (Section 6) and can show that our model?s es-timate is indeed effective.3Language 50t 100t 150t 200tTagalog 0.41 0.29 0.22 0.16Vietnamese 0.51 0.39 0.29 0.22Zulu 0.33 0.26 0.21 0.16Tamil 0.36 027 0.18 0.14Table 1: Mean ?
(d)inferred from 10 hour developmentdata, by number of latent topics5 Model AnalysisBefore looking at the model in terms of retrieval per-formance (Section 6), here we aim to examine howour model captures the repetition of each corpus andhow well it functions as a language model (cf.
Equa-tion 3) in terms of perplexity.To focus on language models for speech retrievalin the limited resource setting, we build and evalu-ate our model under the IARPA Babel Limited Lan-guage Pack (LP), No Target Audio Reuse (NTAR)condition (Harper, 2011).
We selected the Tagalog,Vietnamese, Zulu, and Tamil corpora1to expose ourmodel to as diverse a set of languages as possible (interms of morphology, phonology, language family,etc., in line with the Babel program goals).The Limited LP includes a 10 hour training set(audio and transcripts) which we use for buildingacoustic and language models.
We also estimatethe parameters for our topic model from the sametraining data.
The Babel corpora contain sponta-neous conversational telephone speech, but withoutthe constrained topic prompts of LDC?s Fisher col-lections we would expect a sparse collection of top-ics.
Yet for retrieval we are nonetheless able to lever-age the information.We estimate parameters ?
(t), ?
(d), ?, ?, and ?on the training transcripts in each language, then usethese parameters to infer ?
(d)(topic proportions) and?
(d)(cache usage) for each document in the held-out set.
We use the inferred ?
(d)and ?
(d)to performthe language model interpolation (Eqns.
3, 4).
Butalso, the mean of the inferred ?
(d)values for a cor-pus ought to provide a snapshot of the amount ofrepetition within.Two trends emerge when we examine the meanover ?
(d)by language.
First, as shown in Table 1,1Releases babel106b-v0.2g, babel107b-v0.7, babel206b-v0.1e, and babel204b-v1.1b, respectivelyFigure 2: Cache and corpus frequencies for each wordtype in Vietnamese and Zulu training corpora.the more latent topics are used, the lower the in-ferred ?
values.
Regardless of the absolute value, wesee that ?
for Vietnamese is consistently higher thanthe other languages.
This fits our intuition about thelanguages given that the Vietnamese transcripts hadsyllable-level word units and we would expect to seemore repetition.Secondly we consider which words are drawnfrom the cache versus the topics during the infer-ence process.
Examining the final sampling state,we count how often each word in the vocabularyis drawn from the cache (where kd,i= 1).
Intu-itively, this count is highly correlated (?
> 0.95)with the corpus frequency of each word (cf.
Fig-ure 2).
That is, cache states are assigned to wordtypes most likely to repeat.5.1 PerplexityWhile our measurements of cache usage corre-sponds to intuition, our primary goal is to con-struct useful language models.
After estimatingparameters on the training corpora, we infer ?
(d)and ?
(d)then measure perplexity using document-specific language models on the development set.We compute perplexity on the topic unigram mix-tures according to Pdand Pdc(Eqns.1 & 3).
Herewe do not interpolate with the base N-gram LM, soas to compare only unigram mixtures.
Table 2 givesthe perplexity for standard LDA (Pdonly) and forour model with and without the cache added (?LDA?and ?LDA respectively).With respect to perplexity, interpolating with thecache (?LDA) provides a significant boost in per-plexity for all languages and values of T .
In general,4Language T LDA ?LDA?
?LDATagalog 50 142.90 163.30 134.43100 136.63 153.99 132.35150 139.76 146.08 130.47200 128.05 141.12 129.94Vietnamese 50 257.94 283.52 217.30100 243.51 263.03 210.05150 232.60 245.75 205.59200 223.82 234.44 204.25Zulu 50 183.53 251.52 203.56100 179.44 267.42 217.11150 174.79 269.01 223.90200 175.65 252.03 217.89Tamil 50 273.08 356.40 283.82100 265.02 369.18 297.68150 259.42 361.79 301.92200 236.30 341.32 298.26Table 2: Perplexities of topic unigram mixtures on held-out data, with and without cache.perplexity decreases as the number of latent topicsincreases, excepting certain Zulu and Tamil models.For Tagalog and Vietnamese our cache-augmentedmodel outperforms standard LDA model in terms ofperplexity.
However, as we will see in the next sec-tion, the lowest perplexity models are not necessar-ily the best in terms of retrieval performance.6 Speech RetrievalWe evaluate the utility of our topic language modelfor speech retrieval via the term detection, or key-word search (KWS) task.
Term detection accuracyis the primary evaluation metric for the Babel pro-gram.
We use the topic and cache-augmented lan-guage models (Eqn.
4) to improve the speech recog-nition stage of the term detection pipeline, increas-ing overall search accuracy by 0.5 to 1.7% absoluteover a typical N-gram language model.The term detection task is this: given a corpusof audio documents and a list of terms (words orphrases), locate all occurrences of the key terms inthe audio.
The resulting list of detections is scoredusing Term Weighted Value (TWV) metric.
TWVis a cost-value trade-off between the miss probabil-ity, P (miss), and false alarm probability, P (FA),averaged over all keywords (NIST, 2006).
For com-parison with previously published results, we scoreagainst the IARPA-supplied evaluation keywords.We train acoustic and language models (LMs)on the 10 hour training set using the Kaldi toolkit(Povey and others, 2011), according to the trainingrecipe described in detail by Trmal et al (2014).While Kaldi produces different flavors of acousticmodels, we report results using the hybrid HMM-DNN (deep neural net) acoustic models, trained witha minimum phone error (MPE) criterion, and basedon PLP (perceptual linear prediction) features aug-mented with pitch.
All results use 3-gram LMs withGood-Turing (Tagalog, Zulu, Tamil) or ModifiedKneser-Ney (Vietnamese) smoothing.
This AM/LMcombination (our baseline) has consistently demon-strated state-of-the art performance for a single sys-tem on the Babel task.As described, we estimate our model parameters?
(t), ?
(d), ?, ?, and ?
from the training transcripts.We decode the development corpus with the base-line models, then infer ?
(d)and ?
(d)from the firstpass output.
In principle we simply compute PLdcfor each document and re-score the first pass output,then search for keywords.Practical considerations for cache language mod-els are, for example, just how big should the cachebe, or should it decay, where words further awayfrom the current word are discounted proportionally.In the Kaldi framework, speech is processed in seg-ments (i.e.
conversation turns).
Current tools do notallow one to vary the language model within a par-ticular segment (dynamically).
With that in mind,our KWS experiments construct a different languagemodel (PLdc) for each segment, where Pcis com-puted from all other segments in the current docu-ment except that being processed.6.1 ResultsWe can show, by re-scoring LCVSR output with acache-augmented topic LM, that both the document-specific topic (Pd) and cache (Pc) information to-gether improve our overall KWS performance ineach language, up to 1.7% absolute.Figure 3 illustrates search accuracy (TWV) foreach language under various settings for T .
It alsocaptures alternatives to using ?
(d)as an interpolationweight for the cached unigrams.
To illustrate thiscontrast we substituted the training mean ?trainin-stead of ?
(d)as the interpolation weight when com-puting PLdc(Eqn 4).
Except for Zulu, the inferred5Figure 3: KWS accuracy for different choices of T?
(d)were more effective, but not hugely so.The effect of latent topics T on search accuracyalso varies depending on language, as does the over-all effect of incorporating the cache in addition tolatent topics (?LDA?vs.
?LDA).
For example, inTagalog, we observe most of the improvement overN-grams from the cache information, whereas inTamil, the cache provided no additional informationover latent topics.The search accuracy for the best systems fromFigure 3 are shown in Table 3 with correspondingchoice of T .
Effects on WER was mixed under thecache model, improving Zulu from 67.8 to 67.6%and degrading Tagalog from 60.8 to 61.1%, withVietnamese and Tamil unchanged.7 Conclusions and Future WorkWith our initial effort in formulating model combin-ing latent topics with a cache-based language model,we believe we have presented a model that esti-mates both informative and useful parameters fromLanguage T 3-gram ?LDA?
?LDATagalog 50 0.244 0.247 0.261Vietnamese 50 0.254 0.254 0.259Zulu 100 0.270 0.274 0.278Tamil 200 0.216 0.228 0.227Table 3: Best KWS accuracy (TWV) is each language.the data and supports improved speech retrieval per-formance.
The results presented here reinforce theconclusion that topics and repetition, broad and lo-cal context, are complementary sources of informa-tion for speech language modeling tasks.We hope to address two particular limitations ofour model in the near future.
First, all of our im-provements are obtained adding unigram probabili-ties to a 3-gram language model.
We would natu-rally want to extend our model to explicitly capturethe cache and topic behavior of N-grams.Secondly, our models are restricted by the firstpass output of the LVCSR system.
Keywords notpresent in the first pass cannot be recalled by a re-scoring only approach.
An alternative would be touse our model to re-decode the audio and realizesubsequently larger gains.
Given that our re-scoringmodel worked sufficiently well across four funda-mentally different languages, we are optimistic thiswould be the case.AcknowledgementsThis work was partially supported by the In-telligence Advanced Research Projects Activity(IARPA) via Department of Defense U.S. ArmyResearch Laboratory (DoD / ARL) contract num-ber W911NF-12-C-0015.
The U.S. Government isauthorized to reproduce and distribute reprints forGovernmental purposes notwithstanding any copy-right annotation thereon.
Disclaimer: The views andconclusions contained herein are those of the authorsand should not be interpreted as necessarily repre-senting the official policies or endorsements, eitherexpressed or implied, of IARPA, DoD/ARL, or theU.S.
Government.We would also like to thank all of the reviewersfor their insightful and helpful comments, and aboveall their time.6ReferencesDavid M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent Dirichlet Allocation.
In JMLR, vol-ume 3, pages 993?1022.
JMLR.org.Chaitanya Chemudugunta, Padhraic Smyth, and SteyversMark.
2007.
Modeling General and Specific Aspectsof Documents with a Probabilistic Topic Model.
InAdvances in Neural Information Processing Systems19: Proceedings of the 2006 Conference, volume 19,page 241.
Mit Press.Berlin Chen.
2009.
Latent Topic Modelling of WordCo-occurence Information for Spoken Document Re-trieval.
In Proc.
of ICASSP, pages 3961?3964.
IEEE.Jen-Tzung Chien and Chuang-Hua Chueh.
2011.
Dirich-let Class Language Models for Speech Recogni-tion.
Audio, Speech, and Language Processing, IEEETransactions on, 19(3):482?495.Justin Chiu and Alexander Rudnicky.
2013.
Using con-versational word bursts in spoken term detection.
InProc.
of Interspeech, pages 2247?2251.
ISCA.Kenneth Ward Church and William A Gale.
1995.Poisson Mixtures.
Natural Language Engineering,1(2):163?190.Philip R Clarkson and Anthony J Robinson.
1997.
Lan-guage Model Adaptation Using Mixtures and an Ex-ponentially Decaying Cache.
In Proc.
of ICASSP, vol-ume 2, pages 799?802.
IEEE.Radu Florian and David Yarowsky.
1999.
DynamicNonlocal Language Modeling via Hierarchical Topic-based Adaptation.
In Proc.
of ACL, pages 167?174.ACL.Mary Harper.
2011.
Babel BAA.
http://www.iarpa.gov/index.php/research-programs/babel/baa.Marti A Hearst and Christian Plaunt.
1993.
SubtopicStructuring for Full-length Document Access.
In Proc.of SIGIR, pages 59?68.
ACM.Aaron Heidel, Hung-an Chang, and Lin-shan Lee.
2007.Language Model Adaptation Using Latent DirichletAllocation and an Efficient Topic Inference Algorithm.In Proc.
of Interspeech.
ICSA.Thomas Hofmann.
2001.
Unsupervised Learning byProbabilistic Latent Semantic Analysis.
MachineLearning, 42(1):177?196.Houghton-Mifflin.
1997.
The American Heritage Col-lege Dictionary.
Houghton Mifflin.Bo-June Paul Hsu and James Glass.
2006.
Style & TopicLanguage Model Adaptation Using HMM-LDA.
InProc.
of EMNLP, pages 373?381.
ACL.Songfang Huang and Steve Renals.
2008.
UnsupervisedLanguage Model Adaptation Based on Topic and RoleInformation in Multiparty Meetings.
In Proc.
of Inter-speech.
ICSA.Frederick Jelinek, Bernard Merialdo, Salim Roukos, andMartin Strauss.
1991.
A Dynamic Language Modelfor Speech Recognition.
HLT, 91:293?295.Sanjeev Khudanpur and Jun Wu.
1999.
A Maximum En-tropy Language Model Integrating N-grams and TopicDependencies for Conversational Speech Recognition.In Proc.
of ICASSP, volume 1, pages 553?556.
IEEE.Roland Kuhn and Renato De Mori.
1990.
A Cache-based Natural Language Model for Speech Recogni-tion.
Transactions on Pattern Analysis and MachineIntelligence, 12(6):570?583.Xiaoyong Liu and W Bruce Croft.
2004.
Cluster-basedRetrieval Using Language Models.
In Proc.
of SIGIR,pages 186?193.
ACM.Yang Liu and Feifan Liu.
2008.
Unsupervised Lan-guage Model Adaptation via Topic Modeling Based onNamed Entity Hypotheses.
In Proc.
of ICASSP, pages4921?4924.
IEEE.Andrew Kachites McCallum.
2002.
MALLET: A Ma-chine Learning for Language Toolkit.
http://mallet.cs.umass.edu.Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan Cer-nock`y, and Sanjeev Khudanpur.
2010.
RecurrentNeural Network Based Language Model.
In Proc.
ofInterspeech.
ICSA.David Miller et al 2007.
Rapid and Accurate SpokenTerm Detection.
In Proc.
of Interspeech.
ICSA.NIST.
2006.
The Spoken Term Detection (STD)2006 Evaluation Plan.
http://www.itl.nist.gov/iad/mig/tests/std/2006/docs/std06-evalplan-v10.pdf.
[Online; accessed28-Feb-2013].Barbara et al Peskin.
1996.
Improvements in Switch-board Recognition and Topic Identification.
In Proc.of ICASSP, volume 1, pages 303?306.
IEEE.Daniel Povey et al 2011.
The Kaldi Speech RecognitionToolkit.
In Proc.
of ASRU Workshop.
IEEE.Ronald Rosenfeld.
1994.
Adaptive Statistical LanguageModeling: a Maximum Entropy Approach.
Ph.D. the-sis, CMU.Mark Steyvers and Tom Griffiths.
2007.
ProbabilisticTopic Models.
Handbook of Latent Semantic Analysis,427(7):424?440.Jan et al Trmal.
2014.
A Keyword Search System UsingOpen Source Software.
In Proc.
of Spoken LanguageTechnoloty Workshop.
IEEE.Hanna M Wallach, David M Mimno, and Andrew Mc-Callum.
2009.
Rethinking LDA: Why Priors Matter.In Proc.
of NIPS, volume 22, pages 1973?1981.
NIPS.Xing Wei and W Bruce Croft.
2006.
LDA-based Docu-ment Models for Ad-hoc Retrieval.
In Proc.
of SIGIR,pages 178?185.
ACM.7Jonathan Wintrode and Khudanpur Sanjeev.
2014.
Com-bining Local and Broad Topic Context to ImproveTerm Detection.
In Proc.
of Spoken Language Tech-noloty Workshop.
IEEE.Jonathan Wintrode.
2014a.
Can you Repeat that?
UsingWord Repetition to Improve Spoken Term Detection.In Proc.
of ACL.
ACL.Jonathan Wintrode.
2014b.
Limited Resource Term De-tection For Effective Topic Identification of Speech.
InProc.
of ICASSP.
IEEE.YouTube.
2015.
Statistics - YouTube.http://www.youtube.com/yt/press/statistics.html, February.8
