Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 1?9,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsData Homogeneity and Semantic Role Tagging in ChineseOi Yee Kwong and Benjamin K. TsouLanguage Information Sciences Research CentreCity University of Hong KongTat Chee Avenue, Kowloon, Hong Kong{rlolivia, rlbtsou}@cityu.edu.hkAbstractThis paper reports on a study of semanticrole tagging in Chinese in the absence of aparser.
We tackle the task by identifyingthe relevant headwords in a sentence as afirst step to partially locate the corre-sponding constituents to be labelled.
Wealso explore the effect of data homogene-ity by experimenting with a textbook cor-pus and a news corpus, representingsimple data and complex data respectively.Results suggest that while the headwordlocation method remains to be improved,the homogeneity between the training andtesting data is important especially inview of the characteristic syntax-semantics interface in Chinese.
We alsoplan to explore some class-based tech-niques for the task with reference to exist-ing semantic lexicons, and to modify themethod and augment the feature set withmore linguistic input.1 IntroductionAs the development of language resources pro-gresses from POS-tagged corpora to syntacticallyannotated treebanks, the inclusion of semantic in-formation such as predicate-argument relationsbecomes indispensable.
The expansion of the PennTreebank into a Proposition Bank (Kingsbury andPalmer, 2002) is a typical move in this direction.Lexical resources also need to be enhanced withsemantic information (e.g.
Fellbaum et al, 2001).The ability to identify semantic role relations cor-rectly is essential to many applications such as in-formation extraction and machine translation; andmaking available resources with this kind of in-formation would in turn facilitate the developmentof such applications.Large-scale production of annotated resourcesis often labour intensive, and thus calls for auto-matic labelling to streamline the process.
The taskis essentially done in two phases, namely recognis-ing the constituents bearing some semantic rela-tionship to the target verb in a sentence, and thenlabelling them with the corresponding semanticroles.In their seminal proposal, Gildea and Jurafsky(2002) approached the task using various featuressuch as headword, phrase type, and parse tree path.While such features have remained the basic andessential features in subsequent research, parsedsentences are nevertheless required, for extractingthe path features during training and providing theargument boundaries during testing.
The parseinformation is deemed important for the perform-ance of role labelling (Gildea and Palmer, 2002;Gildea and Hockenmaier, 2003).More precisely, parse information is rathermore critical for the identification of boundaries ofcandidate constituents than for the extraction oftraining data.
Its limited function in training, forinstance, is reflected in the low coverage reported(e.g.
You and Chen, 2004).
As full parses are notalways accessible, many thus resort to shallow syn-tactic information from simple chunking, eventhough results often turn out to be less satisfactorythan with full parses.This limitation is even more pertinent for theapplication of semantic role labelling to languageswhich do not have sophisticated parsing resources.In the case of Chinese, for example, there is con-1siderable variability in its syntax-semantics inter-face; and when one comes to more nested andcomplex sentences such as those from news arti-cles, it becomes more difficult to capture the sen-tence structures by typical examples.Thus in the current study, we approach theproblem in Chinese in the absence of parse infor-mation, and attempt to identify the headwords inthe relevant constituents in a sentence to be taggedas a first step.
In addition, we will explore the ef-fect of training on different datasets, simple orcomplex, to shed light on the relative importanceof parse information for indicating constituentboundaries in semantic role labelling.In Section 2, related work will be reviewed.
InSection 3, the data used in the current study will beintroduced.
Our proposed method will be ex-plained in Section 4, and the experiment reportedin Section 5.
Results and future work will be dis-cussed in Section 6, followed by conclusions inSection 7.2 Related WorkThe definition of semantic roles falls on a contin-uum from abstract ones to very specific ones.Gildea and Jurafsky (2002), for instance, used a setof roles defined according to the FrameNet model(Baker et al, 1998), thus corresponding to theframe elements in individual frames under a par-ticular domain to which a given verb belongs.Lexical entries (in fact not limited to verbs, in thecase of FrameNet) falling under the same framewill share the same set of roles.
Gildea and Palmer(2002) defined roles with respect to individualpredicates in the PropBank, without explicit nam-ing.
To date PropBank and FrameNet are the twomain resources in English for training semanticrole labelling systems, as in the CoNLL-2004shared task (Carreras and M?rquez, 2004) andSENSEVAL-3 (Litkowski, 2004).The theoretical treatment of semantic roles isalso varied in Chinese.
In practice, for example,the semantic roles in the Sinica Treebank mark notonly verbal arguments but also modifier-head rela-tions (You and Chen, 2004).
In our present study,we go for a set of more abstract semantic rolessimilar to the thematic roles for English used inVerbNet (Kipper et al, 2002).
These roles aregeneralisable to most Chinese verbs and are notdependent on particular predicates.
They will befurther introduced in Section 3.Approaches in automatic semantic role label-ling are mostly statistical, typically making use ofa number of features extracted from parsed trainingsentences.
In Gildea and Jurafsky (2002), the fea-tures studied include phrase type (pt), governingcategory (gov), parse tree path (path), position ofconstituent with respect to the target predicate (po-sition), voice (voice), and headword (h).
The la-belling of a constituent then depends on itslikelihood to fill each possible role r given the fea-tures and the target predicate t, as in the following,for example:),,,,,|( tvoicepositiongovpthrPSubsequent studies exploited a variety of im-plementation of the learning component.
Trans-formation-based approaches were also used (e.g.see Carreras and M?rquez (2004) for an overviewof systems participating in the CoNLL shared task).Swier and Stevenson (2004) innovated with an un-supervised approach to the problem, using a boot-strapping algorithm, and achieved 87% accuracy.While the estimation of the probabilities couldbe relatively straightforward, the trick often lies inlocating the candidate constituents to be labelled.A parser of some kind is needed.
Gildea andPalmer (2002) compared the effects of full parsingand shallow chunking; and found that when con-stituent boundaries are known, both automaticparses and gold standard parses resulted in about80% accuracy for subsequent automatic role tag-ging, but when boundaries are unknown, resultswith automatic parses dropped to 57% precisionand 50% recall.
With chunking only, performancefurther degraded to below 30%.
Problems mostlyarise from arguments which correspond to morethan one chunk, and the misplacement of core ar-guments.
Sun and Jurafsky (2004) also reported adrop in F-score with automatic syntactic parsescompared to perfect parses for role labelling inChinese, despite the comparatively good results oftheir parser (i.e.
the Collins parser ported to Chi-nese).
The necessity of parse information is alsoreflected from recent evaluation exercises.
Forinstance, most systems in SENSEVAL-3 used aparser to obtain full syntactic parses for the sen-tences, whereas systems participating in theCoNLL task were restricted to use only shallow2syntactic information.
Results reported in the for-mer tend to be higher.
Although the dataset maybe a factor affecting the labelling performance, itnevertheless reinforces the usefulness of full syn-tactic information.According to Carreras and M?rquez (2004), forEnglish, the state-of-the-art results reach an F1measure of slightly over 83 using gold standardparse trees and about 77 with real parsing results.Those based on shallow syntactic information isabout 60.In this work, we study the problem in Chinese,treating it as a headword identification and label-ling task in the absence of parse information, andexamine how the nature of the dataset could affectthe role tagging performance.3 The Data3.1 MaterialsIn this study, we used two datasets: sentences fromprimary school textbooks were taken as examplesfor simple data, while sentences from a large cor-pus of newspaper texts were taken as complex ex-amples.Two sets of primary school Chinese textbookspopularly used in Hong Kong were taken for refer-ence.
The two publishers were Keys Press andModern Education Research Society Ltd.  Textsfor Primary One to Six were digitised, segmentedinto words, and annotated with parts-of-speech(POS).
This results in a text collection of about165K character tokens and upon segmentationabout 109K word tokens (about 15K word types).There were about 2,500 transitive verb types, withfrequency ranging from 1 to 926.The complex examples were taken from a sub-set of the LIVAC synchronous corpus1 (Tsou et al,2000; Kwong and Tsou, 2003).
The subcorpusconsists of newspaper texts from Hong Kong, in-cluding local news, international news, financialnews, sports news, and entertainment news, col-lected in 1997-98.
The texts were segmented intowords and POS-tagged, resulting in about 1.8Mcharacter tokens and upon segmentation about 1Mword tokens (about 47K word types).
There wereabout 7,400 transitive verb types, with frequencyranging from 1 to just over 6,300.1 http://www.livac.org3.2 Training and Testing DataFor the current study, a set of 41 transitive verbscommon to the two corpora (hereafter referred toas textbook corpus and news corpus), with fre-quency over 10 and over 50 respectively, wassampled.Sentences in the corpora containing the sam-pled verbs were extracted.
Constituents corre-sponding to semantic roles with respect to thetarget verbs were annotated by a trained humanannotator and the annotation was verified by an-other.
In this study, we worked with a set of 11predicate-independent abstract semantic roles.According to the Dictionary of Verbs in Contem-porary Chinese (Xiandai Hanyu Dongci Dacidian,?????????
?
Lin et al, 1994), our se-mantic roles include the necessary arguments formost verbs such as agent and patient, or goal andlocation in some cases; and some optional argu-ments realised by adjuncts, such as quantity, in-strument, and source.
Some examples of semanticroles with respect to a given predicate are shown inFigure 1.Altogether 980 sentences covering 41 verbtypes in the textbook corpus were annotated, re-sulting in 1,974 marked semantic roles (constitu-ents); and 2,122 sentences covering 41 verb typesin the news corpus were annotated, resulting in4,933 marked constituents2.The role labelling system was trained on 90%of the sample sentences from the textbook corpusand the news corpus separately; and tested on theremaining 10% of both corpora.4 Automatic Role LabellingThe automatic labelling was based on the statisticalapproach in Gildea and Jurafsky (2002).
In Sec-tion 4.1, we will briefly mention the features usedin the training process.
Then in Sections 4.2 and4.3, we will explain our approach for locatingheadwords in candidate constituents associatedwith semantic roles, in the absence of parse infor-mation.2 These figures only refer to the samples used in the currentstudy.
In fact over 35,000 sentences in the LIVAC corpushave been semantically annotated, covering about 1,500 verbtypes and about 80,000 constituents were marked.34.1 TrainingIn this study, our probability model was basedmostly on parse-independent features extractedfrom the training sentences, namely:Headword (head): The headword from each con-stituent marked with a semantic role was identified.For example, in the second sentence in Figure 1,??
(school) is the headword in the constituentcorresponding to the agent of the verb ??
(hold),and ??
(contest) is the headword of the nounphrase corresponding to the patient.Position (posit): This feature shows whether theconstituent being labelled appears before or afterthe target verb.
In the first example in Figure 1,the experiencer and time appear on the left of thetarget, while the theme is on its right.POS of headword (HPos): Without features pro-vided by the parse, such as phrase type or parsetree path, the POS of the headword of the labelledconstituent could provide limited syntactic infor-mation.Preposition (prep): Certain semantic roles liketime and location are often realised by preposi-tional phrases, so the preposition introducing therelevant constituents would be an informative fea-ture.Hence for automatic labelling, given the targetverb t, the candidate constituent, and the abovefeatures, the role r which has the highest probabil-ity for P(r | head, posit, HPos, prep, t) will be as-signed to that constituent.
In this study, however,we are also testing with the unknown boundarycondition where candidate constituents are notavailable in advance.
To start with, we attempt topartially locate them by identifying their head-words first, as explained in the following sections.Figure 1  Examples of semantic roles with respect to a given predicate4.2 Locating Candidate HeadwordsIn the absence of parse information, and with con-stituent boundaries unknown, we attempt to par-tially locate the candidate constituents byidentifying their corresponding headwords first.Sentences in our test data were segmented intowords and POS-tagged.
We thus divide the recog-nition process into two steps, locating the head-word of a candidate constituent first, and thenexpanding from the headword to determine itsboundaries.Student?
??
??
??
?
??
?
?Next week school hold tell story contestTime Agent Target PatientExample: (Next week, the school will hold a story-telling contest.)??
?
??
??
??
?
?
(-pl) write essay always feel (neg) anythingExperiencer Target ThemeExample: (Students always feel there is nothing to write about for their essays.)?
?time??
?canTimewrite4Basically, if we consider every word in thesame sentence with the target verb (both to its leftand to its right) a potential headword for a candi-date constituent, what we need to do is to find outthe most probable words in the sentence to matchagainst individual semantic roles.
We start with afeature set with more specific distributions, andback off to feature sets with less specific distribu-tions3.
Hence in each round we look for)|(maxarg setfeaturerPrfor every candidate word.
Ties are resolved bygiving priority to the word nearest to the targetverb in the sentence.Figure 2 shows an example illustrating the pro-cedures for locating candidate headwords.
Thetarget verb is ??
(discover).
In the first round,using features head, posit, HPos, and t, ??
(time)and ??
(problem) were identified as Time andPatient respectively.
In the fourth subsequentround, backing off with features posit and HPos,??
(we) was identified as a possible Agent.
Inthis round a few other words were identified aspotential Patients.
However, they would not beconsidered since Patient was already located in aprevious round.
So in the end the headwords iden-tified for the test sentence are ??
for Agent, ??
for Patient and ??
for Time.4.3 Constituent BoundaryUpon the identification of headwords for potentialconstituents, the next step is to expand from theseheadwords for constituent boundaries.
Althoughwe are not doing this step in the current study, itcan potentially be done via some finite state tech-niques, or better still, with shallow syntactic proc-essing like simple chunking if available.3 In this experiment, we back off in the following order:P(r|head, posit, HPos, prep t), P(r|head, posit, t), P(r | head, t),P(r | HPos, posit, t), P(r | HPos, t).
However, the prep featurebecomes obsolete when constituent boundaries are unknown.5 The Experiment5.1 TestingThe system was trained on the textbook corpus andthe news corpus separately, and tested on both cor-pora (the data is homogeneous if the system istrained and tested on materials from the samesource).
The testing was done under the ?knownconstituent?
condition and ?unknown constituent?condition.
The former essentially corresponds tothe known-boundary condition in related studies;whereas in the unknown-constituent condition,which we will call ?headword location?
conditionhereafter, we tested our method of locating candi-date headwords as explained above in Section 4.2.In this study, every noun, verb, adjective, pronoun,classifier, and number within the test sentence con-taining the target verb was considered a potentialheadword for a candidate constituent correspond-ing to some semantic role.
The performance wasmeasured in terms of the precision (defined as thepercentage of correct outputs among all outputs),recall (defined as the percentage of correct outputsamong expected outputs), and F1 score which is theharmonic mean of precision and recall.5.2 ResultsThe results are shown in Tables 1 and 2, for train-ing on homogeneous dataset and different datasetrespectively, and testing under the known constitu-ent condition and the headword location condition.When trained on homogeneous data, the resultswere good on both datasets under the known con-stituent condition, with an F1 score of about 90.This is comparable or even better to the results re-ported in related studies for known boundary con-dition.
The difference is that we did not use anyparse information in the training, not even phrasetype.
When trained on a different dataset, however,the accuracy was maintained for textbook data, butit decreased for news data, for the known constitu-ent condition.For the headword location condition, the per-formance in general was expectedly inferior to thatfor the known constituent condition.
Moreover,this degradation seemed to be quite consistent inmost cases, regardless of the nature of the trainingset.
In fact, despite the effect of training set onnews data, as mentioned above, the degradation5Sentence:????????????????????????????????????
?During revision, we discover a lot o?f problems which we have not thought of or cannot besolved, then we go and ask father.Candidate  Round 1       ?
Round 4    Final Resulteadwordsn)H??
(revisio    Patient??
(time)  Time            ----       Time??
(we)    Agent       Agentk)    Patient??
(normally)??
(thin?
(can)??
(solve)    Patient??
(problem)  Patient    ----       Patient?
(go)     Patient?
(ask)     Patientfrom known constituent to headword location isnevertheless the least fo??
(father)    Patientr news data when trainedonremature at this stage, given the considerable dif-Figure 2  Example illustrating the procedures for locating candidate headwordsTex ata News adifferent materials.Hence the effect of training data is only obviousin the news corpus.
In other words, both sets oftraining data work similarly well with textbook testdata, but the performance on news test data isworse when trained on textbook data.
This is un-derstandable as the textbook data contain fewerexamples and the sentence structures are usuallymuch simpler than those in newspapers.
Hence thesystem tends to miss many secondary roles likelocation and time, which are not sufficiently repre-sented in the textbook corpus.
The conclusion thattraining on news data gives better result might beference in the corpus size of the two datasets.Nevertheless, the deterioration of results on text-book sentences, even when trained on news data, issimply reinforcing the importance of data homoge-neity, if nothing else.
More on data homogeneitywill be discussed in the next section.pIn addition, the surprisingly low precision underthe headword location condition is attributable to atechnical inadequacy in the way we break ties.
Inthis study we only make an effort to eliminate mul-tiple tagging of the same role to the same targetverb in a sentence on either side of the target verb,but not if they appear on both sides of the targetverb.
This should certainly be dealt with in futureexperiments.tbook D DatP  Precisionrecision Recall F1 Recall F1Known Constituent 93.85 87.50 90.56 90.49 87.70 89.07Headword Location 46.12 61.98 52.89 38.52 52.25 44.35Table 1  Results for Training on Homogeneous Datasets6Tex ata News a  tbook D DatP  Precisionrecision Recall F1 Recall F1Known Constituent 91.85 88.02 89.86 80.30 66.80 72.93Headword Location 38.87 57.29 46.32 37.89 42.01 39.84Table 2  Results for Training on Different Datasets6 Discussionhencuss this below in relation ton?, duration as in?
?d the parse informationwoverb ??
, being verypolhe designhe feature set should benefitm nalysis and input.6.1 Role of Parse InformationAccording to Carreras and M?rquez (2004), thestate-of-the-art results for semantic role labellingsystems based on shallow syntactic information isabout 15 lower than those with access to gold stan-dard parse trees, i.e., around 60.
With homogene-ous training and testing data, our experimentalresults for the headword location condition, withno syntactic information available at all, give an F1score of 52.89 and 44.35 respectively for textbookdata and news data.
Such results are in line withand comparable to those reported for the unknownboundary condition with automatic parses inGildea and Palmer (2002), for instance.
Moreover,when they used simple chunks instead of fullparses, the performance resulted in a drop to below50% precision and 35% recall with relaxed scoring,ce their conclusion on the necessity of a parser.The more degradation in performance observedin the news data is nevertheless within expectation,and it suggests that simple and complex data seemto have varied dependence on parse information.We will further disdata homogeneity.6.2 Data HomogeneityThe usefulness of parse information for semanticrole labelling is especially interesting in the case ofChinese, given the flexibility in its syntax-semantics interface (e.g.
the object after ?
?eat?could refer to the patient as in ???
?eat apple?,location as in ???
?eat cantee?
?eat three years?, etc.
).As reflected from the results, the nature oftraining data is obviously more important for thenews data than the textbook data; and the mainreason might be the failure of the simple trainingdata to capture the many complex structures of thenews sentences, as we suggested earlier.
The rela-tive flexibility in the syntax-semantics interface ofChinese is particularly salient; hence when a sen-tence gets more complicated, there might be moreintervening constituents anuld be useful to help identify the relevant onesin semantic role labelling.With respect to the data used in the experiment,we tried to explore the complexity in terms of theaverage sentence length and number of semanticrole patterns exhibited.
For the news data, the av-erage sentence length is around 59.7 characters(syllables), and the number of semantic role pat-terns varies from 4 (e.g.
??
?to plan?)
to as manyas 25 (e.g.
??
?to proceed with some action?
),with an average of 9.5 patterns per verb.
On theother hand, the textbook data give an average sen-tence length of around 39.7 characters, and thenumber of semantic role patterns only varies from1 (e.g.
??
?to decide?)
to 11 (e.g.
??
?to holdsome event?
), with an average of 5.1 patterns perverb.
Interestingly, theymorphous in news texts, only shows 5 differ-ent patterns in textbooks.Thus the nature of the dataset for semantic rolelabelling is worth further investigation.
Tof the method and tfrom ore linguistic a6.3 Future WorkIn terms of future development, apart from improv-ing the handling of ties in our method, as men-tioned above, we plan to expand our work inseveral respects.
The major part would be on thegeneralization to unseen headwords and unseenpredicates.
As is with other related studies, theexamples available for training for each target verbare very limited; and the availability of trainingdata is also insufficient in the sense that we cannotexpect them to cover all target verb types.
Hence7it is very important to be able to generalize theprocess to unseen words and predicates.
To thisend we will experiment with a semantic lexiconlike Tongyici Cilin (????
?, a Chinese the-saure ofChinese, we intend to improve our method andre linguistic consideration.semantic lexicons,and to modify the method and augment the featureset with more linguistic input.This work is supported by Competitive EarmarkedResearch Grants (CERG) of the Research GrantsHong Kong under grant Nos.RBatheCa troduction to theFeResources, Invited Talk, Pittsburg, PA.Gi D. and Palmer, M. (2002)  The Necessity ofGi kenmaier, J.
(2003)  Identifying Se-Kwr Part-of-speechTagging.
In Proceedings of the Research Note Ses-sion of the 10th Conference of the European Chapterrus) in both training and testing, which we ex-pect to improve the overall performance.Another area of interest is to look at the behav-iour of near-synonymous predicates in the taggingprocess.
Many predicates may be unseen in thetraining data, but while the probability estimationcould be generalized from near-synonyms as sug-gested by a semantic lexicon, whether the similar-ity and subtle differences between near-synonymswith respect to the argument structure and the cor-responding syntactic realisation could be distin-guished would also be worth studying.
Related tothis is the possibility of augmenting the feature set.Xue and Palmer (2004), for instance, looked intonew features such as syntactic frame, lexicalizedconstituent type, etc., and found that enriching thefeature set improved the labelling performance.
Inparticular, given the importance of data homogene-ity as observed from the experimental results, andthe challenges posed by the characteristic natufeature set with mo7 ConclusionThe study reported in this paper has thus tackledsemantic role labelling in Chinese in the absence ofparse information, by attempting to locate the cor-responding headwords first.
We experimentedwith both simple and complex data, and have ex-plored the effect of training on different datasets.Using only parse-independent features, our resultsunder the known boundary condition are compara-ble to those reported in related studies.
The head-word location method can be further improved.More importantly, we have observed the impor-tance of data homogeneity, which is especially sa-lient given the relative flexibility of Chinese in itssyntax-semantics interface.
As a next step, weplan to explore some class-based techniques for thetask with reference to existingAcknowledgementsCouncil ofCityU1233/01H and CityU1317/03H.eferencesker, C.F., Fillmore, C.J.
and Lowe, J.B. (1998)  TheBerkeley FrameNet Project.
In Proceedings of36th Annual Meeting of the Association for Computa-tional Linguistics and the 17th International Confer-ence on Computational Linguistics (COLING-ACL ?98), Montreal, Quebec, Canada, pp.86-90.rreras, X. and M?rquez, L. (2004)  InCoNLL-2004 Shared Task: Semantic Role Labeling.In Proceedings of the Eighth Conference on Compu-tational Natural Language Learning (CoNLL-2004),Boston, Massachusetts, pp.89-97.llbaum, C., Palmer, M., Dang, H.T., Delfs, L. andWolf, S. (2001)  Manual and Automatic SemanticAnnotation with WordNet.
In Proceedings of theNAACL-01 SIGLEX Workshop on WordNet andOther LexicalGildea, D. and Jurafsky, D. (2002)  Automatic Labelingof Semantic Roles.
Computational Linguistics, 28(3):245-288.ldea,Parsing for Predicate Argument Recognition.
In Pro-ceedings of the 40th Meeting of the Association forComputational Linguistics (ACL-02), Philadelphia,PA.ldea, D. and Hocmantic Roles Using Combinatory Categorial Gram-mar.
In Proceedings of the 2003 Conference onEmpirical Methods in Natural Language Processing,Sapporo, Japan.Kingsbury, P. and Palmer, M. (2002)  From TreeBankto PropBank.
In Proceedings of the Third Confer-ence on Language Resources and Evaluation (LREC-02), Las Palmas, Canary Islands, Spain.Kipper, K., Palmer, M. and  Rambow, O.
(2002)  Ex-tending PropBank with VerbNet Semantic Predicates.In Proceedings of the AMTA-2002 Workshop on Ap-plied Interlinguas, Tiburon, CA.ong, O.Y.
and Tsou, B.K.
(2003) Categorial Fluidityin Chinese and its Implications fo8of the Association for Computational Linguistics,Budapest, Hungary, pages 115-118.n, X., Wang, L. and Sun, D. Li (1994)  Dictionary ofLiuation of(Sunapter of the Association for Computa-SwiTsings ofXu2004Yo.109-115.?
ongguo Yuwen.
Primary 1-6,24 volumes, 2004.
Hong Kong: Modern EducationResearch Society Ltd.Verbs in Contemporary Chinese.
Beijing Languageand Culture University Press.tkowski, K.C.
(2004) SENSEVAL-3 Task: AutomaticLabeling of Semantic Roles.
In Proceedings of theThird International Workshop on the EvalSystems for the Semantic Analysis of TextSENSEVAL-3), Barcelona, Spain, pp.9-12., H. and Jurafsky, D. (2004)  Shallow SemanticParsing of Chinese.
In Proceedings of the HumanLanguage Technology Conference of the NorthAmerican Chtional Linguistics (HLT-NAACL 2004), Boston,pp.249-256.er, R.S.
and Stevenson, S. (2004)  UnsupervisedSemantic Role Labelling.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing, Barcelona, Spain, pp.95-102.ou, B.K., Tsoi, W.F., Lai, T.B.Y., Hu, J. and Chan,S.W.K.
(2000)  LIVAC, A Chinese SynchronousCorpus, and Some Applications.
In Proceedthe ICCLC International Conference on ChineseLanguage Computing, Chicago, pp.
233-238.e, N. and Palmer, M. (2004)  Calibrating Features forSemantic Role Labeling.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, Barcelona, Spain, pp.88-94.u, J-M. and Chen, K-J.
(2004)  Automatic SemanticRole Assignment for a Tree Structure.
In Proceed-ings of the 3rd SigHAN Workshop on Chinese Lan-guage Processing, ACL-04, Barcelona, pp??????
Qisi Zhongguo Yuwen.
Primary 1-6, 24volumes, 2004.
Hong Kong: Keys Press.?????
Xiandai Zh9
