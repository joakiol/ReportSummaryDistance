Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 233?243, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsMultiple Aspect Summarization Using Integer Linear ProgrammingKristian Woodsend and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABk.woodsend@ed.ac.uk, mlap@inf.ed.ac.ukAbstractMulti-document summarization involvesmany aspects of content selection and sur-face realization.
The summaries must beinformative, succinct, grammatical, and obeystylistic writing conventions.
We present amethod where such individual aspects arelearned separately from data (without anyhand-engineering) but optimized jointlyusing an integer linear programme.
TheILP framework allows us to combine thedecisions of the expert learners and to selectand rewrite source content through a mixtureof objective setting, soft and hard constraints.Experimental results on the TAC-08 data setshow that our model achieves state-of-the-artperformance using ROUGE and signifi-cantly improves the informativeness of thesummaries.1 IntroductionAutomatic summarization has enjoyed wide popu-larity in natural language processing (see the pro-ceedings of the Document Understanding and TextAnalysis conferences) due to its potential for prac-tical applications but also because it incorporatesmany important aspects of both natural language un-derstanding and generation.
Of the many summa-rization paradigms that have been identified over theyears (see Sparck Jones (1999) and Mani (2001) forcomprehensive overviews), multi-document sum-marization ?
the task of producing summaries fromclusters of thematically related documents ?
hasconsistently attracted attention.Despite considerable research effort, the auto-matic generation of multi-document summaries thatresemble those written by humans remains chal-lenging.
This is primarily due to the task itselfwhich is complex and subject to several constraints:the summary must be maximally informative andminimally redundant, grammatical, coherent, adhereto a pre-specified length and stylistic conventions.An ideal model would learn to output summariesthat simultaneously meet alhese constraints fromdata (i.e., document clusters and their correspond-ing summaries).
This global inference problem is,however, hard ?
the solution space is large and thelack of easily accessible datasets an obstacle to jointlearning.
It is thus no surprise that previous work hasfocused on specific aspects of joint learning.Initial global formulations of the multi-documentsummarization task focused on extractive summa-rization and used approximate greedy algorithms forfinding the sentences of the summary.
Goldstein etal.
(2000) search for the set of sentences that areboth relevant and non-redundant, whereas Filatovaand Hatzivassiloglou (2004) model multi-documentsummarization as an instance of the maximum cov-erage set problem.1 More recent work improves onthe search problem by considering exact solutionsand permits a limited amount of rewriting.
McDon-ald (2007) proposes an integer linear programmingformulation that maximizes the sum of relevancescores of the selected sentences penalized by the1Given C, a finite set of weighted elements, a collection T ofsubsets of C, and an integer k, find those k sets that maximize thetotal number of elements in the union of T ?s members (Hochba,1997).233sum of redundancy scores of all pairs of selectedsentences.
Gillick et al2008) develop an exact so-lution for a model similar to Filatova and Hatzivas-siloglou (2004) under the assumption that the valueof a summary is the sum of values of the unique con-cepts (approximated by bigrams) it contains.
Subse-quent work (Gillick et al2009; Berg-Kirkpatrick etal., 2011) extends this model to allow sentence com-pression in the form of word or constituent deletion.In this paper we propose a model for multi-document summarization that attempts to covermany different aspects of the task such as content se-lection, surface realization, paraphrasing, and stylis-tic conventions.
These aspects are learned separatelyusing specific ?expert?
predictors, but are optimizedjointly using an integer linear programming model(ILP) to generate the output summary.2 All expertsare learned from data without requiring additionalannotation over and above the summaries writtenfor each document cluster.
Our predictors includethe use of unique bigram information to model con-tent and avoid redundancy, positional information tomodel important and poor locations of content, andlanguage modeling to capture stylistic conventions.Learning each predictor separately gives better gen-eralization, while the ILP framework allows us tocombine the decisions of the expert learners throughthe use of objectives, hard and soft constraints.The experts work collaboratively to rewrite thecontent using rules extracted from document clustersand model summaries.
We adopt the synchronoustree substitution grammar (STSG) formalism (Eis-ner, 2003) which can model non-isomorphic treestructures (the grammar rules can comprise trees ofarbitrary depth) and is thus suited to text-rewritingtasks which typically involve a number of local mod-ifications to the input text.
Specifically, we pro-pose quasi-synchronous tree substitution grammar(QTSG) as a flexible formalism to learn general tree-edits from loosely-aligned phrase structure trees.We evaluate our model on the 100-word ?non-2Our task is standard multi-document summarization andshould not be confused with ?guided?
summarization wheresystem and human summarizers are given a list of importantaspects to cover in the summary.
Our usage of the term aspectsbroadly refers to the different types of constraints (e.g., relatingto content or style) a summary must meet, but these are learnedrather than specified in advance.update?
summarization task as defined in the theText Analysis Conference (TAC 2008).
Experimen-tal results show that our method obtains perfor-mance comparable and in some cases superior tostate-of-the-art, in terms of ROUGE and human rat-ings of summary grammaticality and informative-ness.
Importantly, there is nothing inherent in ourmodel that is specific to this particular summariza-tion task.
As all of the different experts are learnedfrom data, it could easily adapt to other summariza-tion styles or conventions as needed.2 Related workRecent years have seen increased interest in globalinference methods for summarization.
ILP-basedmodels have been developed for several subtasksranging from sentence compression (Clarke and La-pata, 2008), to single- and multi-document sum-marization (McDonald, 2007; Martins and Smith,2009; Gillick and Favre, 2009; Woodsend and Lap-ata, 2010; Berg-Kirkpatrick et al2011), and head-line generation (Deshpande et al2007; Wood-send et al2010).
Most of these approaches are ei-ther purely extractive or implement a single rewriteoperation, namely word deletion.
Although it iswell-known that hand-written summaries often ex-hibit additional edits and sentence recombinations(Jing, 2002), the challenges involved in acquiringthe rewrite rules, interfacing them with inference,and ensuring grammatical output make the develop-ment of abstractive models non-trivial.Our work is closest to Gillick et al2008) whoalso develop an ILP model for multi-document sum-marization.
A key assumption in their model whichwe also follow is that input documents contain avariety of concepts, each of which are allocated avalue, and the goal of a good summary is to max-imize the sum of these values subject to the lengthconstraint.
The authors use bigrams as concepts andtheir frequency in the input documents as a proxyfor their value.
This model can also perform sen-tence compression (see also Gillick et al2009)),however, the deletion rules are hand-coded.
Berg-Kirkpatrick et al2011) build on this work by re-casting it as a structured prediction problem.
Theyessentially combine the same bigram content scor-ing system with features relating to the parse tree234which they learn using a maximum-margin SVMtrained on annotated gold-standard compressions.Our multi-document summarization model jointlyoptimizes different aspects of the task involving bothcontent selection and surface realization.
Each indi-vidual aspect has its own dedicated expert, which weargue is advantageous as it renders inference simplerand affords flexibility (e.g., additional aspects can beincorporated into the model or trained separately ondifferent datasets).
Our work differs from Gillick etal.
(2009) and Berg-Kirkpatrick et al2011) in threeimportant respects.
Firstly, we develop a genuinelyabstractive model that is not limited to deletion.Our rewrite rules are encoded in quasi-synchronoustree substitution grammar and learned automaticallyfrom source documents and their summaries.
Un-like previous applications of STSG to sentence com-pression (Cohn and Lapata, 2009; Cohn and Lap-ata, 2008) our quasi-synchronous TSG does not at-tempt to learn the complete translation from sourceto target sentence; it only loosely links the syntacticstructure of the two (Smith and Eisner, 2006), andis therefore well suited to describing the relation-ship between documents and their abstracts.
Sec-ondly, our content selection component extends tofeatures beyond the bigram horizon, as we learn toidentify important concepts based on syntactic andpositional information.
We also learn which wordsare unlikely to appear in a summary.
Thirdly, unlikeBerg-Kirkpatrick et al2011) our model does nottry to learn all the parameters (e.g., content, rewriterules, style) of the summarization problem jointly;although decoupling learning from inference is per-haps less elegant from a modeling perspective, thelearning process is more robust and reliable.3 ModelingThere are many aspects to producing a good sum-mary of multiple documents.
The important con-tent needs to be captured, typically key facts ineach individual document, and information seenacross the cluster.
Stylistic features may be differ-ent in the summary from original documents.
Forinstance, summaries tend to use more concise lan-guage, sources are not attributed as they are in newsarticles, and relative dates are not included.
In addi-tion, the summary must be fluent, coherent, and re-spect a pre-specified maximum length requirement.We present an approach where elements of all theabove considerations are learned from training databy separate dedicated components, and then com-bined in an integer linear programme.
Content se-lection is performed partly through identifying themost salient topics (bigrams); an additional compo-nent learns to identify which information from thesource documents should be in the summary basedon positional information.
Meanwhile, in terms ofsurface realization, a language model identifies thewords that should not be in the output summaries,whereas a separate component learns to excludesentences that are poor candidates for summaries.QTSG rules, learned from the training corpus, areused to generate alternative compressions and para-phrases of the source sentences, in the style suit-able for the summaries.
Finally, an ILP model com-bines the output of these components into a sum-mary, jointly optimizing content selection and sur-face realization preferences, and providing the flexi-bility to treat some components as soft while othersas hard constraints.3.1 Document RepresentationGiven an input sentence, our approach deconstructsit into component phrases and clauses, typical of aphrase structure parser.
In our experiments, we ob-tain this representation from the output of the Stan-ford parser (Klein and Manning, 2003) but any otherbroadly similar parser could be used instead.
Nodesin the parse tree represent points where QTSG rulescan be applied (and paraphrases generated), and theyalso represent decision points for the ILP.
In the fol-lowing, we will refer to these decision nodes as theset N , and decisions for each node using the binaryvariable zi, i ?N .3.2 Content Selection Using BigramsWe follow Gillick et al2008) in modeling the infor-mation content of the summary as the weighted sumof the individual information units it contains.
Werepresent information units as the set of bigrams Bseen in the source documents.
The weight w of eachbigram is calculated from the number of source doc-uments where the bigram was seen.
The summary isthus given the score fB(z), i.e., the weighted sum of235its information units:fB(z) = ?j?Bw jb j (1)where w j is the weight of concept j, b j a binary vari-able to indicate if concept j is present in the sum-mary, and j ?
B .Importantly, each information unit is counted onlyonce; this encourages wide coverage of the sourcedocuments, and removes any drive towards redun-dant information without actively discouraging it,contrary to other global formulations where redun-dancy measures form part of the objective (McDon-ald, 2007).
The counting mechanism is achieved bylinking the variables z indicating nodes in the parsetree and b indicating bigrams:b j ?
?i?N : j?Bizi ?
j ?
B (2)where Bi ?
B is the subset of bigrams that are con-tained in node i.
A drawback of the global natureof this counting mechanism, however, is that it can-not be integrated with local features such as thosedescribed below; our approach takes local featuresinto account but these are weighted by other compo-nents.3.3 Content Selection Using SalienceThe bigram approach is a powerful method foridentifying important concepts within the documentcluster.
It works particularly well in the sentence ex-traction paradigm.
However, additional elements areknown to be good predictors of important informa-tion.
Examples include the position of a sentencein the document (e.g., first sentences often con-tain salient information), whether it contains propernouns, numbers, pronouns, mentions of money, andso on.
We decided to learn which of these elements(represented as nodes in the parse tree) are infor-mative from training data.
Specifically, sentencesin the cluster documents were aligned to sentencesfrom corresponding human summaries.
Alignmentwas based rather simply on identifying the sentencepairs with the highest number of overlapping bi-grams, without compensating for sentence length, ormatching the sequence of information in the sum-maries and source documents (Nelken and Schieber,Weight Feature1.21 From first sentence in document0.73 Contains proper nouns0.68 Contains nouns0.57 From first paragraph0.53 From first three sentences0.51 Contains numbers-0.50 Contains pronouns0.32 Contains moneyTable 1: Weights and features of SVM that predicts thesalience of summary content.
Negative weights indicateinformation that should not be included in the summary.2006).
Matched sentences in the source documentswere given positive labels, while unaligned sen-tences were given negative labels.
These labels werethen propagated to phrase structure nodes.We trained an SVM on this data (tree nodes andtheir labels) using surface features that do not over-lap with bigram information: sentence and para-graph position, POS-tag information.
Table 1 showsthe most important features learned by the model aspredictors of salient content.The summary can be given a salience score fS (z)using the raw SVM prediction scores of the individ-ual parse tree nodes:fS (z) = ?i?N(?
(i) ??
)zi (3)where ?
(i) is the feature vector for node i, and ?
theweights learned by the SVM.3.4 Surface Realization Using StyleSome sentences in the source documents will makepoor summary sentences, despite the informationthey contain, and therefore contrary to the predic-tions of the content selection indicators describedabove.
This may be because the source sentence isvery short, or is expressed as a quotation, or con-tains many pronouns that will not be resolved whenthe sentence is extracted.Our idea is to learn which sentences are poor froma stylistic perspective using again aligned trainingdata.
We train a second SVM on the aligned sen-tences and their labels using surface features at thesentence level, such as sentence length and POS-taginformation.
The most important features learned by236Weight Feature-1.04 Word count less than 10-0.83 Word count less than 20-0.30 Question-0.30 Quotation-0.14 Personal pronounsTable 2: Weights and features of SVM that predicts poorcandidate sentences.the model as predictors of poor sentences, and theweights assigned to them, are shown in Table 2.The predictions of the SVM are incorporated intothe ILP as a hard constraint, by forcing all parse treenodes within those sentences predicted as poor (theset N ?)
to be zero:zi = 0 ?i ?N ?.
(4)3.5 Surface Realization Using LexicalPreferencesHuman-written summaries differ from the sourcenews articles in a number of ways.
They delete ex-traneous information, merge material from severalsentences, employ paraphrases and syntactic trans-formations, change the order of the source sentencesand replace phrases or clauses with more generalor specific descriptions.
We could attempt to learnthe ?language of summaries?
with a language modelwhich we could then use to guide the generationprocess (e.g., by producing maximally probable out-put).
Aside from the logistics of gathering trainingdata large enough to provide robust estimates, webelieve that a more compelling approach is to focuson the words that are unlikely to appear in the sum-mary despite appearing in the source documents.A comparison of the language models generatedfrom the source documents and model summaries,even at the unigram level, is revealing.
Table 3 showslexemes that appear in both source and summarydocuments, but where the likelihood of the lexemeappearing in the summary is much less than thatof it appearing the document, taking into accountthat the summary is much shorter anyway.
The fi-nal column shows the log10-ratio (L(w)) betweenthe two probabilities.
We can see that least prob-able words are those that correspond to attribut-ing information sources (e.g., said, told, accordingLexeme w Source Summary L(w)count countsay 5670 88 -1.63go 638 11 -1.52last 616 9 -1.69get 543 15 -1.05tell 512 8 -1.62come 488 12 -1.17know 404 9 -1.27monday 391 8 -1.35think 382 7 -1.46next 239 7 -0.99spokesman 197 4 -1.36Table 3: Counts of lexemes in the source news articles andsummaries, and measure of the ratio of their probabilities(for most common lexemes with ratio <?0.95).to, spokesman), dates described relatively (e.g., lastMonday), and events that are in the process of hap-pening (e.g., coming, going).As the amount of training data tends to be lim-ited ?
there are usually only a few human-writtensummaries available per document cluster ?
weuse a unigram language model, but conceivably alonger-range n-gram could be employed in the samevein.
We incorporate preferences about summarylanguage into the model as a soft constraint.
Thelog-ratio values fLR (z) are included in the objectiveand defined at the tree node level:fLR (z) = ?i?N?w?WiL(w)zi (5)where L(w), w ?Wi is the log-ratio value for an in-dividual word w:L(w) = log10Psrc(w)Psum(w),Psrc(w) and Psum(w) are the probabilities of word wappearing in the source and summary documents re-spectively, and Wi is the set of words at parse treenode i.
Importantly, we include only those those lex-emes with negative L(w) values.
This guides themodel away from the kind of phrases describedabove, but not towards any particular language pref-erences.2373.6 Quasi-synchronous Tree SubstitutionGrammarRewrite rules involving substitutions, deletions andreorderings are captured in our model using a quasi-synchronous tree substitution grammar.
Given an in-put (source) sentence S1 or its parse tree T1, theQTSG contains rules for generating possible trans-lation trees T2.
A grammar node in the target tree T2is modeled on a subset of nodes in the source tree,with a rather loose alignment between the trees.We extract QTSG rules from aligned sourceand summary sentence pairs represented by theirphrase structure trees.
Our algorithm builds up alist of leaf node alignments based on lexical iden-tity.
Direct parent nodes are aligned where morethan one child node aligns.
This quasi-synchronous?bottom-up?
process gives us better ability to matchnon-isomorphic structures.
We do not assume analignment between source and target root nodes, nordo we require a surjective alignment of all targetnodes to the source tree.
QTSG rules are then cre-ated from aligned nodes above the leaf node level ifall the nodes in the target tree can be explained us-ing nodes from the source.
Individual rewrite rulesdescribe the mapping of source tree fragments intotarget tree fragments, and so the grammar representsthe space of valid target trees that can be producedfrom a given source tree (Eisner, 2003; Cohn andLapata, 2009).Examples of the most frequent QTSG ruleslearned by the above process are shown in Figure 1.Many of the rules relate to the compression of nounphrases through deletion, and examples are shownin the upper box.
Others capture the compression ofverb phrases (middle box).
An important rewrite op-eration is the abstraction of a sentence from a morecomplex source sentence, adding final punctuation ifnecessary (lower box).At generation, paraphrases are created fromsource sentence parse trees by identifying and ap-plying QTSG rules with matching structure.
Thetransduction process starts at the root node of theparse tree, applying QTSG rules to sub-trees un-til leaf nodes are reached.
Note that we do not usethe Bayesian probability model normally associatedwith quasi-synchronous grammars (Smith and Eis-ner, 2006); instead, we ask the QTSG to provide?NP, NP?
?
?
[NP 1 PP], [NP 1 ]?
?NP, NP?
?
?
[NP 1 VP], [NP 1 ]?
?NP, NP?
?
?
[NP 1 SBAR], [NP 1 ]?
?NP, NP?
?
?
[NP 1 , NP ,], [NP 1 ]?
?NP, NP?
?
?
[NP 1 CC NP], [NP 1 ]?
?NP, NP?
?
?
[NNP NNP 1 ], [NNP 1 ]?
?NP, NP?
?
?
[DT 1 JJ NN 2 ], [DT 1 NN 2 ]?
?VP, VP?
?
?
[VP 1 CC VP], [VP 1 ]?
?VP, VP?
?
?
[VP CC VP 1 ], [VP 1 ]?
?VP, VP?
?
?
[VP 1 , CC VP], [VP 1 ]?
?S, S?
?
?
[NP 1 VP 2 ], [NP 1 VP 2 .]?
?S, S?
?
?
[ADVP , NP 1 VP 2 .
], [NP 1 VP 2 .
]?Figure 1: Examples of most frequently learned QTSGrules.
Boxed subscripts show aligned nodes.paraphrases that are acceptable rather than probable,and generate all paraphrases licensed by the QTSG.The alternative paraphrases are incorporated intothe target phrase structure tree as choices that theILP can make.
We use the set C ?
N to be theset of nodes where a choice of paraphrases is avail-able, and Ci ?N , i ?
C to be the actual paraphrasesof i.
Where there are alternatives, it makes sense ofcourse to select only one, which we implement usingthe constraint:?j?Ciz j = zi ?i ?
C , j ?
Ci (6)More generally, we need to constrain the output toensure that a parse tree structure is maintained.
Foreach node i ?N , the set Di ?N contains the list ofdependent nodes (both ancestors and descendants)of node i, so that each set Di contains the nodes thatdepend on the presence of i.
We introduce a con-straint to force node i to be present if any of its de-pendent nodes are chosen:z j?
zi ?i ?N , j ?Di (7)3.7 The ILP ObjectiveThe model we propose for generating a multi-document summary is expressed as an integer linearprogramme and incorporates the content selectionand surface realization preferences, as well as the238soft and hard constraints described in the precedingsections.
The objective of the optimization problemis to maximize the score contributed by the variouselements of content selection ( fB(z) and fS (z)) andsoft surface realization constraints ( fLR (z)) :maxzfB(z)+ fS (z)+ fLR (z) (8)This objective is subject to the constraints (2), (4),(6), and (7) that represent hard constraint decisions,or maintain the logical integrity of the model.
Anoverall length constraint completes the model:?i?Nlizi ?
lmax (9)where li is the number of words generated by choos-ing node i, and lmax is the global word length limit.Note that the scores in the objective are for eachtree node and not each sentence.
This affords themodel flexibility: the content selection elements aregenerally not competing with each other to give adecision on a sentence (see McDonald (2007)).
In-stead, components are marking positive and nega-tive nodes.
The ILP is implicitly searching the gram-mar rules for ways to rewrite the sentence, with theaim of including the salient nodes while removingnegative-scoring nodes (deleting them increases thescore of the node to zero).
Figure 2 shows an exam-ple of a source sentence where the bigram, salienceand language preference components of the ILPwork together to score nodes in the parse tree.
Thenodes NP 1 , VP 3 and VP 4 all have positive scores,while ?said Tuesday?
is negative.
As a rewrite pos-sibility, the rewrite rule shown bottom left is avail-able, which will remove the negative node.
Furtherrewrite rules allow VP 2 to be compressed.
The out-put actually generated by the model used sub-trees(b) and (d) ?
the final text is included in Table 6.4 Experimental Set-upData Our model was evaluated on the TAC non-update multi-document summarization task whichinvolves generating a 100-word-limited summaryfrom a cluster of 10 related input documents; ad-ditionally, TAC provides a set of four model sum-maries for each cluster, written by human experts.We used the 44 document clusters from TAC-2009as training data, to learn the different elements ofthe model.
The 48 document clusters of TAC-2008were reserved for the generation of test summaries.3Training The two components described in Sec-tions 3.3 and 3.4 were trained using binary SVMclassifiers, with labels inferred automatically viaalignment.
The salience classifier was trained on102,754 node instances (16,042 positive and 86,712negative).
The style classifier was trained on 20,443sentence instances (2,083 positive and 18,360 neg-ative).
We learned the feature weights with a linearSVM, using the software SVM-OOPS (Woodsendand Gondzio, 2009).
Because of the high compres-sion rate in this task, sentence alignment leads to anunbalanced data set.
We compensated for this by us-ing different SVM hyper-parameters C+ and C?
asthe loss multiplier for misclassification of positiveand negative training samples respectively.
SVMhyper-parameters were chosen that gave the high-est F1 values using 10-fold cross-validation.
Thesalience SVM obtained a precision of 0.28 and re-call of 0.43.
Precision for the style SVM was 0.20and recall 0.63, respectively.
The classifiers on theirown would thus not be great predictors of salienceor style, but in practice they were useful for break-ing ties in bigram scores.Aligned sentences from the training data werealso used to learn the quasi-synchronous tree sub-stitution grammar, using the process described inSection 3.6.
Rules seen fewer than 3 times were re-moved, resulting in a total of 339 QTSG rules.
Twounigram language models (see Section 3.5) weretrained on the source articles and summaries, respec-tively.
Their probabilities were compared to give theword list shown in Table 3.
We removed words witha source count less than 50, providing a list of 60 lex-emes.
The resulting integer linear programmes weresolved using SCIP,4 and it took 55 seconds on aver-age to read in and solve a document cluster problem.Evaluation We compared our model against twosystems.
As a baseline, we used the ICSI-1 extrac-tive system (Gillick et al2008) which is also basedon ILP and was highly ranked in the TAC-2008evaluation.
We also compared against the ?learnedphrase compression?
system of Berg-Kirkpatrick et3This split follows Berg-Kirkpatrick et al2011).4http://scip.zib.de/239(a) S..VPsaid TuesdayNPa top spaceofficial,,SVP 2SBAR 4if its maiden unmanned spacecraft Chandrayaan-1,slated to be launched by 2008, is successful inmapping the lunar surfaceVP 3will launch more mis-sions to the moonNP 1India(b) S.VP 2NP 1(c) VP 2VP 3(d) VP 2SBAR 4VP 3?S, S?
??
[NP 1 VP 2 ], [NP 1 VP 2 .]?
?VP 2 , VP 2 ?
??
[VP 3 SBAR], [VP 3 ]?
?VP 2 , VP 2 ?
??
[VP 3 SBAR 4 ], [VP 3 SBAR 4 ]?Figure 2: Sentence representation provided to the ILP.
(a) The source sentence representation (child nodes condensedfor space reasons).
Bigrams are shown in bold, slanted text indicates phrases with high salience scores fS , while saidTuesday is penalized by fLR .
Alternative sub-trees (b), (c) and (d) are created using QTSG rules (dashed lines).
Theoutput sentence (see Table 6) was generated from sub-trees (b) and (d).al.
(2011) (henceforth B-K), which has the highestreported ROUGE scores that we are aware of.5 Inaddition to the full model described in Section 3, wealso produced outputs where each of the five compo-nents described in Sections 3.2?3.6 were removed,to assess their individual contribution.We evaluated the output summaries in two ways,using automatic measures and human judgements.Automatic evaluation was performed with ROUGE(Lin and Hovy, 2003) using TAC-2008 parame-ter settings.
We report bigram overlap (ROUGE-2)and skip-bigram (ROUGE-SU4) recall values.
Wealso used Translation Edit Rate (TER, Snover et al(2006)) to examine the systems?
rewrite potential.TER is defined as the minimum number of edits(insertions, deletions, substitutions, and shifts) re-quired to change the system output so that it exactlymatches a reference (here, the reference is the mostclosely aligning source sentence).
The perfect TERscore is 0, however note that it can be higher than 1due to insertions.Our judgement elicitation study was conductedas follows.
We randomly selected ten document5We are grateful to Taylor Berg-Kirkpatrick for making hissystem output available to us.clusters from the test set and generated summarieswith our model (and its lesser variations).
We alsoincluded the corresponding ICSI-1 and B-K sum-maries, and one randomly-selected model summary.The study was conducted over the Internet usingMechanical Turk and was completed by 54 volun-teers, all self reported native English speakers.
Par-ticipants were first asked to read the documents ineach cluster.
Next, they were asked a few compre-hension questions to ensure they had understood andprocessed the documents.
Finally, they were pre-sented with a summary and asked to rate it alongtwo dimensions: grammaticality (is the summaryfluent and grammatical?
), and informativeness (arethe main topics captured in the summary?).
The sub-jects used a 1?5 rating scale, with half-points al-lowed.
Participants who declared themselves as non-native English speakers, did not answer the compre-hension questions correctly or took only a few min-utes to complete the task were eliminated.5 ResultsOur results are summarized in Table 4.
Let us firstdiscuss those obtained using ROUGE-2 (2-R) andROUGE-SU4 (SU4-R) recall values.
As can be seen240Models ROUGE TER (%) Sentences2-R SU4-R Ins Del Sub Shift Count CR (%) Mod (%)ICSI-1 11.03 13.96 ?
?
?
?
200 ?
?B-K 11.71 14.47 0.2 26.2 2.3 0.4 216 74.0 63.9MA-ILP 11.37 14.47 0.7 11.6 5.3 0.6 191 89.1 61.8ILP w/o bigrams 9.24 12.66 0.8 15.4 11.8 1.2 205 85.4 80.0ILP w/o salience 11.38 14.71 1.1 19.1 12.0 1.3 233 82.1 92.3ILP w/o style 11.83 15.09 1.4 17.4 18.9 1.7 271 84.1 86.3ILP w/o log-ratio 11.41 14.70 1.2 16.9 12.5 1.5 223 84.3 90.1ILP w/o QTSG 10.32 13.68 0 0 0 0 163 100.0 0Table 4: Performance of the multiple-aspect ILP model against comparison systems using ROUGE and the four com-ponents of TER (insertion, deletion, substitution, shifts).
In the lower section, performance of our model without (w/o)each component in turn.
The final columns show the number of source sentences, the average compression ratio, andthe proportion of sentences modified.from the upper section of Table 4, the systems incor-porating some form of rewriting gain slightly higherROUGE scores than ICSI-1.
The multiple aspectsILP system (MA-ILP) yields ROUGE scores simi-lar to B-K, despite performing rewriting operationswhich increase the scope for error and without re-quiring any hand-crafted compression rules or man-ually annotated training data.
Indeed, the outputs ofthe two systems are not significantly different underROUGE (using a paired t-test, p > 0.5).In the lower section of Table 4, we show the per-formance of our model when each of the contribut-ing components described in Section 3 are removed.Clearly the bigram content indicators are an impor-tant element for the ROUGE scores, as their removalyields a reduction of 2.46 points (see the row ILPw/o bigrams in Table 4).
The model without QTSGrules (ILP w/o QTSG) is effectively limited to sen-tence extraction, and removing rewrite rules alsolowers ROUGE scores to levels similar to ICSI-1.ROUGE scores are increased by allowing the modelto select ?poor quality?
sentences (ILP w/o style),higher indeed than those of the B-K system.
Theinclusion of non-summary language (ILP w/o log-ratio) does not affect ROUGE scores to the same ex-tent that bigrams and QTSG do.Table 4 includes a break-down of the systems?rewrite operations as measured by TER.
We alsoshow the number of source sentences (Count), theaverage compression ratio (CR %) and the propor-tion of sentences modified (Mod %) by each system.As can be seen, MA-ILP draws on fewer sentences,Models Grammar InformICSI-1 4.68 2.55B-K 4.40 2.70MA-ILP 4.68 3.90ILP w/o style 3.30 2.67Gold 4.90 4.75Table 5: Mean ratings on system output output.performs less deletion and more rewriting than B-K.The number of deletions increases when individualILP components are removed and so does the num-ber of substitutions.
All the subsystems are more ag-gressive in their rewriting than when used in com-bination (higher TER, higher compression rate anda larger number of sentences are modified).
Expect-edly, when removing the QTSG rules, the ILP is lim-ited to a pure extractive system (last row in Table 4).The results of our human evaluation study areshown in Table 5.
We elicited grammaticality and in-formativeness ratings for a randomly selected modelsummary, ICSI-1, B-K, the multiple aspect ILP(MA-ILP), and the ILP w/o style which we in-cluded in this study as it performed best underROUGE.
ICSI-1, B-K, and MA-ILP are rated highlyon the grammaticality dimension.
MA-ILP is in-distinguishable from the sentence extraction sys-tem (ICSI-1).
Both systems are significantly moregrammatical than B-K (?< 0.05, using a Post-hocTukey test).
Notice that summaries created by theILP w/o style are rated poorly by humans, contraryto ROUGE.
The style component stops very short241Florida?s Governor Jeb Bush asked the USSupreme Court to intervene to keep a comatosewoman alive, over the wishes of her husband,who wants to disconnect the feeding tube thathas sustained her for 14 years.
Her husband,Michael Schiavo, and her parents, Robert andMary Schindler, have conflicts of interest that pre-vent them from fairly deciding whether to keepher alive.
Some doctors have testified that TerriSchiavo is in a persistent vegetative state withno hope for recovery.
The state House in Floridapassed a bill Thursday to extend life support for abrain-damaged woman.The space agencies of India and France signed anagreement to cooperate in launching a satellite infour years that will help make climate predictionsmore accurate.
The Indian Space Research Orga-nization (ISRO) has short-listed experiments fromfive nations including the United States, Britainand Germany, for a slot on India?s unmannedmoon mission Chandrayaan-1 to be undertakenby 2006-2007, the Press Trust of India (PTI) re-ported Monday.
India will launch more missionsto the moon if its maiden unmanned spacecraftChandrayaan-1, slated to be launched by 2008, issuccessful in mapping the lunar surface.Table 6: Example summaries generated by the multipleaspects model (MA-ILP).sentences and quotations from being included in thesummary even if they have quite high bigram orcontent scores.
Without it, the model tends to gen-erate summaries that are fragmentary and lackingproper context, resulting in lower grammaticality(and informativeness) when judged by humans.
TheMA-ILP system obtains the highest rating with re-spect to information content.
It is significantly better(?< 0.05) than ICSI-1 and B-K.
This is not entirelysurprising as our model includes additional contentselection elements over and above the bigram units.There is still a significant gap from all systems to thegold-standard human-authored summaries.
Exampleoutput summaries of the full ILP model are shown inTable 6.Overall, we obtain best results when consideringthe contributions from the individual model expertscollectively.
This suggests that additional improve-ments could be obtained with more experts.
It is alsopossible that optimizing the relative weightings ofexperts in the ILP objective would improve output.The TER analysis shows that the experts have a tem-pering effect on each other, resulting in less aggres-sive, but qualitatively better, rewriting than whenused individually.
Generally, experts work togetherto shape an output sentence, but they can also com-pete.
In the future, we also plan to test the abilityof the model to adapt to other multi-document sum-marization tasks, where the location of summary in-formation is not as regular as it is in news articles.We would also like interface our model with sen-tence ordering and more generally with some notionof the coherence of the generated summary.Acknowledgments We are grateful to Micha El-sner for his input on earlier versions of this work.We would also like to thank members of the ILCCat the School of Informatics for valuable discus-sions and comments.
We acknowledge the supportof EPSRC through project grants EP/I032916/1 andEP/I017127/1.ReferencesTaylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 481?490, Portland, Ore-gon.James Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression: An integer linear pro-gramming approach.
Journal of Artificial IntelligenceResearch, 31:273?381.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings of the22nd International Conference on Computational Lin-guistics, pages 137?144, Manchester, UK.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of Artificial In-telligence Research, 34:637?674.Pawan Deshpande, Regina Barzilay, and David Karger.2007.
Randomized decoding for selection-and-ordering problems.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;242Proceedings of the Main Conference, pages 444?451,Rochester, New York.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proceedings of theACL Interactive Poster/Demonstration Sessions, pages205?208, Sapporo, Japan.Elena Filatova and Vasileios Hatzivassiloglou.
2004.A formal model for information selection in multi-sentence text extraction.
In Proceedings of the 20thInternational Conference on Computational Linguis-tics, pages 397?403, Geneva, Switzerland.Dan Gillick and Benoit Favre.
2009.
A scalable globalmodel for summarization.
In Proceedings of the Work-shop on Integer Linear Programming for Natural Lan-guage Processing, pages 10?18, Boulder, Colorado.Dan Gillick, Benoit Favre, and Dilek Hakkani-tu?r.
2008.The ICSI summarization system at TAC 2008.
In Pro-ceedings of the Text Analysis Conference.Dan Gillick, Benoit Favre, Dilek Hakkani-tu?r, BerndtBohnet, Yang Liu, and Shasha Xie.
2009.
TheICSI/UTD summarization system at TAC 2009.
InProceedings of the Text Analysis Conference.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and MarkKantrowitz.
2000.
Multi-document summarizationby sentence extraction.
In Proceedings of the 2000NAACL?ANLP Workshop on Automatic Summariza-tion, pages 40?48, Seattle, Washington.Dorit S. Hochba.
1997.
Approximating coveringand packing problems: Set cover, vertex cover, in-dependent set, and related problems.
In Dorit S.Hochba, editor, Approximation Algorithms for NP-Hard Problems, pages 94?143.
PWS Publishing Com-pany, Boston, MA.Honyang Jing.
2002.
Using Hidden Markov modelingto decompose human-written summaries.
Computa-tional Linguistics, 28(4):527?544.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st As-sociation for Computational Linguistics, pages 423?430, Sapporo, Japan.Chin-Yew Lin and Eduard H. Hovy.
2003.
Automaticevaluation of summaries using n-gram co-occurrencestatistics.
In Proceedings of HLT?NAACL, pages 71?78, Edmonton, Canada.Inderjeet Mani.
2001.
Automatic Summarization.
JohnBenjamins Pub Co.Andre?
Martins and Noah A. Smith.
2009.
Summariza-tion with a joint model for sentence extraction andcompression.
In Proceedings of the Workshop on In-teger Linear Programming for Natural Language Pro-cessing, pages 1?9, Boulder, Colorado.Ryan McDonald.
2007.
A study of global inference algo-rithms in multi-document summarization.
In Proceed-ings of the 29th European conference on IR Research,pages 557?564, Rome, Italy.Rani Nelken and Stuart Schieber.
2006.
Towards ro-bust context-sensitive sentence alignment for monolin-gual corpora.
In Proceedings of the 11th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 161?168, Trento, Italy.David Smith and Jason Eisner.
2006.
Quasi-synchronousgrammars: Alignment by soft projection of syntacticdependencies.
In Proceedings of Workshop on Statis-tical Machine Translation, pages 23?30, NYC.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Conference of the Associa-tion for Machine Translation in the Americas, pages223?231, Cambridge.Karen Sparck Jones.
1999.
Automatic summarizing:Factors and directions.
In Inderjeet Mani and Mark T.Maybury, editors, Advances in Automatic Text Summa-rization, pages 1?33.
MIT Press, Cambridge.Kristian Woodsend and Jacek Gondzio.
2009.
Exploitingseparability in large-scale linear support vector ma-chine training.
Computational Optimization and Ap-plications.Kristian Woodsend and Mirella Lapata.
2010.
Automaticgeneration of story highlights.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics, pages 565?574, Uppsala, Sweden.Kristian Woodsend, Yansong Feng, and Mirella Lapata.2010.
Title generation with quasi-synchronous gram-mar.
In Proceedings of the 2010 Conference on Empir-ical Methods in Natural Language Processing, pages513?523, Cambridge, MA.243
