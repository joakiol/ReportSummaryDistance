Word Selection for EBMT based on Monolingual Similarity and TranslationConfidenceEiji Aramaki, Sadao Kurohashi, Hideki Kashioka and Hideki Tanaka Graduate School of Information Science and Tech.
University of TokyoHongo, Bunkyo-ku, Tokyo 113-8656, Japanaramaki, kuro@kc.t.u-tokyo.ac.jp ATR Spoken Language Translation Research Laboratories2-2 Hikaridai, Seika, Soraku, Kyoto 619-0288, Japanhideki.kashioka, hideki.tanaka@atr.co.jpAbstractWe propose a method of constructing anexample-based machine translation (EBMT)system that exploits a content-aligned bilingualcorpus.
First, the sentences and phrases in thecorpus are aligned across the two languages,and the pairs with high translation confidenceare selected and stored in the translation mem-ory.
Then, for a given input sentences, thesystem searches for fitting examples based onboth the monolingual similarity and the transla-tion confidence of the pair, and the obtained re-sults are then combined to generate the transla-tion.
Our experiments on translation selectionshowed the accuracy of 85% demonstrating thebasic feasibility of our approach.1 IntroductionThe basic idea of example-based machine translation, orEBMT, is that translation examples similar to a part ofan input sentence are retrieved and combined to producea translation(Nagao, 1984).
In order to make a practi-cal MT system based on this approach, a large numberof translation examples with structural correspondencesare required.
This naturally presupposes high-accuracyparsers and well-aligned large bilingual corpora.Over the last decade, the accuracy of the parsers im-proved significantly.
The availability of well-alignedbilingual corpora, however, has not increased despite ourexpectations.
In reality, the number of bilingual cor-pora that share the same content, such as newspapers andbroadcast news, has increased steadily.
We call this typeof corpus a content-aligned corpus.
With these observa-tions, we started a research project that covered all as-pects of constructing EBMT systems starting from usingFigure 1: Translation Example (TE).a content-aligned corpus, i.e., a bilingual broadcast newscorpus.First, the sentences and phrases in the corpus arealigned across the two languages, and the pairs with hightranslation confidence are selected and stored in the trans-lation memory.
Then, translation examples are retrievedbased on both the monolingual similarity and the trans-lation confidence of the pair.
Finally, these examples arecombined to generate the translation.This paper is organized as follows.
The next sec-tion presents how to build the translation memory froma content-aligned corpus.
Section 3 describes our EBMTsystem, paying special attention to the selection of trans-lation examples.
Section 4 reports experimental resultsof word selection, Section 5 describes related works, andSection 6 gives our conclusions.
* Underlined phrases and sentences have no parallel expressions in the other language.Figure 2: NHK News Corpus.2 Building Translation MemoryIn EBMT, an input sentence can hardly be translated bya single translation example, except when an input is ex-tremely short or is a typical domain-dependent sentence.Therefore, two or more translation examples are used totranslate parts of the input and are then combined to gen-erate a whole translation.
Syntactic information is usefulfor composing example fragments.In this paper, we call a structurally aligned bilingualsentence pair a translation example or TE (Figure 1).This section presents our method for building TEs from acontent-aligned corpus.Since the bilingual corpus used in our project does notcontain literal translations, automatic parsing and align-ment inevitably contain errors.
Therefore, we selectedhighly likely TEs to make a translation memory.2.1 NHK News CorpusWe used a bilingual news corpus compiled by the NHKbroadcasting service (NHK News Corpus), which con-sists of about 40,000 Japanese-English article pairs cov-ering a five-year period.
The average number of Japanesesentences in an article is 5.2, and that of English sentenceis 7.4.
Table 2 shows an example of an article pair.As shown in Table 2, an English article is not a literaltranslation of a Japanese article, although their contentsare almost parallel.2.2 Sentence AlignmentWe used a DP matching for bilingual sentence alignment,where we allow the matching of 1-to-1, 1-to-2, 1-to-3, 2-to-1 and 2-to-2 Japanese and English sentence pairs.
Thismatching covered 84% of the following evaluation set.We selected 96 article pairs for the evaluation of sentenceand phrase alignment, and we call this the evaluation set.We use the following score for matching, which is basedon a ratio of corresponding content words (WCR: contentWord Corresponding Ratio).WCR   (1)where is the number of Japanese content words in aunit, is the number of English content words, and is the number of content words whose translation is alsoin the unit, which is found by translation dictionaries?We used the EDR electronic dictionary, EDICT,ENAMDICT, the ANCHOR translation dictionary, andFigure 3: Handling of Remaining Phrases.Figure 4: WCR and Precision.the EIJIRO translation dictionary.
These dictionarieshave about two million entries in total.On the evaluation data, the precision of the sentencealignment (defined as follows) was 60.7%.precision  # of correct system outputs# of system outputs(2)Among types of a corresponding unit, the precision of1-to-1 correspondence was the best, at 77.5%.
Since a 1-to-1 correspondence is suitable for the following phrasealignment, we decided to use only the 1-to-1 correspon-dence results.2.3 Phrase AlignmentThe 1-to-1 sentence pairs obtained in the previous sec-tion are then aligned at phrase level by the method basedon (Aramaki et al, 2001).
The method consists of thefollowing pre-process and two aligning steps.Pre-process: Conversion to phrasal dependency struc-tures.First, the phrasal dependency structures of the sen-tence pair are estimated.
The English parser returnsa word-based phrase structure, which is merged intoa phrase sequence by the following rules and con-verted into a dependency structure by lifting up headphrases.Table 1: Number of TEs.Corpus WCR # of TEs0.3?0.4 18290NHK News 0.4?0.5 69750.5?
2314White Paper ?
2225SENSEVAL ?
69201.
Function words are grouped with the followingcontent word.2.
Adjoining nouns are grouped into one phrase.3.
Auxiliary verbs are grouped with the followingverb.The Japanese parser outputs the phrasal dependencystructure of an input, and that is used as is.
We usedThe Japanese parser KNP (Kurohashi and Nagao,1994) and The English nl-parser (Charniak, 2000).Step 1: Estimation of basic phrasal correspondences.We started with the word-level alignment to get thebasic phrasal alignment.
We used translation dictio-naries for this process.
The word sense ambiguityin the dictionaries is resolved with a heuristics thatthe most plausible correspondence is near other cor-respondences.Step 2: Expansion of phrasal correspondences.Finally, the remaining phrases, which were not han-dled in the step 1, are merged into a neighboringphrase correspondence or are used to establish a newcorrespondence, depending on the surrounding ex-isting correspondences.
Figure 3 shows an exampleof a new correspondence established by a structuralpattern.These procedures can detect the phrasal alignments ina pair of sentences as shown in Figure 1.For phrase alignment evaluation, we selected all of the145 sentence pairs that had 1-to-1 correspondences formthe evaluation set and gave correct content word corre-spondences to these pairs.
The phrase correspondencesdetected by the system were judged correct when the cor-respondences include the manually given content wordcorrespondences.Based on this criterion, the precision of phrase align-ment was 50%.
Then, we found a correlation betweenthe phrase alignment precision and WCR of parallel sen-tences as shown in Figure 4.
Furthermore, the precisionof sentence alignment and WCR also have a correlation.Since their performances nearly reaches their limits whenWCR is 0.3, we decided to use parallel sentences whoseWCR is 0.3 or greater as TEs.Figure 5: Example of Translation.2.4 Building Translation MemoryAs explained in the preceding sections, among sentence-aligned and phrase-aligned NHK News articles, TEs witha 1-to-1 sentence correspondence and whose WCR is 0.3or greater are registered in the translation memory.
Table1 shows the number of TEs for each WCR range.In addition, the Bilingual White Paper and TranslationMemory of SENSEVAL2 (Kurohashi, 2001) were alsophrase-aligned and registered in the translation memory.Sentence alignments are already given for these corpora.Since their parallelism are fairly high and the accuraciesof their phrase alignments are more than 70%, we utilizedall phrase-aligned sentence pairs as TEs (Table 1).3 EBMT SystemOur EBMT system translates a Japanese sentence intoEnglish.
A Japanese input sentence is parsed and trans-formed into a phrase-based dependency structure.
Then,for each phrase, an appropriate TE is retrieved from thetranslation memory that is most suitable for translatingFigure 6: Selection of a TE.the phrase (and its neighboring phrases).
Finally, the En-glish expressions of the TEs are combined to produce thefinal English translation (Figure 5).This section describes our EBMT system, mainly theTE selection part.3.1 Basic Idea of TE SelectionThe basic idea of TE selection is shown in Figure 6.When a part of the input sentence and a part of the TEsource language sentence have an equal expression, thepart of the input sentence is called I and the part of theTE source language sentence is called S. A part of the TEtarget language corresponding to S is called T. The pair Sand T is called fragment of TE (FTE).I, S and T have to meet the following conditions, as anatural consequence of the fact that S-T is used for trans-lating I.1.
I, S and T are each structurally connected phrases.2.
I is equal to S except for function words at theboundaries.3.
S corresponds to T completely, that is, all phrases inS and T are aligned.It might be the case that for an I, two or more FTEs thatmeet the above conditions exist in the translation mem-ory.
Our method takes into account the following rela-tions among I-S-T to select the best FTE:1.
The largest pair of I and S.2.
The similarity between the surroundings of I andthese of S.3.
The confidence of alignment between S and T.The following sections concretely present how to cal-culate these criteria.
For simplicity of explanation, wecall a set of phrasal correspondences between S and T,EQ; that neighboring EQ, CONTEXT; that between S andT, ALIGN (Figure 6).3.2 Monolingual Similarity between JapaneseExpressionsThe equality between I and S is a sum of the equalityscore of each phrase correspondence in EQ, which is cal-culated as follows:EQUAL  	 	 (3)whereis the number of content words in the phrasecorrespondence, 	is the number of function words,is the equality between content words, and 	is the equality between function words.
and 	are given in Table 2.1Usually, the equality score between I and S is equal tothe number of phrases in I (the number of phrase corre-spondences in EQ), but sometimes these are slightly dif-ferent, depending on the conjugation type and functionwords.1All constant values in Table 2 and formulas were decidedbased on preliminary experiments.On the other hand, the similarity between the surround-ings of I and those of S is a sum of the similarity score ofeach phrase correspondence in CONTEXT, which is cal-culated as follows:SIM   (4)Basically the calculation of SIM and EQUAL is thesame, except that SIM considers the relation type betweenthe phrase in I and its outer phrase by .
Whenthe relation is the same, the influence of the surroundingphrases must be large, so is set to 1.0; when therelation is not the same, is set to 0.5.
The rela-tions between phases are estimated by the function wordor conjugation type of the dependent phrase.The monolingual similarity between Japanese expres-sions I and S is calculated as follows:EQUAL SIM (5)3.3 Translation Confidence of Japanese-to-EnglishAlignmentThe translation confidence of phrase alignment betweenS and T is the sum of the confidence score of each phrasecorrespondence in ALIGN, CONF() in Table 2, and it isweighted by the WCR of the parallel sentences.As a final measure, the score of I-S-T is calculated asfollows:EQUAL SIMCONFWCR (6)3.4 Search Algorithm of FTEFor each phrase (P) in an input sentence, the most plausi-ble FTE is retrieved by the following algorithm:1.
FTEs are retrieved from the translation memory, inwhich a Japanese phrase matches P, and it is alignedto an English phrase.
(that is, these are FTEs thatmeet the basic conditions for translation in Section3.1).2.
For each FTE obtained in the previous step, it ischecked whether the surrounding phrase of P andthat of FTE are the same or similar, phrase byphrase, and the largest I-S-T that meets the basicconditions is detected.Table 2: Parameters for Similarity and Confidence Calculation.1.1 exact match1.0 stem match0.5  + 0.3 thesaurus match0.3 POS match0 otherwise* is a similarity calculated based on NTT thesaurus(Ikehara et al, 1997) (max = 1).1.1 exact match1.0 stem match0 otherwise1.0 all content words in alignment  correspond to each other in dicCONF() 0.8 some content words in alignment  correspond to each other in dic0.5 otherwise3.
The score of each I-S-T is calculated, and the bestI-S-T (S-T is the FTE) is selected as the FTE for P.As a result of detecting FTEs for phrases in the input,two FTEs starting from the different phrase might over-lap each other.
In such a case, we employed a greedysearch algorithm that adopts the higher score FTE oneby one; therefore, each previously adopted FTE is onlypartly used for translation.On the other hand, when no FTE is obtained for an in-put phrase, a translation dictionary is utilized (when thephrase contains two or more content words, the longestmatching strategy is used for dictionary look-up).
Whentwo or more possible translations are given from the dic-tionary, the most frequent phrase/word in the NHK NewsCorpus is adopted.Figure 5 shows examples of FTEs detected by ourmethod.23.5 Generating a Target SentenceThe English expressions in the selected FTEs are com-bined, and the English dependency structure is con-structed.
The dependency relations in FTEs are pre-served, and the relation between the two FTEs is esti-mated based on the relation of the input sentences.
Figure5 shows an example of a combined English dependencystructure.When a surface expression is generated from its depen-dency structure, its word order must be selected properly.This can be done by preserving the word order in FTEsand by ordering FTEs by a set of rules governing both thedependency relation and the word-order.The module for controlling conjugation, determiner,and singular/plural is not yet implemented in our currentMT system.2As the bottom example in Figure 5 shows, EBMT can eas-ily handle head-switching translation by using an FTE that con-tains all of the head-switching phenomena in it.4 ExperimentsFor evaluation, we selected 50 sentence pairs from theNHK News Corpus that were not used for the translationmemory.
Their source (Japanese) sentences were trans-lated by our EBMT system, and the selected FTEs wereevaluated by hand, referring to the target (English) sen-tences.A phrase by phrase evaluation was done to judgewhether the English expression of the selected FTE wasgood or bad.
The accuracy was 85.0%.In order to investigate the effectiveness of each com-ponent of FTE selection, we compared the following fourmethods:1.
EQCONTEXTALIGN: The proposed method.2.
EQALIGN: FTE score is calculated as follows, with-out the CONTEXT similarity:EQUAL()CONF()WCR (7)3.
EQCONTEXT: FTE score is calculated as follows,without the ALIGN confidence:EQUAL() SIM() (8)4.
DICONLY: Word selection is based only on dictio-naries and frequency in the corpus.The accuracy of each method is shown in Table 3,and the results indicate that the proposed method, EQ-CONTEXTALIGN, is the best, that is, using context sim-ilarity and align confidence works effectively.
Figure 7Figure 7: Word Selection by EQCONTEXTALIGN and DICONLY.Table 3: Experimental Results.Good Bad AccuracyEQCONTEXTALIGN 268 47 85.0%(246) (35) (87.5%)EQALIGN 254 61 80.6 %(233) (48) (82.9%)EQCONTEXT 234 80 74.2%(213) (68) (75.8%)DICONLY 232 83 73.6%* Values in brackets indicate the accuracy only for FTEs,excluding cases in which the dictionary was used as abackup.shows examples of EQCONTEXTALIGN and DICONLY.EQCONTEXTALIGN usually selects appropriate words,compared to DICONLY.When there are no plausible translation examples in thetranslationmemory, the system selects a low-similarity orlow-confidence FTE.
However we believe this problemwill be resolved as the number of translation examplesincreases, since the News Corpus is increasing day byday.5 Related WorkThe idea of example based machine translation systemswas first proposed by (Nagao, 1984), and preliminarysystems that appeared about ten years (Sato and Na-gao, 1990; Sadler and Vendelmans, 1990; Maruyama andWatanabe, 1992; Furuse and Iida, 1994) showed the basicfeasibility of the idea.Recent studies have focused on the practical aspectsof EBMT, and this technology has even been appliedto some restricted domains.
The work in (Richardsonet al, 2001; Menezes and Richardson, 2001) addressedthe problem of technical manual translation in severallanguages, and the work of (Imamura, 2002) dealt withdialogues translation in the travel arrangement domain.These works select the translation example pairs basedsolely on the source language similarity.
We believe thisis partly due to the high parallelism found in their cor-pora.Our work targets a more general corpus of wider cover-age, i.e., the broadcast news collection.
Generally avail-able corpora like the one we use tend to be more freelytranslated and suffer from lower parallelism.
This com-pelled us to use the criterion of translation confidence,together with the criterion of monolingual similarity usedin the previous works.
As we showed in this paper, thismetric succeeded in meeting our expectations.6 ConclusionIn this paper, we described operations of the entire EBMTprocess while using a content-aligned corpus, i.e., theNHK Broadcast Corpus.
In this process, one of the keyproblems is how to select plausible translation examples.We proposed a new method to select translation exam-ples based on source language similarity and translationconfidence.
In the word selection task, the performanceis highly accurate.AcknowledgementsThis work was supported in part by the 21st Century COEprogram ?Information Science and Technology Strate-gic Core?
at University of Tokyo and by a contract withthe Telecommunications Advancement Organization ofJapan, entitled ?A study of speech dialogue translationtechnology based on a large corpus?.ReferencesEiji Aramaki, Sadao Kurohashi, Satoshi Sato, and HideoWatanabe.
2001.
Finding translation correspondencesfrom parallel parsed corpus for example-based transla-tion.
In Proceedings of MT Summit VIII, pages 27?32.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In In Proceedings of NAACL 2000, pages 132?139.Osamu Furuse and Hitoshi Iida.
1994.
Constituentboundary parsing for example-based machine transla-tion.
In Proceedings of the 15th COLING, pages 105?111.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, AkioYokoo, Hiromi Nakaiwa, Kentarou Ogura, and Yoshi-fumi Oyama Yoshihiko Hayashi, editors.
1997.Japanese Lexicon.
Iwanami Publishing.Kenji Imamura.
2002.
Application of translation knowl-edgeacquired by hierarchical phrase alignment forpattern-based mt.
In Proceedings of TMI-2002, pages74?84.Sadao Kurohashi and Makoto Nagao.
1994.
A syntacticanalysis method of long Japanese sentences based onthe detection of conjunctive structures.
ComputationalLinguistics, 20(4).Sadao Kurohashi.
2001.
Senseval2 Japanese translationtask.
In Proceedings of SENSEVAL2, pages 37?40.Hiroshi Maruyama and Hideo Watanabe.
1992.
Thecover search algorithm for example-based translation.In Proceedings of TMI-1992, pages 173?184.Arul Menezes and Stephen D. Richardson.
2001.
A best-first alignment algorithm for automatic extraction oftransfer mappings from bilingual corpora.
In Proceed-ings of the ACL 2001 Workshop on Data-Driven Meth-ods in Machine Translation, pages 39?46.Makoto Nagao.
1984.
A framework of a mechanicaltranslation between Japanese and english by analogyprinciple.
In In Artificial and Human Intelligence,pages 173?180.Stephen D. Richardson, William B. Dolan, ArulMenezes, and Monica Corston-Oliver.
2001.
Over-coming the customization bottleneck using example-based mt.
In Proceedings of the ACL 2001 Work-shop onData-DrivenMethods in Machine Translation,pages 9?16.V.
Sadler and R. Vendelmans.
1990.
Pilot implementa-tion of a bilingual knowledge bank.
In Proeedings ofthe 13th COLING, pages 449?451.Satoshi Sato andMakoto Nagao.
1990.
Toward memory-based translation.
InProceedings of the 13th COLING,pages 247?252.
