Parsing with generative models of predicate-argument structureJulia HockenmaierIRCS, University of Pennsylvania, Philadelphia, USAandInformatics, University of Edinburgh, Edinburgh, UKjuliahr@linc.cis.upenn.eduAbstractThe model used by the CCG parserof Hockenmaier and Steedman (2002b)would fail to capture the correct bilexicaldependencies in a language with freerword order, such as Dutch.
This paperargues that probabilistic parsers shouldtherefore model the dependencies in thepredicate-argument structure, as in themodel of Clark et al (2002), and definesa generative model for CCG derivationsthat captures these dependencies, includ-ing bounded and unbounded long-rangedependencies.1 IntroductionState-of-the-art statistical parsers for PennTreebank-style phrase-structure grammars (Collins,1999), (Charniak, 2000), but also for CategorialGrammar (Hockenmaier and Steedman, 2002b),include models of bilexical dependencies definedin terms of local trees.
However, this paperdemonstrates that such models would be inadequatefor languages with freer word order.
We use theexample of Dutch ditransitives, but our argumentequally applies to other languages such as Czech(see Collins et al (1999)).
We argue that thisproblem can be avoided if instead the bilexicaldependencies in the predicate-argument structureare captured, and propose a generative model forthese dependencies.The focus of this paper is on models for Combina-tory Categorial Grammar (CCG, Steedman (2000)).Due to CCG?s transparent syntax-semantics inter-face, the parser has direct and immediate accessto the predicate-argument structure, which includesnot only local, but also long-range dependenciesarising through coordination, extraction and con-trol.
These dependencies can be captured by ourmodel in a sound manner, and our experimental re-sults for English demonstrate that their inclusion im-proves parsing performance.
However, since thepredicate-argument structure itself depends only toa degree on the grammar formalism, it is likelythat parsers that are based on other grammar for-malisms could equally benefit from such a model.The conditional model used by the CCG parser ofClark et al (2002) also captures dependencies in thepredicate-argument structure; however, their modelis inconsistent.First, we review the dependency model proposedby Hockenmaier and Steedman (2002b).
We thenuse the example of Dutch ditransitives to demon-strate its inadequacy for languages with a freer wordorder.
This leads us to define a new generative modelof CCG derivations, which captures word-word de-pendencies in the underlying predicate-argumentstructure.
We show how this model can capturelong-range dependencies, and deal with the pres-ence of multiple dependencies that arise through thepresence of long-range dependencies.
In our currentimplementation, the probabilities of derivations arecomputed during parsing, and we discuss the dif-ficulties of integrating the model into a probabilis-tic chart parsing regime.
Since there is no CCGtreebank for other languages available, experimen-tal results are presented for English, using CCGbank(Hockenmaier and Steedman, 2002a), a translationof the Penn Treebank to CCG.
These results demon-strate that this model benefits greatly from the inclu-sion of long-range dependencies.2 A model of surface dependenciesHockenmaier and Steedman (2002b) define a sur-face dependency model (henceforth: SD) HWDepwhich captures word-word dependencies that are de-fined in terms of the derivation tree itself.
It as-sumes that binary trees (with parent category P )have one head child (with category H) and one non-head child (with category D), and that each nodehas one lexical head h=hc; wi.
In the following tree,P=S[dcl]nNP, H=(S[dcl]nNP)=NP, D=NP, hH=h(S[dcl]nNP)=NP; openedi, and hD=hN; doorsi.S[dcl]nNP(S[dcl]nNP)=NPopenedNPits doorsThe model conditions wDon its own lexical cate-gory cD, on hH= hcH; wHi and on the local treein which the D is generated (represented in terms ofthe categories hP;H;Di):P (wDjcD;  = hP;H;Di; hH= hcH; wHi)3 Predicate-argument structure in CCGLike Clark et al (2002), we define predicate-argument structure for CCG in terms of the depen-dencies that hold between words with lexical func-tor categories and their arguments.
We assume thata lexical head is a pair hc; wi, consisting of a wordw and its lexical category c. Each constituent hasat least one lexical head (more if it is a coordinateconstruction).
The arguments of functor categoriesare numbered from 1 to n, starting at the innermostargument, where n is the arity of the functor, eg.
(S[dcl]nNP1)=NP2, (NPnNP1)=(S[dcl]=NP)2.
De-pendencies hold between lexical heads whose cat-egory is a functor category and the lexical headsof their arguments.
Such dependencies can be ex-pressed as 3-tuples hhc; wi; i; hc0 ; w0ii, where c is afunctor category with arity  i, and hc0; w0i is a lex-ical head of the ith argument of c.The predicate-argument structure that corre-sponds to a derivation contains not only local,but also long-range dependencies that are projectedfrom the lexicon or through some rules such as thecoordination of functor categories.
For details, seeHockenmaier (2003).4 Word-word dependencies in DutchDutch has a much freer word order than English.The analyses given in Steedman (2000) assume thatthis can be accounted for by an extended use ofcomposition.
As indicated by the indices (whichare only included to improve readability), in thefollowing examples, hij is the subject (NP3) ofgeeft, de politieman the indirect object (NP2), andeen bloem the direct object (NP1).1Hij geeft de politieman een bloem(He gives the policeman a flower)S=(S=NP3) ((S=NP1)=NP2)=NP3Tn(T=NP2) Tn(T=NP1)<BTn((T=NP1)=NP2)<BS=NP3>SEen bloem geeft hij de politiemanS=(S=NP1) ((S=NP1)=NP2)=NP3Tn(T=NP3) Tn(T=NP2)<(S=NP1)=NP2<S=NP1>SDe politieman geeft hij een bloemS=(S=NP2) ((S=NP1)=NP2)=NP3Tn(T=NP3) Tn(T=NP1)<(S=NP1)=NP2<BS=NP2>SA SD model estimated from a corpus containingthese three sentences would not be able to capturethe correct dependencies.
Unless we assume thatthe above indices are given as a feature on the NPcategories, the model could not distinguish betweenthe dependency relations of Hij and geeft in thefirst sentence, bloem and geeft in the second sen-tence and politieman and geeft in the third sentence.Even with the indices, either the dependency be-tween politieman and geeft or between bloem andgeeft in the first sentence could not be captured by amodel that assumes that each local tree has exactlyone head.
Furthermore, if one of these sentences oc-curred in the training data, all of the dependencies inthe other variants of this sentence would be unseento the model.
However, in terms of the predicate-argument structure, all three examples express thesame relations.
The model we propose here wouldtherefore be able to generalize from one example tothe word-word dependencies in the other examples.1The variables T are uninstantiated for reasons of space.The cross-serial dependencies of Dutch are oneof the syntactic constructions that led people tobelieve that more than context-free power is re-quired for natural language analysis.
Here is anexample together with the CCG derivation fromSteedman (2000):dat ik Cecilia de paarden zag voeren(that I Cecilia the horses saw feed)NP1NP2NP3((SnNP1)nNP2)=VP VPnNP3>B((SnNP1)nNP2)nNP3<(SnNP1)nNP2<SnNP1<SAgain, a local dependency model would systemat-ically model the wrong dependencies in this case,since it would assume that all noun phrases are ar-guments of the same verb.However, since there is no Dutch corpus that isannotated with CCG derivations, we restrict our at-tention to English in the remainder of this paper.5 A model of predicate-argumentstructureWe first explain how word-word dependencies in thepredicate-argument structure can be captured in agenerative model, and then describe how these prob-abilities are estimated in the current implementation.5.1 Modelling local dependenciesWe first define the probabilities for purely local de-pendencies without coordination.
By excluding non-local dependencies and coordination, at most onedependency relation holds for each word.
Considerthe following sentence:S[dcl]NPNSmithS[dcl]nNPS[dcl]nNPresigned(SnNP)n(SnNP)yesterdayThis derivation expresses the following depen-dencies:hhS[dcl]nNP; resignedi; 1; hN; Smithiihh(SnNP)n(SnNP); yesterdayi; 2; hS[dcl]nNP; resignediiWe assume again that heads are generated beforetheir modifiers or arguments, and that word-worddependencies are expressed by conditioning modi-fiers or arguments on heads.
Therefore, the headwords of arguments (such as Smith) are generatedin the following manner:P (wajca; hhch;whi; i; hca; waii)The head word of modifiers (such as yesterday) aregenerated differently:P (wmjcm; hhcm;wmi; i; hch;whi)Like Collins (1999) and Charniak (2000), the SDmodel assumes that word-word dependencies can bedefined at the maximal projection of a constituent.However, as the Dutch examples show, the argumentslot i can only be determined if the head constituentis fully expanded.
For instance, if S[dcl] expandsto a non-head S=(S=NP) and to a head S[dcl]=NP,it is necessary to know how the S[dcl]=NP expandsto determine which argument is filled by the non-head, even if we already know that the lexical cate-gory of the head word of S[dcl]=NP is a ditransitive((S[dcl]=NP)=NP)=NP.
Therefore, we assume thatthe non-head child of a node is only expanded afterthe head child has been fully expanded.5.2 Modelling long-range dependenciesThe predicate-argument structure that correspondsto a derivation contains not only local, but also long-range dependencies that are projected from the lex-icon or through some rules such as the coordinationof functor categories.
In the following derivation,Smith is the subject of resigned and of left:S[dcl]NPNSmithS[dcl]nNPS[dcl]nNPresignedS[dcl]nNP[conj]conjandS[dcl]nNPleftIn order to express both dependencies, Smith hasto be conditioned on resigned and on left:P (w=Smithj N;hhS[dcl]nNP; resignedi; 1; hN; wii;hhS[dcl]nNP; lefti; 1; hN; wii)In terms of the predicate-argument structure,resigned and left are both lexical heads of thissentence.
Since neither fills an argument slot ofthe other, we assume that they are generated inde-pendently.
This is different from the SD model,which conditions the head word of the secondand subsequent conjuncts on the head word ofthe first conjunct.
Similarly, in a sentence suchas Miller and Smith resigned, the current model as-sumes that the two heads of the subject noun phraseare conditioned on the verb, but not on each other.Argument-cluster coordination constructionssuch as give a dog a bone and a policeman a flowerare another example where the dependencies in thepredicate-argument structure cannot be expressedat the level of the local trees that combine theindividual arguments.
Instead, these dependenciesare projected down through the category of theargument cluster:SnNP1((SnNP1)=NP2)=NP3give(SnNP1)n(((SnNP1)=NP2)=NP3)Lexical categories that project long-range depen-dencies include cases such as relative pronouns, con-trol verbs, auxiliaries, modals and raising verbs.This can be expressed by co-indexing their argu-ments, eg.
(NPnNPi)=(S[dcl]nNPi) for relative pro-nouns.
Here, Smith is also the subject of resign:S[dcl]NPNSmithS[dcl]nNP(S[dcl]nNP)=(S[b]nNP)willS[b]nNPresignAgain, in order to capture this dependency, we as-sume that the entire verb phrase is generated beforethe subject.In relative clauses, there is a dependency betweenthe verbs in the relative clause and the head of thenoun phrase that is modified by the relative clause:NPNPNSmithNPnNP(NPnNP)=(S[dcl]nNP)whoS[dcl]nNPresignedSince the entire relative clause is an adjunct, it isgenerated after the noun phrase Smith.
Therefore,we cannot capture the dependency between Smithand resigned by conditioning Smith on resigned.
In-stead, resigned needs to be conditioned on the factthat its subject is Smith.
This is similar to the wayin which head words of adjuncts such as yesterdayare generated.
In addition to this dependency, wealso assume that there is a dependency between whoand resigned.
It follows that if we want to captureunbounded long-range dependencies such as objectextraction, words cannot be generated at the max-imal projection of constituents anymore.
Considerthe following examples:NPNPThe womanNPnNP(NPnNP)=(S[dcl]=NP)thatS[dcl]=NPS=(SnNP)NPI(S[dcl]nNP)=NPsawNPNPThe womanNPnNP(NPnNP)=(S[dcl]=NP)thatS[dcl]=NPS=(SnNP)NPI(S[dcl]nNP)=NP(S[dcl]nNP)=NPsawNP=NPNP=PPa picturePP=NPofIn both cases, there is a S[dcl]=NP with lexical head(S[dcl]nNP)=NP; however, in the second case, theNP argument is not the object of the transitive verb.This problem can be solved by generatingwords at the leaf nodes instead of at the maxi-mal projection of constituents.
After expandingthe (S[dcl]nNP)=NP node to (S[dcl]nNP)=NP andNP=NP, the NP that is co-indexed with woman can-not be unified with the object of saw anymore.These examples have shown that two changes tothe generative process are necessary if word-worddependencies in the predicate-argument structureare to be captured.
First, head constituents have tobe fully expanded before non-head constituents aregenerated.
Second, words have to be generated atthe leaves of the tree, not at the maximal projectionof constituents.5.3 The word probabilitiesNot all words have functor categories or fill argu-ment slots of other functors.
For instance, punctu-ation marks, conjunctions, and the heads of entiresentences are not conditioned on any other words.Therefore, they are only conditioned on their lexicalcategories.
Therefore, this model contains the fol-lowing three kinds of word probabilities:1.
Argument probabilities:P (wjc;hhc0; w0i; i; hc; wii)The probability of generating word w, giventhat its lexical category is c and that hc; wi ishead of the ith argument of hc0; w0i.2.
Functor probabilities:P (wjc;hhc; wi; i; hc0; w0ii)The probability of generating word w, giventhat its lexical category is c and that hc0; w0i ishead of the ith argument of hc; wi.3.
Other word probabilities: P (wjc)If a word does not fill any dependency relation,it is only conditioned on its lexical category.5.4 The structural probabilitiesLike the SD model, we assume an underlying pro-cess which generates CCG derivation trees startingfrom the root node.
Each node in a derivation treehas a category, a list of lexical heads and a (possi-bly empty) list of dependency relations to be filledby its lexical heads.
As discussed in the previoussection, head words cannot in general be generatedat the maximal projection if unbounded long-rangedependencies are to be captured.
This is not the casefor lexical categories.
We therefore assume that anode?s lexical head category is generated at its max-imal projection, whereas head words are generatedat the leaf nodes.
Since lexical categories are gen-erated at the maximal projection, our model has thesame structural probabilities as the LexCat model ofHockenmaier and Steedman (2002b).5.5 Estimating word probabilitiesThis model generates words in three differentways?as arguments of functors that are alreadygenerated, as functors which have already one (ormore) arguments instantiated, or independent of thesurrounding context.
The last case is simple, as thisprobability can be estimated directly, by countingthe number of times c is the lexical category of w inthe training corpus, and dividing this by the numberof times c occurs as a lexical category in the trainingcorpus:^P (wjc) =C(w; c)C(c)In order to estimate the probability of an argumentw, we count the number of times it occurs with lex-ical category c and is the ith argument of the lexicalfunctor hc0; w0i in question, divided by the numberof times the ith argument of hc0; w0i is instantiatedwith a constituent whose lexical head category is c:^P (wjc; hhc0; w0i; i; hc; wii) =C(hhc0; w0i; i; hc; wii)Pw00C(hhc0; w0i; i; hc; w00ii)The probability of a functor w, given that its ith ar-gument is instantiated by a constituent whose lexicalhead is hc0; w0i can be estimated in a similar manner:^P (wjc; hhc; wi; i; hc0; w0ii) =C(hhc; wi; i; hc0; w0ii)Pw00C(hhc; w00i; i; hc0; w0ii)Here we count the number of times the ith argu-ment of hc; wi is instantiated with hc0; w0i, and di-vide this by the number of times that hc0; w0i is theith argument of any lexical head with category c.For instance, in order to compute the probabilityof yesterday modifying resigned as in the previoussection, we count the number of times the transitiveverb resigned was modified by the adverb yesterdayand divide this by the number of times resigned wasmodified by any adverb of the same category.We have seen that functor probabilities are notonly necessary for adjuncts, but also for certaintypes of long-range dependencies such as the rela-tion between the noun modified by a relative clauseand the verb in the relative clause.
In the case of zeroor reduced relative clauses, some of these dependen-cies are also captured by the SD model.
However, inthat model, only counts from the same type of con-struction could be used, whereas in our model, thefunctor probability for a verb in a zero or reducedrelative clause can be estimated from all occurrencesof the head noun.
In particular, all instances of thenoun and verb occurring together in the training data(with the same predicate-argument relation betweenthem, but not necessarily with the same surface con-figuration) are taken into account by the new model.To obtain the model probabilities, the relative fre-quency estimates of the functor and argument prob-abilities are both interpolated with the word proba-bilities ^P (wjc).5.6 Conditioning events on multiple headsIn the presence of long-range dependencies and co-ordination, the new model requires the conditioningof certain events on multiple heads.
Since it is un-likely that such probabilities can be estimated di-rectly from data, they have to be approximated insome manner.If we assume that all dependencies depithat holdfor a word are equally likely, we can approximateP (wjc; dep1; :::; depn) as the average of the individ-ual dependency probabilities:P (wjc; dep1; :::; depn) 1nnXi=1P (wjc; depi)This approximation is has the advantage that it iseasy to compute, but might not give a good estimate,since it averages over all individual distributions.6 Dynamic programming and beam searchThis section describes how this model is integratedinto a CKY chart parser.
Dynamic programming andeffective beam search strategies are essential to guar-antee efficient parsing in the face of the high ambi-guity of wide-coverage grammars.
Both use the in-side probability of constituents.
In lexicalized mod-els where each constituent has exactly one lexicalhead, and where this lexical head can only dependon the lexical head of one other constituent, the in-side probability of a constituent is the probabilitythat a node with the label and lexical head of thisconstituent expands to the tree below this node.
Theprobability of generating a node with this label andlexical head is given by the outside probability of theconstituent.In the model defined here, the lexical head ofa constituent can depend on more than one otherword.
As explained in section 5.2, there are in-stances where the categorial functor is conditionedon its arguments ?
the example given above showedthat verbs in relative clauses are conditioned on thelexical head of the noun which is modified by therelative clause.
Therefore, the inside probability ofa constituent cannot include the probability of anylexical head whose argument slots are not all filled.This means that the equivalence relation definedby the probability model needs to take into accountnot only the head of the constituent itself, but alsoall other lexical heads within this constituent whichhave at least one unfilled argument slot.
As a conse-quence, dynamic programming becomes less effec-tive.
There is a related problem for the beam search:in our model, the inside probabilities of constituentswithin the same cell cannot be directly comparedanymore.
Instead, the number of unfilled lexicalheads needs to be taken into account.
If a lexicalhead hc; wi is unfilled, the evaluation of the proba-bility of w is delayed.
This creates a problem for thebeam search strategy.The fact that constituents can have more than onelexical head causes similar problems for dynamicprogramming and the beam search.In order to be able to parse efficiently with ourmodel, we use the following approximations for dy-namic programming and the beam search: Two con-stituents with the same span and the same categoryare considered equivalent if they delay the evalua-tion of the probabilities of the same words and ifthey have the same number of lexical heads, and ifthe first two elements of their lists of lexical headsare identical (the same words and lexical categories).This is only an approximation to true equivalence,since we do not check the entire list of lexical heads.Furthermore, if a cell contains more than 100 con-stituents, we iteratively narrow the beam (by halv-ing it in size)2 until the beam search has no furthereffect or the cell contains less than 100 constituents.This is a very aggressive strategy, and it is likely toadversely affect parsing accuracy.
However, morelenient strategies were found to require too muchspace for the chart to be held in memory.
A betterway of dealing with the space requirements of ourmodel would be to implement a packed shared parseforest, but we leave this to future work.7 An experimentWe use sections 02-21 of CCGbank for training, sec-tion 00 for development, and section 23 for test-ing.
The input is POS-tagged using the tagger ofRatnaparkhi (1996).
However, since parsing withthe new model is less efficient, only sentences  40tokens only are used to test the model.
A fre-quency cutoff of  20 was used to determine rarewords in the training data, which are replaced withtheir POS-tags.
Unknown words in the test dataare also replaced by their POS-tags.
The modelsare evaluated according to their Parseval scores andto the recovery of dependencies in the predicate-argument structure.
Like Clark et al (2002), wedo not take the lexical category of the dependentinto account, and evaluate hhc; wi; i; h ; w0ii for la-belled, and hh ; wi; ; h ; w0ii for unlabelled recov-ery.
Undirectional recovery (UdirP/UdirR) evalu-ates only whether there is a dependency between wand w0.
Unlike unlabelled recovery, this does not pe-2Beam search is as in Hockenmaier and Steedman (2002b).nalize the parser if it mistakes a complement for anadjunct or vice versa.In order to determine the impact of capturing dif-ferent kinds of long-range dependencies, four differ-ent models were investigated: The baseline model islike the LexCat model of (2002b), since the struc-tural probabilities of our model are like those ofthat model.
Local only takes local dependenciesinto account.
LeftArgs only takes long-range de-pendencies that are projected through left arguments(nX) into account.
This includes for instance long-range dependencies projected by subjects, subjectand object control verbs, subject extraction and left-node raising.
All takes all long-range dependen-cies into account, in particular it extends LeftArgsby capturing also the unbounded dependencies aris-ing through right-node-raising and object extraction.Local, LeftArgs and All are all tested with the ag-gressive beam strategy described above.In all cases, the CCG derivation includes all long-range dependencies.
However, with the models thatexclude certain kinds of dependencies, it is possiblethat a word is conditioned on no dependencies.
Inthese cases, the word is generated with P (wjc).Table 1 gives the performance of all four mod-els on section 23 in terms of the accuracy of lexicalcategories, Parseval scores, and in terms of the re-covery of word-word dependencies in the predicate-argument structure.
Here, results are further bro-ken up into the recovery of local, all long-range,bounded long-range and unbounded long-range de-pendencies.LexCat does not capture any word-word de-pendencies.
Its performance on the recovery ofpredicate-argument structure can be improved by3% by capturing only local word-word dependen-cies (Local).
This excludes certain kinds of depen-dencies that were captured by the SD model.
For in-stance, the dependency between the head of a nounphrase and the head of a reduced relative clause (theshares bought by John) is captured by the SD model,since shares and bought are both heads of the localtrees that are combined to form the complex nounphrase.
However, in the SD model the probability ofthis dependency can only be estimated from occur-rences of the same construction, since dependencyrelations are defined in terms of local trees and notin terms of the underlying predicate-argument struc-LexCat Local LeftArgs AllLex.
cats: 88.2 89.9 90.1 90.1ParsevalLP: 76.3 78.4 78.5 78.5LR: 75.9 78.5 79.0 78.7UP: 82.0 83.4 83.6 83.2UR: 81.6 83.6 83.8 83.4Predicate-argument structure (all)LP: 77.3 80.8 81.6 81.5LR: 78.2 80.6 81.5 81.4UP: 86.4 88.3 88.9 88.7UR: 87.4 88.1 88.8 88.6UdirP: 88.0 89.7 90.2 90.0UdirR: 89.0 89.5 90.1 90.0Non-long-range dependenciesLP: 78.9 82.5 83.0 82.9LR: 79.5 82.3 82.7 82.6UP: 87.5 89.7 89.9 89.8UR: 88.1 89.4 89.6 89.4All long-range dependenciesLP: 60.8 62.6 67.1 66.3LR: 64.4 63.0 68.5 68.8UP: 75.3 74.2 78.9 78.1UR: 80.2 74.9 80.5 80.9Bounded long-range dependenciesLP: 63.9 64.8 69.0 69.2LR: 65.9 64.1 70.2 70.0UP: 79.8 77.1 81.4 81.4UR: 82.4 76.7 82.6 82.6Unbounded long-range dependenciesLP: 46.0 50.4 55.6 52.4LR: 54.7 55.8 58.7 61.2UP: 54.1 58.2 63.8 61.1UR: 66.5 63.7 66.8 69.9Table 1: Evaluation (sec.
23,  40 words).ture.
By including long-range dependencies on leftarguments (such as subjects) (LeftArgs), a furtherimprovement of 0.7% on the recovery of predicate-argument structure is obtained.
This model capturesthe dependency between shares and bought.
In con-trast to the SD model, it can use all instances ofshares as the subject of a passive verb in the train-ing data to estimate this probability.
Therefore, evenif shares and bought do not co-occur in this partic-ular construction in the training data, the event thatis modelled by our dependency model might not beunseen, since it could have occurred in another syn-tactic context.Our results indicate that in order to perform wellon long-range dependencies, they have to be in-cluded in the model, since Local, the model thatcaptures only local dependencies performs worse onlong-range dependencies than LexCat, the modelthat captures no word-word dependencies.
How-ever, with more than 5% difference on labelled pre-cision and recall on long-range dependencies, themodel which captures long-range dependencies onleft arguments performs significantly better on re-covering long-range dependencies than Local.
Thegreatest difference in performance between the mod-els which do capture long-range dependencies andthe models which do not is on long-range dependen-cies.
This indicates that, at least in the kind of modelconsidered here, it is very important to model notjust local, but also long-range dependencies.
It is notclear why All, the model that includes all dependen-cies, performs slightly worse than the model whichincludes only long-range dependencies on subjects.On the Wall Street Journal task, the overall per-formance of this model is lower than that of theSD model of Hockenmaier and Steedman (2002b).In that model, words are generated at the maxi-mal projection of constituents; therefore, the struc-tural probabilities can also be conditioned on words,which improves the scores by about 2%.
It is alsovery likely that the performance of the new modelsis harmed by the very aggressive beam search.8 Conclusion and future workThis paper has defined a new generative model forCCG derivations which captures the word-word de-pendencies in the corresponding predicate-argumentstructure, including bounded and unbounded long-range dependencies.
In contrast to the conditionalmodel of Clark et al (2002), our model capturesthese dependencies in a sound and consistent man-ner.
The experiments presented here demonstratethat the performance of a simple baseline modelcan be improved significantly if long-range depen-dencies are also captured.
In particular, our re-sults indicate that it is important not to restrict themodel to local dependencies.
Future work will ad-dress the question whether these models can berun with a less aggressive beam search strategy, orwhether a different parsing algorithm is more suit-able.
The problems that arise due to the overlyaggressive beam search strategy might be over-come if we used an n-best parser with a simplerprobability model (eg.
of the kind proposed byHockenmaier and Steedman (2002b)) and used thenew model as a re-ranker.
The current implemen-tation uses a very simple method of estimating theprobabilities of multiple dependencies, and more so-phisticated techniques should be investigated.We have argued that a model of the kind proposedin this paper is essential for parsing languages withfreer word order, such as Dutch or Czech, where themodel of Hockenmaier and Steedman (2002b) (andother models of surface dependencies) would sys-tematically capture the wrong dependencies, even ifonly local dependencies are taken into account.
ForEnglish, our experimental results demonstrate thatour model benefits greatly from modelling not onlylocal, but also long-range dependencies, which arebeyond the scope of surface dependency models.AcknowledgementsI would like to thank Mark Steedman and Stephen Clark formany helpful discussions, and gratefully acknowledge supportfrom an EPSRC studentship and grant GR/M96889, the Schoolof Informatics, and NSF ITR grant 0205 456.ReferencesEugene Charniak.
2000.
A Maximum-Entropy-Inspired Parser.In Proceedings of the First Meeting of the NAACL, Seattle.Stephen Clark, Julia Hockenmaier, and Mark Steedman.2002.
Building Deep Dependency Structures using a Wide-Coverage CCG Parser.
In Proceedings of the 40th AnnualMeeting of the ACL.Michael Collins, Jan Hajic, Lance Ramshaw, and ChristophTillmann.
1999.
A Statistical Parser for Czech.
In Pro-ceedings of the 37th Annual Meeting of the ACL.Michael Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, University ofPennsylvania.Julia Hockenmaier and Mark Steedman.
2002a.
AcquiringCompact Lexicalized Grammars from a Cleaner Treebank.In Proceedings of the Third LREC, pages 1974?1981, LasPalmas, May.Julia Hockenmaier and Mark Steedman.
2002b.
GenerativeModels for Statistical Parsing with Combinatory CategorialGrammar.
In Proceedings of the 40th Annual Meeting of theACL.Julia Hockenmaier.
2003.
Data and Models for StatisticalParsing with CCG.
Ph.D. thesis, School of Informatics, Uni-versity of Edinburgh.Adwait Ratnaparkhi.
1996.
A Maximum Entropy Part-Of-Speech Tagger.
In Proceedings of the EMNLP Conference,pages 133?142, Philadelphia, PA.Mark Steedman.
2000.
The Syntactic Process.
The MIT Press,Cambridge Mass.
