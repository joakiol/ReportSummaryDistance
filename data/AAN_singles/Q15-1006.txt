Which Step Do I Take First?
Troubleshooting with Bayesian ModelsAnnie Louis and Mirella LapataSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB{alouis,mlap}@inf.ed.ac.ukAbstractOnline discussion forums and communityquestion-answering websites provide one ofthe primary avenues for online users to shareinformation.
In this paper, we propose textmining techniques which aid users navigatetroubleshooting-oriented data such as ques-tions asked on forums and their suggested so-lutions.
We introduce Bayesian generativemodels of the troubleshooting data and applythem to two interrelated tasks: (a) predictingthe complexity of the solutions (e.g., plugginga keyboard in the computer is easier comparedto installing a special driver) and (b) present-ing them in a ranked order from least to mostcomplex.
Experimental results show that ourmodels are on par with human performanceon these tasks, while outperforming baselinesbased on solution length or readability.1 IntroductionOnline forums and discussion boards have creatednovel ways for discovering, sharing, and distribut-ing information.
Users typically post their ques-tions or problems and obtain possible solutions fromother users.
Through this simple mechanism ofcommunity-based question answering, it is possibleto find answers to personal, open-ended, or highlyspecialized questions.
However, navigating the in-formation available in web-archived data can bechallenging given the lack of appropriate search andbrowsing facilities.Table 1 shows examples typical of the problemsand proposed solutions found in troubleshooting-oriented online forums.
The first problem concerns ashaky monitor and has three solutions with increas-ing degrees of complexity.
Solution (1) is probablyeasiest to implement in terms of user time, effort,and expertise; solution (3) is most complex (i.e., theuser should understand what signal timing is andThe screen is shaking.1.
Move all objects that emit a magnetic field, such as amotor or transformer, away from the monitor.2.
Check if the specified voltage is applied.3.
Check if the signal timing of the computer system iswithin the specification of the monitor.
?Illegal Operation has Occurred?
error message isdisplayed.1.
Software being used is not Microsoft-certified for yourversion of Windows.
Verify that the software is certifiedby Microsoft for your version of Windows (see programpackaging for this information).2.
Configuration files are corrupt.
If possible, save all data,close all programs, and restart the computer.Table 1: Example problems and solutions taken from on-line troubleshooting oriented forums.then try to establish whether it is within the specifi-cation of the monitor), whereas solution (2) is some-where in between.
In most cases, the solutions arenot organized in any particular fashion, neither interms of content nor complexity.In this paper, we present models to automaticallypredict the complexity of troubleshooting solutions,which we argue could improve user experience, andpotentially help solve the problem faster (e.g., byprioritizing easier solutions).
Automatically struc-turing solutions according to complexity could alsofacilitate search through large archives of solutionsor serve as a summarization tool.
From a linguis-tic perspective, learning how complexity is verbal-ized can be viewed as an instance of grounded lan-guage acquisition.
Solutions direct users to carryout certain actions (e.g., on their computers or de-vices) and complexity is an attribute of these ac-tions.
Information access systems incorporating anotion of complexity would allow to take user in-tentions into account and how these translate intonatural language.
Current summarization and infor-mation retrieval methods are agnostic of such typesof text semantics.
Moreover, the models presentedhere could be used for analyzing collaborative prob-73Transactions of the Association for Computational Linguistics, vol.
3, pp.
73?85, 2015.
Action Editor: Eric Fosler-Lussier.Submission batch: 9/2014; Revision batch 1/2015; Published 2/2015.
c?2015 Association for Computational Linguistics.lem solving and its social networks.
Characterizingthe content of discussion forums by their complexitycan provide additional cues for identifying user au-thority and if there is a need for expert intervention.We begin by validating that the task is indeedmeaningful and that humans perceive varying de-grees of complexity when reading troubleshootingsolutions.
We also show experimentally that usersagree in their intuitions about the relative complex-ity of different solutions to the same problem.
Wedefine ?complexity?
as an aggregate notion of thetime, expertise, and money required to implementa solution.
We next model the complexity predic-tion task, following a Bayesian approach.
Specifi-cally, we learn to assign complexity levels to solu-tions based on their linguistic makeup.
We leverageweak supervision in the form of lists of solutions (todifferent problems) approximately ordered from lowto high complexity (see Table 1).
We assume thatthe data is generated from a fixed number of dis-crete complexity levels.
Each level has a probabilitydistribution over the vocabulary and there is a canon-ical ordering between levels indicating their relativecomplexity.
During inference, we recover the vo-cabularies of the complexity levels and the orderingof levels that explains the solutions and their attestedsequences in the training data.We explore two Bayesian models differing in howthey learn an ordering among complexity levels.
Thefirst model is local, it assigns an expected position(in any list of solutions) to each complexity leveland orders the levels based on this expected posi-tion value.
The second model is global, it definesprobabilities over permutations of complexity lev-els and directly uncovers a consensus ordering fromthe training data.
We evaluate our models on a so-lution ordering task, where the goal is to rank so-lutions from least to most complex.
We show thata supervised ranking approach using features basedon the predictions of our generative models is on parwith human performance on this task while outper-forming competitive baselines based on length andreadability of the solution text.2 Related workThere is a long tradition of research on decision-theoretic troubleshooting where the aim is to finda cost efficient repair strategy for a malfunction-ing device (Heckerman et al., 1995).
Typically, adiagnostic procedure (i.e., a planner) is developedthat determines the next best troubleshooting stepby estimating the expected cost of repair for variousplans.
Costs are specified by domain experts and areusually defined in terms of time and/or money in-curred by carrying out a particular repair action.
Ournotion of complexity is conceptually similar to thecost of an action, however we learn to predict com-plexity levels rather than calibrate them manually.Also note that our troubleshooting task is not devicespecific.
Our models learn from troubleshooting-oriented data without any restrictions on the prob-lems being solved.Previous work on web-based user support hasmostly focused on thread analysis.
The idea is tomodel the content structure of forum threads by an-alyzing the requests for information and suggestedsolutions in the thread data (Wang et al., 2011; Kimet al., 2010).
Examples of such analysis includeidentifying which earlier post(s) a given post re-sponds to and in what manner (e.g., is it a question,an answer or a confirmation).
Other related work(Lui and Baldwin, 2009) identifies user characteris-tics in such data, i.e., whether users express them-selves clearly, whether they are technically knowl-edgeable, and so on.
Although our work does notaddress threaded discourse, we analyze the contentof troubleshooting data and show that it is possibleto predict the complexity levels for suggested solu-tions from surface lexical cues.Our work bears some relation to language ground-ing, the problem of extracting representations of themeaning of natural language tied to the physicalworld.
Mapping instructions to executable actionsis an instance of language grounding with applica-tions to automated troubleshooting (Branavan et al.,2009; Eisenstein et al., 2009), navigation (Vogel andJurafsky, 2010), and game-playing (Branavan et al.,2011).
In our work, there is no direct attempt tomodel the environment or the troubleshooting steps.Rather, we study the language of instructions andhow it correlates with the complexity of the impliedactions.
Our results show that it possible to predictcomplexity, while being agnostic about the seman-tics of the domain or the effect of the instructions inthe corresponding environment.Our generative models are trained on existingarchives of problems with corresponding solutions(approximately ordered from least to most complex)74and learn to predict an ordering for new sets of so-lutions.
This setup is related to previous studieson information ordering where the aim is to learnstatistical patterns of document structure which canbe then used to order new sentences or paragraphsin a coherent manner.
Some approaches approxi-mate the structure of a document via topic and entitysequences using local dependencies such as condi-tional probabilities (Lapata, 2003; Barzilay and La-pata, 2008) or Hidden Markov Models (Barzilay andLee, 2004).
More recently, global approaches whichdirectly model the permutations of topics in the doc-ument have been proposed (Chen et al., 2009b).
Fol-lowing this line of work, one of our models usesthe Generalized Mallows Model (Fligner and Ver-ducci, 1986) in its generative process which allowsto model permutations of complexity levels in thetraining data.3 Problem formulationOur aim in this work is to learn models which canautomatically reorder solutions to a problem fromlow to high complexity.
Let G = (c1, c2, .. cN ) bea collection of solutions to a specific problem.
Wewish to output a list G?
= (c?1, c?2, .. c?N ), such thatD(c?j) ?
D(c?j+1), where D(x) refers to the com-plexity of solution x.3.1 Corpus CollectionAs training data we are given problem-solution setssimilar to the examples in Table 1 where the solu-tions are approximately ordered from low to highcomplexity.
A solution set Si is specific to prob-lem Pi, and contains an ordered list of NPi solu-tions Si = (x1, x2, .
.
.
, xNPi ) such that D(xj) <D(xj+1).
We refer to the number of solutions re-lated to a problem, NPi , as its solution set size.For our experiments, we collected 300 problems1and their solutions from multiple web sites includ-ing the computing support pages of Microsoft, Ap-ple, HP, as well as amateur computer help websitessuch as www.computerhope.com.
The prob-lems were mostly frequently asked questions (FAQs)referring to malfunctioning personal computers andsmart phones.
The solutions were provided by com-puter experts or experienced users in the absence1The corpus can be downloaded from http://www.homepages.inf.ed.ac.uk/alouis/solutionComplexity.html.2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Solution set size0510152025303540455055606570FrequencyFigure 1: Histogram of solution set sizesof any interaction with other users or their devicesand thus constitute a generic list of steps to try out.We assume that in such a situation, the solutionproviders are likely to suggest simpler solutions be-fore other complex ones, leading to the solution listsbeing approximately ordered from low to high com-plexity.
In the next section, we verify this assump-tion experimentally.
In this dataset, the solution setsize varies between 2 and 16 and the average numberis 4.61.
Figure 1 illustrates the histogram of solutionset sizes found in our corpus.
We only consideredproblems which have no less than two solutions.
Allwords in the corpus were lemmatized and html linksand numbers were replaced with placeholders.
Theresulting vocabulary was approximately 2,400 wordtypes (62,152 tokens).Note that our dataset provides only weak super-vision for learning.
The relative complexity of so-lutions for the same problem is observed, however,the relative complexity of solutions across differentproblems is unknown.
For example, a hardware is-sue may generally receive highly complex solutionswhereas a microphone issue mostly simple ones.3.2 Task ValidationIn this section, we detail an annotation experimentwhere we asked human judges to rank the randomlypermuted contents of a solution set according to per-ceived complexity.
We performed this study for tworeasons.
Firstly, to ascertain that participants areable to distinguish degrees of complexity and agreeon the complexity level of a solution.
Secondly, toexamine whether the ordering produced by partici-pants corresponds to the (gold-standard) FAQ orderof the solutions.
If true, this would support our hy-75B C D FAQA 0.421 0.525 0.625 0.465B 0.436 0.434 0.252C 0.584 0.303D 0.402Table 2: Correlation matrix for annotators (A?D) andoriginal FAQ order using Kendall?s ?
(values are aver-ages over 100 problem-solution sets).pothesis that the solutions in our FAQ corpus are fre-quently presented according to complexity and thatthis ordering is reasonable supervision for our mod-els.Method We randomly sampled 100 solution sets(their sizes vary between 2 and 16) from theFAQ corpus described in the previous section andrandomly permuted the contents of each set.
Fourannotators, one an author of this paper, and threegraduate and undergraduate students in ComputerScience were asked to order the solutions in eachset from easy to most complex.
An easier solutionwas defined as one ?which takes less time or effortto carry out by a user?.
The annotators saw a list ofsolutions for the same problem on a web interfaceand assigned a rank to each solution to create an or-der.
No ties were allowed and a complete orderingwas required to keep the annotation simple.
The an-notators were fluent English speakers and had someknowledge of computer hardware and software.
Werefrained from including novice users in our study asthey are likely to have very different personal pref-erences resulting in more divergent rankings.Results We measured inter-annotator agreementusing Kendall?s ?
, a metric of rank correlation whichhas been reliably used in information ordering eval-uations (Lapata, 2006; Bollegala et al., 2006; Mad-nani et al., 2007).
?
ranges between ?1 and +1,where +1 indicates equivalent rankings, ?1 com-pletely reverse rankings, and 0 independent rank-ings.
Table 2 shows the pairwise inter-annotatoragreement as well as the agreement between eachannotator and the original FAQ order.
The tableshows fair agreement between the annotators con-firming that this is a reasonable task for humans todo.
As can be seen, there are some individual dif-ferences, with the inter-annotator agreement varyingfrom 0.421 (for A,B) to 0.625 (for A,D).The last column in Table 2 reports the agreementbetween our annotator rankings and the original or-dering of solutions in the FAQ data.
Although thereis fair agreement with the FAQ providing support forits use as a gold-standard, the overall ?
values arelower compared to inter-annotator agreement.
Thisimplies that the ordering may not be strictly increas-ing in complexity in our dataset and that our modelsshould allow for some flexibility during learning.Several reasons contributed to disagreements be-tween annotators and with the FAQ ordering, suchas the users?
expertise, personal preferences, or thenature of the solutions.
For instance, annotators dis-agreed when multiple solutions were of similar com-plexity.
For the first example in Table 1, all anno-tators agreed perfectly and also matched the FAQorder.
For the second example, the annotators dis-agreed with each other and the FAQ.4 Generative ModelsIn the following we introduce two Bayesian topicmodels for the complexity prediction (and rank-ing) task.
In these models, complexity is capturedthrough a discrete set D of L levels and a total or-dering between the levels reflects their relative com-plexity.
In other words, D = (d1, d2, ...dL), whered1 is easiest level and D(dm) < D(dm+1) .
Eachcomplexity level is parametrized by a unigram lan-guage model which captures words likely to occurin solutions with that level.Our two models are broadly similar.
Their gen-erative process assigns a complexity level from Dto each solution such that it explains the wordsin the solution and also the ordering of solu-tions within each solution set.
Words are gener-ated for each solution by mixing problem-specificwords with solution-specific (and hence complexity-related) ones.
Also, each problem has its own distri-bution over complexity levels which allows for someproblems to have more complex solutions on aver-age, some a mix of high and low complexity solu-tions, or otherwise predominantly easier solutions.The main difference between the two models isin the way they capture the ordering between levels.Our first model infers a distribution for each levelover the positions at which a solution with that com-plexity can occur and uses this distribution to orderthe levels.
Levels which on average occur at greaterpositions have higher complexity.
The second modeldefines probabilities over orderings of levels in the76Corpus levelFor each complexity level dm, 1 ?
m ?
L,- Draw a complexity vocabulary distribution?m ?
Dirichlet(?
)- Draw a distribution over positions?m ?
Dirichlet(?
)- Draw a distribution ?
for the proportion of complexity-versus problem-specific vocabulary ?
Beta(?0, ?1)Solution set levelFor each solution set Qi in the corpus, 1 ?
i ?
N ,- Draw a distribution over the complexity levels?i ?
Dirichlet(?
)- Draw a problem-specific vocabulary distribution?i ?
Dirichlet(?
)Individual solution levelFor each solution xij in Qi, 1 ?
j ?
NPi ,- Draw a complexity level assignment,zij ?
Multinomial(?i)- Draw a position depending on the level assigned,rij ?
Multinomial(?zij )Word levelFor each word wijk in solution xij ,- Draw a switch value to indicate if the word isproblem- or complexity-specific, sijk ?
Binomial(?
)- If sijk = 0, draw wijk ?
Multinomial(?zij )- If sijk = 1, draw wijk ?
Multinomial(?i)Figure 2: Generative process for the Position Modelgenerative process itself.
The inference process ofthis model allows to directly uncover a canonical or-dering of the levels which explains the training data.4.1 Expected Position modelThis model infers the vocabulary associated with acomplexity level and a distribution over the numer-ical positions in a solution set where such a com-plexity level is likely to occur.
After inference, themodel uses the position distribution to compute theexpected position of each complexity level.
The lev-els are ordered from low to high expected positionand taken as the order of increasing complexity.The generative process for our model is describedin Figure 2.
A first phase generates the latent vari-ables which are drawn once for the entire corpus.Then, variables are drawn for a solution set, next foreach solution in the set, and finally for the words inthe solutions.
The number of complexity levels Lis a parameter in the model, while the vocabularysize V is fixed.
For each complexity level dm, wedraw one multinomial distribution ?
over the vocab-ulary V , and another multinomial ?
over the pos-sible positions.
These two distributions are drawnfrom symmetric Dirichlet priors with hyperparame-ters ?
and ?.
Solutions will not only contain wordsrelating to their complexity but also to the prob-lem or malfunctioning component at hand.
We as-sume these words play a minor role in determiningcomplexity and thus draw a binomial distribution ?that balances the amount of problem-specific ver-sus complexity-specific vocabulary.
This distribu-tion has a Beta prior with hyperparameters ?0 and ?1.For each solution set, we draw a distribution overthe complexity levels ?
from another Dirichlet priorwith concentration ?.
This distribution allows eachproblem to take a different preference and mix ofcomplexity levels for its solutions.
Another multino-mial ?
over the vocabulary is drawn for the problem-specific content of each solution set.
?
is given asymmetric Dirichlet prior with concentration ?.For each individual solution in a set, we draw acomplexity level z from ?, i.e., the complexity levelproportions for that problem.
A position for the so-lution is then drawn from the position distributionfor that level, i.e., ?z .
The words in the solution aregenerated by first drawing a switch value for eachword indicating if the word came from the problem?stechnical or complexity vocabulary.
Accordingly,the word is drawn from ?
or ?z .During inference, we are interested in the poste-rior of the model given the FAQ training data.
Basedon the conditional independencies of the model, theposterior is proportional to:P (?|?0, ?1)?L?m=1[P (?m|?
)]?L?m=1[P (?m|?
)]?N?i=1[P (?i|?
)]?N?i=1[P (?i|?
)]?N?i=1NPi?j=1[P (zij |?i)P (rij |?zij )]?N?i=1NPi?j=1|xij |?k=1[P (sijk|?
)P (wijk|sijk, ?zij , ?i)]where L is the number of complexity levels, N thenumber of problems in the training corpus, NPi thesize of solution set for problem Pi, and |xij | thenumber of words in solution xij .The use of conjugate priors for the multinomial77and binomial distributions allows us to integrate outthe ?, ?, ?, ?
and ?
distributions.
The simplifiedposterior is proportional to:N?i=1L?m=1?
(Rmi +?m)?
(L?m=1Rmi +?m) ?L?m=1G?r=1?(Qrm+?r)?(G?r=1Qrm+?r)?1?u=0?(Tu+?u)?
( 1?u=0Tu+?u) ?L?m=1V?v=1?
(T 0m(v)+?v)?
(V?v=1T 0m(v)+?v)?N?i=1V?v=1?
(T 1i (v)+?v)?
(V?v=1T 1i (v)+?v)where Rmi is the number of times level m is as-signed to a solution in problem i. Qrm is the num-ber of times a solution with position r is given com-plexity m over the full corpus.
Positions are integervalues between 1 and G. T 0 and T 1 count the num-ber of switch assignments of value 0 (complexity-related word) and 1 (technical word) respectively inthe corpus.
T 0m(v) is a refined count of the numberof times word type v is assigned switch value 0 in asolution of complexity m. T 1i (v) counts the numberof times switch value 1 is given to word type v in asolution set for problem i.We sample from this posterior using a collapsedGibbs sampling algorithm.
The sampling sequencestarts with a random initialization to the hidden vari-ables.
During each iteration, the sampler choosesa complexity level for each solution based on thecurrent assignments to all other variables.
Then theswitch values for the words in each solution are sam-pled one by one.
The hyperparameters are tuned us-ing grid search on development data.
The languagemodel concentrations ?, ?
and ?
are given valuesless than 1 to obtain sparse distributions.
The prioron ?, the topic proportions, is chosen to be greaterthan 1 to encourage different complexity levels to beused within the same problem rather than assigningall solutions to the same one or two levels.
Similarly,?0 and ?1 are > 1.
We run 5,000 sampling iterationsand use the last sample as a draw from the posterior.Using these assignments, we also compute an esti-mate for the parameters ?m, ?i and ?m.
For exam-ple, the probability of a word v in ?m is computedas T 0m(v)+?v?v(T 0m(v)+?v).After inference, we obtain probability distribu-tions for each complexity level dm over the vocabu-lary ?m and positions ?m.
We compute the expectedposition Ep of dm as:Ep(dm) =G?pos=1pos ?
?m(pos) (1)where pos indicates position values.
Then, we rankthe levels in increasing order of Ep.4.2 Permutations-based modelIn our second model we incorporate the orderingof complexity levels in the generative process itself.This is achieved by using the Generalized MallowsModel (GMM; Fligner and Verducci (1986)) withinour hierarchical generative process.
The GMM isa probabilistic model over permutations of itemsand is frequently used to learn a consensus orderinggiven a set of different rankings.
It assumes there isan underlying canonical order of items and concen-trates probability mass on those permutations thatdiffer from the canonical order by a small amount,while assigning lesser probability to very divergentpermutations.
Probabilistic inference in this modeluncovers the canonical ordering.The standard Mallows model (Mallows, 1957)has two parameters, a canonical ordering ?
and adispersion penalty ?
> 0.
The probability of an ob-served ordering pi is defined as:P (pi|?,?)
= e??d(pi,?)?(?)
,where d(pi,?)
is a distance measure such asKendall?s ?
, between the canonical ordering ?
andan observed ordering pi.
The GMM decomposesd(pi,?)
in a way that captures item-specific dis-tance.
This is done by computing an inversion vec-tor representation of d(pi,?).
A permutation pi of nitems can be equivalently represented by a vector ofinversion counts v of length n?1, where each com-ponent vi equals the number of items j > i that oc-cur before item i in pi.
The dimension of v is n?
1since there can be no items greater than the high-est value element.
A unique inversion vector can becomputed for any permutation and vice versa, andthe sum of the inversion vector elements is equalto d(pi,?).
Each vi is also given a separate disper-sion penalty ?i.
Then, the GMM is defined as:GMM(v|?)
??ie?
?ivi (2)78Corpus levelFor each complexity level dm, 1 ?
m ?
L,- Draw a complexity vocabulary distribution?m ?
Dirichlet(?
)- Draw the L?
1 dispersion parameters?r ?
GMM0(?0,v0r)- Draw a distribution ?
for the proportion of complexity-versus problem-specific vocabulary ?
Beta(?0, ?1)Solution set levelFor each solution set Qi in the corpus, 1 ?
i ?
N ,- Draw a distribution over the complexity levels?i ?
Dirichlet(?
)- Draw a problem-specific vocabulary distribution?i ?
Dirichlet(?
)- Draw a bag of NPi levels for the solution setbi ?
Multinomial(?i)- Draw an inversion vector vivi ?
GMM(?
)- Compute permutation pii of levels using vi- Compute level assignments zi using pii and bi.Assign level zij to xij for 1 ?
j ?
NPi .Word levelFor each word wijk in solution xij ,- Draw a switch value to indicate if the word isproblem- or complexity-specific, sijk ?
Binomial(?
)- If sijk = 0, draw wijk ?
Multinomial(?zij )- If sijk = 1, draw wijk ?
Multinomial(?i)Figure 3: Generative process for Permutation Model.And can be further factorized into item-specificcomponents:GMMi(vi|?i) ?
e?
?ivi (3)Since the GMM is a member of the exponential fam-ily, a conjugate prior can be defined for each disper-sion parameter ?i which allows for efficient infer-ence.
We refer the interested reader to Chen et al.
(2009a) for details on the prior distribution and nor-malization factor for the GMM distribution.Figure 3 formalizes the generative story of ourown model which uses the GMM as a component.We assume the canonical order is the strictly increas-ing (1, 2, .., L) order.
For each complexity level dm,we draw a distribution ?
over the vocabulary.
Wealso drawL?
1 dispersion parameters from the con-jugate prior GMM0 density.
Hyperparameters forthis prior are set in a similar fashion to Chen etal.
(2009a).
As in the position model, we drawa binomial distribution, ?
(with a beta prior) overcomplexity- versus problem-specific vocabulary.
Atthe solution set level, we draw a multinomial dis-tribution ?
over the vocabulary and a multinomialdistribution ?
for the proportion of L levels for thisproblem.
Both these distributions have Dirichlet pri-ors.
Next, we generate an ordering for the complex-ity levels.
We draw NPi complexity levels from ?,one for each solution in the set.
Let b denote thisbag of levels (e.g., b = (1, 1, 2, 3, 4, 4, 4) assuming4 complexity levels and 7 solutions for a particularproblem).
We also draw an inversion vector v fromthe GMM distribution which advantageously allowsfor small differences from the canonical order.
Thez assignments are deterministically computed by or-dering the elements of b according to the permuta-tion defined by v.Given the conditional independencies of ourmodel, the posterior is proportional to:L?m=1[P (?m|?
)]?L?1?r=1[P (?r|?0, v0r)]?
P (?|?0, ?1)?N?i=1[P (?i|?)?
P (?i|?)?
P (vi|?)?
P (bi|?i)]?N?i=1NPi?j=1|xij |?k=1[P (sijk|?
)P (wijk|sijk, ?zij , ?i)]where L is the number of complexity levels, N thetotal problems in the training corpus, NPi the sizeof solution set for problem Pi, and |xij | the numberof words in solution xij .
A simplified posterior canbe obtained by integrating out the ?, ?, ?, and ?distributions which is proportional to:N?i=1L?m=1?
(Rmi +?m)?
(L?m=1Rmi +?m) ?L?1?r=1GMM0(?r|?0, v0r)?N?i=1L?1?r=1GMMr(vir|?r)?1?u=0?(Tu+?u)?
( 1?u=0Tu+?u)?L?m=1V?v=1?
(T 0m(v)+?v)?
(V?v=1T 0m(v)+?v) ?N?i=1V?v=1?
(T 1i (v)+?v)?
(V?v=1T 1i (v)+?v)where theR and T counts are defined similarly as inthe Expected Position model.We use collapsed Gibbs sampling to computesamples from this posterior.
The sampling sequence79Level 1free, space, hard, drive, sure, range, make, least, more, op-erate, than, point, less, cause, slowly, access, may, problemLevel 2use, can, media, try, center, signal, network, make, file, sure,when, case, with, change, this, setting, type, removeLevel 19system, file, this, restore, do, can, then, use, hard, issue,NUM, will, disk, start, step, above, run, cleanup, drive, xpLevel 20registry, restore, may, virus, use, bio, setting, scanreg, first,ensure, can, page, about, find, install, additional, we, utilityTable 3: Most likely words assigned to varying complex-ity levels by the permutations-based model.randomly initializes the hidden variables.
For a cho-sen solution set Si, the sampler draws NPi levels(bi), one at a time conditioned on the assignmentsto all other hidden variables of the model.
Thenthe inversion vector vi is created by sampling eachvij in turn.
At this point, the complexity level as-signments zi can be done deterministically given biand vi.
Then the words in each solution set aresampled one at a time.
For the dispersion parame-ters, ?, the normalization constant of the conjugateprior is not known.
We sample from the unnormal-ized GMM0 distribution using slice sampling.Other hyperparameters of the model are tuned us-ing development data.
The language model Dirichletconcentrations (?, ?)
are chosen to encourage spar-sity and ?
> 1 as in the position model.
We runthe Gibbs sampler for 5,000 iterations; the disper-sion parameters are resampled every 10 iterations.The last sample is used as a draw from the posterior.4.3 Model OutputIn this section we present examples of the complex-ity assignments created by our models.
Table 3shows the output of the permutations-based modelwith 20 levels.
Each row contains the highest prob-ability words in a single level (from the distribu-tion ?m).
For the sake of brevity, we only show thetwo least and most complex levels.
In general, weobserve more specialized, technical terms in higherlevels (e.g., restore, scanreg, registry) which onewould expect to correlate with complex solutions.Also note that higher levels contain uncertainty de-noting words (e.g., can, find, may) which again areindicative of increasing complexity.Using these complexity vocabularies, our modelsLow complexity1.0 Cable(s) of new external device are loose or power ca-bles are unplugged.
Ensure that all cables are properlyand securely connected and that pins in the cable orconnector are not bent down.1.0 Make sure your computer has at least 50MB of freehard drive space.
If your computer has less than50MB free, it may cause the computer to operate moreslowly.1.0 If the iPhone is in a protective case, remove it fromthe case.
If there is a protective film on the display,remove the film.Medium complexity5.7 Choose a lower video setting for your imported video.5.5 The file property is set to read-only.
To work aroundthis issue, remove the read-only property.
For moreinformation about file properties, see View the prop-erties for a file.5.1 The system is trying to start from a media device thatis not bootable.
Remove the media device from thedrive.High complexity10.0 If you are getting stopped at the CD-KEY or SerialNumber verification, verify you are entering your cor-rect number.
If you lost your number or key or it doesnot work, you will need to contact the developer of theprogram.
Computer Hope will not provide any userswith an alternate identification number.10.0 The network controller is defective.
Contact an autho-rized service provider.9.9 Network controller interrupt is shared with an expan-sion board.
Under the Computer Setup Advancedmenu, change the resource settings for the board.Table 4: Example solutions with low, medium and highexpected complexity values (position-based model with10 complexity levels).
The solutions come from variousproblem-solution sets in the training corpus.
Expectedcomplexity values are shown in the first column.can compute the expected complexity for any so-lution text, x.
This value is given by [?Lm=1m ?p(m|x)].
We estimate the second term, p(m|x),using a) the complexity level language models ?mand b) a prior over levels given by the overall fre-quency of different levels on the training data.
Ta-ble 4 presents examples of solution texts from ourtraining data and their expected complexity underthe position-model.
We find that the model is able todistinguish intuitively complex solutions from sim-pler ones.
Aside from measuring expected complex-ity in absolute terms, our models can also also or-der solutions in terms of relative complexity (seethe evaluation in Section 5) and assign a complex-ity value to a problem as a whole.80Low Complexity Problems- Computer appears locked up and will not turn off when thepower button is pressed.- A USB device, headphone, or microphone is not recognizedby the computer.- Computer will not respond to USB keyboard or mouse.High Complexity Problems- Game software and driver issues.- Incorrect, missing or stale visible networks.- I get an error message that says that there is not enough diskspace to publish the movie.
What can I do?- Power LED flashes Red four times, once every second, fol-lowed by two second pause, and computer beeps four times.Table 5: Least and most complex problems based on theexpected complexity of their solution set.
Problems areshown with complexity 1?3 (top) and 8?9 (bottom) usingthe position-based model.As mentioned earlier, our models only observe therelative ordering of solutions to individual problems;the relative complexity of two solutions from differ-ent problems is not known.
Nevertheless, the modelsare able to rate solutions on a global scale while ac-commodating problem-specific ordering sequences.Specifically, we can compute the expected complex-ity of the solution set for problem i, using the in-ferred distribution over levels ?i: ?Lm=1m ?
?im.Table 5 shows the complexity of different problemsas predicted by the position model (with 10 levels).As can be seen, easy problems are associated withaccessory components (e.g., mouse or keyboard),whereas complex problems are related to core hard-ware and operating system errors.5 Evaluation ExperimentsIn the previous section, we showed how our mod-els can assign an expected complexity value to a so-lution text or an entire problem.
Now, we presentevaluations based on model ability to order solutionsaccording to relative complexity.5.1 Solution Ordering TaskWe evaluated our models by presenting them witha randomly permuted set of solutions to a problemand examining the accuracy with which they reorderthem from least to most complex.
At first instance,it would be relatively straightforward to search forthe sequence of solutions which has high likelihoodunder the models.
Unfortunately, there are two prob-lems with this approach.
Firstly, the likelihood un-der our models is intractable to compute, so wewould need to adopt a simpler and less precise ap-proximation (such as the Hidden Markov Model dis-cussed below).
Secondly, when the solution set sizeis large, we cannot enumerate all permutations andneed to adopt an approximate search procedure.We opted for a discriminative ranking approachinstead which uses the generative models to computea rich set of features.
This choice allows us to simul-taneously obtain features tapping on to different as-pects learned by the models and to use well-definedobjective functions.
Below, we briefly describe thefeatures based on our generative models.
We alsopresent additional features used to create baselinesfor system comparison.Likelihood We created a Hidden Markov Modelbased on the sample from the posterior of our mod-els (for a similar HMM approximation of a Bayesianmodel see Elsner et al.
(2007)).
For our model, theHMM has L states, and each state sm correspondsto a complexity level dm.
We used the complex-ity language models ?m estimated from the poste-rior as the emission probability distribution for thecorresponding states.
The transition probabilities ofthe HMM were computed based on the complexitylevel assignments for the training solution sequencesin our posterior sample.
The probability of transi-tioning to state sj from state si, p(sj |si), is the con-ditional probability p(dj |di) computed as c(di,dj)c(di) ,where c(di, dj) is the number of times the complex-ity level dj is assigned to a solution immediately fol-lowing a solution which was given complexity di.c(di) is the number of times complexity level di isassigned overall in the training corpus.
We performLaplace smoothing to avoid zero probability transi-tions between states:p(sj |si) =c(di, dj) + 1c(di) + L(4)This HMM formulation allows us to use efficient dy-namic programming to compute the likelihood of asequence of solutions.Given a solution set, we compute an ordering asfollows.
We enumerate all orderings for sets withsize less than 6, and select the sequence with thehighest likelihood.
For larger sizes, we use a sim-ulated annealing search procedure which swaps twoadjacent solutions in each step.
The temperature wasset to 50 initially and gradually reduced to 1.
These81values were set using minimal tuning on the devel-opment data.
After estimating the most likely se-quence for a solution set, we used the predicted rankof each solution as a feature in our discriminativemodel.Expected Complexity As mentioned earlier, wecomputed the expected complexity of a solution xas [?Lm=1m ?
p(m|x)], where the second term wasestimated using a complexity level specific languagemodel ?m and a uniform prior over levels on thetest set.
As additional features, we used the solu-tion?s perplexity under each ?m, and under each ofthe technical topics ?i, and also the most likely levelfor the text argmaxm p(m|x).
Finally, we includedfeatures for each word in the training data.
The fea-ture value is the word?s expected level multiplied bythe probability of the word in the solution text.Length We also investigated whether solutionlength is a predictor of complexity (e.g., simple so-lutions may vary in length and amount of detail fromcomplex ones).
We devised three features based onthe number of sentences (within a solution), words,and average sentence length.Syntax/Semantics Another related class of fea-tures estimates solution complexity based on sen-tence structure and meaning.
We obtained eightsyntactic features based on the number of nouns,verbs, adjectives and adverbs, prepositions, pro-nouns, wh-adverbs, modals, and punctuation.
Otherfeatures compute the average and maximum depthof constituent parse trees.
The part-of-speech tagsand parse trees were obtained using the StanfordCoreNLP toolkit (Manning et al., 2014).
In addi-tion, we computed 10 semantic features using Word-Net (Miller, 1995).
They are the average num-ber of senses for each category (noun, verb, adjec-tive/adverb), and the maximum number of senses forthe same three classes.
We also include the aver-age and maximum lengths of the path to the root ofthe hypernym tree for nouns and verbs.
This classof features roughly approximates the indicators typ-ically used in predicting text readability (Schwarmand Ostendorf, 2005; McNamara et al., 2014).5.2 Experimental SetupWe performed 10-fold cross-validation.
We trainedthe ranking model on 240 problem-solution sets;30 sets were reserved for development and 30 fortesting (in each fold).
The most frequent 20 words ineach training set were filtered as stopwords.
The de-velopment data was used to tune the parameters andhyperparameters of the models and the number ofcomplexity levels.
We experimented with ranges [5?20] and found that the best number of levels was 10for the position model and 20 for the permutation-based model, respectively.
For the expected posi-tion model, positions were normalized before train-ing.
Let solution xir denote the rth solution in thesolution set for problem Pi, where 1 ?
r ?
NPi .We normalize r to a value between 0 and 1 usinga min-max method: r?
= r?1NPi?1 .
Then the [0?1]range is divided into k bins.
The identity of the bincontaining r?
is taken as the normalized position, r.We tuned k experimentally during development andfound that k = 3 performed best.For our ordering experiments we used Joachims?
(2006) SVMRank package for training and testing.During training, the classifier learns to minimize thenumber of swapped pairs of solutions over the train-ing data.
We used a linear kernel and the regulariza-tion parameter was tuned using grid search on thedevelopment data of each fold.
We evaluate howwell the model?s output agrees with gold-standardordering using Kendall?s ?
.5.3 ResultsTable 6 summarizes our results (average Kendall?s ?across folds).
We present the results of the dis-criminative ranker when using a single feature classbased on likelihood and expected complexity (Posi-tion, Permutation), length, and syntactico-semanticfeatures (SynSem), and their combinations (denotedvia +).
We also report the performance of a base-line which computes a random permutation for eachsolution set (Random; results are averaged over fiveruns).
We show results for all solution sets (All) andbroken down into different set sizes (e.g., 2?3, 4?5).As can be seen, the expected position model ob-tains an overall ?
of 0.30 and the permutation modelof 0.26.
These values lie in the range of humanannotator agreement with the FAQ order (see Sec-tion 3.2).
In addition, we find that the modelsperform consistently across solution set sizes, witheven higher correlations on longer sequences whereour methods are likely to be more useful.
Posi-tion and Permutation outperform Random rankingsand a model based solely on Length features.
The82Solution set sizesModel 2?3 4?5 > 5 All(133) (81) (86) (300)Random -0.002 0.027 -0.006 0.004Length -0.052 -0.066 -0.028 -0.002SynSem 0.167 0.109 0.149 0.146Length+SynSem 0.122 0.109 0.158 0.129Position 0.288 0.242 0.381 0.302+Length 0.283 0.251 0.363 0.297+SynSem 0.343 0.263 0.367 0.328+SynSem+Length 0.348 0.263 0.368 0.331Permutation 0.253 0.206 0.341 0.265+Length 0.213 0.229 0.356 0.258+SynSem 0.268 0.263 0.353 0.291+SynSem+Length 0.253 0.254 0.354 0.282Table 6: Kendall?s ?
values on the solution reorder-ing task using 10-fold cross-validation and SVM rankingmodels with different features.
The results are brokendown by solution set size (the number of sets per size isshown within parentheses).
Boldface indicates the bestperforming model for each set size.SynSem features which measure the complexity ofwriting in the text are somewhat better but still in-ferior compared to Position and Permutation.
Us-ing a paired Wilcoxon signed-rank test, we com-pared the ?
values obtained by the different mod-els.
The Position and Permutation performed sig-nificantly better (p < 0.05) compared to Random,Length and SynSem baselines.
However, ?
differ-ences between Position and Permutation are not sta-tistically significant.
With regard to feature com-binations, we observe that both models yield bet-ter performance when combined with Length orSynSem.
The Position model improves with theaddition of both Length and SynSem, whereas thePermutation model combines best with SynSemfeatures.
The Position+SynSem+Length modelis significantly better than Permutation (p < 0.05)but not Permutation+SynSem+Length or Positionalone (again under the Wilcoxon test).These results suggest that the solution orderingtask is challenging with several factors influencinghow a solution is perceived: the words used andtheir meaning, the writing style of the solution, andthe amount of detail present in it.
Our data comesfrom the FAQs produced by computer and operat-ing system manufacturers and other well-managedwebsites.
As a result, the text in the FAQ solutionsis of high quality.
However, the same is not trueModel Rank 1 Rank n BothRandom 15.3 14.6 3.4Length 18.1 16.3 6.8SynSem 19.8 19.8 4.3SynSem+Length 20.6 25.0 5.1Position 31.0 37.0 13.7+Length 36.2 36.2 18.9+SynSem 34.4 35.3 16.3+SynSem+Length 36.2 35.3 17.2Permutation 28.4 37.0 12.0+Length 30.1 37.0 15.5+synsem 32.7 37.0 13.7+SynSem+Length 32.7 37.9 13.7Table 7: Model accuracy at predicting the easiest solutioncorrectly (Rank 1), the most difficult one (Rank n), orboth.
Bold face indicates the best performing model foreach rank.for community-generated solution texts on discus-sion forums.
In the latter case, we conjecture thatthe style of writing is likely to play a bigger role inhow users perceive complexity.
We thus expect thatthe benefit of adding Length and SynSem featureswill become stronger when we apply our models totexts from online forums.We also computed how accurately the modelsidentify the least and most complex solutions.
Forsolution sets of size 5 and above (so that the task isnon-trivial) we computed the number of times therank one solution given by the models was also theeasiest according to the FAQ gold-standard.
Like-wise, we also computed how often the models cor-rectly predict the most complex solution.
With Ran-dom rankings, the easiest and most difficult solu-tions are predicted correctly 15% of the time.
Get-ting both correct for a single problem happens only3% of the time.
The Position+Length model overallperforms best, identifying the easiest and most diffi-cult solution 36% of the time.
Both types of solutionare identified correctly 18% of the time.
Interest-ingly, the generative models are better at predictingthe most difficult solution (35?37%) compared tothe easiest one (28?36%).
One reason for this couldbe that there are multiple easy solutions to try outbut the most difficult one is probably more uniqueand so easier to identify.Overall, we observe that the two generative mod-els perform comparably, with Position having aslight lead over Permutation.
A key difference be-83tween the models is that during training Permutationobserves the full ordering of solutions while Posi-tion observes solutions coming from a few normal-ized position bins.
Also note that in the Permuta-tion model, multiple solutions with the same com-plexity level are grouped together in a solution set.This property of the model is advantageous for or-dering as solutions with similar complexity shouldbe placed adjacent to each other.
At the same time, iflevels 1 and 2 are flipped in the permutation sampledfrom the GMM, then any solution with complexitylevel 1 will be ordered after the solution with com-plexity 2.
The Position model on the other hand,contains no special facility for grouping solutionswith the same complexity.
In sum, Position canmore flexibly assign complexity levels to individualsolutions.6 ConclusionThis work contains a first proposal to organize andnavigate crowd-generated troubleshooting data ac-cording to the complexity of the troubleshooting ac-tion.
We showed that users perceive and agree on thecomplexity of alternative suggestions, and presentedBayesian generative models of the troubleshootingdata which can sort solutions by complexity with aperformance close to human agreement on the task.Our results suggest that search and summariza-tion tools for troubleshooting forum archives can begreatly improved by automatically predicting andusing the complexity of the posted solutions.
Itshould also be possible to build broad coverage au-tomated troubleshooting systems by bootstrappingfrom conversations in discussion forums.
In the fu-ture, we plan to deploy our models in several taskssuch as user authority prediction, expert interven-tion, and thread analysis.
Furthermore, we aim tospecialize our models to include category-specificcomplexity levels and also explore options for per-sonalizing rankings for individual users based ontheir knowledge of a topic and the history of theirtroubleshooting actions.AcknowledgementsWe would like to thank the editor and the anony-mous reviewers for their valuable feedback on anearlier draft of this paper.
We are also thankful tomembers of the Probabilistic Models of Languagereading group at the University of Edinburgh fortheir many suggestions and helpful discussions.
Thefirst author was supported by a Newton InternationalFellowship (NF120479) from the Royal Society andthe British Academy.ReferencesR.
Barzilay and M. Lapata.
2008.
Modeling local coher-ence: An entity-based approach.
Computational Lin-guistics, 34(1):1?34.R.
Barzilay and L. Lee.
2004.
Catching the drift: Proba-bilistic content models, with applications to generationand summarization.
In Proceedings of NAACL-HLT,pages 113?120.D.
Bollegala, N. Okazaki, and M. Ishizuka.
2006.A bottom-up approach to sentence ordering formulti-document summarization.
In Proceedings ofCOLING-ACL, pages 385?392.S.
R. K. Branavan, H. Chen, L. S. Zettlemoyer, andR.
Barzilay.
2009.
Reinforcement learning for map-ping instructions to actions.
In Proceedings of ACL-IJCNLP, pages 82?90.S.
R. K. Branavan, D. Silver, and R. Barzilay.
2011.Learning to win by reading manuals in a Monte-Carloframework.
In Proceedings of ACL-HLT, pages 268?277.H.
Chen, S. R. K. Branavan, R. Barzilay, and D. R.Karger.
2009a.
Content modeling using latent per-mutations.
Journal of Artificial Intelligence Research,36(1):129?163.H.
Chen, S.R.K.
Branavan, R. Barzilay, and D.R.
Karger.2009b.
Global models of document structure usinglatent permutations.
In Proceedings of NAACL-HLT,pages 371?379.J.
Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.2009.
Reading to learn: Constructing features fromsemantic abstracts.
In Proceedings of EMNLP, pages958?967.M.
Elsner, J. Austerweil, and E. Charniak.
2007.
A uni-fied local and global model for discourse coherence.In Proceedings of NAACL-HLT, pages 436?443.M.
A. Fligner and J. S. Verducci.
1986.
Distance-basedranking models.
Journal of the Royal Statistical Soci-ety, Series B, pages 359?369.D.
Heckerman, J. S. Breese, and K. Rommelse.
1995.Decision-theoretic troubleshooting.
Communicationsof the ACM, 38(3):49?57.T.
Joachims.
2006.
Training linear SVMs in linear time.In Proceedings of KDD, pages 217?226.S.
Kim, L. Cavedon, and T. Baldwin.
2010.
Classifyingdialogue acts in one-on-one live chats.
In Proceedingsof EMNLP, pages 862?871.84M.
Lapata.
2003.
Probabilistic text structuring: Experi-ments with sentence ordering.
In Proceedings of ACL,pages 545?552.M.
Lapata.
2006.
Automatic evaluation of informationordering.
Computational Linguistics, 32(4):471?484.M.
Lui and T. Baldwin.
2009.
Classifying user forumparticipants: Separating the gurus from the hacks, andother tales of the internet.
In Proceedings of the 2010Australasian Language Technology Workshop, pages49?57.N.
Madnani, R. Passonneau, N. Ayan, J. Conroy, B. Dorr,J.
Klavans, D. O?Leary, and J. Schlesinger.
2007.Measuring variability in sentence ordering for newssummarization.
In Proceedings of the Eleventh Eu-ropean Workshop on Natural Language Generation.C.
L. Mallows.
1957.
Non-null ranking models.
i.Biometrika, 44(1/2):pp.
114?130.C.
D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J.Bethard, and D. McClosky.
2014.
The StanfordCoreNLP natural language processing toolkit.
In Pro-ceedings of ACL: System Demonstrations, pages 55?60.D.
S. McNamara, A. C. Graesser, P. M. McCarthy, andZ.
Cai.
2014.
Automated evaluation of text and dis-course with Coh-Metrix.
Cambridge University Press.G.
A. Miller.
1995.
WordNet: a lexical database forEnglish.
Communication of the ACM, 38(11):39?41.S.
Schwarm and M. Ostendorf.
2005.
Reading level as-sessment using support vector machines and statisticallanguage models.
In Proceedings of ACL, pages 523?530.A.
Vogel and D. Jurafsky.
2010.
Learning to follownavigational directions.
In Proceedings of ACL, pages806?814.L.
Wang, M. Lui, S. Kim, J. Nivre, and T. Baldwin.2011.
Predicting thread discourse structure over tech-nical web forums.
In Proceedings of EMNLP, pages13?25.8586
