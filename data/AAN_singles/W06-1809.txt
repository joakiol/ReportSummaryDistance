Incorporating User Models in Question Answering to Improve ReadabilitySilvia Quarteroni and Suresh ManandharDepartment of Computer ScienceUniversity of YorkYork YO10 5DDUnited Kingdom{silvia,suresh}@cs.york.ac.ukAbstractMost question answering and informationretrieval systems are insensitive to differ-ent users?
needs and preferences, as wellas their reading level.
In (Quarteroni andManandhar, 2006), we introduce a hybridQA-IR system based on a a user model.In this paper we focus on how the systemfilters and re-ranks the search engine re-sults for a query according to their readingdifficulty, providing user-tailored answers.Keywords: question answering, informa-tion retrieval, user modelling, readability.1 IntroductionQuestion answering (QA) systems are informationretrieval systems accepting queries in natural lan-guage and returning the results in the form of sen-tences (or paragraphs, or phrases).
They movebeyond standard information retrieval (IR) whereresults are presented in the form of a ranked listof query-relevant documents.
Such a finer answerpresentation is possible thanks to the applicationof computational linguistics techniques in orderto filter irrelevant documents, and of a consistentamount of question pre-processing and result post-processing.However, in most state-of-the-art QA systemsthe output remains independent of the questioner?scharacteristics, goals and needs; in other words,there is a lack of user modelling.
For instance, anelementary school child and a University historystudent would get the same answer to the question:?When did the Middle Ages begin?
?.Secondly, most QA systems focus on factoidquestions, i.e.
questions concerning people, dates,numerical quantities etc., which can generally beanswered by a short sentence or phrase (Kwok etal., 2001).
The mainstream approach to QA evalu-ation, represented by TREC-QA campaigns1, haslong fostered the criterion that a ?good?
systemis one that returns the ?correct?
answer in theshortest possible formulation.
Although recent ef-forts in TREC 2003 and 2004 (Voorhees, 2003;Voorhees, 2004) denoted an interest towards listquestions and definitional (or ?other?)
questions,we believe that there has not been enough inter-est towards non-factoid answers.
The real issue is?realizing?
that the answer to a question is some-times too complex to be formulated and evaluatedas a factoid: some queries have multiple, com-plex or controversial answers (take e.g.
?Whatwere the causes of World War II??).
In such sit-uations, returning a short paragraph or text snip-pet is more appropriate than exact answer spot-ting.
For instance, the answer to ?What is ametaphor??
may be better understood with the in-clusion of examples.
This viewpoint is supportedby recent user behaviour studies which showedthat even in the case of factoid-based QA systems,the most eligible result format consisted in a para-graph where the sentence containing the answerwas highlighted (Lin et al, 2003).The issue of non-factoids is related to the usermodelling problem: while factoid answers do notnecessarily require to be contextualized within theuser?s knowledge and viewpoint, the need is muchstronger in the case of definitions, explanationsand descriptions.
This is mentioned in the TREC2003 report (Voorhees, 2003) when discussing theevaluation of definitional questions: however, theissue is expeditiously solved by assuming a fixeduser profile (the ?average news reader?
).We are currently developing an adaptive sys-tem which adjusts its output with respect to a usermodel.
The system can be seen as an enhanced IRsystem which adapts both the content and presen-tation of the final results, improving their quality.1http://trec.nist.gov50 KRAQ06In this paper, we show that QA systems can benefitfrom the contribution of user models, and explainhow these can be used to filter the information pre-sented as an answer based on readability.
Eventu-ally, we describe preliminary results obtained viaan evaluation framework inspired by user-centeredsearch engine evaluation.2 System ArchitectureThe high-level architecture as represented in Fig-ure 1 shows the basic components of the system,the QA module and the user model.QUESTIONPROCESSINGDOCUMENTRETRIEVALANSWEREXTRACTIONQuestionAnswerQA MODULEUSER MODELWebpageWebpagesAge RangeReadingLevelFigure 1: High level system architectureThe QA module, described in the following sec-tion, is organized according to the three-tier parti-tion underlying most state-of-the-art systems: 1)question processing, 2) document retrieval, 3) an-swer generation.
The module makes use of a websearch engine for document retrieval and consultsthe user model to obtain the criteria to filter andre-rank the search engine results and to eventuallypresent them appropriately to the user.2.1 User modelDepending on the application of interest, the usermodel (UM) can be designed to suit the informa-tion needs of the QA module in different ways.Our current application, YourQA2, is a learning-oriented system to help students find informationon the Web for their assignments.
Our UM con-sists of the user?s:?
age range, a ?
{7?
11, 11?
16, adult}?
reading level, r ?
{poor,medium, good}?
webpages of interest/bookmarks, wThe age range parameter has been chosen tomatch the partition between primary school, con-temporary school and higher education age in2http://www.cs.york.ac.uk/aig/aquaBritain; our reading level parameter takes threevalues which ideally (but not necessarily) corre-spond to the three age ranges and may be furtherrefined in the future for more fine-grained mod-elling.Analogies can be found with the SeAn (Ardis-sono et al, 2001), and SiteIF (Magnini and Strap-parava, 2001) news recommender systems, whereinformation such as age and browsing history,resp.
are part of the UM.
More generally, ourapproach is similar to that of personalized searchsystems (Teevan et al, 2005; Pitkow et al, 2002),which construct UMs based on the user?s docu-ments and webpages.In our system, UM information is explicitly col-lected from the user; while age and reading levelare self-assessed, the user?s interests are extractedfrom the document set w using a keyphrase ex-tractor (see further for details).
Eventually, a di-alogue framework with a history component willcontribute to the construction and update of theuser model in a less intruding and thus more user-friendly way.
In this paper we focus on how toadapt search result presentation using the readinglevel parameter: age and webpages will not be dis-cussed.2.2 Related workNon-factoids and user modelling As men-tioned above, the TREC-QA evaluation campaign,to which the vast majority of current QA systemsabide, mainly approaches factoid-based answers.To our knowledge, our system is among the first toaddress the need for a different approach to non-factoid answers.
The structure of our QA compo-nent reflects the typical structure of a web-basedQA system in its three-tier composition.
Analo-gies in this can be found for instance in MUL-DER (Kwok et al, 2001), which is organized ac-cording to a question processing/answer extrac-tion/passage ranking pipeline.
However, a signifi-cant aspect of novelty in our architecture is that theQA component is supported by the user model.Additionally, we have changed the relative im-portance of the different tiers: while we drasticallyreduce linguistic processing during question pro-cessing and answer generation, we give more re-lief to the post-retrieval phase and to the role ofthe UM.
Having removed the need for fine-grainedanswer spotting, the emphasis is shifted towardsfinding closely connected sentences that are highly51 KRAQ06relevant to answer the query.Readability Within computational linguistics,several applications have been designed to addressthe needs of users with low reading skills.
Thecomputational approach to textual adaptation iscommonly based on natural language generation:the process ?translate?
a difficult text into a syntac-tically and lexically simpler version.
In the case ofPSET (Carroll et al, 1999) for instance, a tagger, amorphological analyzer and generator and a parserare used to reformulate newspaper text for usersaffected by aphasia.
Another interesting researchis Inui et al?s lexical and syntactical paraphrasingsystem for deaf students (Inui et al, 2003).
In thissystem, the judgment of experts (teachers) is usedto learn selection rules for paraphrases acquiredusing various methods (statistical, manual, etc.
).In the SKILLSUM project (Williams and Reiter,2005), used to generate literacy test reports, a setof choices regarding output (cue phrases, order-ing and punctuation) are taken by a micro-plannerbased on a set of rules.Our approach is conceptually different from theabove: exploiting the wealth of information avail-able in the context of a Web-based QA system, wecan afford to choose among the documents avail-able on a given subject those which best suit ourreadability requirements.
This is possible thanksto the versatility of language modelling, which al-lows us to tailor the readability estimation of doc-uments to any kind of user profile in a dynamicmanner, as explained in section 3.2.3.3 QA ModuleIn this section we discuss the information flowamong the subcomponents of the QA module (seeFigure 2 for a representative diagram) and focuson reading level estimation and document filter-ing.
For further details on the implementation ofthe QA module, see (Quarteroni and Manandhar,2006).3.1 Question ProcessingThe first step performed by YourQA is query ex-pansion: additional queries are created replacingquestion terms with synonyms using WordNet3.3http://wordnet.princeton.eduQuestionQUERYEXPANSIONDOCUMENTRETRIEVALKEYPHRASEEXTRACTIONESTIMATIONOF READINGLEVELSCLUSTERINGLanguageModelsUM-BASEDFILTERINGSEMANTICSIMILARITYRANKINGUser ModelReadingLevelRankedAnswerCandidatesFigure 2: Diagram of the QA module3.2 Retrieval and Result Processing3.2.1 Document retrievalWe use Google4 to retrieve the top 20 docu-ments returned for each of the queries issued fromthe query expansion phase.
The subsequent stepswill progressively narrow the parts of these docu-ments where relevant information is located.3.2.2 Keyphrase extractionKeyphrase extraction is useful in two ways:first, it produces features to group the retrieveddocuments thematically during the clusteringphase, and thus enables to present results bygroups.
Secondly, when the document parame-ter (w) of the UM is active, matches are soughtbetween the keyphrases extracted from the docu-ments and those extracted from the user?s set ofinteresting documents; thus it is possible to pri-oritize results which are more compatible withhis/her interests.Hence, once the documents are retrieved, weextract their keyphrases using Kea (Witten et al,1999), an extractor based on Na?ve Bayes classifi-cation.
Kea first splits each document into phrasesand then takes short subsequences of these initialphrases as candidate keyphrases.
Two attributesare used to classify a phrase p as a keyphrase ora non-keyphrase: its TF?
IDF score within theset of retrieved documents and the index of p?sfirst appearance in the document.
Kea outputs aranked list of phrases, among which we select thetop three as keyphrases for each of our documents.4http://www.google.com52 KRAQ063.2.3 Estimation of reading levelsIn order to adjust search result presentation tothe user?s reading ability, we estimate the read-ing difficulty of each retrieved document using theSmoothed Unigram Model, a variation of a Multi-nomial Bayes classifier (Collins-Thompson andCallan, 2004).
Whereas other popular approachessuch as Flesch-Kincaid (Kincaid et al, 1975) arebased on sentence length, the language modellingapproach accounts especially for lexical informa-tion.
The latter has been found to be more effectiveas the former when approaching the reading levelof subjects in primary and secondary school age(Collins-Thompson and Callan, 2004).
Moreover,it is more applicable than length-based approachfor Web documents, where sentences are typicallyshort regardless of the complexity of the text.The language modelling approach proceeds intwo phases: in the training phase, given a range ofreading levels, a set of representative documentsis collected for each reading level.
A unigram lan-guage model lms is then built for each set s; themodel consists of a list of the word stems appear-ing in the training documents with their individualprobabilities.
Textual readability is not modelledat a conceptual level: thus complex concepts ex-plained in simple words might be classified as suit-able even for a poor reading level; However wehave observed that in most Web documents lexi-cal, syntactic and conceptual complexity are usu-ally consistent within documents, hence it makessense to apply a reasoning-free technique with-out impairing readability estimation.
Our unigramlanguage models account for the following read-ing levels:1) poor, i.e.
suitable for ages 7 ?
11;2) medium, suitable for ages 11?16;3) good, suitable for adults.This partition in three groups has been chosen tosuit the training data available for our school appli-cation, which consists of about 180 HTML pages(mostly from the ?BBC schools?5, ?Think En-ergy?6, ?Cassini Huygens resource for schools?7and ?Magic Keys storybooks?8 websites), explic-itly annotated by the publishers according to thereading levels above.In the test phase, given an unclassified docu-5http://bbc.co.uk/schools6http://www.think-energy.com7http://www.pparc.ac.uk/Ed/ch/Home.htm8http://www.magickeys.com/books/ment D, the estimated reading level of D is thelanguage model lmi maximizing the likelihoodL(lmi|D) that D has been generated by lmi.
Suchlikelihood is estimated using the formula:L(lmi|D) =?w?DC(w,D) ?
log(P (w|lmi))where w is a word in the document, C(w, d) rep-resents the number of occurrences of w in D andP (w|lmi) is the probability that w occurs in lmi(approached by its frequency).An advantage of language modelling is itsportability, since it is quite quick to create wordstem/frequency histograms on the fly.
This impliesthat models can be produced to represent morefine-grained reading levels as well as the specificrequirements of a single user: the only necessaryinformation are sets of training documents repre-senting each level to be modelled.3.2.4 ClusteringAs an indicator of inter-document relatedness,we use document clustering (Steinbach et al,2000) to group them using both their estimatedreading difficulty and their topic (i.e.
theirkeyphrases).
In particular we use a hierarchi-cal algorithm, Cobweb (implemented using theWEKA suite of tools (Witten and Frank, 2000) asit produces a cluster tree which is visually sim-ple to analyse: each leaf corresponds to one doc-ument, and sibling leaves denote documents thatare strongly related both in topic and in readingdifficulty.
Figure 3 illustrates an example clus-ter tree for the the query: ?Who painted the Sis-tine chapel??.
Leaf labels represent documentkeyphrases extracted by Kea for the correspondingdocuments and ovals represent non-terminal nodesin the cluster tree (these are labelled using the mostcommon keyphrases in their underlying leaves).3.3 Answer ExtractionThe purpose of answer extraction is to present themost interesting excerpts of the retrieved docu-ments according to both the user?s query topicsand reading level.
This process, presented in sec-tions 3.3.1 ?
3.3.4, follows the diagram in Figure2: we use the UM to filter the clustered documents,then compute the similarity between the questionand the filtered document passages in order to re-turn the best ones in a ranked list.53 KRAQ060 chapel1 ceiling 4 michelangelo art7 chapelceilingpaintpope 2paintedceilingfrescoes 3artmichelangelopaint 5artmichelangelodownload 68 chapel michelangelo11 chapelmichelangelopaintingschapel 9chapelmichelangelochrist 1012 chapelred_ballchapelvaticano 15chapelsistine_chapelwalls 13frescocappella_sistinachapel 14Figure 3: Cluster tree for ?Who painted the Sistinechapel??.
Leaf 3 and the leaves grouped undernodes 8 and 12 represent documents with an esti-mated good reading level; leaf 15 and the leavesunderlying node 4 have a medium reading level;leaf 2 represents a poor reading level document.3.3.1 UM-based filteringThe documents in the cluster tree are filtered ac-cording to the UM reading level, r: only thosecompatible with the user?s reading ability are re-tained for further analysis.
However, if the num-ber of retained documents does not exceed a giventhreshold, we accept in our candidate set part ofthe documents having the next lowest readabilityin case r ?
{good,medium} or a medium read-ability in case r = poor.3.3.2 Semantic similarityWithin each of the documents retained, we seekfor the sentences which are semantically most rel-evant to the query.
Given a sentence p and thequery q, we represent them as two sets of wordsP = {pw1, .
.
.
, pwm} and Q = {qw1, .
.
.
, qwn}.The semantic distance from p to q is then:distq(p) =?1?i?m minj [d(pwi, qwj)]where d(pwi, qwj) represents the Jiang-Conrathword-level distance between pwi and qwj (Jiangand Conrath, 1997), based on WordNet 2.0.
Theintuition is that for each question word, we find theword in the candidate answer sentence which min-imizes the word-level distance and then we com-pute the sum of such minima.3.3.3 Passage and cluster rankingFor a given document, we can thus isolate a sen-tence s minimizing the distance to the query.
Thepassage P , i.e.
a window of up to 5 sentences cen-tered on s, will be a candidate result.
We assignto such passage a score equal to the similarity of sto the query; in turn, the score of P is used as thescore of the document containing it.
We also de-fine a ranking function for clusters, which allowsto order them according to the maximal score oftheir component documents.
Passages from thehighest ranking cluster will be presented first tothe user, in decreasing order of score, followed bythe passages from lower ranking clusters.3.3.4 Answer presentationTo present our answers, we fix a threshold forthe number of results to be returned following theranking exposed above.
Each result consists ofa title and document passage where the sentencewhich best answers the query is highlighted; theURL of the original document is also available forloading if the user finds the passage interesting andwants to read more.4 ResultsWe report the results of running our system ona range of queries, which include factoid/simple,complex and controversial questions9.4.1 Simple answerAs an example of a simple query, we present theresults for: ?Who painted the Sistine Chapel?
?,the system returned the following passages:?UMgood: ?Sistine Chapel (sis-teen).
A chapel adjoin-ing Saint Peter?s Basilica, noted for the frescoes of biblicalsubject painted by Michelangelo on its walls and ceilings.?
?UMmed: ?In all Michelangelo painted more than 300different figures on the Sistine Chapel ceiling.?
?UMpoor: ?My name is Jacopo L?Indaco and I wasan assistant to Michelangelo when he painted the SistineChapel.
?To obtain the above answers the system was run 3times with different values for the reading level pa-rameter in the UM, as defined in 3.2.3.
As we cansee, in all cases the correct information is presentalthough not always explicitly, as in the first twocases.
This is because our current semantic sim-ilarity metric only operates at word level.
In thisexample, all sentences containing ?painted?, ?Sis-tine?
and ?Chapel?
obtain a distance of 0 to thequery, regardless of their formulation.
Also no-tice how the difference in language complexity isclearly discernible in the different answers.4.2 Complex answerWe illustrate the results of our system with thequery ?Definition of metaphor?, which relates toa difficult concept.
Our top results, highlighted in9Notice that this partition is not to be interpreted as amethodological division, as we currently approach complexand controversial answers the same way.54 KRAQ06boldface within their containing passages, are:?UMgood: ?Definition of Metaphor.
Metaphor is a pro-cess by which we conceive "one thing in terms of another,and its primary function is understanding" (Lakoff andJohnson, Metaphors We Live By 36).
That is, we often do(and must) talk about things that do not physically exist, thatwe cannot reach out and touch or take apart with our handsto see what is inside and how it works: love, time, anger,life.
[.
.
. ]
We make them tangible by, in a process of thought,applying models of physical things to them: love is falling;argument is war.
?
?UMmed: ?Definition: A metaphor is a figure of speechthat compares unlike things by saying that one thing is theother.
Often metaphors are simple comparisons, but they canbe extended so that different aspects of the things comparedare treated separately.
?
?UMpoor: we did not find any document exactly match-ing the reading level, we thus returned the results forUMmed(see above).As before, there is a clear difference in the lan-guage complexity of the two answers.4.3 Controversial answerWe illustrate this case using the query ?Who wrotethe Iliad?
?, which is a subject of debate: the exis-tence of a single author, as well as the historicityof the poet Homer are under discussion.
These areamong our top results:?UMgood: ?Most Classicists would agree that, whetherthere was ever such a composer as "Homer" or not, theHomeric poems are the product of an oral tradition, agenerations-old technique that was the collective inheritanceof many singer-poets, aoidoi.
[.
.
. ]
Could the Iliad andOdyssey have been oral-formulaic poems, composed on thespot by the poet using a collection of memorized traditionalverses and phases??
?UMmed: ?No reliable ancient evidence for Homer ?earliest traditions involve conjecture (e.g.
conflicting claimsto be his place of origin) and legend (e.g.
Homer as sonof river-god).
General ancient assumption that same poetwrote Iliad and Odyssey (and possibly other poems) ques-tioned by many modern scholars: differences explained bi-ographically in ancient world (e g wrote Od.
in old age);but similarities could be due to imitation.?
?UMpoor: ?Homer wrote The Iliad and The Odyssey (atleast, supposedly a blind bard named "Homer" did).
?In this case we can see how the problem of attri-bution of the Iliad is made clearly visible: in thethree results, document passages provide a contextwhich helps to explain such controversy at differ-ent levels of difficulty.5 Evaluation5.1 MethodologyOur system is not a QA system in the strictsense, as it does not single out one correct an-swer phrase.
The key objective is an improved sat-isfaction of the user towards its adaptive results,which are hopefully more suitable to his read-ing level.
A user-centred evaluation methodologythat assesses how the system meets individual in-formation needs is therefore more appropriate forYourQA than TREC-QA metrics.We draw our evaluation guidelines from (Su,2003), which proposes a comprehensive searchengine evaluation model.
We define the followingmetrics (see Table 1):1.
Relevance:?
strict precision (P1): the ratio betweenthe number of results rated as relevantand all the returned results,?
loose precision (P2): the ratio betweenthe number of results rated as relevantor partially relevant and all the returnedresults.2.
User satisfaction: a 7-point Likert scale10 isused to assess satisfaction with:?
loose precision of results (S1),?
query success (S2).3.
Reading level accuracy (Ar).
This metricwas not present in (Su, 2003) and has beenintroduced to assess the reading level estima-tion.
Given the set R of results returned bythe system for a reading level r, it is the ratiobetween the number of documents ?
R ratedby the users as suitable for r and |R|.
Wecompute Ar for each reading level.4.
Overall utility (U ): the search session as awhole is assessed via a 7-point Likert scale.We have discarded some of the metrics proposedby (Su, 2003) when they appeared as linked totechnical aspects of search engines (e.g.
connec-tivity), and when response time was concerned asat the present stage this has not been considered10This measure ?
ranging from 1= ?extremely unsatisfac-tory?
to 7=?extremely satisfactory?
?
is particularly suitableto assess the degree to which the system meets the user?ssearch needs.
It was reported in (Su, 1991) as the best sin-gle measure for information retrieval among 20 tested.55 KRAQ06an issue.
Also, we exclude metrics relating to theuser interface which are not relevant for this study.Metric field descriptionRelevance P1 strict precisionP2 loose precisionSatisfaction S1 with loose precisionS2 with query successAccuracy Ag good reading levelAm medium reading levelAp poor reading levelUtility U overall sessionTable 1: Summary of evaluation metrics5.2 Evaluation resultsWe performed our evaluation by running 24queries (partly reported in Table 3) on both Googleand YourQA11.
The results ?
i.e.
snippets fromthe Google result page and passages returned byYourQA ?
were given to 20 evaluators.
Thesewere aged between 16 and 52, all having a self-assessed good or medium English reading level.They came from various backgrounds (Universitystudents/graduates, professionals, high school)and mother-tongues.
Evaluators filled in a ques-tionnaire assessing the relevance of each passage,the success and result readability of the singlequeries, and the overall utility of the system; val-ues were thus computed for the metrics in Table 1.P1 P2 S1 S2 UGoogle 0,39 0,63 4,70 4,61 4,59YourQA 0,51 0,79 5,39 5,39 5,57Table 2: Evaluation results5.2.1 RelevanceThe precision results (see Table 2) for the wholesearch session were computed by averaging thevalues obtained for the 20 queries.
Although quiteclose, they show a 10-15% difference in favour ofthe YourQA system for both strict precision (P1)and loose precision (P2).
This suggests that thecoarse semantic processing applied and the visu-alisation of the context contribute to the creationof more relevant passages.11To make the two systems more comparable, we turnedoff query expansion and only submitted the original questionsentence5.2.2 User satisfactionAfter each query, we asked evaluators the fol-lowing questions: ?How would you rate the ratioof relevant/partly relevant results returned??
(as-sessing S1) and ?How would you rate the successof this search??
(assessing S2).
Table 2 denotes ahigher level of satisfaction tributed to the YourQAsystem in both cases.5.2.3 Reading level accuracyAdaptivity to the users?
reading level is the dis-tinguishing feature of the YourQA system: wewere thus particularly interested in its perfor-mance in this respect.
Table 3 shows that alto-gether, evaluators found our results appropriate forthe reading levels to which they were assigned.The accuracy tended to decrease (from 94% to72%) with the level: this was predictable as it ismore constraining to conform to a lower readinglevel than to a higher one.
However this also sug-gests that our estimation of document difficultywas perhaps too ?optimisitic?
: we are currentlyworking with better quality training data which al-lows to obtain more accurate language models.Query Ag Am ApWho painted the Sistine Chapel?
0,85 0,72 0,79Who was the first American in space?
0,94 0,80 0,72Who was Achilles?
best friend?
1,00 0,98 0,79When did the Romans invade Britain?
0,87 0,74 0,82Definition of metaphor 0,95 0,81 0,38What is chickenpox?
1,00 0,97 0,68Define german measles 1,00 0,87 0,80Types of rhyme 1,00 1,00 0,79Who was a famous cubist?
0,90 0,75 0,85When did the Middle Ages begin?
0,91 0,82 0,68Was there a Trojan war?
0,97 1,00 0,83Shakespeare?s most famous play?
0,90 0,97 0,83average 0,94 0,85 0,72Table 3: Queries and reading level accuracy5.2.4 Overall utilityAt the end of the whole search session, usersanswered the question: ?Overall, how was thissearch session??
relating to their search experi-ence with Google and the YourQA system.
Thevalues obtained for U in Table 2 show a clear pref-erence (a difference of ' 1 on the 7-point scale) ofthe users for YourQA, which is very positive con-56 KRAQ06sidering that it represents their general judgementon the system.5.3 Future workWe plan to run a larger evaluation by includingmore metrics, such as user vs system ranking ofresults and the contribution of cluster by clusterpresentation.
We intend to conduct an evaluationalso involving users with a poor reading level, sothat each evaluator will only examine answers tar-geted to his/her reading level.
We will analyse ourresults with respect to the individual reading levelsand the different types of questions proposed.6 ConclusionA user-tailored open domain QA system is out-lined where a user model contributes to elaborat-ing answers corresponding to the user?s needs andpresenting them efficiently.
In this paper we havefocused on how the user?s reading level (a param-eter in the UM) can be used to filter and re-orderthe candidate answer passages.
Our preliminaryresults show a positive feedback from human as-sessors on the utility of the system in an informa-tion seeking domain.
Our short term goals involveperforming a more extensive evaluation, exploit-ing more UM parameters in answer selection andimplementing a dialogue interface to improve thesystem?s interactivity.ReferencesL.
Ardissono, L. Console, and I. Torre.
2001.
An adap-tive system for the personalized access to news.
AICommun., 14(3):129?147.J.
Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-vlin, and J. Tait.
1999.
Simplifying text forlanguage-impaired readers.
In Proceedings ofEACL?99, pages 269?270.K.
Collins-Thompson and J. P. Callan.
2004.
A lan-guage modeling approach to predicting reading dif-ficulty.
In Proceedings of HLT/NAACL.K.
Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.2003.
Text simplification for reading assistance: aproject note.
In ACL Workshop on Paraphrasing:Paraphrase Acquisition and Applications, pages 9?16.J.
J. Jiang and D. W. Conrath.
1997.
Semantic similar-ity based on corpus statistics and lexical taxonomy.In Proceedings of the International Conference Re-search on Computational Linguistics (ROCLING X).J.
Kincaid, R. Fishburne, R. Rodgers, and B. Chissom.1975.
Derivation of new readability formulas fornavy enlisted personnel.
Technical Report BranchReport 8-75, Chief of Naval Training.C.
C. T. Kwok, O. Etzioni, and D. S. Weld.
2001.
Scal-ing question answering to the web.
In World WideWeb, pages 150?161.J.
Lin, D. Quan, V. Sinha, and K Bakshi.
2003.
Whatmakes a good answer?
the role of context in questionanswering.
In Proceedings of INTERACT 2003.Bernardo Magnini and Carlo Strapparava.
2001.
Im-proving user modelling with content-based tech-niques.
In UM: Proceedings of the 8th Int.
Confer-ence, volume 2109 of LNCS.
Springer.James Pitkow, Hinrich Schuetze, Todd Cass, Rob Coo-ley, Don Turnbull, Andy Edmonds, Eytan Adar, andThomas Breuel.
2002.
Personalized search.
Com-mun.
ACM, 45(9):50?55.S.
Quarteroni and S. Manandhar.
2006.
User mod-elling for adaptive question answering and informa-tion retrieval.
In Proceedings of FLAIRS?06.M.
Steinbach, G. Karypid, and V. Kumar.
2000.
Acomparison of document clustering techniques.L.
T. Su.
1991.
An investigation to find appropriatemeasures for evaluating interactive information re-trieval.
Ph.D. thesis, New Brunswick, NJ, USA.L.
T. Su.
2003.
A comprehensive and systematicmodel of user evaluation of web search engines: Ii.an evaluation by undergraduates.
J.
Am.
Soc.
Inf.Sci.
Technol., 54(13):1193?1223.Jaime Teevan, Susan T. Dumais, and Eric Horvitz.2005.
Personalizing search via automated analysisof interests and activities.
In Proceedings of SIGIR?05, pages 449?456, New York, NY, USA.
ACMPress.E.
M. Voorhees.
2003.
Overview of the TREC 2003question answering track.
In Text REtrieval Confer-ence.E.
M. Voorhees.
2004.
Overview of the TREC 2004question answering track.
In Text REtrieval Confer-ence.S.
Williams and E. Reiter.
2005.
Generating readabletexts for readers with low basic skills.
In Proceed-ings of ENLG-2005, pages 140?147.H.
Witten and E. Frank.
2000.
Data Mining: PracticalMachine Learning Tools and Techniques with JavaImplementation.
Morgan Kaufmann.I.
H. Witten, G. W. Paynter, E. Frank, C. Gutwin, andC.
G. Nevill-Manning.
1999.
KEA: Practical au-tomatic keyphrase extraction.
In ACM DL, pages254?255.57 KRAQ06
