Generating Referring Quantified ExpressionsJ ames  Shaw and Kath leen  McKeownDept .
of Computer  Sc ienceCo lumbia  Un ivers i tyNew York,  NY  10027, USAshaw,  kathy*cs ,  co lumbia ,  edu, : ,  ~ .
*~ .Abst ractIn this paper, we describe how quantifiers can begenerated in a text generation system.
By takingadvantage of discourse and ontological information,quantified expressions can replace entities in a text,making the text more fluent and concise.
In ad-dition to avoiding ambiguities between distributiveand collective readings in universal quantificationgeneration, we will also show how different scopeorderings between universal and existential quanti-tiers will result in different quantified expressions inour algorithm.1 In t roduct ionTo convey information concisely and fluently, textgeneration systems often perform opportunistic textplanning (Robin, 1995; Mellish et al, 1998) and em-ploy advanced linguistic constructions such as ellip-sis (Shaw, 1998).
But a system can also take ad-vantage of quantification and ontological informa-tion to generate concise references to entities at thediscourse level.
For example, a sentence such as"'The patient has an infusion line in each arm."
isa more concise version of "The patient has an in-fusion line ir~ his left arm.
The patient has an in-fusion line in his right arm."
Quantification is anactive research topic in logic, language, and philoso-phy(Carpenter, 1997; de Swart.
1998).
Since nat-ural language understanding systems need to ob-tain as few interpretations as possible from text,researchers have studied quantifier scope ambigu-ity extensively (Woods~ 1978;-Grosz et al, 1987;Hobbs and Shieber, 1987: Pereira, 1990; Moran andPereira, 1992: Park, 1995).
Research in quantifica-tion interpretation first transforms a sentence intopredicate logic, raises tim quantifiers to the senten-tial level, and permutes these quantifiers {o obtainas many readings as possible relaled to quantifierscoping.
Then, invalid readings are eliminated usingvarious consl raints.Ambiguity in quantified expressions i caused bytwo main culprits.
The  first type of ambiguity in-volves the distributive reading versus the collectivereading.
In universal quantification, a referring ex-100pression refers to multiple entities.
There is a po-tential ambiguity between whether the aggregatedentities acted individually (distributive) or acted to-gether as one (collective).
Under the distributivereading, the sentence "All the nurses inspected thepatient."
implies that each nurse individually in-spected the patient.
Under the collective reading,the nurses inspected the patient ogether as a group.The other ambiguity in quantification i volves mul-tiple quantifiers in the same sentence.
The sentence"A nurse inspected each patient."
has two possi-ble quantifier scope orderings.
In Vpatient3nurse,the universal quantifier V has wide scope, outscop-ing the existential quantifier 3.
This ordering meansthat each patient is inspected by a nurse, who mightnot be the same in each case.
In the other scopeorder, 3nurseVpatient, a single, particular nurse in-spected every patient.
In both types of ambiguities,a generation system should make the desired readingclear.Fortunately, the difficulties of quantifier scope dis-ambiguation faced by the understanding conmmnitydo not apply to text generation.
For generation, theproblem is the reverse: given an unambiguous rep-resentation of a set of facts as input, how can itgenerate a quantified sentence that unambiguouslyconveys the intended meaning?
In this paper, wepropose an algorithm which selects an appropriatequantified expression to refer .to a set of entities us-ing discourse and ontological knowledge.
The algo-rithm first identifies the entities for quantification i;the input :propositions.
Then an- appropriate con-cept in the ontology is selected to refer to these en-tities.
Using discourse and ontological information,the system determines if quantification is appropri-ate and if it is, which particular quantifier to useto minimize the anabiguity between distributive andcollective readings.
More importantly, when thereare multiple quantifiers hi the same sentence, the al-gorithm generates different expressions for differen~scope orderings.
In this work, we focus on generat-ing referring quantified expressions for entities whichhave been mentioned before in the discourse or canbe inferred from an ontology.
There are quantifiedexpressions that do not refer to particular entitiesin a domain or discourse, such as generics (i.e.
"Allwhales are mammals.
"), or negatives (i.e., "The pa-tient has no allergies.").
The synthesis of such quan-tifiers is currently performed in earlier stages.of the((TYPE EVENT)(PRED ((PRED receive) (ID idl)))(ARGi ((PRED patient) (ID ptl)))(ARG2 ((PRED aprotinin) (ID apl)))generation process.
(MODS ((PRED after) (ID id2).
In the next section;we..vdll..~orapaxe ou_r~.approach .
.
.
.
.
.
: .
.
.
tTYRE_TIME).. ...............with previous work in the generation of quantifiedexpressions.
In Section 3, we will describe the appli-cation where the need for concise output motivated ) ) )our research in quantification.
The algorithm forgenerating universal quantifiers is detailed in Sec-tion 4, including how the system handles ambiguitybetween distributive and collective readings.
Sec-tion 5 describes how our algorithm generates en-tences with multiple quantifiers.2 Related WorkBecause a quantified expression refers to multipleentities in a domain, our work can be categorized asreferring expression generation (Dale, 1992; Reiterand Dale, 1992; Horacek, 1997).
Previous work inthis area did not address the generation of quantifiedexpressions directly.
In this paper, we are interestedin how to systematically derive quantifiers from in-put propositions, discourse history, and ontologicalinformation.
Recent work on the generation ofquan-tifiers (Gailly, 1988; Creaney, 1996; Creaney, 1999)follows the analysis viewpoint, discussing scope ana-biguities extensively.
Though our algorithm gener-ates different sentences for different scope orderings,we do not achieve this through scoping operations asthey did.
Creaney also discussed various imprecisequantifiers, such as some, at least ,  and at most.In regards to generating eneric quantified expres-sions, (Knott et al, 1997) has proposed an algorithmfor generating defeasible, but informative descrip-tions for objects in nmseums.Other researchers (van Eijck and Alshawi, 1992;Copestake t al., 1999) proposed representations i  amachine translation setting which allow underspec-ification in regard to quantifier scope.
Our work isdifferent, in that we perform quantification directlyon the instance-based representation btained fromdatabase tuples.
Our input .does not have the in - .
.formation about which entities are quantified as isthe case in machine translation, where the quanti-tiers are already specified in the input from a sourcelanguage.3 The  Application DomainWe implemented our quantification algorithm aspart of MAGIC (Dalai et al, 1996: McKeown etal., 1997).
MAGIC automatically generates multi-media briefings to describe the post-operative sta-tus of a patient after undergoing Coronary ArteryBypass Graft, surgery.
The system embodies a stan-.
f .
.
.
.
.
.
.
(ARG2 ((PRED critical-point)(NAME intubation) (IDcl)))Figure h The predicate-argument structure of"After intubation, a patient received aprotinin.
"dard text generation system architecture with threemodules (Rambow and Korelsky, 1992): a contentplanner, a sentence planner, and a linguistic realizer.Once the bypass urgery is finished, information thatis automatically collected during surgery such asblood pressure, heart rate, and medications given,is sent to a domain=specific medical inference mod-ule.
Based on the medical inferences and schemas(McKeown, 1985), the content planner determinesthe information to convey and the order to conveyit.The sentence planner takes a set of propositions(or predicate-argument structures) with rhetoricalrelations from the content planner and uses linguisticinformation to make decisions about how to conveythe propositions fluently.
Each proposition is repre-sented as a feature structure (Kaplan and Bresnan,1982; Kay, 1979) similar to the one shown in Fig-ure 1.
The sentence planner's responsibilities includereferring expression generation, clause aggregation,and lexical choice (Wanner and How, 1996).
Thenthe aggregated predicate-argument structure is sentto FUF/SURGE (Elhadad and Robin, 1992), a lin-guistic realizer which t.ransforms the lexicalized se-inantic specification i to a string.
The quantificationalgorithm is implemented in the sentence planner.4 Quantification Algorithmin this:,work, weprefergenerating expressions withuniversal quantifiers over conjunction because, as-suming that the users and the system have tile samedomain model, the universally quantified expres-sions are more concise and they represent the sameamount of information as the expression with con-joined entities.
In contrast,, when given a conjunc-tion of entities and an expression with a cardinalquantifier, the system, by default, would use theconjunction if the conjoined entities can be distin-guished at the surface level.
This is because oncethe system generates a cardinal quantifier when theuniversal quantification does not hold, such as "three101patients", it is impossible for the hearer to recoverthe identities of these patients based on the con-text.
The default heuristics to prefer universal quan-tifier over conjunction over cardinal quantifier canbe superseded by directives f romthe  contentplan-ner which are application specific.The input to our quaatif ica~omalgorit;hm is a setof predicate-argument structures after the referr ingexpression module selected the properties to identifythe entities (Dale, 1992; Dale and Reiter, 1995), butwithout carrying out the assignment of quantifiers.Our quantification algorithm first identifies the setof distinct entities which can be quantified in theinput propositions.
A generalization of the entitiesin the ontology is selected to potentially replace thereferences to these entities.
If universal quantifica-tion is possible, then the replacement is made andthe system must select which particular quantifierto use.
In our system, we have six realizations foruniversal quantifiers: each, every,  a l l  1, both, the,and any, and two for existential quantifiers: the in-definite article, a/an, and cardinal n.4.1 Ident i fy  Themat ic  Ro les  wi th  Dist inctEnt i t iesOur algorithm identifies the roles containing distinctentities among the input propositions as candidatesfor universal and existential quantification.
Supposethe system is given two propositions imilar to theone in Figure 1, "After intubation, Alice receivedaprotinin" and "After start of bypass, Alice receivedaprotinin", each with four roles - PRED, ARG1,ARG2, and MODS-TIME.
By computing similarityanaong entities in the same role, the system deter-mines that the entities in ARG1, PRED, and ARG2are identical in each role, and only the entities inMODS-TIME are different.
Based on this result,the distinct entities in MODS-TIME, "after intuba-tion" and "after start of bypass", are candidates forquantificat ion.4.2 Genera l i zat ion  and Quant i f icat ionWe used the axioms in Figure 2 to determine ifthe distinct entities can be universally or existen-tially quantified.
Though the axioms are similar tothose used in Generalized Quantifier (Barwise andCooper, 1981; Zwarts, 1983; de Swart, 1998).
thesemantics of set X and set D are different.
In theprevious step.
the entities in set X have been iden-tified.
To compute set D in Figure 2. we introducea concept, Class-X.
Class-X is a generalization ofthe distinct entities in set X. Quantification can re-place the distinct entities in the propositions witha reference to their type restricled by a quantifier.accessing discourse and ontological information .toprovide a context.
Our ontology is implemented inl a l i  is rea l ized as "a l i  the" .102?
bo th :  ID  - X \ [  = 0 and I x I  = 2, can have col-lective reading?
every, a l l ,  the: ID-XI = 0 and IX\[ > 2, canhave collective reading?
each: I D - X\[ = 0 and IXI _> 2, only distribu--- ?
tive reading .
.
.
.
.
.
.
.?
any: \ ]D-  X\] = 0, when under the scope ofnegation?
a /an :  IDnXl > 0 and Ixl = 1?
n (cardinal): IOnXl > 0 and \[Xl = nFigure 2: Axioms of the quantifiers discussed in thispaper.CLASSIC(Borgida et al, 1989) and is a subset ofWordNet(Miller et al, 1990) and an online medicaldictionary (Cimino et al, 1994) designed to supportmultiple applications across the medical institution.Given the entities in set X, queries in CLASSIC de-termine the class of each instance and its ancestorsin the ontology.
Based on this information, the gen-eralization algorithm identifies Class-X by comput-ing the most specific class which covers all the enti-ties.
Earlier work (Passonneau et al, 1996) provideda framework for balancing specificity and verbosityin selecting appropriate concepts for generalization.However, given the precision needed in medical re-ports, our generalization procedure selects the mostspecific class.Set D represents the set of instances of Class-X ina context.
Our system currently computes et D forthree different contexts:e discourse: Previous references can provide anappropriate context for universal quantification.For example, if "Alice" and "Bob" were men-tioned in the previous entence, the system canrefer t.o them as "both patients" in the currentsentence.?
domain ontology: The domain ontology pro-vides a closed world from which we can obtain't-he set D by matching all the instances of aconcept in the knowledge base, such as "'ev-ery patient".
In addition, certain concepts inthe ontology have limited types.
For example,knowing that cell savers, platelets and packedred blood cells are the only possible types ofblood products in the ontology, the quantifiedexpression "every blood product" can be usedinstead of referring to each entity.?
domain knowledge: The possessor of the dis-tinct entities in a role might contain a maximumnumber of instances allowed for Class-X.
For ex-ample, because a person has only two arms, the tinguishable xpressions at surface level.
A moreentities "the patient's left arm" and "the pa- developed pragmatic module is needed before quan-tient's right arm" can be referred to as "each tifiers such as some, raps'e, a t  leas t ,  and few, canarm".
be systematically generated.
Indiscriminate applica-tion of imprecise quantification can result in- vagueThe computation of set D can also involve interac- or inappropriate text in our domain, such as "Thetions with a referring expression m0dule(Dale aad~ ~-:patient~rec~ived.~some 61ood~produetS:"'-v.ia-our~e~P -Reiter, 1995).
For example, instead of the expres- plication, knowing exactly what blood products aresion "Alice and Bob" and "both patients" coveredby the current algorithm, by interacting with a refer-ring expression module, the system might determinethat "both CABG patients operated on this morn-ing by Dr. Rose" is a clearer expression to refer tothe entities.
Though this is desirable, we did notincorporate this capability into our system.Although the is often used to indicate a genericreference (i.e., "The lion is the king of jungle.
"), inEnglish, the can also be used as an unmarked uni-versal quantifier when its head noun is plural, suchas "the patients."
Like the quantifier a l l ,  the canbe both distributive and collective.
However, thecannot always replace a l l  as a universal quantifier.the cannot be used when universal quantification isbased on the domain ontology.
For example, it isnot obvious that the quantified expression in "Johnreceived the blood products."
refers to "each bloodproduct" in the ontology.
Although unmarked uni-versal quantifiers can be used to refer to body parts,as in "The lines include an IV in the arms.
", the ex-pression is ambiguous between the distributive andcollective readings.
Of the three contexts discussedabove, the system occationally generates the insteadof every and both in a discourse context, yieldingmore natural output.When the computed set D matches et X exactly(ID - X I = 0), a quantified expression with eithereach, a l l ,  every, both, the, and any, replaces theentities in set X.4.3 Select ing a Par t i cu la r  Quant i f ie rIn general, the universal quantification of a partic-ular type of entity, such as "every patient", refersto all such entities in a context.
As a result, read-ers can recover what a universally quantified expres-sion refers to.
In contrast, readers cannot pinpointwhich entity has been refei'red to.
in an existentially .quantified expression, such as "a patient."
or "twopatients".
Because a universally quantified expres-sion preserves original semantics and is more con-cise than listing each entity, it is the focus of ourquantificalion algorithm.
The universal quantifiershlaplemented in our system include the six possiblerealizations of V in English: every, a l l .
each.
both.the, and any.
The only existential quantifiers im-plemented in our system are the singular indefinitequantifier, a/an.
and cardinal quantifiers, n. Theyare used in sentences with multiple quantifiers andwhen the entities being referred to do not have dis-used is very important.
To avoid generating suchinappropriate sentences, the system only performsgeneralization on the entities which can be univer-sally quantified.
If the distinct entities cannot beuniversally quantified, the system will realize theseentities using coordinated conjunction.Once the system decides that a universally quan-tified expression can be used to replace the entitiesin set X, it must select which universal quantifier.Because our sentence planner opportunistically com-bines distinct entries from separate database ntriesfor conciseness, it is not the case that these aggre-gated entities acted together (the collective read-ing).
Given such input, the referring expression foraggregated entities should have only the distribu-tive reading 2.
The universal quantifier, each, al-ways imposes a distributive reading when applied.In general, each requires a "matching" between thedomain of the quantifier and the objects referredto(McCawley, 1981, pp.
37).
In our algorithm, thismatching process is exactly what happened, thus itis the default universal quantifier in our algorithm.Of course, indiscriminate use of each can result inawkward sounding text.
For example, tile sentence"Every patient is awake" sounds more natural than"Each patient is awake."
However, since quantifiedexpressions with the universal quantifiers a l l  andevery 3 can have collective readings (Vendler, 1967;McCawley, 1981), our system generates every anda l l  under two conditions when the collective read-ing is unlikely.
First if the proposition is a state, asopposed to an event, we assume only the distribu-tive reading is possible 4.
The quantifier every isused in "Ever.q patient tmd.taehycardia.'"
becausethe proposition is a state proposition and containsthe predicate has-attribute, an attributive relation.. .
.
.
.
2For our  system to  generate noun-phrases.wivh ,col}eetivereadings, the quantification process must be performed at thecontent planner level not in the clause aggregation module.3every is also distributive, but it stresses completeness orrather, exhaustiveness(Vendler, 1967).
The sentence "Johntook a picture of everyone in the room."
is ambiguous while"John took a picture os t each person in the room."
is not.4There are cases where state propositions do have dis-teibuted readings (e.g., "Mountains urround the village."
).Sentences with collective readings are bandied earlier in thecontent planner and thus, this type of problem does not occurat this point in our system.
Though .this observation seems tobe true in our medical application, when implementing quan-tifiers in a new domain, we can limit this assumption to onlythe subset of state relations for which it holds.103Second, when the concept being universally quan-tified is marked as having a distributive reading inthe lexicon, such as the concept episode, quantifiersevery will be used instead of each.
These quanti-tiers make the quantified sentences more natural be-cause they do not pick out the redundant distribu-tive meaning.
.
.
.
.
.
.
.~ .
-: ....... =~:: .... ~" :~;The use of prepositions can also affect which quan-tifier to use.
For example, "A f te r  all the episodes,the patient received obutamine" is ambiguous in re-gards to whether the dobutamine is given once dur-ing the surgery, or given after each episode.
In con-trast, the sentence " In  all the episodes, the patientreceived dobutamine."
does not have this problem.The current system looks at the particular preposi-tion (i.e., "before", "after", or "in") before selectingthe appropriate quantifier.4.4 Examples of a Single QuantifierGiven the four propositions, "After intubation,Mrs.
Doe had tachycardia", "After skin incision,Mrs.
Doe had tachycardia", "After start of bypass,Mrs.
Doe had tachycardia',  and "After coming offbypass, Mrs. Doe had tachycardia.
", the algorithmfirst identifies roles with similar entities, ARG1,PRED, ARG2 and removes them from further quan-tification processing while the distinct entities in therole MODS-TIME, "after intubation", "after skin in-cision", "after start of bypass", and "after coming offbypass", are further processed for universal quantifi-cation.
The role MODS-TIME is further separatedinto two smaller roles, one role with the preposi-tions and the other role with different critical points.Since the prepositions are all the same, universalquantification is only applied to the distinct entitiesin set X, in this case, the four critical points.
Queriesto the CLASSIC ontology indicate that the enti-ties in set X, "intubation", "skin-incision", "start-of-bypass", and "conaing-off-bypass" match all thepossible types of the concept c r i t i ca l -po in t ,  sat-isfying the domain ontology context in Section 4.2.Since set D and set X match exactly, generalizationand universal quantification can be used to replacethe references to these entities: "After each criti-cal point, Mrs. Doe had tachycardia."
The systemcurrently does not.perfor.m generMization omeJ~titieswhich failed the univeral quantification test..
In suchcases, a sentence with conjunction will be generated,i.e., "After intubation and skin incision, Mrs. Doehad tachycardia.
"In addition to every,  the system generates bothwhen the number of entities in set X is two.
Inour application, both is used as a universal quanti-tier under discourse context: "Alice had q)isodes ofbradycardia b@)re inductio1~ and start of bypass, h~both episodes, she received Cefazolin and Phen!lle-phrine.
"When a universal quantifier is under the govern-104ment of negation, each, a l l ,  every  and both are in-appropriate, and any should be used instead.
Giventhat the patient went on bypass without compli-cations, the system should generate "The patientwent on bypass without any  problem."
In contrast ,"The patient went on bypass without every  prob-/em.V=-~as-~ a,:differeut.-~meani~g; -,Our, :,system-cur=.rently uses any as a universal quantifier when theuniversal quantification is under the government ofnegation, such as "The patient denied any drug al-lergy.
", or "Her hypertension was controlled withoutany medication."
Currently, the generation of nega-tion sentences about surgery problems and allergiesare handled in the content planner.
They are notsynthesized from multiple negation sentences: "Thepatient is not allergic to aspirin.
The paitent is notallergic to penicillin..."5 Generation of Multiple QuantifiersWhen there are two distinct roles across the proposi-tions, the algorithm tries to use a universal quantifierfor one role and an existential quantifier for another.To generate sentences with 33, both entities beingreferred to must have no proper names; this triggersthe use of existential quantifiers.
We intentionallyignore the cases where two universal quantifiers aregenerated in the same sentence.
The likelihood forinput specifying sentences with W to a text genera-tion system is slim.When generating multiple quantifiers in the samesentence, we differentiate between cases where thereis or isn't a dependency between the two distinctroles.
Two roles are independent of each other whenone is not a modifier of the other.
For example,the roles ARG1 and ARG2 in a proposition are in-dependent.
In "Each patient is given a high sever-ity rating", performing universal quantification onthe patients (ARG3) is a separate decision fromthe existential quantification of the severity ratings(ARG2).
Similarly, in "An abnormal lab result wasseen in each patient with hypertension after bypass".the quantification operations on the abnormal abresults and the patients can be performed indepen-dently.....
When there isa dependency 'between theroles be-ing quantified, the quantification process of each rolemight interact because modifiers restrict the rangeof the entities being modified.
We found that whenuniversal quantification occurs in the MODS role,the quantification of PRED and MODS can be per-formed independently, just as in the cases withou!dependency.
Given the input propositions "Alice hasI I<I in Alice's left arm.
Alice has IV-2 in Alice'sright arm.
", the distinct roles are ARG2 "IV-i" and"IV-T',  and ARG2-MODS "in Alice's left arm" and"in Alice's right arm".
The ARG2-MODS is uni-versally quantified based on domain knowledge that?
Roles without dependency, V Role-l,3 Role-2Each patient is given a high severity rating.?
Roles without dependency, 3 Role-l, 'v' Role-2An abnormal lab result was seen in each patientgeon's name is likely to be known, and the in-put is likely to be "Dr. Rose operated on Alice","Dr~ Rose operated on Bob", and "Dr. Rose oper-ated on Chris".
Given these three propositions, theentities in ARG1 and PRED are identical, and onlywith hypertension after bypass, the distinct entities in ARG2, "Alice", "Bob" and*Roles  with depend~flcy, V PRED,-3 MODS ............. Ghns:,~-~ d.~be:;~qua,atffied.... ~Wilih-,-am:a;ppropriatecontext, the sentence "Dr. Rose operated on each Every patient with a balloon pump had hyper-tension.?
Roles with dependency, 3 PRED, V MODSAlice has an IV in each arm,.Figure 3: Sentences with two quantifiersa patient is a human and a human has a left armand a right arm.
In this example, "an IV in eacharm", the decision to generate universal and exis-tential quantified expressions are independent.
Butin "Every patient with a balloon pump had hyperten-sion", the existentially quantified expression "with aballoon pump" is a restrictive modifier of its head.
Inthis case, the set D does not include all the patients,but only the patients "with a balloon pump".
Whencomputing set D for universal quantification, the al-gorithm takes this extra restriction into account byeliminating all patients without such a restriction.Once a role is universally quantified and the other isexistentially quantified, our algorithm replaces bothroles with the corresponding quantified expressions.Figure 3 shows the sentences with multiple quanti-tiers generated by applying our algorithm.5.1 Ambiguity RevisitedIn Section 4.3, we described how to minimize theambiguity between distributive and collective read-ings when generating universal quantitiers.
Whatabout the scope ambiguity when there are muhiplequantifiers in the same sentence?
If we look at theroles which are being universally and existentiallyquantified in our examples in Figure 3, it is inter-esting to note that the universal quantifiers alwayshave wider scope than the existential quantifiers.
Inthe first, example, ,the.scope: order is Vpatient~high-severity-rating, the second example is Vpatient31ab-result, the third is Vpatient3balloon-pump, and thefourth is Varm3IV.
The scope orderings are all V3.\Vhat happens if a sentence contains an existen-tial quantifier which has a wider scope than a uni-versal quantifier?
In "A suryeon operated on eachpatient.
", tile normal reading is Vpatienl3surgeon.13ut~ if the existentially quantified noun phrase"'a surgeon" refers to tile same surgeon, as in3surgeonVpatient.
tlle system would generate "(Aparticular/The same) surgeon operated on each pa-tient."
In an applied generation system, the sur-patient" will be generated.
If the name of the sur-geon is not available but the identifiers for the sur-geon entities across the propositions are the same,the system will generate "The same surgeon oper-ated on each patient."
As this example indicates,when 3 has a wider scope than V, the first step inour algorithm (described in Section 4.1), identify-ing roles with distinct entities, would eliminate theroles with identical entities from further quantifica-tion processing.
Based on our algorithm, the sen-tences with 3V readings are taken care of by the firststep, identifying roles with distinct entities, while V3cases are handled by quantification operations formultiple roles, as described in Section 5.In Section 4.3, we mentioned that it is importantto know exactly what blood products are used inour application.
As a result, the system would notgenerate the sentence "Each patient received a bloodproduct."
when the input propositions are "Alice re-ceived packed red blood cells.
Bob received platelets.Chris received platelets."
Even though tim conjoinedentities can be generalized to "blood product", thisquantification operation would violate our precondi-tion for using existential quantifiers: the descriptionsfor each of the conjoined entities must be indistin-guishable.
Here, one is "red blood cells" and tile oth-ers are "platelets".
Given these three propositions,the system would generate "Alice received packedred blood cells, and Bob and Chris, platelets."
basedon the algorithm described in (Shaw.
1998).
If inour domain the input propositions could be "'Al-ice received blood-product-1.
Bob received blood-product-2.
Chris received blood-product-2.
", whereeach instance of blood-product-n could be realizedas "blood product", then the system would generate"Each patient received a blood product."
since thedescription of conj0ined entities are not dist~inguish -able at the surface level.6 Conc lus ionWe have described the quantification operators thatcan make the text more concise while preserving theoriginal semantics in the input propositions.
Thoughwe would like to incorporate imprecise quantifierssuch as few.
many, some into our system becausethey have potential to drastically reduce the text.further, these quantifiers do not, have the desiredproperty ill which the readers can recover the exact.entities in the input propositions.
The property of105preserving the original semantics i very importantsince it guarantees that even though the surface x-pressions are modified, the information is preserved.This property allows the operators to be domain in-dependent and reusable in different natural languageNorman Creaney.
1999.
Generating quantified logi:cal forms from raw data.
In Proe.
of the ESSLLI-99 Workshop on the Generation of Nominal Ex-pressions.M.
Dalal~ S. Feiner, K. McKeown, D. Jordan,generation systems.
B. Allen, and Y. alSafadi.
1996.
MAGIC: AnWe have described: an.
algo_r.itlma :which.sy.stemati .............. e:~cpertimeeataL:aystem..for: genetattiag~ .multimediacally derives quantifiers from input propositions, dis-course history and ontological information.
We iden-tified three types of information from the discourseand ontology to determine if a universal quantifiercan be applied.
We also minimnized the ambiguitybetween distributive and collective readings by se-lecting an appropriate universal quantifier.
Mostimportantly, for multiple quantifiers in the same sen-tence, we have shown how our algorithm generatesdifferent quantifed expressions for different scope or-derings.7 AcknowledgementWe would like to thank anonymous reviewers forvaluable comments.
The research is supported inpart by the National Library of Medicine undergrant LM06593-02 and the Columbia UniversityCenter for Advanced Technology in High Perfor-mance Computing and Communications in Health-care (funded by the New York State Science andTechnology Foundation).
Any opinions, findings, orrecommendations expressed in this paper are thoseof the authors and do not necessarily reflect theviews of the above agencies.Re ferencesJon Barwise and Robin Cooper.
1981.
Generalizedquantifiers and natural anguage.
Linguistics andPhilosophy, 4:159-219.Alexander Borgida, Ronald Brachman, DeborahMcGuinness, and Lori Alperin Resnick.
1989.CLASSIC: A structural data model for objects.In A CM SIGMOD International Conference onManagement of Data.Bob Carpenter.
1997.
Type-LogicaISemanties.
MITPress, Cambridge, Massachusetts.James J. Cimino, Paul D. Clayton, George Hripc-sak, and Stephen B. Johnson,: 1994.
Knowledge-.based approaches to the maintenance of a largecontrolled medical terminology.
The Journal ofthe American Medical lnformatics Association,1(1):35-50.Ann Copestake, Dan Flickinger, Ivan A.
Sag.
andCarl J. Pollard.
1999.
Minimal recursion seman-tics: An introduction.
Manuscript available viaht tp://lingo.stan ford.edu/pubs.ht ml.Norman Creaney.
1996.
An algorithm for generat-ing q~rantifiers.
In Proc.
of the 8th InternationalWorkshop on Natural Language Generation, Sus-sex, UK.briefings about post-bypass patient status.
InProc.
1996 AMIA Annual Fall Syrup, pages 684-688, Washington, DC, October 26-30.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the gricean maxims in the gener-ation of referring expressions.
Cognitive Science,19:233-263.Robert Dale.
1992.
Generating Referring Expres-sions: Constructing Descriptions in a Domain ofObjects and Processes.
MIT Press, Cambridge,MA.Henriette de Swart.
1998.
Introduction to NaturalLanguage Semantics.
CSLI Publications.Michael Elhadad and Jacques Robin.
1992.
Con-trolling content realization with functional unifi-cation grammars.
In Aspects of Automated Nat-ural Language Generation, Lecture Notes in Ar-tificial Intelligence, 587, pages 89-104.
Springer-Verlag, Berlin, April.Pierre-Joseph Gailly.
1988.
Expressing quantifierscope in French generation.
In Proceedings of the12th International Conference on ComputationalLinguistics (COLING-88), volumne 1, pages 182-184, Budapest, August 22-27,.Barbara J. Grosz, Douglas E. Appelt, Paul A.Martin, and Fernando C. N. Pereira.
1987.TEAM: An experiment in the design of trans-portable natural-language interfaces.
ArtificialIntelligence, 32(2):173-243, May.Jerry Hobbs and Stuart Shieber.
1987.
An algo-rithm for generating quantifier scopings.
Compu-tational Linguistics, 13(1-2):47-63, January-June.Helmut Horacek.
1997.
An algorithm for generatingreferential descriptions with flexible interfaces.
InProc.
of the 35th ACL and 8th EACL, pages 206213.Ronald M. Kaplan and Joan Bresnan.
1982...... ,-Lexical-functional,granmaar:, A formal system forgrammatical representation.
I  Joan Bresnan, ed-itor, The Mental Representation of GrammaticalRelations, chapter 4.
MIT Press.Martin Kay.
1979.
Functional grammar.
In Proceed-ings of the 5th Annual Meeting of the BerkeleyLinguistic Society, pages 142-158, Berkeley, CA,February 17-19,Alistair Knott, Mick O'Donnell, Jon Oberlander.and Chris Mellish.
1997.
Defeasible rules in con-tent selection and text structuring.
In Proc.
ofthe 6th European Workshop on Natural LanguageGeneration, Duisburg, Germany.106James D. McCawley.
1981.
Everything that linguistshave always wanted to know about logic (but wereashamed to ask).
University of Chicago Press.Kathleen MeKeown, Shimei Pan, James Shaw,Desmond Jordan, and Barry Allen.
1997.
Lan-guage Engine, pages 11-38.
MIT Press, Cam-bridge, MA.Zeno Vendler.
1967.
Each and every, any and all.In Linguistics in Philosophy, pages 70-96.
CornellUniversity Press, Ithaca and London.guage generation for multimedia healthcare brief- Leo Wanner and Eduard Hovy.
1996.
The Health-ings.
In Proc.
of the Fifth:~'-AGl~: Cort\[,,~xm A:NL P, ~.
-~?.:~.
Doe~,.sentence, :planner..
qw:Proc~.
:.of 4he~.Sth :.fnter-pages 277-282.Kathleen R. McKeown.
1985.
Tezt Generation: Us-ing Discourse Strategies and Focus Constraints toGenerate Natural Language Tezt.
Cambridge Uni-versity Press, Cambridge.Chris Mellish, Mick O'Donnell, Jon Oberlander, andAlistair Knott.
1998.
An architecture for oppor-tunistic text generation.
In Proc.
of the 9th Inter-national Workshop on Natural Language Genera-tion., pages 28-37.George Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1990.Five papers on WordNet.
CSL Report 43, Cogni-tive Science Laboratory, Princeton University.Douglas B. Moran and Fernando C. N. Pereira.1992.
Quantifier scoping.
In Hiyan Alshawi, ed-itor, The Core Language Engine, pages 149-172.MIT Press, Cambridge, MA.Jong C. Park.
1995.
Quantifier scope and con-stituency.
In Pvoc.
of the 33rd ACL, pages 205-212.Rebecca Passonneau, Karen Kukich, Vasileios Hatzi-vassiloglou, Larry Lefkowitz, and Hongyan Jing.1996.
Generating summaries of work flow di-agrams.
In Proc.
of the International Confer-ence on Natural Language Processing and Indus-trial Applications, pages 204-210, New Brunswick,Canada.
University of Moncton.Fernando C. N. Pereira.
1990.
Categorial semanticsand scoping.
Computational Linguistics, 16( 1): 1-10.Owen Rainbow and Tanya Korelsky.
1992.
Appliedtext generation.
In Proceedings of the Third A CLConference on Applied Natural Language Process-ing, pages 40-47, Trento, Italy.Ehud Reiter and Robert Dale.
1992.
A fast algo-rithm for the generation of referring expressions.In Proceedings of the I4th International Con-ference on Computational Linguistics (COLING-92), pages 232-238, Nantes, France.Jacques Robin.
t995.
Revision-Based Generation ofNatural Language Sum maries Providing HistoricalBackground.
Ph.D. thesis, Columbia University.James Shaw.
1998.
Segregatory coordination and el-lipsis in text generation, tn Proc.
of the 17th COL-I.
'VG and the 36th .4m~ual Meeting of the ACL..pages 1220-1226.Jan van Eijck and Hiyan Alshawi.
1992.
Logicalforms.
In Hiyan Alshawi, editor, The Core Lan-national Workshop on Natural Language Genera-tion, pages 1-10, Sussex, UK.William A.
Woods.
1978.
Semantics and quantifi-cation in natural language question answering.
InAdvances in Computers, volume 17, pages 1-87.Academic Press.Frans Zwarts.
1983.
Determiners: a relationalperspective.
In A. ter Meulen, editor, Studiesin model-theoretic semantics, pages 37-62.
Dor-drecht: Forts.107
