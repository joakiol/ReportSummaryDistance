Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 33?40,Sydney, July 2006. c?2006 Association for Computational LinguisticsEfficient Hierarchical Entity Classifier Using Conditional Random FieldsKoen DeschachtInterdisciplinary Centre for Law & ITKatholieke Universiteit LeuvenTiensestraat 41, 3000 Leuven, Belgiumkoen.deschacht@law.kuleuven.ac.beMarie-Francine MoensInterdisciplinary Centre for Law & ITKatholieke Universiteit LeuvenTiensestraat 41, 3000 Leuven, Belgiummarie-france.moens@law.kuleuven.beAbstractIn this paper we develop an automaticclassifier for a very large set of labels, theWordNet synsets.
We employ ConditionalRandom Fields (CRFs) because of theirflexibility to include a wide variety of non-independent features.
Training CRFs on abig number of labels proved a problem be-cause of the large training cost.
By tak-ing into account the hypernym/hyponymrelation between synsets in WordNet, wereduced the complexity of training fromO(TM2NG) to O(T (logM)2NG) withonly a limited loss in accuracy.1 IntroductionThe work described in this paper was carried outduring the CLASS project1.
The central objec-tive of this project is to develop advanced learningmethods that allow images, video and associatedtext to be analyzed and structured automatically.One of the goals of the project is the alignment ofvisual and textual information.
We will, for exam-ple, learn the correspondence between faces in animage and persons described in surrounding text.The role of the authors in the CLASS project ismainly on information extraction from text.In the first phase of the project we build a clas-sifier for automatic identification and categoriza-tion of entities in texts which we report here.
Thisclassifier extracts entities from text, and assigns alabel to these entities chosen from an inventoryof possible labels.
This task is closely related toboth named entity recognition (NER), which tra-ditionally assigns nouns to a small number of cate-gories and word sense disambiguation (Agirre and1http://class.inrialpes.fr/Rigau, 1996; Yarowsky, 1995), where the sensefor a word is chosen from a much larger inventoryof word senses.We will employ a probabilistic model that?sbeen used successfully in NER (Conditional Ran-dom Fields) and use this with an extensive inven-tory of word senses (the WordNet lexical database)to perform entity detection.In section 2 we describe WordNet and it?s usefor entity categorization.
Section 3 gives anoverview of Conditional Random Fields and sec-tion 4 explains how the parameters of this modelare estimated during training.
We will drasticallyreduce the computational complexity of training insection 5.
Section 6 describes the implementationof this method, section 7 the obtained results andfinally section 8 future work.2 WordNetWordNet (Fellbaum et al, 1998) is a lexicaldatabase whose design is inspired by psycholin-guistic theories of human lexical memory.
Englishnouns, verbs, adjectives and adverbs are organizedin synsets.
A synset is a collection of words thathave a close meaning and that represent an under-lying concept.
An example of such a synset is?person, individual, someone, somebody, mortal,soul?.
All these words refer to a human being.WordNet (v2.1) contains 155.327 words, whichare organized in 117.597 synsets.
WordNet de-fines a number of relations between synsets.
Fornouns the most important relation is the hyper-nym/hyponym relation.
A noun X is a hypernymof a noun Y if Y is a subtype or instance of X. Forexample, ?bird?
is a hypernym of ?penguin?
(and?penguin?
is a hyponym of ?bird?).
This relationorganizes the synsets in a hierarchical tree (Hayes,1999), of which a fragment is pictured in fig.
1.33Figure 1: Fragment of the hypernym/hyponymtreeThis tree has a depth of 18 levels and maximumwidth of 17837 synsets (fig.
2).We will build a classifier using CRFs that tagsnoun phrases in a text with their WordNet synset.This will enable us to recognize entities, and toclassify the entities in certain groups.
Moreover,it allows learning the context pattern of a certainmeaning of a word.
Take for example the sentence?The ambulance took the remains of the bomberto the morgue.?
Having every noun phrase taggedwith it?s WordNet synset reveals that in this sen-tence, ?bomber?
is ?a person who plants bombs?
(and not ?a military aircraft that drops bombs dur-ing flight?).
Using the hypernym/hyponym rela-tions from WordNet, we can also easily find outthat ?ambulance?
is a kind of ?car?, which in turnis a kind of ?conveyance, transport?
which in turnis a ?physical object?.3 Conditional Random FieldsConditional random fields (CRFs) (Lafferty et al,2001; Jordan, 1999; Wallach, 2004) is a statisticalmethod based on undirected graphical models.
LetX be a random variable over data sequences to belabeled and Y a random variable over correspond-ing label sequences.
All components Yi of Y areassumed to range over a finite label alphabet K.In this paper X will range over the sentences ofa text, tagged with POS-labels and Y ranges overthe synsets to be recognized in these sentences.We define G = (V,E) to be an undirectedgraph such that there is a node v ?
V correspond-ing to each of the random variables representing anelement Yv of Y .
If each random variable Yv obeysthe Markov property with respect to G (e.g., in afirst order model the transition probability dependsonly on the neighboring state), then the model(Y,X) is a Conditional Random Field.
Althoughthe structure of the graph G may be arbitrary, welimit the discussion here to graph structures inFigure 2: Number of synsets per level in WordNetwhich the nodes corresponding to elements of Yform a simple first-order Markov chain.A CRF defines a conditional probability distri-bution p(Y |X) of label sequences given input se-quences.
We assume that the random variable se-quences X and Y have the same length and usex = (x1, ..., xT ) and y = (y1, ..., yT ) for an inputsequence and label sequence respectively.
Insteadof defining a joint distribution over both label andobservation sequences, the model defines a condi-tional probability over labeled sequences.
A novelobservation sequence x is labeled with y, so thatthe conditional probability p(y|x) is maximized.We define a set of K binary-valued features orfeature functions fk(yt?1, yt,x) that each expresssome characteristic of the empirical distribution ofthe training data that should also hold in the modeldistribution.
An example of such a feature isfk(yt?1, yt,x) =????
?1 if x has POS ?NN?
andyt is concept ?entity?0 otherwise(1)Feature functions can depend on the previous(yt?1) and the current (yt) state.
Considering Kfeature functions, the conditional probability dis-tribution defined by the CRF isp(y|x) = 1Z(x)exp{ T?t=1K?k=1?kfk(yt?1, yt,x)}(2)where ?j is a parameter to model the observedstatistics and Z(x) is a normalizing constant com-puted asZ(x) =?y?Yexp{ T?t=1K?k=1?kfk(yt?1, yt,x)}This method can be thought of a generalizationof both the Maximum Entropy Markov model(MEMM) and the Hidden Markov model (HMM).34It brings together the best of discriminative mod-els and generative models: (1) It can accommo-date many statistically correlated features of theinputs, contrasting with generative models, whichoften require conditional independent assumptionsin order to make the computations tractable and (2)it has the possibility of context-dependent learningby trading off decisions at different sequence posi-tions to obtain a global optimal labeling.
BecauseCRFs adhere to the maximum entropy principle,they offer a valid solution when learning from in-complete information.
Given that in informationextraction tasks, we often lack an annotated train-ing set that covers all possible extraction patterns,this is a valuable asset.Lafferty et al (Lafferty et al, 2001) have shownthat CRFs outperform both MEMM and HMMon synthetic data and on a part-of-speech taggingtask.
Furthermore, CRFs have been used success-fully in information extraction (Peng and McCal-lum, 2004), named entity recognition (Li and Mc-Callum, 2003; McCallum and Li, 2003) and sen-tence parsing (Sha and Pereira, 2003).4 Parameter estimationIn this section we?ll explain to some detail how toderive the parameters ?
= {?k}, given the train-ing data.
The problem can be considered as a con-strained optimization problem, where we have tofind a set of parameters which maximizes the loglikelihood of the conditional distribution (McCal-lum, 2003).
We are confronted with the problemof efficiently calculating the expectation of eachfeature function with respect to the CRF modeldistribution for every observation sequence x inthe training data.
Formally, we are given a setof training examples D ={x(i),y(i)}Ni=1whereeach x(i) ={x(i)1 , x(i)2 , ..., x(i)T}is a sequenceof inputs and y(i) ={y(i)1 , y(i)2 , ..., y(i)T}is a se-quence of the desired labels.
We will estimate theparameters by penalized maximum likelihood, op-timizing the function:l(?)
=N?i=1log p(y(i)|x(i)) (3)After substituting the CRF model (2) in the like-lihood (3), we get the following expression:l(?)
=N?i=1T?t=1K?k=1?kfk(y(i)t?1, y(i)t ,x(i))?N?i=1log Z(x(i))The function l(?)
cannot be maximized in closedform, so numerical optimization is used.
The par-tial derivates are:?l(?)?
?k =N?i=1T?t=1fk(y(i)t , y(i)t?1,x(i))?N?i=1T?t=1?y,y?fk(y?, y,x(i)) p(y?, y|x(i))(4)Using these derivates, we can iteratively adjustthe parameters ?
(with Limited-Memory BFGS(Byrd et al, 1994)) until l(?)
has reached an opti-mum.
During each iteration we have to calculatep(y?, y|x(i)).
This can be done, as for the Hid-den Markov Model, using the forward-backwardalgorithm (Baum and Petrie, 1966; Forney, 1996).This algorithm has a computational complexity ofO(TM2) (where T is the length of the sequenceand M the number of the labels).
We have to exe-cute the forward-backward algorithm once for ev-ery training instance during every iteration.
Thetotal cost of training a linear-chained CRFs is thus:O(TM2NG)where N is the number of training examples and Gthe number of iterations.
We?ve experienced thatthis complexity is an important delimiting factorwhen learning a big collection of labels.
Employ-ing CRFs to learn the 95076 WordNet synsets with20133 training examples was not feasible on cur-rent hardware.
In the next section we?ll describethe method we?ve implemented to drastically re-duce this complexity.5 Reducing complexityIn this section we?ll see how we create groups offeatures for every label that enable an importantreduction in complexity of both labeling and train-ing.
We?ll first discuss how these groups of fea-tures are created (section 5.1) and then how bothlabeling (section 5.2) and training (section 5.3) areperformed using these groups.35Figure 3: Fragment of the tree used for labeling5.1 Hierarchical feature selectionTo reduce the complexity of CRFs, we assign aselection of features to every node in the hierar-chical tree.
As discussed in section 2 WordNet de-fines a relation between synsets which organisesthe synsets in a tree.
In its current form this treedoes not meet our needs: we need a tree whereevery label used for labeling corresponds to ex-actly one leaf-node, and no label corresponds toa non-leaf node.
We therefor modify the existingtree.
We create a new top node (?top?)
and add theoriginal tree as defined by WordNet as a subtree tothis top-node.
We add leaf-nodes correspondingto the labels ?NONE?, ?ADJ?, ?ADV?, ?VERB?to the top-node and for the other labels (the nounsynsets) we add a leaf-node to the node represent-ing the corresponding synset.
For example, weadd a node corresponding to the label ?ENTITY?to the node ?entity?.
Fig.
3 pictures a fraction ofthis tree.
Nodes corresponding to a label have anuppercase name, nodes not corresponding to a la-bel have a lowercase name.We use v to denote nodes of the tree.
We callthe top concept vtop and the concept v+ the parentof v, which is the parent of v?.
We call Av thecollection of ancestors of a concept v, including vitself.We will now show how we transform a regularCRF in a CRF that uses hierarchical feature selec-tion.
We first notice that we can rewrite eq.
2 asp(y|x) = 1Z(x)T?t=1G(yt?1, yt,x)with G(yt?1, yt,x) = exp(K?k=1?kfk(yt?1, yt,x))We rewrite this equation because it will enableus to reduce the complexity of CRFs and it hasthe property that p(yt|yt?1,x) ?
G(yt?1, yt,x)which we will use in section 5.3.We now define a collection of features Fv forevery node v. If v is leaf-node, we define Fv as thecollection of features fk(yt?1, yt,x) for which it ispossible to find a node vt?1 and input x for whichfk(vt?1, v,x) 6= 0.
If v is a non-leaf node, we de-fine Fv as the collection of features fk(yt?1, yt,x)(1) which are elements of Fv?
for every child nodev?
of v and (2) for every v?1 and v?2 , children ofv, it is valid that for every previous label vt?1 andinput x fk(vt?1, v?1 ,x) =fk(vt?1, v?2 ,x).Informally, Fv is the collection of featureswhich are useful to evaluate for a certain node.
Forthe leaf-nodes, this is the collection of features thatcan possibly return a non-zero value.
For non-leafnodes, it?s useful to evaluate features belonging toFv when they have the same value for all the de-scendants of that node (which we can put to gooduse, see further).We define F ?v = Fv\Fv+ where v+ is the parentof label v. For the top node vtop we define F ?vtop =Fvtop .
We also setG?
(yt?1, yt,x) = exp???
?fk?F ?yt?kfk(yt?1, yt,x)??
?We?ve now organised the collection of features insuch a way that we can use the hierarchical rela-tions defined by WordNet when determining theprobability of a certain labeling y.
We first seethatG(yt?1, yt,x) = exp???
?fk?Fyt?kfk(yt?1, yt,x)??
?= G(yt?1, y+t , x)G?
(yt?1, yt, x)= ...=?v?AytG?
(yt?1, v, x)we can now determine the probability of a labelingy, given input xp(y|x) = 1Z(x)T?t=1?v?AytG?
(yt?1, v,x) (5)This formula has exactly the same result as eq.
2.Because we assigned a collection of features to ev-ery node, we can discard parts of the search spacewhen searching for possible labelings, obtainingan important reduction in complexity.
We elab-orate this idea in the following sections for bothlabeling and training.365.2 LabelingThe standard method to label a sentence withCRFs is by using the Viterbi algorithm (Forney,1973; Viterbi, 1967) which has a computationalcomplexity of O(TM2).
The basic idea to reducethis computational complexity is to select the bestlabeling in a number of iterations.
In the first itera-tion, we label every word in a sentence with a labelchosen from the top-level labels.
After choosingthe best labeling, we refine our choice (choose achild label of the previous chosen label) in subse-quent iterations until we arrive at a synset whichhas no children.
In every iteration we only haveto choose from a very small number of labels, thusbreaking down the problem of selecting the correctlabel from a large number of labels in a number ofsmaller problems.Formally, when labeling a sentence we find thelabel sequence y such that y has the maximumprobability of all labelings.
We will estimate thebest labeling in an iterative way: we start withthe best labeling ytop?1 = {ytop?11 , ..., ytop?1T }choosing only from the children ytop?1t of the topnode.
The probability of this labeling ytop?1 isp(ytop?1|x) = 1Z ?(x)T?t=1G?
(yt?1, ytop?1t ,x)where Z ?
(x) is an appropriate normalizing con-stant.
We now select a labeling ytop?2 so that onevery position t node ytop?2t is a child of ytop?1t .The probabilty of this labeling is (following eq.
5)p(ytop?2|x) = 1Z ?(x)T?t=1?v?Aytop?2tG?
(yt?1, v,x)After selecting a labeling ytop?2 with maximumprobability, we proceed by selecting a labelingytop?3 with maximum probability etc.. We pro-ceed using this method until we reach a labelingin which every yt is a node which has no childrenand return this labeling as the final labeling.The assumption we make here is that if a nodev is selected at position t of the most probable la-beling ytop?s the children v?
have a larger prob-ability of being selected at position t in the mostprobable labeling ytop?s?1.
We reduce the num-ber of labels we take into consideration by statingthat for every concept v for which v 6= ytop?st , weset G?
(yt?1, v?t ,x) = 0 for every child v?
of v.This reduces the space of possible labelings dras-tically, reducing the computational complexity ofFigure 4: Nodes that need to be taken into accountduring the forward-backward algorithmthe Viterbi algorithm.
If q is the average numberof children of a concept, the depth of the tree islogq(M).
On every level we have to execute theViterbi algorithm for q labels, thus resulting in atotal complexity ofO(T logq(M)q2) (6)5.3 TrainingWe will now discuss how we reduce the compu-tational complexity of training.
As explained insection 4 we have to estimate the parameters ?kthat optimize the function l(?).
We will show herehow we can reduce the computational complex-ity of the calculation of the partial derivates ?l(?)??k(eq.
4).
The predominant factor with regard tothe computational complexity in the evaluation ofthis equation is the calculation of p(yt?1, y|x(i)).Recall we do this with the forward-backward al-gorithm, which has a computational complexityof O(TM2).
We reduce the number of labels toimprove performance.
We will do this by mak-ing the same assumption as in the previous sec-tion: for every concept v at level s, for whichv 6= ytop?st , we set G?
(yt?1, v?t ,x) = 0 forevery child v?
of v. Since (as noted in sect.5.2) p(vt|yt?1,x) ?
G(yt?1, vt,x), this has theconsequence that p(vt|yt?1,x) = 0 and thatp(vt, yt?1|x) = 0.
Fig.
4 gives a graphical repre-sentation of this reduction of the search space.
Thecorrect label here is ?LABEL1?
, the grey nodeshave a non-zero p(vt, yt?1|x) and the white nodeshave a zero p(vt, yt?1|x).In the forward backward algorithm we onlyhave to account every node v that has a non-zerop(v, yt?1|x).
As can be easily seen from fig.
4,the number of nodes is qlogqM , where q is theaverage number of children of a concept.
The to-tal complexity of running the forward-backwardalgorithm is O(T (q logqM)2).
Since we have torun this algorithm once for every gradient compu-37Figure 5: Time needed for one training cycletation for every training instance we find the totaltraining costO(T (q logqM)2NG) (7)6 ImplementationTo implement the described method we need twocomponents: an interface to the WordNet databaseand an implementation of CRFs using a hierar-chical model.
JWordNet is a Java interface toWordNet developed by Oliver Steele (which canbe found on http://jwn.sourceforge.net/).
We used this interface to extract the Word-Net hierarchy.An implementation of CRFs using the hierar-chical model was obtained by adapting the Mallet2package.
The Mallet package (McCallum, 2002)is an integrated collection of Java code useful forstatistical natural language processing, documentclassification, clustering, and information extrac-tion.
It also offers an efficient implementation ofCRFs.
We?ve adapted this implementation so itcreates hierarchical selections of features whichare then used for training and labeling.We used the Semcor corpus (Fellbaum et al,1998; Landes et al, 1998) for training.
This cor-pus, which was created by the Princeton Univer-sity, is a subset of the English Brown corpus con-taining almost 700,000 words.
Every sentence inthe corpus is noun phrase chunked.
The chunksare tagged by POS and both noun and verb phrasesare tagged with their WordNet sense.
Since we donot want to learn a classification for verb synsets,we replace the tags of the verbs with one tag?VERB?.2http://mallet.cs.umass.edu/Figure 6: Time needed for labeling7 ResultsThe major goal of this paper was to build a clas-sifier that could learn all the WordNet synsets in areasonable amount of time.
We will first discussthe improvement in time needed for training andlabeling and then discuss accuracy.We want to test the influence of the number oflabels on the time needed for training.
Therefor,we created different training sets, all of which hadthe same input (246 sentences tagged with POS la-bels), but a different number of labels.
The firsttraining set only had 5 labels (?ADJ?, ?ADV?,?VERB?, ?entity?
and ?NONE?).
The second hadthe same labels except we replaced the label ?en-tity?
with either ?physical entity?, ?abstract entity?or ?thing?.
We continued this procedure, replac-ing parent nouns labels with their children (i.e.hyponyms) for subsequent training sets.
We thentrained both a CRF using a hierarchical feature se-lection and a standard CRF on these training sets.Fig.
5 shows the time needed for one iterationof training with different numbers of labels.
Wecan see how the time needed for training slowlyincreases for the CRF using hierarchical featureselection but increases fast when using a standardCRF.
This is conform to eq.
7.Fig.
6 shows the average time needed for la-beling a sentence.
Here again the time increasesslowly for a CRF using hierarchical feature selec-tion, but increases fast for a standard CRF, con-form to eq.
6.Finally, fig 7 shows the error rate (on the train-ing data) after each training cycle.
We see that astandard CRF and a CRF using hierarchical fea-ture selection perform comparable.
Note that fig7 gives the error rate on the training data but this38can differ considerable from the error rate on un-seen data.After these tests on a small section of the Sem-cor corpus, we trained a CRF using hierarchi-cal feature selection on 7/8 of the full corpus.We trained for 23 iterations, which took approx-imately 102 hours.
Testing the model on the re-maining 1/8 of the corpus resulted in an accuracyof 77.82%.
As reported in (McCarthy et al, 2004),a baseline approach that ignors context but simplyassigns the most likely sense to a given word ob-tains a accuracy of 67%.
We did not have the pos-sibility to compare the accuracy of this model witha standard CRF, since as already stated, trainingsuch a CRF takes impractically long, but we cancompare our systems with existing WSD-systems.Mihalcea and Moldovan (Mihalcea and Moldovan,1999) use the semantic density between words todetermine the word sense.
They achieve an ac-curacy of 86.5% (testing on the first two taggedfiles of the Semcor corpus).
Wilks and Stevenson(Wilks and Stevenson, 1998) use a combinationof knowledge sources and achieve an accuracy of92%3.
Note that both these methods use additionalknowledge apart from the WordNet hierarchy.The sentences in the training and testing setswere already (perfectly) POS-tagged and nounchunked, and that in a real-life situation addi-tional preprocessing by a POS-tagger (such as theLT-POS-tagger4) and noun chunker (such as de-scribed in (Ramshaw and Marcus, 1995)) whichwill introduce additional errors.8 Future workIn this section we?ll discuss some of the work weplan to do in the future.
First of all we wish toevaluate our algorithm on standard test sets, suchas the data of the Senseval conference5 , whichtests performance on word sense disambiguation,and the data of the CoNLL 2003 shared task6, onnamed entity recognition.An important weakness of our algorithm is thefact that, to label a sentence, we have to traversethe hierarchy tree and choose the correct synsetsat every level.
An error at a certain level can notbe recovered.
Therefor, we would like to perform3This method was tested on the Semcore corpus, but usethe word senses of the Longman Dictionary of ContemporaryEnglish4http://www.ltg.ed.ac.uk/software/5http://www.senseval.org/6http://www.cnts.ua.ac.be/conll2003/Figure 7: Error rate during trainingsome a of beam-search (Bisiani, 1992), keepinga number of best labelings at every level.
Westrongly suspect this will have a positive impacton the accuracy of our algorithm.As already mentioned, this work is carried outduring the CLASS project.
In the second phaseof this project we will discover classes and at-tributes of entities in texts.
To accomplish thiswe will not only need to label nouns with theirsynset, but we also need to label verbs, adjec-tives and adverbs.
This can become problem-atic as WordNet has no hypernym/hyponym rela-tion (or equivalent) for the synsets of adjectivesand adverbs.
WordNet has an equivalent relationfor verbs (hypernym/troponym), but this structuresthe verb synsets in a big number of loosely struc-tured trees, which is less suitable for the describedmethod.
VerbNet (Kipper et al, 2000) seems amore promising resource to use when classify-ing verbs, and we will also investigate the useof other lexical databases, such as ThoughtTrea-sure (Mueller, 1998), Cyc (Lenat, 1995), Open-mind Commonsense (Stork, 1999) and FrameNet(Baker et al, 1998).AcknowledgmentsThe work reported in this paper was supportedby the EU-IST project CLASS (Cognitive-LevelAnnotation using Latent Statistical Structure, IST-027978).ReferencesEneko Agirre and German Rigau.
1996.
Word sensedisambiguation using conceptual density.
In Pro-ceedings of the 16th International Conference on39Computational Linguistics (Coling?96), pages 16?22, Copenhagen, Denmark.C.
F. Baker, C. J. Fillmore, and J.
B. Lowe.
1998.
TheBerkeley Framenet project.
In Proceedings of theCOLING-ACL.L.
E. Baum and T. Petrie.
1966.
Statistical in-ference for probabilistic functions of finite statemarkov chains.
Annals of Mathematical Statistics,,37:1554?1563.R.
Bisiani.
1992.
Beam search.
In S. C. Shapiro,editor, Encyclopedia of Artificial Intelligence, NewYork.
Wiley-Interscience.Richard H. Byrd, Jorge Nocedal, and Robert B. Schn-abel.
1994.
Representations of quasi-newton matri-ces and their use in limited memory methods.
Math.Program., 63(2):129?156.C.
Fellbaum, J. Grabowski, and S. Landes.
1998.
Per-formance and confidence in a semantic annotationtask.
In C. Fellbaum, editor, WordNet: An Elec-tronic Lexical Database.
The MIT Press.G.
D. Forney.
1973.
The viterbi algorithm.
In Pro-ceeding of the IEEE, pages 268 ?
278.G.
D. Forney.
1996.
The forward-backward algo-rithm.
In Proceedings of the 34th Allerton Confer-ence on Communications, Control and Computing,pages 432?446.Brian Hayes.
1999.
The web of words.
AmericanScientist, 87(2):108?112, March-April.Michael I. Jordan, editor.
1999.
Learning in GraphicalModels.
The MIT Press, Cambridge.K.
Kipper, H.T.
Dang, and M. Palmer.
2000.
Class-based construction of a verb lexicon.
Proceedingsof the Seventh National Conference on Artificial In-telligence (AAAI-2000).J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceed-ings of the 18th International Conference on Ma-chine Learning.S.
Landes, C. Leacock, and R.I. Tengi.
1998.
Build-ing semantic concordances.
In C. Fellbaum, editor,WordNet: An Electronic Lexical Database.
The MITPress.D.
B. Lenat.
1995.
Cyc: A large-scale investment inknowledge infrastructure.
Communications of theACM, 38(11):32?38.Wei Li and Andrew McCallum.
2003.
Rapid develop-ment of hindi named entity recognition using con-ditional random fields and feature induction.
ACMTransactions on Asian Language Information Pro-cessing (TALIP), 2(3):290?294.Andrew McCallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional ran-dom fields, feature induction and web-enhanced lex-icons.
In Walter Daelemans and Miles Osborne, ed-itors, Proceedings of CoNLL-2003, pages 188?191.Edmonton, Canada.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.A.
McCallum.
2003.
Efficiently inducing features ofconditional random fields.
In Proceedings of theNineteenth Conference on Uncertainty in ArtificialIntelligence.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.2004.
Using automatically acquired predominantsenses for word sense disambiguation.
In Proceed-ings of the ACL SENSEVAL-3 workshop, pages 151?154, Barcelona, Spain.R.
Mihalcea and D.I.
Moldovan.
1999.
A methodfor word sense disambiguation of unrestricted text.In Proceedings of the 37th conference on Associa-tion for Computational Linguistics, pages 152?158.Association for Computational Linguistics Morris-town, NJ, USA.Erik T. Mueller.
1998.
Natural language processingwith ThoughtTreasure.
Signiform, New York.F.
Peng and A. McCallum.
2004.
Accurate infor-mation extraction from research papers using con-ditional random fields.
In Proceedings of HumanLanguage Technology Conference and North Amer-ican Chapter of the Association for ComputationalLinguistics (HLT-NAACL), pages 329?336.L.A.
Ramshaw and M.P.
Marcus.
1995.
Text chunkingusing transformation-based learning.
In Proceed-ings of the Third ACL Workshop on Very Large Cor-pora, pages 82?94.
Cambridge MA, USA.F.
Sha and F. Pereira.
2003.
Shallow parsing with con-ditional random fields.
In Proceedings of HumanLanguage Technology, HLT-NAACL.D.
Stork.
1999.
The openmind initiative.
IEEE Intelli-gent Systems & their applications, 14(3):19?20.A.
J. Viterbi.
1967.
Error bounds for convolutionalcodes and an asymptotically optimal decoding algo-rithm.
IEEE Trans.
Informat.
Theory, 13:260?269.Hanna M. Wallach.
2004.
Conditional random fields:An introduction.
Technical Report MS-CIS-04-21.,University of Pennsylvania CIS.Y.
Wilks and M. Stevenson.
1998.
Word sense disam-biguation using optimised combinations of knowl-edge sources.
Proceedings of COLING/ACL, 98.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceed-ings of the 33rd Annual Meeting of the Associationfor Computational Linguistics, pages 189?196.40
