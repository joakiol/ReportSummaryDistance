Proceedings of the SIGDIAL 2013 Conference, pages 163?172,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsExploring the effects of gaze and pausesin situated human-robot interactionGabriel Skantze, Anna Hjalmarsson, Catharine OertelKTH Speech, Music and HearingStockholm, Swedengabriel@speech.kth.se, annah@speech.kth.se, catha@kth.seAbstractIn this paper, we present a user study where arobot instructs a human on how to draw aroute on a map, similar to a Map Task.
Thissetup has allowed us to study user reactions tothe robot?s conversational behaviour in orderto get a better understanding of how to gener-ate utterances in incremental dialogue systems.We have analysed the participants' subjectiverating, task completion, verbal responses, gazebehaviour, drawing activity, and cognitiveload.
The results show that users utilise the ro-bot?s gaze in order to disambiguate referringexpressions and manage the flow of the inter-action.
Furthermore, we show that the user?sbehaviour is affected by how pauses are real-ised in the robot?s speech.1 IntroductionDialogue systems have traditionally relied onseveral simplifying assumptions.
When it comesto temporal resolution, the interaction has beenassumed to take place with a strict turn-takingprotocol, where each speaker takes discrete turnswith noticeable gaps in between.
While this as-sumption simplifies processing, it fails to modelmany aspects of human-human interaction suchas turn-taking with very short gaps or brief over-laps and backchannels in the middle of utteranc-es (Heldner & Edlund, 2010).
Recently, re-searchers have turned to more incremental mod-els, where the dialogue is processed in smallerunits (Schlangen & Skantze, 2011).
On the out-put side, this allows dialogue systems to startspeaking before processing is complete, generat-ing and synthesizing the response segment bysegment, until the complete response is realised.If a segment is delayed, there will be a pause inthe middle of the system?s speech.
While previ-ous studies have clearly shown the potential ben-efits of incremental speech generation (Skantze& Hjalmarsson, 2012; Dethlefs et al 2012;Buschmeier et al 2012), there are few studies onhow users react to pauses in the middle of thesystem?s speech.Apart from the real-time nature of spoken in-teraction, spoken dialog technology has for along time also neglected the physical space inwhich the interaction takes place.
In applicationscenarios which involve situated interaction,such as human-robot interaction, there might beseveral users talking to the system at the sametime (Bohus & Horvitz, 2010), and there mightbe physical objects in the surroundings that theuser and the system refer to during the interac-tion (Boucher et al 2012).
In such settings, gazeplays a very important role in the coordination ofjoint attention and turn-taking.
However, it is notclear to what extent humans are able to utilizethe gaze of a robot and respond to these cues.Here, we present a user study where a robotinstructs a human on how to draw a route on amap, similar to a Map Task.
The nature of thissetting allows us to study the two phenomenaoutlined above.
First, we want to understand howa face-to-face setting facilitates coordination ofactions between a robot and a user, and how wellhumans can utilize the robot's gaze to disambig-uate referring expressions in situated interaction.The second purpose of this study is to investigatehow the system can either inhibit or encouragedifferent types of user reactions while pausing byusing filled pauses, gaze and syntactic complete-ness.2 Background2.1 Gaze in situated interactionGaze is one of the most studied visual cues inface-to-face interaction, and it has been associat-ed with a variety of functions, such as managingattention (Vertegaal et al 2001), expressing in-timacy and exercising social control (Kleinke,1631986), highlighting the information structure ofthe propositional content of speech (Cassell,1999) as well as coordinating turn-taking(Duncan, 1972).
One of the most influential pub-lications on this subject (Kendon, 1967) showsthat speakers gaze away when initiating a newturn.
At the end of a turn, in contrast, speakersshift their gaze towards their interlocutors as toindicate that the conversational floor is about tobecome available.
Furthermore, it has beenshown that gaze plays an important role in col-laborative tasks.
In a map task study by Boyle etal.
(1994), it was shown that speakers in a face-to-face setting interrupt each other less and usefewer turns, words, and backchannels per dia-logue than speakers who can not see each other.A lot of research has also been done on howgaze can be used to facilitate turn-taking withrobots (Mutlu et al 2006; Al Moubayed et al2013) and embodied conversational agents(Torres et al 1997).
Several studies have alsoexplored situated human-robot interaction, wherethe interlocutors sit around a table with objectsthat can be referred to, thus constituting a sharedspace of attention (Yoshikawa et al 2006; John-son-Roberson et al 2011).
However, there arevery few studies on how the robot?s gaze at ob-jects in the shared visual scene may improve taskcompletion in an interactive setting.
One excep-tion is a controlled experiment presented byBoucher et al(2012), where the iCub robot in-teracted with human subjects.
While the studyshowed that humans could utilize the robot?sgaze, the interaction was not that of a free con-tinuous dialogue.Similarly to the study presented here, Nakanoet al(2003) presented a system that describes aroute to a user in a face-to-face setting.
Based onstudies of human-human interaction, they im-plemented a model of face-to-face grounding.However, they did not provide a detailed analysisof the users?
behaviour when interacting withthis system.Even if we successfully manage to model hu-man-like behaviour in a system, it is not certainto what extent humans react to these signalswhen interacting with a robot.
In the currentwork, we investigate to what extent the robot?sgaze can be used to: (1) help the user disambigu-ate referring expressions to objects in the sharedvisual scene, and (2) to either inhibit or encour-age different types of user reactions while thesystem pauses or at turn endings.2.2 Pauses in the system's speechSpeakers in dialogue produce speech piece bypiece as the dialogue progresses.
When startingto speak, dialogue participants typically do nothave a complete plan of how to say something oreven what to say.
Yet, they manage to rapidlyintegrate information from different sources inparallel and simultaneously plan and realize newdialogue contributions (Levelt, 1989).
Still,pauses occur frequently within utterances and ithas been shown that these play a significant rolein human-human dialogue (for an overview, seeRochester, 1973).
For example, the timing andduration of pauses have important structuralfunctions (Goldman-Eisler, 1972), pauses (filledand silent) are associated with high cognitiveload and planning difficulties (Brennan & Wil-liams, 1995), and whether a pause is detected ornot does not only depend on duration but also onits linguistic context (Boomer & Dittmann,1962).Recently, several studies have looked into thepossibilities of replicating the incremental behav-iour of humans in human-machine interaction.Work on incremental speech generation has fo-cused on the underlying system architecture(Schlangen & Skantze, 2011), how to incremen-tally react to events that occur while realizing anutterance (Dohsaka & Shimazu, 1997,Buschmeier et al 2012), and how to make theincremental processes more efficient in order toreduce the system?s response time (e.g.
Dethlefset al 2012).
In a recent study, we implemented amodel of incremental speech generation in a dia-logue system (Skantze & Hjalmarsson, 2012).
Byallowing the system to generate and synthesizethe response segment by segment, the systemcould start to speak before the processing of theinput was complete.
However, if a system seg-ment was delayed for some reason, the systemgenerated a response based on the informationobtained so far or by generating a pause (filled orunfilled).
The system also employed self-repairswhen the system needed to revise an already re-alised speech segment.
Despite these disfluencies(filled pauses and self-repairs), an evaluation ofthe system showed that in comparison to a non-incremental version, the incremental version hada shorter response time and was perceived asmore efficient by the users.However, pauses do not only have to be aside-effect of processing delays.
Pauses couldalso be used wisely to chunk longer instructionsinto shorter segments, giving the user enough164time to process the information.
In this case, thesystem should instead invite user reactions dur-ing the course of its utterance.
In the currentwork, we investigate to what extent the systemcan use filled pauses, syntactic completeness andgaze as cues to either inhibit or encourage theuser to react when the system pauses.3 Human-robot Map Task dataMap Task is a well establish experimental para-digm for collecting data on human-human dia-logue [30].
Typically, an instruction-giver has amap with landmarks and a route, and is given thetask of describing this route to an instruction-follower, who has a similar map but without theroute drawn on it.
In a previous study, (Skantze,2012) we used this paradigm for collecting dataon how humans elicit feedback in human-computer dialogue.
In that study, the human wasthe instruction-giver.
In the current study, we usethe same paradigm for a human-robot dialogue,but here the robot is the instruction-giver and thehuman is the instruction-follower.
This has re-sulted in a rich multi-modal corpus of varioustypes of user reactions to the robot?s instructions,which vary across conditions.Figure 1: The experimental setup.3.1 A Map Task dialogue systemThe experimental setup is shown in Figure 1.The user is seated opposite to the robot headFurhat (Al Moubayed et al 2013), developed atKTH.
Furhat uses a facial animation model thatis back-projected on a static mask.
The head ismounted on a neck (with 3 degrees of freedom),which allows the robot to direct its gaze usingboth eye and head movements.
The dialogue sys-tem was implemented using the IrisTK frame-work developed at KTH (Skantze & Al Mou-bayed, 2012), which provides a set of modulesfor input and output, including control of Furhat(facial gestures, eye and head movements), aswell as a statechart-based authoring language forcontrolling the flow of the interaction.
Forspeech synthesis, we used the CereVoice unitselection synthesizer developed by CereProc(www.cereproc.com).Between the user and the robot lies a largemap printed on paper.
In addition, the user has adigital version of the map presented on a screenand is given the task to draw the route that therobot describes with a digital pen.
However, thelandmarks on the user?s screen are blurred andtherefore the user also needs to look at the largemap in order to identify the landmarks.
This mapthereby constitutes a target for joint attention.While the robot is describing the route, its gaze isdirected at the landmarks under discussion (onthe large map), which should help the user todisambiguate between landmarks.
In a previousstudy, we have shown that human subjects canidentify the target of Furhat's gaze with an accu-racy that is very close to that of observing a hu-man (Al Moubayed et al 2013).
At certain plac-es in the route descriptions, the robot also looksup at the user.
A typical interaction between therobot and a user is shown in Table 1.
As the ex-ample illustrates, each instruction is divided intotwo parts with a pause in between, which resultsin four phases per instruction: Part I, Pause, PartII and Release.
Whereas user responses are notmandatory in the Pause phase (the system willcontinue anyway after a short silence threshold,as in U.2), the Release requires a verbal re-sponse, after which the system will continue.
Wehave explored three different realisations ofpauses, which were systematically varied in theexperiment:COMPLETE: Pauses preceded by a syntacticallycomplete  phrase (R.5).INCOMPLETE: Pauses preceded by a syntactical-ly incomplete phrase (R.9).FILLED: Pauses preceded by a filled pause (R.1).The phrase before the filled pause was some-times incomplete and sometimes complete.To make the conditions comparable, the amountof information given before the pauses was bal-anced between conditions.
Thus, the incompletephrases still contained an important piece of in-formation and the pause was inserted in the be-ginning of the following phrase (as in R.9).165Table 1: An example interaction.Turn Activity PhaseR.1 [gazing at map] continue towards thelights, ehm...Part IU.2 [drawing] PauseR.3 until you stand south of the stoplights [gazing at user]Part IIU.4 [drawing] alright [gazing at robot] ReleaseR.5 [gaze at map] continue and pass eastof the lights...Part IU.6 okay [drawing] PauseR.7 ...on your way towards the tower[gaze at user]Part IIU.8 Could you take that again?
ReleaseR.9 [gaze at map] Continue to the largetower, you pass...Part IU.10 [drawing] PauseR.11 ...east of the stop lights [gaze at user] Part IIU.12 [drawing] okay, I am at the tower ReleaseFigure 2: An example map.Given the current limitations of conversationalspeech recognition, and lack of data relevant forthis task, we needed to employ some trick to beable to build a system that could engage in thistask in a convincing way in order to evoke natu-ral reactions from the user.
One possibility wouldbe to use a Wizard-of-Oz setup, but that wasdeemed to be infeasible for the time-critical be-haviour that is under investigation here.
Instead,we employed a trick similar to the one used in(Skantze, 2012).
Although the users are told thatthe robot cannot see their drawing behaviour, thedrawing on the digital map, together with a voiceactivity detector that detects the user?s verbalresponses, is actually used by the system to se-lect the next action.
An example of a map can beseen in Figure 2.
On the intended route (whichobviously is not shown on the user?s screen), anumber of hidden ?spots?
were defined ?
posi-tions relative to some landmark (e.g.
?east of thefield?).
Each instruction from the system wasintended to guide the user to the next hiddenspot.
Each map also contained an ambiguouslandmark reference (as ?the tower?
in the exam-ple).Pilot studies showed that there were threebasic kinds of verbal reactions from the user: (1)an acknowledgement of some sort, encouragingthe system to continue, (2) a request for repeti-tion, or (3) a statement that some misunderstand-ing had occurred.
By combining the length of theutterance with the information about the progres-sion of the drawing, these could be distinguishedin a fairly robust manner.
How this was done isshown in Table 2.
Notice that this scheme allowsfor both short and long acknowledgements (U.4,U.6 and U.12 in the example above), as well asclarification requests (U.8).
It also allows us toexplore misunderstandings, i.e.
cases where theuser thinks that she is at the right location andmakes a short acknowledgement, while she is infact moving in the wrong direction.
Such prob-lems are usually detected and repaired in the fol-lowing turns, when the system continues with theinstruction from the intended spot and the userobjects with a longer response.
This triggers thesystem to either RESTART the instruction from aprevious spot where the user is known to havebeen ("I think that we lost each other, could westart again from where you were at the busstop?
"), or to explicitly CHECK whether the useris at the intended location ("Are you at the busstop?
"), which helps the user to correct the path.Table 2: The system?s action selection based onthe user?s voice activity and drawing.UserresponseDrawing ActionShort/Long Continues to thenext spotCONTINUEShort/Long Still at the samespotREPHRASEShort (<1s.)
At the wrong spot CONTINUE (withmisunderstanding)Long (>1s.)
At the wrong spot RESTART or CHECKNo resp.
Any CHECK3.2 Experimental conditionsIn addition to the utterance-level conditions(concerning completeness) described above,three dialogue-level conditions were implement-ed:CONSISTENT gaze (FACE): The robot gazes atthe landmark that is currently being describedduring the phases Part I, Pause and Part II.
In166accordance with the findings in for exampleKendon (1967), the robot looks up at the endof phase Part II, seeking mutual gaze with theuser during the Release phase.RANDOM gaze (FACE): A random gaze behav-iour, where the robot randomly shifts betweenlooking at the map (at no particular landmark)and looking at the user, with an interval of 5-10 seconds.NOFACE: The robot head was hidden behind apaper board so that the user could not see it,only hear the voice.3.3 Data collection and analysisWe collected a corpus of 24 subjects interactingwith the system, 20 males and 4 females betweenthe ages of 21-47.
Although none of them werenative speakers, all of them had a high proficien-cy in English.
First, each subject completed atraining dialogue and then six dialogues thatwere used for the analysis.
For each dialogue,different maps were used.
The subjects were di-vided into three groups with 8 subjects in each:Group A: Three maps with the CONSISTENT(FACE) version and three maps with theNOFACE version.
All pauses were 1.5 s. long.Group B: Three maps with the RANDOM (FACE)version and three maps with the NOFACE ver-sion.
All pauses were 1.5 s. long.Group C: Three maps with the CONSISTENT ver-sion and three maps with the NOFACE ver-sion.
All pauses were 2-4 s. long (varied ran-domly with a uniform distribution).For all groups, the order between the FACEand the NOFACE condition was varied and bal-anced.
Group A and Group B allow us to exploredifferences between the CONSISTENT and RAN-DOM versions.
This is important, since it is notevident to what extent the mere presence of aface affects the interaction and to what extentdifferences are due to a consistent gazing behav-iour.
Group C was added to the data collectionsince we wanted to be able to study users' behav-iour during pauses in more detail.
Thus, Group Cwill only be used to study within-group effects ofdifferent pause types and will not be comparedagainst the other groups.After the subjects had interacted with the sys-tem, they filled out a questionnaire.
First, theywere requested to rate with which version (FACEor NOFACE) it was easier to complete the task.Second, the participants were requested to ratewhether the robot?s gaze was helpful or confus-ing when it came to task completion, landmarkidentification and the timing of feedback.
Allratings were done on a continuous horizontal linewith either FACE or ?the gaze was helpful?
onthe left end and NOFACE or ?the gaze was con-fusing?
on the right end.
The centre of the linewas labelled with ?no difference?.During the experiments, the users?
speech andface were recorded and all events in the systemand the drawing activity were automaticallylogged.
Afterwards, the users' voice activity thathad been automatically detected online wasmanually corrected and transcribed.
Using thevideo recordings, the users?
gaze was also manu-ally annotated, depending on whether the userwas looking at the map, the screen or at the ro-bot.In this study, we also wanted to explore thepossibility of measuring cognitive load in hu-man-robot interaction using EDA (electrodermalactivity).
Hence, in an explorative manner, weinvestigated how the realisation of the system?spauses and the presence of the face affected thecognitive costs of processing the system?s in-structions.
For measuring this, we used a weara-ble EDA device, which exerts a direct current onthe skin of the subject in order to measure skinconductance responses.
For these measurementsas well as the logging of the data the Q-Sensordeveloped by Affectiva 1  was used.
The meas-urements were taken from the fingertips of thesubjects.
The sampling rate was 8 Hz.
All postprocessing was carried out in Ledalab2.
We firstapplied the Butterworth filter and then carriedout a Continuous Decomposition Analysis.
Allskin conductance responses (SCR) with a mini-mum amplitude of 0.01 muS and a minimal dis-tance of 700ms were used for further analysis.Due to problems with the EDA device, we onlyhave data for six subjects in Group A, six inGroup B and none in Group C.4 ResultsAnalyses of the different measures used here re-vealed that they were not normally distributed.We have therefore consistently used non-parametric tests.
All tests of significance aredone using two-tailed tests at the .05 level.1http://www.affectiva.com/2http://www.ledalab.de/1674.1 Subjective ratingsThe questionnaire was used to analyse differ-ences in subjective ratings between Group A andB.
The marks on the horizontal continuous linesin the questionnaire were measured with a rulerbased on their distance from the midpoint (la-belled with ?no difference?)
and normalized to ascale between 0 and 1.
A Wilcoxon SignedRanks Test was carried out, using these rankingsas differences.
The results show that the Con-sistent version differed significantly from themidpoint (?no difference?)
in four dimensionswhereas there were no significant differencesfrom the midpoint for RANDOM version.
Morespecifically, Group A (CONSISTENT) (n=8) foundit easier to complete the task in the face condi-tion than in the no face condition (Mdn=0.88,Z=-2.54, p=.012).
The same group thought thatthe robot?s gaze was helpful rather than confus-ing when it came to task completion (Mdn=0.84,Z=-2.38, p=.017), landmark identification(Mdn=0.83, Z=-2.52, p=.012) and to decidewhen to give feedback (Mdn=0.66, Z=-1.99,p=.046).
The results of the questionnaire are pre-sented in Figure 3.Figure 3: The results from the questionnaire.
Thebars show the median rating for Group A (con-sistent) and Group B (random).4.2 Task completionApart from the subjective ratings, we also want-ed to see whether the face-to-face setting affect-ed task completion.
In order to explore this, weanalysed the time and number of utterances ittook for the users to complete the maps.
On av-erage, the dialogues in Group A (CONSISTENT)were 2.5 system utterances shorter and 8.9 sec-onds faster in the FACE condition than in theNOFACE condition.
For Group B (RANDOM), thedialogues were instead 2.3 system utterances and17.3 seconds longer in the FACE condition(Mann-Whitney U-test, p<.05).
Thus, it seemslike the face facilitates the solving of the task,and that this is not just due to the mere presenceof a face, but that the intelligent gaze behaviouractually contributes.
In fact, the RANDOM gazeworsens the performance, possibly because sub-jects spent time on trying to make sense of sig-nals that did not provide any useful information.Looking at more local phenomena, it seemslike there was also a noticeable difference whenit comes to miscommunication.
The dialogues inthe RANDOM/FACE condition had a total of 18system utterances of the type RESTART (vs. 7 inCONSISTENT), and a total of 33 CHECK utteranc-es (vs. 15 in CONSISTENT).
A chi-square testshows that the differences are statistically signif-icant (?2(1, N=25) = 4.8, p =.028; ?2(1, N=48) =6.75, p=.009).
This indicates that the users thatdid not get the CONSISTENT gaze to a larger ex-tent did not manage to follow the system?s in-structions, most likely because they did not getguidance from the robot?s gaze in disambiguat-ing referring expressions.4.3 Gaze behaviourIn order to analyse the users?
direction of atten-tion during the dialogues, the manual annotationof the participants?
gaze was analysed.
First, weexplored how the completion type of the robot'sutterance affected the users?
gaze.
In this analy-sis, FILLED and INCOMPLETE have been merged(since there was no difference in the users?
gazebetween these conditions).
The percentage ofgaze at the robot over the four different utterancephases for complete and incomplete utterances isplotted in Figure A in the Appendix.
Note thatthe different phases actually are of differentlengths depending on the actual content of theutterance and the length of the pause.
However,these lengths have been normalized in order tomake it possible to analyse the average user be-haviour.
For each phase, a Mann-Whitney U-testwas conducted.
The results show that the per-centage of gaze at Furhat during the mid-utterance pause is higher when the first part ofthe utterance is incomplete than when it is com-plete (U=7573.0, p<.001).
There were, however,no significant differences in gaze direction be-tween complete and incomplete utterance duringthe other three phases (p>.05).
This indicates thatusers gaze at the robot to elicit a continuation ofthe instruction when it is incomplete.Second, we wanted to explore if gaze directioncan be used as a cue of whether the user willprovide a verbal response in the pause or not.The percentage of gaze at the robot over the fourutterance phases for system utterances with and0 0.5 1RANDOM CONSISTENTDid the robot?s gaze help you tounderstand which landmark he wastalking about?
(0=confusing, 1=helpful)Did the robot?s gaze help you tocomplete the task?
(0=confusing,1=helpful)Did the robot?s gaze affect yourdecisions of when to give feedback?
(0=confusing, 1=helpful)When was it easier to completethe task?
(noFace=0, face=1)?No difference?168without user response in the pause is plotted inFigure B in the Appendix.
For each phase, aMann-Whitney U-test was conducted.
The re-sults show that the percentage of gaze at Furhatduring the mid-utterance pause (U=1945.5,p=.008) and Part II (U=2090.0, p=.008) of theutterance is lower when the user gives a verbalresponse compared to when there is no response.There were however no significant differences ingaze direction between complete and incompleteutterance during the other two phases (p>.05).4.4 Verbal feedback behaviourApart from the user?s gaze behaviour, we alsowanted to see whether syntactic completenessbefore pauses had an effect on whether the usersgave verbal responses in the pause.
Figure 4shows the extent to which users gave feedbackwithin pauses, depending on pause type andFACE/NOFACE condition.
As can be seen, COM-PLETE triggers more feedback, FILLED less feed-back and INCOMPLETE even less.
Interestingly,this difference is more distinct in the FACE con-dition (?2(2, N=157) = 10.32, p<.01).
In fact, thedifference is not significant in the NOFACE con-dition (p >.05).Figure 4: Presence of feedback depending onpause type (Group C).In Skantze et al(2013), we have also done amore thorough analysis of the verbal acknowl-edgements from the users.
The analysis showsthat the prosody and lexical choice in theseacknowledgements ("okay", "yes", "yeah","mm", "mhm", "ah", "alright" and "oh") to someextent signal whether the drawing activity isabout to be initiated or has been completed.
Theanalysis also shows how these parameters arecorrelated to the perception of uncertainty.4.5 Drawing behaviourWhereas gaze and verbal responses can be re-garded as communicative signals, the users weretold that the robot could not observe their draw-ing activity.
However, the drawing of the routecan be regarded as the purpose of the interactionand it is therefore important to understand howthis is affected by the system?s behaviour underdifferent conditions.
First, we wanted to see howthe completeness of the robot's utterance in com-bination with the presence of the face affectedthe drawing activity.
In this analysis, FILLED andINCOMPLETE have been merged (since there wasno clear difference).
The mean drawing activityover the four phases of the descriptions is plottedin Figure C in the Appendix.
For each phase, aKruskal-Wallis test was conducted showing thatthere is a significant difference between the con-ditions in the Pause phase (H(3) = 28.8, p<.001).Post-hoc tests showed that FACE/INCOMPLETEhas a lower drawing activity than the other con-ditions, and that NOFACE/INCOMPLETE has alower drawing activity than the COMPLETE con-dition.
Thus, INCOMPLETE phrases before pausesseem to have an inhibiting effect on the user?sdrawing activity in general, but this effect ap-pears to be much larger in the FACE condition.Second, we aimed to investigate to what ex-tent the robot?s gaze at landmarks during ambig-uous references helps users to discriminate be-tween landmarks.
The mean drawing activityover the four phases of the descriptions of am-biguous landmarks is plotted in Figure D in theAppendix.
For each phase, a Kruskal-Wallis testwas conducted showing that there is a significantdifference between the conditions in the Part IIphase (H(2)=10.2, p=.006).
Post-hoc testsshowed that CONSISTENT has a higher drawingactivity than the RANDOM and NOFACE condi-tions.
However, there is no such difference whenlooking at non-ambiguous descriptions.
Thisshows that robot?s gaze at the target landmarkduring ambiguous references makes it possiblefor the subjects to start to draw quicker.4.6 Cognitive loadAs mentioned above, we also wanted to study thecognitive costs of processing the system?s in-structions, as measured with a wearable EDAdevice.
For each system utterance part (Part I andPart II), we calculated the sum of the amplitudesof the skin conductance responses (SoSCR) dur-ing the following three seconds.
The SoSCR dur-ing the pause, depending on pause type areshown in Figure 5.
A Kruskal-Wallis test re-vealed that there is an overall effect (H(2)=8.7,p=.13), and post-hoc tests showed that there is asignificant difference between utterances whichare incomplete and those with filled pauses, indi-169cating that the syntactic incompleteness withouta filled pause leads to a higher cognitive load.We have no good explanation for this, and we donot know whether this is due to how the syntacti-cally incomplete segments were realised by thesynthesizer, or whether the same effect wouldappear in human-human interaction.Figure 5: EDA at different pause types (Group Aand B).A similar analysis was done after both Part I andPart II to see if there is any difference in SoSCRbetween ambiguous and non-ambiguous refer-ences in the different conditions, as shown in inFigure 6.
No such differences were found forGroup B, but for Group A, ambiguous referenceswere followed by a higher SoSCR in theNOFACE condition, indicating that the robot?sgaze helps in disambiguating the referring ex-pressions and reduces cognitive load (Mann-Whitney U-test; U = 6585, p = .001).Figure 6: EDA for Group A (CONSISTENT).5 Conclusions and DiscussionIn this study, we have investigated to what extentthe robot?s gaze can be used to: (1) help the userdisambiguate referring expressions to objects inthe shared visual scene, and (2) to either inhibitor encourage different types of user reactionswhile the system pauses.
The  results show  thatthe robot?s gaze behaviour  was  rated  as  help-ful  rather  than  confusing for  task completion,landmark  identification and feedback timing.These effects were not present when the robotused a random gaze behaviour.
The efficiency ofthe gaze was further supported by the time ittook to complete the task and the number of mis-understandings.
These results in combinationwith a faster drawing activity and lower cogni-tive load when system?s reference was ambigu-ous, suggest that the users indeed utilized thesystem?s gaze to discriminate between land-marks.The second purpose of this study was to inves-tigate to what extent filled pauses, syntacticcompleteness and gaze can be used as cues toeither inhibit or encourage the user to react inpauses.
First, the results show that pauses pre-ceded by incomplete syntactic segments or filledpauses appear to inhibit user activity.
Thus, ouranalyses of gaze and drawing activity show thatusers give less feedback, draw less and look atthe robot to a larger extent when the precedingsystem utterance segment is incomplete thanwhen it is complete.
An interesting observation isthat the inhibiting effect on drawing activity ap-pears to be more pronounced in the face-to-facecondition, which indicates that gaze also plays animportant role here (since the robot looked downat the map during the pauses).
Additionally, thereis less cognitive load when the silence is preced-ed by a filled pause.
These results suggest thatincomplete system utterances prevent furtheruser processing; instead the user waits for moreinput from the system before starting to carry outthe system?s instruction.
After complete utter-ance segments, however, there is more drawingactivity and the user looks less at the robot, sug-gesting that the user has already started to carryout the system?s instruction.The results presented in this study have impli-cations for generating multimodal behavioursincrementally in dialogue systems for human-robot interaction.
Such a system should be ableto generate speech and gaze intelligently in orderto inhibit or encourage the user to act, dependingon the state of the system's processing.
In futurestudies, we plan to extend our previous model ofincremental speech generation (Skantze &Hjalmarsson, 2012) with such capabilities.AcknowledgmentsGabriel Skantze is supported by the Swedish researchcouncil (VR) project Incremental processing in mul-timodal conversational systems (2011-6237).
AnnaHjalmarsson is supported by the Swedish ResearchCouncil (VR) project Classifying and deploying paus-es for flow control in conversational systems (2011-6152).
Catharine Oertel is supported by GetHomeSafe(EU 7th Framework STREP 288667).00.020.040.060.080.10.120.140.160.18Complete Incomplete FilledSoSCR(muS)170ReferencesAl Moubayed, S., Skantze, G., & Beskow, J.
(2013).The Furhat Back-Projected Humanoid Head - Lipreading, Gaze and Multiparty Interaction.
Interna-tional Journal of Humanoid Robotics, 10(1).Anderson, A., Bader, M., Bard, E., Boyle, E.,Doherty, G., Garrod, S., Isard, S., Kowtko, J.,McAllister, J., Miller, J., Sotillo, C., Thompson, H.,& Weinert, R. (1991).
The HCRC Map Task corpus.Language and Speech, 34(4), 351-366.Bohus, D., & Horvitz, E. (2010).
Facilitating multi-party dialog with gaze, gesture, and speech.
InProc ICMI?10.
Beijing, China.Boomer, D. S., & Dittmann, A. T. (1962).
Hesitationpauses and juncture pauses in speech.
Languageand Speech, 5, 215-222.Boucher, J. D., Pattacini, U., Lelong, A., Bailly, G.,Elisei, F., Fagel, S., Dominey, P. F., & Ventre-Dominey, J.
(2012).
I reach faster when I see youlook: gaze effects in human-human and human-robot face-to-face cooperation.
Frontiers in neuro-robotics, 6.Boyle, E., Anderson, A., & Newlands, A.
(1994).
Theeffects of visibility on dialogue and performancein a cooperative problem solving task.
Languageand speech, 37(1), 1-20.Brennan, S., & Williams, M. (1995).
The Feeling ofAnother's knowing: Prosody and Filled Pauses asCues to Listeners about the Metacognitive Statesof Speakers.
Journal of Memory and Language,34, 383-398.Buschmeier, H., Baumann, T., Dosch, B., Kopp, S., &Schlangen, D. (2012).
Combining incrementallanguage generation and incremental speech syn-thesis for adaptive information presentation.
InProceedings of SigDial (pp.
295?303).
Seoul,South Korea.Cassell, J.
(1999).
Nudge, nudge, wink, wink: Ele-ments of face-toface conversation for embodiedconversational agents.
In Cassell, J., Suillivan, J.,Prevost, S., & Churchill, E.
(Eds.
), EmbodiedConversational Agents.
Cambridge, MA: MITPress.Dethlefs, N., Hastie, H., Rieser, V., & Lemon, O.(2012).
Optimising Incremental Dialogue Deci-sions Using Information Density for InteractiveSystems.
In Proceedings of the Conference onEmpirical Methods in Natural Language Pro-cessing (EMNLP) (pp.
82-93).
Jeju, South Korea.Dohsaka, K., & Shimazu, A.
(1997).
System architec-ture for spoken utterance production in collabora-tive dialogue.
In Working Notes of IJCAI 1997Workshop on Collaboration, Cooperation andConflict in Dialogue Systems.Duncan, S. (1972).
Some Signals and Rules for Tak-ing Speaking Turns in Conversations.
Journal ofPersonality and Social Psychology, 23(2), 283-292.Goldman-Eisler, F. (1972).
Pauses, clauses, sentences.Language and Speech, 15, 103-113.Heldner, M., & Edlund, J.
(2010).
Pauses, gaps andoverlaps in conversations.
Journal of Phonetics,38, 555-568.Johnson-Roberson, M., Bohg, J., Skantze, G., Gus-tafson, J., Carlson, R., Rasolzadeh, B., & Kragic,D.
(2011).
Enhanced Visual Scene Understandingthrough Human-Robot Dialog.
In IEEE/RSJ Inter-national Conference on Intelligent Robots andSystems.Kendon, A.
(1967).
Some functions of gaze directionin social interaction.
Acta Psychologica, 26, 22-63.Kleinke, C. L. (1986).
Gaze and eye contact: a re-search review.
Psychological Bulletin, 100, 78-100.Mutlu, B., Forlizzi, J., & Hodgins, J.
(2006).
A story-telling robot: Modeling and evaluation of human-like gaze behavior.
In Proceedings of 6th IEEE-RAS International Conference on Humanoid Ro-bots (pp.
518-523).Nakano, Y., Reinstein, G., Stocky, T., & Cassell, J.(2003).
Towards a model of face-to-face ground-ing.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics (ACL2003) (pp.
553-561).Rochester, S. R. (1973).
The significance of Pauses inSpontaneous Speech.
Journal of PsycholinguisticResearch, 2(1).Schlangen, D., & Skantze, G. (2011).
A General, Ab-stract Model of Incremental Dialogue Processing.Dialogue & Discourse, 2(1), 83-111.Skantze, G., & Al Moubayed, S. (2012).
IrisTK: astatechart-based toolkit for multi-party face-to-face interaction.
In Proceedings of ICMI.
SantaMonica, CA.Skantze, G., & Hjalmarsson, A.
(2012).
Towards In-cremental Speech Generation in ConversationalSystems.
Computer Speech & Language, 27(1),243-262.Skantze, G., Oertel, C., & Hjalmarsson, A.
(2013).User feedback in human-robot interaction: Proso-dy, gaze and timing.
In Proceedings of Inter-speech.Skantze, G. (2012).
A Testbed for Examining theTiming of Feedback using a Map Task.
In Pro-ceedings of the Interdisciplinary Workshop onFeedback Behaviors in Dialog.
Portland, OR.Torres, O., Cassell, J., & prevost, S. (1997).
Modelinggaze behavior as a function of discourse structure.Proc.
of the First International Workshop on Hu-man-Computer Conversation.Vertegaal, R., Slagter, R., van der Veer, G., & Nijholt,A.
(2001).
Eye gaze patterns in conversations:there is more to conversational agents than meetsthe eyes.
In Proceedings of ACM Conf.
on HumanFactors in Computing Systems.Yoshikawa, Y., Shinozawa, K., Ishiguro, H., Hagita,N., & Miyamoto, T. (2006).
Responsive robotgaze to interaction partner.
In Proceedings of ro-botics: Science and systems.171AppendixPart I Pause Part II ReleaseFigure A: Average user gaze depending on pause type (Group C).Part I Pause Part II ReleaseFigure B: Average user gaze depending whether the user responds in the pause (Group A and B).Part I Pause Part II ReleaseFigure C: Average drawing activity depending on pause type and the presence of the face (Group C).Part I Pause Part II ReleaseFigure D: Average drawing activity during ambiguous references depending on condition (Group A and B).172
