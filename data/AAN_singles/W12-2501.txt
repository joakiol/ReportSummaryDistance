Workshop on Computational Linguistics for Literature, pages 1?7,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsComputational Analysis of Referring Expressions inNarratives of Picture BooksChoonkyu Lee Smaranda Muresan Karin StromswoldDepartment of Psychology Library and Information Science Department Department of PsychologyRutgers Center for Cognitive Science School of Communication and Information Rutgers Center for Cognitive ScienceRutgers University ?
New Brunswick Rutgers University ?
New Brunswick Rutgers University ?
New Brunswickchoonkyu@eden.rutgers.edu smuresan@rci.rutgers.edu karin@ruccs.rutgers.eduAbstractThis paper discusses successes and failures ofcomputational linguistics techniques in thestudy of how inter-event time intervals in astory affect the narrator?s use of differenttypes of referring expressions.
The successstory shows that a conditional frequency dis-tribution analysis of proper nouns and pro-nouns yields results that are consistent withour previous results ?
based on manual coding?
that the narrator?s choice of referring ex-pression depends on the amount of time thatelapsed between events in a story.
Unfortu-nately, the less successful story indicates thatstate-of-the-art coreference resolution systemsfail to achieve high accuracy for this genre ofdiscourse.
Fine-grained analyses of these fail-ures provide insight into the limitations of cur-rent coreference resolution systems, and waysof improving them.1 IntroductionIn theories of information structure in extendeddiscourse, various factors of discourse saliencehave been proposed as determinants of information?newness?
vs. ?givenness?
(e.g., Prince, 1981).Based on evidence from speakers?
choice of differ-ent types of referring expressions in referring backto a previously introduced discourse referent,scholars have discovered effects of (a) ?referentialdistance?
(Giv?n, 1992), a text-based measure ofdistance between the antecedent and the re-mention in terms of number of intervening clauses;(b) topic-prominence of the referent in the previousmention (Brennan, 1995); (c) presence of anothercandidate referent (?competitor?)
in linguistic orvisual context (Arnold and Griffin, 2007), amongothers.
In re-mentioning individuals, one can, forexample, simply repeat names or use anaphoricdevices, such as definite descriptions and pronouns.In our work, we have been investigating the roleof mental representation of nonlinguistic situation-al dimensions of the storyline (e.g., Zwaan, 1999)as an additional factor of salience in discourse or-ganization.
From the five situational dimensions ofthe event-indexing model (Zwaan and Radvansky,1998), we have focused on the time dimension.
Ina narrative elicitation study (Lee and Stromswold,submitted; Lee, 2012), we presented picture se-quences from three wordless picture books in Mer-cer Mayer?s ?Boy, Dog, Frog series?
(Mayer,1969; Mayer, 1974; Mayer and Mayer, 1975), andhad 8 adults estimate the inter-event intervals instory time between consecutive scenes with no lin-guistic stimuli, and had a different group of nativeEnglish-speaking adults write stories to go alongwith the pictures.
The 36 adults wrote a total of 58written narratives, which consisted of 2778 sen-tences and 38936 word tokens (48 sentences and671 word tokens per narrative on average).
The useof wordless picture books allows fixed target con-tent and clear visual availability of the charactersand their actions.In our previous analysis (Lee and Stromswold,submitted) of the effect of inter-event time inter-vals on the narrator?s referential choice in referring1S1) Finally though, the boy starts to get tired and de-cides to crawl into bed.
His dog joins him and soon theyare asleep.
The boy forgot to put a lid on the bottle, andMr.
Frog is sneaking out!S2) When the boy wakes up in the morning, he sees thatMr.
Frog is gone.
He is very upset that he lost his newfriend.Figure 1.
Sample ?Long Interval?
Between Scenes S1and S2 (Mean Estimate: 6h 48m 45s).back to characters, we manually annotated criticalsentences selected on the basis of the eight longest(mean duration = 1 hour 7 minutes 2 seconds;henceforth, ?Long Intervals?)
and the eight shortest(mean duration = 10 seconds; henceforth, ?ShortIntervals?)
estimated intervals.
Examples of a LongInterval and a Short Interval between scenes aregiven in Figures 1 and 2, together with sample cor-responding narratives.
For each of the 58 narratives,we analyzed the first sentence after a Long andShort Interval.
Our coding of referring expressionsinvolved frequency counts (ranging from 0 to 3) ofinstances of each of our Referential Types ?
ProperNames (e.g., Mr. Frog), Definite Descriptions (e.g.,the frog), and Pronouns (e.g., he) ?
per critical sen-tence.
We found a significant interaction betweenInterval and Referential Type in both a chi-squaretest of association and an analysis of variance, andthe effect generally held across participants.
Ourfinding demonstrated that narrators used ProperNames more after Long Intervals than after ShortIntervals in story time, and more singular-referentPronouns after Short Intervals than after Long In-tervals.Addressing the issue of the effect of inter-eventinterval on referential choice on a larger scale re-quires accurate automatic methods for identifica-tion of Referential Types and coreferenceresolution for the narratives.
In this paper we firstpresent a simple computational method for analyz-ing the entire scene descriptions after the Long andS3) After staring at the frog for two minutes he says"Ribbittttttt" and she screams and  throws her fork intothe air, and falls back in her chair.
Charles gets scaredby her screaming and jumps off her plate into the air.S4) Luckily, he lands safely into a man's drink.
He ismid-conversation with a beautiful lady and doesn't feelthe new addition to his martini.Figure 2.
Sample ?Short Interval?
Between Scenes S3and S4 (Mean Estimate: 3s).Short Intervals to study how inter-event intervalsaffect referential choice, focusing on Proper Nounsand Pronouns.
Our results from the automaticmethods are consistent with the results obtainedusing manual coding of the critical sentences.
Se-cond, we present an annotation study of nine narra-tives with coreference chains, and also discuss theperformance of two state-of-the-art coreferenceresolution systems on a sample of our data.2 Inter-event Interval Effect on ReferringExpressions: A Basic ComputationalApproachIn order to address the question of how inter-eventintervals affect the choice of referring expressions,we analyzed the frequency of Pronouns and ProperNouns in scenes following the Long and Short In-tervals.
The results in Table 1 are consistent withour previous results obtained based on manual cod-ing of the critical sentences only: The ?Long Inter-val?
(LI) scenes and the ?Short Interval?
(SI) scenesdiverge in relative frequencies of our target part-of-speech tags ?
Pronouns (nominal (PRP) andpossessive (PRP$) forms) vs.
Proper Names (NNP).One can observe that there are generally higherfrequencies of Proper Names for the scenes afterthe Long Intervals compared to the Short Intervals,not only in absolute number but in relative propor-tion to Pronouns as well.
A noticeable exception,Scene 3 of One Frog Too Many (Mayer and2Book Scene# PRP PRP$ NNPFrogGoestoDinner4 (LI) 62 56 1065 (LI) 54 37 9621 (LI) 87 60 1209 (SI) 45 22 2713 (SI) 50 44 5014 (SI) 40 21 40OneFrogTooMany8 (LI) 33 33 5519 (LI) 63 42 9020 (LI) 60 29 883 (SI) 70 65 15815 (SI) 69 50 7323 (SI) 1 2 2Frog,WhereAreYou?2 (LI) 89 70 1433 (LI) 70 65 15818 (SI) 64 56 8619 (SI) 63 42 90Table 1.
Scene-based Frequencies of Pronouns andProper Names after the 16 Long and Short Intervals.Mayer, 1975), is a very early scene in the picturebook, with many character introductions and dis-course-newness (Prince, 1981).
Even with this ex-ception included, the association between Interval(Long vs. Short) and Referential Type (Pronounsvs.
Proper Names) was significant in a new analy-sis based on the entire scene descriptions, ratherthan just the first sentences for these scenes [?2(1)= 9.50, p = .0021].
The significant effect of Inter-val reveals that Proper Names were more common-ly used after Long Intervals than after ShortIntervals, and Pronouns were more commonly usedafter Short Intervals than after Long Intervals.The exception in Scene 3 of One Frog TooMany suggests, however, that excluding first fewmentions in a coreference chain from analysis mayreveal a stronger effect of Interval on referentialtype of re-mentions (although one mention for in-troducing a character does not always establishdiscourse-givenness from the narrator?s perspec-tive (Clancy, 1980)).
Successful automaticcoreference resolution would facilitate this analysisas well.3 Annotation of Referring Expressions inNarratives of Picture BooksIn order to provide descriptive statistics of refer-ring expressions in our narratives of pictures booksand to test the performance of coreference systemsautomatically in the future, we annotated 9 narra-tives manually with coreference chains (3 narra-tives for each of the 3 pictures books, with eachnarrative written by a different writer).
Only ani-mate entities, or characters in the stories, were con-sidered.
We used the MMAX2 annotation tool(M?ller and Strube, 2006).
A coreference schemais available from the Heidelberg Text Corpus(HTC, Malaka and Zipf, 2000) sample directoryincluded in the MMAX2 package.
The HTC sche-ma allows marking a mention in terms of the dis-course entity or coreference chain it corresponds to,as well as ?np_form?
(what type of (pro)nominal itis), ?grammatical_role?
(subject/object/other) and?semantic_class?
(abstract/human/physical ob-ject/other).
We imported the HTC schema to anno-tate the mention level in terms of coreference, andalso created a ?scene?
level for our picture-booknarratives.The narratives were annotated by the authors ofthis paper independently in the initial version, andwith adjudication for the final version.
As the ref-erents were very clear in the narratives for the pic-ture books, there was only one case of initialdisagreement in the authors?
coreference decisions.Table 2 shows statistics related to these 9 narra-tives.Table 2.
Descriptive Statistics for Each Narrative.The density of referring expressions is very high(~22% of tokens/words in a story are referring ex-pressions).
Densities are also consistent across nar-ratives: Narrative #7, which was by far the longestone with 1109 words, also showed a very highdensity (24%).
Numbers of coreference chains arealso consistent within each target picture book re-gardless of writer or narrative length: 8, 5, and 7for One Frog Too Many (Mayer and Mayer, 1975);13, 12, and 11 for Frog, Where Are You?
(Mayer,1969); and 23, 21, and 26 for Frog Goes to Dinner(Mayer, 1974).
Table 2 also shows that the longest3chain contains 60 mentions, and the average chainhas about 8 mentions.4 Performance of Coreference ResolutionSystems on Narratives of Picture BooksIn computational linguistics, the increasing availa-bility of annotated coreference corpora has led todevelopments in machine learning approaches toautomatic coreference resolution (see Ng, 2010).The task of automatic NP coreference resolution isto determine ?which NPs in a text [?]
refer to thesame real-world entity?
(Ng, 2010, p. 1396).
Suc-cessful coreference resolution often requires real-world knowledge of public figures, entity relation-ships, and aliases, beyond linguistic parameterssuch as number and gender features.In this paper, we have chosen two coreferenceresolution systems: Stanford?s Multi-Pass SieveCoreference Resolution System (Lee et al, 2011)(henceforth, Stanford dcoref) and ARKref(O?Connor and Heilman, 2011).
Stanford dcorefconsists of an initial mention-detection module, themain coreference resolution module, and task-specific post-processing.
In this system, global in-formation about the text is shared across mentionsin the same cluster in the form of attributes such asgender and number.
This system received the high-est scores at a recent CoNLL shared task (Pradhanet al, 2011), which the authors attributed to theinitial high-recall component (in mention detec-tion) followed by high-precision classifiers in thecoreference resolution sieves.
ARKref is a syntac-tically rich, rule-based within-documentcoreference system very similar to (the syntacticcomponents of) Haghighi and Klein (2009).We analyzed in depth the performance of thesesystems on one of our narratives for Frog Goes toDinner (Mayer, 1974).
We expected automaticcoreference resolution systems to show poorer per-formance when applied to our written narrativesthan that reported in the literature, because most ofthese systems have been trained on newswire, blog,or conversation corpora, which ?
though quite aheterogeneous set in themselves ?
are not similarto our written narrative data.
Some of the mostnoteworthy particularities of our written narrativecollection include (a) fictional content, in whichanimals occur frequently and are greatly anthro-pomorphized, (b) an imaginary target audience of alimited age range (six- to eight-year-olds), and (c)clear scene-by-scene demarcation in the writingprocess, with a new text input box for each newscene in a picture book.
The first point, in particu-lar, may limit the utility of named entity recogni-tion (NER) and WordNet relations amongnominals in the preprocessing steps prior tocoreference resolution.
As we discuss below, pre-processing errors in parsing and NER did in factcontribute to coreference precision errors.Our written narratives had a lot of singletonmentions for secondary characters and plural com-binations of characters.
We thus evaluated the per-formance based on the B3 measure proposed byBagga and Baldwin (1998), rather than the link-based MUC (Vilain et al, 1995).We computed the B3 with equal weighting forall mentions.
Stanford dcoref achieved B3 scores of0.78 Precision, 0.43 Recall and 0.55 F1, whileARKref scores were 0.67 for precision, 0.45 forrecall, and 0.54 for F1.
Stanford dcoref includes apost-processing module in which singletons areremoved, which partially contributes to the lowrecall score for the system.4.1 Qualitative analysis of coreference outputIn this section, we discuss the errors from bothARKref and Stanford dcoref in depth.
Thecoreference outputs from both ARKref and Stan-ford dcoref demonstrate that preprocessing errorscan lead to errors downstream for coreferenceresolution.
Misparsing is one of the serious issues.For example, in ARKref?s output for our samplenarrative (for Frog Goes to Dinner), the third-person singular verb waves in Billy waves goodbye(Scene 6) and Froggy waves goodbye (Scene 7)was misparsed as a plural nominal and thus aheadword of a mention for a discourse entity, andthese two instances were marked as coreferent.
Leeet al also acknowledged misparsing as a majorproblem for Stanford dcoref.A few surprising errors in the ARKref output in-clude (a) marking the woman and him in the sameclause as coreferent despite the gender mismatch,and (b) leaving the lady as a singleton and startinga new coreference chain for her in the same clause.It is strange that the explicitly anaphoric pronounmention did not lead ARKref to link it to the iden-tified mention the lady.Other noteworthy errors common to both sys-tems?
outputs were the following:4(1) inconsistent mention detection andcoreference resolution for mentions of the frogcharacter with Froggy;(2) failure to recognize cataphora in Withoutknowing Froggy?s in [his]i saxophone, [the saxo-phone player]i tries to blow harder?
and linkingthe pronoun to Froggy instead;(3) starting a new coreference chain at Scene 4at the mention of Billy when the referent (the boy)has been already introduced as Billy Smith in Scene1;(4) the same type of error for another character(the frog) at an indefinite NP a frog in She is soshocked that there is a frog in her salad.With regard to error (1), preprocessing results inthe Stanford dcoref output reveal some NER errorsin which Froggy was mislabeled as an ?organiza-tion,?
which, along with the absence of Froggy inthe name gazetteer for the system (Lee et al, 2011),would lead to both precision and recall errors forFroggy, as we observed.Error (3) reveals the potential pitfall of overreli-ance on headwords for mention/discourse-new de-tection, which leads these systems to miss theinternal structure to people?s names ?
namely,[first name + last name] for the same person, 1which then can be re-mentioned using just the firstname.
Although in news articles and other formalwriting it is typical to mention a person by the lastname (e.g., Obama rather than Barack) as long asthe referent is clear, stories, conversations, andother less formal genres would make more fre-quent use of first names of individuals for re-mention compared to other genres.
Because theimportance of coreference resolution is not limitedto formal writing, coreference resolution systemsneed to incorporate name-specific knowledge, ei-ther in preprocessing stages such as parsing andNER or in coreference resolution after the prepro-cessing.Error (4) is not as undesirable as the other ones:Even for a human annotator, it is more difficult tomake a coreference decision for a case like this one,in which the fact that the salad-eating lady wasshocked would come about similarly for any frog,not just Froggy.
Although there does not seem tobe a rule for classifying an indefinite NP as denot-1 Application to East Asian languages would need to adjust tothe opposite ?family name + given name?
sequence, often evenin English transliteration (e.g., Kim Jong-il).ing a new entity,2 training on a large corpus wouldlead to such a tendency because indefinites usuallydo indicate discourse-newness introducing a newdiscourse referent.In another narrative for the same picture book,there were two definite NPs (the woman and thewaiter) for which the definiteness was due to thevisual availability of the referent in the scene or abridging inference (restaurant ?
waiter) rather thana previous mention.
Definiteness may leadcoreference systems to prefer assigning the men-tion in question to an existing coreference chainrather than creating a new chain, but ARKref pro-cessed both of these possibly misleading definiteNPs successfully by creating a new coreferencechain, and Stanford dcoref got one right and madea recall error for the other.
On the other hand, re-ferring to different secondary male characters simi-larly as the man did lead to a spurious coreferencechain linking all of these mentions.5 Conclusion and Future DirectionsWith the NLP tools discussed above, possibilitiesabound for interesting research on narratives.Based on scene-based segmentation of narrativeswritten for fixed target picture sequences, one cancollect various kinds of linguistic and nonlinguisticdata associated with the picture sequences andconduct regression analysis to see which factor hasthe most predictive value for linguistic variationsuch as Referential Type choice.
Important factorsinclude temporal and thematic (dis)continuity inthe target content (McCoy and Strube, 1999; Vonket al, 1992), and discourse salience factors (Prince,1981), for which we have collected measures inour previous work.Our Interval Effect finding lends support toMcCoy and Strube?s (1999) intuition underlyingtheir referring-expression generation system, forwhich they used reference time change in dis-course as a major predictor of referential type.Gaining further insight into the impact of timechange in content on referential choice in naturallyoccurring discourse can thus lead to a predictivemodel of referring expressions as well.In the future, we plan to use ?semantic_class?
at-tributes and features such as ANIMACY in the2 According to Lee et al (2011), Stanford dcoref correctlyrecognizes coreference in appositive constructions with anindefinite NP after the first mention.5HTC schema as our task-specific filters for select-ing just story characters.
Moreover, we plan to ex-plore other state-of-the-art coreference systemssuch as CherryPicker (Rahman and Ng, 2009).
TheNLP tools and techniques discussed above can beapplied to cross-document coreference resolutionas well (see Bagga and Baldwin, 1998, for discus-sion of a meta document), although training thesystems for narratives like ours would involvemuch more manual annotation and supervision,particularly because different authors usually as-sign different names to a given character.
In orderto limit the amount of manual annotation, unsuper-vised methods for coreference resolution (Ng,2008; Poon and Domingos, 2008; Haghighi andKlein, 2007) could be used.
This, however, wouldrequire a larger number of picture books and hu-man-produced narratives.Coreference is far from a simple phenomenon,both for theory and application.
Nevertheless, ul-timately it would be desirable to improve the au-tomatic coreference resolution systems in waysthat reflect corpus-linguistic and psycholinguisticfindings ?
e.g., referential distance effects (Giv?n,1992), and the privileged status in memory of dis-course entities in the immediately preceding clause(Clark and Sengul, 1979).
The goal would be torepresent as many of the interacting factors in ref-erential choice as possible, with a weightingscheme or a ranking algorithm sensitive to thesemultiple factors.ReferencesJennifer E. Arnold and Zenzi M. Griffin.
2007.
The ef-fect of additional characters on choice of referringexpression: Everyone counts.
Journal of Memory andLanguage, 56: 521-536.Amit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In Proceedings of LRECWorkshop on Linguistic Coreference, pages 563-566.Susan Brennan.
1995.
Centering attention in discourse.Language and Cognitive Processes, 10: 137-167.Patricia M. Clancy.
1980.
Referential choice in Englishand Japanese narrative discourse.
In Wallace L.Chafe, editor, The Pear Stories: Cognitive, Cultural,and Linguistic Aspects of Narrative Production.Ablex, Norwood, NJ.Herbert H. Clark and C. J. Sengul.
1979.
In search ofreferents for nouns and pronouns.
Memory and Cog-nition, 7(1): 35-41.Thomas Giv?n.
1992.
The grammar of referential co-herence as mental processing instructions.
Linguistics,30:5-55.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric Bayesianmodel.
In Proceedings of ACL 2007, pages 848?855.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In Proceedings of EMNLP 2009, pages 1152?1161.Choonkyu Lee.
2012.
Situation model and salience.
TheLSA 2012 Special Session on Information Structureand Discourse: In Memory of Ellen F. Prince.
Port-land, Oregon.Choonkyu Lee and Karin Stromswold.
submitted.
Situa-tion model and accessibility: Referring expressions innarrative production.Heeyoung Lee, Yves Peirsman, Angel Chang, Nathan-ael Chambers, Mihai Surdeanu, and Dan Jurafsky.2011.
Stanford?s multi-pass sieve coreference resolu-tion system at the CoNLL-2011 Shared Task.
In Pro-ceedings of the CoNLL-2011 Shared Task, pages 28-34.Rainer Malaka and Alexander Zipf.
2000.
Deep Map:Challenging IT research in the framework of a touristinformation system.
In Daniel R. Fesenmaier, StefanKlein, and Dimitrios Buhalis, editors, Informationand Communication Technologies in Tourism 2000:Proceedings of the International Conference in Bar-celona, Spain, pages 15-27.
Springer, Wien.Mercer Mayer.
1969.
Frog, Where Are You?
PenguinBooks, New York.Mercer Mayer.
1974.
Frog Goes to Dinner.
PenguinBooks, New York.Mercer Mayer and Marianna Mayer.
1975.
One FrogToo Many.
Penguin Books, New York.Kathleen F. McCoy and Michael Strube.
1999.
Takingtime to structure discourse: Pronoun generation be-yond accessibility.
In Proceedings of the Twenty-First Annual Conference of the Cognitive ScienceSociety, pages 378-383.
Lawrence Erlbaum Associ-ates, Mahwah, NJ.Christoph M?ller and Michael Strube.
2006.
Multi-levelannotation of linguistic data with MMAX2.
In SabineBraun, Kurt Kohn, and Joybrato Mukherjee, editors,Corpus Technology and Language Pedagogy.
NewResources, New Tools, New Methods, pages 197-214.Peter Lang, Frankfurt.Vincent Ng.
2009.
Unsupervised models for coreferenceresolution.
In Proceedings of EMNLP 2008, pages640-649.Vincent Ng.
2010.
Supervised noun phrase coreferenceresearch: The first fifteen years.
In Proceedings ofACL 2010, pages 1396-1411.Brendan O?Connor and Michael Heilman.
2011.ARKref is a Noun Phrase Coreference System.
Web-site at http://www.ark.cs.cmu.edu/ARKref/6Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with Markov logic.In Proceedings of EMNLP 2008, pages 650-659.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 Shared Task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofCoNLL 2011.Ellen Prince.
1981.
Toward a taxonomy of given-newinformation.
In Peter Cole, editor, Radical Pragmat-ics, pages 223-256.
Academic Press, New York.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings ofEMNLP 2009, pages 968-977.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsof the 6th Message Understanding Conference, pages45-52.Wietske Vonk, Lettica G. M. M. Hustinx, and Wim H.G.
Simons.
1992.
The use of referential expressionsin structuring discourse.
Language and CognitiveProcesses, 7(3/4): 301-333.Rolf A. Zwaan.
1999.
Situation models: The mentalleap into imagined worlds.
Current Directions inPsychological Science, 8(1):15-18.Rolf A. Zwaan and Gabriel A. Radvansky.
1998.
Situa-tion models in language comprehension and memory.Psychological Bulletin, 123(2):162-185.7
