Statistical Dependency Parsing of TurkishGu?ls?en Eryig?itDepartment of Computer EngineeringIstanbul Technical UniversityIstanbul, 34469, Turkeygulsen@cs.itu.edu.trKemal OflazerFaculty of Engineering and Natural SciencesSabanci UniversityIstanbul, 34956, Turkeyoflazer@sabanciuniv.eduAbstractThis paper presents results from the firststatistical dependency parser for Turkish.Turkish is a free-constituent order lan-guage with complex agglutinative inflec-tional and derivational morphology andpresents interesting challenges for statisti-cal parsing, as in general, dependency re-lations are between ?portions?
of words?
called inflectional groups.
We haveexplored statistical models that use dif-ferent representational units for parsing.We have used the Turkish DependencyTreebank to train and test our parserbut have limited this initial explorationto that subset of the treebank sentenceswith only left-to-right non-crossing depen-dency links.
Our results indicate that thebest accuracy in terms of the dependencyrelations between inflectional groups isobtained when we use inflectional groupsas units in parsing, and when contextsaround the dependent are employed.1 IntroductionThe availability of treebanks of various sorts havefostered the development of statistical parserstrained with the structural data in these tree-banks.
With the emergence of the important roleof word-to-word relations in parsing (Charniak,2000; Collins, 1996), dependency grammars havegained a certain popularity; e.g., Yamada and Mat-sumoto (2003) for English, Kudo and Matsumoto(2000; 2002), Sekine et al (2000) for Japanese,Chung and Rim (2004) for Korean, Nivre et al(2004) for Swedish, Nivre and Nilsson (2005) forCzech, among others.Dependency grammars represent the structureof the sentences by positing binary dependencyrelations between words.
For instance, Figure 1Figure 1: Dependency Relations for a Turkish andan English sentenceshows the dependency graph of a Turkish andan English sentence where dependency labels areshown annotating the arcs which extend from de-pendents to heads.Parsers employing CFG-backbones have beenfound to be less effective for free-constituent-order languages where constituents can easilychange their position in the sentence withoutmodifying the general meaning of the sentence.Collins et al (1999) applied the parser of Collins(1997) developed for English, to Czech, and foundthat the performance was substantially lower whencompared to the results for English.2 TurkishTurkish is an agglutinative language where a se-quence of inflectional and derivational morphemesget affixed to a root (Oflazer, 1994).
At the syntaxlevel, the unmarked constituent order is SOV, butconstituent order may vary freely as demanded bythe discourse context.
Essentially all constituentorders are possible, especially at the main sen-tence level, with very minimal formal constraints.In written text however, the unmarked order isdominant at both the main sentence and embeddedclause level.Turkish morphotactics is quite complicated: agiven word form may involve multiple derivationsand the number of word forms one can generatefrom a nominal or verbal root is theoretically in-finite.
Derivations in Turkish are very produc-tive, and the syntactic relations that a word is in-89volved in as a dependent or head element, are de-termined by the inflectional properties of the oneor more (possibly intermediate) derived forms.
Inthis work, we assume that a Turkish word is rep-resented as a sequence of inflectional groups (IGshereafter), separated by ?DBs, denoting derivationboundaries, in the following general form:root+IG1 + ?DB+IG2 + ?DB+?
?
?
+ ?DB+IGn.Here each IGi denotes relevant inflectional fea-tures including the part-of-speech for the root andfor any of the derived forms.
For instance, the de-rived modifier sag?lamlas?t?rd?g?
?m?zdaki1would be represented as:2sag?lam(strong)+Adj+?DB+Verb+Become+?DB+Verb+Caus+Pos+?DB+Noun+PastPart+A3sg+P3sg+Loc+?DB+Adj+RelThe five IGs in this are the feature sequences sep-arated by the ?DB marker.
The first IG shows thepart-of-speech for the root which is its only inflec-tional feature.
The second IG indicates a deriva-tion into a verb whose semantics is ?to become?the preceding adjective.
The third IG indicatesthat a causative verb with positive polarity is de-rived from the previous verb.
The fourth IG in-dicates the derivation of a nominal form, a pastparticiple, with +Noun as the part-of-speech and+PastPart, as the minor part-of-speech, withsome additional inflectional features.
Finally, thefifth IG indicates a derivation into a relativizer ad-jective.A sentence would then be represented as a se-quence of the IGs making up the words.
When aword is considered as a sequence of IGs, linguis-tically, the last IG of a word determines its roleas a dependent, so, syntactic relation links onlyemanate from the last IG of a (dependent) word,and land on one of the IGs of a (head) word onthe right (with minor exceptions), as exemplifiedin Figure 2.
And again with minor exceptions, thedependency links between the IGs, when drawnabove the IG sequence, do not cross.3 Figure 3from Oflazer (2003) shows a dependency tree fora Turkish sentence laid on top of the words seg-mented along IG boundaries.With this view in mind, the dependency rela-tions that are to be extracted by a parser should berelations between certain inflectional groups and1Literally, ?
(the thing existing) at the time we caused(something) to become strong?.2The morphological features other than the obvious part-of-speech features are: +Become: become verb, +Caus:causative verb, +PastPart: Derived past participle,+P3sg: 3sg possessive agreement, +A3sg: 3sg number-person agreement, +Loc: Locative case, +Pos: Positive Po-larity, +Rel: Relativizing Modifier.3Only 2.5% of the dependencies in the Turkish treebank(Oflazer et al, 2003) actually cross another dependency link.Figure 2: Dependency Links and IGsnot orthographic words.
Since only the word-final inflectional groups have out-going depen-dency links to a head, there will be IGs which donot have any outgoing links (e.g., the first IG of theword bu?yu?mesi in Figure 3).
We assume that suchIGs are implicitly linked to the next IG, but nei-ther represent nor extract such relationships withthe parser, as it is the task of the morphologicalanalyzer to extract those.
Thus the parsing mod-els that we will present in subsequent sections allaim to extract these surface relations between therelevant IGs, and in line with this, we will employperformance measures based on IGs and their re-lationships, and not on orthographic words.We use a model of sentence structure as de-picted in Figure 4.
In this figure, the top part repre-sents the words in a sentence.
After morphologicalanalysis and morphological disambiguation, eachword is represented with (the sequence of) its in-flectional groups, shown in the middle of the fig-ure.
The inflectional groups are then reindexedso that they are the ?units?
for the purposes ofparsing.
The inflectional groups marked with ?are those from which a dependency link will em-anate from, to a head-word to the right.
Pleasenote that the number of such marked inflectionalgroups is the same as the number of words in thesentence, and all of such IGs, (except one corre-sponding to the distinguished head of the sentencewhich will not have any links), will have outgoingdependency links.In the rest of this paper, we first give a very briefoverview a general model of statistical depen-dency parsing and then introduce three models fordependency parsing of Turkish.
We then presentour results for these models and for some addi-tional experiments for the best performing model.We then close with a discussion on the results,analysis of the errors the parser makes, and con-clusions.3 ParserStatistical dependency parsers first compute theprobabilities of the unit-to-unit dependencies, andthen find the most probable dependency tree T ?among the set of possible dependency trees.
This90Bu eski ev+de +ki g?l+?n b?yle b?y?
+me+si herkes+i ?ok etkile+diModDetModSubjModSubjObjModbu+Deteski+Adjev+Noun+A3sg+Pnon+Loc+Adj g?l+Noun+A3sg+Pnon+Genb?yle+Advb?y?+Verb+Noun+Inf+A3sg+P3sg+Nomherkes+Pron+A3pl+Pnon+Acc?ok+Advetkile+Verb+Past+A3sgThis               old             house-at+that-is         rose's            such                     grow +ing              everyone        very      impressedSuch growing of the rose in this old house impressed everyone very much.+?s indicate morpheme boundaries.
The rounded rectangles show the words while the inflectional groups withinthe words that have more than 1 IG are emphasized with the dashed rounded rectangles.
The inflectional featuresof each inflectional group as produced by the morphological analyzer are listed below.Figure 3: Dependency links in an example Turkish sentence.w1  ##IG1IG2?
?
?
IG?g1IG1 IG2 ?
?
?
IG?g1w2  $$IG1IG2 ?
?
?
IG?g2IGg1+1 ?
?
?
IG?g1+g2.
.
.. .
.wn  ##IG1 IG2 ?
?
?
IG?gn?
?
?
IG?
?n?i =Pik=1 gkFigure 4: Sentence Structurecan be formulated asT ?
= argmaxTP (T, S)= argmaxTn?1?i=1P (dep (wi, wH(i)) |S)(1)where in our case S is a sequence of units (words,IGs) and T , ranges over possible dependencytrees consisting of left-to-right dependency linksdep (wi, wH(i)) with wH(i) denoting the head unitto which the dependent unit, wi, is linked to.The distance between the dependent units playsan important role in the computation of the depen-dency probabilities.
Collins (1996) employs thisdistance ?i,H(i) in the computation of word-to-word dependency probabilitiesP (dep (wi, wH(i)) |S) ?
(2)P (link(wi, wH(i)) |?i,H(i))suggesting that distance is a crucial variable whendeciding whether two words are related, alongwith other features such as intervening punctua-tion.
Chung and Rim (2004) propose a differentmethod and introduce a new probability factor thattakes into account the distance between the depen-dent and the head.
The model in equation 3 takesinto account the contexts that the dependent andhead reside in and the distance between the headand the dependent.P (dep (wi, wH(i)) |S) ?
(3)P (link(wi, wH(i))) |?i ?H(i)) ?P (wi links to some headH(i) ?
i away|?i)Here ?i represents the context around the depen-dent wi and ?H(i), represents the context aroundthe head word.
P (dep (wi, wH(i)) |S) is the prob-ability of the directed dependency relation be-tween wi and wH(i) in the current sentence, whileP (link(wi, wH(i)) |?i ?H(i)) is the probability ofseeing a similar dependency (with wi as the depen-dent, wH(i) as the head in a similar context) in thetraining treebank.For the parsing models that will be describedbelow, the relevant statistical parameters neededhave been estimated from the Turkish treebank(Oflazer et al, 2003).
Since this treebank is rel-atively smaller than the available treebanks forother languages (e.g., Penn Treebank), we have91opted to model the bigram linkage probabilitiesin an unlexicalized manner (that is, by just takingcertain morphosyntactic properties into account),to avoid, to the extent possible, the data sparsenessproblem which is especially acute for Turkish.
Wehave also been encouraged by the success of theunlexicalized parsers reported recently (Klein andManning, 2003; Chung and Rim, 2004).For parsing, we use a version of the BackwardBeam Search Algorithm (Sekine et al, 2000) de-veloped for Japanese dependency analysis adaptedto our representations of the morphological struc-ture of the words.
This algorithm parses a sentenceby starting from the end and analyzing it towardsthe beginning.
Bymaking the projectivity assump-tion that the relations do not cross, this algorithmconsiderably facilitates the analysis.4 Details of the Parsing ModelsIn this section we detail three models that we haveexperimented with for Turkish.
All three modelsare unlexicalized and differ either in the units usedfor parsing or in the way contexts modeled.
Inall three models, we use the probability model inEquation 3.4.1 Simplifying IG TagsOur morphological analyzer produces a rather richrepresentation with a multitude of morphosyntac-tic and morphosemantic features encoded in thewords.
However, not all of these features are nec-essarily relevant in all the tasks that these analysescan be used in.
Further, different subsets of thesefeatures may be relevant depending on the func-tion of a word.
In the models discussed below, weuse a reduced representation of the IGs to ?unlex-icalize?
the words:1.
For nominal IGs,4 we use two different tagsdepending on whether the IG is used as a de-pendent or as a head during (different stagesof ) parsing:?
If the IG is used as a dependent, (and,only word-final IGs can be dependents),we represent that IG by a reduced tagconsisting of only the case marker, asthat essentially determines the syntacticfunction of that IG as a dependent, andonly nominals have cases.?
If the IG is used as a head, then we useonly part-of-speech and the possessiveagreement marker in the reduced tag.4These are nouns, pronouns, and other derived forms thatinflect with the same paradigm as nouns, including infinitives,past and future participles.2.
For adjective IGs with present/past/futureparticiples minor part-of-speech, we use thepart-of-speech when they are used as depen-dents and the part-of-speech plus the the pos-sessive agreement marker when used as ahead.3.
For other IGs, we reduce the IG to just thepart-of-speech.Such a reduced representation also helps alleviatethe sparse data problem as statistics from manyword forms with only the relevant features areconflated.We modeled the second probability term on theright-hand side of Equation 3 (involving the dis-tance between the dependent and the head unit) inthe following manner.
First, we collected statis-tics over the treebank sentences, and noted that,if we count words as units, then 90% of depen-dency links link to a word that is less than 3 wordsaway.
Similarly, if we count distance in terms ofIGs, then 90% of dependency links link to an IGthat is less than 4 IGs away to the right.
Thus weselected a parameter k = 4 for Models 1 and 3 be-low, where distance is measured in terms of words,and k = 5 for Model 2 where distance is measuredin terms of IGs, as a threshold value at and beyondwhich a dependency is considered ?distant?.
Dur-ing actual runs,P (wi links to some head H(i) ?
i away|?i)was computed by interpolatingP1(wi links to some head H(i) ?
i away|?i)estimated from the training corpus, andP2(wi links to some head H(i) ?
i away)the estimated probability for a length of a linkwhen no contexts are considered, again estimatedfrom the training corpus.
When probabilities areestimated from the training set, all distances largerthan k are assigned the same probability.
If evenafter interpolation, the probability is 0, then a verysmall value is used.
This is a modified version ofthe backed-off smoothing used by Collins (1996)to alleviate sparse data problems.
A similar inter-polation is used for the first component on the righthand side of Equation 3 by removing the head andthe dependent contextual information all at once.4.2 Model 1 ?
?Unlexicalized?
Word-basedModelIn this model, we represent each word by a re-duced representation of its last IG when used as adependent,5 and by concatenation of the reduced5Remember that other IGs in a word, if any, do not haveany bearing on how this word links to its head word.92representation of its IGs when used as a head.Since a word can be both a dependent and a headword, the reduced representation to be used is dy-namically determined during parsing.Parsing then proceeds with words as units rep-resented in this manner.
Once the parser linksthese units, we remap these links back to IGs torecover the actual IG-to-IG dependencies.
We al-ready know that any outgoing link from a depen-dent will emanate from the last IG of that word.For the head word, we assume that the link landson the first IG of that word.6For the contexts, we use the following scheme.A contextual element on the left is treated as a de-pendent and is modeled with its last IG, while acontextual element on the right is represented asif it were a head using all its IGs.
We ignore anyoverlaps between contexts in this and the subse-quent models.In Figure 5 we show in a table the sample sen-tence in Figure 3, the morphological analysis foreach word and the reduced tags for representingthe units for the three models.
For each model, welist the tags when the unit is used as a head andwhen it is used as a dependent.
For model 1, weuse the tags in rows 3 and 4.4.3 Model 2 - IG-based ModelIn this model, we represent each IG with re-duced representations in the manner above, butdo not concatenate them into a representation forthe word.
So our ?units?
for parsing are IGs.The parser directly establishes IG-to-IG links fromword-final IGs to some IG to the right.
The con-texts that are used in this model are the IGs tothe left (starting with the last IG of the precedingword) and the right of the dependent and the headIG.The units and the tags we use in this model arein rows 5 and 6 in the table in Figure 5.
Notethat the empty cells in row 4 corresponds to IGswhich can not be syntactic dependents as they arenot word-final.4.4 Model 3 ?
IG-based Model withWord-final IG ContextsThis model is almost exactly like Model 2 above.The two differences are that (i) for contexts weonly use just the word-final IGs to the left and theright ignoring any non-word-final IGs in between(except for the case that the context and the headoverlap, where we use the tag of the head IG in-6This choice is based on the observation that in the tree-bank, 85.6% of the dependency links land on the first (andpossibly the only) IG of the head word, while 14.4% of thedependency links land on an IG other than the first one.stead of the final IG); and (ii) the distance functionis computed in terms of words.
The reason thismodel is used is that it is the word final IGs thatdetermine the syntactic roles of the dependents.5 ResultsSince in this study we are limited to parsing sen-tences with only left-to-right dependency links7which do not cross each other, we eliminated thesentences having such dependencies (even if theycontain a single one) and used a subset of 3398such sentences in the Turkish Treebank.
The goldstandard part-of-speech tags are used in the exper-iments.
The sentences in the corpus ranged be-tween 2 words to 40 words with an average ofabout 8 words;8 90% of the sentences had lessthan or equal to 15 words.
In terms of IGs, thesentences comprised 2 to 55 IGs with an averageof 10 IGs per sentence; 90% of the sentences hadless than or equal to 15 IGs.
We partitioned thisset into training and test sets in 10 different waysto obtain results with 10-fold cross-validation.We implemented three baseline parsers:1.
The first baseline parser links a word-final IGto the first IG of the next word on the right.2.
The second baseline parser links a word-finalIG to the last IG of the next word on theright.93.
The third baseline parser is a deterministicrule-based parser that links each word-finalIG to an IG on the right based on the approachof Nivre (2003).
The parser uses 23 unlexi-calized linking rules and a heuristic that linksany non-punctuation word not linked by theparser to the last IG of the last word as a de-pendent.Table 1 shows the results from our experimentswith these baseline parsers and parsers that arebased on the three models above.
The three mod-els have been experimented with different contextsaround both the dependent unit and the head.
Ineach row, columns 3 and 4 show the percentage ofIG?IG dependency relations correctly recoveredfor all tokens, and just words excluding punctu-ation from the statistics, while columns 5 and 6show the percentage of test sentences for whichall dependency relations extracted agree with the7In 95% of the treebank dependencies, the head is theright of the dependent.8This is quite normal; the equivalents of function wordsin English are embedded as morphemes (not IGs) into thesewords.9Note that for head words with a single IG, the first twobaselines behave the same.93Figure 5: Tags used in the parsing modelsrelations in the treebank.
Each entry presents theaverage and the standard error of the results on thetest set, over the 10 iterations of the 10-fold cross-validation.
Our main goal is to improve the per-centage of correctly determined IG-to-IG depen-dency relations, shown in the fourth column of thetable.
The best results in these experiments are ob-tained with Model 3 using 1 unit on both sides ofthe dependent.
Although it is slightly better thanModel 2 with the same context size, the differencebetween the means (0.4?0.2) for each 10 iterationsis statistically significant.Since we have been using unlexicalized models,we wanted to test out whether a smaller trainingcorpus would have a major impact for our currentmodels.
Table 2 shows results for Model 3 with nocontext and 1 unit on each side of the dependent,obtained by using only a 1500 sentence subset ofthe original treebank, again using 10-fold crossvalidation.
Remarkably the reduction in trainingset size has a very small impact on the results.Although all along, we have suggested that de-termining word-to-word dependency relationshipsis not the right approach for evaluating parser per-formance for Turkish, we have nevertheless per-formed word-to-word correctness evaluation sothat comparison with other word based approachescan be made.
In this evaluation, we assume that adependency link is correct if we correctly deter-mine the head word (but not necessarily the cor-rect IG).
Table 3 shows the word based results forthe best cases of the models in Table 1.We have also tested our parser with a pure wordmodel where both the dependent and the head arerepresented by the concatenation of their IGs, thatis, by their full morphological analysis except theroot.
The result for this case is given in the last rowof Table 3.
This result is even lower than the rule-based baseline.10 For this model, if we connect the10Also lower than Model 1 with no context (79.1?1.1)dependent to the first IG of the head as we did inModel 1, the IG-IG accuracy excluding punctua-tions becomes 69.9?3.1, which is also lower thanbaseline 3 (70.5%).6 DiscussionsOur results indicate that all of our models performbetter than the 3 baseline parsers, even when nocontexts around the dependent and head units areused.
We get our best results with Model 3, whereIGs are used as units for parsing and contexts arecomprised of word final IGs.
The highest accuracyin terms of percent of correctly extracted IG-to-IGrelations excluding punctuations (73.5%) was ob-tained when one word is used as context on bothsides of the the dependent.11 We also noted thatusing a smaller treebank to train our models didnot result in a significant reduction in our accu-racy indicating that the unlexicalized models arequite effective, but this also may hint that a largertreebank with unlexicalized modeling may not beuseful for improving link accuracy.A detailed look at the results from the best per-forming model shown in in Table 4,12 indicatesthat, accuracy decrases with the increasing sen-tence length.
For longer sentences, we should em-ploy more sophisticated models possibly includinglexicalization.A further analysis of the actual errors made bythe best performing model indicates almost 40%of the errors are ?attachment?
problems: the de-pendent IGs, especially verbal adjuncts and argu-ments, link to the wrong IG but otherwise with thesame morphological features as the correct one ex-cept for the root word.
This indicates we may haveto model distance in a more sophisticated way and11We should also note that early experiments using differ-ent sets of morphological features that we intuitively thoughtshould be useful, gave rather low accuracy results.12These results are significantly higher than the best base-line (rule based) for all the sentence length categories.94Percentage of IG-IG Percentage of SentencesRelations Correct With ALL Relations CorrectParsing Model Context Words+Punc Words only Words+Punc Words onlyBaseline 1 NA 59.9 ?0.3 63.9 ?0.7 21.4 ?0.6 24.0 ?0.7Baseline 2 NA 58.3 ?0.2 62.2 ?0.8 20.1 ?0.0 22.6 ?0.6Baseline 3 NA 69.6 ?0.2 70.5 ?0.8 31.7 ?0.7 36.6 ?0.8Model 1 None 69.8 ?0.4 71.0 ?1.3 32.7 ?0.6 36.2 ?0.7(k=4) Dl=1 69.9 ?0.4 71.1 ?1.2 32.9 ?0.5 36.4 ?0.6Dl=1 Dr=1 71.3 ?0.4 72.5 ?1.2 33.4 ?0.8 36.7 ?0.8Hl=1 Hr=1 64.7 ?0.4 65.5 ?1.3 25.4 ?0.6 28.7 ?0.8Both 71.4 ?0.4 72.6 ?1.1 34.2 ?0.7 37.2 ?0.6Model 2 None 70.5 ?0.3 71.9 ?1.0 32.1 ?0.9 36.3 ?0.9(k=5) Dl=1 71.3 ?0.3 72.7 ?0.9 33.8 ?0.8 37.4 ?0.7Dl=1 Dr=1 71.9 ?0.3 73.1 ?0.9 34.8 ?0.7 38.0 ?0.7Hl=1 Hr=1 57.4 ?0.3 57.6 ?0.7 23.5 ?0.6 25.8 ?0.6Both 70.9 ?0.3 72.2 ?0.9 34.2 ?0.8 37.2 ?0.9Model 3 None 71.2 ?0.3 72.6 ?0.9 34.4 ?0.7 38.1 ?0.7(k=4) Dl=1 71.2 ?0.4 72.6 ?1.1 34.5 ?0.7 38.3 ?0.6Dl=1 Dr=1 72.3 ?0.3 73.5 ?1.0 35.5 ?0.9 38.7 ?0.9Hl=1 Hr=1 55.2 ?0.3 55.1 ?0.7 22.0 ?0.6 24.1 ?0.6Both 71.1 ?0.3 72.4 ?0.9 35.5 ?0.8 38.4 ?0.9The Context column entries show the context around the dependent and the head unit.
Dl=1 and Dr=1 indicatethe use of 1 unit left and the right of the dependent respectively.
Hl=1 and Hr=1 indicate the use of 1 unit left andthe right of the head respectively.
Both indicates both head and the dependent have 1 unit of context on both sides.Table 1: Results from parsing with the baseline parsers and statistical parsers based on Models 1-3.Percentage of IG-IG Percentage of SentencesRelations Correct With ALL Relations CorrectParsing Model Context Words+Punc Words only Words+Punc Words onlyModel 3 None 71.0 ?0.6 72.2 ?1.5 34.4 ?1.0 38.1 ?1.1(k=4, 1500 Sentences) Dl=1 Dr=1 71.6 ?0.4 72.6 ?1.1 35.1 ?1.3 38.4 ?1.5Table 2: Results from using a smaller training corpus.Percentage of Word-WordRelations CorrectParsing Model Context Words onlyBaseline 1 NA 72.1 ?0.5Baseline 2 NA 72.1 ?0.5Baseline 3 NA 80.3 ?0.7Model 1 (k=4) Both 80.8 ?0.9Model 2 (k=5) Dl=1 Dr=1 81.0 ?0.7Model 3 (k=4) Dl=1 Dr=1 81.2 ?1.0Pure Word Model None 77.7 ?3.5Table 3: Results from word-to-word correctness evaluation.Sentence Length l (IGs) % Accuracy1 < l ?
10 80.2 ?0.510 < l ?
20 70.1 ?0.420 < l ?
30 64.6 ?1.030 < l 62.7 ?1.3Table 4: Accuracy over different length sentences.95perhaps use a limited lexicalization such as includ-ing limited non-morphological information (e.g.,verb valency) into the tags.7 ConclusionsWe have presented our results from statistical de-pendency parsing of Turkish with statistical mod-els trained from the sentences in the Turkish tree-bank.
The dependency relations are betweensub-lexical units that we call inflectional groups(IGs) and the parser recovers dependency rela-tions between these IGs.
Due to the modest sizeof the treebank available to us, we have usedunlexicalized statistical models, representing IGsby reduced representations of their morphologicalproperties.
For the purposes of this work we havelimited ourselves to sentences with all left-to-rightdependency links that do not cross each other.We get our best results (73.5% IG-to-IG link ac-curacy) using a model where IGs are used as unitsfor parsing and we use as contexts, word final IGsof the words before and after the dependent.Future work involves a more detailed under-standing of the nature of the errors and see howlimited lexicalization can help, as well as investi-gation of more sophisticated models such as SVMor memory-based techniques for correctly identi-fying dependencies.8 AcknowledgementThis research was supported in part by a researchgrant from TUBITAK (The Scientific and Techni-cal Research Council of Turkey) and from IstanbulTechnical University.ReferencesEugene Charniak.
2000.
A maximum-entropy-inspired parser.
In 1st Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, Seattle, Washington.Hoojung Chung and Hae-Chang Rim.
2004.
Un-lexicalized dependency parser for variable word or-der languages based on local contextual pattern.In Computational Linguistics and Intelligent TextProcessing (CICLing-2004), Seoul, Korea.
LectureNotes in Computer Science 2945.Michael Collins, Jan Hajic, Lance Ramshaw, andChristoph Tillmann.
1999.
A statistical parser forCzech.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguis-tics, pages 505?518, University of Maryland.Michael Collins.
1996.
A new statistical parser basedon bigram lexical dependencies.
In Proceedings ofthe 34th AnnualMeeting of the Association for Com-putational Linguistics, Santa Cruz, CA.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Compu-tational Linguistics and 8th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 16?23, Madrid, Spain.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics, pages 423?430, Sapporo,Japan.Taku Kudo and Yuji Matsumoto.
2000.
Japanesedependency analysis based on support vector ma-chines.
In Joint Sigdat Conference On EmpiricalMethods In Natural Language Processing and VeryLarge Corpora, Hong Kong.Taku Kudo and Yuji Matsumoto.
2002.
Japanesedependency analysis using cascaded chunking.
InSixth Conference on Natural Language Learning,Taipei, Taiwan.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedings ofthe 43rd Annual Meeting of the Association forComputational Linguistics (ACL?05), pages 99?106,Ann Arbor, Michigan, June.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In 8th Confer-ence on Computational Natural Language Learning,Boston, Massachusetts.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of 8thInternational Workshop on Parsing Technologies,pages 23?25, Nancy, France, April.Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r,and Go?khan Tu?r.
2003.
Building a Turkish tree-bank.
In Anne Abeille, editor, Building and Exploit-ing Syntactically-annotatedCorpora.
Kluwer Acad-emic Publishers.Kemal Oflazer.
1994.
Two-level description of Turk-ish morphology.
Literary and Linguistic Comput-ing, 9(2).Kemal Oflazer.
2003.
Dependency parsing with anextended finite-state approach.
Computational Lin-guistics, 29(4).Satoshi Sekine, Kiyotaka Uchimoto, and Hitoshi Isa-hara.
2000.
Backward beam search algorithm fordependency analysis of Japanese.
In 17th Inter-national Conference on Computational Linguistics,pages 754 ?
760, Saarbru?cken, Germany.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In 8th International Workshop of ParsingTechnologies, Nancy, France.96
