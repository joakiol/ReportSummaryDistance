Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 40?44,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsInference Protocols for Coreference ResolutionKai-Wei Chang Rajhans SamdaniAlla Rozovskaya Nick RizzoloMark Sammons Dan RothUniversity of Illinois at Urbana-Champaign{kchang10|rsamdan2|rozovska|rizzolo|mssammon|danr}@illinois.eduAbstractThis paper presents Illinois-Coref, a systemfor coreference resolution that participatedin the CoNLL-2011 shared task.
We in-vestigate two inference methods, Best-Linkand All-Link, along with their corresponding,pairwise and structured, learning protocols.Within these, we provide a flexible architec-ture for incorporating linguistically-motivatedconstraints, several of which we developedand integrated.
We compare and evaluate theinference approaches and the contribution ofconstraints, analyze the mistakes of the sys-tem, and discuss the challenges of resolvingcoreference for the OntoNotes-4.0 data set.1 IntroductionThe coreference resolution task is challenging, re-quiring a human or automated reader to identifydenotative phrases (?mentions?)
and link them toan underlying set of referents.
Human readers usesyntactic and semantic cues to identify and dis-ambiguate the referring phrases; a successful auto-mated system must replicate this behavior by linkingmentions that refer to the same underlying entity.This paper describes Illinois-Coref, a corefer-ence resolution system built on Learning BasedJava (Rizzolo and Roth, 2010), that participatedin the ?closed?
track of the CoNLL-2011 sharedtask (Pradhan et al, 2011).
Building on elementsof the coreference system described in Bengtsonand Roth (2008), we design an end-to-end system(Sec.
2) that identifies candidate mentions and thenapplies one of two inference protocols, Best-Linkand All-Link (Sec.
2.3), to disambiguate and clus-ter them.
These protocols were designed to easilyincorporate domain knowledge in the form of con-straints.
In Sec.
2.4, we describe the constraints thatwe develop and incorporate into the system.
Thedifferent strategies for mention detection and infer-ence, and the integration of constraints are evaluatedin Sections 3 and 4.2 ArchitectureIllinois-Coref follows the architecture used inBengtson and Roth (2008).
First, candidate men-tions are detected (Sec.
2.1).
Next, a pairwiseclassifier is applied to each pair of mentions, gen-erating a score that indicates their compatibility(Sec.
2.2).
Next, at inference stage, a coreferencedecoder (Sec.
2.3) aggregates these scores into men-tion clusters.
The original system uses the Best-Linkapproach; we also experiment with All-Link decod-ing.
This flexible decoder architecture allows lin-guistic or knowledge-based constraints to be easilyadded to the system: constraints may force mentionsto be coreferent or non-coreferent and can be option-ally used in either of the inference protocols.
Wedesigned and implemented several such constraints(Sec.
2.4).
Finally, since mentions that are in single-ton clusters are not annotated in the OntoNotes-4.0data set, we remove those as a post-processing step.2.1 Mention DetectionGiven a document, a mention detector generates aset of mention candidates that are used by the subse-quent components of the system.
A robust mentiondetector is crucial, as detection errors will propagateto the coreference stage.
As we show in Sec.
3, thesystem that uses gold mentions outperforms the sys-tem that uses predicted mentions by a large margin,from 15% to 18% absolute difference.40For the ACE 2004 coreference task, a good per-formance in mention detection is typically achievedby training a classifier e.g., (Bengtson and Roth,2008).
However, this model is not appropriate forthe OntoNotes-4.0 data set, in which (in contrast tothe ACE 2004 corpus) singleton mentions are notannotated: a specific noun phrase (NP) may corre-spond to a mention in one document but will notbe a mention in another document.
Therefore, wedesigned a high recall (?
90%) and low precision(?
35%) rule-based mention detection system thatincludes all phrases recognized as Named Entities(NE?s) and all phrases tagged as NPs in the syntac-tic parse of the text.
As a post-processing step, weremove all predicted mentions that remain in single-ton clusters after the inference stage.The best mention detection result on the DEV set1is 64.93% in F1 score (after coreference resolution)and is achieved by our best inference protocol, Best-Link with constraints.2.2 Pairwise Mention ScoringThe basic input to our inference algorithm is a pair-wise mention score, which indicates the compatibil-ity score of a pair of mentions.
For any two mentionsu and v, the compatibility score wuv is producedby a pairwise scoring component that uses extractedfeatures ?
(u, v) and linguistic constraints c:wuv = w ?
?
(u, v) + c(u, v) + t, (1)where w is a weight vector learned from trainingdata, c(u, v) is a compatibility score given by theconstraints, and t is a threshold parameter (to betuned).
We use the same features as Bengtson andRoth (2008), with the knowledge extracted from theOntoNotes-4.0 annotation.
The exact use of thescores and the procedure for learning weights w arespecific to the inference algorithm and are describednext.2.3 InferenceIn this section, we present our inference techniquesfor coreference resolution.
These clustering tech-niques take as input a set of pairwise mention scoresover a document and aggregate them into globally1In the shared task, the data set is split into three sets:TRAIN, DEV, and TEST.consistent cliques representing entities.
We investi-gate the traditional Best-Link approach and a moreintuitively appealing All-Link algorithm.2.3.1 Best-LinkBest-Link is a popular approach to coreferenceresolution.
For each mention, it considers the bestmention on its left to connect to (best accordingthe pairwise score wuv) and creates a link betweenthem if the pairwise score is above some thresh-old.
Although its strategy is simple, Bengtson andRoth (2008) show that with a careful design, it canachieve highly competitive performance.Inference: We give an integer linear programming(ILP) formulation of Best-Link inference in order topresent both of our inference algorithms within thesame framework.
Given a pairwise scorer w, wecan compute the compatibility scores ?
wuv fromEq.
(1) ?
for all mention pairs u and v. Let yuv bea binary variable, such that yuv = 1 only if u and vare in the same cluster.
For a document d, Best-Linksolves the following ILP formulation:argmaxy?u,vwuvyuvs.t?u<vyuv ?
1 ?v,yuw ?
{0, 1}.(2)Eq.
(2) generates a set of connected components andall the mentions in each connected component con-stitute an entity.Learning: We follow the strategy in (Bengtsonand Roth, 2008, Section 2.2) to learn the pairwisescoring function w. The scoring function is trainedon:?
Positive examples: for each mention u, we con-struct a positive example (u, v), where v is theclosest preceding mention in u?s equivalenceclass.?
Negative examples: all mention pairs (u, v),where v is a preceding mention of u and u, vare not in the same class.As a result of the singleton mentions not being anno-tated, there is an inconsistency in the sample distri-butions in the training and inference phases.
There-fore, we apply the mention detector to the trainingset, and train the classifier using the union set of goldand predicted mentions.412.3.2 All-LinkThe All-Link inference approach scores a cluster-ing of mentions by including all possible pairwiselinks in the score.
It is also known as correlationalclustering (Bansal et al, 2002) and has been appliedto coreference resolution in the form of supervisedclustering (Mccallum and Wellner, 2003; Finley andJoachims, 2005).Inference: Similar to Best-Link, for a document d,All-Link inference finds a clustering All-Link(d;w)by solving the following ILP problem:argmaxy?u,vwuvyuvs.t yuw ?
yuv + yvw ?
1 ?u,w, v,yuw ?
{0, 1}.
(3)The inequality constraints in Eq.
(3) enforce thetransitive closure of the clustering.
The solution ofEq.
(3) is a set of cliques, and the mentions in thesame cliques corefer.Learning: We present a structured perceptron al-gorithm, which is similar to supervised clusteringalgorithm (Finley and Joachims, 2005) to learn w.Note that as an approximation, it is certainly pos-sible to use the weight parameter learned by using,say, averaged perceptron over positive and negativelinks.
The pseudocode is presented in Algorithm 1.Algorithm 1 Structured Perceptron like learning al-gorithm for All-Link inferenceGiven: Annotated documents D and initialweight winitInitialize w ?
winitfor Document d in D doClustering y ?
All-Link(d;w)for all pairs of mentions u and v doI1(u, v) = [u, v coreferent in D]I2(u, v) = [y(u) = y(v)]w ?
w +(I1(u, v)?
I2(u, v))?
(u, v)end forend forreturn wFor the All-Link clustering, we drop one of thethree transitivity constraints for each triple of men-tion variables.
Similar to Pascal and Baldridge(2009), we observe that this improves accuracy ?the reader is referred to Pascal and Baldridge (2009)for more details.2.4 ConstraintsThe constraints in our inference algorithm are basedon the analysis of mistakes on the DEV set2.
Sincethe majority of errors are mistakes in recall, wherethe system fails to link mentions that refer to thesame entity, we define three high precision con-straints that improve recall on NPs with definite de-terminers and mentions whose heads are NE?s.The patterns used by constraints to match mentionpairs have some overlap with those used by the pair-wise mention scorer, but their formulation as con-straints allow us to focus on a subset of mentionsto which a certain pattern applies with high preci-sion.
For example, the constraints use a rule-basedstring similarity measure that accounts for the in-ferred semantic type of the mentions compared.
Ex-amples of mention pairs that are correctly linked bythe constraints are: Governor Bush?
Bush; a cru-cial swing state , Florida?
Florida; Sony itself ?Sony; Farmers?
Los Angeles - based Farmers.3 Experiments and ResultsIn this section, we present the performance of thesystem on the OntoNotes-4.0 data set.
A previousexperiment using an earlier version of this data canbe found in (Pradhan et al, 2007).
Table 1 shows theperformance for the two inference protocols, withand without constraints.
Best-Link outperforms All-Link for both predicted and gold mentions.
Addingconstraints improves the performance slightly forBest-Link on predicted mentions.
In the other con-figurations, the constraints either do not affect theperformance or slightly degrade it.Table 2 shows the results obtained on TEST, usingthe best system configurations found on DEV.
Wereport results on predicted mentions with predictedboundaries, predicted mentions with gold bound-aries, and when using gold mentions3.2We provide a more detailed analysis of the errors in Sec.
4.3Note that the gold boundaries results are different from thegold mention results.
Specifying gold mentions requires coref-erence resolution to exclude singleton mentions.
Gold bound-aries are provided by the task organizers and also include sin-gleton mentions.42MethodPred.
Mentions w/Pred.
Boundaries Gold MentionsMD MUC BCUB CEAF AVG MUC BCUB CEAF AVGBest-Link 64.70 55.67 69.21 43.78 56.22 80.58 75.68 64.69 73.65Best-Link W/ Const.
64.69 55.8 69.29 43.96 56.35 80.56 75.02 64.24 73.27All-Link 63.30 54.56 68.50 42.15 55.07 77.72 73.65 59.17 70.18All-Link W/ Const.
63.39 54.56 68.46 42.20 55.07 77.94 73.43 59.47 70.28Table 1: The performance of the two inference protocols on both gold and predicted mentions.
The systems aretrained on the TRAIN set and evaluated on the DEV set.
We report the F1 scores (%) on mention detection (MD)and coreference metrics (MUC, BCUB, CEAF).
The column AVG shows the averaged scores of the three coreferencemetrics.Task MD MUC BCUB CEAF AVGPred.
Mentions w/ Pred.
Boundaries 64.88 57.15 67.14 41.94 55.96Pred.
Mentions w/ Gold Boundaries 67.92 59.79 68.65 41.42 56.62Gold Mentions - 82.55 73.70 65.24 73.83Table 2: The results of our submitted system on the TEST set.
The system uses Best-Link decoding with constraintson predicted mentions and Best-Link decoding without constraints on gold mentions.
The systems are trained on acollection of TRAIN and DEV sets.4 DiscussionMost of the mistakes made by the system are due tonot linking co-referring mentions.
The constraintsimprove slightly the recall on a subset of mentions,and here we show other common errors for the sys-tem.
For instance, the system fails to link the twomentions, the Emory University hospital in Atlantaand the hospital behind me, since each of the men-tions has a modifier that is not part of the other men-tion.
Another common error is related to pronounresolution, especially when a pronoun has severalantecedents in the immediate context, appropriate ingender, number, and animacy, as in ?
E. Robert Wal-lach was sentenced by a U.S. judge in New York tosix years in prison and fined $ 250,000 for his rack-eteering conviction in the Wedtech scandal .?
: bothE.
Robert Wallach and a U.S. judge are appropri-ate antecedents for the pronoun his.
Pronoun errorsare especially important to address since 35% of thementions are pronouns.The system also incorrectly links some mentions,such as: ?The suspect said it took months to repack-age...?
(?it?
cannot refer to a human); ?They seethem.?
(subject and object in the same sentence arelinked); and ?Many freeway accidents occur simplybecause people stay inside the car and sort out...?
(the NP the car should not be linked to any othermention, since it does not refer to a specific entity).5 ConclusionsWe have investigated a coreference resolution sys-tem that uses a rich set of features and two populartypes of clustering algorithm.While the All-Link clustering seems to be capableof taking more information into account for makingclustering decisions, as it requires each mention ina cluster to be compatible with all other mentions inthat cluster, the Best-Link approach still outperformsit.
This raises a natural algorithmic question regard-ing the inherent nature of clustering style most suit-able for coreference and regarding possible ways ofinfusing more knowledge into different coreferenceclustering styles.
Our approach accommodates in-fusion of knowledge via constraints, and we havedemonstrated its utility in an end-to-end coreferencesystem.Acknowledgments This research is supported by the DefenseAdvanced Research Projects Agency (DARPA) Machine Reading Pro-gram under Air Force Research Laboratory (AFRL) prime contract no.FA8750-09-C-0181 and the Army Research Laboratory (ARL) underagreement W911NF-09-2-0053.
Any opinions, findings, and conclu-sion or recommendations expressed in this material are those of the au-thor(s) and do not necessarily reflect the view of the DARPA, AFRL,ARL or the US government.43ReferencesN.
Bansal, A. Blum, and S. Chawla.
2002.
Correlationclustering.
In Proceedings of the 43rd Symposium onFoundations of Computer Science.E.
Bengtson and D. Roth.
2008.
Understanding the valueof features for coreference resolution.
In EMNLP, 10.T.
Finley and T. Joachims.
2005.
Supervised cluster-ing with support vector machines.
In Proceedingsof the International Conference on Machine Learning(ICML).A.
Mccallum and B. Wellner.
2003.
Toward condi-tional models of identity uncertainty with applicationto proper noun coreference.
In The Conference onAdvances in Neural Information Processing Systems(NIPS).D.
Pascal and J. Baldridge.
2009.
Global joint models forcoreference resolution and named entity classification.In Procesamiento del Lenguaje Natural.S.
Pradhan, L. Ramshaw, R. Weischedel, J. MacBride,and L. Micciulla.
2007.
Unrestricted Coreference:Identifying Entities and Events in OntoNotes.
In inProceedings of the IEEE International Conference onSemantic Computing (ICSC), September 17-19.S.
Pradhan, L. Ramshaw, M. Marcus, M. Palmer,R.
Weischedel, and N. Xue.
2011.
Conll-2011 sharedtask: Modeling unrestricted coreference in ontonotes.In Proceedings of the Annual Conference on Compu-tational Natural Language Learning (CoNLL).N.
Rizzolo and D. Roth.
2010.
Learning Based Javafor Rapid Development of NLP Systems.
In Proceed-ings of the International Conference on Language Re-sources and Evaluation (LREC), Valletta, Malta, 5.44
