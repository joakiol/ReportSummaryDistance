Scaling Web-based Acquisition of Entailment RelationsIdan Szpektoridan@szpektor.net?ITC-Irst, Via Sommarive, 18 (Povo) - 38050 Trento, Italy?DIT - University of Trento, Via Sommarive, 14 (Povo) - 38050 Trento, Italy?Department of Computer Science, Bar Ilan University - Ramat Gan 52900, IsraelDepartment of Computer Science, Tel Aviv University - Tel Aviv 69978, IsraelHristo Tanev?tanev@itc.itIdo Dagan?dagan@cs.biu.ac.ilBonaventura Coppola?
?coppolab@itc.itAbstractParaphrase recognition is a critical step for nat-ural language interpretation.
Accordingly, manyNLP applications would benefit from high coverageknowledge bases of paraphrases.
However, the scal-ability of state-of-the-art paraphrase acquisition ap-proaches is still limited.
We present a fully unsuper-vised learning algorithm for Web-based extractionof entailment relations, an extended model of para-phrases.
We focus on increased scalability and gen-erality with respect to prior work, eventually aimingat a full scale knowledge base.
Our current imple-mentation of the algorithm takes as its input a verblexicon and for each verb searches the Web for re-lated syntactic entailment templates.
Experimentsshow promising results with respect to the ultimategoal, achieving much better scalability than priorWeb-based methods.1 IntroductionModeling semantic variability in language hasdrawn a lot of attention in recent years.
Many ap-plications like QA, IR, IE and Machine Translation(Moldovan and Rus, 2001; Hermjakob et al, 2003;Jacquemin, 1999) have to recognize that the samemeaning can be expressed in the text in a huge vari-ety of surface forms.
Substantial research has beendedicated to acquiring paraphrase patterns, whichrepresent various forms in which a certain meaningcan be expressed.Following (Dagan and Glickman, 2004) we ob-serve that a somewhat more general notion neededfor applications is that of entailment relations (e.g.
(Moldovan and Rus, 2001)).
These are directionalrelations between two expressions, where the mean-ing of one can be entailed from the meaning of theother.
For example ?X acquired Y?
entails ?X ownsY?.
These relations provide a broad framework forrepresenting and recognizing semantic variability,as proposed in (Dagan and Glickman, 2004).
Forexample, if a QA system has to answer the question?Who owns Overture??
and the corpus includes thephrase ?Yahoo acquired Overture?, the system canuse the known entailment relation to conclude thatthis phrase really indicates the desired answer.
Moreexamples of entailment relations, acquired by ourmethod, can be found in Table 1 (section 4).To perform such inferences at a broad scale, ap-plications need to possess a large knowledge base(KB) of entailment patterns.
We estimate such aKB should contain from between a handful to a fewdozens of relations per meaning, which may sumto a few hundred thousands of relations for a broaddomain, given that a typical lexicon includes tens ofthousands of words.Our research goal is to approach unsupervised ac-quisition of such a full scale KB.
We focus on de-veloping methods that acquire entailment relationsfrom the Web, the largest available resource.
Tothis end substantial improvements are needed in or-der to promote scalability relative to current Web-based approaches.
In particular, we address twomajor goals: reducing dramatically the complexityof required auxiliary inputs, thus enabling to applythe methods at larger scales, and generalizing thetypes of structures that can be acquired.
The algo-rithms described in this paper were applied for ac-quiring entailment relations for verb-based expres-sions.
They successfully discovered several rela-tions on average per each randomly selected expres-sion.2 Background and MotivationsThis section provides a qualitative view of priorwork, emphasizing the perspective of aiming at afull-scale paraphrase resource.
As there are stillno standard benchmarks, current quantitative resultsare not comparable in a consistent way.The major idea in paraphrase acquisition is oftento find linguistic structures, here termed templates,that share the same anchors.
Anchors are lexicalelements describing the context of a sentence.
Tem-plates that are extracted from different sentencesand connect the same anchors in these sentences,are assumed to paraphrase each other.
For example,the sentences ?Yahoo bought Overture?
and ?Yahooacquired Overture?
share the anchors {X=Yahoo,Y =Overture}, suggesting that the templates ?X buyY?
and ?X acquire Y?
paraphrase each other.
Algo-rithms for paraphrase acquisition address two prob-lems: (a) finding matching anchors and (b) identify-ing template structure, as reviewed in the next twosubsections.2.1 Finding Matching AnchorsThe prominent approach for paraphrase learningsearches sentences that share common sets of mul-tiple anchors, assuming they describe roughly thesame fact or event.
To facilitate finding manymatching sentences, highly redundant comparablecorpora have been used.
These include multipletranslations of the same text (Barzilay and McKe-own, 2001) and corresponding articles from multi-ple news sources (Shinyama et al, 2002; Pang etal., 2003; Barzilay and Lee, 2003).
While facilitat-ing accuracy, we assume that comparable corporacannot be a sole resource due to their limited avail-ability.Avoiding a comparable corpus, (Glickman andDagan, 2003) developed statistical methods thatmatch verb paraphrases within a regular corpus.Their limited scale results, obtaining several hun-dred verb paraphrases from a 15 million word cor-pus, suggest that much larger corpora are required.Naturally, the largest available corpus is the Web.Since exhaustive processing of the Web is not feasi-ble, (Duclaye et al, 2002) and (Ravichandran andHovy, 2002) attempted bootstrapping approaches,which resemble the mutual bootstrapping methodfor Information Extraction of (Riloff and Jones,1999).
These methods start with a provided knownset of anchors for a target meaning.
For example,the known anchor set {Mozart, 1756} is given as in-put in order to find paraphrases for the template ?Xborn in Y?.
Web searching is then used to find occur-rences of the input anchor set, resulting in new tem-plates that are supposed to specify the same relationas the original one (?born in?).
These new templatesare then exploited to get new anchor sets, whichare subsequently processed as the initial {Mozart,1756}.
Eventually, the overall procedure results inan iterative process able to induce templates fromanchor sets and vice versa.The limitation of this approach is the requirementfor one input anchor set per target meaning.
Prepar-ing such input for all possible meanings in broaddomains would be a huge task.
As will be explainedbelow, our method avoids this limitation by find-ing all anchor sets automatically in an unsupervisedmanner.Finally, (Lin and Pantel, 2001) present a notablydifferent approach that relies on matching sepa-rately single anchors.
They limit the allowed struc-ture of templates only to paths in dependency parsesconnecting two anchors.
The algorithm constructsfor each possible template two feature vectors, rep-resenting its co-occurrence statistics with the twoanchors.
Two templates with similar vectors aresuggested as paraphrases (termed inference rule).Matching of single anchors relies on the gen-eral distributional similarity principle and unlike theother methods does not require redundancy of setsof multiple anchors.
Consequently, a much largernumber of paraphrases can be found in a regularcorpus.
Lin and Pantel report experiments for 9templates, in which their system extracted 10 cor-rect inference rules on average per input template,from 1GB of news data.
Yet, this method also suf-fers from certain limitations: (a) it identifies onlytemplates with pre-specified structures; (b) accuracyseems more limited, due to the weaker notion ofsimilarity; and (c) coverage is limited to the scopeof an available corpus.To conclude, several approaches exhaustivelyprocess different types of corpora, obtaining vary-ing scales of output.
On the other hand, the Web isa huge promising resource, but current Web-basedmethods suffer serious scalability constraints.2.2 Identifying Template StructureParaphrasing approaches learn different kinds oftemplate structures.
Interesting algorithms are pre-sented in (Pang et al, 2003; Barzilay and Lee,2003).
They learn linear patterns within similar con-texts represented as finite state automata.
Threeclasses of syntactic template learning approachesare presented in the literature: learning of predicateargument templates (Yangarber et al, 2000), learn-ing of syntactic chains (Lin and Pantel, 2001) andlearning of sub-trees (Sudo et al, 2003).
The lastapproach is the most general with respect to the tem-plate form.
However, its processing time increasesexponentially with the size of the templates.As a conclusion, state of the art approaches stilllearn templates of limited form and size, thus re-stricting generality of the learning process.3 The TE/ASE Acquisition MethodMotivated by prior experience, we identify two ma-jor goals for scaling Web-based acquisition of en-tailment relations: (a) Covering the broadest pos-sible range of meanings, while requiring minimalinput and (b) Keeping template structures as gen-eral as possible.
To address the first goal we re-quire as input only a phrasal lexicon of the rel-evant domain (including single words and multi-word expressions).
Broad coverage lexicons arewidely available or may be constructed using knownterm acquisition techniques, making it a feasibleand scalable input requirement.
We then aim toacquire entailment relations that include any of thelexicon?s entries.
The second goal is addressed by anovel algorithm for extracting the most general tem-plates being justified by the data.For each lexicon entry, denoted a pivot, ourextraction method performs two phases: (a) ex-tract promising anchor sets for that pivot (ASE,Section 3.1), and (b) from sentences contain-ing the anchor sets, extract templates for whichan entailment relation holds with the pivot (TE,Section 3.2).
Examples for verb pivots are:?acquire?, ?fall to?, ?prevent?
.
We will use the pivot?prevent?
for examples through this section.Before presenting the acquisition method we firstdefine its output.
A template is a dependency parse-tree fragment, with variable slots at some tree nodes(e.g.
?X subj?
prevent obj?
Y?
).
An entailment rela-tion between two templates T1 and T2 holds ifthe meaning of T2 can be inferred from the mean-ing of T1 (or vice versa) in some contexts, butnot necessarily all, under the same variable instan-tiation.
For example, ?X subj?
prevent obj?
Y?
entails?Xsubj?
reduceobj?
Y risk?
because the sentence ?as-pirin reduces heart attack risk?
can be inferred from?aspirin prevents a first heart attack?.
Our outputconsists of pairs of templates for which an entail-ment relation holds.3.1 Anchor Set Extraction (ASE)The goal of this phase is to find a substantial num-ber of promising anchor sets for each pivot.
A goodanchor-set should satisfy a proper balance betweenspecificity and generality.
On one hand, an anchorset should correspond to a sufficiently specific set-ting, so that entailment would hold between its dif-ferent occurrences.
On the other hand, it should besufficiently frequent to appear with different entail-ing templates.Finding good anchor sets based on just the inputpivot is a hard task.
Most methods identify good re-peated anchors ?in retrospect?, that is after process-ing a full corpus, while previous Web-based meth-ods require at least one good anchor set as input.Given our minimal input, we needed refined crite-ria that identify a priori the relatively few promisinganchor sets within a sample of pivot occurrences.ASE ALGORITHM STEPS:For each pivot (a lexicon entry)1.
Create a pivot template, Tp2.
Construct a parsed sample corpus S for Tp:(a) Retrieve an initial sample from the Web(b) Identify associated phrases for the pivot(c) Extend S using the associated phrases3.
Extract candidate anchor sets from S:(a) Extract slot anchors(b) Extract context anchors4.
Filter the candidate anchor sets:(a) by absolute frequency(b) by conditional pivot probabilityFigure 1: Outline of the ASE algorithm.The ASE algorithm (presented in Figure 1) per-forms 4 main steps.STEP (1) creates a complete template, called thepivot template and denoted Tp, for the input pivot,denoted P .
Variable slots are added for the ma-jor types of syntactic relations that interact with P ,based on its syntactic type.
These slots enable us tolater match Tp with other templates.
For verbs, weadd slots for a subject and for an object or a modifier(e.g.
?X subj?
prevent obj?
Y?
).STEP (2) constructs a sample corpus, denoted S,for the pivot template.
STEP (2.A) utilizes a Websearch engine to initialize S by retrieving sentencescontaining P .
The sentences are parsed by theMINIPAR dependency parser (Lin, 1998), keepingonly sentences that contain the complete syntactictemplate Tp (with all the variables instantiated).STEP (2.B) identifies phrases that are statisticallyassociated with Tp in S. We test all noun-phrasesin S , discarding phrases that are too common onthe Web (absolute frequency higher than a thresh-old MAXPHRASEF), such as ?desire?.
Then we se-lect the N phrases with highest tf ?idf score1.
Thesephrases have a strong collocation relationship withthe pivot P and are likely to indicate topical (ratherthan anecdotal) occurrences of P .
For example, thephrases ?patient?
and ?American Dental Associa-tion?, which indicate contexts of preventing healthproblems, were selected for the pivot ?prevent?.
Fi-1Here, tf ?idf = freqS(X) ?
log(NfreqW (X))where freqS(X) is the number of occurrences in S containingX , N is the total number of Web documents, and freqW (X)is the number of Web documents containing X .nally, STEP (2.C) expands S by querying the Webwith the both P and each of the associated phrases,adding the retrieved sentences to S as in step (2.a).STEP (3) extracts candidate anchor sets for Tp.From each sentence in S we try to generate one can-didate set, containing noun phrases whose Web fre-quency is lower than MAXPHRASEF.
STEP (3.A)extracts slot anchors ?
phrases that instantiate theslot variables of Tp.
Each anchor is markedwith the corresponding slot.
For example, theanchors {antibioticssubj?
, miscarriage obj?}
were ex-tracted from the sentence ?antibiotics in pregnancyprevent miscarriage?.STEP (3.B) tries to extend each candidate set withone additional context anchor, in order to improveits specificity.
This anchor is chosen as the highesttf ?idf scoring phrase in the sentence, if it exists.
Inthe previous example, ?pregnancy?
is selected.STEP (4) filters out bad candidate anchor sets bytwo different criteria.
STEP (4.A) maintains onlycandidates with absolute Web frequency within athreshold range [MINSETF, MAXSETF], to guaran-tee an appropriate specificity-generality level.
STEP(4.B) guarantees sufficient (directional) associationbetween the candidate anchor set c and Tp, by esti-matingProb(Tp|c) ?freqW (P ?
c)freqW (c)where freqW is Web frequency and P is the pivot.We maintain only candidates for which this prob-ability falls within a threshold range [SETMINP,SETMAXP].
Higher probability often correspondsto a strong linguistic collocation between thecandidate and Tp, without any semantic entail-ment.
Lower probability indicates coincidental co-occurrence, without a consistent semantic relation.The remaining candidates in S become the in-put anchor-sets for the template extraction phase,for example, {Aspirinsubj?
, heart attackobj?}
for ?pre-vent?.3.2 Template Extraction (TE)The Template Extraction algorithm accepts as its in-put a list of anchor sets extracted from ASE for eachpivot template.
Then, TE generates a set of syntactictemplates which are supposed to maintain an entail-ment relationship with the initial pivot template.
TEperforms three main steps, described in the follow-ing subsections:1.
Acquisition of a sample corpus from the Web.2.
Extraction of maximal most general templatesfrom that corpus.3.
Post-processing and final ranking of extractedtemplates.3.2.1 Acquisition of a sample corpus from theWebFor each input anchor set, TE acquires from theWeb a sample corpus of sentences containing it.For example, a sentence from the sample corpusfor {aspirin, heart attack} is: ?Aspirin stops heartattack??.
All of the sample sentences are thenparsed with MINIPAR (Lin, 1998), which gener-ates from each sentence a syntactic directed acyclicgraph (DAG) representing the dependency structureof the sentence.
Each vertex in this graph is labeledwith a word and some morphological information;each graph edge is labeled with the syntactic rela-tion between the words it connects.TE then substitutes each slot anchor (see section3.1) in the parse graphs with its corresponding slotvariable.
Therefore, ?Aspirin stops heart attack?
?will be transformed into ?X stop Y?.
This way allthe anchors for a certain slot are unified under thesame variable name in all sentences.
The parsedsentences related to all of the anchor sets are sub-sequently merged into a single set of parse graphsS = {P1, P2, .
.
.
, Pn} (see P1 and P2 in Figure 2).3.2.2 Extraction of maximal most generaltemplatesThe core of TE is a General Structure Learning al-gorithm (GSL ) that is applied to the set of parsegraphs S resulting from the previous step.
GSLextracts single-rooted syntactic DAGs, which arenamed spanning templates since they must span atleast over Na slot variables, and should also ap-pear in at least Nr sentences from S (In our exper-iments we set Na=2 and Nr=2).
GSL learns maxi-mal most general templates: they are spanning tem-plates which, at the same time, (a) cannot be gener-alized by further reduction and (b) cannot be furtherextended keeping the same generality level.In order to properly define the notion of maximalmost general templates, we introduce some formaldefinitions and notations.DEFINITION: For a spanning template t we definea sentence set, denoted with ?
(t), as the set of allparsed sentences in S containing t.For each pair of templates t1 and t2, we use the no-tation t1  t2 to denote that t1 is included as a sub-graph or is equal to t2.
We use the notation t1 ?
t2when such inclusion holds strictly.
We define T (S)as the set of all spanning templates in the sample S.DEFINITION: A spanning template t ?
T (S) ismaximal most general if and only if both of the fol-lowing conditions hold:CONDITION A: For ?t?
?
T (S), t?
 t, it holds that?
(t) = ?(t?
).CONDITION B: For ?t?
?
T (S), t ?
t?, it holds that?
(t) ?
?(t?
).Condition A ensures that the extracted templates donot contain spanning sub-structures that are more?general?
(i.e.
having a larger sentence set); con-dition B ensures that the template cannot be furtherenlarged without reducing its sentence set.GSL performs template extraction in two mainsteps: (1) build a compact graph representation ofall the parse graphs from S; (2) extract templatesfrom the compact representation.A compact graph representation is an aggregategraph which joins all the sentence graphs from Sensuring that all identical spanning sub-structuresfrom different sentences are merged into a singleone.
Therefore, each vertex v (respectively, edgee) in the aggregate graph is either a copy of a cor-responding vertex (edge) from a sentence graph Pior it represents the merging of several identicallylabeled vertices (edges) from different sentences inS.
The set of such sentences is defined as the sen-tence set of v (e), and is represented through the setof index numbers of related sentences (e.g.
?
(1,2)?in the third tree of Figure 2).
We will denote withGi the compact graph representation of the first isentences in S. The parse trees P1 and P2 of twosentences and their related compact representationG2 are shown in Figure 2.Building the compact graph representationThe compact graph representation is built incremen-tally.
The algorithm starts with an empty aggregategraph G0 and then merges the sentence graphs fromS one at a time into the aggregate structure.Let?s denote the current aggregate graph withGi?1(Vg, Eg) and let Pi(Vp, Ep) be the parse graphwhich will be merged next.
Note that the sentenceset of Pi is a single element set {i}.During each iteration a new graph is created asthe union of both input graphs: Gi = Gi?1 ?
Pi.Then, the following merging procedure is per-formed on the elements of Gi1.
ADDING GENERALIZED VERTICES TO Gi.For every two vertices vg ?
Vg, vp ?
Vp havingequal labels, a new generalized vertex vnewg is cre-ated and added to Gi.
The new vertex takes the samelabel and holds a sentence set which is formed fromthe sentence set of vg by adding i to it.
Still withreference to Figure 2, the generalized vertices in G2are ?X?, ?Y?
and ?stop?.
The algorithm connects thegeneralized vertex vnewg with all the vertices whichare connected with vg and vp.2.
MERGING EDGES.
If two edges eg ?
Eg andep ?
Ep have equal labels and their correspondingadjacent vertices have been merged, then ea and epare also merged into a new edge.
In Figure 2 theedges (?stop?, ?X? )
and (?stop?, ?Y? )
from P1 andP2 are eventually merged into G2.3.
DELETING MERGED VERTICES.
Every vertexv from Vp or Vg for which at least one generalizedvertex vnewg exists is deleted from Gi.As an optimization step, we merge only verticesand edges that are included in equal spanning tem-plates.Extracting the templatesGSL extracts all maximal most general templatesfrom the final compact representation Gn using thefollowing sub-algorithm:1.
BUILDING MINIMAL SPANNING TREES.
Forevery Na different slot variables in Gn having acommon ancestor, a minimal spanning tree st isbuilt.
Its sentence set is computed as the intersec-tion of the sentence sets of its edges and vertices.2.
EXPANDING THE SPANNING TREES.
Everyminimal spanning tree st is expanded to the maxi-mal sub-graph maxst whose sentence set is equal to?(st).
All maximal single-rooted DAGs in maxstare extracted as candidate templates.
Maximalityensures that the extracted templates cannot be ex-panded further while keeping the same sentence set,satisfying condition B.3.
FILTERING.
Candidates which contain an-other candidate with a larger sentence set are filteredout.
This step guarantees condition A.In Figure 2 the maximal most general template inG2 is ?Xsubj?
stopobj?
Y?
.3.2.3 Post-processing and ranking of extractedtemplatesAs a last step, names and numbers are filtered outfrom the templates.
Moreover, TE removes thosetemplates which are very long or which appear withjust one anchor set and in less than four sentences.Finally, the templates are sorted first by the numberof anchor sets with which each template appeared,and then by the number of sentences in which theyappeared.4 EvaluationWe evaluated the results of the TE/ASE algorithmon a random lexicon of verbal forms and then as-sessed its performance on the extracted data throughhuman-based judgments.P1 : stopsubjzzz||zzzzobjAAAAAAAP2 : stopsubjzzz||zzzzobjbyJJJJ%%JJJJG2 : stop(1, 2)subj(1,2)rrrrxxrrrrobj(1,2)by(2)OOOO''OOOOX Y X Y absorbing X(1, 2) Y (1, 2) absorbing(2)Figure 2: Two parse trees and their compact representation (sentence sets are shown in parentheses).4.1 Experimental SettingThe test set for human evaluation was generated bypicking out 53 random verbs from the 1000 mostfrequent ones found in a subset of the Reuters cor-pus2.
For each verb entry in the lexicon, we pro-vided the judges with the corresponding pivot tem-plate and the list of related candidate entailmenttemplates found by the system.
The judges wereasked to evaluate entailment for a total of 752 tem-plates, extracted for 53 pivot lexicon entries; Table1 shows a sample of the evaluated templates; all ofthem are clearly good and were judged as correctones.Pivot Template Entailment TemplatesX prevent Y X provides protection against YX reduces YX decreases the risk of YX be cure for YX a day keeps Y awayX to combat YX accuse Y X call Y indictableX testifies against YY defense before XX acquire Y X snap up YY shareholders approve XbuyoutY shareholders receive sharesof X stockX go back to Y Y allowed X to returnTable 1: Sample of templates found by TE/ASE andincluded in the evaluation test set.Concerning the ASE algorithm, threshold pa-rameters3 were set as PHRASEMAXF=107, SET-MINF=102, SETMAXF=105, SETMINP=0.066,and SETMAXP=0.666.
An upper limit of 30 wasimposed on the number of possible anchor sets usedfor each pivot.
Since this last value turned out tobe very conservative with respect to system cover-2Known as Reuters Corpus, Volume 1, English Language,1996-08-20 to 1997-08-19.3All parameters were tuned on a disjoint development lexi-con before the actual experiment.age, we subsequently attempted to relax it to 50 (seeDiscussion in Section 4.3).Further post-processing was necessary over ex-tracted data in order to remove syntactic variationsreferring to the same candidate template (typicallypassive/active variations).Three possible judgment categories have beenconsidered: Correct if an entailment relationshipin at least one direction holds between the judgedtemplate and the pivot template in some non-bizarrecontext; Incorrect if there is no reasonable contextand variable instantiation in which entailment holds;No Evaluation if the judge cannot come to a definiteconclusion.4.2 ResultsEach of the three assessors (referred to as J#1, J#2,and J#3) issued judgments for the 752 differenttemplates.
Correct templates resulted to be 283,313, and 295 with respect to the three judges.
Noevaluation?s were 2, 0, and 16, while the remainingtemplates were judged Incorrect.For each verb, we calculate Yield as the absolutenumber of Correct templates found and Precision asthe percentage of good templates out of all extractedtemplates.
Obtained Precision is 44.15%, averagedover the 53 verbs and the 3 judges.
Considering LowMajority on judges, the precision value is 42.39%.Average Yield was 5.5 templates per verb.These figures may be compared (informally, asdata is incomparable) with average yield of 10.1and average precision of 50.3% for the 9 ?pivot?templates of (Lin and Pantel, 2001).
The compar-ison suggests that it is possible to obtain from the(very noisy) web a similar range of precision as wasobtained from a clean news corpus.
It also indi-cates that there is potential for acquiring additionaltemplates per pivot, which would require further re-search on broadening efficiently the search for addi-tional web data per pivot.Agreement among judges is measured by theKappa value, which is 0.55 between J#1 and J#2,0.57 between J#2 and J#3, and 0.63 between J#1and J#3.
Such Kappa values correspond to moder-ate agreement for the first two pairs and substantialagreement for the third one.
In general, unanimousagreement among all of the three judges has beenreported on 519 out of 752 templates, which corre-sponds to 69%.4.3 DiscussionOur algorithm obtained encouraging results, ex-tracting a considerable amount of interesting tem-plates and showing inherent capability of discover-ing complex semantic relations.Concerning overall coverage, we managed to findcorrect templates for 86% of the verbs (46 out of53).
Nonetheless, presented results show a substan-tial margin of possible improvement.
In fact yieldvalues (5.5 Low Majority, up to 24 in best cases),which are our first concern, are inherently depen-dent on the breadth of Web search performed bythe ASE algorithm.
Due to computational time, themaximal number of anchor sets processed for eachverb was held back to 30, significantly reducing theamount of retrieved data.In order to further investigate ASE potential, wesubsequently performed some extended experimenttrials raising the number of anchor sets per pivotto 50.
This time we randomly chose a subset of10 verbs out of the less frequent ones in the origi-nal main experiment.
Results for these verbs in themain experiment were an average Yield of 3 and anaverage Precision of 45.19%.
In contrast, the ex-tended experiments on these verbs achieved a 6.5Yield and 59.95% Precision (average values).
Theseresults are indeed promising, and the substantialgrowth in Yield clearly indicates that the TE/ASEalgorithms can be further improved.
We thus sug-gest that the feasibility of our approach displays theinherent scalability of the TE/ASE process, and itspotential to acquire a large entailment relation KBusing a full scale lexicon.A further improvement direction relates to tem-plate ranking and filtering.
While in this paperwe considered anchor sets to have equal weights,we are also carrying out experiments with weightsbased on cross-correlation between anchor sets.5 ConclusionsWe have described a scalable Web-based approachfor entailment relation acquisition which requiresonly a standard phrasal lexicon as input.
This min-imal level of input is much simpler than requiredby earlier web-based approaches, while succeedingto maintain good performance.
This result showsthat it is possible to identify useful anchor sets ina fully unsupervised manner.
The acquired tem-plates demonstrate a broad range of semantic rela-tions varying from synonymy to more complicatedentailment.
These templates go beyond trivial para-phrases, demonstrating the generality and viabilityof the presented approach.From our current experiments we can expect tolearn about 5 relations per lexicon entry, at least forthe more frequent entries.
Moreover, looking at theextended test, we can extrapolate a notably largeryield by broadening the search space.
Together withthe fact that we expect to find entailment relationsfor about 85% of a lexicon, it is a significant steptowards scalability, indicating that we will be ableto extract a large scale KB for a large scale lexicon.In future work we aim to improve the yield by in-creasing the size of the sample-corpus in a qualita-tive way, as well as precision, using statistical meth-ods such as supervised learning for better anchor setidentification and cross-correlation between differ-ent pivots.
We also plan to support noun phrasesas input, in addition to verb phrases.
Finally, wewould like to extend the learning task to discover thecorrect entailment direction between acquired tem-plates, completing the knowledge required by prac-tical applications.Like (Lin and Pantel, 2001), learning the contextfor which entailment relations are valid is beyondthe scope of this paper.
As stated, we learn entail-ment relations holding for some, but not necessarilyall, contexts.
In future work we also plan to find thevalid contexts for entailment relations.AcknowledgementsThe authors would like to thank Oren Glickman(Bar Ilan University) for helpful discussions and as-sistance in the evaluation, Bernardo Magnini for hisscientific supervision at ITC-irst, Alessandro Vallinand Danilo Giampiccolo (ITC-irst) for their help indeveloping the human based evaluation, and Prof.Yossi Matias (Tel-Aviv University) for supervisingthe first author.
This work was partially supportedby the MOREWEB project, financed by ProvinciaAutonoma di Trento.
It was also partly carried outwithin the framework of the ITC-IRST (TRENTO,ITALY) ?
UNIVERSITY OF HAIFA (ISRAEL) col-laboration project.
For data visualization and analy-sis the authors intensively used the CLARK system(www.bultreebank.org) developed at the BulgarianAcademy of Sciences .ReferencesRegina Barzilay and Lillian Lee.
2003.
Learningto paraphrase: An unsupervised approach us-ing multiple-sequence alignment.
In Proceedingsof HLT-NAACL 2003, pages 16?23, Edmonton,Canada.Regina Barzilay and Kathleen R. McKeown.
2001.Extracting paraphrases from a parallel corpus.
InProceedings of ACL 2001, pages 50?57, Toulose,France.Ido Dagan and Oren Glickman.
2004.
Probabilis-tic textual entailment: Generic applied modelingof language variability.
In PASCAL Workshop onLearning Methods for Text Understanding andMining, Grenoble.Florence Duclaye, Franc?ois Yvon, and OlivierCollin.
2002.
Using the Web as a linguistic re-source for learning reformulations automatically.In Proceedings of LREC 2002, pages 390?396,Las Palmas, Spain.Oren Glickman and Ido Dagan.
2003.
Identifyinglexical paraphrases from a single corpus: a casestudy for verbs.
In Proceedings of RANLP 2003.Ulf Hermjakob, Abdessamad Echihabi, and DanielMarcu.
2003.
Natural language based reformula-tion resource and Web Exploitation.
In Ellen M.Voorhees and Lori P. Buckland, editors, Proceed-ings of the 11th Text Retrieval Conference (TREC2002), Gaithersburg, MD.
NIST.Christian Jacquemin.
1999.
Syntagmatic andparadigmatic representations of term variation.In Proceedings of ACL 1999, pages 341?348.Dekang Lin and Patrick Pantel.
2001.
Discovery ofinference rules for Question Answering.
NaturalLanguage Engineering, 7(4):343?360.Dekang Lin.
1998.
Dependency-based evaluationof MINIPAR.
In Proceedings of the Workshopon Evaluation of Parsing Systems at LREC 1998,Granada, Spain.Dan Moldovan and Vasile Rus.
2001.
Logic formtransformation of WordNet and its applicabilityto Question Answering.
In Proceedings of ACL2001, pages 394?401, Toulose, France.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations:Extracting paraphrases and generating new sen-tences.
In Proceedings of HLT-NAACL 2003, Ed-monton, Canada.Deepak Ravichandran and Eduard Hovy.
2002.Learning surface text patterns for a Question An-swering system.
In Proceedings of ACL 2002,Philadelphia, PA.Ellen Riloff and Rosie Jones.
1999.
Learning dic-tionaries for Information Extraction by multi-level bootstrapping.
In Proceedings of the Six-teenth National Conference on Artificial Intelli-gence (AAAI-99), pages 474?479.Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo,and Ralph Grishman.
2002.
Automatic para-phrase acquisition from news articles.
In Pro-ceedings of Human Language Technology Con-ference (HLT 2002), San Diego, USA.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern represen-tation model for automatic IE pattern acquisition.In Proceedings of ACL 2003.Roman Yangarber, Ralph Grishman, PasiTapanainen, and Silja Huttunen.
2000.
Un-supervised discovery of scenario-level patternsfor Information Extraction.
In Proceedings ofCOLING 2000.
