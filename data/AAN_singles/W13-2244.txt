Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 359?364,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsThe TALP-UPC Approach to System Selection: ASIYA Featuresand Pairwise Classification using Random ForestsLlu?
?s Formiga1, Meritxell Gonza`lez1, Alberto Barro?n-Ceden?o1,2Jose?
A. R. Fonollosa1 and Llu?
?s Ma`rquez11 TALP Research Center, Universitat Polite`cnica de Catalunya, Spain2 Facultad de Informa?tica, Universidad Polite?cnica de Madrid, Spain{lluis.formiga,jose.fonollosa}@upc.edu, {mgonzalez,albarron,lluism}@lsi.upc.eduAbstractThis paper describes the TALP-UPC par-ticipation in the WMT?13 Shared Taskon Quality Estimation (QE).
Our partic-ipation is reduced to task 1.2 on SystemSelection.
We used a broad set of fea-tures (86 for German-to-English and 97for English-to-Spanish) ranging from stan-dard QE features to features based onpseudo-references and semantic similarity.We approached system selection by meansof pairwise ranking decisions.
For that,we learned Random Forest classifiers es-pecially tailored for the problem.
Evalua-tion at development time showed consider-ably good results in a cross-validation ex-periment, with Kendall?s ?
values around0.30.
The results on the test set droppedsignificantly, raising different discussionsto be taken into account.1 IntroductionIn this paper we discuss the TALP-UPC1 partici-pation in the WMT?13 Shared Task on Quality Es-timation (QE).
Our participation is circumscribedto task 1.2, which deals with System Selection.Concretely, we were required to rank up to five al-ternative translations for the same source sentenceproduced by multiple MT systems, in the absenceof any reference translation.We used a broad set of features; mainly avail-able through the last version of the ASIYA toolkitfor MT evaluation2 (Gime?nez and Ma`rquez,2010).
Concretely, we derived 86 features forthe German-to-English subtask and 97 features forEnglish-to-Spanish.
These features cover differentapproaches and include standard Quality Estima-tion features, as provided by the above mentioned1Center for Language and Speech Technologies and Ap-plications (TALP), Technical University of Catalonia (UPC).2http://asiya.lsi.upc.eduASIYA toolkit and Quest (Specia et al 2010),but also a variety of features based on pseudo-references (Soricut and Echihabi, 2010), explicitsemantic analysis (Gabrilovich and Markovitch,2007) and specialized language models.
See sec-tion 3 for details.In order to model the ranking problem associ-ated to the system selection task, we adapted itto a classification task of pairwise decisions.
Wetrained Random Forest classifiers (and comparedthem to SVM classifiers), expanding the work ofFormiga et al(2013), from which a full rankingcan be derived and the best system per sentenceidentified.Evaluation at development time, using cross-validation, showed considerably good and stableresults for both language pairs, with correlationvalues around 0.30 (Kendall ?
coefficient) classi-fication accuracies around 52% (pairwise classifi-cation) and 41% (best translation identification).Unfortunately, the results on the test set were sig-nificantly lower.
Current research is devoted to ex-plain the behavior of the system at testing time.
Onthe one hand, it seems clear that more research re-garding the assignment of ties is needed in orderto have a robust model.
On the other hand, the re-lease of the gold standard annotations for the testset will facilitate a deeper analysis and understand-ing of the current results.The rest of the paper is organized as follows.Section 2 describes the ranking models studied forthe system selection problem.
Section 3 describesthe features used for learning.
Section 4 presentsthe setting for parameter optimization and featureselection and the results obtained.
Finally, Sec-tion 5 summarizes the lessons learned so far andoutlines some lines for further research.2 Ranking ModelWe considered two learning strategies to obtain thebest translation ranking model: SVM and Random359Forests.
Both strategies were based on predictingpairwise quality ranking decisions by means of su-pervised learning.
These decision was motivatedfrom our previous work (Formiga et al 2013)were we learned that they were more consistent toselect the best system (according to human and au-tomatic metrics) compared to absolute regressionapproaches.
In that work we used only the subsetof features 1, 2, 3 and 8 described in Section 3.For this shared task we have introduced additionalsimilarity measures (subsets 4 to 7) that feature se-mantic analysis and automatic alignments betweenthe source and the translations.The rationale for transforming a ranking prob-lem to a pairwise classification problem has beendescribed previously in several work (Joachims,2002; Burges et al 2005).
The main idea is to en-semble the features of both individuals and assigna class {-1,1} which tries to predict the pairwiserelation among them.
For linear based approachthis adaptation is as simple to compute the differ-ence between features between all the pairs of thetraining data.We used two different learners to perform thattask.
First, we trained a Support Vector Machineranker by means of pairwise comparison usingthe SVMlight toolkit (Joachims, 1999), but withthe ?-z p?
parameter, which can provide systemrankings for all the members of different groups.The learner algorithm was run according to thefollowing parameters: RBF-kernel, expanding theworking set by 9 variables at each iteration, for amaximum of 50,000 iterations and with a cachesize of 100 for kernel evaluations.
The trade-offparameter was empirically set to 0.001.
This im-plementation ignores the ties for the training stepas it only focuses in better than/ worse than rela-tions.Secondly, we used Random Forests (Breiman,2001), the rationale was the same as ranking-to-pairwise implementation from SVMlight.
How-ever, SVMlight considers two different data pre-processing methods depending on the kernel ofthe classifier: LINEAR and RBF-Kernel.
Weused the same data-preprocessing algorithm fromSVMlight in order to train a Random Forest clas-sifier with ties (three classes: {0,-1,1}) basedupon the pairwise relations.
We used the RandomForests implementation of scikit-learn toolkit (Pe-dregosa et al 2011) with 50 estimators.Once the classes are given by the Random For-est, we build a graph by means of the adjacencymatrix of the pairwise decision.
Once the adja-cency matrix has been built, we assign the finalranking through a dominance scheme similar toPighin et al(2012).
In that case, however, thereare not topological problems as the pairwise rela-tions are complete across all the edges.3 Features SetsWe considered a broad set of features: 97 and86 features for English-to-Spanish (en-es) andGerman-to-English (de-en), respectively.
Wegrouped them into the following categories: base-line QE metrics, comparison against pseudo-references, source-translation, and adapted lan-guage models.
We describe them below.
Unlessnoted otherwise, the features apply to both lan-guage pairs.3.1 Baseline FeaturesThe baseline features are composed of well-knownquality estimation metrics:1.
Quest Baseline (QQE)Seventeen baseline features from Specia etal.
(2010).
This set includes token counts(and their ratio), LM probabilities for sourceand target sentences, percentage of n-gramsin different quartiles of a reference corpus,number of punctuation marks, and fertilityratios.
We used these features in the en-espartition only.2.
ASIYA?s QE-based features (AQE)Twenty-six QE features provided byASIYA (Gonza`lez et al 2012), comprisingbilingual dictionary ambiguity and overlap;ratios concerning chunks, named-entities andPoS; source and candidate LM perplexitiesand inverse perplexities over lexical forms,chunks and PoS; and out-of-vocabulary wordindicators.3.2 Pseudo-Reference-based FeaturesSoricut and Echihabi (2010) introduced the con-cept of pseudo-reference-based features (PR) fortranslation ranking estimation.
The principle isthat, in the lack of human-produced references,automatic ones are still good for differentiatinggood from bad translations.
One or more sec-ondary MT systems are required to generate trans-lations starting from the same input, which are360taken as pseudo-references.
The similarity to-wards the pseudo-references can be calculatedwith any evaluation measure or text similarityfunction, which gives us all feature variants in thisgroup.
We consider the following PR-based fea-tures:3.
Derived from ASIYA?s metrics (APR)Twenty-three PR features, including GTM-l(l?
{1,2,3}) to reward different length match-ing (Melamed et al 2003), four variants ofROUGE (-L, -S*, -SU* and -W) (Lin andOch, 2004), WER (Nie?en et al 2000),PER (Tillmann et al 1997), TER, andTERbase (i.e., without stemming, synonymylook-up, nor paraphrase support) (Snover etal., 2009), and all the shallow and full pars-ing measures (i.e., constituency and depen-dency parsing, PoS, chunking and lemmas)that ASIYA provides either for Spanish or En-glish as target languages.4.
Lexical similarity (NGM)Cosine and Jaccard coefficient similaritymeasures for both token and charactern-grams considering n ?
[2, 5] (i.e., sixteenfeatures).
Additionally, one Jaccard-basedsimilarity measure for ?pseudo-prefixes?
(considering only up to four initial charactersfor every token).5.
Based on semantic information (SEM)Twelve features calculated with namedentity- and semantic role-based evaluationmeasures (again, provided by ASIYA).
Sen-tences are automatically annotated usingSwiRL (Surdeanu and Turmo, 2005) andBIOS (Surdeanu et al 2005).
We used thesefeatures in the de-en subtask only.6.
Explicit semantic analysis (ESA)Two versions of explicit semantic analy-sis (Gabrilovich and Markovitch, 2007), asemantic similarity measure, built on top ofWikipedia (we used the opening paragraphsof 100k Wikipedia articles as in 2010).3.3 Source-Translation Extra FeaturesSource-translation features include explicit com-parisons between the source sentence and its trans-lation.
They are meant to measure how adequatethe translation is, that is, to what extent the trans-lation expresses the same meaning as the source.Note that a considerable amount of the featuresdescribed in the baseline group (QQE and AQE)fall in this category.
In this subsection we includesome extra features we devised to capture source?translation dependencies.7.
Alignment-based features (ALG / ALGPR)One measure calculated over the alignedwords between a candidate translation andthe source (ALG); and two measures based onthe comparison between these alignments fortwo different translations (e.g., candidate andpseudo-reference) and the source (ALGPR).38.
Length model (LeM)A measure to estimate the quality likeli-hood of a candidate sentence by consideringthe ?expected length?
of a proper translationfrom the source.
The measure was introducedby (Pouliquen et al 2003) to identify docu-ment translations.
We estimated its param-eters over standard MT corpora, includingEuroparl, Newswire, Newscommentary andUN.3.4 Adapted Language-Model FeaturesWe interpolated different language models com-prising the WMT?12 Monolingual corpora (EPPS,News, UN and Gigafrench for English).
The in-terpolation weights were computed as to minimizethe perplexity according to the WMT TranslationTask test data (2008-2010)4.
The features are asfollow:9.
Language Model Features (LM)Two log-probabilities of the translation can-didate with respect to the above described in-terpolated language models over word formsand PoS labels.4 Experiments and ResultsIn this section we describe the experiments car-ried out to select the best feature set, learner, andlearner configuration.
Additionally, we presentthe final performance within the task.
The set-up experiments were addressed doing two separate10-fold cross validations on the training data andaveraging the final results.
We evaluated the re-sults through three indicators: Kendall?s ?
with no3Alignments were computed with the Berkeley alignerhttps://code.google.com/p/berkeleyaligner/4http://www.statmt.org/wmt13/translation-task.html361penalization for the ties, accuracy in determiningthe pairwise relationship between candidate trans-lations, and global accuracy in selecting the bestcandidate for each source sentence.First, we compared our SVM learner againstRandom Forests with the two variants of datapreprocessing (LINEAR and RBF).
In terms ofKendall?s ?
, we found that the Random Forests(RF) were clearly better compared to SVM imple-mentation.
Concretely, depending on the final fea-ture set, we found that RF achieved a ?
between0.23 and 0.29 while SVM achieved a ?
between0.23 and 0.25.
With respect to the accuracy mea-sures we did not find noticeable differences be-tween methods as their results moved from 49% to52%.
However, considering the accuracy in termsof selecting only the best system there was a dif-ference of two points (42.2% vs. 40.0%) betweenmethods, being RF again the best system.
Regard-ing the pairwise preprocessing the results betweenRBF and LINEAR based preprocessing were com-parable, being RBF slightly better than LINEAR.Hence, we selected Random Forests with RBFpairwise preprocessing as our final learner.de-en ?
with ties AccuracyIgnored Penalized All BestAQE+LeM+ALGPR+LM 33.70 15.72 52.56 41.57AQE+SEM+LM 32.49 14.61 52.72 40.92AQE+LeM+ALGPR+ESA+LM 32.08 13.81 52.71 41.37AQE+ALG+ESA+SEM+LM 32.06 13.96 52.20 40.64AQE+ALG+LM 31.97 14.29 52.00 40.83AQE+LeM+ALGPR+SEM+LM 31.93 13.57 52.52 40.98AQE+ESA+SEM+LM 31.79 13.68 52.50 40.76AQE+LeM+ALGPR+ESA+SEM+LM 31.72 14.01 52.65 40.83AQE+ALG+SEM+LM 31.17 12.86 52.18 40.51AQE+ALG+SEM 30.72 12.58 51.75 39.66AQE+LeM+ALGPR+ESA+SEM 30.47 11.79 51.85 39.58AQE+ESA+LM 30.31 12.23 52.60 40.69AQE+ALG+ESA+LM 30.26 12.40 52.03 40.99AQE+LeM+ALGPR 30.24 11.83 51.96 40.42AQE+LeM+ALGPR+SEM 30.23 11.84 52.10 40.32AQE+LeM+ALGPR+ESA 29.89 11.87 51.83 40.07AQE+ALG+ESA 29.81 11.30 51.37 39.47AQE+SEM 29.80 12.06 51.75 39.52AQE+NGM+APR+ESA+SEM+LM 29.34 10.58 51.33 38.55AQE+ESA+SEM 29.31 11.46 51.66 39.24AQE+ESA 29.13 11.12 51.82 39.90AQE+ALG+ESA+SEM 28.35 10.32 51.37 38.98AQE+NGM+APR+ESA+SEM 27.55 9.22 51.01 38.12Table 1: Set-up results for de-enFor the feature selection process, we consideredthe most relevant combinations of feature groups.Table 1 shows the set-up results for the de-en sub-task and Table 2 shows the results for the en-essubtask.In terms of ?
we observed similar results be-tween the two language pairs.
However accura-cies for the de-en subtask were one point abovethe ones for en-es.
Regarding the features used, wefound that the best feature combination to use wascomposed of: i) a baseline QE feature set (Asiyaor Quest) but not both of them, ii) Length Model,iii) Pseudo-reference aligned based features andthe use of iv) adapted language models.
However,within the de-en subtask, we found that substitut-ing Length Model and Aligned Pseudo-referencesby the features based on Semantic Roles (SEM)could bring marginally better accuracy.
We alsonoticed that the learner was sensitive to the fea-tures used so selecting the appropriate set of fea-tures was crucial to achieve a good performance.en-es ?
with ties AccuracyIgnored Penalized All BestQQE+LeM+ALGPR+LM 33.81 15.87 51.66 41.01AQE+LeM+ALGPR+LM 33.75 16.44 51.56 41.52QQE+AQE+LM 32.71 14.59 51.18 41.02QQE+AQE+LM+ESA 32.69 15.30 51.48 41.30QQE+AQE+LeM+ALGPR+LM+ESA 32.63 13.64 51.39 40.48QQE+AQE+LeM+ALGPR+LM 32.41 14.06 51.43 40.49QQE+LeM+ALGPR+LM+ESA 31.66 13.39 51.37 41.05QQE+AQE+ALG+LM 31.46 13.62 51.28 41.29AQE+LeM+ALGPR+LM+ESA 31.29 14.10 51.55 41.43QQE+AQE+ALG+LM+ESA 31.25 13.58 51.64 41.66QQE+AQE+NGM+APR+LM+ESA 30.58 12.48 50.93 40.66QQE+AQE+NGM+APR+LM 29.94 12.54 50.95 40.25QQE+AQE 28.98 10.92 49.97 39.65QQE+AQE+LeM+ALGPR 28.94 10.48 49.99 39.71QQE+AQE+NGM+ESA+LM 28.85 11.88 50.90 40.22AQE+LeM+ALGPR 28.81 10.11 50.06 40.01QQE+AQE+ESA 28.68 10.31 49.96 39.27AQE+ESA 28.67 10.81 50.35 39.18AQE 28.65 10.68 49.76 38.90QQE+AQE+ALG 28.47 9.63 49.67 39.66QQE+AQE+NGM+APR+ESA 28.43 9.75 49.67 38.74QQE+AQE+NGM 27.23 9.10 49.44 38.98QQE+AQE+ALG+ESA 27.08 7.93 50.26 39.71QQE+AQE+LeM+ALGPR+ESA 27.03 8.65 50.35 40.49AQE+LeM+ALGPR+ESA 26.96 8.26 50.30 39.47QQE+AQE+NGM+ESA 26.59 7.56 49.52 38.62QQE+AQE+NGM+APR 25.39 6.97 49.90 39.53Table 2: Setup results for en-esde-en ?
(ties penalized,ID non-symmetric between [-1,1])Best 0.31UPC AQE+SEM+LM 0.11UPC AQE+LeM+ALGPR+LM 0.10Baseline Random-ranks-with-ties -0.12Worst -0.49Table 3: Official results for the de-en subtask (tiespenalized)en-es ?
(ties penalized,ID non-symmetric between [-1,1])Best 0.15UPC QQE+LeM+ALGPR+LM -0.03UPC AQE+LeM+ALGPR+LM -0.06Baseline Random-ranks-with-ties -0.23Worst -0.63Table 4: Official results for the en-es subtask (tiespenalized)In Tables 3, 4, 5 and 6 we present the official re-sults for the WMT?13 Quality Estimation Task, inall evaluation variants.
In each table we compareto the best/worst performing systems and also tothe official baseline.We can observe that in general the results onthe test sets drop significantly, compared to our362de-en ?
(ties ignored, Non-tiesID symmetric /between [-1,1]) (882 dec.)Best 0.31 882UPC AQE+SEM+LM 0.27 768UPC AQE+LeM+ALGPR+LM 0.24 788Baseline Random-ranks-with-ties 0.08 718Worst -0.03 558Table 5: Official results for the de-en subtask (tiesignored)en-es ?
(ties ignored, Non-tiesID symmetric /between [-1,1]) (882 dec.)Best 0.23 192UPC QQE+LeM+ALGPR+LM 0.11 554UPC AQE+LeM+ALGPR+LM 0.08 554Baseline Random-ranks-with-ties 0.03 507Worst -0.11 633Table 6: Official results for the en-es subtask (tiesignored)set-up experiments.
Restricting to the evaluationsetting in which ties are not penalized (i.e., cor-responding to our setting during system and pa-rameter tuning), we can see that the results corre-sponding to de-en (Table 5) are comparable to ourset-up results and close to the best performing sys-tem.
However, in the en-es language pair the finalresults are comparatively much lower (Table 6).We find this behavior strange.
In this respect, weanalyzed the inter-annotator agreement within thegold standard.
Concretely we computed the Co-hen?s ?
for all overlapping annotations concerningat least 4 systems for both language pairs.
The re-sults of our analysis are presented in Table 7 andtherefore it confirms our hypothesis that en-es an-notations had more noise providing an explanationfor the accuracy decrease of our QE models andsetting the subtask into a more challenging sce-nario.
However, further research will be needed toanalyze other factors such as oracles and improve-ment on automatic metrics prediction and reliabil-ity compared to linguistic expert annotators.Another remaining issue for our research con-cerns investigating better ways to deal with ties,as their penalization lowered our results dramati-cally.
In this direction we plan to work further on# of Lang Cohen?s # ofsystems ?
elements4 en-es 0.210 560de-en 0.369 6405 en-es 0.211 130de-en 0.375 145Table 7: Golden standard test set agreement coef-ficients measured by Cohen?s ?the adjacency matrix reconstruction heuristics andpresenting the features to the learner in a struc-tured form.5 ConclusionsThis paper described the TALP-UPC participationin the WMT?13 Shared Task.
We approached theQuality Estimation task based on system selection,where different systems have to be ranked accord-ing to their quality.
We derive a full ranking andidentify the best system per sentence on the basisof Random Forest classifiers.After the model set-up, we observed consid-erably good and robust results for both transla-tion directions, German-to-English and English-to-Spanish: Kendall?s ?
around 0.30 as well asaccuracies around 52% on pairwise classificationand 41% on best translation identification.
How-ever, the results over the official test set weresignificantly lower.
We have found that the lowinter-annotator agreement between users on thatset might provide an explanation to the poor per-formance of our QE models.Our current efforts are centered on explainingthe behavior of our QE models when facing the of-ficial test sets.
We are following two directions: i)studying the ties?
impact to come out with a morerobust model and ii) revise the English-to-Spanishgold standard annotations in terms of correlationwith automatic metrics to facilitate a deeper un-derstanding of the results.AcknowledgmentsAcknowledgementsThis work has been partially funded by theSpanish Ministerio de Econom?
?a y Competitivi-dad, under contracts TEC2012-38939-C03-02and TIN2009-14675-C03, as well as fromthe European Regional Development Fund(ERDF/FEDER) and the European Commu-nity?s FP7 (2007-2013) program under thefollowing grants: 247762 (FAUST, FP7-ICT-2009-4-247762) and 246016 (ERCIM ?AlainBensoussan?
Fellowship).ReferencesLeo Breiman.
2001.
Random forests.
Machine Learn-ing, 45(1):5?32.Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,Matt Deeds, Nicole Hamilton, and Greg Hullender.3632005.
Learning to rank using gradient descent.
InProceedings of the 22nd international conference onMachine learning, pages 89?96.
ACM.Llu?
?s Formiga, Llu?
?s Ma`rquez, and Jaume Pujantell.2013.
Real-life translation quality estimation for mtsystem selection.
In Proceedings of 14th MachineTranslation Summit (MT Summit), Nice, France,September.
EAMT.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.
In Proceedingsof the 20th International Joint Conference on Artifi-cial Intelligence, pages 1606?1611, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010.
Asiya: AnOpen Toolkit for Automatic Machine Translation(Meta-)Evaluation.
The Prague Bulletin of Mathe-matical Linguistics, (94):77?86.Meritxell Gonza`lez, Jesu?s Gime?nez, and Llu??sMa`rquez.
2012.
A graphical interface for mt evalu-ation and error analysis.
In Proceedings of the ACL2012 System Demonstrations, pages 139?144, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Thorsten Joachims, 1999.
Advances in Kernel Methods?
Support Vector Learning, chapter Making large-Scale SVM Learning Practical.
MIT Press.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In ACM, editor, Proceed-ings of the ACM Conference on Knowledge Discov-ery and Data Mining (KDD).Chin-Yew Lin and Franz Josef Och.
2004.
Auto-matic evaluation of machine translation quality us-ing longest common subsequence and skip-bigramstatistics.
In Proceedings of the 42nd Meetingof the Association for Computational Linguistics(ACL?04), Main Volume, pages 605?612, Barcelona,Spain, July.I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and recall of machine translation.In HLT-NAACL.Sonja Nie?en, Franz Josef Och, Gregor Leusch, andHermann Ney.
2000.
An evaluation tool for ma-chine translation: Fast evaluation for mt research.In Proceedings of the 2nd Language Resources andEvaluation Conference (LREC 2000).F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine Learn-ing in Python .
Journal of Machine Learning Re-search, 12:2825?2830.Daniele Pighin, Llu?
?s Formiga, and Llu?
?s Ma`rquez.2012.
A graph-based strategy to streamline trans-lation quality assessments.
In Proceedings of theTenth Conference of the Association for MachineTranslation in the Americas (AMTA?2012), SanDiego, USA, October.
AMTA.Bruno Pouliquen, Ralf Steinberger, and Camelia Ignat.2003.
Automatic Identification of Document Trans-lations in Large Multilingual Document Collections.In Proceedings of the International Conference onRecent Advances in Natural Language Processing(RANLP-2003), pages 401?408, Borovets, Bulgaria.Matthew G. Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
TER-Plus: Paraphrase,Semantic, and Alignment Enhancements to Trans-lation Edit Rate.
Machine Translation, 23(2):117?127.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translationsvia ranking.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 612?621, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.Machine Translation Evaluation Versus Quality Es-timation.
Machine Translation, 24:39?50, March.Mihai Surdeanu and Jordi Turmo.
2005.
SemanticRole Labeling Using Complete Syntactic Analysis.In Proceedings of CoNLL Shared Task.Mihai Surdeanu, Jordi Turmo, and Eli Comelles.
2005.Named Entity Recognition from Spontaneous Open-Domain Speech.
In Proceedings of the 9th Inter-national Conference on Speech Communication andTechnology (Interspeech).C.
Tillmann, S. Vogel, H. Ney, A. Zubiaga, andH Sawaf.
1997.
Accelerated dp based search forstatistical translation.
In Proceedings of EuropeanConference on Speech Communication and Technol-ogy.364
