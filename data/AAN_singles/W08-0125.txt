Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 156?163,Columbus, June 2008. c?2008 Association for Computational LinguisticsModelling and Detecting Decisions in Multi-party DialogueRaquel Ferna?ndez, Matthew Frampton, Patrick Ehlen, Matthew Purver, and Stanley PetersCenter for the Study of Language and InformationStanford University{raquel|frampton|ehlen|mpurver|peters}@stanford.eduAbstractWe describe a process for automatically de-tecting decision-making sub-dialogues in tran-scripts of multi-party, human-human meet-ings.
Extending our previous work on ac-tion item identification, we propose a struc-tured approach that takes into account the dif-ferent roles utterances play in the decision-making process.
We show that this structuredapproach outperforms the accuracy achievedby existing decision detection systems basedon flat annotations, while enabling the extrac-tion of more fine-grained information that canbe used for summarization and reporting.1 IntroductionIn collaborative and organized work environments,people share information and make decisions exten-sively through multi-party conversations, usually inthe form of meetings.
When audio or video record-ings are made of these meetings, it would be valu-able to extract important information, such as thedecisions that were made and the trains of reason-ing that led to those decisions.
Such a capabilitywould allow work groups to keep track of coursesof action that were shelved or rejected, and could al-low new team members to get quickly up to speed.Thanks to the recent availability of substantial meet-ing corpora?such as the ISL (Burger et al, 2002),ICSI (Janin et al, 2004), and AMI (McCowan etal., 2005) Meeting Corpora?current research on thestructure of decision-making dialogue and its use forautomatic decision detection has helped to bring thisvision closer to reality (Verbree et al, 2006; Hsuehand Moore, 2007b).Our aim here is to further that research by ap-plying a simple notion of dialogue structure to thetask of automatically detecting decisions in multi-party dialogue.
A central hypothesis underlying ourapproach is that this task is best addressed by tak-ing into account the roles that different utterancesplay in the decision-making process.
Our claim isthat this approach facilitates both the detection ofregions of discourse where decisions are discussedand adopted, and also the identification of importantaspects of the decision discussions themselves, thusopening the way to better and more concise report-ing.In the next section, we describe prior work on re-lated efforts, including our own work on action itemdetection (Purver et al, 2007).
Sections 3 and 4 thenpresent our decision annotation scheme, which dis-tinguishes several types of decision-related dialogueacts (DAs), and the corpus used as data (in this studya section of the AMI Meeting Corpus).
Next, in Sec-tion 5, we describe our experimental methodology,including the basic conception of our classificationapproach, the features we used in classification, andour evaluation metrics.
Section 6 then presents ourresults, obtained with a hierarchical classifier thatfirst trains individual sub-classifiers to detect the dif-ferent types of decision DAs, and then uses a super-classifier to detect decision regions on the basis ofpatterns of these DAs, achieving an F-score of 58%.Finally, Section 7 presents some conclusions and di-rections for future work.2 Related WorkRecent years have seen an increasing interest in re-search on decision-making dialogue.
To a greatextent, this is due to the fact that decisions have156been shown to be a key aspect of meeting speech.User studies (Lisowska et al, 2004; Banerjee et al,2005) have shown that participants regard decisionsas one of the most important outputs of a meeting,while Whittaker et al (2006) found that the develop-ment of an automatic decision detection componentis critical to the re-use of meeting archives.
Identify-ing decision-making regions in meeting transcriptscan thus be expected to support development of awide range of applications, such as automatic meet-ing assistants that process, understand, summarizeand report the output of meetings; meeting trackingsystems that assist in implementing decisions; andgroup decision support systems that, for instance,help in constructing group memory (Romano andNunamaker, 2001; Post et al, 2004; Voss et al,2007).Previously researchers have focused on the in-teractive aspects of argumentative and decision-making dialogue, tackling issues such as the detec-tion of agreement and disagreement and the levelof emotional involvement of conversational partic-ipants (Hillard et al, 2003; Wrede and Shriberg,2003; Galley et al, 2004; Gatica-Perez et al, 2005).From a perhaps more formal perspective, Verbree etal.
(2006) have created an argumentation scheme in-tended to support automatic production of argumentstructure diagrams from decision-oriented meetingtranscripts.
Only Hsueh and Moore (2007a; 2007b),however, have specifically investigated the auto-matic detection of decisions.Using the AMI Meeting Corpus, Hsueh andMoore (2007b) attempt to identify the dialogue acts(DAs) in a meeting transcript that are ?decision-related?.
The authors define these DAs on the ba-sis of two kinds of manually created summaries: anextractive summary of the whole meeting, and anabstractive summary of the decisions made in themeeting.
Those DAs in the extractive summary thatsupport any of the decisions in the abstractive sum-mary are then manually tagged as decision-relatedDAs.
They trained a Maximum Entropy classifierto recognize this single DA class, using a variety oflexical, prosodic, dialogue act and topical features.The F-score they achieved was 0.35, which gives agood indication of the difficulty of this task.In our previous work (Purver et al, 2007), we at-tempted to detect a particular kind of decision com-mon in meetings, namely action items?public com-mitments to perform a given task.
In contrast tothe approach adopted by Hsueh and Moore (2007b),we proposed a hierarchical approach where indi-vidual classifiers were trained to detect distinct ac-tion item-related DA classes (task description, time-frame, ownership and agreement) followed by asuper-classifier trained on the hypothesized class la-bels and confidence scores from the individual clas-sifiers that would detect clusters of multiple classes.We showed that this structured approach producedbetter classification accuracy (around 0.39 F-scoreon the task of detecting action item regions) than aflat-classifier baseline trained on a single action itemDA class (around 0.35 F-score).In this paper we extend this approach to the moregeneral task of detecting decisions, hypothesizingthat?as with action items?the dialogue acts in-volved in decision-making dialogue form a ratherheterogeneous set, whose members co-occur in par-ticular kinds of patterns, and that exploiting thisricher structure can facilitate their detection.3 Decision Dialogue ActsWe are interested in identifying the main conver-sational units in a decision-making process.
We ex-pect that identifying these units will help in detect-ing regions of dialogue where decisions are made(decision sub-dialogues), while also contributing toidentification and extraction of specific decision-related bits of information.Decision-making dialogue can be complex, ofteninvolving detailed discussions with complicated ar-gumentative structure (Verbree et al, 2006).
Deci-sion sub-dialogues can thus include a great deal ofinformation that is potentially worth extracting.
Forinstance, we may be interested in knowing what adecision is about, what alternative proposals wereconsidered during the decision process, what argu-ments were given for and against each of them, andlast but not least, what the final resolution was.Extracting these and other potential decision com-ponents is a challenging task, which we do not in-tend to fully address in this paper.
This initial studyconcentrates on three main components we believeconstitute the backbone of decision sub-dialogues.A typical decision sub-dialogue consists of threemain components that often unfold in sequence.
(a)157key DDA class descriptionI issue utterances introducing the issue or topic under discussionR resolution utterances containing the decision that is adoptedRP ?
proposal ?
utterances where the decision adopted is proposedRR ?
restatement ?
utterances where the decision adopted is confirmed or restatedA agreement utterances explicitly signalling agreement with the decision madeTable 1: Set of decision dialogue act (DDA) classesA topic or issue that requires some sort of conclu-sion is initially raised.
(b) One or more proposals areconsidered.
And (c) once some sort of agreement isreached upon a particular resolution, a decision isadopted.Dialogue act taxonomies often include tagsthat can be decision-related.
For instance, theDAMSL taxonomy (Core and Allen, 1997) in-cludes the tags agreement and commit, as wellas a tag open-option for utterances that ?sug-gest a course of action?.
Similarly, the AMIDA scheme1 incorporates tags like suggest,elicit-offer-or-suggestion and assess.These tags are however very general and do not cap-ture the distinction between decisions and more gen-eral suggestions and commitments.2 We thereforedevised a decision annotation scheme that classifiesutterances according to the role they play in the pro-cess of formulating and agreeing on a decision.
Ourscheme distinguishes among three main decision di-alogue act (DDA) classes: issue (I), resolution (R),and agreement (A).
Class R is further subdivided intoresolution proposal (RP) and resolution restatement(RR).
A summary of the classes is given in Table 1.Annotation of the issue class includes any utter-ances that introduce the topic of the decision discus-sion.
For instance, in example (1) below, the utter-ances ?Are we going to have a backup??
and ?Butwould a backup really be necessary??
are tagged asI.
The classes RP and RR are used to annotate thoseutterances that specify the resolution adopted?i.e.the decision made.
Annotation with the class RPincludes any utterances where the resolution is ini-1A full description of the AMI Meeting Corpus DA schemeis available at http://mmm.idiap.ch/private/ami/annotation/dialogue acts manual 1.0.pdf, afterfree registration.2Although they can of course be used to aid the identificationprocess?see Section 5.3.tially proposed (like the utterance ?I think maybe wecould just go for the kinetic energy.
.
.
?).
Sometimesdecision discussions include utterances that sum upthe resolution adopted, like the utterance ?Okay,fully kinetic energy?
in (1).
This kind of utteranceis tagged with the class RR.
Finally, the agreementclass includes any utterances in which participantsagree with the (proposed) resolution, like the utter-ances ?Yeah?
and ?Good?
as well as ?Okay?
in di-alogue (1).
(1) A: Are we going to have a backup?Or we do just?B: But would a backup really be necessary?A: I think maybe we could just go for thekinetic energy and be bold and innovative.C: Yeah.B: I think?
yeah.A: It could even be one of our selling points.C: Yeah ?laugh?.D: Environmentally conscious or something.A: Yeah.B: Okay, fully kinetic energy.D: Good.3Note that an utterance can be assigned to morethan one of these classes.
For instance, the utter-ance ?Okay, fully kinetic energy?
is annotated bothas RR and A.
Similarly, each decision sub-dialoguemay contain more than one utterance correspondingto each class, as we saw above for issue.
Whilewe do not a priori require each of these classes tobe present for a set of utterances to be considereda decision sub-dialogue, all annotated decision sub-dialogues in our corpus include the classes I, RP andA.
The annotation process and results are describedin detail in the next section.3This example was extracted from the AMI dialogueES2015c and has been modified slightly for presentation pur-poses.1584 Data: Corpus & AnnotationIn this study, we use 17 meetings from the AMIMeeting Corpus (McCowan et al, 2005), a pub-licly available corpus of multi-party meetings con-taining both audio recordings and manual transcrip-tions, as well as a wide range of annotated infor-mation including dialogue acts and topic segmenta-tion.
Conversations are all in English, but they caninclude native and non-native English speakers.
Allmeetings in our sub-corpus are driven by an elicita-tion scenario, wherein four participants play the roleof project manager, marketing expert, interface de-signer, and industrial designer in a company?s de-sign team.
The overall sub-corpus makes up a totalof 15,680 utterances/dialogue acts (approximately920 per meeting).
Each meeting lasts around 30minutes.Two authors annotated 9 and 10 dialogues each,overlapping on two dialogues.
Inter-annotatoragreement on these two dialogues was similar to(Purver et al, 2007), with kappa values rangingfrom 0.63 to 0.73 for the four DDA classes.
Thehighest agreement was obtained for class RP and thelowest for class A.4On average, each meeting contains around 40DAs tagged with one or more of the DDA sub-classes in Table 1.
DDAs are thus very sparse, cor-responding to only 4.3% of utterances.
When welook at the individual DDA sub-classes this is evenmore pronounced.
Utterances tagged as issue makeup less than 0.9% of utterances in a meeting, whileutterances annotated as resolution make up around1.4%?1% corresponding to RP and less than 0.4%to RR on average.
Almost half of DDA utterances(slightly over 2% of all utterances on average) aretagged as belonging to class agreement.We compared our annotations with the annota-tions of Hsueh and Moore (2007b) for the 17 meet-ings of our sub-corpus.
The overall number of ut-terances annotated as decision-related is similar inthe two studies: 40 vs. 30 utterances per meeting onaverage, respectively.
However, the overlap of theannotations is very small leading to negative kappascores.
As shown in Figure 1, only 12.22% of ut-4The annotation guidelines we used are available on-line at http://godel.stanford.edu/twiki/bin/view/Calo/CaloDecisionDiscussionSchema  !"##%&##'('Figure 1: Overlap with AMI annotationsterances tagged with one of our DDA classes corre-spond to an utterance annotated as decision-relatedby Hsueh & Moore.
While presumably this is aconsequence of our different definitions for DDAs,it seems also partially due to the fact that some-times we disagreed about where decisions were be-ing made.
Most of the overlap is found with ut-terances tagged as resolution (RP or RR).
Around32% of utterances tagged as resolution overlap withAMI DDAs, while the overlap with utterances anno-tated as issue and agreement is substantially lower?around 7% and 1.5%, respectively.
This is perhapsnot surprising given their definition of a ?decision-related?
DA (see Section 2).
Classes like issue andespecially agreement shape the interaction patternsof decision-sub-dialogues, but are perhaps unlikelyto appear in an extractive summary.55 Experiments5.1 ClassifiersOur hierarchical approach to decision detection in-volves two steps:1.
We first train one independent sub-classifier forthe identification of each of our DDA classes,using features derived from the properties ofthe utterances in context (see below).2.
To detect decision sub-dialogues, we then traina super-classifier, whose features are the hy-pothesized class labels and confidence scores5Although, as we shall see in Section 6.2, they contributeto improve the detection of decision sub-dialogues and of otherDDA classes.159from the sub-classifiers, over a suitable win-dow.6The super-classifier is then able to ?correct?
theDDA classes hypothesized by the sub-classifiers onthe basis of richer contextual information: if a DA isclassified as positive by a sub-classifier, but negativeby the super-classifier, then this sub-classification is?corrected?, i.e.
it is changed to negative.
Hencethis hierarchical approach takes advantage of the factthat within decision sub-dialogues, our DDAs can beexpected to co-occur in particular types of patterns.We use the linear-kernel support vector machineclassifier SVMlight (Joachims, 1999) in all classifi-cation experiments.5.2 EvaluationIn all cases we perform 17-fold cross-validation,each fold training on 16 meetings and testing on theremaining one.We can evaluate the performance of our approachat three levels: the accuracy of the sub-classifiers indetecting each of the DDA classes, the accuracy ob-tained in detecting DDA classes after the output ofthe sub-classifiers has been corrected by the super-classifier, and the accuracy of the super-classifierin detecting decision sub-dialogues.
For the DDAidentification task (both uncorrected and corrected)we use the same lenient-match metric as Hsueh andMoore (2007b), which allows a margin of 20 sec-onds preceding and following a hypothesized DDA.7We take as reference the results they obtained on de-tecting their decision-related DAs.For the evaluation of the decision sub-dialoguedetection task, we follow (Purver et al, 2007) anduse a windowed metric that divides the dialogue into30-second windows and evaluates on a per windowbasis.
As a baseline for this task, we compare theperformance of our hierarchical approach to a flatclassification approach, first using the flat annota-tions of Hsueh and Moore (2007a) that only includea single DDA class, and second using our annota-tions, but for the binary classification of whether anutterance is decision-related or not, without distin-guishing among our DDA sub-classes.6The width of this window is estimated from the trainingdata and corresponds to the average length in utterances of adecision sub-dialogue?25 in our sub-corpus.7Note that here we only give credit for hypotheses based ona 1?1 mapping with the gold-standard labels.5.3 FeaturesTo train the DDA sub-classifiers we extracted utter-ance features similar to those used by Purver et al(2007) and Hsueh and Moore (2007b): lexical un-igrams and durational and locational features fromthe transcripts; prosodic features extracted from theaudio files using Praat (Boersma, 2001); general DAtags and speaker information from the AMI annota-tions; and contextual features consisting of the sameset of features from immediately preceding and fol-lowing utterances.
Table 2 shows the full feature set.Lexical unigrams after text normalizationUtterance length in words, duration in seconds,percentage of meetingProsodic pitch & intensity min/max/mean/dev,pitch slope, num of voice framesDA AMI dialogue act classSpeaker speaker id & AMI speaker roleContext features as above for utterancesu +/- 1. .
.u +/- 5Table 2: Features for decision DA detection6 Results6.1 BaselineOn the task of detecting decision-related DAs,Hsueh and Moore (2007b) report an F-score of 0.33when only lexical features are employed.
Usinga combination of different features allows them toboost the score to 0.35.
Although the differencesboth in definition and prior distribution betweentheir DAs and our DDA classes make direct com-parisons unstraightforward (see Sec.
4), we considerthis result a baseline for the DDA detection task.As a baseline system for the decision sub-dialogue detection task, we use a flat classifiertrained on the word unigrams of the current utter-ance (lexical features) and the unigrams of the im-mediately preceding and following utterances (+/-1-utterance context).
Table 3 shows the accuracy per30-second window obtained when a flat classifier isapplied to AMI annotations and to our own anno-tations, respectively.8 In general, the flat classifiersyield high recall (over 90%) but rather low precision(below 35%).8Note that the task of detecting decision sub-dialogues is notdirectly addressed by (Hsueh and Moore, 2007b).160As can be seen, using our DA annotations (CALODDAs) with all sub-classes merged into a singleclass yields better results than using the AMI DDAflat annotations.
The reasons behind this result arenot entirely obvious.
In principle, our annotatedDDAs are by definition less homogeneous than theAMI DDAs, which could lead to a lower perfor-mance in a simple binary approach.
It seems how-ever that the regions that contain our DDAs areeasier to detect than the regions that contain AMIDDAs.Flat classifier Re Pr F1AMI DDAs .97 .21 .34CALO DDAs .96 .34 .50Table 3: Flat classifiers with lexical features and +/?1-utterance context6.2 Hierarchical ResultsPerformance of the hierarchical classifier with lex-ical features and +/- 1-utterance context is shownin Table 4.
The results of the super-classifier canbe compared directly to the baseline flat classifierof Table 3.
We can see that the use of the super-classifier to detect decision sub-dialogues gives asignificantly improved performance over the flat ap-proach.
This is despite low sub-classifier perfor-mance, especially for the classes with very low fre-quency of occurrence like RR.
Precision for decisionsub-dialogue detection improves around 0.5 points(p < 0.05 on an paired t-test), boosting F-scores to0.55 (p < 0.05).
The drop in recall from 0.96 to0.91 is not statistically significant.sub-classifiers superI RP RR A classifierRe .25 .44 .09 .88 .91Pr .21 .24 .14 .18 .39F1 .23 .31 .11 .30 .55Table 4: Hierarchical classifier with lexical features and+/?1-utterance contextWe investigated whether we could improve resultsfurther by using additional features, and found thatwe could.
The best results obtained with the hierar-chical classifier are shown in Table 5.
We appliedfeature selection to the features shown in Table 2using information gain and carried out several trialclassifier experiments.
Like Purver et al (2007) and(Hsueh and Moore, 2007b), we found that lexicalfeatures increase classifier performance the most.As DA features, we used the AMI DA tagselicit-assessment, suggest and assess forclasses I and A; and tags suggest, fragment andstall, for classes RP and RR.
Only the DA featuresfor the Resolution sub-classes (RP and RR) gave sig-nificant improvements (p < 0.05).
Utterance andspeaker features were found to improve the recallof the sub-classes significantly (p < 0.05), and theprecision of the super-classifier (p < 0.05).
As forprosodic information, we found minimum and max-imum intensity to be the most generally predictive,but although these features increased recall, theycaused precision and F-scores to decrease.When we experimented with contextual features(i.e.
features from utterances before and after thecurrent dialogue act), we only found lexical contex-tual features to be useful.
With the current dataset,for classes I, RP and RR, the optimal amount of lex-ical contextual information turned out to be +/- 1utterances, while for class A increasing the amountof lexical contextual information to +/-5 utterancesyielded better results, boosting both precision andF-score (p < 0.05).
Speaker, utterance, DA andprosodic contextual features gave no improvement.The scores on the left hand side of Table 5 showthe best results obtained with the sub-classifiers foreach of the DDA classes.
We found however thatthe super-classifier was able to improve over theseresults by correcting the hypothesized labels on thebasis of the DDA patterns observed in context (seethe corrected results on Table 5).
In particular, preci-sion increased from 0.18 to 0.20 for class I and from0.28 to 0.31 for class RP (both results are statisti-cally significant, p < 0.05).
Our best F-score forclass RP (which is the class with the highest over-lap with AMI DDAs) is a few points higher than theone reported in (Hsueh and Moore, 2007b)?0.38vs.
0.35, respectively.Next we investigated the contribution of the classagreement.
Although this class is not as informa-tive for summarization and reporting as the otherDDA classes, it plays a key role in the interactiveprocess that shapes decision sub-dialogues.
Indeed,including this class helps to detect other more con-tentful DDA classes such as issue and resolution.161sub-classifiers corr.
sub-classifiers corr.
sub.
w/o A super superI RP RR A I RP RR A I RP RR w/o A with ARe .45 .49 .18 .55 .43 .48 .18 .55 .43 .48 .18 .91 .88Pr .18 .28 .14 .30 .20 .31 .14 .30 .18 .30 .14 .36 .43F1 .25 .36 .16 .39 .28 .38 .16 .39 .
26 .37 .16 .52 .58Table 5: Hierarchical classifier with uncorrected and corrected results for sub-classifiers, with and w/o class A; lexical,utterance, and speaker features; +/?1-utt lexical context for I-RP-RR and +/?5-utt lexical context for A.Table 5 also shows the results obtained with the hi-erarchical classifier when class A is ignored.
In thiscase the small correction observed in the precision ofclasses I and RP w.r.t.
the original output of the sub-classifiers is not statistically significant.
The perfor-mance of the super-classifier (sub-dialogue detec-tion) also decreases significantly in this condition:0.43 vs. 0.36 precision and 0.58 vs. 0.52 F-score(p < 0.05).6.3 Robustness to ASR outputFinally, since the end goal is a system that can au-tomatically extract decisions from raw audio andvideo recordings of meetings, we also investigatedthe impact of ASR output on our approach.
Weused SRI?s Decipher (Stolcke et al, 2008)9 to pro-duce word confusion networks for our 17 meetingsub-corpus and then ran our detectors on the WCNs?best path.
Table 6 shows a comparison of F-scores.The two scores shown for the super-classifier cor-respond to using the best feature set vs. using onlylexical features.
When ASR output is used, the re-sults for the DDA classes decrease between 6 and11 points.
However, the performance of the super-classifier does not experience a significant degrada-tion (the drop in F-score from 0.58 to 0.51 is notstatistically significant).
The results obtained withthe hierarchical detector are still significantly higherthan those achieved by the flat classifier (0.51 vs.0.50, p < 0.05).F1 I RP RR A super flatWCNs .22 .30 .08 .28 .51/.51 .50Manual .28 .38 .16 .39 .58/.55 .50Table 6: Comparison of F-scores obtained with WCNsand manual transcriptions9Stolcke et al (2008) report a word error rate of 26.9% onAMI meetings.7 Conclusions & Future WorkWe have shown that our earlier approach to actionitem detection can be successfully applied to themore general task of detecting decisions.
Althoughthis is indeed a hard problem, we have shown thatresults for automatic decision-detection in multi-party dialogue can be improved by taking accountof dialogue structure and applying a hierarchicalapproach.
Our approach consists in distinguish-ing between the different roles utterances play inthe decision-making process and uses a hierarchi-cal classification strategy: individual sub-classifiersare first trained to detect each of the DDA classes;then a super-classifier is used to detect patterns ofthese classes and identify decisions sub-dialogues.As we have seen, this structured approach outper-forms the accuracy achieved by systems based onflat classifications.
For the task of detecting deci-sion sub-dialogues we achieved 0.58 F-score in ini-tial experiments?a performance that proved to berather robust to ASR output.
Results for the individ-ual sub-classes are still low and there is indeed a lotof room for improvement.
In future work, we plan toincrease the size of our data-set, and possibly extendour set of DDA classes, by for instance includinga disagreement class, in order to capture additionalproperties of the decision-making process.We believe that our structured approach can helpin constructing more concise and targeted reports ofdecision sub-dialogues.
An immediate further ex-tension of the current work will therefore be to in-vestigate the automatic production of useful descrip-tive summaries of decisions.Acknowledgements We are thankful to the threeanonymous SIGdial reviewers for their helpful com-ments and suggestions.
This material is basedupon work supported by the Defense Advanced Re-search Projects Agency (DARPA) under ContractNo.
FA8750-07-D-0185/0004.
Any opinions, find-162ings and conclusions or recommendations expressedin this material are those of the authors and do notnecessarily reflect the views of DARPA.ReferencesSatanjeev Banerjee, Carolyn Rose?, and Alex Rudnicky.2005.
The necessity of a meeting recording and play-back system, and the benefit of topic-level annotationsto meeting browsing.
In Proceedings of the 10th Inter-national Conference on Human-Computer Interaction.Paul Boersma.
2001.
Praat, a system for doing phoneticsby computer.
Glot International, 5(9?10).Susanne Burger, Victoria MacLaren, and Hua Yu.
2002.The ISL Meeting Corpus: The impact of meeting typeon speech style.
In Proceedings of the 7th Inter-national Conference on Spoken Language Processing(INTERSPEECH - ICSLP), Denver, Colorado.Mark Core and James Allen.
1997.
Coding dialogueswith the DAMSL annotation scheme.
In D. Traum,editor, Proceedings of the 1997 AAAI Fall Symposiumon Communicative Action in Humans and Machines.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agreementand disagreement in conversational speech: Use ofBayesian networks to model pragmatic dependencies.In Proceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics (ACL).Daniel Gatica-Perez, Ian McCowan, Dong Zhang, andSamy Bengio.
2005.
Detecting group interest levelin meetings.
In IEEE International Conference onAcoustics, Speech, and Signal Processing (ICASSP).Dustin Hillard, Mari Ostendorf, and Elisabeth Shriberg.2003.
Detection of agreement vs. disagreement inmeetings: Training with unlabeled data.
In Compan-ion Volume of the Proceedings of HLT-NAACL 2003 -Short Papers, Edmonton, Alberta, May.Pei-Yun Hsueh and Johanna Moore.
2007a.
Whatdecisions have you made?
: Automatic decision de-tection in meeting conversations.
In Proceedings ofNAACL/HLT, Rochester, New York.Pey-Yun Hsueh and Johanna Moore.
2007b.
Automaticdecision detection in meeting speech.
In Proceedingsof MLMI 2007, Lecture Notes in Computer Science.Springer-Verlag.Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhillon,Jane Edwards, Javier Marc?
?as-Guarasa, Nelson Mor-gan, Barbara Peskin, Elizabeth Shriberg, AndreasStolcke, Chuck Wooters, and Britta Wrede.
2004.
TheICSI meeting project: Resources and research.
In Pro-ceedings of the 2004 ICASSP NIST Meeting Recogni-tion Workshop.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods ?
Sup-port Vector Learning.
MIT Press.Agnes Lisowska, Andrei Popescu-Belis, and Susan Arm-strong.
2004.
User query analysis for the specificationand evaluation of a dialogue processing and retrievalsystem.
In Proceedings of the 4th International Con-ference on Language Resources and Evaluation.Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,S.
Bourban, M. Flynn, M. Guillemot, T. Hain,J.
Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,M.
Lincoln, A. Lisowska, W. Post, D. Reidsma, andP.
Wellner.
2005.
The AMI Meeting Corpus.
In Pro-ceedings of Measuring Behavior 2005, the 5th Interna-tional Conference on Methods and Techniques in Be-havioral Research, Wageningen, Netherlands.Wilfried M. Post, Anita H.M. Cremers, and Olivier Blan-son Henkemans.
2004.
A research environment formeeting behaviour.
In Proceedings of the 3rd Work-shop on Social Intelligence Design.Matthew Purver, John Dowding, John Niekrasz, PatrickEhlen, Sharareh Noorbaloochi, and Stanley Peters.2007.
Detecting and summarizing action items inmulti-party dialogue.
In Proceedings of the 8th SIG-dial Workshop on Discourse and Dialogue, Antwerp,Belgium.Nicholas C. Romano, Jr. and Jay F. Nunamaker, Jr. 2001.Meeting analysis: Findings from research and prac-tice.
In Proceedings of the 34th Hawaii InternationalConference on System Sciences.Andreas Stolcke, Xavier Anguera, Kofi Boakye, O?zgu?rC?etin, Adam Janin, Matthew Magimai-Doss, ChuckWooters, and Jing Zheng.
2008.
The icsi-sri spring2007 meeting and lecture recognition system.
In Pro-ceedings of CLEAR 2007 and RT2007.
Springer Lec-ture Notes on Computer Science.Daan Verbree, Rutger Rienks, and Dirk Heylen.
2006.First steps towards the automatic construction ofargument-diagrams from real discussions.
In Proceed-ings of the 1st International Conference on Computa-tional Models of Argument, volume 144, pages 183?194.
IOS press.Lynn Voss, Patrick Ehlen, and the DARPA CALO MAProject Team.
2007.
The CALO Meeting Assistant.In Proceedings of NAACL-HLT, Rochester, NY, USA.SteveWhittaker, Rachel Laban, and Simon Tucker.
2006.Analysing meeting records: An ethnographic studyand technological implications.
In MLMI 2005, Re-vised Selected Papers.Britta Wrede and Elizabeth Shriberg.
2003.
Spot-ting ?hot spots?
in meetings: Human judgements andprosodic cues.
In Proceedings of the 9th EuropeanConference on Speech Communication and Technol-ogy, Geneva, Switzerland.163
