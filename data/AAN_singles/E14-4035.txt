Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 180?184,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsSome Experiments with a Convex IBM Model 2Andrei SimionColumbia UniversityIEOR DepartmentNew York, NY, 10027aas2148@columbia.eduMichael CollinsColumbia UniversityComputer ScienceNew York, NY, 10027mc3354@columbia.eduClifford SteinColumbia UniversityIEOR DepartmentNew York, NY, 10027cs2035@columbia.eduAbstractUsing a recent convex formulation of IBMModel 2, we propose a new initializationscheme which has some favorable compar-isons to the standard method of initializingIBM Model 2 with IBM Model 1.
Addition-ally, we derive the Viterbi alignment for theconvex relaxation of IBM Model 2 and showthat it leads to better F-Measure scores thanthose of IBM Model 2.1 IntroductionThe IBM translation models are widely used inmodern statistical translation systems.
Unfortu-nately, apart from Model 1, the IBM models leadto non-convex objective functions, leading to meth-ods (such as EM) which are not guaranteed to reachthe global maximum of the log-likelihood function.In a recent paper, Simion et al.
introduced a con-vex relaxation of IBM Model 2, I2CR-2, and showedthat it has performance on par with the standard IBMModel 2 (Simion et al., 2013).In this paper we make the following contributions:?
We explore some applications of I2CR-2.
Inparticular, we show how this model can beused to seed IBM Model 2 and compare thespeed/performance gains of our initializationunder various settings.
We show that initializ-ing IBM Model 2 with a version of I2CR-2 thatuses large batch size yields a method that hassimilar run time to IBM Model 1 initializationand at times has better performance.?
We derive the Viterbi alignment for I2CR-2 andcompare it directly with that of IBM Model2.
Previously, Simion et al.
(2013) had com-pared IBM Model 2 and I2CR-2 by using IBMModel 2?s Viterbi alignment rule, which is notnecessarily the optimal alignment for I2CR-2.We show that by comparing I2CR-2 with IBMModel 2 by using each model?s optimal Viterbialignment the convex model consistently has ahigher F-Measure.
F-Measure is an importantmetric because it has been shown to be corre-lated with BLEU scores (Marcu et al., 2006).Notation.
We adopt the notation introduced in(Och and Ney, 2003) of having 1m2ndenote thetraining scheme of m IBM Model 1 EM iterationsfollowed by initializing Model 2 with these parame-ters and running n IBM Model 2 EM iterations.
Thenotation EGmB2nmeans that we run m iterations ofI2CR-2?s EG algorithm (Simion et al., 2013) withbatch size of B, initialize IBM Model 2 with I2CR-2?s parameters, and then run n iterations of Model2?s EM.2 The IBM Model 1 and 2 OptimizationProblemsIn this section we give a brief review of IBM Mod-els 1 and 2 and the convex relaxation of Model 2,I2CR-2 (Simion et al., 2013).
The standard ap-proach in training parameters for Models 1 and 2 isEM, whereas for I2CR-2 an exponentiated-gradient(EG) algorithm was developed (Simion et al., 2013).We assume that our set of training examples is(e(k), f(k)) for k = 1 .
.
.
n, where e(k)is the k?thEnglish sentence and f(k)is the k?th French sen-tence.
The k?th English sentence is a sequence ofwords e(k)1. .
.
e(k)lkwhere lkis the length of the k?thEnglish sentence, and each e(k)i?
E; similarlythe k?th French sentence is a sequence f(k)1. .
.
f(k)mkwhere each f(k)j?
F .
We define e(k)0for k = 1 .
.
.
nto be a special NULL word (note that E contains theNULL word).
IBM Model 2 is detailed in severalsources such as (Simion et al., 2013) and (Koehn,2004).The convex and non-convex objectives of respec-tively IBM Model 1 and 2 can be found in (Simion180et al., 2013).
For I2CR-2, the convex relaxation ofIBM Model 2, the objective is given by12nn?k=1mk?j=1log?lk?i=0t(f(k)j|e(k)i)(L+ 1)+12nn?k=1mk?j=1log?lk?i=0min{t(f(k)j|e(k)i), d(i|j)} .For smoothness reasons, Simion et al.
(2013) de-fined log?
(z) = log(z + ?)
where ?
= .001 is asmall positive constant.
The I2CR-2 objective is aconvex combination of the convex IBM Model 1 ob-jective and a direct (convex) relaxation of the IBM2Model 2 objective, and hence is itself convex.3 The Viterbi Alignment for I2CR-2Alignment models have been compared using meth-ods other than Viterbi comparisons; for example,Simion et al.
(2013) use IBM Model 2?s optimalrule given by (see below) Eq.
2 to compare mod-els while Liang et al.
(2006) use posterior de-coding.
Here, we derive and use I2CR-2?s Viterbialignment.
To get the Viterbi alignment of a pair(e(k), f(k)) using I2CR-2 we need to find a(k)=(a(k)1, .
.
.
, a(k)mk) which yields the highest probabilityp(f(k), a(k)|e(k)).Referring to the I2CR-2 objective,this corresponds to finding a(k)that maximizeslog?mkj=1t(f(k)j|e(k)a(k)j)2+log?mkj=1min {t(f(k)j|e(k)a(k)j), d(a(k)j|j)}2.Putting the above terms together and using themonotonicity of the logarithm, the above reduces tofinding the vector a(k)which maximizesmk?j=1t(f(k)j|e(k)a(k)j)min {t(f(k)j|e(k)a(k)j), d(a(k)j|j)}.As with IBM Models 1 and 2, we can find the vectora(k)by splitting the maximization over the compo-nents of a(k)and focusing on finding a(k)jgiven byargmaxa(t(f(k)j|e(k)a)min {t(f(k)j|e(k)a), d(a|j)}) .
(1)In previous experiments, Simion et al.
(Simion etal., 2013) were comparing I2CR-2 and IBM Model2 using the standard alignment formula derived in asimilar fashion from IBM Model 2:a(k)j= argmaxa(t(f(k)j|e(k)a)d(a|j)) .
(2)4 ExperimentsIn this section we describe experiments using theI2CR-2 optimization problem combined with thestochastic EG algorithm (Simion et al., 2013) for pa-rameter estimation.
The experiments conducted hereuse a similar setup to those in (Simion et al., 2013).We first describe the data we use, and then describethe experiments we ran.4.1 Data SetsWe use data from the bilingual word alignmentworkshop held at HLT-NAACL 2003 (Michalceaand Pederson, 2003).
We use the Canadian Hansardsbilingual corpus, with 247,878 English-French sen-tence pairs as training data, 37 sentences of devel-opment data, and 447 sentences of test data (notethat we use a randomly chosen subset of the orig-inal training set of 1.1 million sentences, similar tothe setting used in (Moore, 2004)).
The developmentand test data have been manually aligned at the wordlevel, annotating alignments between source and tar-get words in the corpus as either ?sure?
(S) or ?pos-sible?
(P ) alignments, as described in (Och and Ney,2003).As a second data set, we used the Romanian-English data from the HLT-NAACL 2003 workshopconsisting of a training set of 48,706 Romanian-English sentence-pairs, a development set of 17 sen-tence pairs, and a test set of 248 sentence pairs.We carried out our analysis on this data set aswell, but because of space we only report the de-tails on the Hansards data set.
The results on theRomanian data were similar, but the magnitude ofimprovement was smaller.4.2 MethodologyOur experiments make use of either standard train-ing or intersection training (Och and Ney, 2003).For standard training, we run a model in the source-target direction and then derive the alignments onthe test or development data.
For each of the181Training 21015210EG1125210EG11250210Iteration Objective0 -224.0919 -144.2978 -91.2418 -101.22501 -110.6285 -85.6757 -83.3255 -85.58472 -91.7091 -82.5312 -81.3845 -82.14993 -84.8166 -81.3380 -80.6120 -80.96104 -82.0957 -80.7305 -80.2319 -80.40415 -80.9103 -80.3798 -80.0173 -80-10096 -80.3620 -80.1585 -79.8830 -79.91967 -80.0858 -80.0080 -79.7911 -79.80488 -79.9294 -79.9015 -79.7247 -79.72849 -79.8319 -79.8240 -79.6764 -79.675110 -79.7670 -79.7659 -79.6403 -79.6354Table 1: Objective results for the English?
French IBMModel 2 seeded with either uniform parameters, IBMModel 1 ran for 5 EM iterations, or I2CR-2 ran for 1 iter-ation with either B = 125 or 1250.
Iteration 0 denotes thestarting IBM 2 objective depending on the initialization.models?IBM Model 1, IBM Model 2, and I2CR-2?
we apply the conventional methodology to in-tersect alignments: first, we estimate the t and dparameters using models in both source-target andtarget-source directions; second, we find the mostlikely alignment for each development or test datasentence in each direction; third, we take the in-tersection of the two alignments as the final outputfrom the model.
For the I2CR-2 EG (Simion et al.,2013) training, we use batch sizes of eitherB = 125or B = 1250 and a step size of ?
= 0.5 throughout.We measure the performance of the models interms of Precision, Recall, F-Measure, and AER us-ing only sure alignments in the definitions of the firstthree metrics and sure and possible alignments in thedefinition of AER, as in (Simion et al., 2013) and(Marcu et al., 2006).
For our experiments, we reportresults in both AER (lower is better) and F-Measure(higher is better).4.3 Initialization and Timing ExperimentsWe first report the summary statistics on the test setusing a model trained only in the English-French di-rection.
In these experiments we seeded IBM Model2?s parameters either with those of IBM Model 1 runfor 5, 10 or 15 EM iterations or I2CR-2 run for 1 it-eration of EG with a batch size of either B = 125 or1250.
For uniform comparison, all of our implemen-tations were written in C++ using STL/Boost con-tainers.There are several takeaways from our experi-ments, which are presented in Table 2.
We first notethat with B = 1250 we get higher F-Measure andlower AER even though we use less training time: 5iterations of IBM Model 1 EM training takes about3.3 minutes, which is about the time it takes for 1 it-eration of EG with a batch size of 125 (4.1 minutes);on the other hand, using B = 1250 takes EG 1.7minutes and produces the best results across almostall iterations.
Additionally, we note that the initialsolution given to IBM Model 2 by running I2CR-2for 1 iteration with B = 1250 is fairly strong andallows for further progress: IBM2 EM training im-proves upon this solution during the first few iter-ations.
We also note that this behavior is global:no IBM 1 initialization scheme produced subsequentsolutions for IBM 2 with as low in AER or high inF-Measure.
Finally, comparing Table 1 which listsobjective values with Table 2 which lists alignmentstatistics, we see that although the objective progres-sion is similar throughout, the alignment quality isdifferent.To complement the above, we also ran inter-section experiments.
Seeding IBM Model 2 byModel 1 and intersecting the alignments producedby the English-French and French-English modelsgave both AER and F-Measure which were betterthan those that we obtained by any seeding of IBMModel 2 with I2CR-2.
However, there are still rea-sons why I2CR-2 would be useful in this context.
Inparticular, we note that I2CR-2 takes roughly halfthe time to progress to a better solution than IBMModel 1 run for 5 EM iterations.
Second, a possibleremedy to the above loss in marginal improvementwhen taking intersections would be to use a more re-fined method for obtaining the joint alignment of theEnglish-French and French-English models, such as?grow-diagonal?
(Och and Ney, 2003).4.4 Viterbi ComparisonsFor the decoding experiments, we used IBM Model1 as a seed to Model 2.
To train IBM Model 1, wefollow (Moore, 2004) and (Och and Ney, 2003) inrunning EM for 5, 10 or 15 iterations.
For the EG al-gorithm, we initialize all parameters uniformly anduse 10 iterations of EG with a batch size of 125.Given the lack of development data for the align-ment data sets, for both IBM Model 2 and the I2CR-2 method, we report test set F-Measure and AER re-sults for each of the 10 iterations, rather than pickingthe results from a single iteration.182Training 21015210110210115210EG1125210EG11250210Iteration AER0 0.8713 0.3175 0.3177 0.3160 0.2329 0.26621 0.4491 0.2547 0.2507 0.2475 0.2351 0.22592 0.2938 0.2428 0.2399 0.2378 0.2321 0.21803 0.2593 0.2351 0.2338 0.2341 0.2309 0.21764 0.2464 0.2298 0.2305 0.2310 0.2283 0.21685 0.2383 0.2293 0.2299 0.2290 0.2268 0.21886 0.2350 0.2273 0.2285 0.2289 0.2274 0.22057 0.2320 0.2271 0.2265 0.2286 0.2274 0.22138 0.2393 0.2261 0.2251 0.2276 0.2278 0.22239 0.2293 0.2253 0.2246 0.2258 0.2284 0.221710 0.2288 0.2248 0.2249 0.2246 0.2275 0.2223Iteration F-Measure0 0.0427 0.5500 0.5468 0.5471 0.6072 0.59771 0.4088 0.5846 0.5876 0.5914 0.6005 0.62202 0.5480 0.5892 0.5916 0.5938 0.5981 0.62153 0.5750 0.5920 0.5938 0.5947 0.5960 0.61654 0.5814 0.5934 0.5839 0.5952 0.5955 0.61295 0.5860 0.5930 0.5933 0.5947 0.5945 0.60806 0.5873 0.5939 0.5936 0.5940 0.5924 0.60517 0.5884 0.5931 0.5955 0.5941 0.5913 0.60248 0.5899 0.5932 0.5961 0.5942 0.5906 0.60009 0.5899 0.5933 0.5961 0.5958 0.5906 0.599610 0.5897 0.5936 0.5954 0.5966 0.5910 0.5986Table 2: Results on the Hansards data for English ?French IBM Model 2 seeded using different methods.The first three columns are for a model seeded with IBMModel 1 ran for 5, 10 or 15 EM iterations.
The fourthand fifth columns show results when we seed with I2CR-2 ran for 1 iteration either withB = 125 or 1250.
Iteration0 denotes the starting statistics.Training 15210110210115210EG10125EG10125Viterbi Rule t?
d t?
d t?
d t?
d t?min{t?
d}Iteration AER0 0.2141 0.2159 0.2146 0.9273 0.92731 0.1609 0.1566 0.1513 0.1530 0.15512 0.1531 0.1507 0.1493 0.1479 0.14633 0.1477 0.1471 0.1470 0.1473 0.14654 0.1458 0.1444 0.1449 0.1510 0.14825 0.1455 0.1438 0.1435 0.1501 0.14826 0.1436 0.1444 0.1429 0.1495 0.14817 0.1436 0.1426 0.1435 0.1494 0.14688 0.1449 0.1427 0.1437 0.1508 0.14899 0.1454 0.1426 0.1430 0.1509 0.148110 0.1451 0.1430 0.1423 0.1530 0.1484Iteration F-Measure0 0.7043 0.7012 0.7021 0.0482 0.04821 0.7424 0.7477 0.7534 0.7395 0.75072 0.7468 0.7499 0.7514 0.7448 0.75833 0.7489 0.7514 0.7520 0.7455 0.75854 0.7501 0.7520 0.7516 0.7418 0.75605 0.7495 0.7513 0.7522 0.7444 0.75676 0.7501 0.7501 0.7517 0.7452 0.75747 0.7493 0.7517 0.7507 0.7452 0.75808 0.7480 0.7520 0.7504 0.7452 0.75639 0.7473 0.7511 0.7513 0.7450 0.759010 0.7474 0.7505 0.7520 0.7430 0.7568Table 3: Intersected results on the English-French datafor IBM Model 2 and I2CR-2 using either IBM Model 1trained to 5, 10, or 15 EM iterations to seed IBM2 and us-ing either the IBM2 or I2CR-2 Viterbi formula for I2CR-2.In Table 3 we report F-Measure and AER resultsfor each of the iterations under IBM Model 2 andI2CR-2 models using either the Model 2 Viterbi ruleof Eq.
2 or I2CR-2?s Viterbi rule in Eq.
1.
Wenote that unlike in the previous experiments pre-sented in (Simion et al., 2013), we are directly test-ing the quality of the alignments produced by I2CR-2 and IBM Model 2 since we are getting the Viterbialignment for each model (for completeness, we alsohave included in the fourth column the Viterbi align-ments we get by using the IBM Model 2 Viterbi for-mula with the I2CR-2 parameters as Simion et al.
(2013) had done previously).
For these experimentswe report intersection statistics.
Under its properdecoding formula, I2CR-2 model yields a higher F-Measure than any setting of IBM Model 2.
SinceAER and BLEU correlation is arguably known to beweak while F-Measure is at times strongly relatedwith BLEU (Marcu et al., 2006), the above resultsfavor the convex model.We close this section by pointing out that the maindifference between the IBM Model 2 Viterbi rule ofEq.
2 and the I2CR-2 Viterbi rule in Eq.
1 is thatthe Eq.
1 yield fewer alignments when doing inter-section training.
Even though there are fewer align-ments produced, the quality in terms of F-Measureis better.5 Conclusions and Future WorkIn this paper we have explored some of the details ofa convex formulation of IBM Model 2 and showedit may have an application either as a new initial-ization technique for IBM Model 2 or as a modelin its own right, especially if the F-Measure is thetarget metric.
Other possible topics of interest in-clude performing efficient sensitivity analysis on theI2CR-2 model, analyzing the balance between theIBM Model 1 and I2CR-1 (Simion et al., 2013) com-ponents of the I2CR-2 objective, studying I2CR-2?s intersection training performance using methodssuch as ?grow diagonal?
or ?agreement?
(Liang etal., 2006), and integrating it into the GIZA++ opensource library so we can see how much it affects thedownstream system.AcknowledgmentsMichael Collins and Andrei Simion are partly sup-ported by NSF grant IIS-1161814.
Cliff Stein is183partly supported by NSF grants CCF-0915681 andCCF-1349602.
We thank Professor Paul Blaer andSystems Engineer Radu Sadeanu for their help set-ting up some of the hardware used for these experi-ments.
We also thank the anonymous reviewers formany useful comments; we hope to pursue the com-ments we were not able to address in a followup pa-per.ReferencesPeter L. Bartlett, Ben Taskar, Michael Collins and DavidMcallester.
2004.
Exponentiated Gradient Algorithmsfor Large-Margin Structured Classification.
In Pro-ceedings of NIPS.Steven Boyd and Lieven Vandenberghe.
2004.
ConvexOptimization.
Cambridge University Press.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert.
L. Mercer.
1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, 19:263-311.Michael Collins, Amir Globerson, Terry Koo, XavierCarreras and Peter L. Bartlett.
2008.
ExponentiatedGradient Algorithms for Conditional Random Fieldsand Max-Margin Markov Networks.
Journal MachineLearning, 9(Aug): 1775-1822.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum Likelihood From Incomplete Data via theEM Algorithm.
Journal of the royal statistical society,series B, 39(1):1-38.Alexander Fraser and Daniel Marcu.
2007.
Measur-ing Word Alignment Quality for Statistical Ma-chine Translation.
Journal Computational Linguistics,33(3): 293-303.Joao V. Graca, Kuzman Ganchev and Ben Taskar.
2007.Expectation Maximization and Posterior Constraints.In Proceedings of NIPS.Yuhong Guo and Dale Schuurmans.
2007.
Convex Re-laxations of Latent Variable Training.
In Proceedingsof NIPS.Simon Lacoste-Julien, Ben Taskar, Dan Klein, andMichael Jordan.
2008.
Word Alignment via QuadraticAssignment.
In Proceedings of the HLT-NAACL.Phillip Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings of theEMNLP.Phillip Koehn.
2008.
Statistical Machine Translation.Cambridge University Press.Kivinen, J., Warmuth, M. 1997.
Exponentiated GradientVersus Gradient Descent for Linear Predictors.
Infor-mation and Computation, 132, 1-63.Percy Liang, Ben Taskar and Dan Klein.
2006.
Alignmentby Agreement.
In Proceedings of NAACL.Daniel Marcu, Wei Wang, Abdessamad Echihabi,and Kevin Knight.
2006.
SPMT: Statistical Ma-chine Translation with Syntactified Target LanguagePhrases.
In Proceedings of the EMNLP.Rada Michalcea and Ted Pederson.
2003.
An Evalua-tion Exercise in Word Alignment.
HLT-NAACL 2003:Workshop in building and using Parallel Texts: DataDriven Machine Translation and Beyond.Robert C. Moore.
2004.
Improving IBM Word-Alignment Model 1.
In Proceedings of the ACL.Stephan Vogel, Hermann Ney and Christoph Tillman.1996.
HMM-Based Word Alignment in StatisticalTranslation.
In Proceedings of COLING.Franz Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational-Linguistics, 29(1): 19-52.Andrei Simion, Michael Collins and Cliff Stein.
2013.
AConvex Alternative to IBM Model 2.
In Proceedingsof the EMNLP.Kristina Toutanova and Michel Galley.
2011.
Why Ini-tialization Matters for IBM Model 1: Multiple Optimaand Non-Strict Convexity.
In Proceedings of the ACL.Ashish Vaswani, Liang Huang and David Chiang.
2012.Smaller Alignment Models for Better Translations:Unsupervised Word Alignment with the L0-norm.
InProceedings of the ACL.184
