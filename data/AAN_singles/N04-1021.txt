A Smorgasbord of Features for Statistical Machine TranslationFranz Josef OchUSC/ISIDaniel GildeaU.
of RochesterSanjeev KhudanpurJohns Hopkins U.Anoop SarkarSimon Fraser U.Kenji YamadaXerox/XRCEAlex FraserUSC/ISIShankar KumarJohns Hopkins U.Libin ShenU.
of PennsylvaniaDavid SmithJohns Hopkins U.Katherine EngStanford U.Viren JainU.
of PennsylvaniaZhen JinMt.
HolyokeDragomir RadevU.
of MichiganAbstractWe describe a methodology for rapid exper-imentation in statistical machine translationwhich we use to add a large number of featuresto a baseline system exploiting features from awide range of levels of syntactic representation.Feature values were combined in a log-linearmodel to select the highest scoring candidatetranslation from an n-best list.
Feature weightswere optimized directly against the BLEU eval-uation metric on held-out data.
We present re-sults for a small selection of features at eachlevel of syntactic representation.1 IntroductionDespite the enormous progress in machine translation(MT) due to the use of statistical techniques in recentyears, state-of-the-art statistical systems often producetranslations with obvious errors.
Grammatical errors in-clude lack of a main verb, wrong word order, and wrongchoice of function words.
Frequent problems of a lessgrammatical nature include missing content words andincorrect punctuation.In this paper, we attempt to address these problems byexploring a variety of new features for scoring candidatetranslations.
A high-quality statistical translation systemis our baseline, and we add new features to the exist-ing set, which are then combined in a log-linear model.To allow an easy integration of new features, the base-line system provides an n-best list of candidate transla-tions which is then reranked using the new features.
Thisframework allows us to incorporate different types of fea-tures, including features based on syntactic analyses ofthe source and target sentences, which we hope will ad-dress the grammaticality of the translations, as well aslower-level features.
As we work on n-best lists, we caneasily use global sentence-level features.We begin by describing our baseline system and then-best rescoring framework within which we conductedour experiments.
We then present a selection of new fea-tures, progressing from word-level features to those basedto part-of-speech tags and syntactic chunks, and then tofeatures based on Treebank-based syntactic parses of thesource and target sentences.2 Log-linear Models for Statistical MTThe goal is the translation of a text given in some sourcelanguage into a target language.
We are given a source(?Chinese?)
sentence f = fJ1 = f1, .
.
.
, fj , .
.
.
, fJ ,which is to be translated into a target (?English?)
sentencee = eI1 = e1, .
.
.
, ei, .
.
.
, eI Among all possible targetsentences, we will choose the sentence with the highestprobability:e?I1 = argmaxeI1{Pr(eI1|fJ1 )} (1)As an alternative to the often used source-channel ap-proach (Brown et al, 1993), we directly model the pos-terior probability Pr(eI1|fJ1 ) (Och and Ney, 2002) us-ing a log-linear combination of feature functions.
Inthis framework, we have a set of M feature functionshm(eI1, fJ1 ),m = 1, .
.
.
,M .
For each feature function,there exists a model parameter ?m,m = 1, .
.
.
,M .
Thedirect translation probability is given by:Pr(eI1|fJ1 ) =exp[?Mm=1 ?mhm(eI1, fJ1 )]?e?I1exp[?Mm=1 ?mhm(e?I1, fJ1 )](2)We obtain the following decision rule:e?I1 = argmaxeI1{ M?m=1?mhm(eI1, fJ1 )}(3)The standard criterion for training such a log-linearmodel is to maximize the probability of the parallel train-ing corpus consisting of S sentence pairs {(fs, es) : s =1, .
.
.
, S}.
However, this does not guarantee optimal per-formance on the metric of translation quality by whichour system will ultimately be evaluated.
For this reason,we optimize the parameters directly against the BLEUmetric on held-out data.
This is a more difficult optimiza-tion problem, as the search space is no longer convex.Figure 1: Example segmentation of Chinese sentence andits English translation into alignment templates.However, certain properties of the BLEU metric can beexploited to speed up search, as described in detail byOch (2003).
We use this method of optimizing featureweights throughout this paper.2.1 Baseline MT System: Alignment TemplatesOur baseline MT system is the alignment template systemdescribed in detail by Och, Tillmann, and Ney (1999)and Och and Ney (2004).
In the following, we give ashort description of this baseline model.The probability model of the alignment template sys-tem for translating a sentence can be thought of in distinctstages.
First, the source sentence words fJ1 are grouped tophrases f?K1 .
For each phrase f?
an alignment template z ischosen and the sequence of chosen alignment templatesis reordered (according to piK1 ).
Then, every phrase f?produces its translation e?
(using the corresponding align-ment template z).
Finally, the sequence of phrases e?K1constitutes the sequence of words eI1.Our baseline system incorporated the following featurefunctions:Alignment Template Selection Each alignmenttemplate is chosen with probability p(z|f?
), estimated byrelative frequency.
The corresponding feature function inour log-linear model is the log probability of the productof p(z|f?)
for all used alignment templates used.Word Selection This feature is based on the lexicaltranslation probabilities p(e|f), estimated using relativefrequencies according to the highest-probability word-level alignment for each training sentence.
A translationprobability conditioned on the source and target positionwithin the alignment template p(e|f, i, j) is interpolatedwith the position-independent probability p(e|f).Phrase Alignment This feature favors monotonicalignment at the phrase level.
It measures the ?amountof non-monotonicity?
by summing over the distance (inthe source language) of alignment templates which areconsecutive in the target language.Language Model Features As a language modelfeature, we use a standard backing off word-based tri-gram language model (Ney, Generet, and Wessel, 1995).The baseline system actually includes four different lan-guage model features trained on four different corpora:the news part of the bilingual training data, a large Xin-hua news corpus, a large AFP news corpus, and a set ofChinese news texts downloaded from the web.Word/Phrase Penalty This word penalty featurecounts the length in words of the target sentence.
Withoutthis feature, the sentences produced tend to be too short.The phrase penalty feature counts the number of phrasesproduced, and can allow the model to prefer either shortor long phrases.Phrases from Conventional Lexicon The baselinealignment template system makes use of the Chinese-English lexicon provided by LDC.
Each lexicon entry isa potential phrase translation pair in the alignment tem-plate system.
To score the use of these lexicon entries(which have no normal translation probability), this fea-ture function counts the number of times such a lexiconentry is used.Additional Features A major advantage of the log-linear modeling approach is that it is easy to add newfeatures.
In this paper, we explore a variety of featuresbased on successively deeper syntactic representations ofthe source and target sentences, and their alignment.
Foreach of the new features discussed below, we added thefeature value to the set of baseline features, re-estimatedfeature weights on development data, and obtained re-sults on test data.3 Experimental FrameworkWe worked with the Chinese-English data from the recentevaluations, as both large amounts of sentence-alignedtraining corpora and multiple gold standard referencetranslations are available.
This is a standard data set,making it possible to compare results with other systems.In addition, working on Chinese allows us to use the ex-isting Chinese syntactic treebank and parsers based on it.For the baseline MT system, we distinguish the fol-lowing three different sentence- or chunk-aligned paralleltraining corpora:?
training corpus (train): This is the basic trainingcorpus used to train the alignment template transla-tion model (word lexicon and phrase lexicon).
Thiscorpus consists of about 170M English words.
Largeparts of this corpus are aligned on a sub-sentencelevel to avoid the existence of very long sentenceswhich would be filtered out in the training processto allow a manageable word alignment training.?
development corpus (dev): This is the training cor-pus used in discriminative training of the model-parameters of the log-linear translation model.
Inmost experiments described in this report this cor-pus consists of 993 sentences (about 25K words) inboth languages.?
test corpus (test): This is the test corpus used toassess the quality of the newly developed featurefunctions.
It consists of 878 sentences (about 25Kwords).For development and test data, we have four English (ref-erence) translations for each Chinese sentence.3.1 Reranking, n-best lists, and oraclesFor each sentence in the development, test, and the blindtest corpus a set of 16,384 different alternative transla-tions has been produced using the baseline system.
Forextracting the n-best candidate translations, an A* searchis used.
These n-best candidate translations are the basisfor discriminative training of the model parameters andfor re-ranking.We used n-best reranking rather than implementingnew search algorithms.
The development of efficientsearch algorithms for long-range dependencies is verycomplicated and a research topic in itself.
The rerank-ing strategy enabled us to quickly try out a lot of newdependencies, which would not have been be possible ifthe search algorithm had to be changed for each new de-pendency.On the other hand, the use of n-best list rescoring lim-its the possibility of improvements to what is availablein the n-best list.
Hence, it is important to analyze thequality of the n-best lists by determining how much of animprovement would be possible given a perfect rerankingalgorithm.
We computed the oracle translations, that is,the set of translations from our n-best list that yields thebest BLEU score.1We use the following two methods to compute theBLEU score of an oracle translation:1. optimal oracle (opt): We select the oracle sentenceswhich give the highest BLEU score compared to theset of 4 reference translations.
Then, we computeBLEU score of oracle sentences using the same setof reference translations.2.
round-robin oracle (rr): We select four differ-ent sets of oracle sentences which give the highestBLEU score compared to each of the 4 referencestranslations.
Then, we compute for each set of or-acle sentences a BLEU score using always thosethree references to score that have not been cho-sen to select the oracle.
Then, these 4 3-referenceBLEU scores are averaged.1Note that due to the corpus-level holistic nature of theBLEU score it is not trivial to compute the optimal set of oracletranslations.
We use a greedy search algorithm for the oracletranslations that might find only a local optimum.
Empirically,we do not observe a dependence on the starting point, hence webelieve that this does not pose a significant problem.Table 1: Oracle BLEU scores for different sizes of then-best list.
The avBLEUr3 scores are computed withrespect to three reference translations averaged over thefour different choices of holding out one reference.avBLEUr3[%] BLEUr4n rr opt opthuman 35.8 -1 28.3 28.3 31.64 29.1 30.8 34.516 29.9 33.2 37.364 30.6 35.6 38.7256 31.3 37.8 42.81024 31.7 40.0 45.34096 32.0 41.8 47.3The first method provides the theoretical upper bound ofwhat BLEU score can be obtained by rescoring a given n-best list.
Using this method with a 1000-best list, we ob-tain oracle translations that outperform the BLEU scoreof the human translations.
The oracle translations achieve113% against the human BLEU score on the test data(Table 1), while the first best translations obtain 79.2%against the human BLEU score.
The second method usesa different references for selection and scoring.
Here, us-ing an 1000-best list, we obtain oracle translations with arelative human BLEU score of 88.5%.Based on the results of the oracle experiment, andin order to make rescoring computationally feasible forfeatures requiring significant computation for each hy-pothesis, we used the top 1000 translation candidates forour experiments.
The baseline system?s BLEU score is31.6% on the test set (equivalent to the 1-best oracle inTable 1).
This is the benchmark against which the contri-butions of the additional features described in the remain-der of this paper are to be judged.3.2 PreprocessingAs a precursor to developing the various syntactic fea-tures described in this report, the syntactic represen-tations on which they are based needed to be com-puted.
This involved part-of-speech tagging, chunking,and parsing both the Chinese and English side of ourtraining, development, and test sets.Applying the part-of-speech tagger to the often un-grammatical MT output from our n-best lists sometimesled to unexpected results.
Often the tagger tries to ?fixup?
ungrammatical sentences, for example by looking fora verb when none is present:China NNP 14 CD open JJ border NNcities NNS achievements VBZ remarkable JJHere, although achievements has never been seen as averb in the tagger?s training data, the prior for a verbin this position is high enough to cause a present tenseverb tag to be produced.
In addition to the inaccura-cies of the MT system, the difference in genre from thetagger?s training text can cause problems.
For example,while our MT data include news article headlines with noverb, headlines are not included in the Wall Street Journaltext on which the tagger is trained.
Similarly, the taggeris trained on full sentences with normalized punctuation,leading it to expect punctuation at the end of every sen-tence, and produce a punctuation tag even when the evi-dence does not support it:China NNP ?s POS economic JJdevelopment NN and CC opening VBGup RP 14 CD border NN cities NNSremarkable JJ achievements .The same issues affect the parser.
For example theparser can create verb phrases where none exist, as in thefollowing example in which the tagger correctly did notidentify a verb in the sentence:These effects have serious implications for designingsyntactic feature functions.
Features such ?is there a verbphrase?
may not do what you expect.
One solution wouldbe features that involve the probability of a parse subtreeor tag sequence, allowing us to ask ?how good a verbphrase is it??
Another solution is more detailed featuresexamining more of the structure, such as ?is there a verbphrase with a verb?
?4 Word-Level Feature FunctionsThese features, directly based on the source and targetstrings of words, are intended to address such problems astranslation choice, missing content words, and incorrectpunctuation.4.1 Model 1 ScoreWe used IBM Model 1 (Brown et al, 1993) as one of thefeature functions.
Since Model 1 is a bag-of-word trans-lation model and it gives the sum of all possible alignmentprobabilities, a lexical co-occurrence effect, or triggeringeffect, is expected.
This captures a sort of topic or seman-tic coherence in translations.As defined by Brown et al (1993), Model 1 gives aprobability of any given translation pair, which isp(f |e; M1) =(l + 1)mm?j=1l?i=0t(fj |ei).We used GIZA++ to train the model.
The training data isa subset (30 million words on the English side) of the en-tire corpus that was used to train the baseline MT system.For a missing translation word pair or unknown words,where t(fj |ei) = 0 according to the model, a constantt(fj |ei) = 10?40 was used as a smoothing value.The average %BLEU score (average of the best fouramong different 20 search initial points) is 32.5.
We alsotried p(e|f ; M1) as feature function, but did not obtainimprovements which might be due to an overlap with theword selection feature in the baseline system.The Model 1 score is one of the best performing fea-tures.
It seems to ?fix?
the tendency of our baseline sys-tem to delete content words and it improves word selec-tion coherence by the triggering effect.
It is also possiblethat the triggering effect might work on selecting a properverb-noun combination, or a verb-preposition combina-tion.4.2 Lexical Re-ordering of Alignment TemplatesAs shown in Figure 1 the alignment templates (ATs)used in the baseline system can appear in various con-figurations which we will call left/right-monotone andleft/right-continuous.
We built 2 out of these 4 models todistinguish two types of lexicalized re-ordering of theseATs:The left-monotone model computes the total proba-bility of all ATs being left monotone: where the lowerleft corner of the AT touches the upper right corner of theprevious AT.
Note that the first word in the current ATmay or may not immediately follow the last word in theprevious AT.
The total probability is the product over allalignment templates i, either P (ATi is left-monotone) or1 ?
P (ATi is left-monotone).The right-continuous model computes the total prob-ability of all ATs being right continuous: where thelower left corner of the AT touches the upper right cor-ner of the previous AT and the first word in the cur-rent AT immediately follows the last word in the pre-vious AT.
The total probability is the product over allalignment templates i, either P (ATi is right-continuous)or 1 ?
P (ATi is right-continuous).In both models, the probabilities P have been esti-mated from the full training data (train).5 Shallow Syntactic Feature FunctionsBy shallow syntax, we mean the output of the part-of-speech tagger and chunkers.
We hope that such featurescan combine the strengths of tag- and chunk-based trans-lation systems (Schafer and Yarowsky, 2003) with ourbaseline system.5.1 Projected POS Language ModelThis feature uses Chinese POS tag sequences as surro-gates for Chinese words to model movement.
Chinesewords are too sparse to model movement, but an attemptto model movement using Chinese POS may be moresuccessful.
We hope that this feature will compensate fora weak model of word movement in the baseline system.Chinese POS sequences are projected to English us-ing the word alignment.
Relative positions are indicatedfor each Chinese tag.
The feature function was also triedwithout the relative positions:CD +0 M +1 NN +3 NN -1 NN +2 NN +314 (measure) open border citiesThe table shows an example tagging of an English hy-pothesis showing how it was generated from the Chinesesentence.
The feature function is the log probability out-put by a trigram language model over this sequence.
Thisis similar to the HMM Alignment model (Vogel, Ney, andTillmann, 1996) but in this case movement is calculatedon the basis of parts of speech.The Projected POS feature function was one of thestrongest performing shallow syntactic feature functions,with a %BLEU score of 31.8.
This feature function canbe thought of as a trade-off between purely word-basedmodels, and full generative models based upon shallowsyntax.6 Tree-Based Feature FunctionsSyntax-based MT has shown promise in thework of, among others, Wu and Wong (1998) andAlshawi, Bangalore, and Douglas (2000).
We hope thatadding features based on Treebank-based syntacticanalyses of the source and target sentences will addressgrammatical errors in the output of the baseline system.6.1 Parse Tree ProbabilityThe most straightforward way to integrate a statisticalparser in the system would be the use of the (log of the)parser probability as a feature function.
Unfortunately,this feature function did not help to obtain better results(it actually seems to significantly hurt performance).To analyze the reason for this, we performed an ex-periment to test if the used statistical parser assigns ahigher probability to presumably grammatical sentences.The following table shows the average log probability as-signed by the Collins parser to the 1-best (produced), or-acle and the reference translations:Hypothesis 1-best Oracle Referencelog(parseProb) -147.2 -148.5 -154.9We observe that the average parser log-probability ofthe 1-best translation is higher than the average parselog probability of the oracle or the reference translations.Hence, it turns out that the parser is actually assigninghigher probabilities to the ungrammatical MT output thanto the presumably grammatical human translations.
Onereason for that is that the MT output uses fewer unseenwords and typically more frequent words which lead toa higher language model probability.
We also performedexperiments to balance this effect by dividing the parserprobability by the word unigram probability and usingthis ?normalized parser probability?
as a feature function,but also this did not yield improvements.6.2 Tree-to-String AlignmentA tree-to-string model is one of several syntax-based translation models used.
The model is aconditional probability p(f |T (e)).
Here, we useda model defined by Yamada and Knight (2001) andYamada and Knight (2002).Internally, the model performs three types of opera-tions on each node of a parse tree.
First, it reorders thechild nodes, such as changing VP ?
VB NP PP intoVP ?
NP PP VB.
Second, it inserts an optional word ateach node.
Third, it translates the leaf English words intoChinese words.
These operations are stochastic and theirprobabilities are assumed to depend only on the node, andare independent of other operations on the node, or othernodes.
The probability of each operation is automaticallyobtained by a training algorithm, using about 780,000 En-glish parse tree-Chinese sentence pairs.
The probabilityof these operations ?
(eki,j) is assumed to depend on theedge of the tree being modified, eki,j , but independent ofeverything else, giving the following equation,p(f |T (e)) =????(eki,j)p(?
(eki,j)|eki,j) (4)where ?
varies over the possible alignments between thef and e and ?
(eki,j) is the particular operations (in ?)
forthe edge eki,j .The model is further extended to incorporate phrasaltranslations performed at each node of the input parsetree (Yamada and Knight, 2002).
An English phrase cov-ered by a node can be directly translated into a Chinesephrase without regular reorderings, insertions, and leaf-word translations.The model was trained using about 780,000 Englishparse tree-Chinese sentence pairs.
There are about 3 mil-lion words on the English side, and they were parsed byCollins?
parser.Since the model is computationally expensive, weadded some limitations on the model operations.
As thebase MT system does not produce a translation with abig word jump, we restrict the model not to reorder childnodes when the node covers more than seven words.
Fora node that has more than four children, the reorderingprobability is set to be uniform.
We also introduced prun-ing, which discards partial (subtree-substring) alignmentsif the probability is lower than a threshold.The model gives a sum of all possible alignment prob-abilities for a pair of a Chinese sentence and an Englishparse tree.
We also calculate the probability of the bestalignment according to the model.
Thus, we have the fol-lowing two feature functions:hTreeToStringSum(e, f) = log(????(eki,j)p(?
(eki,j)|eki,j))hTreeToStringViterbi(e, f) = log(max???(eki,j)p(?
(eki,j)|eki,j))As the model is computationally expensive, we sorted then-best list by the sentence length, and processed themfrom the shorter ones to the longer ones.
We used 10CPUs for about five days, and 273/997 development sen-tences and 237/878 test sentences were processed.The average %BLEU score (average of the best fouramong different 20 search initial points) was 31.7 forboth hTreeToStringSum and hTreeToStringViterbi.
Among the pro-cessed development sentences, the model preferred theoracle sentences over the produced sentence in 61% ofthe cases.The biggest problem of this model is that it is compu-tationally very expensive.
It processed less than 30% ofthe n-best lists in long CPU hours.
In addition, we pro-cessed short sentences only.
For long sentences, it is notpractical to use this model as it is.6.3 Tree-to-Tree AlignmentA tree-to-tree translation model makes use of syntac-tic tree for both the source and target language.
As inthe tree-to-string model, a set of operations apply, eachwith some probability, to transform one tree into another.However, when training the model, trees for both thesource and target languages are provided, in our casefrom the Chinese and English parsers.We began with the tree-to-tree alignment model pre-sented by Gildea (2003).
The model was extended to han-dle dependency trees, and to make use of the word-levelalignments produced by the baseline MT system.
Theprobability assigned by the tree-to-tree alignment model,given the word-level alignment with which the candidatetranslation was generated, was used as a feature in ourrescoring system.We trained the parameters of the tree transformationoperations on 42,000 sentence pairs of parallel Chinese-English data from the Foreign Broadcast Information Ser-vice (FBIS) corpus.
The lexical translation probabili-ties Pt were trained using IBM Model 1 on the 30 mil-lion word training corpus.
This was done to overcomethe sparseness of the lexical translation probabilities es-timated while training the tree-to-tree model, which wasnot able to make use of as much training data.As a test of the tree-to-tree model?s discrimination, weperformed an oracle experiment, comparing the modelscores on the first sentence in the n-best list with candi-date giving highest BLEU score.
On the 1000-best list forthe 993-sentence development set, restricting ourselvesto sentences with no more than 60 words and a branchingfactor of no more than five in either the Chinese or En-glish tree, we achieved results for 480, or 48% of the 993sentences.
Of these 480, the model preferred the pro-duced over the oracle 52% of the time, indicating thatit does not in fact seem likely to significantly improveBLEU scores when used for reranking.
Using the prob-ability of the source Chinese dependency parse aligningwith the n-best hypothesis dependency parse as a featurefunction, making use of the word-level alignments, yieldsa 31.6 %BLEU score ?
identical to our baseline.6.4 Markov Assumption for Tree AlignmentsThe tree-based feature functions described so far have thefollowing limitations: full parse tree models are expen-sive to compute for long sentences and for trees with flatconstituents and there is limited reordering observed inthe n-best lists that form the basis of our experiments.
Inaddition to this, higher levels of parse tree are rarely ob-served to be reordered between source and target parsetrees.In this section we attack these problems using a simpleMarkov model for tree-based alignments.
It guaranteestractability: compared to a coverage of approximately30% of the n-best list by the unconstrained tree-basedmodels, using the Markov model approach provides 98%coverage of the n-best list.
In addition, this approach isrobust to inaccurate parse trees.The algorithm works as follows: we start with wordalignments and two parameters: n for maximum numberof words in tree fragment and k for maximum height oftree fragment.
We proceed from left to right in the Chi-nese sentence and incrementally grow a pair of subtrees,one subtree in Chinese and the other in English, such thateach word in the Chinese subtree is aligned to a word inthe English subtree.
We grow this pair of subtrees un-til we can no longer grow either subtree without violat-ing the two parameter values n and k. Note that thesealigned subtree pairs have properties similar to alignmenttemplates.
They can rearrange in complex ways betweensource and target.
Figure 2 shows how subtree-pairs forparameters n = 3 and k = 3 can be drawn for thissentence pair.
In our experiments, we use substantiallybigger tree fragments with parameters set to n = 8 andk = 9.Once these subtree-pairs have been obtained, we caneasily assert a Markov assumption for the tree-to-tree andtree-to-string translation models that exploits these pair-ings.
Let consider a sentence pair in which we have dis-covered n subtree-pairs which we can call Frag0, .
.
.,Fragn.
We can then compute a feature function for thesentence pair using the tree-to-string translation model asfollows:hMarkovTreeToString =logPtree-to-string(Frag0) + .
.
.
+ logPtree-to-string(Fragn)Using this Markov assumption on tree alignments withFigure 2: Markov assumption on tree alignments.the Tree to String model described in Section 6.2 we ob-tain a coverage improvement to 98% coverage from theoriginal 30%.
The accuracy of the tree to string modelalso improved with a %BLEU score of 32.0 which is thebest performing single syntactic feature.6.5 Using TAG elementary trees for scoring wordalignmentsIn this section, we consider another method for carvingup the full parse tree.
However, in this method, instead ofsubtree-pairs we consider a decomposition of parse treesthat provides each word with a fragment of the originalparse tree as shown in Figure 3.
The formalism of Tree-Adjoining Grammar (TAG) provides the definition whateach tree fragment should be and in addition how to de-compose the original parse trees to provide the fragments.Each fragment is a TAG elementary tree and the compo-sition of these TAG elementary trees in a TAG deriva-tion tree provides the decomposition of the parse trees.The decomposition into TAG elementary trees is done byaugmenting the parse tree for source and target sentencewith head-word and argument (or complement) informa-tion using heuristics that are common to most contempo-rary statistical parsers and easily available for both En-glish and Chinese.
Note that we do not use the wordalignment information for the decomposition into TAGelementary trees.Once we have a TAG elementary tree per word,we can create several models that score word align-ments by exploiting the alignments between TAG ele-mentary trees between source and target.
Let tfi andtei be the TAG elementary trees associated with thealigned words fi and ei respectively.
We experimentedwith two models over alignments: unigram model overalignments:?i P (fi, tfi , ei, tei) and conditional model:?i P (ei, tei | fi, tfi) ?
P (fi+1, tfi+1 | fi, tfi)We trained both of these models using the SRI Lan-guage Modeling Toolkit using 60K aligned parse trees.We extracted 1300 TAG elementary trees each for Chi-Figure 3: Word alignments with TAG elementary trees.nese and for English.
The unigram model gets a %BLEUscore of 31.7 and the conditional model gets a %BLEUscore of 31.9.%BLEUBaseline 31.6IBM Model 1 p(f |e) 32.5Tree-to-String Markov fragments 32.0Right-continuous alignment template 32.0TAG conditional bigrams 31.9Left-monotone alignment template 31.9Projected POS LM 31.8Tree-to-String 31.7TAG unigram 31.7Tree-to-Tree 31.6combination 32.9Table 2: Results for the baseline features, each new fea-ture added to the baseline features on its own, and a com-bination of new features.7 ConclusionsThe use of discriminative reranking of an n-best list pro-duced with a state-of-the-art statistical MT system al-lowed us to rapidly evaluate the benefits of off-the-shelfparsers, chunkers, and POS taggers for improving syntac-tic well-formedness of the MT output.
Results are sum-marized in Table 2; the best single new feature improvedthe %BLEU score from 31.6 to 32.5.
The 95% confi-dence intervals computed with the bootstrap resamplingmethod are about 0.8%.
In addition to experiments withsingle features we also integrated multiple features usinga greedy approach where we integrated at each step thefeature that most improves the BLEU score.
This featureintegration produced a statistically significant improve-ment of absolute 1.3% to 32.9 %BLEU score.Our single best feature, and in fact the only single fea-ture to produce a truly significant improvement, was theIBM Model 1 score.
We attribute its success that it ad-dresses the weakness of the baseline system to omit con-tent words and that it improves word selection by em-ploying a triggering effect.
We hypothesize that this al-lows for better use of context in, for example, choosingamong senses of the source language word.A major goal of this work was to find out if we can ex-ploit annotated data such as treebanks for Chinese andEnglish and make use of state-of-the-art deep or shal-low parsers to improve MT quality.
Unfortunately, noneof the implemented syntactic features achieved a statisti-cally significant improvement in the BLEU score.
Poten-tial reasons for this might be:?
As described in Section 3.2, the use of off-the-shelftaggers and parsers has various problems due to vari-ous mismatches between the parser training data andour application domain.
This might explain that theuse of the parser probability as feature function wasnot successful.
A potential improvement might be toadapt the parser by retraining it on the full trainingdata that has been used by the baseline system.?
The use of a 1000-best list limits the potential im-provements.
It is possible that more improvementscould be obtained using a larger n-best list or a wordgraph representation of the candidates.?
The BLEU score is possibly not sufficiently sensi-tive to the grammaticality of MT output.
This couldnot only make it difficult to see an improvement inthe system?s output, but also potentially mislead theBLEU-based optimization of the feature weights.
Asignificantly larger corpus for discriminative train-ing and for evaluation would yield much smallerconfidence intervals.?
Our discriminative training technique, which di-rectly optimizes the BLEU score on a developmentcorpus, seems to have overfitting problems withlarge number of features.
One could use a larger de-velopment corpus for discriminative training or in-vestigate alternative discriminative training criteria.?
The amount of annotated data that has been usedto train the taggers and parsers is two orders ofmagnitude smaller than the parallel training datathat has been used to train the baseline system (orthe word-based features).
Possibly, a comparableamount of annotated data (e.g.
a treebank with 100million words) is needed to obtain significant im-provements.This is the first large scale integration of syntactic analy-sis operating on many different levels with a state-of-the-art phrase-based MT system.
The methodology of usinga log-linear feature combination approach, discriminativereranking of n-best lists computed with a state-of-the-artbaseline system allowed members of a large team to si-multaneously experiment with hundreds of syntactic fea-ture functions on a common platform.AcknowledgmentsThis material is based upon work supported by the Na-tional Science Foundation under Grant No.
0121285.ReferencesAlshawi, Hiyan, Srinivas Bangalore, and Shona Douglas.
2000.Learning dependency translation models as collections offinite state head transducers.
Computational Linguistics,26(1):45?60.Brown, Peter F., Stephen A. Della Pietra, Vincent J. DellaPietra, and R. L. Mercer.
1993.
The mathematics of sta-tistical machine translation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?311.Gildea, Daniel.
2003.
Loosely tree-based alignment for ma-chine translation.
In Proc.
of the 41st Annual Meeting of theAssociation for Computational Linguistics (ACL), Sapporo,Japan.Ney, Hermann, M. Generet, and Frank Wessel.
1995.
Ex-tensions of absolute discounting for language modeling.
InProc.
of the Fourth European Conf.
on Speech Communica-tion and Technology, Madrid, Spain.Och, Franz Josef.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proc.
of the 41st Annual Meetingof the Association for Computational Linguistics (ACL), Sap-poro, Japan.Och, Franz Josef and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statistical ma-chine translation.
In Proc.
of the 40th Annual Meeting of theAssociation for Computational Linguistics (ACL), Philadel-phia, PA.Och, Franz Josef and Hermann Ney.
2004.
The alignment tem-plate approach to statistical machine translation.
Computa-tional Linguistics.
Accepted for Publication.Och, Franz Josef, Christoph Tillmann, and Hermann Ney.1999.
Improved alignment models for statistical machinetranslation.
In Proc.
of the Joint SIGDAT Conf.
on Empiri-cal Methods in Natural Language Processing and Very LargeCorpora, College Park, MD.Schafer, Charles and David Yarowsky.
2003.
Statistical ma-chine translation using coercive two-level syntactic transduc-tion.
In Proc.
of the 2003 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), Philadel-phia, PA.Vogel, Stephan, Hermann Ney, and Christoph Tillmann.
1996.HMM-based word alignment in statistical translation.
InCOLING ?96: The 16th Int.
Conf.
on Computational Lin-guistics, Copenhagen, Denmark.Wu, Dekai and H. Wong.
1998.
Machine translation with astochastic grammatical channel.
In COLING-ACL ?98: 36thAnnual Meeting of the Association for Computational Lin-guistics and 17th Int.
Conf.
on Computational Linguistics,Montreal, Canada.Yamada, Kenji and Kevin Knight.
2001.
A syntax-based sta-tistical translation model.
In Proc.
of the 39th Annual Meet-ing of the Association for Computational Linguistics (ACL),Toulouse, France.Yamada, Kenji and Kevin Knight.
2002.
A decoder for syntax-based MT.
In Proc.
of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), Philadelphia,PA.
