A Simple Named Entity Extractor using AdaBoostXavier Carreras and Llu?
?s Ma`rquez and Llu?
?s Padro?TALP Research CenterDepartament de Llenguatges i Sistemes Informa`ticsUniversitat Polite`cnica de Catalunya{carreras,lluism,padro}@lsi.upc.es1 IntroductionThis paper presents a Named Entity Extraction (NEE)system for the CoNLL-2003 shared task competition.
Asin the past year edition (Carreras et al, 2002a), we haveapproached the task by treating the two main sub?tasks ofthe problem, recognition (NER) and classification (NEC),sequentially and independently with separate modules.Both modules are machine learning based systems, whichmake use of binary and multiclass AdaBoost classifiers.Named Entity recognition is performed as a greedy se-quence tagging procedure under the well?known BIO la-belling scheme.
This tagging process makes use of threebinary classifiers trained to be experts on the recognitionof B, I, and O labels, respectively.
Named Entity classifi-cation is viewed as a 4?class classification problem (withLOC, PER, ORG, and MISC class labels), which is straight-forwardly addressed by the use of a multiclass learningalgorithm.The system presented here consists of a replication,with some minor changes, of the system that obtained thebest results in the CoNLL-2002 NEE task.
Therefore, itcan be considered as a benchmark of the state?of?the?art technology for the current edition, and will allow alsoto make comparisons about the training corpora of botheditions.2 Learning the DecisionsWe use AdaBoost with confidence rated predictions aslearning algorithm for the classifiers involved in the sys-tem.
More particularly, the basic binary version has beenused to learn the I, O, and B classifiers for the NERmodule, and the multiclass multilabel extension (namelyAdaBoost.MH) has been used to perform entity classifi-cation.The idea of these algorithms is to learn an accuratestrong classifier by linearly combining, in a weighted vot-ing scheme, many simple and moderately?accurate baseclassifiers or rules.
Each base rule is learned sequen-tially by presenting the base learning algorithm a weight-ing over the examples, which is dynamically adjusted de-pending on the behavior of the previously learned rules.AdaBoost has been applied, with significant success, toa number of problems in different areas, including NLPtasks (Schapire, 2002).
We refer the reader to (Schapireand Singer, 1999) for details about the general algorithms(for both the binary and multiclass variants), and (Car-reras and Ma`rquez, 2001; Carreras et al, 2002b) for par-ticular applications to NLP domains.In our setting, the boosting algorithm combines sev-eral small fixed?depth decision trees, as base rules.
Eachbranch of a tree is, in fact, a conjunction of binary fea-tures, allowing the strong boosting classifier to work withcomplex and expressive rules.3 Feature RepresentationA window W anchored in a word w represents the localcontext of w used by a classifier to make a decision onthat word.
In the window, each word around w is cod-ified with a set of primitive features, together with itsrelative position to w. Each primitive feature with eachrelative position and each possible value forms a final bi-nary feature for the classifier (e.g., ?the word form atposition(-2) is street?).
The kind of information codedin those features may be grouped in the following kinds:?
Lexical: Word forms and their position in the win-dow (e.g., W (3)=?bank?).
When available, wordlemmas and their position in the window.?
Syntactic: Part-of-Speech tags and Chunk tags.?
Orthographic: Word properties with regard to howis it capitalized (initial-caps, all-caps), the kindof characters that form the word (contains-digits,all-digits, alphanumeric, roman-number), the pres-ence of punctuation marks (contains-dots, contains-hyphen, acronym), single character patterns (lonely-initial, punctuation-mark, single-char), or the mem-bership of the word to a predefined class (functional-word1), or pattern (URL).?
Affixes: The prefixes and suffixes of the word (up to4 characters).?
Word Type Patterns: Type pattern of consecutivewords in the context.
The type of a word is ei-ther functional (f), capitalized (C), lowercased (l),punctuation mark (.
), quote (?)
or other (x).
Forinstance, the word type pattern for the phrase ?JohnSmith payed 3 euros?
would be CClxl.?
Left Predictions: The {B,I,O} tags being predictedin the current classification (at recognition stage), orthe predicted category for entities in left context (atclassification stage).?
Bag-of-Words: Form of the words in the window,without considering positions (e.g., ?bank??
W ).?
Trigger Words: Triggering properties of windowwords.
An external list is used to determine whethera word may trigger a certain Named Entity (NE)class (e.g., ?president?
may trigger class PER).?
Gazetteer Features: Gazetteer information for win-dow words.
An external gazetteer is used to deter-mine possible classes for each word.4 The NER ModuleThe Named Entity recognition task is performed as acombination of local classifiers which test simple deci-sions on each word in the text.According to a BIO labelling scheme, each word istagged as either the beginning of a NE (B tag), a wordinside a NE (I tag), or a word outside a NE (O tag).We use three binary classifiers for the tagging, one cor-responding to each tag.
All the words in the train set areused as training examples, applying a one-vs-all binariza-tion.
When tagging, the sentence is processed from left toright, greedily selecting for each word the tag with maxi-mum confidence that is coherent with the current solution(e.g., O tags cannot be followed by I tags).
Despite itssimplicity, the greedy BIO tagging performed very wellfor the NER task.
Other more sophisticated represen-tations and tagging schemes, studied in the past edition(Carreras et al, 2002a), did not improve the performanceat all.The three classifiers use the same information to codifyexamples.
According to the information types introducedin section 3, all the following features are considered foreach target word: lexical, syntactic, orthographic, andaffixes in a {-3,+3} window; left predictions in a {-3,-1}1Functional words are determiners and prepositions whichtypically appear inside NEs.window; and all the word type patterns that cover the 0position in a {-3,+3} window.The semantic information represented by the restof features, namely bag-of-words, trigger words, andgazetteer features, did not help the recognition ofNamed Entities, and therefore was not used.5 The NEC ModuleNEC is regarded as a classification task, consisting of as-signing the NE type to each already recognized NE.
Incontrast to the last year system, the problem has not beenbinarized and treated in an ECOC (error correcting out-put codes) combination scheme.
Instead, the multiclassmultilabel AdaBoost.MH algorithm has been used.
Thereason is that although ECOC provides slightly better re-sults, its computational cost is also much higher than therequired for AdaBoost.MH.The algorithm has been employed with different pa-rameterizations, by modeling NEC either as a three-classclassification problem (in which MISC is selected onlywhen the entity is negatively classified as PER, ORG andLOC) or as a four-class problem, in which MISC is justone more class.
The latter turned out to be the best choice(with very significant differences).The window information described in section 3 isused in the NEC module computing all features for a{-3,+3} window around the NE being classified, ex-cept for the bag-of-words group, for which a {-5,+5}window is used.
Information relative to orthographic,left predictions, and bag-of-words features is straight-forwardly coded as described above, but other requiresfurther detail:?
Lexical features: Apart from word form and lemmafor each window position, two additional binary fea-tures are used: One is satisfied when the focus NEform and lemma coincide exactly, and the otherwhen they coincide after turning both of them intolowercase.?
Syntactic features: Part-of-Speech (PoS) andChunk tags of window words (e.g., W (3).PoS=NN).PoS and Chunk pattern of the NE (e.g.,NNPS POS JJ for the NE ?People ?s Daily?)?
Affix features: Prefixes and suffixes of all windowwords.
Prefixes and suffixes of the NE being classi-fied and of its internal components (e.g., consideringthe entity ?People ?s Daily?, ?ly?
is taken as a suf-fix of the NE, ?ple?
is taken as a suffix of the firstinternal word, etc.).?
Trigger Words: Triggering properties of windowwords (e.g., W (3).trig=PER).
Triggering propertiesof components of the NE being classified (e.g., forthe entity ?Bank of England?
we could have a fea-ture NE(1).trig=ORG).
Context patterns to the leftof the NE, where each word is marked with its trig-gering properties, or with a functional?word tag ifappropriate (e.g., the phrase ?the president of UnitedStates?, would produce the pattern f ORG f for theNE ?United States?, assuming that the word ?presi-dent?
is listed as a possible trigger for ORG).?
Gazetteer Features: Gazetteer information forthe NE being classified and for its components(e.g., for the entity ?Bank of England?, featuresNE(3).gaz=LOC and NE.gaz=ORG would be acti-vated if ?England?
is found in the gazetteer as LOCand ?Bank of England?
as ORG, respectively.?
Additionally, binary features encoding the length inwords of the NE being classified are also used.6 Experimental SettingThe list of functional words for the task has been automat-ically constructed using the training set.
The lowercasedwords inside a NE that appeared more than 3 times wereselected as functional words for the language.Similarly, a gazetteer was constructed with the NEs inthe training set.
When training, only a random 40% of theentries in the gazetteer were considered.
Moreover, weused external knowledge in the form of a list of triggerwords for NEs and an external gazetteer.
These knowl-edge sources are the same that we used in the last yearcompetition for Spanish NEE.
The entries of the trigger?word list were linked to the Spanish WordNet, so theyhave been directly translated by picking the correspond-ing synsets of the English WordNet.
The gazetteer hasbeen left unchanged, assuming interlinguality of most ofthe entries.
The gazetteer provided by the CoNLL-2003organization has not been used in the work reported inthis paper.In all cases, a preprocess of attribute filtering was per-formed in order to avoid overfitting and to speed?uplearning.
All features that occur less than 3 times in thetraining corpus were discarded.For each classification problem we trained the corre-sponding AdaBoost classifiers, learning up to 4,000 basedecision trees per classifier, with depths ranging from 1(decision stumps) to 4.
The depth of the base rules andthe number of rounds were directly optimized on the de-velopment set.
The set of unlabelled examples providedby the organization was not used in this work.7 ResultsThe described system has been applied to both languagesin the shared task, though German and English environ-ments are not identical: The German corpus enables theuse of lemma features while English does not.
Also, theused trigger word list is available for English but not forGerman.The results of the BIO model for the NER task onthe development and test sets for English and Germanare presented in table 1.
As will be seen later for thewhole task, the results are systematically better for En-glish than for German.
As it can be observed, the be-haviour on the development and test English sets is quitedifferent.
While in the development set the NER mod-ule achieves a very good balance between precision andrecall, in the test set the precision drops almost 4 points,being the F1 results much worse.
On the contrary, de-velopment and test sets for German are much more sim-ilar.
In this case, recall levels obtained for the languageare much lower compared to precision ones.
This fact isindicating the difficulty for reliably detecting the begin-nings of the Named Entities in German (all common andproper nouns are capitalized).
Probably, a non?greedytagging procedure would have the chance to improve therecognition results.Precision Recall F?=1English devel.
95.65% 95.51% 95.58English test 91.93% 94.02% 92.96German devel.
88.15% 71.55% 78.99German test 85.87% 72.61% 78.68Table 1: Results of the BIO recognizer for the NER taskRegarding NEC task, optimal feature selection is dif-ferent for each language: Chunk information is almostuseless in English (or even harmful, when combined withPoS features), but useful in German.
On the contrary, al-though the use of left predictions for NEC is useful forEnglish, the lower accuracy of the German system ren-ders those features harmful (they are very useful whenassuming perfect left predictions).
Table 2 presents NECaccuracy results assuming perfect recognition of entities.English Germanfeatures accuracy features accuracybasic 91.47% basic 79.02%basic+P 92.14% basic+P 79.29%basic+C 91.60% basic+C 79.04%basic+PC 92.12% basic+PC 79.91%basic+Pg 93.86% basic+PCg 81.54%basic+PG 95.05% basic+PCG 85.12%basic+PGT 95.14%Table 2: NEC accuracy on the development set assuminga perfect recognition of named entitiesThe basic feature set includes all lexical, orthographic,affix and bag?of?words information.
P stands for Part-of-Speech features, C for chunking?related information, Tfor trigger?words features and g/G for gazetteer?relatedinformation2.
In general, more complex features setsyield better results, except for the C case in English, ascommented above.Table 4 presents the results on the NEE task obtainedby pipelining the NER and NEC modules.
The NECmodule used both knowledge extracted from the trainingset as well as external sources such as the gazetteer ortrigger word lists.Almost the same conclusions extracted from the NERresults apply to the complete task, although here the re-sults are lower due to the cascade of errors introduced bythe two modules: 1) Results on English are definitely bet-ter than on German; 2) Development and test sets presenta regular behaviour in German, while for English they aresignificantly different.
We find the latter particularly dis-appointing because it is indicating that no reliable con-clusions can be extracted about the generalization errorof the NEE system constructed, by testing it on a 3,000sentence corpus.
This may be caused by the fact that thetraining set is no representative enough, or by a too biasedlearning of the NEE system towards the development set.Regarding particular categories, we can see that for En-glish the results are not extremely dissimilar (F1 valuesfall in a range of 10 points for each set), being LOC andPER the most easy to identify and ORG and MISC the mostdifficult.
Comparatively, in the German case bigger dif-ferences are observed (F1 ranges from 52.58 to 80.79 inthe test set), e.g., recognition of MISC entities is far worsethan all the rest.
Another slight difference against Englishis that the easiest category is PER instead of LOC.In order to allow fair comparison with other systems,table 3 presents the results achieved on the developmentset without using external knowledge.
The features usedcorrespond to the basic model plus Part-of-Speech infor-mation (plus Chunks for German), plus a gazetteer buildwith the entities appearing in the training corpus.Precision Recall F?=1English devel.
90.34% 90.21% 90.27English test 83.19% 85.07% 84.12German devel.
74.87% 60.77% 67.09German test 74.69% 63.16% 68.45Table 3: Overall results using no external knowledgeAcknowledgmentsThis research has been partially funded by the EuropeanCommission (Meaning, IST-2001-34460) and the Span-ish Research Dept.
(Hermes, TIC2000-0335-C03-02; Pe-2g refers to a gazetteer containing only entities appearing inthe training set while G includes also external knowledgeEnglish devel.
Precision Recall F?=1LOC 95.33% 94.39% 94.86MISC 89.94% 83.41% 86.55ORG 86.98% 88.14% 87.56PER 91.79% 94.68% 93.21Overall 91.51% 91.37% 91.44English test Precision Recall F?=1LOC 88.14% 90.41% 89.26MISC 82.02% 75.36% 78.54ORG 78.40% 80.43% 79.41PER 86.36% 91.65% 88.93Overall 84.05% 85.96% 85.00German devel.
Precision Recall F?=1LOC 75.72% 73.67% 74.68MISC 72.34% 42.48% 53.52ORG 76.89% 63.82% 69.75PER 83.84% 68.88% 75.63Overall 77.90% 63.23% 69.80German test Precision Recall F?=1LOC 70.31% 70.92% 70.61MISC 64.91% 44.18% 52.58ORG 71.70% 54.08% 61.65PER 87.59% 74.98% 80.79Overall 75.47% 63.82% 69.15Table 4: Final results for English and Germantra - TIC2000-1735-C02-02).
Xavier Carreras holds agrant by the Catalan Government Research Department.ReferencesX.
Carreras and L. Ma`rquez.
2001.
Boosting Trees forClause Splitting.
In Proceedings of the 5th CoNLL,Tolouse, France.X.
Carreras, L. Ma`rquez, and L. Padro?.
2002a.
NamedEntity Extraction using AdaBoost.
In Proceedings ofthe 6th CoNLL, Taipei, Taiwan.X.
Carreras, L. Ma`rquez, V. Punyakanok, and D. Roth.2002b.
Learning and Inference for Clause Identifica-tion.
In Proceedings of the 14th European Conferenceon Machine Learning, ECML, Helsinki, Finland.R.
E. Schapire and Y.
Singer.
1999.
Improved BoostingAlgorithms Using Confidence-rated Predictions.
Ma-chine Learning, 37(3).R.
E. Schapire.
2002.
The Boosting Approach to Ma-chine Learning.
An Overview.
In Proceedings of theMSRI Workshop on Nonlinear Estimation and Classi-fication, Berkeley, CA.
